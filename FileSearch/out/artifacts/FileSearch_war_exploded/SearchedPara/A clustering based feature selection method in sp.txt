 1. Introduction
One of the determinant issues in the performance of speech recognition systems is the proce ss of acoustic representation of speech signals. Successful examples of audio representations are Mel scaled frequency cepstral coefficients ( Davis and Mermelstein, 1980 ) and spectro-temporal features ( Chi et al., 2005 ; Mesgarani et al., 2006 ; Mesgarani et al., 2008 ) that are both inspired from human hearing models. In particular , spectro-temporal features use a simplified model of human brain co rtical stage after successful modeling of internal ear functionalities. Although there had been some modeling investigations on internal ear ( Yang et al., 1992 )and auditory cortical system ( Wang and Shamma, 1995 ) for engineering applications; they had not been employed in engineering applications for ten years. Recently, a comput ational auditory model has been obtained according to neurology, biology and investigations at various stages of the auditory system of brain ( Chi et al., 2005 )andhasbeen developed in various applications such as phoneme classification ( Mesgarani et al., 2008 ), voice activity detection ( Mesgarani et al., 2006 ; Valipour et al., 2010 ), speaker separation ( Elhilali and Shamma, 2004 ; Rigaud et al., 2011 ), auditory attention ( Shamma et al., 2011 ) and speech enhancement systems ( Mesgarani and Shamma, 2005 )in recent years. This model has two mai n stages. In the stage of auditory modeling, an auditory spectrogram is extracted for the input acoustic signal. In the next stage, the spect ro-temporal features of speech are extracted by applying a set of two dimensional spectro-temporal receptive field (STRF) filters on the spectrogram. STRF filters are scaled versions of a two dimensional impulse response ( Chi et al., 2005 ). It is observed that modified versions of these features are more robust in noisy environments in comparison to cepstral coefficients ( Bouvrie et al., 2008 ). The main drawback of spectro-temporal analysis is the large number of extracted features which may affect the parameter estimation accuracy in the training phase of a speech classifier. Some methods such as P CA, LDA and neural networks are used to reduce the number of features in spectro-temporal domain ( Mesgarani et al., 2006 ; Meyer and Kollmeier, 2011 ). These methods are general feature selection methods. Therefore, these methods are not exactly compatible with the speech classification problems.
In addition, there are some approaches which try to find out the best 2D impulse response (best scale-best rate) to extract the appropriate features ( Mesgarani et al., 2008 ).

This study is motivated by the clustered behavior of informa-tion in the spectro-temporal domain. In fact, the phonemes X  information is concentrated in the specific parts of the spectro-temporal features space. In other words, it is desirable to represent the phoneme as the parameters of a number of clusters in the spectro-temporal domain. Some studies have shown that the space of MFCC features is not properly clustered ( Kinnunen et al., 2001 ). It means that this space does not have distinct clusters to represent the short time properties of an uttered speech. Therefore, applying clustering methods to MFCC may be just considered as a kind of space coverage. In contrast, there are some domains that clustering results in better secondary features for signal representation and classification ( Yu and Kamarthi, 2010 ; Ahmed and Mohamad, 2008 ; Yu et al., 2007 ). In this study, it is tried to study the effect of clustering to represent the speech in spectro-temporal domain. It will be shown that in the new clustered features space, phonemes are more separable. ture model (GMM) ( Duda et al. 2001 ), K-means and weighted
K -means (WKM) clustering ( Kerdprasop et al., 2005 ), support vector clustering ( Ping et al., 2010 ) and scale space statistics clustering ( Sakai and Imiya, 2009 ). Clustering may be used either as a classification tool for audio and speech signals ( Dhanalakshmi et al., 2011 ) or as a tool to extract and select a set of secondary acoustic features. This study is focused on the second approach.
Two clustering methods are studied to reduce the spectro-tem-poral features into a few effective secondary features for each frame. GMM and WKM clustering algorithms are shown to be useful in many practical image segmentation applications ( Blekas et al., 2005 ; Abras and Ballarin, 2005 ). Specifically, GMM is a good choice to model irregular data well. Therefore, in this paper, spatial GMM is employed to cluster the feature space as a feature reduction approach. Spatial GMM input vectors include the posi-tion attributes in addition to the representation attributes at that point. This makes the vector large and may lead the system to have inaccuracy problems in parameter estimation. To reduce the size of the vectors in the clustering procedure, the vectors should be weighted due to their importance in the representation of the corresponding frame. Therefore, WKM clustering algorithm is investigated as another clustering method which may be useful to cluster the spectro-temporal space.
 representation is briefly discussed in Section 2 . The proposed secondary feature extraction algorithm for phonemes using the behavior of GMM and WKM clusters in spectro-temporal domain is presented in Section 3 . The proposed features are experimentally evaluated in the features space an d tested on a phoneme classifica-tion task in Section 4 . The paper is concluded in Section 5 . 2. Spectro-temporal feature representation mathematical model of internal ear and the first layer of auditory brain section that are used for speech processing applications in recent years. The block diagram of the auditory model is shown in
Fig. 1 . 2.1. The primary stage of the auditory model
When an audio signal enters the ear, the neural sensors of the basilar membrane of the cochlea convert one dimensional audio signal into a two-dimensional auditory spectrogram image which the frequency axis of this 2D image is a tonotopic (nearly logarithmic) axis. Basilar membrane can be considered as a band-pass filter bank. This filter bank includes 128 asymmetric band-pass filters with the impulse response h cochlea  X  t ; f  X  which are uniformly distributed along the tonotopic axis. The cochlear filter outputs y cochlea ( t , f ) are converted into auditory nerve patterns y  X  t , f  X  by an inner hair cell stage (IHC). IHC stage consists of a high-pass filter in time domain, an instantaneous nonlinear compression g hc (.) and a time domain low-pass filter m hc
The last part of this stage is a model of lateral inhibitory network (LIN) activity, which increases the frequency selectivity of the cochlear filters. LIN is approximated by a first order derivative along the tonotopic axis. The output of this stage y LIN ( t , f )is obtained by using a half wave rectifier to remove the negative outputs and the final output is approximated by integrating y
LIN ( t , f ) during a short time window with an impulse response mathematical formulation of this stage is formulated as below. y cochlea  X  t , f  X  X  s  X  t  X  n h cochlea  X  t ; f  X  X  1  X  y  X  t , f  X  X  g hc y
LIN  X  t , f  X  X  max y  X  t , f  X  X  y LIN  X  t , f  X  n m midbrain  X  t ; t  X  X  4  X  2.2. The cortical stage of auditory model
The primary auditory stage of the brain analyzes the auditory spectrogram as an image. At this stage, a two-dimensional wavelet transform of auditory spectrogram is calculated. This transform is performed using a spectro-temporal mother wavelet, similar to a two-dimensional Gabor function. In other words, the spectral and temporal modulation contents of the auditory spectrogram are estimated via a bank of modulation-selective 2-D filters. Each filter is tuned to a range of spectral X  X emporal modulations.
Spectro-temporal impulse responses of these filters are called spectro-temporal response fields (STRFs). There are two primitive 2-D STRF types which are named upward (  X  ) and downward ( ), respectively which are demonstrated as positive and negative rates, respectively. Therefore, the cortical representation of speech has four dimensions, scale ( O in cycles/octave), rate or velocity ( o in Hz), frequency ( f , the number of the band-pass filter) and time ( t , the frame number). Each of STRFs in the bank of directional selective filters can be generated by multiplying two uncoupled complex functions of time and frequency. The resulted STRF is the real part of this multiplication. 3. Block diagram of the auditory model
The output of each branch of the filter-bank may be modeled as r  X  t , f ; o , O ; y , j  X  X  y  X  t , f  X  n t , f STRF  X   X  t , f ; where o and O are the rate and scale parameters of the filters. In addition, y and j denote feature characteristic phases that show the degree of asymmetry along time and frequency, respec-tively. r  X  and r can be defined using auxiliary variables z z as below ( Chi et al., 2005 ): r  X  t , f ; o , O ; y , f  X  X  9 z  X  9 cos  X  + z  X  y f  X  X  7  X  r  X  t , f ; o , O ; y , f  X  X  9 z 9 cos  X  + z  X  y  X  f  X  X  8  X  where z  X  and z are the complex responses to upward and downward selective filters which are determined using h rw h sw . 9 z  X  9 and + z  X  are the magnitude and phase of the complex responses of upward selective filters and 9 z 9 and + z are the magnitude and phase of the complex responses of downward selective filters. z  X  t , f ; O , o  X  X  y  X  t , f  X  n tf  X  h n rw  X  t ; o  X  h sw z  X  t , f ; O , o  X  X  y  X  t , f  X  n tf  X  h rw  X  t ; o  X  h sw above equation, h rw and h sw are low-pass equivalent of the impulse responses h r and h s for different rates and scales in positive frequencies and times. h  X  t ; o  X  X  h r  X  t ; o  X  X  j ^ h r  X  t ; o  X  X  11  X  h  X  f ; O  X  X  h s  X  f ; O  X  X  j ^ h s  X  f ; O  X  X  12  X  here, ^ h r  X  t ; o  X  and ^ h s  X  f ; O  X  are Hilbert transforms of h h ( f ; O ). The latter functions are obtained from scaling Gamma function and second derivative of a G aussian function, respectively: h  X  t ; o  X  X  o 4 t 3 e 4 o t cos  X  2 po t  X  X  13  X  h  X  f ; O  X  X  O  X  1 O 2 f 2  X  e o 2 f 2 = 2  X  14  X 
It was shown that y and f can be ignored for speech representation in classification tasks ( Mesgarani, 2005 ). In this paper, the magnitudes of z  X  and z , are used for phoneme classification purposes.

The rate-frequency representation of cortical output for different scales of one frame of phoneme/g/ is shown in Fig. 2 .
This representation shows that the concentration of energy is in the mid-range of the frequency axis. The right panel with positive rates is the response of upward filters and the left panel with negative rates is the downward ones. As another example, the rate-scale cortical representation of phoneme/aa/ is shown in
Fig. 3 . As it is shown, the clustered behavior of the space may be observed in this section of the space too. The main motivation of this study is to extract the attributes of these clusters as secondary high level features for classification.

It has been shown that selective STRFs in input auditory cortex have led the researchers to good discriminative features for classi-fication of phonemes which is consistent with physiological findings ( Mesgarani et al., 2008 ). However, the dimensions of this feature space and the degrees of freedom in this space are very large which may bring the system to the curse of dimensionality limitation in the training phase of a speech recognition system. Therefore, the reduction of features space dimensions is a crucial task to train the parameters of artificial speech classifiers efficiently. 4. Phoneme feature extraction using the clustered behavior in the spectro-temporal space 4.1. Overall architecture speech frame was calculated. Then, the spectro-temporal features were extracted using previously described auditory spectrogram andauditorycortexmodel.Theinformationintheoutputofthe cortical stage for each frame was distributed in the three dimen-sional space of frequency, rate and scale axes. Because of large dimensions of spectro-temporal features space, a clustering block was added in the proposed model after the spectro-temporal representation block to nonlinearly transform the spectro-temporal features space attributes into s econdary features with valuable discriminative information. The idea of clustering for extraction of high level secondary features has previously been employed for image and video segmentation tasks ( Ahmed and Mohamad, 2008 ;
Yu et al., 2007 ) where foreground and background deterministic objects have been extracted using spatial color Gaussian mixture models (SCGMM). In another study, the wavelet features of sta-tionary images have been clustered to calculate new secondary features ( Yu and Kamarthi, 2010 ). In all of the applied approaches, a probabilistic clustering algorithm has been fit to deterministic extracted data of the image signal, which is just similar to our proposed model for 4-D spectro-temporal speech space to find out the concentration points of energy in this space.
 clusters using GMM or WKM clustering algorithms. As a result, by determining the main clusters in each speech frame, new secondary feature vectors were ext racted with reduced dimensions.
Gaussian mixture model is one of the common methods for unsupervised classification. In this method, each cluster is mod-eled as a single Gaussian probability distribution function in the input space and the probability distribution of the features space is expressed as a linear combination of K Gaussian mixtures.
GMM parameters are mean vectors, covariance matrixes and mixing weights that are estimated in EM iterative method to fit the model to the dataset ( Duda et al. 2001 ).

In WKM clustering method, C clusters are distributed in the space to model the space. Each sample in the feature space is assigned to the nearest cluster. In addition, a weight is assigned to each feature vector, which is determined according to the impor-tance of each sample in the quality of the clustering overall fitness function ( Kerdprasop et al., 2005 ). In this clustering algorithm, the weight of each point is considered in the clusters estimation, not in the recruitment phase of clusters. The weight of each point may be interpreted as soft repetition of the vector in the cluster center calculation. We used a fixed weight for each point in spectro-temporal space for WKM clustering purpose. The magnitude component of each point was considered as the weight in WKM algorithm to emphasis on high energy points of the space in the clustering procedure.

The extracted secondary features are sorted in descend by their estimated weight in the clustering algorithm. In GMM case, the magnitudes of the estimated clusters were considered as clusters weights. In contrast, in WKM case, the weight of the cluster was defined as the mean magnitude of all recruited samples for each cluster.
 4.2. Spectro-temporal extracted features
To cluster the spectro-temporal space using GMM, the outputs of the upward and downward cortex STRFs (two amplitudes and two phases of the complex outputs) was calculated for each point of four-dimensional space of scale ( O in cycles/octave), rate ( o in Hz), frequency and time as the coordinates of the auditory output using Eqs. (9) and (10). As it is common in spectro-temporal space literature, the downward paramet ers may be represented as nega-tive rates to make a unified 3D space for each frame. In other words, to combine both upward and downward STRF responses, the output of both sets of filters were concatenated in the rate axis. Therefore, in each frame, two 3D cubes with rate axis, scale axis and frequency axis were combined to have a new 3D cube with twice lengthened in the rate axis. Scale, rate and frequency are the coordinates of the location at the new space in each frame which should be considered as the primary feature vectors.

In this paper, the auditory spectrogram was obtained using an infinite impulse response filter bank with 128 frequency channels between 180 and 7246 Hz at the resolution of 24 channels per octave. In addition, a time constant of 8 ms was used for the leaky time integration and filter-bank outputs were sampled every 4 ms to compute the auditory spectrogram. Temporal parameter of the filters (rate), ranging from 2 to 32 Hz and spectral parameter of the filters (scale), ranging from 0.25 to 8 cycle/octave, were considered to represent the spectro-temporal modulations of the speech signal. Thus, the dimensionality of the resulted spectro-temporal feature space became very large (11 (scale filters) 18(rate filters) 128 (frequency channels)  X  25,344 attributes). 4.3. Secondary feature vectors
The secondary feature extraction mechanisms in each frame are shown in Fig. 5 using two clustering methods (GMM and WKM). In the first stage of the proposed method, the samples in the primary feature space (denoted as the vector v i ) were applied to the clustering algorithm to extract secondary features. Each point in the input space was represented as a five dimensional v rate, s is the scale, f is the frequency, A is the magnitudes and C denotes the phase of the output of upward or downward STRFs at each points of the space ( C  X  + z 7 ). As another approach, in WKM clustering, the magnitude components of points were considered as the weighting factors of input vectors. In this case, the input feature vectors had just three dimensions: scale, rate and frequency as v i  X  X  r i , s i , f i  X  . 4.3.1. Secondary feature extraction using GMM clustering
In the regions of spectro-temporal space which the speech signal exists, the magnitude component of points of this region is large. As a result, clusters with less magnitude, have low valuable information. Therefore, the magnitude was considered as one of the main attributes of the clustered vectors. However, the phase was observed to behave uniformly distributed in the feature space and bears no discriminative information.

Assuming that the shapes of clusters are Gaussian, the primary vectors were clustered based on GMM with diagonal covariance matrix. The parameters of the model were determined using EM algorithm and the attributes of all clusters (i.e., the means and variances vectors of the clusters) were considered as secondary features of the frame. It was assumed that the number of clusters was predefined and the center of cluster with larger magnitude was considered as more valuable feature. Therefore, the features were sorted relative to the clusters amplitude. This may cause the system sensitive in noisy conditions; however there is no other evidence to arrange secondary features for single isolated frames.
It will be shown in the experiments that the resulting features are well discriminative and outperform conventional speech features.
Therefore, the resulted secondary feature vectors after GMM clustering was V  X  X  m 1 , m 2 , m 3  X  where each mean vector consists of five components as m i  X  X  m r for each frame. The primary results of this approach were reported in ( Esfandian et al., 2010 ). The other proposed feature set is mean plus variance feature vector. Due to less discriminative character-istics of the phase components in the clustering procedure, they were removed from the feature vector in the mean plus variance case, to avoid the curse of dimensionality problem. In this feature vector, the magnitude components of cluster centers were sorted in descending order and variance components of cluster centers were sorted according to the value of their mean components as four components as m i  X  X  m r
Again, it was assumed that three clusters are enough to represent each frame. Thus, the mean feature vectors had 15 elements and mean plus variance feature vector had 24 elements. 4.3.2. Secondary feature extraction using WKM clustering In the second approach, the feature space was clustered using
WKM clustering. The magnitude components of points were con-sidered as the weight vector and the scale, the rate and the frequency of each point were ass umed as attributes of the three dimensional input vector for clu stering. These primary feature vectors were clustered using WKM algorithm and the centers of clusters were considered as secondary feature vectors. Again, the secondary feature vectors may be considered as the mean vector
V  X  X  m 1 , m 2 , m 3  X  or the mean plus variance vector V  X  X  , s , s 3  X  of the cluster. Each mean vector consists of three components as m i  X  X  m r 9 elements and mean plus variance feature vector had 18 elements. 4.4. Spatial thresholding temporal domain is the processing computational complexity of the secondary feature extraction stage. In another experiment, a thresholding technique was used to reduce the number of feature vectors as the input of the secondary feature extraction stage in each frame. For this purpose, the points of spectro-temporal domain, with the amplitudes that were smaller than an empiri-cally determined threshold, were removed from the input vectors before clustering. 4.5. Computational complexity of the proposed secondary feature extraction summation, multiplication and exponential operations in the sec-ondary feature extraction stage. Computational complexities of two proposed methods are tabulated in Table 1 . In addition, the numbers of operations were calculated for typical values of variables. These variables are t, the numbers of frames for each phoneme, K ,the numbersofclustersand L , the numbers of repeated loops. In addition, the size of data matrix in spectro-temporal domain was assumed to be m n . Computational complexity was estimated for extraction of mean plus variance feature vector with m  X  4forGMM clustering and m  X  3 for WKM clustering; because, scale, rate, frequency and amplitude of each point in spectro-temporal domain was considered in mean plus variance secondary feature vector in
GMM clustering. In contrast, scale, rate and frequency of each point in spectro-temporal domain were considered in mean plus variance secondary feature vector for WKM clustering.
 tions are much higher in GMM in comparison to WKM.

In addition, the exponential operation makes GMM more expen-sive. In experiments, it was shown that WKM computational cost may be even more reduced by spatial thresholding of the non-dominant points of the space. 5. Experimental results 5.1. Experimental setup were conducted on classification of phonemes in main categories of phonemes. Most of experiments are performed on common /b/,/d/,/g/ difficult to process phonemes classifier which is one of hard to discriminate sets of phonemes and it is the benchmark of many studies in this field ( Gas et al., 2004 ; Waibel et al., 1989 ;
Yousefi Azar and Razzazi, 2010 ), however, after investigating the characteristics of the proposed features, the features are evalu-ated on different categories of phonemes in the final achieved system. The ideas are evaluated on clean speech and the pho-nemes are extracted from the whole TIMIT database. TIMIT contains 6300 sentences, where 10 sentences have been spoken by each of 630 speakers from 8 major dialect regions of the United States ( Fisher et al., 1986 ).

In the first stage of the evaluation system, the phonemes were selected from TIMIT database. Then, secondary features were extracted from clustered spectro-temporal space for each pho-neme. After secondary feature extraction procedure, the new feature vectors were applied to two classifiers, a nonlinear SVM classifier with Gaussian kernel and a Hidden Markov Model (HMM) based classifier, for phoneme classification.
 The system was primarily evalua ted on a frame-wise framework.
In this evaluation, each frame w as clustered and the resulted secondary feature vector was extracted for each frame. In the next evaluation strategy, the phoneme classification rate was evaluated.
For this purpose, the extracted s econdary feature vectors of all frames of each phoneme were class ified by using an SVM classifier separately and the final class of each phoneme was determined by majority voting among the phonemes classes of all frames of each phoneme utterance. This mechanism was called voting after classi-fication. In addition, all secondary features of a phoneme were injected to an HMM classifier in a separate evaluation test. The optimum values of RBF-SVM parameters (kernel parameter g and miss-classification cost C ) were empirically determined using a grid search strategy to optimize the classification rate. 5.2. Results 5.2.1. Discriminative analysis
In a feature extraction/selection procedure, it is essential to investigate the discrimination capability of individual resulted features. The discriminative capability of each feature may be assessed using the common discrimination measure, defined as: D where D P 1 , P 2  X  i  X  is discrimination measure of ith attribute of P phonemes m P nents of all frames of P 1 and P 2 . Greater values of D P that these components are more discriminative in phoneme classification.
 Overall discrimination of the proposed features is evaluated in
Table 2 . As it can be observed, the obtained discrimination results of GMM and WKM based features show that both approaches show good discrimination properties; while the extracted WKM based secondary features are slightly more discriminative in comparison to GMM based secondary feature vectors.

In addition, the independency of proposed secondary features was evaluated by principle com ponent analysis of features.
The results show that the energies of eigenvalues are damped in very high components, which is an evidence of approximate inde-pendency of features. In GMM clustering approach, classification results on PCA reduced features show that the recognition rate decreases from 72.9 to 71.8 and 66 .5 while reducing the number of features from 24 to 20 and 10, respectively. In WKM clustering approach, this classification rate reduction is 72.0 to 70.0 and 67.5 while reducing the number of features from 18 to 16 and 10, respectively. Although in many pra ctical solutions, fine tuning of the accuracy is not necessary by adding four last features; to have a state of the art classifier, in the rest of the paper, the full feature vector was assumed for classification. 5.2.2. Classification test
The results of phonemes /b/,/d/,/g/ classification using MFCC features (consisting of 13 MFCC features) and the proposed feature vectors with SVM classifiers are tabulated in Table 3 .In this test, each frame is classified and evaluated in the presented results. The results of /b/,/d/,/g/ phonemes classification using
MFCC features and proposed feature vectors with SVM and HMM classifiers are shown in Table 4 . HMM model was implemented in
Kevin Murphy HMM toolkit ( Murphy, 1998 ). The HMM para-meters are three no-skipping left to right states with sixteen diagonal Gaussian mixtures. These results show that the new feature vectors gave better results in comparison to MFCC features in all dialects. In addition, it is obvious that phoneme classification results using mean plus variance feature vectors gave improved results comparing to mean only feature vectors.
Phoneme classification using WKM clustering approach outper-formed in the phoneme classification task, comparing to GMM clustering in most of TMIT database dialects.

In Fig. 6 , the classification rates of phonemes /b/,/d/,/g/ is shown for MFCC (Mel Frequency Cepstral Coefficients), LPC (linear predictive coding), NPC (neural predictive coding) features ( Gas et al., 2004 ) and they are compared with the mean plus variance feature vector in frame-wise classification. The new feature vector gave considerable better results for phoneme classification.
The results showed us that WKM outperforms GMM in terms of computational complexity and accuracy issues. Therefore the overall test of system on main categories of phonemes was performed on WKM clustering approach. 5.3. Thresholding effect
The extracted mean plus variance feature vectors using thresh-old technique in WKM clustering was obtained from clustering of the refined set of spectro-temporal primary features using the following thresholding equation: v i  X  where A denotes the amplitude in each frame of a phoneme in spectro-temporal domain and T is the threshold value. The results of phoneme classification for a set of threshold values are evaluated. The best results was obtained with T  X  0.1. Although the determination of the threshold was based on trial and error in this study, automatic estimation of the optimum threshold was remained as an open problem for future studies. The number of achieved data samples was decreased by increasing the threshold value, however, the accuracy was not high in very high and very low thresholds. It seems that with increasing the training samples, more clusters should be used for feature extraction. 5.4. Phoneme classification on different categories of phonemes categories of phonemes using MFCC feature vectors and mean plus variance WKM based secondary feature vectors are shown in
Tables 5 and 6 , respectively. In all vowels and consonants subsets, except nasals, the classification rates were improved using the proposed feature vector with respect to MFCC features. made significant improvement in the voiced plosives with respect to MFCC features in terms of classification rate. The absolute improvement in classification rate of voiced plosives comparing to MFCC features is 5.9%. However, in the unvoiced plosives, voiced fricatives and unvoiced fricatives slight improvements were 1.7%, 0.5% and 1.8%, respectively. However, the proposed features vectors did not succeeded to improve nasals classification rate. This may be due to the nature of nasals which is based on a spectral hole rather than a concentration point in the spectro-temporal domain.
 that the improvements were 7.4% and 3.4% for front and back vowels, respectively. The reason of greater improvement in front vowels comparing to back vowels is the intrinsic discrimination of back vowels with respect of front vowels which made the MFCC results to be saturated.

In voiced plosives, the greatest improvement was in phoneme /b/. In unvoiced plosives, the greatest improvement is observable in the phoneme /k/. In voiced fricatives and unvoiced fricatives, the greatest improvements were obtained in /dh/ and /f/, respec-tively. In front vowels, the greatest improvements were achieved in /ih/ and /eh/ phonemes. In back vowels the greatest improve-ments were obtained in /uw/, /uh/ and /ow/ phonemes. 6. Conclusion
In this paper, a new method was presented for spectro-temporal secondary features selection/extraction in order to reduce the features space dimensions. This method is based on clustering in the spectro-temporal domain to extract the main energy concentra-tion points of each acoustic event in the spectro-temporal domain. In the proposed method, the position of clusters is determined using
GMM and WKM at each frame of speech. Two types of feature vectors were used for phoneme classification. The mean vectors and covariance matrices elements of th e clusters were considered in the mean plus variance feature vector of each frame. However, only the mean vectors of clusters are cons idered in the mean feature vector.
In these proposed approaches, a tr aining procedure is required to fulfill the feature extraction for each frame. As a result, this feature extraction method may be regarded as an adaptive trainable feature extraction method which tries to consider the frames conditions.
In addition, as a complementary approach, a thresholding technique was employed to reduce the dimensionality of data matrix in each frame and therefore to reduc e of processing complexity.
The new secondary feature vectors were applied to phonemes classification on phonemes subsets of TIMIT database. The results indicate that the proposed features performed better in the phoneme classification task, comparing to MFCC. In addition, mean plus variance feature vectors give better results in compar-ison to the mean only feature vectors. Comparison of WKM and GMM clustering results show that WKM gives better results in phoneme classification in most of the dialects of TIMIT database. Now the research is in progress to track the spectro-temporal clusters through noisy frames sequence.
 References
