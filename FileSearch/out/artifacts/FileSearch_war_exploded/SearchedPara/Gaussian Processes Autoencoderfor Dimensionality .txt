 Dimensionality Reduction (DR) aims to find the corresponding low dimensional representation of data in a high-dimensional space without incurring significant information loss and has been widely utilized as one of the most crucial pre-processing steps in data analysis such as applications in computer vision [15]. Theoretically the commonly-faced tasks in data analysis such as regression, clas-sification and clustering can be viewed as DR. For example, in regression, one tries to estimate a mapping function from an input (normally with high dimen-sions) to an output space (normally with low dimensions).

Motivated by the ample applications, DR techniques have been extensively studied in the last two decades. Linear DR models such as Principal Compo-nent Analysis (PCA) and Linear Discriminant Analysis (LDA) may be the most well-known DR techniques used in the settings of unsupervised and supervised learning [2]. These methods aim to learn a linear mapping from high dimensional observation to the lower dimension spac e (also called latent space). However, in practical applications the high dimensional observed data often contain highly nonlinear structures which violates the basic assumption of the linear DR models, hence various non-linear DR models have been developed, such as Multidimen-sional Scaling (MDS) [11], Isometric Mapping (ISOMAP) [19], Locally Linear Embedding (LLE) [16], Kernel PCA (KPCA) [17], Gaussian Process Latent Vari-able Model (GPLVM) [9], Relevance Units Latent Variable Model (RULVM) [6], and Thin Plate Spline Latent Variable Model (TPSLVM) [7].
 Among the above mentioned nonlinear DR approaches, the Latent Variable Model (LVM) based DR models attract considerable attention due to their in-tuitive explanation. LVM explicitly models the relationship between the high-dimensional observation space and the low-dimensional latent space, thus it is able to overcome the out-of-sample problems (projecting a new high-dimensional sample into its low-dimensional representation) or pre-image problems (project-ing back from the low-dimensional space to the observed data space). The linear Probabilistic PCA (PPCA) [20] and GPLVM [9] may be the most well-known LVM based DR techniques, where the mapping from the low dimensional latent space (latent variables) to the high dimensional observation space (observed vari-ables) is represented by a linear model and a nonlinear Gaussian Process (GP), respectively. Since the nonlinear DR technique GPLVM performs very well in many real-world data sets, this model has become popular in many applications, such as movement modelling and generating [9]. Meanwhile, many GPLVM ex-tensions have been developed to further improve performance. For instance, the Gaussian Process Dynamical Model (GPDM) [22] allows modelling dynamics in the latent space. The back constraint GPLVM (BC-GPLVM) was proposed in [10] to maintain a smooth function from observed data points to the cor-responding latent points thus enforcing the close observed data to be close in latent space. Other extensions, such as Bayesian GPLVM, shared GPLVM and supervised GPLVM which further extend the classical GPLVM to unsupervised and supervised settings, can be referred to [4,21,8].

The autoencoder [3] can be regarded as an interesting DR model, although originally it is a neural network (NN) architecture used to determine latent rep-resentation for observed data. The idea of autoencoder is to resolve the latent embedding within the hidden layer by training the NN to reproduce the input observed data as its output. Intuitively this model consists of two parts: the encoder which maps the input observed data to a latent representation, and the decoder which reconstructs the input through a map from the latent represen-tation to the observed input (also called output). Basically, the two mappings in the encoder and decoder are modelled by neural network (NN). Recently, au-toencoder has regained popularity becaus e it has been efficiently used for greedy pre-training of deep neural network (DNN) [1].

The relationship between GP and NN was established by Neal [12], who demonstrated that NN could become GP in the limit of infinite hidden units and the model inference may be simpler. With specific covariance function (NN covariance) [14], the back constrai nt GPLVM (BC-GPLVM) can be seen as autoencoders[18], where the encoder is NN and the decoder is GP. The superi-ority of GP over NN lies in small-scale model parameters, easy model inference and training [12,14], and in many real-world applications GP outperforms NN. Motivated by the comparison, we propose the Gaussian Processes Autoencoder Model (GPAM), which can be viewed as BC-GPLVM where GP represents the smooth mapping from latent space to observation space, and also as an au-toencoder where both the encoder and decoder are GPs. It is expected that the proposed GPAM will outperform typica lGPLVM,BC-GPLVMandautoencoder models.

The rest of the paper is organized as follows. In Section 2 we briefly review the GP, GPLVM, and autoencoder models. The proposed Gaussian Processes Autoencoder Model (GPAM) will be introduced in Section 3. Then, real-world data sets are used to verify and evaluate the performance of the newly proposed algorithm in Section 4. Finally, the concluding remarks and comments are given in Section 5. In this and following section, we use the following notations: X =[ x 1 , ..., x N ] T are observed (inputs) data in a high dimensional space R D , i.e., x n  X  X  D ; Y =[ y 1 , ..., y N ] T are observed (outputs or labels) data with each y n  X  X  q ;and with p D where each z n is associated with x n . For the sake of convenience, we also consider X as an N  X  D matrix, Y an N  X  q and Z an N  X  p matrix. observed dataset. Data items are assumed to be i.i.d. Let N (  X ,  X  )denotethe Gaussian distribution with mean  X  and covariance  X  . 2.1 Gaussian Process Gaussian Process Regression (GPR ) is concerned with the case when q =1.It aims to estimate the predictive distribution p ( y | x  X  ) for any test data x  X  .Inthe classical GPR model, each sample y n is generated from the corresponding latent functional variable g with independent Gaussian noise where g is drawn from a (zero-mean) GP which only depends on the covari-ance/kernel function k (  X  ,  X  ) defined on input space and is the additive Gaussian noise with zero mean and covariance  X  2 .

Given a new test observation x  X  , it is easy to prove that the predictive distri-bution conditioned on the given observation is where K s are the matrices of the covariance/kernel function values at the cor-responding points X and/or x  X  . 2.2 Gaussian Process Latent Variable Model (GPLVM) Lawrence introduced GPLVM in [9], including the motivation of proposing the GPLVM and the relationship between PPCA and GPLVM. Here we just review GPLVM from the view of GP straightforwardly.

Given a high dimensional dataset D = { x 1 , ..., x N } X  X  D without any given labels or output data. We aim to obtain the latent/unknown variables z n  X  X  p corresponding to each data item x n ( n =1 , 2 , ..., N ). GPLVM [9] defines a generative mapping from the latent variables z n to its corresponding observed variables x n which is governed by a group of GPs x n = g ( z n )+ where g = [ g , ..., g D ] T is assumed to be a group of D GPs, and is an independent Gaussian noise with zero mean and covariance  X  2 I , which means the likelihood of the observations is Gaussian
Suppose that each GP g i ( i =1 , ..., D ) has the same covariance function k (  X  ,  X  ), then the data likelihood defined by equation (2.2) can be marginalized with respect to the given GP priors over all g d s, giving rise to the following overall marginalized likelihood of the observations X where K = K ZZ +  X  2 I is the kernel matrix over latent variables Z .
The model learning is implemented by maximizing the above marginalized data likelihood with respect to the latent variables Z and the parameters of the kernel function k .

Although GPLVM provides a smooth mapping from latent space to the ob-servation space, it does not ensure smoothness in the inverse mapping. This can be undesirable because it only guarantees that samples close in latent space will be close in data space, while points close in data space may be not close in latent space. Besides, due to the lack of direct mapping from observation space to latent space the out-of-sample problem becomes complicated, meaning that the latent representations of testing data must be optimized conditioned on the latent embedding of the training examples [9]. In order to address this issue, the back constraint GPLVM (BC-GPLVM) was proposed in [10]. The idea behind this model is to constrain latent points to be a smooth function of the corre-sponding data points, which forces points which are close in data space to be close in latent space. The back constraint smooth function can be written by where  X  are parameters of the smooth function. Typically, we can use a linear model, a kernel based regression (KBR) m odel or a multi-layer perception (MLP) model to represent this function. As the function f m is fully parameterized in terms of a set of new parameters  X  , the learning process becomes an optimization process aiming at maximizing the likelihood (2.3) w.r.t. the latent variables X and parameters  X  . 2.3 Autoencoder The autoencoder[3] is based on NN, which will be termed NNAM (NN Autoen-coder Model) for short throughout the paper. Basically it is a three-layer NN with one hidden layer where the input and output layers are the observation data. Our goal is to find the latent representation over the hidden layer of the model through minimizing reconstruction errors. The autoencoder model can be separated into two parts: an encoder (mapping the input into latent represen-tation) and a decoder (reproducing the input through a map from the latent representation to input).

With the above notations, let X  X  define the encoder as a function z = f ( x , X  , and the decoder as a function x = g ( z , X  ). Given a high dimensional dataset D = { x 1 , ..., x N } X  X  D , we jointly optimize the parameters of the encoder  X  and decoder  X  by minimizing the least-squ ares reconstruction cost: where g d (  X  )isthe d th output dimension of g (  X  ). When f and g are linear transformations, this model is equival ent to PCA. However, nonlinear projec-tions show a more powerful performance. This function is also called the ac-tive function in NN framework. In this paper we use the sigmoidal function f ( x , X  )=(1+exp(  X  x T  X  ))  X  1 as the active function. The model can be opti-mized by gradient-based algorithms, such as scaled conjugate gradient (SCG). Based on the relationship between GP and NN, we introduce the detailed model inference of Gaussian Processes Autoencoder Model (GPAM). The fundamental idea of this novel model is to use Gau ssian Process (GP) to replace Neural Networks (NN) that was originally used in autoencoder.

Given a high dimensional dataset D = { x 1 , ..., x N } X  X  D without any labels or output data, where each sample x n is assumed to be associated with the la-tent/unknown variables z n  X  X  p ( n =1 , 2 , ..., N ). Our goal is to find these latent variables which should clearly show the intrinsic structures of the observation data for data visualization or preprocessing.

The idea behind GPAM is to define a mapping from the observation variables x n to the corresponding latent variables z n (encoder) and a mapping from the latent variables z n to the corresponding observation variables x n (decoder) by using Gaussian Processes Regressions (GPRs) defined as follows and D GPs with hyperparameters  X  and  X  , respectively, and both 1 and 2 are the independent Gaussian noises with zero mean and covariance  X  2 I .Thusitis easy to see that the likelihood of the observations is Gaussian, P ( Z | f ,X, X  )=
Let X  X  further assume that both functions f and g are nonlinearly modelled by GPs By marginalizing over the unknown functions f and g ,wehave with K X = K X,X +  X  2 1 I and K Z = K Z,Z +  X  2 2 I where K X,X and K Z,Z are the covariance matrices defined over the input data X , and the latent variables Z , respectively.

Furthermore, in order to do model inference let X  X  assume that the input X of encoder function f is different from the output X of decoder function g , whichisrewrittenby X c . Thus the notation of marginal likelihood P ( X | Z,  X  ) can be changed to P ( X c | Z,  X  ). Based on the conditional independence prop-erty of graphical model the posterior distribution over latent variables Z given observation ( X,X c ) can be derived as follows
In order to learn the unknown variables ( Z,  X ,  X  ), we maximize the log poste-rior distribution P ( Z | X,X c , X , X  ) (3.4) w.r.t. ( Z,  X ,  X  )
For the sake of convenience, we simply denote the negative log posterior dis-tribution P ( Z | Y,X, X , X  )by where P ( X c | X, X , X  ) has been omitted because it is irrelevant to Z .
The process of model training is equal to simultaneously optimizing a GPR (corresponding to the encoder distribution P ( Z | X, X  )) and a GPLVM (corre-sponding to the decoder distribution P ( X c | Z,  X  )). To apply a gradient based op-timization algorithm like SCG algorithm to learn the parameters of the model, we need to find out the gradient of L w.r.t. the latent variables Z , and the kernel parameter (  X , X  ).

Firstly for the part of GPR corresponding to P ( Z | X, X  ) we can simply obtain the following gradients
As for the parameter  X  in kernel K X , since we consider the output of the mapping z = f ( x , X  ) as the known quantity in the GPR model, the optimization process is identical to the procedure of determining parameters for a typical GPR model from training data. Thus we can derive the partial derivative of the hyperparameter  X  by chain rule (refer to Chapter 5 in [14])
Subsequently for the second part of GPLVM corresponding to P ( X c | Z,  X  )it is easy to evaluate the gradients of L l w.r.t. the latent variables Z where the gradients of log likelihood w.r.t. kernel matrix K Z is evaluated by Similarly the gradient of L l w.r.t. the hyperparameter  X  can be calculated by and the computation of the derivative of the kernel matrix w.r.t. the latent variable Z and hyperparameter depend on a specific kernel function.
By combining equations (3.7) with equation (3.8) and (3.9), it is quite simple to get the complete gradients of L w.r.t. the latent variables Z (  X  X / X  X  ). Once we get all the derivative ready, the derivative based algorithms like SCG can be utilized to iteratively optimize these parameters. However, when we perform experiments, we find that the value of L r (corresponding to the encoder distri-bution P ( Z | X, X  )) is much smaller than that of L l (corresponding to the decoder distribution P ( X c | Z,  X  )), leading to very little performance improvement com-pared to GPLVM. Thus we propose a novel algorithm to train the model based on two-stage optimization; this is to say, we try to asynchronously optimize the model consisting of GPR and GPLVM rather than simultaneously learn it. The algorithm is detailed in Algorithm 1.
 Algorithm 1. Train and Test GPAM
To sum up, there are two ways to view the proposed GPAM. Firstly, it can be seen as the generalization of classic NNAM. While GPAM makes use of GPR model to encode and decode the data, NN is utilized to do encoding and decoding in classic NNAM. Based on the superiority of GP over NN, we believe that the proposed GPAM will outperform typical NNAM. Secondly, the proposed GPAM can also be considered as the BC-GPLVM where the back constrain function is modelled by GPR. Compared to classic BC-GPLVM, such as the KBR or MLP based models, the smooth mapping from the observation space to the latent space in the proposed GPAM is modelled by GPR, which results in better performance than typical KBR and MLP based BC-GPLVM. In this section, we compare the proposed GPAM with original GPLVM [9], BC-GPLVM [10] and NNAM [3], in two real-world tasks to show the better perfor-mance that GPAM provides. In order to assess the performance of these models in visualizing high dimensional data sets, we perform dimensionality reduction by using a 2D latent space for visualization. Moreover, the nearest neighbour classification error is tested in the low d imensional latent space to objectively evaluate the quality of visualization for training data. After the DR models are learnt, we further use them as feature extraction, followed by a k-Nearest Neigh-bour (kNN) classifier for testing data. Of course we can use other classifier such as GP Classifier (GPC) rather than a simple kNN to classify the testing data, but the final goal is to reduce the dimensionality of the observation, and the learnt low-dimensional data would be utilized for other proposes, such as data visualization and compression, so the simple kNN classifier is better to evaluate the quality of DR models. By comparing t he classification accuracies in low di-mensional latent space for testing data, we demonstrate the improvement of the proposed model again. The experimental results verify that the proposed GPAM is an efficient DR model and outperforms GPLVM, BC-GPLVM and NNAM.
For a fair comparison, we ran 500 iterations for all the models, and the co-variance used for GPLVM, BC-GPLVM and GPAM was optimally selected from RBF(ARD), POLY(ARD), and MLP(ARD) in Neil D. Lawrence X  X  MATLAB packages Kern . The back constraint function of BC-GPLVM is manually picked from KBR and MLP. The code GPLVM/BC-GPLVM and NNAM are based on Neil D. Lawrence X  X  MATLAB packages FGPLVM 1 , and R. B. Palm X  X  Deep Learning Toolbox 2 [13], respectively. Since the learning rate of NNAM needs to be selected manually, we varied it between 0.1 and 10 optimally with sigmoidal active function. 4.1 Oil Flow Data The oil flow data set [17] consists of 12 dimensional measurements of oil flow within a pipeline. There are 3 phases of flow associated with the data and 1000 samples in the data set. For all four models, we use 600 samples (200 points from each class) to learn the corresponding 2D latent data for the purpose of data visualization, and the remaining 400 samples are the testing data. RBF covariance function is used for GPLVM/BC-GPLVM (MLP back-constraint) and GPAM (RBF covariance for both GPR and GPLVM in the model). As can be seen from Figure 1, the proposed GPAM is superior to GPLVM/BC-GPLVM and NNAM remarkably because the novel model makes the points in the latent space which belong to the same class in the original feature space much closer than the rest of three models.

Furthermore, in order to objectively evaluate the new DR technique we com-pare the nearest neighbour errors and the classification accuracies based on kNN classifier in the learnt 2D latent space provided by the four models on this data set, respectively. All the four DR models are firstly learnt from training data with the 2D latent space corresponding to the training data where the nearest neighbour errors are evaluated, and then based on the trained four DR models the testing data will be projected to the low dimensional latent/feature space (2D in our experiments) where kNN is pe rformed to compare the testing accu-racies ( K = 10 in kNN). Table I tells us that the proposed GPAM outperforms GPLVM/BC-GPLVM and NNAM in terms of nearest neighbour errors and clas-sification accuracies for training and test ing data respectively, which verifies that the novel DR model is better than the other three techniques. 4.2 Iris Data The Iris data set [5] contains three cl asses of 50 instances each, where each class refers to a type of iris plant. The re are four features for each instance. All 150 data points are utilized to learn the 2D latent space. POLY covariance achieves the best results than the other two covariance functions (RBF and MLP) for GPLVM/BC-GPLVM (MLP back-constraint) and GPAM (POLYARD and POLY covariances for GPR and GPLVM re spectively). Figure 2 and Table II show the same conclusion as stated for the oil flow data set. Since there is no more testing data for this data set, the classification comparison for testing is not given. As for the model complexity, we have to admit that the training algorithm of GPAM is time-consuming compared to GPLVM, BC-GPLVM and NNAM due to the two-stage optimization. However, in the testing step GPAM is as fast as BC-GPLVM and NNAM without iterative optimization like classical GPLVM. In this paper a novel LVM-based DR technique, termed Gaussian Processes Autoencoder Model (GPAM), has been introduced. It can be seen as the gen-eralization of classic Neural Network Autoencoder Model (NNAM) model by replacing the NNs with GPs, leading to sim pler model inference and better per-formance. Also, we can view the new mo del as the back constraint GPLVM where the smooth back constraint function is represented by GP, and the model is trained by minimizing the reconstruction error. The experimental results have demonstrated the performance of the newly developed model.

For the future work, inspired by recent works in deep learning [1] we will ex-tend the GP Autoencoder to sparse and denoising GP Autoencoder models, and then we also want to study the deep GP model by stacking the GP Autoencoder. Acknowledgments. Xinwei Jiang X  work is supported by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan). Junbin Gao and Xia Hong X  X  work is supported by the Australian Re-search Council (ARC) through the grant DP130100364.

