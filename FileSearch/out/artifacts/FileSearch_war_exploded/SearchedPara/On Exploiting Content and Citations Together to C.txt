 Department of Computer Software, In computing the similarity of sc ientific papers, previous text-based and link-based similarity meas ures look at only a single side of the content and citations. In this paper, we propose a novel approach called SimCC that effectively combines the content and citation information to accurately compute the similarity of scien-tific papers. Unlike previous approaches, SimCC effectively rep-resents both authority and context of a scientific paper simultane-ously in computing similarities. Also, we propose SimCC+A to consider recently-published papers. The effectiveness of our pro-posed method is demonstrated via extensive experiments on a real-world dataset of scientific papers, with more than 100% im-provement in accuracy compared with previous methods. H.2.8 [ DATABASE MANAGEMEN ]: Database application X  Data mining Authority, Citation, Content, Scientific Papers, Similarity 
Scientific papers are primary sources to share information and knowledge among scholars. Scientif ic literature search engines such as CiteSeerX and Google Scholar aid researchers to find papers in their areas of interest and to make sure through search or recommendation services, whether their research problems are novel. Similarity measure is one of the challenging issues in these systems to find relevant papers according to the user requirement. 
To compute the similarity of sc ientific papers, text-based and link-based similarity m easures can be applied. Text-based similar-ity measures such as Cosine [7], Dice Coefficient [7], Kullback-Leibler Distance (KLD) [1], and BM25 [8] focus on the content of the papers but neglect the cita tion relationship between them. Link-based similarity measures su ch as SimRank [5], P-Rank [14], and rvs-SimRank [14] consider the citation relationship be-tween scientific papers but ignore the content. Citations are se-lected manually by the authors according to the content, so con-tent and citations are interrelated in scientific papers. However, previous text-based and link-based similarity measures only put emphasis on a single aspect of sc ientific papers to compute the similarity. In [2, 12, 13], the content of a target paper is extended by adding terms from those papers that have citation relationship with it. These methods simply combine the content and citation information together. In [9], similarities based on citation and content are computed separately and their results are combined into the one. This method, however, ignores the relationship be-tween citation and content information. 
In this paper, we propose a novel approach called SimCC that effectively combines the content and citation information in order to accurately compute the similarity of scientific papers. Unlike [2, 9, 12, 13], SimCC combines th e content and citations in order to effectively represent the authority and context of a scientific paper simultaneously by exploiting their relationship by carefully analyzing the citation graph. The intuition behind our method is that, the number of citations to a paper is not enough to under-stand its authority. Rather, it is more important to know how much the paper contributes to other p apers which cited it and enriches their context . We define this as a contribution score that indicates the authority of a paper on a sing le term. The contribution score is the key factor in our proposed me thod and is computed for every term in a paper independently . To compute the contribution scores, all required processes are done offline , so there is no extra computation overhead in query execution time. To evaluate the effectiveness of SimCC, we perf orm extensive experiments with a real-world dataset. Our experimental results show that the accura-cy improves dramatically by using SimCC. Also, we propose SimCC+A to improve the accuracy of SimCC for recently-published papers that have less chance to obtain citations from other papers. 
The text-based similarity measures look at a paper as a bunch of terms and two papers are considered more similar if they have more common terms. In the literat ure, there are various types of text-based similarity measures. Cosine [7] considers the similarity between two papers as the cosine of the angle between their vec-tors. Dice Coefficient [7] computes the similarity of papers ac-cording to their vector representations. Kullback-Leibler Distance (KLD) [1] looks at a paper as a probability distribution over all existing terms in the dataset and computes the distance between two papers according to these pr obabilities. BM25 [8] is another probabilistic similarity measure that puts more emphasis on the term frequency and paper le ngth to compute similarity. 
The text-based similarity measures only use the content of a pa-per. Content is important information that can be used to find similar papers. However, it cannot represent the authority of the paper. According to the content, tw o papers can be similar, even if one of them is much more aut horitative than other one. Hence, they are not appropriate to comput e the similarity of scientific papers because they ignore the existing underlying relationship between content and citation information. 
The link-based similarity measur es compute the similarity of scientific papers by considering the citation graph where nodes denote the papers and edges repr esent the citation relationships. SimRank [5], P-Rank [14], and rvs-SimRank [14] are most fa-mous link-based similarity measures in the literature. SimRank considers in-links recursively and neglects out-links, so similarity between two papers is only base d on the number of papers that cite both of them. P-Rank considers both the in-links and out-links recursively to compute similarity . P-Rank provides a unified simi-larity measure such that other link-based similarity measures such as SimRank are its special cases. rvs-SimRank is another special case of P-Rank that computes sim ilarity of papers only based on the out-links recursively, so similarity between two papers is only based on the number of papers cited by both of them. 
The link-based similarity measures work on the citation graph to compute the similarity of scientific papers by ignoring the content. However, citations cannot sufficiently represent the content of a paper. Like text-based similarity measures, we argue that the link-based similarity measures are not appropriate to compute the simi-larity of scientific papers. However, the text-based similarity measures compute the similarity of scientific papers more accu-rately than the link-based similarity measures [10].
To improve the accuracy of sim ilarity measures on scientific papers, some previous studies tried to combine the content and citation information. In [2, 12, 13 ], the content information of a paper is enriched by adding the terms from those papers which have citation relationship with the paper. CEBC [2] and Key-words-Extension [13] combine the TF/IDF values of the terms in the target paper with the TF/IDF values of its neighbor papers in the citation graph, without appl ying any weighting schemes; how-ever, in [12], Cosine similarity between papers is used as the weighting scheme for combination. First, all these methods take into account only directly connec ted papers and do not exploit the citation graph effectively. Sec ond, the linear combination of TF/IDF values is used for all terms according to the same weighting scheme without considering the importance of the terms in the target paper and it s directed neighbors. In others words, by simply combining the content and citation information, the underlying relationship between citations and content is not considered effectively. In [9], the text-based and link-based simi-larities of web pages are com puted independently and are com-bined together by a linear combination. It is clear that the same approach can be taken for the scie ntific papers. This method takes into account both the content and citations, however, completely neglects their underlying relation ship. Therefore, this method is not effective to compute the simi larity of scientific papers. 
Scientific papers contain two interrelated information: content and citations. Therefore, to impr ove the accuracy of similarity measures, both of them should be taken into account by consider-ing their relationships. The conten t of a paper represents its con-text that can be used to find similar papers. On the other hand, the citations are selected manually by the authors as a set of related and authoritative scientific works. Hence, citation relationship can be also applied to supplement and enrich the content of a paper. In other words, a paper can contribute to those papers which cited it. Unlike some works such as [11], we argue that the number of citations to a paper is not enough to indicate its authority. Rather, it is more important to understand how much the paper contributes to other papers which cited it and enrich their context. We define this as a contribution score that measures the authority of a paper on a single term indicating how much a paper contributes to an-other paper on the term. The contribution score is a key factor in our proposed method, called Sim CC. SimCC is a novel method that effectively combines the citation and content information of scientific papers in order to enrich the paper representation by considering the context and authority of a paper simultaneously . Previous approaches [2, 12, 13], simply combine the content and citation information and extend the content of a paper by adding new terms from neighboring papers in the citation graph. Instead, SimCC improves the content of a paper by reflecting the authority of the paper according to the citation information. 
We explain the SimCC working mechanism by a simple exam-ple. Assume that paper q cites paper p . In order to indicate the authority of paper p, we compute the contribution score of every term t in paper p to paper q .Then, to compute the SimCC score of term t , we add contribution score of term t with its relevance score. Relevance scores are TF/IDF values of the paper terms and only represent the content of the paper. Therefore, the SimCC and its contribution score to paper q , and represents both the con-tent and authority of paper p on term t. To compute the contribu-tion score of a term in a paper, all required processes are done offline . Consequently, there is no extra computation overhead in the query processing time. In or der to compute the contribution score of paper p to paper q on term t , we consider some intuitions: First, if, in overall , q is a valuable and authoritative paper on t, the probability that p is an important and valuable paper on t is in-creased. Likewise, if, in overall , q is not a valuable and authorita-tive paper on t, with a high probability, p is not an important and valuable paper on t either. Second, if p is more valuable than q on papers including p on t , the contribution of p to q is decreased because the authors of q cited a number of other papers to sup-plement the content of q on t . 
The SimCC score of a term in a paper is obtained by the combi-nation of its relevance score and contribution scores to other pa-pers. In order to compute the cont ribution score, we only consider incoming citations to the paper. Let D(p,d) be a set of the papers which are connected to paper p by a path with length d . For ex-ample, consider a chain of citations as  X  X  X  X  X  X  ; here  X   X  1, X   X   X   X  X  X  X  ,  X   X  2, X   X   X  X  X  X  X  ,  X   X  1, X   X   X  X  X  X  X  . Then, the SimCC score of term t in paper p can be obtained by the following formula:  X  X  X  X  X   X   X   X   X   X  X  X   X   X   X   X   X   X   X 1 X   X  X  X   X   X   X   X  X  X  X   X   X , X   X   X   X   X , X   X  ,  X 1 X  where  X   X   X  X  X  X  is the relevance score of term t in paper p ,  X  the contribution score of term t in paper p to paper q , and  X  X 0 X  1 is the relative importance factor to combine the relevance score and contribution score of term t . In order to compute the contribu-tion score, we consider four intuitions, according to the first intui-tion we introduce Equation ( 2): paper q that connects to paper p by a path with length d . If p and q are connected together directly ( d =1), then where reference(q) is a set of the papers that are cited by paper q . It is obvious that Equation (3) satisfies the other three intuitions. If  X 2 X  ,  X   X   X   X   X , X   X  is determined by the contribution ratios of all papers in the path. Let  X  X  X   X   X  X  X  X  X   X  X  X   X  X  be a citation path with length d from q to p , then we have: Finally, the SimCC score is obtained by Equation (5):  X  X  X  X  X   X   X   X   X   X  X  X   X   X   X   X   X   X   X 1 X   X  X  X   X   X   X   X   X   X , X   X   X   X   X   X   X 
If a new paper is added to the dataset, it is obvious that there is no need to update the SimCC scores for all the papers because the contribution score computation is done locally by analyzing the citation graph. 
In the basic SimCC, incoming cita tions are taken into account to compute the contribution scores. Recently published papers have a less chance to be cited by othe r papers and therefor, have a less chance to contribute to them. In order to alleviate this problem, we add the notation of recency to the basic SimCC formulation. Therefore, in addition to old papers, recent papers have chance to appear in the result set of similarity-based queries. To compute the recency of paper p , we use the method mentioned in [4]: where age(p) denotes the age of paper p . We compute the SimCC+A score of term t in paper p as follows:  X  X  X  X  X  X  X   X   X   X   X   X  X   X   X   X   X   X   X   X  X   X   X  X  X   X   X   X  X  X  X   X   X , X   X   X   X   X , X   X   X  where  X   X   X  X   X   X  X   X   X 1 . 
We crawled information of 1,071,973 papers from DBLP and obtained their citation information from MS Academic Search. In order to make a precise ground truth, we used a famous data mining textbook [3]. We considered all the papers in the bibliographic section of every chapter. Finally, we selected 11 research topics from different chapters whose related papers exist in our dataset: data processing, mining frequent patterns and association rules, classification, clustering, mining data streams, link mining, graph mining, data cubes, spatial database, OLAP and data warehouse, and web min ing. Therefore, our ground truth contains 11 sets and for each pape r in our dataset, we have title, abstract, and citation information. 
In order to carefully evaluate the effectiveness of our proposed method, we used Cosine, Dice, BM25, and KLD with relevance scores as a baseline method, CE BC [2] as the un-weighted combi-nation, WCO as the weighed combination [12], SimCC, and SimCC+A. In addition, to combin e the text-based and link-based similarity measures according to [9], we computed the similarity by applying Cosine, Dice, BM25, and KLD on the baseline and linearly combined their results with the result obtained by P-Rank. We selected P-Rank in the fifth iteration because we compared the accuracy of SimRank, rvs-SimRank, and P-Rank in 1 to 5 itera-tions with our dataset. We had the best accuracy with P-Rank in the fifth iteration. We combined the similarity results of P-Rank and the text-based similarity measure according to a weighted linear combination by using  X  X  X   X  X  X  X  X  X  [6] 1 . We used MAP, preci-sion at top 10 results (P@10), and re call at top 10 results (R@10) as our evaluation measures. 
The accuracy of SimCC depends on d , the length of the path in the citation graph that we traverse to compute the contribution score, and the value of  X  , the relative importance factor. In order to find the optimal values of d and  X  for Cosine, Dice, BM25, and KLD, we assigned a value to them in the ranges 1 to 5 and 0.1 to 0.9, respectively. However, due to the space limitations, we only show the result of Cosine. Figure 1 shows MAP, precision, and recall for the different values of d and  X  by using Cosine similari-ty. The optimal values of d and  X  were obtained as 3 and 0.7, re-spectively; although, MAP had a larger value on d =2 and  X  =0.5. In all of these experiments, we obtained the common interesting results: First, if d becomes larger, the optimal value of  X  also in-creases. The reason is clear; by going further in the citation graph, contribution scores become noisier, so for large values of d , the SimCC score depends more on the relevance scores than contribu-tion scores. Second, SimCC absolu tely outperforms the baseline method independently of values of d and  X  . Third, as a surprising result, for all similarity measures, the optimal value of d is 3; however, the optimal value of  X  is 0.7 for Dice, 0.5 for BM25, and 0.6 for KLD. Therefore, we claim that the optimal value of d is independent of the applied text -based similarity measure and short range and could be set from 0.5 to 0.7 because, as we can see in Figure 1, there is no tangible difference in accuracy of Co-sine on d =3 and  X  = 0.5 to 0.7; we were al so faced with the same stable situation for Dice, BM25, and KLD. 
Figure 2 shows the results of Cosine, Dice, BM25, and KLD each one applied on the baseline, CEBC, WCO, and SimCC. Al-so, this figure shows the results of P-Rank and its linear combina-tion with Cosine, Dice, BM25, and KLD. P-Rank has the worst accuracy in terms of MAP, precision, and recall compared to the text-based similarity measures. Combination methods, CEBC and WCO, and the weighted linear combination of the text-based simi-larity measures and P-Rank (+Pr ank), significantly outperform the baseline method. However, CEBC and WCO outperform the +Prank because CEBC and WCO enrich the content information of a paper by adding the terms from its neighboring papers in the citation graph, so they consider the relationship between content and citations more effectively than +Prank. +Prank completely neglects the relations hip between content and citations and con-siders them as isolated inform ation. The SimCC method dramati-cally improves the accuracy of the baseline method and absolutely outperform s all competitor methods because it exploits the cita-tion graph in the best way to effectively combine the authority and context of the scientific papers . SimCC improves the accuracy of Cosine and Dice in the baseline method more than 100% in terms of MAP, precision, and recall. The improvement for BM25 and KLD is more than 60%. Figure 3 shows the accuracy of similarity computation by using SimCC and SimCC+A scores. To compute the SimCC+A scores, we found the optimal values of  X   X   X ,  X  , and  X  all similarity measures as we did for  X  in Figure 1. By adding recency to the basic SimCC, the accuracy in terms of MAP, preci-sion, and recall was improved for al l similarity measures expect http://www.cs.cornell.edu/peopl e/tj/svm_light/svm_rank.html for Cosine where the improvement only happened in MAP. In order to carefully evaluate the effectiveness of SimCC+A, in addi-tion to MAP, precision, and recall, we performed another evalua-tion. For each paper in our ground truth sets as a query paper, we selected top 10 similar papers by applying Cosine, Dice, BM25, and KLD with the SimCC and Sim CC+A scores separately. Then, for each similarity measure, we compared the average publication year of the papers in the two diff erent result sets obtained by us-ing SimCC and SimCC+A scores for the same query paper. Ac-cording to this experiment, for more than 73% of the query pa-pers, the average publication year of the result sets obtained by using SimCC+A is less than Si mCC, which indicates that SimCC+A returns more recent and relevant papers rather than SimCC. 
In this paper, we proposed a novel approach called SimCC that effectively combines the content and citation information to im-prove the accuracy of similarity measures for scientific papers. SimCC is based on the contributi on score, which measures the authority of a paper on a term . We also proposed SimCC+A to improve the accuracy of SimCC for recently published papers. Our extensive experimental results on a real-world scientific da-taset show that the accuracy of similarity measures for scientific papers improves dramatically by SimCC and SimCC+A. 
This research was supported by Basic Science Research Pro-gram through the National Research Foundation of Korea (NRF) funded by the Ministry of Edu cation, Science and Technology (No. 2012R1A1A2007817). [1] Bigi, B., Using Kullback-Leibler Distance for Text Categori-[2] Chiki, N., Rothenburger, B., Gilles, N., Combining Link and [3] Han, J., Kamber, M., Data Mining: Concepts and Tech-[4] Hwang, W., Chae, S., Kim, S. , Yet another Paper Ranking [5] Jeh, J., Widom, J., SimRank: A Measure of Structural-[6] Joachims, T., Optimizing Search Engines using Clickthrough [7] Lin, D., An Information-Theoretic De fi nition of Similarity, [8] Lv, Y., Zhai, C., When Documents Are Very Long, BM25 [9] Menczer, F., Combining Link and Content Analysis to Esti-[10] Reyhani Hamedani, M., Lee, S., Kim, S., Text-based and [11] Strohman, T., Croft, W., Jensen, D., Recommending Cita-[12] Sugiyama, K., Kan. M., Scholarly Paper Recommendation [13] Yoon, S., Kim, S., Kim, J., On Computing Text-based Simi-[14] Zhao, P., Han, H., Yizhou S. , P-Rank: a Comprehensive 
