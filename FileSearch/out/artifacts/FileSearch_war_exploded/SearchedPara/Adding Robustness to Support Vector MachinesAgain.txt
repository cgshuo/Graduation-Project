 Many classification algorithms have been successfully de-ployed in security-sensitive applications including spam fil-ters and intrusion detection systems. Under such adversarial environments, adversaries can generate exploratory attacks against the defender such as evasion and reverse engineering. In this paper, we discuss why reverse engineering attacks can be carried out quite efficiently against fixed classifiers, and investigate the use of randomization as a suitable strategy for mitigating their risk. In particular, we derive a semidef-inite programming (SDP) formulation for learning a distri-bution of classifiers subject to the constraint that any single classifier picked at random from such distribution provides reliable predictions with a high probability. We analyze the tradeoff between variance of the distribution and its pre-dictive accuracy, and establish that one can almost always incorporate randomization with large variance without in-curring a loss in accuracy. In other words, the conventional approach of using a fixed classifier in adversarial environ-ments is generally Pareto suboptimal. Finally, we validate such conclusions on both synthetic and real-world classifica-tion problems.
 I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  Learning ; K.6.5 [ Computing Milieux ]: Management of Computing and Information Systems  X  Security and Pro-tection Security; Algorithms; Experimentation adversarial learning; reverse engineering; linear SVM
Many machine learning algorithms have been successfully deployed in security-sensitive applications. These include spam and malicious email behavior detection [37, 22, 49], fraud detection [12], as well as intrusion detection [20, 23, 32, 34, 39, 19, 31]. The machine learning techniques employed cover a wide spectrum ranging from supervised learning al-gorithms such as neural networks, support vector machines (SVM), decision trees, the na  X   X ve Bayes classifier, and k -NN to unsupervised outlier detection algorithms such as density estimation-based methods and one-class SVM.

Unfortunately, however, security-sensitive applications are characterized by the presence of adversaries , and most promi-nent machine learning algorithms such as SVM were not originally designed for such adversarial environments [9, 21, 30, 40, 55]. In the traditional Probably Approximately Cor-rect (PAC) setting, machine learning algorithms assume that training data as well as unforeseen future data are gener-ated i.i.d. from the same underlying distribution. Hence, by building a learning algorithm that works well for a given training dataset and by restricting its capacity to avoid over-fitting, the learning algorithm is expected to work well with unforeseen future examples as well. In security-sensitive ap-plications, where an adversary can interfere either during or post training, such assumption is no longer valid.

The economic incentives for adversaries to mislead learn-ing algorithms cannot be overstated. For instance, it is not unusual for spammers to generate revenues that exceed 20% of the value of products sold through spam emails [16]. Carpinter and Hunt interpret this by noting that a response rate of only 0 . 001% to a single spam email advertising a $50-product could generate more than $25,000 of returns to spammers due to the sheer number of Internet users [16]. Furthermore, it has been estimated that e-commerce fraud activities may generate over $10 billion dollars worldwide per annum in addition to the $1 billion dollars generated annually from telecommunications fraud [12].

Driven by ample economic incentives, an adversary can generate attacks against a learning system such as spam detectors either during the training phase or post training when the system is operational. These two different realms of attacks have been coined causative and exploratory at-tacks respectively [5]. One common example of a causative attack that is carried out during the training phase is to inject carefully-crafted  X  X oisonous X  training examples in or-der to mislead the learning system [10, 46]. By contrast, a common example to an exploratory attack is evasion such as the  X  X ood words attack X , where words indicative of a non-spam email are added to spam emails in order to evade the spam detection system [5, 43, 42]. Ideally, machine learning algorithms ought to be robust against both types of attacks.
Whereas defense strategies against causative attacks have recently received a growing interest in the machine learn-ing community, c.f. Section 2, less effort has been devoted towards protecting a classifier against exploratory attacks. One exploratory attack that is of particular interest in this paper is reverse engineering , whose dilemma is quite intri-cate.

Consider classification tasks such as spam detection. On one hand, given that observations ( x t , y t ) are generated i.i.d. from a fixed underlying distribution D , the learning task of the defender is to predict the label y t once we know x Of course, the defender can employ any of the state-of-the-art methods such as SVM to achieve such goal. Let  X  y t the predicted label of the resultant classifier. On the other hand, for an adversary that can probe the classifier with queries x q and observe  X  y q , the new pair ( x q ,  X  y q learning problem. Similar to the defender, the adversary can employ any of the state-of-the-art methods to learn  X  y q the adversary learns to predict  X  y q with sufficient accuracy, exploratory attacks such as evasion become easier to carry out, especially if the decision region is convex such as in linear classifiers.

The dilemma with reverse engineering is two-fold. First, development of new powerful machine learning algorithms not only facilitate the task of detecting intrusive behavior, but they also facilitate the task of reverse engineering the classifier by adversaries. To reiterate, this is because reverse engineering is another learning problem for which any learn-ing algorithm can be employed 1 . Second, once the classifier is up and running, it has been designed at the outset to perform well according to some pre-defined metrics such as precision, recall, or accuracy. While altering its behavior during operation on a regular basis can indeed make the re-verse engineering task more difficult, it will likely lead to loss of accuracy as a result, which indirectly translates into an adversarial gain.

Consequently, the goals in defending against reverse engi-neering can be summed up as follows. First, we would like to build at the outset a classification system that can always make reliable predictions while revealing as little informa-tion about its decision boundary as possible. Second, we would like to achieve such goals even if the adversary knows the system. In this paper, we show that a suitable method of randomization can achieve such objectives. Specifically, in-stead of learning a fixed classifier, the defender can use train-ing data to infer a distribution of classifiers. During predic-tion, a single classifier is drawn at random from the learned distribution. We will show that the problem of learning such distribution of classifiers can be formulated as a convex op-timization problem, which can be solved quite efficiently. In addition, the randomization method can mitigate the re-verse engineering risk without incurring any notable loss in predictive accuracy.
Note that the adversary does not need to know the exact feature space or the learning algorithm used by the classifier. Instead, the adversary can go through the normal process of feature selection and model selection in order to estimate  X  y Figure 1: Existing literature on adversarial learning can be classified into four qudrants.

The rest of the paper is structured as follows. In Section 2, we review existing literature on adversarial learning, and discuss where the reverse engineering problem falls in the literature. After that, we describe the reverse engineering problem in more details in Section 3 and formalize notation. In Section 4, we show that a suitable method of randomiza-tion can indeed achieve the desired objectives. Finally, we conclude in Section 5 with a comprehensive evaluation that validates the proposed method.
Existing literature on adversarial learning can be broadly classified along two dimensions as depicted in Figure 1. The first dimension is type of the attack ; whether it is causative or exploratory. The second dimension is nature of the proposed solution ; whether it is an attack or a defense strategy. We review these four quadrants next.
Causative machine learning attacks (see Section 1) have enjoyed a growing interest over the past 20 years. On the theoretical side, various frameworks have been used to an-alyze such attacks including PAC, game theory, and math-ematical optimization. For example, Kearns and Lin used the PAC framework in 1993 to prove that in order to achieve a prediction accuracy that exceeds 1  X  in an adversarial en-vironment, the adversary X  X  influence over the training data must be limited to, at most, / (1 + ) of training examples only [6, 28]. In addition, causative attacks can be formalized as a game between the classifier and the adversary, in which each player seeks an optimal strategy knowing that its op-ponent would also seek an optimal strategy. Several lines of research have used such game-theoretic setting [14, 15, 21, 33]. Moreover, Biggio et al. provided a gradient ascent method for poisoning attacks, in which the adversary could inject malicious data into the training dataset to mislead the SVM classifier [10]. Similarly, Xiao et al. proposed a method for adversarial label flipping attacks, in which they formulated an optimization framework for finding the label flips that minimize the classifier X  X  predictive accuracy [54].
On the practical side, Saini presented attack strategies against the popular SpamBayes classifier and concluded that the performance of SpamBayes could severely degrade if the adversary gained access to less than 1% of the train-ing dataset [46]. Some of these attacks could be as simple as contaminating legitimate emails with a few spam terms be-fore they were used for training. Also, Newsome et al. sim-ulated many causative attacks against automatic polymor-phic worm signature generation algorithms and concluded that even if training examples were always labeled correctly, the adversary could still obstruct learning by manipulating training examples [44]. One such attack is red-herring , in which irrelevant patterns are consistently added into worms during training so that the classifier focuses on such irrele-vant patterns and ignores the real signatures.
In general, both theoretical and practical work indicate that causative attacks against machine learning algorithms can be quite effective. In order to defend against such at-tacks, several tactics have been proposed such as adversar-ial collective classification, ensemble methods, kernel ma-trix correction, robustness/regularization, multiple-instance learning, game-theoretic strategies, as well as others [6, 7, 8, 9, 15, 14, 21, 26, 27, 29, 33, 48, 50, 55].

For the sake of brevity, we only describe one simple, yet effective, defense strategy against some causative attacks that is called reject-on-negative-impact (RONI). In RONI, a preliminary screening of all training examples is conducted first, and those training examples that were found to be quite influential to the decision boundary are subsequently rejected [51]. Intuitively, the learning system should not be extremely sensitive to any single training example, espe-cially in such adversarial environments. While simple, such strategy was found to be viable at preventing some causative attacks [46, 51].
Unlike causative attacks, exploratory attacks are carried out while the system is operational. It is assumed that no learning takes place during such attacks. For example, an attacker might probe the classifier with queries in order to reveal some confidential information about the training dataset that was used by the system [6]. Perhaps the two most prominent examples of exploratory attacks are evasion and reverse engineering [53].

In an evasion attack, the adversary has an attack message x that is likely to be detected by the classifier. Instead of sending x , the adversary creates a new message x 0 , which is quite similar to x but can evade the detection system. De-spite the fact that adversaries are constrained in how much they could camouflage their original messages, e.g. a spam-mer would still need to deliver the message of the spam, evasion attacks have been successful. For example, Lowd and Meek concluded through experiments that good-word attacks could be quite effective against statistical spam fil-ters such as na  X   X ve Bayes and maximum entropy filters [36]. In a different study, they also showed that evasion could be carried out quite efficiently, i.e. in polynomial time, for lin-ear classifiers, while Nelson et al. extended such result to the general class of convex-inducing classifiers [35, 41, 43, 51]. Both results show that evasion can be carried out with-out building an accurate model of the classifier X  X  decision boundary.

Reverse engineering is a second important exploratory at-tack in which the adversary attempts to model the clas-sifier X  X  decision boundary. Such attack might be used as a preliminary step towards more sophisticated exploratory attacks or it can be used as a goal by itself if, for exam-ple, the decision boundary can reveal sensitive information [6]. Needless to mention, reverse engineering might be con-ducted partially . In the case of linear classifiers, for example, one simple reverse engineering tactic is the sign witness test , whereby an adversary can infer with relative ease whether a feature contributes positively or negatively to the classifier X  X  final decision [35, 53]. In general, some important types of classifiers can be reverse-engineered using a few probe mes-sages. For example, Frieze et al. showed that hypercube decision surfaces, of which half-spaces in linear classifiers can be thought of as special cases, can be learned quite ef-ficiently [25]. In active learning, a number of researchers used PAC-style analysis to establish that homogenous lin-ear classifiers could be approximated well with a few probe messages such as by using uncertainty sampling or selective sampling [3, 4, 18]. We will describe the reverse engineering problem in details in Section 3.
In a recent work, Barreno et al. analyzed the security of machine learning algorithms and proposed a taxonomy of at-tacks [5]. They described the problem of defending a classi-fier against exploratory attacks and suggested three general strategies. The first suggested strategy is to keep the entire learning process as confidential as possible. This includes the choice of training data, features, cost function, as well as the learning algorithm employed. While such disinforma-tion strategy is certainly sound, security of machine learning algorithms cannot rely on obscurity as it goes against Ker-ckhoffs X  principle, put forth in the 19th century and later popularized by Shannon, which states that one should al-ways assume that the adversary knows the system [38].
The second suggested strategy is to increase complexity of the hypothesis space so that it is harder to learn by the adversary. However, it is not clear how to implement such strategy without causing over-fitting. The third strategy is randomization. Instead of predicting a binary label, the classifier could predict a probability P ( y | x )  X  [0 , 1] and use such probability estimate to pick a label  X  y  X  { 0 , 1 } at random. In a different line of work, Barreno et al. also listed randomization as a valid defense strategy although little, if any, work has been conducted to implement such approach [6]. In fact, Barreno et al. conclude that: One objective of this paper is to formalize the trade-off be-tween accuracy and randomization. A noteworthy result is that it is often possible to incorporate randomization in or-der to increase the adversary X  X  reverse engineering effort, but without increasing the learner X  X  predictive error rate.
Suppose a classifier is supplied with m training examples { ( x 1 , y 1 ) ,..., ( x m , y m ) } , drawn i.i.d. from a fixed unknown distribution D , where x i  X  R n and y i  X  { +1 ,  X  1 } . The objective of the classifier is to make a prediction  X  y a new example x t such that [ X  y t = y t ] holds with a high probability. In spam detection systems, for example, x t Figure 2: In this figure, the blue line marks the classifier X  X  true decision boundary whereas the red line is the adversary X  X  estimate. be an individual email represented by a bag of words while y indicates whether the email is spam or ham. Once training is concluded, the classifier is now equipped with a hypothesis  X  f and makes predictions according to  X  y = sign (  X  f ( x )).
In order to reverse-engineer the classifier, an adversary can send probe queries x q to the classifier and observe  X  y A spammer, for example, could simply open a new email account at the targeted service provider and observe whether or not specific emails are marked as spam. The adversary, now, owns a new training dataset { ( x q,t ,  X  y q,t ) } t =1 , 2 ,... can be used to learn to predict  X  y .

In the field of active learning , several strategies have been developed for query selection [47]. These strategies differ depending on whether or not one has access to samples generated from the underlying distribution D . In general, queries should be selected intelligently to maximize infor-mation gain. In this work, we consider three query selection strategies. In all strategies, we assume that the adversary starts with one positive example x (+) and one negative ex-ample x (  X  ) , which is a common assumption in the literature. The adversary, then, builds a pool P of random samples generated i.i.d. from a Gaussian distribution with mean ( x (+) + x (  X  ) ) / 2 and a sufficiently large width. The final choice of query depends on the strategy employed: 1. Random Sampling: This is the simplest scheme, in 2. Selective Sampling: Here, we consider the sampling 3. Uncertainty Sampling: In this scheme, the adver-
Once the query has been labeled, the adversary updates its estimate of the separating hyperplane using soft-margin SVM. Here, we assume the worst-case scenario in which the adversary knows that the defender uses a randomized clas-sifier , and hence soft-margin SVM is used instead of hard-margin SVM. Figure 2 displays results of applying selective sampling to a fixed linear classifier. As shown in the figure, only 20 probes are sufficient to reconstruct the separating hyperplane in R 2 .

Consequently, the reverse-engineering problem is for the classifier to be able to predict reliably while revealing little information about its decision boundary to the adversary. Since linear classifiers are much easier to reverse-engineer than non-linear classifiers [6, 25, 35], we will restrict at-tention to linear classifiers only, and leave its extension to non-linear classifiers to future work. It is worth mentioning that linear classifiers have become one of the most impor-tant learning methods in practice today, especially for large sparse datasets, and that they have found important appli-cations in security-sensitive systems such as spam detection [22, 24, 52, 53].
In this section, we derive a convex optimization problem, which the defender can use to learn a distribution of classi-fiers subject to the constraint that any single classifier picked at random from such distribution provides reliable predic-tions. Once learning is concluded, the defender can mitigate the risk of reverse engineering attacks by picking a classifier at random for every query observed. We will illustrate how the method works using synthetic datasets and discuss how to interpret the accuracy-variance tradeoff curves.
Our starting point is the following problem. Suppose we have a system of linear inequalities Aw  X  b , where w  X  N (  X ,  X ) is a multi-variate random vector whose covaraince  X  = diag (  X  2 1 ,..., X  2 n ) is a diagonal matrix. Assume first that  X  is fixed but  X  can vary. Because w is randomized, we seek a distribution N (  X ,  X ) such that the system of linear inequalities is satisfied with a probability that exceeds  X  , where 1 2 &lt;  X  &lt; 1, if w is drawn at random from N (  X ,  X ). That is, we would like to find  X  given A, b,  X , and  X  . First, if we let  X  a T i be the i -th row of A , then the constraint: can be re-written as: cording to the standard Gaussian density. Therefore, the above constraint is equivalent to: where  X  is the cumulative density function of the standard Gaussian density. Consequently, we have: Since, the only variable is  X  , the entire problem can be re-duced to a system of linear inequalities. Note that because we would like to satisfy a system of linear inequalities for an entire distribution, we ended up with a new system of linear inequalities that is stricter than the one we started with assuming  X  &gt; 1 2 .
Second, suppose that  X  is not fixed beforehand and that we would like to find a distribution N (  X ,  X ) that both sat-isfies the constraint with a high probability and has a large variance. Assuming that w  X  R n , one convenient metric that can be used to measure the spread of a multivariate Gaussian density is given by the following definition:
Definition 1. Let N (  X ,  X ) be a multivariate Gaussian density with mean  X   X  R n and covariance  X  = diag (  X  2 1 We define its normalized variation 0  X   X   X  1 using:
The motivation behind choosing  X  as a measure of spread can be understood by noting that if ( w 1 , w 2 , ..., w sequence of random draws from N (  X ,  X ), then: In other words,  X  can be interpreted as a measure of the expected distance between two distinct draws w t and w t +1 compared to their expected norm E || w t || 2 . In adversarial environments, having a large value of  X  implies that the decision boundaries used for two different queries will be quite different from each other, which invariably makes the reverse engineering task more difficult to carry out.
It is straightforward to observe that maximizing  X  can function of the latter form is not convex. One method to con-vexify it is to introduce new variables s j =  X  2 j , which would turn the non-convex objective function into a quadratic-over-linear term  X  T  X / 1 T s , which is jointly convex on  X  and s in the domain 1 T s &gt; 0 [1]. Next, the constraint in Eq. (1) is not convex. In order to retain convexity of the feasibility region, we linearize by omitting the square-root sign: We are now left with one final ingredient, which is to han-dle the case when Aw  X  b is not feasible by incorporating slack variables  X  in a manner that is quite similar to soft-margin SVM. Intuitively speaking, this corresponds to the case where most of the constraints can be satisfied but not all. The final optimization problem, thus, becomes:
The above randomization method can be immediately ap-plied to linear classification of the form  X  y t = sign ( w Because the original constraints in binary classification are of the form y i  X  ( x T i w )  X  1, i.e. separate by a margin of 1, we make the substitutions  X  a i =  X  y i  X  x i ,  X  a ij and b i =  X  1 in (3). This yields the following optimization problem: The optimization problem in (4) is convex and can be used to learn an entire distribution of classifiers w  X  N (  X ,  X ). Its objective function ensures that the learned distribution of classifiers has a high variance, while the constraints ensure that the learned distribution has a high predictive accuracy.
The optimization problem in (4) can be efficiently solved using semidefinite programming (SDP) solvers such as MOSEK. Except for the quadratic-over-linear term  X  T  X  1 T s terms in (4) are linear in the optimization variables. The quadratic-over-linear term, however, can be minimized by minimizing a scalar t  X  R subject to the convex constraint  X   X / (1 T s )  X  t . It can be shown using Shur X  X  complement that the latter constraint is equivalent to the constraints [2]: Here, the notation A 0 implies that A belongs to the posi-tive semidefinite cone. Both constraints can be immediately handled by SDP solvers.

Experimentally, it often takes less time to solve the opti-mization problem in (4) than to solve SVM using a state-of-the-art solver such as LIBSVM. For example, qsar and theorem UCI datasets contain 1,000 and 3,000 training ex-amples respectively. It takes MOSEK around 0.7s and 2s to solve (4) for the two datasets respectively, while LIBSVM takes 2.2s and 4s to find a solution for the same datasets.
Looking closely into the optimization problem (4), we note an inherent tradeoff between accuracy and variance. For example, as C  X  0, the objective function favors maximizing  X  over predictive accuracy and the opposite statement holds if C  X   X  . Similarly, as  X   X  1 (  X  )  X  0, the cost of increasing 1 s = P n j =1  X  2 j becomes negligible, and the algorithm favors large variance. This tradeoff can be captured more formally using the following lemma.

Lemma 1. Let (  X  ? , s ? ) be the optimal solutions to the randomized SVM problem (4) for some fixed C and  X  . Let m be the number of training examples, and define model fitness f model of the solution (  X  ? ,s ? ) using the total sum of positive margins exceeding 1 on the training set: where M i = y i (  X  T x i ) and [ z ] + = max { z, 0 } . Then, the normalized variation  X  ? of the solution (  X  ? , s ? ) is related to model fitness f model by the inequality: Hence, one can always increase normalized variation  X  in exchange for a reduction in model fitness f model . In addi-tion,  X   X  1 as C  X  0 .

Proof. Because the optimization problem is convex, we use KKT optimality conditions [13]. By taking the deriva-tive of the Lagrangian with respect to s j , we obtain: where 0  X   X  i  X  C are the Lagrange multipliers. However, by complementary slackness , we know that: which holds for all training examples 1  X  i  X  m . In ad-dition, we know by primal feasibility that s j  X  0 for all j . Combining all inequalities yields:  X  Here, the second line follows from Eq. (7), while the third line follows from Eq. (8) and because  X  i = max { 1  X  y i at optimality. The last line follows because 0  X   X  i  X  C and by definition of f model . Knowing that s j =  X  2 j and using definition of normalized variation  X  in Eq. (2), statement of the lemma follows immediately.

An illustration to the tradeoff between accuracy and vari-ance is provided in Figure 3. In this figure, the two classi-fication problems in R 2 are depicted at the top along with the 80% and 90% confidence regions of the decision bound-ary. Recall that the decision boundary  X  y t = sign ( w randomized because w is picked at random from N (  X ,  X ). Clearly, the confidence regions follow extremely well the ac-tual shape of the two classes. Because the two classes on the left are well-separated from each other, size of the 80% and 90% confidence regions is much bigger than in the classifica-tion problem on the right. Mathematically, this corresponds to having a large value of  X  .
 Aside from confidence regions, the plots at the bottom of Figure 3 reveal the tradeoff curves between predictive accu-racy E ( X  y t = y t ) and  X  for each classification problem. On the left, we note that  X  can be increased without incurring any notable reduction in accuracy because the two classes themselves are well-separated. However, it is more difficult to incorporate variance in the classification problem on the right without suffering a loss in accuracy because the two classes are close to each other in the original space.
The accuracy-variance tradeoff curves convey a wealth of information. Before this is discussed, we recall the following definition: Figure 3: In the top figures, the 80% and 90% confi-dence regions for the decision boundary are high-lighted when applying the randomized SVM for-mulation in (4) to two synthetic datasets. In the bottom figures, the tradeoff curves between accu-racy and variance are plotted. A classifier can oper-ate anywhere within the blue-shaded region, whose boundary forms the set of achievable Pareto opti-mal points. The red cicles mark the Pareto optimal points that achieve maximum accuracy, which does not always coincide with having  X  = 0 .

Definition 2 (Pareto Optimality). A Pareto opti-mal point is a pair (  X ,  X  ) , where  X  is predictive accuracy and  X  is given by Definition 1, such that no gain in  X  can be made without causing a loss in  X  and vice versa. The set of Pareto optimal points form the tradeoff curve.
 Any Pareto optimal point is a sound strategy. Keeping Pareto optimality in mind, the following remarks are useful when interpreting the tradeoff curves: 1. Every Pareto optimal point along the tradeoff curve is 2. If we let  X  max be the maximum attainable accuracy 3. The point of intersection between the tradeoff curve
The optimal tradeoff between accuracy and variance de-pends obviously on the application at hand, and every Pareto optimal point along the tradeoff curve is a sound strategy as discussed earlier. In our experiments, however, we will always choose the Pareto optimal point that maximizes pre-dictive accuracy. To reiterate, such choice of Pareto opti-mality is not equivalent to choosing  X  = 0 as highlighted in red in Figure 3. Needless to mention, this corresponds to the most conservative policy, and better gains of randomization can be attained when a small drop in accuracy is tolerated.
The central statement of this paper is that choosing a fixed classifier is a sub-optimal strategy . That is, one can almost always learn a distribution of classifiers and incor-porate randomization without incurring a loss in accuracy. Such approach can mitigate the adversarial risk of reverse engineering the classifier as will be established next.
As illustrated earlier in Figure 1, the only two strate-gies suggested in the literature for mitigating exploratory attacks, such as reverse engineering, are randomization and disinformation . To reiterate,  X  X isinformation X  refers to the act of keeping the learning process, including datasets and training algorithms, as confidential as possible. Clearly, dis-information does not compete with randomization because both supplement each other.

For randomization, the only method suggested previously in the literature that the authors are aware of is to randomize predicted labels according to confidence estimates [6]. That is, if the learning algorithm outputs confidence estimates such as probabilities or functional margins, randomization may be introduced for uncertain labels . In SVM, one method to implement this approach is to predict using: where  X  is a number drawn uniformly at random from the set [  X  1 , +1]. This approach is valid because SVM is constructed at the outset to separate the positive and negative classes by a separating hyperplane with a margin of 1. Obviously, the prediction rule in Eq. (9) does not impact those points whose functional margin exceeds 1.

Alternatively, one may compute probability estimates and introduce randomization according to P ( y | x ). However, be-cause probability estimates are typically defined using some monotone function of margins, see for instance [45], probability-based randomization and margin-based randomization are equivalent. Therefore, we will restrict our experiments to confidence-based randomization of the form given in Eq. (9). Throughout the sequel, we will refer to this method as conf-rand whereas our proposed method of randomiz-ing classifiers will be referred to as class-rand .
Throughout the experiments, we compare class-rand against the two baseline methods: conf-rand and learning a fixed classifier. We will show that class-rand introduces more variance for the same level of accuracy, and that this advantage translates into having more robustness against reverse engineering as well as evasion attacks.
In order to validate performance, we used 15 datasets in our experiment selected from the UCI machine learning repository [11]. The datasets are listed in Table 1. Lin-ear SVM was implemented using the LIBSVM library [17], whereas class-rand was modeled using CVX and solved using MOSEK solver [1]. We used a training-to-test split ratio of 9:1, and the same training-to-test split was used in all methods. The same experiment was repeated 10 times and averages are reported 2 .
In each dataset, the first exercise is to plot the tradeoff curve. This can be implemented by recording predictive accuracy and  X  for various choices of C and  X  . Once a model ( C,  X  ) is selected and the optimization problem in (4) is solved, we obtain the desired distribution N (  X ,  X ).
Having obtained a distribution N (  X ,  X ) for linear classi-fiers w , prediction is performed in three steps: 1. Step 1 : Receive a new query x t . 2. Step 2 : Pick a new w at random from N (  X ,  X ). 3. Step 3 : Predict using  X  y t = sign ( w T x t ). The same process is to be repeated for every new test ex-ample. For conf-rand and fixed classifier methods, on the other hand, the value of C is also selected using 10-fold cross-validation, which is implemented for each method separately. In fixed svm , prediction is performed using  X  y = sign ( w T x ) whereas prediction in conf-rand is per-formed using Eq. (9).
 Results of applying the above approach are provided in Table 1. Note that classification accuracy is not impacted in all datasets when randomization is introduced using either conf-rand or class-rand methods. However, class-rand introduces significantly more variance for the same level of accuracy as will be demonstrated next. This is due to the fact that confidence-based randomization such as by using Eq. (9) can be interpreted as methods of randomizing the offset term b only. On the other hand, our proposed method of randomizing classifiers learns an entire diagonal covari-ance matrix  X  and introduces randomization to the offset term b as well as all coefficients of w  X  R n .
Reverse engineering attacks are harder to carry out when the decision boundary is picked at random from a distribu-tion with large variance. This is demonstrated experimen-tally in Table 2 and Figure 4. In Table 2, we simulated three adversarial probing techniques, described earlier in Section 3, which are quite common in the active learning setting: (1) random sampling , (2) selective sampling , and (3) uncertainty sampling . Table 2 lists estimation error after 1,000 probes for each of the 15 classification problems, where estimation error is defined as: In Eq. (10), w is the true classifier used by the defender while  X  w is the adversary X  X  estimate of w . In order to appro-priately compare hyperplanes, both w and  X  w are normalized to unit norm. As shown in Table 2, randomizing classifiers yields better performance, typically by an order of magni-tude, regardless of which probing technique is used by the adversary. Figure 4 plots estimation error against the num-ber of probes on a log-scale, when uncertainty sampling is
Datasets and MATLAB routines will be made available online at: http://mine.kaust.edu.sa/Pages/Software.aspx Table 1: Results of applying linear SVM ( fixed) , conf-rand results of the evasion attacks experiment in Section 5.2.3. employed by the adversary. The advantage of randomizing classifiers is quite visible across all figures. To reiterate, this advantage of randomization is reaped without incurring a loss in predictive accuracy.
Finally, once the adversary obtains an estimate of the defender X  X  decision boundary using reverse engineering, the adversary can generate additional attacks such as evasion . In this third experiment, we show that evasion attacks are facilitated by reverse engineering, and that randomization increases robustness against such attacks, too. The objec-tive of this experiment is not to develop sophisticated eva-sion attacks, which is outside the scope of our paper, but to illustrate why the defender X  X  decision boundary must be protected against adversarial reverse engineering.

During the experiment, 20% of examples are reserved for adversarial use, which are meant to simulate the adversary X  X  knowledge about the ground truth during attacks. The en-tire experiment was repeated 10 times, with different adver-sarial attacks in each trial, and averages are reported.
The exact experiment runs as follows. First, the adver-sary uses 1,000 probes, i.e. the maximum number of probes shown in Figure 4, and estimates the defender X  X  decision boundary. Let sign ( w T x ) be the defender X  X  decision, and let  X  w be the adversary X  X  estimate of w . Second, the adver-sary goes over the list of its attack messages and marks any other attacks are discarded by the adversary. Intuitively, the adversary abstains from sending an attack message if it is likely to be detected correctly by the defender.

The performance metric is attack success rate , which is the fraction of attacks that went unnoticed by the defender. Mathematically, we have: Attack Success Rate = P y 6 = sign ( w T x ) y 6 = sign (  X  w Results of conducting such experiment are shown in the last three columns of Table 1. As shown in the table, random-ization increases robustness against evasion attacks in all datasets.
Machine learning algorithms have been immensely suc-cessful in numerous applications including environmental sciences, bioinformatics, recommendations systems, and text classification to just name a few. They are also attractive solutions for security-sensitive applications such as spam fil-tering, intrusion detection, and fraud detection. Neverthe-less, such adversarial environments pose new challenges to the learning systems, and part of the challenge is to be able to make the learning system robust against exploratory at-tacks such as evasion and reverse engineering.

In this paper, we establish that the risk of reverse engi-neering can be mitigated by learning a distribution of clas-sifiers, as opposed to the conventional practice of learning a single fixed classifier. Such distribution of classifiers can, in fact, be learned quite efficiently using semidefinite pro-gramming (SDP) solvers such as MOSEK and SDPT3. We provide a convex formulation for the learning task and apply it to real world classification problems. Using the accuracy-variance tradeoff curves, we show that robustness against reverse engineering attacks can be increased for many clas-sification problems without having to incur a loss in accu-racy. In other words, choosing a fixed classifier is generally a suboptimal strategy. By drawing classifiers at random from a distribution with large variance, the adversary X  X  effort to carry out a reverse engineering attack will increase at little cost to the learning system. While this work establishes that using a fixed classifier in adversarial environments is Pareto suboptimal, the authors are currently investigating effective-ness of randomization in mitigating reverse engineering at-tacks against spam filters. [1] CVX: Matlab software for disciplined convex [2] Mosek modeling manual, 2014. [3] M.-F. Balcan, A. Broder, and T. Zhang. Margin based [4] M.-F. Balcan, S. Hanneke, and J. W. Vaughan. The [5] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar. [6] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and [7] B. Biggio, I. Corona, G. Fumera, G. Giacinto, and [8] B. Biggio, G. Fumera, and F. Roli. Multiple classifier [9] B. Biggio, B. Nelson, and P. Laskov. Support vector [10] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks [11] C. L. Blake and C. J. Merz. UCI repository of [12] R. J. Bolton and D. J. Hand. Statistical fraud [13] S. P. Boyd and L. Vandenberghe. Convex [14] M. Br  X  uckner and T. Scheffer. Nash equilibria of static [15] M. Br  X  uckner and T. Scheffer. Stackelberg games for [16] J. Carpinter and R. Hunt. Tightening the net: A [17] C. Chang and C. J. Lin. LIBSVM: A library for [18] D. Cohn, L. Atlas, and R. Ladner. Improving [19] M. Cova, C. Kruegel, and G. Vigna. Detection and [20] C. Curtsinger, B. Livshits, B. G. Zorn, and C. Seifert. [21] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. [22] H. Drucker, D. Wu, and V. N. Vapnik. Support vector [23] E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and [24] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [25] A. Frieze, M. Jerrum, and R. Kannan. Learning linear [26] A. Globerson and S. Roweis. Nightmare at test time: [27] Z. Jorgensen, Y. Zhou, and M. Inge. A multiple [28] M. Kearns and M. Li. Learning in the presence of [29] A. Ko lcz and C. Teo. Feature weighting for improved [30] P. Laskov and R. Lippmann. Machine learning in [31] P. Laskov and N.  X  Srndi  X c. Static detection of malicious [32] A. Lazarevic, L. Ertoz, V. Kumar, A. Ozgur, and [33] G. L X  X uillier, R. Weber, and N. Figueroa. Online [34] Y. Liao and V. R. Vemuri. Using text categorization [35] D. Lowd and C. Meek. Adversarial learning. In [36] D. Lowd and C. Meek. Good word attacks on [37] T. A. Meyer and B. Whateley. SpamBayes: Effective [38] S. Mrdovic and B. Perunicic. Kerckhoffs X  principle for [39] S. Mukkamala, G. Janoski, and A. Sung. Intrusion [40] B. Nelson, B. Biggio, and P. Laskov. Understanding [41] B. Nelson, B. Rubinstein, L. Huang, A. Joseph, [42] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, [43] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, [44] J. Newsome, B. Karp, and D. Song. Paragraph: [45] J. Platt. Probabilistic outputs for support vector [46] U. Saini. Machine learning in the presence of an [47] B. Settles. Active learning literature survey. Computer [48] G. Stempfel and L. Ralaivola. Learning SVMs from [49] S. J. Stolfo, S. Hershkop, K. Wang, O. Nimeskern, and [50] M. Torkamani and D. Lowd. Convex adversarial [51] J. Tygar. Adversarial machine learning. Internet [52] B. Wang, G. J. Jones, and W. Pan. Using online linear [53] H. Xiao. A tutorial of adversarial learning, 2011. [54] H. Xiao, H. Xiao, and C. Eckert. Adversarial label [55] Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and
