 Michael R. James mrjames@umich.edu Satinder Singh baveja@umich.edu Predictive state representations (PSRs; Littman, Sut-ton, &amp; Singh, 2001) are a recently proposed way (in-spired by the work of Jaeger (2000) and Rivest &amp; Schapire (1994)) of building models of controlled dy-namical systems. PSRs capture the state of a system as a vector of predictions or outcome probabilities for tests (or experiments) that one can do on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test X  X  observation-sequence happening if the test X  X  action-sequence were to be executed on the system. PSR-based models have parameters that define how the predictive state rep-resentation changes over time as actions are executed and observations noted. A novel 1 aspect of PSR-based models is that their state is expressed entirely in terms of observable quantities. In contrast, partially observ-able Markov decision process or POMDP-based mod-els express state in terms of probability distributions over unobservable and hypothetical underlying-states of the system (see, e.g., Lovejoy, 1991; Littman, 1996). Despite this key difference, PSR-models are as flexi-ble and powerful as POMDP models; indeed, Littman et al. showed that any dynamical system that can be modeled as a POMDP can also be modeled as a PSR of size no larger than that of the POMDP model. On the other hand, because of this key difference, there is a pressing and yet unmet need to develop algorithms for learning PSR-models from streams of experience because existing POMDP-based model-learning meth-ods scale quite poorly (Littman et al., 2001; Shatkay &amp; Kaelbling, 1997).
 Learning a PSR-based model from data has two im-portant subproblems: 1) discovery of the tests whose predictions constitute state, and 2) learning the model parameters that define how the predictive state repre-sentation is updated over time as actions are executed and observations noted. So far, there have been no results for the discovery problem, while for the learn-ing problem an approximate gradient-based algorithm has been proposed by Singh et al. (2003) with mixed results (it works on some domains and not on others). In this paper, we present the first linear-PSR discov-ery algorithm and a new learning algorithm, both for the restricted class of dynamical systems that have a reset . We present evaluations of our algorithms on the same domains (with an added reset) that the previous gradient algorithm got mixed results on. 1.1. Related Work PSR-based models are closely related to Jaeger X  X  (2000; 2003) Observable Operator Models or OOMs. These were developed initially for uncon-trolled dynamical systems but were then extended to input-output OOMs, or IO-OOMs, to handle controlled dynamical systems. Jaeger developed two different versions, interpretable and non-interpretable, of both OOMs and IO-OOMs; of these, only the interpretable versions are of practical interest because discovery and learning algorithms can be developed only for them. In recent work (Singh et al., 2004), we have shown that the class of dynamical systems that can be modeled by interpretable IO-OOMs is a small subset of systems that can be modeled as POMDPs which in turn is a strict subset of systems that can be modeled as PSRs, i.e., that PSRs are significantly more general than interpretable IO-OOMs. We consider finite, discrete-time, and controlled dy-namical systems, henceforth simply dynamical sys-tems, that accept actions from a discrete set A , and produce observations from a discrete set O . Let-ting a i  X  A and o i  X  O denote the action and ob-servation at time i , the probability of some history h = { a 1 o 1 a 2 o 2 . . . a n o n } is the conditional probabil-ity of the observation sequence o 1 o 2 . . . o n being ob-tained if the action sequence a 1 a 2 . . . a n were executed from time 1 onward, i.e., P ( h ) = prob ( o 1 = o 1 , o 2 o , . . . , o n = o n | a 1 = a 1 , a 2 = a 2 , . . . , a n namical system is characterized by a probability dis-tribution over all possible histories, P : {AO}  X   X  [0 , 1]. A test , like a history, is also a sequence of action-observation pairs, but unlike a history, a test is not constrained to start at time 1. The condi-tional probability of a test t = a 1 o 1 a 2 o 2 . . . a m some history h (w.l.o.g., of length n ), is P ( t | h ) = P ( ht ) /P ( h ) = prob ( o n +1 = o 1 , o n +2 = o 2 , . . . , o o is the probability of obtaining the test X  X  observation-sequence if the test X  X  action-sequence were executed from history h onward. For ease of exposition, hence-forth, we will abbreviate expressions like the right-hand side of the definition of P ( t | h ) above as simply prob ( o 1 o 2 . . . o m | ha 1 a 2 . . . a m ). A set of tests Q = { q 1 q 2 . . . q | Q | } consti-tutes a PSR if its prediction-vector , P ( Q | h ) = for all histories, i.e., for any test t , P ( t | h ) = f ( P ( Q | h )) for some function f t  X  i.e., P ( Q | h ) cap-tures all the information in h relevant to predicting the future. We will distinguish such a set of tests that constitutes a PSR by calling it a core set of tests and reserving the label Q for it. In general, the functions f t can be linear or non-linear. In this paper we will only consider linear PSRs in which  X  t P ( t | h ) = P T ( Q | h ) m t , for a weight vector m (by default vectors are assumed to be column vectors and so the transpose operator,  X  T , is used to obtain row vectors). On taking action a from some history h and observing o , one can update the prediction-vector for Q as follows: for each q i  X  Q where we used the fact that aoq i is just another test. Therefore, a linear-PSR model is specified by core-set Q and the model-parameters which are the weight vec-tors m aot (all of size | Q | X  1) for all a  X  A , all o  X  O and for all t  X  Q  X  X   X  } (where the null string  X  was added because of the parameters m ao in the denomi-nator of the right-hand side of Equation 2). Learning a PSR model from data involves two subproblems: the discovery of a core-set Q , and learning the model-parameters.
 In building a POMDP representation of a dy-namical system one starts by hypothesizing a set S = { 1 , 2 , . . . , |S|} of unobservable states. For any history h , the counterpart to the predic-tion vector P ( Q | h ) is the belief-state b ( S| h ) = the probability of the system being in each underly-ing state of S at history h . The model-parameters are a set of transition-probability matrices T a (for all a  X  X  ) such that T a ij is the probability of a transition to state j  X  X  upon taking action a in state i  X  X  , and a set of diagonal observation probability matrices O a,o for all a  X  X  and o  X  X  such that O a,o ii is the probabil-ity of receiving observation o upon taking action a and transitioning to state i . On taking action a in history h and observing o , the belief-state can be updated as follows: where 1 n is a n  X  1 vector of all 1 X  X . Learning a POMDP model from data also involves discovery and learning subproblems, where the discovery problem is to figure out how many states to use, while the learning problem is to estimate the model-parameters.
 We can relate PSR-models to POMDP-models using a conceptual state-test prediction matrix U (see Fig-ure 1) whose rows correspond to the states in S and whose columns correspond to all possible tests ar-ranged in order of increasing size, and within a size in some lexicographic order (we let T denote the set of all possible tests in this ordering). The entry U ij is the prediction for the test t j (the one associated with column j ), given that the state of the system is i . Then, by construction, for any h and any t j , P ( t j | h ) = b T ( S | h ) U ( t j ), where U ( t j ) is the j umn of U , and for any set of tests C , the vector P
T ( C | h ) = b T ( S | h ) U ( C ) where U ( C ) is the sub-matrix corresponding to the tests in C . Thus, the matrix U allows us to translate belief-states into pre-diction vectors.
 We can also motivate and derive PSR core-tests Q from the U matrix as follows. Let M be the set of tests corresponding to any maximal-set of linearly-independent columns of U . Then for any test t , U ( t ) = U ( M ) w t , for some vector of weights w t (by construction of M the column in U for any test must be linearly dependent on the columns of sub-matrix b ( S | h ) U ( M ) w t = P T ( M | h ) w t which is exactly the requirement for M to constitute a linear-PSR (see Equation 1). Note that the number of linearly in-dependent columns is the rank of the matrix U and is therefore upper-bounded by the number of rows |S| . Thus, through this conceptual matrix U we have proven that for any POMDP, there exists a linear-PSR with  X  |S| core-tests and that any maximal-set of linearly-independent columns of U correspond to core-tests Q . Note that the core-tests are not unique. Littman et al. also prove by a constructive argument that there exists a core-set Q in which no test is longer than |S| . These results together suggest the power and flexibility of PSRs as a representation. In this section we derive a discovery and learning al-gorithm for dynamical systems with reset and provide some partial analysis supporting our algorithms. First, note that if we could somehow estimate the ma-trix U from data, then we could discover core-tests Q by finding a maximal-set of linearly-independent columns of the estimated U . But, we cannot directly estimate U because its rows correspond to a set S that is unobservable. A key insight in this paper is to re-place the state-test prediction matrix U used in the original PSR paper (Littman et al., 2001) by a new history-test prediction matrix Z that as we will show can be estimated from data in certain classes of dy-namical systems (e.g., those with reset), and that has many of the same desirable properties that U has. The history-prediction matrix Z (see Figure 1) has columns corresponding to the tests in T just as for U , but has rows corresponding to all possible histories H instead of the hypothetical S as in U . Note that H = T  X   X  where  X  is the null history. The entry Z ij = p ( t j | h is the prediction of the test t j given history h i . Note that Z , unlike U , is expressed entirely in terms of ob-servable quantities and thus, as we will show next, can be directly used to derive discovery and learning algo-rithms.
 But first, some notation. For any set of tests C , let Z ( C ) ( U ( C )) denote the submatrix of Z ( U ) contain-ing the columns corresponding to the tests in C . Also, let Q T denote the tests corresponding to a maximal-set of linearly independent columns of Z and let Q H be the histories corresponding to a maximal-set of lin-early independent rows of Z . Again, note that both Q T and Q H are not unique.
 Lemma 1 For any dynamical system and its cor-responding history-test prediction matrix Z , the tests corresponding to any maximal-set of linearly-independent columns of Z constitute a linear-PSR. Furthermore, if the dynamical system is a POMDP with state-test prediction matrix U , the size of the linear-PSR derived from Z will be no more than the size of the linear-PSR derived from U .
 Proof For any test t , Z ( t ) = Z ( Q T ) w t for some w t (because every column of Z is linearly dependent on the columns of Z ( Q T )). P ( t | h ) is the element cor-responding to row h in vector Z ( t ), while P T ( Q T | h ) is the row of Z ( Q T ) corresponding to h . Therefore,  X  t P ( t | h ) = P T ( Q T | h ) w t , which in turn implies that the set Q T constitutes a linear-PSR (see Equation 1). For the second part of the lemma, let b T ( S |H ) be a (  X   X  | S | ) matrix whose i th row corresponds to the belief-state for history h i . Then Z = b T ( S |H ) U . By a standard result from linear algebra the rank of a prod-uct of matrices is less than or equal to the smaller of the ranks of the two matrices being multiplied. There-fore the rank of Z is upper bounded by the rank of U . The proof then follows from the fact that the size of the linear PSR is equal to the rank.
 Lemma 1 shows that Q T constitutes a linear-PSR, and hence we will refer to Q T as core-tests. By analogy we will refer to Q H as core histories.
 Lemma 2 For a dynamical system that is a POMDP, the set of belief-state vectors corresponding to the set Q H of core-histories are linearly indepen-dent.
 Proof Let H  X  X  be an arbitrary finite set of histo-ries, and let b ( S| H ) be the associated |S| X | H | belief-state matrix (each column is a belief-state). If b ( S| h ), the belief-state for some history h , is linearly depen-dent on the set of belief-states associated with the his-P
T ( T| h ) = b T ( S| h ) U ( T ) = w T b T ( S| H ) U ( T ) = w
T P ( T| H ) = w T Z ( H ). In other words if the belief-state for some history h is linearly dependent on the belief-states of some set of histories H , then the row corresponding to h in Z will be linearly dependent on the rows corresponding to histories H in Z . Contra-positively, this implies that if we find a set of rows of Z that are linearly independent of each other then their associated belief-states are linearly independent. Lemma 2 is interesting because it shows that core-histories are a kind of basis set for the space of histories H , in that the belief-state vectors corresponding to core-histories span the full space of feasible belief-state vectors.
 Lemma 3 The set of tests and histories correspond-ing to a set of linearly independent columns and rows of any submatrix of Z are subsets of core-tests and core-histories respectively.
 Proof A set of columns (rows) that are linearly in-dependent in any submatrix of Z are also linearly in-dependent in the full matrix Z . The proof follows from Lemmas 1 and 2 Lemma 3 frees us from having to deal with the infinite matrix Z and instead allows us to consider finite, and hopefully small, submatrices of Z in building discovery and learning algorithms. 3.1. Analytical Discovery and Learning (ADL) We present our algorithm for discovery and learning in 2 stages. In the first stage, we will develop an al-gorithm for discovery alone as well as an algorithm for both discovery and learning under the assumption that the algorithm has the ability to somehow compute prediction P ( t | h ) exactly for any test t and history h . In the second stage, we will remove that assumption by allowing the algorithm to instead experiment with the dynamical system itself and empirically estimate P ( t | h )  X  the second-stage algorithms will be the first-stage algorithms with the estimated test-predictions replacing the true test-predictions but with additional machinery to deal with potential problems introduced by inaccurate test-predictions.
 Under the assumption that we can compute P ( t | h ) ex-actly, the analytical discovery algorithm (AD) works iteratively as follows. At the first iteration the algo-rithm computes a submatrix of Z , denoted Z 1 , con-taining all histories up to length one and all tests up to length one. Then it calculates the rank of this sub-matrix,  X  1 , the set of core-tests found so far, Q T 1 (these are any  X  1 linearly independent tests in Z 1 ), and the set of core-histories found so far, Q H 1 (these are any  X  linearly independent histories in Z 1 ). At the next it-eration, it computes the submatrix of Z with columns corresponding to the union of the tests in Q T 1 and all one-step extensions to all tests in Q T 1 , and with rows corresponding to the union of the histories in Q H 1 and all one-step extensions to all histories in Q H 1 . The rank  X  2 , core-tests Q T 2 , and core-histories Q H 2 are then calculated. This process is repeated until the rank remains the same for two consecutive iterations (this is AD X  X  stopping condition ). If the algorithm stops after iteration i , then it returns Q T i and Q H i as the discovered core-tests and core-histories respec-tively.
 For any controlled dynamical system with a history-prediction matrix Z of rank n , the AD algorithm can-not execute for more than n + 1 iterations; for to con-tinue iterating the rank must increase by least one at every iteration. Lemma 3 shows that the core-tests and core-histories returned by the AD algorithm are all correct , i.e., there exist core-tests and core-histories of the dynamical system that are supersets of the dis-covered core-tests and histories. Or in other words, the discovered core tests and histories correspond to linearly independent columns and rows of the full Z matrix. However, note that this does not show that the AD algorithm cannot stop prematurely (i.e., before discovering all n core tests). Thus it is possible that the algorithm may stop without having discovered all the core-tests and core-histories. We empirically in-vestigated this possibility and report the results in the section on empirical results. Subsequent to our ex-perimental work we were able to answer this question theoretically as well (and we present a very brief sketch of our theoretical result in the appendix).
 We also define an analytical discovery and learning (ADL) algorithm. Recall that the learning problem is to compute the various m vectors (  X  a  X  A , o  X  O , t  X  Q T ), in the following prediction equations: where Q T and Q H are the unknown set of complete core tests and histories. Note that these are the same equations as in the update Equation 2 except that we have separated the numerator and the denomina-tor and clustered the equations into a matrix form. The vectors P ( ao | Q H ) and P ( aot | Q H ) are composed of elements corresponding to P ( ao | h ) and P ( aot | h ) for all h  X  Q H . The matrix P ( Q T | Q H ) is the sub-matrix of Z with columns corresponding to the tests in Q T and rows corresponding to Q H . By the def-initions of Q T and Q H , P ( Q T | Q H ) is a square and invertible matrix. Under our assumption that the al-gorithm can compute P ( t | h ) for any t and h , it could compute P ( Q T | Q H ), P ( ao | Q H ) and P ( aot | Q H then calculate m ao = P  X  1 ( Q T | Q H ) P ( ao | Q H ), and m Of course, Q T and Q H are unknown. And so ADL first runs the analytical discovery algorithm to obtain correct but potentially incomplete core tests and histo-ries Q 0 T and Q 0 H (where the prime symbol on Q denotes this possibility of incompleteness). The algorithm then computes P ( Q 0 T | Q 0 H ), P ( ao | Q 0 H ) and P ( aot | Q finally calculates m 0 ao = P  X  1 ( Q 0 T | Q 0 H ) P ( ao | Q m invertible because Q 0 T and Q 0 H are correct if incom-plete. Finally, if analytical discovery returns a com-plete set of core tests and histories then it is clear that ADL will compute an exact PSR-based model of the dynamical system, else it will only be an approximate model. 3.2. Discovery and Learning (DL) algorithm Of course, the analytical discovery and learning algo-rithm will rarely be feasible because in general there will be no way of computing P ( t | h ) exactly. So now we turn to the more realistic case where the algorithm has access to the controlled dynamical system which it wishes to model and can somehow use that system to estimate P ( t | h ) rather than compute it exactly. In this paper we consider dynamical systems with reset, i.e., systems in which the algorithm can choose to reset the system to the null history (corresponding to the first row of the Z matrix). Note that while this is a strong assumption, it is not as restrictive as it may seem, e.g., a reset can always be done in any simulated sys-tem, and it can also be achieved (inefficiently) in any episodic system in which the system automatically re-sets after each episode.
 The key advantage of dynamical systems with reset is that we can repeat history and thereby repeatedly gen-erate an iid sample from P ( t | h ) for any (feasibly-sized) t and h , by the following algorithm. First, we gener-ate the history by repeatedly reseting the system and executing h  X  X  action sequence, until h  X  X  observation se-quence occurs. Then, we execute t  X  X  action sequence and return success if t  X  X  observation sequence occurs, otherwise return failure. However, this sampling pro-cedure is obviously grossly inefficient. We use the fol-lowing sampling algorithm instead. If, for whatever reason, the algorithm takes action sequence a 1 a 2 . . . a after a reset and observes sequence o 1 o 2 . . . o k , it can update the estimated prediction for all the contigu-ous history-test pairs contained in this sequence. For example, consider the sequence a 1 o 1 a 2 o 2 . Within this sequence, we can gather data for history  X  and success-ful test a 1 o 1 ; history  X  and successful test a 1 o 1 a and history a 1 o 1 and successful test a 2 o 2 . In addition, there are many unsuccessful tests: e.g., for history a 1 o all tests with a 2 o : ( o 6 = o 2 ) are unsuccessful, and so on. Note that while this sampling algorithm produces a lot of data from each sequence the empirical predic-tions for the constituent history-test pairs are corre-lated. On the other hand the samples obtained for any history-test pair from different sequences are uncorre-lated and thus any bad effect of correlation among the errors in the estimated predictions decreases rapidly with increasing number of data sequences.
 The discovery and learning (DL) algorithm we present in this section is nearly identical to the ADL algorithm of the previous section. DL takes an input parameter, n , which is a lower-bound on the number of samples we will generate for any prediction estimate we use in DL. Just like in ADL, the DL(n) algorithm will first do discovery and then learning. Discovery in ADL pro-ceeds in iterations computing the rank, a set of core tests, and a set of core histories for a submatrix of Z at each iteration. Discovery in DL(n) proceeds in iterations in exactly the same manner using the sam-pling algorithm defined above to generate at least n samples for each estimated-P ( t | h ) that we would have computed exactly in ADL. In DL(n) rank, core tests and core histories are computed from the estimated submatrix of Z instead of the exact submatrix of Z as in ADL. The main difficulty in DL(n) is that be-cause of the errors in the estimated predictions, rows and columns that were linearly dependent in ADL may become linearly independent in DL(n) and conversely rows and columns that were linearly independent in ADL may become linearly dependent in DL(n). This will introduce errors in computing rank, core tests and core histories in DL(n). Note that mistakes in one di-rection are acceptable, i.e., if the estimated predictions in DL(n) make two rows appear dependent while they would be independent in ADL, this may just make DL(n) run for more iterations than ADL but it would not introduce incorrect core tests or histories. A mis-take in the other direction is more problematic, i.e., if the estimated predictions in DL make two rows inde-pendent when they are truly dependent, then DL(n) can overestimate rank and find incorrect core tests and histories. Our solution is to be conservative in com-puting rank of the estimated submatrix of Z . Golub and VanLoan, (1996, Section 5.5.8) consider the question of estimating rank of an unknown matrix, A , given a noisy estimate of that matrix,  X  A , which is exactly the case for DL. Their method involves deter-mining a singular value cutoff  X  cutoff , then comput-ing the singular values of  X  A and defining the estimated rank as the number of singular values above the cutoff. They define  X  cutoff = k  X  A k  X  , where is the average error in the matrix entries. To find an estimate for in DL(n) we use Chebyshev X  X  inequality (with a large certainty parameter) to compute a bound on the error in each estimated prediction based on n , the number of samples that went into the estimate (here we assume iid samples, i.e., we ignore the correlations introduced in the sampling algorithm). We use this bound to cal-culate the average error and from that the singular value cutoff which then gets used in calculating the estimated rank. Note that the smaller the n the larger the singular value cutoff will be and the more conser-vative our estimate of rank will be.
 In each iteration (say k th ) of DL(n) we first compute an estimated rank,  X   X  k , and then select core histories and core tests by selecting the rows and columns that are most likely to be linearly independent. Given  X  Z k at iteration k , we incrementally remove histories and tests until exactly  X   X  k rows and columns remain. At each removal step, the history (test) to be removed is determined by removing each candidate history (test) and computing how well-conditioned the resulting ma-trix is. The history (test) whose removal yields the most well-conditioned matrix is then removed. The goal of this process is to find the ( X   X  k  X   X   X  k ) submatrix of  X 
Z k that is most well-conditioned 2 . Having the most well-conditioned full-rank submatrix serves two pur-poses. First, the rows and columns of this submatrix are most likely to turn out to be linearly independent with more sampling. Second, this submatrix is most suitable for solving linear equations via inversion, as needed in the learning algorithm presented next. The discovery part of DL(n) terminates when the estimated rank remains the same for two itera-tions, and returns a set of core tests and histories that may be incorrect and so we label them  X  Q T and  X  Q H . The learning part proceeds just as in ADL: n samples each are used to compute estimates  X  P (  X  culate  X  m ao =  X  P  X  1 (  X  Q T |  X  Q H )  X  P ( ao |  X  Q H  X  P As mentioned earlier, the only previous learning al-gorithm for PSRs was a myopic gradient-based algo-rithm (Singh et al., 2003) and it obtained mixed results on a set of dynamical systems taken from Cassandra X  X  web-page (Cassandra, 1999) on POMDPs, indicating that these systems collectively offer a range of diffi-culty. Hence we chose to test our algorithms on these same simulated systems (listed in the first column of Table 1)  X  note, however, that we added a reset action to each system that when executed takes the system back to an initial configuration. Thus, the error rates for reset learning and myopic learning cannot be com-pared directly, but the rates for myopic learning are given for reference. 4.1. On Analytical Discovery Given the uncertainty as to whether the analytical dis-covery algorithm X  X  stopping condition can lead to pre-mature termination (with correct but incomplete core tests and histories), we tested it out on the systems in the first column of Table 1. We could implement analytical discovery because we could compute p ( t | h ) for any test t and history h given the POMDP mod-els of the systems. As the column labeled analytical discovery states, for each system a full set of correct core tests and histories were found. Jaeger X  X  (2003) algorithms for discovery of interpretable OOMs and IO-OOMs use the same stopping condition and his ex-perimental work also supports a positive conjecture for the stopping condition.
 However, subsequent to this experiment we were able to prove that the discovery algorithm X  X  stopping con-dition is in fact not guaranteed to always converge to a full set of core tests and histories. Despite our neg-ative result using a carefully constructed counterex-ample (sketched briefly in the appendix), the stopping condition seems to work as desired in practice. 4.2. On Reset Learning In this section, we tested the learning part of DL as-suming perfect discovery (via analytical discovery as in the previous section). The goal here was to see if we could get significantly lower error than obtained by Singh et al. using their myopic learning algorithm (which also assumed knowledge of the full set of core tests). We ran the learning part of DL(n) for increas-ing values of n and measured error for the result-ing model parameters. The error function we used was the one used by Singh et al  X  , the average one-step prediction error per time step on a long test run with uniformly random actions. The error for a run is the true probability of observation o in history h t and  X  P ( o | h t ) is the corresponding estimated probability that uses our estimated model parameters to maintain the prediction vectors (as in Equation 2), and to com-pute the one-step predictions (as in the denominator of Equation 2). For this experiment, we used L = 10 , 000. Column 5 in Table 1 (labeled Reset Learning) shows that for each system we were able to get significantly lower error than myopic learning (shown in column 6). 4.3. Discovery and Learning We also measured the performance of our algorithm for simultaneous discovery and learning, DL ( n = 1 , 000), in two ways: by the number of core tests found, and by testing the learned model parameters using the same error function ( L = 100 , 000) as in the previous sec-tion. The last two columns show the number of core tests found and the error of the approximate PSR-model learned by DL(1000). In all cases the error was low and in all systems except Network and Float-reset all the core tests were correctly found. In Network, only 3 out of 7 core tests were found and in Float-reset only 3 out of 5 core tests were found, but regardless the error in the model was low in both cases. We also explored the dependence of the performance of DL(n) on the number of samples n . We ran DL(n) for various values of n and computed the number of core tests found as a function of n and the test-error as a function of n . Figure 2 show the results for all the problems in Table 1; in all cases error dropped sig-nificantly with a few hundred samples per history-test pair. In all cases except Network and Float-reset, the core tests were discovered with a few hundred sam-ples. As above, in Network and Float-reset the error becomes small without the discovery of all the core tests. These final results show that DL can discover and learn in these small systems fairly rapidly. Replacing the state-test prediction matrix U used in the original PSR paper with the history-prediction ma-trix Z opens up new possibilities for learning and dis-covery algorithms. In this paper we focused on the class of controlled dynamical systems with reset and presented a discovery algorithm (a first for PSRs on a general class of systems) as well as a new learning algorithm. Our empirical results on the learning sub-problem show that our algorithm was able to find good model parameters on the dynamical systems where the previous myopic algorithm had difficulty. Our empir-ical results on our discovery and learning algorithm (DL(n)), showed that discovery and learning can be fairly rapid. As future work we intend to continue em-pirical investigation of our algorithms on larger sys-tems as well as to develop new algorithms that do not require a reset. The deterministic uncontrolled system in Figure 3 al-ways produces observation 0 except in the transition from state 4 to state 0. The null history,  X  , starts the system in state 0. Consider Z 1 ( Z 2 ) the submatrix of Z for this system that contains all rows for histories up to length 1 (2) and all columns for tests up to length 1 (2). Each column of both Z 1 and Z 2 will either be all 0 X  X  or all 1 X  X . Therefore the rank of both Z 1 and Z 2 is 1 and so analytical discovery will stop after itera-tion 2 and only find one core test and one core history. In this example, there are 5 core histories and tests. (Jaeger X  X  2003 OOM discovery algorithm is subject to a similar difficulty with this problem.) A detailed ver-sion of this proof is available in Singh et al. (2004). Cassandra, A. (1999). Tony X  X  pomdp page. http://www.cs.brown.edu/research/ai/pomdp/index.html . Golub, G., &amp; VanLoan, C. (1996). Matrix computa-tions . The Johns Hopkins University Press. 3rd edi-tion.
 Jaeger, H. (2000). Observable operator processes and conditioned continuation representations. Neural Computation , 12 , 1371 X 1398.
 Jaeger, H. (2003). Discrete-time, discrete-valued ob-servable operator models: a tutorial.
 Littman, M. L. (1996). Algorithms for sequential deci-sion making (Technical Report CS-96-09). Ph.D the-sis, Department of Computer Science, Brown Uni-versity.
 Littman, M. L., Sutton, R. S., &amp; Singh, S. (2001). Pre-dictive representations of state. Advances In Neural Information Processing Systems 14 .
 Lovejoy, W. S. (1991). A survey of algorithmic meth-ods for partially observed markov decision processes. Annals of Operations Research , 28 , 47 X 65.
 Rivest, R. L., &amp; Schapire, R. E. (1994). Diversity-based inference of finite automata. Journal of the ACM , 41 , 555 X 589.
 Shatkay, H., &amp; Kaelbling, L. P. (1997). Learning topo-logical maps with weak local odometric information.
Proceedings of Fifteenth International Joint Confer-ence on Artificial Intelligence (IJCAI-97) (pp. 920 X  929).
 Singh, S., James, M., &amp; Rudary, M. (2004). Predic-tive state representation: A new theory for modeling dynamical systems. Submitted.
 Singh, S., Littman, M. L., Jong, N. K., Pardoe, D., &amp; Stone, P. (2003). Learning predictive state repre-sentations. The Twentieth International Conference
