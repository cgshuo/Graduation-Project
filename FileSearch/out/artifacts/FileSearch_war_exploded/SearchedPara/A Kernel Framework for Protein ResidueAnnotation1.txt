 Experimental methods to determine the structure and function of proteins have been out-paced with the abundance of available sequence data. As such, over the past decade several computational methods have been developed to characterize the structural and functional aspects of proteins from sequence information [26].
Support vector machines (SVMs) [28] along with other machine learning tools have been extensively used to successfully predict the residue-wise structural or functional properties of proteins [20, 4, 15]. The task of assigning every residue with a discrete class label or continuous value is defined as a residue anno-tation problem. Examples of structural annotation problems include the sec-ondary structure prediction [22,15,11], local structure prediction [14,5], and con-tact order prediction [27, 18]. Examples of function property annotation include prediction of interacting residues [20] (e.g., DNA-binding residues, and ligand-binding residues), solvent accessible surface area estimation [21,25], and disorder prediction [9, 4].
 We have developed a general purpose protein residue annotation toolkit called Pro SAT . This toolkit uses a support vector machine framework and is capable of predicting both a discrete label or a continuous value. Pro SAT allows use of any type of sequence information with resi dues for annotation. For every residue, Pro SAT encodes the input information from the residue and its neighbors. We introduce a new flexible encoding scheme that differentially weighs information extracted from neighboring residues, based on the distance to the central residue. Pro SAT also uses an exponential second-order kernel function shown to be effec-tive in capturing pairwise interaction s between residues, and hence improve the classification and regression performance for the annotation problems [15].
To the best of our knowledge, Pro SAT isthefirsttoolthatisdesignedto allow life science researchers to quickly and efficiently train SVM-based models for annotating protein residues with any desired property. The kernel functions implemented are also optimized for speed, by utilizing fast vector-based oper-ation routines within the CBLAS library [29]. Pro SAT 1 is made available as a pre-compiled binary on several differe nt architectures and environments.
In this paper we report our evaluation studies highlighting the different fea-tures of Pro SAT on the disorder prediction [4] and contact order estimation [27] problem. Pro SAT shows a statistically significant improvement on both the disor-der prediction (1%) and contact order estimation problems (20%) in comparison to previously established m ethods. We have also tested Pro SAT on the DNA-binding [20], and local structure prediction problem (results not reported here). Pro SAT improves over state-of-t he-art transmembrane helix prediction meth-ods [12], as evaluated by an independent benchmark [17]. Recently, Pro SAT was used to develop the best performing transmembrane-helix segment identification and orientation system called TOPTMH [1], and improve the comparative mod-eling ligand-binding regions of proteins [16]. The models trained by Pro SAT are also used to generate predictions for a webserver developed by us called MON-STER (Minnesota prOteiN Sequence annoTation servER) available at http:// bio.dtc.umn.edu/monster . In this paper, we will refer to protein sequences by X and Y , and an arbitrary residue by x . Given a sequence X of length n , with it are associated derived features F ,a n  X  d matrix where d is the dimensionality of the feature space. The features associated with the i th residue x i are the i th row of the matrix F denoted as F i . When multiple types of features are considered, the l th feature matrix is specified by F l . In Figure 1 (a) we show the PSI-BLAST derived position specific scorin g matrix of dimensions n  X  20 (discussed in Section 3.2).
In order to encode information for a residue Pro SAT uses the information from neighboring residues as well. Pro SAT uses a w mer -based encoding to capture se-quence information for residue x i to perform the residue-wise prediction. Pro SAT uses the (2 w +1) rows of the matrix F , F i  X  w ...F i + w to encapsulate the feature information associated with the w mer centered at residue x i . This submatrix is denoted by w mer ( F i ) and is linearized to generate a vector of length (2 w +1) d , where d is the dimension of the matrix F .

As seen in Figure 1(b) for the circled residue, three residues above and below are also selected and the corresponding in-formation from the fea-ture matrix is extracted.
 Further Figure 1(c) rep-resents the linearized sub-matrix as a vector which encodes the information for the problem. We approach the pro-tein residue annotation problem by utilizing lo-cal sequence information around each residue in a supervised machine learn-ing framework. We use support vector machines (SVM) [28] in both clas-sification and regression formulations to address the problem of annotating residues with discrete la-bels and continuous values respectively. We use the publicly available SVM light program [10] for the discriminatory learning. 3.1 Support Vector Classi fication and Regression The task of assigning a label to the residue x from one of the K possible anno-tation labels is a typical multiclass classification problem. The general strategy is to build K one-versus-rest binary SVM cla ssification models that assign a residue to be in a particular class or not. Let A + refer to the residues with on particular label, the positive class, and A  X  refer to the remaining residues, the negative class. In its dual formulation, a support vector machine learns a classification function f ( x )oftheform where  X  + i and  X   X  i are non-negative weights that are computed during training to provide the best possible prediction, and K ( ., . )isa kernel function designed to capture the similarity between pairs of residues. Having learned the func-tion f ( x ), a new residue x is predicted to be positive or negative depending on of x to be a member of the positive or negative class and can be used to obtain a meaningful ranking of a set of the residues.

We use the error insensitive support vector regression -SVR [28] for learning a function f ( x ) for estimation in case of determining a quantity, as in the case of where y i is the continuous value to be estimated for residue x i ,the -SVR aims to learns a function of the form computed during training by maximizing a quadratic objective function. The objective of the maximization is to determine the flattest f ( x ) in the feature space and minimize the estimation errors for instances in  X  +  X   X   X  . Hence, The parameter controls the width of the regression deviation or tube. 3.2 Sequence-Ba sed Information Pro SAT can use any general user-supplied fea tures. In our empirical evaluation for a given protein X of length n we encode the sequence information using PSI-BLAST position specific scoring matrices , predicted seconda ry structure, and position independent scoring matrices like BLOSUM62. These feature matrices are referred to as P , S ,and B , respectively and are described below. Position Specific Scoring Matrices. The profile of a protein is derived by com-puting a multiple sequence alignment of it with a set of sequences that have a statistically significant sequence similarity, i.e., they are sequence homologs as ascertained by PSI-BLAST [2]. In Figur e 1 (a) we show the PSI-BLAST derived position specific scoring matrix for a sequence of length n . The dimensions of this matrix n  X  20. For every residue the PSI-BLAST matrix captures evolution-ary conservation information by providing a score for each of the twenty amino acids.
 The profiles in this study were generated using the latest version of the PSI-BLAST [2] (available in NCBI X  X  blast release 2.2.10 using blastpgp -j 5 -e 0.01 -h 0.01 ) searched against NCBI X  X  NR database that was downloaded in November of 2004 and contains 2,171,938 sequences.
 Predicted Secondary Structure Information. We use YASSPP [15] to predict sec-ondary structure and generate a position-specific secondary structure matrices. For a length n sequence, the result is S ,a n  X  3 feature matrix. The ( i, j )th entry of this matrix represents the propensity for residue i to be in state j , where j  X  X  1 , 2 , 3 } corresponds to the three secondary structure elements: alpha helices, beta sheets, and coil regions.
 Position Independent Scoring Matrices. A less computationally expensive fea-ture of protein sequences may be obtained from a position independent scoring matrix such as the BLOSUM62 substitution matrix. The primary motivation for using BLOSUM62-derived feature vectors is to improve the classification accu-racy in cases where a sequence does not have a sufficiently large number of ho-mologous sequences in NR. In these cases PSI-BLAST fails to compute a correct alignment for some segments of the sequence giving a misleading PSSM [9, 15]. To make effective use of Pro SAT  X  X  capabilities we create a n  X  20 feature matrix, referred to as B , where each row of the matrix is a copy of the BLOSUM62 row corresponding to the amino acid at that position in the sequence.

By using both PSSM-and BLOSUM62-based information, the SVM learner can construct a model that is partially based on non-position specific infor-mation. Such a model will remain valid in cases where PSI-BLAST could not generate correct alignm ents due to lack of homology to sequences in the nr database [15]. 3.3 Kernel Functions A kernel function computes a similarity between two objects and selection of an appropriate kernel function for a problem is key to the effectiveness of support vector machine learning. We consider se veral individual kernels of interest and then proceed to describe combinations of kernels used in this study. Throughout this section we use F and G be the feature matrix for sequences X and Y respectively. A specific residue of X is denoted x i and its associated vector of features is F i .
 Window Kernel. Our contribution in this work is a two-parameter linear window-kernel, denoted by W w,f which computes the similarity between two w mers , w mer ( x i )and w mer ( y j ) according to their features w mer ( F i )and w mer ( G j ), respectively. The kernel function is defined as
The parameter w governs the size of the w mer considered in computing the kernel while f offers control over the fine-grained versus coarse-grained sections of the window. Rows within  X  f contribute an individual dot product to the total similarity while rows outside this range are first summed and then their dot product is taken. In all cases f  X  w and as f approaches w , the window kernel becomes simply a sum of the dot products, the most fine-grained simi-larity measure considered. This window encoding is shown in Figure 1(d) where the positions away from the central residue are averaged to provide a coarser representation, whereas the positions closer to the central residue provide a finer representation. The rationale behind this kernel design is that some problems may require only approximate information for sequence neighbors which are far away from the central residue while nearby sequence neighbors are more im-portant. Specifying f w merges these distant neighbors into only a coarse contribution to the overall similarity, as it only accounts for compositional infor-mation and not the specific positions where these features occur. The window kernel is defined as a dot-product, which makes it equivalent to linear kernel with a feature encoding scheme that takes int o account the two variable parameters, w and f . Hence, we can embed the dot-product based W within other complex kernel functions.
 Exponential Kernels. Another individual kernel we use extensively is the second order exponential kernel, K soe , developed in our earlier works for secondary structure and local structure information prediction [15, 23]. Given any base kernel function K , we define K 2 as which is a second-order kernel in that it computes pairwise interactions between the elements x and y . We then define K soe as which normalizes K 2 and embeds it into an exponential space.

We also use the standard radial basis kernel function ( rbf ), defined for some and using normalized unit length vectors the standard rbf kernel can be shown equivalent (upto a scaling factor) to a first order exponential kernel obtained by removing the K 2 ( x, y ) term in Equation 4, and plugging the modified kernel in Equation 5.

In this paper, we denote the soe to be the kernel K soe using the W w,f as the base, rbf to be the kernel K rbf using the normalized form with W w,f as the base, and lin to be the base linear kernel W w,f . 3.4 Integrating Information To integrate the different information, we use a linear combination of the kernels derived for different feature matrices. Consider two sequences with features F l and G l for l =1 ,...,k , our fusion kernel using the is defined where the weights  X  l are supplied by the user. Note the soe kernel in Equation 6 can be replaced by the lin ,and rbf kernels.

In the future we intend to explore the possibility of automatically learning the weights  X  l . This can be done by using some of the recent multiple kernel integration work that combines heterogeneous information using semidefinite programming [19], second order cone programming [3], and semi-infinite linear programming [24].
 Pro SAT was tested on a wide variety of local structure and function prediction problems. Here we present a case study on the disorder prediction, contact order estimation and transmembrane-helix pr ediction problems. We review the meth-ods used for solving the problems, and provide comparative results by using standard benchmarks which are described below.

Pro SAT was also tested on the DNA-binding prediction problem [20], ligand-binding prediction problem, solvent accessibility surface area estimation [25,21], and local structure alphabet prediction p roblem [5]. The results of these exper-iments are not reported here for sake of brevity. Pro SAT showed comparable to the state-of-the-art prediction systems for the different problems. 4.1 Experimental Protocol The general protocol we used for evaluating the different parameters, and fea-tures, as well as comparing to previously established studies remained fairly consistent across the different problems. In particular we used a n -fold cross validation methodology, where 1/n th of the database in consideration was used for testing and the remaining dataset was used for training, with the experiment being repeated n times. 4.2 Evaluation Metrics We measure the quality of the methods u sing the standard receiver operating characteristic ( ROC )scores.The ROC score is the normalized area under the curve that plots the true positives against the false positives for different thresh-olds for classification [8]. We also compute other standard statistics, and report the F 1 score which takes into account b oth the precision and recall for the prediction problem.

The regression performance is assessed by computing the standard Pearson correlation coefficient ( CC ) between the predicted and observed true values for every protein in the datasets. We also compute the root mean square error rmse between the predicted and observed val ues for every proteins. The results reported are averaged across the different proteins and cross validation steps. For the rmse metric, a lower score implies a better quality prediction.
We also compute a statistical significance test, errsig to differentiate between the different methods. errsig is the significant difference margin for each score and is defined as the standard deviation divided by the square root of the number of proteins. 4.3 Disorder Prediction Some proteins contain regions which are intrinsically disordered in that their backbone shape may vary greatly over time and external conditions. A disor-dered region of a protein may have multiple binding partners and hence can take part in multiple biochemical processes in the cell which make them criti-cal in performing various functions [7]. Disorder region prediction methods like IUPred [6], Poodle [9], and DISPro [4] mainly use physiochemical properties of the amino acids or evolutionary information within a machine learning tool like bi-recurrent neural network or SVMs.

Pro SAT was evaluated on the disorder prediction problem by training binary classification model to discriminate between residues that belong to part of dis-ordered region or not. For evaluating the disorder prediction problem we used the DisPro [4] dataset which consisted of 723 sequences (215612 residues), with the maximum sequence identity between sequence pairs being 30%.

We used the PSI-BLAST profile matrix denoted by P ,aBLOSUM62derived scoring matrix denoted by B , and predicted secondary st ructure mat rix denoted by S feature matrices both independently, and in combinations . We varied the w ,and f parameters for the W , and also compared the lin , rbf ,and soe kernels. Table 1 shows the binary classification performance measured using the ROC and F 1 scores achieved on the disorder dataset after a ten fold cross validation experiment, previously used to evaluate the DISPro prediction method.
Comparing the ROC performance of the P soe , P rbf ,and P lin models across different values of w and f used for parameterization of the base kernel ( W ), we observe that the soe kernel shows superior performance to the lin kernel and slightly better performan ce compared to the normalized rbf kernel used in this study. This verifies results of our previous studies for predicting secondary structure [15] and predicting RMSD between subsequence pairs [23], where the soe kernel outperformed the rbf kernel.

The performance Pro SAT on the disorder prediction problem was shown to improve when using the P , B ,and S feature matrices in combination rather than individually. We show results for the PS and PSB features in Table 1. The flexible encoding introduced by Pro SAT shows a slight merit for the disorder prediction problem. These improvements are statistically significant as evaluated by the errsig measure.

The best performing fusion kernel imp roves the accuracy by 1% in comparison to DisPro [4] that encapsulates profile, secondary structure and relative solvent accessibility information within a bi-recurrent neural network. 4.4 Contact Order Estimation Pairs of residues are consid ered to be in contact if their C  X  atoms are within a threshold radius, generally 12  X  A. Residue-wise contact order [27] is an aver-age of the distance separation between contacting residues within a sphere of set threshold. Previously, a support vector regression method [27] has used a combination of local sequence-derived information in the form of PSI-BLAST profiles [2] and predicted secondary structure information [11], and global infor-mation based on amino acid composition and molecular weight for good quality estimates of the residue-wise contact order value. Amongst other techniques, critical random networks [18] use PSI-BL AST profiles as a global descriptor for this estimation problem.

Pro SAT was used to train -SVR regression models for estimating the residue-wise contact order on a previously used dataset [27] using the fusion of P and S features, with a soe kernel. This dataset consisted of 680 sequences (120421 residues), and the maximum pairwise sequence identity for this dataset was 40%.
In Table 3 we present the regression performance for estimating the residue wise contact order by performing 15-fold cross validation. These results are eval-uated by computing the correlation coeffi cient and rmse values averaged across the different proteins in the dataset.

Analyzing the effect of the w and f parameters for estimating the residue-wise contact order values, we observe that a model trained with f&lt;w generally shows better CC and rmse values. The best models as measured by the CC scores are highlighted in Table 3. A model with equivalent CC values but having a lower f value is considered better because of the reduced dimensionality achieved by such models.

The best estimation perfo rmance achieved by our -SVR based learner uses a fusion of the P and S feature matrices and improves CC by 21%, and rmse value by 17% over the -SVR technique of Song and Barrage [27]. Their method uses the standard rbf kernel with similar local sequence-derived amino acid and predicted secondary struct ure features. The major improvement of our method can be attributed to our fusion-based kernel setting with efficient encoding, and the normalization introduced in Equation 5. 4.5 Transmembrane-Helix Prediction Proteins which span the cell membrane have proven difficult to crystallize in most cases and are generally too large for NMR studies. Computational meth-ods to elucidate transmembrane protein structure are a quick means to obtain approximate topology. Many of these proteins are composed of a inter-cellular, extra-cellular, transition, and membrane portions where the membrane portion contains primarily hydrophobic residues in helices (a multi-class classification problem). Accurately labeling these four types of residues allows helix segments allows them to be excluded from function studies as they are usually not involved in the activity of the protein. MEMSAT [12] in its most recent incarnation uses profile inputs to a neural network to predict whether residues in a transmem-brane protein are part of a transmembrane helical region or not.

Kernytsky and Rost have benchmarked a number of methods and maintain a server to compare the performance of new methods which we employ in our evaluation [17]. We evaluate Pro SAT using this independent static benchmark. Firstly, we perform model selection on a set of 247 sequences used previously by the Phobius algorithm [13]. We use the P soe kernel with w and f parameters set to 7 to train a four-way classification model for predicting the residue to be in either the helical region, non-helical region, inter-cellular region, and extra-cellular region. Using the trained model we annotate each of the 2247 sequences in the static benchmark (no true labels known to us) 2 . The performance of Pro SAT is shown in Table 4, which is better in comparison to state-of-the-art methods. The predictions from Pro SAT were further smoother using a second-level model to build the best performing transmembrane helix identification system called TOPTMH [1]. The reader is encouraged to find more details about experimental results in the TOPTMH [1] study. 4.6 Runtime Performance of Optimized Kernels We also benchmark the learning phase of Pro SAT on the disordered dataset com-paring the runtime performance of the program compiled with and without the CBLAS subroutines. These results are reported in Table 2 and were computed on a 64-bit Intel Xeon CPU 2.33 GHz processor for the P lin , P rbf ,and P soe kernels varying the w mer size from 11 to 15. Table 2 also shows the number of kernel evaluations for the different models. We see speedups ranging from 1.7 to 2.3 with use of the CBLAS library. Similar experiments were performed on other environments and other prediction problems, and similar trends were seen. In this work we have developed a general purpose support vector machine based toolkit for easily developing predictive models to annotate protein residue with structural and functional properties. Pro SAT was tested with different sets of features on several annotation problems. Besides the problems illustrated here Pro SAT was used for developing a webserver called MONSTER 3 that predicts several local structure and functional properties using PSI-BLAST profiles only. Pro SAT also showed success in predicting and modeling ligand-binding site re-gions from sequence information only [16].

The empirical results presented here showed the capability of Pro SAT to ac-cept information in the form of PSI-BLAST profiles, BLOSUM62 profiles, and predicted secondary structure. Pro SAT wastestedwiththe soe , rbf ,and lin kernel function. In addition, the results showed that for some problems (contact order estimation), by incorporating local information at different levels of granu-larity with the flexible encoding, Pro SAT was able to achieve better performance when compared to the traditional fine-grain approach.

Presently we are studying different multi ple kernel integration methods that would automatically weight the contribution of different information in Equa-tion 6. An optimal set of weights can be learned using semi-definite program-ming [19], and semi-infinite linear programming [24]. Currently, Pro SAT auto-matically performs a grid search over the different parameters for selecting the best model. The multiple kernel integra tion work can also be used to select the best model. This would allow the biologist to use Pro SAT effectively. Further like the TOPTMH [1] system, we would like to smooth the predictions obtained from the residue-level predictors. This can be done by training a second level model or incorporating domain specific rules. A second level SVM-based model [15] has been implemented in Pro SAT already, and preliminary results show good promise.

We believe that Pro SAT provides to the practitio ners an efficient and easy-to-use tool for a wide variety of annotation problems. The results of some of these predictions can be used to assist in solving the overarching 3D structure prediction problem. In the future, we intend to use this annotation framework to predict various 1D features of a pro tein and effectively integrate them to provide valuable supplementary information for determining the 3D structure of proteins.

