 The explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advances in video retrieval, recommendation and discovery. In this paper, we focus on the task of video suggestion, commonly found in many online applications. The current state-of-the-art video suggestion techniques are based on the collaborative filtering analysis, and suggest videos that are likely to be co-viewed with the watched video. In this paper, we propose augmenting the collab-orative filtering analysis with the topical representation of the video content to suggest related videos. We propose two novel methods for topical video representation. The first method uses information retrieval heuristics such as tf-idf, while the second method learns the optimal topical represen-tations based on the implicit user feedback available in the online scenario. We conduct a large scale live experiment on YouTube traffic, and demonstrate that augmenting col-laborative filtering with topical representations significantly improves the quality of the related video suggestions in a live setting, especially for categories with fresh and topically-rich video content such as news videos. In addition, we show that employing user feedback for learning the optimal topi-cal video representations can increase the user engagement by more than 80% over the standard information retrieval representation, when compared to the collaborative filtering baseline.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Video retrieval, related video suggestion, video representa-tion
The World Wide Web project was originally conceived as a means to publish, share and retrieve textual information and did not aim  X  X o do research into fancy multimedia facil-ities such as sound and video X  1 . However, with the growth of the web, video and other multimedia formats became an increasingly large and important part of it. The recent pro-liferation of mobile devices that enable instant video cap-turing, sharing and consumption, further accelerated this trend.

As an example, at the time of the writing of this pa-per, over 6 billion hours of video are watched monthly on YouTube, the most popular video-sharing website, an in-crease of 50% compared to last year. Furthermore, 100 hours of new video content are uploaded to YouTube every minute [2].

These statistics demonstrate that there is an ever-growing abundance of video content on the web. This content ranges from six-second long mobile video clips on Vine and user-uploaded footage on YouTube, to news stories on news web-sites and TV shows and movies on subscription-based stream-ing services such as Hulu and Netflix.

This abundance of content supply and demand has brought about an urgent need for increasingly sophisticated tech-niques for video retrieval, recommendation and discovery. In particular, in this paper, we focus on the task of video suggestion , which is common in many online applications.
We define video suggestions as a ranked list of related videos shown to the user in response to the video that she is currently viewing (which we refer to as the watch video ). Video suggestions are an important part of the experience of watching video on the web today, and they are steadily becoming prevalent on more and more video websites. As an illustration, Figure 1 showcases three major use cases for video suggestions, including suggestion of (a) news stories on CNN, (b) TV show episodes on Hulu, and (c) related videos on YouTube.

As shown in previous work, successful deployment of video suggestion algorithms can result in a significant positive ef-
As stated in the original HyperText proposal: http://www.w3.org/Proposal.html . video on CNN, Hulu and YouTube websites. fect on user engagement and user experience [7, 13]. For instance, Davidson et al. [13] show that co-visitation based video suggestions more than double the click-through rate, compared to a baseline that does not take the watch video into account.

Video suggestions can be either based on the watch video alone [7], or can be personalized by incorporating informa-tion about the user (e.g., search history [11]). Current state-of-the-art video suggestion systems are based on the collab-orative filtering analysis. Most generally, videos suggested by these systems are likely to be co-viewed with the watch video by the majority of the users [7, 11, 13].

While this approach works well for popular and often watched videos, it is less applicable to fresh videos or tail videos with few views, since they have very sparse and noisy co-view data. For these videos, collaborative filtering anal-ysis based solely on co-views may yield either low-quality suggestions or no suggestions at all.

To address the problem of tail content in other domains such as recommendations based on textual information, re-searchers developed hybrid approaches [10] that combine in-formation about the item content with the collaborative fil-tering information to improve recommendation. While there have been some small scale studies on hybrid recommenda-tion systems for video suggestion [31], this paper, to the best of our knowledge, is the first published research study of a large scale deployment of such a system.

To achieve the goal of content-based video representation, we first model a video as a set of topics that are mined from a variety of sources including the video metadata, frequently used uploader keywords, common search queries, playlist names, Freebase entities and Wikipedia articles. These top-ics are then used to retrieve related videos for suggestion.
Thus, our approach to content-based video suggestion is akin to the standard information retrieval scenario. The topical representation of the watch video is the query that is issued to the inverted index that stores the topical video representations as documents . Since we cast the video sug-gestion problem as a retrieval over an inverted index, we can use query optimization algorithms [8, 14] to efficiently find the optimal video suggestions even in a very large corpus.
The highest ranked documents retrieved in response to the query are returned as content-based video suggestions. These suggestions can be either directly shown to the user, or used to augment the output of the co-view based video suggestion system.

In order to achieve an effective topical video representa-tion, we assign weights to each topic associated with a video. We propose two techniques for topical video representation.
The first technique is based on the well known informa-tion retrieval heuristics such as computing topic frequencies and topic inverse document frequencies. The second tech-nique leverages the implicit user feedback (such as video co-watches) available in the online setting. This feedback is used for a supervised learning of the optimal topic weights.
We empirically evaluate these two topical representation methods. We find that both of these representations have a significantly positive effect on the quality of video sug-gestions. Furthermore, our evaluation demonstrates that learning topic weights from user feedback can increase the user engagement (compared to the collaborative filtering ap-proach) by more than 80% over the standard information retrieval representation.

To evaluate our approaches, we conduct a large scale live experiment on millions of YouTube video. The live exper-iment demonstrates that a hybrid video suggestion system that incorporates topic-based retrieval significantly outper-forms a purely co-view based suggestion system. These im-provements are especially high for fresh videos and videos with rich topical representations.
 There are several important contributions in this paper. First, we formulate the video suggestion task as an informa-tion retrieval problem and demonstrate that this formulation enables effective and efficient deployment of video suggestion on a very large scale. Previous work on video retrieval was mainly performed offline on relatively small collections such as TRECVID collections [20], or small pre-selected samples of online videos [31]. To the best of our knowledge, this is the first published study to address the challenges of real-time video retrieval using topical representation in a large scale web collection.

Second, we demonstrate that the richness of the implicit user feedback available in the online setting can be leveraged to improve the effectiveness of topical video representations. To this end we employ a novel learning technique that de-rives the optimal topic weights from co-view information.
Third, we describe the architecture of the hybrid video suggestion system that combines the collaborative filtering information with the topic-based video information.
Finally, we thoroughly evaluate the performance of our system using both user simulation and a large scale live ex-periment. This evaluation demonstrates the superiority of the proposed hybrid video suggestion system to the current state-of-the-art collaborative filtering approaches.
The rest of the paper is organized as follows. Section 2 describes video representations using topics. Section 3 de-scribes retrieval using topics weighted by information re-trieval heuristics, and Section 4 describes retrieval using topic transitions learned from user watch behavior. Section 5 provides a holistic view of the proposed video suggestion system. In Section 6 we overview the related work. Sec-tion 7 desribes the evaluation of our techniques using both user simulation and a large scale live experiment. Section 8 concludes the paper.
In this section, we discuss how a video is represented via a set of associated topics . These topics serve as a semantic representation of the video content, and are used by the retrieval algorithms in the next sections. The topics are derived from a variety of sources that can be associated with a video such as video metadata, uploader keywords, common search queries, playlist names, etc.

For the purpose of this paper, we use the notation V both to refer to the video itself, and the set of topics associated with it, interchangeably. Therefore, the notation  X   X  V is used to state that video V is annotated with topic  X  .
There is a large variety of methods described in the scien-tific literature (e.g., [16, 27, 28, 29, 30] among many others) on how to derive a set of annotations for images and videos. Some of these methods involve image and video analysis, while others rely on textual annotations, anchor text infor-mation and query logs.

In this paper, we follow the latter approach. This is due to the fact that there is a plethora of sources that can be utilized to annotate online videos with semantic informa-tion, including the video metadata, frequently used uploader keywords, common search queries, playlist names, and even sources from different domains such as Freebase entities and Wikipedia articles. The interested reader may refer to the YouTube official blog [1, 3] or work by Simonet [25] for more information about the YouTube video topic assignment pro-cess.

For the purpose of the retrieval algorithms discussed in the next sections, we will assume that we can reliably annotate a given video V with a set of topics using the sources described above [1, 3]. These topics represent the different semantic aspects of the video and map the video representation V into some existing concept space (e.g., Freebase concepts as shown in [3]).
 As an example, Figure 2 demonstrates a movie trailer World War Z along with the topics associated with it and the topic weights. As can be seen in Figure 2, these topics capture several aspects of the video content including movie title, genre and leading actor, as well as information about the video itself.

In the rest of this paper, we will assume that the topical annotation can serve as a faithful representation of the video content. Therefore, we will use these annotations to retrieve videos related to the watch video  X  video that is currently being viewed by the user.

For this purpose, we will represent both the watch video and the potentially related videos as vectors of topic weights, Figure 2: Example of topics associated with a video, and their corresponding weights. and compute a vector dot product to determine a similarity of a &lt; watch video, related video &gt; pair. Related videos will then be ranked by their similarity score to the watch video.
It is important to note that the topical video represen-tation described in the previous section has very different characteristics from the co-view video representation, used in previous work on related video discovery [7]. In the co-view video representation, each video is represented by a node in the co-view graph. A node is then linked to the other nodes in the graph if it is often viewed with them in the same session [7]. In this approach, related video sugges-tion is done based on the nodes that are in the proximity to the watch video in the co-view graph.

Therefore, in the co-view approach two videos are poten-tially related if and only if they have a strong connection in the co-view graph, i.e., they were often watched in the same session. This approach works well for popular videos with many views and high node connectivity. However, it is much less reliable for videos with little or no views. For these videos, using the co-view approach may lead to spuri-ous results, or yield no candidates for suggestions.
In contrast, the topical video representation does not re-quire an explicit co-view information to deem two video re-lated. Instead, if two videos share (some of) the same topics they will be related, even if they were never watched in the same session before.

This semantic approach enables discovery of fresh, di-verse and relevant content. It has been shown that implicit user feedback is often influenced by presentation bias [32], and click metrics do not fully correlate with relevance [21]. Topic-based video suggestion can therefore limit the rich get richer effect that can potentially arise when using solely the co-view information and disregarding the video content.
Video representation using topic weight vectors is akin to the bag-of-words document representation often used in the information retrieval applications. Therefore, we index the topic video representations in a standard inverted index structure [19] for efficient retrieval. Each video is repre-sented using a topic weight vector, and indexed as an entry in the posting lists of its corresponding topics.
The inverted index structure enables efficient scoring of the related candidate videos in response to a watch video. Since we are only interested in a limited number of highest scoring related videos for a given watch video, we employ the WeakAnd query optimization technique first proposed by Broder et al. [8]. WeakAnd query optimization fully scores only a small fraction of the videos with score upper bound greater than a given threshold. This threshold is revised at query runtime as more videos are scored. To avoid fully scoring the document, the WeakAnd optimization maintains an upper bound of a document score, based on the maximum weight in each of the posting lists evaluated in response to the query. For more detailed description of the WeakAnd algorithm see [8, 14]. In Section 2.3 we described the topic indexing process. In this section, we use this topic index to develop a related video suggestion algorithm that is based on the standard information retrieval practices.

Recall that the videos in our system are represented as vectors of topic weights. Given this representation, for a watch video and related video pair  X  V W ,V R  X  we derive the following score sc ( V W ,V R ) = q ( V R ) X The score in Equation 1 has several components that are based on standard information retrieval practices.

First, the topic count function c (  X ,V ) returns a normal-ized count of videos that are annotated with the topic  X  and are co-viewed with the video V . The topic count function estimates the topicality of video V with respect to topic  X  .
Second, log(1 + df (  X  )) is an inverse document frequency component that penalizes frequently occuring topics. In-verse document frequency demotes overly broad, vague and non specific topics, similarly to the idf term weighting in information retrieval applications.
 Third, Equation 1 includes an indicator function where df max is set to some large constant. The indicator function I s (  X  ) is inspired by the stopword removal in infor-mation retrieval applications, which removes very frequent stopwords at either indexing time or query time. We found that such stopword removal technique is useful for disregard-ing noisy and redundant topic matches from score computa-tion, and improves both efficiency and effectiveness at query time.

Finally, Equation 1 takes into account the overall quality of the related video q ( V R ). Function q ( V R ) is based, among other factors on the video age, uploader,  X  X humbs up X  and  X  X humbs down X  counts, video popularity and freshness.
Note that since all the videos have roughly the same (small) number of topics associated with them, we do not apply any document length normalization method such as cosine similarity or pivoted length normalization [26]. Instead we simply use an unnormalized vector dot product as a scoring function in Equation 1.
In Section 3 we described a retrieval approach that assigns topic weights based on a set of information retrieval heuris-tics (topic counts, inverse document frequency and stopword removal). In this section, we show that given the richness of the implicit user feedback available in the online setting, it is possible to learn the optimal transitions between the topics in the watch and the related videos. These transitions can then be directly leveraged for related video retrieval. Consider, for instance, the example of the trailer for the World War Z movie shown in Figure 2. Assuming that the topic weights in Figure 2 are derived from the nor-malized co-occurence counts in Equation 1, the topic Hor-ror Movie is weighted lower than the topic World War Z . However, the Horror Movie topic might be important for finding other related videos such as a trailer for the Juras-sic Park IV movie. Therefore, a topic can have relatively low co-occurence counts for both the watch and the related videos, yet still be beneficial for the retrieval of relevant re-lated videos.

Based on this intuition, we propose a novel machine learn-ing approach that takes into account topic interactions and learns topic transition weights based on implicit user feed-back. Our approach is based on pairwise classification of &lt; watch video, related video &gt; pairs.

In Sections 4.1 and 4.2 we describe how the topic tran-sitions are represented. Then, in Section 4.3 we describe the optimization of the topic transition weights. Finally, in Section 4.4 we describe related video retrieval using topic transition weights.
Suppose that two potentially related videos are suggested to the user in response to the watch video V W . One of the videos, V (+) R , was clicked and viewed by the user. The other video, V (  X  ) R , was ignored by the user.

Most generally, we seek topic transition weight assign-ments such that the suggested related video V (+) R preferred by the model to the suggested video V (  X  ) R . This problem formulation is inspired by the pairwise classification approach which is common in learning-to-rank applications (e.g., see Burges et al. [9] or Joachims [17]).

Formally, we represent a potentially related video V R that was suggested in response to a watch video V W using a bi-nary feature vector where T is the lexicon of all available topics, and I V R transition function from topic  X  W to  X  R such that Given a pair of related videos we represent the pair using a ternary feature vector: where the transition function I P R (  X  W , X  R ) is defined such that Figure 3: Topic transition matrix. Diagonal topic transitions are marked in grey.

It is easy to show that following the formulation in Equa-tion 2, transition function I P R can take three values: 1. I P R (  X  W ,  X  R ) = +1 iff  X  R is associated only with the 2. I P R (  X  W ,  X  R ) =  X  1 iff  X  R is associated only with the 3. I P R (  X  W ,  X  R ) = 0 in all other cases.
Note that in the unconstrained form presented in Equa-tion 2, we capture transitions between all pairs of topics in the lexicon T . Therefore, given a transition matrix in Fig-ure 3 we need to learn a transition weight for each cell in the matrix.

This approach is clearly infeasible for large open domain lexicons, which can contain millions of unique topics, since there are T 2 possible transitions. Moreover, the transition matrix will be very sparse since there will be relatively few topic pairs with observed transitions. Even for pairs where we do observe transitions, the number of observed examples may be too small to learn a reliable model.

Therefore, we restrict ourselves to learning the transi-tion weights only for the diagonal of the transition ma-trix (marked in grey in Figure 3). This makes the learning tractable, and leads to a more reliable model with less over-fitting, since a diagonal transition is likely to be observed more frequently than a transition between two arbitrary top-ics.

Also, note that the diagonal transitions mirror the infor-mation retrieval approach described in Section 3, where only matches between topics that occur both in the watch and the related video are considered (Equation 1). This facil-itates a fair comparison between the performance of these approaches, as discussed in Section 7.
Given the diagonal topic transition representation described in Section 4.2, the train set S P is defined over all the non-discordant pairs of related videos, sampled from user traffic. For the i -th pair P i  X  S P , a transition vector is defined as such that x ij denotes the j -th diagonal topic transition for the i -th pair of related videos.

Then, we seek a weight vector w  X  , which minimizes the l -regularized logistic loss over the train set S P .
Algorithm 1: The parallel-update optimization al-gorithm [12] for learning topic transition weights. t = 1; repeat until convergence or max # of iterations reached ;
Formally, given the loss function over an arbitrary set of examples S , parameterized by a weight vector w where  X  &gt; 0 is the regularization parameter, we seek an optimal vector w  X  such that
There is a wide variety of methods that could be employed to minimize the loss function in Equation 3. In this paper, we use the parallel-update optimization algorithm, first pro-posed by Collins et al. [12].

The parallel-update algorithm for finding the optimal di-agonal transition weight vector w  X  is outlined in Algorithm 1. The parallel-update algorithm iteratively updates the weight vector w t at round t by vector  X  t which is com-puted using the loss on each individual instance q t ( i ).
The main advantage of the parallel-update algorithm is its scalability in the number of examples and the number of features. Note that in Algorithm 1, the q t ( i ) and  X  parameters are computed independently for each instance i and transition j , which naturally enables parallelization of the weight updates. In addition, the parallel-update al-gorithm can continuously update the weight vector w new instances are added to the train set. This property is particularly important in the online setting, where training instances are continuously added based on new user feed-back.
Given the optimal diagonal topic transition weight vector w  X  computed by the Algorithm 1, we use it to rank the related video suggestions in response to a watch video V W The scoring function is simply given by
Note that Equation 5 does not require utilizing any ad-ditional heuristics such as idf weighting, in addition to the transition weights. This is due to the fact the weights w optimal for transitioning from watch video with topic  X  i a related video with topic  X  i , according to the loss criterion defined in Equation 3.
In this section, we provide a holistic overview of the gen-eral architecture of the experimental video suggestion sys-tem, which was designed for evaluating the topic retrieval methods described in the previous sections. This system was designed specifically for the purpose of the experiments in this paper, and does not correspond to any system used in production.

We built this experimental system in order to integrate both the co-view based and the topic-based retrieval meth-ods into a single video suggestion system, shown in Figure 4. The purpose of this integration is to demonstrate the benefits of our topic-based video suggestion approach, as compared to the standard co-view based video suggestion approaches [7, 13].

For a given watch video V W , two retrieval processes are run in parallel. First, we retrieve a set of k related candi-dates based on the link structure of the co-view graph. See Baluja et al. [7] (surveyed in Section 2.2) for the detailed description of an implementation of such a process. Most generally, this process retrieves all the videos that were most oftenly co-watched with the watch video V W , regardless of their topicality.

Second, we retrieve top-k ranking candidate videos using a topic-based retrieval process. This process is based either on the weighted topic retrieval or the diagonal topic transitions, described in Section 3 and Section 4, respectively.
This set of k + k highest ranked results from both co-view and topic retrieval processes is sent to the reranking model. For fairness of our experimental setup, we ensure that the number of candidate videos from both co-view and topic retrieval processes are equal.

The final reranking model is out of the scope of this pa-per. However, for our purposes, it is important to make two general comments about it.

First, the reranking model takes into account a very large number of features based on the watch-related video pair. This makes the reranking model prohibitively expensive to run on the entire video corpus. Instead, the retrieval pro-cesses (based on either co-views or topics) provide a small set of tens of potentially relevant video candidates to the reranking model. By augmenting the co-view retrieval with the top results from the topic retrieval, we seek to diversify and improve this candidate set.

Second, to avoid bias in our evaluation, the reranking model excludes any topic-related features, which are used in the topic retrieval process. Therefore, we strive to ensure that the candidates from the topic retrieval process are not given an unfair advantage by the reranking model.

In the experiments in Section 7 we describe how the re-lated video suggestion system design outlined in Figure 4 is used to evaluate the benefits of the topic-based video re-trieval using both user simulation and a live traffic experi-ment.
In this paper, we propose two retrieval methods for re-lated video suggestion. The first retrieval method, described in Section 3, is based on the standard practices in informa-tion retrieval including idf term weighting [23] and stopword removal [19] applied to the topical video representations.
The second retrieval method, described in Section 4, is based on learning the optimal transition weights between the watch video and the related video topics. To learn the weights, we use a pairwise classification approach, as pro-posed by e.g., Burges et al. [9] and Joachims [17], which is common in learning-to-rank applications [18].

The novelty of our approach is due to the fact that we are able to leverage the rich user feedback available in the online setting to learn a large scale model of topic transitions. To this end, we use a parallel-update algorithm, first proposed by Collins et al. [12], which allows continuous and scalable weight learning (see Section 4.3 for details).

Our approach to related video suggestion complements previous work by Baluja et al. [7] that uses co-view infor-mation to suggest related videos. As we show in Section 7, our approach can be used to augment the information from a co-view graph, which is especially beneficial for videos with sparse co-view data or rich topical content.

It is important to note the connection between this work and the ongoing research on hybrid recommender systems [4, 10, 15, 31]. Similar to the hybrid recommender systems, the related video suggestion system described in Section 5 combines collaborative information (retrieval from the co-view graph) with content information (topic retrieval) to improve the system performance, especially for the  X  X old-start X  videos with sparse co-view data, or videos with rich topical representations.

Finally our work is also related to some of the current work on supervised topic weighting for video [31] and text [5] retrieval. However, this related work focuses on offline evaluation of the proposed methods. In contrast, we address the challenge of performing evaluation of our methods on live traffic, and demonstrate that our system can be successfully deployed on global scale.
In this section, we empirically evaluate the benefits of the integration of the topic video retrieval methods into the ex-perimental video suggestion system described in Figure 4. To this end, we first describe the methodology and the met-rics used in our experiments (Section 7.1). Then, in Section 7.2 we use a simulation method to estimate the effect of the topic-based retrieval on the overall performance of our ex-perimental video suggestion system. Finally, in Section 7.3 we describe the results of a large scale live traffic experiment.
There are several possible options for evaluating a large scale recommendation system such as the one described in this paper. These options include user simulation based on historical data, user studies and online evaluation [24]. Each of these options has its advantages and limitations, however one important consideration in the choice of the evaluation method is that the test subjects must represent as closely as possible the population of the users in the actual system [24].

This is especially true in the case of large scale web ap-plications that are used by millions of users. In this case, obtaining a sample of test subjects that would faithfully rep-resent the real user population is virtually impossible. In addition to the biased sample problem, conducting a user study to evaluate a large scale video recommendation sys-tem has several other disadvantages.

First, even given a very detailed set of instructions, it would be difficult for the test subject to judge what would be the best related video to suggest, since this decision is very subjective and may be influenced by factors such as user demographics, geographic location, emotional state and cultural preferences. Even for relatively objective evaluation tasks, such as document retrieval [6] the inter-judge agree-ment is low. We expect the agreement rate to be even lower for rating video relatedness, which is highly subjective.
Second, as research shows [22], there is often a discon-nect between what the subjects really want to watch and what they would like to have watched . This leads to a situ-ation where there is little correlation between the explicitly solicited judgments and the observed user behavior in the system.

Therefore, in the next sections we evaluate the perfor-mance of the proposed methods using user simulations and a large scale online experiment, and forego evaluation of our methods on manually labeled data.
Given the user-centric evaluation method of our system, in this section we address the question of what is the most suitable evaluation metric in this particular setting.
One possible choice of a metric is a click-through rate on the related video suggestions presented to the user by the system. However, research shows that the click-through rate can be highly biased by factors such as position and attrac-tiveness of the presentation [32]. We expect this bias to be very strong in our setting, where the results are presented in ranked order, and each related result is presented as a small snapshot from the video.

Another choice of metric is based on the main function-ality of the related video suggestion system, and it mea-sures the watch times of the related videos. Intuitively, a systematic improvement in functionality will generate more relevant suggestions, which will result in a higher user en-gagement with the system, and lead to longer watch times of the suggested related videos.

Following this intuition, we choose a watch time metric, which estimates how much time the user spends watching videos during the session following a click on a related video suggestion. While the watch time metric has its limitations (e.g. it might prefer videos with longer watch times), it is a good proxy for measuring performance, since it faithfully represents the core functionality of the evaluated system.
In this paper we presented two possible ways of integrat-ing topic retrieval into the related video suggestion system. First, in Section 3 we discussed a retrieval algorithm that assigns weights to topics using co-occurence based heuris-tics. Second, in Section 4 we presented a novel algorithm for directly learning weights on topic transitions.
We evaluate both of these methods by integrating them into the general related video suggestion system architecture as described in Figure 4. First, the highest ranked results produced by one of the two proposed retrieval methods are introduced into the reranking model. Then we measure the changes in the overall system performance, using either a simulation experiment (Section 7.2) or an experiment using live traffic (Sectrion 7.3).

In the next sections, we refer to the retrieval algorithm presented in Section 3 as IRTopics , since it makes use of information retrieval heuristics. We refer to the retrieval algorithm from Section 4 as TransTopics , since it is based on learning transitions between the topics.
In this section we describe a user simulation method for estimating the performance of our retrieval methods. We ex-ploit the reranking model described in Section 5 to simulate user interaction with the system.

Since the reranking model is trained to optimize the sys-tem performance (in terms of click-through rate and watch time) on live traffic, we use it to simulate a behavior of a typ-ical user in the system. Then we measure how many of the results returned by the topic retrieval method will be added by the simulated user to the top related results, compared to a system that only uses co-view retrieval. By system de-sign, if no results from the topic retrieval are selected by the simulated user, there is no benefit from performing this retrieval, since none of the results will be shown to the real users.

There are two things we are interested in measuring. First, we measure how many new results our method introduces that were not previously returned by the co-view retrieval approach. Second, and more importantly, we want to ob-serve how many of these results are actually considered as relevant related videos by the simulated user (i.e., positioned at high ranks by the reranking model).
 We run the user simulation for a large sample of videos. Figure 5(a) shows the percentage of new videos ranked among the top-K results by the simulated user that were intro-duced by the topic retrieval for either the IRTopics or the TransTopics methods. Figure 5(b) shows the percentage of watch videos that have at least one new related video among the top-K results as a result of topic retrieval.

As Figure 5(a) shows, there is a significant number of re-sults added at the top ranks as a result of topic retrieval. Figure 5(b) demonstrates that these results are spread among watch videos. More than 70% of watch videos are affected by our retrieval, and have at least one new top-20 result coming from the topic retrieval stage.

Both of the proposed methods retrieve roughly the same number of new results, with the TransTopics method in-troducing slightly more results at the higher ranks (13.6% compared to 12.9% at the top ten results). This potentially indicates the higher relevance of the new results introduced by the TransTopics method.
 Figure 5(b) shows a similar trend. At top ten results, the TransTopics method affects 73.1% of watch videos, com-pared to the 64.6% affected by the IRTopics method.
While the simulation method described in this section is suitable for the purposes of validation and testing differ-ent variants of the proposed methods, it does not provide a definitive answer whether the proposed topic-based videos will indeed have a positive effect on the actual user experi-ence. To this end, we conduct a large scale live experiment that is described in the next section. To evaluate the performance of the IRTopics and the TransTopics methods, we conducted a large scale exper-iment on a random sample of live YouTube traffic. The experiment was run during a single month in 2013, and affected related video suggestions returned for millions of watch videos. The topic weights for both retrieval methods were updated on the daily basis during the experiment, ac-cording to the method descriptions in Section 3 and Section 4, respectively.

Figure 6 presents a summary of the live experiment find-ings. The differences in Figure 6 are reported with respect to a baseline system that does not employ a topic retrieval stage (but does employ the co-view retrieval and the rerank-ing model, as described in Figure 4).

In Figure 6 we report the differences in three metrics re-lated to watch time. The first metric is the watch time it-self (as described in Section 7.1). The second metric is the completion rate , which measures how many of the suggested videos were fully watched from start to finish. The third metric is the abandonment rate , which measures the fraction of watch videos for which no related videos were watched.
As can be seen in Figure 6, the addition of the topic re-trieval stage to the related video suggestion system results in improvements in all three watch metrics: watch time and completion rate increase, while the abandonment rate de-creases. As the confidence intervals shown by the error bars in Figure 6 demonstrate, these improvements are statisti-cally significant.

In absolute metrics, the TransTopics method achieves overall 1.3% increase in watch time over the baseline setup that does not employ topic retrieval. This is an impressive increase, given billions of hours of video watched monthly on YouTube [2].

In addition, the TransTopics method is significantly more effecitve compared to the IRTopics method. % change in the watch time is 80% higher for the TransTopics compared to the IRTopics . Similarly, % change in the completion rate is more than double, and the % change in the abandoment rate drops by more than 90% when comparing the TransTopics method to the IRTopics method.

These effectiveness improvements highlight the importance of directly learning topic transitions from user feedback. As Figure 6 clearly demonstrates, the learned weights in the TransTopics method can significantly improve the system performance compared to the hand-crafted heuristic weight-ing in the IRTopics method.
In addition to the summary presented in the previous sec-tion, it is interesting to further analyze the performance of our methods by video type. In Table 1 we break down the changes in the watch time metric by video category (specified by the uploader of the video) and video age . 95% observed confidence intervals are shown by the error bars. confidence intervals are shown in the parentheses.

As our retrieval methods are topic based, we expect them to be most beneficial for videos with rich topical content and videos with low co-view signal. The results in Table 1 confirm this hypothesis.

For categories with richer topical representations like News and Science and Technology , the improvements obtained by our method are approximately three times higher than the average improvement in Figure 6. The highest improve-ments are obtained for the Pets and Animals category, which has many tail videos with little co-view information. On the other hand, for Music and Gaming categories, which have more popular videos with co-view data, the topic retrieval has smaller positive benefits (or even a slight negative im-pact for the IRTopics method in the Music category).
Similarly, for fresh videos (e.g., less than a month old), our improvements are more significant (especially for the TransTopics method) than for the older videos. This is due to the fact that the fresher videos have a weaker co-view signal.

The results in Table 1 demonstrate that our topic retrieval methods improve the related video suggestions by augment-ing the co-view results with fresher and more topically rel-evant videos. In agreement with the general results in Fig-ure 6, the TransTopics method is always significantly more effective than the IRTopics method across different video categories and age groups.
In this paper, we focused on the task of related video suggestion, commonly found in many online applications. The majority of the current approaches to related video suggestion are based on the collaborative filtering analysis. Most generally, the collaborative filtering approaches sug-gest videos that are likely to be co-viewed with the currently watched video.

While such suggestions work well for popular videos with many views, it is less applicable in situations where little or no view data is available. This includes fresh videos, or tail videos with few views. To address this challenge, we propose a hybrid approach to video suggestion, which combines the video content with the co-view information to improve the related video suggestions.

To this end, we represent the video using a set of topics mined from various sources including the video metadata, frequent uploader keywords, common search queries, playlist names and Freebase entities. In order to achieve an effective topical video representation, we assign weights to each topic, and propose two approaches for topic weighting.

The first approach is based on the standard information retrieval heuristics such as topic frequency and inverse docu-ment frequency weighting and stopword removal. The down-side of this approach is that it does not directly take into the account the user behavior.
Accordingly, we develop a second approach, which lever-ages the implicit user feedback (such as video co-watches) available in the online setting. This feedback is used for supervised learning of the optimal topic weights. Unlike the standard collaborative filtering analysis, our approach takes into account topic-to-topic rather than video-to-video co-view information. This enables suggesting a related video even if it was never explicitly viewed with the watched video.
We empirically evaluate these two topical representation methods in a large scale live experiment on YouTube traffic. We find that both of the representations have a significantly positive effect on the quality of video suggestions, compared to the standard collaborative filtering approach. This effect is especially visible for videos with rich topical representa-tion such as videos in the News and Science and Technology categories (more than 3.5% increase in watch time), and for fresh videos with sparse co-view data (more than 3% increase in watch time).

Furthermore, our evaluation demonstrates that learning topic weights from user feedback can increase the user en-gagement (as compared to the collaborative filtering base-line) by more than 80% over the standard information re-trieval representation. This demonstrates the importance of incorporating user feedback in content representation.
While this paper focuses on the task of related video sug-gestion, the findings of this work are broad, and are applica-ble in a variety of large scale retrieval and recommendation systems that employ topical content representations. As this work shows, learning topic weights from user feedback have the potential to significantly improve performance over the standard non-supervised weighting approaches.
The authors would like to thank Ajith Ramanathan, Jian-ming He, Su-Lin Wu, Tomas Lloret Llinares and Tamas Sar-los for their advice and assistance in conducting the experi-ments described in this paper.
