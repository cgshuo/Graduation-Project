 Citation Recommendation is useful for an author to find out the papers or books that can support the materials she is writing about. It is a challengeable problem since the vo-cabulary used in the content of papers and in the citation contexts are usually quite different. To address this prob-lem, we propose to use translation model, which can bridge the gap between two heterogeneous languages. We conduct an experiment and find the translation model can provide much better candidates of citations than the state-of-the-art methods.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation Citation Recommendation, Translation Model
The citation recommendation system is very useful for an author who is writing a paper or a book. It is a general case that the author forgets the specific paper containing some material when she wrote about it. The material may be a method, some statistic data or the conclusion from other researchers. If the author misses the detailed citation for these materials, the reviewers or editors may ask the author to add. Figure 1 shows a wikipedia example, in which the editor labels  X  X itation needed X  at the location where a citation is needed. (Such location is usually called citation placeholder ). For the authors or the editors, it would be easier if the citation recommendation system can provide some candidate papers containing the required information about the used material.
 Fi gure 1: A Citation Needed Example on Wikipedia
A quick solution is to take the context of the citation placeholder as a query, and search the papers X  content with some state-of-the-art IR methods. Unfortunately, this kind of method cannot work very well. The main reason is that the vocabulary used in the citation context is not same as that used in the paper X  X  content. For example, the abstract usually only contains very brief information about a paper, but the full text may contain too much noise. In some other cases, the jargon may be different for different research fields or/and for different periods.

Instead of the paper X  X  own content, some researchers lever-age the paper X  X  cited context to represent the paper [3]. The assumption of this method is that the cited contexts of one paper may be similar to each other. However, one difficul-ty for this method is that the cited context of one paper is usually sparse due to the long tail phenomenon. For some recent paper, they may even have no citation yet. Therefore, it is very challengeable to find out the papers with very few citations.

In this paper, we address the citation recommendation problem with the translation model. The translation model is originally used in translation text in one language to an-other, and it is also applied in retrieving the document whose content is heterogeneous to the queries (e.g., cross-language retrieval). In the citation recommendation problem, the ci-tation contexts and the content of papers are actually quite different in nature, and the translation model can bridge the gap between them. We conduct an experiment on a paper collection, and find the translation model can perform better than the state-of-art method.

The contribution of this paper is as follows: 1. We propose translation model for the citation recom-2. We deploys an experiment to verify that the transla-3. We compare the performance of translation models on
In this section, we will introduce the related in citation recommendation and translation model.
Citation recommendation is a research problem of recom-mending citation candidate for a manuscript or a citation placeholder in a manuscript. Shaparenko et al. [8] proposed to employ language model to compare the citation context and the source paper content, but it usually leads to low recall due to the mismatching vocabulary used. He et al. [3] employed other citation contexts to represent one paper. In addition, they also consider other citation contexts in the same manuscript of the query citation placeholder . Some researchers [10, 6] proposed to use topic models to predict whether there should be a link (citation relation) between two documents.

Besides the text information, some researchers also make use of the citation information for this task. They general-ly require the manuscript has already contained some oth-er citations. McNee et al. [5] employed the collaborative filtering technique to make use of the existing citation in-formation. They assumed the documents that are usually co-cited with the existing citations are likely cited by the current manuscript. Zhou et al. [13] proposed a semisuper-vised learning method to spread the citation relation in mul-tiple graphs (e.g., paper-paper citation graph, paper-author writing graph, etc.). Some researchers [9, 11] combined the text information and partial citation information to recom-mend citations. In this paper, we focus on modeling the text information for citation recommendation, and our method can potentially be combined with the existing citation based methods.
Translation Model was introduced to be used for informa-tion retrieval by Berger et al. [1]. The main idea is that it can translate the words in the documents to the query terms, so it can bridge the vocabulary gap between the query and the document. Because of its translation manner, it can be naturally applied in the cross-language retrieval [7, 4] and other applications where queries and documents use differ-ent vocabularies. Xue et al. [12] employed it to find the relevant QA pair for the question in natural language. Gao et al. [2] used it to bridge the vocabulary gap between the Web search query and the page title. There is also vocabu-lary gap between the paper content and the citation context, so it is appropriate to use translation model to address the citation recommendation problem.
In this section, we first introduce the method for estimat-ing the translation model, and then apply the translation model for ranking the papers.
Translation model defines the probability of translating one word in one language into a word in another language. For the citation recommendation, we assume the languages used in the citation contexts and in the papers X  content (doc-ument) are different, so we need to bridge these two lan-guages by translating one word in the document ( w d ) to one word in the citation context ( w c ).

To estimate the translation model for the citation recom-mendation problem, we need a training data including a set of citation context and document pair T = f ( c;d ) g , in which the citation context c references the document d . The trans-lation model can be estimated by maximizing the likelihood of citations contexts with their corresponding documents: The translation model with this formula can be estimated by the EM algorithm. In the practice, the translation model is usually heuristically approximated by a simpler version, which can compute much more efficiently [2]: where count ( w c ;w d ) is the count of ( c;d ) pairs in the train-ing data T , in which document d contains the word w d , and the context c contains word w c , and count ( w d ) is the count of ( c;d ) pairs in which the document d contains word w d
The translation model usually cannot work very well due to the small self translation probability P ( w j w ). In the c-itation recommendation problem, the small self translation probability may lead to underestimate the score of the docu-ment containing the words in the citation context. Xue et al. [12] proposed a method to boost the self translation proba-bility and showed improvement in retrieval performance:
P self ( w c j w d ) = 1( w c = w d ) + (1 ) P ( w c j w d where 1( w c = w d ) is a signal function which outputs 1 when the word w c is same as the word w d , and P ( w c j w d ) is the pure translation probability calculated by Eq 2. We can find the model without self translation boosting is a special case of Eq. 3 when = 0.

One concern of using translation model is that it would be very expensive, both in word-to-word translation probabil-ity matrix storage and the online retrieval processing. One heuristic method is to store only top K translated words for each word, so that the storage and processing complexity would be drastically reduced. We would test the sensitive-ness of parameter K in our experiment.
In this section, we will utilize the translation model de-rived in the previous section for ranking the documents. Denote the citation context containing m words as c = f c 1 ;:::;c m g , and the source content containing n words as d = f d 1 ;:::;d n g . Taking the citation context as the query, one document d can be scored by the query likelihood model: where P ( c i j d ) is the likelihood of word c i of the document language model of d .

In the practice, the query likelihood is generally calculat-ed by smoothing the maximum likelihood estimation of the document with the collection language model.
 wh ere P ml ( w j C ) and P ml ( w j d ) the maximum likelihood es-timation of a word in the collection and a document d re-spectively.

Since the vocabulary used in the citation contexts and the documents are usually different, it may not perform very well by deriving the word c i in the context from the document d  X  X  language model directly, and the translation model can bridge the gap as follows:
P ( c i j d ) = P ml ( c i j C )+(1 ) where P ( c i j d j ) is the translation probability from word d word c i calculated by Eq. 3. This model takes into account both document generative probability and the translation probability.
In this section, we describe our experiment setup and re-port the results. The dataset is a collection of 5,183 papers from 1988 to 2010, mainly in the information retrieval and text mining direc-tion. For each paper, we extract all citations placeholders and the corresponding citation contexts (three surrouding sentences). In the collection, we find that 1,499 papers have been cited by the other papers, and 6,166 citation contexts reference the papers in the collection. In the experiment, all papers in the collection composed the document collection, and a randomly selected set of 200 citation contexts are used as queries.
 For each citation placeholder , we search the papers that may be referenced at this citation placeholder. Each retrieval model would return a ranked list of papers. Since there may be one or more references for one citation context, we use Mean Average Precision (MAP) as the evaluation metric: where R ( d i ) is a binary function indicating whether docu-ment d i is relevant or not . For our problem, the papers really cited at the citation placeholder are judged as the relevant documents.
 We use two baselines: query likelihood language model and the context-aware relevance model [3]. For both the query likelihood language model and our translation model, it can consider the abstract or the full text as the document con-tent, so we have two versions for these two models. For the translation model, we use the self translation boosting method described in section 3.1, so we still need to test the effect of the self translation boosting in our experiment. The compared methods are described in Table 1.
Table 2 shows the best result for each compared method on our dataset. The parameter tuning results would be p-resented in Section 4.3. The first row shows the methods, Name De scription LM a qu ery likelihood language model on abstract LM f qu ery likelihood language model on full text CRM co ntext-aware relevance model by He et al.[3] TM a tra nslation model on abstract TM f tra nslation model on full text TM sa tra nslation model with self boosting on abstract TM sf tra nslation model with self boosting on full text T able 2: Performance of the Citation Recommend-ing Methods an d the second row shows the average MAP scores across the test queries. We find the difference between each pair of methods is significant with p -value 0 : 05 by paired t -test.
There are some interesting findings from the results: First, the translation models perform much better than the lan-guage model and the context-aware relevance model. It in-dicates the effectiveness of using translation models for the citation recommendation problem.

Second, the translation models on both abstract and full text are better than the corresponding query likelihood lan-guage models. It reminds that the vocabulary between the citation context and the source content are different, so the such kind of  X  X ranslation X  is needed.

Third, for the translation model (with or without self translation boosting), it performs better on the abstract than on the full text. On the contrary, the language model performs better on the full text than on the abstract. The abstract may miss some information of a paper, and the ci-tation context may talk about the missing information. For the language model, it cannot generate the missing infor-mation from the abstract, but for the translation model, it is still possible to generate such information from transla-tion. The full text contains all information that can help to generate the citation context, so it benefits the language model, but it may harm the translation model due to too much noise introduced to the translation matrix.

Finally, we validate that the context-aware relevance mod-el outperforms the language model, but it is still not as good at the translation model. In this section, we report the results of parameter tuning. There are three parameters used in our translation model: 2 (0 ; 1] controls the mixture weight of the global collec-tion model, 2 [0 ; 1] controls how much the self translation boosting is, and the parameter K controls the number of translated words for each word. We iterate all the ranges for these three parameters (for K , we iterate between (0 ; 1000]), and we report the results for each parameter tuning when the other parameters are set as the optimal values.
In the experiment, we find the performance would increase when value becomes smaller, and it keeps stable when the value is quite small, so we just set to be a very small value (10 5 in our experiment) to prevent the zero probability in some cases. It indicates that the collection model smooth is almost useless in this scenario. One possible explanation is th at the word probability has been implicitly smoothed by the translation model.

We tune the value in the range from 0 to 1, finding the best performance can be acquired when the value is be-tween 0 and 0.2. Figure 2 presents the results for in the range between 0 and 0.2. From the results, we can find the it needs larger self translation boosting for the language on abstract. The words in the abstract are generally more im-portant than those in the full text, so abstract words should be more translated to themselves. On the contrary, the full text words contain more noise, so they are better represented in their translated forms.
 Fi gure 2: Parameter Tuning on Self Translation Probability Boosting
Figure 3 presents the results for different translated word numbers. We note that the translated word number K can also affect the efficiency of the model, the model with larger K value will process more slowly. The optimal K value is 400 for the full text translation model and 800 for the abstract translation model. The abstract of a paper is usually short and brief, thus it needs to translate to more words that possibly appear in the citation context. From the result, we can find the performance is quite robust to the K value selection.
 Fi gure 3: Parameter Tuning on Translated Word Number
We propose to use the translation retrieval model for the citation recommendation problem. The citation recommen-dation problem is challengeable due to the vocabulary gap between the citation context and the paper X  X  content, and translation model can well bridge this gap. In the future, we can extend to consider more features such as other cita-tions in the paper and the paper X  X  global content for better citation recommendation.
 This work has been partially supported by HGJ 2010 Grant 2011ZX01042-001-001 and NSFC with Grant No.61073082, 60933004. [1] Berger, A., and Lafferty, J. Information retrieval [2] Gao, J., He, X., and Nie, J.-Y. Clickthrough-based [3] He, Q., Pei, J., Kifer, D., Mitra, P., and Giles, [4] Lavrenko, V., Choquette, M., and Croft, W. B. [5] McNee, S. M., Albert, I., Cosley, D., [6] Nallapati, R. M., Ahmed, A., Xing, E. P., and [7] Nie, J.-Y., Simard, M., Isabelle, P., and [8] Shaparenko, B., and Joachims, T. Identifying the [9] Strohman, T., and Croft, W. B. Efficient [10] Tang, J., and Zhang, J. A discriminative approach [11] Torres, R., McNee, S. M., Abel, M., Konstan, [12] Xue, X., Jeon, J., and Croft, W. B. Retrieval [13] Zhou, D., Zhu, S., Yu, K., Song, X., Tseng,
