 The database summarization system coined SaintEtiQ pro-vides multi-resolution summaries of structured data stored into a centralized database. Summaries are computed online with a conceptual hierarchical clustering algorithm. How-ever, most companies work in distributed legacy environ-ments and consequently the current centralized version of SaintEtiQ is either not feasible (privacy preserving) or not desirable (resource limitations).

To address this problem, we propose new algorithms to generate a single summary hierarchy given two distinct hier-archies, without scanning the raw data. The Greedy Merg-ing Algorithm (GMA) takes all leaves of both hierarchies and generates the optimal partitioning for the considered data set with regards to a cost function (compactness and separation). Then, a hierarchical organization of summaries is built by agglomerating or dividing clusters such that the cost function may emphasize local or global patterns in the data. Thus, we obtain two different hierarchies according to the performed optimisation. However, this approach breaks down due to its exponential time complexity.

Two alternative approaches with constant time complex-ity w.r.t. the number of data items, are proposed to tackle this problem. The first one, called Merge by Incorporation Algorithm (MIA) , relies on the SaintEtiQ engine whereas the second approach, named Merge by Alignment Algorithm (MAA) , consists in rearranging summaries by levels in a top-down manner.

Then, we compare those approaches using an original qual-ity measure in order to quantify how good our merged hier-archies are. Finally, an experimental study, using real data sets, shows that merging processes (MIA and MAA) are ef-ficient in terms of computational time.
 Categories and Subject Descriptors: H.2.4 [Systems]: Relational databases, Distributed databases. H.3.3 [Infor-mation Search and Retrieval]: Clustering General Terms: Algorithms, Experimentation, Human Factors, Performance.
 Keywords: Database Summary, Distributed Clustering, Tree Alignment.
Because of the ever increasing amount of information stor-ed each day into databases, users can no longer have an exploratory approach for visualizing, querying and analyz-ing their data without facing the problem often referred as  X  X nformation Overload X . Means to circumvent those prob-lems include data reduction techniques and, among them, the SaintEtiQ [1] [2] database summarization model which is considered in this paper.

SaintEtiQ enables summarization and classification of structured data stored into a database. It applies a concep-tual clustering algorithm for partitioning the incoming data in an incremental and dynamic way. The algorithm takes as input a table and produces a hierarchical data structure that shows how clusters are related. By cutting the hier-archy at a desired level, a partitioning of data items into disjoint groups is obtained. Thus, the main concern in the clustering process is to reveal the organization of patterns into  X  X ensible X  groups, which allow us to discover similarities and differences, as well as to derive useful conclusions about them. This idea is applicable in many fields [3], such as life sciences, medical sciences and engineering.

Actually, most companies work in distributed legacy en-vironments and consequently a large number of peers (e.g. PC X  X  connected to the Internet) can be pooled together to share resources, information and services. Because of con-cerns related to confidentiality, storage, communication band-width and/or power limitation, the current centralized ver-sion of SaintEtiQ is either not feasible (privacy reasons) or not desirable (resource limitations). For instance, in medi-cal database, only anonymous and statistical information are available since individual information (such as name, address and phone number) can violate patient confidentiality.
To address this problem, we propose new algorithms that generate a single summary hierarchy given two input hierar-chies, without scanning the raw data. The Greedy Merging Algorithm (GMA) takes all leaves of both hierarchies and computes the optimal partitioning for the given data set. Then, a hierarchical organization of clusters is built by ag-glomeration and division. However, this approach breaks down due to its exponential time complexity.

Two alternative approaches are proposed to tackle this problem: Merge by Incorporation Algorithm (MIA) and Mer-ge by Alignment Algorithm (MAA) .

The main concern of this work is to introduce and compare those approaches and list the pros and cons of each one. This rises for questions: 1. How we could define a set of best representatives (or 2. What are the main criterions (time complexity, model 3. How good merged hierarchies are?
The rest of the paper is organized as follows. In the next section we present the SaintEtiQ model and its properties and we illustrate the process with a toy example. Then, in section 3 we describe the GMA algorithm in detail. Two alternative approaches that overcome GMA approach lim-itations, are presented in section 4. Section 5 introduces how to evaluate the correctness of merging algorithm re-sults using appropriate and original clustering validity in-dices. Moreover, an experimental study using real data sets is presented in section 6. Section 7 describes related work. Finally, in section 8, we give conclusion and future direction of our work.
Our work relies on the summaries provided by the Sain-tEtiQ system described in [1, 2]. In this section, we intro-duce the main ideas of the summary model and give useful definitions and properties regarding our proposal. Then, we present a toy example that will be used throughout that document.
SaintEtiQ takes tabular data as input and produces multi-resolution summaries of records through both online map-ping and summarization processes.
SaintEtiQ system relies on Zadeh X  X  fuzzy set theory [4] and, more specifically on linguistic variables [5] and fuzzy partitions [6] to represent data in a concise form. The fuzzy set theory is used to translate records in accordance with a Knowledge Base (KB) provided by the user. Basically, the operation replaces the original values of every record in the table by a set of linguistic descriptors defined in the KB . For instance, with a linguistic variable on the attribute IN-COME (figure 1), a value t.IN COM E = 440 . 86 e is mapped to { 0.3/tiny , 0.7/very small } where 0 . 3isamembership grade that tells how well the label tiny describes the value 440 . 86. Extending this mapping to all the attributes of a relation could be seen as locating the cells in a grid-based multidimensional space that map records of the original ta-ble. The grid is provided by KB and corresponds to the user X  X  perception of the domain.

Thus, tuples of table 1 are mapped into two distinct grid-cells denoted by c 1 and c 2 in table 2. A priori, young is a fuzzy label provided by KB on the attribute AGE and it perfectly matches (with degree 1) range [19 , 24] of raw values. Besides, tuple count column gives the proportion of records in R that belongs to the cell and 0 . 3/ tiny says that tiny fits the data only with a small degree (0 . 3). It is Figure 1: Fuzzy linguistic partition defined on the attribute INCOME computed as the maximum of membership grades of tuple values to tiny in c 1 .

Flexibility in the vocabulary definition of KB permits to express any single value with more than one fuzzy descrip-tor and avoid threshold effect due to a smooth transition be-tween two descriptors. Besides, KB leads to the point where tuples become indistinguishable and then are grouped into grid-cells such that there are finally many more records than cells. Every new (coarser) tuple stores a record count and attribute-dependant measures (min, max, mean, standard deviation, etc.). It is then called a summary .
Summarization service is the last and the most sophisti-cated step of the SaintEtiQ system. It takes grid-cells as input and outputs a collection of summaries hierarchically arranged from the most generalized one (the root) to the most specialized ones (the leaves). Summaries are clusters of grid-cells, defining hyperrectangles in the multidimensional space. In the basic process, leaves are grid-cells themselves and the clustering task is performed on K cells rather than N tuples ( K&lt;&lt;N ).

From the mapping step, cells are introduced continuously in the hierarchy with a top-down approach inspired of D.H. Fisher X  X  Cobweb, a conceptual clustering algorithm [8]. Then, they are incorporated into best fitting nodes descending the tree. Three more operators could be apply, depending on partition X  X  score, that are create , merge and split nodes. They allow developing the tree and updating its current state. Figure 2 represents the summary hierarchy built from the cells c 1 and c 2 .
Definition 1. Summary Let E = A 1 ,...,A n be a n -dimensional space equipped with a grid that defines basic n -dimensional areas called cells in E .Let R be a relation defined on the cartesian product of domains D A i of dimen-sions A i in E . Summary z of relation R is the lower bound of the including boxes of the cluster of cells populated by records of R .

The above definition is constructive since it proposes to build generalized summaries (hyperrectangles) from cells that are specialized ones. In fact, it is equivalent to performing an addition on cells such that: where c i  X  L z ,thesetof p cells (summaries) covered by z .
A summary z is then an intentional description associated with a set of tuples R z as its extent and a set of cells L are populated by records of R z .

Thus, summaries are areas of E with hyperrectangle shapes provided by KB. They are nodes of the summary tree built by the SaintEtiQ system.

Definition 2. Summary Tree A summary tree is a col-lection Z of summaries connected by , the following partial order:
The above link between two summaries provides a generali-zation-specialization relationship. And assuming summaries are hyperrectangles in a multidimensional space, the partial ordering defines nested summaries from the larger one to single cells themselves.

Providing to the end user with a reduced set of represen-tatives from the data, we need to extract a subset of the summaries in the tree. The straightforward way of perform-ing such a task is to define a summary partitioning.
Definition 3. Summary Partitioning The set P of leaves of every rooted sub-tree of the summary hierarchy provides a partitioning of relation R .

We denote by P z the top-level partition of z in the sum-mary tree. It is then the most general partitionning of R we can provide from the tree. Note that the most specialized one is the set of cells covering R z ,thatis L z .
A partitioning can be obtained a posteriori to set the com-pression rate regarding user X  X  needs. For instance, general trends in the data could be identified in the very first lev-els of the tree whereas precise information has to be looked at near the leaves. Moreover, such partitioning verifies two basic properties: disjunction and coverage.
 Property 1. Disjunction
Summaries z and z are disjoint iff  X  k  X  [1 ..n ] ,z.A k  X  z .A k =  X  .

Regarding such a property, summaries of a partition do not overlap one with each other.
 Property 2. Coverage
Partitions guarantee coverage of the all relation R since, by definition, representatives of every branch are included into P .

Moreover, SaintEtiQ process relies on an objective func-tion, the Summary Utility ( U ) defined in [1], that guides lo-cal search through the space of possible summary trees. U a combination of two well-known measures: typicality [9] and contrast [10]. Those measures maximize between-summary dissimilarity and within-summary similarity. Thus, going from the root (the highest partition) to the leaves (the low-est one) of the summary tree, the internal cohesion increases while the contrast between summaries decreases. Thanks to the aggregative function, trade-off values are found in the middle of the hierarchy and correspond to summaries with a high level of generalization while still informative. Figure 3 shows the evolution of those measures regarding the level of the hierarchy built on a TV programs data set (see section 6 . 1) that contains 2 . 127 tuples. On this example, trade-off values are found at level 2. Note that this monotonicity relies on some cognitive assessments [9].

To illustrate our proposal, we introduce here a sample data set with 25 records represented on three attributes: A , B and C . We suppose that { a 1 ,a 2 ,a 3 } , { b 1 ,b { c 1 ,c 2 } are sets of linguistic labels defined respectively on the attributes A , B and C . Figure 4 shows summary hierar-chy H SEQ provided by SaintEtiQ performed on our data set R .
 Figure 4: Summary tree H SEQ of the data set R
To serve merging processes, we first perform horizontal fragmentation of the data set: we then obtain two relations R 1 and R 2 , with respectively 15 and 10 tuples. Then we also apply SaintEtiQ on these two derived relations to produce the corresponding summary trees (figure 5) that should be merged.

In the next section, we present the Greedy Merging Algo-rithm that aims at merging two summary hierarchies from distinct data sets with the same schema.
The Greedy Merging Algorithm (GMA) is the straightfor-ward way of merging distributed summaries. It operates exhaustively on all the populated cells to find out the best partitioning of the global data set. Indeed, GMA takes all leaves (the most specialized summaries) of both the input hi-erarchies and generates their optimal partitioning schema. We define here the optimality as the result of performing the following algorithm on a set of summaries Z : 1. compute the partition lattice L of Z ; 2. For each partition P  X  X  , build summary descriptions 3. filter from L , the set of candidate summary partitions 4. the optimal partitioning of Z is the partition P best  X  X 
Note that at the fourth stage, we can also compute the summary utility of the worst partitioning P worst of Z .We will use such a partition in the evaluation step.
From the above optimal partitioning P best , a hierarchical organization of summaries is built by agglomerating and di-viding summaries to provide higher and lower nested par-titions such that the U function may emphasize local or global patterns in the data. Thus, we obtain two differ-ent hierarchies according to the performed optimization (lo-cal or global). None of them is preferred to the other and third-party application or user X  X  requirements are key fac-tors to selecting the right optimization mode. Indeed, the former based on local optimization seems more appropriate for applications including querying, analyzing and browsing operations. It supports fruitful strategies to explore large information space. The latter is a more relevant choice for applications that address the problem of resource limitation to substitute an entire summary partition to the original data set.

These two above optimization approaches are described in detail in the following sections.
The main idea of this approach is rather simple. It starts from a given summary partition and recursively searches in two directions for the highest U -valuated partitions that are nested or including. It stops when it reaches respectively the cells (the leaves) and a single hyperrectangle (the root z ).

Recall that R = R 1  X  R 2 . We are trying to build H R , the summary tree of relation R from H R 1 and H R 2 .The global optimization-based GMA is performed on the optimal partition P best of the union L of leaves from H R 1 and H as follows: 1. l =0; P l  X  P best ; 2. While P l = { z } do 3. return the nested partitions P k , k  X  [0 ..l ] as upper
Function top ( G l ) returns the first item in the list, that is the summary partition with the highest U value. The above algorithm builds the upper part of the summary tree H
R , from the root P l = { z } to the best partitioning P 0 P best . The lower part is similarly computed, with the stop condition P l = L and G l is S l , the list of summary partitions that specialize P l .

For instance, figure 6 illustrates the merged hierarchy ob-tained from summary trees shown in figure 5 according to the global optimization approach.
 Figure 6: Example of GO-based GMA on H R 1 and H
This approach is a greedy top-down clustering method that takes all leaves L of both hierarchies H R 1 and H R 2 computes the highest U -valuated partition P best of L as the first level of the global hierarchy H R . Then, it recursively applies to every node of P best . The method uses a local op-timization since each summary (node in the tree) is subject to an individual clustering, from the root to the leaves.
Thus, the LO-based GMA computes the hierarchy H R from H R 1 and H R 2 as follows: 1. search for P best on L ; P best is then the top-level parti-2. for each z  X  P best do L z is the set of cells covered by the hyperrectangle z .
For example, figure 7 gives the result hierarchy of sum-mary trees from figure 5 according to the local optimization approach.
 Figure 7: Example of LO-based GMA on H R 1 and H
Both GO and LO-based GMA are expensive since they rely on finding the best partition P best of various data sets with nesting constraints. And this principle requires to ex-plore all the possible partitions each time.

The number of partitions of a set E with n elements, ac-cording to the usual definition, grows exponentially with its cardinality. It is called a Bell number andisdefinedas . Thus, the time complexity C GM A of both approaches de-scribed above verifies: where L 1 and L 2 are the set of leaves of H R 1 and H R 2
Nevertheless, in the computation of the optimal partition-ing it is more efficient to perform checking of disjunction property within calculation of the partition lattice. Indeed, if { z 1 ,z 2 } and { z 3 } overlap, every partition verifying the pattern {{ z 1 ,z 2 } , { z 3 } ,X } ,with X a partition of the com-plement to { z 1 ,z 2 ,z 3 } are trivially not disjoint. This obser-vation allows to prune large parts of the Hasse-diagram of the set of candidate summary partitions P and could dras-tically reduce time cost.

Besides, note that the uniqueness of such optimal par-titioning is not guaranteed. To address this problem, we can select in both the LO and GO-based GMA the overall hierarchy that has the largest area under the U curve (see figure 3). It is given by the sum of U values in every level of the tree.

Hence, the GMA approaches provide, by construction, op-timal hierarchies but they suffer from an exponential com-plexity and thereafter they are inappropriate for scaling. Management of massive data sets requires efficient algo-rithms to merge summary trees.

Note that SaintEtiQ is designed for converging to the locally optimized hierarchy since the tree is updated at each node every time a new cell is incorporated. There is no global optimization that could break the incremental aspect of the algorithm. The following alternative approaches try to achieve the local-based hierarchy as well.
In this section, we present two alternative algorithms in order to overcome GMA approach limitations. The first proposal, the Merge by Incorporation Algorithm (MIA), is basically inspired by the SaintEtiQ engine. The second approach, so-called Merge by Alignment Algorithm (MAA), relies on graph merging techniques. This method is based on the incremental conceptual clus-tering algorithm used by SaintEtiQ : leaves (or cells) of a summary tree are introduced into the other hierarchy one at a time with a top-down approach and they are incorporated into best fitting nodes descending the tree. Indeed, at each node z , the algorithm considers incorporating the current cell c into each child node of z , updating the description of the targeted summary. Furthermore, the process evaluates the preference for merging two children nodes of z , splitting one child node and creating a new child node accommodat-ing c . Then it uses the U score to rank candidate partitions for z + c and it changes state of the hierarchy according to the best operator to apply each time a new cell descending the tree encounters a summary. Once the new cell c reaches the leaves, if there exists a duplicate, then it is updated regarding information of the new cell, else, we develop the tree with a new level such that leaves remain single cells only. The previous leaf becomes an intermediate node with two cells, the new leaves, as children nodes.

According to the example described in section 2.3, figure 8 (respectively 9) represents the hierarchy produced when H is incorporated into H R 1 (respectively H R 1 into H R 2 The time complexity C MIA of the Merge by Incorporation Algorithm is linear w.r.t. the number of cells in the input hierarchies: where coefficient k corresponds to the set of operations per-formed when a new cell is introduced in the tree. At least, since the hierarchy reaches a stable state 1 , an upper bound for k is the number of operations needed for incorporating a new cell into the more complex and unstable tree. It is worth noticing that in the stable state, learning task (cre-ate, merge, split) is completed and the process acts like a deterministic classifier (incorporate cells only).
Besides, MIA is natively asymmetrical. Indeed, it assumes one of the two input trees as the base model and the other one is deconstructed to incorporate every single leaf into the first one. Thus, an important issue for performance and quality requirements of the result hierarchy is to define the base tree for the operation. The basic assumption we made is that we prefer the largest tree to be the base model since it is expected to be stable or at least not so far from the stable state. Hence, in the best case, MIA performs a classification task only.
We propose a second method for merging summary trees that is drastically different than the previous one. Indeed, Merge by Alignment Algorithm (MAA) is inspired of tree alignment techniques whereas MIA, the previous one, is based once new cells are duplicates only. on the SaintEtiQ engine. A comparison between both the two approaches is provided in section 6.
 MAA consists in rearranging summaries by levels in a top-down manner. It is based on a recursive algorithm which operates fusions of summaries guided by nodes of the in-put hierarchies H R 1 and H R 2 in order to produce a global hierarchy H R .

The result hierarchy H R is built from trees H R 1 and H R 2 as follows: 1. z  X  z 1 + z 2 ; 2. compute P best for the union of children nodes of z 1 and 3. connect P best to z as the top-level of H R ; 4. for each z  X  P best do
The operator + figures out the union of the descriptions of two summaries to build the including hyperrectangle as a resulting summary.

Figure 10 represents the MAA hierarchy of the input hi-erarchies shown in figure 5: The time complexity C MAA of MAA is linear w.r.t. the number of cells L : where | inodes | =( d  X  1)  X | L | X  1 is the set of intermediate nodes of H R and d is the average degree of the tree.
Note that in the second step of the algorithm, the best partitioning P best of a set E of summaries that specialize z can be singleton { z } . For instance, it occurs when none of the candidate partition verifies the disjunction property. In that case, we look for the best partitioning in a specializa-tion of E instead of E itself, that is we replace part of the summaries of E by their children and we continue recursively until we find a non trivial (cardinality greater than or equal to 2) partitioning. In the worst case, specialization stops when E contains all leaves of hierarchies H R 1 and H R 2 best partitioning of E is then the set of leaves with duplicate elimination since at this stage it is the unique partition of z that checks disjunction property.

Currently, when it happens we simply try to specialize one single summary that is not a leaf. However, we can use any other heuristic function to support specialization. For example, we could evaluate U on a given subset of specialized partitions and retain the highest U -valuated candidate only.
In the following, we discuss an important issue for merging process regarding the quality assessment of the results.
In this work, we aim at merging two hierarchical clustering schemes, and then the final result requires an evaluation. The question we wish to answer in this section is how good our merged hierarchies are?
In [3], a number of clustering techniques and algorithms have been reviewed. These algorithms behave in different ways depending on the features of the data set (geometry and density distribution of clusters) and/or the input pa-rameter values (e.g. number of clusters, diameter or radius of each cluster). Thus, the quality of clustering results de-pends on the setting of these parameters.

Soundness of clustering schemes is checked using validity measures (indices) available in the literature [3]. Indices are classified into three categories: external, internal, and rela-tive. The two first ones rely on statistical measurements and aim at evaluating the extent to which a clustering scheme maps a pre-specified structure known about the data set. The third category of indices aims at finding the best clus-tering scheme that an algorithm can provide under some given assumptions and parameters.

As one can observe, these indices give information about the validity of the clustering parameters regarding a given data set and thus may be viewed as data dependent mea-sures.

Remind that we try to evaluate the validity of the merg-ing processes MIA and MAA. Therefore usual validity mea-sures do not apply since the main purpose of the evaluation is to compare the distributed summary construction pro-cess with the centralized approach, everything else being equal (objective function, parameters, grid and data set). Within this framework, we consider hierarchies H GM A GO and H GM A LO provided by the GMA approach as the ref-erence hierarchies. Then, we compare MIA , MAA and the basic centralized approach of SaintEtiQ with those refer-ence hierarchies. Thus, there is a need for a quality measure of summary tree.

A valid and useful hierarchy quality measure must be:
A basic hierarchy quality measure can be computed as an aggregate (e.g. mean) of local scores (summary utilities). But, this measure does not verify the second above require-ment. For instance, it takes value 0 . 26572 for the hierarchy H
GM A GO (figure 6) and 0 . 343662 for the hierarchy H MAA (figure 10).

Thus, in the following, we define two different measures according to the reference hierarchy considered ( H GM A GO H
GM A LO ). The first one relies on a level-based analysis and is relevant when the reference hierarchy is produced with the global optimization. The second measure is then well-suited for locally optimized hierarchies.
The basic idea is to study summary utility level by level as the GO-based GMA do. For a given hierarchy H ,we then define as many partitions as there are levels, from the root ( P 0 )totheleaves( P h ), where h is the height of the tree. P i +1 is the direct specialization of every summary in P , except for the leaves. And every P i , i  X  [0 ..h ] satisfies both the disjunction and coverage properties. Thus, we as-sociate to each level i of the tree the utility value U ( P the related partition P i and consequently an utility vector
U ( P 0 ) ,..., U ( P h ) .

Then, the overall score  X  of H is defined as the sum of all those utilities (the  X  X rea under the curve X  of summary utility):
Table 3 gives utility vectors and  X  -values according to every approach. Levels are going from zero to four, and SEQ stands for the SaintEtiQ process.

Both MIA and MAA algorithms give values comparable to the one provided by SaintEtiQ . Thus, even if the three algorithms have not been designed to be globally optimal, they offer similar quality regarding the  X  measurement. It means that merging processes are, from the GO point of view, as effective as the centralized version of summary con-struction.

Furthermore, it is worth noticing that, as expected, this measure is not adapted to local analysis. Indeed, it gives the lowest score for the hierarchy H GM A LO . To address this problem, we then present in the following section a new dissimilarity measure between summary trees.
The basic idea is to valuate local differences between two hierarchical clustering schemes. Assume a reference sum-mary tree H ref ;eachnode z covers a part R z of relation R , and provides a partitioning P z of R z thanks to the top-level of the sub-tree rooted by z in H ref . Given a targeted tree H target , we then propose to valuate the partitioning of R for each z in H ref , according to H target . In other words, we try to locate R z inside H target and compose the partitioning P z of R z w.r.t. summaries from H target . Hence, we compute the utility value for P z to compare with the utility value of P . Among all the candidate partitioning of R z in H target we retain the most generalized one thanks to a bottom-up traversal algorithm.

The above process provides two utility vectors: v = q 1 ,... ,q n with values related to H ref and v = q 1 ,...,q n for H target . As one can observe, the dimension of these vec-tors is equal to the number n of intermediate nodes in the reference hierarchy H ref .

Thus, a dissimilarity measure between H ref and H target is defined as the distance between the above utility vectors v and v . As an example, the Euclidian distance is used here:
Even if it relies on a distance, the dissimilarity measure  X  is asymmetric since the utility vector v of H target is relative to data subsets provided by H ref .

In the following, we use the hierarchy H GM A LO , provided by the GMA process based on local optimization, as the ref-erence hierarchy ( H ref = H GM A LO ). Denote by  X  LO ( H )=  X  ( H, H GM A LO ) the dissimilarity measure between a hierar-chy H and the locally optimized one H GM A LO .Notethat by construction, for each summary z of H GM A LO and for any hierarchy H target we have P z = P best and consequently U
Hence, as shown in the below table, the dissimilarity mea-sure is semantically consistent.

Moreover, if we execute GMA with local optimization 100 times with random choice of partition at each step of the process, the average of all  X  LO outputs is 1 . 35 that is very greater than 1 . 04, the worst  X  LO of all merged hierarchies.
It allows us to conclude that MIA and MAA provide well-founded summary trees. This section present experimental results achieved with MIA and MAA processes. We first introduce the data set, then we provide an analysis based on various parameter X  X  observations.
We used metadata broadcasted with french TV programs and represented on eight attributes: Schedule , Channel , Ti-tle , Topic , Category , Actors , Director and a brief Summary . To perform our merging processes, we previously computed a set of couples ( H, G ) of hierarchies where H is built from 50 tuples with the SaintEtiQ process, and G covers 50,100,150, ..., 500 records taken from the data set. For each couple ( H, G ), H merge is the result of merging H and G .Forevery couple ( H, G ) we also process the union of R H and R G to provide the hierarchy H SEQ with the centralized approach.
Finally, note that all the experiments were done on a 2 . 0 GHz P 4-based computer with 768 MB memory.
In this section, we validate our fusion processes with re-gard to performance (computation times) and structural properties (number of nodes and leaves, average depth and average width).
 Theoretical complexity of each approach allows us to say that merging processes (MIA and MAA) of H and G are very faster than the SaintEtiQ process performed on R H  X  R
G . That is the main result of figure 11 that shows the performance evolution according to the number of tuples used to generate G . Furthermore, if we take into account the time cost of building H and G , both MIA and MAA remain quite faster than the SaintEtiQ process performed on R H  X  R G .

Thus, merging processes are able to drastically reduce time cost of the summarization task of a very large data set by horizontal fragmentation into several sub-relations that would be summarized separately and then merged.
 As expected, figure 12 shows the average depth of the hier-archy provided by MAA and MIA ( H in G )thatispretty much equal to the average depth of G thanks to the stability of G .Moreover,MIA( G in H )andthe SaintEtiQ process provide hierarchies that are deeper than G .

Finally, observe that hierarchies provided by MAA are wider than that generated with MIA ( H in G ). The last one is quite similar to G and wider than the hierarchies provided by MIA ( G in H )and SaintEtiQ as well. Average width evolution of every hierarchy has been reported, according to the number of tuples used to generate G , in figure 13.
We could see also that the number of nodes and leaves is quite similar through the merged and centralized hierarchies and is almost equal to the sum of H and G nodes and leaves.
To conclude our experimental analysis, we may remark that our merging processes produce hierarchies with almost the same structural properties than a classical hierarchy. Adding to this, fusion processes times are interesting: con-stant w.r.t. the number of R H  X  R G tuples. Clusteringlargedatasetsrecentlyhasemergedasanim-portant area of research. The ever-increasing size of data sets and poor scalability of clustering algorithms has drawn attention to parallel and distributed clustering for partition-ing large data sets. In [7] a parallel version of DBSCAN and in [11] a parallel version of k-means were introduced. Both algorithms start with the complete data set residing on one central sever and then distribute the data among different clients. But in the case of parallel k-means, the process re-quires synchronized communication during each iteration, which might become difficult and costly in a wide area net-work. Moreover, there might be constraints such as data could not be shared between different distributed locations due to privacy or security of the data. There is some work where data in distributed environment has been clustered in-dependently i.e. without any message passing among them and multiple partitions combined using limited knowledge sharing ([12], [13], [14], [15], and [16]). Knowledge reuse framework [17] has also been explored, where label vectors of different partitions are combined without using any feature values ([18] and [16]). In [12] and [14] distributed clustering has been discussed under two different settings that impose severe constraints on the nature of the data or knowledge shared between local data sites. In [13], local sites are first clustered using the DBSCAN algorithm and then represen-tatives from each local site are sent to a central site, where DBSCAN is applied again to find a global model. Another density estimation-based distributed clustering has been dis-cussed in [15]. In contrast to our proposal, these approaches do not offer a solution to the distributed hierarchical clus-tering problem; they are based on partitioning techniques and generate flat clustering of the data.

Johnson and Kargupta propose in [19] an algorithm to merge local clusters generated by hierarchical clustering al-gorithm into a global one. But this algorithm is designed to handle vertically partitioned data (features are different in different sites). Finally, similarly to our approach, RACHET [20] assumes horizontal data i.e. each site has the same set of features but different set of data points. RACHET builds a global dendrogram by merging locally generated dendro-grams using a hierarchical algorithm similar to our MAA algorithm. This algorithm uses statistical bounds to rep-resent clusters efficiently across sites whereas we propose a grid-based approach with the use of linguistic variables and fuzzy partitions.
 Graphs and trees are the most useful mathematical objects for representing any information with relational or hierar-chical structure. Thus, they are common and well-studied combinatorial structures in computer science. One of the popular problems they pose is how to merge graphs together. It has been extensively studied and applied to several areas such as version control, schema/ontology integration, index merging, etc.

Keeping data synchronized across a variety of devices and environments raises the need for reconciliation of concur-rently modified data. In [21] and [22] documents are mod-elled as ordered trees and methods for merging them based on tree edit operations [23] are outlined. Although there is a restricted approach [24] that leads to merge unordered trees, one of the results from Zhang [25] is that finding the opti-mal matching function for such trees is NP-complete prob-lem. Since SaintEtiQ summaries are unordered trees and the constraints given in the definition of mappings in [24] do not meet summary requirement (subtrees should be mapped to disjoint subtrees), such algorithms cannot be used to merge them.

Vast amount of data populate the internet and conse-quently retrieval of relevant documents is often a non trivial task. Thus, schemas/ontologies matching ([26], [27]) is a critical operation in many application domains. It takes as input two schemas/ontologies, each consisting of a directed acyclic graph, and determines as output the relationships (equivalence, subsumption) holding between them. Most al-gorithms in this case, are based on various techniques (ter-minological, structural and semantic) and lead to identify a mapping as a correspondence between close concepts using external resources like WordNet. In contrast to our proposal, these approaches focus on local mappings that need user evaluation and so cannot perform a total schemas/ontologies mapping.

Many scenarios in application and system maintenance require merging of two indexes: data migration in parallel database system, batch insertion in a centralized database system, etc. A B-tree merging algorithm [28] integrates two indexes covering the same key range into a single one. This algorithm is based on basic B-tree properties (balanced, high fanout, some minimal fill factor) and thus it cannot be used for the summaries merging issue.
In this communication, we propose new algorithms for merging summary hierarchies obtained from distinct sets of database records with the same schema. The first one, Greedy Merging Algorithm produces good results but breaks down due to its exponential time complexity since partition lattice is computed several times.

Two alternative approaches are proposed to tackle this problem: Merge by Incorporation Algorithm (MIA) based on the incremental conceptual clustering algorithm used by SaintEtiQ and Merge by Alignment Algorithm (MAA) in-spired of tree alignment techniques. We show that MIA and MAA processes achieve high quality hierarchies and are very efficient in terms of computational time.

As future work, we plan to: [1] G. Raschia et al , X  SaintEtiQ : a fuzzy set-based [2] R. Saint-Paul et al ,  X  X eneral purpose database [3] Halkidi et al ,  X  X lustering validation, X  Journal of [4] L. A. Zadeh,  X  X nformation and Control, X  Fuzzy Sets , [5] L. A. Zadeh,  X  X oncept of a linguistic variable and its [6] L. A. Zadeh,  X  X uzzy sets as a basis for a theory of [7] X. Xu et al ,  X  X  Fast Parallel Clustering Algorithm for [8] K. Thompson et al ,  X  X oncept Formation in Structured [9] E. Rosch et al ,  X  X amily ressemblances: studies in the [10] A. Tversky,  X  X eatures of similarity, X  Psycol. Rev. , [11] S. Inderjit Dhillon et al ,  X  X  Data-Clustering [12] J. Ghosh et al ,  X  X istributed Clustering with Limited [13] E. Januzaj et al ,  X  X owards Effective and Efficient [14] J. Ghosh et al ,  X  X  Consensus Framework for [15] M. Klusch et al ,  X  X istributed clustering based on [16] P. E. Jouve et al ,  X  X  New Method for Combining [17] K. D. Bollacker et al ,  X  X  supra-classifier architecture [18] A. Strehl et al ,  X  X luster Ensembles  X  A Knowledge [19] E. L. Johnson et al ,  X  X ollective, Hierarchical [20] N. F. Samatova et al , X  X ACHET:AnEfficient [21] T. Lindholm,  X  X  three-way merge for XML [22] T. Mens,  X  X  State-of-the-Art Survey on Software [23] S. M. Selkow,  X  X he tree-to-tree editing problem, X  [24] K. Zhang,  X  X  Constrained Edit Distance Between [25] K. Zhang et al ,  X  X ome MAX SNP-hard results [26] Y. Kalfoglou et al ,  X  X ntology Mapping: The State of [27] P. Shvaiko et al ,  X  X  Survey of Schema-Based Matching [28] X. Sun et al ,  X  X nline B-tree merging, X  SIGMOD  X 05:
