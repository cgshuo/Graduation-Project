 1. Introduction
Data mining techniques are useful for extracting hidden and important information from databases. Sev-the ways in which attributes are associated in a database and produce frequent itemsets and/or association rules with two thresholds, minimum support and minimum confidence. The classification techniques partition a database into classes by certain features extracted from attributes and elucidates useful information from ilar properties. Although there exist many useful mining methods in each category and each method has its own merits, it has been observed that combining mining methods from different categories to offset the short-comings of each has become an important area of data mining research. For example, by combining clustering techniques with association rules mining techniques [11,22,23,35,39] , system performance can be improved by reducing members in clusters.

Association rules mining is useful for discovering the ways in which items are associated. It has been very successful in many applications, such as bringing to light the relationships between customers in banking databases. However, most association rules mining algorithms may incur problems related to either perfor-mance or space. Problems of performance result mainly from the generation of numerous candidate itemsets and the multiple scans of databases needed to verify them; while problems related to space are mainly due to the need for in-memory structures to hold database transactions or their processing results. Recently, Jea et al. [12] proposed a novel mining method, called OCA, which is based on the concept of support approx-imation to solve these problems. Instead of scanning a database many times to verify the supports of item-sets, OCA generates frequent itemsets by directly verifying their approximate supports calculated with the
Principle of Inclusion and Exclusion in Combinatorial Mathematics. OCA achieves good performance with less space. Nevertheless, due to the errors in support approximation, its mining accuracy may become unsta-ble in different databases. It is therefore the primary motivation of our research to overcome this drawback and to develop an effective and viable data mining method using a support approximation-based approach such as OCA.

In this paper, a new mining method, called CAC, combining the clustering technique and support approx-
CAC method to improve the accuracy of support approximation by excluding the members with low similar-the time complexity of searching whole subsets of an itemset in support approximation. Experimental results and analyses show that the clustering technique can effectively improve mining accuracy and that CAC dis-covers frequent itemsets with relatively stable accuracy and good performance. By successfully integrating of the new clustering technique, this study also demonstrates the effectiveness of combining support approx-imation and clustering techniques for data mining.

The CAC method can be applied to many diverse applications, such as calling emergency services, diagno-quickly is more important than accuracy. CAC can fulfill this requirement with its relatively stable perfor-mance and good accuracy. Note that, CAC discovers frequent itemsets based on support approximation, it is thus not appropriate for use in applications where mining results must be exact and based on the precise support values of itemsets. However, such applications are uncommon since itemset frequency depends on not only its support value but on the minimum support threshold, and most applications rely on tuning the threshold to generate a useful set of frequent itemsets.

This paper is organized as follows: Section 2 presents the related work, and Section 3 describes the problem of concern. Section 4 presents the details of the CAC method. Section 5 proposes the SC table and the CAC algorithm. Section 6 presents an impact analysis of clustering, and Section 7 shows our experimental results.
Finally, Section 8 concludes this paper. 2. Related work 2.1. Association rule-based mining Several algorithms governing the discovery of association rules and frequent itemsets have been proposed.
The Apriori algorithm [1] was proposed to discover association rules by generating candidate itemsets with a bottom-up method and repeatedly verifying their supports by scanning a database. The algorithm stops min-ing when no further candidate itemsets can be generated. It is simple to implement but generates too many candidate 2-itemsets, DHP [24] stores the candidate 2-itemsets into a hash table and speeds up access to them the majority of candidate itemsets do not appear in the candidate 2-itemsets. FP-growth [10] creates an
FP-tree in memory by picking out all frequent 1-itemsets and rearranging them by decreasing order of their supports. By tracing the paths in the FP-tree, FP-growth can discover all frequent itemsets efficiently.
Mining performance can be improved by altering the pruning technique or the database format. Pincer-search [16] adopts both top-down and bottom-up pruning techniques. Frequent itemsets discovered in the top-down step are used to prune infrequent candidate itemsets generated during the bottom-up step. This property can speed up performance by reducing access time. The algorithms in [34,38] boost mining perfor-mance by taking advantage of different database format or data representation. Using a vertical database for-mat, MaxClique [38] determines the ID list of the transactions for each item wherever it may appear. By counting the intersections of these ID lists, MaxClique discovers association rules by scanning the database only once. By representing each transaction as a bit-vector showing whether attributes appear in that trans-action, DLG [34] adopts the Boolean  X  X  X nd X  X  operation between vectors to discover association rules. On the such as CHARM [37] and CLOSET [25] . The FCI concept has two benefits: the number of FCI is less than of FCI can improve mining performance.

The above methods complete their mining tasks by a single technique. Another issue of combining other section. 2.2. Mining by combining association rules and clustering
Many different clustering techniques [11,14,22,23,27,29,35,38] for data mining have been developed to par-niques, and tools for clustering, such as preprocessing techniques and bag-oriented clustering. By combining with other mining techniques, several methods [9,11,15,22,29] extend clustering techniques to improve performance.

Combining association rule methods with clustering techniques [11,27,29,38] has been widely discussed the clusters. CDAR thus improves performance by decreasing the members in clusters. MaxClique [38] par-structed for each cluster. By searching these sub-lattices with an efficient traversal technique, MaxClique speeds up the time and reduces the space required to discover frequent itemsets. Skewedness however is a potential problem with the clustering step in MaxClique.

Another important direction of association rule mining is to develop various support approximation tech-niques. They trade the accuracy of mining results for mining performance. These techniques are described in the next section. 2.3. Mining by approximation
The concept of support approximation [3,6 X 8,12,20,21,32,33] is used to improve performance when dis-covering frequent itemsets. The method in [33] reduces the number of frequent itemsets by creating con-densed representation of frequent itemsets, called FNAD (frequent non-almost-derivable itemsets).
Through FNAD, [33] can approximate the lower/upper bounds of their supersets X  supports. On the other hand, [6,7] proposed a structure called free-sets to overcome the problem presented by frequent queries. Its key intuition is replacing an itemset X with one of its subsets; this step reduces the complexity of repeat-edly mining itemset X .

Considering that much data noise and many measurement errors may exist in real datasets, Liu et al. [20,21] proposed a noise-tolerant itemset model to approximate the support of frequent itemsets. According to a user-defined error threshold, the method in [21] transforms the minimum support into a noise-tolerant support threshold used to complete the mining task. The neural network approach has also been applied to support approximation. With the Self-Organizing Map (SOM), a well-known neural network model used for data clus-tering, Baez-Monroy and O X  X eefe [3] proposed a probabilistic approach to identify the support of possible itemsets and to discover frequent itemsets.

Additionally, Jea et al. [12] developed an approximation technique named OCA to count itemsets by inte-grating the Principle of Inclusion and Exclusion in Combinatorial Mathematics [18] with an approximating equation [17] . The OCA algorithm uses this technique to verify and derive frequent itemsets by calculation, rather than by repeatedly scanning the database. Although it is stable and performs well, at times, support approximation may be inaccurate. Thus there is room for further improvement. Based on OCA, this paper proposes a new algorithm named CAC, which combines data clustering with support approximation to improve both the accuracy and performance of frequent itemset mining. 3. Problem description
The OCA algorithm [12] is used to efficiently discover the itemset supports based on the concept of support approximation. Two important equations, the Principle of Inclusion and Exclusion (shown in Eq. (1) below) and an approximating equation [17] (shown in Eq. (2) below), are adopted in OCA to approximate the sup-the parameters of Chebyshev polynomial a m , n , Eq. (2) can approximate the support of the union term with n items (where m &lt; n ): Symbols including sup OCA ( X ), subset( X , z ) and tsup the combination of items is also an itemset.)  X  Let X be an itemset.  X  sup OCA ( X ) denotes the support of X derived by OCA. j Q j = z }. of sup OCA ( Q ) where Q 2 subset( X , z )).
 tsup OCA ( X ,3) is the summation of sup OCA ( abc ), sup
Using these notations, we reformulate the support approximation equation in OCA as follows: For a section term of Eq. (1) and of Eq. (2) ). Substituting these intersection terms for tsup and (2) ,wehave By substituting Eq. (3) into Eq. (4) : Since tsup OCA ( X , i ) represents the total support of the members in subset( X , i ), we have tsup sup OCA ( X ). Then
By Eq. (5) , the OCA algorithm can calculate the approximate supports of itemsets of distinct length recur-sively without repeatedly scanning the database. An example of the approximating steps in OCA is shown be-respectively. There are five items, a , b , c , d and e , in this database. To derive sup (2) , OCA derives the support of bcd , i.e., sup OCA ( bcd ) = 2.38 (note that the actual support is 2).
At times, mining accuracy may be unstable in OCA. For example, as OCA calculates the support of itemset of itemset abc , it displays significant approximation error. If accuracy can be improved, the OCA algorithm stable performance.

In this paper, our focus is on proposing an efficient algorithm to discover frequent itemsets from approx-imated supports using the clustering and the approximation methods. OCA is a good candidate since it has superior performance due to support approximation and its unstable mining accuracy can be improved with clustering techniques. Thus we propose (the CAC method) of combining clustering and supports approxima-tion (OCA). As opposed to the traditional use of clustering methods to reduce the members in clusters, the
CAC method adopts a clustering technique to improve the mining accuracy of supports approximation with minimal additional cost. The details of the CAC method are presented in Section 4 . CAC also uses a data structure (called SC table) to preserve the quality of mining performance as discussed in Section 5 . 4. CAC method (combinatorial approximation with clustering)
CAC adopts three steps, itemset clustering, subset compensation and support approximation, for executing less relevant to an itemset so as to improve approximation accuracy. The subset compensation technique, were excluded during the previous clustering step. Finally, CAC completes the support approximation by accuracy of CAC X  X  approximation, as verified by our experimental results. 4.1. Items clustering
The CAC method adopts the Principle of Inclusion and Exclusion and uses Eq. (1) to calculate the approx-its approximate support. For example, in calculating the support of itemset abcd , too many subsets b coming imation becomes important in CAC. In order to generate a cluster effectively, the concepts of relationship evaluation and overlapped itemsets are defined with regard to the CAC method, as shown in Definition 1 . Y .

CAC method. To improve the mining accuracy of support approximation, CAC excludes less relevant itemsets central itemset of this cluster is below an assigned threshold e . Two functions, Class
Class e ( G ) represents a cluster of G within similarity threshold ( m = e ). j Class is, the number of members in this cluster. Class e ( G ) can be generated using Eq. (6) : where D is a transaction database and e is the similarity threshold.
 in accordance with the similarity threshold. An example of Class tical sketches represent itemsets (members of a cluster) and clusters, respectively. In Fig. 1 , Class the threshold, for example m = 2, fewer members are included in the cluster. In Fig. 1 , only three itemsets, the similarity threshold. Note that, according to the definition of a cluster in Eq. (6) , itemset abc is not included in cluster abc .

Itemset clustering in our method has a reduced space requirement. Instead of preserving the details of each member, CAC only retains the size of the Class m ( G ) (denoted as j Class spatial and clustering complexity of CAC. For example, in Fig. 1 the CAC method only retains j Class 2 ( abc ) j = 3, rather than the itemsets ab , abd , abc and abcd . 4.2. Subset compensation
The CAC method must compensate the approximate support of an itemset since legitimate subsets with low subsection, we analyze their size and use its upper limit to offset the approximate support.
Definition 2. Let X and G be itemsets, and Class e ( G ) represent the cluster with central itemset G in the similarity threshold e . Itemset X is called an over-discarded subset if the following two conditions hold: 1. X is an m -overlapped itemset of G , where m &lt; e . 2. X G , i.e., X is a proper subset of G .
 An itemset X  X  over-discarded subsets are those of its subsets with similarity below the similarity threshold.
For example, itemset b (an over-discarded subset) is excluded from a cluster Class is less than 2, in spite of the fact that itemset b is a proper subset of itemset abc .
The subset compensation step compensates for the support of an itemset by including its over-discarded ity, the CAC method only retains the size of cluster, i.e., j Class derived by CAC, respectively. The problem of reduced information arises when the CAC method starts to sup m CAC  X  ab  X  X  8, sup m CAC  X  ac  X  X  10 and sup m CAC  X  bc  X  X  6 to generate the cluster Class be derived from the existing information because they have been excluded from Class over-discarded subsets cannot be exactly recalled, the maximum of their size can be determined (as shown in
Lemma 1 ) and used for compensation. Corollary 1 demonstrates an essential property of supersets, which is used in Lemma 1 .

Corollary 1. Let G, X and Y be itemsets, X and G be the supersets of Y and X, respectively, such that
Proof. Since G is a superset of X and Y , there are C j G jj Y j to generate X such that X is a superset of Y and j X j = j Y j + 1. Because j G j = j X j +1, C Thus, Corollary 1 is proven. h For any Y, sup OCA (Y) in the cluster Class j X j (G) is limited to the range  X  Max Max X Y sup  X  X  X  is the maximum of sup OCA (X). That is, Max
Proof. Because j X j is the similarity threshold of Class (3) , itemsets X and Y have been included and excluded, respectively, from cluster Class ollary 1 , sup OCA ( Y ) in such a cluster can only come from the two supersets (labeled as X of Y such that j X j = j Y j + 1; that is, in Class j X j sup OCA ( Y ) except those transactions containing X 1 or X taining both X 1 and X 2 , labeled as D ( X 1 X 2 ), will be over counted when we calculate sup mation of sup OCA ( X 1 ) and sup OCA ( X 2 ). Thus, after removing the over counted value, i.e., D ( X is computed as
The maximum value of sup OCA ( Y ) appears when D ( X when D ( X 1 X 2 ) is maximal, i.e., D ( X 1 X 2 ) equals the minimum of sup sup OCA ( Y ) = sup OCA ( X 1 ) + sup OCA ( X 2 ) minimum(sup both cases, we prove the lemma. h Consider the example in Table 1 where sup OCA ( bc ), sup
By Lemma 1 , the range of item b is [6,9] where the lower bound is generated from the maximum of 6 and 3, and the upper bound is derived from the summation of 6 and 3. Note that, because item b is not a subset of range in Lemma 1 , CAC compensates for the support of the over-discarded subsets.
 CAC can generate more candidate itemsets (and increase the hit ratio) than the previously presented
OCA algorithm by utilizing the range in Lemma 1 . In Eq. (5) , a and 1. A strategy of choosing to use the lower or upper bound is implemented in CAC to obtain larger approximate itemset supports during the calculations of Eq. (5) . For a given similarity m and i 5 m ,if a negative, the lower bound in Lemma 1 will be used in CAC; otherwise, the upper bound in Lemma 1 will be used. Thus
By Eqs. (5) and (7) , we have
Corollary 2. Let X be an itemset of z items and a i be the factor  X  1  X  i, a i  X  tsup CAC (X,i) = a i  X  tsup OCA (X,i).

Proof. According to Lemma 1 , Max X Y sup  X  X  X  5 sup OCA (1) if a i = 0, then (2) if a i &lt; 0, then Therefore, this corollary is proven. h
Lemma 2. Let X be an itemset comprising z items. For a given similarity m, sup
Proof. By Eqs. (5) and (8) According to Corollary 2 , we have this lemma. h
Compared with OCA, CAC not only obtains larger approximate supports (as shown in Lemma 2 ) but also ratio Jea et al. [12] (as proven in Theorem 1 ) than the OCA algorithm. The analyses in Section 6 and exper-imental results in Section 7 also verify the accuracy of the CAC method.
 5. Subsets-counting table and CAC algorithm CAC employs a data structure (namely SC table) to record the approximate supports of itemsets X  subsets.
SC table. Section 5.3 explains in detail the CAC algorithm. Based on the SC table, CAC counts the supports of itemsets very efficiently as seen in Section 7 . 5.1. Subsets-counting table (SC table)
The CAC method maintains a table, called SC table, for all frequent itemsets of length z . For each frequent calculate the support of candidate itemsets of length z +1. Fig. 2 shows the record format of the SC table, X  X  X  approximate support. By contrast, field 1 to field z 1 store the values of tsup m from 1 to z 1, respectively.

Fig. 3 shows a sample record for itemset abcd in the SC 4 field 5 and field 4 store the itemset abcd and its support 2, respectively. The subsequent fields store
The advantage of the SC table is avoiding redundant calculations by storing the previous calculating results for example, the value 12 in field 3 is the summation of the supports of abcd  X  X  subsets with a length of 3; using the SC table. 5.2. Counting candidate itemsets using the SC table all of its subsets of length z are frequent. By scanning the z th field of the SC a frequent z -itemset), CAC can generate all candidate ( z + 1)-itemsets. For each candidate itemset X ,anSC k th field in all of its subsets in the SC z table.

Instead of scanning the database to check whether X is frequent, our CAC method directly calculates, according to Eq. (8) , the support of X by accumulating the supports of its subsets stored in the SC table. Eq. (8) requires the supports of X  X  X  subsets of different lengths to approximate sup m provides these values in its distinct fields. If itemset X is frequent, i.e., sup m support value, sup m CAC  X  X  X  will be stored in the ( z + 1)th field of X  X  X  SC inserted into the SC z +1 table. Otherwise, the X X  X  SC z +1 after all frequent ( z + 1)-itemsets have been discovered, the original SC transferred to the SC z +1 table.

Consider the example in Fig. 4 . In order to calculate the support of a candidate itemset bcde , the four records of its subsets, i.e., itemsets bcd , bce , bde and cde , in the SC in each corresponding field are accumulated, calculated (by Lemma 3 below), and stored into the correspond-ming up the values 16, 18, 15 and 14 (the values in field 1 of all bcde  X  X  subset records in the SC dividing the sum 63 by a factor 3 ( z k =4 1 = 3) according to Lemma 3 . The sum 63 must be divided by 3 into the field 3 and field 2, respectively, of bcde  X  X  SC From these field values, the support of candidate itemset bcde , i.e., sup m
Eq. (8) : where m represents the length of bcde and k = m 1. If bcde is frequent, i.e., 2 = the minimum support value, bcde  X  X  SC 4 record, respectively. This record is then inserted into the SC
Lemma 3. Let X be an itemset of z items and F i be the ith member of subset(X,z 1) where 1 6 i 6 z. For any similarity m, tsup m CAC  X  X ; k  X  X  1 z k
Proof. Let g k , j denote the j th member of subset( X , k ) and f i tively, where 1 6 k 6 z 2and1 6 j 6 C z k . Since X has z items, the subset( X , z 1) contains z members (i.e., F i ,1 5 i 5 z ). For every F i and a given k (1 6 k 6 z 2), since tsup m of the members in subset( F i , k ), we get tsup m CAC  X  F for X
Since F i is a subset of X , the items constituting the subset( F Eq. (9) shows that set( F i , k ). Then, by Eq. (10) , replacing the summation of subset( F we have g k , j F i X and X , F i and g k , j comprise z , z 1 and k items, respectively, to form a superset F g k , j , one can select ( z 1 k ) items from the ( z k ) items in ( X g
P thus proven. h 5.3. The CAC algorithm and its complexity analysis
To accomplish the clustering and approximating steps of the CAC method as discussed in Sections 4 and 5 , respectively, the CAC Algorithm presented in Fig. 5 completes three major functions: deciding the size of cluster, creating the SC table, and calculating the approximate supports. In order to complete the clustering step with similarity e , CAC needs the supports of the frequent itemsets from L imate those itemsets having length larger than e . In Line 1, CAC generates these frequent itemsets by scanning the database e times against the threshold value ms (as the Apriori algorithm does). In Line 2, CAC generates candidate ( e + 1)-itemsets (like the Apriori algorithm) from the results in Line 1. The CAC algorithm computes both the j Class e ( G ) j and the support of itemsets in Line 6. In Lines 7 X 10,
CAC finds frequent ( e + 1)-itemsets and creates the SC e +1 all frequent itemsets whose lengths are larger than ( e + 1). Line 17 searches the subset records in the
SC table and accumulates each field of records. Using the correcting factor in Lemma 3 , Line 18 corrects the supports. In Line 21, CAC calculates the support of each candidate m -itemset A by Eq. (8) , and the field values of A  X  X  record in the SC m 1 table. Lines 22 X 26 check these supports against ms and insert the been generated.

The comparison of complexity between OCA and CAC is given in Table 2 . To make a fair comparison, the analysis of CAC assumes the special similarity threshold, i.e., m = 2, meaning that CAC only uses the same information (of frequent 1-and 2-itemsets) as OCA in support approximation. Let n and d be the total number of attributes and the maximal length of transactions in a database, respectively. The time and space complexities of OCA can be found in [12] . Unlike OCA which retains all subsets of an itemset,
CAC only retains the SC table in memory, which in the worst case, requires C consisting of d + 1 fields. Hence the space complexity of CAC is O  X  d C shows that CAC demonstrates less space complexity than OCA. On the other hand, by Eq. (1) , OCA requires all subsets of an itemset to complete its approximation step. Time complexity is O  X  sets. However, by storing the needed information in the SC table, it is not necessary for the CAC method to repeatedly calculate all subsets. This advantage allows CAC to remove a for-loop step in OCA and reduces its time complexity (as shown in Table 2 ). The space consumption of CAC (mainly the space of SC table) depends on the number of frequent itemsets. For example, as shown in Table 6 in Section 7 , the maximum length of frequent itemsets in the transactions of the T20I6L150D100K dataset is 14 and CAC discovers 17,853 itemsets with minimum support of 0.05. Among them, the 7-itemset has the largest number, 3451, of occurrences. In the worst case, CAC must store the 3451 SC records in the SC table, each consisting of 7 fields. Assume that the first field of each record uses 20 bytes to store the name of an itemset and the others use 4 bytes for each field to store a subset X  X  support. CAC thus needs about 0.14 MB, 3451 * (20 + 4 * 6) bytes, to store the SC table. 6. Impact analysis of clustering approximate support ( Lemma 4 ) of CAC. It also shows the impact of the similarity threshold on clustering this analysis.

Lemma 2 indicates CAC always generates a larger support than OCA. It implies CAC generates more fre-quent itemsets than OCA, as stated in Corollary 3 .
 Corollary 3. For any similarity m, W m CAC = W OCA .
 Proof. For every itemset X and any given minimum support ms ,if X is counted into W sup OCA ( X ) = ms . According to Lemma 2 , sup m CAC  X  X  X  sup m CAC  X  X  X  = ms and X is also counted into W m CAC . Therefore, W Theorem 1. For any similarity m, H m CAC = H OCA .

Proof. Let W T be the number of real frequent itemsets in a database for some minimum support ms . By the definition of hit ratio, H m CAC  X  W m CAC = W T and H OCA
Lemma 4. For any itemset X comprising z items, sup m  X  1 CAC
Proof. By Eq. (8) , sup m  X  1 CAC  X  X  X  sup m CAC  X  X  X  X  a m .By Corollary 2 , sup m  X  1 CAC  X  X  X  sup m CAC  X  X  X  = 0. Thus, the lemma is proven.
Corollary 4. For a given similarity m, W m  X  1 CAC = W m CAC
Proof. For every itemset X and any given minimum support ms ,if X is counted into W
Corollary 5. For a given similarity m, H m  X  1 CAC = H m CAC Proof. Let W T be the number of real frequent itemsets in a database. By the definition of hit ratio,
H
H
Corollary 5 indicates that the hit ratio resulting from CAC is a monotonically increasing function of the
CAC. On the other hand, in order to produce a clustering step with a larger similarity threshold (generating more over-discarded subsets), CAC must complete additional subset compensation steps (as shown in Section reducing the computation time. Without using any statistical approach [30] and information about database distribution, Lemma 5 and Corollary 5 provide a clue for CAC to select the similarity threshold.
Lemma 5. Let X be the central itemset of some cluster. With respect to X, also let N itemsets filtered out by similarity m + 1 and similarity m, respectively. Then, N
Proof. Let N be the total number of transactions in a database. Since each transaction can be treated as an itemset, there are N itemsets in total in the database. With respect to the central itemset X , N
N j Class m +1 ( X ) j , and N m = N j Class m ( X ) j , respectively. Thus, N
For any itemset Y ,if Y belongs to Class m +1 ( X ), then j X \ Y j P m + 1 according to Eq. (6) . Hence, j Class m +1 ( X ) j , it is also counted into j Class
N
Lemma 5 indicates that for a cluster Class m ( X ), N m resulting from CAC is also a monotonically increasing clustering step, increasing the accuracy of the approximation. However, to support a larger N running time. It is then a trade-off between approximation accuracy and mining performance in selecting the similarity value m .

In order to investigate the effect of the new clustering technique (developed in CAC) on support approxi-mation, we conduct several experiments in the next section by selecting m = 2 for the CAC algorithm. The reason is that the support approximation technique of OCA (the predecessor of CAC) starts approxima-tion-based on the frequent 1-and 2-itemsets which are generated by scanning a database twice (with respect to m = 2 in CAC). This selection makes a fair comparison of OCA (pure support approximation without clus-tering) and CAC (support approximation with clustering). 7. Experimental results Our experiments have been conducted on a personal computer with a 550 MHz CPU and 128 MB RAM. The operating system is RedHat 9.0, and the simulation program is implemented in C language.
Test databases, including synthetic and real-life datasets, are generated from the programs provided by the IBM Data Mining Research Group [41] and FIMI04 [40] , respectively. Each synthetic dataset has 1000 attributes and their parameters are listed in Table 4 . The standard solution (the number of real fre-quent itemsets in a database) is generated by DHP [24] . To meet the complex properties of data mining discussed in Section 1 , we increase the co-relation between items in our synthetic datasets by decreasing the L y parameter, such as T15I6L100D100K and T20I6L150D100K. This makes the mining task more intractable in lower minimum supports. Two real-life datasets, connect and accidents, downloaded from [40] are also used in the experiments. They consist of 67,557 transactions (with 42 attributes) and sets are listed in [40] .
 been evaluated with the compared algorithms (CAC and OCA). Due to support approximation, both CAC and OCA may generate frequent itemsets that are not the real (as discovered by DHP). The recall ratio is defined as the number of real frequent itemsets discovered by CAC or OCA to the total number of real fre-quent itemsets discovered by DHP. The precision ratio is defined as the number of the frequent itemsets dis-covered by CAC (or OCA) to the total number of frequent itemsets by CAC (or OCA) itself. The F -measure, a well-known metric to evaluate a clustering method, is defined as 2 higher F -measure value results from a better clustering method. Based on these measurements, the experimen-tal results are listed and compared in Tables 5 and 6 .

The CAC algorithm retains higher percentages of the recall ratio (called hit ratio in OCA) while testing minimum supports. The average recall ratio of CAC and that of OCA are 90.66% and 67.2%, respectively.
That is, the average approximating accuracy of the CAC algorithm increases by 23.46% that of the OCA algo-rithm. In the best case, CAC is 90.05% higher than OCA in database T20I6L150D100K with minimum sup-port of 5%. The CAC algorithm excludes the disturbances of m -overlapped itemsets effectively, and generates a higher average accuracy (recall ratio).

On the other hand, CAC results in losses amounting to small percentages of the precision ratio. In the exper-
CAC and OCA are 98.77% and 99.57%, respectively. Although CAC generates more itemsets to increase accu-tal data, CAC has a precision ratio only 0.8% less than OCA. By contrast, since CAC effectively increases the recall ratio, it also increases the F -measure, i.e., 2 * the F -measure of CAC is always larger than that of OCA. The averages of the F -measure in CAC and OCA are 93.88% and 77.26%, respectively. In the best case, CAC is 89.09% greater than OCA in database T20I6L150D100K with minimum support of 5%.

Through support approximation, CAC can effectively discover a large number of real frequent itemsets based on frequent 1-and 2-itemsets. Tables 5 and 6 show that the total frequent itemsets discovered is 4.4 ratio (93.95%). This fact shows that CAC is useful and effective for generating long frequent-itemsets (of length 14) from short ones (i.e., frequent 1-and 2-itemsets).

CAC preserves high performance in testing minimum support. Figs. 6 and 7 list the running time of differ-ent mining algorithms, including CAC, OCA, FP-growth [42] , and DHP. Some results (e.g., in the lower min-imum supports of DHP and OCA in Fig. 6 and that the all minimum supports of FP-growth in Fig. 7 ) are not shown because their running times become exponentially high in these minimum supports. Not only with low minimum supports, but also in a large database (e.g., 1000 K transactions in Fig. 6 ) CAC maintains high per-the database so it can preserve high performance. Although CAC has higher requirements to complete the clustering steps, the additional cost is only 2.3% of the total running time on average. The CAC algorithm on average improves system performance by 39.90% compared to OCA, and it uses less running time than the other algorithms in the experimental results. In the testing experiments, CAC on average uses 71.45% the running time of that of FP-growth (as shown in Fig. 6 ). Fig. 7 shows a comparison of running times between FP-growth and CAC in the two real-life datasets, connect and accidents. In these testing minimum supports, CAC maintains high performance and outperforms FP-growth. In the testing experiments, CAC only uses 64.36% the running time of FP-growth on average. Specifically, in the low minimum supports (less than 50% for the connect dataset in Fig. 7 ), CAC can complete the mining task more efficiently than FP-growth.
 8. Conclusion
Improving performance and increasing space efficiency in data mining are important and urgent issues. By port approximation method, CAC algorithm, to calculate the approximate supports of itemsets using the Prin-ciple of Inclusion and Exclusion. Without scanning a database repeatedly, CAC preserves the system performance with a reduced space requirement. CAC also adopts a clustering technique with that costs less can be improved. Different from other support approximation methods discussed in Section 2.3 , CAC can dis-cover frequent itemsets without the need to assign an error ratio [6,7,21,32] and scan a database repeatedly.
In the experiments, CAC on average increases by 23.46% mining accuracy and by 39.90% system perfor-mance compared with its predecessor, the OCA algorithm (which does not use any clustering technique).
Experimental results also show that, the CAC method preserves both relatively stable performance (64.36% running time of FP-growth on average) and mining accuracy (recall ratio of 90.66% on average), even in lower minimum supports or in databases with greater numbers of transactions. In the applications that allow toler-to discover frequent itemsets in databases.

References
