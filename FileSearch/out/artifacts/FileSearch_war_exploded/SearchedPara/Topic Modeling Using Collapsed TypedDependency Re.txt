 A large amount of text corpora and discrete data demands more on improving people X  X  ability to interpret and comprehend them. Previously, texts were col-lected and stored in large text reposito ries and retrieved by a set of keywords. Documents were seldom analyzed using t heir themes, because there were very few technologies to extract their thematic structures. During the past decade, topic modeling has emerged to remedy the situation. Topic modeling is a powerful statistical tool to uncover hidden thematic structures of documents, to facilitate document summarization and organization in a variety of applications in natu-ral language processing, vision, social network analysis, and text mining [1 X 3]. Most topic models consider documents to be a weighted mixture of topics, where each topic is a multinomial distribution over words. An inferred topic model of a corpus assigns high probability to members of the corpus as well as to other similar documents [1, 2]. Text documents are the only observed data in most conventional topic models. However, mo re recent topic models extend previous models by incorporating extra information [4]. Extra information is obtained by enriching text representation to include information, such as authors of the documents [5], images associated with the t ext [6], style of writing and reviewers of the documents [7]. The aforementioned topic models represent documents as a bag-of-words, where the order of words, thus important linguistic structures of documents are neglected [1, 2].

In order to include richer linguistic structures of text documents, many meth-ods were proposed to incorporate local word dependencies into topic models [8 X 12]. Local word dependencies are either dependencies between a set of con-secutive words, or a set of nonconsecuti ve words with arbitrary distances. For example, the term 1  X  X ata mining X  contains two words  X  X ata X  and  X  X ining X  that are consecutively related. In addition, in sentence  X  X here are countries that deny human basic civil rights. X  ,theterm  X  X uman rights X  contains two nonconsecutive words  X  X uman X  and  X  X ights X  that are syntactically related. In order to capture sequential consecutive dependencies between words, the Bigram Topic Model [8] and Topical n -gram Model [9] extend word gen eration by conditioning on n pre-vious words. However, the n -gram topic models do not capture relations between nonconsecutive words.

To remedy this problem, some recent m ethods integrate g rammatical regu-larities of text documents into topic models. HMM-LDA [11] uses the states of a Hidden Markov Model to represent syntactic and semantic words. Then, the model assumes that words are either sampled from topics randomly drawn from the topic mixture of the documents or from a syntactic class sampled from a distribution of associated syntactic cla sses [12]. Their model only considers lo-cal dependencies between variables of t he syntactic states and fails to obtain syntactic or semantic dependencies between words. The Syntactic Topic Model (STM) [10] was proposed to integrate grammatical regularities in the text to detect syntactically relevant topics. I n STM, documents are collections of de-pendency parse trees, in which words in the sentence are the nodes in the graph and grammatical regularities are the edge labels [13]. The root in the dependency parse tree is used as a governor. Topic assignment of the root node affects topic assignments of all its children. Moreover, STM does not draw words from just the document distribution over topics. Rather, it draws a word from a distri-bution formed by the document distribution over topics weighted by the parse tree distributions. Thus, topic assignment of a word depends on both the doc-ument X  X  theme as well as the parents of the word in the parse tree. Although, STM improves topic modeling by combining syntactic and thematic structures of documents, it does not fully distinguish topic assignment of the words that share the same parent in the tree, i.e., children of a node. This problem specifically occurs when a root node has many children [10].

Moreover, text documents consist of w ords with possible conceptual simi-larities, called synonyms , defined in lexical resources like WordNet [14]. It is reasonable to expect the distribution of topics over synonymous words to be similar.

In this paper, a novel topic model is prop osed to consider syntactic and semantic structures of text documents in probabilistic topic models. In essence, we enrich text documents with the collapsed typed dependency relations to circumvent ob-stacles in acquiring consecutive and non consecutive dependencies between words. In addition, we investigate the influence of enforcing similar topic distribution over conceptually similar words by generalizing words with their synonyms.
The structure of this paper is as follows: In Section 2, we discuss our proposed topic model incorporated with collapsed typed depe ndency relations. We also explain our method for generalizing wor ds using synonyms. Section 3 introduces some criteria to eval uate topic models. Then, it demonstrates the effectiveness of our approach through experiments. Finally, Section 4 concludes the paper with some remarks on our future work. In this section, we first explain the collapsed typed dependency relations and how to find them from the HPSG parse trees. These relations are used in capturing consecutive and nonconsecutive dependenc ies between words of text documents. We then describe our topic model and how it embodies collapsed typed de-pendency relations. In addition, we propose a method to enforce similar topic distribution over synonymous words of text documents. Lastly, we explain the relationship between our contributions and other related work. 2.1 Collapsed Typed Dependency Relations and HPSG Parse Trees The bag-of-words representation of text documents is of particular interest in most topic models. However, this representation does not contain information about the relations between words. Relations could hold over a consecutive or nonconsecutive neighborhood of a word [15].

In this work, we use the collapsed typed dependency relations to acquire syntactic and semantic structures of text documents. This acquisition enables us to further capture consecutive and non consecutive relations between words of text documents. The collapsed typed dep endency relations are extracted from typed dependency parse trees. The typed dependency parse tree of a sentence provides a tree representation of detaile d grammatical relations between words in the sentence [16]. Words in the sentence are nodes of the tree and grammatical relations are the edge labels. The total number of grammatical relations that can be assigned by typed dependency par se trees is 48 [16]. Table 1 shows most common grammatical relatio ns used in typed dependency parse trees. For more information on this set of relations, please see [13].
 Typed dependency parse trees ar e constructed according to the Head-Driven Phrase Structure Grammar (HPSG). HPSG, developed by Pollard et al. [17], is a highly structured grammati cal representation of text documents that effectively analyzes syntactic relations concerning multi-word constituents [15, 16]. The HPSG-based parse tree of a sentence starts from a root and ends in leaf nodes which represent words. Internal nodes of the tree represent syntactic roles of the connected leaf nodes. For example, Figure 1 2 represents the HPSG-based parse tree of the sentence  X  X here are countries that deny human basic civil rights. X  . In this tree, the leftmost branch, node NP represents the role of  X  X oun phrase X  for the leaf node  X  X here X  .
HPSG provides a high level syntactic rep resentation of sentences in text docu-ments [16]. However, we need to capture s pecific relations between every individ-ual related pair of words. Thus, we need to elaborate HPSG to include additional labeled grammatical relat ions between words. This is achieved by constructing typed dependency parse trees from HPSG-based parse trees, using an algorithm described in [16]. This algorithm has two phases: dependency extraction and de-pendency typing. In the first phase, a sentence is parsed with a phrase structure grammar parser (HPSG). The output of this phase is arranged hierarchically and rooted with the most generic relation. In the second phase, when the rela-tion between an internal node and its connected leaf node can be identified more precisely, more specific grammatical relations further down in the hierarchy is used. Figure 2 shows the typed dependency parse tree constructed from Figure 1 for the sentence  X  X here are countries that deny human basic civil rights. X  .As illustrated in this figure, nonconsecutive relations between words with gaps, i.e.  X  X uman rights X  is captured under the amod relation. Typed dependency parse trees are constructed using the Stanford parser toolkit that has phrase structured grammars integrated in [13, 16] 3 .

For each edge in the tree, we extract a relation rel ( w i ,w j ), where rel is the edge label representing a relation and w i and w j are two nodes of the edge. For example, the set of relations extract ed from the typed dependency parse tree, illustrated in Figure 2, is as follows: { expl (are, There), nsubj (are, countries), nsubj (deny, that), rcmod (countries, deny), amod (rights, human), amod (rights, basic), amod (rights, civil), dobj(deny, rights) } . These relations enable us to bet-ter distinguish topic assignments for the relations involving the same parent. For instance, a tree including a parent with c children, will be represented by c relations, where each relation denot es the edge connecting the child and the parent. Each relation can have a discriminate topic.

The relations from typed dependency p arse trees are further processed by collapsing relations involving prepositions and conjuncts to get direct dependen-cies between content words [16]. For instance, in the set of the aforementioned typed dependency relations, the relations involving the preposition  X  X hat X  will be collapsed. Thus, relations rcmod (countries, deny) and nsubj (deny, that) will become rcmod (countries, deny) and nsubj (deny, countries). As a result, collapsed typed dependency relations not only cap ture relations between consecutive and nonconsecutive words, but they also eliminate less informative relations involv-ing prepositions. In our work, we use the collapsed typed dependency relations to represent the corpus. Note that the order of words in the collapsed typed dependency relations matters. 2.2 Probabilistic Topic Model Using Collapsed Dependency We assume that corpus D consists of M documents denoted by D = { d 1 ,d 2 ,  X  X  X  , d Each document is represented by R collapsed dependency relations between words of the document, denoted by R = { r 1 ,r 2 ,  X  X  X  ,r R } . These relations are instances of the 48 grammatical relations describedinSection2.1,eachofwhich consists of two words.

Our topic model assumes that each document d l has a multinomial distribution over K topics with parameters  X  ( d l ) . Thus, for a relation in document d l , P ( z l = j |D = d model, the j th topic is represented by a multinomial distribution over R relations provide a procedure to generate documents. In this procedure, each document d l is generated by first drawing a distribution over topics (  X  ( d l ) ), generated from a Dirichlet distribution with parameter  X  . The relations in the document are then generated by drawing a topic j from this distribution and then drawing a relation from that topic according to a multinomial distribution with parameters (  X  ( j ) r l ), generated from a Dirichlet distribution with parameter  X  .

Note that the only observed variables are the relations in the collection of documents. Document distribution over topics and topic distribution over rela-tions are latent variables generated from Dirichlet distributions with parameters  X  and  X  , respectively. We use Gibbs sampling to obtain approximate estimates for the latent variables. Gibbs sampling is a simple Markov chain Monte Carlo algorithm that sequentially replaces the value of one of the latent variables by a value drawn from the distribution of that variable conditioned on the values of the remaining variables [19].

We adopt Gibbs sampling algorithm proposed by Griffiths et al. [2, 18] to draw a topic from the conditional distribution iteratively. For each topic j the distribution is given by where z  X  l and R  X  l denote the z and R for all relations other than r l .Thisexpres-likelihood is obtained by integrating over the parameters  X  , which results in where n ( . )  X  l,j is the total number of relations assigned to topic j , excluding the current one, and n ( r l )  X  l,j is the total number of relation r l assigned to topic j , excluding the current one.
 Similarly, the prior is calculated by integrating over the parameters  X  : d , excluding the current one.
 Then, the conditional distribution for the topic assignments is given by 2.3 Generalizing Words Using Synonyms Text documents often contain words that are synonyms. Sets of synonyms can be obtained from lexical resources like WordNet [14]. In this work, we investigate the influence of generalizing words using a synonym on topic modeling.
Similar to LDA [1], we assume that a document is a multinomial distribution over K topics, where each topic is a multinomial distribution over N words. We also assume that documents are represen ted by a sequence of words, denoted by W = { w 1 ,w 2 ,  X  X  X  ,w N } ,where w n  X  W is the n th word in the sequence. Given the fact that a set of synonyms shares a similar concept, it is reasonable to expect them to have similar probabilities under topics. For example, if a text document is about happiness, the inferred topic should assign higher probabilities to words such as delighted , blessed ,and prosperity ; and lower probabilities to words such as sad , bitter ,and sorrow . In order to ensure that topics are similarly distributed over synonyms, we propose the following algorithm to replace all synonyms of a word with an equivalent synonym with the highest frequency in WordNet: 1. Group the words from WordNet, based on their conceptual similarities. Each 2. For each group, find the frequency of the words in the group. The frequency 3. Select the most frequent word in the group as the group representative . 4. For each w i  X  W : For example, consider a text document that contains the word prosperous . This word belongs to the following group of synonyms { delighted , blessed , pros-perous , happy , fortunate } . Our algorithm finds the frequency of each synonym in WordNet. It selects happy as the group representative because it is the most frequent word in the group. Finally, our algorithm replaces the word prosperous with the word happy . 2.4 Relationships to Other Work In this work, we go beyond the bag-of-words representation of documents to incorporate syntax and semantics of text documents into topic models. This section reviews the theoretical relationships of our contributions with previous topic models that used syntactic and semantic structures of texts.

Our proposed topic model is similar to STM [10] due to using typed depen-dency trees to represent syntactic stru ctures of sentences. However, our topic model has following major differences with STM. Firstly, STM draws a word from a single distribution formed by the document distribution over topics weighted by the parse tree distributions. Thus, topic assignment of a word depends on both the document X  X  theme as well as the parent of the word in the parse tree. However, in our model we use two distributions: document distribution over top-ics and topic distribution over the collapsed dependency relations. We first draw a distribution over topics; then, we select a topic from this distribution and then draw a relation from that topic distribution over the collapsed dependency rela-tions. Secondly, STM does not fully distinguish topic assignments of the words that share the same parent in the dependency parse tree, i.e., children of a node, as stated by Boyd-Graber et al. [10]. However, in our model each pair of related nodes in the parse tree introduces a discriminate relation. Thus, topic assign-ment to the relations involving the same parent is better distinguished. Thirdly, STM does not use labeled dependency relations and lexicalization. However, our model uses the labels of dependency relations to distinguish and further col-lapse relations involving prepositions and conjuncts to get direct dependencies between content words. Finally, STM computes the posterior topic distributions by Bayesian variational methods. Our model uses Gibbs sampling to infer pos-terior topic distributions. This final difference is complementary rather than competitive.

In addition, our proposed topic model differs from the n -gram topic models [8] in capturing dependencies between words of a sentence. Our topic model con-siders dependencies between nonconsecutive words with a distance; while the n -gram topic model is limited to captur ing dependencies between consecutive words.
Moreover, our proposed model, uses WordNet to enforce topic similarity for words with conceptual similarities, by generalizing similar words with their syn-onyms. Lexical resources, i.e. WordNet , were previously used in topic models. Musat et al. [20] employs WordNet to improve topic models by removing unre-lated words from the simplified topic descriptions. Mei et al. [21] used WordNet to label each topic in a multinomial topic model. Newman et al. [22] uses Word-Net to evaluate topic coherence. None of them uses synonyms to generalize words prior to building topic models. We conducted experiments on two text corpora to compare the performance of four following topic models: LDA [1], LDA on generalized words using synonyms, explained in Section 2.3, the Bigram Topic Model [8], and the HPSG Topic Model, explained in Section 2.2 4 . The first three topic models were trained with 1000 iterations of Gibbs sampling [2, 18] used in the MALLET [23]. However, the HPSG Topic Model was trained with 1000 iterations of Gibbs sampling. Initial values for the hyperparameters (  X ,  X  ) applied to all our experiments were  X  =50 . 0and  X  =0 . 01. Note that these parameters are default parameters of the MALLET [23].
 In our experiments we used Associated Press corpus 5 that consists of 2246 Associated Press articles, 33872 words, and 454370 collapsed typed dependency relations. In addition, we used Reuters-21578 Distribution 1 . 0 6 that includes 22 files. Each of the first 21 files contain 1000 documents, while the last file contains 578 documents. This corpus contains a total number of 43012 words and 793345 collapsed typed dependency relations.

Table 2 illustrates top 10 terms of the most probable topics generated by aforementioned topic models on the Reuters corpus. The first column shows the words generated by LDA. Some words in this topic are ambiguous and can have multiple meanings. To identify the correct meaning of each word, one needs to consider other words in the topic. For example, the word  X  X hare X  has many meanings. Observing other words in the topic, such as  X  X ank X  and  X  X rofit X  ,helps to identify the correct meaning of the word  X  X hare X  that is  X  X ssets belonging to an individual X  . The second column shows the results of LDA on generalized words using synonyms. These words are similar to the words in the first column and still suffer from ambiguity. The terms generated by the Bigram Topic Model and the HPSG Topic Model are shown in columns three and four, respectively. These topic models have less ambiguity, given the fact that they generate terms that include pairs of words that are more descriptive than single words. In addition, as opposed to the Bigram Topic Model, terms generated by the HPSG Topic Model are not only limited to consecutive pairs of words of a sentence, but they also contain pairs of related words with gaps.

Given the text corpora, we compare our work with other topic models based on the following criteria:  X  High likelihood on a held-out test set (perplexity) [1].  X  Stable distribution of topics over words across samples [5].  X  Coherent distribution of words learned by individual topics [22].  X  Accurate distribution of topics over words.

These criteria and experimental results a re discussed in the subsequent sections. 3.1 Perplexity Perplexity is the most common criterion to evaluate the quality of topic mod-els [24]. Perplexity measures the cross-entropy between the word distribution learned by the topic model and the distribution of words in an unseen test document. Thus, lower perplexity score indicates that the model is better in predicting distribution of the test document [1, 25]. We evaluate perplexity as a function of number of topics for both Asso ciated Press and Reuters corpora. We trained the topic models on 90% of the corpus to estimate the held out proba-bility of previously unseen 10% of the corpus. We compute the perplexity of the held-out test set with respect to the HPSG Topic Model by where D test is the test corpus with M documents, R d denotes the set of collapsed typed dependency relations in document d , | R d | is the total number of collapsed typed dependency relations in document d ,and P ( R d ) is the probability estimate assigned to R d by the HPSG topic model. The perplexity of D test by other topic models, such as LDA, is defined similarly, except that R d is replaced by W d , the set of words in the corpus.

The results are illustrated in Figures 3, 4, 5, 6. The x-axis shows the number of topics ( K ) used in each model; the y-axis shows the perplexity. These figures clearly indicate that the perplexity of o ur proposed topic model drastically de-creases the perplexity of LDA and LDA o n generalized words using synonyms. Moreover, the perplexity of our proposed topic model is slightly better than the perplexity of the Bigrams Topic Model.
 3.2 Stability Stability is the similarity of topic distributions over words across different sam-ples [5]. We follow the algorithm proposed by Rosen-Zvi et al. [5] to find the best one-to-one topic alignment across samples. The algorithm finds the best symmetrized Kullback Leibler (KL) divergences between the K topic distribu-tions over relations from samples S 1 and S 2 . KL divergence is calculated by tions in the samples [26]. We compare the stability of topic distributions over relations across samples, generated by the HPSG Topic Model and LDA on the Reuters corpus. The results, illustrated in Tables 3 and 4, show that our pro-posed topic model is comparably as stable as LDA in producing similar topic distributions over words across multiple samples. Similar results were obtained using the Bigram Topic Model. 3.3 Topic Coherence Topic coherence measures the integrity or coherence of a set of words generated by a topic model. Words generated by topic T , denoted by T = { w 1 ,w 2 ,  X  X  X  ,w n } , are coherent if they are semantically similar. In order to calculate the topic co-herence score, we adopted the method proposed by Newman et al. [22]. We calculate the semantic similarity scor es between every pair of words in a topic using the Lesk algorithm [27] 7 . Then, we compute their arithmetic means. We compared the topic coherence of top 50 words from 20 topics generated by LDA, LDA on generalized words using synonyms, the Bigram Topic Model, and the HPSG Topic Model on Reuters corpus. The results are shown in Table 5. The HPSG Topic Model generates slightly more coherent topic distributions over words than The Bigram Topic Model. The HPSG Topic Model performs com-parable to LDA in topic coherence. Howe ver, LDA on generalized words using synonyms results in more coherent topic distribution over words. This coherence is due to the fact that we replaced concep tually related words with one general word, prior to modeling the topic assignments. 3.4 Accuracy The accuracy of a topic model is the degree of closeness of the topic distribution over words of a test corpus to actual topic distribution over words of a topic-labeled corpus. Note that calculating accuracy depends on the availability of the topic-labeled corpus.
 Weassumethatthetestcorpus T consistsof M documents T = { d 1 ,d 2 ,  X  X  X  ,d M } . Each document consists of H actual topic labels, denoted by L = { l 1 ,l 2 ,  X  X  X  ,l H } , where each l i  X  L represents an actual topic-label for the document. As mentioned earlier, a topic model generates K topics, where each topic is a distribution over n words, denoted by T = { w 1 ,w 2 ,  X  X  X  ,w n } . The accuracy score of the topic model is notes the semantic similarity between two sets of T j and L . This semantic similarity is measured using the Lesk algorithm, explained in Section 3.3.

We compared the accuracy of LDA, LD A on generalized words using syn-onyms, the Bigram Topic Model, and the HPSG Topic Model on a subset of Reuters corpus that contains topic-labeled documents. As illustrated in Table 6, these algorithms are comparable in terms of accuracy. However, LDA is slightly better. We proposed a novel method that incorporates syntactic and semantic struc-tures of text documents into probabilistic topic models. This representation has several benefits. It captures relations between consecutive and nonconsecutive words of text documents. In addition, the labels of the collapsed typed depen-dency relations help to eliminate less important relations, i.e., relations involving prepositions. Also, words of text documents, regardless of their parents in the collapsed typed dependency parse trees, are distinguished in topic assignment. Furthermore, our experimen tal studies show that the proposed topic model sig-nificantly outperforms LDA and is also better than the Bigram Topic Model in terms of perplexity. We also show that ou r model achieves comparable results with other models in terms of stability, coherence, and accuracy. Besides, the results from our topic model have less ambiguity, given the fact the generated terms include pairs of words that are more descriptive than single words.
Moreover, we introduced a method to enf orce topic similarity to conceptually similar words. As a result, this algorithm led to more coherent topic distribution over words.

In the future, we will extend our topic model to effectively capture more dependencies between words in sentence or document levels. In addition, we will investigate the influence of the order of words in the collapsed typed dependency relations.
 Acknowledgement. This project is funded in part by the Center for Infor-mation Visualization and Data Driven Design (CIV/DDD) established by the Ontario research fund.

