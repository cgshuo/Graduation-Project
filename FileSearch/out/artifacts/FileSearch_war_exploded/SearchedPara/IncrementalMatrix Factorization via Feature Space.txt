 Matrix factorization is widely used in Recommender Sys-tems. Although existing popular incremental matrix factor-ization methods are effectively in reducing time complexity, they simply assume that the similarity between items or users is invariant. For instance, they keep the item feature matrix unchanged and just update the user matrix without re-training the entire model. However, with the new users growing continuously, the fitting error would be accumulat-ed since the extra distribution information of items has not been utilized. In this paper, we present an alternative and reasonable approach, with a relaxed assumption that the similarity between items (users) is relatively stable after up-dating. Concretely, utilizing the prediction error of the new data as the auxiliary features, our method updates both fea-ture matrices simultaneously, and thus users X  preference can be better modeled than merely adjusting one corresponded feature matrix. Besides, our method maintains the feature dimension in a smaller size through taking advantage of ma-trix sketching. Experimental results show that our proposal outperforms the existing incremental matrix factorization methods.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Information Filtering Recommender Systems, Matrix Factorization, Sketching
Recommender Systems have become ubiquitous in our lives, helping us filter out the huge amount of useless in-formation by preference learning. The most widely used recommendation technique is matrix factorization which fac-torizes the ratings matrix into two smaller matrices. Tradi-tional matrix factorization methods have been proved to be c  X  effective, but most of them are designed for stationary data. However, in real-world online applications, users and item-s grow continuously, which can rarely meet the stationary condition. One strategy to tackle this rapid data expansion is to re-train the model if the data growth exceeds a pre-defined threshold. But the key challenge lies in that the factorization process is very time-consuming. Therefore, it is crucial and challenging on addressing the problem towards high complexity of factorization model and the rapid expan-sion of data.

A feasible way to deal with such situation is the use of incremental model updating. Recent works [6][7][8] perform incremental matrix factorization by adopting locally updat-ing strategies which only modify a small fraction of feature matrix at a time. For instance, sarwar [7] proposed a novel incremental SVD algorithm to locally update the user fea-ture matrix while keep the item feature matrix invariant. Rendel [6] also put forward an online regularized kernel ma-trix factorization(RKMF) method through merely updating one of feature matrices. These locally updating strategies are based on an underlying assumption that the similari-ty between items (users) is invariant to the new-user(new-item) problem. For instance, as for the new-user problem, supposed that the category of items is invariant, and the amount of coming new users in one round is much small-er than the existing users, then these strategies reduce the time cost through keeping the item feature matrix invariant. In other words, they omit the new ratings X  influence on the item features so as to speed up the calculation. However, with the new users growing continuously, the fitting error would be accumulated since the extra distribution informa-tion of items has not been utilized. Therefore, the item feature matrix should also be modified in each round. Here, we raise a relaxed and reasonable assumption which consid-ers the similarity between items (users) is relatively stable after updating. The projection of items in feature space will be modified, since these new users in one round may bring with some extra information of items. But the similarity between items still remains relatively stable.

In this paper, we propose an incremental matrix factor-ization method via feature space re-learning strategy(IMF-FSR). The feature space is re-learnt via auxiliary feature learning and matrix sketching strategies. Concretely, 1) we exploit matrix extension to induce an auxiliary feature learning strategy for better modeling the new preferences, in which the residual error of new data is normalized and added to the existing factorized matrices as auxiliary fea-tures. 2) We try to address the increasing feature size issue by modifying the item feature matrix(user feature matrix) so as to maintain a sketch with a smaller feature size.
Given a rating matrix R  X  R m n , the goal of regularized matrix factorization(RMF) is to construct a low rank de-composition of it. Let f denote the dimension of the feature space, P  X  R m f and Q  X  R f n denote the user as well as item feature matrix respectively. And the approximation of the rating by user u on item i can be confined to  X  r ui = p where p u and q i stand for the u th row of P and i th column of Q . Studies show that incorporating bias can further im-prove the prediction performance. Based on[3], the modified equation is expressed as: where b avg ; b u ; b i stand for the average rating bias, the user average bias and the item average bias respectively. stands for the regularized factor.
Suppose the online Recommender System has an initial dataset R 0 , which consists of a group of users U 0 and items I , then in each round there will be a series of new users signing in and giving ratings. We denote the set of new users as U new and the rating set as R new . Symmetrically, in each round there will be a series of new items being added to the system. We denote the set of new items as I new . The task is to predict the missing ratings for these new data.
In this paper, we propose a novel incremental matrix fac-torization to address the above problem, considering tak-ing advantage of the coming new data as auxiliary informa-tion to partly update another invariant factorized matrix Q . Without loss of generality, we describe the detailed method in terms of the new-user problem. While everything can be applied to the new-item problem as well since the RMF models are symmetric. Our idea is demonstrated in Fig-ure 1, which consists of three stages: 1) auxiliary feature learning, 2) matrix sketching, 3) updating decision strategy. In real-world online applications, data arrives in a stream. Re-training the entire model in each round is impractical s-ince the factorization process is very time-consuming. A feasible way to deal with such situation is the use of incre-mental model updating. In this paper, we exploit online matrix extension to induce an auxiliary feature strategy for faster modeling the new preferences. Similar to the method of online matrix completion for streaming data[1], the resid-ual error of new data is normalized and added to the exist-ing factorized matrices as auxiliary features. A particular expression is given in lemma 2.1.

Lemma 2.1. As for new-user problem, where R new  X  R c n , let R 1 be R new appended into the initial matrix R 0 . We can obtain P 1 ; Q 1 such that R 1 = P 1 Q 1 without computing the full factorization. Let w = R new Q 0 y and err = R new  X  be the least-squares weights and associated residual. The ma-trix R 1 can be factorized as where Q 0 y denotes the pseudo-inverse of Q 0 , || err || norm of err . Different from those locally updating strategies in related work, this step not only adjusts the user feature matrix P , but also makes an extension of item feature ma-trix Q . The residual error contains a distribution of new data that can not be well modeled by existing factorization. In a consequence, this auxiliary strategy can efficiently fit the new users X  preferences. It is worth pointing out two is-sues: 1) The feature size gradually increases along with the coming new data . 2) The feature size extension step may also introduce some over-fitting as well. Thus, designing a feasible method to address these issues is essential.
Due to the sparseness of data, the feature dimension f of matrix factorization in Recommender Systems is often set to much smaller than that of data. But the auxiliary feature learning brings with a side-effect that the feature di-mension continuously increases, resulting in a worse perfor-mance. Therefore, we try to address this issue via modifying the item feature matrix and maintaining a sketch with a s-maller feature size. A strict definition is given in Theorem 2.2.
 Theorem 2.2. Low Rank Matrix Sketching: Suppose A  X  R m n is a large row-wise matrix. It maintains a sketch B  X  R l n containing only l  X  m rows but still guarantees that A T A  X  B T B . More accurately,
In this paper, we make use of the low rank matrix sketch-ing algorithms[5][4] to compress the factorized feature ma-trix Q 1 into a smaller feature size. Our foundations are as follows: In accordance with our proposed comprehensive view of space projection, Q can be viewed as the coefficient A lgorithm 1 IMF-FSR for New-User Problem 4: end for 5: repeat 6: random select u  X  X  U k | 8: until convergence 14: if Y &gt; L k then 17: end if ma trix, in which each column represents an item X  X  coordi-nate, consisting of a series of coefficients corresponding to the base vectors of P . Q 1  X  R ( f + c ) n is viewed as the large row-wise matrix. We try to maintain a sketch  X  Q 1  X  R f n that retains most of the information. Here, we have to guar-antee that Q 1 T Q 1  X   X  Q T 1  X  Q 1 . Based on equation 3, the more accurately formula can be expressed as: where Q 1 T Q 1  X  R n n can be viewed as a similarity metric matrix of item-item pairs, while the compressed matrix  X  Q still maintains the similarity metric of Q 1 , which is in line with the assumption that the similarity of items in one round on new-user problem is relatively stable. We then re-learn the rest features through the function RM F .
The residual error err in each round may contain not only the model updating bias err m = r old ij  X  r new ij , but also the real predictive bias err p = r ij  X  r opt ij , where r opt the rating prediction by the best modified model. Thus, we should consider whether it is worth of updating the model. Based on model selection in Figure 1(3), the updating model should fall into region A . An optional strategy is to make constraints on the updating degree. We put forward two different strategies as follows: where L k is a variable determined by the number of new users in each round, representing the upper bound limit. The former makes constraints on the prediction difference between the new and old model, while the latter focuses on the difference of prediction error. We reject updating if the model does not satisfy the above constraint condition. Here, we simply select the difference of prediction error as our updating decision strategy.

We apply the above scheme to our incremental matrix factorization. Alg.1 describes the k th updating.
The total time complexity of our method in one updat-ing period consists of several parts and the primary cost is summarized as follows: 1) The matrix sketching algorith-m has time complexity near O( | T m | ( f + c ) nf ) where is the total number of iteration in sketching, f is the fea-ture size, c is the number of new users per round, n is the sum of items. 2) The incremental matrix factorization without the matrix sketching algorithm has time complex-ity near O( | T || D new | f 2 ) where | T | is the total iteration of SGD algorithm, | D new | is the number of new ratings. S-ince | T m | ( f + c ) n  X  X  T || D new | f , the final time complexity is near O( | T || D new | f 2 ). Since the number of new ratings |
D new | is much smaller than the total ratings size, the pro-posed method is efficient. Two real-world datasets used here are MovieLens 1 and Douban 2 . MovieLens is widely used in related work. Douban is real-world dataset crawled from the publicly available website Douban. For experiments, we obtain 1,000,209 ob-servations from 6040 users and 3900 items in MovieLens1M, 2,152,962 observations from 6000 users and 14648 items in Douban.
We set the experiments as follows: The experiments are started with a fixed ratings size of R 0 and the initial fac-torized matrices were given. After each round, a set of new users signing in (new items added) and ratings are given. Here, for simplicity, we just use a fixed number of new data. e.g. U new or I new = 20 per round. While for each new user(item) i in U new ( I new ), we randomly selec-t D i new  X  [20% ; 80%] observed ratings as the train set so as to model the variety of signing new data. Then we up-date the models and evaluate the new-user(new-item) prob-lem based on a standard metric, namely Root Mean Square Error( RM SE ). All the experiments were run on machines with the same hardwares (an Intel Core i7 CPU and 16GB RAM, WIN7 64 OS).
SVD++(Fully and Partly) [3]: A well known matrix factorization model proposed by Koren et al. Here,  X  X ul-ly X  and  X  X artly X  represent re-training the model entirely or partly.
 Online-updating RKMF(Non-negative with linear k-ernel) [6]: Non-negative with linear kernel achieves the highest performance in the experiments.
 Incremental Non-negative Matrix Factorization [2]: An incremental learning algorithm for matrix factorization. IMF-FSR : IMF-FSR(w) and IMF-FSR denote our method without and with the updating strategy respectively. Table 1 shows the quality of evaluation metrics on the MovieLens as well as Douban dataset for both the new-user and new-item problem respectively. For each problem, we h ttp://grouplens.org/datasets/movielens/ http://www.douban.com running time in average round.
 randomly sample the train and test set and implement the same experiments 4 times. Based on cross validation, we set = 0 : 05, the learning rate = 0 : 005, total iterations T = 400 for MovieLens. We set = 0 : 04, = 0 : 002, T = 100 for Douban. We set the feature size f = 20, the update limit L
Our proposed methods, which take advantage of the ma-trix sketching algorithm, are superior to those incremental baselines. This may contribute to our methods effectively demonstrating the assumption. It is also interesting to find that our methods even outperform SVD++(f) on new-user problem. This may be related to the facts that 1) if we re-gard the auxiliary features as a prior information, then the re-learning step can take advantage of this prior potential-ly. 2) Matrix sketching algorithm compresses the feature dimension into a smaller size while still maintains a stable similarity.

We also learn that the incremental updates are much faster than the full re-train model in each round. Our methods are proved to have the same time complexity as those incremen-tal baselines. Although the time cost is slightly lower than Non-neg, the RM SE performance greatly outperforms the latter. We omit the new-item running time.
Updating influence on old data: It is shown in Figure 2 (a) that along with the dataset increases, the RM SE of old data by our methods falls down quickly, even better than that by SVD++(f). Besides, it is also worth pointing out that the IMF-FSR is superior to IMF-FSR(w) since the former has the capability of preventing from over-fitting.
Observed ratings analysis: Figure 2 (b) shows the relationship between the RM SE metric and the observed ratings of new users. We draw a conclusion that RM SE decreases along with the amount of observed data increases and our methods still work well when comes to few observed ratings.
We propose an incremental matrix factorization algorith-m, which updates both the feature matrices based on a set of new users (items). Empirical results on different datasets show that our approach gains satisfactory consequences.
This work was supported in part by National Natural Sci-ence Foundation of China (Grant No. 61170127).
