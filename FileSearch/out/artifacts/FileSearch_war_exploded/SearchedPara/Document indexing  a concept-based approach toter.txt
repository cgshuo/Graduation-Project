 1. Introduction
The growth of the Internet has seen an explosion in the amount of information available, leading to the need for increasingly efficient methods for information retrieval. To intelligently retrieve information, indexing should be based not only on the occurrences of terms in a document, but also on the content of the document. Despite this obvious need, most existing indexing and the weighting algorithms analyze term occurrences and do not attempt to resolve the meaning of the text. As a result, existing indexing methods do not comprehend the topics referred to in a text, and therefore have dificulty in extracting semantically important indexes.

To address this shortcoming, in the present study we developed a novel indexing method that regards a document as a conglomeration of concepts. In the proposed methodology, indexes are extracted from a document based on these concepts and then weighted according to their degree of semantic importance tic importance degree of each concept and lexical item within a document.

This paper is organized as follows. Section 2 describes related work on indexing. In Section 3, we present our concept-based indexing methodology, and in Section 4 we present the results of, and compare, exper-iments using the proposed methodology and a traditional indexing scheme. Our conclusions are given in
Section 5. 2. Related work most such functions developed to date depend on statistical methods or on the document s term distribution tendency. Representative weighting functions include such factors as term frequency (TF), inverse document frequency (IDF), the product of TF and IDF, and length normalization (LN).

Most indexing and weighting functions based on statistical methods suffer from limitations that dimin-ish the precision of the extracted indexes ( Moens, 2000 ). TF is useful when indexing long documents, but not short ones. However, TF algorithms do not generally represent the exact TF because they do not take into account characteristics such as anaphoras, synonyms, and so on. In addition, IDF is inappro-priate for indexing a reference collection that changes frequently because the weight of each index term needs be recomputed every time the documents change. LN was proposed to account for the fact that TF factors are numerous for long documents but negligible for short ones, obscuring the real importance of terms. However, as this approach uses the TF function, it suffers from the same shortcomings as TF does.

A further drawback of most TF-based methods is that they have diffculties in extracting semantically the important terms that could be topics of the text are yoga , exercise , health and mind etc.
However, the TF weight of the word yoga is 1, which is the same as that of semantically unimportant words such as nurture and development . Thus, the TF approach fails to capture the topics of the text and cannot discriminate the degree of semantic importance of each lexical item within the text. Various at-tempts have been made to enhance the indexing performance by exploiting linguistic phenomena ( Kazman, the sample text shown in Fig. 1 , we obtain the nine chains taking a term that has no relation with other by a hyponym/hypernym relation. This approach correctly indicates that the important words of the text are yoga , exericse , health , mind and body other than nurture and development .

It is generally agreed that lexical chains represent the discourse structure of a document and provide such as Roget s, for specifying whether a cohesive relation exists between words. Two words can be con-sidered related if they are connected in the thesaurus in one of the following five ways: 1. Their index entries point to the same thesaurus category or to adjacent categories. 2. The index entry of one word contains the other. 3. The index entry of one word points to a thesaurus category that contains the other. 4. The index entry of one word points to a thesaurus category that in turn contains a pointer to a category pointed to by the index entry of the other. 5. The index entries of each word point to thesaurus categories that in turn contain a pointer to the same category.

Morris and Hirst constructed and evaluated the lexical chains by hand using five texts. However, they were never able to implement an automated version of their algorithm because on-line thesauri were not available to them.
 In an attempt to transfer Morris and Hirst s lexical chain algorithm to the online lexical knowledge base, the structure of WordNet is quite different from that of Roget s Thesaurus, they needed to replace the
Roget s-based definition of semantic relatedness used by Morris and Hirst with one based on WordNet, while retaining the algorithm s essential properties. They defined three kinds of relation: extra-strong, strong and medium-strong. An extra-strong relation holds only between a word and its literal repetition; such relations have the highest weight. A strong relation has a lower weight than an extra-strong relation but a higher weight than a medium-strong relation; there are three kinds of strong relations. Finally, they postulated that two words are related in a medium-strong fashion if there exists an allowable path connect-and conforms to one of the eight patterns described by Hirst and St-Onge.
 Carthy had research into the use lexical chains to build effective topic tracking systems ( Carthy, 2002 ). to the news stream that discuss the given event. We could see that LexTrack outperforms the keyword-based system, KeyCos, that they implemented, in terms of recall performance. KeyCos was based on tra-incoming story.

Barzilay and Elhadad investigate the use of lexical chains as a model of the source text for the purpose of producing a summary ( Barzilay &amp; Elhadad, 1997 ). They presented the new algorithm to compute lexical chains in a text, merging several robust knowledge sources: WordNet, a part-of-speech tagger, shallow par-ser and a segmentation algorithm. They used the three kinds of relations that Morris and Hirst defined.
Al-Halimi and Kazman ( Kazman et al., 1996 ; Kominek &amp; Kazman, 1997 ) developed a method for indexing transcriptions of conference meetings by topic using lexical trees, the two-dimensional version of lexical chains. They conducted a preliminary study to verify the utility of lexical trees for automatic indexing of arbitrary text. However, although their method demonstrated the potential usefulness of lexical trees in text indexing and retrieval, in its present form their method is inappropriate for use in document retrieval. For an indexing method to be of use in information retrieval, each index term for the document should be a topic and have a weight that represents the degree of semantic importance of the term within the document. However, although the method of Al-Halimi and Kazman can extract topics as index terms from the transcript of a conference seminar, it does not contain a function to estimate the weight of each extracted topic.

Therefore, in the present study, we propose a new, conceptual approach based on lexical chains for extracting terms from a text and assigning them weighting that can capture the semantic content of a doc-ument and represent the importance of a word within a document considering concepts. 3. Concept-based indexing
To address the semantic issues of TF-based indexing methods, we propose an approach that considers not just the terms but also the concepts of a document. In this approach, the concepts of a document are extracted, and, from those concepts, the semantic indexes and their weights are derived. 3.1. System overview
A schematic overview of the proposed methodology is shown in Fig. 2 . When applied to a input docu-ment, the proposed method first clusters semantically related terms that can represent the semantic content
Then, each scored representative concept is represented as a vector in concept vector space, and the impor-tance of the terms within a document is then computed according to the overall text vector and the concept vector in which the terms are included. Finally, semantic indexes and their weights are extracted.
The proposed system has three main components:  X  Clustering based on lexical chains.  X  Weight estimation based on word relations.  X  Weight re-estimation based on concept vector space.

The clustering component employs lexical chains, and the latter two components are related to the index term weighting based on the term relations and the concept vector space. 3.2. Concept clusters
Documents generally contain various concepts, and we must determine those concepts if we are to com-lexical chains provide a good representation of discourse structures and topicality of segments ( Morris, of the meaning of a document.

Morris and Hirst were able to use various kinds of syntactic categories (except pronouns, articles, and high-frequency words) when composing lexical chains, because they used Roget s Thesaurus as a knowl-edge base ( Morris, 1988 ; Morris &amp; Hirst, 1991 ). However, in the present work we use WordNet, which has far fewer cross-category connections compared to Roget s Thesaurus. Because of WordNet s limited cross-category connections, we followed the approach of previous investigators and limited our investiga-tions to nouns ( Budanitsky, 1999 ).

Hirst and St-Onge adapted the Roget s-based relations of Morris and Hirst to WordNet-based ones, more than five links and conforms to one of the eight patterns described in Hirst and St-Onge. Conse-quently, numerous kinds of relations can be used in composing lexical chains. The large number of possible word relations means that, if the proposed method is used to index a massive number of documents, there would be a large number of parameters and hence the indexing and retrieval computations would take a long time. Therefore, in the present work on the clustering of lexical items, we considered only four kinds of relations X  X  X dentity, synonymy, hypernymy (hyponymy), and meronymy. Hypernymy and hyponymy are regarded as one relation because they are inter-definable.

The use of traditional lexical chains for indexing is based on the notion that such sequences of lexical items contain clues about the discourse structure or topicality of a document. However, traditional lexical within a document, and no explicit function has yet been developed to measure the relative semantic impor-tance within the lexical chain method. The notion of a concept cluster proposed here not only groups re-lated lexical items, but also assigns each lexical item and concept a weight that represents its semantic importance degree within a document. Therefore, we define a concept cluster as a weighted lexical chain that represents one aspect of the meaning of a document and expresses the semantic importance degree within a document using the following definition.

Definition 1 (Concept cluster). Let N ={ N 1 , N 2 , ... , N R = {identity, synonym, hypernym(hyponym), meronym} be the set of lexical relations. Let
C ={ C 1 , C 2 , ... , C m } be the set of concept clusters in a document. Concept cluster C and R k , where R k 2 R , N i 2 N and C j 2 C . Each N i and C degrees of semantic importance within a document.
 As an example of this approach, we apply our clustering approach to the sample text considered above resentative clusters than the other clusters because those clusters consist of a larger number of terms and lexical relations that are exploited to search for the semantically important terms in a document.
The way to construct a concept cluster from a given document is described in the following section in more detail. 3.3. Concept cluster identification 3.3.1. Clustering based on lexical chains
Clustering is achieved by four inter-noun relations: identity, synonymy, hypernym(hyponym), mero-nym(holonym). If we were to consider more relations (e.g., transitive, antonym, etc.), we would need to manage more parameters when calculating the word weights, which would degrade the information retrie-the present work to show that the proposed approach enhances IR performance.

Clustering information is obtained from WordNet. Because the verb file has no relation with the three research. By the same reason, in this paper, we take only nouns of clustering candidates. Furthermore, this result in stopwords elimination as side effect.

The procedural steps for clustering algorithms are simply described in Algorithm 1: Firstly, a document is tagged, and the chaining candidates are selected. Then, they are loaded into the clustering candidate array ber of nouns, the system stops the chaining process, otherwise it continues the chaining procedure. 3.3.2. Weight estimation based on term relations
The natural networks such as the World Wide Web have been found that they have a hub structure that
WWW is defined as a page that points to many nodes. Characteristic patterns of hubs and authorities can the Web s structure is leading to improved methods for accessing and understanding the available informa-tion. Steyvers and Tenenbaum presented the graph-theoretic analysis of three types of semantic networks, word association, WordNet and Reget s thesaurus. They showed that these semantic networks also have a small world structure similar to that found in WWW ( Steyvers &amp; Tenenbaum, submitted for publication ).
These researches on semantic networks and natural networks focuses on the analysis of the network char-acteristics, and use the link information as an important factor to find out the network structure.
Because the node that have many links with other nodes, such as a hub, play an important role in iden-tifying the topical structure of a network, the node that have many links with other nodes in a network may be regarded as more important node than ones that have little links with other nodes. From this point of view, we deal the link information of a concept cluster as means to measure the importance degree of a word within a document.

To estimate the semantic importance of terms within a given document, we think that the words having more relations with other words are semantically more important in a document. Therefore, based on word tions 2 and 3.

Definition 2 (Score of noun). Let NR k W represents the weight of relation k . Then the score S NOUN as: Algorithm 1 (Clustering algorithm) Input: Input document file Output: A number of weighted clusters
Procedure: 1. Tag a document; 2. Select chaining candidates, C w ={ W 1 , W 2 , ... , W 3. Initialize the current candidate pointer i 4. while ( i &lt; n ) do 5. for ( j = i +1; j &lt; n ; j ++) do 6. If HasRelation( W i , W j ) defined in WordNet where W 7. end for 8. Increase the current candidate pointer i 9. end while S in the order listed: identity, synonym, hypernym(hyponym), meronym (i.e., identity highest and meronym lowest) ( Fellbaum et al., 1998 ).
 Based on Definition 2, we now define the scoring function of a concept cluster.

Definition 3 (Score of concept cluster). The score S CONCEPT where S NOUN ( W i ) is the score of noun W i , and W 1 , ... , W
Thus, S CONCEPT ( C x ) is obtained by summing the scores of all the terms in C CEPT (C x ) indicates that C x is a semantically important concept in the document.

For example, consider the system in Fig. 4 , in which the identity relation weight, SR and the synonym relation weight, SR syn , is set to 0.5. Given that noun W calculation:
From the weighted concept clusters, we discriminate representative concepts to represent the content of the document, since we cannot deal with all the concepts of a document. For example, in the system shown rion 4.

Definition 4 (Criterion for a representative concept). Concept clusters, C criterion are considered representative concepts:
After extracting the representative concepts of a document, we re-estimate the semantic importance of the terms according to the importance of a concept in which the terms are included, which leads to the cap-ture of the semantic index terms and their weights for the document. 3.3.3. Weight re-estimation based on concept vector space
In this section, we describe the method for estimating the semantic importance degree of each lexical item in a document based on the concept vector space. We can discern which concepts are important and which are not using Definitions 2 X 4 in Section 3.3.2. However, we need to re-estimate the fine-tuned weight of a term considering the concept in which the term is included.

For example, Fig. 5 shows the concept clusters of the sample text shown below. Four representative con-cept clusters are composed of 12 words and weighted using Definitions 2 X 4 in Section 3.3.2. The words is more than the weight of C3, which contains director .

This indicates that, within this document, practice belongs to a more important concept than does direc-more important cluster in the document. concept cluster in which it resides. When two lexical items are assigned the same score, the lexical item whose concept cluster has a higher score is deemed the semantically more important of the two items.
We require a measure that recomputes the weight of lexical items taking into consideration the concept cluster importance. To achieve this, a vector model is employed for the concept space model. In this model, the document is represented by a complex of concepts and the sematic importance degree of lexical items are discriminated by the vector space property.

Definition 5 (Concept vector space model). Concept space is an n -dimensional space composed of n independent concept axes. Each concept vector, ~ C i , represents one concept, and has a magnitude of j concept space, a document vector, ~ T is represented by the sum of n -dimensional concept vectors,
Thus, the overall text concept vector is determined by the magnitude of each concept vector. For example,
From the composed six concept clusters, we discriminate cluster C 3and C4 as the representative concepts device , and anesthetic within the overall text vector.

The semantic importance of each concept and lexical item can be derived from the properties of this con-cept vector space. Fig. 8 depicts the process by which the semantic importance of each concept is derived from the overall text vector. The text vector, ~ T , is derived from concept vectors tudes of vectors ~ C 1 and ~ C 2 are j ~ C 1 j and j ~ C 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi j ~ C 1 j 2  X j ~ C 2 j 2 q and the part that concept ~ C 2 contributes is y . Each concept is composed of words and has a weight w
By generalizing this vector space property, the weights of lexical items and concepts can be estimated as follows.
 the overall document weight. The text weight quantity ( X quantity ( X W ) are defined as follows:
The text weight quantity, X T , is the combined magnitude of all concepts that form a text. The concept weight quantity, X C same method as that used to derive x and y in Fig. 8 . The word weight quantity, X the product of the overall text weight and the ratio of word weight to text weight. W to the weight quantity of a text, concept or word. W C j T The formulas for calculating the weights of words and concepts are given above in Definitions 2 and 3. W concept to the text. The weight ratio of a word to the overall text is denoted by W Fig. 9 shows the process followed to derive the word weight quantity ( X other of two words. When two concept clusters are constructed from a document, the weights that we can derive directly from the constructed concept clusters are the weight of each concept ( S text vector magnitude ( X T , j ~ T j ) and each concept weight portion ( X of the weight of each word ( X W ) and the word weight ratio ( W derived from the weights such as S NOUN ( W i ), S CONCEPT
After computing the numerical values of the weight quantity and weight ratio of each lexical item within an overall text, we extract nouns satisfying the following definition as semantic indexes.
Definition 8 (Semantic index). The semantic index that represents the content of a document is defined as follows:
Although the weight quantity of a word is the same in documents, the relative importance of each word in a document differs according to the document weight quantity. Therefore, we view the weight ratio rather than the weight quantity as the semantic weight of indexes to a document.
 Definition 9 (Weight of a semantic index). The weight of a semantic index, denoted by SW follows: 4. Experimental results In this section, we conducted document retrieval experiments in which the proposed method and
TF  X  IDF method were applied to the TREC-2 collection of 1990 Wall Street Journal (WSJ) documents, which comprises 21,705 documents.
 The TF  X  IDF has shown its superior performance for document indexing ( Salton &amp; Buckley, 1988 ). Therefore, this weighting scheme is used as a standard for comparison with the proposed method. We used
IDF to both weighting scheme of TF and the proposed indexing for representing the importance degree of the term within a document collection, because the TF and semantic weight only represent the importance degree of a term within a document, and cannot express the importance degree of the term within a doc-ument collection.

As the measurement of the retrieval effectiveness, we used precision and recall as following equations is the documents that is relevant for a given query, in other words, answer documents for a given query.
For the weights of basic relations (identity, hypernym, etc.), no general guidelines exist except for the tion weight is the highest and the meronym relation weight is the lowest. In the present work, we followed the principle of WordNet when assigning the relation weights. The weights of the five relations used in the clustering of terms were set from 0.1 to 1.5 (identity highest and meronymy lowest).

These experiments were carried out on a Pentium IV 2.8GHz computer with 1GByte of RAM. For the relevance judgement of TREC-2 collection, 50 queries from query number 101 X 150 are supported. How-ever, we only used 40 queries because there are not 1990 WSJ documents for the other 10 queries and we have no answer set to compare the relevance for that 10 queries. The relevance degree between the query and the document was calculated using the vector model. Comparison of the retrieval results obtained in the two sets of experiments showed that the proposed weighting scheme outperforms the traditional weight-ing scheme. 4.1. Precision and recall
The average precision for 40 queries ranging from Top 1 to Top 20 are shown in Fig. 10 . The precision result shows that the proposed system outperforms the TF  X  IDF weighting method, especially in regard to the few highest-ranked documents. The average precision of TF  X  IDF for the 40 queries from Top 1 to Top 20 was 13.90% and that of the proposed method was 11.59%. Therefore, the proposed system (SW  X  IDF) increased the precision of the traditional TF  X  IDF method by as much as 2.31%.
 Table 1 shows the overall search results, average precision and recall for the Top 1, Top 5, Top 10, and
Top 20 documents. The search results show that the proposed system outperforms the TF  X  IDF weighting the precision of the proposed system for the top-ranked document is 12.5% higher than that of the TF  X  IDF system, and the precision of the proposed system for the top 5 documents is 4% higher. Moreover, shown in of total queries) for Top 5, whereas the proposed method does not give any relevant results for 21 queries (i.e., 52.5%).

When users search the Web for information, they tend to focus on the document with the highest rank-ing. Thus, the relevance of the highest-ranked document plays an important role in user satisfaction. More-over, many commercial search systems show only the top 10 or so relevant documents on the first result web page, and users tend not to look beyond this page. Therefore, the superior search performance of the proposed system compared to the traditional TF  X  IDF weighting scheme indicates that the proposed scheme should give enhanced user satisfaction in a commercial setting. 4.2. Dimension reduction of index terms
The index term dimension is the number of index terms that are used to represent a document. When a document is indexed based on the TF, all the terms in the document are used as indexes, and hence the index term dimension simply equals the number of words in the document. However, when we index a doc-ument using the proposed indexing scheme, we first extract representative concepts from the document and then extract index terms from those concepts. Hence, the index term dimension of the document will be less than that obtained using the TF approach. This is clearly demonstrated in the present experiments on the 1990 WSJ documents, for which the average index term dimension was 89.55 using the TF method but 17.8 using the proposed method. Thus, on average the TF method represents the 1990 WSJ documents using about 90 words as index terms whereas the proposed method requires only about 18 words, indicating that the proposed scheme reduces the index term dimension by about 80% compared to the TF method. When a document is searched, the retrieval system will already have loaded the index file to the memory. size of the indexing file ( Table 2 ).

For the 1990 WSJ documents, the index file size was 61.9Mbytes using the TF method and 12.9Mbytes using the proposed method. Loading the TF index file to the main memory required 11.719s whereas load-ing that of the proposed method required only 1.75s, which represents an 85.07% saving in loading time.
When we searched the documents for 40 queries using the TF index files, the average search time was 41.3s; in contrast, the proposed weighting scheme required on average only 12.4s per search to carry out the same searches. Thus, the search time using the proposed scheme is on average 70.09% less than that using the traditional TF method. Fig. 12 shows the search time required for Top 100 documents of each query using the two methods. In all cases, the proposed method is faster than the TF method. Therefore, the proposed method is expected to enable high-speed searches in the real search environment. 5. Concluding remarks
In this paper, we have presented a novel approach to document searching that uses a concept vector space model to extract and weight indexes. Experiments comparing the proposed approach with results semantic importance of words within an overall document. Other experiments in which the proposed ap-proach was compared with the traditional TF method highlighted the superior performance of the pro-posed scheme, especially in regard to top few documents ranked as most relevant. Importantly, the the index term dimension obtained using the proposed method was 80% less than that obtained using the tra-ditional TF method, which should significantly reduce the search time in a real environment. In sum, the proposed method overcomes some of the limitations of existing stochastic indexing methods and should prove useful in a commercial setting.
 References
