 In this paper we address the task of automatically finding an expert within the organization, known as the expert search problem. We present the theoretically-based probabilistic algorithm which models retrieved documents as mixtures of expert candidate language models. Experiments show that our approach outperforms existing theoretically sound solutions.
 H.3 [ Information Storage and Retrieval ]: ;; H.3.3 [ In-formation Search and Retrieval ]: ; Algorithms, Theory, Performance, Experimentation expert finding, expertise, enterprise search, e-mail
Expert finding is a new rapidly evolving direction of In-formation Retrieval research [4]. An expert search system finds persons with certain expertise within an organization. It uses a short user query and the information stored on per-sonal desktops or within centralized databases as an input. There are mainly two approaches to do expert candidates modeling and ranking. The first approach is profile-centric . All documents related to a candidate expert are merged into a single personal profile. The personal profiles are ranked as in standard document retrieval and corresponding best candidates are returned to the user. The second approach is document-centric . It runs a query against all documents and ranks candidates by summarized scores of associated documents. Our generative modeling method combines the features of the both approaches: it ranks candidates using their language models built from retrieved documents.
The most popular and effective assumption in expert find-ing research states that the level of personal expertise can be determined by the analysis of co-occurrence of the query terms and personal id within the context of a document or a passage [1]. We similarly suppose that our task comes to the estimation of the joint probability P ( e, q 1 , ..., q observing the candidate expert e together with the query terms q 1 ...q k in the documents ranked by the query. In this paper we examine two estimation methods.

Method 1 considers a candidate and the query terms to be conditionally independent given a ranked document (see Figure 1a). Thus, the total joint probability is,
This method is analogous to the most successful and the-oretically sound approach proposed so far [1]. Thus, it serves as a baseline in our experiments. Method 2, which is the contribution of our paper, is based on the assump-tion of dependency between the query terms and a can-didate. We suppose that candidates actually generate the query terms within retrieved documents (see Figure 1b). We calculate the required joint probability as follows consider-ing the query terms to be sampled independently given an expert candidate: We set the candidate prior P ( e )tobe:
So, now we need to estimate the probabilities P ( q i Since, we have already postulated that candidates are re-sponsible for generating query terms in the documents they are mentioned in, we represent the language model of a ranked document as a mixture of expert candidate language Figure 1: Dependence networks for two methods of estimating the joint probability P ( e, q 1 , ..., q k ) models and the global language model. We define the like-lihood of the top retrieved documents, set R ,as:
Here, e 1 , ..., e m are the experts, c ( w, D ) is the count of term w in document D ,  X  G is the probability that a term will be generated from the global language model and not from any of candidate language models. P ( w | G ) is the global lan-guage model estimated over the whole document collection. Further, we apply the EM algorithm [3], traditionally used to estimate unknown parameters, to calculate P ( w | e i ). We propose the following updating formulas to be used recur-sively to maximize the likelihood of set R : E-step: M-step:
The probabilities P ( e | D ) are calculated using association scores a ( e, D ) between the document and expert candidates: P ( e | D )= a ( e, D ) / m i =1 a ( e i ,D ). The probability distribu-tion P ( D ) is considered to be uniform in the both methods.
Our approach to expert candidates modeling is based on the similar hypothesis with one used in model-based pseudo-relevance feedback methods for document retrieval [5]. It considers that the relevance model of a user can be mined from the top of retrieved documents. The significant differ-ence is that we represent the relevance model, which is in fact the model of the query topic, as a mixture of models of expert candidates who actually hold and share the desired knowledge.
For the evaluation of our approach we utilize data from the expert search task in the Enterprise track, TREC 2006. This track contains 1092 expert candidates and 50 queries with respective lists of experts. We use only the email part of the collection since it allows us to extract candidate-document associations easily, using from , to and cc email fields and given the candidate X  X  email addresses. Association scores are taken to be 1.5, 1.0 and 2.5 respectively what is the best combination according to recent studies [2]. The number of retrieved documents for modeling is restricted to 1000 what is the standard for document retrieval tasks in TREC. The standard language model based IR approach is used for the retrieval of documents. Table 1 contains the results of both expert candidates ranking methods.
 Table 1: Performance of expert ranking methods
We see that our Method 2 improves the baseline Method 1 over all standard IR measures including mean average preci-sion, mean reciprocal rank, R-Precision and precisions for 5, 10 and 20 top expert candidates. It shows that the assump-tion of independence of terms and candidates (see Figure 1a) associated with a document is less realistic. It seems important for expert ranking methods to model candidates, queries and documents considering that occurrence of the specific candidate in a document determines which terms the document consists of (see Figure 1b).
We presented a new generative model-based method for expert finding and evaluated it using the TREC Enterprise collection. The result suggests that it is more effective to drop the assumption of independence between candidates and document terms, while this claim should be carefully studied with additional experiments. We see the potential of the presented method for the understanding of expertise distribution in enterprise. In the future, we plan to take a closer look at pseudo-relevance techniques, namely query expansion, since our approach inherits much of this family of approaches. We thank Sergey Chernov, Gianluca Demartini and Julien Gaugaz from L3S Lab Hannover for the help with data pre-processing and series of fruitful discussions. [1] K. Balog, L. Azzopardi, and M. de Rijke. Formal [2] K. Balog and M. de Rijke. Finding experts and their [3] A. Dempster, N.M.Laird, and D.B.Rubin. Maximum [4] D. Hawking. Challenges in enterprise search. In ADC [5] C. Zhai and J. Lafferty. Model-based feedback in the
