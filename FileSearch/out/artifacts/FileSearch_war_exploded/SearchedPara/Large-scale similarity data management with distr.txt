 1. Introduction apply similarity ranking methods on a collection of images selected by a preceding keyword search. General public and would scale to these data volumes.
 throughput, and building costs on various hardware infrastructures.
 numeric domain. This approach enables to actually store the data in well-established structures such as the B  X  variants.
 performance in terms of response times and query throughput.
 various hardware infrastructures. Finally, Section 6 summarizes and concludes the text. 2. Related work approaches.
 by numerous experiments.
 allowing to utilize practically unlimited distributed resources.
 outperforms M-Chord search that was realized on the same dataset ( Novak et al., 2008 ).
There are several on-line demonstrations of large-scale techniques for content-based image-retrieval. ALIPR set of images according to automatically generated annotations. ImBrowse texture, shapes (and combinations) employing five independent engines. Furthermore, id X e system database of 2.8 million images according to image signatures and GazoPa limited throughput and scalability. efficiency outperforms standard LSH functions on Euclidean vector space ( Novak, Kyselak, &amp; Zezula, 2010 ). 3. Distributed M-Index
The centralized version of the M-Index ( Novak et al., in press ) utilizes the B an efficient and scalable similarity management system. 3.1. M-Index principles always be determined and the transformed space retains all properties of the original one. 3.2. Data-space mapping in M-Index
This schema uses a fixed set of n reference objects (pivots) { p
To describe the mapping schema precisely, we need one preliminary definition. For an object o 2 D , let ( ) tation of pivot indexes {0,1, ... , n 1} such that
In other words, sequence p  X  0  X  closest pivot p i  X  clusters C i are formed in this way (in other words, (0) cluster C i is partitioned into n 1 clusters by the same procedure using set of n 1 pivots { p clusters C i , j , where j is index of the second closest pivot to objects in cluster C is an integer 1 6 l 6 n .

The M-Index with l levels further defines a mapping of the data space to a numeric domain key gral part of key l ( o ), o 2 D results from a numbering schema of the clusters. Specifically, cluster C
The fractional part of key l ( o ) is the distance between the object o and its closest pivot d  X  p
Fig. 1 (right) sketches this M-Index mapping principle with the level l = 2 (only for clusters C tree-depth ( Novak et al., in press )  X  the tree has an a priori given maximal level 1 structure for l max = 3 is sketched in Fig. 2 .

The nodes of the cluster tree have the following structure: h l ;  X  i ( i cluster tree has always the following root node: h 0 ;  X  X  ;  X  ptr 1 3.3. Distributed index architecture ries are exploited by the M-Index search algorithms  X  B + kept up-to-date. This service can be replicated to improve the fault-tolerance. tions and queries.
 schema of the system. 3.4. Distributed M-Index with full cluster tree 3.4.1. Range algorithm tances d ( p i , q ), i =0, ... , n 1 and sorts them to find pivot permutation ( ) partitioning. According to double-pivot distance constraint ( Zezula et al., 2006 ), cluster C
Because the Voronoi partitioning is repeated l -times for cluster C cluster  X  once for each pivot p i ered for Voronoi partitioning on level l . Therefore, this pivot cannot be used as pivot p tified as the pivot with the smallest distance d ( p j , q ) that is not among p cluster C i 0 ; ... ; i l 1 contain distances from pivot p domain to be searched within that cluster (line 17): number of cluster C ). A set intervals of query-relevant key intervals is created by this mechanism. where r min and r min are the minimal and maximal object-pivot distances within cluster C (note that r fractional parts of C .minKey and C . maxKey, respectively).
 Algorithm 1. Distributed range search algorithm 1 A ; 2 for i 0to n 1 do 3 calculate d ( p i , q ) 4 sort the pivots to find p  X  0  X  5 intervals empty list of intervals; 6 Q empty queue; 7 Q .enqueue(root) 8 while : Q .empty do 9 //double-pivot distance constraint 10 j smallest j P 0 s.t. ( j ) q R { i 0 , ... , i l 2 }; 11 if l &gt;0 ^ d  X  p i 12 Continue; 13 if node is internal then 14 for i 0to n 1 do 15 Q .enqueue( dereference  X  ptr l  X  1 i  X  ) 16 else ( node is a leaf node ) 17 intervals .add( cluster  X  C i 0 ; ... ; i l 1  X  X  d  X  p 18 subanswers sendSearchRequest( intervals ); 19 foreach answer set A P in subanswers do A .addAll( A P Algorithm 2. Range search alg. at peer P Input search request for R ( q , r )at intervals
Output Set A P ={ o 2 X  X  key ( o ) 2 intervals \ P. interval ^ d ( q , o ) 1 A P ; ; 2 foreach I 2 intervals \ P . interval do 3 C P .getCluster(clusterNumber( I )); 4 if I .upperBound &lt; C .minKey _ I .lowerBound &gt; C .maxKey 5 Continue; 6 if C .hasMetricIndex() 7 A P .addAll( C .processQuery( R ( q , r ))) 8 else 9 data C .getDataForInterval( I ); 10 for each object o in data do 11 if max n 1 i  X  0 j d  X  p i ; q  X  d  X  p i ; o  X j &gt; r then 12 Next o ; 13 if d ( q , o ) 6 r then 14 A P .addObject( o ); 15 P .sendAnswer( A P , originator ); executed on this index and the result is added to set A P (lines 11 X 12).
 rithm 1 ). The presented search algorithm is also the base for other similarity queries. 3.4.2. Nearest-neighbors search algorithm two-phase strategy: 1. employ a heuristic to locate k data objects  X  X  X ear X  X  q and measure the distance . is an upper bound on the distance to the actual k th nearest-neighbor of q ; 2. execute the R ( q , . k ) query and return the k objects with the lowest distances from the query result. ( Doulkeridis, Vlachou, Kotidis, and Vazirgiannis, 2007 ).

During evaluation of the R ( q , . k ) query, only k nearest objects are always kept and the actual radius . processing. 3.4.3. Approximate kNN search ter X  X  interval is specified as cluster  X  C i 0 ; ... ; i l 1 to peers responsible for these c intervals midpoints.

The cluster-ordering heuristic ( Novak et al., in press ) is based purely on analysis of the d ( p ter C  X  0  X  q ; ... ;  X  l 1  X  q , in which object q would be stored in the M-Index (let us denote it C penalty equal to 0. Cluster C q has the smallest sum of distances d  X  p ters. This is reflected by the penalty in order to express the  X  X  X roximity X  X  of the cluster to q . Specifically:
Fig. 4 provides an example in which C q = C 0,3 and the query-pivots distances are d ( p radius . k for pivot filtering (lines 11 X 12).
 peers, in case the cluster spreads over more than one peer ( Novak et al., 2008 ). 3.5. Dynamic M-Index with distributed cluster tree to another local tree node or to a different peer that contains given part of the tree. each of which uses a different set of pivots and thus brings an orthogonal mapping of the dataset. 4. M-Index efficiency evaluation from Section 3.4 . A summary of the various symbols and M-Index parameters is provided in Fig. 5 . 4.1. Settings and measurements representation can be viewed as a 280-dimensional vector (occupying about 1 kB of memory) together with a complex various sizes from 1 million to full 100 million images.

Section 3.4.1 . These local indexes are dynamic with l max all local M-Indexes work with the same set of pivots).
 dataset. 4.2. Precise similarity queries 10-million CoPhIR dataset  X  we present results for three configurations of distributed M-Index: 1. l = 2 and n = 16 which results in maximally 16 15 = 240 clusters distributed among 50 peers, 2. l = 2 and n = 32 with a maximum of 991 clusters, 3. l = 3 and n = 8 with a maximum of 336 clusters.
 combinations we have tested; the results are summarized in Table 2 .
 Section 5 of this work.
 total costs seem to have slightly sublinear trends for growing dataset volume. which is indicated by almost constant parallel number of block reads. 4.3. Approximate Similarity Searching objects accessed on all peers visited during the approximate search.
 local M-Index processing. With local = 6000 or higher, the search can achieve even 99% recall. recall while accessing a tiny fraction of the database.
 umes of accessed data.
 objects in total ( c = 20 for 1-and 10-million structures and c = 10 for hundred-million network). 5. Developing a real similarity-search application ware design to a fully functional application with a Web interface.
 the application. 5.1. Image retrieval by M-Index database)  X  this searching paradigm is usually referred to as the query-by-example . combine several such features to get overall similarity measure. 5.1.1. Dataset defined for each descriptor  X  three descriptors have a weighted L structure over such data. 5.1.2. Queries sponse times low. 5.1.3. Hardware MB).
 speed of the storage (disks or main memory) and the number of CPUs. 5.1.4. Interface search (see Fig. 12 ).

In general, the search procedure is the following: from the previous search result, searched for using keywords, or given explicitly by uploading an image. skip this step. images in the database and their distances from the query image q . 4. Finally, a Web page is generated showing thumbnail images for the returned identifiers. ation are beyond the scope of this paper. 5.1.5. M-Index setup and n = 32. Each peer organizes 200,000 objects on average. 5.2. Building the index structure significantly by parallelization.
 Algorithm 3. Insert algorithm Input Object o 2 D
Output Insert confirmation 1 for i 0to n 1 do 2 calculate d ( p i , o ); 3 bind d ( p i , o ) with o for pivot filtering; 4 sort the pivots to find p  X  0  X  5 calculate key ( o ); 6 send o to peer P responsible for key ( o ); 7 store object o in local index of peer P ; 8 handle split if local storage of peer P overflows; 9 confirm object insertion; Algorithm 4. Bulk-insert algorithm (static lvl.) 1 for o 2 X do 2 for i 0to n 1 do 3 calculate d ( p i , o ); 4 bind d ( p i , o ) with o for pivot filtering; 5 sort the pivots to find p  X  0  X  6 calculate key l ( o ); 7 split X into clusters C i 0 ; ; i l 1 ; 8 Q all clusters sorted by cluster ( C ); 9 for i 1to# peers do 10 while # objects ( P i )&lt;(# objects ( X )/# peers ) do 11 assign dequeue ( Q ) to peer P i ; 12 build peer-to-peer network; ject o at any peer by executing the insert algorithm. The algorithm first evaluates the distances to pivots p split.

Observe that, given a static level l and a set of pivots p processing in the MapReduce fashion ( Dean et al., 2008 ). 5.2.1. Building costs infrastructure with 42 CPUs.
 Algorithm 4 ).
 peer which resulted in more peers.
 insert technique.
 5.3. Searching the index
Having the index prepared, we can measure the search performance of the distributed M-Index on different hardware mentally discovered in Section 4 . Specifically, we set local =6,000, c = 10, and k = 30. map the peer-to-peer network to a specific hardware and run a batch of 1000 queries in each experiment. 5.3.1. Hardware and measurements i.e. the number of queries answered per second. 5.3.2. Experiment results disk-based storage. We varied the number of CPUs on which the M-Index was mapped. tem started to become a bottleneck for experiments with more than 8 CPUs, and we can observe that increase of the throughput and response times are slowing.
 application and the throughput of nearly nine simultaneous queries per second is acceptable too. in 600 ms using 6 CPUs and in 300 ms on 18 CPU configuration.
 problem, mirroring the disks instead of using RAID5 can be applied. 6. Conclusions supports interval queries, such as the structured peer-to-peer networks P-Grid or Skip Graphs. orders of magnitude.
 the performance of the search system and thus customize the application for various environments. Acknowledgements This work has been supported by national research projects VF20102014004, GACR 201/09/0683, GACR 103/10/0886,
GACR P202/10/P220, and MSMT 1M0545. The hardware infrastructure was provided by the METACentrum under the re-search intent MSM6383917201. References
