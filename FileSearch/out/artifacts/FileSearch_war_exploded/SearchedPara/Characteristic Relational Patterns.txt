 Research in relational data mining has two major directions: find-ing global models of a relational database and the discovery of lo-cal relational patterns within a database. While relational patterns show how attribute values co-occur in detail, their huge numbers hamper their usage in data analysis. Global models, on the other hand, only provide a summary of how different tables and their at-tributes relate to each other, lacking detail of what is going on at the local level.

In this paper we introduce a new approach that combines the pos-itive properties of both directions: it provides a detailed description of the complete database using a small set of patterns. More in particular, we utilise a rich pattern language and show how a data-base can be encoded by such patterns. Then, based on the MDL-principle, the novel RDB-K RIMP algorithm selects the set of pat-terns that allows for the most succinct encoding of the database. This set, the code table, is a compact description of the database in terms of local relational patterns. We show that this resulting set is very small, both in terms of database size and in number of its local relational patterns: a reduction of up to 4 orders of magnitude is attained.
 H.2.4 [ Database Management ]: Systems X  Relational databases ; H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Experimentation Frequent Patterns, Relational Data Mining, MDL
Relational data mining seeks to generalise traditional, single-table data analysis to the analysis of multiple inter-related tables. As relational models are more expressive than their single table counterparts, they allow for a more succinct description of the data-base than when one has to summarize each and every table and all their connections separately. Current research in relational data mining follows one of two directions. Either one aims to find a  X  X lobal X  model of the complete database, or one seeks interesting  X  X ocal X  patterns in the database.

Both directions have their specific merits and shortcomings. A  X  X lobal X  database model, for example a Probabilistic Relational Model (PRM) [9], is often small and interpretable, but it lacks de-tail as it only shows how the attributes of the tables interact. On the other hand, one can use an approach like WARMR to find fre-quent patterns [1, 3]. These patterns can be regarded as  X  X ocal X  models that describe a partial structure of the relational database. A well-known drawback of this method is the exponential pattern set growth when mining for less frequent, but often interesting, pat-terns.

In this paper we present a new approach, RDB-K RIMP combines the strengths of both approaches. It finds a global model that describes the complete database using only a small set of char-acteristic relational patterns. While collectively the patterns show the global structure, individually they reveal the local interactions in the database. A global model that describes the complete rela-tional database must capture the behaviour of all tables and their interactions, and thus requires a focus on all tables. While frequent pattern mining techniques prune the pattern search space using a single  X  X arget X  table, all tables function as  X  X argets X  in our global ap-proach. Hence, our pattern language is essentially that of FARMER [13] except we do not restrict the patterns to have their  X  X oots X  in a single target table.

Given this pattern set, we use the Minimum Description Length (MDL) principle [7] to select a small set of characteristic patterns for the database. RDB-K RIMP uses the MDL-principle to find those patterns that together describe the database well. In contrast to K RIMP [14], this enriched pattern language allows RDB-K to find more complex patterns in the database. However, finding these richer models for relational databases requires a novel loss-less encoding scheme that relies on its ordering. During the en-coding, RDB-K RIMP reorders the tuples based on the patterns of the model such that the encoded database can always be decoded in a lossless fashion. RDB-K RIMP proves effective: with its loss-less encoding, it can find models that stay compact and utilise our enhanced pattern language.

After presenting our theoretical framework and the RDB-K RIMP algorithm, we validate our claims with experimental results ob-tained on publicly available KDDcup databases. We show that the global models stay compact, in contrast to the original candidate pattern sets that grow large for lower minimum supports.
Furthermore, we show that we attain good results as a result of the specific characteristics of our enhanced pattern language. We show that simpler pattern sets lead to worse results and that target table based approaches are much less effective. We conclude by discussing the readability of our models, and their related work.
In this section we formally define the data type, relational pat-terns, how such patterns occur in the database, and how to calculate the support of a pattern.
We assume that the data resides in a multi-relational database in which the relations between the tuples in the various tables is coded, as usual, via foreign keys . We assume that any pair of two ta-bles have at most one foreign key relationship between them. This is without loss of generality as databases can always be losslessly recoded such that this assumption holds. Moreover, we assume that all attributes of all tables have a categorical domain. To introduce our notation, we give a brief formal description.

The database db consists of a set of tables, db = { T 1 , . . . , T and we assume that all the table names (the T i ) are unique. Each table T has a schema S ( T ) . This schema consists of a key , 0 or more foreign keys , and 1 or more attributes, i.e, in which: In our example database, shown bottom-right in Figure 1, the DISPOSITION table has as key: disp ID (depicted in bold) and as foreign key account ID . Moreover, it has an attribute Type whose domain is: { Owner, Disponent } .

Next to a schema, each table has an extend , consisting of a set of tuples. As usual, we blur the distinction between the table and its extend and say that a tuple t is in table T i ; denoted by t  X  T The database as whole should satisfy referential integrity . That is, foreign-key values in a tuple refer to existing tuples in the table for which this foreign key is the key. More formally we have:
A tuple for table T i with S ( T i ) = ( K i , F i , A i ) is given by: in which: Figure 1: An illustrative relational database: an excerpt from the Financial database. Again as usual, we will suppress the labels in tuples whenever pos-sible. That is, we simply write (40 , 10 , Owner )  X  DISPOSITION for a tuple in our example database in Figure 1.
The prototypical example of patterns are item sets. In the case of a (single) table of categorical data, an item set generalises to a selection . Clearly such patterns should be included into our pattern language. However,  X  X rue X  relational patterns should cross multiple tables. That is, they should describe related selections over multiple tables.

After formally introducing our pattern definition, we will illus-trate it with an example.

D EFINITION 1 (P ATTERN ). Let db = { T 1 , . . . , T n } be a data-base for which each table T i has schema S ( T i ) = ( K i
The third component is a generalisation of the second, which is included to simplify part of the following definitions.

In our pattern definition we define the alphabet patterns as those patterns that select one attribute and assign it to one value (i.e. p = T ( A j = v j ) ).

Also, we define the size of a pattern as the number of attributes within the pattern:
Using our example database shown in Figure 1, we can now illustrate our pattern definition. An example of the first compo-nent, a single table selection on the ACCOUNT table, would be ACCOUNT ({Frequency =2}). This pattern can be seen in the data-base in the set of tuples with account-ids 10 and 13.

The second component allows a pattern to have multiple se-lections from a table. As an example, in our database we see the pattern that accounts of Frequency 2 have dispositions of type Owner (seen at account-id 10). A more complex example is that there are accounts of Frequency 3 that have both dispositions of type Owner and dispositions of type Disponent . For this last pat-tern, one tuple in the ACCOUNT table is related to two distinct tu-ples in the DISPOSITION table (account-ids 11 and 12). This last pattern is represented as follows: ACCOUNT ({Frequency = 3}) [ DISPOSITION ({Type = Owner}), DISPOSITION ({Type = Disponent})].

The third component extends this last example by allowing the patterns to span more than two tables.

Note that in our pattern language a pattern can  X  X tart X  at one of these patterns to be at one specific target table, because interest-ing patterns within the database can start from all possible tables. Furthermore, as one single tuple can be joined with multiple other tuples, the structure of our patterns matches the complexity of the database.
 Having a richer pattern language is no virtue in its own right. Alternatively, we could have used pattern languages such as those used in WARMR [3], FARMER [13], or in our earlier work [10]. The experiments however show that our pattern set, consisting of FARMER pattern sets without a fixed target table, allows us to cap-ture more important structure within the data.
Our the patterns can become rather complicated structures, con-to consider what the domain of such patterns is. That is, what does an instance look like? The definition of these domains follows the inductive structure of the patterns.

D EFINITION 2 (D OMAIN ). Let db = { T 1 , . . . , T n } be a data-base for which each table T i has schema S ( T i ) = ( K i Moreover, let p  X  X  i . The domain of p , denoted by D ( p ) , is given by Algorithm 1 Generate P  X 
Generate P  X  ( db,  X  ) 1: for all T i  X  db do 2: P i  X  = FARMER ( db,  X , target = T i ) 3: end for 4: P  X  = 5: return P  X  Note that this domain definition is broad: it does not enforce that patterns describe related tuples. The reason for this liberal defini-tion is that here we are only interested in the general structure of the domain. Referential integrity does play its role in the definition of an occurrence of a pattern.

While these domains may have a rather complicated structure, there is some simplicity. For a pattern p  X  X  i , the domain is either D ( T i ) or it is the cartesian product of D ( T i ) with a complicated list domain. That is, D ( p ) = D ( T i )  X  X , in which X denotes a list domain. This observation has a useful consequence. It means that if p is a pattern for T i (i.e., p  X  P i ), and t is an instance of p (i.e, t  X  D ( p ) ), we can project t on the keys, the foreign keys and the attributes of T i . We will use these projections in the definition of an occurrence.

An occurrence of a pattern in the database is an instance of that pattern in the database. However, different from these instances, for occurrences we do require referential integrity . That is, the oc-currences should consist of related tuples only. This makes the definition slightly more complex.

D EFINITION 3 (O CCURRENCE ). Let db = { T 1 , . . . , T n a database for which each table T i has schema S ( T i ) = ( K A ) . Moreover, let p  X  X  i . As usual, the support of a pattern is the number of its occurrences. We say that a pattern is frequent if the support exceeds some user-defined threshold called the minimum support  X  .

Note that we use lists in our occurrence definition. Each occur-rence can span multiple tuples within one table T i that are stored in a list [ t i 1 , . . . , t i k Figure 2: The database is partially covered with the two first patterns of the code table using RDB-K RIMP . The uncoloured part of the database is covered by alphabet patterns. Note that for a lossless decoding we incorporate the database order (seen at the swap of LOAN :loan-id=35 and 36). is essential to encode the database in a lossless manner, as we will show below.

Given this pattern definition, in order to derive the set of frequent patterns P  X  one can resort to existing relational mining algorithms like FARMER [13] or attribute tree miners like FATminer [2]. Par-tial frequent pattern sets generated by either approach can be com-bined into P  X  (see Algorithm 1).

Finally, we define a canonical order on our patterns. We assume for each table T i , each attribute A j , and each attribute value v unique (string) label. We denote by l ( X ) the unique label assigned to X ( X  X  { T i , A j , v k } ). Canonical forms are simply strings, hence we have the familiar lexicographic order, denote by &lt; them. Using this order, we define:
This allows us to define a canonical order on our patterns: p &lt; can p 1 iff canonical ( p 0 ) &lt; lex canonical ( p
Now we have defined our patterns and database, we can present our problem formally. In order to find a good global model for our relational database, we use the minimum description length (MDL) [7] principle, which is a practical application of Kolmogorov Com-plexity [11]. Given a set of models H , we want to find a model H that minimises L ( H ) + L ( D | H ) , in which
The data that is encoded by our model resides within a relational database as defined in Section 2, or more specifically within its attribute data. Similar to [10, 14] our models are code tables. Such Algorithm 2 R E O RDER DB
R E O RDER DB ( reorder, [ t 1 , . . . , t i ]) 1: if [ t 1 , . . . , t i ]  X  reorder  X  ord reorder then 2: reorder = reorder  X  [ t 1 , . . . , t i ] 3: return true 4: else 5: return false 6: end if
A : [ a 1 , . . . , a n ]  X  ord B : [ b 1 , . . . , b m ] 1: if A =  X  then 2: return true 3: else if B =  X  then 4: return false 5: else if a 1 = b 1 then 6: return [ a 2 , . . . , a n ]  X  ord [ b 2 , . . . , b m 7: else if a 1 6 = b 1 then 8: return [ a 1 , . . . , a n ]  X  ord [ b 2 , . . . , b m 9: end if a code table CT is a two column table. On the left hand side reside relational patterns as defined above, on the right hand side reside the codes. The codes are taken from a prefix code C . For each code, we calculate a Shannon entropy based length: more frequently used codes obtain smaller lengths. Further, the number of patterns in the code table is denoted by | CT | .

We encode the relational database using the patterns from a code table CT . Figure 2 shows an example on how this encoding comes about through a database cover. Here, we show two patterns that (partially) cover the database. Using our pattern notation, we write the first code table pattern as:
We cover the database by replacing the related attribute values with the code of the pattern. As this pattern occurs at two yet un-covered locations in the database (id=10 and 13), it is used to de-scribe this part of the database. Note that in this figure each code table element has its own distinct colour.

In order for the encoding to be lossless, we need to be able to decode every part of the occurrence. As an occurrence may span multiple tuples within the database, we need to write the code at each tuple covered by this occurrence. Furthermore, as each pattern has just one code the necessity of a database-and pattern order becomes clear: we need to know which tuple is covered with which pattern from the list.

We match the order of the tuples within the database with the order of the pattern. In the first pattern ( p 1 in fig. (2)), LOAN : { Date = 06 / 2008 , Duration = 12 } is ordered before LOAN : { Date = 09 / 2008 , Payment = B } . Note the swap of tuples 35 and 36 to align the database order with the order of the pattern. Once covered, the complete database is encoded and looks like a mosaic, which can be decoded using the database order, and the code table (see fig. 2).

So, for unambiguous decoding, the order of the tuples in the terns. Therefore, we allow a (partial) re-ordering of the tuples in the database. However, a pair of tuples is only re-ordered once, Algorithm 3 C OVER and C OVER DB
C OVER ( db, p, reorder ) 1: frequency = 0 2: for all { t } X  occ ( p ) do 3: if p  X  X  A t } then 4: if R E O RDER DB ( reorder, { t } ) then 5: { A t } = { A t }\ p 6: frequency ++ 7: end if 8: end if 9: end for 10: return frequency
C OVER DB ( db, CT ) 1: reorder =  X  2: for all c i  X  CT do 3: frequency ( c i ) = C OVER (db, c i , reorder ) 4: count ( c i ) = frequency ( c i )  X | coverspots ( c i ) | 5: end for otherwise the unambiguous decoding property will be lost. Hence, we keep track of the order. Initially this ordered list, reorder , is empty. Whenever two (or more tuples) that are not yet in the list are re-ordered their identifiers are appended.

We cover the database with patterns p until the database is com-pletely covered. An occurrence of a pattern can only cover the database if the database order can be aligned to match the pattern. This is the case when the tuples related to the occurrence are not yet ordered, or if they are already ordered in the correct order (e.g. the order of the tuples matches the order of the pattern). We up-date and check partial database order via the R E O RDER DB algo-rithm (see Algorithm 2). The algorithm takes as input the current (partial) database order reorder and a list of tuples of the cur-is an empty list as the database is unordered. To check whether [ t , . . . , t i ]  X  reorder is an ordered subset of reorder we use the  X  ord operator (line 1). If so, the order is updated by appending the current list of tuples that are not yet part of the database order (2). Otherwise, this particular occurrence cannot be used to cover the database.

Now that we can adjust reorder to align with a pattern occur-rence, we can partially cover a database given a single code ta-ble pattern (see Algorithm 3). For each pattern, we have a set of occurrences occ ( p ) (1). C OVER iterates over all occurrences and evaluates whether it can be covered (3). We only cover the current occurrence (the list of tuples) if all related attributes are still uncov-ered (4), and if the current occurrence is an ordered subset of the current database order (5). If so, the attributes of the occurrence are covered and the frequencies are updated (6,7). We define the fre-quency of a pattern as the number of times we cover the database with it.

Using the code table CT , we cover the complete relational data-base using the C OVER DB algorithm (see Algorithm 3). Consider-ing the patterns in the code table, it covers the database using the C
OVER algorithm (2-3). During the cover process, we obtain for each code table pattern its frequency (line 3).

To be able to decode the encoded database, we have to write its code at each involved tuple. We define coverspots as the set of tuples at which we need to write down the code for pattern p . More Algorithm 4 RDB-K RIMP
RDB-K RIMP ( db, P  X  ) 1: CT = CT init 2: for all c  X  P  X  do 3: CT new = CT + c in order 4: C OVER DB ( db, CT new ) 5: if L ( CT new , db ) &lt; L ( CT, db ) then 6: CT = CT new 7: end if 8: end for formally, we define coverspots as: We denote the number of coverspots by: | coverspots ( c ) | .
Consider the first pattern, p 1 , from Figure 2. The number of coverspots is 5, as each occurrence has five distinct tuples in the database associated with it (one in ACCOUNT , two in ORDER , and two in LOAN ). We denote the count as the total number of times we have to write the code given a code table pattern c : count ( c ) = frequency ( c )  X | coverspots ( c ) | (line 4).

Given the obtained counts, we can now determine the code length for each code table element c i . For optimal encoding, we use a Shannon code, i.e.,
Each code table element c has a standard length, which is the decoded length using alphabet patterns only: L st ( c ) . The encoded length of the complete code table then is:
In covering the database, we write count ( c i ) codes for each code table element. Given the code lengths, the encoded size of the data-base becomes In our approach, we define the total encoded size as:
Now that we can derive the total encoded size of the database given a code table, we can formulate our problem as follows.
P ROBLEM S TATEMENT . Let db = { T 1 , . . . T n } be a relational database as defined in Section 2. Find the code table CT that minimises L ( CT, db ) .
Finding the optimal code table, the one that compresses the data-base best, is a very hard problem. The search space is extremely large and there is no useful structure to prune it. Hence, we have to use heuristics and we therefore resort to an approach similar to the one described in [14].

The algorithm we introduce to this end, RDB-K RIMP , approxi-mates the optimal code table for a given database. It does this by starting with the simplest possible code table and iteratively testing Table 1: Characteristics of the used databases. Shown are for each table, the number of tuples ( # t ), the number of attributes ( # a ), the number of keys ( K and F ) ( # k ), and the average number of joins a single tuple can make ( join ). FINANCIAL ACCOUNT 682 2 3 5.43 GENES GENES1 862 4 1 6.86 HEPATITIS BIO 694 5 2 1 each pattern in a candidate set. A candidate pattern is only kept in the code table if it improves the compression. The RDB-K algorithm is shown in pseudo code in Algorithm 4.
 RDB-K RIMP starts with a database db and a frequent pattern set P  X  as input. To ensure that the complete database can be covered always, the code table is initialized with CT init (line 1), which contains all alphabet elements (i.e. p = T i ( A j = v j ) ). One by one, it takes a candidate pattern from P  X  and tests whether it con-tributes to improve compression (2-8). To do this, a new code table CT new is constructed by adding the candidate pattern to the previ-ous code table CT (3). Using this code table we compute a cover of the database (4) and the compressed sizes of the old and new code table are compared (5). If the addition of the new candidate pattern improves compression, it is kept in the code table (6). Otherwise, it is permanently discarded.

Note that we need to define two orders: the first on the candidate pattern set P  X  and the second on the patterns in the code table. For P  X  we define an order for all pattern pairs ( p 1 , p 2 ) :
For the code table, we define the following order on all pattern pairs ( p 1 , p 2 ) :
We assume that both the frequent pattern sets and code tables are always ordered in this fashion. Note that we are not after the actual attained compression, but rather the patterns that contribute to it. We use lossless compression as a means, not as a goal, to find a good set of descriptive patterns. Figure 3: Results for different minimum support values  X  : (a) The encoded length obtained for the database, (b) the number of frequent patterns, and (c) the number of code table patterns in CT .
To experimentally validate our approach we run experiments on publicly available relational data sets from previous KDD-cups (see Table 1). These databases are: the financial 1 , genes interaction and hepatitis 3 databases. We use a frequent attributed tree miner [2] to generate the frequent pattern sets, as our relational patterns can be represented as attributed trees. In order to find the optimal model of the database, RDB-K would ideally evaluate all patterns. However, in order to be ef-ficient, we evaluate all frequent patterns. To measure the effect of the minimum support value, we generate a frequent candidate set P  X  for various  X  . Given a P  X  we compress the database using RDB-K RIMP , which results in a code table CT and an encoded database size L ( CT, db ) .

The effect of sweeping the minimum support is shown in Figure 3a. For all used databases, we see that increasingly lower encoded database sizes are obtained for lower minimum support values.
The smaller encoded database sizes relate to the larger available sets of candidate patterns (see Figure 3b). In all cases we see that this candidate set growth is exponential. These larger candidate sets contain more patterns that can be inserted in the code table to contribute to the database description.

While we see that P  X  grows exponentially for lower values of  X  , we do not see this trend in the size of the code table (see Figures http://lisp.vse.cz/challenge/ http://pages.cs.wisc.edu/ dpage/kddcup2001/ http://lisp.vse.cz/challenge/ Figure 4: Choosing a particular table as a target decreases per-formance. For the lowest used minimum supports, the use of a target table leads to a worse encoding on all databases. Even for the star-shaped Hepatitis database, which seems well suited for a target table based approach. 3b and 3c respectively). Our code table grows for lower values of  X  , but still only a small set is necessary to model the data. With respect to the original candidate set, our code tables achieve up to 4 orders of magnitude reduction in terms of number of patterns.
As described in Section 3, RDB-K RIMP re-orders the database in order to ensure a lossless encoding for the database. As the initial database order is not determined by our algorithm, it can potentially influence the resulting tuple order. Therefore, we need to evaluate the extend of its influence on the resulting database order and the resulting encoded database length. In order to evaluate the effect of this initial order, we randomly shuffle the tuples within the tables.
The experiments indicate that the influence on the resulting com-pression is minimal. The number of patterns used to encode the database is very similar to the original versions and lead to a similar database compression. The deviation for the database encoding for the financial , genes , and hepatitis database is respectively 1.12%, 0.35%, and 0.01%. Hence, RDB-K RIMP is robust with respect to the initial database order.
In our approach, we generalise the FARMER pattern language such that we do not rely on a specific target table. We expect a better description of the database by considering patterns starting from all possible tables. In order to evaluate this, we compare the results from alternative trials using a fixed target table as is usual in relational data mining [3, 13]. In these experiments, we encode the complete database using solely patterns that originate from a specific target table.

For all possible target tables, we determine how well we can ap-proximate the original result that is obtained for the lowest used minimum support. We have depicted the deviation from the orig-inal result in Figure 4. We see that fixing the candidate set to a specific target table has a negative influence on the encoding of the database. The compression deteriorates up to 24% compared to the best obtained encoded size. This shows that allowing patterns to start at any table leads to a better description of the database. The hepatitis database shows some additional interesting results. This database is a prime example of a target table based database, Figure 5: Our generalised relational patterns lead to better re-sults. While the number of candidates grows large for low mini-mum supports (b), we obtain good database encodings (a) using compact code tables (c) (shown for the genes database). as all (medical test) data surrounds a main table: patient . Even in this case, we see that picking a single target table leads to an increase in encoding length (a worse code table). Apparently, we can describe some data better when we do not pick the patient table as the single target table.
In Section 2, we defined a rich pattern language to match the database complexity. To measure whether or not we can describe the database better using more intricate patterns, we here compress the database with an increasingly more general pattern definition. A better database description would lead to smaller encoded database sizes.

A single complex pattern can describe structure in the database that would otherwise require multiple simpler patterns, possibly leading to a better compression. We have used the following char-acteristic pattern definitions to evaluate:
Single Table Patterns. Here we only allow p = T i ( { A 1 A } ) patterns in our P  X  . In other words, all candidate patterns only cover one table, and no joins are allowed. With this candidate set, we compress the database solely with single table patterns.
WARMR-like Patterns. In this candidate set, each pattern cov-ers one tuple per table at most. In our notation, we define these used in WARMR-like approaches, which use an existential quanti-fier to select strictly one tuple from a table. Unlike WARMR ap-plications, we do allow these patterns to start at any table in the database, in stead of a single target table [3].

Our language. Here we consider patterns as defined in Section 2. In this set, the patterns are defined as p = p 0 [[ p 1 [ p , . . . p q k Table 2: On all databases more general patterns lead to smaller encoded sizes for the database. financial 91% 29 76% 130 76% 117 genes 87% 72 86% 191 83% 342 hepatitis 99% 5 98% 13 97% 26 generated pattern sets for each table.

Note that we order all three candidate sets as defined in Section date set size | P  X  | , the number of code table elements | CT | , and the compressed encoded size of the database L ( CT, db ) .

The depicted result of the genes database in Figure 5a shows a typical result in terms of compressing the database. We see that the single table patterns lead to the worst encoded length for the database. A better compression can be derived when we allow WARMR patterns in our candidate set, which consequently can be improved by allowing all patterns to cover the database. We outline the obtained results on all databases in Table 2.

As before, we see that the number of candidate patterns grow very steeply for lower minimum support values. Also, we see that for more general pattern types we see a steeper candidate set growth (see fig. 5b). Note that we use a log scale to depict the number of patterns.

We have seen that achieving a smaller database encoding relies on the ability to draw patterns from an enriched pattern set. The inclusion of these enriched patterns indicates that essential char-acteristic structure within the database can be best described with these type of patterns: simpler patterns are apparently not suffi-cient. Although RDB-K RIMP steers to small code tables, it allows these additional patterns in the code table only if they aid in the description of essential structure that would otherwise be described in a much less efficient manner (see fig. 5c).
Compactness is not the only feat of interest. In order for our model to provide insight in the database it should be interpretable. As our code table contains a collection of characteristic patterns, we can pick single code table patterns to examine.

To show an example, we pick a code table pattern that is intu-itive without expert knowledge. Shown in Figure 6 is a pattern that reads as follows:  X  X  gene localised in the nucleus having a tran-scription function that has two distinct physical interactions X  . As one would expect, a characteristic pattern for the GENES database is that transcription often involves physical interactions in order to copy information and that it occurs in the nucleus. Note that this pattern selects two distinct tuples from the INTERACTION table.
The small encoded database lengths are obtained largely due to the availability of the more complex patterns in our pattern lan-guage. Patterns that select multiple tuples from one table are used more often in the database description, leaving simpler patterns to  X  X ill up X  the database.

In the code tables obtained for the lowest used minsup, pat-terns that select multiple tuples from one table make up for 16%, 63%, and 71% of the content for the financial , genes , and hepatitis databases respectively.
 Figure 6: A partial description: a code table element from the description of the Genes database. Note that these patterns se-lect multiple tuples from the same table. Our code tables are models for a complete database. Similarly, Probabilistic Relational Models (PRM) are graph based models that can be applied to model relational databases [5, 9]. A PRM is one graph, in which attribute values are linked with their co-occurrence probability. This contrasts to our code table, which is not a single graph, but a collection of local tree-like patterns. Moreover, an attribute value can occur multiple times in different local models, if this aids in describing the database better. This means that we can regard an attribute value in different contexts, instead of one.
In the field of Relational Data Mining (RDM), ILP based ap-proaches, such as the WARMR algorithm, allow for the discovery of relational patterns [3, 15]. As seen in our experiments, we ob-tain better database descriptions when we use our pattern language compared to WARMR-type patterns (see fig. 5).

The main application for a WARMR-like approach is when one is specifically interested in a target table, for example when trying to improve the classification scores on a target table given relational information [16]. This target table leads to an effective manner to prune the search space, and allows for aggregate functions to im-prove the efficiency [8]. An efficient implementation is FARMER [13], which in addition allows for a more general pattern language similar to the patterns used in this work.

In the work of both [4] and [12] the goal to find a different type of pattern: multi-valued dependencies (MVD) and functional depen-dencies (FD). These are  X  X lobal X  patterns: a dependency is a  X  X igher order X  pattern similar to a constraint on the relation, in contrast to code table patterns, which are local patterns.
In earlier work, we introduced R-K RIMP which finds patterns the patterns used in RDB-K RIMP , and thus have less descriptive potential. R-K RIMP patterns are similar to the work of [6], who define these patterns as simple conjunctive queries .

In RDB-K RIMP a single code table element can describe more structure in the database than with R-K RIMP . As an example, con-sider a RDB-K RIMP -style pattern from our illustrative database:  X  X  account with frequency 3 having both a disponent-type disposi-tion and a owner-type disposition X  (see fig. 2). This single patterns translates requires two individual R-K RIMP -style patterns:  X  X  ac-count with frequency 3 having a disponent-type disposition X  and  X  X  account with frequency 3 having a owner-type disposition X  .
Note that R-K RIMP in this case would cover ACCOUNT ./ ORDER in an overlapping manner. As tuples can occur multiple times within a join, R-K RIMP patterns allow for an overlapping cover. Thus, even in the initial case, when solely alphabet patterns are used, the covering of duplicate tuples will lead to a very different encoding.
In our experiments, we see that compared to the candidate set the code table growth shows a much slower pace.

Not only it stays small compared to the candidate set, it also stays compact compared to the original database. From the original set of frequent candidates only a few patterns contribute to the database description, leading up to 4 orders of magnitude reduction.
When we look at the influence of the initial database order, we see that this does not have much effect. We obtain only a slight deviation, up to around 1% from the original encoded length.
Enforcing a single target table rather than treating all tables equal yields a much worse database description. The encoded database length shows an increase of up to 25% compared to the original result.

In the hepatitis database all tables center around one single cen-tral table: the PATIENT table. While this database seems like a good case to pick as a target table, even here we obtain our best description when patterns are allowed to start at other tables.
Allowing a richer pattern language is a fruitful effort. We see that we generate more potentially interesting patterns in this manner, from which we can select those that describe the database best. Compared to WARMR-like patterns, we see that we can describe the database better: we achieve shorter encoded lengths. To obtain these good database descriptions, the code tables rely for the large part on these enriched patterns, which are of the type that selects multiple tuples from a table.

Our code tables stay compact and do not show exponential set growth as frequent pattern sets do for lower minimum supports. RDB-K RIMP shows to be successfully to select compact models. Even under its MDL-selection pressure, larger enriched patterns are selected to describe the database, indicating that these patterns are very characteristic for the database.
Currently, in order to obtain insight from a relational database, either one aims to find a  X  X lobal X  model of the complete database, or one seeks a collection of  X  X ocal X  patterns in the database. While both approaches have their merits, they have their shortcomings. Global models tend to blur out interesting local structure for the sake of the global structure, and pattern collections tend to drown the global picture in a sea of patterns. In this paper, we propose a method that combines the merits of both directions: it mines a com-pact but detailed description of the complete relational database.
For a database model to be descriptive it should be able to reflect the patterns that are present within it. Relational databases allow for tuples in one table to be connected to multiple tuples from other tables. Hence, if we want to find interesting patterns in a database, our pattern language should be rich enough to reflect such relations.
In this paper, we introduce RDB-K RIMP , an algorithm that de-scribes the complete relational database using only a small collec-tion of characteristic patterns: the code table. While the code table serves as a global model for the complete database, its patterns pre-serve the local details. Using the MDL principle, RDB-K results in a compact set of patterns that together describe the data-base well. With respect to the original set of frequent candidates, we obtain up to 4 orders of magnitude reduction.

Our rich relational pattern language allows RDB-K RIMP to draw from a larger pool of interesting frequent candidate patterns. The experiments reported on in this paper verify our claims both on the usefulness of our pattern language and on the ability of RDB-K
RIMP to select just a few highly descriptive patterns. Firstly, each code table heavily relies on the introduced pattern language: up to 70% of the patterns select multiple tuples from one table. Secondly, experiments show that our pattern language leads to a far better compression. In other words, these patterns highlight characteristic structure in the database. The fact that these patterns are not only characteristic but also easily interpretable is shown by an example from the Genes database (see fig. 6). As part of the code table we find a pattern that describes gene interactions:  X  X  gene localised in the nucleus having a transcription function that has two distinct physical interactions X  .

In contrast to current frequent pattern mining approaches, our approach does not rely on a target table. That is, our patterns can start from any table. In all cases this yields to improvements, of up to almost 25%. Even in a typical  X  X arget table X  style database, we see that we obtain a better description without the use of a single table as target.
