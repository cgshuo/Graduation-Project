 Richard Combes RCOMBES @ KTH . SE KTH, Royal Institute of technology, Stockholm, Sweden Alexandre Proutiere ALEPRO @ KTH . SE KTH, Royal Institute of technology, Stockholm, Sweden Stochastic Multi-Armed Bandits (MAB) ( Robbins , 1952 ; Gittins , 1989 ) constitute the most fundamental sequen-tial decision problems with an exploration vs. exploita-tion trade-off. In such problems, the decision maker se-lects an arm in each round, and observes a realization of the corresponding unknown reward distribution. Each de-cision is based on past decisions and observed rewards. The objective is to maximize the expected cumulative re-ward over some time horizon by balancing exploitation (arms with higher observed rewards should be selected often) and exploration (all arms should be explored to learn their average rewards). Equivalently, the perfor-mance of a decision rule or algorithm can be measured through its expected regret , defined as the gap between the expected reward achieved by the algorithm and that achieved by an oracle algorithm always selecting the best arm. MAB problems have found many fields of appli-cation, including sequential clinical trials, communicat ion systems, economics, see e.g. ( Cesa-Bianchi &amp; Lugosi , 2006 ; Bubeck &amp; Cesa-Bianchi , 2012 ).
 In their seminal paper ( Lai &amp; Robbins , 1985 ), Lai and Rob-bins solve MAB problems where the successive rewards of a given arm are i.i.d., and where the expected rewards of the various arms are not related. They derive an asymp-totic (when the time horizon grows large) lower bound of the regret satisfied by any algorithm, and present an algo-rithm whose regret matches this lower bound. This ini-tial algorithm was quite involved, and many researchers have tried to devise simpler and yet efficient algorithms. The most popular of these algorithms are UCB ( Auer et al. , 2002 ) and its extensions, e.g. KL-UCB ( Garivier &amp; Capp  X e , 2011 ; Capp  X e et al. , 2013 ) (note that KL-UCB algorithm was initially proposed in ( Lai , 1987 ), see (2.6)). When the expected rewards of the various arms are not related ( Lai &amp; Robbins , 1985 ), the regret of the best algorithm is essentially of the order O ( K log( T )) where K denotes the number of arms, and T is the time horizon. When K is very large or even infinite, MAB problems become more challenging. Fortunately, in such scenarios, the ex-pected rewards often exhibit some structural properties th at the decision maker can exploit to design efficient algo-rithms. Various structures have been investigated in the literature, e.g., Lipschitz ( Agrawal , 1995 ; Kleinberg et al. , 2008 ; Bubeck et al. , 2008 ), linear ( Dani et al. , 2008 ), con-vex ( Flaxman et al. , 2005 ).
 We consider bandit problems where the expected reward is a unimodal function over partially ordered arms as in ( Yu &amp; Mannor , 2011 ). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or conti n-uous, in which case arms belong to a bounded interval. This unimodal structure occurs naturally in many practical deci -sion problems, such as sequential pricing ( Yu &amp; Mannor , 2011 ) and bidding in online sponsored search auctions ( B. , 2005 ).
 Our contributions. We mainly investigate unimodal ban-dits with finite sets of arms, and are primarily interested in cases where the time horizon T is much larger than the number of arms K . (a) For these problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This lower bound does not depend on the structure of the graph, nor on its size: it actually corresponds to the regret lower bound in a classical bandit problem ( Lai &amp; Robbins , 1985 ), where the set of arms is just a neighborhood of the best arm in the graph. (b) We propose OSUB (Optimal Sampling for Unimodal Bandits), a simple algorithm whose regret matches our lower bound, i.e., it optimally exploits the unimodal struc -ture. The asymptotic regret of OSUB does not depend on the number of arms. This contrasts with LSE (Line Search Elimination), the algorithm proposed in ( Yu &amp; Mannor , 2011 ) whose regret scales as O (  X D log( T )) where  X  is the maximum degree of vertices in the graph and D is its diam-eter. We present a finite-time analysis of OSUB, and derive a regret upper bound that scales as O (  X  log( T )+ K ) . Hence OSUB offers better performance guarantees than LSE as soon as the time horizon satisfies T  X  exp( K/ X D ) . Al-though this is not explicitly mentioned in ( Yu &amp; Mannor , 2011 ), we believe that LSE was meant to address bandits where the number of arms is not negligible compared to the time horizon. (c) We further investigate OSUB performance in non-stationary environments where the expected rewards smoothly evolve over time but keep their unimodal struc-ture. (d) We conduct numerical experiments and show that OSUB significantly outperforms LSE and other classi-cal bandit algorithms when the number of arms is much smaller than the time horizon. (e) Finally, we briefly discuss systems with a continuous set of arms. We show that using a simple discretization of the set of arms, UCB-like algorithms are order-optimal, and ac-tually outperform more advanced algorithms such as those proposed in ( Yu &amp; Mannor , 2011 ). This result suggests that in discrete unimodal bandits with a very large number of arms, it is wise to first prune the set of arms, so as to reduce its size to a number of the order of Unimodal bandits have received relatively little attentio n in the literature. They are specific instances of bandits in metric spaces ( Kleinberg , 2004 ; Kleinberg et al. , 2008 ; Bubeck et al. , 2008 ). In this paper, we add unimodality and show how this structure can be optimally exploited. Uni-modal bandits have been specifically addressed in ( Cope , 2009 ; Yu &amp; Mannor , 2011 ). In ( Cope , 2009 ), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O ( some strong regularity assumptions on the reward func-tion. In ( Yu &amp; Mannor , 2011 ), for the same problem, the authors present LSE, an algorithm whose regret scales as O (  X  sumption. The LSE algorithm is based on Kiefer X  X  golden section search algorithm. It iteratively eliminates subse ts of arms based on PAC-bounds derived after appropriate sam-pling. By design, under LSE, the sequence of parameters used for the PAC bounds is pre-defined, and in particular does not depend of the observed rewards. As a conse-quence, LSE may explore too much sub-optimal parts of the set of arms. For bandits with a continuum set of arms, we actually show that combining an appropriate discretiza-tion of the decision space (i.e., reducing the number of arms to  X  form LSE in practice (this is due to the adaptive nature of UCB). Note that the parameters used in LSE to get a regret of the order O ( In ( Yu &amp; Mannor , 2011 ), the authors also present an exten-sion of the LSE algorithm to problems with discrete sets of arms, and provide regret upper bounds of this algorithm. These bounds depends on the structure of the graph defin-ing unimodal structure, and on the number of arms as men-tioned previously. LSE performs better than classical ban-dit algorithms only when the number of arms is very large, and actually becomes comparable to the time horizon. Here we are interested in bandits with relatively small number of arms.
 Non-stationary bandits have been studied in ( Hartland et al. , 2007 ; Garivier &amp; Moulines , 2008 ; Slivkins &amp; Upfal , 2008 ; Yu &amp; Mannor , 2011 ). Except for ( Slivkins &amp; Upfal , 2008 ), these papers deal with environments where the expected rewards and the best arm change abruptly. This ensures that arms are always well separated, and in turn, simplifies the analysis. In ( Slivkins &amp; Upfal , 2008 ), the expected rewards evolve ac-cording to independent brownian motions. We consider a different, but more general class of dynamic environments: here the rewards smoothly evolve over time. The challenge for such environments stems from the fact that, at some time instants, arms can have expected rewards arbitrarily close to each other.
 Finally, we should mention that bandit problems with struc-tural properties such as those we address here can often be seen as specific instances of problems in the control of Markov chains, see ( Graves &amp; Lai , 1997 ). We leverage this observation to derive regret lower bounds. However, algo-rithms developed for the control of generic Markov chains are often too complex to implement in practice. Our algo-rithm, OSUB, is optimal and straightforward to implement. We consider a stochastic multi-armed bandit problem with K  X  2 arms. We discuss problems where the set of arms is continuous in Section 6 . Time proceeds in rounds indexed by n = 1 , 2 , . . . . Let X k ( n ) be the reward obtained at time n if arm k is selected. For any k , the sequence of rewards ( X k ( n )) n  X  1 is i.i.d. with distribution and expectation de-noted by  X  k and  X  k respectively. Rewards are independent across arms. Let  X  = (  X  1 , . . . ,  X  K ) represent the expected rewards of the various arms. At each round, a decision rule or algorithm selects an arm depending on the arms chosen in earlier rounds and their observed rewards. We denote by k  X  ( n ) the arm selected under  X  in round n . The set  X  of all possible decision rules consists of policies  X  satis-fying: for any n  X  1 , if F  X  n is the  X  -algebra generated by ( k 3.1. Unimodal Structure The expected rewards exhibit a unimodal structure, simi-lar to that considered in ( Yu &amp; Mannor , 2011 ). More pre-cisely, there exists an undirected graph G = ( V, E ) whose vertices correspond to arms, i.e., V = { 1 , . . . , K } , and whose edges characterize a partial order (initially unknow n to the decision maker) among expected rewards. We as-sume that there exists a unique arm k  X  with maximum expected reward  X   X  , and that from any sub-optimal arm k 6 = k  X  , there exists a path p = ( k 1 = k, . . . , k m length m (depending on k ) such that for all i = 1 , . . . , m  X  1 , ( k i , k i +1 )  X  E and  X  k set of vectors  X  satisfying this unimodal structure. This notion of unimodality is quite general, and includes, as a special case, classical unimodality (where G is just a line). Note that we assume that the decision maker knows the graph G , but ignores the best arm, and hence the partial order induced by the edges of G . 3.2. Stationary and non-stationary environments The model presented above concerns stationary environ-ments, where the expected rewards for the various arms do not evolve over time. In this paper, we also consider non-stationary environments where these expected rewards could evolve over time according to some deterministic dy-namics. In such scenarios, we denote by  X  k ( n ) the ex-pected reward of arm k at time n , i.e., E [ X k ( n )] =  X  and ( X k ( n )) n  X  1 constitutes a sequence of independent random variables with evolving mean. In non-stationary environments, the sequences of rewards are still assumed to be independent across arms. Moreover, at any time n ,  X  ( n ) = (  X  1 ( n ) , . . .  X  K ( n )) is unimodal with respect to some fixed graph G , i.e.,  X  ( n )  X  X  G (note however that the partial order satisfied by the expected rewards may evolve over time). 3.3. Regrets The performance of an algorithm  X   X   X  is characterized by its regret up to time T (where T is typically large). The way regret is defined differs depending on the type of envi-ronment.
 Stationary Environments. In such environments, the re-gret R  X  ( T ) of algorithm  X   X   X  is simply defined through the number of times t  X  k ( T ) = P 1  X  n  X  T 1 { k  X  ( n ) = k } that arm k has been selected up to time T : R  X  ( T ) = P an asymptotic (when T  X  X  X  ) regret lower bound satisfied by any algorithm in  X  , and (2) to devise an algorithm that achieves this lower bound.
 Non-stationary Environments. In such environments, the regret of an algorithm  X   X   X  quantifies how well  X  tracks the best arm over time. Let k  X  ( n ) denote the op-timal arm with expected reward  X   X  ( n ) at time n . The regret of  X  up to time T is hence defined as: R  X  ( T ) = P In this section, we consider unimodal bandit problems in stationary environments. We derive an asymptotic lower bound of regret when the reward distributions belong to a parametrized family of distributions, and propose OSUB, an algorithm whose regret matches this lower bound. 4.1. Lower bound on regret To simplify the presentation, we assume here that the reward distributions belong to a parametrized family of distributions. More precisely, we define a set of dis-tributions V = {  X  (  X  ) }  X   X  [0 , 1] parametrized by  X   X  [0 , 1] . The expectation of  X  (  X  ) is denoted by  X  (  X  ) for any  X   X  [0 , 1] .  X  (  X  ) is absolutely continuous with respect to some positive measure m on R , and we denote by p ( x,  X  ) its density. The Kullback-Leibler (KL) divergence number between  X  (  X  ) and  X  (  X   X  ) is: KL (  X ,  X   X  ) = R denote by  X   X  a parameter (it might not be unique) such that  X  (  X   X  ) =  X   X  , and we define the minimal diver-gence number between  X  (  X  ) and  X  (  X   X  ) as: I min (  X ,  X  Finally, we say that arm k has parameter  X  k if  X  k =  X  (  X  k ) , and we denote by  X  G the set of all parameters  X  = (  X  1 , . . . ,  X  K )  X  [0 , 1] K such that the correspond-ing expected rewards are unimodal with respect to graph G :  X  = (  X  1 , . . . ,  X  K )  X  U G . Of particular interest is the family of Bernoulli distributions: the support of m is { 0 , 1 } ,  X  (  X  ) =  X  , and I min (  X ,  X   X  ) = I (  X ,  X  gence number between Bernoulli distributions of respectiv e means  X  and  X   X  .
 We are now ready to derive an asymptotic regret lower in parametrized unimodal bandit problems as defined above. Without loss of generality, we restrict our atten-tion to so-called uniformly good algorithms, as defined in ( Lai &amp; Robbins , 1985 ) (uniformly good algorithms exist as shown later on). We say that  X   X   X  is uniformly good if Theorem 4.1 Let  X   X   X  be a uniformly good algorithm, and assume that  X  k =  X  (  X  k )  X  V for all k . Then for any  X   X   X  G , lim inf The above theorem is a consequence of results in optimal control of Markov chains ( Graves &amp; Lai , 1997 ). All proofs are presented in ( Combes &amp; Proutiere , 2014 ). As in classi-cal discrete bandit problems, the regret scales at least log -arithmically with time (the regret lower bound derived in ( Lai &amp; Robbins , 1985 ) is obtained from Theorem 4.1 as-suming that G is the complete graph). We also observe that the unimodal structure, if optimally exploited, can bring significant performance improvements: the regret lower bound does not depend on the size K of the decision space. Indeed c (  X  ) includes only terms corresponding to arms that are neighbors in G of the optimal arm (as if one could learn without regret that all other arms are sub-optimal). In the case of Bernoulli rewards, the lower regret bound GLSE, the algorithms proposed in ( Yu &amp; Mannor , 2011 ), have performance guarantees that do not match our lower bound: when G is a line, LSE achieves a regret bounded by 41 /  X  2 log( T ) , whereas in the general case, GLSE in-curs a regret of the order of O (  X D log( T )) where  X  is the maximal degree of vertices in G , and D is its diameter. The performance of LSE critically depends on the graph structure, and the number of arms. Hence there is an im-portant gap between the performanceof existing algorithms and the lower bound derived in Theorem 4.1 . In the next section, we close this gap and propose an asymptotically optimal algorithm. 4.2. The OSUB Algorithm We now describe OSUB, a simple algorithm whose re-gret matches the lower bound derived in Theorem of 4.1 for Bernoulli rewards, i.e., OSUB is asymptotically opti-mal. The algorithm is based on KL-UCB proposed in ( Lai , 1987 ; Capp  X e et al. , 2013 ), and uses KL-divergence upper confidence bounds to define an index for each arm. OSUB can be readily extended to systems where reward distri-butions are within one-parameter exponential families by simply modifying the definition of arm indices as done in ( Capp  X e et al. , 2013 ). In OSUB, each arm is attached an index that resembles the KL-UCB index, but the arm se-lected at a given time is the arm with maximal index within the neighborhood in G of the arm that yielded the highest empirical reward. Note that since the sequential choices of arms are restricted to some neighborhoods in the graph, OSUB is not an index policy. To formally describe OSUB, we need the following notation. For p  X  [0 , 1] , s  X  N , and n  X  N , we define: with the convention that F ( p, 0 , n ) = 1 , and F (1 , s, n ) = 1 , and where c &gt; 0 is a constant. Let k ( n ) be the arm selected under OSUB at time n , and let t k ( n ) denote the number of times arm k has been selected up to time n . The empirical reward of arm k at time n is  X   X  k ( n ) = 0 otherwise. We denote by L ( n ) = arg max 1  X  k  X  K  X   X  the index of the arm with the highest empirical reward (ties are broken arbitrarily). Arm L ( n ) is referred to as the leader at time n . Further define l k ( n ) = P n t =1 1 { L ( t ) = k } the number of times arm k has been the leader up to time n . Now the index of arm k at time n is defined as: Finally for any k , let N ( k ) = { k  X  : ( k  X  , k )  X  E } X  X  k } be the neighborhood of k in G . The pseudo-code of OSUB is presented below.
 Algorithm OSUB Input: graph G = ( V, E ) For n  X  1 , select the arm k ( n ) where: where  X  is the maximal degree of nodes in G and ties are broken arbitrarily.
 Note that OSUB forces us to select the current leader often: L ( n ) is chosen when l L ( n ) ( n )  X  1 is a multiple of  X  + 1 . This ensures that the number of times an arm has been se-lected is at least proportional to the number of times this arm has been the leader. This property significantly simpli-fies the regret analysis, but it could be removed. 4.3. Finite-time analysis of OSUB Next we provide a finite time analysis of the regret achieved under OSUB. Let  X  denote the minimal sepa-ration between an arm and its best adjacent arm:  X  = known a priori.
 Theorem 4.2 Assume that the rewards lie in [0,1] (i.e., the support of  X  k is included in [0 , 1] , for all k ), and that (  X  1 , . . . ,  X  K )  X  U G . The number of times suboptimal arm k is selected under OSUB satisfies: for all  X  &gt; 0 and all T  X  3 , E [ t k ( T )]  X  where  X  (  X  ) &gt; 0 , and 0 &lt; C 1 &lt; 7 , C 2 &gt; 0 , C constants.
 To prove this upper bound, we analyze the regret accu-mulated (i) when the best arm k  X  is the leader, and (ii) when the leader is arm k 6 = k  X  . (i) When k  X  is the leader, the algorithm behaves like KL-UCB restricted to the arms around k  X  , and the regret at these rounds can be analyzed as in ( Capp  X e et al. , 2013 ). (ii) Bounding the number of rounds where k 6 = k  X  is not the leader is more involved. To do this, we decompose this set of rounds into further subsets (such as the time instants where k is the leader and its mean is not well estimated), and control their expected cardinalities us-ing concentration inequalities. Along the way, we establis h Lemma 4.3 , a new concentration inequality of independent interest.
 Lemma 4.3 Let { Z t } t  X  Z be a sequence of independent random variables with values in [0 , B ] . Define F n the  X  -algebra generated by { Z t } t  X  n and the filtration F = ( F n ) n  X  Z . Consider s  X  N , n 0  X  Z and T  X  n 0 . We is a F t  X  1 -measurable random variable. Further define t time such that either t  X   X  s or  X  = T + 1 .
 Then we have that: P [ S  X   X  t  X   X  ,  X   X  T ]  X  exp(  X  2 s X  2 B  X  2 ) . As a consequence: P [ | S  X  |  X  t T ]  X  2 exp(  X  2 s X  2 B  X  2 ) .
 Lemma 4.3 concerns the sum of products of i.i.d. ran-dom variables and of a previsible sequence, evaluated at a stopping time (for the natural filtration). We believe that concentration results for such sums can be instrumen-tal in bandit problems, where typically, we need infor-mation about the empirical rewards at some specific ran-dom time epochs (that often are stopping times). Refer to ( Combes &amp; Proutiere , 2014 ) for a proof. A direct con-sequence of Theorem 4.2 is the asymptotic optimality of OSUB in the case of Bernoulli rewards: Corollary 4.4 Assume that rewards distributions are Bernoulli (i.e for any k ,  X  k  X  Bernoulli (  X  k ) ), and that  X   X   X  G . Then the regret achieved under  X  =OSUB sat-isfies: lim sup T  X  +  X  R  X  ( T ) / log( T )  X  c (  X  ) . We now consider time-varying environments. We assume that the expected reward of each arm varies smoothly over time, i.e., it is Lipschitz continuous: for all n, n  X   X  1 and 1  X  k  X  K : |  X  k ( n )  X   X  k ( n  X  ) | X   X  | n  X  n  X  | . We further assume that the unimodal structure is preserved (with respect to the same graph G ): for all n  X  1 ,  X  ( n )  X  U G . Considering smoothly varying rewards is more challenging than scenarios where the environment is abruptly changing. The difficulty stems from the fact that the rewards of two or more arms may become arbitrarily close to each other (this happens each time the optimal arm changes), and in such situations, regret is difficult to con-trol. To get a chance to design an algorithm that efficiently tracks the best arm, we need to make some assumption to limit the proportion of time when the separation of arms becomes too small. Define for T  X  N , and  X  &gt; 0 : Assumption 1 There exists a function  X  and  X  0 such that for all  X  &lt;  X  0 : lim sup T  X  +  X  H ( X  , T ) /T  X   X ( K ) X  . 5.1. OSUB with a Sliding Window To cope with the changing environment, we modify the OSUB algorithm, so that decisions are based on past choices and observations over a time-window of fixed du-ration equal to  X  + 1 rounds. The idea of adding a sliding window to algorithms initially designed for stationary en-vironments is not novel ( Garivier &amp; Moulines , 2008 ); but here, the unimodal structure and the smooth evolution of rewards make the regret analysis more challenging. Define: t  X  k ( n ) = P n t = n  X   X  1 { k ( t ) = k } ;  X   X  (1 /t  X  k ( n )) P n t = n  X   X  1 { k ( t ) = k } X k ( t ) if t  X   X  ( n ) = 0 otherwise; L  X  ( n ) = arg max 1  X  k  X  K  X   X   X  k l ( n ) = P n t = n  X   X  1 { L  X  ( t ) = k } . The in-dex of arm k at time n then becomes: b  X  k ( n ) = OSUB is presented below.
 Algorithm SW-OSUB Input: graph G = ( V, E ) , window size  X  + 1 For n  X  1 , select the arm k ( n ) where: k ( n ) = 5.2. Regret Analysis In non-stationary environments, achieving sublinear regr ets is often not possible. In ( Garivier &amp; Moulines , 2008 ), the environment is subject to abrupt changes or breakpoints. It is shown that if the density of breakpoints is strictly pos-itive, which typically holds in practice, then the regret of any algorithm has to scale linearly with time. We are inter-ested in similar scenarios, and consider smoothly varying environments where the number of times the optimal arm changes has a positive density. The next theorem provides an upper bound of the regret per unit of time achieved un-der SW-OSUB. This bound holds for any non-stationary environment with  X  -Lipschitz rewards.
 Theorem 5.1 Let  X  : 2  X   X  &lt;  X  &lt;  X  0 . Assume that for any n  X  1 ,  X  ( n )  X  U G and  X   X  ( n )  X  [ a, 1  X  a ] for some a &gt; 0 . Further suppose that  X  k (  X  ) is  X  -Lipschitz for any k . The regret per unit time under  X  = SW-OSUB with a sliding window of size  X  + 1 satisfies: if a &gt;  X  X  , then for any T  X  1 , R  X  ( T ) where C 1 , C 2 are positive constants and g 0 = ( a  X   X  X  )(1  X  a +  X  X  ) / 2 .
 Corollary 5.2 Assume that for any n  X  1 ,  X  ( n )  X  X  G and  X  ( n )  X  [ a, 1  X  a ] for some a &gt; 0 , and that  X  k (  X  ) is  X  -Lipschitz for any k . Set  X  =  X   X  3 / 4 log(1 / X  ) / 8 . The regret per unit of time of  X  = SW-OSUB with window size  X  + 1 satisfies: lim sup for some constant C &gt; 0 , and some function j such that These results state that the regret per unit of time achieved under SW-OSUB decreases and actually vanishes when the speed at which expected rewards evolve decreases to 0. Also observe that the dependence of this regret bound in the number of arms is typically mild (in many practical sce-narios,  X ( K ) may actually not depend on K ).
 The proof of Theorem 5.1 relies on the same types of ar-guments as those used in stationary environments. To es-tablish the regret upper bound, we need to evaluate the per-formance of the KL-UCB algorithm in non-stationary en-vironments (the result and the corresponding analysis are presented in ( Combes &amp; Proutiere , 2014 )). In this section, we briefly discuss the case where the de-cision space is continuous. The set of arms is [0 , 1] , and the expected reward function  X  : [0 , 1]  X  R is assumed to be Lipschitz continuous, and unimodal: there exists x  X   X  [0 , 1] such that  X  ( x  X  )  X   X  ( x ) if x  X   X  [ x, x x  X  [ x  X  , x ] . Let  X   X  =  X  ( x  X  ) denote the highest expected reward. A decision rule selects at any round n  X  1 an arm x and observes the corresponding reward X ( x, n ) . For any x  X  [0 , 1] , ( X ( x, n )) n  X  1 is an i.i.d. sequence. We make the following additional assumption on function  X  . Assumption 2 There exists  X  0 &gt; 0 such that (i) for all x, y in [ x  X  , x  X  +  X  0 ] (or in [ x  X   X   X  0 , x  X  ] ), C 1 | x  X  y |  X  ( y ) | ; (ii) for  X   X   X  0 , if | x  X  x  X  | X   X  , then |  X  ( x C This assumption is more general than that used in ( Yu &amp; Mannor , 2011 ). In particular it holds for functions with a plateau and a peak :  X  ( x ) = max(1  X  X  x  X  x  X  | / X , 0) . Now as for the case of a discrete set of arms, we de-note by  X  the set of possible decision rules, and the re-gret achieved under rule  X   X   X  up to time T is: R  X  ( T ) = under  X  at time n .
 There is no known precise asymptotic lower bound for con-tinuous bandits. However, we know that for our problem, the regret must be at least of the order of O ( logarithmic factor. In ( Yu &amp; Mannor , 2011 ), the authors show that the LSE algorithm achieves a regret scaling as O (  X  show that combining discretization and the UCB algorithm as initially proposed in ( Kleinberg , 2004 ) yields lower re-grets than LSE in practice (see Section 7 ), and is order-optimal, i.e., the regret grows as O ( For  X  &gt; 0 , we define a discrete bandit problem with K =  X  1 / X   X  arms, and where the rewards of k -th arm are distributed as X (( k  X  1) / X , n ) . The expected reward of the k -th arm is  X  k =  X  (( k  X  1) / X  ) . Let  X  be an algorithm run-ning on this discrete bandit problem. The regret of  X  for the initial continuous bandit problem is at time T : R UCB(  X  ) the UCB algorithm ( Auer et al. , 2002 ) applied to the discretized bandit. In the following proposition, we show that when  X  = (log( T ) / optimal. In practice, one may not know the time horizon T in advance. In this case, using the  X  X oubling trick X  (see e.g . ( Cesa-Bianchi &amp; Lugosi , 2006 )) would incur an additional logarithmic multiplicative factor in the regret. Proposition 1 Consider a unimodal bandit on [0 , 1] with rewards in [0 , 1] and satisfying Assumption 2 . Set  X  = (log( T ) / 7.1. Discrete bandits We compare the performance of our algorithm to that of KL-UCB ( Capp  X e et al. , 2013 ), LSE ( Yu &amp; Mannor , 2011 ), UCB ( Auer et al. , 2002 ), and UCB-U. The latter algorithm is obtained by applying UCB restricted to the arms which are adjacent to the current leader as in OSUB. We add the prefix  X  X W X  to refer to Sliding Window versions of these algorithms.
 Stationary environments. In our first experiment, we con-sider K = 17 arms with Bernoulli rewards of respec-wards are unimodal (the graph G is simply a line). The regret achieved under the various algorithms is presented in Figure 1 and Table 1 . The parameters in LSE algorithm are chosen as suggested in Proposition 4.5 ( Yu &amp; Mannor , 2011 ). Regrets are calculated averaging over 50 indepen-dent runs. OSUB significantly outperforms all other algo-rithms. The regret achieved under LSE is not presented in Figure 1 , because it is typically much larger than that of other algorithms. This poor performance can be explained by the non-adaptive nature of LSE, as already discussed earlier. LSE can beat UCB when the number of arms is not negligible compared to the time horizon (e.g. in Fig-ure 4 in ( Yu &amp; Mannor , 2011 ), K = 250 . 000 and the time horizon is less than 3 K ): in such scenarios, UCB-like algo-rithms perform poorly because of their initialization phas e (all arms have to be tested once).
 In Figure 2 , the number of arms is 129, and the expected re-wards form a triangular shape as in the previous example, with minimum and maximum equal to 0.1 and 0.9, respec-tively. Similar observations as in the case of 17 arms can be made. We deliberately restrict the plot to small time hori-zons: this corresponds to scenarios where LSE can perform well.
 Non-stationary environments. We now investigate the per-formance of SW-OSUB in a slowly varying environment. There are K = 10 arms whose expected rewards form a moving triangle: for k = 1 , . . . , K ,  X  k ( n ) = ( K  X  1) /K  X  | w ( n )  X  k | /K , where w ( n ) = 1+( K  X  1)(1+sin( n X  )) / 2 . Figure 3 presents the regret as a function of time under var-ious algorithms when the speed at which the environment evolves is  X  = 10  X  3 . The window size are set as follows for the various algorithms:  X  =  X   X  4 / 5 for SW-UCB and SW-KL-UCB (the rationale for this choice is explained in ( Combes &amp; Proutiere , 2014 )),  X  =  X   X  3 / 4 log(1 / X  ) / 8 for SW-UCB-U and OSUB. In Figure 4 , we show how the speed  X  impacts the regret per time unit. SW-OSUB pro-vides the most efficient way of tracking the optimal arm. 7.2. Continuous bandits In Figure 5 , we compare the performance of the LSE and UCB(  X  ) algorithms when the set of arms is continuous. The expected rewards form a triangle:  X  ( x ) = 1 / 2  X  X  x  X  1 / 2 | so that  X   X  = 1 / 2 and x  X  = 1 / 2 . The parameters used in LSE are those given in ( Yu &amp; Mannor , 2011 ), whereas the discretization parameter  X  in UCB(  X  ) is set to  X  = log( T ) / time: an appropriate discretization of continuous bandit problems might actually be more efficient than other meth-ods based on ideas taken from classical optimization the-ory.
 Figure 6 compares the regret of the discrete version of LSE (with optimized parameters), and of OSUB as the number of arms K grows large, T = 50 , 000 . The average rewards of arms are extracted from the triangle used in the contin-uous bandit, and we also provide the regret achieved under UCB(  X  ). OSUB outperforms UCB(  X  ) even if the number of arms gets as large as 7500! OSUB also beats LSE unless the number of arms gets bigger than 0 . 6  X  T . In this paper, we address stochastic bandit problems with a unimodal structure, and a finite set of arms. We provide asymptotic regret lower bounds for these problems and de-sign an algorithm that asymptotically achieves the lowest regret possible. Hence our algorithm optimally exploits th e unimodal structure of the problem. Our preliminary anal-ysis of the continuous version of this bandit problem sug-gests that when the number of arms become very large and comparable to the time horizon, it might be wiser to prune the set of arms before actually running any algorithm. Agrawal, R. The continuum-armed bandit problem. SIAM
J. Control and Optimization , 33(6):1926 X 1951, Novem-ber 1995.
 Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite time anal -ysis of the multiarmed bandit problem. Machine Learn-ing , 47(2-3):235 X 256, 2002.
 B., Edelman. Strategic bidder behavior in sponsored search auctions. In Proc. of Workshop on Sponsored
Search Auctions, ACM Electronic Commerce , pp. 192 X  198, 2005.
 Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit prob-lems. Foundations and Trends in Machine Learning , 5 (1):1 X 122, 2012.
 Bubeck, S., Munos, R., Stoltz, G., and Szepesv  X ari, C. On-line optimization in x-armed bandits. In Advances in Neural Information Processing Systems 22 , 2008. Capp  X e, O., Garivier, A., Maillard, O., Munos, R., and
Stoltz, G. Kullback-leibler upper confidence bounds for optimal sequential allocation. Annals of Statistics , 41(3): 516 X 541, June 2013.
 Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and Games . Cambridge University Press, 2006.
 Combes, R. and Proutiere, A. Unimodal bandits: Regret lower bounds and optimal algorithms. Technical Report, people.kth.se/  X alepro/pdf/tr-icml2014.pdf, 2014. Cope, E. W. Regret and convergence bounds for a class of continuum-armed bandit problems. IEEE Trans. Au-tomat. Contr. , 54(6):1243 X 1253, 2009.
 Dani, V., Hayes, T. P., and Kakade, S. M. Stochastic linear optimization under bandit feedback. In Proc. of Confer-ence On Learning Theory (COLT) , pp. 355 X 366, 2008. Flaxman, A., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: gradient de-scent without a gradient. In Proc. of ACM/SIAM sym-posium on Discrete Algorithms (SODA) , pp. 385 X 394, 2005.
 Garivier, A. and Capp  X e, O. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proc. of Con-ference On Learning Theory (COLT) , 2011.
 Garivier, A. and Moulines, E. On upper-confidence bound policies for non-stationary bandit problems. In Proc. of Algorithmic Learning Theory (ALT) , 2008.
 Gittins, J.C. Bandit Processes and Dynamic Allocation In-dices . John Wiley, 1989.
 Graves, T. L. and Lai, T. L. Asymptotically efficient adaptive choice of control laws in controlled markov chains. SIAM J. Control and Optimization , 35(3):715 X  743, 1997.
 Hartland, C., Baskiotis, N., Gelly, S., Teytaud, O., and
Sebag, M. Change point detection and meta-bandits for online learning in dynamic environments. In Proc. of conf  X  erence francophone sur l X  X pprentissage automa-tique (CAp07) , 2007.
 Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed ban-dits in metric spaces. In Proc. of the 40th annual ACM
Symposium on Theory of Computing (STOC) , pp. 681 X  690, 2008.
 Kleinberg, R. D. Nearly tight bounds for the continuum-armed bandit problem. In Proc. of the conference on Neural Information Processing Systems (NIPS) , 2004. Lai, T. L. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics , 15(3): 1091 X 1114, 09 1987.
 Lai, T.L. and Robbins, H. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics , 6(1): 4 X 2, 1985.
 Robbins, H. Some aspects of the sequential design of ex-periments. Bulletin of the American Mathematical Soci-ety , 58(5):527 X 535, 1952.
 Slivkins, A. and Upfal, E. Adapting to a changing environ-ment: the brownian restless bandits. In Proc. of Confer-ence On Learning Theory (COLT) , pp. 343 X 354, 2008. Yu, J. and Mannor, S. Unimodal bandits. In Proc. of Inter-national Conference on Machine Learning (ICML) , pp.
