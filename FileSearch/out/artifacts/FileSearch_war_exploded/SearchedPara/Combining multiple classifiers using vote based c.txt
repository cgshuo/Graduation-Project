 1. Introduction monetary expressions, etc.) and  X  none-of-the-above".

The main approaches to NER can be grouped into three main categories, namely rule-based, machine learning based and and languages.
 approaches used in NER are Hidden Markov Model (HMM) [10,11] , Maximum Entropy (ME) [12,13] , Decision Tree [14,15] , develop new methods using strongest points from each one.

Besides the above mentioned works, there are handsome amounts of other existing works too. Majority of the languages the required measure. There have been some initiatives [21 important research question. In this paper we first pose this classifier ensemble (i.e., the first version determine whether a particular classifier is allowed to vote for a particular class or not. two different solutions based on single and multiobjective optimization techniques. optimization (MOO) [27] that is capable of simultaneously optimizing more than one classification quality measure. The proposed approaches are evaluated for NER in three resource-constrained Indian languages such as Bengali, Hindi and Learner (MBL) [30], Hidden Markov Model (HMM) [11], Maximum Entropy (ME) [12], Conditional Random Field (CRF) [17] and single and multiobjective based methods are applied to construct an ensemble system. Instead of searching for the best operators are used to accelerate the convergence of GA. For both the approaches, we also use elitism.
We use the NE-annotated 250 K word forms of the Bengali news corpus [33]. This was annotated with a coarse-grained NE also use the IJCNLP-08 NER on South and South East Asian Languages (NERSSEAL) objective optimization based classifier ensemble techniques for all the languages.
The related works on ensembles for NER are reported in [34 the objective functions are exploited.

The main contributions of the present paper are listed below: 1. We use Naive Bayes, Decision Tree, Memory Based Learner, HMM, ME, CRF and SVM as the base classifiers. However, the proposed technique is a very general approach and its performance may further improve depending upon the choice and/or the number of classifiers as well as the use of more complex features. 2. Single and multiobjective genetic algorithm based techniques are developed for determining either appropriate voting ensemble is developed. relatively resource-poor languages like Bengali, Hindi and Telugu. domain of NLP, especially in NER is also new. (e.g. [22,26] , etc.) can be further improved with our proposed frameworks. the nature of problems or the requirement of the users, appropriate solutions can be selected.
Section 4 presents our proposed approach. The problems of NER under the various machine learning frameworks are briefly roadmaps in Section 8 . 2. Optimization techniques a popular multiobjective technique named Non-dominated Sorting Genetic Algorithm (NSGA)-II [32]. 2.1. Overview of genetic algorithm shown in Fig. 1 . 2.2. Multiobjective algorithms
The multiobjective optimization (MOO) can be formally stated as follows [27]. Find the vectors x  X  x variables that simultaneously optimize the M objective values f
An important concept of MOO is that of domination. In the context of a maximization problem, a solution x x if  X  k  X  1 ; 2 ; ... ; M ; f k x i  X  X   X  f k x j and  X  k  X 
Among a set of solutions P , the nondominated set of solutions P set of solutions that are not dominated by any solution encountered by it. 2.2.1. Nondominated Sorting Genetic Algorithm-II (NSGA-II) which is equal to its non-domination level in the partial order. A child population Q t iteration, a combined population R t = P t + Q t is formed. The size of R non-domination. If the total number of solutions belonging to the best nondominated set F included in P (t+1) . The remaining members of the population P The pseudocode of NSGA-II is provided in Fig. 2 .
 3. Problem formulation and multiobjective optimization frameworks. 3.1. Single objective formulation of classi fi er ensemble problem
Let, the N number of available classifiers be denoted by C classes. The classifier ensemble problem is then stated as follows: (binary vote based ensemble) of size N  X  M or a real array (real/weighted vote based ensemble) of size N  X  M .
In the case of Boolean array: V ( i , j ) denotes the decision whether i denotes that the i th classifier is allowed to vote for the j vote for the j th class.

In the case of real array: V ( i , j ) denotes the weight of vote of the i given less weight. V ( i , j )  X  [0,1] denotes the degree of confidence of the i classifiers.
 precision and F-measure. Thus, F  X  {recall, precision, F-measure}. Here, we choose F =F-measure, which is a combination (harmonic mean) of both recall and precision. 3.2. Multiobjective formulation of classi fi er ensemble problem The multiobjective formulation of classifier ensemble problem is as follows:
Find the combination of votes per classifier V such that: maximize [ F 1 ( B ), F 2 ( B )] where F 1 , F 2  X  {recall, precision, F-measure} and F based ensemble) or a real array (real vote based ensemble) of size N  X  M . We choose F 3.2.1. Selection of objectives
Performance of MOO largely depends on the choice of the objective functions which should be as much contradictory as below: real vote based MOO approach. This again supports the contradictory nature of these two objective functions. below: information on the problem is needed. 4. Proposed approach proposed in [35]. 4.1. String representation and population initialization the chromosome is M  X  O . Each chromosome encodes the weights of votes for possible O classes for each classifier. M =3 and O =4 (i.e., a total of 12 votes can be possible). The chromosome represents the following voting combination: Classifier 1 is allowed to vote for classes 1 and 4; Classifier 2 is allowed to vote for classes 1 and 2; Classifier 3 is allowed to vote for classes 2, 3 and 4.

The entries of each chromosome are randomly initialized to either 0 or 1. Here, if the i represents that ( i /4+1) th classifier is not allowed to vote for the ( i mod 4) population are initialized as discussed above.
Fig. 5 . Here, M =3 and O =3 (i.e., a total of 9 votes can be possible). The chromosome represents the following voting combination: classifier 3.
 discussed above. 4.2. Fitness computation execute the following steps to compute the objective values. be F i , i =1 ... M , respectively. and i th class. The combined score of a particular class for a particular word w is:
Here, op ( w , m ) denotes the output class provided by the m combined score is selected as the joint decision. objective approach, we use F-measure value as the objective function, i.e. f the objective functions corresponding to a particular chromosome are f maximize these two objective functions using the search capability of NSGA-II. 4.3. Genetic operators used for single objective optimization based approach ensemble. 4.3.1. Selection a probability of selection with each individual chromosome. If f being selected is where, N is the number of individuals in the population.
 still a chance that they may be. 4.3.2. Crossover Here, we use the normal single point crossover [39]. As an example, let the two chromosomes be: P1 : 0.24 0.16 0.54 0.87 0.66 0.76 0.01 0.88 0.21 P2 : 0.12 0.09 0.89 0.71 0.65 0.82 0.69 0.43 0.15 number between 1 and 9. Let the crossover point, here, be 4. Then after crossover, the two new offsprings are: O1: 0.24 0.16 0.54 0.87 0.65 0.82 0.69 0.43 0.15 (taking the first 4 positions from P1 and rest from P2) O2: 0.12 0.09 0.89 0.71 0.66 0.76 0.01 0.88 0.21 (taking the first 4 positions from P2 and rest from P1) is calculated as:
Here, as in [40], the values of k 1 and k 3 are kept equal to 1.0. Note that, when f  X  is low so as to reduce the likelihood of disrupting a good solution by crossover. 4.3.3. Mutation
Each chromosome undergoes mutation with a probability  X  m chromosome as in [40]. The expression for mutation probability, converges to a local optimum, i.e. when f max  X  f decreases, optimum. The use of elitism will also keep the best solution intact. For a solution with the maximum fitness value,
In the case of binary encoding, we apply mutation operator to each entry of the chromosome where the entry is randomly replaced by either 0 or 1. But in the case of real encoding, each position in a chromosome is mutated with probability following way. The value is replaced with a random variable drawn from a Laplacian distribution , p  X   X  probability of generating a value near the old value is more. 4.3.4. Termination condition population. Thus on termination, this location contains the best classifier ensemble. 4.4. Genetic operators used for MOO based approach non-dominated solutions [27] among the parent and child populations are propagated to the next generation. The near-Pareto-optimal strings of the last generation provide the different solutions to the ensemble problem. 4.5. Selection of a solution from the fi nal Pareto optimal front in MOO based approach solution from a set of solutions is now developed.
 selecting a solution from the final Pareto optimal front. 5. Base classi fi ers for NER system based on weighted voting. Brief descriptions of these classifiers are presented below. several domains including pattern recognition, NLP, information retrieval, etc. [42 to the class c k [41]. Here at first the probability Pc k estimation of Pc k x  X  is impossible in most cases.
 Px  X  c k can be decomposed by assuming the conditional independence of the elements of a vector as follows, where x j is the j th element of vector x  X  . Then the equation becomes: hold in many practical cases. However due to its simplicity it has been applied in many tasks including NLP [44].
The experiments were conducted using the WEKA toolkit [46]. This provides implementations of several machine learning and presentation of results. 5.1. Decision Tree of a target variable based on several input variables. A tree root to the leaf.
 information entropy, decision trees are built from a set of training data in C4.5. Let the training set be S = s some already classified samples. Each sample s i = x 1 , x training data another vector is available C = c 1 , c 2 , normalized information gain is selected for decision making. The C4.5 algorithm then recurs in the smaller sublists. parameter tuning. 5.2. Memory Based Learning tasks.
 as it adds new training samples to memory. This memory is known as two main components. The first component has n feature-value pairs, whereas the second component is known as the stored examples Y is computed using some distance metric  X  subsequently it is assigned the most frequent category. Tie breaking resolution is deployed in case of ties.
We use a memory based tagger (MBT) [48] which makes use of TiMBL, an open source implementation of memory based metric with gain ratio feature weighting is used. Number of nearest neighbors (i.e. k) is set to 1. 5.3. Hidden Markov Model based named entity tagging
The goal of NER is to find a stochastic optimal tag sequence T = t dependent only on a small, fixed number of previous NE tags. Here, in this work, a trigram model has been used. So, the probability of a NE tag depends on two previous tags, and then we have, equation looks as:
Due to sparse data problem, the linear interpolation method is used to smooth the trigram probabilities as follows: P  X 
P ( t n )+  X  2 P ( t n | t n  X  1 )+  X  3 P ( t n | t n  X  2 [50]: 1. set  X  1 =  X  2 =  X  3 =0 2. for each trigram ( t 1 , t 2 , t 3 ) with freq ( t 1 , t 3. normalize  X  1 ,  X  2 ,  X  3 .
 is 0, then the result of that expression is defined to be 0. The taking unseen data into account.
 calculated as: P ( W | T )  X  P ( w 1 | t 1 ) X  P ( w 2 | t
The emission probabilities in the above equation can be calculated from the training set as, Pw 5.3.1. Context dependency
To make the Markov model more powerful, additional context dependent features were introduced to the emission assigned to the current word. Now, P ( W | T ) is calculated by the equation: So, the emission probability can be calculated as: calculated as:
The values of  X  s should be different for different words. But the calculation of hence  X  s are calculated for the entire training corpus. In general, the values of adopted in calculating  X  s. 5.3.2. Viterbi algorithm work. The pseudo code of the algorithm is shown bellow. for i =1 to Number_of_Words_in_Sentence sequence going to state c at time i . end
So if every word can have S possible tags, then the Viterbi algorithm runs in O ( S length of the sentence. 5.3.3. Handling the unknown words
For words which have not been seen in the training set, P ( w symbols are also considered.

For Bengali, to handle the unknown words further, a lexicon [51], which was developed in an unsupervised way from the heuristic is that  X  if an unknown word is found to appear in the lexicon, then most likely it is not a named entity 5.4. Maximum entropy framework for NER with the maximum likelihood distribution, and has the exponential form where, t is the NE tag, h is the context (or history), f j
The problem of NER can be formally stated as follows. Given a sequence of words w sequence of NE tags t 1 , ... , t n , drawn from a set of tags T , which satisfies: where, h i is the context for the word w i .
 example:
We use the OpenNLP Java based MaxEnt package 6 for the computation of the values of the parameters weights to the features. We use the Generalized Iterative Scaling [52] algorithm to estimate the MaxEnt parameters. 5.5. Conditional Random Field framework for NER input nodes. The conditional probability of a state sequence s = calculated as: where, f k ( s t  X  1 , s t , o , t ) is a feature function whose weight normalization factor, which as in HMMs, can be obtained efficiently by dynamic programming.

To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequences: where { b o ( i ) , s ( i ) &gt;} is the labeled training data. The second sum corresponds to a zero-mean, parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set parameters maximize the penalized log-likelihood using Limited-memory BFGS [53], a quasi-Newton method that is significantly more efficient, and which results in only minor changes in accuracy due to changes in sequence is its corresponding label sequence.

A feature function f k ( s t  X  1 , s t , o , t ) has a value of 0 for most cases and is only set to be 1, when s observation has certain properties. We have used the C ++ implementation of CRF for segmenting or labeling sequential data. 5.6. Support Vector Machine framework for NER we have a set of training data for a two-class problem: {( x the training data and y  X  {+1,  X  1} is the class to which x positive and negative examples by the hyperplane written as:
SVMs find the  X  optimal  X  hyperplane (optimal parameter w stands for the distance of the hyperplane to the origin. The classification rule of a SVM is: example):
The SVM model has an equivalent dual formulation, characterized by a weight vector support vectors . The dual classification rule is:
The  X  vector can be calculated also as a quadratic optimization problem. Given the optimal optimization problem, the weight vector w  X  that realizes the maximal margin hyperplane is calculated as:
The b  X  has also a simple expression in terms of w  X  and the training examples ( x dimension of the real feature space can be very high or even infinite.

By simply substituting every dot product of x i and x j in dual form with any kernel function K ( x hypotheses. Among the many kinds of kernel functions available, we will focus on the d -th polynomial kernel : combination of features up to d .
We have used YamCha 8 toolkit, an SVM based tool for detecting classes in documents and formulating the NER task as a
TinySVM-0.07 9 classifier. 6. Named entity features
The main features for the NER task are identified and selected mostly without using any deep domain knowledge and/or Decision Tree, Memory based Learner, Hidden Markov Model, Maximum Entropy, Conditional Random Field and Support Vector that surrounding words carry effective information for the identification of NEs. characters of the word  X  ObAmA  X  [Obama] are  X  A  X  ,  X  mA notation. 10 The prefixes of length up to 3 characters of the word included with the observation that NEs share some common suffixes and/or prefixes. for Bengali as NEs generally appear in the first position of the sentences in news-wire data. 4. Length of the word: This binary valued feature checks whether the number of characters in a token is less than a probably not the NEs.
NEs. distinguishes NEs from the verbs. token. These features are digitComma (token contains digit and comma), digitPercentage (token contains digit and contains digit and hyphen) and digitFour (token consists of four digits only). the combination of the output labels of the current and previous tokens. 9. Content words in surrounding contexts: We consider all unigrams in contexts w boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation feature corresponding to token t is set at 1 if and only if the context w 7. Experimental setup, datasets and evaluation results
In this section, we report the details of experimental setup, datasets of experiments and the evaluation results. 7.1. Experimental setup
For GA, the following parameter values are used: population size=50, number of generations=40. The mutation and
The proposed approaches are compared with three different baseline ensemble systems, which are defined as below:  X  voting of the output class labels. If all the outputs differ then any one is selected randomly.  X  weighted vote.  X  weight of any classifier is set at the average F-measure value of the corresponding class, assigned by it. 7.2. Datasets for NER NER on South and South East Asian Languages (NERSSEAL) 11
NE only. For example, mahatmA gAndhi roDa (Mahatma Gandhi Road) was annotated as location and assigned the tag mahatmA (Mahatma) and gAndhi (Gandhi) are NE title person (NETP) and person name (NEP), respectively. The task was to categories NETP (Title person) and NEP (Person name), respectively.
 (NEO), number expressions (NEN), time expressions (NETI) and measurement expressions (NEM). The NEN, NETI and NEM tags are mapped to the MISC class that denotes miscellaneous entities. Other classes of the shared task are mapped to the  X  other-than-NE  X  category, denoted by  X  O  X  . Hence, the tagset mapping now becomes as shown in Table 2 .
In order to properly denote the boundaries of NEs, four basic NE tags are further divided into the format I-TYPE (TYPE gAndhi [Mahatma Gandhi] is tagged as mahatmA [Mahatma]/I-PER gAndhi [Gandhi]/I-PER. But, the names mahatmA gAndhi [Mahatma Gandhi] ect rabIndrAnAth thAkur [Rabindranath Tagore] are to be tagged as: mahatmA [Mahatma]/I-PER gAndhi standard IOB format that was followed in the CoNLL-2003 shared task [56]. and test data are presented in Table 3 . 7.3. Evaluation results and discussion from the following set of features:
Various context window within the previous three and next three words, i.e. w
Thereafter, we apply our proposed single objective GA and multiobjective NSGA-II based approaches to determine the real vote based approaches perform better than the binary vote based approaches for both the single and multiobjective an improvement of 1.27 percentage F-measure points compared to the binary coded GA.
The MOO with real vote based approach shows the highest performance with the recall, precision and F-measure values of coded MOO.
 based classifier ensemble techniques really outperform the best individual classifier, three baseline ensembles and the that MOO based techniques truly perform better than the corresponding single objective GA based techniques. respectively. The corresponding final Pareto optimal front is also shown in Fig. 6 . Evaluation results of these two classifiers are shown in Table 8 .
 these available classifiers. Overall evaluation results are presented in Table 9 .
The binary coded GA based approach attains the recall, precision and F-measure values of 89.72%, 91.25% and 90.48%, objective with binary vote based classifier ensemble techniques, respectively. the corresponding single objective GA based ensembles.
 optimal front is also shown in Fig. 3 .
 approach with more than 3.36 percentage F-measure point. The MOO based approach also shows better performance over the corresponding single objective version. 7.4. Summary of results framework [35] can be more effective in comparison with the binary vote based approach under MOO framework. 8. Conclusion and future works
We have presented two different versions of the classifier ensemble problem under each of the single and multiobjective without using any domain knowledge and/or language specific resources. The algorithms have been evaluated for three resource-constrained languages, namely Bengali, Hindi and Telugu. The proposed techniques achieve the state-of-the-art techniques perform superiorly to the single objective based methods.
 evaluate the proposed techniques for some other Indian languages, English and biomedical domain.
References
