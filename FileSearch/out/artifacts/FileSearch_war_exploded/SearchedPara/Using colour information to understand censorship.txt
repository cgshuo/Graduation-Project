 ORIGINAL PAPER Oronzo Altamura  X  Margherita Berardi  X  Michelangelo Ceci  X  Donato Malerba  X  Antonio Varlaro Abstract Many European film archives are involved in the digitization of 20th century historical paper doc-uments. In the context of the IST project COLLATE three of them were interested in the semi-automatic annotation of censorship cards and their subsequent re-trieval on the basis of both annotations and content. Processing censorship cards, which is the main subject of this paper, leads to a number of challenges for many document image analysis (DIA) systems. Problems arise due to the low layout quality and standard of such material, which introduces a considerable amount of noise in its description. The layout quality is often neg-atively affected by the presence of stamps, signatures, ink specks, manual annotations and so on that overlap those layout components involved in the understand-ing or annotation processes. In order to effectively re-duce the presence and the effect of noise, we propose an improved version of the knowledge-based DIA system WISDOM++ allowing it to take full advantage of the use of colour information in all processing steps: namely, im-age segmentation, layout analysis, document image clas-sification and understanding. Experiments have been conducted on a corpus of multi-format documents con-cerning rare historic film censorships provided by the three film archives involved in the COLLATE project. Keywords Historical paper documents  X  Color image segmentation  X  Inductive learning from examples 1 Introduction Nowadays the governments of many countries are interested in the valorisation of cultural heritage, since it is widely recognized that cultural heritage resources have significant implications for development (both as a knowledge basis and in terms of commercial exploita-tion). For this aim, many institutions which collect and preserve cultural heritage have shown a great interest in the digitalization of their resources and in the exploita-tion of mechanisms to provide online access to digital-ized products.
 Accordingtothedefinitionreportedinthe1972UNE-SCO  X  X orld Heritage Convention X  -Article 1-cultural heritage refers to  X  X onuments X ,  X  X roups of buildings X  and  X  X ites X  which are of outstanding universal value his-torically, artistically or scientifically. However, the con-cept of cultural heritage has recently assumed a broader connotation and includes, among other things, tangible, moveable objects such as works of art, artefacts, sci-entific specimens, photographs, books, manuscripts and recorded moving image and sound.

Historical documents are a kind of cultural heritage for which several research projects have recently been promoted for the purposes of preservation, storage, indexing and on-line fruition.
One of the first EU-funded projects is MASTER [17] that has developed a standard for computer-readable descriptions of medieval manuscripts. The specification of metadata (e.g. coloured drop capitals, original text, text from the second copyist, illustrations) has led to the development of a document image analysis (DIA) sys-tem that localizes all connected components and then classifies them according to the features selected by users. MEMORIAL [3] is another EU-funded project whose goal is the establishment of a digital document workbench enabling the creation of distributed virtual archives of typewritten documents related to prison-ers in World-War II concentration camps. Templates of physical and logical entities on a document guide the application of the OCR to specific locations in the im-age, thus producing a  X  X illed X  document structure (XML file). Two national projects strictly related to DIA are Bovary [28] and D-SCRIBE [10,11]. The former con-cerns the digitalization of 5,000 original manuscripts handwritten by the French writer Gustave Flaubert. The non-linear arrangement of the text and numerous edi-torial marks define a complex structure layout whose extraction is performed by a set of hand-coded lay-out rules. D-SCRIBE aims to develop an integrated system for digitization and processing of Old Greek manuscripts. Binarization of original grey-scale images [11] and character ligature detection are the two main challenges.

This paper presents some issues encountered and problems addressed in the EU-funded project COL-LATE (Collaboratory for annotation, indexing and re-trieval of digitized historical archive material), whose main goal is that of providing professional users oper-ating in film archives with adequate access to historic film-related documents and their associated metadata [9]. The cultural material consists of digitized versions of documents on European films in the 1920s and 1930s. Such documents can be censorship documents, newspa-per articles, posters, advertisement material, registration cards and photos that cannot be used for access, indexing and retrieval as they are. Therefore, methods have to be applied to extract as much information from both textual and pictorial material and make them as machine-acces-sible as possible. This implies going beyond mere OCR techniques for textual documents, and applying  X  X ntel-ligent X  DIA techniques in order to extract both logical and semantic content.

In the framework of the COLLATE project we inves-tigatedtheapplicabilityof theDIA system WISDOM++ 1 [2] to a portion of the original collection of digitized document pages available in three national film archives, namely Deutsches Filminstitut (Germany), Filmarchiv Austria and N X rodn X  Filmov X  Archiv (Czech Repub-lic). Specifically, censorship cards that share a common layout and logical structure have been selected for our pilot study, since the document structures can be auto-matically built by means of machine learning methods, instead of being manually specified as in MEMORIAL. Indeed a distinctive feature of WISDOM++ is the exten-sive usage of machine learning tools in order to automat-ically extract useful knowledge from training examples.
WISDOM++ was originally developed to fully sup-port the transformation of multi-page printed docu-ments into XML format. Since most of information in the documents is typewritten, the system appeared to be a potentially useful tool for the conversion of scanned documents into XML format. Nevertheless, some prob-lems which arose in processing the available historical documents have shown that a more sophisticated ap-proach was required. The low layout quality and stan-dard of such material introduces a considerable amount of noise in its description. The layout quality is often negatively affected by both the degradation of the doc-uments and the presence of frames, stamps, signatures, ink specks and manual annotations that overlap those layout components involved in the understanding or annotation processes.

To effectively process these documents, it is neces-sary to exploit information conveyed by colour since signatures, stamps and manual annotations are often characterized by colours which are different from those present in the background and typewritten texts. Actu-ally, this consideration does not apply specifically to our application domain, but to historical documents in gen-eral, since colour information permits DIA systems to catch differences that, otherwise, cannot be adequately represented. First, it is possible to identify noise on the basis of colour homogeneity. This is particularly useful when the original document presents stains, tears and has an irregular accumulation of dirt due to repeated handling [4]. Second, it is possible to isolate and consider separately overlapped interesting blocks. This aspect is particularly interesting in legal documents, where blue stamps or revenue stamps often overlap signatures or typewritten text. Third, it is possible to isolate interest-ing blocks from uninteresting ones. This is particularly useful when there are manual annotations (typically of a different colour) that overlap those layout com-ponents involved in the understanding or annotation processes.

A na X ve approach to colour document image process-ing would be to separate different colours by means of a well-established colour quantization method and to pro-cess images corresponding to each colour separately, as if they were many independent black and white images. However, this approach is based on the simplified assumption that a logical component can be associated with a single colour. In practice, this assumption is rarely true. First, historical documents tend to deteriorate: pa-per colour darkens with age, while handwritten or typed printed parts tend to fade [25]. These two factors acting simultaneously narrow the discrimination gap between background and textual components. Second, when the document is written or typed on both sides, and the back side is visible from the front side, the system should be able to filter out the introduced noise. Third, the locality of components should be taken into account because some portions of a document page require different treatment with respect to others. For this reason, a more sophisticated approach is necessary.

In the literature, several colour segmentation algorithms have been proposed. They make use of sev-eral different clustering techniques such as  X  X istogram-based X  [38],  X  X uclidean Minimum Spanning Trees X  [39],  X  X uzzy c-means X  [5]. However, according to Cheng [7],  X  X he image segmentation is basically one of the psy-chophysical perception, therefore not susceptible to a purely analytical solution X . Therefore, there are many methods for image segmentation and each method is tailored for a particular kind of image and a particular kind of application [21]. We also observe that most of proposed methods only operate in colour space (typi-cally on the original RGB (red blue green) space repre-sentation [35]) and do not take any spatial information into account [31]. This turns out to be a severe limita-tion in document layout analysis, where colours should be associated to layout components. Notable exceptions are the works [34] and [13] on the identification of tex-tual components, and [14] on the identification of stamps in historical documents. Nevertheless, these works are based on the assumption that a layout component is associated to a single colour. On the other hand, in our domain, where not only textual components are of inter-est, it is necessary to provide the system with the capabil-ity to also identify multicolour logical components such as pictures or revenue stamps.

Two works that combine colour and spatial features to identify multicolour blocks are reported in [27] and [15]. In the former segmentation is performed by a fuzzy region growing method that aims at capturing varia-tions of the same colour. However, in our case, mul-ticolour logical components may be characterized by the presence of completely different colours. The work by Karatzas and Antonacopoulos faces the problem of text segmentation of web images, such as banners, head-ers and illustrations. The limited applicability to tex-tual components and the absence of problems typical of historical documents, justify the investigation of differ-ent solutions.

WISDOM++, originally developed to process black-and-white (binary) TIFF images, has been substantially extended to take full advantage of colour information in all processing steps, namely, image segmentation, lay-out analysis, document image classification and under-standing. In particular, colour quantization, obtained by standard libraries, is followed by a preliminary step that separates foreground from background colours. After-wards, the black/white projection of each foreground colour is generated and the classic RLSA segmentation algorithm is applied. Subsequently, a merging phase, based both on spatial and colourimetric measures is performed in order to group together blocks of differ-ent colours that should contribute to the identification of single layout components. Blocks are then classified according to the type of content and the layout analysis is performed to detect structures among blocks. Tak-ing into account the information on the colour, the re-sult of the layout analysis allows the system to identify overlapping components. A proper logic description of layout structures is generated and used to train a ma-chine learning system to classify and understand docu-ment images. In this step, colour information helps both to isolate noise and to identify better colour-dependent components of interest. Actually, the use of a richer rep-resentation also raises efficiency problems in the learn-ing phase, which are resolved by the integration of some caching techniques in the learning strategy.

All these aspects are explained in detail in the follow-ing sections, where the new document processing steps implemented in WISDOM++ are reported. The new lay-out analysis method is illustrated in Sect. 2, while the document understanding process is described in Sect. 3. The new system has been tested on a collection of multi-format censorship cards of historic films from the 20s and 30s. The goal is to evaluate the effectiveness of colour information in the complex process of layout analysis and document understanding. Results are reported and discussed in Sect. 4. Finally, conclusions are drawn in Sect. 5. 2 Colour-based layout analysis In WISDOM++ the paper document process is orga-nized in several steps:
WISDOM++ makes an extensive use of knowledge and XML technologies for semantic indexing of paper documents. This is a complex process involving several steps: 1. The image is segmented into basic layout compo-2. A perceptual organization phase (layout analysis) 3. The first page is classified to identify the member-4. The layout structure of each page is mapped into the 5. OCR is applied only to those logical components of
The result of the process is an XML file that repre-sents the layout structure, the logical structure, and the textual content returned by the OCR for some specific logical components.

Four out of five processing steps make use of explicit knowledge expressed in the form of decision trees and rules which are automatically learned by means of two distinct machine learning systems: ITI [36], which re-turns decision trees useful for block classification (first step), and ATRE [22], which returns rules for layout analysis correction (second step) [23], document image classification (third step) and document image under-standing (fourth step) [24].

In this section we focus our attention on the first two steps, while details on the subsequent two steps are re-ported in the next section. For the last processing step, an off-the-shelf OCR software has been integrated in the current version of WISDOM++ with the aim of read-ing texts from segments of the original colour image. Indeed, the usage of German and Czech dictionaries and the variety of fonts do not permit the application of experimental OCR programs.

The colour image segmentation algorithm imple-mented in WISDOM++ operates in three steps. First, a colour reduction is performed by means of the sim-ple octree-based clustering algorithm that performs a quantization of the colours [12]. Second, a colour merg-ing strategy is applied to further reduce the number of colours. Third, overlapping regions are merged together on the basis of spatial criteria. This multi-step approach is similar to those proposed by Tremeau et al. [35] and Lucchese and Mitra [20]: the main difference is that col-our homogeneity of spatially contiguous pixels is not the only criterion, since in our application it is also impor-tant to isolate multi-colour blocks.
 Algorithm 1 Top-level pseudocode of the segmentation algorithm
In Algorithm 1, a high-level description of the algo-rithm is reported. In the first phase, the quantization algorithm reduces the number of colours and the user can manually select background colours. Later on, the algorithm works on a List of binary images. Each binary image corresponds to a foreground colour. For each bi-nary image, 1 corresponds to the foreground colour and 0 corresponds to either the background colours or to the other foreground colours.

Each binary image is segmented by applying a variant of the run length smoothing algorithm (RLSA) [37], which scans the image only twice, with no addi-tional cost [33]. Furthermore, the smoothing parameters are adaptively defined [2]. The output of RLSA, when applied on each binary image, is a list of rectangular blocks ( BBSet ).

Once the set of basic blocks, for each binary im-age, has been extracted, the colourimetric merging is performed. This merging step aims to cluster colours associated to binary images on the basis of a colouri-metric distance measure. Binary images whose colours belong to the same cluster are removed from the List and replaced by the merging result.

The second merging step is performed on the updated list of binary images, taking images in pairs. Images are segmented again and the spatial merging of intersecting blocks is performed. The result is an updated list of both binary and multi-colour images.

Both merging steps take into account only the pixels contained in the set of basic blocks. Noise pixels that do not contribute to the identification of a basic block are ignored.

In the following subsections, the three steps of colour image segmentation as well as the layout anal-ysis procedure are described in detail. 2.1 The quantization process The quantization process follows the method proposed by Gervauz and Purgathofer [12], whose basic idea is to build a tree structure always containing a maximum of K different leaves, where each leaf corresponds to a colour.
 The tree represents the RGB colour space. In level i Bit i of each node the red, green and blue binary value is used as the selector for the successors. So, an inner node stores a triple of bits, one for each basic colour. Since each red, green and blue value is between 0 and 255, the maximum depth of the tree is eight and an internal node has at most 8 successors. The leaves keep information of the colour value, the colour index and a counter for the pixels that are already mapped into that leaf.
The image is read twice. The first time, colours are iteratively added to the tree. When the tree already con-tains K colours and a further colour has to be added to the tree, its colour value has to be merged with the most likely one that is already in the tree. Both values are substituted by their frequency-weighted mean and the counter is updated.

For our application domain, we set K = 16 since hu-mans normally do not  X  X ee X  more than nine colours in a censorship card. Obviously, human perception is biased from background knowledge on the different sources of colours (typewritten text, manual annotations, stamps, signatures). Nevertheless, this should be considered as a clear indication that a much finer discrimination among colours would be useless.

It is noteworthy that the tree structure depends on the sequence of the colours to be added. However, this aspect is marginal in our application, where the num-ber of selected colours is relatively small and frequent colours suddenly tend to prevail over others.
Quantization is performed at the second reading of the image. The colour values of each pixel are used for traversing the octree data structure and the search along a path through the tree is finished when a leaf node is reached. Thus, the K -colours image is transformed into K different binary images, each of which is related to a colour expressed in RGB colour space. Among the K colours, the user manually selects a subset of m back-ground colours. In practice, we observed that m ranges between two and three in our application domain be-cause of the various forms of degradation of the doc-uments. The exploration of the automatic selection of background colours is postponed for future research. 2.2 Colourimetric merging The list of K-m images is used in the merging phase. The first merging process is a colourimetric merging which aims to merge those binary images whose colours can be considered to be light variations. This is a necessary step, since the value K fixed a priori in the quantiza-tion process may turn out to be too large for a specific document.

The process is based on a hierarchical clustering algo-rithm. At each step, the dissimilarity between two clus-ters of colours (inter-cluster dissimilarity) is evaluated on the basis of two measures: (a) the lowest Euclid-ean distance between two colours taken from distinct clusters (nearest neighbour-based dissimilarity); (b) the Euclidean distance between the centroids of the two clusters (centroid-based dissimilarity). Two clusters of colours are merged when both computed measures are lower than a threshold. Clusters whose nearest neigh-bour-based dissimilarity is lowest are considered first. The threshold is automatically defined as the standard deviation value computed by considering all the distances between each colour of one cluster and each colour of another cluster. In extreme cases, all K-m non-background colours can be grouped together, or else none of them can be grouped. Finally, for each remain-ing cluster a new image representing the average colour of original images is generated.
 All distances are computed in the CIELab space. CI-ELab space is obtained by a non-linear transformation of the original RGB space. In CIELab space, the princi-pal dimension is luminosity and colours are disposed in two dimensions, such that green and red as well as blue and yellow are opposite. We used CIELab for a twofold reason: it takes hue, value and chroma into account and is considered  X  X isually uniform X  because adjacent col-our samples represent equal intervals of visual percep-tion [7]. In Fig. 1 the result of the colourimetric merging is shown. 2.3 Spatial merging The degree of overlapping of the layout extracted from different colour images can also be an indication of the fact that two images have to be merged together. By tak-ing into account this spatial information that is ignored in colourimetric merging, it is possible to group together multi-colour blocks and remove some useless low-den-sity blocks (containing few pixels), that generally cap-ture colour shades of the same layout component.
Spatial merging operates on the results of RLSA when it is applied to the possibly reduced set of binary images determined by colourimetric merging. For each couple of binary images, intersecting blocks are identi-fied and eventually merged on the basis of three percep-tual criteria hereinafter specified. The first criterion aims to identify multicolour blocks. The second criterion aims to extend multicolour blocks by enclosing overlapping spurious blocks from binary images. The third criterion aims to extend binary images by enclosing overlapping spurious blocks from other binary images.

The first criterion can be summarized by the produc-tion rule in algorithm 2. It identifies multicolour layout components, such as revenue stamps. For each couple of intersecting blocks, when the percentage of intersec-tion exceeds a user-defined threshold ( th _ IntSM ) and the percentage of occupation (defined as the ratio be-tween the area of the block and the entire image area) for both candidate blocks is in the interval [ th _ MinOcc , th _ MaxOcc ], then a new multicolour image is generated. The new image is built as the union of partitions of the original images enclosed in the blocks. Original binary images are also  X  X leaned X  by removing pixels added to the multicolour image.

The second criterion (algorithm 3) is based on the rationale that if a block strongly overlaps a block of a multicolour image, the intersecting part has to be con-Algorithm 2 First Criterion Algorithm 3 Second Criterion sidered as composing the multicolour block. The pixels enclosed in the intersection are removed from the binary image ( ImgForeground2 ) and added to the multicolour image ( ImgForeground1 ).

The third criterion (algorithm 4) aims to extend bi-nary images by enclosing overlapping spurious blocks from other binary images. The first rule is based on the rationale that if a small block has a high degree of overlap with a block of another image, it has to be con-sidered a spurious block and included in the image asso-ciated to the  X  X redominant X  block ( ImgForeground1 ). Algorithm 4 Third Criterion The second rule states that if two blocks have a high degree of overlapping, then the intersecting part of the block with lower density (non-predominant block) has to be added to the image of the predominant one. The density of a block is defined as the ratio between the number of pixels contained in a block and the area of the block.

Like most of the algorithms for colour segmentation proposed in the literature, our algorithm allows the user to set thresholds [35]. In particular, we use four different thresholds listed in the following:  X  th _ IntSM defines the minimal intersection percent- X  th _ InclSM defines the minimal inclusion percentage  X  th _ MinOcc and th _ MaxOcc define the range of occu-
All the values are user-defined and depend on the specific type of processed documents. Although it is pos-sible to find the optimal value of these parameters on the basis of a training set of documents, this aspect has not been explored in this work.

At the end of the spatial merging process, List con-tains the final list of binary images. The RLSA segmen-tation is applied to each image separately (if not yet computed) and each RLSA execution returns a set of rectangular blocks that are grouped together in a sin-gle set of basic blocks. Each basic block can be either associated to a single colour or labelled as multicolour. 2.4 Layout analysis The segmentation algorithm returns (possibly) overlap-ping blocks that may contain either textual or graphical information and are either single colour or multicolour. A first step towards the reconstruction of the layout structure consists of classifying the blocks according to their content type: text, horizontal line, vertical line, pic-ture (i.e. halftone images) and graphics (e.g. line draw-ings). The classification of blocks is performed by means of a decision tree automatically built from a set of train-ing examples (blocks) of the five classes.

The layout structure is built by exploiting not only the result of the classification of basic blocks and their geo-metrical features, but also the colour information ob-tained during the segmentation process. WISDOM++ combines the top-down segmentation of the image into basic layout components with a bottom-up layout analysis method to assemble basic blocks into larger components. More precisely, the layout structure is extracted in two steps: 1. A global analysis of the document image to deter-2. A local analysis of the document to group together
The layout structure extracted for each document page is a hierarchy with five levels: basic blocks , lines , set of lines , frame 1 and frame 2 . In Fig. 2 blocks of the frame 2 level, for each final colour are shown. 3 Document image understanding Document image understanding, that is, mapping the layout structure of a document page into a correspond-ing logical structure, is based on the assumption that documents can be understood by means of their layout structures alone. For instance, the date of a censorship decision is usually located at the top of the first page of a censorship card, while the signature of the censurer is at the bottom of the last page. The mapping of the layout structure into the logical structure can be performed by means of a set of rules.

Generally, logical structures depend on the particu-lar class of documents. For instance, the censorship card provided by Deutsches Filminstitut in the COLLATE project includes the names of assessors on the top right of the first page, while there is no such information in a censorship card made available by FilmArchiv Austria. Hence, the mapping of the layout structure into the logical structure is subordinated to document classi-fication. In WISDOM++, the document classification is also performed by matching a set of rules.

Several works report a similar rule-based approach to document image understanding. Lee et al. [18] use hand-coded rules to identify sensible components in binary images of technical journals, while Niyogi and Sriha-ri [29] propose a rule-based approach to understand newspaper pages. Klink and Kieninger [16] organize document image understanding in two phases: the first phase concerns the recognition of common document structures like headings, footnotes and lists, while the secondphaseconcerns therecognitionof domain-depen-dent logical elements. As in the previous two works, the rule base is handcoded and no colour information is exploited. A machine learning approach is proposed by Aiello et al. [1] and Palmero et al. [30]. The former apply the decision tree learning system C4.5 [32] to learn clas-sification rules for textual layout components. The latter develop a neuro-fuzzy learning algorithm that ranks, for each new (unseen) block, candidate labels and selects the best.

As already pointed out, WISDOM++ makes extensive use of machine learning methods, also for document image understanding. Differently from some related works, where machine learning tools are like-wise involved, in WISDOM++ only layout information is used to identify logical components. Text, font size and additional information returned by the OCR cannot be used, since document image understanding precedes the application of OCR. Swapping the order of the two pro-cessing steps would not help in any case, because the high level of noise present in our historical documents prevents a direct and effective application of the OCR. Another important difference with respect to related works is the more powerful representation formalism adopted in WISDOM++ to describe the layout of train-ing documents. WISDOM++ resorts to first-order logic, which allows spatial relations between layout compo-nents to be effectively and naturally represented. On the contrary, competing systems resort to decision trees and neural network which are unsuitable for represent-ing a variable number of spatial neighbours of a layout component together with their attributes. Details on the representation languages adopted by the learning sys-tem ATRE, embedded in WISDOM++, are explained in the following subsection. 3.1 Learning rules with ATRE ATRE 2 is the system used to learn rules for document image understanding. The learning problem solved by ATRE can be formulated as follows: Given  X  X setof concepts C 1 , C 2 ,..., C r to be learned  X  X setof observations O described in a language L O  X  X  background theory BK  X  X  language of hypotheses L H  X  a user X  X  preference criterion PC Find
A logical theory T expressed in the language L H and defining the concepts C 1 , C 2 , ... , C r , such that T is com-plete and consistent with respect to O and satisfies the preference criterion PC .

The completeness property holds when the theory T explains all observations in O of the r concepts C 1 , C 2 ... , C theory T explains no counter-example in O of any con-cept C i . The satisfaction of these properties guarantees the correctness of the induced theory, with respect to the given observations O . Whether the theory T is correct with respect to additional observations not in O is an extra-logical matter, since no information on the gener-alization accuracy can be drawn from the training data themselves. In fact, the selection of the  X  X est X  theory is always made on the grounds of an inductive bias embed-ded in some heuristic function or expressed by the user of the learning system (preference criterion).
In the document image understanding domain, each concept to be learned corresponds to a logical label of interest.

As to the representation languages, the basic com-ponent is the literal , which can be of the two distinct forms: f ( t f ( t where f and g are function symbols called descriptors , t are terms (constants or variables) and Range is a closed interval of possible values taken by f . Some examples of literals are: colour(X1) = red , height(X1)  X  [1.1 ... and ontop(X, Y) = true . The original list of descriptors used to represent the layout of multi-page documents for document understanding tasks (see [23]) has been extended to exploit colour information. Each block is described not only in terms of its geometry and content type, but also in terms of its corresponding RGB val-ues (descriptors red , green and blue ). For multi-colour layout components, such as revenue stamps, the only predicate multi-colour is used, since no RGB value can be univocally associated.

In ATRE, training observations are represented by ground multiple-head clauses [19], called objects , which have a conjunction of simple literals in the head. The head of an object contains positive and negative examples describing the set of user-assigned logical labels, while the body contains the description of layout components on the basis of geometrical features (e.g. width, height, centroid position of a block) and topolog-ical relations (e.g. vertical and horizontal alignments) existing among blocks, as well as the type of the content (e.g. text, horizontal line, image). Terms of literals in objects can only be constants, where different constants represent distinct layout components within a page. An example of a training object is the following: where the constant 1 denotes the whole page and the remaining constants identify distinct layout components. In this example, the block number 2 corresponds to a block enclosing the textual information on the registra-tion authority of the censorship card. The block is 28 pixels wide and 5 pixels high and it is characterized by the colour  X  X 0 X , whose RGB values are 159, 146, 109 . In addition, it is placed over the block number 26 which is a multi-colour block.

The learned theory is composed of a set of logical clauses, each of which describes some conditions that characterize a subset of positive examples of a concept C . Two examples of learned clauses are:
The first clause states that if a block is quite short (height is between 20 and 37 pixels) and wide (width is between 269 and 304 pixels) and its red RGB component has a value in the range between 7 and 75, then it can be classified as the chairmen of the censorship card. The second clause states that if X 1 is quite a large block (width is between 348 and 364 pixels) placed over the block related to the chairmen , then it can be labelled as the assessors of the censorship card.

The last clause exemplifies a distinguishing charac-teristic of ATRE, namely the possibility to discover dependencies between concepts to be learned (e.g. asses-sors and chairmen ). Indeed, logical components may be related to each other, and such dependence can be reflected by some geometric relationships between the layout components associated to those logical compo-nents. Rules learned for document image understanding shouldreflect dependencies betweenlogical components toenableacontext-sensitiverecognition. However, most of the studies on inductive learning presented in the machine learning literature make the implicit assump-tion that concepts are independent ( independence assumption ). The learning strategy implemented in ATRE is a notable exception.

The high-level learning algorithm in ATRE belongs to the family of sequential covering (or separate-and-con-quer ) algorithms [26], since it is based on the strategy of learning one clause at a time (conquer stage), removing the covered examples (separate stage) and iterating the process on the remaining examples.

The most relevant novelties of the learning strategy implemented in ATRE are embedded in the design of the conquer stage. Firstly, the conquer stage of our algo-rithm aims to generate a clause that covers a specific positive example, called seed . Secondly, the search space explored by ATRE is a forest of as many search-trees (called specialization hierarchies ) as the number of cho-sen seeds, where at least one seed per incomplete con-cept definition is kept. Each search-tree is rooted with a unit clause and ordered by generalized implication. The forest can be processed in parallel by as many con-current tasks as the number of search-trees (parallel-conquer search). Each task traverses the specialization hierarchies top-down (or general-to-specific), but syn-chronizes traversal with the other tasks at each level. Initially, some clauses at depth one in the forest are examined concurrently. Each task is actually free to adopt its own search strategy and to decide which clauses are worth being tested. If none of the tested clauses is consistent, clauses at depth two are considered. The search proceeds towards deeper and deeper levels of the specialization hierarchies until at least one consis-tent clause is found (see Fig. 3). Task synchronization is performed after all  X  X elevant X  clauses at the same depth have been examined. A supervisor task decides whether the search should carry on or not, on the basis of the results returnedbytheconcurrent tasks. Whenthesearch is stopped, the supervisor selects the  X  X est X  consistent clause according to the user X  X  preference criterion PC.

This strategy has the advantage that simpler consis-tent clauses are found first, independently of the con-cepts to be learned. Moreover, the synchronization allows tasks to save much computational effort when the distribution of consistent clauses in the levels of the different search-trees is uneven. This separate-and-par-allel-conquer search strategy provides us with a solution to the problem of interleaving the induction process for distinct concept definitions [23].

This separate-and-parallel-conquer search presents some efficiency problems and leaves a large margin for optimization. One of the reasons is that every time a clause is added to the partially learned theory, the spe-cialization hierarchies are reconstructed for a new set of seeds, which may intersect the set of seeds explored in the previous step. Therefore, it is possible that the sys-tem explores the same specialization hierarchies several times, since it has no memory of the work done in previ-ous steps. This is particularly evident when concepts to be learnt are not mutually dependent. Intuitively, cach-ing the specialization hierarchies explored at a certain step of the separate-and-conquer strategy and reusing part of them in the following step seems to be a good strategy to decrease the learning time while keeping memory usage under acceptable limits.

The second source of inefficiency is the repeated exe-cution of  X  X overage X  tests of generated clauses on the set of observations. Luckily, it can be proved that for independent clauses (i.e. clauses that do not express any concept dependency), the lists of negative examples remain unchanged between two subsequent learning steps. Therefore, by caching the list of negative exam-ples for each independent clause, the learning system can save the computational cost of many redundant tests. A different observation concerns the list of pos-itive examples covered by independent clauses, since it can decrease between two successive learning steps. We observe, however, that the set of positive examples of a clause C generated at the ( i + 1 ) -th step can be calculated as the intersection of the cached set com-puted at the i -th step of the learning strategy and the set of positive examples covered by the parent clause of C in the specialization hierarchy computed at the ( i + 1 ) -th step. Therefore, by caching also the list of pos-itive examples it is possible to reduce the computational complexity [6].

The side-effect of caching is the increased space complexity and the additional time spent to retrieve cached statistics. Whether caching is actually useful depends on the task in hand. In the next section we will also evaluate the time performance of ATRE for the specific task of learning rules for document image understanding. 4 Application to censorship cards In this section we empirically evaluate the proposed ap-proach on a corpus of documents collected in the context of the project COLLATE. We want to empirically prove the effectiveness of the colour-based approach both in the identification of the layout structure and in the doc-ument image understanding task. Results are compared with the original version of WISDOM++ that processes black-and-white images.

Before describing the results, we illustrate the corpus used in this study. 4.1 Description of the corpus Processeddocument images havebeenprovidedbythree European film archives, namely Deutsches Filminstitut (DIF), Filmarchiv Austria (FAA) and N X rodn X  Filmov X  Archiv (NFA), involved in the project COLLATE 3 . Generally, documents are multi-page, where each page is a colour image representing rare historic film censor-ships from the 1920s and 1930s.

Each document page corresponds to an RGB 24 bit colour image in TIFF format. We considered 58 multi-page documents belonging to 3 distinct classes, one for each archive (see Table 1) and we applied WISDOM++ to 108 document images in all. Documents belonging to the same class present similar layout structures.
For each document class, film archivists defined a set of logical labels, useful for indexing and retrieval purposes, to be associated to layout components. For the FAA class, labels are dep_signature, adhesive_stamp, stamp, registration_au, date_place, department, applicant, reg_number, film_length, film_producer, film_genre, film_title . For the DIF class, labels are cens_signature, cert_signature, object_title, cens_authority, chairmen, assessors, session_data, representative . For the NFA class, labels are round_stamp, date_place, film_content, deliv-ery_date, cens_card, cens_process, recommendation, dispatch_office, no_reels, film_genre, applicant, film_length, film_producer, film_title, rubber_stamp, stamp, no_prec_doc, no_censor_cards, registration_au, register_office, official_notes, exec_date, cens_advisory, applic_notes. 4.2 Layout analysis: improvements over the b/w layout In this section we evaluate the colour-based layout analysis effectiveness in terms of capability to isolate interesting blocks of different colour for subsequent log-ical labelling. To evaluate this aspect, we compared the output of the proposed colour-based layout analysis with the output of the black and white (b/w) layout analysis implemented in the original version of WISDOM++ [2].
Both layout analysis algorithms were applied to the same set of 108 document pages. In the case of the col-our-based layout analysis, the following threshold values were used: th_IntSM = 70%, th_InclSM = 75%, th_Min-Occ = 1.5% and th_MaxOcc = 4.5%. For a fair compari-son, in both cases no preprocessing was applied and the layout was not manually corrected [24]. Once the layout structures had been extracted, the same user manually labelled interesting components according to interesting labels.

In Figs. 4, 5 and 6, examples of outputs of the black-and-white and colour-based layout analysis processes are shown. It is noticeable that the colour-based layout analysis is able to isolate interesting blocks better than the previous version. For example, in Fig. 4 the black-and-white layout analysis returns very few blocks. In particular, labels such as stamp, film_genre, film_length, adhesive_stamp have not been separated and co-occur in the same frame2 block. On the contrary, colour-based layout analysis is able to isolate them. By closely look-ing at the image, we can draw another consideration: the dep_signature (in violet at the bottom of the original image) has not been represented at all in the black-and-white image. This can be explained by the approximation performed by the embedded binarization algorithm that has been used to transform the original colour image into black-and-white. This loss of layout components does not occur in colour-based layout analysis, where binarization is not necessary.

By looking at Fig. 5, we note that the colour-based lay-out analysis is able to identify overlapping blocks, that is, cens_signature and stamp . On the contrary, the black-and-white layout analysis identifies two blocks, and the stamp has been split.

In Fig. 6, a document image belonging to the class with the most complex layout structure, that is, NFA, is shown. In this case, the document contains manual annotations ( no_prec_doc , top right-hand corner), blue stamps ( register_office and dispatch_officer , bottom page), red stamps ( rubber_stamp , top left-hand corner) and revenue stamps ( stamp , middle page). The colour-based layout analysis is abletoisolatethem, while the black-and-white layout analysis returns a single lay-out block for the whole central part of the document image and two spurious blocks extracted from the bot-tom of the image. This poor result is due to the presence of both vertical and horizontal lines, which affect the RLSA segmentation, especially when colours are not differentiated.

In Table 2, we present statistics on the number of frame 2 layout components that the user is able to label. While in the case of DIF censorship cards the number of labelled components with the colour-based layout anal-ysis is comparable to the case of b/w layout analysis, the situation is different for the other classes, where it is pos-sible to identify more logical components. This can be explained by the minor relevance of colour in the case of DIF images. On the contrary, in the case of both FAA and NFA, several logical components are characterized by colour information. This is particularly evident in the case of NFA documents that represent the most com-plex task because of the overall low quality of the paper and because they contain manual annotations, rubber stamps and revenue stamps of different colours. 4.3 Document image understanding In this subsection we investigate both effectiveness and efficiency of the proposed solution in the document im-age understanding task.

To investigate the effectiveness of the proposed solu-tion in the document image understanding problem, we analysed the DIF and FAA documents described in Table 1 by means of a fivefold cross-validation, that is, each dataset was first divided into five folds of near-equal size (five documents per fold) and for every fold, ATRE was trained on the union of the remaining folds and tested on the hold-out fold. On the other hand, for the NFA dataset we used a leave-one-out cross-valida-tion, which is simply an n -fold cross-validation, where n is the number of documents in the dataset. The leave-one-out validation is due to the very low number of available examples.

Two settings are compared: in the first setting docu-ments are processed by means of the colour-based lay-out analysis and the ATRE descriptions include colour information, in the second setting, documents are pro-cessed by means of the black-and-white layout analysis and ATRE descriptions do not include colour informa-tion.

For each learning problem, the number of omission/ commission errors is recorded. Omission errors occur when logical labelling of layout components is missed, while commission errors occur when wrong logical label-ling is  X  X ecommended X  by a rule.

The number of objects for ATRE corresponds to the total number of document pages, namely 50 for FAA, 50 for DIF and 8 for NFA documents. The total number of examples is 21,073 for FAA, 16,552 for DIF and 8,112 for NFA documents in the colour setting and 6,036 for FAA, 7,600 for DIF and 1,768 for NFA in the black-and-white setting. They correspond to the total number of literals in the heads of clauses. Given the set of concepts to be learned, positive examples are only 140 for FAA (2.32%), 149 for DIF (1.96%) and 12 for NFA (0.68%): they correspond to recognized layout components in the black-and-white setting. The situation is different for the colour-based setting, where only 205 out of 21,073 (0.97%) for FAA, 133 out of 16,552 (0.8%) for DIF and 64 out of 8,112 (0.79%) for NFA examples are positive. This means that there is a disparity between the num-ber of positive and negative examples. Moreover, it is important to consider that this disparity is much more evident in the colour-based representations, where the percentage of positive examples is (in two cases out of three) less than half of the corresponding value for the b/w setting. This means that the learning task in the col-our setting is more difficult than in the black-and-white setting.

In Tables 3 and 4, cross-validation results for the doc-ument understanding task are reported. Results con-cern the FAA and the DIF datasets. It is noteworthy that, although the omission errors increase in the case of the colour setting, the percentage of commission errors decreases. Rules learned in the colour setting are actu-ally more specific, so they are more precise, but do not capture all the variability of the layout.

By analysing some results in more detail, we observe that in the case of b/w setting, some concepts that are strongly dependent on the colour information are very hard to understand and, sometimes the classifier is not able to identify them at all. In Table 5, the example of the adhesive_stamp of the FAA class is reported. In this case, the classifier learned in the colour setting outper-forms by a wide margin the classifier in the b/w setting both in terms of omission and commission errors.
As regards the learned theories, in the following some examples of rules for the classes FAA and DIF are re-ported:
The first rule expresses the condition that a multicol-our block, whose height is between 100 and 109 is an adhesive stamp affixed to the censorship card.
The second and the last rules take into account col-our information. In particular, the second rule states that a coloured block with a red component of the RGB space varying between 75 and 111 (the admis-sible interval is 0 ... 255) and with its barycentre at a point between 21 and 23 on the y -axis is the registra-tion authority of the censorship document. The last rule states that a layout component with its barycentre at a point of 19 on the y -axis and that is characterized by a red component of 72 is the censorship authority of the censorship card.

The third and fourth rules are examples of rules expressing spatial relations among layout components. More specifically, the third rule expresses that quite a large layout component (width between 139 and 169) that is vertically aligned with a short layout component (width equal to 8) is the title of the film. The fourth rule shows a conceptual dependency. In fact, it expresses that a large layout component (width between 50 and 200) that is horizontally aligned with the layout component labelled as cens_authority is the session_data of the cen-sorship document.

A different analysis should be made in the case of the NFA dataset. In fact, in this case it was not possi-ble to run ATRE in the b/w setting since the user was able to label only 12 examples for three different con-cepts ( date_place, register_office, dispatch_office ). If we had run ATRE, the learned theory would have been based on very few training examples and almost no test examples. On the contrary, in the colour setting, we had enough positive examples to run ATRE (results are shown in Table 6).

As in the case of DIF and FAA, in the following we report some examples of learned rules for the NFA data-set.
The first rule shows that a rubber_stamp can be clas-sified only considering colour information (in this case both red and green RGB dimensions are used as dis-criminating features). The second rule expresses the condition that a layout component with its barycen-tre at a point between 202 and 213 on the y -axis and with the red RGB component between 95 and 161 is a stamp .

The second aspect we investigate concerns the effi-ciency of the learning system ATRE. In particular, we evaluate the effective advantage of the use of the caching technique described in Sect. 3. For this aim, we present results concerning the comparison, in terms of time com-plexity, of the original version of ATRE with respect to the optimized version obtained by caching techniques. In Table 7, results on the DIF dataset in the colour set-ting are reported. Results show that the gain in terms of learning time of the optimized version is of more than 56% on average. The table also shows that this reduced complexity does not negatively affect the classification accuracy. 5 Conclusions and future work In this paper, we presented some extensions of the DIA system WISDOM++ required to meet the challenges arising from the processing of censorship cards from European film archives of the 1920s and 1930s. WIS-DOM++ was originally developed to support the trans-formation of printed documents from black-and-white images into an HTML/XML format. However, to effectively process low layout quality and standard doc-uments, such as censorship cards, the exploitation of information provided by colour has been necessary. In-deed, signatures, stamps and manual annotations, which are the main cause of noise in processing these docu-ments, are often characterized by colours which differ from those present in the background and in typewrit-ten texts. To take full advantage of colour information, all processing steps in WISDOM++ have been substan-tially revised. In particular, a new colour image segmen-tation algorithm has been proposed. Colour information extracted by means of this step has been profitably ex-ploited in the layout analysis, as well as in the steps of image classification and understanding. Moreover, the richer representation employed in document classifica-tion and understanding raises efficiency problems and has led to the implementation of some caching tech-niques which have been integrated in the learning sys-tem ATRE embedded in WISDOM++. The application of the improved version of WISDOM++ on a collec-tion of multi-format censorship cards provided by three national film archives, namely Deutsches Filminstitut (Germany), Filmarchiv Austria and N X rodn X  Filmov X  Archiv (Czech Republic) has been presented. The goal of the experimental study is to empirically evaluate the effectiveness of the proposed approach both in the iden-tification of the layout structure and in the document image understanding task. For this purpose, firstly, we prove the effectiveness of the colour-based layout anal-ysis. Secondly, we evaluate the effectiveness of the use of colour information in document image understand-ing. Thirdly, we prove the efficiency of the learning task. Results permit us to draw four main conclusions: 1. For the colour-based layout analysis, a compari-2. The learning system suffers from an increase in the 3. Some logical components, strictly related to colour 4. The proposed caching techniques implemented in For future work, we intend to explore the possibility to support thelayout analysis withtheautomaticprediction of the spatial merging threshold values. Moreover, diffi-culties met for document image understanding suggest the investigation both of representation issues and prob-lems related to uncertainty management in learning and classification. In particular, we intend to further enrich the representation language adopted to describe layout structures and to explore the opportunity of relaxing the definition of a subsumption test between clauses [8], in order to weaken the conditions of applicability of rules and to significantly recover omission errors. References
