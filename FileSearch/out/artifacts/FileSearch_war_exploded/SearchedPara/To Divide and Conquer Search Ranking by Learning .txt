 Learning to rank plays an important role in information retrieval. In most of the existing solutions for learning to rank, all the queries with their returned search results are learnt and ranked with a single model. In this paper, we demonstrate th at it is highly beneficial to divide queries into multiple groups and conquer search ranking based on query difficulty. To this end, we propose a method which first characteri zes a query using a variety of features extracted from user search behavior, such as the click entropy, the query reformulation probability. Next, a classification model is built on these extracted features to assign a score to represent how difficult a query is. Based on this score, our method automatically divides quer ies into groups, and trains a specific ranking model for each group to conquer search ranking. Experimental results on RankS VM and RankNet with a large-scale evaluation dataset show that the proposed method can achieve significant impr ovement in the task of web search ranking. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Performance, Theory Learning to Rank, Query Difficulty. Search users currently pay more and more attention to search engines X  relevance since it is a challenge task to rank the Web-scale WebPages successfully for va rious queries. Learning to rank is one of the techniques used to improve the relevance of search ranking. By leveraging machine learning techniques into model construction, learning to rank is known to build the ranking model theoretically and practically effectively [1] [7] [3]. Recently, many learning-based methods have been proposed to employ different strategies in learning to rank, such as RankSVM [2] [7], RankNet [1] and Rankboost [3], and show their effectiveness to improve search relevance. In most of the previous works in learning to rank, all the queries with their returned search results are learnt and ranked using a single ranking model. However, as found by [9], using one ranking model for all the queries is not an optimal solution to conquer the search ranking challenge. As is also found by [8], a good ranking algorithm for the topic relevance task does not always perform well for the homep age finding task. [11] observed that there are a lot of variations across queries. Inspired by these observations, if queries could be divided properly into different groups, it is possible to design different ranking models for each group to improve the search ranking. However, dividing queries for learni ng to rank is not a trivial task. An intuitive way to divide queries is based on search intent, as discussed in [8], which used the query content information, URL and the link information from the retrieval results to classify queries into two categories, the topic query and the homepage-finding query. Another intuitive way to divide queries is based on the query length. [11] had shown that the query length is a key feature to distinguish query am biguity. Intuitively, for long queries, a learning model is supposed to include more features to build a more complicated ranking function than short queries, like the query proximity in [10]. In this paper, we propose a method to characterize a query using a variety of features extracted from the large-scale user behavior in a commercial search engine, such as the click entropy, the query reformulation probability and th e session click number. Through investigating all of these features, our method formalizes the query difficulty prediction as a binary classification problem. The output value of the classifier is defined as the query difficulty score in this paper. According to this score, we explore several strategies to divide queries into multiple groups and learn the ranking models sepa rately. We conduct expe riments on a large-scale dataset using RankSVM an d RankNet as the separate ranking model. Experimental resu lts show that our basic binary strategy, which divides queries into difficult and easy categories, can improve the Mean-NDCG value by 2.5% and 1.1% in RankSVM and RankNet respectively, where Mean-NDCG is the mean value of NDCG@1 through NDCG@10. Our cluster-based strategy, which divides queries into multiple groups based on the query difficulty score, can improve Mean-NDCG by 4.1% using RankSVM. This work was done when the first author was visiting Microsoft Research Asia. To summarize, the contributions of this paper include: 1) We propose a method to measure query difficulty through 2) We explore several strategies to divide queries to conquer 3) The proposed method is validated on the large-scale dataset The rest of the paper is organize d as follows. Section 2 describes our method to predict query difficulty. In Section 3, we present our method to divide and conquer learning to rank. Experimental results with discussion are presen ted in Section 4. And Section 5 concludes the paper with future work. Query difficulty represents how difficult a query is for a ranking algorithm to return the satisfactory results to users. Generally, if the search results for a query are bad and users are not satisfied with them, we will regard it as a difficult query; vice versa. In this paper, we use large-s cale human judged data to identify whether a query is a difficult query or an easy query. We consider a wide range of features to characterize a query. The description of each feature and their explanations are presented in Table 1. We drilled down the featur es according to whether it is a Query feature or a Click feature, and whether it needs the session information to aggregate the feature value, where Session value with Yes means it needs session information to construct the value for this feature and Session No re presents the opposite. For each query, there are some aggregation features in the Click type with Session, like the click distribution at K for query q , denoted as  X   X  X  X   X   X , X   X  , and the click entropy , which is a feature used in work [11] to measure the consistenc y for the clicked pages given a query q . We define their formulas as the followings: We further explain some other features in Table 1, Probability of Reformation is the fraction of reforming a new query to replace the current query; Query Suggestion Probability is the fraction that the current query generated by clicking query suggestion other than inputting to the search box; Probability of Clicking Ads is the fraction of advertisement click in the search click. However, the human judged effort s can only cover a small range of queries and leave a large range of queries unlabeled. Therefore, we formalize the query difficulty prediction as a binary classification problem. To build a cl assifier to distinguish difficult queries from easy queries, each huma n judgment is used as a label, and a variety of features extracte d from the user behavior log is used as the feature vector. For an unlabeled query, given the query feature vector as the input, the out put value of the classifier is defined as the query difficulty score indicating how difficult a query is. In this paper, we se lect Support Vector Machine (SVM) as our classification al gorithm. Given a query q , we define its query difficulty score  X  X   X   X   X   X   X   X , X   X   X   X  X   X  X  , where  X  is the SVM separation hyper-plane and  X   X   X   X  represents the feature vector of query q . In computer science, divide an d conquer (D&amp;C) is an important strategy based on multi-branched recursion, which works by breaking down a problem into multiple sub-problems of the same type, until it becomes simple e nough to be solved directly. When we apply the D&amp;C strate gy into the problem of learning to rank, the first action we are facing is how to divide the query effectively and conquer learning to rank by ac hieving better search relevance. An intuitive way to divide queries is based on the length of the query. [11] showed that the query length is a key feature to distinguish query ambiguity. Intuitively, for long queries, learning model is supposed to include mo re features to build a more complicated ranking function than short queries, like query proximity in [10]. Hence, in the following experiments, we use the query length dividing strategy as a baseline in the comparative study, and referred it as the length-based strategy . As is stated in [11], if a system could know which queries are hard, it can devote the appropriate resources to improve the results for those queries. In the task of lear ning to rank, if we could get a highly confident score indicating how difficult a query is, we can train different ranking models for different queries to conquer the ranking problem. Based on the query difficulty score  X  X  X  X  X  X  discussed in the previous section, we divide all the queries into two categories  X  the difficult queries and the eas y queries, depending on whether  X  X   X   X   X   X 0 . In the training process, we train a separate ranking model for either kind of query. In the testing process, we first use SVM to classify a query. Dependi ng on whether it is a difficult query, the corresponding ranking m odel will be used for the prediction. We call this method the Difficult&amp;Easy strategy . Although the Difficult&amp;Easy strategy seems to be a simple and practical solution to divide queries, it is possible to investigate a finer dividing strategy. Indeed, th e difficulty score that presents the confidence of the classification [6] motivates us to divide queries into multiple categories. Therefore, an unsupervised dividing strategy is applied he re using a clustering algorithm, Kmeans, performed on the output value of the classification function, which is  X  X  X  X  X  X  in the previous section. The reason that motivates us to choose Kmeans as the clustering algorithm is that K, the number of clusters, is a free parameter, enabling us to actively decide the number of query clusters. For each cluster, a separate model is trained. In the testing process, each query is first put into a cluster by classifying it to the closest centroid, the corresponding ranking model is then retrieved to generate the ranking result. We call this strategy the Pre-cluster strategy . Table 1. Extracted features used to predict query difficulty Throughout this paper, two state-of-the-art algorithms RankSVM [7] and RankNet [1] are to be used as the ranking model. We will also use their performance on the entire dataset as the baselines to compare with the divide and conquer technique. To learn the query difficulty sc ore and conquer search ranking, we use two datasets obtained from a commercial search engine: the explicit dataset and the implicit dataset, both containing the same set of queries. A total of 5,676 queries were sampled from the log of a commercial search engine according to the query frequency. In the explicit dataset, top 50 search results for each query are labeled by five assessors to repr esent the relevance score between the query and each result, ranging from 0 to 5. There are a total of 283,800 query-URL pairs. In order to conduct learning to rank on the realistic dataset, we pulled the Meta features from the search engine, which are used to predict ranking in the realistic search engine. A total of 200 features are incorporated. For the implicit dataset, we retrieve the user behavior data from Sep. 1 st 2008 to Sep. 30 th 2008. There are a total of 46,392,929 user sessions, around 52,000,000 URLs and around 15,800,000 distinct users. For each query, a set of features is extracted from this dataset to build a feature vector according to Table 1. Note that we did not use th e public dataset like LETOR or OHSUMED, because there is no user behavior data for most of the queries in them and the number of queries in them is too small to perform persuasive experiments in our problem. To generate the ground-truth training data to build a binary classier for query difficulty prediction, we leverage the explicit dataset to distinguish difficult qu eries from easy ones. As stated in Section 2, query diffi culty could be used to measure whether a search engine could return sati sfactory and relevant results to users. With all the query-URL relevance values and the realistic ranking positions in the explicit dataset, we can measure the query difficulty based on a calculation metric, like Precision, DCG and NDCG in [5]. However, which me tric could perform the best for query difficulty prediction is an open question. We conduct a classification experiment to seek for answers. We retrieve 1,000 queries from th e explicit dataset. For the splitting between training and testing, we leverage the 5-fold splitting method in work [6] and average the classification accuracy [6] of each fold. The results for SVM with linear model are presented in Figure 1. From Figure 1 we can see that the prediction with DCG performs significantly better than that with Precision and NDCG, partially due to the fact that DCG depicts the difficulty based on an ideal ranking. In the following experime nts, we will use DCG@10 as the metric since 10 is the search result number for most search engines. In Table 2, we demonstrate the feature weights generated by the classifier. The larger weight indicates the more reasonable this feature can explain the query difficulty. We can see from the table that our created features in Section 2 play a substantial role in explaining the query difficulty. Based on the query relevance score ca lculated with the best metric, DCG, we sort all the 5,676 queries based on their DCG scores and split them into 2 subsets of equal size. The set with lower DCG is labeled as the difficult query set and the other is labeled as the easy set. Based on this data, the SVM classifier is learned to predict query difficulty scores to be used in the next sub-section. Now we verify whether the divide and conquer strategy for learning to rank is well-justified. For all dividing strategies in Section 3, we use the 5-fold strategy to split training and testing sets to make sure every query ha s an opportunity to be evaluated. The experimental results reported are the average of each fold. We use RankSVM and RankNet as base models. The parameters for RankSVM are set to default. The number of epoch in RankNet is set to 200 as default. Since we want to evaluate how the divide and conquer strategy performs on learning to rank, we use its opposite, a single model strategy, as the baseline, referred as single in figures. Since dividing by length is an intuitive strategy, length-based strategy is used as the other baseline. To verify the effectiveness of the Difficulty&amp;Easy (D&amp;E) strategy, we compare its NDCG and MAP scores with those of the length-based strategy and the single m odel strategy. We use RankSVM and RankNet as the base ranking models and present the results in Figure 2 and Figure 3. In Figure 2, the length-based strategy and the Difficulty&amp;Easy strategy can achieve improvements of 1.9% and 2.5% respectively in the Mean-NDCG. These results justify that the dividing strategy could bring improvements in learning to rank. Furthermore, the Difficult &amp;Easy strategy performs much better than length-based strategy, indicating that employing more features beyond length to estimate the query difficulty can benefit learning to rank. In Figure 3, an informative observation is that the length-based strategy could not achieve a better performance than the single model strategy. However, the Difficult&amp;Easy strategy can still bring improvements. The Mean-NDCG improvement in RankNet is 1.1%. Although the improvement of RankNet is a little smaller than that of RankSVM, it still draws the same conclusion as RankSVM. Thus, we wi ll only use RankSVM in the following experiments due to sp ace consideration. The only parameter in the Kmeans algorithm is K, the number of clusters. We divide the first fold data into two subsets where 4/5 of the data is used as training, and the other 1/5 data is used to tune a K value with the best NDCG. Based on our experiment, we use K=17 in the pre-cluster strategy. In Figure 4, we can see that Pre-cl uster strategy brings significant improvements on all NDCG positi ons and the MAP measurement. It outperforms the single model stra tegy, the length-based strategy, and the Difficult&amp;Easy strategy by 4.1%, 2.2% and 1.5% respectively on the Mean-NDCG. We continue to conduct the t-test on the improvements with results indicating the improvements are statistically significant (p-value&lt;0.05). When compared with the Difficult&amp;Easy strategy, the 1. 5% improvement verifies that using a cluster strategy on the di fficulty score is more successful than the strategy with two categories. We have achieved significant im provement over the single model baseline, and a continuing investigation could be which kind of query plays a more substantial ro le in obtaining the improvement. To answer this question, we ca lculate the average improvement delta between the single model baseline and the pre-cluster strategy, for both the difficult query set and the easy query set. Experimental results from Figure 5 are informative, in which we can see that difficult queries have much more improvements than easy queries for all the NDCG positions and MAP. We hence conclude that difficult queries ha ve more potential to be improved and need to be handled separately to conquer learning to rank. In this paper, we proposed a me thod to measure query difficulty and utilize it to divide and conquer learning to rank. In contrast to existing methods, we leverage th e explicit and implicit user data to predict query. Experimental results on a large-scale realistic dataset show that the dividing and conquering strategy based on the query difficulty score can achieve significant improvements when compared with the single model baseline. Among all the dividing strategies, the Pre-clus ter strategy on query difficulty score performs the best with a 4.1% improvement on the Mean-NDCG, when compared with the baseline. There are several aspects in this direction that we can pursue to further enhance our method. For que ry difficulty prediction, we plan to investigate other nonlinea r solutions for example using rbf kernels. For the dividing and c onquering strategies, we hope to investigate more dividing strategi es, such as by navigational and informational, and see the possibility to perform a comparable study for different kinds of dividing strategies. [1] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., [2] Cao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y., and Hon, H.-[3] Freund, Y., Schapire, R. E., and Singer, Y. An efficient [4] Geng, X., Liu, T.-Y., Qin, T., Arnold, A., Li, H., and Shum, [5] J X rvelin, K. and Kek X l X inen, J. IR evaluation methods for [6] Joachims, T. L earning to Classify Text Using Support Vector [7] Joachims, T. Optimizing s earch engines using clickthrough [8] Kang, I. and Kim, G. Quer y type classification for Web [9] Qin, T., Zhang, X.-D., Wang, D.-S., Liu, T.-Y., Lai, W., and [10] Tao, T. and Zhai, C. An e xploration of proximity measures [11] Teevan, J., Dumais, S. T., and Liebling, D. J. To Personalize 
