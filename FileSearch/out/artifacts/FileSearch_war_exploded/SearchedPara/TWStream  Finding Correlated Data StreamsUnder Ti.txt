 The advances in hardware technology have made it possible to automatically col-lect data in a stream-like manner. Typical applications of data streams include sensor network, financial data analysis and moving object tracking. The process-ing and mining of data streams have attrac ted upon intensive research recently. Some extensively studied problems include summarization [19], clustering [11], similarity search [9], etc.

In this paper, we investigate another in teresting problem, n amely generalized correlation detection, that is to monitor multiple streaming time series 1 and detect all correlated pairs in real time. The correlation is of general sense, that is two series are considered similar if a significant part of them (subsequences) demonstrate highly alike rise/fall patterns, neglecting the shifts and lags in the time axis. Fig.1(a) shows two sample time series X and Y , whose correlated parts (subsequences of same length) are highlighted. The one of Y is lagging its counterpart of X by about 3000 time units. Fig.1(b) illustrates two ways of matching the subsequences and computing the correlations. The left plot is the traditional one-by-one alignment method. Since the patterns of these two subsequences are not synchronized, it will produce a dissimilarity measure. While the time warping shown in the right plot allows for flexible shifts in the time axis, and produces more intuitive result. It ca n be expected the correlation based on the match produced by time warping is more accurate than the canonical one.
Detecting such generalized correlation s in data streams is challenging for sev-eral reasons: (1) Streams grow continuously in length at high-rate. It is imprac-tical to store voluminous historical data in memory. Thus the na  X   X ve algorithm of sequentially comparing each pair of s ubsequences is unaccep table. (2) We can-not assume any knowledge regarding the length of subsequence to be compared, which is usually unavailable a priori. (3) The computation of time warping is expensive. Consequently, employing ti me warping for each pair of subsequences is prohibitive.

In this paper, we aim to overcome all th e problems above in detecting gener-alize correlations in data streams. To be specific, our main contributions are as follows:  X  We propose the concept of generalized correlation , which to the best of our  X  We present a method called TWStream 2 , that captures generalized correla- X  Experimentsonbothsyntheticandreallifedataaredone.Ourmethod
The remainder of the paper is organized as follows: Section 2 gives a brief survey of the related work on stream processing and mining. The details of our TWStream framework are presented in Sect ion 3. We give a theoretical analysis of the accuracy and complexity of our a pproach in Section 4. Section 5 reviews the experimental results and S ection 6 concludes this paper. Recently the processing and mining of str eaming data have attr acted upon inten-sive research. Some well studied problems include summarization [19], clustering [11] and similarity search [9]. We focus on the work on detecting correlation between streams: Yi et al. [18] propose a method to analyze co-evolving time sequences by modelling the problem as a multi-variate linear regression. Zhu et al. [19] propose StatStream for monitoring multiple time series. It divides a user-specified window of the most recen t data into a set of basic windows, and maintains DFT coefficients for each basic window. It allows batch update and efficient computation of inner product. Ho wever, it cannot detect the correlation between two subsequences with lag larger than the basic window. Meanwhile, the setting of the length of sliding window requires a priori knowledge, which influences the sensitivity of algorithm to a large extent. Very recently, Sakurai et al. [16] propose BRAID for detecting correlation between streams with arbi-trary lags. They use geometric probing and smoothing to approximate the exact correlation. However, their method comp ares two series on the whole sequence level, and will clearly miss all the subsequence correlations. Moreover, most of these methods are based on the classical c orrelation, and fail in detecting the out-of-phase correlation.

To the best of our knowledge, none existing algorithm can satisfy the require-ment listed in the introduction. 3.1 Preliminaries Cross Correlation. The streaming time series is of the model X = { x i } (1  X  i  X  t ), where x unit. The cross correlation between two series X and Y is defined as: where  X  x and  X  ( x ) are the average and standard deviation of X respectively, and  X  x is the z -norm of x i . The same notations apply to Y . The symbols used in this paper are listed in Fig.2(a).

Note that the cross correlation is the inner product of z -norms, therefore  X  can be computed using the Euclidian distance of the normalized series:
From this fact we can conclude that cross correlation is simply another simi-larity measure directly based on the Euclidian distance, and consequently suffers from the same problems as Euclidian based metrics.
 Local Dynamic Time Warping. It is forcefully proved that Euclidian dis-tance is a brittle similarity measure [1], while Dynamic Time warping (DTW) is much more robust than Euclidian based metrics. Intuitively, time warping allows to flexibly shift the time axis, and matches the rise/fall patterns of two sequences as much as possible. The formal definition of the DTW distance between X and Y is given by: The computation of the DTW distance defines a time warping path in the matrix composed of the entries of ( i, j ), corresponding to the alignment between x i and y j . This path represents an optimal mapping between X and Y ,asshown in Fig.2(b). Under time warping, X is transformed to a new sequence X = { x To prevent pathological paths, where a r elatively small section of one series is mapped onto a relatively large portion of the other, we adopt the Local Dynamic Time Warping (LDTW) [20] which restricts the warping path within a beam of width (2 k +1) along the diagonal path, as shown in Fig.2(b). It is trivial to prove the computation of LDTW is of complexity O ( kw ), given w as the length of sequence.
 Generalized Correlation. The correlated parts of two streams can occur on any unknown scale, with arbitrary lag, or even out of phase. To accommodate such  X  X ny-scale X ,  X  X ny-time X  and  X  X ny-shape X  correlation, we introduce the con-cept of generalized correlation (GC), which combines cross correlation with time warping, and measures the correlation on subsequence level. Following we give out the formal definition of GC between two sequences X = { x i } and Y = { y i } (1  X  i  X  t ). Without loss of generality, we assume the correlated pattern in Y is lagging its counterpart of X .
 Definition 1. Given streams X and Y , their generalized correlation is a func-tion of index i and scale w , which determine x [ i : i + w  X  1] and y [ t  X  w +1: t ] as the subsequences to be compared. Let x = { x k } and y = { y k } (1  X  k  X  K ) be the transformed sequences of x [ i : i + w  X  1] and y [ t  X  w +1: t ] under time warping, then the GC  X  g ( i, w ) is defined as follows: where  X  x and  X  ( x ) represent the average and standard deviation of x respec-tively, the same notations apply to y .
 Note that time warping favors the positive correlation, in order to detect high negative correlation, we can transform { x j } ( i  X  j  X  i + w  X  1) to its symmetric above to compute GC.
 The problem we are to solve is: at any time point t , for each pair of streams X and Y , compute and report the GC value for any combination of scale w and index i in real time. 3.2 Overview The exact implementation can be deriv ed directly from the definition of GC: at time point t , we compute time warping for each pair of x [ i : i + w  X  1] and y [ t  X  w +1 : t ] for all combinations of i and w , and calculate the correlation between the transformed sequences. Howev er such brute force technique requires more than O ( t )spaceand O ( t 2 ) computation time.

Following we introduce our TWStream algorithm, based on four observations, which gain significant improvement over the na  X   X ve method.
 Geometric Time Frame. Given the scale w , instead of matching the pair of x [ i : i + w  X  1] and y [ t  X  w +1 : t ] for all i s, we take the snapshots of x [ i : i + w  X  1] at particular i sofgeometricorders( i is called the index of the snapshot). Specifically, snapshots are classified into orders ranging from 0 to log 2 t . The indices i sof j th order satisfy (1) i mod 2 j = 0 and (2) i mod 2 j +1 = 0, that is they occur at time interval of 2 j +1 .Foreachlevel j ,only the last  X  snapshots are stored. For example, suppose t =20and  X  =1,the snapshots of i = 19, 18, 12, 8 and 16 are taken, corresponding to level 0, 1, 2, 3 and 4 respectively. Based on the GC values computed for these specific i s, the correlation coefficients for the rest indi ces can be obtained by interpolation.
The justification for geometric time frame is: (1) In processing streaming data, we provide more importance for r ecent data. In geomet ric time frame, for more recent data, there is shorter dist ance between successi ve snapshots, and more points to interpolate to achieve better accuracy; (2) It achieves dramatic reduction in the time and space complexity, since currently we only need to store and match O (log t ) subsequences, instead of O ( t ). We will prove in Section 4 that this approximation introduces negligible error.
 Piecewise Smoothing. To support time windows of varying scales, we can store the snapshots of different sizes. Suppose the basic (minimum) window size j  X  log computed for snapshots of these specific scales, correlations for other scales can be estimated by interpolation. Nevertheless, under this approximation, the space and time requirement still grow linearly with time t , since the maximum window size is proportional to the length of sequence.

We propose to use the piecewise smoothing (or piecewise aggregate approx-imation [13]) to solve this problem. To be specific, for a time window of size 2 w 0 , instead of operating on the original series, we keep its piecewise aggregate (PA) of order j . That is we divide the time window into w 0 non-overlapping short windows of size 2 j , and compute the mean for each short window as its PA.Formally,let s 0 = { s 0 i } (1  X  i  X  2 j w  X  1) be the original series in the time window, its PA of order j , s j = { s j i } (1  X  i  X  w ), where s j i = i 2 We use PA( w , i ) to denote PA for a time window with scale w and index i . This approximation reduces the space required for storing snapshot of any size to a constant w 0 . Moreover, the time complexity of matching two subsequences is also reduced to w 0 . We will show the theoretical justification for piecewise smoothing in Section 4.
 Incremental Update. The improvements above significantly reduce the time and space complexity, however, they a lone are not sufficient to constitute a streaming algorithm. The problem of efficient update remains unsolved.
Here, we propose an incremental updat e strategy to achieve constant update time per time unit. For both series X and Y , we maintain a set of hierarchical sliding windows, which contains PAs for the most recent 2 j w 0 (0  X  j  X  log 2 t ) data points respectively. The windows are organized into a hierarchy where the window size doubles as level j increases. Fig.3(a) illustrates this hierarchy, in which the sliding windows are highlighted. At time t , we incrementally update Since sliding window of level j is updated per 2 j time units, on average the complexity of update per time unit is constant (  X  k =0 1 / 2 j  X  2). Note that the set of hierarchical sliding windows serves different purposes for X and Y .For Y , it contains the  X  X ueries X , which will be used to find correlated subsequences in X , while for X , the PAs in sliding window are added as snapshots to update the  X  X atabase X .
 Filtering. The last but not least observation is that LDTW is a relatively expensive operation. If the user desires only the report of GC value higher than a threshold  X  , instead of computing time warping for each pair, we can filter those pairs whose cross correlation value lower than a threshold  X  (positively correlated with  X  ). The cross correlation of two sequences can be computed efficiently if their sufficient statistics (sum, sum of square, inner product) are available. The maintenance of sufficient st atistics can be seamlessly incorporated into our framework. We omit this part due to the space limit, and more details are referred to [16].
 3.3 Algorithm Based on the observations above, we propose TWStream, an algorithm that captures correlated streams under time warping. TWStream maintains the snap-shots of different granularity for the  X  X ase X  series X , and uses the PAs of the most recent data of Y as  X  X ueries X  to detect correlation. For the snapshots of X ( w = 2 w 0 , i in the geometric time frame), the corre lations are computed exactly. The GC values for other combinations of ( w , i ) can be approximated by interpola-tion with the values of their neighbors. Fig.3(b) illustrates a typical interpolation scheme: on each level of scale,  X  snapshots are kept, which form an  X   X  log 2 t  X  X rid X . The curve passing the k th (1  X  k  X   X  ) snapshots of every level is called the k th interpolation curve .Theleftmost( k = 1) interpolation curve represents a least time limit . The GC values for all the combinations of ( w, i )onitsright side can either be computed or approxim ated. In this scheme, more recent time gets better accuracy, since GCs for snapshots of  X  X iner X  levels are available for interpolation.

The detailed TWStream algorithm is presented in Fig. 4 and Fig. 5. For each coming data point, it first incrementally updates the PAs in the hierarchical sliding windows ( AddNewElement ). It then adds the newly generated PAs as snapshots to the proper levels, and deletes the stale ones ( UpdateSnapshots ). Finally, the GC values are computed for the snapshots of the base series, and approximated by interpolation for other combinations of ( w, i )( CalGC ). In this section, we present a theoretical analysis of the accuracy and complexity of our approach, and provide the justification for the approximations we made in TWStream. 4.1 Accuracy The experimental results show that the two approximations, geometric time frame and piecewise smoothing introduce n egligible error in e stimating GC val-ues. Following, we provide the theoretical proof.
 Lemma 1. Let h (2 j  X  h&lt; 2 j +1 ) be an arbitrary time window, t  X  be the index nearest to ( t  X  h ) within the geometric time frame, then | ( t  X  h )  X  t  X  | X  2 Proof. For each level k , the geometric time frame stores the last  X  (  X   X  2) snapshots, with interval 2 k +1 between two successive indices, which covers a in one interval of level 2 k  X  +1 , | ( t  X  h )  X  t  X  | X  2 j  X  log 2 (  X   X  1) . Thus for any user-specified index ( t  X  h ), we can find a snapshot within the radius less than the time window h , which means that we can approximate the GC value for any time instance by careful interpo lation, though more recent time (small h ) gets better accuracy. Also we can enhance the approximation by setting large  X  . Meanwhile it can be proved that there exists a lower bound ( w 0 /2) for  X  , which guarantees no loss of most recent information.

The second error source is the approxim ation of piecewise smoothing. How-ever, if the sequence is low-frequency dominant, the error introduced by piecewise smoothing is small, and can even be zer o. It has been proved in [16] that for sequences with low frequencies, smoothi ng introduces only small error in com-puting cross correlation, while Keogh et al [12] show that the dynamic time warping after smoothing (PDTW) is a tight approximation for the DTW of the original series.
 Lemma 2. Piecewise smoothing introduces small errors in estimating the gen-eralized correlation, given the sequences are low-frequency dominant. Proof. Combine the two facts above, this conclusion is straightforward. 4.2 Complexity TWStream is efficient in terms of both time and space complexity. Specifically, the space required to store the snapshots is O (log t ) and the amortized time for update per time unit is O (1). If output is required, the time required for the computation of GC values and interpolation is O (log t ).  X  For each series of length t , TWStream has to keep snapshots for  X  log t  X  For each series, we maintain the slidin g windows for the most recent data at  X  For the same reason, on average, out of log t sliding windows, only one To evaluate the effectiveness and efficien cy of our approach, we performed exper-iments on both synthetic and real data. We compared TWStream with the exact implementation, aiming to answer the following questions: (1) How well does the estimation of TWStream approach the exact correlation? (2) How does the time warping influence its sensitivity to correlation? (3) How does the computation time of TWStream scale with the sequence length t ?
All the experiments are performed on a PC with Pentium IV 2.67 GHz and 512M memory, running Linux. The synthetic dataset we used is the sine/cosine , which consists of two sequences of length 65536, as shown in Fig.6(a). Each series is composed of a mixture of sine/cosine waves of different frequencies. The real life series come from the intraday trade and quota data provided by the NYSE Trade and Quote database. We chose two sequences of length 31200 for our experiment, as shown in Fig.7(a). The default setting of the parameters is: w 0 = 32,  X  = 16, and correlation threshold  X  =0.8. 5.1 Effectiveness Fig.6 and Fig.7 show the estimation of TWStream for synthetic and real data respectively. In each case, we randomly take two snapshots from the interpolated surface, one for fixed scale (scale = 512) and the other along one interpolation curve, as plotted in Fig.6(b) and Fig.7(b). The dotted line represents the GC values computed by the exact implemen tation, while TWSteam computes corre-lations for snapshots, and approximates the missing values by interpolation. It is evident that in both cases, TWStream tig htly approximates the exact method.
For both data sets, at different time points, we measured the number of high correlations (larger than  X  ) detected by three methods: exact method with time warping (Exact I), exact implementation without time warping (Exact II), and TWStream. The results are listed in Fig.8(a). It is shown clearly that TWStream can detect high correlation as effective a s the ExactI most of the time, and the relative error is typically around 1%.

We also measure the influence time warping has on the algorithm X  X  sensi-tivity to high correlation. As can be seen in Fig.8(a), the number of detected high correlations using time warping (Exact I) is significantly larger than that without time warping (Exact II), which indicates that the time warping makes the method more sensitive to the out-of-phase correlation, that can hardly be detected by canoni cal correlation. 5.2 Efficiency Fig.8(b) illustrates how the wall processing time of TWStream and Exact I varies as the length of sequence grows. It can be noticed that the computation time of exact implementation increases nearly quadratically with the sequence length. In contrast, the increase in the processing time of TWstream is unnoticable. This confirms our theoretical analysis, that is TWStream requires constants update time, and the computation of GCs and interpolation have the complexity of O (log t ), which causes the insignificant increase. Typically, TWStream performs 10 5 times faster than the exact method when the sequence length reaches 1e+6. We tackled the problem of monitoring multiple data streams and finding corre-lated pairs in real time. The correlated patterns can occur on any unknown scale, with arbitrary lag, or even out of phase. W e proposed the concept of generalized correlation to capture such  X  X ny-scale X ,  X  X ny-time X  and  X  X ny-shape X  correlations. In our method TWStream, we use careful approximations and smoothing to achieve a good balance between scalability and accuracy. The experiments on both synthetic and real data confirmed the theoretical analysis: our approach worked as expected, detecting the gene ralized correlations with high accuracy and low resource consumption.

