 Active learning may hold the key for solving the data scarcity problem in supervised learning, i.e., the lack of labeled data. Indeed, labeling data is a costly process, yet an active learner may request labels of only selected instances, thus reducing labeling work dramatically. Most previous works of active learning are, however, pool-based; that is, a pool of unla-beled examples is given and the learner can only select ex-amples from the pool to query for their labels. This type of active learning has several weaknesses. In this paper we propose novel active learning algorithms that construct ex-amples directly to query for labels. We study both a specific active learner based on the decision tree algorithm, and a general active learner that can work with any base learning algorithm. As there is no restriction on what examples to be queried, our methods are shown to often query fewer exam-ples to reduce the predictive error quickly. This casts doubt on the usefulness of the pool in pool-based active learning. Nevertheless, our methods can be easily adapted to work with a given pool of unlabeled examples.
 I.2.6 [ Artificial Intelligence ]: Learning X  induction, knowl-edge acquisition Algorithms Active learning, classification, supervised learning
Active learning is very important in classification tasks in machine learning and data mining, as it may hold the key for solving the data scarcity problem, i.e., the lack of labeled data. 1 Indeed, labeling data is a very costly process. For example, in webpage (or image, movie, news articles, face) classification, it is crucial to have a set of correctly labeled examples for supervised learning, yet the labels are often given by human experts, and thus, it is a costly and time-consuming process. Active learning, on the other hand, is able to actively request labels of a small number of examples, and thus, reducing the labeling cost significantly. However, most previous work of active learning is  X  X ool-based X ; that is, a pool of unlabeled examples is given, and the learner can only select examples from the pool to query for their labels. (See a review of pool-based active learning in Section 2). The pool-based active learning has several weaknesses. First of all, the computational time of most previous pool-based learning algorithms is high. This is because most pool-based methods must evaluate each example in the pool to see which one is most  X  X ncertain X  or  X  X nformative X  (see Sec-tion 2). Sometimes new models for each additional example in the pool are built. If the pool is relatively large, the time for deciding which example in the pool to be selected is of-ten quite significant. Second, as examples to be selected must be from the pool, they can be quite limited, especially if the pool is small, and thus, they may not be effective in reducing the error rate rapidly. Third, the pool of unlabeled examples themselves must be collected first, which can be a time-consuming process.
 In this paper we propose novel Active learners with Direct Query construction (called ADQ for short) and query for their labels. This is also called  X  X embership query X  stud-ied previously but mostly in theoretical setting [1]. More specifically, we first study a specific active learner based on the decision tree algorithm (called Tree-ADQ) to construct its queries. Then we propose a general active learner that can work with any base learning algorithm (called wrapper-ADQ, as it is like a wrapper enclosing any base learning algorithm). As there is no restriction on what examples to be queried, our ADQ algorithms are shown to often query fewer examples to reduce the predictive error quickly. Fur-thermore, our ADQ can also be easily adapted to work with a given pool of unlabeled examples. Our ADQ algorithms are also shown to be more time-efficient than the traditional pool-based methods.
Another promising research is semi-supervised learning, such as co-training.
Though our methods of direct query construction can be regarded as a special case of the pool-based method, when the pool is assigned to contain all of the rest of the examples not in the training set, such a method is often dreadfully in-efficient. This is because with a large number of attributes, the size of all possible examples is exponentially large (to the number of attributes). For example, if the training set con-tains web pages on politics, then the constructed pool would include all other possible web pages (meaningful or mean-ingless ones), and this number is huge (if the total number of words in the page is bounded, the number of possible pages is finite but huge). Thus, the traditional pool-based meth-ods would be extremely inefficient to choose which example to label. Our ADQ can construct examples directly to query, and does not need a pool of unlabeled examples. Another potential argument against our work is that the constructed examples may not be valid. For example, in learning hand-written digits, the learner may construct an image dissimilar to any digit. This can be easily handled. In binary classi-fication, the label of such invalid examples can simply be labeled as negative. In multi-class cases, a new class, such as  X  X nvalid X , can be created for labeling all invalid exam-ples. However, sometimes the examples and feature values are different. For example, webpages are examples, but are transferred to feature values (a vector of word counts) for the learning algorithms. The active learner will only con-struct new feature values (vectors of word counts), which may be difficult to be labeled by human. We will study this issue in our future research.

The rest of the paper is organized as follows. Section 2 re-views previous works on active learning. Section 3 describes our tree-based active learner Tree-ADQ, and Section 4 de-scribes our wrapper-based active learner Wrapper-ADQ. In both cases experimental comparisons are conducted to com-pare ADQs with the traditional pool-based methods. Sec-tion 6 presents conclusions and future work.
The most popular type of active learning is called  X  X ool-based X  active learning. A pool of unlabeled examples is given, and the learner can choose which one(s) to label dur-ing learning. Many works have been published in recent years on pool-based active learning, including, for instance, [18, 20, 15, 16, 2, 9, 10]. 2 In these previous works, each ex-ample in the pool is evaluated, sometimes extensively eval-uated by building new learning models with each example added in the current training set (e.g., query by committee), to decide which example is most uncertain [15, 10], or most informative if the label is known [20]. As there is no restric-tion on what examples to query, our ADQ can often query fewer examples to reduce the error rate quickly. Also, pool-based methods are more time consuming, especially when the pool is relatively large, than our active learners with direct query construction (ADQ). See Sections 3 and 4 for details.
We have only included several typical works published in recent years. See [2] for a good review of active learning approaches.

Active learning with  X  X embership queries X  can construct examples and request labels; however, as far as we know, very few works have been published. Some are theoretical study in the PAC learning framework (e.g., [1]). [5] pro-poses a version-space search algorithm in neural networks for query construction but the algorithm works only for sim-ple concepts. As we discussed in Section 1, although it can be regarded as pool-based learning when the pool contains all possible examples not in the labeled training set, such an approach is very inefficient. Last, in the  X  X tream-based X  active learning, the learner is given a stream of unlabeled examples, and the learner must decide, for each example in the stream, if or not to request its label ([6, 17, 11]). This approach can be regarded as an online version of the pool-based active learning.

Some recent works on active learning turn to a new direc-tion of feature-value acquisition at cost during learning. In the work of ([8, 7]), a fixed budget is given, and the cost of feature value acquisition cannot exceed this hard budget.
In this section we propose a specific active learning algo-rithm with direct query construction based on decision trees (thus, it is called Tree-ADQ). Though the basic idea can be modified and applied to other base learners, Tree-ADQ is ap-plicable to the decision tree learning algorithm specifically; that is, a different method might be needed if naive Bayes is used as the base learner. In Section 4 we will describe a generic method that can be applied to any base learning algorithm.
The general idea of Tree-ADQ is straightforward: it tries to find the most uncertain examples from the current de-cision tree (or other learned model), and request labels for those examples. The most uncertain examples are those whose predicted probability, either for the positive or nega-tive examples, is most uncertain, or close to 50%. For exam-ple, a predicted positive example (or negative example) with 50% probability is most uncertain, and with 100% is most certain. Given a decision tree constructed from the labeled examples, we can find, for each leaf, its predicted label and its probability, and find the most uncertain leaves.
More specifically, Tree-ADQ consists of three general steps. For each iteration, the predictive error rate on the (separate) test examples is monitored and used to plot the error curves (see Section 3.2).

Details of each step above are described below.
In Step 1, the standard decision learning algorithm C4.5 [14], implemented as j48 in Weka [19], is used to construct a pruned decision tree from the current labeled examples.
In Step 2, the uncertainty of each leaf in the tree is de-termined by its error rate upper bound. More specifically, the error rate of the each leaf is first calculated as the ratio of the incorrectly classified examples to the total number of examples in the leaf. However, the ratio itself is often not a good indicator of how uncertain the leaf is, especially when the leaf contains a small number of examples. For example, if a leaf contains only 5 examples and 1 of them belongs to a minority class, then 20% is not a reliable estimate of the true error rate of the leaf, as the number of examples (5) is too small. Statistically we can put an error bound on the ratio, and obtain a pessimistic estimation of the true error. We use the normal distribution to approximate the error rate distribution, and use 95% confidence to estimate the error bound. Thus, the error rate upper bound is calculated as follows to represent the pessimistic uncertainty of the leaf [12]: where error ( h ) is the error rate of each leaf, n is the total number of examples in the leaf, and the constant 1 . 96 re-flects a 95% confidence interval in the normal distribution. Then, new examples can be constructed in the most uncer-tain leaves. To increase the variety of the newly constructed examples from different uncertain leaves, new examples are sampled from leaves with the sampling probability propor-tional to their error rate upper bounds. This way, more un-certain leaves have high probabilities to be sampled. When a leaf is sampled, a new example is constructed in this leaf. Its attribute values are determined as follows: for attributes appearing on the path from the root to the leaf, their values are determined by the attribute values on the path. Other attributes (not on the path) are assigned random (valid) attribute values. Clearly, the time complexity of the con-struction is only linear to the tree depth, and the number of attributes (thus, linear to the number of attributes). The Tree-ADQ is thus time-efficient.

In step 3, the constructed examples are queried to obtain their labels, and then included in the labeled training set for the next iteration.

As we mentioned in Section 1, Tree-ADQ can also be adapted easily to work with a pool of given unlabeled ex-amples effectively (without evaluating each example in the pool as in the traditional pool-based approaches). We use the following simple strategy to modify Tree-ADQ when a pool is given. In each iteration, the most uncertain exam-ples are still directly constructed following the Steps 1 and 2 of Tree-ADQ described above. Instead of querying the label of these examples (as the Step 3 in Tree-ADQ without the pool), we calculate the Euclidean distance between the con-structed example and all examples in the pool, and choose the closest (or most similar) one in the pool to be queried. This way, examples to be queried are selected from the pool, and they are similar to the queries constructed directly by breast-cancer 9 277 196/81 breast-w 9 699 458/241 colic 22 368 232/136 credit-a 15 690 307/383 credit-g 20 1000 700/300 diabetes 9 768 500/268 heart-statlog 13 270 150/120 hepatitis 19 155 32/123 sick 29 3772 3541/231 sonar 60 208 97/111 tic-tac-toe 10 958 332/626 vote 16 435 267/168 Tree-ADQ. We call such a method Tree-ADQ-p.
In this subsection we experimentally compare Tree-ADQ and Tree-ADQ-p with the traditional pool-based active learn-ing algorithm. The traditional pool-based active learner se-lects the most uncertain examples (using the decision tree) from the pool to be labeled.
We conduct experiments to compare Tree-ADQ algorithms with the traditional pool-based active learning using 12 real-world datasets from the UCI Machine Learning Repository [3]. These datasets have discrete attributes and binary class without missing values. Information on these datasets is tabulated in Table 1. Each dataset is randomly split into three (3) disjoint parts: 20% as the initial labeled training examples, 20% as the testing examples, and the rest (60%) as the  X  X nlabeled X  examples (examples given in the pool).
One problem when using the ADQ algorithms on the UCI datasets is how queries constructed by ADQ are labeled if such queries are not in the original dataset. In such cases the labels are unknown. We use the following approach to solve this problem. We first construct a pruned decision tree (using J48) based on the original, whole dataset, and des-ignate this tree as an approximate target function. Then, this tree is used to answer queries constructed by Tree-ADQ (and other active learning algorithms with direct query con-struction proposed in this paper). Clearly, this tree is the closest we can get from a given dataset when the true target function is unknown.

Both Tree-ADQ, Tree-ADQ-p, and the traditional pool-based active learner (simply called pool) are implemented in Weka [19] with J48 (i.e., C4.5) as the decision tree algorithm. For traditional pool-based active learner, a decision tree is constructed based on the current labeled dataset. Then each example in the pool is evaluated by the tree, and the most pessimistic error rate (see Section 3.1) is calculated. The most uncertain examples are then selected to query for their labels, and added into the training set. For each iteration, 1, 5 or 10 examples are queried and added into the training set, depending on the size the dataset. Table 2: Summary of the t-test on the average error rates.

The experiment is repeated for 100 times for each dataset, and the average predictive error rate on the separate test sets and time of running different algorithms are plotted and recorded.
Figure 1 plots the curves for the predictive error rate on the test sets of the 12 UCI datasets for the three methods (Tree-ADQ, Tree-ADQ-p, pool) in comparison. From these figures, we can see clearly that for most datasets, Tree-ADQ has the lowest predictive error rates on the test sets. Only in one dataset ( X  X ic-tac-toe X ), Tree-ADQ performs slightly worse. To quantitatively compare the learning curves (which is often difficult if one curve does not dominate another), we measure the actual values of the error rates in 10 equal-distance points on the x-axis. The 10 error rates of one curve are compared with the 10 error rates of another curve using the two-tailed, paired t-test with a 95% confidence level. The results are summarized in Table 2. Each entry in the table, w/t/l , means that the algorithm in the corresponding row wins in w datasets, ties in t datasets, and loses in l datasets, compared to the algorithm in the corresponding column.

From the table, we can see that Tree-ADQ is better than pool in 11 datasets, ties in 0 dataset, and loses only in 1 dataset ( X  X ic-tac-toe X ). This indicates clearly that Tree-ADQ produces lower predictive errors on the test sets com-pared to the traditional pool-based active learner in most datasets. We can also see that Tree-ADQ is much better than Tree-ADQ-p (wins in 11 datasets, ties in 0, and loses in 1). Both of these results seem to indicate limitations of the pool. Tree-ADQ is free to choose whatever examples best for reducing the predictive error rates, and this is a major advantage of the ADQ algorithms. Section 5 further demonstrates that the pool is actually putting a harmful limitation on the pool-based active learning. We will show that the larger the pool, the better the pool-based active learners perform. This casts some doubt on the common assumption of the pool in the traditional pool-based active learning.

From Table 2, We can also compare Tree-ADQ-p with the traditional pool-based active learner (pool), and see that Tree-ADQ-p is better than pool in 2 datasets, ties in 6 datasets, and loses in 4 datasets. This indicates that Tree-ADQ-p is comparable to (or only slightly worse than) the traditional pool-based active learner. This is expected as both methods choose examples from the same pool.

The computer time used for these active learners in com-parison is reported in Table 3 on the largest dataset (the  X  X ick X  dataset which has most examples). The computer used is a PC with an Intel Core 2 Quad Q6600 (2.67 GHz) Table 3: Running time on  X  X ick X  for active learning algorithms in comparison.
 CPU and 4G memory, and the computer time is reported in seconds. From Table 3, we can see that Tree-ADQ is most efficient, and Tree-ADQ-p is similar but still faster than the traditional pool-based active learner (pool).
In the previous section we described a specific ADQ based on the decision tree algorithm. Tree-ADQ is very efficient in constructing examples to reduce the error rate quickly but the algorithm only works on decision trees. In this section we propose a generic ADQ that can work with any base learning algorithms that can produce the probability estimation for the prediction. As it can  X  X rap X  around any base learning algorithm, we call it Wrapper-ADQ.  X  X rapper-ADQ X  X ses the standard hill climbing algorithm to search for the most uncertain examples outside a learning algorithm to query their labels. More specifically, it starts with a randomly generated new example. Then every at-tribute value of this example is changed to a different value once a time, and those new examples (with one attribute value difference) are evaluated by the base learning algo-rithm (which returns its predicted label and its uncertainty). The one that is most uncertain is retained (i.e., greedy hill climbing) and the process repeats, until the example is not altered from the last iteration. The final unaltered example is queried for the label and then added into the training set. Note that, though the greedy hill climbing may only find the  X  X ocally X  most uncertain example instead of the  X  X lobe X  one, it still works well because it increases the diversity of new examples constructed.

The Wrapper-ADQ does rely on accurate and more dis-criminating probability estimates of the prediction. If we use a single decision tree as the base learner, its probabil-ity estimates are not fine enough, as examples in the same leaf are assigned the same probability. Thus, we use an ensemble of decision trees as the base learner here. We use the basic process of bagging, as it has been shown to produce the tree-based classifier with accurate probability estimates [13]. More specifically, 100 decision trees are first constructed from the current labeled examples as in the ran-dom forest [4]. That is, a bootstrapping new training set is drawn from the original training set (bagging), and a de-cision tree is grown on the new training set using random attribute selection (for all attributes with non-negative en-tropy reduction). This way, a variety of different trees can be constructed. For each query evaluated in the hill climb-ing search, each tree X  X  prediction (with its own uncertainty estimation; see Section 3.1) is combined to produce a final prediction and its uncertainty. the curve, the better. Table 4: Summary of the t-test on the average error rates
We use the same datasets and similar experiment setup to compare Wrapper-ADQ and pool-based active learning (called pool, which also uses an ensemble of 100 trees to choose the most uncertain examples from the pool; it is also called  X  X uery by committee X  in active learning). Similarly, Wrapper-ADQ can be adapted easily to work with a pool of given labeled examples by first constructing the most uncer-tain examples, and then finding the one in the pool that is closest to them. Such a method is called Wrapper-ADQ-p. All of these three methods (Wrapper-ADQ, Wrapper-ADQ-p, pool) are compared and the results are reported in Figure 2 and Table 4 (with the same notation as in Table 2).
We can see that Wrapper-ADQ is better than pool in 10 datasets, ties in 2 datasets, and loses in 0 dataset. This indi-Table 5: Running time on  X  X ick X  for active learning algorithms in comparison. cates clearly that Wrapper-ADQ produces lower predictive errors on the test sets compared to the traditional pool-based active learner, which is still comparable to Wrapper-ADQ-p.

The computer time used for these wrapper-based active learning algorithms in comparison is reported in Table 5. We also draw the similar conclusion that Wrapper-ADQ is the most efficient, and Wrapper-ADQ-p still outperforms the traditional pool-based active learner (pool). The difference is not as large as the tree-based approaches (Table 3) because here, a lot of the time is spent on the hill-climbing search. Table 6 (with the same notation as in Table 2) compares Tree-ADQ and Wrapper-ADQ on the 12 datasets used in our experiments. It shows that Tree-ADQ wins in 7 datasets, whole unlabeled set, U 2 is equal to U 1 plus another quarter of the whole unlabeled set, U 3 is U 2 plus another quarter, and U 4 is the whole unlabeled set. Thus, these unlabeled sets (pools) are all from the original real dataset with known labels. They increase in size, and U 1  X  U 2  X  U 3  X  U 4 .
Tree-ADQ with the pool (Tree-ADQ-p), Wrapper-ADQ with the pool (Wrapper-ADQ-p), and the traditional pool-based learning algorithm (pool) are applied with different unlabeled sets ( U 1 to U 4 ). The results are shown in Figure 3. We can see that, overall, active learners with the large pools perform better. This is particularly evident in the traditional pool-based algorithm (pool). We can see that at the beginning of the query process, the algorithm with different pool sizes has a similar error rate. Then the differ-ence starts to emerge. The algorithm with U 4 , the largest pool, performs the best (the lowest error rate), with U 2 U 3 performs similarly, and with U 1 performs the worst (the highest error rate). For Tree-ADQ-p and Wrapper-ADQ-p, the results are somewhat mixed but when more and more examples are queried, we can still see that algorithms with larger pools have slightly lower error rates in general. Simi-lar results have been observed in other datasets as well.
In this experiment we use a smaller dataset  X  X onar X . As  X  X onar X  has the most attributes (60 attributes and 10 at-tribute values for each attribute), we can easily generate ar-tificial examples not in the original dataset. This time, the original real dataset is split into three disjoint subsets: 20% for the labeled training set, 20% for the test set, and 60% for the initial pool of unlabeled set (called U 1 ). Artificial examples are randomly generated (with a uniform distribu-tion on attribute values) and incrementally inserted into U forming U 2 , U 3 , and U 4 . The size of U i is i times the size of U . Thus, U 1 is a part of the original dataset, but U 2 to U include increasingly more artificial examples. We also have U 1  X  U 2  X  U 3  X  U 4 .
 Similarly, Tree-ADQ with the pool (Tree-ADQ-p), Wrapper-ADQ with the pool (Wrapper-ADQ-p), and the traditional pool-based learning algorithm (pool) are applied with dif-ferent unlabeled sets ( U 1 to U 4 ). The results are shown in Figure 4. These results are somewhat mixed, but we can still see that algorithms with U 1 , the smallest pool, perform the worst (the largest error rates). Algorithms with larger pools in general perform similarly and better than with U Similar results have also been observed in other datasets.
These experiment results indicate that, indeed, the pool, especially when it is too small, limits the scope of examples to be queried for active learners. This casts doubt on the pool assumed in most previous work of active learning. This also further confirms the advantage of our ADQ algorithms (Tree-ADQ and Wrapper-ADQ)  X  they construct queries directly without the restriction of the pool, and often reduce the predictive error rates more quickly, as shown in Sections 3 and 4. Most previous active learning algorithms are pool-based. Our work indicates that the pool has several weaknesses. It may limit the range of examples to be queried for effective active learning. In our work, we eliminate the pool com-pletely by allowing the active learners to directly construct queries and ask for labels. We design a specific Tree-ADQ algorithm based on decision trees, and a generic Wrapper-ADQ algorithm that can use any base learning algorithms. Our experiments show that our ADQ algorithms can con-struct queries that reduce the predictive error rates more quickly compared to the traditional pool-based algorithms. The ADQ algorithms are also more computationally effi-cient. Our algorithms can also be adapted easily to work with the pool if examples have to be selected from a given pool.

Co-training is another promising research for solving the problem of the lack of labeled data. In co-training, a set (pool) of unlabeled examples is given, and is used to improve supervised learning. In our future work, we will study the usefulness of the pool in co-training. [1] D. Angluin. Queries and concept learning. Machine [2] Y. Baram, R. El-Yaniv, and K. Luz. Online choice of [3] C. Blake, E. Keogh, and C. J. Merz. Uci repository of [4] L. Breiman. Random forests. Machine Learning , [5] D. A. Cohn, L. Atlas, and R. E. Ladner. Improving [6] Y. Freund, S. H. Seung, E. Shamir, and N. Tishby. [7] R. Greiner, A. J. Grove, and D. Roth. Learning [8] A. Kapoor and R. Greiner. Learning and classifying [9] M. Lindenbaum, S. Markovitch, and D. Rusakov. [10] D. D. Margineantu. Active cost-sensitive learning. In [11] A. Mccallum and K. Nigam. Employing em and [12] T. M. Mitchell. Machine Learning . McGraw-Hill better. the better. [13] F. Provost and P. Domingos. Tree induction for [14] R. R. Quinlan. C4.5: programs for machine learning . [15] N. Roy and A. Mccallum. Toward optimal active [16] M. Saar-Tsechansky and F. Provost. Active sampling [17] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [18] S. Tong and D. Koller. Support vector machine active [19] I. H. Witten and E. Frank. Data Mining: Practical [20] T. Zhang and F. J. Oles. A probability analysis on the
