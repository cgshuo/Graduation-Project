 1. Introduction
This paper describes the development of a machine learning junction control system that employs pattern matching techni-ques including logistic regression and neural network classifica-made by the human expert and the state of a simulated network as described by simulated localization probe data. 1.1. Background
In the United Kingdom urban signalized road junctions are usually controlled by one of two systems, MOVA ( Vincent and 1982 ), which can coordinate multiple adjacent junctions. Both these systems use sensors, including inductive loops ( Sreedevi, 2005 ) and microwave emitter/detectors ( Wood et al., 2006 ), to detect the presence of vehicles at fixed locations on the roads around the junction. The data from these sensors are used as a descriptor of the state of the network by the control algorithms to inform decisions on which colour to set the traffic lights.
Data collected from counts of vehicles at fixed locations are called census data. Previous reviews (e.g. Rose, 2006 ) have suggested that localization probe data, that are dynamic position and speed data from on board vehicle sensors, can present a different view of the state of the network. The European Commis-sion has recently invested significant resources in three major and vehicle to vehicle (V2V) communications ( Kompfner, 2008 ;
COOPERS, 2010 ; SAFESPOT, 2010 ). Furthermore common Eur-opean protocols have been set for this type of communication (IEEE 802.11p). This has laid the ground for this technology to become commonplace in Europe in the near future. This techno-logical advance would enable localization probe data to be collected and employed in urban traffic control (UTC) systems. 1.2. Context and motivation
Early work to investigate the scenario of localization probe data in signal control has employed simulation to develop and evaluate control methods. Box and Waterson (2010a) presents an auctioning agent control method. This work showed the auctioning agent approach outperforming MOVA in simulations on an isolated T-junc-agent system was subjected to a rigorous quantitative stochastic accuracy and the fraction of vehicles equipped with probes.
The simulation test bed used for the work presented in this paper is described in detail in Box and Waterson (2010a) , Waterson and Box (accepted for publication) . In summary: it uses
S-Paramics microsimulation software to model networks and simulate the movement of individual vehicles through junctions.
Built around this are a number of bespoke software modules for simulating localization probe data, making control decisions, and implementing control directly in the simulation.

Box et al. (2010) showed that a human interface layer can be connected to the simulation test bed allowing an expert human to control the signals at simulated junctions. Results indicated that an expert human controller can outperform both MOVA and the auctioning agent approach from Box and Waterson (2010a) in terms of delay across the junction.

This motivates the development of machine learning junction control systems that can mine the data generated when a human expert controls simulated junctions and emulate human control strategies under automated control. Box et al. (2010) also demon-strated how the auctioning agent method could be adapted, employing the pattern recognition technique of logistic regres-sion, to create a learning junction agent .

In this paper both the auctioning agent system and the learning agent are developed further. The principal contributions are as follows: 1. An updated structure for the auctioning agent method intro-ducing the lane agent . 2. A new learning junction agent, which employs a two layer neural network with back propagation to learn strategies from a human expert. 3. A comparison between the logistic regression and neural network learning junction agents including variation in the resolution of the training data. 4. Simulation tests carried out on a two junction network, which models the High Road area of Southampton, UK.

Other important works where pattern recognition and machine learning techniques have been applied to junction control include ( Choy et al., 2003 ; Mikami and Kakazu, 1993 ; Chen and
Heydecker, 2009 ). This work has shown how to use neural networks and other techniques to optimize certain parameters the work presented here, machine learning techniques are used to select signal control decisions by directly classifying state space using evidence data generated by a human expert ( Section 3 ). 2. Signal control strategies overview Simulation tests were carried out on two network models.
Fig. 1 shows a view of the first, the Simple T-junction. This is an isolated junction with three signal stages. Fig. 2 shows a view of the second, the High Road junction. This is a model of the High Rd area of Southampton, UK. It consists of two signalized junctions a short distance apart. The westerly junction has four signal stages and the easterly junction has three.

This section presents an overview of the junction control strategies that were investigated using simulations on these junc-tions. These are MOVA, auctioning agents using the High Bid method and auctioning agents using the learning junction agent. 2.1. MOVA
The MOVA control strategy ( Vincent and Peirce, 1988 )isa common strategy that is employed on many isolated junctions in the real world, therefore it is used as a baseline for junction control performance in these tests. The MOVA control strategy was tested on the Simple T-junction only. The S-Paramics simula-tion models 11 inductive loop detectors (examples are marked in from these detectors serve as inputs to the MOVA algorithm. In fact the Simple T-junction model was designed by The Transpor-tation Research Laboratory (TRL) as an exemplar for MOVA control to accompany their MOVA X  X -Paramics API. The full details of the MOVA set up for this model are given in TRL (2007) . 2.2. Auctioning agents
The auctioning agent system defines the state of the network using bids . Sections of road or lane in the network models are monitored by lane agents . A lane agents receives data from vehicles in the simulation whose reported position indicates that they are in that agent X  X  section. The lane agent then uses the to be a measure of the need for priority at the next junction coming from that section of road. Eq. (1) below shows how the bid B is calculated: B  X  Here C is the set of all vehicles monitored by the lane agent. V vehicle speed and X c is the distance of the vehicle from the junction. a and b are coefficients which can be tuned to adjust the relative influence that the number of vehicles, the vehicle speed and the vehicle distance each have on the size of the bid. In previous work ( Box and Waterson, 2010b ) it has been shown that the terms). These values are adopted in this work.

The lane agents fit into a larger hierarchical agent structure shown in Fig. 3 . Above the lane agents are stage agents, one for each stage of each junction. Stage agents receive data from any lane agents whose lanes receive the green light from that stage. As an example: Fig. 4 shows a simple network containing two junctions, each with the same two-stage structure. Considering stage 1 of the West junction: the lane agents assigned to this assigned to more than one stage agent (see Fig. 3 and Section 4 ).
High Bid control : Each stage agent generates an initial bid, which is simply the sum of the bids of all its lane agents. Above the stage agents in the hierarchy is the junction agent. The simplest type of control that the junction can employ is High -Bid control. In this scenario the junction agents request bids from the stage agents at a pre-set time interval called the auctioning green light to the stage with the highest bid.

Coordinated High Bid control : In the case where two junctions are closely spaced, as in Fig. 4 , it can be advantageous to coordinate their control. The High Bid control method can be extended to control such junctions with the addition of a zone agent above the junction agents in the hierarchy ( Fig. 3 ). Each
The zone agent then picks the junction with the highest overall stage bid to lead the coordination. This means that the winning junction continues as normal and assigns priority to the stage with the highest bid. The zone agent then assigns some of the lane agents from the winning junction to the stage agents of the losing junction to encourage coordination.

An example of this is shown in Fig. 4 . In this example the East junction is the winning junction and East stage 1 is the winning stage. If controlled independently then the West junction X  X  stage discussed above). However, in the coordinated case lane agent ( c ) its bid based on the longer distance to the West junction and the bid is also multiplied by a pre-set coefficient to account for the 1 of the West junction to take into account the vehicles that are going to be released by the East junction. That is the principle coordinated High Bid control. Detailed descriptions of this control method applied to specific junctions are given in Waterson and
Box (accepted for publication) . 2.3. Machine learning auctioning agents
The auctioning agent system can be adapted, using a machine learning approach, to include a learning junction agent that can be the zone agent is discarded because coordination can be achieved with junction agents operating independently ( Fig. 5 ).
Human interface : The human interface to the simulator is both a junction control method itself and a tool for training the learning junction agent. The human interface consists of a screen and keyboard. The screen displays a realistic 3D scene of the network with simulated vehicles driving around. As with High Bid control, called the simulation is paused and the user is prompted on screen to assign priority to one stage for each of the junctions in the simulation. Thus the human user controls the simulated junction.
While human control is taking place the lane and stage agents are madebythehumanarestoredinadatabaseforofflinetrainingthat
State descriptor : Unlike High Bid control where the stage the learning junction agent considers the bids from the stage agents simply as a description of the state of the network. Because bids from the lane agents directly, bypassing the stage agents altogether (see Fig. 5 ). Because there are generally more lane agents than stage agents they can provide a potentially richer description of the state of the network, although this is at the expense of increasing the dimensionality of the problem to be solved (see Section 3 ). In the experiments presented in Section 4 both the stage agent bids and the lane agent bids are used to describe the network. We adopt the terminology of referring to the stage agent bids as the short bid data and the lane agent bids as the long bid data. Interestingly although the bids are no longer used as proxies for the need for priority the fact that they are designed as such has important implications for the solution of the learning problem that will be described in Section 3 .
Junction coordination : The learning junction agent is not restricted to considering agents that belong only to that junction but can also consider those of neighbouring junctions. If the human expert who trains the junction agents attempts to coordi-nate them in her decision making then this should result in patterns between bids and coordinated decisions. Therefore, in the experiments presented here, there is no explicit coordination of junction agents, rather they operate independently and if the patterns are captured by the learning junction agents then coordination should emerge under automated control. 3. Learning agent
There is a number: ( J 1) of agents that are used to describe the state of the network for the learning junction agent. The bids from these agents define a J 1 dimensional bid space . Each new set of bids in the future will define a point in this bid space. In order to be able to assign all possible future combinations of bids to a given signal stage we need to divide this bid space up into regions that correspond to signal stage decisions.

Such a division of bid space can be done using the data generated by the human expert controller. The expert X  X  decisions represent patterns between points in bid space and signal stage decisions, therefore they can be used as evidence for how the bid space should be divided. In the field of machine learning this is a standard classification problem and there are a number of ways to approach it (for examples see Bishop, 2006 ). In this paper we will layer neural network with back propagation . Both approaches are closely related as will be explained below. 3.1. Multi-class logistic regression
The approach of multi-class logistic regression is to fit a prob-ability function to the bid space for each of the K possible stage decisions. Thus for each new point in bid space we can determine here is the softmax function (2) ( Bishop, 2006 ): p  X  k 9 b  X  X  exp  X  a k  X  P where a k is a linear function of the vector of bids b : a  X  w T k b  X  3  X  the error between the probability surface defined by (2) and the evidence data provided by the human expert. The probability given for a given stage k conforms to any probability distribution in the exponential family (e.g. Gaussian) ( Bishop, 2006 ). 3.1.1. Learning the parameters In this section we show how to learn the parameters pattern b n in the set of N patterns we can define a target vector t which has K elements t nk , which equal 1 if b n is associated with stage k and 0 otherwise.

Ameasureofthe difference between the probability functions and the evidence data is given by the cross-entropy error function: E  X  W  X  X 
The values of W that minimize E  X  W  X  are found numerically using the Newton X  X aphson update formula:
W where the second and third terms are, respectively: the second and first derivatives (Hessian matrix and gradient) of the cross-entropy error function. The precise method for calculating these is given in Box et al. (2010) . This approach is known as iteratively re-weighted least squares. W is initialized randomly using the approach recommended by Nabney (2002) shown below: w N 0 1 ffiffiffiffiffiffiffiffiffiffiffiffi
Typically the above algorithm is run a number of times to avoid a result in a poor local minimum. 3.1.2. Applying the logistic regression
Once the values of the parameters W have been learned, the probability that a new point in bid space belongs to stage k can be found using (2). Thus a region in bid space where stage k has the highest probability can be defined. This region will be separated from neighbouring regions by J 2 dimensional quadratic bound-aries where both stages have equal probability. This approach to classification can work well where the evidence points for each stage are well separated into regions that can be partitioned by such boundaries. In fact this is why the bids, which are designed of the state of the network in this case. If they are at all stages will be well separated in bid space. In cases where the separation is less clear a more powerful approach may be needed, such as the two layer neural network. 3.2. Two-layer neural network with back propagation
The neural network approach allows a more flexible division of the bid space where stage k can exist in multiple regions separated from neighbouring regions by J 2 hyper-surfaces that can have complex shape. The neural network achieves this by applying a transformation function to the bid space that arranges the evidence points for each stage into positions that are more favourable for a logistic regression. That is to say that after the transformation the softmax function fits the evidence data with lower error that it would if it were applied before the transforma-tion. This transformation function can be parametrized and the parameters learned from the evidence data in the same way as the parameters for the softmax function are learned in the logistic regression.

The standard structure for the neural networks used here is a two layer network with J input units, H hidden units, and K output units. Here J is the number of bids plus one, K is the number of signal stages and the number of hidden units H is a variable that can be tuned to control the complexity of the transformation. A low number of hidden units can limit the complexity of the transformation giving a poor fit to the evidence data. A high number of hidden units allow more complexity in the transfor-mation but can lead to over fitting of the data. 3.2.1. Forward propagation
The J dimensional bid vector b is defined as in Section 3.1 . For each pattern there is also a K dimensional target vector t with elements t k A f 0 ; 1 g , where P k A K t k  X  1.

Layer 1 : The first layer of the neural network forward propaga-tion transforms the bid vector b onto the H dimensional hidden units vector z using the following transformation: z  X  tanh  X  a  X  1  X  h  X  X  7  X  where W  X  1  X  is a H J matrix of parameters (to be learned).
Layer 2 : The second layer of the neural network forward propagation substitutes the new transformed bid z into the softmax function: y  X  where W  X  2  X  is a K H matrix of parameters to be learned. 3.2.2. Error back propagation
The error E between the output of the neural network y and the target vector t is measured using the cross-entropy error function: E  X  We define the K dimensional error vector d  X  2  X  using d This error is back propagated to the first layer of the neural network using (13) to give the H dimensional first layer error d surface with respect to the weights in each layer is given by r E  X  W  X  2  X   X  X  d  X  2  X  z T  X  14  X  r E  X  W  X  1  X   X  X  d  X  1  X  b T  X  15  X  3.2.3. Learning the parameters
The forward propagation and back propagation of error described above relate to a single pattern in the evidence data.
To calculate the error and gradient for the whole data set of N patterns we simply sum them r E  X  W  X  X 
The values of the parameters that minimize the error E can be learned by gradient descent using the following iteration step:
W new  X  W old Z r E  X  W  X  X  17  X  where Z is the learning rate that can be adjusted dynamically during the gradient descent to control step size (e.g. reduced if error increases).

The elements of W are initialised randomly using (6) and in the tests presented in this paper the gradient descent is per-formed 20 times and the result with the lowest error is selected to avoid results in a poor local minimum. 4. Simulation experiments
Simulation tests, designed to evaluate the junction control strategies described in Sections 2 and 3 , were carried out on the two networks shown in Figs. 1 and 2 .

All simulation tests covered a simulated 4 h period. During the tests the junctions in the simulations were controlled either by
MOVA or by one of the auctioning agent algorithms. Auctioning agent tests used either the High Bid method, or the learning agent method and the auctioning rate was d t  X  10 s in all cases.
Learning agents were trained ( Section 4.4 ) using either the logistic regression (Logit) method or the two layer neural network with back propagation (neural net). Finally for each of these training methods either the stage agent data (short data) were used to describe the network state or the lane agent data (long data) were used. 4.1. Demand profile
Each experiment used the same demand profile. Demand is set between origin and destination zones, these are at the end of each modelled road. The demand between zone i and zone j ( D ij vehicles per minute) is a function of the basic demand between i of the simulation time (Eq. (18)):
D
The transient demand multiplier causes the level of demand to vary over the time of the simulation. Under normal operation a junction will usually experience two peaks in demand over a 24 h period i.e. in the morning and evening rush hours. To conserve simulation time all simulation tests presented here were carried out over a 4 h period. This represents a  X  X  X ompressed day X  X  with peaks in demand at 1 h and 3 h. Fig. 6 shows the profile of the demand multiplier over the test period.

The basic demands represent the permanent trend in demand between particular origins and destinations. The basic demand matrices for the Simple T-junction and the High Road junction are given in Tables 2 and 4 , respectively. 4.2. Delay calculation
During the simulations S-Paramics records detailed informa-tion about the journeys of every simulated vehicle. In the analysis presented here the main measurements used are journey time t its origin i to its destination j is its journey time t p to travel between i and j if it were unimpeded by other vehicles or two times: y  X  t 4.3. Test junctions 4.3.1. Simple T-junction Fig. 1 shows a view of the model of the Simple T-junction.
Fig. 7 shows a schematic of the junction indicating the staging and the road sections that are monitored by lane agents.

Agents : Fig. 7 shows that there are four lane agents in this and these sections cover the entire length of the modelled road.
Table 1 shows which lane agents are assigned to which stage agents in the auctioning agent hierarchy.

Demand : The basic level of demand d ij in vehicles per minute between the origin and destination zones in the Simple T-junction model is shown in Table 2 . 4.3.2. High Rd junction Fig. 2 shows a view of the model of the High Road junction.
Fig. 8 shows a schematic of the junction indicating the staging and the road sections that are monitored by lane agents.

Agents : Fig. 8 shows that there are nine lane agents in this network labelled a  X  i . Each lane agent section is indicated and these sections cover the entire length of the modelled road.
Table 3 shows which lane agents are assigned to which stage agents in the auctioning agent hierarchy.

In the case of coordinated High Bid control agent reassignment takes place following the principle outlined in Section 2 .
A detailed description of agent reassignment for a similar two junction network model can be found in Waterson and Box (accepted for publication) .

In the case of machine learning control there are two learning junction agents and both of these agents receive data from all of the stage/lane agents in the network.

Demand : The basic level of demand d ij in vehicles per minute between the origin and destination zones in the High Road junction model is shown in Table 4 . 4.4. Training phase
Training of the learning junction agent took place in a separate series of simulations called the training phase .Thesesimulations above.

Training sessions were short (30 simulated minutes) with a constant level of demand. For each network (Simple T and High demand. The aim of this approach was to minimize the amount of training time required and make sure that the learning junction agent had equal exposure to data generated at different levels of demand. The total demand between each origin i and destination j was calculated using the following equation:
D where d x is the demand multiplier for test x . Table 5 shows the demand multipliers used in each of the six training sessions.
Number of hidden units : As discussed in Section 3.2 , when using the neural network approach to learn strategies the complexity of 0.2 0.4 0.6 0.8 Transient demand multiplier the fit can be controlled by varying the number of hidden units used in the neural network. Table 6 shows the number of hidden units used for each of the neural networks employed in these tests. These values were selected based on best performance on new simulated data. 5. Test results 5.1. Simple T-junction
This section presents results from simulation tests carried out on the Simple T-junction model. Five different control strategies were used for the tests presented here: MOVA, High Bid, Logit (using both long and short bid vectors) and neural network control (using both long and short bid vectors).

The second column of Table 7 compares the measured value of delay for the six strategies tested. Here delay is averaged across all journeys for the duration of the test. This shows that MOVA, the baseline strategy, is outperformed by all other strategies.
The High Bid strategy outperforms MOVA, despite its simpli-city, due to the fact that the localization probe data provide a richer description of the state of the network than the inductive loop data. Of the machine learning strategies tested, the Logit algorithm using the short bid vector has the lowest performance. Previous research has shown this strategy outperforming High
Bid on an isolated T-junction and matching human performance ( Box et al., 2010 ). The main difference between the tests pre-sented here and the results previously published is the varying demand profile. In the previous tests demand was constant during training and tests. Introducing the profiled demand has increased the complexity of the problem leaving the Logit (short) 500 1000 1500 2000 2500 3000 3500 Frequency 500 1000 1500 2000 2500 3000 3500 Frequency 500 1000 1500 2000 2500 3000 3500 Frequency 500 1000 1500 2000 2500 3000 3500 Frequency strategy less effective. When using the long bid vector the Logit algorithm acts on a bid space with higher dimensionality ( n  X  4vs n  X  3) and is able to capture some of this higher complexity. The Logit (long) strategy does outperform High Bid. The neural net-work strategies can capture more complexity in the training data the bid space. Neural network control using the short bid vector matches the performance of Logit (long), while neural (long) is the best performing strategy with a 25 % reduction in delay over the MOVA baseline.

Fig. 9 shows the transient delay traces for four strategies (MOVA, High Bid, Logit (long) and neural network (long)). Here delay is averaged across all journeys that ended within a 5 min These results show that the performance of the strategies is similar throughout most of the test, with the exception of the two peaks in demand at 1 h and 3 h. This indicates that the algorithms that perform better do so because they deal better with high levels of demand and can postpone or avoid the situation of saturated flow, which leads to high levels of delay.
While delay is the single most informative metric on the performance of the signalized junction it does not tell the full story. It is also important that the junction is equitable. For example a strategy that holds a single vehicle at a red light for 1 h could still achieve a low average delay if significant numbers of other vehicles are passing through the junction with low delays. However, this scenario is not acceptable in real world is of particular concern for the High Bid and machine learning strategies, which do not explicitly consider temporal effects in their decision making.

Fig. 10 shows the journey time distribution for four of the strategies tested (MOVA, High Bid, Logit (long) and neural (long)). The distribution for MOVA shows the peak journey time is in the 30 X 40 s range with significant numbers of journeys up to 120 s. There are also outlier journeys up to 300 s in length. The distribution for the High Bid strategies indicates that it is out-performing MOVA by getting more journeys into the peak 30 X 40 s range. However, the fact that temporal effects are not considered in the High Bid algorithm shows in the fact that there are significant numbers of journeys with times up to 150 s. There are also outliers up to 300 s. The machine learning algorithms are not explicitly given any temporal information, however, the human expert who generates the training data will be considering temporal effects when making decisions. Therefore this may be captured by the machine learning algorithms if there are patterns relating these decisions to the instantaneous state of the network.
The distributions for the Logit and neural network strategies show that they have less journeys in the peak 30 X 40 s range than High and no journeys longer than 190 s. The distributions show that the machine learning strategies, which are the best in terms of delay, are also the most equitable strategies. 5.2. High Rd junction
The following sections present results from tests of the control strategies carried out on the High Road junction model. The third column of Table 7 shows delay, averaged across all journeys for
Logit (long), neural (short) and neural (long)). These results show poor junction control. This is because the division of the J 1 dimensional bid space into regions separated by J 2 quadratic boundaries is insufficient to capture the complexity of the relationship between the network state and the human control-ler X  X  decisions in the more complex High Road junction system.
However, the neural network strategies can capture some of the complexity of the human decision making process on the High
Road junction and these algorithms outperform High Bid. The best performance is achieved by neural network control using the long bid vector, with a reduction in delay of 10 % over High Bid. Fig. 11 shows transient delay traces for three strategies (High
Bid, neural (short) and neural (long)). Here the results show the same trend that was seen with the Simple-T results. The better performing strategies show lower delay mainly at the two peaks in demand at 1 h and 3 h. This indicates that these strategies achieve their performance advantage by better controlling flow at high demand and avoiding or postponing the saturated flow condition.

Fig. 12 shows the distribution over journey times across the (short) and neural (long)). The distribution for the Logit strategy indicates why its performance is poor. While it has approximately the same amount of trips in the 30 X 40 s peak as the High Bid beyond 1000 s. Interestingly the neural network strategies do not have as many vehicles getting through the junction quickly as the
High Bid and Logit strategies with their peaks in the 40 X 50 s Time (mins) range. However, the distributions have low variance, with almost no trips above 150 s. 5.3. Stranded vehicles
A potential floor in the auctioning agent method is that it evaluates the state of the network using an instantaneous snap-shot of vehicle position and speed and does not consider any temporal effects. A manifestation of this potential floor would be inequality in the delay experienced by different vehicles passing through the network. An example of this would be a single vehicle held at a red light for an indefinite length of time (the stranded vehicle problem ).

The High Rd junction network in Southampton is particularly apt to testing the stranded vehicle scenario as the southern arm of the West junction leads to a small industrial car park and has very low demand both as an origin and as a destination (see Table 4 ).
In fact this problem does arise under High Bid control, while the distribution of journey times presented in Fig. 12 is generally tight there are a few outliers with one unlucky vehicle spending 16 min in the simulation. However, it is a different story under neural network control. Here the human controller is considering temporal effects and while the learning junction agent does not receive any explicit temporal information the instantaneous bids may still be informative. The results show that the neural net-work strategies are the most equitable strategies that were tested, more so even than MOVA. On the High Rd junction test the longest vehicle journey under neural network (long) control was 3.5 min. 5.4. Stochastic analysis
In all the experiments described so far in this paper perfect information has been assumed. That is to say that it has been assumed that vehicle position is known and accurately commu-nicated by every vehicle in the simulation. Or in the case where inductive loops have been used for MOVA controlled simulations it has been assumed that all detectors are working and detect every vehicle that passes over. In the real world localization sensors in vehicles would be subject to noise and errors making vehicles would have working systems.

This section we examine results of simulation tests where uncertainty in position estimates and reduced fractions of instru-mented vehicles has been modelled. The method used to model uncertainty has been described in detail in Waterson and Box (accepted for publication) . To summarize: the position estimates from the simulated localization systems are degraded by the addition of Gaussian noise with 1 s ranging between 2 and 32 m.
The number of vehicles in the simulation that is equipped with simulated localization systems is varied between 5% and 100%. 200 400 600 800 1000 1200 1400 Frequency 200 400 600 800 1000 1200 1400 Frequency 200 400 600 800 1000 1200 1400 Frequency 200 400 600 800 1000 1200 1400 Frequency Fig. 13 shows results for 12 sets of simulation test on the Simple T-junction controlled by the neural network (long) con-troller and 12 simulation tests on the High Road junction controlled again by the neural network (long) controller. Each test consisted of 10 simulation runs and the results are averaged. simulated position of the vehicles had Gaussian noise with a different variance added to it. The graph shows plots of delay averaged across all vehicles. This shows the expected trend of increasing delay with increase in the uncertainty of position. Significantly higher delays are observed in the case of the Simple T-junction. The reason for this is that the demand placed on the Simple T-junction in these tests was comparatively higher than that placed on the High Road junction (see Tables 2 and 4 ). This means that if a poor decision is made on the Simple T-junction there is a higher chance that this will lead to long queues or even saturated flow. This unstable behaviour at high demands was also observed in similar tests looking at the High Bid algorithm that was published in Waterson and Box (accepted for publication) .
Theplotontherightin Fig. 13 shows results for six pairs of tests show average delay increasing approximately exponentially with reducing fraction of instrumented vehicles. These results indicate with reasonable performance maintained down to 20%. The plot for the Simple T-junction experiments shows a steeper increase in delay quite a low percentage (20 X 40%) of the vehicle fleet instrumented, this simulation results indicate the low delay is achievable. 6. Conclusions
The principal results of the tests presented in Section 5 are summarized below:
High Bid control can outperform the MOVA control system on an isolated junction in terms of delay. This is largely due to the fact that the auctioning agent control system uses localization probe data to describe the state of the network. This is a richer source of information than the inductive loop data employed by MOVA.

Tests using machine learning auctioning agents showed that in all cases the neural network method produced signal control strategies that out performed those produced using the logistic regression method. This was due to the more flexible division of bid space enabled by the neural network approach.

In all cases the long data set performed better than the short data set. This was due to the fact that there were more lane agents and therefore more information. In principle the num-ber of lane agents in a network can be varied without upper limits. This means it is possible to increase the resolution of information in the state estimate at the cost of increasing the dimensionality of the bid space.

Transient delay traces indicated that the performance of most of the control strategies was similar when the demand was low and that the main differences in performance occurred in the peaks of demand. This suggests that the strategies that are performing better are achieving this by postponing or avoiding the condition of saturated flow at high demand.

Analysis of the journey time distributions showed that neural network control was the most equitable strategy and robust to the stranded vehicle problem.

Stochastic tests indicated that the neural network control strat-egy is robust to variation in the positioning accuracy of localiza-tion probes and to the fraction of vehicles equipped with probes.
This work has demonstrated the potential for applying machine learning techniques to control signalized junctions with expert human data used to train the system. While this super-vised learning task has been shown to be useful there are some human expert trainer the junction controller X  X  performance is that better control is achievable. Second, this approach implicitly assumes that all decisions made by the human are good ones, where in reality we know the human will make mistakes.
Because the positions and speeds of vehicles passing through the junction are being measured it is possible in principle to evaluate decisions and classify them as good or bad. This raises the possibility of feeding this information back into the learning agent as training data, resulting in a junction control agent that learns from experience. A ( Reinforcement Learning ) approach to this problem by the method of temporal differences ( Sutton and Barto, 1998 ) is the focus of the current work in this area. References Average Delay (s) 40
 1. Introduction 1.1. Background
Urban signalized road junctions are usually controlled by active systems (e.g. Vincent and Peirce, 1988 ; Hunt et al., 1982 ), which use sensors to measure the state on the road. The state is then used by the control algorithm to inform decisions on which colour to set the traffic lights. Sensors such as inductive loops ( Sreedevi, 2005 ) and microwave emitter/detectors ( Wood et al., 2006 ) are commonplace and widely deployed in developed areas.
New sensing technologies such as vehicle to infrastructure WiFi communications have been extensively investigated in recent years ( Kompfner, 2008 ; COOPERS, 2010 ; SAFESPOT, 2010 ) leading to an Europe wide reservation of frequencies (IEEE 802.11p) for this type of communication.

The profusion of sensing technology leads to rich data that can be used for Urban Traffic Control (UTC). This enables the develop-ment of increasingly sophisticated control systems for signalized road junctions. In particular, data hungry machine learning algo-rithms can be employed to develop junction control systems that can learn improved strategies through various forms of training.
Recent important work on the optimisation of traffic signals has investigated a number of approaches including dynamic programming ( Heydecker et al., 2007 ; Heung et al., 2005 ), genetic algorithms ( Mikami and Kakazu, 1993 ), fuzzy-neural networks ( Choy et al., 2003 ) and reinforcement learning ( Chen and
Heydecker, 2009 ). This work has shown how to use learning techniques to optimise parameters in signal control strategy or to select pre-defined strategies.

Here we are concerned with a pattern recognition approach where control decisions are made purely based on a classification of state space. Earlier work using this approach has shown how to use supervised learning to enable a junction controller to learn strategies from a human expert trainer ( Box and Waterson, 2012 ).
In this paper the approach is extended by the application of reinforcement learning to enable a junction controller to learn strategies through experience. 1.2. Context and motivation
Earlier work by the authors investigating the use of (vehicle transmitted) GPS  X  WiFi data in signal control has employed simulation to develop and evaluate control systems.

Under the auctioning agent control system ( Waterson and Box, accepted for publication ) the road network is discretized into regions and software agents monitoring each region calculate a bid for priority. The bid is based on the positions and speeds of vehicles as reported over WiFi. At the junction a junction agent assigns the green light to those sections of road with the highest bid . Coordination between junctions is achieved through a zone agent which can re-weight bids to encourage coordination. In simulation tests the auctioning agent system using WiFi data outperformed the MOVA control system ( Vincent and Peirce, 1988 ), which uses inductive loops.
 The human trained neural network control system ( Box and
Waterson, 2012 ) uses the same system of bids as the auctioning agent system. However instead of using the bids as proxies for priority it uses the set of bids as an abstract simplified state describing the situation at the junction. A neural network is used to classify the resulting state space into decisions, namely which stage of the junction gets the green light. The training data for the network is provided by a human expert when they control the simulated junction via a computer game interface. Thus the human trained neural network is a machine learning junction control system that learns strategies from a human expert. In simulation tests the human trained neural network outperformed the auctioning agent control system.

The above control systems were both developed and tested on a simulation test-bed. This uses SIAS-Paramics micro-simulation software to simulate the movement of vehicles through the network. SIAS-Paramics is connected with a number of specially developed software modules to simulate sensor data, make control decisions and implement the control (i.e. change the traffic light colour) in the simulation. The same test-bed has been used in the research presented in this paper. It is described in full in Box and Waterson (2010a , b , 2012) .

There are two principal shortcomings to using human experts to train machine learning junction controllers. Firstly, to imple-ment this in practice would be costly because human time is relatively expensive. Secondly, the best possible performance of the system is limited to being as good as the human trainer. These shortcomings motivate the investigation into extending the supervised learning approach of the human trained neural network to build a reinforcement trained neural network.

As already described, junction control systems use measure-ments to determine the state on the road in order to make control decisions. However the state on the road right now tells us something about the decisions that were made in the past. In principal the controller can evaluate whether decisions made in the past were good or bad and learn from these data just as it learns from the data generated by the human trainer. This is the approach of temporal difference learning.

Research in other applications of artificial intelligence has shown that problems that can be solved using a neural network trained by supervised learning can also respond well to a neural network trained by temporal difference learning. A well known example of this is the work of Tesauro (2002) who developed the computer Backgammon program Neurogammon , which employed a neural network to learn strategies from human expert back-gammon players. He then went on to develop  X  X  X D-gammon X  X , a
Backgammon program that used a neural network trained by temporal difference (TD) learning in simulations where the program competed against itself.

In this paper we present an adaptation to the neural network based junction control system described in Box and Waterson (2012) . This adaptation enables the controller to be trained under simulation by temporal difference reinforcement learning. The principal contributions of the paper are as follows. 1. A new machine learning junction controller, which employs a two layer neural network to learn strategies through temporal difference reinforcement learning. 2. A comparison between the performance of the human trained junction controller and the TD trained junction controller in simulation tests. 3. Simulation tests of performance on both a simple isolated
T-junction, and a pair of closely spaced junctions, where coordination is necessary. 2. Machine learning junction control strategies 2.1. Overview 2.1.1. Bids
When simulating GPS  X  WiFi data from vehicles we can collect estimates of the position and speed of every vehicle in the simulation. At any given time these data describe the state of the network. To make the problem more tractable and to speed up calculation time this raw description of the state is simplified in a pre-processing operation that generates bids .

To affect this the road network around the junction is divided into regions. Fig. 1 shows a schematic of one of the junctions discussed in this paper with the regions marked ( a d ). Each region is monitored by an agent, which calculates a bid based on the position and speed data of the vehicles within that region. The bid is calculated using B  X  here C is the set of all vehicles monitored by the lane agent; V the vehicle speed and X c is the distance of the vehicle from the junction; a and b are the coefficients that can be tuned to adjust the relative influence that the number of vehicles, the vehicle speed and the vehicle distance each have on the size of the bid. In previous work ( Box and Waterson, 2010b ) it has been shown that (assuming S.I. units are used) values of a  X  0 : 01 sm 1 and b  X  0 : 001 m 1 provide a good balance between influences. These values were adopted in this work.

The term  X  X  X id X  X  is used because this method was first employed by the auctioning agents signal control algorithm ( Waterson and Box, accepted for publication ) where this bid was designed to be indicative of the need for priority on a section of road. For example more vehicles increase the bid, slower moving vehicles increase the bid and vehicles closer to the end of the road section increase the bid (and vice-versa). In the work presented here the set of bids from each of the regions is simply a description of the state of the road network. Therefore it is unimportant that the bid is not a true representation of the need for priority. However if it is at least reasonably representative it can make the classification task easier ( Box and Waterson, 2012 ). In the example shown in Fig. 1 there are four regions and thus the state is described by four bids. 2.1.2. Training and using the neural network
Fig. 2 shows a process flowchart of the system described in this paper. At each time step the simulator outputs raw measure-ments of the state on the road. This is then processed to generate bids, these form the input units to the neural network. The neural network then calculates which stage of the simulated junction should have the green light and passes this information to the simulator. The time step used in this paper is d t  X  10 s. This time step is quite course and could be reduced, however the results presented here and in earlier work ( Box and Waterson, 2012 ) demonstrate good comparative performance using this time step.
The decision of which stage to give the green light is governed by the values of the weights in the neural network these weights can be trained either by human input or by TD learning.
Under human training a human controls the simulation through a computer game interface, choosing which stage to give the green light every d t s. Every time the human makes a decision this generates a pattern linking a set of bids ( state ) to a stage decision. These patterns are then used to train the network as described in Section 2.2 .

Under TD learning information contained in the raw state measurement at time step t is used to evaluate the decision made at time step t 1. Feedback is provided to the neural network to adjust the values of its weights accordingly. This is described in detail in Section 2.3 . 2.2. Supervised learning with a human expert
For each pattern recorded there is a list of bids. A unit  X  X  X ffset X  X  element is appended to this list to create the J dimensional bid vector b (where J is the number of bids plus one). For each pattern there is also a K dimensional target vector t with elements t f 0 ; 1 g , where
Layer 1. The first layer of the neural network forward propaga-tion transforms the bid vector b onto an H dimensional hidden units vector z using the following transformation: z  X  tanh  X  a  X  1  X  h  X  X  2  X   X  W  X  1  X  b  X  3  X  where W  X  1  X  is a H J matrix of weights (to be learned).
The length of vector z , called the number of hidden units ,isa variable that can be tuned to control the complexity of the transformation. A low number of hidden units can limit the complexity of the transformation giving a poor fit to the evidence data. A high number of hidden units allows more complexity in the transformation but can lead to over fitting of the data.
Layer 2. The second layer of the neural network forward propagation substitutes the transformed bid z into the softmax function ( Bishop, 2006 ). y  X 
P  X  W  X  2  X  z  X  5  X  where W  X  2  X  is a K H matrix of weights to be learned.
The weights of the network are learned by attempting to minimizing the error between the human generated training data and the network output y k . This is done using back propagation and gradient descent. The method is detailed in Box and Waterson (2012) . In effect the neural network above parametrizes
K functions ( y k ) (recall K is the number of stages), each of these functions can be described as something like: the probability that the human expert would pick stage k given the current bids b . 2.3. Reinforcement learning by temporal difference
Reinforcement learning involves learning the combinations of states and decisions that maximize some cumulative numerical reward. This approach differs from the supervised learning approach described above because there is no  X  X  X rainer X  X  providing explicit examples of correct state-decision combinations, rather learning happens on-line via an exploration of the state-decision space and the receipt of feedback ( Sutton and Barto, 1998 ). Therefore reinfor-cement learning can be said to be learning through experience.
Temporal Difference (TD) is a method for reinforcement learning ( Sutton and Barto, 1998 ; Sutton, 1988 ). In large contin-uous state spaces this approach employs a value function (dis-cussed below) representing the nominal  X  X  X alue X  X  of decisions at points in state space. At each step in the learning process the value of the previous state is adjusted according to both the feedback received and the value of the current state. Thus adjustments to the value function are made even if the receipt of feedback is delayed (bootstrapping).

The TD learning employed here uses the same two layer neural network described in Section 2.2 . Under TD learning the human is no longer involved so we could describe the functions y k the probability that k is the right stage to pick given the current bids b . However, although the values of the K functions will sum to unity across the bid space, we have no reason to believe that the learning process is driving the functions towards valid probabilities so it is usual in reinforcement learning literature to refer to the function y k as the  X  X  X alue X  X  of picking stage k for any given combination of bids b . Thus the functions y k become the value functions for TD learning (discussed above) and the neural network parametrizes these functions. 2.3.1. Q-learning
Under temporal difference learning the value of a stage decision at a particular state, y k  X  b  X  is adjusted upwards or down-wards slightly according to feedback received after that state-decision combination is encountered in training. The algorithm used for doing this in this paper is the Q-learning algorithm ( Watkins, 1989 ), shown below. y  X  b
 X   X  y k  X  b t  X  X  1 a  X  X  a R t  X  1  X  g max where R is the feedback signal. Parameter a is the learning rate, which can be tuned to determine how heavily the functions are adjusted after each decision. Parameter g is the discount factor, which adjusts how much relative weight is given to the feedback from the last decision compared to the long term movement of the function due to all the feedback received. According to (6) at each step where a decision is made the value of the state-decision function from the previous step y k  X  b t  X  is adjusted as a function of the feedback received since the last decision and the maximum value of the current state-decision. We specify here max k because under reinforcement learning we do not automatically select the stage with the highest value as we do with supervised learning. This is discussed further in Section 2.3.3 . 2.3.2. Back-propagation
The Q-learning algorithm shown in (6) assumes that the values ( y  X  b t  X  ) can be adjusted directly, however this is not the case when using a neural network to parametrize the functions. Here we must affect the change in value by adjusting the weights ( W  X  and W  X  2  X  ) of the neural network. This is done through back-propagation. y c , t is defined as the element of y t which corresponds to the chosen stage ( k  X  c ). The gradient of y c , t with respect to the two weights matrices is given by r y r y where d  X  2  X  t is given by d t  X  X  I t Y t  X  y c , t  X  9  X 
I is a K dimensional vector with elements I k  X  1if k  X  c and I otherwise. d  X  1  X  t has H elements d
To update the weights of the neural network the following algorithm is used: where E  X  1  X  is given by
The purpose of the first term in (12) is to convey reward back to previous decisions, other than the immediately previous one, in amounts that decrease as decisions stretch further back in time.
The l coefficient controls how quickly the reward decays. In the limiting case when l  X  0 (12) becomes E  X  1  X   X  r y c , t reward is assigned to the previous decision only. The purpose of setting l above zero is to recognize that multiple decisions leading up to the receipt of reward may deserve  X  X  X redit X  X . The same process in (11) and (12) is performed to update W  X  2  X  2.3.3. Stage selection strategy
The above back-propagation of feedback will adjust the value attached to combinations of bids and stage decisions. In the supervised learning case, when the network has been trained, then in operation the stage with the highest probability (or value) is always selected. In reinforcement learning literature this is called a  X  X  X reedy X  X  strategy. This is not so helpful when learning by temporal difference as it can leave areas of bid-decision space unexplored. To encourage the junction controller to try out new options while learning by TD a strategy called E -greedy is employed. Here the greedy strategy is employed most of the time but occasionally (with some probability E ) the stage decision is made at random. 2.3.4. Feedback signal
In temporal difference learning the feedback signal ( R t  X  1 in Eq. (6)) is designed to give positive  X  X  X eward X  X  if the junction controller makes a good decision and negative reward if a bad decision is made. The reward signal used in this paper is calculated as a function of total vehicle lifetime L . A vehicle is defined to be  X  X  X orn X  X  when it enters a fixed region surrounding the junction (in this paper this is simply the limits of the simulated area). The vehicle  X  X  X ies X  X  when it crosses the junction stop line. Therefore the total lifetime at time t ( L t ) is the sum of the ages in seconds of all vehicles  X  X  X live X  X  at time t .

The reward term R t  X  1 is a measure of the change in lifetime between one time-step and the next. It is calculated as R and is bounded using 2 r R t  X  1 r 2  X  14  X 
Therefore events which lead to a reduction in lifetime such as the release of a queue of vehicles at a junction will receive a positive feedback. Events that lead to an increase in lifetime such as building up a queue will receive negative feedback. 3. Simulation experiments 3.1. Junction models
The simulation experiments presented in this paper used two junction models. The first (shown in Fig. 3 ) is a simple t-junction (Simple-T). The second (shown in Fig. 4 ) is a model of the high road area in Southampton, UK, which contains two signalized junctions a short distance apart (High-Rd).
 Figs. 1 and 5 show schematics of the Simple-T model and the High-Rd model respectively. The schematics indicate the road regions that are covered by the lane agents monitoring the junctions. They also indicate which lanes are given the green light in each of the junction stages. 3.1.1. Demand
The demand in the simulation is set as the number of vehicles per minute that flow between origin and destination zones. There is one zone at the end of each road in the model. At any time the demand between zone i and zone j is calculated as
D where d i , j is a base level of demand between zones i and j and d a demand multiplier. This coefficient can vary over the course of a simulation. The base level demand d i , j between each of the origin and destination zones for the Simple-T model and the High-Rd model are shown in Tables 1 and 2 respectively. In the tables the zones are labelled using appropriate compass points. 3.1.2. Neural network structure
Each of the modelled signalized junctions, one in the Simple-T model and two in the High-Rd model are controlled by a neural network. The standard structure is a two layer network with J input units H hidden units and K output units. Recall from Section 2 that J is the number of bids that describe the state of the network plus one and K is the number of signal stages. The number of hidden units H is a variable that can be tuned to control the complexity of the approximating function. The values of H employed in this paper are the same as those used for neural networks designed for training by a human ( Box and Waterson, 2012 ).

In the Simple-T model X  X  network J  X  5, K  X  3 and H  X  7. In the network controlling the East junction in the High-Rd model J  X  10, K  X  3 and H  X  11. For the West junction J  X  10, K  X  4 and H  X  11.
Note that the networks controlling the East and West Junctions in the High-Rd model take data from all the lane agents in the model as inputs. This allows each junction to  X  X  X now X  X  the state of the other and, although operating independently, coordination can be an emergent property ( Box and Waterson, 2012 ). 3.2. Training phase The training phase for human training is described in detail in Box and Waterson (2012) here we discuss only the training under TD learning as laid out in Section 2.3 .

From Section 2.3 there are four parameters which must be set ahead of TD training. These are the discount factor g , the reward decay rate l , the learning rate a and the probability of random stage selection E . Through a testing process where these para-meters were systematically varied individually values of g  X  1and l  X  0 : 1 were selected on the basis of best performance. Parameters a and E were varied in the three stages of training described below.
The training phase was divided into three stages. To begin with, the weights of the neural networks were initialized ran-domly. Training took place over repeated simulations of fixed duration S with a constant level of demand defined by the demand multiplier d t . Values of S and d t were varied during the training stages as described below.

Training Stage 1. Values a  X  0 : 1, E  X  0 : 1 and S  X  15 min were used. The demand multiplier was initially set to d t  X  0.2 and was thereafter increased evenly in increments of 0.2 up to d t policy of repeatedly restarting the simulation every 15 min and gradually increasing the demand is useful in the early stages of training because with very naive strategies queues in the simula-tion can build up rapidly. Therefore a lot of time can be spent learning how to discharge large queues before it is possible to learn strategies under more normal conditions.

Training Stage 2. Values a  X  0 : 1, E  X  0 : 1 and S  X  30 min were used. Now d t was cycled more rapidly varying as before but incrementing at every simulation restart.
 Training Stage 3. Was as training stage 2 but with a  X  0 : 01 and  X  0 : 01. The reduction in the learning rate and the probability of random stage selection is designed to reduce the exploration of different strategies and encourage more stable learning and improvement on the current strategy.

The duration of the three training stages was different for training on the Simple-T model and the High-Rd model. Table 3 shows the durations used. 3.3. Testing phase
Once the parameters of the neural networks had been trained they were tested on the Simple-T and High-Rd models with the training mode disabled. Test runs were up to 4 h in length and during the tests the demand multiplier d t (Eq. 15) was varied according to the plot shown in Fig. 6 .

During the simulations S-Paramics records detailed information about the journeys of every simulated vehicle. In the analysis presented here the main measurements used are journey time t and delay y .Foragivenvehicle p the time it takes to travel from its origin i to its destination j is its journey time t p .Thevehicle X  X free between i and j if it were unimpeded by other vehicles or red signals.
The delay for vehicle p isthedifferencebetweenthesetwotimes. y 4. Test results
There is no consensus in the literature for what constitutes optimal performance for a signalized junction controller.
A number of metrics can be used to judge performance including vehicle flow rates, speeds, journey times or even emission levels. In this paper we broadly assume optimal performance to mean minimizing delay ( y ) averaged across journeys and time periods and also minimizing the variance of the distribution over journey times. The latter is important because it is an indicator or equitability. A distribution with low variance indicates that all vehicles are receiving reasonably equal treatment. A distribution with higher variance indicates that some vehicles are getting through the junction more quickly than others. 4.1. Performance
After 260 simulated hours of training the neural network controlling the Simple-T model was tested in a 4 h test as described above. The mean delay measured over all journeys in this test was 14.36 s. After 1500 simulated hours of training the neural network controlling the High-Rd model was tested in the same way. The measured mean delay was 20.82 s. Table 4 compares these values of mean delay with those measured in otherwise identical tests where the controlling neural networks were trained by a human expert as described in Box and Waterson (2012) . These data show that the performance of the temporal difference trained neural network is very similar on the Simple-T model, but the delay is 3 s higher on the High-Rd model. The large amount of training and the lower performance of the High-Rd neural network are indicative of the increased complex-ity of this model where two junctions need to be coordinated.
Figs. 7 and 8 show how the delay varied over the course of the tests for the Simple-T and High-Rd models respectively. Here the values plotted at each time are the measured delay over the previous 5 min. Fig. 7 shows that the performance of the human trained and temporal difference (TD) trained networks are very close over the course of the test with the exception of the second peak in delay. Fig. 8 shows a similar trend but with the human trained network more consistently outperforming the TD trained network in the middle 2 h of the test.

While delay is a useful indicator of a junction controller X  X  performance it is also important to consider equitability, indi-cated by the variance of the journey time distributions. Fig. 9 shows the journey time distributions for the tests on the Simple-T model. This shows that while human training very slightly out-performed TD training in terms of delay, TD training very slightly outperforms human training in terms of equitability with a variance of 0.15 vs 0.16 in the journey time distributions.
Fig. 10 shows the journey time distributions for the High-Rd model. Here the human training is very slightly more equitable than the TD training but the difference is again marginal. 4.2. Analysis of training
To investigate how the performance of the TD trained neural network evolved over the course of the training we can take samples of the (partially) trained network weights that were recorded throughout training. These semi-trained weights were tested on short (20 min) tests using the demand profile in Fig. 6 between 165 and 185 min. Fig. 11 shows the total mean delay for 350 tests with weights sampled over the first 80 h of training on the Simple-T model. This shows that, as expected, performance is very bad for the initial randomly initiated weights. Delay in the first test is 210 s. This very quickly improves with delay dropping down below 50 s in the first hour. Thereafter perfor-mance is characterized by long periods of stagnation punctuated by sudden  X  X  X reakthroughs X  X  such as the steps in delay reduction at around 20 h and 40 h. 4.3. Journey time distribution shape Despite similarities in the approach between TD learning and
Dynamic Programming it is not possible (for the system pre-sented here) to analytically prove convergence on an optimal strategy ( Sutton and Barto, 1998 ).

We can expect the optimal control strategy to produce a journey time distribution with mean as close as possible to the free flow travel time, and variance as low as possible. It follows that the journey time distribution under optimal control will have a defined mean, variance and shape .
Previous research has demonstrated that the journey time distribution for vehicles on a link (that is a stretch of road between junctions) can be well approximated by the lognormal distribution ( Montgomery and May, 1987 ; Rakha et al., 2006 ) and this fact has been employed in many models of traffic flow ( Kaparias et al., 2008 ). The evidence for the journey time dis-tribution across junctions is less well established, however it is reasonable to assume that under perfect control the journey times experienced by vehicles should differ by as little as possible from the condition of free flow on a link.

In fact the evidence from the results in Section 4.1 shows that the journey time distributions across the junctions are a good fit to the lognormal distribution. The R 2 value of the fit to an ideal lognormal distribution is shown on each plot in Figs. 9 and 10 .
To examine whether there is a relationship between the performance of a control strategy and its fit to the lognormal distribution we measured the R 2 value of the journey time distributions for the same 20 min tests described in Section 4.2 , but that were carried out on the High-Rd model. Fig. 12 shows the measured R 2 value in these tests plotted against the performance in terms of total mean delay. These data indicate a trend that the better performing strategies are also a closer fit to the lognormal distribution. 5. Conclusions
The results presented in Section 4 show that temporal differ-ence reinforcement learning can be used to train a junction controller from a position of no prior information, where it is changing the lights at random, to a position of high performance.
On the Simple-T junction model the temporal difference trained neural network matched the performance of the human trained neural network in terms of both delay and equitability. In previous work ( Box and Waterson, 2012 ) the human trained neural network has been shown to significantly outperform the auctioning agent junction controller ( Waterson and Box, accepted for publication ) and the MOVA junction controller ( Vincent and Peirce, 1988 ).

On the more complex High-Rd model TD training matched human training in terms of equitability and exhibited lower performance in terms of delay with on average 3 s more delay per vehicle. The High-Rd model was also subject to 6 times more training in terms of simulated hours. This is indicative of the increased complexity of the problem to be solved in the High Rd model where two closely spaced junctions must be coordinated for best performance.
 The performance improvement of the neural networks during TD training was characterized by long periods of stagnation punctuated by  X  X  X reakthroughs X  X  where performance suddenly increased ( Fig. 11 ). This makes it difficult to know if the best possible performance under TD training has been reached and performance may improve further with more training.

The motivations behind investigating TD learning ( Section 1.2 ) were to find an alternative to human training and to potentially outperform human training. While the former has been achieved the latter has not. Future investigations to improve the perfor-mance of TD learning will include more training and use of different feedback signals ( Section 2.3.4 ). However it is of course possible that the performance of the human trained network in this system is very close to optimal.
 References
