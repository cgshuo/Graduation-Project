 This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statis-tical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statis-tic that is strictly distribution-free under the null hypot he-sis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.
 G.3 [ Mathematics of Computing ]: PROBABILITY AND STATISTICS X  reliability ; H.2.8 [ DATABASE MANAGE-MENT ]: Database Applications X  algorithms Reliability, Algorithms Change detection, density test, kernel density estimation
This paper considers the problem of building a rigorous statistical test for detecting change of distribution in mu lti-dimensional data. Such a test would have numerous appli-cations in a variety of disciplines. For example, one may monitor the information about the set of sales observed in the most recent day (in a commercial enterprise) or the pre-scriptions written recently (in a hospital) or the phone cal ls or emails recently observed (in the security domain) and ask : Material in this paper is based upon work supported by the National Science Foundation under grants CCF-0325459, IIS-0347408, and IIS-0612170.
 is the distribution of recently observed data different from what has been observed before?
In developing a test for distributional change, the primary goal should be to construct a test with the greatest power possible  X  that is, with the ability to detect the most subtle changes in the data while still maintaining a low false posi-tive rate. The need for power results from the way that such a test will be applied in a typical data mining scenario. The test will usually be run many times during the search pro-cess, comparing the data against multiple baselines, or usi ng multiple definitions of  X  X ecent X  (day, week, month, etc.). I n such a situation, a multiple hypothesis testing correction such as Bonferroni X  X  inequality would be required to deal with the fact that repeating the test increases the chance for error [14]. This lowers the acceptable false positive ra te for each test. Unfortunately, lowering the false positive r ate always lowers the power. Thus, a test should have naturally high power to be useful in this scenario.
The abstract problem we consider is as follows. We as-sume two sets of i.i.d. samples S and S  X  , taken from two unknown, multi-dimensional, non-parametric distributio ns F
S and F S  X  , respectively. In practice, S is a baseline data set, and S  X  contains the most recently observed data that we wish to test for change. We then define the null hypoth-esis H 0 , which asserts that F S and F S  X  are in fact identical. The goal is to design a proper statistical test that is able to refute H 0 if it is not true. If H 0 is true, then the probability of making an error (where the test says that F S and F S  X  different when in fact they are not) should be at most p , where p is a user-supplied parameter.

For uni-dimensional data, many tests from statistics fit into this framework. However, there has been little at-tention to the problem of extending such tests to multi-dimensional data. Methods such as outlier detection apply to such data [6, 11], but they do not detect a mismatch in distribution among data sets; they check for single points that are not in-keeping with the baseline distribution. Som e work from data mining is relevant to describing changes in multi-dimensional data [4, 3] but it does not address the statistical significance of those changes. Statistical tes ts for significant spatial irregularities such as Kulldorff X  X  spat ial scan statistic [12, 15] and those for detecting disease out-breaks [21] are closely related but are more specific in that they do not check for a generic change of distribution  X  they look for  X  X ot spots X  in the data that may signal some sort of disease outbreak.

In fact, aside from solutions that rely on mapping two or three-dimensional data to a single dimension and applying one of the uni-dimensional tests [13], one of the only tests proposed by the statistics community for this problem was published recently, called the cross-match test [16]. How-ever, as we demonstrate experimentally, the cross-match te st may be of questionable power, and it is computationally ex-pensive, requiring a maximal matching computation over a graph having O (( | S | + | S  X  | ) 2 ) edges. To function, all of these edges must be stored in main memory; even then, the running time is cubic in the size of the data set. One ad-ditional multidimensional change detection test related t o Kulldorff X  X  test was proposed by Dasu et al.[8], but this is a test that relies on a discretization of the data space. Space partitioning schemes tend to suffer from the curse of dimen-sionality. As such, one might expect that the power of the test will suffer in more than a few dimensions  X  an issue that we consider experimentally later in this paper.
This paper X  X  contribution is the definition and experimen-tal evaluation of an alternative test for multidimensional distributional change, which we call the density test . The density test itself makes use of several novel statistical t ech-niques to avoid using a space partitioning. For example, in order to obtain a sensitive distance metric even for high-dimensional data, we make use of a unique Expectation Maximization [9] algorithm in conjunction with a kernel den -sity estimator to infer the baseline distribution.
In addition to this key technical contribution, the den-sity test is experimentally compared with two alternative tests via a total of more than 60 different change detection tasks over 13 real data sets, having from 3 to 26 dimensions each. The strengths and weaknesses of the various tests are carefully considered. The density test is found to have sub-stantially more power (especially for detecting changes al ong multiple dimensions at once) compared to the two existing methods, and has a computational cost that is competitive. The remainder of the paper is organized as follows. In Section 2, we give the high-level overview of the density test. We discuss kernel density estimation in Section 3. In Section 4, we define the test statistic and discuss its null distribution. In Section 5 we explain why we need to run the test on two different directions. Section 6 experimental ly tests the power and accuracy of the density test compared with the two existing alternatives. Related work is covered in Section 7, and Section 8 concludes the paper.
A generic hypothesis test framework can be used to test for change from S to S  X  as follows. First, a test statistic  X  ( S, S  X  ) is defined. In general,  X  may encode any arbitrary computation over S and S  X  . Let  X  be a random variable whose distribution is exactly the distribution of  X  under the null hypothesis. Then in order to refute the null hypothesis at a significance level p , we simply check whether  X  is found to have an extreme value (assuming a one-tailed test, we may check whether P r [ X   X   X  ] is less than p ). If it is, then we can reject the null hypothesis and declare that there has probably been a distributional change observed in the data.
An excellent example of a test that makes use of the generic hypothesis test framework is Kulldorff X  X  spatial sc an statistic [12] which identifies abnormally  X  X ense X  regions in a spatial data set that may signal distributional change in spatial data. First, the data space is discretized into a set of cells, and then the baseline distribution parameters are learned by counting the number of baseline data points falling into each cell. The test statistic  X  used by Kulldorff X  X  method is the maximum value of a likelihood ratio statistic, tested over all possible spatial regions defined by the dis-cretization. The null distribution of the test statistic is ob-tained by the Monte-Carlo method based on the assumption that the underlying distribution is either a non-homogeneo us Poisson distribution or a Bernoulli distribution.
This section describes a new test for distributional change that we call the density test . At a high level the density test works as follows: 1. First, the baseline S is randomly partitioned into two 2. Next, to compute the statistic  X  , we first compute a 3. Due to the Central Limit Theorem, we know that  X 
While the above describes the basic outline of the den-sity test, a few points regarding some specifics of the test bear further discussion in the remainder of the paper. First , we consider the problem of learning a kernel model for S 1 Specifically, we discuss why classical fixed-bandwidth meth -ods are a poor choice, and propose a data-dependent learn-ing algorithm for computing a more appropriate kernel model . Second, we consider a few issues associated with computing  X  . Third, we show how to obtain the null distribution of  X  by applying the Central Limit Theorem. A bootstrap re-sampling technique is proposed to estimate the variance of the null distribution. Fourth, we discuss why (for maximum power) the density test should actually be run in two differ-ent directions: checking for a change from S to S  X  and also checking for a change from S  X  to S .
There is a tradeoff between | S 1 | and | S 2 | . A larger | S gives more accurate density model, but the variance of the null distribution will increase due to the smaller size of | S (see Section 4). Currently we use two equal-sized halves. Determining an optimal partitioning is an avenue for future research.
In order to compute  X  , the density test makes use of a kernel density estimator (KDE), which is one of the most common non-parametric approaches for learning a data dis-tribution. A KDE approximates the data distribution via superposition of many individual kernel functions at point s sampled from the distribution that is being modeled. By al-lowing each sample to spread its density to nearby areas of the data space, kernel estimators smooth out the contribu-tion of each observed data point over a local neighborhood.
Formally, let x 1 , . . . x | S If S 1 is smoothed using a KDE, then the estimated density at any point s is:
In this equation, G is the kernel function. One kernel is centered at each of the data points in S 1 .  X  x i is the bandwidth or spread of the kernel function centered at x i and in the case of a k -dimensional Gaussian kernel (which is used by the density test),  X  x i is a k  X  k positive-definite co-variance matrix.

It is generally acknowledged that in practice, the quality o f a kernel estimate depends less on the shape of kernel defined by G than on the bandwidth of each kernel. Unfortunately, in defining the density test we faced a significant techni-cal hurdle with respect to choosing the kernel bandwidth: virtually all applications of the KDE method (and almost all papers in the literature) make the implicit assumption that all kernels in the model share the same bandwidth; that is,  X  x i =  X  x j for any ( x i , x j ) pair. Since the power of the density test fundamentally relies on the quality of the kernel model and its ability to determine when data are ab-normal, such an assumption is very problematic. Different portions of the data space tend to have different character-istics in  X  X eal-life X  multi-dimensional distributions. O ften, there is a clear boundary past which it is clear from the data that no density exists; kernels close to the boundary should have bandwidths that project no density past this boundary. Kernels in an occupied but sparse portion of the data space should have large bandwidths. Kernels in dense regions of the data space should have tighter bandwidths. Visualiza-tions of high dimensional data may show  X  X pokes X  radiating out in different directions from a central hub in the data. In such a situation, the bandwidth within each spoke should be elongated along the spoke but not allow the projection of density into the empty areas within the spokes.
Given the dearth of appropriate methods for modeling such real-life distributional characteristics, we choose to use a variation on the standard statistical method of maximum likelihood estimation to choose the kernel bandwidths that were most likely to have produced the data. Rather than attempting to maximize the likelihood (or log-likelihood) directly as is usually done, we instead attempt to maximize the so-called  X  X seudo log-likelihood X  of the model over all possible choices of each  X  x i , defined as:
Note that the pseudo log-likelihood is nothing but the log-likelihood of S 1 over the model, with the constraint that it is known that no data point was generated by the kernel centered at it. Thus, the probability that a data point is generated by itself is zero. We do not maximize the log-likelihood directly because each data point occurs exactly on the center of a kernel. Thus, a model constructed via a direct maximization of the log-likelihood would simply ass o-ciate a zero-bandwidth kernel with each data point, so that G ( X  x i , x i  X  x i ) is always infinity.
 To perform this maximization, we derive an Expectation-Maximization (EM) algorithm. Since the kernel model with Gaussian kernels can be viewed as an instance of a Gaussian Mixture Model (GMM) with | S 1 | components, we can use a simple variation on classical Gaussian EM in order to learn the  X  X ptimal X  bandwidths. Actually, applying EM to learn the bandwidths is somewhat simpler than applying EM on a general GMM because the centroids of the Gaussian com-ponents are fixed at the data points of S 1 , and all Gaussian components have equal weight 1 | S do not need to be learned in classical GMM.

The remainder of this section assumes a basic understand-ing of the EM framework, and how it is used to learn a GMM. Many excellent tutorials cover this subject (Bilmes X  tutorial [5] is one of the best known and most complete).
Assume S 1 has dimensionality k . Let  X  1 , . . .  X  | S Gaussian kernels centered at x 1 , . . . x | S update rule for the bandwidth of the kernel associated with data point i is:
In Equation (3), P (  X  i | x j ) is the probability that x generated by kernel associated with x i . This probability is often referred to as the  X  X oft membership X  of point x j in Gaussian component  X  i . The soft membership is computed with the formula given in Equation (4). The second line of the equation is customized to our particular situation, where we take into account the constraint that the j th data point cannot be generated by the kernel centered at that j  X  X , j = 1 , 2 , . . . | S 1 | : 8 &lt; :
In this equation, p ( x j |  X  i ) is the density of the i kernel at the point x j and it is calculated by Equation (5). Again, the second line in the equation is given because of the special pseudo log-likelihood objective function: 8 &gt; &lt; &gt; :
Now we can summarize how we optimize the bandwidth of the Gaussian kernels in Algorithm 1. According to this algorithm, we stop the computation whenever the iteration number exceeds the maximum iteration number allowed, or when the objective function L converges toward its opti-mal value. We decide that the objective function has con-verged if the fractional difference of two consecutive L val-ues is less than  X  . In our implementation,  X  = 0 . 01 and MaxIteration = 100. Algorithm 1 LearnBandwidth( S 1 , MaxIteration ,  X  ) 1: t = 0 2: while t &lt; MaxIteration do 3: Compute density p ( x j |  X  i ) for all i, j by Equation (5) 4: Compute soft membership P (  X  i | x j ) for all i, j by 5: Compute bandwidth  X  x i for all i by Equation (3) 6: Compute the objective function L t by Equation (2) 8: break 9: end if 10: t + + 11: end while Ultimately, the answer to this question is determined by the accuracy of the resulting change detection test, but it is ea sy to give some anecdotal evidence of how effective the KDE resulting from the EM method described above is.

For example, consider the following simple experiment over the 24-dimensional Streamflow data set (see Section 6 for a description of this data set). We first sample 1000 points from the distribution associated with the data set, and then learn two different kernel models from those 1000 points. The first is learned using Scott X  X  rule [17] , which is a standard, uniform bandwidth selection method. The second is learned using the EM algorithm. Figure 1: Samples from the original distribution (top), a KDE using Scott X  X  bandwidth (center), and a KDE learned using our EM method (bottom).

We then sample three additional 1000-point samples. One from the original distribution, one from a standard band-width estimator, and one from our EM estimator. Projec-tions of these three samples onto two dimensions (the 23rd and 24th dimensions of the Streamflow data set) are shown in Figure 1. It is very clear from the three plots that a great deal of information regarding the original distribut ion has been lost through the application of Scott X  X  estimator. Sampling from the resulting KDE seems to have created a large and shapeless  X  X lob X  of points. On the other hand, the EM estimator seems to do an excellent job of preserving the characteristics of the original distribution. The experim ents in Section 6 will show how sensitive the resulting KDE can be when it is used to detect changes in high-dimensional data.
In the density test,  X  is defined using the log probability density of K S 1 at S  X  and at S 2 . Specifically,  X  is simply the difference between the log-likelihood (LLH) of S  X  and the scaled log-likelihood of S 2 under the inferred density function K S 1 :
Since both S 2 and S 1 are from F S , if S  X  is quite different from S 2 , K S 1 will be more likely to generate S 2 than to generate S  X  , which will result in an extreme small negative value for  X  . Thus, a hypothesis test using the above test statistic is a lower one-sided test.

This particular test statistic is chosen for three reasons: 1.  X  calculated under the null hypothesis is a sample from 2. The expected value of mean of  X  is always zero, which 3. The variance of  X  can be estimated in a robust fashion
For these reasons, the density test is easily applicable sin ce the null distribution has a known parametric distribution and only one parameter must be estimated.
In this subsection, we consider the distribution function of  X  in detail.
We can apply the Central Limit Theorem to argue for the normality of  X  by expanding the formula for the test statistic  X  :  X  = LLH ( K S 1 , S  X  )  X  | S
Under the null hypothesis, S  X  and S 2 are two sets of i.i.d. samples from F S . That means S  X  and S 2 are sample points from a set of independent random variables whose probabil-ity distribution is F S . Assume S  X  is generated by the set of random variables  X  T 1 , T 2 , . . . , T | S  X  |  X  , and S
Beginning with Equation (7), the random variable used to produce  X  can then be denoted as:  X  = where,
It is well known that if we apply a measurable function to a random variable, the result will still be a random variable . Since both f and g are measurable functions, both f ( T i and g ( T i ) are random variables. Let:
Given this, we can make the following three observations with respect to  X  1 : 1. Since all T  X  i s, i = 1 , 2 , . . . follow the same distribution 2. Since all T  X  i s, i = 1 , 2 , . . . are independent random vari-3.  X  1 is the sum of | S  X  | independent and identically-
The Central Limit Theorem can then be applied, and as a result, we know that  X  1 approaches a normal distribution  X  in practice, this happens quite quickly, with only a few doze n samples. A similar argument holds for  X  2 , which also has a normal limiting distribution. In Equation (10) and Equatio n and  X  2 are independent and  X  follows a normal limiting distribution as well.
The normal distribution has two parameters: its mean and variance. To calculate the mean, assume E [ f ( T i )] =  X  . The expected value of  X  is given by:
Similarly we can derive the variance. Assuming Var [ f ( T  X  , the variance of  X  is given by:
Var [ X  X  = Var [ X  1  X   X  2 ]
As long as we know the value of  X  2 , we have the exact null distribution. Unfortunately, this value is unknown an d must be estimated.

The first idea that comes to mind is to estimate  X  2 with the unbiased estimate | S 2 | | S is a bad idea. If this estimator is less than the true  X  2 estimated distribution of  X  will improperly shrink towards the mean. This will introduce an additional type I (false positive) error, that must be quantified.

In order to define an estimator V such that Pr  X   X  2 &gt; V  X  =  X  , we use the bootstrap percentile method [10]. Rather than constructing a two-tailed confidence interval, we need only compute the one-sided (1  X   X  ) upper confidence limit on  X  and then use this upper limit as the estimator of  X  2 . Al-gorithm 2 gives the process, which introduces an additional type I error of  X  .

There are now two sources of type I error. First,  X  could be far out in the tail of null distribution, even if it is a sample from  X . Assume this error is  X  . Second, we could underestimate the variance of  X  by using any particular estimate. This error is denoted by  X  in Algorithm 2. A bound on the type I error is given by p =  X  +  X  .
For a user-supplied p , there is a tradeoff between  X  and  X  . Algorithm 2 EstVar( V , S 2 , estSize ,  X  ) 1: Initialize the array Est 2: for t = 1 to estSize do 3: Bootstrap resample R from S 2 , where | R | = | S 2 | 5: end for 6: V  X  [ estSize  X  (1  X   X  )] th percentile of Est Figure 2: Two distributions that must be checked for deviation in both directions, because one is cov-ered by the other.
 Fortunately, it is easily possible to choose  X  and  X  such that  X  +  X  = p and the power of the resulting test is maximized. To do this, we consider each possible  X  value (each of which corresponds to a different bootstrapped variance), and com-pute the resulting cutoff value for declaring a change result. Over all possible  X  values, we choose the resulting cutoff that is most aggressive, and use that during our actual test. Algorithm 3 gives the process to find such a critical value. Algorithm 3 CriticalValue( p , stepSize ) 1: M = p stepSize  X  1, M is the number of (  X ,  X  ) pairs 2:  X  i = i  X  stepSize, i = 1 , 2 , . . . , M 3:  X  i = p  X   X  i , i = 1 , 2 , . . . , M 4: Let C be an array of critical values 5: for i = 1 to M do 6: Run EstVar( V , S 2 , estSize ,  X  i ) to obtain  X  Var [ X  X . 7: Find c such that P ( X   X  c ) =  X  i , C [ i ] = c . 8: end for 9: C max  X  largest value in array C .
In this subsection, we briefly review the full test proce-dure. First, S is partitioned into S 1 and S 2 . Then EM is used to learn the kernel model over S 1 . Next we calculate the value of  X  for a given pair of S 2 and S  X  through Equa-tion (7). Second, we obtain the critical value C max through Algorithm 3. Third, we compare  X  with this critical value. If  X  &lt; C max , we will declare a change . Otherwise, it means  X  is not significantly small and we will declare noChange .
The density test of Section 4.3 is complete, and can be used exactly as described to check for a change of distribu-tion. However, the power of the test may be substantially increased if it is run twice, once in each direction. This is because the test is not symmetric, and change from S  X  to S may be more obvious than change from S to S  X  . For exam-ple, imagine that F S is a uni-dimensional Gaussian, while F
S  X  is a uni-dimensional mixture of low-variance Gaussians, where Var ( F S ) = Var ( F S  X  ) and E ( F S ) = E ( F S  X  is illustrated in Figure 2, where F S  X  is a mixture of three Gaussians. A typical sample from F S  X  will have data points located only in relatively high-density regions of F S , which means that the  X  statistic may be useless for comparing F and F S  X  . However, a large enough sample from F S will likely have data points located in the low-density troughs between the peaks in F S  X  . This renders a test for distribu-tional change from S  X  to S far more sensitive than a test of change from S to S  X  .
 In order to remove the directionality, we run the test twice. First, the test is run exactly as described in Section 4.3, except that the false positive rate is held at p/ 2. If no change is observed, the test is run a second time in exactly the same way, except that the roles of S and S  X  are reversed. Running the test twice with a false positive rate of p/ 2 for each run ensures that the overall false positive rate is less than p .
This section details a set of experiments designed to test the utility of the density test, as well as two other availabl e tests for multidimensional data. We describe three sets of experiments, each of which is designed to address one of the following questions: 1. Is the false positive rate of our test correctly governed 2. How does the power (false negative rate) of our ap-3. How scalable is our approach compared with the cross-Datasets. Our tests make use of 13 experimental data sets, which are generated from the 13 real data sources described in Table 1. The dimensionality is indicated after each data source.

We require that the size of each data source be reasonably large so that we can sample with replacement in order to ob-tain a true multivariate probability distribution without too many duplicate values. Among the 13 data sources, five have relatively small size: CAPeak, Bodyfat, Boston, FCA-Tread and FCATmath . For these small-sized data sources, we  X  X ump up X  the data set size to 20,000 points while main-taining the distributional characteristics of the data as f ol-lows. We first randomly draw a sample point A from the small-sized data source. We then sample five points, A 1 to A 5 (with replacement) from A X  X  five nearest neighbors. A and A 1 to A 5 are averaged to produce a new data point. We repeat the process 20,000 times.

For our experiments, we divide the data sources into 2 groups, the low-D group and the high-D group . The low-D group consists of data sources from 3 to 10 dimensions. The high-D group consists of data sources from 15 dimensions to 26 dimensions. All three change detection methods are tested using | S | = | S  X  | = 850 for the low-D group. 850 is Table 2: False positive % on low-D group data sets. cross-match 8 7 11 7 7 8 10 Table 3: False positive % on high-D group data sets. density 6 4 7 3 8 10 2 8 kdq-tree 1 1 0 0 0 0 4 0 chosen because this is the largest size that is computation-ally feasible using the cross-match test. However, for the high-D group, more samples are required to accurately de-tect distributional change  X  a consequence of the well-know n  X  X urse of dimensionality X . For this group, | S | = | S  X  | = 3500 is used, and the cross-match test is not evaluated.
To test whether the various methods can correctly con-trol the false positive rate, we sample N (1700 or 7000) data points (with replacement) from a data space source and call these N data points an instance . We repeat this sampling 100 times to obtain 100 instances. Because an instance is sampled with replacement from a finite population, each of the data points in an instance is identically distributed an d any split of the instance in two (without looking at the data) will result in two subsets with the same generative distribu -tion.

To estimate the false positive rate of each of the change detection methods over a data set, we split each instance evenly and run the various tests. The change detection test either says there is a distributional change in this instanc e, or says there is not any distributional change in this in-stance. The false positive of a test can be estimated by computing the fraction of the time that the test refutes the null hypothesis over the 100 instances of a data set.
The p value is 0 . 08 for all the three methods. In the den-sity test, estSize = 4000 in Algorithm 2. In Algorithm 3, p = 0 . 04 (due to the two-directional test) and stepSize = 0 . 002. In the kdq-tree test, we use the same parameter set-tings as in the original paper [8]. All experiments were run on an Intel Xeon machine with dual CPU of 2.8GHz, and 5GB of RAM. We keep this setup for all the experiments throughout this section.
 The false positive results are reported in Table 2 for low-D group and in Table 3 for high-D group. In those tables, D i represents the experiment data set created out of the i th data source given in Table 1. The average false positive rate over 13 experiment data sets is given in the  X  X verage X  column of the tables.
 Discussion. These results show that all the three change detection methods have an average false positive rate at or less than the 8% allowed by the supplied p value, and hence a positive result from any of the tests seems to be a safe indicator of distributional change. Both the density test a nd the cross-match test have a false positive rate very close to the desired p value, and the kdq-tree test tends to be rather conservative.

We have also run similar tests at other p values, and find that the density test also correctly controls the false posi tive rate in this case. For p = 0 . 04 and p = 0 . 06, the average false positive rate over all 13 data sets was 3 . 5% and 4 . 7% respectively. Experiment Setup. In order to test the false negative rate of the various tests (a.k.a the  X  X ower X ), it is necessary to c re-ate an instance h S, S  X  i such that S and S  X  are samples from F S and F S  X  respectively, where F S  X  is not equivalent to F Because any distributional change in the real world takes place gradually, if we take a snapshot of the distributional change at some moment, F S  X  will be a mixture of distribu-tion F S and a different distribution F C . Thus, we choose to model distributional change by creating F S  X  by sampling from some F C with a probability of L and sampling from F
S with a probability of 1  X  L for some parameter L . Unless otherwise specified, F S is the data source described in the beginning of Section 6.

We consider five different types of distributional change, created by five different methods for defining F C . Three of the changes take place in the full-dimensional space, and two are single-dimensional changes. F C is defined as follows. Full-Dimensional Changes gauss : In this case, F C is obtained by sampling from the gmm : In this case, F C is a GMM which has three equal-cluster : We divide the data source into two clusters by a K-Single-Dimensional Changes add1D : In this case, F C is created by adding a standard scale1D : Here F C is created just as in the previous case, ex-
For each data source, we create five experimental data sets, with each one corresponding to one type of distribu-tional change. Each experimental data set contains 100 indi -vidual instances. The parameter L is chosen based upon the characteristics of the data and the type of change. Although the value of L will not affect the fairness of experiment be-cause all the three detection methods are tested on the same sets of data, we still need to carefully choose L in order to make the experiment informative: it should not be too easy to detect the change, nor should it be too difficult. We cali-brate the value of L so that the cross-match test (for low-D group) and the kdq-tree test (for high-D group) can detect around 50% of the changes.
The false negative rates over the low-D group are reported in Table 4. The false negatives of high-D group are reported in Table 5. In order to make our experiments reproducible, we give the values of parameter L for each type of change over all the 13 data sources in Table 6.

Table 4: The false negative (%) of low-D group. changes test avg D 1 D 2 D 3 D 4 D 5 D 6 gauss cross-match 46 47 47 45 66 29 44 gmm cross-match 43 24 46 34 56 53 44 cluster cross-match 47 49 36 67 40 51 41 add 1 D cross-match 60 77 56 62 60 52 51 scale 1 D cross-match 52 49 44 49 68 55 48
Table 5: The false negative (%) of high-D group. changes test avg D 7 D 8 D 9 D 10 D 11 D 12 D 13 gauss kdq-tree 55 44 32 36 61 60 79 70 gmm kdq-tree 63 69 52 69 67 70 56 55 cluster kdq-tree 69 86 57 72 79 60 75 51 add 1 D kdq-tree 75 87 79 75 80 60 71 70 scale 1 D kdq-tree 70 62 72 68 54 96 74 65 Discussion. Table 4 and Table 5 show that the density test is the most powerful out of the three methods compared. Though it is true that there was some variation from data set to data set, it seems clear that the density test is the mos t obvious, general-purpose choice. For each of the five types of changes, the density test had the lowest average false negative rate. Depending upon the type of change, the kdq-tree test and the cross-match test were generally comparabl e on the low-D group, with the former doing better on full-dimensional changes, and the latter doing better on single-dimensional changes.
Our final set of experiments test ability of the three change detection methods to scale to larger data sets.
 Experiment Setup. We test the scalability of the methods in terms of their running time on data sets of different size. The experiment is run on the 24-dimensional data source Streamflow . We sample repeatedly (with replacement) from the Streamflow source to create a series of data sets of size 100, 200, 300, . . . 1000, 2000, . . . 7000.

We record the length of time required to run the change detection test to completion. The density test was imple-mented in Matlab. The cross-match test was implemented in Matlab as well, with the matching algorithm that is required by the test as an external subroutine being implemented in C (the C implementation was the one recommended by the designer of the cross-match test). The kdq-tree test is im-plemented in C. The results are reported in Table 7. Discussion. The kdq-tree has significant performance ben-efits over both the density test and the cross-match test. Given that the kdq-tree relies mostly on a computationally efficient recursive partitioning of the data space, this is no t too surprising. However, the density test still scales to re a-sonable data set sizes (see the discussion in the Conclusion Section of the paper). The cross-match test is generally un-usable past a few thousand data points.

We also point out that the density test has a very high, one-time, fixed cost for the construction of the KDE. After this cost has been paid, the model can be saved and the test can be run repeatedly in a very small fraction of the time reported in Table 7. To illustrate the fraction of the total time associated with the density test that is a one-time cost, consider Table 8. This Table shows the fraction of the running time for the density test that is devoted to one-time computation for each of the data set sizes. The average fraction is around 84%. These results show that if the same algorithm has to be repeated multiple times, the computational cost of the density test is only about 4 times more than the the tree-based method. For a dataset with 7000 points, the overall requirements are on the order of a minute. This makes it practical for a number of applications .
In this section, we briefly review the three tests from the literature that are closest to our own. We also briefly discus s alternative methods for kernel bandwidth selection.
The first test, based upon Kulldorff X  X  spatial scan statistic [12], can be used to find significant spatial clusters in data; the particular application that it was developed for is de-tecting disease outbreaks. This statistic is obtained by fir st partitioning the geographic space into cells which may be of irregular shapes. Then a zone (denoted by z ) is defined as the combined region of any number of contiguous cells which can be enclosed by a circle. Denote the probability that an individual is a case within a zone by p , and use q to denote the probability associated with the region outside the zone . Kulldorff X  X  test statistic is defined as the likelihood ratio of observing the cases inside and outside zone z simultaneously. The null hypothesis for the resulting test is H 0 : p = q , and the alternative hypothesis is H 1 : p &gt; q . The null distri-bution of the likelihood ratio test statistic is obtained by the Monte-Carlo method based on the assumption that the points are generated by either an non-homogeneous Poisson process or a Bernoulli process. Several research groups hav e looked at speeding this test [1, 2, 15].

Though Kulldorff X  X  test does check for a change of dis-tribution, it looks for a very specific distributional chang e. As such, Kulldorff X  X  test is not really suitable for general-purpose change detection. Dasu et al. [8] proposed an information-theoretic approach to detecting changes in mu lti-dimensional data streams that generalizes Kulldorff X  X  test . Their approach depends on a spatial partitioning scheme (called a kdq-tree ) to partition the data space into small cells. Based on the data count in each cell, two empirical dis -tributions are built, representing the reference window an d the testing window respectively. The Kullback-Leibler (KL ) distance is used to measure the distance between the two em-pirical distributions. In order to measure the significance of the obtained KL-distance, they use a non-parametric boot-strap method.

A third test for multivariate distributional change from the statistics literature is the cross-match test [16]. In t he cross-match test, every observation in S  X  S  X  is ranked along each dimension. The rank of an observation x i , denoted by r , is a vector with k elements. Then the inter-point dis-tance  X  ij between two observations x i and x j is defined to be the Mahalanobis distance between their ranks. Formally, the distance is given by  X  ij = ( r i  X  r j ) T M  X  1 ( r M is the sample variance-covariance matrix of the ranks r . After that, all the used to construct an optimal non-bipartite matching, i.e. a matching of the observations into disjoint pairs to minimiz e the total distance within pairs. The cross-match statistic , A , is defined to be the number of pairs containing one ob-servation from S and one from S  X  . A 1 is shown to have a restricted occupancy distribution. Furthermore, the cond i-tional distribution of A 1 given | S  X  | converges to the normal distribution.

Several data-driven methods have been proposed for ker-nel bandwidth selection. The plug-in rule [18] seems to be the most widely used. This method assumes fixed band-width for all the kernels and is not suitable when data ex-hibits local scale variations. The optimal bandwidth selec -tion methods proposed by Silverman [19] and Wand and Jones [20] are of limited practical use for multidimensiona l data as the derivation of asymptotics involves multivariat e derivatives and higher order Taylor expansions. We did not find any method in the the literature that associates differen t bandwidth for each kernel for more than three dimensional datasets. The method proposed by Comaniciu [7] allows for variable bandwidth but assumes that the range of data scales is known and chooses the bandwidth that is the most stable across scales for each data point.
We have described a new test for distributional change called the density test , and evaluated the test via an exten-sive set of experiments. Across all of our experiments, the density test uniformly shows the most power compared to the available alternatives. In terms of running time, the de n-sity test is not the most efficient of the three methods tested, but it does easily scale to data sets that are thousands of points in size and is competitive. Furthermore, for an ap-plication where new data are repeatedly tested against the same baseline, the high one-time cost associated with con-structing the kernel model can be amortized across all runs of the test, lowering the computational cost by nearly 90%. [1] D. Agarwal, A. McGregor, J. Phillips, [2] D. Agarwal, J. M. Phillips, and [3] C. Aggarwal. A framework for change diagnosis of [4] S. Bay and M. Pazzani. Detecting group differences: [5] J. Bilmes. A gentle tutorial on the em algorithm and [6] M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. Lof: [7] D. Comaniciu. An algorithm for data-driven [8] T. Dasu, S. Krishnan, S. Venkatasubramanian, and [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. [10] B. Efron and R. J. Tibshirani. An introduction to the [11] E. Knorr, R. Ng, and V. Tucakov. Distance-based [12] M. Kulldorff. A spatial scan statistic. Comm. in [13] J.-F. Maa, D. Pearl, and R. Bartoszynski. Reducing [14] R. Miller. Simultaneous Statistical Inference . [15] D. Neill and A. Moore. Rapid detection of significant [16] P. R. Rosenbaum. An exact distribution-free test [17] D. Scott. Multivariate Density Estimation:Theory, [18] S. Sheather and M. Jones. A reliable databased [19] B. W. Silverman. Density Estimation for Statistics [20] M. Wand and M. Jones. Kernel Smoothing . Chapman [21] W.-K. Wong, A. Moore, G. Cooper, and M. Wagner. Data source Description Spruce (10-D) Refer to the pine data set.
 Krummholz (10-D) Refer to the pine data set.
 Pitching (22-D) Refer to batting data set.
 FCATmath (26-D) refer to FCATread data set.

S minutes, and h stands for hours. be re-used for multiple tests.
