 University of Cambridge, UK University of Cambridge, UK
Latent Dirichlet Allocation (LDA) model of topical structure in documents; when applied to predicate X  X rgument data, topic models automatically induce semantic classes of arguments and extensions to the model and evaluate them on a va riety of semantic prediction tasks, demon-1. Introduction
Computational models of lexical semantics attempt to represent aspects of word mean-ing. For example, a model of the meaning of dog may capture the facts that dogs are animals, that they bark and chase cats, that they are often kept as pets, and so on. Word meaning is a fundamental component of the way language works: Sentences (and larger structures) consist of words, and their mean ing is derived in part from the contributions mapping between conceptual  X  X orld knowledge X  and knowledge of language.
 linguistic structure in which it appears is not unidirectional; while the word contributes to the meaning of the structure, the structure also clarifies the meaning of the word. Taken on its own a word may be vague or ambiguous, in the senses of Zwicky and specification of additional details that affect its interpretation (e.g., what color/breed was the dog ?). This specification comes through context , which consists of both linguistic and extralinguistic factors but s hows a strong effect of the immediate lexical and syntactic environment X  X he other wo rds surrounding the word of interest and their syntactic relations to it. we develop a framework for modeling word meaning and how it is modulated by are learned from observed co-occurrences between words and contexts in corpus data. automatically induced classes of distributional behavior and associate each word with model of selectional preference , the semantic phenomenon by which predicates such as verbs or adjectives more plausibly co mbine with some classes of arguments than with others. It also has an interpretation as a disambiguation model: The different latent variable values correspond to different asp ects of meaning and a word X  X  distribution over those values can be modified by information coming from the context it appears in. We present a number of specific models wi thin this framework and demonstrate that they can give state-of-the-art performance o n tasks requiring models of preference and disambiguation. More generally, we illustrate that probabilistic modeling is an effective general-purpose framework for distributio nal semantics and a useful alternative to the popular vector-space framework.

Section 2 presents background on distributional semantics and an overview of prior work on selectional preference learning and on modeling contextual effects. Section 3
Section 4 presents our experimental results on four data sets. Section 5 concludes and sketches promising research directions for the future. 2. Background and Related Work 2.1 Distributional Semantics
The distributional approach to semantics is often traced back to the so-called  X  X istri-butional hypothesis X  put forward by mid -century linguists such as Zellig Harris and J.R. Frith: 588
In Natural Language Processing (NLP), the term distributional semantics encompasses potential for learning sema ntic knowledge from text was recognized very early in the development of NLP (Sp  X  arck Jones 1964; Cordier 1965; Harper 1965), but it is with the technological developments of the past t wenty years that this data-driven approach to semantics has become dominant. Distributional approaches may use a representation they all share the common property of estim ating their parameters from empirically observed co-occurrences.
 we may define the context of interest to be a document, a fixed-size window around data set of co-occurrence observations we can extract an indexed set of co-occurrence counts f w for each word of interest w ;eachentry f wc counts the number of times that w was observed in context c . Alternatively, we can extract an indexed set f context.
 semantics; under this conception f w is treated as a vector in vocabulary of contexts. As such, f w is amenable to computations known from lin-
Euclidean distance; we can cluster neighboring vectors; we can project a matrix of co-occurrence counts onto a low-dimensional subspace; and so on. This is per-haps the most popular approach to distributional semantics and there are many good general overviews covering the possib ilities and applications of the vector space model (Curran 2003; Weeds and Weir 2005; Pad  X  o and Lapata 2007; Turney and Pantel 2010).
 as constituting a vector, it is equally natural to view it as defining a probability distri-bution. When normalized to have unit sum, f w parameterizes a discrete distribution information theory such as the Kullback X  X e ibler or Jensen X  X hannon divergences (Lee 1999); the effects of clustering and dimensionality reduction can be achieved through the use of latent variable models (see Section 3.2.2). Additionally, Bayesian priors on parameter distributions provide a flexibl e toolbox for performing regularization and tical strength, while maintaining guarant ees of well-normalized behavior thanks to the laws of probability. In this article we focus on selectional preference learning and contextual disambiguation but we believe that the probabilistic approach exempli-modeling. 2.2 Selectional Preferences 2.2.1 Motivation. A fundamental concept in linguistic knowledge is the predicate ,by which we mean a word or other symbol that combines with one or more arguments to produce a composite representation with a composite meaning (by the principle of compositionality). The archetypal predicate is a verb; for example, transitive drink takes sentence. However, the concept is a general one, encompassing other word classes as well as more abstract items such as semantic relations (Yao et al. 2011), semantic frames distinction between predicate and argument is analogous to that between context and word in the more general distributional framework.
 animate entity (human or animal) and the object of drink is typically a beverage. The subject of eat is also typically an animate entity but its object is typically a foodstuff. formalized in terms of a predicate X  X  selectional preference : a function that assigns a numerical score to a combination of a predicate and one or more arguments according to the semantic plausibility of that combination. This score may be a probability, a rank, examples such as the following are semantica lly infelicitous despite being syntactically well-formed:
Psycholinguistic experiments have shown that the time course of human sentence processing is sensitive to predicate X  X rgu ment plausibility (Al tmann and Kamide 1999;
Rayner et al. 2004; Bicknell et al. 2010): Reading times are faster when participants are presented with plausible combinations than when they are presented with implausible combinations. It has also been proposed that selectional preference violations are cues that trigger metaphorical interpret ation. Wilks (1978) gives the example My car drinks gasoline , which must be understood non-literally since car strongly violates the subject preference of drink and gasoline is also an unlikely candidate for something to drink. investigate whether this aspect of human conceptual knowledge can be learned automatically from text corpora. If the predi ctions of a computational model correlate with judgments collected from human behavioral data, the assumption is that the model itself shares some properties with human linguistic knowledge and is in some sense a  X  X ood X  semantic model. More practically, NLP researchers have shown that selectional preference knowledge is usef ul for downstream applications, including metaphor detection (Shutova 2010), identification of non-compositional multiword 590 expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011). 2.2.2 The  X  X ounting X  Approach. The simplest way to estimate the plausibility of a predicate X  X rgument combination from a co rpus is to count the number of times that and that given enough data the resulting estimates will be relatively accurate. For exam-ple, Keller and Lapata (2003) es timate predicate X  X rgument plausibilities by submitting appropriate queries to a Web search engine a nd counting the number of  X  X its X  returned. To estimate the frequency with which the verb drink takes beer as a direct object,
Keller and Lapata X  X  method uses the query &lt; drink &lt; tasty pizza | pizzas &gt; . Where desired, these joint frequency counts can be normalized by unigram hit counts to estimate conditional probabilities such as P ( pizza massive corpora of raw text. On the other hand, it is hindered by the facts that only shallow processing is possible and that even in a Web-scale corpus the probability esti-mates for rare combinations will not be accurate. At the time of writing, Google returns zero hits for the query &lt; draughtsman | draughtsmen whistle and 1,570 hits for &lt; onion | onions whistle | whistles plausible conclusion that an onion is far more likely to whistle than a draughtsman. association by using pointwise mutual infor mation (PMI) rather than raw co-occurrence frequency to quantify selectional preference:
The role of the PMI transformation is to correct for the effect of unigram frequency: A common word may co-occur often with another word just because it is a common word rather than because there is a semantic association between them. However, it does not provide a way to overcome the problem of inaccurate counts for low-probability co-occurrences. Zhou et al. X  X  goal is to incorporate selectional preference features into a parsing model and they do not perform any evaluation of the semantic quality of the resulting predictions. 2.2.3 Similarity-Based Smoothing Methods. During the 1990s, research on language mod-eling led to the development of various  X  X moothing X  methods for overcoming the data sparsity problem that inevitably arises when estimating co-occurrence counts from finite corpora (Chen and Goodman 1999). The general goal of smoothing algorithms
Some also incorporate semantic information on the assumption that meaning guides the distribution of words in a text. one can extrapolate from observed co-occurrences by implementing the distributional hypothesis:  X  X imilar  X  words will have simila r distributional properties. A general form for similarity-based co-occurrence estimates is investigate a number of options. S ( w 1 , w 2 )isasetofcomparisonwordsthatmaydepend on w 1 or w 2 , or neither: Essen and Steinbiss (1992) use the entire vocabulary, whereas
Dagan, Lee, and Pereira use a fixed number of the most similar words to w their similarity value is above a threshold t .
 estimate co-occurrence probabilities for pr edicates and arguments, as was noted early based X  selectional preference model called EPP. In the EPP model, the set of comparison words is the set of words observed for the predicate p in the training corpus, denoted Seenargs ( p ): the corpus used to estimate similarity need not be the same as that used to estimate predicate X  X rgument co-occurrence, which is useful when the corpus labeled with these co-occurrences is small (e.g., a corpus labeled with FrameNet frames). 2.2.4 Discriminative Models. Bergsma, Lin, and Goebel (2008) cast selectional preference acquisition as a supervised learning problem t o which a discriminatively trained classi-fier such as a Support Vector Machine (SVM) can be applied. To produce training data for a predicate, they pair  X  X ositive X  arguments that were observed for that predicate fied threshold (measured by mutual information) with randomly selected  X  X egative X  standard way to predict a positive or negative score for unseen predicate X  X rgument pairs.
 represent the training and testing items. Bergsma, Lin, and Goebel include conditional lists of named entities, and p recompiled semantic classes. 592 2.2.5 WordNet-Based Models. An alternative approach to preference learning models the argument distribution for a predicate as a distribution over semantic classes provided by a predefined lexical resource. The most popular such resource is the WordNet lexical hierarchy (Fellbaum 1998), which provides semantic classes and hypernymic structures for nouns, verbs, adjectives, and adverbs. 3 Incorporating knowledge about the WordNet taxonomy structure in a preference model enables the use of graph-based regularization techniques to complement distributional information, while also expanding the cov-other hand, taxonomy-based methods build in an assumption that the lexical hierarchy chosen is the universally  X  X orrect X  one an d they will not perform as well when faced with data that violates the hierarchy or contains unknown words. A further issue faced by these models is that the resources they rely on require significant effort to create and will not always be available to model data in a new language or a new domain.
WordNet classes based on the empirical distribution of words of each class (and their hyponyms) in a corpus. Abney and Light (1999) conceptualize the process of generating an argument for a predicate in terms of a Markovian random walk from the hierarchy X  X  root to a leaf node and choosing the word associated with that leaf node. Ciaramita and Johnson (2000) likewise treat WordNet as defining the structure of a probabilistic graphical model, in this case a Bayesian network. Li and Abe (1998) and Clark and Weir (2002) both describe models in which a predicate  X  X uts X  the hierarchy at an appropriate level of generalization, such that all classes below the cut are considered appropriate arguments (whether observed in data or not) a nd all classes above the cut are considered inappropriate.
  X 
OS  X  eaghdha and Korhonen (2012) do investigate a number of Bayesian preference models that incorporate WordNet classes and structure, finding that such models outperform previously proposed WordNet-based models and perform comparably to the distributional Bayesian models presented here. 2.3 Measuring Similarity in Context disambiguated and modulated by the context in which it appears. The word body clearly has a different sense in each of the following text fragments: 1. Depending on the present position of the planetary body in its orbital path, . . . 2. The executive body decided. . . 3. The human body is intriguing in all its forms.

In a standard word sense disambiguation experiment, the task is to map instances of ambiguous words onto senses from a manually compiled inventory such as WordNet.
An alternative experimental method is to hav e a system rate the suitability of replacing an ambiguous word with an alternative wo rd that is synonymous or semantically similar in some contexts but not others. For example, committee is a reasonable substitute for body in fragment 2 but less reasonable in fragment 1. An evaluation of semantic models based on this principle wa s run as the English Lexical Substitution Task in SemEval 2007 (McCarthy and Navigli 2009). The annotated data from the
Lexical Substitution Task have been used by numerous researchers to evaluate models of lexical choice; see Section 4.5 for further details.
 ity of a pair of words w o , w s in a given context C = { the task is substitution, w o is the original word and w s general approach is to compute a representation Rep ( w compare it with Rep ( w s ), our representation for w n : where sim is a suitable similarity function for comparing the representations. This general framework leaves open the questio n of what kind of representation we use for space semantics and in Section 3.5 we describe representations based on latent-variable models.
 provided by research on semantic composition, namely, how the syntactic effect of goal is to represent the combination of a co ntext and an in-context word, not just to represent the word given the context. The co-o ccurrence models described in this article are not designed to scale up and provide a representation for complex syntactic struc-tures, 4 but they are applicable to evaluation scenarios that involve representing binary co-occurrences. distributional semantics casts word meanings as vectors of real numbers and uses linear algebra operations to compare and combine these vectors. A word w is represented by avector v w that models aspects of its distribution in the training corpus; the elements of this vector may be co-occurrence counts (in which case it is the same as the frequency vector f w ) or, more typically, some transformation of the raw counts.
 occurring words. Given pre-computed word vectors v w , v w vided by a function g that may also depend on syntax R and background knowledge K : finding that elementwise multiplication is a simple and consistently effective choice: 594
The motivation for this  X  X isambiguation by m ultiplication X  is that lexical vectors are sparse and the multiplication operation has the effect of sending entries not supported in both v w and v w towards zero while boosting entries that have high weights in both vectors.
 contexts for a verb and a noun may have no dependency labels in common and hence multiplying their vectors will not give useful results. Erk and Pad  X  o (2008) propose a structured vector space approach in which each word w is associated with a set of  X  X xpectation X  vectors R w , indexed by dependency label, in addition to its standard co-occurrence vector v w . The expectation vector R w ( r )forword w and dependency label r is an average over co-occurrence vectors for seen arguments of w and r in the training corpus:
Whereas a standard selectional preference model addresses the question  X  X hich words the question  X  X hat does a typical co-occurrence vector for an argument of the pred-predicate ( w , r ), Erk and Pad  X  o combine the expectation vector R vector v w : vector spaces for disambiguation. The model of Thater, F  X  urstenau, and Pinkal (2011), which is simpler and better-performing, sets the representation of w in the context of ( r , w )tobe where  X  quantifies the compatibility of the observed predicate ( w , r ) with the smooth-follows:
Pad  X  o, and Pad  X  o (2010); each entry in the vector v of the preference of ( w , r )for w . EPP uses seen arguments of ( w , r ) for smoothing, whereas Thater, F  X  urstenau, and Pinkal (2011) take a complementary approach and smooth with seen predicates for w . In order to combine the disambiguatory effects of multiple predicates, a sum over contextualized vectors is taken: co-occurrence vector to a vector representation of the word X  X  meaning in context. This allows us to calculate the similarity between two in-context words or between a word and an in-context word using standard vector similarity measures such as the cosine. In applications where the task is to judge the appropriateness of substituting a word w an observed word w o in context C = { ( r 1 , w 1 ), ( r 2 is to compute the similarity between the contextualized vector v and the uncontextualized word vector v w this approach yields better performance than contextualizing both vectors before the similarity computation. 3. Probabilistic Latent Variable Models for Lexical Semantics 3.1 Notation and Terminology cabulary of contexts C and w is a word belonging to the word vocabulary otherwise stated, the contexts considered in t his article are head-lexicalized dependency edges c = ( r , w h )where r  X  R is the grammatical relation and w
We notate grammatical relations as p h : label : p d ,where p speech, p d is the dependent word X  X  part of speech, and label is the dependency label. We use a coarse set of part-of-speech tags: n (noun), v (verb), j (adjective), r (adverb).
The dependency labels are the grammatical relations used by the RASP system (Briscoe 2006; Briscoe, Carroll, and Watson 2006), though in principle any dependency formalism could be used. The assumption that predicates correspond to head-lexicalized depen-dency edges means that they have arity one.

C comprising all the dependency edges incident to w . In the sentence fragment The executive body decided. . . ,theword body has two incident edges: 596 ( v:ncsubj:n , decide ) indicates that body is the subject of decide and ( j:ncmod denotes that it stands in an inverse non-clausal modifier relation to executive (we assume that nouns are the heads of th eir adjectival modifiers).
 from a corpus of observations O . Each observation is a co-occurrence of a predicate and an argument. The set of observations for context c is denoted O ( c ). The co-occurrence frequency of context c and word w is denoted by f cw frequency of c by f c = w  X  W f cw . 3.2 Modeling Assumptions 3.2.1 Bayesian Modeling. The Bayesian approach to probabilistic modeling (Gelman et al. 2003) is characterized by (1) the use of prior distributions over model parameters to encode the modeler X  X  expectations about the values they will take; and (2) the explicit rather than point estimates. 7 discrete sample space (e.g., the vocabulary of words or a set of semantic classes). This leads to the use of a categorical or multinomial distribution for the data likelihood. This distribution is parameterized by a unit-sum vector  X   X   X  sample space. The probability that an observation o takes value k is then:
The value of  X   X   X  must typically be learned from data. The maximum likelihood estimate (MLE) sets  X  k proportional to the number of times k was observed in a set of observa-tions O ,whereeachobservation o i  X  K : unlikely to give accurate estimates for low-probability types (Evert 2004). Items that do not appear in the training data will be assigned zero probability of appearing in unseen data, which is rarely if ever a valid assumption. Sparsity increases further when the sample space contains composite i tems (e.g., context-words pairs).
 language modeling is to  X  X mooth X  the distribution by taking probability mass from frequent types and giving it to infrequent types. The Bayesian approach to smoothing is to place an appropriate prior on  X   X   X  and apply Bayes X  Theorem:
A standard choice for the prior distribution over the parameters of a discrete distribu-tion is the Dirichlet distribution:
Here,  X   X   X  is a | K | -length vector where each  X  k &gt; setting the sum k  X  k to a small value will encode the expectation that the parameter conjugate prior for multinomial and categorical likelihoods, in the sense that the poste-multinomial or categorical and P (  X   X   X  ) is Dirichlet: where  X  indicates elementwise addition of the observed count vector f parameter vector  X   X   X  . Furthermore, the conjugacy pro perty allows us to do a number of important computations in an efficient way. In many applications we are interested in predicting the distribution over values K for a  X  X ew X  observation given a set of prior observations O while retaining our uncertainty about the model parameters. We can average over possible values of  X   X   X  , weighted according to their probability P (  X  X ntegrating out X  the parameter and still retain a simple closed-form expression for the posterior predictive distribution: Expression (21) is central to the implementation of collapsed Gibbs samplers for details of these derivations, see Heinrich (2009).
 process and the Pitman X  X or process (Goldwater, Griffiths, and Johnson 2011). The  X  X on-parametric X  in the sense of varying the size of its support according to the data; mixture components to be learned rather than fixed in advance. The Pitman X  X or process law distributions. This makes it particularly suitable for language modeling where the
Dirichlet distribution or Dirichlet proces s would not produce a long enough tail due to their preference for sparsity (Teh 2006). On t he other hand, Dirichlet-like behavior may be preferable in semantic modeling, where we expect, for example, predicate X  X lass and class X  X rgument distributions to be sparse. 3.2.2 The Latent Variable Assumption. In probabilistic modeling, latent variables are random variables whose values are not provided by the input data. As a result, their 598 values must be inferred at the same time as the model parameters on the basis of the training data and model structure. The latent variable concept is a very general one that is used across a wide range of probabilistic frameworks, from hidden Markov models to neural networks. One important applicat ion is in mixture models, where the data likelihood is assumed to have the following form: with a distribution over observations x , and the resulting likelihood is an average of the component distributions weighted by the mixing weights P ( z ). The set of possible for P ( x | z ) is informed by all datapoints assigned to component z .
 variables c and w , drawn from vocabularies C and W , respectively.
 latent variables shares the same basic motivat ions as other, not necessarily probabilistic, dimensionality reduction techniques such as Latent Semantic Analysis or Non-negative
Matrix Factorization. An advantage of proba bilistic models is their flexibility, both in terms of learning methods and model structures. For example, the models considered in this article can potentially be extended to multi-way co-occurrences and to hierarchi-cally defined contexts that cannot easily be expressed in frameworks that require the input to be a | C | X | W | co-occurrence matrix.
 occurrence data in the context of noun clustering by Pereira, Tishby, and Lee (1993). They suggest a factorization of a noun n  X  X  distribution over verbs v as argument, in effect defining an inverse selectional preference model. Pereira, Tishby, &amp; Lee also observe that given certain assumptions Equation (24) can be written more symmetrically as based on Maximum Entropy. Rooth et al. (1999) propose a much simpler Expectation
Maximization (EM) procedure for estimating the parameters of Equation (25). 3.3 Bayesian Models for Binary Co-occurrences
Combining the latent variable co-occurrence model (23) with the use of Dirichlet priors naturally leads to Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003). Often described as a  X  X opic model, X  LDA is a mod el of document content that assumes each document is generated from a mixture of multi nomial distributions or  X  X opics. X  Topics are shared across documents and correspond to thematically coherent patterns of word usage. For example, one topic may assign high probability to the words finance , fund , bank ,and invest , whereas another topic may assign high probability to the words football , applications and extensions, and the topic modeling framework remains an area of active research in machine learning.
 LDA is essentially a latent-variable model o f document X  X ord co-occurrence. Adapting
LDA for selectional preference mode ling was suggested independently by (2010) and Ritter, Mausam, and Etzioni (2010). Conceptually the shift is straightforward and intuitive: Documents become contex ts and words become argument words. The selectional preference probability P ( w | c ) is modeled as arguments for predicates and also presents a plate diagram indicating the dependencies between variables in the model. Table 1 illustrates the semantic representation induced by a 600-topic LDA model trained on predicate X  X oun co-occurrences extracted from the British National Corpus (for more details of this training data, see Section 4.1). The than a hard partitioning; therefore we present the eight most probable words for each. for topic z  X  X  1 ... | Z |} do (Draw a distribution over words)
 X   X   X  end for for context c  X  X  1 ... | C |} do (Draw a distribution over classes)  X   X   X  for observation o i  X  O ( c ) do end for end for 600 topic model trained on document X  X ord co-occurrences will find topics that reflect broad thematic commonalities, the model tr ained on syntactic co-occurrences finds semantic classes that capture a much tighter sense of similarity: Words assigned high probability in the same topic tend to refer to en tities that have similar properties, that perform similar actions, and have similar a ctions performed on them. Thus Class 1 is represented by attack , raid , assault , campaign, and so on, forming a coherent semantic revenue , eyebrow, and awareness hardly belong together as a coherent conceptual class. result the modifier nature is often associated with it.

Lee (1993) and Rooth et al. (1999) that is formalized in Equation (25). This model can (Draw a distribution over topics)  X   X   X   X  Dirichlet (  X   X   X  ) for topic z  X  X  1 ... | Z |} do (Draw a distribution over words)
 X   X   X  (Draw a distribution over contexts)
 X   X   X  end for for observation o i  X  O do (Draw a topic) z i  X  Multinomial (  X   X   X  ) (Draw a word) w i  X  Multinomial (  X   X   X  z (Draw a context) c i  X  Multinomial (  X   X   X  z end for be  X  X ayesianized X  by placing Dirichlet priors on the component distributions; adapting Equation (25) to our notation, the resulting joint distribution over contexts and words is The generative story and plate diagram for this model, which was called R in  X  OS  X  eaghdha (2010), are giv en in Figure 2. Whereas LDA induces classes of arguments, R
OOTH -LDA induces classes of predicate X  X rgume nt interactions. Table 2 illustrates some classes learned by R OOTH -LDA from BNC verb X  X bject co-occurrences. One class used . As with LDA, there are some over-generalizations; the fact that an eye or mouth can be opened , closed, or shut does not necessarily entail that it can be locked or unlocked . accounts for general semantic regularities an d idiosyncratic lexical patterns. This sug-gests the idea of combining a distribution ov er semantic classes and a predicate-specific 602 distribution over arguments. One way of doing this is through the model depicted probability P ( w | c )as where  X  c isavaluebetween0and1thatcanbeinterpretedasameasureofargument lexicalization or as the probability that an observation for context c is drawn from the lexical distribution P lex or the class-based distribution P the LDA preference model. The value of  X  c will vary across predicates according to how well their argument preference can be fit by the class-based models; a predicate with high  X  c will have idiosyncratic argument patterns that are best learned by observing that predicate X  X  co-occurrences in isolation. In many cases this may reflect idiomatic or non-compositional usages, though it is also to be expected that frequency; given sufficient data for a context, smoothing becomes less important. As an example we trained the L EX -LDA model on BNC verb-object co-occurrences and estimated posterior mean values for  X  c for all verbs occurring more than 100 times and taking at least 10 different object argument types. The verbs with highest and lowest values are listed in Table 3. Although almost anything can be discussed or highlighted , for topic z  X  X  1 ... | Z |} do end for for context c  X  X  1 ... | C |} do end for verbs such as pose and wreak have very lexicalized argument preferences. The semantic classes learned by L EX -LDA are broadly comparable to those learned by LDA, though it is less likely to mix classes on the basis of a single argument lexicalization; whereas nature reserve and raise eyebrow ,L EX -LDA models trained on the same data can explain these through lexicalization effects and separate out body parts, conservation areas, and investments in different classes. 3.4 Parameter and Hyperparameter Learning
Bayesian models. The two standard approaches are variational inference, in which sampling, in which convergence to the true pos terior is guaranteed in theory but rarely verifiable in practice. In some cases the ch oice of approach is guided by the model, but often it is a matter of personal preference; for LDA, there is evidence that equivalent levels of performance can be achieved through variational learning and sampling given methods based on Gibbs sampling, following Griffiths and Steyvers (2004). The basic probability distribution determined by the cu rrent observed and latent variable values priors, we can integrate out their parameters using Equation (21).
 is assigned value z is computed as is the number of observations in that set assigned latent variable z , f of observations with context c i assigned latent variable z ,and f observations with word w i assigned latent variable z . 604
We  X  X lock X  the sampling for z i and s i to improve convergence. The Gibbs sampling distribution is where  X  indicates that no topic is assigned. The fact that topics are not assigned for all tokens means that L EX -LDA is less useful in situations that require representational power they afford X  X or example, the contex tual similarity paradigm described in Section 3.5.
 and the number of observations to complete one iteration. Yao, Mimno, and McCallum (2009) present a new sampling algorithm for LDA that yields a considerable speedup by reformulating Equation (30) to allow caching of intermediate values and an intelligent sorting of topics so that in many cases only a small number of topics need be iterated though before assigning a topic to an obser vation. In this article we use Yao, Mimno, &amp; McCallum X  X  algorithm for LDA, as well as a transformation of the R L
EX -LDA samplers that can be derived in an analogous fashion. converge to the true posterior after a finite n umber of iterations; however, this number is unknown and it is difficult to detect convergence. In practice, we run the sampler final sampling state (assignments of all z and s variables) and/or a set of intermediate sampling states.
 estimated using posterior mean estimates of  X  c and  X  z : For R OOTH -LDA, the joint probability P ( c , w ) is given by For L EX -LDA, P ( w | c ) is given by single state S i . As the sampler is initialized randomly and will take time to find a good area of the search space, it is standard to wait until a number of iterations have passed before using any samples for prediction. States S 1 , ... discarded.
 from more than one sampling state; for example, we can produce an averaged estimate of P ( w | c ) from a set of states S :
It is also possible to average over states drawn from multiple chains. However, averaging of any kind can only be performed o n quantities whose interpretation does not depend on the sampling state itself. For example, we cannot average over estimates of P ( z 1 | c ) drawn from different samples as the topic called z identical to the topic called z 1 in another; even within the same chain, the meaning of a topic will often change gradually from state to state. 606 efficiency advantages but also the problem of choosing an appropriate value for more classes a model has, the greater its ca pacity to capture fine distinctions between entities. However, this finer granularity inevitably comes at a cost of reduced general-ization. One approach is to choose a value that works well on training or development over the entirety of a data set, meaning that if we wish to compare those results we cannot hold out any portion. If the method is re latively insensitive to the parameter it may be sufficient to choose a default value. Rooth et al. (1999) suggest cross-validating on the training data likelihood (and not on the ultimate evaluation measure). An alter-native solution is to average the predictions of models trained with different choices of | Z | ; this avoids the need to pick a default and can give better results than any one value as it integrates contributions at diff erent levels of granularity. As mentioned in
Section 3.4.2 we must take care when averaging predictions to compute with quan-tities that do not rely on topic iden tity X  X or example, estimates of P ( a combined whereas estimates of P ( z 1 | p ) cannot. 3.4.4 Hyperparameter Estimation. Although the likelihood parameters can be integrated out, the parameters for the Dirichlet and Beta priors (often referred to as  X  X yperparame-ters X ) cannot and must be specified either manually or automatically. The value of these parameters affects the sparsity of the learned posterior distributions. Furthermore, the use of an asymmetric prior (where not all its parameters have equal value) implements observations have been made. Wallach, Mimno, and McCallum (2009) demonstrate that on performance, recommending in conclusion a symmetric prior on the  X  X mission X  likelihood P ( w | z ) and an asymmetric prior on t he document topic likelihoods P ( z this article we follow these recommendations and, like Wallach, Mimno, and McCallum, we optimize the relevant hyperparameters using a fixed point iteration to maximize the log evidence (Minka 2003; Wallach 2008). 3.5 Measuring Similarity in Context with Latent-Variable Models
The representation induced by latent variab le selectional preference models also allows us to capture the disambiguatory effect of context. Given an observation of a word in can also estimate the probability that the semantic classes suggested by the observation would have licensed an alternative word. Tak en together, these can be used to estimate in-context semantic similarity. The fundamental intuitions are similar to those behind the vector-space models in Section 2.3.2, but once again we are viewing them from the perspective of probabilistic modeling.
 alternative term w s in context C with the similarity between the probability distribution latent variables associated with w s :
This assumes that we can associate a distribution over the same set of latent variables with each context item c  X  C . As noted in Section 2.3.2, previous research has found that conditioning the representation of both the observed term and the candidate substitute on the context gives worse performance than conditioning the observed term alone; we also found a similar effect. Dinu and Lapata (2010) present a specific version of this framework, using a window-based definition of context and the assumption that the similarity given a set of contexts is the produ ct of the similarity value for each context: derive a well-motivated approach to incorpor ating multiple contexts inside the prob-performance on a lexical substitution data set.
 preference model that generates words given a context, it is straightforward to calculate the distribution over latent variables cond itioned on an observed context X  X ord pair:
Given a set of multiple contexts C , each of which has an opinion about the distribution over latent variables, this becomes
The uncontextualized distribution P ( z | w s ) is not given directly by the LDA model. It unsmoothed estimate. 8 We denote this model C  X  T to note that the target word is generated given the context.
 dependency contexts), we can alternatively learn a model that generates contexts given a target word; we denote this model T  X  C : Again, we can generalize to non-singleton context sets: 608 where unlike many applications of such models we tr ain the experts independently and thus avoid additional complexity in the learning p rocess. The uncontextualized distribution P ( z | w s ) is an explicit component of the T  X  C model.

Here there is no asymmetry as the context and target are generated jointly. The distri-bution over topics for a context c and target word w o is given by while calculating the uncontextualized distribution P ( z set of possible contexts C X :
Because the interaction classes learned by R OOTH -LDA are specific to a relation type, this model is less applicable than LDA to problems that involve a rich context set C .
The information theory literature has provided many such measures; in this article we use the Bhattacharyya coefficient (Bhattacharyya 1943): as the Jensen X  X hannon Divergence or the L 1 distance (Lee 1999;
Copestake 2008). 3.6 Related Work
As related earlier, non-Bayesian mixture or l atent-variable approaches to co-occurrence
Blitzer, Globerson, and Pereira (2005) describe a co-occurrence model based on a literature on neural language models. Brody and Lapata (2009) use the clustering effects of LDA to perform word sense induction. Vlachos, Korhonen, and Ghahramani (2009) use non-parametric Bayesian methods to clus ter verbs according to their co-occurrences with subcategorization frames. Reisinger and Mooney (2010, 2011) have also investigated Bayesian methods for lexical se mantics in a spirit similar to that adopted here. Reisinger and Mooney (2010) describe a  X  X iered clustering X  model that, like L
LDA, mixes a cluster-based preference model with a predicate-specific distribution over words; however, their model does not encourage sharing of classes between different predicates. Reisinger and Mooney (2011) propose a very interesting variant of the latent-by different  X  X iews, X  each of which has its own distribution over latent variables; this model can give more interpretable classes than LDA for higher settings of
Markov model (HMM) in a single model structure, allowing each word to be drawn from either the document X  X  topic distribution or a latent HMM state conditioned on the preceding word X  X  state; Moon, Erk, and Baldridge (2010) show that combining HMM and LDA components can improve unsupervised part-of-speech induction. Wallach (2006) also seeks to capture the influence of the preceding word, while at the same time generating every word from inside the LDA model; this is achieved by conditioning topic. Boyd-Graber and Blei (2008) propose a  X  X yntactic topic model X  that makes topic selection conditional on both the document X  X  topic distribution and on the topic of the word X  X  parent in a dependency tree. Although these models do represent a form of local context, they either use a very restrictive one-word window or a notion of syntax that ignores lexical or dependency-label eff ects; for example, knowing that the head of a noun is a verb is far less informative than knowing that the noun is the direct object of eat .
 duce a finer-grained grammar and tease out in tricacies of word X  X ule  X  X o-occurrence. X  Finkel, Grenager, and Manning (2007) and Liang et al. (2007) propose a non-parametric Bayesian treatment of state splitting. This is very similar to the motivation behind an
LDA-style selectional preference model. One difference is that the parsing model must selectional preference models described he re each head X  X ependent relation is treated as an independent observation (though this c ould be changed). These differences allow our selectional preference models to be trained efficiently on large corpora and, by fo-cusing on lexical choice rather than syntax, to home in on purely semantic information.
Titov and Klementiev (2011) extend the idea of latent-variable distributional modeling to do  X  X nsupervised semantic parsing X  and reason about classes of semantically similar lexicalized syntactic fragments. 4. Experiments 4.1 Training Corpora In our experiments we use two training corpora:
BNC the written component of the British National Corpus, 610
WIKI a Wikipedia dump of over 45 million sentences (almost 1 billion words) tagged,
Although two different parsers were used, they both have the ability to output gram-matical relations in the RASP format and hence they are interoperable for our purposes as downstream users. This allows us to cons truct a combined corpus by simply concate-nating the BNC and WIKI corpora.
 observations from the parsed corpora. Prior to extraction, the dependency graph for each sentence was transformed using the preprocessing steps illustrated in Figure 4.
We then filtered for semantically discrimina tive information by ignoring all words with part of speech other than common noun, verb, adjective, and adverb. We also ignored instances of the verbs be and have and discarded all words containing non-alphabetic characters and all words with fewer than three characters. possibilities in this article:
Syn The context of w is determined by the syntactic relations r and words w incident
Win5 The context of w is determined by the words appearing within a window of five issues. The window-based approach is particularly susceptible to an explosion in the number of extracted contexts, as each token in the data can contribute 2 context observations, where W is the window size. We reduced the data by applying a simple downsampling technique to the training corpora. For the WIKI/Syn corpus, the WIKI/Win5 corpus we divided all counts by 70; this number was suggested by Dinu and Lapata (2010), who used the same ratio for downsampling the similarly sized
English Gigaword Corpus. Being an order of magnitude smaller, the BNC required less pruning; we divided all counts in the BNC/Win5 by 5 and left the BNC/Syn corpus unaltered. Type/token statistics fo r the resulting sets of observations are given in Table 4. 4.2 Evaluating Selectional Preference Models preference models. One popular method is  X  pseudo-disambiguation, X  in which a sys-tem must distinguish between actually occurring and randomly generated predicate X  argument combinations (Pereira, Tishby, and Lee 1993; Chambers and Jurafsky 2010).
These two approaches take a  X  X anguage modeling X  approach in which model quality
Although this metric should certainly corre late with the semantic quality of the model, it may also be affected by frequency and other idiosyncratic aspects of language use unless tightly controlled. In the context of document topic modeling, Chang et al. (2009) find that a model can have better predictive performance on held-out data while inducing topics that human subjects judge to be less semantically coherent.
 with semantic judgments elicited from huma n subjects. These judgments take various 612 between pairs of predicate X  X rgument combin ations. In Section 4.5 we use judgments of substitutability for a target word as disambiguated by its sentential context. Taken together, these different experimental designs provide a multifaceted analysis of model quality. 4.3 Predicate X  X rgument Plausibility 4.3.1 Data. For the plausibility-based evaluation we use a data set of human judgments collected by Keller and Lapata (2003). This comprises data for three grammatical re-lations: verb X  X bject, adjective X  X oun, and noun X  X oun modification. For each relation, 30 predicates were selected; each predicate was paired with three noun arguments from different predicate X  X rgument frequency bands in the BNC as well as three noun subsets ( Seen and Unseen ) of 90 items each were assembled for each predicate. Human cal judgments were then normalized, log-t ransformed, and averaged in a Magnitude Estimation procedure.
 the correlation between system predicti ons and the human judgments. Keller and correlation coefficient  X  for a non-parametric evaluation. Each system prediction is log-transformed before calculating the cor relation to improve the linear fit to the gold standard. 4.3.2 Methods. We evaluate the LDA, R OOTH -LDA, and L EX erence models, trained on predicate X  X rgument pairs ( c , w )extractedfromtheBNC. have observed that our Bayesian models are relatively robust to the choice of of which is obtained by sampling P ( c , w ) every 50 iterations after a burn-in period of 200 iterations. R OOTH -LDA gives joint probabilities by definition (25), but LDA and L
EX -LDA are defined in terms of conditional p robabilities (24). There are two options for training these models:
P  X  A : Model the distribution P ( w | c )overargumentsforeachpredicate.

A  X  P : Model the distribution P ( c | w ) over predicates for each argument.
As the descriptions suggest, the definition of  X  X redicate X  and  X  X rgument X  is arbitrary; it is equally valid to talk of the selectional preference of a noun for verbs taking it as a direct object as it is to talk of the preference of a verb for nouns taking it as a direct object. We expect both configurations to perform comparably on average, though there may be linguistic or conceptual reasons why one configuration is better than the other for specific classes of co-occurrence.
 frequency (MLE) estimate of the probability of the conditioning term: As well as evaluating P  X  A and A  X  P implementations of LDA and L we can evaluate a combined model P  X  A that simply averages the two sets of other.
 for their search-engine method using AltaVista and Google alternative methods that we have reimplemented and trained on identical data:
BNC (MLE) A maximum-likelihood estimate propor tional to the co-occurrence fre-BNC (KN) BNC relative frequencies smoothed with modified Kneser-Ney (Chen and
Resnik The WordNet-based association strength of Resnik (1993). We used WordNet Clark/Weir The WordNet-based method of Clark and Weir (2002), using WordNet 3.0. Rooth-EM Rooth et al. (1999) X  X  latent-variable model without priors, trained with EM.
Disc A discriminative model inspired by Bergsma, Lin, and Goebel (2008) (see Sec-614 correlated correlation coefficients proposed by Meng, Rosenthal, and Rubin (1992). This is more appropriate than a standard test for independent correlation coefficients as it takes into account the strength of correlat ion between two sets of system outputs as well as each output X  X  correlation with the gold standard. Essentially, if the two sets of system outputs are correlated there is less chance that their difference will be deemed significant. As we have no a priori reason to believe that one model will perform better than another, all tests are two-tailed. 4.3.3 Results. Results on the Keller and Lapata (2003) plausibility data set are presented in Table 5. 15 For common combinations (the Seen da ta) it is clear that relative corpus frequency is a reliable indicator of plausibility, especially when Web-scale resources are available. The BNC MLE estimate outperforms the best selectional preference model
Keller and Lapata (2003) outperforms the best selectional preference model on every applicable Seen evaluation. For the rarer Unseen combinations, however, MLE esti-mates are not sufficient and the latent-variable selectional preference models frequently outperform even the Web-based predictions. The results for BNC(KN) improve on the
MLE estimates for the Unseen data but do not match the models that have a semantic component.
 the previously proposed selectional preference models under almost every evaluation.
Among the latent-variable models there is no one clear winner, and small differences in performance are as likely to arise through random sampling variation as through qualitative differences between models. That said, R OOTH higher than LDA in a majority of cases. As expected, the bidirectional P tend to perform at around the midpoint of the P  X  A and A can also exceed both; this suggests that they are a good choice when there is no intuitive reason to choose one direction over the other.
 evaluation measures. As before, all the Bayesian latent-variable models achieve a roughly similar level of performance, consis tently outperforming the models selected from the literature and frequently reaching statistical significance (p for selectional preference modeling. 616 contribution for the i th item is
Spearman X  X   X  is equivalent to the r correlation between ranks and so a similar quantity can be computed. Table 7 illustrates the items with highest and lowest contributions for one evaluation (Spearman X  X   X  on the Keller and Lapata Unseen data set). We have attempted to identify general factors that p redict the difficulty of an item by measuring rank correlation between the per-item pseudo-coefficients and various corpus statis-arguments with high corpus frequency tend to incur larger errors for the P variable models and R OOTH -LDA, whereas predicates with high corpus frequency tend to incur smaller errors; with the A  X  P the effect is lessened but not reversed, suggesting 4.4 Predicate X  X rgument Similarity 4.4.1 Data. Mitchell and Lapata (2008, 2010) collected human judgments of similarity between pairs of predicates and arguments corresponding to minimal sentences. compositionality models but their data sets are also suitable for evaluating predicate X  argument representations.
 of 15 verbs, yielding 60 reference combinat ions. Each verb X  X oun tuple was matched with two verbs that are synonyms of the reference verb in some contexts but not in others. In this way, Mitchell and Lapata created a data set of 120 pairs of predicate X  argument combinations. Similarity judgme nts were obtained from human subjects for each pair on a Likert scale of 1 X 7. Examples of the resulting data items are given in article we use the same split.
 ference that instead of keeping arguments constant across combinations in a pair, both predicates and arguments vary across comparand combinations. They also consider a range of grammatical relations: verb X  X bject, adjective X  X oun, and noun X  X oun modifica-tion. Human subjects rated similarity betw een predicate X  X rgument combinations on a 1 X 7 scale as before; examples are given in Table 9. Inspection of the data suggests that the subjects X  annotation may conflate semant ic similarity and relatedness; for example, football club and league match are often given a high similarity score. Mitchell and Lapata again split the data into development and testing sections, the former comprising 54 subjects X  ratings and the latter com prising 108 subjects X  ratings.
 original paper; for each grammatical relatio n, the annotators are partitioned in three groups and the Spearman X  X   X  correlation computed for each group is combined by av-eraging. 16 The analogous approach for the Mitchell and Lapata (2008) data set calculates a single  X  value by pairing of each annotator-item score with the system prediction for the appropriate item. Let s bethesequenceofsystempredictionsfor 618 be the scores assigned by annotator a  X  A to those | I | items. Then the  X  X oncatenated X  correlation  X  cat is calculated as follows: 17
The length of the y cat and s cat sequences is equal to the total number of annotator-item scores. For the Mitchell and Lapata (2010) data set, a  X  cat of the three annotator groups and these are then averaged. As Turney observes, this approach seems to have the effect of underestimating model quality relative to the inter-
Therefore, in addition to Mitchell and Lapata X  X   X  cat evaluation that computes the average correlation  X  ave between the system output and each individual annotator: 4.4.2 Models. For the Mitchell and Lapata (2008) data set we train the following models on the BNC corpus:
LDA An LDA selectional preference model of verb X  X ubject co-occurrence with simi-Mult Pointwise multiplication (6) using Win5 co-occurrences.

We also compare against the best figures reported in previous studies; these also used the BNC for training and so should be directly comparable:
M+L08 The best-performing system of Mitchell and Lapata (2008), combining an addi-
SVS The best-performing system of Erk and Pad  X  o (2008); the Structured Vector Space | | = 100 the BNC corpus: R
OOTH -LDA/Syn AR OOTH -LDA model trained on the appropriate set of syntactic
LDA/Win5 An LDA model trained on the Win5 window-based co-occurrences. Be-Combined This model averages the similarity prediction of the R Mult Pointwise multiplication (6) using Win5 co-occurrences.

We report results for an average over all predictors as well as for the subset that per-forms best on the development data. We also list results that were reported by Mitchell and Lapata:
M+L10/Mult A multiplicative model (6) using a vector space based on window co-
M+L10/Best The best result for each grammatical relation from any of the semantic
Tables 10 and 11. 19 The LDA preference models clearly outperform the previous state scoring  X  cat = 0 . 38,  X  ave = 0 . 41, and the best optimized combination scoring  X  estimated by Mitchell and Lapata X  X  to be  X  ave = 0 . 40. Optimizing on the development data consistently gave better performance th an averaging over all predictors, though in most cases the differences are small. 620 | | = 100
Table 13. 20 Again the latent-variable models per form well, comfortably outperforming the Mult baseline, and with just one exception the Combined models surpass Mitchell and Lapata X  X  reported results. Combini ng the syntactic co-occurrence model R
LDA/Syn and the window-based model LDA/Win5 consistently gives the best perfor-mance, suggesting that the human ratings in this data set are sensitive to both strict similarity and a looser sense of relatedness. As Turney (2012) observes, the average-per-group approach of Mitchell and Lapata leads to lower performance figures than averaging across annotators; with the latter approach (Table 12) the values approach the level of human interannotator agreement for two of the three relations: noun X  X oun and adjective X  X oun modification. 4.5 Lexical Substitution 4.5.1 Data. The data set for the English Lexical Substitution Task (McCarthy and Navigli 2009) consists of 2,010 sentences sourced from Web pages. Each sentence features one of 205 distinct target words that may be nouns, verbs, adjectives, or adverbs. The sentences have been annotated by human judges to sugge st semantically acceptable substitutes for their target words. Table 14 gives example sentences and annotations for the target verb charge . For the original shared task the data was divided into development and test sections; in this article we follow subsequ ent work using parameter-free models and usethewholedatasetfortesting.
 butional model to reason about such terms, we remove these substitutes from the gold standard. 21 We remove entirely the 17 sentences that have only multiword substitutes in the gold standard, as well as 7 sentences for which no gold annotations are provided. This leaves 1,986 sentences.
 resources to constrain the set of substitutes considered. Most subsequent researchers using the Lexical Substitution data to evaluate models of contextual meaning have adopted a slightly different experimental design, in which systems are asked to rank only the list of attested substitutes for the t arget word in each sentence. For example, 622 the list of substitute candidates for an instance of charge is the union of the substitute lists in the gold standard for every sentence containing charge as a target word: levy, predictions for a given sentence then invo lves comparing the ranking produced by the system with the implicit ranking produced b y annotators, assuming that any candidates
Dinu and Lapata (2010) use Kendall X  X   X  b , a standard rank correlation measure that is appropriate for data containing tied ranks. Thater, F  X  urstenau, and Pinkal (2010, 2011) use Generalized Average Precision (GAP), a precision-like measure originally proposed by Kishida (2005) for information retrieval: where x 1 , ... , x n are the ranked candidate scores provided by the system, y the ranked scores in the gold standard and I ( x ) is an indicator function with value 1 if x &gt; 0and0otherwise.
 sentence and averaged. The open-vocabulary design of the original Lexical Substitution
Task facilitated the use of other evaluation measures such as  X  X recision out of ten X : the proportion of the first 10 words in a system X  X  r anked substitute list that are contained in the constrained-vocabulary scenario considered here; when there are fewer than 10 candidate substitutes for a target word, the precision will always be 1. 4.5.2 Models. We apply both window-based and syntactic models of similarity in context to the lexical substitution data set; we expect the latter to give more accurate predictions but to have incomplete coverage when a test sentence is not fully and correctly parsed or when the test lexical items were not seen in the appropriate contexts in training. therefore also average the predictions of the two model types in the hope of attaining superior performance with full coverage.
 Win5 An LDA model using 5-word-window contexts (so |
C  X  T An LDA model using syntactic co-occurrences with similarity computed accord-
T  X  C An LDA model using syntactic co-occurrences with similarity computed accord-
T  X  C A model averaging the predictions of the C  X  T and T  X  Win5 + C  X  T ,Win5+ T  X  C ,Win5+ T  X  C A model averaging the predictions of
TFP11 The vector-space model of Thater, F  X  urstenau, and Pinkal (2011). We report fig-
No Context A model that ranks substitutes n by computing the Bhattacharyya similar-
No Similarity A model that ranks substitutes n by their context-conditioned probabil-
We report baseline results for the T  X  C syntactic model, but performance is similar with other co-occurrence types.
 between models we use stratified shuffling (Yeh 2000). 23 4.5.3 Results. Table 15 presents results on the Lexical Substitution Task data set. expected, the window-based LDA models attain good coverage but worse performance than the syntactic models. The combined model Win5 + T  X  C trained on BNC+WIKI gives the best scores (GAP = 49.5,  X  b = 0.23). Every combined model gives a statistically significant improvement (p &lt; 0 . 01) over the corresponding window-based Win5 model.
Our TFP11 reimplementation of Thater, F  X  urstenau, and Pinkal (2011) has sligh tly less than complete coverage, and performs wo rse than almost all combined LDA models.
To compute statistical significance we only use the sentences for which TFP11 made predictions; for both the BNC and BNC+WIKI corpora, the Win5 + T 624 gives a statistically significant (p &lt; 0 . 05) improvement over TFP11 for both GAP and  X  , while Win5 + T  X  C gives a significant improvement for GAP and training corpus. The no-context and no-similarity baselines are clearly worse than the full models; this difference is statistically significant (p and all models.
 basic Win5 model. The full Win5 + T  X  C outperforms our reimplementation of Thater, models trained on the English Gigaword Corpus. This corpus is of comparable size to the BNC+WIKI corpus, but we note that the res ults reported by Thater, F  X  urstenau, and
Pinkal (2011) are better than those attained by our reimplementation, suggesting that uncontrolled factors such as choice of corpus, parser, or dependency representation may be responsible. Thater, F  X  urstenau, and Pinkal X  X  (2011) results remain the best reported for this data set; our Win5 + T  X  C results are better than Dinu and Lapata (2010) and
Thater, F  X  urstenau, and Pinkal (2010) in this uncontrolled setting. 5. Conclusion
In this article we have shown that the probab ilistic latent-variable framework provides a flexible and effective toolbox for distributional modeling of lexical meaning and gives state-of-the-art results on a number of semantic prediction tasks. One useful feature of this framework is that it induces a representation of semantic classes at the same time as it learns about selectional preference di stributions. This can be viewed as a kind of coarse-grained sense induction or as a kind of concept induction. We have demonstrated that reasoning about these classes leads to an accurate method for calculating semantic similarity in context. By applying our models we attain state-of-the-art performance on a range of evaluations involving plausibility prediction, in-context similarity, and lexical substitution. The three models we have investigated X  X DA, R L
EX -LDA X  X ll perform at a similar level for predicting plausibility, but in other cases the representation induced by one model may be more suitable than the others. rate methods for other tasks where disambiguation is required; an obvious candidate would be traditional word sense disambiguation, perhaps in combination with the probabilistic WordNet-based preference models of  X  OS  X  eaghdha and Korhonen (2012).
More generally, we expect that latent-variab le models will prove useful in applications where other selectional preference models have been applied, for example, metaphor interpretation and semantic role labeling.
 learned by the model. As previously mentio ned, probabilistic generative models are modular in the sense that they can be integrated in larger models. Bayesian methods for learning tree structures could be applied to learn taxonomies of semantic classes from Bayesian hierarchical language modeling (Teh 2006), one could build a model of selectional preference and disambiguation i n the context of arbitrarily long dependency paths , relaxing our current assumption that only the immediate neighbors of a target word affect its meaning. Our class-based preference model also suggests an approach to identifying regular polysemy alternation by finding class co-occurrences that repeat
WordNet (Boleda, Pad  X  o, and Utt 2012). In principle, any structure that can be reasoned about probabilistically, from syntax trees to coreference chains or semantic relations, can be coupled with a selectional preference mod el to incorporate disambiguation or lexical smoothing in a task-oriented architecture.
 Acknowledgments References 626 628 630
