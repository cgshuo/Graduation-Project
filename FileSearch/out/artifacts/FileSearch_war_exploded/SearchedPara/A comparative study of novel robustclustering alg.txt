
CPIB, School of Bioscience, The Un iversity of Nottingham, Nottingham, UK School of Computer Science, The Uni versity of Nottingham, Nottingham, UK 1. Introduction parameters w.r.t. appropriate priors [15].
 model has the following form: where H 0 is the base distribution,  X &gt; 0 is a positive scaling parameter, x comes from a distribution p with parameter  X  the joint distribution of the parameters p (  X  1 ,  X  X  X  , X  value of  X  sampled from DP with probability  X  convenient way of comparing the two automatic model selection approaches. robust clustering. Section 7 concludes the paper. 2. Variational inference bound of the evidence is as follows: energy F .
 coordinate ascent search. 3. Robust Bayesian mixture modelling be pointed out. 3.1. A brief review of robust Bayesian mixtures t -distribution. That is: mean  X  and precision  X  de fi ned as follows: the same latent variables, i.e. z,u , and the same parameters  X , X   X  , while a uniform prior is assumed in all other SMM.
 Type-I SMM differs from other SMM in its dependency between the latent variables z allow us to study the role of the priors on the robust clustering performance. fully factorised as
In summary, we list the differences among these SMM as follows:  X  Type-I, II and IV SMM have no prior assumed for the degree of freedom  X  described in the literature. 3.2. Type-IV SMM our setting, we choose the following conjugate priors over the parameters {  X , X ,  X  } : where The prior on the precision  X  that is: Note that in [2] the Normal-Wishart distribution is assumed: which means that the priors of  X  and  X  are dependent.
 ables can be written as follows: K } is a vector of parameters of the model, and H B = { u variables. The terms are de fi ned as follows: Moreover, the prior of the latent variable z 3.2.1. Tree-structured variational inference sation in this paper as well. We factorise the auxiliary posterior q as follows: 3.2.2. Posterior distributions of the latent variables analytically.

Speci fi cally, the auxiliary posteriors of the latent variables U derivation of these auxiliary posterior. If we de fi ne and Then q ( u
The variational posterior of class assignment q ( z We have z tering algorithms, it can be seen from Eq. (12) that the latent variable u as the Mahalanobis distance. 3.2.3. Differences among the robust clustering methods lighted as follows. First of all, the parameters of the auxiliary posteriors of U Type-II SMM (which are also gamma distributions) are as follows: From these equations, we see that Type-IV posterior of U
The respons ibilities q ( z the different priors on robust clustering performance.
 3.2.4. Posterior distributions of the parameters then q (  X  have the following equalities: The posterior of q (  X  The suf fi cient statistics of q (  X  The variational posterior q (  X  ) can be easily derived as q (  X  )= D (  X  |  X   X  ) where Algorithm Secant Algorithm ( f,x 1 ,x 2 ) Input: , x 1 , x 2 Output: The root x  X  . 1. initialise e =1 , n =1 and x 2. while true 3. n = n +1 4. x 6. if ( x 7. then x 8. else x 9. x 10. if ( e&lt; ) break end if 11. end if 12. end while
Finally, since no prior is put on the degree of freedom  X  by solving the following nonlinear equation: subject to  X  solution in these optimisation algorithms violates the constraint (  X  as illustrated in Fig. 2.
 relation: violates the constraint. If this happens, we propose to update the x divide the search space  X  f ( a ) f ( b ) &lt; 0 . 4. Robust nonparametric DP mixtures and the robust Bayesian approach will be highlighted.
 as follows: 1. Draw V 2. Draw  X   X  3. Draw  X   X  4. For the n -th data points: where for k = { 1 , 2 ,  X  X  X } ,  X  called the atom.
 presented in the last section, the DP mixture put the same priors on  X  V = { V 1 ,V 2 ,  X  X  X } ,  X   X  = { (  X   X  1 ,  X   X  1 ) ,  X  X  X } , U =( u 1 ,u 2 ,  X  X  X  ,u written as follows: probabilities appearing in Eq. (16) are written as follows: ferences are those of p ( z Bayesian mixture, the prior for z  X  . While the prior of z variational distribution, as in [5]. We fi xavalue K ,andlet q ( v proportions  X  where the factorisation of q ( u
We obtain that the variational distributions of q ( u ables in the Markov Blanket of u and their formulae can be obtained from previous sections.
To calculate the derivative of F D w.r.t q ( v the appendix for a detailed description. It turns out that q ( v and  X 
For the posterior q ( z v ) + log p ( u n |  X  ) u 5. Interpreting the models the outlier detec tion criterion. For each data point t as follows: the following value follows:
For RDPM, the predictive distribution is computed as follows: test data. With the trained parameters  X   X  , the log-likelihood bound can be computed. 6. Experimental results section, the characteristics and where the datasets are used. II SMM lies in the assumption for the prior of  X  SMM is only on the optimisation for  X  density estimation. 6.1. Comparison between Type-IV and Type-II SMM the two algorithms in the two situations, respectively. point is assigned to a label k if q ( z improve clustering performance.
 parameters obtained are also not sensitive to the outliers. 6.2. Comparison between RDPM and conventional DPM other Gaussian-based fi nite mixture models. datasets. 6.3. Comparison between Type-IV SMM and RDPM experiments in an attempt to highlight differences between the two algorithms. 6.3.1. Comparison on previously used datasets Type-IV SMM.
 6.3.2. Outlier detection and model selection model selection ability.
 outlier detection.
 are very similar in various experimental settings.
 model selection. 6.3.3. Comparison on UCI datasets From the plots, it can be observed that the two algorithms perform similarly. 6.3.4. Further experiments to highlight differences [  X  100 is sampled as the test data set. clusters.
 SMM obtains a higher log-likelihood bound on the hold-out data set than that of RDPM. 7. Conclusions recommend the use of Type-IV SMM for robust clustering. References Appendix The auxiliary posteri ors corresponding to the Bayesian model
The relevant F B terms with the Lagrange penalty for q ( u
Taking derivatives of F We can evaluate the nominator as follows: q ( u n | z n = k ) . We omit these derivations.
 The auxiliary posteriors corresponding to the Dirichlet Process mixtures
Discard the terms in F D which are independent of v where
We need to calculate the expectation E follows: Note that Eq. (26) holds because of the fact that since q ( v The term E
Then we can evaluate Eq. (24) and  X  as follows: The lower bound lated, respectively, as follows: where  X  is framework. The related terms in F C are as follows: The expectation of the complete log-likelihood of the Bayesian model is where The expectation of the complete log-likelihood F D is where Equation (25) gives the evaluation of E E q [log(1
