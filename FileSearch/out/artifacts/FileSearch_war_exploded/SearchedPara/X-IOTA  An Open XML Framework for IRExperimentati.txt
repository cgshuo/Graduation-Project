 New ideas and proposals for solutions in the field of Information Retrieval (IR) must be generally validated by indexing experiments using test collections. A test collection gathers an important mass (giga byte) of documents and a significant number of queries (about hundred). These queries are solved; it means that the relevant documents for them are known. We then run our system on these queries, and compare the results. Campaigns TREC 1 are the canonical examples of this kind of evaluation. IR Experiments are specific for their uniqueness and the originality of the tests, the great amount of parameters, and the important size of the handled corpora. The developments carried out for these experiments arequiteexpensive(intimeandenergy)becauseprototypesaresometimesused only once. It is then very important to manage carefully the time allowed between additionnal programming, or installation of the experiment itself, and the time spent running experiments. According to the complexity of the treatments, and number of parameters that one wishes to test, the time of running experiments can be counted in weeks. puter science, there is paradoxically only a small set of IR tools easily available and quickly modifiable and adaptable to realize experiments. The most famous of these tools is certainly the SMART 2 system of Gerald Salton [1], and devel-oped by Chris Buckley. Even if SMART is rather badly documented, it allows for good adaptation to experiments if one is willing to go through the source code to adapt it. Nevertheless, it is necessary to remain very close to the under-lying IR model, and then stick to the presets schemas. Simple modifications, like changing the weighting scheme, are sometimes complicated to achieve, because changes must be made deep in the modules of the code, and lead sometimes to modifying the queries directly rather that the code of the program. Amherst X  X  Center for Intelligent Retrieval Information, became in 1998 a system marketed by the company Sovereign Hill Software Inc. Therefore, it is no longer available for research. The famous probabilistic Okapi System [3], known for It is also an integrated system that should be used without too many modi-fications. HySpirit [4] comes also from a university, but became a commercial product more dedicated to applications and less to experimentations. Among the systems available for experiments, we can cite MG 4 [5] which is dedicated for the indexing of large quantities of texts, and uses compression techniques to store index files. This system is based on an architecture integrating the data. This means that indexing modules, querying modules, etc, are independent programs (Unix), connectable by data flow (ex: Unix pipe). It is then much simpler to carry out a new experiment. On the other hand, it does not allow easily access to the data (ex: weighting) without altering the code. The structures of the files on the other hand are very clearly documented.
 to set up an open and modern software framework, using programming object concepts. This system is then a set of classes, and the final system to be tested is obtained by assembling classes that are adapted to the experimentation. Un-fortunately, this system does not seem available any more and is based on a strong integration at the level of the source code (in C++). The LEMUR Sys-tem [7] is also an experimental modular IR system. It was conceived mainly for experimenting with IR based on a language model [8]. It also works with more traditional models. Just like ECLAIR, it is based on a strong integration at the level of the source code. The SIRE system [9] is made from a set of modules which communicate using a common text mode line based protocol: each entity is described on one line. A character indicates the type of entity (beginning of document, words, etc), then one integer describes its size, then comes the entity itself. SIRE is closed to X-IOTA, the difference is mainly in the use of XML stan-dard for the transmission of information between modules, increasing flexibility. In that case, the description language of data is set, and not the format itself and the content. The IRTool project 5 also called TeraScale 6 aims to provide an IR experimental system under GPL license. The project does not seem enough advanced to be really usable. The Bow system[10] is a software GPL library for texts statistics and for general IR. It seems possible to build an IRS at the source code level integration (in C). It is used in Rainbow, and CrossBow which are more classifiers than IR systems. The Terrier system [11] is a set of Java classes, optimized for IR on large document collections. It is not yet available, apart from the Glasgow Research team. Lucene 7 is also in Java but is already avaliable.
 [12] built from the Wais system, or Xapian [13] from the Muscat search en-gine. However, these systems are dedicated to IR applications, mainly Web sites indexing. For experimentation, they do not help that much. Finally, there are currently two choices for IR experimentation: either to use one of the existing systems, almost without modifications (or with minors modifications remain-ing close to the underlying IR model), or to build a new complete system from scratch for each model of IR one wants to test. We think that there is a third way: easing experiments through a framework with following characteristics: 1. Open framework: An open framework means the possibility of reorganizing 2. Open Data Structure: Exchange of data between modules should not be 3. Distributed Architecture: A distributed architecture means the possibility of 4. Programming language independence: It is unrealistic to impose a single 5. Operating System independence: For the same reason, it is preferable not to available for research purposes with free access on the Web. The column  X  X n-tegration X  indicate the modules integration type;  X  X ode X  means by the source code, whereas  X  X ata X  indicate that only by data exchange between modules. The column  X  X anguage X  is the programming language mainly used. The architecture type and the data flow diagrams for the documents and requests treatment, can be preset ( X  X reset X ) and imposed by the software, or programmable ( X  X rog X ) in the vase of a software library, or freer ( X  X ree X ) if no sequence is imposed except the compatibilities constraints of the modules. The structure of the pro-duced data in X  X ata structure X  can be in binary format not clearly documented ( X  X in X ), documented binary format ( X  X in+doc X ), or in  X  X ML X  and documented. The column  X  X nterface X  indicates if the system has a user interface other than the basic input line mode. An interface is either in text mode or imbedded in a Web site (standard  X  X TTP X ).
 of independent modules with connections between them at the operating sys-tem level. Also, we enable module connection using network of the type socket TCP/IP with data exchanges in XML. We detail this architecture in the follow-ing part. Then, we give some simple examples of modules connection in Unix shell language. The last part relates the use of this framework to test basic weighting scheme, the scheme of Okapi often used and supposed to be a very good one, and a new one which is  X  X ivergence From Randomness X . We perform these tests on two collections from the CLEF conference. This IR framework is composed of two parts: a set of modules to carry out the effective experimentation, and a user interface which pilots this functional core and displays partial or final results. This user interface uses a Web technology. We mainly present the functional core and some basic modules.
 produces XML data to carry out a particular experiment. For example, document indexing and querying are carried out by two XML data processing module sequences. This architecture allows researcher to access the intermediate data system which is oriented toward experiments. In particular the XML language ensures that at all times, data are humanly readable and can be easily checked, browsed with simple viewer tool because no particular decoding is needed. For example, a simple presentation in a web browser is possible. XML coding also brings extensibility and flexibility, while making possible parameterization of effective XML tags to be treated and while preserving flow and other additional information not understood by the module. This functionality is an important issue that turns this framework into a very flexible system.
 modules, will limit the risks of source code strong dependences and thus makes easier the maintenance, updating and correction of the source code. This archi-tecture is also supported by a multiprocessors environment 9 since modules of a processing sequence will be automatically distributed by the system, on each available processors. The limitations of this choice is related to the scaling up of data size and to the coding and decoding the XML flow for all treatments, which generates some overhead. This overhead can be limited by buffering the input and output of XML lines during the process. Modules can be classified according the following characteristics: 1. Input genericity/specificity: modules with generic input accepts any XML 2. Output genericity/specificity: This is the converse characteristic of the pre-3. Transparence/opacity: this is a very interesting feature of this framework: it 4. Data flow/direct access processing: the data flow processing mode of treat-has generic input and output, full transparency and that processes its input by flow. It is also advisable to preserve the module transparency property, even in the case specific inputs/outputs. This transparency allows variation in the XML input without disturbing the rest of the processing. For example, in the current modules of X-IOTA, the module xml2vector produces a vector, (XML type vector ), composed by a set of coordinates formed by an identifier and a weight: &lt;vector id="DOC0001"&gt; &lt;c id="1968" w="1"/&gt; &lt;c id="accompagn" w="1"/&gt; &lt;/vector&gt; dition, and not as a sufficient definition. That means that modules that are input specific to this type of structure, must nevertheless tolerate more information, like additional attributes which will be simply retransmitted without any mod-ifications. In the example below, we can add part of speech information to each coordinate (pos), and continue to use modules that process vector data XML type. This added information will simply flow throughout the module. &lt;vector id="DOC0001"&gt; &lt;c id="1968" w="1" pos="NUMBER"/&gt; &lt;c id="accompagn" w="1" pos="VERB"/&gt; &lt;/vector&gt; the following part, a canonical example of indexing and querying steps which uses very basic modules. This example is based on the vector space model. This section illustrates the possible operation of this system by presenting basic sequences for an indexing and a traditional interrogation in the vector space model. 3.1 Indexing The first part is indexing which consists in producing a direct matrix file after some treatment of documents. A direct matrix file stores each document as a vector of terms and weights. The chain of treatments includes the suppression of contained in the file common_words.fr , a stemming in the French style, then the production of the document vector. Each XML entity identified by the tag -docTag will be regarded as a document. This process is described as set of piped shell commands interpreted by a system shell script. We are using here, thepipemechanism(symbol | ) of Unix system. Any other script language can be used instead of shell script (make, PHP, Perl, etc). The only constraint is the connection of the input and output of the modules in the process flow. Also, any kind of transparent treatment can be included into this chain to supplement it, and adapt it to a particular experiment. cat OD1 | xmldeldia | xmlcase | xmlAntiDico -dico common_words.fr extraction of the text contained between the tags div . In this example, each document has an identification in the parameter id of tag div which is preserved. All the modules are generic in input and output and transparent. Only the module xml2vector produces a particular type of data: a weighted vector of degree of transparency 10 . This data processing sequence produces, for example, the following vector for the first document of the french test collection OFIL of the Amaryllis part of the CLEF test campaign: &lt;vector id="2271448" size="188"&gt; &lt;c id="1968" w="1"/&gt; &lt;c id="accompagn" w="1"/&gt; &lt;c id="achev" w="1"/&gt; &lt;c id="an" w="2"/&gt; &lt;c id="analys" w="1"/&gt; ... &lt;/vector&gt; is a simple list of coordinates identified with a weight. The size value of each vector, indicates the number of coordinates that follows. Vector identifier is the one which was taken from the document in the tag indicated in the parameter -docTag and -id . The weighting scheme associated with w attribute is in this example only the number of occurrence of the terms in the document (i.e. term frequency).
 vectors. The identifier corresponds here directly to the document one, found direct matrix and calculating weights. The current version of X-IOTA, proposes a inversion module working in memory. That enables high speed inversion times, but limits the size of the handled matrices to that of main memory. For OFIL which is about 34Mo, approximately 50Mo of memory is necessary. The type of weighting is coded in this inversion module. It is possible to carry out an inversion without weighting and to build separately a module which calculates all weightings of the matrix. See the next section for more details on the weight scheme we have used. cat OD1.vector | xmlInverseMatrix -w ltc &gt; OD1.inverse.ltc thevectorsidentifiersaretermsandthoseofthecoordinatescorrespondtothe documents id. A generic module ( xmlIdxSelect ) with direct access selects and to query the matrix. For obvious efficiency reasons, the file must be first indexed (the module xmlindex ) to builds a direct access index to the file XML knowing the tag to be indexed and the identifier attribute name. This module can be also used to access the document corpus using the document id, as it works on any XML file. xmlIndex OD1.inverse.ltc vector id id of the file OD1.inverse.ltc . The matrix is then ready for querying. At this stage it is possible to extract any term vector, knowing its id term using the direct access module xmlIdxSelect : xmlIdxSelect OD1.inverse.ltc baudoin baudoin . This example illustrates the simplicity of the implementation of an indexing chain using this framework, and also proves it is possible to intervene at any moment in the indexing process thanks to all data remaining coded in XML. We now will examine how to run queries. 3.2 Querying Querying within this framework consists in launching all the queries of the test collection. Queries must undergo the same type of treatment, so that dimensions of the vectors are compatible. The querying procces is in fact a matrix product between the direct matrix of the queries (queries id x term id) and the inverse document matrix (term id x doc id) done by the xmlVectorQuery module. cat OT1.vector | xmlVectorQuery OD1.inverse.ltc direct access to term vectors. The module xmlvector2trec is charged to con-vert the answer matrix (query x document), into a compatible format with the trec_eval tool which produces the recall and precision standardized curve. We note that the installation of such an experimental indexing chain and querying breaks up into just few modules. To vary the parameters or to introduce a partic-ular treatment, it is easy to insert a special treatment into the XML flow of the original chain. For example, it is enough to insert the generic module xmldelsub to suppress some fields of the XML query tree in order to choose the fields of the requests to be used: cat OT1 | xmldelsub ccept | xmlAntiDico -dico antiDico.txt transforming them into vectors. In the same way, modification of the weighting of the terms of the request can be carried out either in an integrated way by modifying the code of construction of the vectors (i.e. xml2vector ), or by adding a specialized module in to the flow. The code of xml2vector is of a small size (like the majority of the modules), so the modification is very localized and thus involves less risks than the modification of the source code of a bigger integrated system like SMART.
 module, hence not affecting other modules. It is not the case, when system integration is carried out by the source code: a writing error can affect any part of the system, because no memory protection exists between functions of the same process, but strong memory protection is carried out by the system on separate running programs. 3.3 Distributed Operation A modular nature at the level of the operating system enables one to quickly set up a distribution of the modules on other computers and operating systems. Modules can easily be transformed into server that receive and re-emit data in XML format on a particular internet port. Any input/output redirection tech-nique can be used to access a server. The complexity to set up such a distributed mechanism depends on the operating system used. For example, Unix systems propose a very simple method 11 . Users must only provide the module that im-plements the protocol layer on standard input/output. The rest is provided by the OS. Every module can then become a server. The only adaptation to be realized is related to the parameter exchanges. One possible format can be the RCP protocol for XML and HTTP transactions. The French parser of the IOTA system is already installed in this way on one of the machines of the Grenoble IR team. The problem of the rights for the access and safety of such distributed architecture, can be solved with technologies of SSH protocol coupled with the mechanism of tunneling for a connection of modules through fire-walls. 3.4 Efficiency In this section we compare the XIOTA framework with the system SMART using English collections of the CLEF2003 campaign. We just want to test the speed of both systems, and the index size so we have chosen the simple indexation: case changing on words (SMART full option). Tables 2 and 3 present results. is compatible with SGML format and can handle these collections, without any conversion. The SMART system need a conversion or need a new module to be programmed and integrated into the source code. We have used an external conversion written in Perl. For the XIOTA system, the index size is the sum of the inverted matrix, and the hash code index of this matrix. For the SMART system, the index size is the sum of all dictionary files, with the inverse file. collection is stored in several files, creates an overhead of about 7 times more to compute the same index. It is then the  X  X rice X  to pay for having more flexibility inthetreatments.Thereisalsoanoverheardinthesizeusedtostoretheindex because of the use of XML and also because all terms and document references are stored in the actual inverse file. In fact, SMART is not able to manage document ID that has to be extracted and stored separately.
 system running on a P4 3.0 Gz with 2Go of main memory and 120 Go of disk of the I2R lab. The table 5 suggest that an integrated IR system like SMART, is a good choice when very few modification are needed. The XIOTA system is a good choice when experimenting new ideas, because in this framework, a change is counted in minutes or hours of programming time (ex: adding a XML line filter in perl), as for SMART it can take days if the modification is possible. In this part, we present how we have used this system on a multilingual collection of the CLEF conference. Our purpose is the studies of classic and statistical weighting scheme in order to choose the best one for the CLEF experiments. We have work on the Finnish and French collections. We will present at first, the weighting scheme we would like to test. It has been shown that for textual IR purposes, one of the best weighting schemes is the one used in Okapi system [3]. This weighting is based on a probabilistic IR modeling. We will compare this weighting with a new challenger that is based on a deviation from random behavior [14]. 4.1 The Underlying IR Model These experiments are grounded on the classic vector space model, because it in-cludes both models we want to test. The goal of the experiment is to compare the statistical Okapi model with Deviation From Randomness model, versus more classical weightings. This comparison will be done on two different languages. Basically, the final matching process is achieved by a product between query and document vectors, which computes the Relevant Status Value (RSV) for all documents against one query. For a query vector Q =( q 1 ...q t )of t indexing terms, and document vector D j =( d 1 j ...d tj ), the RSV is computed by: and query processing to select indexing terms, and in the weighting scheme. We recall here the scheme that is inspired by the SMART system. We suppose the previous processing steps have produced a matrix ( d ij ). Initially the value d ij is only the result of indexing term i counting in the document j , called term frequency tf ij . Each weighting scheme can be decomposed in three steps: a local, a global and a normalization step. The local is related to only one vector. All these transformations are listed in table 6. For all measure we use the following symbols: n number of document in the corpus t number of unique indexing terms in the corpus tf ij frequency of term i in document j f i frequency of term i in the corpus: f i = j  X  [1 ..n ] tf ij d ij current value in the matrix (initialy tf ij ) w ij new value in the matrix d ij a normalization of d ij (see below)  X  i the fraction f i / S df i number of document indexed by term i (document frequency) c, k 1 ,b constants for DFR and Okapi L awrL mean document length: awrL =( k  X  [1 ..n ] L k ) /n q i weight of term i of query q into account the relative importance of a term regarding the whole document collection. The most famous is the Inverse Document Frequency Idf. The table 7 lists the global weighting we have tested. The Okapi measure described in [15, 3], uses the length of the document and also a normalization by the average length of all documents in the corpus. This length is related to the number of indexing terms in a document. The Okapi measure uses 2 constants values called k 1 and b . Finally we tested a new measure describe in [14] called  X  X ivergence from Randomness X  (DFR). We just give here the final formula we have implemented in our system: the average size of all document in the corpus : awrL .Aconstantvalue c adjusts the effect of the document length in the weight: scheme is composed by the combination of the local, global and final weighting. We represent a weighting scheme by 3 letters. For example, nnn is only the raw term frequency. The scheme bnn for both documents and queries leads to a sort of Boolean model where every term in the query is considered connected by a conjunction. In that case the RVS counts the terms intersection between documents and queries.
 computation of the cosine between these two vectors. This is the classical vector space model if we use the ltc scheme for document and queries. The scheme nOn for the documents, and npn with the queries, is the Okapi model, and the use of nRn for document and nnn for the queries is the DFR model. For these two models, constants has to be defined.
 query by i q 2 i . For each query this is a constant value which does not influence the relative order of answered document list. It follows that this normalization is useless for queries and we will not use it. 4.2 Finnish IR We have used this framework to choose the best weighting scheme for this lan-guage in order to compare classic IR approached with IR based on Natural lan-guage techniques. We present in this paper only this first part and not the NPL techniques and results. We have used the 2003 collection of CLEF documents in Finnish and in French. This section presents the Finnish results. This collection is composed of 55344 documents. We have used 2003 topics. Documents and tags from documents or queries. Then we transform XML special characters to their ISO counterpart. We delete all diacritic characters, and change to lower case. At this stage we still have special Finnish characters and accents. We elim-inate common words using a list provided by Savoy 13 and then suppress all accents from characters. We apply a Finnish stemmer also proposed by Savoy and modifies to accept XML input/output to produce the final vector. xmlFilterTag | xml2Latin1 | xmldeldia | xmlcase | xmlAntiDico -dico common_word.fi | xmlcase -noAcc | xmlStemFi For documents only the text field has been used. Table 10 sum up the average precision results for all combination of weighting. We only keep a maximum of 1000 documents per query. For the DFR we have fixed the constant c to 0.83. The table 9 shows the results of the constant variation for the nRn nnn weighting scheme. The second column, we show the number of relevant and effectively retrieved document.
 twicethecconstant.ThisvalueistheonepromotedbyAmatiin[16].The maximum average precision is obtained by the value 0.83, so we keep this value for all experiments. For the Okapi weighting, we have use the same value as in value for the French collection: it seems these values are on average good ones. interesting documents and queries weighting schemes. Results show clearly that the DFR weighting is very stable under every weighting scheme except for the binary (bnn). Also this measure is better than Okapi and all others, but Okapi is still very close. Initially, the DFR measure is supposed to be used with only the query term frequency (nnn). We discover one little improvement using ltn and dtn for the queries. 4.3 French IR We have used the French corpus of CLEF 2003. We have used our own stemmer, andourownlistforremovalofcommonFrenchterms.Inthiscollection,there are 3 sets of documents. For each collection we have selected the following fields: lemonde94 TITLE TEXT, and TI KW LD TX ST for sda 94 and 95. For the queries, we have selected the fields FR-title FR-desc FR-narr. We have tested the same combination of weighting schemes as the one tested in the Finnish collection. The results are in the table 11.
 (nOn) and we have computed some variation of the two constant k 1 and b .The which confirm the choice usually taken for this measure. In this language, we also note that the stability of the DFR measure (nRn) is better than other query weightings, except with binary queries (bnn).
 (ntn). We have not performed any special treatments for the queries, like re-moving terms that are not related to the theme (ex: document, retrieved, etc). The results show that a natural language analysis of the query to remove these empty words should improve the results.
 IR, by providing a flexible and open architecture based on recent technologies. This architecture allows fast integration of new modules for natural language treatments in IR. We think that to sacrifice the speed of treatment in order to gain flexibility in the construction of experiments, is a worthwhile bet. It is true that one can argue against the choice of XML coding, for its lack of compactness and also the overhead generated by coding and decoding of infor-mation. The question is also raised in the XML community and leads to the definition of a possible  X  X ML Binary X  format which represents original data in a more compact way. System X-IOTA was already used for the test cam-paign CLEF 2003/04 [17]. Indexing and query time performances are reduced compared to the integrated system SMART. Nevertheless, the effectiveness is with a sufficient number of modules to carry out experiments of bases in IR is freely available at http://xiota.imag.fr . Because of its simplicity, this system could also be adapted to teaching IR experiments. Concerning the experiment of the weighting scheme, we recommend the use of the DFR which its very sta-ble weighting that gives very good results almost independently of the query weighting.
 Fran  X  cois Paradis. The experience of his PIF experimental IR system, has inspired orientation of initial IOTA system [18] towards current system X-IOTA. I also thank Lonce LaMare Wyse and Leong Mun Kew from the Institute for Infocomm Research for their useful comments on this paper.

