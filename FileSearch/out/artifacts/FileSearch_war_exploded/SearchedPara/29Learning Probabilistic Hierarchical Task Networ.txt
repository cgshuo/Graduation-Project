 WILLIAM CUSHING, SUBBARAO KAMBHAMPATI, and SUNGWOOK YOON , Application of learning techniques to planning is an area of long-standing research interest [Zimmerman and Kambhampati 2003]. Most work (e.g., [Ilghami et al. 2002; Yang et al. 2005, 2007; Langley and Choi 2006; Hogg et al. 2008]) in this area to date has, however, only considered learning domain physics or search control. Knowledge acquired by these algorithms helps planners to generate feasible plans, either with greater reliability or less computation. A relatively neglected alternative application of learning, and the focus of this work, is to produce higher-quality plans: we apply automated learning to acquire users X  preferences concerning plans.

It has long been understood that, in practice, (1) the framework of classical plan-ning [Fikes and Nilsson 1971] is not up to the task of representing complex constraints on whole plans [Nau et al. 2003], and (2) that user preferences can indeed be quite complex in this regard [Baier and McIlraith 2008]. Hierarchical task networks (HTNs) are, among other choices, an effective and popular means of representing such complex constraints on plans.

We view task networks as (context-free) grammars [Kambhampati et al. 1998; Geib and Steedman 2007], saying that a plan is parseable if it satisfies the constraints represented by an HTN, and feasible if it satisfies the remainder of the constraints, that is, the classical planning constraints. Then, we can represent absolute preferences unparseable yet feasible plans are held to be so loathed by the user at hand that it would be better to fail to generate the plan than to offer the alternative [Kambhampati et al. 1998]. To represent degree of preference, we generalize our chosen representation language to probabilistic hierarchical task networks (pHTNs).

Elaborating, consider Figure 1. It depicts the preferences, modeled as an HTN, of a hypothetical user for the travel domain. So, for this user, both Gobytrain and Goby-bus are acceptable reductions of the Travel task. In contrast, the plan of hitchhiking (modeled as a single action), while executable and goal achieving, is not considered valid X  X he user in question loathes that mode of travel to the point of giving up rather than hitchhiking. For some other user, we might very well have included an arrow directly from the Travel task to hitchhiking. Such boolean preferences are themselves interesting, but clearly it is desirable to accommodate degrees of preference. To do so, we attach probabilities to the methods that reduce tasks into subtasks, equating  X  X robable X  with  X  X referred X , arriving at probabilistic hierarchical task networks (pH-TNs). The preferences, as we will formally explain later, are defined with respect to the reductions used.
 Note that one can employ HTNs to encode complex executability constraints as well. This is the view taken when learning domain physics in the form of HTNs. Indeed, one need not even attach any meaning to the primitives of a domain at all, instead representing all planning constraints by parseability of the given network. Likewise, one can very well go about representing all constraints purely in the form of precon-ditions and effects. We do not seek a homogenous representation of all constraints. We envisage separate representations for simple physical constraints as preconditions and effects of primitives, for simple quality constraints as goals of achievement, for complex physical constraints as task reductions of a HTN (or set of logical assertions in linear-time temporal logic (LTL), Golog, . . . ), and, the topic of this article, for complex quality constraints as probabilistic task reductions of a pHTN.

To automatically learn pHTNs, we view them as mere probabilistic context-free grammars (pCFGs) and try to simply apply the proven techniques for probabilistic grammar induction [Collins 1997; Charniak 2000; Lari and Young 1990]. Doing so requires significant compromises, since HTNs, by default, are a significantly richer formalism than CFGs [Erol et al. 1996]. Moreover, the application areas of planning and natural language processing are quite distinct: the mapping between plans as sequences of actions and sentences as sequences of words (etc.) is only sound at a first pass. Then the work here undertaken is to adapt the techniques of (probabilistic) grammar induction to account for the differences between the appropriate assumptions for natural language processing and the assumptions appropriate for planning. We make two contributions. (1) We eliminate the need for structurally annotated input. (2) We learn the user X  X  desired, rather than typical, behavior.
 The following two sections elaborate on the two issues considered and our approach to resolving them. (Lack of) Known Structure. The first order of business is to overcome the assumption from natural language processing that significant information is available concerning the correct structure of the grammar. Since the point of language is to communicate mental constructs, it is indeed reasonable to suppose that significant information con-cerning the  X  X ight X  grammar nonprimitives could be obtained prior to learning. That is, the  X  X ight X  non-primitives ( verb , noun , noun phrase , . . . ) are common to many users. So specifically, algorithms for part-of-speech tagging, for example, are acceptable pre-processing steps in grammar induction. In contrast, preference is personal: the correct non-primitives are not common to an overwhelming majority of users.

There have been some ideas from previous work on grammar induction that addresses structure learning by searching for the target grammar by adding or merging nonterminal symbols (e.g., [Wolff 1982; Langley and Stromsten 2000; Stolcke 1994; Cook and Holder 1994; Vanlehn and Ball 1987; Nevill-Manning and Witten 1997; Unold and Jaworski 2010]). In our case, rather than trying to build a structure learner that is superior to the existing algorithms, we develop a structure hypothesizer (SH) that is suitable for the current learning task. Using existing structure learners is also possible. In the future, we would like to carry out more experiments to see which learning algorithm is more suitable for the learning task. For now, the proposed SH invents (out of thin air) the concepts needed by the subsequent application of the specific expectation-maximization (EM) approach we borrow from the literature on probabilistic grammar induction [Lari and Young 1990]. We empirically demonstrate that, despite the lack of correctness guarantees of SH, the final performance is nonetheless promising. We further validate our approach by comparing it to the well-known inside-outside algorithm for grammar induction [Lari and Young 1990] and demonstrating the superiority of our approach.

Feasibility Constraints. The second order of business is to correct for the assumption that the point of learning is to reproduce the input distribution [Li et al. 2009b]. For, say, supporting expectation-based parsing (analogously: plan recognition [Geib and Steedman 2007]), one does indeed wish to learn the typical distribution of utterances. But, for preferences, the desired output of learning is the distribution over what the user wants to do (i.e.,  X  X ay X ), rather than the distribution encoding the user X  X  typical behavior (i.e., typical  X  X tterances X ). As a first pass, we use this insight merely to motivate the choice of performance metric in our empirical evaluations.

The second part of the article considers in greater depth the issue that feasibility constraints can prevent users from engaging in preferred behavior and thus also ob-scure our ability to infer preferences by simple observation of behavior. To resolve the issue, we permit ourselves to observe the alternatives that the user might have hy-pothetically executed (in addition to the actual choice of behavior). For example, in (computer-aided) travel planning systems (e.g., Orbitz), we can indeed directly observe possible flight reservations shown to, but not chosen, by the user. 1 More generally, we propose the use of diverse (automated) planning to fill in any gaps of knowledge concerning plausible alternatives to observed behavior. In any case, we implement a simple rescaling approach that uses this additional information to rescale the input distribution to the learner so as to better reflect desire rather than behavior, and em-pirically demonstrate its effectiveness [Li et al. 2009a]. More accurately, the approach consists of clustering , transitive closure , and, finally, rescaling .

In the following sections, we start by formally stating the problem of learning prob-abilistic hierarchical task networks (pHTNs). Next, we present an algorithm that ac-quires pHTNs by observing only plan traces. The algorithm modifies the techniques of probabilistic grammar induction by performing structure-learning prior to the sub-sequent application of an expectation-maximization (EM) approach to parameter-learning. We compare the proposed approach with a straightforward application of pCFG learning method by evaluating the acquired pHTNs against idealized models of users. The experimental results show that the learner outperforms the inside-outside algorithm [Lari and Young 1990], and at least in the ideal case, can in fact capture its input distribution given a reasonable number of samples. Subsequently, we consider possible obfuscation of user preference due to feasibility constraints. The approach is to rescale the input so that the initially described learner will behave as desired without further modification. We again conduct an empirical evaluation for idealized models of users by considering worst-case models of feasibility constraints, thereby giving the technique maximum room for improving performance. Finally we discuss related as well as future work and summarize our contributions. In this section we formally define the problem of learning user preferences using proba-bilistic hierarchical task networks (pHTNs) as the model. These are exactly equivalent in form, but not in application, to probabilistic context-free grammars.

Table I provides an example of a candidate pHTN potentially modeling the pref-erences of some hypothetical user in the Travel domain (see Figure 1). The example emphasizes the precise nature of the problem faced by the pHTN learner we develop. In particular, unlike the case of learning pCFGs for natural language, the internal non-primitives have no externally ascribed interpretation.

We begin with several supporting definitions. HTN. A hierarchical task network (HTN) H = A , T , M consists of a set of actions A (the primitives), a set of tasks T (the non-primitives), and a set of methods M (the rules). We follow the (notably restricted) SHOP notion of HTNs [Nau et al. 2003]. Each method m  X  M is an ( + 1)-tuple, Z , t 1 , t 2 ,..., t , also written Z  X  t 1 ... t , specifying how the task Z  X  T may be performed by sequentially performing each t i . (In terms of production rules, one rewrites Z into the right-hand side of the chosen method.) Denote all the available methods for performing a task Z as M ( Z ) ={ m  X  M | m = Z ,... } .
Chomsky Normal Form. Without loss of generality, 2 we immediately restrict our attention to Chomsky Normal Form : each method decomposes a task into either two tasks or one primitive. So for any method m ,either m = Z , X , Y (also written Z  X  XY ), with X , Y  X  T ,or m = Z , a (also written Z  X  a )with a  X  A .

Actions: Classical Planning. We suppose that actions are defined as in classical planning [Fikes and Nilsson 1971]. Abstractly, a planning domain delinates a set of possible states and ascribes partial (state) transition functions to each of the actions (the primitives A ). For simplicity, we identify actions with their partial transition functions. Then an action a is executable from s if a ( s ) is defined. A plan  X  = a 1 , a 2 ,..., a n (an action sequence) is just defined by left-to-right partial function composition (  X  ( s ) = a achieves G from s if  X  is executable from s and  X  ( s )  X  G is true, equivalently:  X  ( s )  X  G is defined and true. A planning problem is given by its initial state and goal (semantically a set of states). A feasible plan achieves the goal from the initial state. Tasks: HTN Planning. HTN planning is an extension of classical planning by some HTN H = A , T , M and top-level task T  X  T . Plans remain as action sequences. An action sequence is a solution if it is both feasible and parseable to the task T by the methods M . In short, a parse just records generating a plan from the top-level task according to the given methods.

Formally, a parse (tree) X = ( V , E ) of an action sequence  X  = a 1 ,..., a n by the methods M to the task T is an ordered labeled binary tree on n leaves with root r satisfying the following. Label each vertex v  X  V with task T ( v )  X  T and method m ( v )  X  M ( T ( v )). Partition the vertices V into the ordered leaves L = parents P (internal vertices): V = L  X  P . Then, the following. (1) T ( r ) = T . (2) For each parent p  X  P , with ordered children x , ym ( p ) = T ( p )  X  T ( x ) T ( y ). (3) For each leaf i  X  L : m ( i ) = T ( i )  X  a i .

Now we come to the generalization of the formal model to probabilities. pHTNs. A probabilistic hierarchical task network (pHTN) H = A , T , M , X  is an ex-tension of the underlying HTN A , T , M by an assignment of conditional probabilities  X  (a.k.a. parameters )tothemethods M such that, for each task Z  X  T Recall that M ( Z ) denotes all methods by which one might perform the task Z : pHTNs extend HTNs by specifying the prior distribution over each such choice. The parameters  X  then also induce a prior distribution on action sequences as follows.
Prior Probability. Fix the top-level task T and the pHTN H = A , T , M , X  .Let X be any parse (to T by M ). Say X { v } is the subtree of X with root v  X  V ( X ). With x and y the ordered children of any parent p  X  P ( X ), the prior probability of the subtree X { p } , given its root task, is, recursively, The base cases, that is, the trivial subtrees on each leaf  X  L ( X ), are just So the prior probability of an entire parse tree is So P ( X ) =  X  ( X ). In general, when the top-level task and pHTN are not fixed, then explicitly note the dependency by writing P ( X | T , H ) =  X  ( X ) instead.
Then the prior probability of an action sequence is the sum of prior probabilities of every parse of that sequence (every complete parse is a disjoint event). Let X (  X  )be those parses, that is, X (  X  ) ={ X | X isaparseof  X  to T by M ( H ) } .So, This is the probability of generating  X  by simply evaluating H top-down from T .Note that plans with nonzero priors are parseable (and thus are considered valid according to nonprobabilistic HTNs, c.f. [Kambhampati et al. 1998]).

Most Probable Parse. A nice feature of Chomsky Normal Form is that the prior probability of a plan is straightforward to calculate exactly: there can only be finitely-many possible parses of a fixed sequence of primitives. 3 Nonetheless, there may very well be exponentially many possible parses, which is not much better, in practice, than infinitely many possible parses. So in the remainder, we will end up considering just the most probable parse of  X  . With respect to some fixed pHTN H and top-level task T , define The (same) nice properties of Chomsky Normal Form are, in this case, actually ex-ploitable: the most probable parse can be computed reasonably efficiently by the cubic-time dynamic programming algorithm described in Li et al. [2009b].

Posterior Probability. Now suppose that not all plans are possible. Say F is the set of feasible plans in some specific situtation, and write [  X   X  F ] to mean converting true to 1 and false to 0. Then the posterior distribution on plans given F is Plans with nonzero posterior probability are then feasible and parseable, that is, they are solutions.

Quality. As a compromise, we use  X  X robable X  as a surrogate for  X  X referable X . There are very interesting ways in which the connections between probability and preference are stronger than they may initially appear [Koller and Friedman 2009, page 15]. So, given a top-level task T  X  T and pHTN H = A , T , M , X  , define the quality of a plan  X  is twice as valuable/good as option B , in the sense that we have observed the choice of A twice as often as we have observed the choice of B .

Note that solutions can be ranked by either posterior or prior probability: the two are proportional over solutions. It is ever so slightly better, though, to define quality as independent of feasibility as much as possible, where the quality of the solution does not only depend on observation frequencies. For example, is a measure of the gap between (our limited understanding of) the reality F and the user X  X  true preference.

Learning pHTNs. Abstractly, the problem is of automatically learning a user-specific quality metric on plans based on pure observation of behavior. The preceding sets up the specific concrete interpretation we pursue. Most importantly, use pHTNs to define the notions of plan and quality. Formally, fix the total number of task symbols k , 4 and the top-level task T . Given i.i.d. samples ={  X  1 , X  2 ,..., X  n } of observed action sequences, find the most likely Chomsky Normal Form pHTN on k task symbols, H  X  :
The likelihood of a model, though, has not been defined. We employ Bayes rule to transform to the problem of maximizing the likelihood of the observation: Our formalization of (probabilistic) hierarchical task networks is isomorphic to for-mal definitions of (probabilistic) context-free grammars. This comes at a price: HTNs, even just SHOP [Nau et al. 2003], are normally richer languages for representing con-straints over plans. The advantage is that grammar induction techniques are more or less directly applicable. As far as the insight from pCFGs is concerned, we adapt the expectation-maximization (EM) approach from Lari and Young [1990] for parameter learning.

However, despite their formal equivalence, casting the problem as learning pHTNs (rather than pCFGs) does make a difference in what assumptions are appropriate. For example, we do not allow hints concerning non-primitives to be given in any form, and in particular, we do not permit such hints as annotations on the primitives of observations. For language learning, the non-primitives of interest are widely agreed upon: noun , verb phrase , and so forth. It is impossible to communicate without such agreement. In particular, information sources such as dictionaries and informal grammars can be mined relatively cheaply in order to provide useful annotations, as in part-of-speech tagging.

In contrast, in the case of preference learning for plans, the non-primitives of interest, preferences, are user-specific mental constructs. Then it is unreasonable to rely upon annotated observations: our system must invent non-primitives of its own accord. We develop a structure hypothesizer (SH) to, among other things, engage in such invention of non-primitives. Because of the manner in which it functions, it could be easily modified to accept (certain kinds of) hints concerning non-primitives should such be available. By default though, it assumes only plain observations of behavior.
In the remainder of this section, we describe the details of the full learner. It oper-ates in two phases. First, the structure hypothesizer considers the problem of inventing sufficiently rich structure (the tasks and methods) so as to allow the parsing of each ob-servation to the top-level task. At its conclusion, we have an HTN. In the second phase, the probabilities of the methods are set by a variant of the expectation-maximization (EM) approach from Lari and Young [1990]. The final result is, naturally, a local optima in the space of pHTNs. We develop a structure hypothesizer in order to generate a set of methods that can, at least, parse all plan examples. More than that, it seeks to parse all the plan examples without resorting to various kinds of trivial grammars (for e.g., parsing each plan example with a disjoint set of methods). The basic idea is to iteratively factor out frequent common subsequences, in particular, frequent common pairs, since we work in Chomsky Normal Form. We describe the details in the following. Figure 2 summarizes the algorithm in pseudocode.
 SH learns methods in a bottom-up fashion. It starts by initializing H with tasks, Z , each distinctly reducing to one of the primitives: Z a  X  a for all a  X  A . Then all plan examples are rewritten, backwards (i.e., parsed), using this initial set of methods. All this amounts to is  X  X onverting to upper case X  X  X he initialization is minor notational fantasy to formally satisfy the requirements of Chomsky Normal Form. 6
Next the algorithm enters its main loop: hypothesizing additional structure until all plan examples can be parsed to an instance of the top-level task, T . 7 In short, SH hypothesizes a method, rewrites the plan examples using the new method as much as possible, and repeats until done. At that point, probabilities are initialized randomly, that is, by assigning uniformly distributed numbers per method and normalizing by task (so that  X  ( M ( Z )) = 1) X  X he EM phase is responsible for fitting the probabilities to the observed distribution of plans.
In order to hypothesize a method, SH first searches for evidence of simple loops: subsequences of symbols in the form { SZ , SSZ , SSSZ } or { ZS , ZSS , ZSSS } . Certainly patterns, such as ZABABAB , also have looping structure; these are identified at a later stage. The frequency of such simple repetitions in the entire plan set is measured, as is their average length. If both meet minimum thresholds, then the appropriate method (e.g., Z  X  ZS ) is added to H . Note that such loops already possess base cases due to the bottom-up strategy. This process corresponds to lines 7 X 9 in Figure 2. The thresholds themselves are functions of the average length and total number of plan examples in . For example, if the average length of such repetitions is longer than 30% of the average length of plan examples, and the repetitions appear in more than 10% of the plan examples, the method is added.

If one or both thresholds are not met, then the frequency count of every pair of symbols is computed, and the maximum pair is added as a reduction from a distinct (i.e., new) task. This is done in lines 11 X 12 of Figure 2. In the prior example of a symbol sequence ZABABAB , eventually AB might win the frequency count and be replaced with some new symbol, say S . After rewriting, the example sequence becomes ZSSS , lending evidence in future iterations, of the kind SH recognizes, to the existence of a simple loop (of the form Z  X  ZS ). If eventually that method is added, then the example gets rewritten to just Z .

Example. Consider a variant of the Travel domain (Figure 1) allowing the traveler to purchase a day pass (instead of a single-trip ticket) for the train. Two training plans are shown in Figure 3. First SH rewrites the primitives (line 1): A 1  X  Buyticket, A Next, since A 2 A 3 is the most frequent pair in the plans (and there is insufficiently obvious evidence of looping), SH introduces the task and method S 1  X  A 2 A 3 (line 11). A
S 1 S 1 S 1 ), and so adds the method A 1  X  A 1 S 1 (line 8). After rewriting (line 13), all plans are parseable to the symbol A 1 (let T = A 1 ), so SH is done: the final structure is at the bottom left of Figure 3.
 In this section, we describe the expectation-maximization (EM) approach we take to fit-ting appropriate parameters to the HTN returned by the preceding structure learning step (SH), thereby arriving at a pHTN.

In general, EM is a gradient-ascent method with two phases to each iteration: first the current model is used to compute Expected values for the hidden variables (the E-step), and then the model parameters are updated to maximize the likelihood of those particular values for the hidden variables (the M-step). To show convergence, it is more useful to characterize both steps as maximizing a single many-dimensional loss function , by holding disjoint subsets of dimensions fixed in each step. So expected actually means that one determines values for the hidden variables that maximize some loss function while holding model parameters constant. Then both steps are monotonically increasing the same function and convergence follows; of course, global optimality does not follow. 8 The following gives the details of the E-steps and M-steps as applied to learning pHTNs.

Setup. The structure learning step performs a trivial initialization of the probabilities (with no regard for the training data): say H 0 = A , T , M , X  0 is the initial pHTN. Later iterations update the pHTN, say H t at iteration t , by updating the parameters  X  t .
E-step. In the E-step, the current parameters  X  t are used to compute the most probable parse tree X  X  t (  X  ) of each example plan  X  = a 1 ,..., a n  X  (from the given start task X reasonably efficiently in a bottom-up fashion, since any subtree of a most probable parse is also a most probable parse (of the subsequence it covers, given its root, etc.). That is, the following identity holds: 9 The parsing then consists of computing all of its instantiations. More specifically, it suffices to bottom-up record the preceding products in a four-dimensional table indexed by i &lt;&lt; j  X  [ n ] and each of the methods M , interleaving that with filling out the smaller three-dimensional table (recording the left-hand maximizations) indexed by i , j , and the tasks T . By also recording the specific methods and midpoints witnessing the left-hand maximizations, the presently most probable parse of  X  , X  X  t (  X  ), can be easily extracted top-down, that is, by beginning at the method and midpoint witnessing max X P ( X | a 1 ,..., a n , T , H t ). Let r be the number of methods. Then parsing is bounded by O ( n 3 r ).

Notes. (a) In theory, the number of methods could be cubic in the number of tasks, but in practice, the point of structure learning is to prevent this. A careful implemen-tation can exploit that assumption (that the grammar will not really permit anything close to allowing every symbol to reduce to every other pair of symbols) so as to not quite so literally fill out the tables described. (b) This simple (dynamic programming) description of the parsing is convenient as it also outlines the framework for bottom-up generation of all possible parses: replace the maximizations with manipulation of weighted sets of parses. (c) In other contexts, namely, learning pCFGs, parsing easily accommodates direct, even noisy, observations of non-primitives (as generated by, say, a part-of-speech tagger). This is because the parsing computes most probable subtrees conditioned on every conceivable subtree root; with minor modifications, additional observations can be permitted in the form of nonuniform priors over the subtree roots.
M-step. After getting the most probable parse trees (with respect to the current parameters) for all plan examples, the learner moves on to the M-step. In this step, the probabilities associated with each method are updated by maximizing the likelihood of generating those particular parse trees singled out by the E-step. This merely consists of setting each probability according to its relative frequency in the parse trees just computed.

To derive that update rule, let M [event] count events in the parse forest of most probable parse trees computed in the E-step: X t =  X   X  X  X  t (  X  ). Let V = V ( X )beallof the vertices of all the most probable parse trees. Then more specifically, let M [ Z ] count those vertices v  X  V satisfying T ( v ) = Z ,andlet M [ m ] count those satisfying m ( v ) = m . one of the components, a most probable parse tree, of X t ,then The last step is justified because M [ m ] will be 0 for any method not appearing in any of the parses, so the extra terms being introduced into the product are all 1: take 0 0 = 1. (This will indeed be the form of the extra terms for the maximizing choice of  X  .)
Each subproduct (i.e., for each Z  X  T ) is a multinomial in the variables  X  ( M ( Z )), subject to the constraint  X  ( M ( Z )) = 1. There are no other constraints, so, each such subproduct can be independently maximized. Abstracting, the form of the problem is This is a classic problem [Koller and Friedman 2009, chapter 17]. One usually differ-entiates the logarithm in order to solve it. The answer is  X  i , x i = y i  X  (for  X  = i y i ). So in the parameters  X  t + 1 we have, as claimed, for all Z  X  T , m  X  M ( Z ):
Carrying out this simple update is linear in X t . So an efficient implementation need not even remember all the parse trees. Indeed, the M [] counters can be computed during parsing itself, obviating the need to even remember a single complete parse tree. So the runtime per iteration, an E-step followed by an M-step, is linear in the training iterations, then estimate by O ( X ) instead.) In any case, the per-iteration time is not the big question. How many iterations to run EM for is. While guaranteed to converge, convergence is only to a local optima. So, somewhat less obvious stopping rules, such as  X  X un for 1,000 iterations X  can be quite effective if say, EM is embedded in one of the many meta-learning strategies for dealing with locally optimizing learners given multimodal objectives. In our experiments plain, EM sufficed, and was more than fast enough to just permit it to run for very large iteration counts.

Summary. The E-step completes the input data by computing the parses of expected by H . Subsequently the M-step treats those parses as ground truth, by setting the new method probabilities to the observed frequency of their application in the completed data. This improves the likelihood of the model (under the assumptions of the preceding derivation, such as i.i.d. random variables), and the process is repeated until convergence to a local optima of the likelihood function. Hard-EM. As described, the method is the more intuitive, but nonstandard, version of EM known as hard-EM. In our case the choice of hard-EM has the specific effect of introducing bias in favor of less ambiguous grammars. That is, letting X denote the forest of parse trees (over the plans ), hard-EM is actually examining the question of maximizing P ( H , X | ) rather than the true objective of maximizing P ( H | ). (Soft-EM examines the true objective.) This single design choice has quite a few advantages, or if you prefer, one fundamental effect that presents itself in different ways. Beyond (a) the computational advantage of considering only most probable parses, and (b) the well-known generalization advantage afforded by bias against complex models, there is a less-discussed advantage that we exploit in the following: (c) hard-EM seeks models which are easier to explain/justify in terms of a limited number of examples. The very same effect also carries the disadvantage that hard-EM can be expected to fail to produce the true solution to the learning problem, even provided data well in excess of sample complexity. For much more analysis and discussion of the specific trade-offs between hard-and soft-EM, see Kearns et al. [1997] and [Kandylas et al. 2007]; for a comprehensive treatment of theoretical and practical issues in probabilistic reasoning over graphs in general, see Koller and Friedman [2009].

Structure versus Parameter Learning. Although the EM phase of learning does not introduce new methods, it does (potentially) participate in structure learning in the sense that it can effectively delete methods by assigning zero probability. Accordingly, the implementation, in post-processing, actually deletes such methods. Moreover, it deletes any methods with probabilities too close to 0 (on the grounds of numerical instability and/or further regularization). For this reason, SH does not attempt to find a completely minimal grammar before running EM. More specifically, SH may introduce slightly more tasks and methods than strictly necessary. Then the EM phase has some freedom to play a (limited) role in the choice of the structure of the final grammar. As-is, this increase in the number of tasks is quite small, and there is little danger of ruining goodness-of-fit, sample complexity, and other such curse-of-dimensionality issues. The important issues and trade-offs to be considered when examining the relationship between parameter and structure learning techniques are beyond the scope of this article see Koller and Friedman [2009].

But to illustrate the issues, no simple, loop ( Z  X  ZS ) fits as closely as the disjunction over all finite prefixes of the recursion actually witnessed in the training data. Provided with such rules and the right starting parameters, the EM phase will delete the loop in preference to its unrollings, because it can achieve perfect fit that way. Of course, the resulting model will not fare so well on the test data. Conversely, roughly this very same behavior is indeed correct in the slightly elaborated situation that the majority of training data consists of examples of long repeating sequences with just a minority of short examples. The hypothesis that such (bimodal) data was produced by a loop with a stochastic exit event is weak. (It is much more plausible that a single event determines short/long, with any subsequent variation in length explained perhaps as loops with high exit probability.) An ideal parameter learning approach would, given the opportunity, have no difficulty in making the correct, structural, decision: delete the explanation as a single simple loop. But treating all structure as parameters is hardly feasible, which is of course the motivation for considering structure learning as a separate problem in the first place. In short the interplay is a complex topic.
Contextual Dependencies. In general, one might very well want to model/learn pref-erences, such as  X  X f in Europe, prefer traveling by trains to planes. X  However, the setup as given insists on purely context-free statements. There is a general mapping between the context-sensitive and context-free settings best illustrated by example. One considers each term like  X (buy ?customer ?vendor ?location ?product ?price ?unit) X  and every specific instantiation such as  X (buy mike joe walmart bat 3 dollars) X , and instead represents these along the lines of  X  X uy-mike-joe-walmart-bat-3-dollars X . (For the sake of discussion, push all implicit dependencies, as on global variables, explicitly into parameters.) Then methods which appear to be restricted to context-free settings can be seen to be implicitly performing context-sensitive inference X  X ut the mapping is an exponential translation. So, taken literally, such mappings are far from practical. Nonetheless, one can get a surprising amount of mileage from the perspective so long as one does not literally write down the full ground representation ahead of time. The trick is to only write down small pieces of the ground representation, that is, on an as-needed basis, in some clever fashion.

Concretely, applying our techniques to learn context-sensitive preferences entails a feature-selection step to write down primitives along the lines of BuyTicketInEurope rather than just BuyTicket . Note that such a feature-selection step already exists X  X e have already chosen to write BuyTicket rather than merely Buy . Future work could build on this work by automating the feature-selection step in order to better address contextual dependencies [Guyon and Elisseeff 2003; Liu and Yu 2005]. To evaluate our pHTN learning approach, we designed and carried out experiments in both synthetic and benchmark domains. All the experiments were run on a 3.06GHz Mac machine with 4GB of RAM. Although we focus on accuracy (rather than CPU time), we should clarify up-front that the runtime for learning is quite reasonable X  X etween less than a millisecond to 4ms per training plan. We take an oracle-based experimental strategy, that is, we generate an oracle pHTN H  X  to represent a possible user and then subsequently use it to generate a set of preferred plans . Our learner then induces a pHTN H from only , allowing us to assess the effectiveness of the learning in terms of the differences between the original and learned models. In some settings (e.g., knowledge discovery), it is very interesting to directly compare the syntax of learned models against ground truth, but for our purposes, such comparisons are much less interesting: we can be certain that, syntactically, H will look nothing like a real user X  X  preferences (as expressed in pHTN form) for the trivial reason (among others) that H will be in Chomsky normal form. For our purposes, since user preferences are expressed as the distribution of observed plans, the correctness of H should be measured as to whether it is able to generate an approximately correct distribution on plans. So the ideal evaluation is some measure of the distance between distributions (on plans), for example, Kullback-Leibler (KL) divergence: where P H and P H  X  are the distributions of plans generated by H and H  X  , respectively. This measure is lower-bounded by 0, achieved by equal distributions, and otherwise di-verges to positive infinity. (There are both positive and negative terms, but the positive terms always dominate.) It is not symmetric.

However, as given the summation is over the infinite set of all plans, we instead ap-proximate by sampling, but this exacerbates a deeper problem: the measure is trivially infinite if P H gives 0 probability to any plan (that P H  X  does not). So in the following, for every oracle pHTN H  X  with n tasks, we take measurements by sampling 100 n plans from each of H  X  and H , obtaining sample distributions  X  P H  X  and  X  P H , then we prune any compute This is not a good approach if the intersection is small, but in our experiments, |  X  P remains nonsymmetric, nonnegative, and divergent in a sense. But effectively, the mea-sure is upper-bounded by O (log n ), because probabilities cannot be smaller than 1 n in distributions defined by n samples. In these experiments, we randomly generate the oracle pHTN H  X  by randomly gener-ating a set of recursive and nonrecursive methods on n tasks. In nonrecursive domains, the randomly generated methods form a binary and-or tree with the goal as the root. The probabilities are also assigned randomly. Generating recursive domains is similar, with the only difference being that 10% of the methods generated are recursive. For measuring overall performance, we provide 10 n training plans and take 100 n samples for testing; for any given n , we repeat the experiment 100 times and plot the mean. We compare the performance of the proposed learner with the inside-outside algorithm. The results are shown in Figures 4(a) and 4(b). We also discuss two additional, more specialized, evaluations.

Learning Curves. In order to test the learning speed, we first measured KL diver-gence values with 15 non-primitives given different numbers of training plans. The results are shown in Figure 4(a). We can see that even with a relatively small number of training examples, our learning mechanism can still construct pHTNs with diver-gence no more than 0.2. As expected, the performance further improves given many training examples. As briefly discussed in the setup, our measure is not interesting unless the learned pHTN can reproduce most testing plans with nonzero probability, since any 0 probability plans are ignored in the measurement, so we do not report results given only a very small number of training examples (the value would be ar-tificially close to 0). Here  X  X ery small X  means too small to give at least one example of every/most reductions in the oracle pHTN; without at least one example, the structure hypothesizer will (rightly) prevent the generation of plans with such structure.
Comparison with Inside-Outside. To better understand the effectiveness of the pro-posed learner, we also compared our schema learner with the famous inside-outside algorithm [Lari and Young 1990], which is a generalization of the forward-backward algorithm for parameter estimation on hidden Markov models to pCFGs. 10 We choose this algorithm because it serves as a standard way to estimate parameters for pCFGs. In comparison with the proposed algorithm, the inside-outside algorithm needs to be given a fixed number of non-primitives needed for the grammar, whereas our algorithm automatically adds new non-primitives as needed. The inside-outside algorithm was also tested with domains of size 15 given an increasing number of training examples. Since the inside-outside algorithm requires a prespecified number of non-primitives symbols given in addition to the training traces as input, we give it the actual number of non-primitives used by the oracle. Our modified measure of distribution divergence evaluating the proposed learner. The same manipulation removes about 15% of plans in the case of the inside-outside algorithm, significantly boosting its apparent per-formance. Despite the advantage, our approach still outperforms the inside-outside algorithm: 0.046 versus 0.268 in the nonrecursive domains, and 0.120 versus 0.240 in the recursive domains, given 150 training plans, on the performance metric originally defined.

In an attempt to quantify the magnitude of the advantage given to the inside-outside algorithm under the original performance metric, we also evaluated a different measure of divergence between distributions, normalized KL divergence , as follows. First we compute  X  P 1 compute the KL divergence between  X  P H and  X  P 1 evaluation framework: the learner gets credit for any output whatsoever. In particular, the worst possible learner, producing only plans loathed by a user, for this measure expression), the measure is bounded by 0 and 1. The measure is still nonsymmetric.
As shown in Figure 4(b), the proposed learner consistently outperforms the inside-outside algorithm with different numbers of the training plans. Sign tests show that the differences are significant ( p &lt; 0 . 0001) across various numbers of training plans. A reasonable explanation begins by noting that the inside-outside algorithm begins with nonzero probability of every possible method, and inspection reveals that in our exper-iments many methods are retained (probability not close to 0) even after learning. The design choice of soft-EM does not help in this regard. In contrast, our learner begins with a structure learning step to effectively limit the scope of the subsequent parameter learning. (Furthermore it uses hard-EM, thereby encouraging grammar unambiguity.) Having a plethora of parameters brings at least two dangers into play: overfitting and sample complexity . As the test distributions were in fact identical to the training distri-butions (up to sampling error), the greatest obstacle facing the inside-outside algorithm is likely the lack of sufficiently-many samples to set its many parameters reliably. Al-beit, already discussed, about 15% of plans failed to appear in the intersection during testing, so overfitting may also be playing a role. This catastrophic ambiguity of the pHTNs considered by the inside-outside algorithm has one final negative feature that we will note (once more exacerbated by soft-EM): its per-plan training times range from 100 X 303 milliseconds. (Our learner uses less than 4 milliseconds per plan.) Although giving the inside-outside algorithm a smaller number of non-primitives may help in reducing the overfitting issue, it is not clear (1) whether there exists a grammar with a smaller number of non-primitives that is able to capture the oracle user preference, and (2) even if there exists one, how the algorithm could find the number. In this case, our algorithm does not require a search process of such a number.

Conciseness. The conciseness of the learned model is also an important factor measur-ing the quality of the approach (despite being a syntactic rather than semantic notion), since allowing huge pHTNs will overfit (with enough available symbols, the learner could, in theory, just memorize the training data). A simple measure of conciseness, the one we employ, is the ratio of non-primitives in the learned model to non-primitives in the oracle ( n ) X  X he learner is not told how many symbols were used to generate the training data. Figure 4(c) plots the results. For small domains (around n = 10), the learner uses between 10% and 20% more non-primitives, a fairly positive result. How-ever, for larger domains, this result degrades to 60% more non-primitives, a somewhat negative result. Albeit the divergence measure improves on the test set, so while there is some evidence of possible overfitting, the result is not alarming. Future work in structure learning should nonetheless examine this issue (conciseness and overfitting) in greater depth.

Effectiveness of the EM Phase. To examine the effect of the EM phase, we carried out experiments comparing the divergence (to the oracle) before and after running the EM phase. Figures 5(a) and 5(b) plot results in the nonrecursive and recursive cases, respectively. Overall, the EM phase is quite effective, for example, with 50 non-primitives in the nonrecursive setting, the EM phase is able to improve the divergence from 0.818 (the divergence of the model produced by SH) to the much smaller divergence of 0.066. The result is statistically significant under sign-testing, p &lt; 0 . 001, except for domains of size 5 (where the difference in performance is not statistically significant). Our best explanation based on careful inspection of the experimental results is that random domains on 5 non-primitives are just too simple to sufficiently penalize the default assigment SH makes.

Note. Divergence in the recursive case is consistently larger than in the nonrecursive case across all experiments: this is expected. In the recursive case, the plan space is actually infinite; in the nonrecursive case, there are only finitely-many plans that can be generated. So, for example, in the nonrecursive case, it is actually possible for a finite sample set to perfectly represent the true distribution: simply memorizing the training data will produce 0 divergence eventually. In infinite plan spaces, no finite set of samples can perfectly represent the true distribution. In addition to the experiments with synthetic domains, we also evaluated on two domains inspired by benchmarks of the International Planning Competition [Helmert et al. 2008]. In each, we handcraft pHTNs, encoding our own preferences and from there continue to employ the same oracle-based evaluation approach. The two domains are Logistics and Gold Miner; but in both, we simplify by taking our primitives as just the operator names (rather than a full ground representation). For Logistics, the preference is for planes over trucks, and fewer vehicles over more vehicles. In Gold Miner, the preference encompasses the entire solution strategy (rather than the more localized preferences of logistics). Both domains feature simple looping behavior, which in the preceding synthetic experiments had a notable negative impact on performance, despite the fact that SH is specifically built to recognize simple looping behavior. Logistics Planning. The domain we used in the first experiment is a variant of the Logistics planning domain, inside which both planes and trucks are available to move packages, and every location is reachable from every other. There are four primitives in the domain: load , fly , drive ,and unload . We use 11 tasks to express, in the form of an oracle pHTN H  X  (in Chomsky Normal Form, hence 11 tasks), our preferences concerning logistics planning. We presented 100 training plans of lengths ranging from 3 to 15 to the learning system; by inspection, we verified that these demonstrate our preference for moving packages by planes rather than trucks and for using overall fewer vehicles. The average divergence of the learned models is 0.078 against a testset of size 1,000 on 100 runs, which is significantly ( p &lt; 0 . 0001) lower than the average divergence (0.267) of the models learned by the inside-outside algorithm.

It is interesting to note that the learned structure is smaller than the oracle struc-ture. As the oracle structure associated meaningful names with nonterminals, it is indeed plausible that there is a degree of approximate redundancy to the manner of its encoding X  X t could very well be that legitimate encoding of our preferences can be had more compactly. So, while we are generally unconcerned with the syntax of the learned model, it is interesting to consider in this case: Table II shows the learned model. With some effort, one can verify that the learned schemas do capture our preferences: the second and third methods for  X  X ovePackage X  encode delivering a package by truck and by plane, respectively (and delivering by plane has significantly higher probability), and the first method permits repeatedly moving packages between vehicles, but with relatively low probability. That is, it is possible to recursively expand  X  X ovePackage X  so that one package ends up transferring between vehicles, but the plan that uses only one instance of the first method per package is significantly more probable (by 0 . 17  X  k , according to the learner, for k transfers between vehicles).

Gold Miner. The second domain we used is inspired by Gold Miner, introduced in the learning track of the 2008 International Planning Competition. The setup is a (futuristic) robot tasked with retrieving gold (blocked by rocks) within a mine; the robot can employ bombs and/or a laser cannon. The laser cannon can destroy both hard and soft rocks, while bombs only destroy soft rocks. However, the laser cannon will also destroy any gold immediately behind its target. The desired strategy, which we encode in pHTN form using 12 tasks ( H  X  ), for this domain is roughly (1) get the laser cannon, (2) shoot the rock until reaching the cell next to the gold, (3) get a bomb, (4) use the bomb to get gold. As in Logistics, though, the training data consists merely of the names of the operators (so there is no way to express  X  X ard rock X ,  X  X oft rock X , and  X  X ext to the gold X ).

We gave the system 100 training plans of various lengths ranging from 6 to 49 (generated by H  X  ). The learner achieved an average divergence of 0.353 on 100 runs. We could not obtain the divergence of the model learned by the inside-outside algorithm, since it was taking too long. But based on previous results, we believe that our proposed learner should perform significantly better. This is a much larger divergence than in the case of Logistics, which can perhaps be explained by the significantly longer applications of looping behavior (using the laser cannon repeatedly). As noted in the random experiments, performance is negatively impacted by the use of recursion/loops.
Nonetheless, the learner did succeed in qualitatively capturing our preferences, which can be seen by inspection of the learned model in Table III. Specifically, the learned model only permits plans in the order previously given: get the laser cannon, shoot, get and then use the bomb, and finally get the gold. Observe that the learner made such a leap of faith on just 100 training examples, failing to demonstrate any other possible order: the use of a separate structure learning step is presumably the reason for this success. One does not imagine a robust implementation of EM, or any other form of parameter-learning, capable of driving so many parameters to 0 on just 100 training examples. At least it tends to be the case, for parameter learners, that once logical certainty is reached by a parameter, it becomes stuck there forevermore. For robustness then, one takes steps to prevent assigning 0 or 1. In general, users will not be so all-powerful that behavior and desire coincide. Instead, a user must settle for one (presumably the most desirable) of the feasible possibilities. Supposing those possibilities remain constant, then there is little point in distinguish-ing desire and behavior; indeed, the philosophy of behaviorism defines preference by considering such controlled experiments. Supposing instead that feasible possibilities vary over time, then the distinction becomes very important. Consider for example the fact that the observed travel behaviors of a poor grad student might all consist of her driving. Only in the rare situation that such a student X  X  travel is funded do we get to observe her true preference for planes over cars (or for that matter, cars over walking). In addition to that example, consider the requirement to go to work on weekdays (so the constraint does not hold on weekends). Clearly, the weekend activities are the pre-ferred activities. However, the learning approach developed so far would be biased X  X y a factor of 5 2  X  X n favor of the weekday activities. In the following, we consider how to account for this effect: the effect of feasibility constraints upon learning preferences.
Recall that we assume that we can directly observe a user X  X  behavior, for example, by building upon the work in plan recognition . In this, section we additionally assume that we have access to the set of feasible alternatives to the observed behavior X  X or example, by assuming access to the planning problem the user faced and building upon the work in automated planning [Nau et al. 2004]. In cases where we only have access to the planning problem description (i.e., the initial state and the goal), we could use planners capable of generating diverse plans [Srivastava et al. 2007; Nguyen et al. 2009]. Note that there might be an enormous number of feasible alternatives, but only a subset of them may have been chosen by the user at least once. Never-chosen plans are considered as undesired plans, and thus do not need to be modeled by the acquired pHTNs. Therefore, when considering feasible alternatives, we can restrict our attention to a subset on the order of the number of observed plans. So, in this section, the input to the learning problem becomes the following.

Input. The i th observation, (  X  i , F i )  X  , consists of a set of feasible possibilities, F i , along with the chosen solution:  X  i  X  F i .

In the rest of the section, we consider how to exploit this additional training infor-mation (and how to appropriately define the new learning task). The main idea is to rescale the input (i.e., attach weights to the observed plans  X  i ) so that rare situations are not penalized with respect to common situations. One way of viewing the rescal-ing mechanism is that it makes appropriate numbers (i.e., weights) of plan duplicates based on different feasibility situations. Hence, in the preceding example, even if we have only observed that the poor graduate student travels by plane once, since this is a rare but informative situation, we could attach a large weight to the travel-by-plane plan, like we pretend that we have seen this plan carried out much more often in the  X  X deal X  world situation.

We approach the learning problem from the perspective that our evidence for pref-erence consists just of  X  i over the remainder of F i . The question then becomes how to merge such evidence across differing feasibility situations: F i = F j . The approach is to consider plans in the intersection of both situations, using these to mediate an indirect comparison. That is, we attempt to take our original evidence and transitively close it. This may still leave us with disconnected components of situations. Here we essentially give up and permit the system to answer  X  X nknown X  concerning pairs of plans from dis-tinct components. This additional capability somewhat complicates evaluation (as the base system can only answer  X  X es X  or  X  X o X  to such queries). To answer queries about comparable plans, we apply the base learner to each rescaled component, arriving at a set of pHTNs capturing user preferences. Previously we assumed the training data (observed plans) was sampled (i.i.d.) di-rectly from the user X  X  true preference distribution (say U ): But now we assume that varying feasibility constraints intervene. For the sake of notation, imagine that such variation is in the form of a distribution, say F , over planning problems (but all that is actually required is that the variation is independent of preferences, as assumed next). Note that a planning problem is logically equivalent to its solution set. Then we can write P ( F | F ) to denote the prior probability of any particular set of solutions F . Since the user chooses among such solutions, we have that chosen plans are sampled from the posterior of the preference distribution: Again, since what is possible should not depend upon desire, and desire should not depend upon what is possible, we assume that preferences and feasibility constraints are mutually independent. One can certainly imagine either dependence X  X espectively, Murphy X  X  law (or its complement) and the fox in Aesop X  X  fable of Sour Grapes (or envy) X  but it seems to us more reasonable to assume independence. Then we can rewrite the posterior of the preference distribution: Assuming independence is important, because it makes the preference learning prob-lem attackable. In particular, the posteriors preserve relative preferences X  X or all  X , X   X  F ,the odds of selecting  X  over  X  are Therefore we can, given sufficiently-many of the posteriors, reconstruct the prior by transitive closure; consider  X , X  , X  with  X , X   X  F and  X  , X   X  F : So then the prior can be had by normalization: Of course, none of these distributions are accessible; the learning problem is only given .Let M F [  X  ] =|{ i | (  X , F ) = (  X  i , F i )  X  }| , M F =  X  M F [  X  ], and M = F M F = | | . Then defines a sampling distribution, for any F : in particular, However, for anything less than an enormous amount of data, one expects  X  O F and  X  O F to differ considerably for F = F , therein lying one of the subtle aspects of the following rescaling algorithm. The intuition is, however, simple enough: pick some base plan  X  and set its weight to an appropriately large value w , and then set every other weight, differing estimates  X  O F which we will describe in more detail later). Finally, give the weighted set of observed plans to the base learner. From the preceding analysis, in the limit of infinite data, this setup will learn the (closest approximation, within the base learner X  X  hypothesis space, to the) prior distribution on preferences.

To address the issue that different situations will give different estimates (due to sampling error) for the relative preference of one plan to another (  X  O F and  X  O F will differ), we employ a merging process on such overlapping situations. Consider two weighted sets of plans, c and d , and interpret the weight of a plan as the number of plan in the intersection, {  X  }= c  X  d , then there is only one way to take a transitive closure X  X or all  X  in c and  X   X  d \ c : so in particular, we can merge d into c by first rescaling d : to  X  . Then, in the case that there are multiple plans in the intersection, we are faced with multiple ways of performing a transitive closure, that is, a set of scale factors. These will normally be different from one another, but in the limit of data, assuming preferences and feasibility constraints are actually independent of one another, every scale factor between two clusters will be equal. So then we take the average: In short, if all the assumptions are met and enough data is given, the described process will reproduce the correct prior distribution on preferences. Figure 6 provides the remaining details in pseudocode, and in the following, we discuss these details and the result of the rescaling/merging process operating in the Travel domain. Output. The result of rescaling is a set of clusters of weighted plans, C ={ C 1 , C ,..., C we write p  X  C for membership and w C ( p ) for the associated weight.

Clustering. First, we consider input records associated with the same set of feasible plans are produced under the same or similar situations. We collapse all of the input records from the same or similar situations into single weighted clusters, with one count going towards each instance of an observed plan participating in the collapse. For example, suppose we observe three instances of Gobyplane chosen in preference to Gobytrain and one instance of the reverse in similar or identical situations. Then we will end up with a cluster with weights 3 and 1 for Gobyplane ,and Gobytrain respectively. In other words, w C ( p ) is the number of times p was chosen by the user in the set of situations collapsing to C (or if p was never chosen). This happens in lines 2 X 20 in Figure 6, which also defines  X  X imilar X  (as set inclusion). Future work should consider more sophisticated clustering methods.

Transitive Closure. Next we make indirect inferences between clusters; this happens by iteratively merging clusters with non-empty intersections. Consider two clusters, C and D , in the Travel domain. As shown in Table IV, D contains Gobyplane and Gobytrain with counts 3 and 1, respectively, and c contains Gobytrain and Gobybike with counts 5 and 1, respectively. From this, we infer that Gobyplane would be executed 15 times more frequently than Gobybike in a situation where all three plans are possible, since it is executed three times more frequently than Gobytrain , which is in turn executed five times more frequently than Gobybike . We represent this inference by scaling one of the clusters so that the shared plan has the same weight, and then take the union. In the example, supposing we merge D into C , then we scale D so that C  X  D ={ Gobytrain } has the same weight in both C and D , that is, we scale D by 5 = w C (Gobytrain) w pairs of clusters with more than one shared plan, we scale D \ C by the average of (  X  ) for each plan in the intersection, but we leave the weights of C could consider several alternative strategies for plans in the intersection). Computing the scaling factor happens in lines 21 X 26 and the entire merging process happens in lines 21 X 31 shown in Figure 6. We learn a set of pHTNs for C by applying the base learner (with the obvious general-ization to weighted input) to each C  X  C : While the input clusters will be disjoint, the base learner may very well generalize its input such that various pairs of plans become comparable in multiple pHTNs within H . Any disagreement is resolved by voting; recall that, given a pHTN H and a plan p , we can efficiently compute the most probable parse of p by H .Let H ( p ) denote  X  X he (a priori) likelihood of p  X . We actually set it to the probability of the most probable parse of p . Given two plans p and q ,welet  X  H order p and q by H (  X  ), that is, p  X  H q  X  X  X  H ( p ) &lt; H ( q ); if either is not parseable (or tied), then they are incomparable by H . Given a set of pHTNs H ={ H 1 , H 2 ,... } , we take a simple majority vote to decide p  X  So, each pHTN votes, based on likelihood, for p  X  q (meaning p is preferred to q ), or q  X  p ( q is preferred to p ), or abstains (the preference is unknown). Summarizing, the input is (1) clustered, (2) transitively closed producing a smaller set of clusters, and (3) each is given to the base learner resulting in a set of pHTNs H . Finally, the learned pHTNs H model the user X  X  preferences via the relation  X  H . In this part, we are primarily interested in evaluating the rescaling extension of the learning technique, that is, the ability to learn preferences despite feasibility con-straints. We design a simple experiment to demonstrate that learning purely from observations is easily confounded by constraints placed in the way of user preferences and that our rescaling technique is able to recover preference knowledge despite ob-fuscation. 4.4.1. Setup.

Performance. We again take an oracle-based experimental strategy. That is, we imag-ine a user with a particular ideal pHTN, H  X  , representing that user X  X  preferences, and then test the efficacy of the learner at recovering knowledge of preferences based on observations of the imaginary user. More specifically, we test the learner X  X  performance in the following game. After training, the learner produces H r ; to evaluate the effec-tiveness of H r , we pick random plan pairs and ask both H  X  and H r , to pick the preferred plan. There are three possibilities: H r agrees with H  X  ( + 1 point), H r disagrees with H  X  (  X  1 point), and H r declines to choose (0 points). 12
The distribution on testing plans is not uniform and will be described next. The num-ber of plan pairs used for testing is scaled by the size of H  X  ; 100 t pairs are generated, where t is the number of non-primitives. The final performance for one instance of the game is the average number of points earned per testing pair. Pure guessing, then, would get (in the long-term) 0 performance.

Domains/Users. As in the prior evaluation, we evaluate on (1) randomly generated pHTNs modeling possible users, and on (2) handcrafted pHTNs modeling our prefer-ences in Logistics and Gold Miner. Both are extended with the same randomized model of feasibility constraints.

Training Data. We generate random problems by generating random solution sets in a particular fashion. That is, we model feasibility constraints using a particular ran-dom distribution on solution sets. To evaluate whether the proposed approach is able to recover the user X  X  true preference when the observed plan distribution is obfuscated by feasibility constraints, the random solution sets model the worst case of feasibility con-straints, in the sense that it is the least preferred plans that are most often feasible X  much of the time the hypothetical user will be forced to pick the least evil rather than the greatest good. We describe this process in detail next, but note that the learning algorithm is general and not restricted to the type of feasibility obfuscation tested here.
We begin by constructing a list of plans, P , from 100 t samples of H  X  , removing duplicates by maintaining only the first appearance of the same plans (so | P |  X  100 t ). Since more preferred/probable plans are more likely to be generated first, in general, the order will be roughly from most to least preferred. We reverse that order, still denoted P , and associate it with (a discrete approximation to) a power-law distribution. The result is that least preferred plans are, under P , most likely. Both training and test plans are drawn from this distribution. Then, for each training record (  X  i , F i ), we take a random number 13 of samples from P as F i . We sample the nominally observed plan,  X  , from F i by , that is, the probability of a particular choice  X  i = p is ( p )
Baseline. The baseline for our experiments will be the original approach: the base learner without rescaling. That is, we take a single cluster, where the weight of each plan is the number of times it is observed w (  X  ) = | { i |  X  =  X  i } | , and apply the base learner, obtaining a single pHTN, H b ={ H } , and score it in the same manner that the extended approach is scored by.
Learning Curves. Figure 7(a) presents the results of a learning-rate experiment against randomly selected H  X  . For these experiments, the number of non-primitives is fixed at 5, while the amount of training data is varied; we plot the average performance, over 100 samples of H  X  , at each training set size.

We can see that with a large number of training records, rescaling before learning is able to capture nearly full user preferences, whereas learning alone performs slightly worse than random chance. This is expected, since without rescaling, the learning is attempting to reproduce its input distribution, which was the distribution on observed plans, and feasibility is inversely related to preference by construction. That is, given the question  X  X s A preferred to B ? X , the learning alone approach instead answers the question  X  X s A executed more often than B ? X .

Size Dependence. We also tested the performance of the two approaches under vary-ing number of non-primitives (using 50 t training records); the results are shown in Figure 7(b). For technical reasons discussed at the end of Section 4.4.3, the base learner is much more effective at recovering user preferences despite feasibility ob-fuscation when these take the form of recursive schemas, so there is less room for improvement. Nonetheless, the rescaling approach improves upon learning alone in both experiments.
 Significance. As shown in Figure 7(b), the learners with rescaling ( EA-NR , EA-R ) outperform the base learners ( OA-NR , OA-R ). To test whether the difference is significant or not, we carried out a sign test. For each domain size (i.e., 5, 10, 20), we compare the scores of the extended learner with the scores of the base learner over 100 schemas. The result shows that in both recursive and nonrecursive domains, the score of the learner with rescaling is significantly higher ( p &lt; 0.02) for all domain sizes. 4.4.3. Results: Hand-crafted H  X  . We reuse the same pHTNs encoding our preferences in Logistics and Gold Miner from the first set of evaluations. As mentioned, we use the same setup as in the random experiments, so it continues to be the case that the distribution on random solutions is biased specifically against the encoded preferences. Moreover, due to the level of abstraction used (truncating to action names), as well as the nature of the pHTNs and domains in question, the randomly generated sets of alternatives, F i , are in fact sets of solutions to some problem expressed in the normal fashion (i.e., as an initial state and goal).

Logistics. After training with 550 training records (50 t , for 11 non-primitives) the baseline system scored only 0.342 (0 is the performance of random guessing), whereas rescaling before learning performed much better with a score of 0.847 (0.153 away from perfect performance).

Gold Miner. After training with 600 examples (50 t for 12 non-primitives), learning alone scored a respectable 0.605. However, rescaling before learning still performed better with a score of 0.706. The greater recursion in Gold Miner, as compared to Logistics, is both hurting and helping. On the one hand, the full approach scores worse (0.706 vs. 0.847), while on the other hand, the baseline X  X  performance is hugely improved (0.605 vs. 0.342). As discussed previously, the presence of recursion in the preference model makes the learning task much harder (since the space of acceptable plans is then actually infinite), which continues to be a reasonable explanation of the first effect (degrading performance).

The latter effect is more subtle. The experimental setup, roughly speaking, inverts the probability of selecting a plan, so that using a recursive method many times in an observed plan is more likely than using the same method only a few times. Then the baseline approach is attempting to learn a distribution skewed towards more recursion than less recursion, all else being equal. However, there is no pHTN that prefers more recursion to less recursion, all else being equal: fewer uses of a recursive method always increases the probability of a plan. The closest the baseline can come is simply to fit an inappropriately large probability to any recursive method so it will assess the likelihood of plans incorrectly. But, queried about the relative ordering on two plans, differing only in their depth of recursion over some method, the baseline will nonetheless produce the correct answer (prefer less recursion), despite assigning the wrong likelihoods. No other result is possible given the definition of pHTNs: fewer uses of a method are (monotonically) more probable. Naive bayes classifiers exhibit an analogous effect [Koller and Friedman 2009, Box 17.A]. In the planning community, HTN planning has for a long time been given two distinct and sometimes conflicting interpretations (c.f., [Kambhampati et al. 1998]): it can be interpreted either in terms of domain abstraction 14 or in terms of expressing complex plan constraints on plans. 15 The original HTN planners were motivated by the former view (improving efficiency via abstraction). In this view, only top-down HTN planning makes sense, as the HTN is supposed to express effective search control. Paradoxically, with respect to that motivation, the complexity of HTN planning is substantially worse than planning with just primitive actions [Erol et al. 1996b]. The latter view explains the seeming paradox easily X  X inding a solution should be easier, in general, than finding one that also satisfies additional complex constraints. From this perspective, both top-down and bottom-up approaches to HTN planning are appropriate (the former if one is pessimistic concerning the satisfiability of the complex constraints, and the latter if one is optimistic). Indeed, this perspective led to the development of bottom-up approaches to HTN planning [Barrett and Weld 1994].

Despite this dichotomy, most prior work on learning HTN models (e.g., [Ilghami et al. 2002; Langley and Choi 2006; Yang et al. 2007; Hogg et al. 2008]) has focused only on the domain abstraction angle. Typical approaches here either require the structure of the reduction schemas to be given as input or need additional information such as annotated plan traces and tasks to assist the learning process, whereas our work only requires plan traces as input. Moreover, these efforts focus on learning domain physics or search control. As we mentioned, a significant amount of work was also directed at learning domain physics as action schemas [Yang et al. 2005]. In contrast, our work focuses on learning HTNs as a way to capture user preferences, given only successful plan traces. The difference in focus also explains the difference in evalua-tion techniques. While most previous HTN learning efforts are evaluated in terms of whether the learned schemas and applicability conditions are able to assist the planner to find feasible and goal-achieving plans, we evaluate them in terms of how close the distribution of plans generated by the learned model is to the distribution generated by the actual model.

An intriguing question is whether pHTNs learned to capture user preferences can, in the long run, be overloaded with domain semantics. In particular, it would be in-teresting to combine the two HTN learning strands by sending our learned pHTNs as input to the method applicability condition learners. Presuming user preferences are amenable, the applicability conditions thus learned might then allow efficient top-down interpretation (of course, the user preferences could, in light of the complexity results for HTN planning, be so antithetical to the nature of the domain that efficient top-down interpretation is impossible). An interesting theoretical result is that context-free lan-guages are not closed under intersection; one might be unable to effectively merge two HTNs modeling different sets of complex constraints.

The connection between HTN schemas and grammars has been identified by several authors [Kambhampati et al. 1998; Geib and Steedman 2007]. Recently, Geib [2009] proposed an algorithm, ELEXIR, which represents plans to be recognized with Com-binatory Categorial Grammars (CCG), and shows that this mechanism prevents early commitment to plan goals. Our work exploits the same connections to learn the pHTNs as grammars.
 Our framework also incorporates ideas from other research on grammar induction. For example, the E-step in the algorithm for building the most probable parse trees bears a clear resemblance to the parsing algorithms in Collins [1997] and Charniak [2000]. Collin X  X  [1997] parser represents parse trees using probabilities of dependencies, while our approach uses reduction schemas to represent parse trees. Charniak X  X  [2000] work defines a maximum-entropy-inspired score function based on features chosen from certain feature schemata to measure the quality of the parse. The parser then returns the parse tree with the highest score. In contrast, our approach scores parse trees based on a probabilistic model of the derivation process (i.e., Equation (5)).
Other research on pCFG acquisition is also quite relevant. Most work in this area divides the learning process into two steps, as we do. The learning algorithms first acquire the grammar rules using CFG induction algorithms. Due to the high com-plexity of CFG learning, typical approaches in the direction either require additional structural information besides training examples to be given (e.g., [Sakakibara 1992]), or focus on restricted classes of CFGs (e.g., [Takada 1988; Ishizaka 1989]). In the sec-ond step, the learning algorithm uses the grammar rules acquired from the previous step and estimates the probabilities that fit best (e.g., [Lari and Young 1990; Ra and Stockman 1999; Sakakibara et al. 1994]). Genetic algorithms are also used to acquire pCFGs directly [Kammeyer and Belew 1996; Keller and Lutz 2005]. To the best of our knowledge, we are the first to apply pCFG learning to the area of user preference acquisition. An interesting future study would be to compare the performance of the other learning algorithms on the problem of acquiring user preferences, despite the fact that these algorithms were not designed for this application. However, considering the performance of the inside-outside algorithm in our empirical evaluations and the performance of our base learner once we took feasibility constraints into account, it does not seem likely that these algorithms would perform well when the surrounding context is dramatically altered.

Besides pHTNs, there are other representations for expressing user preferences, such as trajectory constraints expressed in linear temporal logic (e.g., [Baier and McIlraith 2008; Gerevini et al. 2009]). Sohrabi et al. [2009], in particular, extend the Planning Domain Definition Language PDDL3 [Gerevini et al. 2009] with HTN-like preference constructs. These works consider modeling user preferences, not automatically learn-ing them. It will be interesting to explore methods for learning preferences in those representations, too, and to see to what extent typical user preferences are actually expressible in (p)HTNs and alternatives.

Due to the high cost of a user study, we have not evaluated the proposed algorithm with human users, but we believe that the proposed pHTN learner is applicable in many domains, such as travel route recommendation, patient behavior understanding, and so on. In order to apply the algorithm to real-world applications, several inter-esting extensions can be made. First, learning in plan recognition (e.g., [Kautz and Allen 1986; Lesh and Etzioni 1995]) has much in common with our proposed approach, since both learning tasks are interested in creating models that understand users from plans. Instead of modeling user preferences as we did in this work, research on plan recognition focuses on recognizing user behavior/intent based on observed plans. Prob-abilistic models that were used in these efforts include Bayesian networks [Charniak and Goldman 1993; Albrecht et al. 1998; Yin et al. 2004], N-gram models [Blaylock and Allen 2003], and so on. Most similarly, Pynadath and Wellman [2000] use probabilistic state-dependent grammars to recognize high-level user behavior. An interesting future direction would be to extend the proposed approach to plan recognition. In this case, a user X  X  intended goal would be the non-primitives associated with the observed plan.
Another issue is data sparsity. In order to provide personalized experience, past user behavior is required. However, collecting such data is hard. Hence, the collected data is often sparse. In this case, applying collaborative filtering techniques [Terveen and Hill 2001] to the proposed algorithm could be helpful. One possible future work related to this would be to extend the proposed algorithm to incorporate background knowledge in addition to observations. Currently, the proposed algorithm acquires pHTNs based on observed plans. If some options were not available before, they may be viewed as not preferred, while they may indeed be better choices. For example, getting to a nearby place by horse may be cheaper and thus more preferred to some users, but since such an option is not always available, the planner may not have observed such behavior. Hence, the proposed algorithm may not present the horse option to the user. If we can extend the proposed algorithm to incorporate the background knowledge that one method for transportation costs less than another is preferable, the planner should be able to show this option to the users. Then, the extended algorithm would not be constrained to generating a model based only on the observations it has received. Learning in the context of planning has received significant interest. However, most prior work focused only on learning domain physics or search control. In this article, we expanded this scope by learning user preferences concerning plans. We developed a framework for learning probabilistic HTNs from a set of example plans, drawing from the literature on probabilistic grammar induction. Assuming the input distribution is in fact sampled from a pHTN, we demonstrated that the approach finds a pHTN generating a similar distribution. It is, however, a stretch to imagine that we can sample directly from preference distributions X  X bserved behavior arises from a com-plex interaction between preferences and domain physics. We demonstrate a technique overcoming the effect of such feasibility constraints by reasoning about the available alternatives to the observed user behavior. The technique is to rescale the distribution to fit the assumptions of the base pHTN learner developed in the first part. We evalu-ate our approach, and demonstrate both that the base learner is easily confounded by constraints placed upon the preference distribution, and that rescaling is effective at reversing this effect.

We also discussed several remaining important directions for future work to ad-dress. Of these, the most directly relevant technical pursuit is learning parameter-ized pHTNs, or more generally, learning conditional preferences. Fully integrating an automated planner with the learner would gain the important abilities to (a) auto-matically generate feasible alternatives prior to learning, and (b) exploit the learned knowledge during planning (so as to make better recommendations). Subsequently running user studies, that is, on humans, is a very important pursuit.

