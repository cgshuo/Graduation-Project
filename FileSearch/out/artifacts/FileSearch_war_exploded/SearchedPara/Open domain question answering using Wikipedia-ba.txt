 1. Introduction
The goal of a question answering (QA) system is to directly return answers, rather than documents containing answers, in response to a natural language question. The answers can be fact-based short answers, lists of instances, or descriptions 2007; Voorhees, 2004 ), have focused on mining unstructured texts such as news sites and blogs. However, these systems show a relatively low performance, with at most 71% accuracy for factoid questions, 48% F-score for list questions, and 33% F-score for descriptive questions. On the other hand, specialized QA systems have relied on well-structured knowledge building large-scale, well-structured knowledge bases for a general domain QA is a very expensive task.
Wikipedia is a semi-structured and wide covering, rapidly growing knowledge source that has been built through a collaborative effort of volunteers. Wikipedia has become a stable and sufficiently large knowledge source for many Simmons, 2012 ). However, these systems utilized only parts of Wikipedia information. Thus, we developed an open-domain
QA system that fully utilizes semi-structured Wikipedia knowledge model. The knowledge model can serve as certified information sources and enable a QA system to generate correct answers in a general domain. We exploit the category  X  a QA system. We assume each knowledge source has its own strengths for answering different types of answer formats such ture is effective in answering lists of questions.

Well-organized knowledge bases do not guarantee high performance if the questions are in natural language instead of formal query. Mapping linguistic expression in questions to knowledge representation in knowledge-base is another hard task are about 50%, which are much lower than expected result ( http://greententacle.techfak.uni-bielefeld.de/~cunger/ qald/ ). The task covers extracting answers from well-organized knowledge-bases for given natural language questions. So, our system is based on a conventional QA system, which consists of a question analysis module, document retrieval module, and answer matching module. To this end, the different types of knowledge sources are converted into text documents for the document retrieval module. Instead, specialized answer matching modules are developed for the knowledge types.
Section 2 describes the question analysis, while Section 3 describes our Wikipedia QA system. Section 4 describes the experiment used, and concluding remarks are given in Section 5 . 2. Question analysis further divided into answer formats corresponding to the classes used in TREC ( Voorhees, 2004 ). While TREC used a ( Oh et al., 2009 ).

A user question in natural language form is analyzed using multiple linguistic analysis techniques including POS tagging, the Nile River located X  looks for a single factoid answer, whereas  X  X  X ho are American politicians who have emigrated from object or description sought by the question, such as PERSON, LOCATION, and DATE for a factoid; list answer format; and
DEFINITION, REASON, and METHOD for a descriptive answer format. We used a total of 147 answer themes, which are orga-located. X  The key elements in detecting the question target are the predicate-argument structure or noun phrase structure in the dependency structure of the given question. When the property is not clear, it can remain empty. means as follows; where S Q () is a score for question analysis, and S AF (), S malized between 0 and 1. 3. Wikipedia QA system
Ideally, questions should be answered through a direct comparison to a well-structured knowledge base. However, the
QA system architecture consisting of a question analyzing module, document retrieval module, and answer matching mod-S ( r | q ), document retrieval score S D ( d | r ) and answer matching score S between 0 and 1. An additional answer merging module combines and ranks the answers generated from the answer match-
Wikipedia definition and redirection databases are managed for accurate and wide coverage answers. 3.1. Article content module
We extract answers from article content using a traditional answering method. Most state-of-the-art QA systems imple-answer candidates for descriptive questions. Answer matching score is measured based on the distance between question words and answer candidate a in document d for factoid, list questions. For descriptive questions, the score is measured based on the content similarity and the pattern similarity measures. The content similarity is a similarity between words in the question and words in the answer candidate. The pattern similarity is the similarity between sentence patterns for descriptive answers and answer candidates. We defined lexico-syntactic patterns in regular expression format that embed Korean sentence styles for definition, reason, and method descriptions ( Table 2 ).
 3.2. Article structure module
Because the sections are divided based on important properties or issues that many users are interested in, the article X  X  section structure is a valuable knowledge source in a QA system. Sometimes, it is very hard to determine proper answer themes for certain questions. For example, the answer theme for the question,  X  X  X hat about damage in Sumner area by 2011 Christchurch earthquake, X  is not clear because our answer theme structure does not have  X  X  X amage X  type. The answer is assigned to each section, where d t =&lt; t A , t U1 , its upper level section titles t U1 , ... , t UN , and d c
Thus, the object and property names of the QT are used as a query for the document retrieval module. The answer matching the document as follows; corresponding section content document selected as an answer candidate. 3.3. Category structure module edged subsumption hierarchy, but only a thematically organized thesaurus. Paths through a non-isa link may connect to noisy category names. We applied the method of Ponzetto and Strube (2007) to filter non-isa relations from the upper  X  X  to  X  X  X rnold Schwarzenegger X  in Fig. 3 .

For each article, after isa upper categories are determined, a category structure document is generated. The document upper category path as three. Fig. 4 shows an example document for the category structure in Fig. 3 . After a category structure document is retrieved through the document retrieval module, the article name, located at the beginning of the document, is selected as an answer candidate. Answer matching score for category structure module is as follows;
For the above question, the category document of Fig. 4 is retrieved as a relevant document, and  X  X  X rnold Schwarzenegger X  is selected as an answer.
 3.4. Infobox module ument for  X  X  Barack Obama . X 
For a given question, the document retrieval system searches relevant infobox documents, and the answer matcher com-The answer matching score of answer candidate a by infobox module is as follows: 3.5. Definition module
We manage Wikipedia article titles and their first paragraph as a definition knowledge base. If the answer theme of a one or more articles are found, the system suggests contents of the articles as answer candidates. 3.6. Answer merging module
Our QA system makes use of multiple QA modules employing different answer finding methods. A strategy that deter-mines the sequence of module invocations to be invoked when finding an answer is selected based on several factors such as the expected AF, AT and QT of the question as in Table 4 . Given two modules, QA1 modules are invoked in parallel. We set the thresholds of QA modules based on repeated experimental results. 4. Experiment
We downloaded Korean Wikipedia from a Wikipedia dump site. as POS tagging, and named entity tagging. Evaluations were made by three human judges who understand the functionality of (MRR), precision, recall, and F-score. We grouped our QA modules as follows: Group 1 : a traditional QA module for article contents (AC, Baseline).
 and definition module (DEF).
 Group 3 : dual combined modules including a merged module of AC and IB (+IB), AC and CS (+CS), AC and AS (+AS), AC and DEF (+DEF).
 Group 4 : a merged module of all modules (+ALL).
 4.1. Overall results
The overall evaluation results are shown in Table 8 . The +ALL module shows the highest performance as expected. When a baseline module was chosen for the given query set, a total of 162 answers were correct. On the other hand, 316 correct answers were returned when all the modules were invoked and the answers merged. This indicates that the additional correct answers were extracted from semi-structured knowledge sources. All modules in group 2 show high precision but low recall. This means that we can easily extract correct answers from the semi-structured information, but coverage of and Wikipedia covers many of the concepts or entities referred to in these questions. The +AS module can find answers for as most Wikipedia categories represent abstract concepts and have member entities.
 All modules show high MRR scores of between 0.840 and 0.910 in groups 1, 3, and 4. Single modules in group 2 show 1.0 answer candidates even if they are in the fifth rank. Therefore, the improvement in MRR of the +ALL module is only 8.3%, which is relatively lower than the improvement of the F-measure. 4.2. Effects on different question types
We analyzed the effects of Wikipedia X  X  semi-structure information on different types of questions: factoid, list, and descriptive questions. Table 9 shows a comparison among the six methods for the three question types. The AC (baseline) module shows a lower performance than the +ALL module in all question types. The +IB module improved the performance of the factoid questions, and generated 18 additional answers, 16 of which are correct. The +IB module does not generate to 24 more questions. The +AS module improved the performance of factoid and descriptive questions. The module responded with correct reason-and method-type answers to 10 more questions. The +DEF module responded to 92 more descriptive questions, showing the highest impact in our experiment. 4.3. Error analysis
Since multiple components involved in answering process, an incorrect answer should be traced back to identify the first correct answers were cut-off, especially for factoid questions in the AC module. The answer theme analysis and document indexing/retrieval module generated a considerable number of incorrect answers (38.3%) mostly in the AC module where possible answers in a document should be preliminarily indexed with their entity types. This observation indicates that a traditional QA module based on simple text documents shows lower performance than rich knowledge based modules.
Document retrieval module can be improved by utilizing Wikipedia title matching strategy rather than simple content matching strategy. Errors in the question format analysis also induced incorrect answers (12.8%). When the incorrect three wrong answers (6.4%), when prior module generated incorrect answers with high confidence in the module invocation decision. However the percentage of the strategy error is relatively lower than other error types. This means that the modules have their own strength to specific question types, and the strict answer merging strategy is effective. 5. Conclusion
The main motivation behind this work was to devise a way to utilize the existing semi-structured, large-size Wikipedia database as a knowledge source for a QA system without building high-cost knowledge base. To this end, we categorized the
English QA system using the proposed method and compare the performance based on famous benchmark such as TREC datasets.
 References
