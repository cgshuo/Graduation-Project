 In this paper, we present a novel Chinese language model, and study its applications, in particular in Chinese pinyin-to-character conversion. In the new model, each word is associated with supporting context constructed by mining the frequent sets of nearby phrases and their distances to the word. Such information was usually overlooked in pre-vious n-gram model and its variants. We apply the model to Chinese pinyin-to-character conversion and find that it of-fers a better solution to Chinese input. The model has lower perplexity in our evaluation and higher prediction accuracy than the state-of-the-art n-gram Markov model for Chinese language.
 Categories and Subject Descriptors: H.4 [Information Systems Applications]: Miscellaneous; I.2.7 [Natural Lan-guage Processing]: Language models.
 General Terms: Algorithms, Performance, Design, Exper-imentation Keywords: Chinese language model, word-context sup-port, pinyin-to-character conversion
A language model provides estimation of probability dis-tribution for natural language by applying either statistical methods or linguistic knowledge. In particular, statistical language model has been widely applied in recent years. In Chinese language processing, most if not all state-of-the-art research [5] are based on trigram Markov model. With tri-gram Markov assumption, the next word is predicted based on the two immediately preceding words with conditional probability P ( W i | W i  X  1 W i  X  2 ). Thus the probability of a sen-tence is estimated as P ( S )= i P ( W i | W i  X  1 W i  X  2 Pinyin Input Method Editors(PinyinIME), most of which apply trigram Markov model [5] in a pinyin-to-character conversion system, are the major input solutions for Chinese characters. Pinyin, the phonetic symbol of Chinese charac-ters, uses only English alphabets thus suffices for Chinese input through standard keyboard.

However, with the strong independent assumption, tri-gram Markov model and other variants suffer from the prob-lem that long distant dependencies are not captured. In addition, related applications have limitations in user expe-rience: the system can only process the input in sequence either from left or right. There have been some attempts to solve these issues, mostly for English. However we would like to address this problem with two principles: one sense one collocation , which describes that a word only conveys one sense when certain context is given; the sense of a spe-cific word is expressed by the context information from the entire sentence [7]. We thus propose to incorporate a formal text mining process together with maximum entropy princi-ple to construct a coherent computational model for Chinese language.
In our model, the objective function is P ( S )or P ( W 1 Based on our previous principles, P ( S ) should be a function of all the words W i and W i should depend on its supporting context presented within the entire sentence. We consider context information include the context words and the rel-ative position, thus define Context i,j = { ( W pos ,pos  X  i Such combination increases the problem dimension, how-ever, at the same time enhances the discriminative power [4]. In sum, every word contributes to the sentence and any in-dividual word depends on the supporting context presented in the entire sentence, therefore our integrated formula is P (
S )=Agg 1 tions Agg 1 () and Agg 2 (). Here we denote the Wo r d -C o ntext Pair as P air i,j ,whichisthepairof W i and Context i,j . We then propose two computational models, namely Basic-Support and MaxEntSupport . BasicSupport model assumes that different P air i,j with different semantic meaning will not happen in a same sentence. Thus arithmetic summation is applied. Subsequently we applied the arithmetic average as Agg 1 () for computation convenience: where n is the number of words in S and 1 n is a normalizing factor.
We also develop MaxEntSupport model with maximum en-tropy principle, which was first introduced to natural lan-guage processing by Berger et al [3]. It is mathematically suitable for the second aggregate function Agg 2 (). The ME Model is formulated as P ( y | x )= 1 Z ( x ) exp( i  X  where f i () is the feature function,  X  i is the weight for the feature. Again we applied the arithmetic average as Agg 1 where n is the number of words in S and 1 n is a normalizing factor. Eventually the probability of any sentence in Max-EntSupport model only depends on the P air i,j presented, same as in BasicSupport .

We adopt the frequent item sets mining technique from association rule mining by Agrawal et al [2]. In such method, we extract Context i,j from sentences by mining the most frequent item sets of words. Every sentence S is modeled as a transaction and every word-position ( W pos ,pos  X  i modeled as a item for frequent itemset mining. In order to prevent over fitting, we incorporated cross validation on the threshold value.
We first evaluate our model based on language model per-plexity [5, 6], which is defined as PP =2  X  1 N log 2 P ( W i It can been seen as the word branching factor. Generally ap-plications have better performance when its model has lower perplexity. The data set is annotated  X  X eople X  X  Daily X (the major newspaper in P.R.China) corpus prepared by Peking University. It contains more than 1 Million Chinese words from newspaper articles in the form of pre-segmented sen-tences. We split the corpus into training, validation and testing part respectively with the ratio 3:1:1. In particular, the validation part tunes the threshold of frequent word-context pairs during mining process to the one with lowest perplexity. We consider two baselines which applied varia-tions of trigram Markov model [5, 6]. We also implements a unigram model to identify the reduction in perplexity by word-context support model. In Table 1, there are two ex-periments for MaxEntSupport . The original testing data in experiment MaxEntSupport 1 contains short sentences with less than four words. We remove such short sentences and conduct another experiment MaxEntSupport 2 . The big gap in between the two results suggests possible over learning. This overall comparison shows our significant improvement on model perplexity with newspaper articles.

We also conduct the experiment on our Chinese pinyin-to-character conversion system. In such system, users in-put pinyin sequence and the desired output from the sys-tem is the most probable Chinese sentence correspond to that pinyin sequence. The problem of homophones in Chi-nese poses a great challenge for this task. Previous research incorporated SLM into pinyin-to-character conversion with  X  S =argmax S P ( S | E ) where E is the input pinyin sequence and S is the hypothesized Chinese sentence. In our model, the probability is estimated, however, by the aggregate func-tions which was mentioned in last section.

We discover several novel features for Chinese PinyinIME with our model and prototype an intelligent positioning and selection feature. Current commercial PinyinIMEs fail to provide a good solution for users to edit the incorrect pre-dicted sentence while they commonly produced such. The user has to enter the complicated modification process which they usually avoid. We attempt to solve this problem by promoting the most probable errant word to the user with possible alternates by examining whether there is candidates for same pinyin syllable with close probability P ( S | W Agg 2 j ( P air ic i ,j ) in the same context. In our experiment, for mostofthetimeusersareabletofindthecorrectsentence with only one key stroke. In sharp contrast, it might take as many as ten key strokes on commercial PinyinIMEs, largely due to the drawback of Markov model. We then allow one key stroke to be taken by the user and compare our accu-racy rate in this case with commercial products. In Figure 1, we show the performance comparison in terms of num-ber of wrong characters with the same 100-sentence scale on x-axis. As shown, the model achieves 20% drop of wrong characters over Google Pinyin IME(GooglePY1.2) and up to 50% drop over Microsoft Pinyin IME 2007(MSPY2007) and Sogou Pinyin IME(SogouPY3.2). More user experience demonstration can be found on our project site [1].
The work presented in this paper aims at improving per-formance of Chinese language processing applications by providing a more accurate model. In this paper, we pro-posed two word-context support model named BasicSup-port and MaxEntSupport . Based on model perplexity, we formally evaluated the models and showed that the models have lower perplexity than several baselines. We also as-sessed the pinyin-to-character conversion system with cur-rent commercial products and obtained significant drop in the error rate.
