 Word embeddings have become increasingly pop-ular lately, proving to be valuable as a source of features in a broad range of NLP tasks with lim-ited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014). and GloVe 2 (Pennington et al., 2014) are among the most widely used word embedding models to-day. Their success is largely due to an efficient and user-friendly implementation that learns high-quality word embeddings from very large corpora.
Both word2vec and GloVe learn low-dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target word. However, the underlying models are equally applicable to different choices of context types. For example, Bansal et al. (2014) and Levy and Goldberg (2014) showed that using syntactic contexts rather than window contexts in word2vec captures functional similarity (as in lion:cat ) rather than topical similarity or relatedness (as in lion:zoo ). Further, Bansal et al. (2014) and Melamud et al. (2015b) showed the benefits of such modified-context embeddings in dependency parsing and lexical substitution tasks. However, to the best of our knowledge, there has not been an extensive evaluation of the effect of multiple, diverse context types on a wide range of NLP tasks.
Word embeddings are typically evaluated on in-trinsic and extrinsic tasks. Intrinsic tasks mostly in-clude predicting human judgments of semantic re-lations between words, e.g., as in WordSim-353 (Finkelstein et al., 2001), while extrinsic tasks in-clude various  X  X eal X  downstream NLP tasks, such as coreference resolution and sentiment analysis. Re-cent works have shown that while intrinsic evalu-ations are easier to perform, their correlation with results on extrinsic evaluations is not very reliable (Schnabel et al., 2015; Tsvetkov et al., 2015), stress-ing the importance of the latter.

In this work, we provide the first extensive eval-uation of word embeddings learned with different types of context, on a wide range of intrinsic simi-larity and relatedness tasks, and extrinsic NLP tasks, namely dependency parsing, named entity recogni-tion, coreference resolution, and sentiment analy-sis. We employ contexts based of different word window sizes, syntactic dependencies, and a lesser-known substitute words approach (Yatbaz et al., 2012). Finally, we experiment with combinations of the above word embeddings, comparing two ap-proaches: (1) simple vector concatenation that offers a wider variety of features for a classifier to choose and learn weighted combinations from, and (2) di-mensionality reduction via either Singular Value Decomposition or Canonical Correlation Analysis, which tries to find a smaller subset of features.
Our results suggest that it is worthwhile to care-fully choose the right type of word embeddings for an extrinsic NLP task, rather than rely on intrinsic benchmark results. Specifically, picking the optimal context type and dimensionality is critical. Further-more, once the benefit from increasing the embed-ding dimensionality is mostly exhausted, concatena-tion of word embeddings learned with different con-text types can yield further performance gains. 2.1 Learning Corpus We use a fixed learning corpus for a fair compari-son of all embedding types: a concatenation of three large English corpora: (1) English Wikipedia 2015, (2) UMBC web corpus (Han et al., 2013), and (3) English Gigaword (LDC2011T07) newswire corpus (Parker et al., 2011). Our concatenated corpus is diverse and substantial in size with approximately 10B words. This allows us to learn high quality em-beddings that cover a large vocabulary. After ex-tracting clean text from these corpora, we used Stan-ford CoreNLP (Manning et al., 2014) for sentence splitting, tokenization, part-of-speech tagging and cased, and sentences were shuffled to prevent struc-tured bias. When learning word embeddings, we ig-nored words with corpus frequency lower than 100, 2.2 Window-based Word Embeddings We used word2vec  X  X  skip-gram model with neg-ative sampling (Mikolov et al., 2013b) to learn method embeds both target words and contexts in the same low-dimensional space, where the embed-dings of a target and context are pushed closer to-gether the more frequently they co-occur in a learn-ing corpus. Indirectly, this also results in similar em-beddings for target words that co-occur with similar contexts. More formally, this method optimizes the following objective function: where v t and v 0 c are the vector representations of tar-get word t and context word c . PAIRS is the set of window-based co-occurring target-context pairs considered by the model that depends on the win-dow size, and NEGS ( t,c ) is a set of randomly sam-pled context words used with the pair ( t,c ) . 6
We experimented with window sizes of 1, 5, and 10, and various dimensionalities. We denote a window-based word embedding with window size of n and dimensionality of m with W n m . For ex-window size of 5 and dimensionality of 300. 2.3 Dependency-based Word Embeddings We used word2vecf 7 (Levy and Goldberg, 2014), to learn dependency-based word embeddings from the parsed version of our corpus, similar to the ap-proach of Bansal et al. (2014). word2vecf ac-cepts as its input arbitrary target-context pairs. In the case of dependency-based word embeddings, the context elements are the syntactic contexts of the target word, rather than the words in a win-dow around it. Specifically, following Levy and Goldberg (2014), we first  X  X ollapsed X  prepositions (as implemented in word2vecf ). Then, for a tar-get word t with modifiers m 1 ,..., m k and head h , we paired the target word with the context elements ( m 1 ,r 1 ) ,..., ( m k ,r k ) , ( h,r  X  1 the dependency relation between the head and the modifier (e.g., dobj , prep of ) and r  X  1 denotes an in-verse relation. We denote a dependency-based word embedding with dimensionality of m by DEP m . We note that under this setting word2vecf optimizes the same objective function described in Equa-tion (1), with PAIRS now comprising dependency-based pairs instead of window-based ones. 2.4 Substitute-based Word Embeddings Substitute vectors are a recent approach to represent-ing contexts of target words, proposed in Yatbaz et al. (2012). Instead of the neighboring words them-selves, a substitute vector includes the potential filler words for the target word slot, weighted according to how  X  X it X  they are to fill the target slot given the neighboring words. For example, the substitute vec-tor representing the context of the word love in  X  X  love my job X , could look like: [ quit 0.5, love 0.3, hate 0.1, lost 0.1]. Substitute-based contexts are generated using a language model and were suc-cessfully used in distributional semantics models for part-of-speech induction (Yatbaz et al., 2012), word sense induction (Baskaya et al., 2013), functional se-mantic similarity (Melamud et al., 2014) and lexical substitution tasks (Melamud et al., 2015a).

Similar to Yatbaz et al. (2012), we consider the words in a substitute vector, as a weighted set of con-texts  X  X o-occurring X  with the observed target word. For example, the above substitute vector is consid-ered as the following set of weighted target-context pairs: { ( love , quit , 0.5), ( love , love , 0.3), ( love , hate , 0.1), ( love , lost , 0.1) } . To learn word embed-dings from such weighted target-context pairs, we extended word2vecf by modifying the objective Table 1: The top five words closest to target word playing in different embedding spaces. function in Equation (1) as follows: where  X  t,c is the weight of the target-context pair ( t,c ). With this simple modification, the effect of target-context pairs on the learned word represen-tations becomes proportional to their weights.
To generate the substitute vectors we followed the methodology in (Yatbaz et al., 2012; Mela-mud et al., 2015a). We learned a 4-gram Kneser-Ney language model from our learning corpus us-ing KenLM (Heafield et al., 2013). Then, we used FASTSUBS (Yuret, 2012) with this language model to efficiently generate substitute vectors, where the weight of each substitute s is the conditional proba-bility p ( s | C ) for this substitute to fill the target slot given the sentential context C . For efficiency, we pruned the substitute vectors to their top-10 sub-stitutes, s 1 ..s 10 , and normalized their probabilities such that ated only up to 20,000 substitute vectors for each tar-get word type. Finally, we converted each substitute vector into weighted target-substitute pairs and used our extended version of word2vecf to learn the 2.5 Qualitative Effect of Context Type To motivate the rest of our work, we first qualita-tively inspect the top most-similar words to some target words, using cosine similarity of their respec-tive embeddings. As illustrated in Table 1, in embed-dings learned with large window contexts, we see both functionally similar words and topically similar words, sometimes with a different part-of-speech. With small windows and dependency contexts, we generally see much fewer topically similar words, which is consistent with previous findings (Bansal et al., 2014; Levy and Goldberg, 2014). Finally, with substitute-based contexts, there appears to be even a stronger preference for functional similarity, with a tendency to also strictly preserve verb tense. As different choices of context type yield word embeddings with different properties, we hypothe-size that combinations of such embeddings could be more informative for some extrinsic tasks. We ex-perimented with two alternative approaches to com-bine different sets of word embeddings: (1) Sim-ple vector concatenation, which is a lossless com-bination that comes at the cost of increased dimen-sionality, and (2) SVD and CCA, which are lossy combinations that attempt to capture the most use-ful information from the different embeddings sets with lower dimensionality. The methods used are described in more detail next. 3.1 Concatenation Perhaps the simplest way to combine two different sets of word embeddings (sharing the same vocabu-lary) is to concatenate their word vectors for every word type. We denote such a combination of word embedding set A with word embedding set B using the symbol ( + ). For example W10+DEP 600 is the the dimensionality of the concatenated embeddings is the sum of the dimensionalities of the component embeddings. In our experiments, we only ever com-bine word embeddings of equal dimensionality.
The motivation behind concatenation relates pri-marily to supervised models in extrinsic tasks. In such settings, we hypothesize that using concate-nated word embeddings as input features to a classi-fier could let it choose and combine (i.e., via learned weights) the most suitable features for the task. Con-sider a situation where the concatenated embedding to a named entity recognition classifier. In this case, the classifier could choose, for instance, to represent entity words mostly with dependency-based embed-ding features (reflecting functional semantics), and surrounding words with large window-based embed-ding features (reflecting topical semantics). 3.2 Singular Value Decomposition Singular Value Decomposition (SVD) has been shown to be effective in compressing sparse word representations (Levy et al., 2015). In this work, we use this technique in the same way to reduce the di-mensionality of concatenated word embeddings. 3.3 Canonical Correlation Analysis Recent work used Canonical Correlation Analysis (CCA) to derive an improved set of word embed-dings. The main idea is that two distinct sets of word embeddings, learned with different types of in-put data, are considered as multi-views of the same vocabulary. Then, CCA is used to project each onto a lower dimensional space, where correlation be-tween the two is maximized. The correlated infor-mation is presumably more reliable. Dhillon et al. (2011) considered their two CCA views as embed-dings learned from the left and from the right con-text of the target words, showing improvements on chunking and named entity recognition. Faruqui and Dyer (2014) and Lu et al. (2015) considered multi-lingual views, showing improvements in several in-trinsic tasks, such as word and phrase similarity.
Inspired by this prior work, we consider pairs of word embedding sets, learned with different types of context, as different views and correlate them 999 or WordSim-353-R intrinsic benchmark (sec-et al., 2012). This results in different projections for each of these tuning objectives, where SimLex-999/WordSim-353-R is expected to give some bias towards functional/topical similarity, respectively. 4.1 Intrinsic Benchmarks We employ several commonly used intrinsic bench-marks for assessing how well word embeddings mimic human judgements of semantic similarity of words. The popular WordSim-353 dataset (Finkel-stein et al., 2001) includes 353 word pairs manually annotated with a degree of similarity. For exam-ple, computer : keyboard is annotated with 7.62, in-dicating a relatively high degree of similarity. While WordSim-353 does not make a distinction between different  X  X lavors X  of similarity, Agirre et al. (2009) proposed two subsets of this dataset, WordSim-353-S and WordSim-353-R , which focus on functional and topical similarities, respectively. SimLex-999 (Hill et al., 2014) is a larger word pair similarity dataset with 999 annotated pairs, purposely built to focus on functional similarity. We evaluate our em-beddings on these datasets by computing a score for each pair as the cosine similarity of two word ranking of word pairs induced from the human an-notations and that from the embeddings is reported.
The TOEFL task contains 80 synonym selection items, where a synonym of a target word is to be se-lected out of four possible choices. We report the overall accuracy of a system that uses cosine dis-tance between the embeddings of the target word and each of the choices to select the one most similar to the target word as the answer. 4.2 Extrinsic Benchmarks The following four diverse downstream NLP tasks 1) Dependency Parsing ( PARSE ) The Stanford Neural Network Dependency (NNDEP) parser (Chen and Manning, 2014) uses dense continuous representations of words, parts-of-speech and de-pendency labels. While it can learn these repre-sentations entirely during the training on labeled data, Chen and Manning (2014) show that initializa-tion with word embeddings, which were pre-trained on unlabeled data, yields improved performance. Hence, we used our different types of embeddings to initialize the NNDEP parser and compared their per-formance on a standard Penn Treebank benchmark. We used WSJ sections 2 X 21 for training and 22 for development. We used predicted tags produced via 20-fold jackknifing on sections 2 X 21 with the Stan-ford CoreNLP tagger. 2) Named Entity Recognition ( NER ) We used the NER system of Turian et al. (2010), which allows adding word embedding features (on top of various other features) to a regularized averaged perceptron classifier, and achieves near state-of-the-art results using several off-the-shelf word representations. We varied the type of word embeddings used as features when training the NER model, to evaluate their ef-fect on NER benchmarks results. Following Turian et al. (2010), we used the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003) with 204K/51K train/dev words, as our main bench-mark. We also performed an out-of-domain eval-uation, using CoNLL-2003 as the train set and the 3) Coreference Resolution ( COREF ) We used the Berkeley Coreference System (Durrett and Klein, 2013), which achieves near state-of-the-art results with a log-linear supervised model. Most of the fea-tures in this model are associated with pairs of cur-rent and antecedent reference mentions, for which a coreference decision needs to be made. To evaluate the contribution of different word embedding types to this model, we extended it to support the follow-{ a i  X  c i } i =1 ..m , where a i or c i is the value of the i th dimension in a word embedding vector represent-ing the antecedent or current mention, respectively. We considered two different word embedding repre-sentations for a mention: (1) the embedding of the head word of the mention and (2) the average em-bedding of all words in the mention. The features of both types of representations were presented to the learning model as inputs at the same time. They were added on top of Berkeley X  X  full feature list ( X  X I-NAL X ) as described in Durrett and Klein (2013). We evaluated our features on the CoNLL-2012 corefer-ence shared task (Pradhan et al., 2012). 4) Sentiment Analysis ( SENTI ) Following Faruqui et al. (2014), we used a sentence-level binary decision version of the sentiment analysis task from Socher et al. (2013). In this setting, neutral sentences were discarded and all remaining sentences were labeled coarsely as positive or neg-ative. Maintaining the original split into train/dev results, we get a dataset containing 6920/872 sentences. To evaluate different types of word embeddings, we represented each sentence as an average of its word embeddings and then used an L2-regularized logistic regression classifier trained on these features to predict the sentiment labels. 5.1 Intrinsic Results for Context Types The results on the intrinsic tasks are illustrated in Figure 1. First, we see that the performance on all tasks generally increases with the number of dimen-sions, reaching near-optimal performance at around 300 dimensions, for all types of contexts. This is in line with similar observations on skip-gram word embeddings (Mikolov et al., 2013a).

Looking further, we observe that there are sig-nificant differences in the results when using dif-ferent types of contexts. The effect of context choice is perhaps most evident in the WordSim-353-R task, which captures topical similarity. As might be expected, in this benchmark, the largest-window word embeddings perform best. The performance decreases with the decrease in window size and then reaches significantly lower levels for depen-dency (DEP) and substitute-based (SUB) embed-dings. Conversely, in WordSim-353-S and SimLex-999, both of which capture a more functional simi-larity, the DEP embeddings are the ones that perform best, strengthening similar observations in Levy and Goldberg (2014). Finally, in the TOEFL benchmark, all contexts except for SUB, perform comparably. 5.2 Extrinsic Results for Context Types The extrinsic tasks results are illustrated in Figure 2. A first observation is that optimal extrinsic results may be reached with as few as 50 dimensions. Fur-thermore, performance may even degrade when us-NER and PARSE with only up to 300 dimensions embeddings, and ing too many dimensions, as is most evident in the NER task. This behavior presumably depends on various factors, such as the size of the labeled train-ing data or the type of classifier used, and highlights the importance of tuning the dimensionality of word embeddings in extrinsic tasks. This is in contrast to intrinsic tasks, where higher dimensionality typi-cally yields better results.

Next, comparing the results of different types of contexts, we see, as might be expected, that de-pendency embeddings work best in the PARSE task. More generally, embeddings that do well in func-tional similarity intrinsic benchmarks and badly in topical ones (DEP, SUB and W1) work best for PARSE , while large window contexts perform worst, similar to observations in Bansal et al. (2014).
In the rest of the tasks it X  X  difficult to say which context works best for what. One possible expla-Table 2: NER MUC out-of-domain results for dif-ferent embeddings with dimensionality = 25. nation to this in the case of NER and COREF is that the embedding features are used as add-ons to an al-ready competitive learning system. Therefore, the total improvement on top of a  X  X o embedding X  base-line is relatively small, leaving little room for signif-icant differences between different contexts.
We did find a more notable contribution of word Figure 3: Mean development set results for the tasks PARSE and SENTI .  X  X ean X  and  X  X ean+ X  stand for mean results across all single context types and con-text concatenations, respectively. embedding features to the overall system perfor-mance in the out-of-domain NER MUC evaluation, described in Table 2. In this out-of-domain setting, all types of contexts achieve at least five points im-provement over the baseline. Presumably, this is be-cause continuous word embedding features are more robust to differences between train and test data, such as the typical vocabulary used. However, a de-tailed investigation of out-of-domain settings is out of scope for this paper and left for future work. 5.3 Extrinsic Results for Combinations A comparison of the results obtained on the ex-trinsic tasks using the word embedding concate-nations ( concats ), described in section 3.1, versus the original single context word embeddings ( sin-gles ), appears in Table 3. To control for dimen-sionality, concats are always compared against sin-gles with identical dimensionality. For example, the
Looking at the results, it seems like the bene-fit from concatenation depends on the dimension-ality and task at hand, as also illustrated in Fig-ure 3. Given task X and dimensionality d , if d the range where increasing the dimensionality yields significant improvement on task X , then it X  X  better to simply increase dimensionality of singles from d to d rather than concatenate. The most evident ex-ample for this are the results on the SENTI task with d = 50 . In this case, the benefit from concatenat-ing two 25-dimensional singles is notably lower than that of using a single 50-dimensional word embed-ding. On the other hand, if d near-optimal performance is reached on task X , then concatenation seems to pay off. This can be seen in SENTI with d = 600 , PARSE with d = 200 , and NER with d = 50 . More concretely, looking at the best performing concatenations, it seems like combina-tions of the topical W10 embedding with one of the more functional ones, SUB, DEP or W1, typically perform best, suggesting that there is added value in combining embeddings of different nature.
 Finally, our experiments with the methods using SVD (section 3.2) and CCA (section 3.3) yielded degraded performance compared to single word em-beddings for all extrinsic tasks and therefore are not reported for brevity. These results seem to further strengthen the hypothesis that the information cap-tured with varied types of context is different and complementary, and therefore it is beneficial to pre-serve these differences as in our concatenation ap-proach. There are a number of recent works whose goal is a broad evaluation of the performance of different word embeddings on a range of tasks. However, to the best of our knowledge, none of them focus on embeddings learned with diverse context types as we do. Levy et al. (2015), Lapesa and Evert (2014), and Lai et al. (2015) evaluate several design choices when learning word representations. How-ever, Levy et al. (2015) and Lapesa and Evert (2014) performed. perform only intrinsic evaluations and restrict con-text representation to word windows, while Lai et al. (2015) do perform extrinsic evaluations, but re-strict their context representation to a word window with the default size of 5. Schnabel et al. (2015) and Tsvetkov et al. (2015) report low correlation be-tween intrinsic and extrinsic results with different word embeddings (they did not evaluate different context types), which is consistent with differences we found between intrinsic and extrinsic perfor-mance patterns in all tasks, except parsing. Bansal et al. (2014) show that functional (dependency-based and small-window) embeddings yield higher pars-ing improvements than topical (large-window) em-beddings, which is consistent with our findings.
Several works focus on particular types of con-texts for learning word embeddings. Cirik and Yuret (2014) investigates S-CODE word embed-dings based on substitute word contexts. Ling et al. (2015b) and Ling et al. (2015a) propose extensions to the standard window-based context modeling. Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manually-constructed resources, such as WordNet. These techniques could be complementary to our work. Fi-nally, Yin and Sch  X  utze (2015) and Goikoetxea et al. (2016) propose word embeddings combinations, us-ing methods such as concatenation and CCA, but evaluate mostly on intrinsic tasks and do not con-sider different types of contexts. In this paper we evaluated skip-gram word embed-dings on multiple intrinsic and extrinsic NLP tasks, varying dimensionality and type of context. We show that while the best practices for setting skip-gram hyperparameters typically yield good results on intrinsic tasks, success on extrinsic tasks requires more careful thought. Specifically, we suggest that picking the optimal dimensionality and context type are critical for obtaining the best accuracy on ex-trinsic tasks and are typically task-specific. Further improvements can often be achieved by combining complementary word embeddings of different con-text types with the right dimensionality.
 We thank Do Kook Choe for providing us the jack-knifed version of WSJ. We also wish to thank the IBM Watson team for helpful discussions and our anonymous reviewers for their comments. This work was partially supported by the Israel Science Foundation grant 880/12 and the German Research Foundation through the German-Israeli Project Co-operation (DIP, grant DA 1600/1-1).
