 Traditional image annotation studies, a basic assumption is that all the proper labels of every training image are given and correct. In real environment, this assumption hardly holds since getting all the proper labels is usually expensive, time consuming and people usually add a few labels, rather than an exhaus-tive list of relevant terms. Moreover, not all of the labels are relevant to the image content ,for example, images labeled with  X  X ar X  might be taken from a car, rather than depicting one. It is evident that this scenario is quite differ-ent from the classic image annotation setting where all proper labels for train-ing data are assumed to be given. Images in benchmark set are also usually weakly labeled(showed in Table 1). Meanwhile, large variations in the frequency of different labels can reduce the performance of the labeling method on the low-frequency labels. E.g., in an exper iment on the Corel5K dataset, we found that for the 20% least frequent labels, JEC [1] achieves an F-score of 19.7%, whereas it gives reasonably good performance for the 20% most frequent labels with F-score being 50.6%. In this work, the meaning of the terminology  X  X eak labels X  is threefold: (1) the given labels may be incomplete, namely only a subset of labels are attached to images according to the ground truth; (2)even for the labels provided, there may be noisy labels; (3) there is large variations in the frequency of different labels(semantic imbalance). Image annotation from weakly labeled dataset is important since weakly-labeled problems are prevalent in the popular datasets as well as real-world environment. In[2], the authors showed performance improvement where for each training instance, only one of its class assignments is correct. In[3], a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction is explored. In [4], the author focuses on remov-ing false class assignments for training set. In [5], the author proposed ranking based multi-label learning to learn from incompletely data. Our work is more comprehensive and address a more realistic and challenging scenario where the datasets seriously suffer from weakly labeled issues. Based on the idea that negative impact of the weak label can be reduced under the guidance of neighbors, the training image X  X  labels are replenished by minimiz-ing the label X  X  weighted error function, then  X  X emantic balanced neighborhood X  is set up based on the replenishing lab els to address the large differences in these label X  X  frequency. Linear metric embedded in multiple label information is learned to obtain the consistency of distance measure and image semantic. Then the images X  partial correlation is obtained by image X  X  nonnegative sparse linear combination between neighbors. The neighbors in the final neighborhood have higher global similarity, partial correlation and conceptual similarity along with semantic balance. Label predict ion is performed in the neighborhood by minimizing label X  X  reconstruction error loss, and noise labels are handled by two regulation terms. 2.1 Semantic Balanced Neighborhood By semantic balanced neighborhood (short for BN), we mean, for a given image, there should not have larg e differences in the frequency of different labels in it X  X  neighborhood. Considering labels for training image may be incomplete, we replenish missing labels firstly. Denote vocabulary as C = { c 1 ,c 2 , ..., c q } the i-th image has the j-th label and y ij =0otherwise. Y =[ y 1 , ..., y l ] T be the corresponding label indicator matrix. We want to learn a replenished function of i-th image, and we use matrix F =[ f 1 ,f 2 , ..., f l ] to present the replenished label matrix. The error function is E ( f )= E 1 ( f )+  X E 2 ( f ),where  X   X  0isa controlling parameter. Thus, the optimization problem is: E ( f ) represents the weighted error function, u ij represents the weight between sample x i and j-th label, u ij =1if y ij =1,and  X  otherwise (0  X   X   X  1). Minimizing E 1 ( f ) is equivalent to requiring the output of f is similar to the original labels and can replenish the missing labels. Minimizing the second term is equivalent to requiring the smoothness output of f on each sample X  X  neighbor according to their similarity.The approximate optimal solution can be derived by least squares. Based on the replenished labels, image X  X  BN is constructed as follows. Let L i  X  L (  X  i  X  X  1 , 2 , ..., q } ) be the subset of training data that contains all the images annotated with the label c i ,we consider it as a semantic group. Given an image x , from each semantic group we pick k 2 images that are most similar to x and form corresponding sets L x,i  X  L i .Thus,each L x,i contains images that are most informative in predicting the probability of the label c i for x . We merge them all to form the semantic balanced neighborhood appears (at least) k 2 times, thus addressing the semantic imbalance issue. 2.2 Semantic Consistent Neighborhood By semantic consistent neighborhood(short for CN), we mean, the neighbors should have both the global similarity and partial correlation along with con-ceptual similarity. We select the partial correlated neighbors in target image X  X  BN by sparse representation. Note that, from signal reconstruction point of view, when target signal is reconstructed from signals in different subspace(semantic subspace), the reconstructi on coefficients have lost their physical meaning. It is obvious that we cannot guarantee that sample X  X  in BN are all semantically similar, since image pair X  X  semantic similarity depends on the corresponding label set instead of single label. As shown in figure1(a), x p  X  X  semantically simi-lar neighbors (denoted by circle) and neighbors that are semantically dissimilar neighbors(denoted by square) are all in x p  X  X  BN. If semantically dissimilar neigh-bors lie outside smaller radius with a margin of at least one unit distance, as shown in figure2(b), then we can reconstruct x p by neighborhood (b). Let a and b be two training images, and each represented by n features { f 1 A , ..., f n A } and { f
B , ..., f u i and w are usually taken as a non-negative normalized unit vector, u i can be assigned appropriate weights to individual dimensions of a feature vector in the feature space, w is to optimally combine multiple feature distances. Given an image x p , along with its label vector y p , we want to learn weights such that its target neighbor x q from the semantic groups { L x p ,r } r are pulled closer, x r from the remaining semantic groups are pushed far.That is, minimize the error function: Here,  X  is the controlling parameters, [ z ] + = max (0 ,z ) is the hinge loss,  X  pq and  X  pr scale the error loss depending on the overlap between the label sets of images. We solve it by alternatively using stochastic sub-gradient descent and projection steps (similar to Pegasos [6]) to obtain an approximate optimal solution of w and u i . Then, given image x i , we find it X  X  k nearest neighbor by Equation (1) to is the reconstruction coefficients vector for x i . Note that negative coefficient has not explicit meaning to describe semanti c, so we reformulate the reconstruction relationship as x i = B i  X  i +  X  ,where  X  i ( p )  X  0and negative term  X  + ,noiseterm  X  =  X  +  X   X   X  , |  X  | =  X  + +  X   X  . Then we can solve min B be solved efficiently using L1 optimization toolbox like YALL. Then, x i is repre-sented by a sparse linear combination of it X  X  neighbors, and it X  X  semantic consis-tent neighborhood(CN) is composed by the neighbors x i p where a i ( p ) &gt; 0. Let C =[ c ij ] denotes neighborhood weight matrix, where c ij =  X  i ( p ). 2.3 Label Inference in CN Let f =[ f L f U ] T be label matrix of all samples, where f L represents the training set X  X  label matrix, f U represents the unlabeled ones  X  label matrix(initialized by zero). Assuming that each image X  X  labe l vector can be reconstructed by it X  X  neighbors in it X  X  CN, while the reconstruc tion coefficients are the same as their visual reconstruction coefficient. Thus we can predict the labels of the unlabeled samples by the weight in neighborhood matrix C . This prediction is based on the assumption that the weight c ij reflects the likelihood for sample x i to have the same label as sample x j . So the labels of the unlabeled samples can be inferred by minimizing label reconstruction error as follows: where y i is the replenished label vector of x i . We use generalized minimum residual method (GMRES [7]) to obtain an approximated solution. As aforemen-tioned, the associated labels are often in complete and imprecise, so the training labels cannot be fixed during the inference process as in Equation (2) should be refined. However, the training labels should be consistent with the original labels to a certain extent. So the optimization target should be where f L is the training images labels that are propagated, and ideal label vector of the training images. The first term of this formula is the same as in Equation (2). The second term enforces the ideal labels of the training images to be consistent with the labels propagated. The third term constrains that only a limited number of labels are noisy or imprecise. We validate the effectiveness of o ur proposed approach on IAPR-TC12,ESPGAME and FLICKR2.5M datasets. Each image is annotated with 5 most relevant keywords. Results are summarized in Table 2. From the results we find that our method significantly improve over the current state-of-the-art. On ESP-GAME image dataset, we achieve 30.8% and 11.1% performance im-provement in terms of precision and recall. On FLICKR, we achieve 41.2% and 24% performance improvement in terms of precision and recall. Image annotation with weakly labeled images is important since weakly-labeled problems are prevalent in the popular web datasets as well as real-world envi-ronment. Experiments show that the proposed framework is more effective than state-of-the-art over web image dataset.

