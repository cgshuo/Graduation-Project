 such that y = f ( x ) + , where f is a function able to characterize y when x is observed and is a residual error.
 observations, or to show the correlations that exist between the set of explanatory variables and the response variable, and thus, give an interpretation to the model.
 is to say f ( x ) = x X  . In a predictive perspective, x X  produces an estimate of y , for any observation x . In a descriptive perspective, |  X  j | can be interpreted as a degree of relevance of variable x j . Ordinary Least Squares (OLS) minimizes the sum of the residual squared error. When the explana-tory variables are numerous and many of them are correlated, the variability of the OLS estimate tends to increase. This leads to reduced prediction accuracy, and an interpretation of the model becomes tricky.
 Coefficient shrinkage is a major approach of regularization procedures in linear regression models. It overcomes the drawbacks described above by adding a constraint on the norm of the estimate  X  . According to the chosen norm, coefficients associated to variables with little predictive information may be shrunk, or even removed when variables are irrelevant. This latest case is referred to as variable selection . In particular, ridge regression shrinks coefficients with regard to the ` 2 -norm, while the lasso ( Least Absolute Shrinkage and Selection Operator ) [1] and the lars ( Least Angle Regression Stepwise ) [2] both shrink and remove coefficients using the ` 1 -norm. Figure 1: left: toy-example of the original structure of variables; right: equivalent tree structure considered for the formalization of the scaling problem.
 In some applications, explanatory variables that share a similar characteristic can be gathered into groups  X  or factors . Sometimes, they can be organized hierarchically. For instance, in genomics, where explanatory variables are (products of) genes, some factors can be identified from the prior methods that retain meaningful factors instead of individual variables.
 Group-lasso and group-lars [3] can be considered as hierarchical penalization methods, with trees of height two defining the hierarchies. They perform variable selection by encouraging sparseness over predefined factors. These techniques seem perfectible in the sense that hierarchies can be extended to more than two levels and sparseness integrated within groups. This papers proposes a penalizer, derived from an adaptive penalization formulation [4], that highlights factors of interest by balancing and shrinks variables within groups, to favor solutions with few leading terms.
 presented in Section 3. Section 4 shows how this framework can be applied to linear and kernel regression. We conclude with a general survey of our future works. 2.1 Formalization We introduce hierarchical penalization by considering problems where the variables are organized in a tree structure of height two, such as the example displayed in figure 1. The nodes of height cardinality is d k . As displayed on the right-hand-side of figure 1, a branch stemming from the root and going to node k is labelled by  X  1 ,k , and the branch reaching leaf j is labelled by  X  2 ,j . We consider the problem of minimizing a differentiable loss function L (  X  ) , subject to sparseness constraints on  X  and the subsets of  X  defined in a tree hierarchy. This reads where  X  &gt; 0 is a Lagrangian parameter that controls the amount of shrinkage, x/y is defined by continuation at zero as x/ 0 =  X  if x 6 = 0 and 0 / 0 = 0 . The second term of expression (1a) penalizes  X  , according to the tree structure, via scaling factors  X  1 and  X  2 . The constraints (1b) shrink the coefficients  X  at group level and inside groups. In what follows, we show that problem (1) is convex and that this joint shrinkage encourages sparsity at the group level. 2.2 Two important properties We first prove that the optimization problem (1) is tractable and moreover convex. Then, we show an equivalence with another optimization problem, which exhibits the exact nature of the constraints applied to the coefficients  X  .
 Proposition 1 Provided L (  X  ) is convex, problem (1) is convex.
 Proof: A problem minimizing a convex criterion on a convex set is convex. Since L (  X  ) is convex and  X  is positive, the criterion (1a) is convex provided f ( x,y,z ) = x 2  X  yz is convex. To show this, we compute the Hessian: Hence, the Hessian is positive semi-definite, and criterion (1a) is convex.
 Next, constraints (1c) define half-spaces for  X  1 and  X  2 , which are convex sets. Equality constraints (1b) define linear subspaces of dimension K  X  1 and d  X  1 which are also convex sets. The intersec-tion of convex sets being a convex set, the constraints define a convex admissible set, and problem (1) is convex.
 Proposition 2 Problem (1) is equivalent to Sketch of proof: The Lagrangian of problem (1) is Hence, the optimality conditions for  X  1 ,k and  X  2 ,j are After some tedious algebra, the optimality conditions for  X  1 ,k and  X  2 ,j can be expressed as where s k = P 2.3 Sparseness Proposition 2 shows how the penalization influences the groups of variables and each variable in each group. Note that, thanks to the positivity of the squared term in (2), the expression can be further simplified to where, for any L (  X  ) , there is a one-to-one mapping from  X  in (2) to  X  in (3). This expression can be interpreted as the Lagrangian formulation of a constrained optimization problem, where the admissible set for  X  is defined by the multiplicand of  X  .
 We display the shape of the admissible set in figure 2, and compare it to ridge regression , which does not favor sparsity, lasso , which encourages sparsity for all variables but does not take into account the group structure, and group-lasso , which is invariant to rotations of within-group variables. One sees that hierarchical penalization combines some features of lasso and group-lasso . By looking at the curvature of these sets when they meet axes, one gets a good intuition on why ridge regression does not suppress variables, why lasso does, why group-lasso suppresses groups of variables but not within-group variables, and why hierarchical penalization should do both. This intuition is however not correct for hierarchical penalization because the boundary of the admissible set is differentiable in the within-group hyper-plane (  X  1 , X  2 ) at  X  1 = 0 and  X  2 = 0 . However, encouraged.
 To go beyond the hints provided by these figures, we detail here the optimality conditions for  X  minimizing (3). The first-order optimality conditions are These equations signify respectively that Overall, hierarchical penalization is thus expected to provide solutions with few active groups and few leading variables within each group. To solve problem (3), we use an active set algorithm, based on the approach proposed by Osborne et al. [5] for the lasso . This algorithm iterates two phases: first, the optimization problem is solved the current active set of variables,  X  = {  X  j } j  X  X  , the vector of coefficients associated to A , and G k = { J k  X  X } , the subset of coefficients  X  associated to group k . Then, at each iteration, we solve the problem by alternating steps A and B described below. Second, the set of active variables is incrementally updated as detailed in steps C and D.
 A Compute a candidate update from an admissible vector  X  B Obtain a new admissible vector  X   X  C Test optimality of  X  D Select the variable that enters the active set The algorithm is initialized with A =  X  , and the first variable is selected with the process described at step D. We illustrate on two datasets how hierarchical penalization can be useful in exploratory analysis and in prediction. Then, we show how the algorithm can be applied for multiple kernel learning in kernel regression. 4.1 Abalone Database The Abalone problem [6] consists in predicting the age of abalone from physical measurements. The dataset is composed of 8 attributes. One concerns the sex of abalone, and has been encoded infant. This variable defines the first group. The second group is composed of 3 attributes concerning size parameters (length, diameter and height), and the last group is composed of weight parameters (whole, shucked, viscera and shell weight).
 We randomly selected 2920 examples for training, including the tuning of  X  by 10-fold cross val-idation, and left the 1257 other for testing. The mean squared test error is at par with lasso (4.3). The coefficients estimated on the training set are reported in table 4.1. Weight parameters are a main contributor to the estimation of the age of an abalon, while sex is not essential, except for infant. 4.2 Delve Census Database The Delve Census problem [7] consists in predicting the median price of a house in different survey regions. Each 22732 survey region is represented by 134 demographic information measurements. Several prototypes are available. We focussed on the prototype  X  X ouse-price-16L X , composed of 16 variables. We derived this prototype by including all the other variables related to these 16 variables. The final dataset is then composed of 37 variables, split up into 10 groups 1 .
 We randomly selected 8000 observations for training and left the 14732 for testing. We divided the training observations into 10 distinct datasets. For each dataset, the parameter  X  was selected by a 10-fold cross validation, and the mean squared error was computed on the testing set. We reported on table 4.2 the mean squared test errors obtained with the hierarchical penalization (hp), the group-lasso (gl) and the lasso estimates.
 group-lasso on 6 datasets, and obtains equal results on 2 datasets. However the lowest overall mean error is achieved by group-lasso . 4.3 Multiple Kernel Learning Multiple Kernel Learning has drawn much interest in classification with support vector machines (SVMs) starting from the work of Lanckriet et al. [8]. The problem consists in learning a convex combination of kernels in the SVM optimization algorithm. Here, we show that hierarchical penal-smoothing in the regression setup.
 Kernel smoothing has been studied in nonparametric statistics since the 60 X  X  [9]. Here, we consider the model where the response variable y is estimated by a sum of kernel functions where  X  h is the kernel with scale factor (or bandwidth) h , and i is a residual error. For the purpose of combining K bandwidths, the general criterion (3) reads The penalized model (5) has been applied to the motorcycle dataset [9]. This uni-dimensional prob-kernels, with 7 bandwidths ranging from 10  X  1 to 10 2 .
 Figure 3 displays the results obtained for different penalization parameters: the estimated function obtained by the combination of the selected bandwidths, and the contribution of each bandwidth to the model. We display three settings for the penalization parameter  X  , corresponding to slight over-null and are thus not displayed. As expected, when the penalization parameter  X  increases, the fit becomes smoother, and the number of contributing bandwidths decreases. We also observe that the effective contribution of some bandwidths is limited to a few kernels: there are few leading terms in the expansion. Hierarchical penalization is a generic framework enabling to process hierarchically structured vari-subgroups of variables defined at each level of the hierarchy. The fitted model is then biased to-promote a small number of groups of variables, with a few leading components.
 In this paper, we detailed the general framework of hierarchical penalization for tree structures of height two, and discussed its specific properties in terms of convexity and parsimony. Then, we proposed an efficient active set algorithm that incrementally builds an optimal solution to the prob-lem. We finally illustrated how the approach can be used when groups of features, or when discrete variables exist, after being encoded by several binary variables, result in groups of variables. Fi-nally, we also shown how the algorithm can be used to learn from multiple kernels in regression. We are now performing quantitative empirical evaluations, with applications to regression, classification and clustering, and comparisons to other regularization schemes, such as the group-lasso . We then plan to extend the formalization to hierarchies of arbitrary height, whose properties are currently under study. We will then be able to tackle new applications, such as genomics, where the available gene ontologies are hierarchical structures that can be faithfully approximated by trees. [1] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical [4] Y. Grandvalet and S. Canu. Adaptive scaling for feature selection in SVMs. In Advances in [5] M. R. Osborne, B. Presnell, and B. A. Turlach. On the lasso and its dual. Journal of Computa-[6] C.L. Blake D.J. Newman, S. Hettich and C.J. Merz. UCI repository of machine learning [7] Delve: Data for evaluating learning in valid experiments. URL http://www.cs.toronto. [8] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, and W. Noble. A statistical framework for [9] W. H  X  ardle. Applied Nonparametric Regression , volume 19. Economic Society Monographs,
