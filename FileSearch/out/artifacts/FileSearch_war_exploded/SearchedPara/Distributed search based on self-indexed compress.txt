 1. Introduction
Major Web search engines (WSEs) process millions of user queries per day so they are compelled to be extremely efficient optimizations, which as a whole lead to efficient performance. Given the scale of the hardware resources devoted to host to the average amount of hardware hit by queries. Search engines are built on thousands of computers organized in highly communicating groups which are implemented as clusters of distributed memory processors. Thus, for reasons such as power consumption, it is relevant to devise query processing strategies capable of reducing the use of hardware resources whilst satisfying a target throughput and response time bound.

The query process, from reception to response, consists of several steps, many of which can take place in parallel. Typi-uments that best match the query, constructing the answer Web page (this involves extracting small pieces of text, the snippets , from the resulting documents), plus other steps such as advertising, spelling, suggestions and facets.  X 
This paper proposes mixing into a single task and cluster the steps of top-K determination and snippet extraction, by using recently proposed self-indexed compressed text techniques (i.e., text indexes that compress the text and are able to extract any part of it) in combination with posting lists caching. This is achieved for similar memory space requirements and number of processors of a standard cluster in charge of the top-K determination. We study how already-known sequen-tial compressed self-indexes can be employed to support efficient and fault-tolerant query processing in search engines.
Fault-tolerance is supported by a form of replication in which the cluster can be seen as a P D array of search node pro-creases throughput, since query traffic in each column can be evenly distributed across the D replicas.
Notice that even though the secondary memory contents of replicas are identical, one is free to decide which contents to upload from secondary to main-memory during normal operation. In the case of a processor failure, the remaining D 1 pro-cessors can reconstruct the main memory contents of the failed processor into their own main memories by just reading data from secondary memory. Occasional imbalance can be easily cleared by temporarily diverting skewed query traffic to rep-licas selected uniformly at random if necessary.
 show that the proposed indexing scheme is able to answer queries efficiently. We emphasize that the advantage is that we for text and extra communication from interaction with an additional cluster in charge of snippet extraction.
Our main result is that the WT can be adapted to our P D context. That is, by doing term clustering along the D replicas, we can keep in main-memory a very optimized realization of the WT, containing only the terms assigned to the respective replica. We keep the inverted lists for the terms assigned to other replicas in secondary memory.
We also show how the capabilities of the WT can be used to dynamically maintain a cache with the most recently used posting lists of an inverted index generated on-demand. Hence, the posting lists of non-used terms can be disregarded, whereas the posting lists of missing terms can be constructed on-the-fly from the WT itself. The result is an scheme that queries, a type of query supported by major search engines which we believe is a much less frequent one than the conjunc-this paper we explore just a few of them. 2. Background and previous work 2.1. Inverted files
WSEs. It is composed of a vocabulary table (which contains the V distinct relevant terms found in the document collection) with additional data used for ranking purposes. To solve a query, one must fetch the posting lists for the query terms, compute the intersection among them, and then compute the ranking of the resulting intersection set. Hence, an inverted index allows for the very fast computation of the top-K relevant documents for a query, because of the pre-computed data. 2.2. The architecture of a distributed WSE the posting lists for these terms. Both methods have advantages and drawbacks, and the choice depends on the particular scenarios ( Baeza-Yates &amp; Ribeiro-Neto, 1999; Tomasic &amp; Garcia-Molina, 1996 ).

In large document collections, some posting lists contain millions of items. Thus, large index portions must be kept on secondary storage. However, observations from query logs tell us that certain terms in queries are more frequent than oth-for frequently-used posting lists, and the document servers keep a cache for frequently-required Web documents. 2.3. Query processing in a distributed WSE Upon a user query, each of the top-K results obtained is displayed along with a short snippet surrounding the query terms. performs the following tasks: relevant search nodes (to every node in document-partitioned indexes);
K results; 3. sends those results and the query to the document server for snippets determination; 4. once the broker receives the snippets from the document server, builds up the result X  X  page (URLs + snippets) and responds to the user. 2.4. The self-indexing technology and WSEs main-memory, avoiding the high secondary storage costs. Another feature is that self-indexes can search without accessing
A recent work proposes compressed self-indexes ( Arroyuelo et al., 2010 ) as an alternative to inverted indexes, achieving involved in the processing of queries, yet achieving a similar query throughput. Namely, the standard two-stepped proce-dure (i.e., steps 2 and 3 above) is reduced to one step, in which document ranking and snippet determination are combined into a single operation in each processor. This should reduce query processing time and communication. 2.5. Succinct data structures for rank and select queries
Given a bit sequence B [1 ... n ], we define operation rank
B [1 ... i ]. Operation select 1 ( B , i ) (similarly select
S [1 ... n ] over an alphabet R = {1, ... , V }, we generalize the definition for rank 3. Self-indexes for document retrieval
Let D  X f D 1 ; ... ; D N d g be a document collection of N
D T  X  1 ... n  X  $ D 1 $ D 2 $ $ D N d $ of length n  X  1  X  P identifier (docid, for short) i .

If we represent T with a rank / select data structure, we can easily obtain both the docid of the document that contains a vocabulary corresponds to a leaf A WT supports extracting any text symbols and compute operations rank and select in produce an index that is bigger than inverted indexes (actually, bigger than the text itself). Therefore, in this paper we use Huffman-shaped WTs ( Claude &amp; Navarro, 2008 ), achieving n ( H 2008 ). We use the Huffman-shaped WT implementation from Arroyuelo et al. (2010) .

A recent work ( Arroyuelo et al., 2010 ) shows that WTs can be used to index document collection and support the follow-ing functionality within n ( H 0 ( T ) + 1) + o ( n ( H 0 of a given query term t 2 R ; (2) answer conjunctive queries of the form t index; (3) extract a snippet surrounding the occurrences of a given term; (4) obtain within-document term frequencies; and the functionality of an inverted index. However, the space usage should be reduced by using a WT. For instance, positional in that space. However ( Arroyuelo et al., 2010 ) lacks a complete comparison with inverted indexes in practice.
The work ( Arroyuelo et al., 2010 ) introduces three algorithms to support conjunctive queries, namely a simple worst-case scheme, an adaptive scheme, and a hybrid scheme that combines the previous methods. Because of its simplicity and per-formance, we will use the former method in this paper (called
First, as it generates first the shortest inverted list among those of the query terms, and then quickly check whether the remaining terms occur within these documents).

We compare the performance of a search engine based on WTs, with one bases on inverted indexes. Our aim is to show that though WTs can be slower than inverted indexes for computing conjunctive queries, the former are competitive if we baseline scheme. First, we divide the document collection into blocks, each consisting of b documents. We compress each block using the LZMA compression algorithm (using the LZMA Software Development Kit, http://www.7-zip.org/sdk.html ).
This algorithm is able to decompress about 20 X 30 MB of text per second. The compressed blocks are maintained in main-memory. Given the top-10 results (obtained with the inverted index) of a given query, we decompress the blocks containing these documents, and then perform a text search over these documents (for instance, using a Booyer-More like algorithm). block sizes yield different time/space trade-offs, since smaller blocks produce more overhead in the compression.
Table 1 shows the comparison of space usage between WTs and inverted indexes using three of the most effective compres-ments we used a sample of the UK Web from which we selected at random a total of 429,895 documents. This demands a total space usage of about 1.5 GB (no html data is included in the text). The computer used is an Intel(R) Core(TM)2 Duo CPU at 2.8 GHz, with 64 KB of L1 cache, 3072 KB of L2 cache and 5 GB of RAM, and running version kernel.

We use b = 1 for the snippet extraction with inverted indexes, which compresses the document collection to about 13,404 snippets of length 11 per second. We also use b = 150, so that overall the S9 compressed inverted index uses about the same space than that of the WT.

In Table 2 we compare the query performance for the different steps of the query process. The first line shows the base intersection speed, and then each row shows the performance obtained by adding a step to the previous one. As it can be b = 1, however the latter uses 1.27 times more memory than WTs. This space could be used by the WT to store a cache with the most frequently-used inverted lists (see Section 5 ). For b = 150, snippet extraction becomes extremely slow, whereas
WTs maintain about the same performance. This is because WTs extract about 131,227 snippets per second. 4. Distributed web search engines based on self-indexes
Inverted indexes must store extra information to provide extra functionality needed by WSEs, such as ranking and posi-to handle wavelet trees on a distributed-memory environment, which shall be used in Section 5 to develop space-efficient distributed in-memory search engines. We assume in the following an array of P 1 processors. 4.1. Document partitioned self-index In this simple approach, we divide the document collection among the cluster nodes, such that every node stores about
N / P documents. Given the documents of a given search node i of the cluster, we construct a text T concatenation of these documents. Then, at each processor i we construct the wavelet tree W na, 1996 ). 4.2. Term partitioned self-index
We divide the vocabulary such that every search node is responsible for a fraction V / P of the vocabulary. Let V vocabulary for processor i . Then, the local-text T i is the global-text T but considering only the terms in V since these depend on the occurrences of each individual symbol in V sequence B i [1 ... n ], such that B i [ j ]= 1 , T [ j ] 2 V by processor i .

Though the total (logical) number of bits in all bit vectors is nP , there are only n indexed by one and only one processor. If we use the sarray text position for processor i is rank 1 ( B i , j ), which is supported in time O log P  X  local-text position j in processor i , the corresponding global-text position is select ( Okanohara &amp; Sadakane, 2007 ).

An interesting property is that we do not lose compression, not matter how the term distribution is done. This is because wavelet trees are zero-order compressors, and the overall frequency of these terms is not affected if we distribute them. Moreover, we can achieve further compression, because of two facts. First, the vocabulary at each processor has about V / on average. Besides, we will obtain shallower wavelet trees, hence improving the running time. Second, with a careful term distribution we could achieve further compression, as we will see next. 4.2.1. Experimental results Total size (KB) Gonzalez, &amp; Gil-Costa, 2008 ).

We compared the space of the wavelet tree for the whole text versus the sum of the spaces for the term-distributed wave-in different processors ( Marin et al., 2008 ). 4.2.2. Query processing
At search time, queries must be sent only to the processors that store the involved query terms. We suffer from the same can use its secondary memory to solve the intersection. 4.2.3. Snippet extraction
To extract a snippet, let us say that a term occurrence appears at position j in T
B has a 1 in some of the surrounding positions, and in such a case returns to the broker the corresponding terms along with the global text position (accessing a particular bit in B
We carried out experiments to determine the snippet extraction capabilities of WTs in parallel. We tested with P =1,2,4 pets per second. Finally, for P = 8 we get 688,345 snippets per second on average. Thus, our method scales well with the number of partitions used, since at a given time there are more WTs generating snippets in parallel. 5. Deployment on P D search nodes
In this section we describe an instance of use of the wavelet tree (WT) in the context of a large-scale Web search engine producing individual posting lists and pair-wise intersection lists on-demand. This role resembles the three-level caching scheme proposed in Long and Suel (2005) , so we compare our proposal against that approach.

The general WSE architecture and query processing strategy is as follows. The WSE is assumed to be composed of a set of so-called search nodes and a query receptionist machine called broker. The set of search nodes is organized as a 2-dimen-at random. Then the P search nodes respond with the top-K results for the query, which are then merged to produce the sends the global top-K results to the broker which constructs the answer page for the query. 5.1. The baseline strategy inverted lists. For each intersection involving the terms a and b , they define two projection lists I same document IDs in the intersection set, but keep data from a and b respectively that are used to score the documents. of the intersection cache is set to consume a 40% of the space occupied by the inverted file.
 on-demand from secondary memory if they happen to be stored in the intersection cache and were not found in the main-memory LRU list cache at query processing time. This requires secondary memory accesses but ( Long &amp; Suel, 2005 ) is a rule to decide whether or not storing projection lists I caching scheme is completed by assuming the existence of a result cache in the broker.

To increase the efficiency of main-memory space usage, we apply term partitioning along the D replicas of each of the P compressed text for extracting snippets of top-K results is kept evenly distributed in the main memories of the P D search nodes. 5.2. The WT strategy
For the sake of a fair comparison, we also assume the existence of an inverted file stored on disk. We keep in main-mem-search nodes, the total main-memory space occupied by the WT plus the LRU list cache is the same than the space occupied the space occupied by the compressed text, and thereby the LRU list cache of the WT strategy is smaller than the LRU list cache of the baseline strategy.

Like in the baseline strategy, we assume term-partitioning along the D replicas of the P partitions. For the WT this also to indicate how many terms t between two consecutive terms a and b in the WT of a given row are in the actual text. These terms t are indexed in the WTs located in other rows of the same column. The array of bits together with the D WTs of the column are used to build the snippets for the documents that are part of the global top-K results for a query. The same scheme of D WTs and bit arrays is constructed in the remaining columns by considering, in each case, only the documents allocated to the partition associated with the respective column.

For a query with terms a and b we define the projection lists I lists are found or not in the LRU list cache: ( i ) Neither I  X  X  nor L  X  X  are in the cache, then the WT is used to generate I ( ii ) one of the I  X  X  is in the cache, say I a ? b , then we use I ( iii ) I a ? b and L b are in the cache, then we use the items of I ( iv ) only L b is in the cache, then we use L b on the WT to generate I
When one of the terms is not indexed by the WT, we retrieve its posting list from secondary memory if required. Notice is necessary to compute the intersection between the two posting lists L one or both of those lists could have to be retrieved from disk. As the lists L than the respective projection lists, storing L a and L b icant number of other projection lists that can be required by upcoming queries. The WT prevents from disk accesses by quickly computing the intersection without having to generate the lists L tends to increase disk accesses whereas the WT can fastly generate posting lists on-demand from main-memory. 5.3. Experimental results
The experiments were performed using a log of 36,389,567 queries submitted to the AOL Search service between March 1 query trace has 16,900,873 queries, where 6,614,176 are unique queries and the vocabulary consists of 1,069,700 distinct query terms. The results were obtained after processing 60% of the queries by using a discrete-event simulator described by considering the different costs involved in the processing of queries. These costs were determined from the programs executed for the experiments shown in the previous sections of this paper and benchmarks from Marin et al. (2010) for com-for efficient caching.

We simulated query processing on three P D search nodes configurations such that P D = 512. The configurations are 256 2,128 4 and 64 8. We assume that the baseline strategy is capable of keeping in its LRU list cache, for each was constructed using a 1.5TB sample of the UK web from 2005. We used the greedy heuristic proposed in Long and Suel (2005) to initialize each intersection cache by using the whole set of queries that hit the respective cache. Notice that most disk accesses performed by the baseline come from accesses to retrieve posting lists of non-local terms. Namely, hit ratio did not increase for other configurations in which more space was assigned to cache local posting lists.
The results cache size was set to achieve a 20% hit ratio. We have found this 20% hit setting convenient for our query log so we were able to achieve a good balance between the increased average number of terms that hit search nodes as discussed in Long and Suel (2005) and a large enough number of queries hitting the nodes during experimentation.
The results are shown in Table 3 and they show that the WT strategy reduces accesses to secondary memory and achieves better throughput ( Q / s ) than the baseline strategy (values normalized to 1). 6. Conclusions
In this paper we have presented an indexing and query processing strategy based on a novel adaptation of a self-indexed compressed text, that significantly improves performance of Web search engines implemented by using the standard array of P D processors. The proposed optimization involves both the accesses required to get the pieces of index used to rank the documents to be included in the top-K results and the accesses required to build the answer Web pages for queries. We achieve this by combining strategies related to caching of posting lists with self-indexed compressed text.
The experimental results show the following facts considering the space A occupied by a baseline strategy based on a compressed text plus a compressed inverted file, and the space B occupied by the wavelet tree (WT) strategy. For P =1, D = 1 and A = B , the throughput achieved by the WT strategy is about five times better than the baseline strategy. ment on P D search nodes. The results show that the WT strategy improves 40% overall query throughput over a baseline ity of the WT to fastly produce posting list intersections and snippets.
 References
