 Topic detection and tracking (TDT) automatically organizing a stream of news as it associate incoming stories with known topics. A topic is a seminal event with all sample stories. A story is  X  X n-topic X  if its events relate to the topic X  X  seminal event. 
Currently, most studies break each story into a  X  X ag of words X  and judge if the story is related to a topic by their overlapping words [1]. But it assumes that words are independent. It ignores that words in the same event have strong dependency relations important for describing the event. It causes the semantic confusion and words in different events can form wrong semantics. For example, the seminal event of topic 41002 in TDT4 is:  X  Kim Dae-jung received Nobel Peace Prize  X  and two related events are  X  South Korean President Kim Dae-jung won the Nobel Peace Prize for efforts toward reconciliation with North Korea  X  and  X  Chen Shui-bian sent telegrams of congratulations to Kim Dae-jung and forecasted cross-strait relations between Taiwan and mainland China  X . The  X  X ag of words X  can form wrong semantic:  X  efforts this are judged on-topic by mistake.  X  X ag of words X  can X  X  represent events accurately. 
An event can be captured by a few semantic elements, such as what, who, where and when [3], called keywords. Their relations are important for describing events, which can make the tracking easy. For example, a story is judged on-topic where  X  X im Dae-jung X  and  X  X obel Peace Prize X  depend on each other. But the story where  X  X obel Peace Prize X  and  X  X nited Nations X  have strong relations is judged off-topic. 
This paper proposes a tracking algorithm based on keywords dependency profile with three main characteristics. First, document summarization technology is utilized to select keywords, which finds indicative content of story [4]. Second, dependency relations of keywords are evaluated by their co-occurrence frequency in the same sentences. The relations are considered st rong if they often co-occur in the same sentences. Third, profile of each story is built basing on keywords, the dependency stories can describe the seminal event and topic tracking based on it is more effective. 
This paper is organized as follows: Section 2 introduces the related works. Section 3 describes the method to build the profile basing on co-occurrence of keywords. Section 4 describes how to use profile to complete topic tracking task. Section 5 and 6 discuss the experiment and results. Finally, we draw conclusions in Section 7. Most previous approaches in topic tracking ar e based on statistical strategy [5]. Some methods in pattern classification are utilized. CMU applies k-NN algorithm to topic tracking [6]. It selects k most similar stories of current story and relates current story to the topic which most of the k stories are related to. Binary classification [7] is also applied. But sparsity of annotated stories is a main factor limiting their performance. Another trend of research is to use Natural Language Processing technology. Named Entity Recognition (NER) [8] and time information extraction [9] are utilized [8]. POLYU [10] uses NER to build profiles of stories. It calculates information gains of NE to evaluate their importance, and selects words in important sentences as features. The profile is built by clustering features to different groups. But it ignores the relations among features which are important for describing the events. 
Some research finds the relations among words to improve performance. N-gram language model [11] is applied to topic tracking which considers words X  relations. Class-based models [12] use dependency relations to divide words to different groups. UMass [13] uses co-occurrence among NE in the same story to find their relations. But it doesn X  X  apply that to sentence which is a more integrated semantic unit. As mentioned above, keywords dependency relations can describe the core events of a story and co-occurrence of these keywords in the same sentences can reflect their dependency relations. For example, there are two stories: story1 :  X  X  X  X   X  X  X   X  X  X  X   X  X  X   X  X  X   X   X  X  X  ......  X  X  X   X   X  X  X   X  X  X  X   X  X  X  : X   X  X  X  X  ......  X  X  X   X   X  X  X  21  X  X  X  X   X   X  X  X   X   X  X  X  ......  X  X   X  X  X  X   X  X  X  X   X   X  X  X  X  X  X  X   X   X  X  X  X  Story1 introduces the reasons why Kim Dae-jung received Nobel Peace Prize. Story2 discusses activities between America and South Korea. In story1 ,  X  X im Dae-jung(  X  X  X  X  ) X  and  X  X obel Peace Prize(  X  X  X   X   X  X  X  ) X  (as the Chinese words with underlines in story1 ) mostly co-occur in the same sentences, so this story is related to topic 41002. In story2 ,  X  X im Dae-jung X  appears frequently and co-occurs with  X  X merica (  X  X  X  ) X ,  X  X outh Korea (  X   X  ) X  and  X  X orth Korea (  X   X  ) X , but seldom with  X  X obel Peace Prize X . So story2 is off-topic. 
On the whole, keywords and their dependency relations can describe core events in method to build the keywords dependency profile (KDP) of a story is introduced. 3.1 Selecting Keywords MEAD, an important method in document summarization, is utilized to select keywords in a story. The reason is that keywords are content which can describe events completely [3] and the goal of MEAD exactly is to find the indicative sentences of the story. It assumes that sentences similar to the centroid are more story. The score of each sentence is calculated as follows [4]: are parameters which are set to 1, 2 and 1 [4]. 
The sentence with the highest score is chosen. After word segmentation and removing stop words, the remaining words in that sentence are chosen as candidate  X  X resident X ,  X  X im Dae-jung X ,  X  X eceived X ,  X  X his year X ,  X  X obel X  and  X  X eace Prize X  X . 3.2 Building Keywords Dependency Profile Building the profile of a story by dependency relations of keywords includes three steps. Firstly, the network of dependency relations among candidate keywords in a n ( m l  X   X  tf(e k,l ) is the frequency that key k and key l co-occur in the same sentences. 
Secondly, the weights of nodes and edges are calculated. KDP determines weight of each node by two factors: z The more important are nodes which a node adjacent to, the higher is the weight z The more edges connecting to a node, the higher is the weight of this node. The 
The equation to calculate the weight of each node is: n co-occurrence frequency of keywords key k and key l in the same sentences. After calculation, the nodes in figure 1 with the highest weights are n 3 , n 4 , n 6 and n 7 . 
The rule to determine the weight of each edge is: the higher are weights of the important if it reflects dependency relation of two important keywords. For example, the weight of edge e 3,7 ,  X  X im Dae-jung X - X  Peace Prize  X , is high since the weights of nodes it connects to are the highest. The equation to calculate the weight of edge is: edges connecting to n k . e important keyword  X  X obel Peace Prize X  is mistakenly broken into  X  X obel X  and  X  X eace Prize X  by word segmentation system, they still depend strongly on each other ( e 6,7 ). Then weights of nodes and edges are normalized by dividing the total weights. 
At last, KDP of the story is built by appending contexts to nodes and edges. The C={ ) ( 1 n c of each node and edge. In this section, KDP is utilized in topic tracking which includes three steps: z Firstly, the topic model is built by profiles of all sample stories in the topic. The z Secondly, the profile of each incoming story is built according to the keywords z At last, the topic model and the profile of each incoming story are compared to 4.1 Building Topic Model each on-topic sample story. The edges set E T is formed in the same way. The contexts topic sample stories. W T , weights of all nodes and edges, is calculated next. 
A node X  X  weight is determined by the rule: the higher is the weight of a node in on-topic model. The idea is that nodes abundant in on-topic stories are important for definition of  X  X n-topic X . The weight of each node is calculated as follows: Where n k is a node in KDP T and ) ( k stories in it. S off contains KDP of off-topic sample stories. The nodes with weights less than zero are deleted. The weights of words in context of node are calculated similarly: Where n k is a node in topic model. ) ( contexts vectors in topic model, on-topic and off-topic sample stories. The weights of all edges and words in their context are calculated in the same way. At last the whole topic model is built up. 4.2 Building Profile of Incoming Story All keywords in topic model are extracted and then these keywords are utilized to build KDP of each incoming story by the method introduced in section 3.2. 
The nodes and edges in topic model are useful to describe the seminal event of the seminal event. So whether keywords in topic model appear in the story and whether their dependency relations are the same as those in the topic model is evaluated. Therefore, keywords in topic model are utilized to build the profile of each incoming story, instead of keywords selected from the story as introduced in section 3.1. 4.3 Measuring Similarity well as similarity between the story and notes set. Sim(T, S) . Its value is set to 0.9 according to experiments in section 6. The story will be judged on-topic if the similarity is higher than the threshold. Where e k,l is an edge. ) (
K and similarity in VSM. S word is its value of tf*idf . ) ), ( ( cos
In the numerator of equation (7), the third multiplier in numerator represents the similarity between the same edge in topic and story by its different contexts. The and an edge of the topic. Therefore equation (7) calculates the similarity between the story and the edges set of the topic. Sim X (N T ,S) is calculated in the same way. 5.1 Dataset Mandarin resource of TDT4 corpus is used as the training dataset and TDT5 as testing dataset. Both the TDT4 and TDT5 contain English, Mandarin and Arabic stories. TDT4 contains more than 20,000 mandarin stories collected from seven sources: newswire, radio and television sources. Radio and television sources were manually transcribed at closed-caption quality. TDT5 contains about 50,000 mandarin stories. 
TDT4 contains 53 annotated topics for mandarin stories and TDT5 contains 50, each of which was defined by at most 4 on-t opic sample stories and 2 off-topic ones. 5.2 Evaluation Metrics Each incoming story will be judged if it is related to given topics. The detection cost [2] is one way to evaluate the performance. C Miss and C FA , costs of a miss and false alarm, are 1.0 and 0.1 in TDT5. P targe and P probabilities of miss and false alarms. Where P Miss = #(Missed Detections) / #Targets , P
Fa = #(False Alarms) / # Non_Targets . Then C Det is normalized to (C Det ) Norm . 
The decision error tradeoff (DET) curve is another method to measure the performance. It X  X  a visualization of the trade-off between the P Miss and P FA . Each point on it corresponds to a P Miss and P FA with a certain threshold. The closer is a curve to the lower-left corner of the graph, the better is the system X  X  performance. The value that a system could reach at the best possible threshold. 5.3 Experiment Setup The baseline system is tracking system of Carnegie Mellon University [14]. In year 2004, this system won the first place in TDT5 evaluation. It used an improved its similarity with the topic model is higher than a fixed threshold.  X  is a parameter in improved Rocchio algorithm wh ich needs to be trained. 3.1 and  X  in equation (6). To verify the effectiveness of KDP algorithm, several systems are experimented on in section 6. Their abbreviations are: z KDP : Topic tracking system based on keywords dependency profile. z KDP-NoCacuWeight : System using KDP algorithm but using tf*idf of each node z KDP-NoOffTrain : System using KDP algorithm but building topic model just z KDP-NoTestProfile : System using KDP algorithm but building profile of each z KDP-NoContext : System using KDP algorithm but building the profile only with In this section, results of experiments introduced in section 5.3 are represented. and MAX are set as their values. The systems perform better with increase of N centroid because keywords selected by MEAD are more indicative when more words are first. This shows keywords dependency relations play a more important role than independent keywords in measuring similarity. The performance reaches best as equals 0.9 but not 1 because sparsity of the same edges between topic and story has a negative influence. But it can be mitigated by combination of edges and nodes. 
Figure 3 shows the training result of  X  in baseline. The system performs best when  X  0.8 and threshold equals 0.3. So 0.8 is set as  X   X  X  value. 
Next, several parts of the algorithm are in spected by systems introduced in section 5.3. Figure 4 shows their DET curves and table 1 shows their best results. The results show the KDP X  X  performance decreased after removing any part of the algorithm. The performance of KDP-NoCacuWeight system decreases by 0.88% comparing to KDP . This explains it reasonable to evaluate weights in KDP by the co-occurrence information. The reason is that if a keyword strongly depends on important keywords, it is also important to describe the core events. Also, their relations are important. The performance of KDP-NoOffTrain decreases by 12.82% comparing to KDP . This explains the importance of using off-topic sample stories in building topic model. On-topic sample stories can be used to extract valuable keywords and their relations. Off-topic sample stories can be used to reduce the weight of noises and their relations. 
The performance of KDP-NoTestProfile decreases by 12.90%. Building profile of incoming story by topic X  X  keywords is reasonable because it judges on-topic story by if seminal event is involved in story but not if core event in the story is seminal event. 
The performance of KDP-NoContext decreases by 27.84%. This proves contexts are important for building profile. A dependency relation of two keywords in different stories may express different meanings. So contexts of the same dependency relation in different stories must participate in the similarity measuring between two profiles. 
Table 2 shows the importance of each part in KDP in another form. It is completed by adding each part to the algorithm one by one and evaluating the improvement after adding each part. Context denotes the part to append context. TestProfile is the part to CacuWeight is the part to calculate the weights.  X  is the percentage of improvement comparing to baseline. The results show each part improves the system X  X  performance. 
Figure 5 and 6 show the performances of the baseline and KDP algorithm on the training and testing dataset. Almost all values of (C Det ) Norm in KDP are less than that in the baseline. KDP outperforms baseline by 13.25% on TDT4 and 7.49% on TDT5. 
At the same time, the speed of our system is faster than baseline. Time spending in processing one story is about 0.1 second in our system and 0.3 second in baseline. It shows that our system can get better performance without adjust topic model. But the baseline system has to adjust topic model continuously which leads to its low speed. This paper proposes a method to build the keywords dependency profile of topic by keywords co-occurrence frequency in topic tracking. It was found that the relations can describe events accurately. But building the profile with just keywords and their different meanings. This can be solved by appending contexts to them. Also, we find it preferable to reevaluate the importance of keywords by their dependency relations in the profile. Our algorithm outperforms baseline almost at all threshold. This proves the keywords dependency profile is a good representation of the topic and story. 
But the topic model using for describing the seminal event is sensitive to the quality of sample stories. In the future work, we will use incoming stories to reinforce the topic X  X  profile and reduce the profile X  X  sensitivity to sample stories. Acknowledgments. This research was supported by National Natural Science Foundation of China (60736044, 60503072, 60435020). 
