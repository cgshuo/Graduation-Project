 Search and browsing activity is known to be a valuable source of information about user X  X  search intent. It is ex-tensively utilized by most of modern search engines to im-prove ranking by constructing certain ranking features as well as by personalizing search. Personalization aims at two major goals: extraction of stable preferences of a user and specification and disambiguation of the current query. The common way to approach these problems is to extract infor-mation from user X  X  search and browsing long-term history and to utilize short-term history to determine the context of a given query. Personalization of the web search for the first queries in new search sessions of new users is more difficult due to the lack of both long-and short-term data.

In this paper we study the problem of short-term person-alization. To be more precise, we restrict our attention to the set of initial queries of search sessions. These, with the lack of contextual information, are known to be the most challenging for short-term personalization and are not cov-ered by previous studies on the subject. To approach this problem in the absence of the search context, we employ short-term browsing context. We apply a widespread frame-work for personalization of search results based on the re-ranking approach and evaluate our methods on the large scale data. The proposed methods are shown to significantly improve non-personalized ranking of one of the major com-mercial search engines. To the best of our knowledge this is the first study addressing the problem of short-term per-sonalization based on recent browsing history. We find that performance of this re-ranking approach can be reasonably predicted given a query. When we restrict the use of our method to the queries with largest expected gain, the re-sulting benefit of personalization increases significantly. D.4.6 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process By analyzing the search and browsing logs of a major com-mercial search engine we demonstrate that a considerable volume of queries has very little short-term contextual in-formation from the search session itself. These queries are challenging for the state-of-the-art short-term personaliza-tion methods based on search context , so we adopt a person-alization framework based on re-ranking using short-term browsing context . We also extract various statistics from browsing sessions of other users to improve ranking for the current user. Each re-ranking approach has its applicabil-ity limits and inappropriate employment could even harm the quality of a search engine. To cope with this issue we present a method of automatic filtering of queries suitable for context-aware personalization.

Our study is completely data-driven , i.e. both sets for training and evaluation are extracted from user logs.
To sum up, the contributions of the present study are:
The rest of the paper is organized as follows. In Section 2 we describe related work relevant to our research. We briefly describe the data employed for further evaluations in Sec-tion 3. In Section 4 we give examples of short-and long-term-based personalization cases and observe some statis-tics about typical search sessions to motivate our study. In Section 5 we describe in detail our method based on a com-mon framework for personalization. We discuss features em-ployed in our study in Section 6 and experiments themselves in Section 7. Finally, we conclude our research in Section 8.
Personalization of web search gains more and more inter-est in the last years. It has been realized that employment of contextual and personal information from browsing and search logs has the potential to significantly improve ranking quality.

The first studies on the subject mostly used short-term search context and long-term search history. In [15] Shen et al. incorporated previous queries and their click-throughs to specify the actual information need of a searcher. Authors analyzed several context-sensitive language models based on queries submitted in the same session and snippets clicked on by a user. All methods were evaluated on the TREC collec-tion augmented with click-through data. Xiang et al. [20] de-veloped Shen X  X  initial approach further by integrating learn-ing to rank ideas into short-term personalization. They rent query. The idea of a series of pages (queries) with the same intent is usually formalised in the notion of a browsing (search) logical session .
 Definition. Browsing (resp. search) logical session (or log-ical session ) is a subset of the user X  X  browsing (resp. search) log, consisting of intent-related pages, i.e. pages visited with the same or similar search goal.

The problem of accurate identification of logical sessions is rather hard and is a subject of independent studies [3, 9]. To avoid unnecessary complications most studies apply common convention: a set of successive browsed pages (sub-mitted queries) is attributed to one logical session, unless it is followed by 30 minutes of inactivity. This simple heuristic allows efficiently and effectively divide the whole history log into intent-related parts. It is an interesting research ques-tion (which is addressed in [3, 9, 18] for search sessions) how quality of the partition affects quality of the data extracted from logical sessions. However this question is out of the scope of our study and, following previous works, we divide the browsing log into sessions on the basis of 30 minutes in-activity timeout. Further these sessions are simply referred to as browsing sessions .

Similarly, the web pages visited by a user prior to for-mulating a query q in the same session are called browsing context and queries issued before q respectively are called search context .

Now we describe the data organization and information that is available in our study. Our personalization algo-rithm was trained and evaluated on the fully anonymized real browsing sessions collected from one of the major search engines via its special browser toolbar for 8 days. The tool-bar store URLs of all non-personal pages visited by a user and links followed during the browsing. It does not store texts of the visited web pages. In total we have collected 200M browsing sessions. 175M sessions from first 7 days were used for session statistics extraction, see Section 6. The last 8th day was used for the training/test dataset construc-tion: we keep all browsing sessions with at least one query and with at least one page in the browsing session preced-ing the first query. To purify our data for the 8th day we filtered out all browsing sessions with deliberately useless context, namely we deleted a query if all preceding browsed pages are private or from the same host as the search engine. Since our personalized ranking step and the final evaluation metrics rely solely on click-through data, we deleted all the queries with no clicks on their SERPs. After filtering and all purifications we sampled 500K browsing sessions for per-sonalization training and evaluation.
Let us start this section with an example of cases cov-ered by each of the long-and short-term personalization methods. First, assume that we have a researcher with spe-cialization in Information Retrieval submitted query  X  X RR X  into a search engine. Then he is likely to be interested in an article on  X  X ean Reciprocal Rank X . At the same time, if an identical query is submitted by an amateur sportsmen in UK, the search engine should return at the first position an article on  X  X anchester Road Race X . This is a simple exam-ple of an ambiguous query, which is covered by long-term personalization. Second, let the same query be submitted by a searcher for which a search engine has no complete in-formation about her personality, after reading a page with One obvious solution is to incorporate long-term history. It covers all queries of users with sufficient search history and in that way allows to cope with first requests in search sessions, though it does not help new users. Therefore we de-cided to analyze the utility of the preceding browsing session for short-term personalization. Figure 2 shows the distribu-tion of lengths of browsing sessions prior to formulating the first query. From this plot we conclude that the majority of first queries in search sessions has very rich browsing con-text: all except 41% of queries have at least one preceding web page in the same session. Although most queries have some browsing context, we are interested in the range of applicability of this data and restrict ourselves considering K = 1 ,..., 10 pages visited prior to the current query. In this section we describe our method of personalization. We start with presenting the general personalization frame-work based on a re-ranking approach [20, 8, 2]. This frame-work is rather popular due to the simplicity of its implemen-tation, the transparency of the ranking process and possi-bility of additional tuning. This framework is the core part of our approach and we also discuss its specifications. Es-sentially, the method runs in two steps: click prediction and re-ranking. Afterwards, we describe the additional query fil-tering step, which identifies requests that can be potentially enhanced by our personalization method.
Let L = { u,q, S ( q ) , C ( q ) } be a set of records from a click-through log: each record contains user X  X  u query q , search engine result page S ( q ) = { s 1 ,...,s 10 } and her clicks C ( q ) = { c 1 ,...,c 10 } on the SERP, c i  X  { 0 , 1 } . At the click prediction step we aim to recover c i for each sample of the form { u,q,s i } . Namely, a sample { u,q,s i } receives a posi-tive judgement if document s i was clicked by the user for the query q and gets negative judgement otherwise. Our goal is to learn labels c i from the set of features. That is a standard way to get personalized relevance judgements [2, 16, 11].
The features employed in this step are determined by the type of personalization performed and by the available data. For instance, in the case of long-term personalization we can leverage historical information about clicks, queries and pages visited by the user. On the contrary, for short-term personalization we use user X  X  actions in the search engine and browser during the current session right before formu-lating the query. Note that the effective combination of these methods is an interesting and challenging research ques-tion [18, 2], although it is not the subject of the current study. As we have mentioned, we restrict our attention to the first queries of search sessions, so only browsing context could by employed for short-term personalization. For these queries with lack of search context we consider K last web pages from the browsing session as a context:
B = { b K ,b K  X  1 ,...,b 1 } | {z } Browsing session
In general click prediction problem is very hard and ad-mits accurate solution in very restrictive cases (see e.g. [12]). However, we are interested in ranking quality of our predic-tor rather than in its recall, precision or perplexity. training set generation for query filtering step can be done automatically given click prediction features and its valida-tion set. So, learning of filter does not require any additional data and is specific for our re-ranking method.

Learned predictor f ( u,q,S ) allows subtle tuning of a per-sonalization method: re-ranking is applied exactly for the queries q with f ( u,q,S ) greater than some fixed threshold  X  . Setting  X  is equivalent to the choice of portion of queries affected by personalization.
In this section we fix some notations and give a detailed account of features derived from the available data. Let u be a user who started her search session with a query q . As-sume that pages B = { b K ,...,b 1 } were visited by u in the same logical session prior to formulating query q (see equa-tion (1)). As above, let S ( q ) = { s 1 ,...,s 10 } be a result page for q and C ( q ) = { c 1 ,...,c 10 } user X  X  clicks on the result page. We pick out up to 10 pages b i visited before q as the source of contextual information. The tuple { u, B ,q, S ( q ) , C ( q ) } is referred to as configuration . Our primary goal is to find the best ranking for every configuration. Each configura-tion plus a document from the result page s i forms a sample for the click prediction problem with ground truth c i .
Now we list all features used by our personalized ranker and give a brief motivation for their employment. In gen-eral, for accurate short-term personalization it is necessary to solve two problems: 1) understand whether available con-textual information is useful for the ranking of the current configuration, and 2) if so, find out which documents of S ( q ) are relevant to the contextualized information need. We ex-tract features for both of these problems and use them twice: during the click prediction step (Section 5.1) and during the query filtering step (Section 5.3). We report coverage of each feature, as it is important to understand the qual-ity of sparse features such us click-through-based statistics, see Table 2. Some of personalization algorithms utilize tex-tual information from long-and short-term data.However, a browser toolbar records visited URLs only and does not send the content of pages to the search engine in real-time (due to privacy and network latency related reasons).
We summarise employed classes of features as follows: data for the feature.
 Features 5, 6, 7, 11-22 were computed for each j  X  1 ,...K . Afterwards they were averaged over the most resent k browsed pages b j for each k  X  K . For example, feature log P O ( s i | b j ) results into K features: log P O ( s i | b 1 ) , 1 Thus, each feature 14-23 and its analogue among features 33-42 results into K features. If there are l browsed pages and l &lt; 10 for a given configuration, then we put aver-ages over all l browsed documents instead of averages over k &gt; l documents into the final feature vector. This re-sults into 17 + 25  X  K features, depending on the number of browsed pages under consideration. To measure the value of the browsing context, we collect the set of features for each K = 1 ,..., 10 and train 10 separate personalization algorithms.
During the evaluation step we compare our re-ranking al-gorithm with the baseline  X  non-personalized ranking of a commercial search engine. We stress that commercial en-gine is highly tuned, thus any (even relatively small) sig-nificant improvement of its results is notable achievement. There are several common methods of re-ranking evalua-tion. One way is to run some online comparison test i.e. present to users both rankings (e.g. interleaved, side by side or via AB-testing [13, 11, 10]) and measure their prefer-ences. However, since presenting possibly inferior results to users is highly undesirable, it should be supported by strong evidence originating from a certain offline evaluation step.
In our study we have conducted offline experiments based on the common click-through-based evaluation , see e.g. [5, 11, 2]. In this approach we assign positive judgements ex-actly to the results which were clicked on by a user and com-pute some standard click metric (e.g. average first click po-sition, mean reciprocal rank) before and after re-ranking  X  to compute the metric on a re-ranked list we assume that the user clicks the same set of documents in spite of re-ranking. The relative change of a click metric measures the quality of re-ranking. The method allows effective tuning of parameters and does not require any additional judge-ments or users X  effort besides implicit relevance feedback. However, click-through-based evaluation is known to have certain weak points. Since users tend to click on the doc-uments ranked higher in the result page independently on their relevance and to skip possibly relevant result in the bottom of the ranking list, this evaluation method just gives the lower bound on the algorithm performance, as was also pointed out in [16].
 Table 4: Improvements of click metrics ( K = 10 ) on the whole stream and on measurably different queries. *significant with p &lt; 0 . 01 (t-test). apply any smoothing or query filtering and report all metric changes on the test log. Further we use the similar way to analyze the benefit of personalization as [2, 16]. A query q is said to be measurably different if the original ranking and the personalized one have different MinRR scores. Obviously, the major part of queries is not affected by personalization, since the large portion of queries either has already the opti-mal value of a click metric (like the only click at top 1), or it is supported by a weakly relevant browsing context. How-ever, even in this case we detect significant improvements of click metrics on the full query stream for all models and affect up to 6% of all queries. Interestingly, in [2] authors re-port 5,42% of queries being affected by search session-based short-term personalization.

We re-rank documents according to the learned models and evaluate its quality. In Table 4 we report relative change of click metrics among all queries: min reciprocal rank, mean reciprocal rank and first click position. Due to proprietary nature of our data we do not report absolute values of the metrics. The numbers in the second column are improve-ments on the whole dataset and the numbers in the third column are improvements only over measurably different queries. Further we show how to increase the degree of im-provement by focusing on specific queries.
Now we report the impact of K  X  the number of pages extracted from the browsing session for each query on the performance of personalization. Intuitively the more pages we consider, the better the quality of the personalized rank-ing. At the same time we expect some saturation with the growth of K , since distant browsed pages give less and less new and useful information for personalization. To support these expectations for each K = 1 ,... 10 we extract the set of features from Section 6 and learn separate personalization algorithm. Improvements of MinRR on measurably differ-ent queries and fractions of affected queries as functions of K are shown on figures 3 and 4 respectively. Both plots con-firm monotonic behaviour of the functions. Thus, with the growth of browsing context both coverage of personalization and its quality increase. Further we consider only K = 10 as the most beneficial case.
Usually personalization is most beneficial on underspec-ified ambiguous queries (like  X  X RR X ), and it is widely ac-cepted to measure its value for a search engine by analyz-ing not the entire query flow, but those hard cases [2, 16]. We evaluated our personalization independently on naviga-tional/informational queries (according to our proprietary classifier) and on one word / two words / verbose (  X  3 words) queries. Relative improvements of click metrics are Table 6: Top 10 features according to the contribu-tion to the query filtering model reported in Table 5. All improvements are reported on mea-surably different queries.

As we expected, the one word queries receive significantly more gain than any other class of queries, because of their ambiguity. Expectedly, portion of navigational queries af-fected by personalization is relatively small. Though, sur-prisingly, its improvement (on measurably different queries) even more than for non-navigational queries. Reported met-ric improvements are comparable with other works on per-sonalization, see e.g. [16, 4].
We use every click prediction model learned on one of the 10 training folds to measure the change of MRR on the con-figurations from the corresponding validation fold. On this data we learn and tune query filtering using gradient boost-ing decision tree learning approach. The resulting function f ( u,d,q,S ) estimates expected gain from the personaliza-tion for the corresponding configuration.
In Table 6 we report contributions of top 10 features to the performance of the query filtering model f (  X  ). Char-acteristics of a query are still the most important for the query filtering model. Expectedly, features of the most re-cent browsed page b 1 are more valuable then others.
Personalization of search often harms the quality of rank-ing, thus one needs some tools to control its aggressiveness. We have implemented two methods: smoothing from Equa-tion (2) and query filtering from Section 5.3.

The smoothing just levels out the impact of re-ranking on all configurations and gives more weight to the original rank-ing. This approach reduces detriment of re-ranking, though it is not much known about its influence on the set of queries still affected by personalization. The extent of smoothing is controlled by parameter  X  . On the contrary, query filtering does not change re-ranking, but bounds the volume of con-figurations affected by personalization. To tune and control it, one chooses threshold  X  and applies re-ranking to config-uration { u,d,q,S } , if and only if f ( u,d,q,S ) &gt;  X  .
Dependence of re-ranking performance on smoothing pa-rameter is not as interesting, thus instead we report its de-pendence on a portion of measurably different configura-tions. On Figure 5 we demonstrate two plots: performance of the smoothing and the query filtering as functions of a these are exactly configurations with maximal possible ben-efit from personalization.
In the current paper we developed a framework for person-alization based on the short-term browsing context. Main target of this personalization are queries with the lack of short-term search session context, particularly, the first queries in the search sessions. We show, that, in the absence of the previous searches, the previously browsed pages provide a rich contextual data for the current query. Our person-alization algorithm comprises several orthogonal types of features and finally affects up to 6% of queries. The al-gorithm demonstrates significant improvements of the ba-sic click metrics over the competitive baseline on the real browsing log. To avoid undesirable harm of personaliza-tion on certain queries, we explore the problem of prelimi-nary query filtering and construct a framework for automatic model-specific query filtering. This filtering advances a com-mon smoothing method and significantly increases metrics on the impacted queries. As well as the most of the context-aware re-ranking algorithms our method has the largest po-tential for improvement on one word queries and informa-tional queries.

Our study can be developed in several directions. First, it will be interesting to estimate the range of applicability of filtering methods and evaluate them for other re-ranking algorithms. Second, the present work touches on the prob-lem of query formulation rationales. That is, prediction of the appearance of an information need during the browsing session and detection of the dissatisfaction with the current browsing session. In this study we have employed just sev-eral simple statistics to measure the probability of switching from a browsing session to a search session, so in the future we are planning to develop a more complicated approach to predict the emergence of an information need. [1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find [2] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais, [3] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [4] K. Collins-Thompson, P. N. Bennett, R. W. White, [5] Z. Dou, R. Song, and J.-R. Wen. A large-scale [6] J. H. Friedman. Stochastic gradient boosting. Comput.
