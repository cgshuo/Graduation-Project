 Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world ap-plications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of  X  X omparing apples to oranges X  under the large s-cale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing (RaHH), which provides a general framework for generating hash codes of data entities sit-ting in multiple heterogeneous domains. Unlike some ex-isting hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns op-timal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with im-proved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites, Tencent Wei-bo , and the other is an open dataset of Flickr (NUS-WIDE). The experimental results clearly demonstrate that the RaH-H outperforms several state-of-the-art hashing methods with significant performance gains.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Heterogeneous Hashing; Heterogeneous Similarity Search; Heterogeneous Network; Scalability; Big Data
With the fast growth of heterogeneous networks, especial-ly social media networks like Facebook , Flickr and Twit-ter , it has attracted increasing attention to study and ex-plore the interactions across heterogeneous domains. For example, image tagging aims at giving descriptive textual tags to images, recommendation on microblogging service focuses on providing relevant posts to certain users, and the targeted advertising is to send advertisements to potential customers via tracking their online traits. In general, these applications tend to find similar or relevant entities across different relational domains in heterogeneous networks. As those networks are often huge and associated with big data, there is an emerging need to design an efficient and scalable mechanism to evaluate heterogeneous similarity and search heterogeneous entities.

Hashing is a highly scalable indexing strategy for approxi-mate nearest neighbor search [1, 13, 23, 25]. It encodes data entities into binary hash codes in Hamming space, where the search can be extremely efficient. In addition, the learned hash functions are usually in a simple form and the gen-eration of hash codes can be done in a realtime manner. However, most of the existing hashing technologies are de-signed for homogeneous data, i.e., the data points indexed by a hash table should be of the same type. There are critical challenges for indexing heterogeneous data entities. First, different domains often have different characteristics, thus the corresponding mapped Hamming spaces should be also different. How to effectively bridge the gaps between those Hamming spaces and perform search across different hash tables? Second, it is often to see that there exist het-erogeneous relationships and connections between the data entities from different domains. How to leverage such re-lationships into the hashing function learning process? Al-though there are a few recent studies designing new hashing techniques to index multi-modal data entities into a common Hamming space [4, 26, 15, 20, 27], such a single-Hamming s-pace mapping strategy could be problematic and does not fit into real world scenarios due to the intrinsic heterogeneity of the multi-modal representations. In addition, the heteroge-neous relationships are very critical but have not been con-sidered by most of the existing approaches. Hence, rapidly searching similar data entities over heterogeneous domains s till remains as an open issue.

To address these challenges, in this paper, we propose a novel hashing technique, namely Relation-aware Hetero-geneous Hashing (RaHH), for indexing and searching large scale heterogeneous data. It utilizes the data features, the homogeneous relationship within each single domain, and the heterogeneous relationship across different domains to learn hash functions for each type of data entities, as well as optimal mappings between the hash codes of different types of data entities. In particular, we formulate the learning pro-cedure as a joint optimization problem with respect to both hash functions and mapping functions. An efficient Block-Coordinate Descent (BCD) strategy [21] is applied to derive optimal solutions. Finally we validate the performance of the RaHH approach on two large datasets; one is crawled from Tencent Weibo, and the other is an open dataset of Flickr , i.e. NUS-WIDE[5].

The rest of this paper is organized as follows. Section 2 briefly reviews the related works. The detailed formulation and solution of RaHH is presented in Section 3. Experi-mental validation on two real world datasets is presented in Section 4, followed by our conclusion in Section 5.
The rapid growth of the applications with big data in many areas, including social media, genomics, sensor net-works, and even business analytics, promotes the study of large scale search and retrieval. Due to computational and memory efficiency, hashing based indexing techniques have attracted more attentions in the recent decade. In partic-ular, many new methods are developed through leveraging sophisticated machine learning and data mining algorithms to boost up the search efficiency and accuracy. In this sec-tion, we will briefly survey the existing hashing techniques and motivate our study for designing heterogeneous hashing technique.

The earliest hashing methods, including the well-known locality sensitive hashing (LSH) [10] and MinHash [3], are based on either random projections or permutations, re-sulting in data-independent hash functions. Although the asymptotic property is theoretically guaranteed, the prac-tical performance is often limited [24, 25]. Realizing the limitations of the random techniques, many new hashing methods are designed through integrating either data prop-erties or supervision information to achieve compact hash codes with improved accuracy. Representative unsupervised methods include spectral hashing [25], graph hashing [17], iterative quantization hashing [8], isotropic hashing [12], an-gular quantization hashing [7], spherical hashing [9], and so on. The key idea for those data-dependent hashing methods lies in the exploration of data properties for hash function design. For instance, spectral hashing explores the data distributions and the graph hashing utilizes the geometric structure of data for designing data-dependent hash func-tions. Supervised learning paradigms, ranging from kernel learning to metric learning to deep learning, have been ex-ploited to learn binary codes and many supervised hashing methods are proposed [14, 16, 19, 22]. In addition, the semi-supervised hashing method was recently proposed to achieve accurate yet balanced hash codes [24].

Most of the aforementioned hashing techniques are de-signed for single type homogeneous features. Note that Figure 1: The flowchart of how RaHH is applied to a new query and get its similar items from different domains. many practical applications involve the usage of heteroge-neous feature representations. Recently, several new hash-ing methods are proposed to index the data points using multi-modal or multi-view feature representations [26, 27, 20, 4, 15]. For instance, Zhen et al. proposed to use a co-regularization framework to generate binary codes from each modality, while enforcing the agreement between the hash codes from different feature modalities [27]. As men-tioned earlier, most of these existing heterogeneous hashing methods attempt to project all the heterogeneous features or entities to a common Hamming space. However, such a Hamming embedding often results in poor indexing perfor-mance due to the lack of commonality among the heteroge-neous domains. In addition, besides heterogeneous entities, a typical heterogeneous network, like Facebook and Twitter , also contains heterogeneous connections or relationships be-tween those entities, which have not been really considered during the design of hash functions by most existing work-s. Hence, for such a social network, it remains as an open issue to design an efficient and accurate hashing technique which could leverage all available homogeneous and hetero-geneous information into the learning process. To address the above issues, in the following section, we will present a heterogeneous hashing technique.
In this section we will introduce our Relation-aware Het-erogeneous Hashing (RaHH) method in detail. As stated in the introduction section, the goal of RaHH is to learn a Hamming embedding for each type of data, and mappings between different Hamming embeddings such that we can get the corresponding hash codes in its relational domains. In this way, given a new query, we can use RaHH to fast re-trieve similar entities in its own data domain as well as sim-ilar data entities from other relational domains. Fig.1 pro-vides a conceptual diagram of the procedure of using RaHH to retrieve similar items from heterogeneous domains. We formulate the RaHH as a joint optimization problem over the homogeneous data hash codes and heterogeneous hash code mappings, in which we utilize data features, homoge-neous and heterogeneous relationships. Fig.2 demonstrates the leveraged information for learning the RaHH functions. In the following, the notations and symbols are first intro-duced and will be used throughout the paper. Then, we give the detailed formulation of the proposed RaHH method, fol-lowed by an efficient optimization strategy using the block coordinate descent approach. We also provide an out-of-sample extension for calculating the hash codes in an on-line setting. Finally, the complexity analysis for both offline training and online hash code generation are discussed. Figure 2: Different types of information we used in R aHH. Suppose we have a set of data items V = {V p } P p =1 from P relational domains, where V p = { v p i } m p i =1 is the dataset in the p -th domain with v p i being the i -th datum. We use X p = [ x p 1 , x p 2 ,  X   X   X  , x p m trix of the p -th domain, and d p is the dimensionality of the feature space of the p -th domain. H p = [ h p 1 , h p 2 ,  X   X   X  , h p -th domain, with h p i being the hash code vector for v ship matrix of the p -th domain, and R pq  X  R m p  X  m q is the heterogeneous relationship matrix between the p -th domain and the q -th domain. We assume H p can be mapped to H q the following optimization problem Here J ho is the homogeneous loss term, and J he is the heterogeneous loss term.  X  &gt; 0 is the tradeoff parameter.  X  is the set imposing constraints on H p . Now we will introduce in detail how these terms are defined in RaHH.
In order to construct J ho , we assume that: (1) data ob-jects with similar individual features are similar to each oth-er; (2) data objects with strong relationships are similar to each other; (3) similar data objects tend to have similar hash codes. For item (1), we can use the data inner product similarity matrices for each domain if we assume the data in all domains are normalized to unit norm. For item (2), we can use the homogeneous relationship matrix R p to capture the data similarity. Then we can construct the following composite data similarity matrix to encode the pairwise da-ta similarities where the constant  X  &gt; 0 is the combination weight.
For item (3), we can construct a smoothness term for the data hash codes to enforce that similar data would have similar codes. Specifically, we design the following J ho where A p ij is the ( i, j )-th element of A p . Clearly, when min-imizing J ho ( { H p } ), a larger A p ij will cause a closer h h . We further construct the constraint set  X  p as We impose the constraint H p 1 = 0 to preserve the balance of each bit, and H p ( H p )  X  = m p I to enforce that different bits capture complementary information, as suggested in [24, 25].
As the data from multiple domains might be associated with different metric spaces, we cannot measure the similar-ity between heterogeneous items directly. To search similar items from relational domains, RaHH first assumes that the hash code for a datum in domain p can be linearly mapped to the Hamming space of a relational domain q . Then the mapped hash code is used to search nearest neighbors in the domain q . More concretely, RaHH maps H p to each bit(row) of H q respectively through utilizing the heterogeneous rela-tion matrix R pq  X  R m p  X  m q . By treating H p as a feature matrix and H q k as class labels, we cast the mapping prob-lem as a series of binary classification problems and define J he ( { H p } , { W pq } ) as where p  X  q indicates domain p has relationship with domain q , and the logistic loss measures the prediction loss after we map the hash code of v to the k -th bit on the q -th domain. To minimize the loss, H kj and ( w suggests that for strongly associated v p i and v q j , the mapped hash code of v p i in the domain q should be as similar as the hash code of v q j .
Bringing Eq.(3), Eq.(4) and Eq.(5) together into the o-riginal cost function defined in Eq. (1), we can derive the final cost function. Due to the binary constraint expressed in  X  p , the cost function in Eq.(1) is not differentiable. More-over, the balance constraint also makes problem 1 NP-hard to solve [25]. Therefore, we propose to relax those hard con-straints and convert them into soft penalty terms. Specif-ically, we add the following three regularizers to the cost function, all-one vector, and I is an identity matrix. It is easy to see that these three regularizers correspond to the three relaxed constraint sets in {  X  p } . Then the relaxed version of the original cost function is To minimize the above cost J , we will present a Block Co-ordinate Descent (BCD) approach, as described in the fol-lowing.
Since the final cost function in Eq. (10) is not jointly convex with respect to all the variables, here we use BCD method [21] to search a local optimal solution. Specifically, the gradients are calculated as  X  J The gradient components in Eq.(12) are given as follows  X  J ho ( { H p } )  X  J he ( { H p } , { W pq } )  X  X  1 ( { H p } )  X  X  2 ( { H p } )  X  X  3 ( { H p } ) where { H p ( X p )  X  } , { H p 1 } and { ( H p ( H p )  X  statistics denoted by S , which will be used to accelerate the optimization algorithm.
 Algorithm 1 R elation-aware Heterogeneous Hashing (RaH-H) Require: { X p } , { R p } , { R p q } Ensure: { H p } , { W pq } 1: initialize { H p } by CVH and { W pq } as identity matrix 2: initialize S 3: while the value of objective function don X  X  converge do 4: for each domain p do 5: for each entity i in domain p do 6: calculate the gradients with respect to h p i 7: update h p i by one step gradient descent 8: update statistics S 9: end for 10: for each domain q do 11: for each bit k of domain q do 12: calculate gradients with respect to w pq k 13: update w pq k by one step gradient descent 14: end for 15: end for 16: end for 17: end while Algorithm 2 Ou t-of-sample Extension for Relation-aware Heterogeneous Hashing Require: s tatistics S , x p m Ensure: h p m 1: initialize h p m 2: while the value of objective function don X  X  converge do 3: calculate gradients with respect to h p m 4: update h p m 5: end while
Finally, we optimize the objective function by iteratively u pdating H and W until the value of objective function converges. We describe the training procedure in Algorithm 1.
It is critical to derive the out of sample extension for com-puting the hash code for any query datum in an online set-ting. In the formulation of the proposed RaHH, we can easily compute the hash code for an out-of-sample entity v i by minimizing Eq. (10). Since the hash tables are con-structed and the mappings { W pq } P p,q =1 are learned during the offline training process, we only need to minimize the cost in Eq. (10) with respect to the new entry v p i . Similar to the method introduced in Section 3.4, a gradient descent can be applied to efficiently compute the optimal hash code for the entity v p i . The detailed procedure for out of sample extension is described in Algorithm 2.
We first analyze the online complexity for computing the hash code of an out-of-sample query point v p i . With the s-tatistics S calculated previously, the time complexity of cal-culating gradients and updating for a single entity v p i where s p i i s the number of homogeneous relations connected with v p i , s pq i is the number of heterogeneous relations con-nected with v p i in domain q , and d p is the dimensionality of the features in domain p . We can see that the time com-plexity for generating the hash code for v p i is linear with respect to the number of relations connected with it. As the relations are often sparse, RaHH can be very efficient to generate the hash codes and can be applied to large scale and real-time applications.

During the training procedure, besides the computational cost of updating for single entities, the additional time com-plexity for updating statistics S is O ( d p r p + ( r p ) the time complexity of calculating gradients and updating W pq is O ( s pq r p r q ), where s pq is the number of heteroge-neous relations across domain p and q . Hence, the total time complexity of training RaHH is where s p is the number of homogeneous relations in domain p . We can see that the training time complexity is linear to the size of the training set and the number of relations. In practice, all the scale-free networks, like Facebook and Twitter , are often sparsely connected. Early study shows that the total connections for a reliable scale-free network is sublinear to the number of entities [6]. Therefore, in prac-tice, the training cost is with linear complexity to the num-ber of training entities, and the worst case in theory is with quadratic complexity when the network is fully connected. In summary, the proposed RaHH method has affordable of-fline training cost and the online hash code generation is extremely fast.
In this section we will present the experimental results on applying our RaHH algorithm to two real world data sets. First we will introduce the basic information of them. We run experiments implemented by Matlab on a machine running Windows Server 2008 with 12 2.4GHz cores, 32GB memory.
Tencent Weibo Dataset is crawled from Tencent Wei-bo 1 which is one of the biggest microblogging service in Chi-na. We use two domains, users and posts, in our evaluation. The dataset contains 34,676 users and 439,509 posts. Each user has 3 user labels 2 at least, and there are 4,385 user la-bels in total. Each post contains at most 140 Chinese char-acters. We use probability distribution on 50 topics detected by Latent Dirichlet Allocation (LDA) [2] from user labels as user feature vector, and friendship as homogeneous relation-ship for user domain. The post feature vector is construct-ed with the probability distribution of 50 topics detected by LDA on post, and no homogeneous relationship in the post domain is used. We use user-post adoption/rejection behaviors as positive/negative heterogeneous relationships between users and posts. User-post adoption behaviors are recorded when users post or forward posts. However, user-post rejection behavior, which is defined as a user does not http://t.qq.com
T he user labels are specified by the users themselves ac-cording to their personal interests and biography. like a post, cannot be observed explicitly, as we are not sure a user did not adopt a post because he(she) did not like it or just did not see it. Just like [11], based on the assumption that users will see the posts around the adopted posts, we assume t (which is set to 5 in our experiment) nearest un-adopted posts around adopted post on user X  X  Timeline 3 as rejected. We obtain 483,038 positive heterogeneous relation-ships and 1,039,441 negative heterogeneous relationships. NUS-WIDE Dataset is a fully labeled web image dataset. It contains about 260,000 images from flickr. Each image is labeled by at least one concept, and also described by text tags. We use the probability distribution of 100 L-DA topics on text tags as feature vector for text tags, and 500-dimensional Shift Invariant Feature Transform (SIFT) feature [18] as feature vector for images. No homogeneous relationship is used in either image or text tag domains. We compare RaHH with the following baseline algorithms.
Cross View Hashing(CVH) [15] performs multi-modal hashing via a Canonical Correlation Analysis procedure. In our implementation, we treat each positive relation as a da-ta object, and the two domains as two modalities. For nov-el queries, CVH can obtain their codes in both modalities based on their features.

Modified Spectral Hashing(MSH) is a straightfor-ward extension of Spectral Hashing(SH)[25]. We construct a unified similarity matrix. Similarity between entities in same domain is { A p } , and similarity between heterogeneous entities is positive part of R pq (i.e. ( R pq + | R pq | out-of-sample data, in order to exploit relation, we calculate hash codes by Nystr  X  om method.

Multimodal Latent Binary Embedding(MLBE) [26] is a probabilistic model which formulates hash codes as la-tent variables and learns them in discrete form. It can pre-serve both homogeneous similarity and heterogeneous rela-tion.

RaHH NR i s a variant of RaHH which does not exploit heterogeneous relation in out-of-sample extension.
RaHH NC i s a variant of RaHH without any regularizers in both training and out-of-sample extension procedures. Here CVH is a representative purely feature-based method. MSH and MLBE are representative relation-aware method-s. RaHH NR and RaHH NC are used to demonstrate the e ffectiveness of heterogeneous relations and regularizers.
We use precision, recall and Mean Average Precision (MAP) as our evaluation metrics. We propose two ways (i.e. global and entity-wise) to calculate precision and recall. Specifi-cally, let P and N be the set of positive and negative pairs of heterogeneous entities respectively, HD ( i, j ) be the Ham-ming Distance between data entities v i and v j , then the metrics are defined below. P osts are pushed to users in chronological order in Tencent Weibo, and the set of posts pushed to users in this order is called Timeline
We use two settings to evaluate the effectiveness of RaHH: (1) given user as query, and retrieve similar posts; (2) given post as query, and retrieve similar users. We conduct exper-iment on ten small scale subsets and one large scale subset. Each small set contains about 500 users and 500 posts, and the large scale set contains 19,330 users and 169,696 post-s. We adopt an iterative reduction strategy to obtain the datasets. All users and posts that are not involved in any adoption behaviors are deleted from the original data set. For all those data sets, we use 90% data as the training set, and the rest 10% data as the query set. There are four tradeoff parameters in the objective of RaHH as can be seen from Eq.(10).  X  controls the weight of heterogeneous relationship term, and  X  1 ,  X  2 ,  X  3 control the weight of regularizer terms. To obtain a good group of parameters, we did grid search, and change those four pa-rameters independently. Table 1 shows the mean MAP of the two retrieval settings using various numbers of hash bits. Parameter group 4 achieves the best performance, and we adopt those parameter values in the rest of the experiments. Table 1: Mean MAP of RaHH for different parame-t er groups on small scale subsets of Tencent Weibo Dataset using various numbers of hash bits. F igure 4: MAP curve on small scale subsets of Ten-cent Weibo dataset by varying the numbers of hash bits. F igure 5: MAP curve on large scale subset of Ten-cent Weibo dataset by varying the numbers of hash bits.
Figure 3 shows the precision-recall curve obtained on these datasets. Figure 4 and 5 show the MAP on those small scale subsets and large scale subset. MLBE and MSH requires large computational storage and cannot be tested on our large scale subset, we only report the results with RaHH and CVH. From these results we can see that RaHH significantly outperforms those baselines in all datasets. Some potential weakness of those baseline algorithms include: Comparing the performance of RaHH and RaHH NR, we c an observe that heterogeneous relationships are comple-mentary with homogeneous similarity and adding hetero-geneous relationships yields great improvement of perfor-mance. Comparing the performance of RaHH and RaH-H NC, we can see that RaHH NC overfits the training set, w hich leads to poor generalization performance on the query set. F igure 6: Time costs curve on small subsets of Ten-cent Weibo dataset by varying the numbers of hash bits. The time unit is second.

We also test the scalability of RaHH and baseline algo-rithms and the results are shown in Figure 6. Because of its quadratic complexity with respect to the training set size and sextic complexity with respect to the number of hash bits, MLBE costs most training time which is about one or-der of magnitude longer than our algorithm. RaHH costs more test time when the length of hash codes is small, how-ever, as the number of bits grows, MLBE costs more test time.

With the flexible mapping across domains, RaHH is al-lowed to learn hash codes with different lengths for different domains. Table 2 shows the empirical results on small sub-sets , where first column and first row are the lengths of hash codes for user domain and post domain respectively. We can see that MAPs in left down are larger than that in upper right which suggests that users tend to need more bits to represent than posts. One possible reason is that one user is often interested in more than one topic and one post often belongs to only one topic, users need more bits to represent their diverse interests. RaHH achieves best MAP when as-sign 32 bits for user domain and 16 bits for post domain. Comparing with the best result (assign 32 bits for each do-main) in uniform code length setting, the best result with different code lengths can achieve higher MAP and save 50% storage in post domain.
 Table 2: MAP of RaHH on small subsets of Tencent Weibo dataset for different pairs of bit numbers U ser
S imilar to the experiment on Tencent Weibo Dataset, we also designed two cross-domain retrieval settings on NUS-WIDE dataset: (1) given text (set of tags) as query, and retrieve similar images; (2) given image as query, and re-trieve similar texts. We select the images and their text tags belong to 10 largest concepts as our experimental database, then randomly select 300 images as training set, 10000 im-ages as test set, 2000 images as query set, and 5% hetero-geneous relations for training. We applied experiment on 5 such datasets. Figure 8 shows the average MAP curves, our F igure 8: MAP curve on NUS-WIDE dataset by varying the number of hash bits. algorithm outperforms the best baseline about 60% at most. Figure 7 shows precision-recall curve in the top two rows, and the curve of mean precision within Hamming radius 2 and corresponding success rate in the bottom row. Just like the results on Tencent Weibo dataset, the precision-recall curves of RaHH are much better than the others. When the length of hash codes is small(e.g. 8 bits and 16 bits), RaHH obtain better mean precision than the others due to both high success rate and precision in successful retrievals. MSH and MLBE obtain poor performance in retrieval set-ting(1) due to the low success rate. When the length of hash codes grows longer, as the success rate of retrieval decreases rapidly, the mean precision of most methods decrease to a low level that cannot be applied in practice. The mean pre-cision of MSH in retrieval setting(2) didn X  X  decrease, but, the mean precision of MSH is also too low to be applied in practice. F igure 9: Time costs curve on NUS-WIDE dataset by varying the number of hash bits. The time unit is second.

Figure 9 shows the training time and test time of different lengths of hash codes. MLBE is the most time-consuming method in most situations. The test time cost of one query in our method is about 30 milliseconds which is reasonable for real-time applications. I n this paper, we propose a Relation-aware Heterogeneous Hashing (RaHH) approach for efficient similarity search on large scale heterogeneous data. In particular, through lever-aging both homogeneous and heterogeneous relationships, we learn a Hamming embedding to construct hash tables for each data domain. Meanwhile, linear mappings are si-multaneously learned as bridges between different Hamming embeddings. To achieve efficient training process, a Block Coordinate Descent method is applied to derive optimal so-lutions. With this flexible hashing framework, we can cap-ture the characteristics of different data domains to gener-ate more effective and accurate hash codes. The empirical study demonstrates the superiority of the proposed RaHH method, compared to several state-of-the-art hashing tech-niques. Our future directions include applying sophisticated learning algorithms, e.g., kernel learning and metric learn-ing, to further improve the performance with an affordable training cost.
This work is supported by National Natural Science Foun-dation of China, No. 61003097, No. 60933013, and No. 61210008; International Science and Technology Coopera-tion Program of China, No. 2013DFG12870; National Pro-gram on Key Basic Research Project, No. 2011CB302206. [1] A. Andoni and P. Indyk. Near-optimal hashing [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] A. Z. Broder, M. Charikar, A. M. Frieze, and [4] M. Bronstein, A. Bronstein, F. Michel, and [5] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and [6] C. I. Del Genio, T. Gross, and K. E. Bassler. All [7] Y. Gong, S. Kumar, V. Verma, and S. Lazebnik. [8] Y. Gong and S. Lazebnik. Iterative quantization: A [9] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon. [10] P. Indyk and R. Motwani. Approximate nearest [11] M. Jiang, P. Cui, F. Wang, Q. Yang, W. Zhu, and [12] W. Kong and W.-J. Li. Isotropic hashing. In Proc. of [13] B. Kulis and K. Grauman. Kernelized [14] B. Kulis, P. Jain, and K. Grauman. Fast similarity [15] S. Kumar and R. Udupa. Learning hash functions for [16] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. [17] W. Liu, J. Wang, S. Kumar, and S.-F. Chang. Hashing [18] D. G. Lowe. Object recognition from local [19] M. Norouzi, D. Fleet, and R. Salakhutdinov.
 [20] J. Song, Y. Yang, Z. Huang, H. T. Shen, and R. Hong. [21] D. Sontag, A. Globerson, and T. Jaakkola.
 [22] A. Torralba, R. Fergus, and Y. Weiss. Small codes and [23] J. Wang, S. Kumar, and S.-F. Chang. Sequential [24] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised [25] Y. Weiss, A. Torralba, and R. Fergus. Spectral [26] Y. Zhen and D. Yeung. A probabilistic model for [27] Y. Zhen and D.-Y. Yeung. Co-regularized hashing for
