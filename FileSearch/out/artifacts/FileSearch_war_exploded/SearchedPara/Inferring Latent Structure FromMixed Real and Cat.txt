 Esther Salazar 1 esther.salazar@duke.edu Matthew S. Cain 2 matthew.s.cain@duke.edu Elise F. Darling 2 ed75@duke.edu Stephen R. Mitroff 2 mitroff@duke.edu Lawrence Carin 1 lcarin@duke.edu The inference of low-dimensional latent structure in matrix and tensor data constitutes a problem of in-creasing interest. For example, there has been a sig-nificant focus on exploiting low-rank and related struc-ture in many types of matrices, primarily for matrix completion (Lawrence &amp; Urtasun, 2009; Yu et al., 2009; Salakhutdinov &amp; Mnih, 2008). In that prob-lem one is typically given a very small fraction of the total matrix, and the goal is to infer the missing en-tries. In other problems, all or most of the matrix is given, and the goal is to infer relationships between the rows, and between the columns. For that problem, co-clustering has received significant attention (Dhillon et al., 2003; Wang et al., 2011). In co-clustering the rows/columns are typically mapped to hierarchical clusters, which may be overly restrictive in some cases. Specifically, there are situations for which two or more rows/columns may have a subset of (latent) charac-teristics in common, but differ in other respects, and therefore explicit assignment to clusters is inappro-priate. This motivates so-called mixed-membership models. For instance, in (Meeds et al., 2007) the au-thors develop a model in which each row and column has an associated binary feature vector, representing each in terms of the presence/absence of particular la-tent features. Rather than explicitly assigning cluster membership, the binary features assign  X  X ixed mem-berships, X  because rows/columns may partially share particular latent features. In (Meeds et al., 2007) the latent binary features are mapped to observed matrix elements via an intervening regression matrix, which is also inferred. Rather than using binary features to represent the rows and columns, one may also use a sparse real feature vector for each row and column (Salakhutdinov &amp; Mnih, 2008; Wang et al., 2010), as is effectively done in factor analysis (Carvalho et al., 2008). As noted in (Meeds et al., 2007), and discussed further below, the use of binary feature vectors aids model interpretation, and may also enhance data shar-ing.
 The Indian buffet process (IBP) (Griffiths &amp; Ghahra-mani, 2005) is a natural tool for modeling latent bi-nary feature vectors, and that approach was taken in (Meeds et al., 2007). The IBP is closely related to the beta-Bernoulli process (Thibaux &amp; Jordan, 2007), which implies that each row (subject) effec-tively selects a given feature i.i.d. from an under-lying Bernoulli distribution, with feature-dependent (but subject-independent) Bernoulli probability. It is expected that many subjects may be closely re-lated, and therefore these are likely to have similar latent binary features, with the same expected of the columns. This statistical correlation between sub-sets of rows/columns motivates the co-clustering ap-proaches, but for reasons stated above clustering is often too restrictive.
 To address these limitations of existing approaches, we propose a new model, in which the rows/columns are each characterized by latent binary feature vectors, as in (Meeds et al., 2007). However, a new construc-tion is used to model the binary row/column features, moving beyond the i.i.d. assumption that underlies the IBP. Assume the matrix of interest is characterized by N rows. For each of the latent features of the rows, an N -dimensional real vector is drawn from a zero-mean multivariate Gaussian distribution, with an un-known covariance structure. This N -dimensional real vector is employed with a probit link function to con-stitute an N -dimensional binary vector, manifesting the row-dependent binary value for one of the latent features. This is done for all latent binary features. By inferring the underlying covariance matrix, we un-cover the latent correlation between the rows, without explicit clustering. Multivariate probit models are uti-lized jointly for the simultaneous analysis of rows and columns; this embodies the joint analysis of rows and columns manifested by co-clustering, while performing such in a mixed-membership setting.
 For large N ( i.e. , many rows or columns), one must im-pose structure on the row/column covariance matrices, to achieve computational efficiency, and to enhance the uncovering of structure. This is implemented by im-posing a low-rank covariance matrix model, via factor analysis with sparse factor loadings (Carvalho et al., 2008). The rows (columns) that share non-zero values in the factor loadings are inferred to be statistically correlated, without the necessity of imposing explicit clustering.
 Bayesian model inference is performed, via efficient MCMC. The model is demonstrated on three real datasets. 2.1. Problem statement We consider data from N subjects, with the data in general a mix of categorical and real. There are M 1 categorical entries and M 2 real entries. The categorical data are represented as an N  X  M 1 matrix X , and the real entries are represented by the N  X  M 2 matrix Y ; we wish to analyze X and Y jointly.
 The transpose of the column vector x i represents the i th row of X , and the transpose of the col-umn vector y i represents the i th row of Y . Vector x i = ( x i 1 ,...,x iM 1 ) T contains categorical observa-tions where x ij  X  { 0 ,...,q j  X  1 } and q j corresponds to the number of categories associated with the j th component. 2.2. Factor analysis A q j -dimensional probit-regression model is employed for x ij . Specifically, assume that there is a feature below, the need to set K x disappears in the final form of the model). The observed multinomial variable x ij is modeled in terms of a latent variable  X  ij  X  R q j  X  1 such that where p = 1 ,...,q j  X  1, S j = ( s (1) j ,..., s ( q j R corresponds to the base category. Note that  X  j is a ( q j  X  1)  X  ( q j  X  1) covariance matrix, and the first element of  X  j is fixed to 1 in order to avoid identifia-bility problems (Chib &amp; Greenberg, 1998; Zhang et al., 2008). The covariance matrix  X  j infers statistical cor-relation between the q j possible categories associated with attribute j , and an inverse-Wishart prior is em-ployed for  X  j .
 The p th component of  X  ij is given by sents a feature vector associated with the choice p  X  { 1 ,...,q j  X  1 } for the component j of x i . A similar construction is employed for the components of the real matrix Y , without the need for the pro-bit link. Specifically, for row i and column j we re-spectively define real feature vectors a i  X  R K y and b with K y again disappearing in the final form of the model. 2.3. Binary row and column feature vectors Let r i  X  X  0 , 1 } K represent a latent binary feature vec-tor characteristic of row i in both X and Y . We assu-me that d j  X  X  0 , 1 } K represent the latent binary feature vec-tor associated with s ( p ) j , with c j  X  { 0 , 1 } K so defined for b j (for notational simplicity we write all binary vec-tors as being of same dimension K , but in practice the model infers the number of binary components needed to represent each of these vectors). We assume above constructions, we can rewrite (1) and (2) as Note that the need to set the aforementioned dimen-sions K x and K y has been removed, and what remains large upper bound on the number of binary features needed to represent the rows and columns, with only a subset of these K variables inferred as important when performing computations.
 The advantage of this construction is that real feature terms of binary feature vectors, with the regression matrices M ( X ) and M ( Y ) between the binary and real vectors shared for all rows and columns. This im-poses significant structure and sharing on the learning et al., 2007). This paper differs from (Meeds et al., 2007) in three ways: ( i ) we jointly consider real and categorical data jointly, ( ii ) we impose low-rank struc-ture on M ( X ) and M ( Y ) (discussed next), and ( ii ) a new framework is developed for modeling the binary 2.4. Low-rank regression matrices We model M ( X ) and M ( Y ) as low-rank matrices: with  X  ( X ) l  X  X  0 ,  X  (0 , X  2  X  ), corresponding to a truncated normal distribution, over (0 ,  X  ), with an inverse-gamma prior on  X  2  X  ;  X  ( Y ) l is defined similarly. The ilarly, and we discuss one in detail, for conciseness. Specifically, we draw u ( X ) l  X  N (0 , I K ), where I the K  X  K identity matrix. The variables b ( X ) l and b l are binary, and they allow inference of the as-sociated matrix rank. Again illustrating one of these for conciseness, we employ a sparseness-inducing beta-Bernoulli representation, with b ( X ) l  X  Bernoulli(  X  ( X ) ), with  X  ( X )  X  Beta(1 /K, 1).
 Using, for example, (7) in (5), we observe that the model imposes  X  where &lt;  X  ,  X  &gt; corresponds to a vector inner product, and S defines the set of indices for which b ( X ) l Note that for the probit link function this construction implies that we do not require random-effect terms for the subjects and attributes (as is typically done when using real feature vectors (Wang et al., 2010)), since the sum of terms in (9) automatically allow random-effect terms, if needed (such will correspond to one of the terms in the sum).
 From (9), and considering (1), note that we may rep-resent v i as a vector composed of for l  X  S ; s ( p ) j is similarly defined by d j , v construction in (7), we infer K x to be the size of set S (rank of M ( X ) ). As discussed when presenting results, the low-rank construction in (8) allows us to similarly infer K y . This property is a principal reason for impos-ing low-rank structure on M ( X ) and M ( Y ) , it aiding interpretation of the model results. We describe in detail the proposed modeling of r i with a similar construction employed for { d ( p ) j { c j } . A sparse multivariate probit model is imposed:  X  k  X  N (0 ,  X  ) , with r ik |  X  ik = 1 if  X  ik &gt; 0, and r ik |  X  ik = 0 otherwise; k = 1 ,...,K , with  X  ik the i th component of  X  k , and r ik is the k th component of r i . Marginally, r ik  X  Ber(  X  ik ) where  X  ik = Pr(  X  ik &gt; 0). The covariance matrix  X   X  R N  X  N imposes an under-lying correlation structure between ( r 1 k ,...,r Nk ), for all binary features k .
 We must now place a prior on the covariance matrix  X  . A large class of models impose sparsity on the inverse of  X  ( i.e. , the precision matrix), corresponding to a sparse Gaussian graphical models (GGM). The GGM approach for covariance matrix estimation is attrac-tive and many approaches have been proposed (Atay-Kayis &amp; Massam, 2005; Dobra et al., 2011). Alter-natively, other approaches have been proposed for di-rectly modeling the covariance matrix, placing shrink-age priors on various parameterizations of  X  . For in-stance, (Liechty et al., 2004) considered shrinkage pri-ors in terms of the correlation matrix, and (Yang &amp; Berger, 1994) used reference priors based on the spec-tral decomposition of  X  .
 We choose to directly model the components of  X  . In fact, given that N (dimensionality of the covariance matrix) is typically larger than K , standard estimators are liable to be unstable (Sun &amp; Berger, 2006; Hahn et al., 2012). Hence, we impose a factor structure on a covariance matrix, in a similar fashion to (Hahn et al., 2012). Such regularization is crucial when the number of variables is large relative to the sample size, and also when the covariance corresponds to an unobservable latent variable (Rajaratman et al., 2008).
 We assume that cov(  X  k ) = BB T +  X  where  X   X  R N  X  N is a diagonal matrix, rank( B ) = K &lt; N and B  X  R N  X  K . That construction implies that  X  k  X  N ( Bf k ,  X  ) where f k  X  X  (0 , I K ). The matrix B must be constrained to be zero for upper-triangular entries and positive along the diagonal, to avoid identifiability problems. Further,  X  is fixed to be the identity to allow a simple identification strategy. The prior on the loadings B is given by with v i  X  IG( c/ 2 ,cd/ 2) and  X  i  X  Beta(1 , 1), with IG an inverse-gamma distribution and  X  0 a unit point measure concentrated at 0. The sparsity prior per-mits some of the unconstrained elements in the factor-loadings matrix B to be identically zero. An approximation to the full posterior of model pa-rameters is performed based on a Gibbs sampler, with Metropolis-Hastings updates for a subset of the param-eters. We now briefly describe how to sample some of the most interesting parameters, based on their full conditional posterior distributions. m l = V  X  ( d j ,..., d  X  (  X  i 1 ,...,  X   X  In order to sample r ik  X  { 0 , 1 } , i = 1 ,...,N , k = 1 ,...,K , let y i =  X  M Y r i + i and  X   X  M  X  ( r ik | X  )  X  Bernoulli( p 1 / ( p 1 + p 2 )) , where p 1 =  X   X  ik ) exp { X  0 . 5(  X   X  2 y y e i = y  X  ik = Pr(  X  ik &gt; 0). Samples for c jk , d b l are obtained similarly.  X  The parameter-extended Metropolis-Hastings algo-rithm is employed to sample  X  j given the restriction {  X  j } 11 = 1 (Zhang et al., 2008) only when q j &gt; 2, otherwise  X  j is fixed to one. Considering a Wishart prior  X  j  X  W( m 0 ,  X  j ), the algorithm is as follows: (1) at iteration t , set the values ( R ( t ) j ,D ( t ) j relation matrix and D ( t ) j the diagonal variance ma-trix with the first element equal to one. (2) Generate D j is a diagonal matrix without restrictions. (3) Accept the new values (replacing { D  X  j } 11 = 1) with p ( R j ,D j | X  ) is the joint posterior distribution and q (  X |  X  j ) is the proposal distribution given by the prod-uct of the Jacobian term for the transformation from  X  j to ( R j ,D j ) and the Wishart density W ( m,  X  j 5.1. Analysis of the animals dataset We first test the performance of the proposed model on the animals dataset (Kok &amp; Domingos, 2007; Sutskever et al., 2009). This consists of 50 animal classes and 85 binary attributes (with no missing data). Note that in this experiment we only have a categorical (binary) observation matrix X  X  X  0 , 1 } 50  X  85 .
 The model is fitted using the proposed MCMC scheme. We ran the algorithm considering 20,000 iterations with a burn-in of 5,000 draws, and we collect ev-ery third sample that give us a total of 5,000 saved samples. The analysis was performed with K = 20, c = d = 1, and  X  2  X  = 1 (many other similar settings yielded similar results). For the sparse probit factor model (discussed in Section 3) we consider six factors; larger models are possible and were considered, how-ever in our experiments we noticed that less than six factors were enough to capture the underlying corre-lation structure of the binary features. Indeed, only three and four of the columns of B r and B d , respec-tively, have non zero elements. In general, the results are very insensitive to the setting of K , as long as it is set relatively large.
 We examine the latent correlations (between the rows and columns) learned by the model by inspecting the most likely sample produced by the Gibbs sampler. Figure 1 shows the latent correlation structure be-tween { r i } i =1 ,..., 50 learned for the animals as well as between { d j } j =1 ,..., 85 learned for the attributes. By the analysis of those correlations, we are able to iden-tify hierarchical clusters and affinities between rows and columns; we use a clustering algorithm (Kaufman &amp; Rousseeuw, 1990) to identify row and column hi-erarchical cluster structure based on the inferred cor-relation matrix (this is done for illustration; we do not perform clustering when implementing the model, rather the full covariance between rows and columns is inferred). The hierarchical clustering algorithm yields a dendrogram, plotted jointly with the learned corre-lation matrices in Figure 1. The closeness of any two clusters is determined by a dissimilarity matrix I  X  R where R is the correlation matrix (see Eisen et al., 1998; Wilkinson &amp; Friendly, 2009, for more details). The learned groups are described on the right panel of the figure. Some interesting interpretations are de-rived from the correlation structure for the attributes. For example, cluster G1 (which includes attributes of marine animals) is highly correlated to cluster G8 and negative correlated to cluster G10 and G11 (which in-cludes attributes like quadrupedal, ground and moun-tain). 5.2. Senate voting data We next examine a binary vote matrix from the United States Senate during the 110th Congress, from Jan-uary 3, 2007 to January 3, 2009. The binary matrix, X , has dimension 102  X  657, where each row corre-sponds to a senator and each column corresponds to a piece of legislation; X is manifested by mapping all  X  X es X  votes to one and  X  X o X  votes (or abstentions) to zero. The percentage of missing values is about 7.1%. We perform analysis of the voting data consid-ering K = 50. We use the same priors and MCMC setup considered in the previous application. We in-ferred that there are approximately 10 binary features for the senators, 13 for the legislation, and M ( X ) had a rank of K x  X  4, with one dominant factor, with dominant corresponding  X  ( X ) l (consistent with related research that indicates one dominant factor for such data (Wang et al., 2010)). Figure 2 shows the dendro-gram derived from the correlation matrix associated with the senators, to illustrate the clustering of people. The correlation matrix reveals significant differences between two groups of senators, which are constituted by Democrats and Republicans. As an example in-terpretation of the dendrogram for the senators, note that Republican senators Collins, Snowe and Specter are inferred as being at the  X  X dge X  of the Republican party, nearest to the Democrats; these were the only Republicans who joined most Democrats in voting for the 2009 Economic Stimulus Package (and Specter later switched to the Democratic party). Also, Barack Obama and Hillary Clinton, who competed closely for the Democratic presidential nomination in 2008, are very weakly correlated with any of the Republicans. In Figure 2, middle panel, we show the reordered vot-ing data matrix X . The matrix was reordered by rows and columns according to similarities learned from the correlations matrices associated with the senators and legislation. The matrix reveals interesting pat-terns. For example, the first 300 columns are primar-ily Democrat-sponsored legislation, the following 200 legislation are primarily Republican-sponsored legisla-tion, and the last columns are unanimous votes, for things like nominations for various government posts. We performed LDA (Blei et al., 2003) topic model-ing on the text documents (separate analysis), to in-fer structure in the legislation, and help interpret the inferred relationships; three types of legislation so in-ferred are shown in Figure 2.
 We compare our results with two related models. The first follows the proposed construction, except that the latent binary vectors are modeled via an IBP; the sec-ond is the logistic binary matrix factorization (BMF) model (Meeds et al., 2007); the main difference be-tween the first and second alternatives is that the for-mer imposes the low-rank model of Section 2.4. Fig-ure 2, right panel, shows the average fraction of correct predictions for each model as a function of the fraction of missing data (held-out data, averaged over 15 runs). These results reveal the advantage of the low-rank con-struction (by comparing the two IBP solutions), and of the imposition of correlation in the latent binary fea-tures (omitted in the two IBP-based constructions). 5.3. Behavioral dataset The behavioral dataset comes from a survey conducted by the Duke Visual Cognition Lab during 2010 and 2011 (details omitted here to keep authors anonymous during review). The 508 responders were members of a university community, answering different types of questionnaires; the questions regarded media multi-tasking (MMI), an attention deficit hyperactivity dis-order (ADHD) test, the Autism Spectrum Quotient, eating attitudes (EAT) test, video games (VG) activ-ities, a NEO-AC personality inventory (neuroticism, extraversion, openness, agreeableness, conscientious-ness scores) and Barratt Impulsivity Scale (BIS-11); almost all of these questions come from standard sur-veys in the respective areas (discussed further below). The total dataset consists of M 1 = 20 categorical and M 2 = 106 real-valued questions. Among the 20 cate-gorical variables considered in the analysis, there are 16 binary observations and 4 variables with more than 2 nominal categories. Concerning the real-valued ob-servations, the 106 studied variables were classified as follows: 40 variables related to VG-playing habits, 23 variables related to passtime activities, 30 associated with the NEO-AC facets, 5 autism subscales, 3 im-pulsivity subscales, and the last 5 variables related to EAT score, MMI, age, years of education and ADHD score. The percentage of missing values is approxi-mately 13%.
 We perform a joint analysis of the categorical and real-valued data matrices considering K = 50. The real-valued data matrix Y was column-normalized to zero mean and unit variance before the analysis. In addi-tion,  X  2  X  = 10, m 0 = 8,  X  = I q j  X  1 and c = d = 1. The MCMC algorithm was run for 50,000 iterations, with the first 25,000 discarded, and then every 5th collected to produce a posterior sample of size 5,000.
 Figure 3 shows the approximate posterior distribu-tion for the number of features associated with ques-tions (categorical and real-valued answers) and people. From these results we note that approximately 8 and 6 features in d ( p ) j and c j are used by the model, while there are approximately 20 binary features inferred as associated with the people.
 In a similar fashion to the analysis of the Animals dataset, we analyze the learned correlation matrices associated with the questions and people. Figure 4 shows those matrices with the rows and columns or-dered such that similar rows and columns are near each other. In the vertical margin appears the hierarchi-cal cluster tree derived from the correlation (as dis-cussed above). Based upon these results, we are able to identify blocks of correlated questions and clusters of people. Interesting interpretations can be derived from these results. For example, men are highly cor-related with fighting and real time strategy VG and negatively correlated with normal vision and monolin-gual. Figure 4, right panel, displays the learned cor-relation structure between people. It shows three big clusters; G1 is primarily composed of women (82.3% women, 17.7% men), G2 represents a heterogeneous-gender group (59% women, 41% men), and G3 is pre-dominantly men (20.6% women, 79.4% men).
 We are also interested in the analysis of questions re-lated to behavior scores like the NEO-AC characteris-tics, autism, ADHD, EAT and MMI. The analysis of these variables is of particular interested in Psychol-ogy, where the Big Five factors of personality traits (McCrae &amp; John, 1992; Costa &amp; McCrae, 1992) has emerged as a robust model to describe human person-ality. Specifically, the five factors are directly related with the NEO-AC data (real, non-categorical answers, represented in Y ) and we seek to connect our inferred latent features with what is known from Psychology, to analyze the quality of inferred structure. From (6) and (8) we have that y ij = P l  X  X   X  ( Y ) l &lt; r i , u c , v ( Y ) l &gt; +  X  ij , and therefore from (2) we may express the l th component of b j as b jl = this corresponding to the factor loading for question j , factor l . Considering the most likely sample in the Gibbs sampling, we infer b j  X  R 6 , with K y = 6 factors coming from the rank of M ( Y ) .
 Figure 5 shows a diagram where groups of questions are associated to the 6 inferred factors. The plot shows connections between factors and questions in terms of the major values on each factor loading. An interesting finding is that the model uncovered the proper num-ber of factors, i.e. , five factors that group thirty facets of personality and an additional factor that groups autism scores (to our knowledge, this is the first quan-titative analysis of this sort that demonstrates that the question in these questionnaires indeed capture the aspects of personality intended by subject-matter experts in their design). The first five features are clearly related to personality traits, each of them in-volving different facets of neuroticism (N), extraver-sion (E), openness (O), agreeableness (A) and con-scientiousness (C). Autism scores like communication, social skill and imagination form an additional inde-pendent factor. Also, impulsivity scores belong to the factor associated with the conscientiousness charac-teristic but with negative values. 5.4. Computations The code for these experiments was written in Mat-lab, and the computations were run on a computer with 2.53GHz processor and 4GB memory. To give a sense of computation times, for the Behavioral dataset considered above, approximately 11 seconds were re-quired per MCMC sample.
 A new model has been developed for representing real, categorical and mixed real-categorical relational data. A multivariate probit model was employed, jointly im-posing correlation between the subjects and between the attributes. These covariances were used in the ex-periments to infer hierarchical structure between the subjects and between the attributes. Encouraging re-sults were demonstrated on three real-world data sets, the last of which is new, characterized by mixed real-categorical survey data for several interesting psycho-logical conditions.
 The research reported here was supported by ARO, ONR and DARPA (under the MSEE Program).

