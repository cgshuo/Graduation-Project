 Tian Huang 1 , Yongxin Zhu 1( Time series discords are the subsequences of a time series that are maximally different to all the rest subsequences [ 8 , 11 ]. Discord can be found by comput-ing the pair-wise distances among all subsequences of a time series. Recently, finding time series discord has attracted much attention. The definition, despite its simplicity, captures an important class of anomalies. Its relevance has been shown in several data mining applications [ 3 , 9 , 13 , 16 , 19 ]. have been proposed to speed up discord discovery, but they are still time con-suming when the datasets are large. For example, to discover the top discord from an ECG time series that contains 648 K data instances, a Java implemen-tation of HOTSAX on a modern PC costs about 20 min, which is close to the time period of the ECG time series. In fact, any dataset in existing literature of memory based methods contains no more than 64 K data instances. We believe the time consumption is one of the main limitations to discover the discords of larger time series.
 Low utilization of computing resources is another issue when discovering discords from large datasets. A disk-aware method [ 20 ] discovers discords from 100-million scale time series, which can only be fitted onto hard disks. Disk I/Os take more than 50 % of the total time consumption, leaving the computing resources in idle state for most of the time during the discord discovery. Parallel computing is a potential way to mitigate these issues. However dis-cord discovery is hard to parallelize. The results discovered from segmentations of a time series are non-combinable [ 11 ], which means divide-and-conquer method-ology may leads to incorrect results that do not conform to the original definition of discord.
 To mitigate the above issues, we propose Parallel Discord Discovery (PDD), which divides discord discovery problem in a combinable manner and solves its sub-problems in parallel. PDD stores a long time series in distributed memory of multiple computing nodes. These nodes work together to reduce the time con-sumption of discord discovery. As far as we know, this is the first work that discov-ers time series discords in parallel. Our contributions are summarized as follows:  X  We accelerate discord discovery by harnessing multiple computing nodes.  X  We ensure the correctness of the results by dividing discord discovery problem in a combinable manner.  X  We improve the utilization of computing resources in large scale discord dis-covery by using an in-memory computing framework.
 We implement PDD using Apache Spark [ 18 ]. Experiments show that given 10 computing nodes, PDD achieves 7 times speedup against HOTSAX. PDD is able to handle larger datasets than traditional memory based methods. PDD achieves nearly twice the utilization of computing resources compared to the disk-aware method [ 20 ].
 The rest of the paper is organized as follows. Section 2 presents related works and their issues. Section 3 analyzes the feasibility of parallelization and describes the detailed implementation of the Parallel Discord Discovery (PDD) method. Section 4 presents an empirical evaluation of PDD. Section 5 draws the conclusion. Discords can be discovered by comparing every pair of subsequences with two-layer nested for-loops. The outer loop considers each possible candidate subse-quence, and the inner loop is a linear scan to identify the non-overlapping nearest neighbor of the candidates. The computational complexity of a naive discord discovery method is O ( m 2 ), where m is the size of the dataset. Various meth-ods are proposed to accelerate discord discovery. Keogh et al. propose HOTSAX [ 11 ], which applies heuristic sorting techniques and early abandon technique to reduce the computational complexity. The heuristic order of the outer loop helps HOTSAX visit the most unusual subsequences in the first few iterations, while the heuristic order of the inner loop helps HOTSAX visit the subsequences that are similar to the current candidate subsequence. A conditional branch in the inner loop helps HOTSAX skip the calculations of the normal subsequences. HOTSAX speeds up by three orders of magnitude compared with the naive method. heuristic inner and outer order of the nested loop. They reduce the dimension-ality of time series data and the computational complexity of discord discovery. indexes to find nearest neighbor distance more efficiently and reduce the com-putational complexity of discord discovery.
 be stored in the memory of a single computer. Yankov et al. propose a disk-aware method [ 20 ] to deal with the time series whose scale grows beyond the capacity of the memory of a single computing node. The method improves the efficiency of the disk I/O operation by linearly scanning the disk to obtain all subsequences. The disk-aware method eliminates the limitation of the memory based method in terms of the scale of datasets.
 of computing power and storage of a single computing node. As the scale of time series increases unceasingly, the utility of discord discovery deteriorates. Besides, the disk-aware method suffers from the low utilization of computing resources. cord discovery is hard to parallelize. The results of discord discovery is non-combinable [ 11 ]. In other words, divide-and-conquer methodology may lead to incorrect results, which do not conform to the original definition of discord. (PDD). PDD enables accelerating discord discovery with multiple computing nodes. We implement PDD with Apache Spark so that PDD has better utiliza-tion of computing resources than the disk-aware method. To parallelize discord discovery, we must divide the problem into independent sub-problems, and solve each sub-problem respectively in different computing nodes. [ 11 ] states that divide-and-conquer methodology may yield incorrect results. We need to find another feasible way to partition discord discovery prob-lem. To better analyze the feasibility of parallel time series discord discovery, we review the concept of discord, analyze the communication computation ratio and the parallelism. 3.1 Dividing the Problem Before analyzing the formal definition of discord, we describe a set of preliminary notations [ 11 ]. We use T to denote a time series t 1 , ..., t of the time series, t i  X  R . We denote a subsequence of a time series t , ..., t Because subsequences of the same length are compared in a discovery of discords, we use C p as an abbreviation of C p,n in the rest of the paper. Since every subsequence could be a discord, we will use a sliding window , whose size is equal to the length of subsequences, to extract all possible subsequences from The process of time series discord discovery can be expressed by the following equation: p indicates the start position of the first discord. nnDist neighbor distance of a subsequence C p . n is the length of the sliding window. stands for the length of the time series. argmax in Eq. ( 1 ) means the subsequence with the largest nnDist , which is the discord of the time series. AsshowninEq.( 1 ), the processes of solving each the nearest neighbor dis-tance nnDist ( C p ) of each subsequence C p is independent. Therefore, we divide discord discovery into independent sub-problems. Each sub-problem finds the nearest neighbor of the subsequence C p and the distance between them. The results of all sub-problems can be combined. The subsequences with the largest nnDist is the discords of the time series. 3.2 Improving Computation Communication Ratio To find the nearest neighbor of a subsequence C p , we transmit non-overlapping [ 6 ] subsequence to one or more computing nodes to calculate the distance between them. However, the time consumption of transmitting two subsequences and computing the distance between them is of the same order of magnitude. This will results in low Computation Communication Ratio (CCR) and therefore low utilization of computing resources. We have to improve CCR. We utilize the overlapped region between two adjacent subsequences to improve CCR. We transmit continuous data instances so that the overlapped region can be reused by multiple subsequences. Taking the sliding window of length n = 100 as an example, the transmission of the first 100 data forms one subsequence. Each transmitted data instance forms a new subsequence with the previous transmitted 99 data instances. If we transmit 299 continuous data instances, we get 299  X  100 + 1 = 200 subsequences of length CCR is improved by a factor of 200 299  X  1 100  X  67. 3.3 Overview of PDD We divide discord discovery in a combinable manner. Next we design Parallel Discord Discovery (PDD). We first present the overview of PDD.
 sequences of a time series. b is the number of continuous subsequences in each transmission. This method has two steps. First, PDD estimates global the discord with Distributed Discord Estimation (DDE) method (line 2 X 3). Then PDD linearly scans C to find the nnDist of the true discord of the time series. This linear scanning step consists of multiple rounds. Continuous subsequences are transmitted in batches among computing nodes for better CCR. In each round, computing nodes work separately and exchange intermediate results at the end of the round (line 5). After each round, the current best-so-far distance ( bsf Dist ) is updated (line 6 X 9). DDE and linear scanning (line 4) are the most time consuming parts. We present the detailed implementation of these parts. 3.4 Distributed Discord Estimation The first step of discord discovery is to estimate nnDist tion together with an early abandon technique can efficiently reduce the number of calls to the distance function (Algorithm 1 , line 2). The closer the estimation is to the ground truth, the less the distance function is invoked. Traditional methods usually set an index to achieve this. However, when a time series is divided into several segments and stored into non-unified memory spaces, creat-ing a centralized index for these distributed data is inefficient because it degrades the CCR. We propose a method called Distributed Discord Estimation (DDE), which estimates the distance and minimizes the communication between com-puting nodes.
 information of a time series. DDE outputs the estimated nnDist the real nnDist of the discord is nnDist ( C d ), then  X  d  X  nnDist 1 X 3). The approximated representation of a subsequence C p Similar subsequences have the same approximation symbol. DDE divides the subsequences into groups according to their symbolic representations. The num-ber of subsequences in one group reflects the frequency of the group (line 4). DDE selects the group with the least number of members, named candidates of discords. For each subsequence C p in this A finds a local nnDist ( C p )foreach S . The minimum of all local called global nnDist ( C p ). The maximum among all global estimation of the global discord nnDist of the time series.
 The number of subsequence in the A  X  d group, denoted as estimation. Empirically we select 2 to 10 groups of subsequences to get better precision for the estimation.
 As we can see, the computing process of the approximation is independent, which means it can be concurrently executed on a computing cluster. Each com-puting node processes different subsequences at the same time. Then PDD aggre-gates all data to find the globally estimated nnDist . 3.5 Linearly Scanning the Entire Dataset The second step of PDD is to calculate the nnDist of every subsequence and update bsf P os and bsf Dist . Linear scanning is the most time consuming part of PDD. In order to improve the CCR, a bulk of continuous subsequences is trans-mitted and calculated in batch. During this process, early abandon technique is used to reduce the computational complexity. We describe the linear scanning step from two perspectives.
 Life Cycle of a Bulk of Subsequences. Algorithm 3 explains the life cycle of a bulk of subsequences. The inputs of this pseudocode are formed by a bulk of subsequences C b ,thesetofsegment S ,and bsf Dist .
 As the life cycle of a bulk of subsequences C b starts at the beginning of Algorithm 3 ,the nnDist of all subsequences in the bulk is initialized as positive infinity (line 1). C b visits all computing nodes, each of which contains different segment of the time series (line 3). During this process, the local subsequence C p with respect to the current segment is calculated (line 4 X 7). If the global best so far Distance ( bsf Dist ) is larger than the 8), subsequence C p could be abandoned from C b (line 9), and the follow-up calculating about C p is skipped (line 10). Heuristic access order improves the efficiency of the early abandon technique (line 5). After calculating the distance between C p and all other subsequences in this segment without triggering the early abandon technique, the nnDist [ p ] becomes the global this circumstance it must hold nnDist [ p ]  X  bsf Dist . Hence pointed to current subsequences(line 14 X 15). bsf Dist must be synchronously shared by all computing nodes, and updated after a bulk visit to all computing nodes (line 18).
 the nnDist of every subsequence nnDist [:], are transmitted between computing nodes (line 3). The DDE method mentioned above initiates the as possible in order to call the Early Abandon Technology more frequently (line 8 X 10). Therefore most subsequences of bulk C b are dumped and the number of subsequences in bulk C b continually decreases. So the number of data and intermediate results being transmitted is quite small.
 Communication Among Computing Nodes. In each round, computing nodes receive bulk, work separately and exchange intermediate results with other computing nodes. We give an example to explain the details. The time series is divided into segments S 0 ...S ( a  X  1) and restored in nodes The subsequences are grouped into bulks b 0 ,b 1 ,b 2 ... ing of transmissions among computing nodes. Allocating New Bulk. At the beginning of each round, each node receives a new bulk, which has never visited other nodes. The nnDist in this new bulk is set as + inf (line 1 in Algorithm 3 ). For example, the node node process continues until all new bulks are allocated to nodes.
 Transmission Between Computing Nodes. Every bulk is sent to all com-puting nodes to find the global nnDist of every subsequence. After one node finishes the nnDist of subsequences of a bulk against one segment (e.g. bulk is sent to the next node (e.g. node 1 ). There are many paths to transmit between nodes, and we choose the rotation shift method. For example, at bulks b means all data segments) and will not be sent to node 0 at of b finishes here. 3.6 Improving Utilization of Computing Resources Load imbalance may occurs among computing nodes. Some computing nodes finish computations and exchanges earlier and enter an idle state before the round ends. The idle state degrades the utilization of computing resource. The idle state can be alleviated by allocating extra bulks and using a shared input queue. In each round, PDD creates a shared queue which contains bulks several times the number of the computing nodes. Once a computing node finishes the processing of a bulk, the node is assigned to the next bulk from the shared queue. The round is finished when all bulks in the shared queue are processed. 3.7 Guarantee on Correctness of PDD PDD discovers exact results, which completely conform to the definition of dis-cord. PDD provides the guarantee on the correctness of results by discovering discords according to the definition of discord.
 3 in Algorithm 3 ) by calculating the distances (line 5 X 7 in Algorithm 3 )of every other subsequence C q (line 2 and line 4 in Algorithm 3 ) of a time series. PDD finds the nnDist of all candidate subsequences (line 4 in Algorithm 1 )and returns the subsequence with the maximum nnDist (line 6 X 9 in Algorithm 1 )as the top discords.
 of PDD. PDD still produces correct results when it skips the step of DDE and sets the initial value of bsf Dist in Algorithm 3 to positive infinity. Other design details of PDD, such as transmission among nodes and allocation of bulks, do not affect the correctness of PDD. In this section we empirically evaluate the practical performance of PDD. We use randomly generated time series datasets for the following reasons: (1) Random time series is generally more challenging than real-world time series in terms of experimental results of time consumption on random time series are more con-vincing. (2) Large scale random time series is easier to acquire. For better repro-ducibility we choose random time series for our datasets.
 memory-computing feature of Spark to accelerate the detection of discord. We establish a Spark cluster consisting of 10 computing nodes, each of which is equipped with 512 MB memory. The time series are initially stored in Hadoop distributed file system [ 17 ], which is accessible to all computing nodes. 4.1 Scalability PDD method can be scaled to discover discords. In this paper, we evaluate the scalability of PDD in terms of parallelism and data sizes, and compare the time consumptions between PDD and the classic method HOTSAX. For better comparability, we implement HOTSAX with Java and run HOTSAX on one of the computing nodes of the Spark cluster.
 lelism to detect the top discord from a 1  X  10 5 dataset. PDD achieves a speed-up ranging from 1.54 to 6.75 compared to HOTSAX (column 5 and row 2 X 6). Then we detect the top discord from the datasets of sizes ranging from 1 1  X  10 6 with 10 computing nodes of the Spark cluster. PDD achieves a speed-up ranging from 6.75 to 8.04 compared to HOTSAX (column 5 and row 6 X 11). PDD (column 6 of Table 2 ). On average, each additional computing node of PDD provides more than 0.7 times speed-up compared to HOTSAX. The speed-up per node is not sensitive to the size of the dataset or the parallelism of PDD. We also perform experiments of PDD and HOTSAX on a 1  X  10 HOTSAX collapses because of the limitation of memory capacity of a single computing node. PDD successfully discovers the top discord with non-united memory spaces, which is 10 times bigger than that of one computing node. This experiment indicates PDD relieves the limitation of single computer memory in large scale time series discord discovery. 4.2 Utilization of Computing Resources In this part we evaluate the utilization of computing resources of PDD. We detect the top discord from a 1  X  10 5 dataset with 10 computing nodes of the Spark cluster. Table 3 shows the utilization of computing resources of PDD and the disk-aware method [ 20 ].
 In the first row, PDD allocates 10 data blocks per round (BPR), which means each computing node gets one data block. The idling time caused by load imbal-ance accounts for 18 . 9 % of the total PDD time consumption. Along with the increment of BPR, idling time decreases. When BPR = 500, the idling time drops to 4 . 71 % of the total time consumption, and the total time consumption of PDD also drops from 580 s to 500 s.
 When BPR equals to 1000, the total time rises to 547 s. This is because computing nodes receive all possible subsequences in a single round such that early abandon technique has less effect on reducing computation complexity (line 8 X 11 in Algorithm 3 ). Empirically, BPR  X  (#subsequences / no larger than one tenth of the total number of subsequences of a time series so that early abandon technique functions adequately.
 We also use Computing time to Running time Ratio (CRR) in the last two columns of Table 3 to evaluate the utilization. The CRR of PDD is about 95 % in different conditions of BPR. Besides computing time, the running time also includes the time for backstage job of the Spark, such as scheduling, task deseri-alization, Java garbage collection, data transmission between computing nodes, etc. Compared with the disk-aware method [ 20 ], the utilization of computing resources of PDD is about twice more than that of [ 20 ]. Discords are subsequences that are maximally different to all the other subse-quences of a time series. Existing methods of discord discovery are all sequential. They suffer from the limitation of computing power and storage of a single com-puting node. Also, because the results discovered from segmentations of a time series are non-combinable, discord discovery is hard to parallelize. In this paper, we propose Parallel Discord Discovery (PDD), which divides discord discovery in a combinable manner and solves its sub-problems in parallel. Experiments show that given 10 computing nodes, PDD is 7 times faster than the classical discord discovery method HOTSAX. PDD handles larger datasets, which cannot be handled by the memory based methods. Experiments indicate the computing time accounts for more than 90 % of the execution time, and reduces the negative effects on performance caused by disk I/O operations.

