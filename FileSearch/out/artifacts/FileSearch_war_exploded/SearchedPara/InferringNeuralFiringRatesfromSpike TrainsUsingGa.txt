 Neuronal acti vity , particularly in cerebral corte x, is highly variable. Ev en when experimental con-ditions are repeated closely , the same neuron may produce quite dif ferent spik e trains from trial to trial. This variability may be due to both randomness in the spiking process and to dif ferences in cogniti ve processing on dif ferent experimental trials. One common vie w is that a spik e train is generated from a smooth underlying function of time (the ring rate) and that this function carries a signicant portion of the neural information. If this is the case, questions of neuroscientic and neural prosthetic importance may require an accurate estimate of the ring rate. Unfortunately , these rate. Typically , researchers average across man y trials to nd a smooth estimate (averaging out spik-ing noise). Ho we ver, averaging across man y roughly similar trials can obscure important temporal features [1]. Thus, estimating the underlying rate from only one spik e train (or a small number of spik e trains belie ved to be generated from the same underlying rate) is an important but challenging problem.
 The most common approach to the problem has been to collect spik es from multiple trials in a peri-stimulus-time histo gram (PSTH), which is then sometimes smoothed by con volution or splines [2], [3]. Bin sizes and smoothness parameters are typically chosen ad hoc (but see [4], [5]) and the result Ag ain, the kernel shape and time scale are frequently ad hoc . For multiple trials, researchers may average over multiple kernel-smoothed estimates. [2] gives a thorough revie w of classical methods. More recently , point process lik elihood methods have been adapted to spik e data [6] X [8]. These be learned as a function of an observ ed covariate, such as a sensory stimulus or limb mo vement. In the unsupervised setting of interest here, it is constrained only by prior expectations such as smoothness. Probabilistic methods enjo y two adv antages over kernel smoothing. First, the y allo w periods). Second, as we will see, the probabilistic frame work pro vides a principled way to share information between trials and to select smoothing parameters.
 In neuroscience, most applications of point process methods use maximum lik elihood estimation. In the unsupervised setting, it has been most common to optimize x ( t ) within the span of an arbitrary basis (such as a spline basis [3]). In other elds, a theory of generalized Cox processes has been developed, where the point process is conditionally Poisson, and x ( t ) is obtained by applying a link function to a dra w from a random process, often a Gaussian process (GP) ( e.g. [9]). In this approach, parameters of the GP , which set the scale and smoothness of x ( t ) can be learned by optimizing the (approximate) mar ginal lik elihood or evidence, as in GP classication or regression. Ho we ver, the link function, which ensures a nonne gative intensity , introduces possibly undesirable artif acts. For instance, an exponential link leads to a process that gro ws less smooth as the intensity increases. Here, we mak e two adv ances. First, we adapt the theory of GP-dri ven point processes to incorpo-rate a history-dependent conditional lik elihood, suitable for spik e trains. Second, we formulate the tractability . We also demonstrate the power of numerical techniques that mak es application of GP methods to this problem computationally tractable. We sho w that GP methods emplo ying evidence optimization outperform both kernel smoothing and maximum-lik elihood point process models. Spik e trains can often be well modelled by gamma-interv al point processes [6], [10]. We assume the underlying nonne gative ring rate x ( t ) : t 2 [0 ; T ] is a dra w from a GP , and then we assume that our spik e train is a conditionally inhomogeneous gamma-interv al process (IGIP), given x ( t ) . The spik e train is represented by a list of spik e times y = f y train as an IGIP 1 , y j x ( t ) is by denition a rene wal process, so we can write: where p observ ed on ( y The true p plify these distrib utions as interv als of an inhomogeneous Poisson process (IP). This step, which we nd to sacrice very little in terms of accurac y, helps to preserv e tractability . Note also that we write the distrib ution in terms of the inter -spik e-interv al distrib ution p ( y We now discretize x ( t ) : t 2 [0 ; T ] by the time resolution of the experiment ( , here 1 ms), to yield a series of n evenly spaced samples x = [ x N + 1 time indices into x , with N much smaller than n . The discretized IGIP output process is now (ignoring terms that scale with ): where the nal two terms are p varying ring rate function from spik e times. Loosely , instead of being restricted to only one family of functions, GP allo ws all functions to be possible; the choice of kernel determines which functions are more lik ely , and by how much. Here we use the standard squared exponential (SE) kernel. Thus, x N ( 1 ; ) , where is the positi ve denite covariance matrix dened by For notational con venience, we dene the hyperparameter set = [ ; ; ; 2 GP mean is set to 0 . Since our intensity function is nonne gative, howe ver, it is sensible to treat instead as a hyperparameter and let it be optimized to a positi ve value. We note that other standard kernels -including the rational quadratic, Matern = 3 the SE; thus we only present the SE here. For an in depth discussion of kernels and of GP , see [12]. to be generated from the same ring rate prole. Our method naturally incorporates this case: dene p ( f y g m 1 j x ) = Y m model is unchanged. 3.1 Algorithmic Appr oach over the hyperparameters ), but this problem is intractable. We consider two approximations: replacing the inte gral by evaluation at the modal , and replacing the inte gral with a sum over a discrete grid of values. We rst consider choosing a modal hyperparameter set (ML-II model selection, see [12]), i.e. p ( x j y ) q ( x j y ; ) where q ( ) is some approximate posterior , and (This and the follo wing equations hold similarly for a single observ ation y or multiple observ ations we are modelling, and thus we must use an approximation technique. Laplace approximation and Expectation Propag ation (EP) are the most widely used techniques (see [13] for a comparison). The Laplace approximation ts an unnormalized Gaussian distrib ution to the inte grand in Eq. 5. Belo w we sho w this inte grand is log conca ve in x . This fact mak es reasonable the Laplace approximation, since we kno w that the distrib ution being approximated is unimodal in x and shares log conca vity with the normal distrib ution. Further , since we are modelling a non-zero mean GP , most of the Laplace approximated probability mass lies in the nonne gative orthant (as is the case with the true posterior). Accordingly , we write: where x is the mode of the inte grand and = r 2 both and (and x , implicitly) are functions of the hyperparameters . Thus, Eq. 6 can be conjug ate gradients) can be used to nd (locally) optimal hyperparameters. Algorithmic details and the gradient calculations are typical for GP; see [12]. The Laplace approximation also naturally pro vides condence interv als from the approximated posterior covariance ( 1 + ) 1 . We can also consider approximate inte gration over using the Laplace approximation abo ve. The Laplace approximation produces a posterior approximation q ( x j y ; ) = N x ; ( + 1 ) 1 and a model evidence approximation q ( j y ) (Eq. 6). The approximate inte grated posterior can be written as p ( x j y ) = E (which again gives condence interv als on the estimates). Since the dimensionality of is small, and since we nd in practice that the posterior on is well beha ved (well peak ed and unimodal), we nd that a simple grid of This approximate inte gration consistently yields better results than a modal hyperparameter set, so we will only consider approximate inte gration for the remainder of this report.
 For the Laplace approximation at any value of , we require the modal estimate of ring rate x , which is simply the MAP estimator: Solving this problem is equi valent to solving an unconstrained problem where p ( x ) is a truncated multi variate normal (but this is not the same as indi vidually truncating each mar ginal p ( x [14]). Typically a link or squashing function would be included to enforce nonne gativity in x , but this can distort the intensity space in unintended ways. We instead impose the constraint x 0 , which reduces the problem to being solv ed over the (con vex) nonne gative orthant. To pose the problem as a con vex program, we dene f ( x ) = log p ( y j x ) p ( x ) : where C represents constants with respect to x . From this form follo ws the Hessian where D = diag ( x 2 block diagonal with N blocks. Each block is rank 1 and associates its positi ve, nonzero eigen value with eigen vector [0 ; : : :; 0 ; b T total rank N and is positi ve semidenite. Since is positi ve denite, it follo ws then that the Hessian efciently solv e for the global MAP estimator of ring rate x [15].
 In the case of multiple spik e train observ ations, we need only add extra terms of negative log lik e-lihood from the observ ation model. This o ws through to the Hessian, where r 2 and = 3.2 Computational Practicality This method involv es multiple iterati ve layers which require man y Hessian inversions and other ma-trix operations (matrix-matrix products and determinants) that cost O ( n 3 ) in run-time comple xity and O ( n 2 ) in memory , where ( x 2 IR n ). For any signicant data size, a straightforw ard implemen-tation is hopelessly slo w. With 1 ms time resolution (or similar), this method would be restricted to spik e trains lasting less than a second, and even this problem would be burdensome. Achie ving computational impro vements is critical, as a nai ve implementation is, for all practical purposes, in-tractable. Techniques to impro ve computational performance are a subject of study in themselv es and are beyond the scope of this paper . We give a brief outline in the follo wing paragraph. In the MAP estimation of x , since we have analytical forms of all matrices, we avoid explicit representation of any matrix, resulting in linear storage. Hessian inversions are avoided using the matrix inversion lemma and conjug ate gradients, lea ving matrix vector multiplications as the single costly operation. Multiplication of any vector by can be done in linear time, since is a (block-wise) vector outer product matrix. Since we have evenly spaced resolution of our data x in time indices t methods [16]. These techniques allo w exact MAP estimation with linear storage and nearly linear run time performance. In practice, for example, this translates to solving MAP estimation problems of 10 3 variables in fractions of a second, with minimal memory load. For the modal hyperparameter scheme (as opposed to approximately inte grating over the hyperparameters), gradients of Eq. 6 must also be calculated at each step of the model evidence optimization. In addition to using similar techniques as in the MAP estimation, log determinants and their deri vatives (associated with the Laplace approximation) can be accurately approximated by exploiting the eigenstructure of . in seconds or minutes (on a modern workstation). These data sizes translate to seconds of spik e data at 1 ms resolution, long enough for most electroph ysiological trials. This algorithm achie ves a reduction from a nai ve implementation which would require lar ge amounts of memory and would require man y hours or days to complete. We tested the methods developed here using both simulated neural data, where the true ring rate was kno wn by construction, and in real neural spik e trains, where the true ring rate was estimated by a PSTH that averaged man y similar trials. The real data used were recorded from macaque premotor corte x during a reaching task (see [17] for experimental method). Roughly 200 repeated trials per neuron were available for the data sho wn here.
 We compared the IGIP-lik elihood GP method (hereafter , GP IGIP) to other rate estimators (kernel smoothers, Bayesian Adapti ve Re gressions Splines or BARS [3], and variants of the GP method) using root mean squared dif ference (RMS) to the true ring rate. PSTH and kernel methods approxi-mate the mean conditional intensity ( t ) = E (by the time rescaling theorem [7], [11]) that ( t ) = x ( t ) , and thus we can compare the GP IGIP (which nds x ( t ) ) directly to the kernel methods. To conrm that hyperparameter optimization im-pro ves performance, we also compared GP IGIP results to maximum lik elihood (ML) estimates of methods with x ed bases or smoothness parameters. To evaluate the importance of an observ ation model with spik e history dependence (the IGIP of Eq. 3), we also compared GP IGIP to an inho-mogeneous Poisson (GP IP) observ ation model (ag ain with a GP prior on x ( t ) ; simply = 1 in Eq. 3).
 The hyperparameters have prior distrib utions ( p ( ) in Eq. 5). For normal priors to enforce meaningful values ( i.e. nite, positi ve, and greater than 1 in the case of ). Specically , we set log( 2 The variance of with the matrix inversion lemma (see 3.2). For the approximate inte gration, we chose a grid consisting of the empirical mean rate for (that is, total spik e count N divided by total time T ) and ( ; log( 2 produced similar results to man y other very nely sampled grids. The four examples in Fig. 1 represent experimentally gathered ring rate proles (according to the methods in [17]). In each of the plots, the empirical average ring rate of the spik e trains is sho wn in bold red. For simulated spik e trains, the spik e trains were generated from each of these empirical average ring rates using an IGIP ( = 4 , comparable to ts to real neural data). For real neural data, the spik e train(s) were selected as a subset of the roughly 200 experimentally recorded spik e trains that were used to construct the ring rate prole. These spik e trains are sho wn as a train of train or group of spik e trains is the only input given to each of the tting models. In thin green and magenta, we have two kernel smoothed estimates of ring rates; each represents the spik e trains con volv ed with a normal distrib ution of a specied standard deviation (50 and 100 ms). We also smoothed these spik e trains with adapti ve kernel [18], x ed ML (as described abo ve), BARS [3], and 150 ms kernel smoothers. We do not sho w these latter results in Fig. 1 for clarity of gures. These standard methods serv e as a baseline from which we compare our method. In bold blue, we see x , the results of the GP IGIP method. The light blue envelopes around the bold blue GP ring rate estimate represent the 95% condence interv als. Bold cyan sho ws the GP IP method. This color scheme holds for all of Fig. 1.
 We then ran all methods 100 times on each ring rate prole, using (separately) simulated and real neural spik e trains. We are interested in the average performance of GP IGIP vs. other GP methods (a x ed ML or a GP IP) and vs. kernel smoothing and spline (BARS) methods. We sho w these results in Fig. 2. The four panels correspond to the same rate proles sho wn in Fig. 1. In each panel, the top, middle, and bottom bar graphs correspond to the method on 1, 4, and 8 spik e trains, respecti vely . GP IGIP produces an average RMS error , which is an impro vement (or , less often, a deterioration) over a competing method. Fig. 2 sho ws the percent impro vement of the GP IGIP Figure 2: Average percent RMS impro vement of GP IGIP method (with model selection) vs. method indicated in the column title. See full description in text.
 trains. The general positi ve trend indicates impro vements, suggesting the utility of this approach. Note that, in the few cases where a kernel smoother performs better ( e.g. the long bandwidth kernel in panel (b), real spik e trains, 4 and 8 spik e trains), outperforming the GP IGIP method requires an optimal kernel choice, which can not be judged from the data alone. In particular , the adapti ve kernel method generally performed more poorly than GP IGIP . The relati vely poor performance of GP IGIP vs. dif ferent techniques in panel (d) is considered in the Discussion section. The data sets here are by no means exhausti ve, but the y indicate how this method performs under dif ferent conditions. We have demonstrated a new method that accurately estimates underlying neural ring rate functions and pro vides condence interv als, given one or a few spik e trains as input. This approach is not without complication, as the technical comple xity and computational effort require special care. Estimating underlying ring rates is especially challenging due to the inherent noise in spik e trains. Ha ving only a few spik e trains depri ves the method of man y trials to reduce spiking noise. It is important here to remember wh y we care about single trial or small number of trial estimates, since we belie ve that in general the neural processing on repeated trials is not identical. Thus, we expect this signal to be dif cult to nd with or without trial averaging.
 environment for this method, since the underlying ring rate is kno wn, but it lacks the experimental proof of real neural spik e trains (where spiking does not exactly follo w a gamma-interv al process). For the real neural spik e trains, howe ver, we do not kno w the true underlying ring rate, and thus we can only mak e comparisons to a noisy , trial-a veraged mean rate, which may or may not accurately of the general impro vements offered by this method.
 Panels (a), (b), and (c) in Fig. 2 sho w that GP IGIP offers meaningful impro vements in man y cases and a small loss in performance in a few cases. Panel (d) tells a dif ferent story . In simulation, GP IGIP generally outperforms the other smoothers (though, by considerably less than in other panels). In real neural data, howe ver, GP IGIP performs the same or relati vely worse than other methods. This may indicate that, in the low ring rate regime, the IGIP is a poor model for real neural spiking. It may also be due to our algorithmic approximations (namely , the Laplace approximation, which allo ws density outside the nonne gative orthant). We will report on this question in future work. Furthermore, some neural spik e trains may be inherently ill-suited to analysis. A problem with this and any other method is that of very low ring rates, as only occasional insight is given into the underlying generati ve process. With spik e trains of only a few spik es/sec, it will be impossible for any method to nd interesting structure in the ring rate. In these cases, only with man y trial averaging can this structure be seen.
 Several studies have investig ated the inhomogeneous gamma and other more general models ( e.g. [6], [19]), including the inhomogeneous inverse gaussian (IIG) interv al and inhomogeneous Mark ov interv al (IMI) processes. The methods of this paper apply immediately to any log-conca ve inhomo-geneous rene wal process in which inhomogeneity is generated by time-rescaling (this includes the IIG and several others). The IMI (and other more sophisticated models) will require some changes in implementation details; one possibility is a variational Bayes approach. Another direction for this work is to consider signicant nonstationarity in the spik e data. The SE kernel is standard, but it is also stationary; the method will have to compromise between areas of cate gorically dif ferent covariance. Nonstationary covariance is an important question in modelling and remains an area of research [20]. Adv ances in that eld should inform this method as well.
 This work was supported by NIH-NINDS-CRCNS-R01, the Michael Flynn SGF , NSF , NDSEGF , Gatsby , CDRF , BWF , ONR, Sloan, and Whitak er. This work was concei ved at the UK Spik e Train Workshop, Ne wcastle, UK, 2006; we thank Stuart Bak er for helpful discussions during that time. We thank Vikash Gilja, Stephen Ryu, and Mack enzie Risch for experimental, sur gical, and animal care assistance. We thank also Araceli Na varro.
 [1] B. Yu, A. Afshar , G. Santhanam, S. Ryu, K. Sheno y, and M. Sahani. Advances in NIPS , 17, [2] R. Kass, V. Ventura, and E. Bro wn. J. Neur ophysiol , 94:8 X 25, 2005. [3] I. DiMatteo, C. Geno vese, and R. Kass. Biometrika , 88:1055 X 1071, 2001. [4] H. Shimazaki and S. Shinomoto. Neur al Computation , 19(6):1503 X 1527, 2007. [5] D. Endres, M. Oram, J. Schindelin, and P. Foldiak. Advances in NIPS , 20, 2008. [6] R. Barbieri, M. Quirk, L. Frank, M. Wilson, and E. Bro wn. J Neur osci Methods , 105:25 X 37, [7] E. Bro wn, R. Barbieri, V. Ventura, R. Kass, and L. Frank. Neur al Comp , 2002. [8] W. Truccolo, U. Eden, M. Fello ws, J. Donoghue, and E. Bro wn. J Neur ophysiol. , 93:1074 X  [9] J. Moller , A. Syv ersv een, and R. Waagepetersen. Scandanavian J. of Stats. , 1998. [10] K. Miura, Y. Tsubo, M. Okada, and T. Fukai. J Neur osci. , 27:13802 X 13812, 2007. [11] D. Dale y and D. Vere-Jones. An Intr oduction to the Theory of Point Processes . Springer , 2002. [12] C. Rasmussen and C. Williams. Gaussian Processes for Mac hine Learning . MIT Press, 2006. [13] M. Kuss and C. Rasmussen. Journal of Mac hine Learning Res. , 6:1679 X 1704, 2005. [14] W. Horrace. J Multivariate Analysis , 94(1):209 X 221, 2005. [15] S. Bo yd and L. Vandenber ghe. Con vex Optimization . Cambridge Uni versity Press, 2004. [16] B. Silv erman. Journal of Royal Stat. Soc. Series C: Applied Stat. , 33, 1982. [17] C. Chestek, A. Batista, G. Santhanam, B. Yu, A. Afshar , J. Cunningham, V. Gilja, S. Ryu, [18] B. Richmond, L. Optican, and H. Spitzer . J. Neur ophys. , 64(2), 1990. [19] R. Kass and V. Ventura. Neur al Comp , 14:5 X 15, 2003. [20] C. Paciorek and M. Schervish. Advances in NIPS , 15, 2003.
