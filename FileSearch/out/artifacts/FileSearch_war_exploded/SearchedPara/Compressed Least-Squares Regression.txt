 y define the empirical (quadratic) error and the generalization (quadratic) error f  X  /  X  X  . For any regression function b f , we define the excess risk function space F . {  X  When the number of data K is larger than the number of features N , the ordinary Least-Squares Regression (LSR) provides the LS solution f (  X  the pseudo-dimension is at most N + 1 ). For example, let f error is bounded as: f (low N ) and small approximation error (large N ).
 Whenever N is larger than K we face the overfitting problem since there are more parameters than risk. To overcome the problem, several approaches have been proposed in the literature: the penalty term measures the capacity of the function space.
 for the empirical error minimizer in a (randomly generated) lower dimensional subspace G M  X  X  N (where M &lt; K ).
 Our contribution: We consider a set of M random linear combinations of the initial N features and perform our favorite LS regression algorithm (possibly regularized) using those  X  X ompressed the Johnson-Lindenstrauss Lemma stated in Proposition 1.
 main versus increased approximation error introduced by the random projection) and the numerical complexity (reduced complexity of solving the LSR in the compressed domain versus the additional load of performing the projection).
 As a consequence, we show that by choosing M = O ( Least-Squares Regression which uses O ( NK 3 / 2 ) elementary operations to compute a regression function with estimation error (relatively to the initial function space F N ) of order log K/ with the best methods, up to our knowledge.
 Related works: Using dimension reduction and random projections in various learning areas has For data Y = X X  +  X  , where  X  is the target and  X  a standard noise, they use compression of the They provide an analysis of the LASSO estimator built from these compressed data, and discuss a consider a compressed (input and/or output) data space but a compressed feature space instead. In [11], the authors discuss how compressed measurements may be useful to solve many detection, (translation invariant) kernels. F i.i.d. elements drawn for some distribution  X  . Examples of distributions are: approximately preserved through random projections (this is a simple consequence of the Johnson-Lindenstrauss Lemma): M  X  1 P thus G M is a subspace of F N . 2.1 Approximation error We now compare the approximation error assessed in the compressed domain G M versus in the function in F N .
 Theorem 1 For any  X  &gt; 0 , any M  X  15 log(8 K/ X  ) , let A be a random M  X  N matrix defined probability at least 1  X   X  , inf This theorem shows the tradeoff in terms of estimation and approximation errors for an estimator obtained in the compressed domain compared to an estimator b f obtained in the initial domain: error assessed in the compressed domain G M is bounded as f M log(8 K/ X  ) . For M  X  15 log(8 K/ X  ) we have  X  &lt; 3 / 4 thus M  X  applies and says that on an event E of probability at least 1  X   X / 2 , we have for all k  X  K , On the event E , we have with probability at least 1  X   X  0 , where we applied two times Chernoff-Hoeffding X  X  inequality. Combining with (3), unconditioning, and setting  X  0 =  X / 2 then with probability at least (1  X   X / 2)(1  X   X  0 )  X  1  X   X  we have (2). 2.2 Computational issues operations) of an algorithm A to compute the regression function b f when provided with the data D K and function space F N .
 estimator, (iii) the cost for making one prediction (i.e. computing b f ( x ) for any x ): Note that the values mentioned for the compressed domain are upper-bounds on the real complexity matrix computations, see e.g. [2, 1]). We now analyze the specific case of Least-Squares Regression. 3.1 Excess risk of ordinary Least Squares regression function. The ordinary LS regression provides the regression function f Note that  X  X  T Then the truncated predictor is: b f L ( x ) def = T L [ f Truncation after the computation of the parameter bounds. Indeed, the excess risk of b f L is bounded as E Y conditionally on the input data: Remark: Note that because we use the quadratic loss function, by following the analysis in [3], not to provide the tightest possible bounds, but rather to show how the excess risk bound for LS regression in the initial domain extends to the compressed domain. 3.2 Compressed Least-Squares Regression (CLSR) CLSR is defined as the ordinary LSR in the compressed domain. Let b  X  =  X   X  Y  X  R M , where  X  b g ( x ) def = T L [ g the CLSR estimate: Corollary 1 For any  X  &gt; 0 , set M = 8 ||  X  + || bounded as expected excess risk of the CLSR estimate conditionally on the input samples is upper bounded as E Proof: Whenever M  X  15 log(8 K/ X  ) we deduce from Theorem 1 and (5) that the excess risk of b g L is bounded as E  X  M By optimizing on M and noticing that inf f  X  X  N || f  X  f  X  || 2 P (  X  k ) 1  X  k  X  K are linearly independent, we deduce the second result.
 Remark 1 Note that the second term in the parenthesis of (7) is negligible whenever K log 1 / X  . Thus we have the expected excess risk Complexity of CLSR: The complexity of LSR for computing the regression function in the com-is of order O ( K 5 / 2 ) when we choose the optimized number of projections M = O ( the leading term when using CLSR is the cost for building the  X  matrix: O ( NK 3 / 2 ) . 4.1 The factor ||  X  + || p E ||  X  ( X ) || 2 In light of Corollary 1, the important factor which will determine whether the CLSR provides low (for CLSR) should be such that the norm of those features as well as the norm of the parameter  X  whether this product can be made small for appropriate choices of features. We now provide two decreases when the regularity increases, and may even vanish.
 the uniform measure and P X has bounded density.
 any f  X  F N decomposes as f = P N i =1  X  f, X  i  X   X  i = P N i =1 b i c we have: ||  X  || 2 = P N i =1 ( b i c Now, linear approximation theory (Jackson-type theorems) tells us that assuming a function f  X   X  L example the class of functions with bounded total variation may be decomposed with Fourier basis Sobolev spaces) lead to larger values of  X  related to the order of differentiability. By choosing c i = i  X   X / 2 , we have ||  X  + || p E ||  X  || 2  X  if 0 &lt;  X  &lt; 1 , then it is bounded by O ( N 1  X   X  ) .
 such as wavelets, that would decompose the function at different scales, may be interesting. l ) sup x  X  X  ||  X  ( x ) || 2 (thus on p E ||  X  ( X ) || 2 ) by a constant independent of N : c  X  4.2 Comparison with other methods terms of F N ) of CLSR is O (log K/ O ( N log K/K ) .
 assumed to be sparse. From [12, 15, 24] one deduces that under some assumptions, the estimation best regressor f + in F N . If S &lt; risk. Otherwise CLSR may be an interesting alternative although this method does not make any the regression function using only M coefficients.
 LASSO algorithm is O ( NK 2 ) in the best cases (assuming that the number of steps required for simple competitor to LASSO. We considered the case when the number of features N is larger than the number of data K . The random subspace of dimension M = O ( F
N ) bounded by O (log K/ dom subspace of lower dimension and then performs an empirical risk minimizer in this subspace. for which the term ||  X  + || p E ||  X  ( X ) || 2 is small.
 Acknowledgements: The authors wish to thank Laurent Jacques for numerous comments and Alessandro Lazaric and Mohammad Ghavamzadeh for exciting discussions. This work has been supported by French National Research Agency (ANR) through COSINUS program (project EXPLO-RA, ANR-08-COSI-004).
 [1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with bi-[5] Peter J. Bickel, Ya X  X cov Ritov, and Alexandre B. Tsybakov. Simultaneous analysis of Lasso [6] Avrim Blum. Random projection, margins, kernels, and feature-selection. Subspace, Latent [7] Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal [8] Emmanuel Candes and Terence Tao. The Dantzig selector: Statistical estimation when p is [9] Emmanuel J. Candes and Justin K. Romberg. Signal recovery from random projections. vol-[10] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM [11] Mark A. Davenport, Michael B. Wakin, and Richard G. Baraniuk. Detection and estimation [14] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear predic-[16] D. Pollard. Convergence of Stochastic Processes . Springer Verlag, New York, 1984. [17] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Neural [24] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regulariza-[25] Shuheng Zhou, John D. Lafferty, and Larry A. Wasserman. Compressed regression. In John C.
