 A compound is a lexeme that consists of more than one stem. Informally, a compound is a combina-tion of two or more words that function as a single unit of meaning. Some compounds are written as space-separated words, which are called open com-pounds (e.g. hard drive ), while others are written as single words, which are called closed compounds (e.g. wallpaper ). In this paper, we shall focus only on closed compounds because open compounds do not require further splitting.

The objective of compound splitting is to split a compound into its corresponding sequence of con-stituents. If we look at how compounds are created fromlexemesinthefirstplace,wefindthatforsome languages, compounds are formed by concatenating existing words, while in other languages compound-ing additionally involves certain morphological op-erations. These morphological operations can be-come very complex as we illustrate in the following case studies. 1.1 Case Studies Below,welookatsplittingcompoundsfrom3differ-ent languages. The examples introduce in part the notation used for the decision rule outlined in Sec-tion 3.1. 1.1.1 English Compound Splitting The word flowerpot can appear as a closed or open compound in English texts. To automatically split the closed form we have to try out every split point and choose the split with minimal costs according to a cost function. Let's assume that we already know that flowerpot must be split into two parts. Then we have to position two split points that mark the end of each part (one is always reserved for the last charac-ter position). The number of split points is denoted by K (i.e. K = 2 ), while the position of split points is denoted by n 1 and n 2 . Since flowerpot consists of 9 characters, we have 8 possibilities to position split point n 1 within the characters c 1 , . . . , c 8 . The final splitpointcorrespondswiththelastcharacter,thatis, n 2 = 9 . Trying out all possible single splits results in the following candidates: If we associate each compound part candidate with a cost that reflects how frequent this part occurs in a large collection of English texts, we expect that the correct split flower + pot will have the lowest cost. 1.1.2 German Compound Splitting Thepreviousexamplecoveredacasewherethecom-pound is constructed by directly concatenating the compound parts. While this works well for En-glish,otherlanguagesrequireadditionalmorpholog-ical operations. To demonstrate, we look at the Ger-man compound Verkehrszeichen (traffic sign) which consists of the two nouns Verkehr (traffic) and Zei-chen (sign). Let's assume that we want to split this word into 3 parts, that is, K = 3 . Then, we get the following candidates.
 Using the same procedure as described before, we canlookupthecompoundpartsinadictionaryorde-termine their frequency from large text collections. This yields the optimal split points n 1 = 7 , n 2 = 8 , n 3 = 15 . The interesting part here is the addi-tional s morpheme, which is called a linking mor-pheme,becauseitcombinesthetwocompoundparts to form the compound Verkehrszeichen . If we have a list of all possible linking morphemes, we can hypothesize them between two ordinary compound parts. 1.1.3 Greek Compound Splitting The previous example required the insertion of a linking morpheme between two compound parts. We shall now look at a more complicated mor-phological operation. The Greek compound  X  X  X  X  X  X  X  X  X  X  ( cardboard box ) consists of the two parts  X  X  X  X  X  ( paper ) and  X  X  X  X  X  ( box ). Here, the problem is that the parts  X  X  X  X  X  and  X  X  X  X  X  are not valid words in Greek. To lookup the correct words, we must substitute the suffix of the compound part candidates with some other morphemes. If we allow the compound part candidates to be transformed by some morphological operation, we can lookup the transformed compound parts in a dictionary or de-termine their frequencies in some large collection of Greektexts. Let'sassumethatweneedonlyonesplit point. Then this yields the following compound part candidates: Here, g k : s / t denotes the k th compound part which is obtained by replacing string s with string t in the original string, resulting in the transformed part g k . 1.2 Problems and Objectives Our goal is to design a language-independent com-pound splitter that is useful for machine translation. The previous examples addressed the importance of acostfunctionthatfavorsvalidcompoundpartsver-sus invalid ones. In addition, the examples have shownthat, dependingonthelanguage, themorpho-logical operations can become very complex. For most Germanic languages like Danish, German, or Swedish, the list of possible linking morphemes is rather small and can be provided manually. How-ever, in general, these lists can become very large, and language experts who could provide such lists might not be at our disposal. Because it seems in-feasible to list the morphological operations explic-itly, we want to find and extract those operations automatically in an unsupervised way and provide them as an additional knowledge source to the de-compounding algorithm.

Another problem is how to evaluate the quality of the compound splitter. One way is to compile for every language a large collection of compounds together with their valid splits and to measure the proportion of correctly split compounds. Unfortu-nately, such lists do not exist for many languages. Whilethetrainingalgorithmforourcompoundsplit-ter shall be unsupervised, the evaluation data needs to be verified by human experts. Since we are in-terested in improving machine translation and to cir-cumvent the problem of explicitly annotating com-pounds, we evaluate the compound splitter within a machine translation task. By decompounding train-ingandtestdataofamachinetranslationsystem, we expect an increase in the number of matching phrase table entries, resulting in better translation quality measured in BLEU score (Papineni et al., 2002). If BLEU score is sensitive enough to measure the quality improvements obtained from decompound-ing, thereisnoneedtogenerateaseparategoldstan-dard for compounds.

Finally, we do not want to split non-compounds and named entities because we expect them to be translated non-compositionally. For example, the Germanword Deutschland (Germany)couldbesplit into two parts Deutsch (German) + Land (coun-try). Although this is a valid split, named entities should be kept as single units. An example for a non-compound is the German participle vereinbart (agreed) which could be wrongly split into the parts Verein (club) + Bart (beard). To avoid overly eager splitting,wewillcompilealistofnon-compoundsin an unsupervised way that serves as an exception list for the compound splitter. To summarize, we aim to solve the following problems: Previous work concerning decompounding can be divided into two categories: monolingual and bilin-gual approaches.

Brown(2002)describesacorpus-drivenapproach for splitting compounds in a German-English trans-lation task derived from a medical domain. A large proportion of the tokens in both texts are cognates withaLatinorGreeketymologicalorigin. Whilethe English text keeps the cognates as separate tokens, they are combined into compounds in the German text. To split these compounds, the author compares boththeGermanandtheEnglishcognatesonachar-acter level to find reasonable split points. The algo-rithm described by the author consists of a sequence of if-then-else conditions that are applied on the two cognates to find the split points. Furthermore, since the method relies on finding similar character se-quences between both the source and the target to-kens, the approach is restricted to cognates and can-not be applied to split more complex compounds.
Koehn and Knight (2003) present a frequency-based approach to compound splitting for German. The compound parts and their frequencies are es-timated from a monolingual corpus. As an exten-sion to the frequency approach, the authors describe a bilingual approach where they use a dictionary ex-tracted from parallel data to find better split options. The authors allow only two linking morphemes be-tween compound parts and a few letters that can be dropped. In contrast to our approach, those opera-tions are not learned automatically, but must be pro-vided explicitly.

GareraandYarowsky(2008)proposeanapproach to translate compounds without the need for bilin-gual training texts. The compound splitting pro-cedure mainly follows the approach from (Brown, 2002) and (Koehn and Knight, 2003), so the em-phasis is put on finding correct translations for com-pounds. To accomplish this, the authors use cross-language compound evidence obtained from bilin-gual dictionaries. In addition, the authors describe a simple way to learn glue characters by allowing the deletion of up to two middle and two end charac-ters. 1 More complex morphological operations are not taken into account.

Alfonseca et al. (2008b) describe a state-of-the-art German compound splitter that is particularly ro-bust with respect to noise and spelling errors. The compound splitter is trained on monolingual data. Besides applying frequency and probability-based methods, the authors also take the mutual informa-tionofcompoundpartsintoaccount. Inaddition,the authors look for compound parts that occur in dif-ferent anchor texts pointing to the same document. All these signals are combined and the weights are trainedusingasupportvectormachineclassifier. Al-fonseca et al. (2008a) apply this compound splitter on various other Germanic languages.

Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lat-tices that serve as input to a translation system. To train the model, reference segmentations are re-quired. Here, we produce only single best segmen-tations, but otherwise do not rely on reference seg-mentations. In this section, we describe the underlying optimiza-tion problem and the algorithm used to split a token into its compound parts. Starting from Bayes' de-cision rule, we develop the Bellman equation and formulate a dynamic programming-based algorithm thattakesawordasinputandoutputstheconstituent compound parts. We discuss the procedure used to extract compound parts from monolingual texts and tolearnthemorphologicaloperationsusingbilingual corpora. 3.1 Decision Rule for Compound Splitting Given a token w = c 1 , . . . , c N = c N sequence of N characters c i , the objective function istofindtheoptimalnumber  X  K andsequenceofsplit points  X  n  X  K of the token, where 2 n 0 : = 0 and n K : = N : w = c N 1  X  (  X  K,  X  n  X  K 0 ) = with p ( n 0 ) = p ( n K | X  )  X  1 . Equation 2 requires that token w can be fully decomposed into a sequence of lexemes, the compound parts. Thus, determin-ingtheoptimalsegmentationissufficientforfinding the constituents. While this may work for some lan-guages, the subwords are not valid words in general as discussed in Section 1.1.3. Therefore, we allow the lexemes to be the result of a transformation pro-cess, where the transformed lexemes are denoted by g w = c N 1  X  (  X  K,  X  n  X  K 0 ,  X  g  X  K 1 ) = The compound part probability is a zero-order model. Ifwepenalizeeachsplitwithaconstantsplit penalty  X  , and make the probability independent of the number of splits K , we arrive at the following decision rule: w = c N 1  X  (  X  K,  X  n  X  K 1 ,  X  g  X  K 1 ) 3.2 Dynamic Programming We use dynamic programming to find the optimal split sequence. Each split infers certain costs that are determined by a cost function. The total costs of a decomposed word can be computed from the in-dividual costs of the component parts. For the dy-namicprogrammingapproach,wedefinethefollow-ing auxiliary function Q with n k = j :
Q ( c j that is, Q ( c j mum probability) that we assign to the prefix string c where we have used k split points at positions n k This yields the following recursive equation: Algorithm 1 Compound splitting Input: input word w = c N Output: compound parts Q (0) = 0
Q (1) =  X  X  X  = Q ( N ) =  X  for i = 0 , . . . , N  X  1 do end for with backpointer UsinglogarithmsinEquations 7and8, wecan inter-pretthequantitiesasadditivecostsratherthanproba-bilities. This yields Algorithm 1, which is quadratic in the length of the input string. By enforcing that each compound part does not exceed a predefined constantlength ` ,wecanchangethesecond for loop as follows: With this change, Algorithm 1 becomes linear in the length of the input word, O ( | w | ) . The performance of Algorithm 1 depends on the cost function cost (  X  ) , that is, the probability rates knowledge about morpheme transformations, morphemepositionswithinacompoundpart,andthe compound parts themselves. 4.1 Learning Morphological Operations using Let s and t be strings of the (source) language al-phabet A . A morphological operation s / t is a pair of strings s, t  X  A  X  , where s is replaced by t . With the usual definition of the Kleene operator  X  , s and t can be empty, denoted by  X  . An example for such a pair is  X  / es , which models the linking morpheme es in the German compound Bundesagentur (federal agency): Note that by replacing either s or t with  X  , we can model insertions or deletions of morphemes. The explicit dependence on position n k  X  1 in Equation 6 allows us to determine if we are at the beginning, in the middle, or at the end of a token. Thus, we can distinguish between start, middle, or end mor-phemes and hypothesize them during search. 3 Al-though not explicitly listed in Algorithm 1, we dis-allow sequences of linking morphemes. This can be achieved by setting the costs to infinity for those morpheme hypotheses, which directly succeed an-other morpheme hypothesis.

To learn the morphological operations involved in compounding, we determine the differences be-tweenacompoundanditscompoundparts. Thiscan be done by computing the Levenshtein distance be-tween the compound and its compound parts, with the allowable edit operations being insertion, dele-tion, or substitution of one or more characters. If we store the current and previous characters, edit opera-tion and the location (prefix, infix or suffix) at each position during calculation of the Levenshtein dis-tance then we can obtain the morphological opera-tions required for compounding. Applying the in-verseoperations, thatis, replacing t with s yieldsthe operation required for decompounding. 4.1.1 Finding Compounds and their Parts To learn the morphological operations, we need compoundstogetherwiththeircompoundparts. The basic idea of finding compound candidates and their compound parts in a bilingual setting are related to the ideas presented in (Garera and Yarowsky, 2008). Here, we use phrase tables rather than dictionaries. Althoughphrasetablesmightcontainmorenoise,we believe that overall phrase tables cover more phe-nomenaoftranslationsthanwhatcanbefoundindic-tionaries. The procedure is as follows. We are given a phrase table that provides translations for phrases from a source language l into English and from En-glish into l . Under the assumption that English does not contain many closed compounds, we can search the phrase table for those single-token source words f inlanguage l , which translate intomulti-token En-glish phrases e 1 , . . . , e n for n &gt; 1 . This results in a list of ( f ; e 1 , . . . , e n ) pairs, which are poten-tialcompoundcandidatestogetherwiththeirEnglish translations. If for each pair, we take each token e i from the English (multi-token) phrase and lookup the corresponding translation for language l to get g , we should find entries that have at least some partial match with the original source word f , if f is a true compound. Because the translation phrase table was generated automatically during the train-ing of a multi-language translation system, there is no guarantee that the original translations are cor-rect. Thus, the bilingual extraction procedure is subject to introduce a certain amount of noise. To mitigate this, thresholds such as minimum edit dis-tance between the potential compound and its parts, minimumco-occurrencefrequenciesfortheselected bilingual phrase pairs and minimum source and tar-get word lengths are used to reduce the noise at the expense of finding fewer compounds. Those entries that obey these constraints are output as triples of form: where The following example for German illustrates the process. Suppose that the most probable translation for  X berweisungsbetrag is transferamount usingthe phrase table. We then look up the translation back to Germanforeachtranslatedtoken: transfer translates to  X berweisung and amount translatesto Betrag . We then calculate the distance between all permutations of the parts and the original compound and choose the one with the lowest distance and highest transla-tion probability:  X berweisung Betrag . 4.2 Monolingual Extraction of Compound The most important knowledge source required for Algorithm 1 is a word-frequency list of compound parts that is used to compute the split costs. The procedure described in Section 4.1.1 is useful for learningmorphologicaloperations,butitisnotsuffi-cienttoextractanexhaustivelistofcompoundparts. Suchlistscanbeextractedfrommonolingualdatafor which we use language model (LM) word frequency lists in combination with some filter steps. The ex-tractionprocessissubdividedinto2passes,oneover a high-quality news LM to extract the parts and the other over a web LM to filter the parts. 4.2.1 Phase 1: Bootstrapping pass Inthefirstpass,wegeneratewordfrequencylistsde-rivedfromnewsarticlesformultiplelanguages. The motivation for using news articles rather than arbi-trary web texts is that news articles are in general less noisy and contain fewer spelling mistakes. The language-dependentwordfrequencylistsarefiltered according to a sequence of filter steps. These filter steps include discarding all words that contain digits or punctuations other than hyphen, minimum occur-rence frequency, and a minimum length which we set to 4. The output is a table that contains prelim-inary compound parts together with their respective counts for each language. 4.2.2 Phase 2: Filtering pass In the second pass, the compound part vocabulary is further reduced and filtered. We generate a LM vocabularybasedonarbitrarywebtextsforeachlan-guageandbuildacompoundsplitterbasedonthevo-cabulary list that was generated in phase 1. We now try to split every word of the web LM vocabulary based on the compound splitter model from phase 1. For the compound parts that occur in the com-pound splitter output, we determine how often each compoundpartwasusedandoutputonlythosecom-pound parts whose frequency exceed a predefined threshold n . 4.3 Example Suppose we have the following word frequencies output from pass 1: In pass 2, we observe the word flowerpot . With the above list, the only compound parts used are flower and pot . If we did not split any other words and threshold at n = 1 , our final list would consist of flower and pot . This filtering pass has the advantage of outputting only those compound part candidates which were actually used to split words from web texts. The thresholding also further reduces the risk ofintroducingnoise. Anotheradvantageisthatsince the set of parts output in the first pass may contain a high number of compounds, the filter is able to re-move a large number of these compounds by exam-ining relative frequencies. In our experiments, we have assumed that compound part frequencies are higher than the compound frequency and so remove words from the part list that can themselves be split and have a relatively high frequency. Finally, after removingthelowfrequencycompoundparts,weob-tain the final compound splitter vocabulary. 4.4 Generating Exception Lists To avoid eager splitting of non-compounds and namedentities, weuseavariantoftheprocedurede-scribedinSection4.1.1. Byemittingallthosesource wordsthattranslatewithhighprobabilityintosingle-token English words, we obtain a list of words that should not be split. 4 4.5 Final Cost Function The final cost function is defined by the following components which are combined log-linearly. Although the cost function is language dependent, we use the same split penalty weight  X  = 20 for all languagesexceptforGerman,wherethesplitpenalty weight is set to 13 . 5 . To show the language independence of the approach within a machine translation task, we translate from languages belonging to different language families intoEnglish. ThepubliclyavailableEuroparlcorpus is not suitable for demonstrating the utility of com-pound splitting because there are few unseen com-pounds in the test section of the Europarl corpus. The WMT shared translation task has a broader do-main compared to Europarl but covers only a few languages. Hence, we present results for German-EnglishusingtheWMT-07dataandcoverotherlan-guagesusingnon-publiccorporawhichcontainnews as well as open-domain web texts. Table 1 lists the various corpus statistics. The source languages are grouped according to their language family.
For learning the morphological operations, we al-lowedthesubstitutionofatmost2consecutivechar-acters. Furthermore, we only allowed at most one morphological substitution to avoid introducing too much noise. The found morphological operations were sorted according to their frequencies. Those which occurred less than 100 times were discarded. Examples of extracted morphological operations are given in Table 2. Because the extraction procedure describedinSection4.1isnotpurelyrestrictedtothe case of decompounding, we found that many mor-phological operations emitted by this procedure re-flect morphological variations that are not directly linked to compounding, but caused by inflections.
To generate the language-dependent lists of com-pound parts, we used language model vocabulary lists 5 generated from news texts for different lan-guages as seeds for the first pass. These lists were filtered by discarding all entries that either con-taineddigits,punctuationsotherthanhyphens,orse-quences of the same characters. In addition, the in-frequent entries were discarded as well to further re-duce noise. For the second pass, we used the lists generated in the first pass together with the learned morphological operations to construct a preliminary compound splitter. We then generated vocabulary lists for monolingual web texts and applied the pre-liminary compound splitter onto this list. The used compoundpartswerecollectedandsortedaccording to their frequencies. Those which were used at least 2 times were kept in the final compound parts lists. Table 3 reports the number of compound parts kept after each pass. For example, the Finnish news vo-cabulary list initially contained 1.7M entries. After removingnon-alphaandinfrequentwordsinthefirst filter step, we obtained 190K entries. Using the pre-liminary compound splitter in the second filter step resulted in 73K compound part entries.

The finally obtained compound splitter was in-tegrated into the preprocessing pipeline of a state-of-the-art statistical phrase-based machine transla-tion system that works similar to the Moses de-coder (Koehn et al., 2007). By applying the com-pound splitter during both training and decoding we ensured that source language tokens were split in the same way. Table 4 presents results for vari-ous language-pairs with and without decompound-ing. Both the Germanic and the Uralic languages show significant BLEU score improvements of 1.3 BLEU points on average. The confidence inter-vals were computed using the bootstrap resampling normalapproximationmethoddescribedin(Noreen, 1989). While the compounding process for Ger-manic languages is rather simple and requires only a few linking morphemes, compounds used in Uralic languages have a richer morphology. In contrast to the Germanic and Uralic languages, we did not ob-serve improvements for Greek. To investigate this lack of performance, we turned off transliteration and kept unknown source words in their original script. We analyzed the number of remaining source characters in the baseline system and the system us-ing compound splitting by counting the number of Greekcharactersinthetranslationoutput. Thenum-ber of remaining Greek characters in the translation output was reduced from 6 , 715 in the baseline sys-temto 3 , 624 inthesystemwhichuseddecompound-ing. In addition, a few other metrics like the number of source words that consisted of more than 15 char-acters decreased as well. Because we do not know how many compounds are actually contained in the Greek source sentences 6 and because the frequency of using compounds might vary across languages, wecannotexpectthesameperformancegainsacross languages belonging to different language families. An interesting observation is, however, that if one languagefromalanguagefamilyshowsperformance gains, then there are performance gains for all the languagesinthatfamily. Wealsoinvestigatedtheef-fectofnotusinganymorphologicaloperations. Dis-allowing all morphological operations accounts for a loss of 0.1 -0.2 BLEU points across translation systems and increases the compound parts vocabu-larylistsbyupto 20 %,whichmeansthatmostofthe gains can be achieved with simple concatenation.
The exception lists were generated according to the procedure described in Section 4.4. Since we aimed for precision rather than recall when con-structing these lists, we inserted only those source Language Split source translation words whose co-occurrence count with a unigram translation was at least 1 , 000 and whose translation probability was larger than 0 . 1 . Furthermore, we re-quiredthatatleast 70 %ofalltargetphraseentriesfor agivensourcewordhadtobeunigrams. Alldecom-pounding results reported in Table 4 were generated using these exception lists, which prevented wrong splits caused by otherwise overly eager splitting. We have presented a language-independent method for decompounding that improves translations for compounds that otherwise rarely occur in the bilin-gual training data. We learned a set of morpholog-ical operations from a translation phrase table and determined suitable compound part candidates from monolingual data in a two pass process. This al-lowedustolearnmorphemesandoperationsforlan-guages where these lists are not available. In addi-tion, we have used the bilingual information stored inthe phrasetable toavoid splittingnon-compounds as well as frequent named entities. All knowledge sources were combined in a cost function that was applied in a compound splitter based on dynamic programming. Finally,wehaveshownthisimproves translation performance on languages from different language families.

The weights were not optimized in a systematic way but set manually to their respective values. In the future, the weights of the cost function should be learned automatically by optimizing an appropriate error function. Instead of using gold data, the devel-opment data for optimizing the error function could be collected without supervision using the methods proposed in this paper.
