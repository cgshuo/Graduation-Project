 University of Oxford University of Sydney lexicalized grammar. The models are  X  X ull X  parsing models in the sense that probabilities are used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with statistical parsing literature in under three hours.

Surprisingly, given CCG X  X   X  X purious ambiguity, X  the parsing speeds are significantly higher techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG X  X  nonstandard derivations. This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate X  X rgument dependencies from CCGbank. The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types. The evaluation on DepBank raises a number of issues regarding parser evaluation.
We demonstrate that both accurate and highly efficient parsing is possible with CCG. 1. Introduction Log-linear models have been applied to a number of problems in
POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks.
 but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption is usually sufficient for efficient estimation and decoding. However, for wide-coverage grammars extracted from a treebank, enumerating all parses is infeasible. In this article we apply the dy-namic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a consid-erable amount of memory: up to 25 GB . We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster.
 (
CCG ; Steedman 2000). A number of statistical parsing models have recently been devel-oped for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and
Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for as a new model and efficient parsing algorithm which exploits all including the nonstandard ones.

For the conditional log-linear models used in this article, computing expectations re-quires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. To solve this problem, we have adapted the dynamic programming method of Miyao and Tsujii (2002) to packed CCG charts. A packed chart efficiently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside X  X utside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990).
 the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling ( models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS , we use the limited-memory
Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). sentences in the training data requires up to 25 GB of RAM article. There are a number of ways to solve this problem. Possibilities include using a subset of the training data; repeatedly parsing the training data for each iteration of 494
These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation. Even given the large number of derivations and the large feature sets in our models, the estimation time for the best-performing model is less than three hours. This gives us a practical framework for developing a statistical parser.
 ative clauses and coordinate constructions is that the standard predicate X  X rgument re-lations can be derived via nonstandard surface derivations. The addition of  X  X purious X  derivations in CCG complicates the modeling and parsing problems. In this article we consider two solutions. The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996). In this approach we recover only one derivation leading to a given set of predicate X  X rgument dependencies and ignore the rest.
 cies themselves, by summing the probabilities of all derivations leading to a given set of dependencies. We also define a new efficient parsing algorithm for such a model, based on Goodman (1996), which maximizes the expected recall of dependencies. The development of this model allows us to test, for the purpose of selecting the correct predicate X  X rgument dependencies, whether there is useful information in the additional derivations. We also compare the performance of our best log-linear model against existing CCG parsers, obtaining the highest results to date for the recovery of predicate X  argument dependencies from CCGbank.
 (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words plausible lexical categories for each word in the sentence. Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999). One possible criticism of CCG has been that highly efficient parsing is not possible because of the additional  X  X purious X  derivations. In fact, we show that a novel method which tightly integrates the supertagger and parser leads to parse times significantly faster than those reported for comparable parsers in the literature. sortium). In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson allow easy comparison. However, there are still considerable difficulties associated with a cross-formalism comparison, which we describe. Even though the are being mapped into another representation, the accuracy of the 81% F-score on labeled dependencies, against an upper bound of 84.8%. The also outperforms RASP overall and on the majority of dependency types.
 full log-linear parsing model for an automatically extracted grammar, on a scale as large as that reported anywhere in the NLP literature. Second, the article provides a compre-hensive blueprint for building a wide-coverage CCG parser, including theoretical and practical aspects of the grammar, the estimation process, and decoding. Third, we inves-tigate the difficulties associated with cross-formalism parser comparison, evaluating the parser on DepBank. And finally, we develop new models and decoding algorithms for
CCG , and give a convincing demonstration that, through use of a supertagger, highly efficient parsing is possible with CCG . 2. Related Work colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999). Similar to
Della Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear models from the perspective of maximizing entropy, subject to certain constraints. Ratnaparkhi probabilities of each decision are multiplied together to give a score for the complete which corresponds to the most probable derivation.
 for attribute-value grammars, such as Head-driven Phrase Structure Grammar (
Rather than define a model in terms of parser moves, Abney defines a model directly over the syntactic structures licensed by the grammar. Another difference is that Abney uses a global model, in which a single log-linear model is defined over the complete space of attribute X  X alue structures. Abney X  X  motivation for using log-linear models is to overcome various problems in applying models based on PCFG value grammars. A further motivation for using global models is that these do not suffer from the label bias problem (Lafferty, McCallum, and Pereira 2001), which is a potential problem for Ratnaparkhi X  X  approach.
 where f i (  X  ) is a feature, or feature function, and  X  i normalizing constant, also known as the partition function . In much work using log-functions which take the value 0 or 1. However, in Abney X  X  models, and in the models used in this article, the feature functions are integer valued and count the number of times some feature appears in a syntactic analysis. 1 Abney calls the feature functions frequency functions and, like Abney, we will not always distinguish between a feature and its corresponding frequency function.
 likelihood solution during estimation involves calculating expectations of feature val-ues, which are sums over the complete space of possible analyses. Abney suggests a
Metropolis-Hastings sampling procedure for calculating the expectations, but does not experiment with an implementation.
 conditional likelihood function. In this case the likelihood function is the product of the conditional probabilities of the syntactic analyses in the data, each probability condi-tioned on the respective sentence. The advantage of this method is that calculating the conditional feature expectations only requires a sum over the syntactic analyses for the 496 sentences in the training data. The conditional-likelihood estimator is also consistent for the conditional distributions (Johnson et al. 1999). The same solution is arrived at by Della Pietra, Della Pietra, and Lafferty (1997) via a maximum entropy argument.
Another feature of Johnson et al. X  X  approach is the use of a Gaussian prior term to avoid overfitting, which involves adding a regularization term to the likelihood function; the regularization term penalizes models whose weights get too large in absolute value.
This smoothing method for log-linear models is also proposed by Chen and Rosenfeld (1999).
 mar licenses a large number of analyses for some sentences. This is not a problem for
Johnson et al. (1999) because their grammars are hand-written and constraining enough to allow the analyses for each sentence to be enumerated. However, for grammars with wider coverage it is often not possible to enumerate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample.
 age, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of parsing; an implementation is described in Kaplan et al. (2004).
 in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (
Head Driven Phrase Structure Grammar ( HPSG ). One of Miyao and Tsujii X  X  motivations is to model predicate X  X rgument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted
LTAG which uses a simple unigram model of the elementary trees together with a log-linear model of the attachments. Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar. A simple unigram model how we use a CCG supertagger to restrict the size of the charts.
 different grammar formalisms, are as follows. The CCG supertagger is a key component of our parsing system. It allows practical estimation of the log-linear models as well as highly efficient parsing. The Maximum Entropy supertagger we use could also be applied to Miyao and Tsujii X  X  grammars, although whether similar performance would be obtained depends on the characteristics of the grammar; see subsequent sections for more discussion of this issue in relation to LTAG . The second major difference is in our use of a cluster and parallelized estimation algorithm. We have found that significantly increasing the size of the parse space available for discriminative estimation, which is possible on the cluster, improves the accuracy of the resulting parser. Another advan-tage of parallelization, as discussed in Section 5.5, is the reduction in estimation time. Again, our parallelization techniques could be applied to Miyao and Tsujii X  X  framework. HPSG grammar for Dutch. One similarity is that their parsing system uses an
Maximum Entropy tagger which allows more flexibility in terms of the features that can be encoded; for example, we have found that using Penn Treebank significantly improves supertagging accuracy. Another difference is that Malouf and van Noord use the random sampling method of Osborne (2000) to allow practical estimation, whereas we construct the complete parse forest but use the supertagger to limit the size of the charts. Their work is also on a somewhat smaller scale, with the
Dutch Alpino treebank containing 7,100 sentences, compared with the 36,000 sentences we use for training.
 for English. The main difference is that the LFG grammar is hand-built, resulting in less ambiguity than an automatically extracted grammar and thus requiring fewer resources for model estimation. One downside of hand-built grammars is that they are typically less robust, which Kaplan et al. address by developing a  X  X ragment X  grammar, together with a  X  X kimming mode, X  which increases coverage on Section 23 of the Penn Treebank from 80% to 100%. Kaplan et al. also present speed figures for their parser, comparing with the Collins parser. Comparing parser speeds is difficult because of implementation and accuracy differences, but their highest reported speed is around 2 sentences per second on sentences from Section 23. The parse speeds that we present in Section 10.3 are an order of magnitude higher.
 grammar formalisms is large and growing. Statistical parsers have been developed 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others.
The motivation for using these formalisms is that many NLP
Translation, Information Extraction, and Question Answering, could benefit from the more sophisticated linguistic analyses they provide.
 been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000). Also, the supertagging idea which is central to the efficiency of the
TAG (Bangalore and Joshi 1999). Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted 91% when the tagger is run in n -best mode, but at a considerable cost in ambiguity, with 8 supertags per word. Nasr and Rambow (2004) investigate the potential impact of
LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments. They find that, with the perfect supertagger , extremely high parsing accuracies and speeds can be obtained. Interestingly, the accuracy of using automatically extracted grammars is significantly below the accuracy of the supertagger. One possible way to increase the accuracy of
Maximum Entropy, rather than HMM , tagger (as discussed previously), but this is likely to result in an improvement of only a few percentage points. Thus whether the differ-ence in supertagging accuracy is due to the nature of the formalisms, the supertagging methods used, or properties of the extracted grammars, is an open question. 3. Combinatory Categorial Grammar
Combinatory Categorial Grammar ( CCG ) (Steedman 1996, 2000) is a type-driven lex-icalized theory of grammar based on Categorial Grammar (Wood 1993). 498 a semantic interpretation. In this article we are concerned with the syntactic compo-nent; see Steedman (2000) for how a semantic interpretation can be composed dur-ing a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in this article.
 tence), N (noun), NP (noun phrase), and PP (prepositional phrase). Complex categories category for the transitive verb bought specifies its first argument as a noun phrase to its right, its second argument as a noun phrase to its left, and its result as a sentence: syntactic features such as number, gender, and case. For the grammars in this article, categories are augmented with some additional information, such as head information, and also features on S categories which distinguish different types of sentence, such as declarative, infinitival, and wh -question. This additional information will be described in later sections.

Categorial Grammar (Bar-Hillel 1953), which is context-free, there are two rules of functional application : where X and Y denote categories (either basic or complex). The first rule is forward example derivation using these rules.
 tional combinatory rules. The first is forward composition , which Steedman denotes by &gt; B (because B is the symbol used by Curry to denote function composition in combinatory logic; Curry and Feys 1958): Forward composition is often used in conjunction with type-raising ( T ), as in Figure 2.
In this case type-raising takes a subject noun phrase and turns it into a functor looking to the right for a verb phrase; the fund is then able to combine with reached using forward composition, giving the fund reached the category S [ dcl ] / NP (a declarative sentence missing an object). It is exactly this type of constituent which the object relative pronoun category is looking for to its right: ( NP \ NP ) / ( S [ dcl ] / NP ).
 disagreed with the agreement , even though this construction is often described as  X  X on-constituent coordination. X  In this example, the fund reached and investors disagreed with have the same type, allowing them to be coordinated, resulting in the fund reached but of constituency which leads to so-called spurious ambiguity, because even the simple sentence the fund reached an agreement will have more than one derivation, with each derivation leading to the same set of predicate X  X rgument dependencies.
 of the Z category in (5). For example, the following combination allows analysis of sentences such as I offered, and may give, a flower to a policeman (Steedman 2000):
This example shows how the categories for may and give combine, resulting in a cate-gory of the same type as offered , which can then be coordinated. Steedman (2000) gives a more precise definition of generalized forward composition.
 ( &lt; B ) and backward crossed composition ( &lt; B X ): 500
Backward composition provides an analysis for sentences involving  X  X rgument cluster coordination, X  such as I gave a teacher an apple and a policeman a flower (Steedman 2000).
Backward crossed composition is required for heavy NP shift and coordinations such as I shall buy today and cook tomorrow the mushrooms . In this coordination example from
Steedman (2000), backward crossed composition is used to combine the categories producing categories of the same type which can be coordinated. This rule is also generalized in an analogous way to forward composition.
 producing a further category of that type. This rule can be implemented by assuming the following category schema for a coordination term: ( X category.
 combinatory rules, such as substitution, have been suggested in the literature to deal with certain linguistic phenomena, but we chose not to implement them. The reason is that adding new combinatory rules reduces the efficiency of the parser, and we felt that, in the case of substitution, for example, the small gain in grammatical coverage was not worth the reduction in speed. Section 9.3 discusses some of the choices we made when implementing the grammar.
 normal-form derivations. Informally, a normal-form derivation is one which uses type-raising and composition only when necessary. Eisner (1996) describes a technique for eliminating spurious ambiguity entirely, by defining exactly one normal-form deriva-combination of categories produced by composition; more specifically, any constituent which is the result of a forward composition cannot serve as the primary (left) functor in another forward composition or forward application. Similarly, any constituent which in another backward composition or backward application. Eisner only deals with a grammar without type-raising, and so the constraints cannot guarantee a normal-form derivation when applied to the grammars used in this article. However, the constraints various normal-form constraints used in our experiments.

Baldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approaches to Categorial Grammar (Moortgat 1997). One possible extension to the parser and grammar described in this article is to incorporate the multi-modal approach; Baldridge suggests that, as well as having theoretical motivation, a multi-modal approach can improve the efficiency of CCG parsing. 3.1 Why Use CCG for Statistical Parsing?
CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004).
 grammar is relatively straightforward. It has a completely transparent interface between syntax and semantics and, because CCG is a lexicalized grammar formalism, providing a compositional semantics simply involves adding semantic representations to the lexical entries and interpreting the small number of combinatory rules. Bos et al. (2004) show how this can be done for the grammar and parser described in this article. malisms, such as TAG , LFG ,and HPSG , although CCG is especially well-suited to analysing coordination and long-range dependencies. For example, the analysis of  X  X on-constituent coordination X  described in the previous section is, as far as we know, unique to CCG .
 coverage parser. Later we show that use of a supertagger (Bangalore and Joshi 1999) prior to parsing can produce an extremely efficient parser. The supertagger uses statistical sequence tagging techniques to assign a small number of lexical categories to each word the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG , refer to supertagging as almost parsing .
The parser is able to parse 20 Wall Street Journal ( WSJ ) sentences per second on standard hardware, using our best-performing model, which compares very favorably with other parsers using linguistically motivated grammars.
 space for estimation of the log-linear parsing models. By focusing on those parses which result from the most probable lexical category sequences, we are able to perform effective discriminative training without considering the complete parse space, which for most sentences is prohibitively large.
 grammars used in this article, the automatically extracted been too large to enable effective supertagging (as discussed in the previous section). We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar. 3.2 Previous Work on CCG Statistical Parsing The work in this article began as part of the Edinburgh wide-coverage project (2000 X 2004). There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005).
 sented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all. The conditional probability of a depen-dency structure  X  , given a sentence S , is factored into two parts. The first part is the probability of the lexical category sequence, C , and the second part is the dependency 502 erated first, conditioned on the sentence, and then attachment decisions are made to form the dependency links. The probability of the category sequence is estimated using a maximum entropy model, following the supertagger described in Clark (2002). The probabilities of the dependencies are estimated using relative frequencies, following Collins (1996).
 dencies, as well as local dependencies. However, there are a number of problems with the model, as the authors acknowledge. First, the model is deficient, losing probability mass to dependency structures not generated by the grammar. Second, the relative frequency estimation of the dependency probabilities is ad hoc, and cannot be seen as maximum likelihood estimation, or some other principled method. Despite these flaws, the parser based on this model was able to recover dependencies at around 82% overall F-score on unseen WSJ text.
 model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003). A nodes being conditioned on some limited context from the previously generated struc-ture. Hockenmaier X  X  parser uses rule instantiations read off CCGbank (see Section 3.3) and some of these will be instances of type-raising and composition; hence the parser can produce non-normal-form derivations. However, because the parsing model is estimated over normal-form derivations, any non-normal-form derivations will receive low probabilities and are unlikely to be returned as the most probable parse. baseline model based on a PCFG . Various extensions to the baseline are considered: projection; conditioning the probability of a rule instantiation on the grandparent node (Johnson 1998); adding features designed to deal with coordination; and adding dis-tance to the dependency features. Some of these extensions, such as increased lexicaliza-tion and generating a lexical category at its maximal projection, improved performance, whereas others, such as the coordination and distance features, reduced performance.
Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of data sparseness, which becomes particularly severe for the generative model when the number of features is increased. The best performing model outperforms that of Clark,
Hockenmaier, and Steedman (2002), recovering CCG predicate X  X rgument dependencies with an overall F-score of around 84% using a similar evaluation.
 tions, which is based on the dependencies in the predicate X  X rgument structure, in-cluding long-range dependencies, rather than the dependencies defined by the local trees in the derivation. Hockenmaier also argues that, compared to Hockenmaier and
Steedman (2002b), the predicate X  X rgument model is better suited to languages with freer word order than English. The model was also designed to test whether the inclu-sion of predicate X  X rgument dependencies improves parsing accuracy. In fact, the results given in Hockenmaier (2003b) are lower than previous results. However, Hockenmaier (2003b) reports that the increased complexity of the model reduces the effectiveness of the dynamic programming used in the parser, and hence a more aggressive beam search is required to produce reasonable parse times. Thus the reduced accuracy could be due to implementation difficulties rather than the model itself.
 of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman (2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a). For example, adding long-range dependency features to the log-linear model is straightforward. We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy. Another feature of conditional log-linear models is that they are trained discriminatively, by maximizing the condi-sentence. Generative models, in contrast, are typically trained by maximizing the joint probability of the training sentence, parse pairs, even though the sentence does not need to be inferred. 3.3 CCGbank
The treebank used in this article performs two roles: It provides the lexical category set used by the supertagger, plus some unary type-changing rules and punctuation rules used by the parser, and it is used as training data for the statistical models. The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Penn Treebank conversions have also been carried out for other linguistic formalisms, including (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), HPSG (Miyao, Ninomiya, and Tsujii 2004).
 bank into CCG normal-form derivations. Some preprocessing of the phrase-structure trees was required, in order to allow the correct CCG analyses for some constructions, such as coordination. Hockenmaier (2003a) gives a detailed description of the procedure used to create CCGbank. Figure 3 shows an example normal-form derivation for an (ab-breviated) CCGbank sentence. The derivation has been inverted, so that it is represented as a binary tree.
 for wh -questions, and [ for ] for small clauses headed by for ; see Hockenmaier (2003a) for the complete list. S categories also carry features in verb phrases; for example, S [ b ] 504 is a bare-infinitive; S [ to ] \ NP is a to -infinitive; S [ pss ] mode. Note that, whenever an S or S \ NP category is modified, any feature on the S is carried through to the result category; this is true in our parser also. Finally, determiners specify that the resulting noun phrase is non-bare: NP [ nb ] / N , although this feature is largely ignored by the parser described in this article.
 ward application, forward and backward composition, backward-crossed composi-tion, type-raising, coordination of like types X  X CGbank contains a number of unary type-changing rules and rules for dealing with punctuation. The type-changing rules typically change a verb phrase into a modifier. The following examples, taken from
Hockenmaier (2003a), demonstrate the most common rules. The bracketed expression has the type-changing rule applied to it:
Another common type-changing rule in CCGbank, which appears in Figure 3, changes a noun category N into a noun phrase NP . Appendix A lists the unary type-changing rules used by our parser.
 ple, Figure 3 contains a rule which takes a comma followed by a declarative sentence and returns a declarative sentence:
There are a number of similar comma rules for other categories. There are also similar which treats a comma as a coordination: Appendix A contains the complete list of punctuation rules used in the parser. children X  X o not correspond to any of the CCG combinatory rules, or the type-changing rules or punctuation rules. This is because some of the phrase structure subtrees in the
Penn Treebank are difficult to convert to CCG combinatory rules, and because of noise introduced by the Treebank conversion process. 3.4 CCG Dependency Structures
Dependency structures perform two roles in this article. First, they are used for parser evaluation: The accuracy of a parsing model is measured using precision and recall over CCG predicate X  X rgument dependencies. Second, dependency structures form the core of the dependency model : Probabilities are defined over dependency structures, and the parsing algorithm for this model returns the highest scoring dependency structure.
 dencies. They are defined as sets, rather than multisets, because the lexical items in a dependency are considered to be indexed by sentence position; this is important for evaluation purposes and, for the dependency model, determining which derivations items need to be considered independently of sentence position, for example when defining feature functions in terms of dependencies. Such cases should be clear from the context.
 lexical categories. Thus the transitive verb category, ( S argument relations associated with it, one corresponding to the object NP argument and one corresponding to the subject NP argument. In order to distinguish different argument slots, the arguments are numbered from left to right. Thus, the subject relation for a transitive verb is represented as ( S \ NP 1 ) / NP where h f is the lexical item of the lexical category expressing the dependency relation, f is the lexical category, s is the argument slot, h a is the head word of the argument, and l encodes whether the dependency is non-local. For example, the dependency encoding company as the object of bought (as in IBM bought the company ) is represented as follows:
The subscripts on the lexical items indicate sentence position, and the final field ( indicates that the dependency is a local dependency.
 dependencies are created during a derivation as argument slots are filled. Long-range dependencies are created by passing head information from one category to another using unification. For example, the expanded category for the control verb persuade is: object, using the variable X . Unification then passes the head of the object to the subject implementation, the head and dependency markup depends on the category only and not the lexical item. This gives semantically incorrect dependencies in some cases; for 506 example, the control verbs persuade and promise have the same lexical category, which means that promise Brooks to go is assigned a structure meaning promise Brooks that Brooks will go .
 iliary and control verbs, modifiers, and relative pronouns. Among the constructions that project unbounded dependencies are relativization and right node raising. The following relative pronoun category (for words such as who, which, and that ) shows how heads are co-indexed for object-extraction:
In a sentence such as The company which IBM bought , the co-indexing will allow com-pany to be returned as the object of bought , which is represented using the following dependency:
The final field indicates the category which mediated the long-range dependency, in this case the object relative pronoun category.
 example, the marked up category for about (as in about 5,000 pounds )is: argument in (12) allows the dependency between about and 5,000 to be captured. to a dependency relation. This means, for example, that the parser produces subjects of to-infinitival clauses and auxiliary verbs. In the sentence IBM may like to buy Lotus , IBM will be returned as the subject of may , like , to ,and buy . The only exception is during evaluation, when some of these dependencies are ignored in order to be consistent with the predicate X  X rgument dependencies in CCGbank, and also DepBank. In future work we may investigate removing some of these dependencies from the parsing model and the parser output. 4. Log-Linear Parsing Models for CCG
This section describes two parsing models for CCG . The first defines the probabil-ity of a dependency structure, and the second X  X he normal-form model X  X efines the simpler than modeling dependency structures, as the rest of the article will demonstrate.
However, there are a number of reasons for modeling dependency structures. First, for many applications predicate X  X rgument dependencies provide a more useful output than derivations, and the parser evaluation is over dependencies; hence it would seem reasonable to optimize over the dependencies rather than the derivation. Second, we want to investigate, for the purposes of parse selection, whether there is useful infor-mation in the nonstandard derivations. We can test this by defining the probability of a dependency structure in terms of all the derivations leading to that structure, rather than emphasising a single derivation. Thus, the probability of a dependency structure,  X  , given a sentence, S , is defined as follows: where  X  (  X  ) is the set of derivations which lead to  X  .
 who define the probability of a dependency structure simply in terms of the depen-dencies. One reason for modeling derivations (either one distinguished derivation or a set of derivations), in addition to predicate X  X rgument dependencies, is that derivations may contain useful information for inferring the correct dependency structure. parse is defined using a log-linear form. However, the meaning of parse differs in the two cases. For the dependency model, a parse is taken to be a d ,  X  pair, as in Equa-tion (13). For the normal-form model, a parse is simply a (head-lexicalized) derivation.
We define a conditional log-linear model of a parse  X  follows: where  X   X   X   X  f f f (  X  ) = i  X  i f i (  X  ). The function f the i th feature;  X  i is the weight of the i th feature; and Z ensures that P (  X  | S ) is a probability distribution: are defined over single derivations, including local word X  X ord dependencies arising from lexicalized rule instantiations. The feature set is derived from the gold-standard normal-form derivations in CCGbank. For the dependency model, features are defined over dependency structures as well as derivations, and the feature set is derived from all derivations leading to gold-standard dependency structures, including nonstandard derivations. Section 7 describes the feature types in more detail. 4.1 Estimating the Dependency Model
For the dependency model, the training data consists of gold-standard dependency structures, namely, sets of CCG predicate X  X rgument dependencies, as described earlier.
We follow Riezler et al. (2002) in using a discriminative estimation method by maximiz-ing the conditional log-likelihood of the model given the data, minus a Gaussian prior 508 term to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999). Thus, given training sentences S 1 , ... , S m , gold-standard dependency structures,  X  definition of the probability of a dependency structure from Equation (13), the objective function is: where L (  X  ) is the log-likelihood of model  X  , G (  X  ) is the Gaussian prior term, and n is the number of features. We use a single smoothing parameter  X  ,sothat  X  i ; however, grouping the features into classes and using a different  X  for each class is worth investigating and may improve the results.
 general numerical optimization methods, requires calculation of the gradient of the ob-jective function at each iteration. The components of the gradient vector are as follows:
The first two terms are expectations of feature f i derivations for each sentence in the training data, and the first is over only the deriva-tions leading to the gold-standard dependency structure for each sentence. (ignoring the Gaussian prior term). Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al. 2002). The Gaussian prior term prevents overfitting by penalizing any model whose weights get too large in absolute value.

Della Pietra, Della Pietra, and Lafferty (1997), because setting the gradient in Equation (17) to zero yields the usual maximum entropy constraints, namely that the expected value of each feature is equal to its empirical value (again ignoring the Gaussian prior term). However, in this case the empirical values are themselves expectations, over all derivations leading to each gold-standard dependency structure. 4.2 Estimating the Normal-Form Model
For the normal-form model, the training data consists of gold-standard normal-form derivations. The objective function and gradient vector for the normal-form model are: where d j is the the gold-standard normal-form derivation for sentence S set of possible derivations for S j .Notethat  X  ( S j ) could contain some non-normal-form derivations; however, because any non-normal-form derivations will be considered incorrect, the resulting model will typically assign low probabilities to non-normal-form derivations.
 the feature appears in the gold-standard normal-form derivations. The second term in
Equation (19) is an expectation over all derivations for each sentence. 4.3 The Limited-Memory BFGS Algorithm
The limited memory BFGS ( L -BFGS ) algorithm is a general purpose numerical optimiza-tion algorithm (Nocedal and Wright 1999). In contrast to iterative scaling algorithms such as GIS , which update the parameters one at a time on each iteration, topology of the feature space and moving in a direction which is guaranteed to increase the value of the objective function.
 this leads to the method of steepest-ascent . Hence steepest-ascent uses the first partial derivative (the gradient) of the objective function to determine parameter updates. -
BFGS improves on steepest-ascent by also considering the second partial derivative (the Hessian). In fact, calculation of the Hessian can be prohibitively expensive, and so -
BFGS estimates this derivative by observing the change in a fixed number of previous gradients (hence the limited memory ).
 ods applied to log-linear models. He also presents a convincing demonstration that gen-eral purpose numerical optimization methods can greatly outperform iterative scaling methods for many NLP tasks. 3 Malouf uses standard numerical computation libraries 510 as the basis of his implementation. One of our aims was to provide a self contained estimation code base, and so we implemented our own version of the as described in Nocedal and Wright (1999). 5. Efficient Estimation
The L -BFGS algorithm requires the following values at each iteration: the expected and the value of the likelihood function. For the normal-form model, the empirical expected values and the likelihood can be easily obtained, because these only involve the single gold-standard derivation for each sentence. For the dependency model, the computations of the empirical expected values and the likelihood function are more complex, because these involve sums over just those derivations leading to the gold-standard dependency structures. We explain how these derivations can be found in
Section 5.4. The next section explains how CCG charts can be represented in a way which allows efficient estimation. 5.1 Packed CCG Charts as Feature Forests
The packed charts perform a number of roles. First, they compactly represent every derivation, dependency-structure pair, by grouping together equivalent chart entries.
Entries are equivalent when they interact in the same manner with both the genera-tion of subsequent parse structure and the statistical parse selection. In practice, this means that equivalent entries have the same span; form the same structures, that is, the remaining derivation plus dependencies, in any subsequent parsing; and generate the same features in any subsequent parsing. Back pointers to the daughters indicate how an individual entry was created, so that any derivation plus dependency structure can be recovered from the chart.
 derivation or dependency structure without enumerating all derivations. And finally, packed charts are an instance of a feature forest , which Miyao and Tsujii (2002) show can be used to efficiently estimate expected values of features, even though the expec-tation may involve a sum over an exponential number of trees in the forest. One of the contributions of this section is showing how Miyao and Tsujii X  X  feature forest approach can be applied to a particular grammar formalism, namely CCG sentence, but provide the mathematical tools for estimation once the feature forest has been constructed.
 type, identical head, and identical unfilled dependencies. The equivalence test must account for heads and unfilled dependencies because equivalent entries form the same dependencies in any subsequent parsing. Individual entries in the chart are obtained by combining canonical representatives of equivalence classes, using the rules of the grammar. Equivalence classes in the chart are sets of equivalent individual entries.
The interpretation of a packed chart as a feature forest is straightforward. First, only entries which are part of a derivation spanning the whole sentence are relevant. These entries can be found by traversing the chart top-down, starting with the entries which span the sentence. Individual entries in a cell are the conjunctive nodes, which are either lexical category, word pairs at the leaves, or have been obtained by combining two equivalence classes (or applying a unary rule to an equivalence class). The equivalence classes at the roots of the CCG derivations are the root disjunctive nodes. feature function defined over conjunctive nodes, that is, for each f a corresponding f i : C  X  N which counts the number of times feature f particular conjunctive node. The value of f i for a parse is then the sum of the values of f for each conjunctive node in the parse.
 that features can be defined in terms of long-range dependencies, even though such dependencies may involve words which are a long way apart in the sentence. Our earlier definition of equivalence is consistent with these feature types.
 forward composition rule: The equivalence class of the resulting individual entry is determined by the The dependencies are not shown, but there are two subject dependencies on the first
NP , one encoding the subject of will and one encoding the subject of buy , and there is an object dependency on the second NP encoding the object of buy . Entries in the same equivalence class are identical for the purposes of creating new dependencies for the remainder of the parsing. 5.2 Feature Locality
It is possible to extend the locality of the features beyond single rule instantiations and local dependencies. For example, the definition of equivalence given earlier allows the incorporation of long-range dependencies as features. The equivalence test considers unfilled dependencies which are both local and long-range; thus any individual entries which have different long-range dependencies waiting to be filled will be in different include such features; Hockenmaier (2003b) describes the difficulties in including such 512 features in a generative model. One of the early motivations of the Edinburgh parsing project was to see if the long-range dependencies recovered by a long-range dependencies to any of the models described in this article has no impact on accuracy. One possible explanation is that the long-range dependencies are so rare that a much larger amount of training data would be required for these dependencies to have an impact. Of course the fact that CCG enables recovery of long-range dependencies is still a useful property, even if these dependencies are not currently useful as features, because it improves the utility of the parser output.
 log-linear framework, as the long-range dependency example demonstrates, but the need for dynamic programming for both estimation and decoding reduces the range of features which can be used. Any extension to the  X  X ocality X  of the features would reduce the effectiveness of the chart packing and any dynamic programming performed over the chart. Two possible extensions, which we have not investigated, include defining de-pendency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more  X  X lobal X  features. 5.3 Calculating Feature Expectations
For estimating both the normal-form model and the dependency model, the following expectation of each feature f i , with respect to some model  X  ,isrequired: pendency model, features can be defined in terms of dependencies as well as the derivations. Dependencies can be stored as part of the individual entries (conjunctive individual entries which make up the derivations.
 sentence S in the training data. The key to performing this sum efficiently is to write the sum in terms of inside and outside scores for each conjunctive node. The inside and outside scores can be defined recursively. If the inside score for a conjunctive node c is denoted  X  c , and the outside score denoted  X  c , then the expected value of f written as follows: where C S is the set of conjunctive nodes in the packed chart for sentence S . the inside and outside scores for conjunctive node c 5 . The inside score for a disjunctive node,  X  d , is the sum of the inside scores for its conjunctive node daughters: of c  X  X  disjunctive node daughters: the exponentiation of the sum of the feature weights on that node.
 node mother: of the outside score of the mother, the inside score of the disjunctive node sister, and the feature weights on the mother. For example, the outside score of d sum of two product terms. The first term is the product of the outside score of c inside score of d 5 , and the feature weights at c 5 ; and the second term is the product of the outside score of c 2 , the inside score of d 3 , and the feature weights at c is as follows; the outside score for a root disjunctive node is 1, otherwise: 514 nodes: lated before the scores for mother nodes (and vice versa for the outside scores). This can easily be achieved by ordering the nodes in the bottom-up 5.4 Estimation for the Dependency Model
For the dependency model, the computations of the empirical expected values (17) and the log-likelihood function (16) require sums over just those derivations leading to the gold-standard dependency structure. We will refer to such derivations as correct derivations. As far as we know, this problem of identifying derivations in a packed chart which lead to a particular dependency structure has not been addressed before in the NLP literature.
 correct derivations. cdeps ( c ) returns the number of correct dependencies on conjunctive node c , and returns the incorrect marker  X  if there are any incorrect dependencies on c ; dmax ( c ) returns the maximum number of correct dependencies produced by any sub-derivation headed by c , and returns  X  if there are no sub-derivations producing only correct dependencies; dmax ( d ) returns the same value but for disjunctive node d .
Recursive definitions of these functions are given in Figure 5; the base case occurs when conjunctive nodes have no disjunctive daughters.
 just the correct dependencies, and traverses the chart top-down marking the nodes nodes in the same equivalence class, if one node heads a sub-derivation producing more correct dependencies than the other node (and each sub-derivation only produces correct dependencies), then the node with less correct dependencies cannot be part of a correct derivation.
 complete forests can be used to estimate the required log-likelihood value and feature values in Equation (17) can be obtained by calculating E  X  for each sentence S j in the training data, and also E  X  j The log-likelihood in Equation (16) can be calculated as follows: where log Z  X  and log Z  X  are the normalization constants for  X  and  X  . 5.5 Estimation in Practice
Estimating the parsing models consists of generating packed charts for each sentence in the training data, and then repeatedly calculating the values needed by the estimation algorithm until convergence. Even though the packed charts are an efficient representation of the derivation space, the charts for the complete training data (Sec-tions 02 X 21 of CCGbank) take up a considerable amount of memory. One solution is to only keep a small number of charts in memory at any one time, and to keep reading in the charts on each iteration. However, given that the L -of iterations to converge, this approach would be infeasibly slow.
 the L -BFGS training algorithm and running it on an 18-node Beowulf cluster. As well as solving the memory problem, another significant advantge of parallelization is the reduction in estimation time: using 18 nodes allows our best-performing model to be estimated in less than three hours.
 (Gropp et al. 1996). The parallel implementation is a straightforward extension of the
BFGS algorithm. Each machine in the cluster deals with a subset of the training data, 516 holding the packed charts for that subset in memory. The key stages of the algorithm are the calculations of the model expectations and the likelihood function. For a single-process version these are calculated by summing over all the training instances in one place. For a multi-process version, these are summed in parallel, and at the end of each iteration the parallel sums are combined to give a master sum. Producing a master operation across a cluster using MPI is a reduce operation. In our case, every node needs to be holding a copy of the master sum, so we use an all reduce operation. optimal way of summing across the nodes of the Beowulf cluster (typically it is done communication between the nodes, parallelizing the estimation code is an example of an embarrassingly parallel problem. One difficult aspect of the parallel implementation is that debugging can be much harder, in which case it is often easier to test a non-version of the program first. 6. The Decoder derivation from a packed chart. For each equivalence class, we record the individual equivalence classes were defined so that any other individual entry cannot be part of the highest scoring derivation for the sentence. The score for a subderivation d is highest-scoring subderivations can be calculated recursively using the highest-scoring equivalence classes that were combined to create the individual entry.

Clark and Curran (2003) outline an algorithm for finding the most probable dependency structure, which keeps track of the highest scoring set of dependencies for each node in the chart. For a set of equivalent entries in the chart (a disjunctive node), this involves summing over all conjunctive node daughters which head sub-derivations leading to the same set of high scoring dependencies. In practice large numbers of such conjunctive nodes lead to very long parse times.
 developed an algorithm which maximizes the expected labeled recall over dependen-cies. Our algorithm is based on Goodman X  X  (1996) labeled recall algorithm for the phrase-structure PA R S E VA L measures. As far as we know, this is the first application of Goodman X  X  approach to finding highest scoring dependency structures. Watson,
Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relations output by the RASP parser.
 where  X  i ranges over the dependency structures for S . The expectation for a single de-pendency structure  X  is realized as a weighted intersection over all possible dependency structures  X  i for S . The intuition is that, if  X  i is the gold standard, then the number of dependencies recalled in  X  is |  X   X   X  i | . Because we do not know which  X  standard, then we calculate the expected recall by summing the recall of  X  relative to each  X  i , weighted by the probability of  X  i .
 written in terms of a sum over the individual dependencies in  X  , rather than a sum over each dependency structure for S . The inner sum is over all derivations which contain a particular individual dependency  X  . Thus the final score for a dependency structure  X  is a sum of the scores for each dependency  X  in  X  ; and the score for a dependency  X  is the sum of the probabilities of those derivations producing  X  . This latter sum can be calculated efficiently using inside and outside scores: tive nodes in the packed chart for sentence S and deps ( c ) is the set of dependencies on conjunctive node c . The intuition behind the expected recall score is that a dependency structure scores highly if it has dependencies produced by high probability derivations. use of the packed chart: The score for an individual dependency can be calculated using dynamic programming (as explained previously), and the highest scoring dependency structure can be found using dynamic programming also. The algorithm which finds  X  max is essentially the same as the Viterbi algorithm described earlier, efficiently finding a derivation which produces the highest scoring set of dependencies. 7. Model Features
The log-linear modeling framework allows considerable flexibility for representing the parse space in terms of features. In this article we limit the features to those defined over local rule instantiations and single predicate X  X rgument dependencies. The fea-ture sets described below differ for the dependency and normal-form models. The 518 dependency model has features defined over the CCG predicate X  X rgument dependen-cies, whereas the dependencies for the normal-form model are defined in terms of local rule instantiations in the derivation. Another difference is that the rule features for the normal-form model are taken from the gold-standard normal-form deriva-tions, whereas the dependency model contains rule features from non-normal-form derivations.
 the dependency model and the normal-form model. 5 First, there are features which represent each word, lexical-category pair in a derivation, and generalizations of these which represent POS , lexical-category pairs. Second, there are features representing root category; this latter feature is then generalized using the previously described). Third, there are features which encode rule instantiations X  X ocal trees consisting of a parent and one or two children X  X n the derivation. The first set of rule features encode the combining categories and the result category; the second set of features extend the first by also encoding the head of the result category; and the third set generalizes the second using POS tags. Table 1 gives an example for each of these feature types.
 defined as 5-tuples as in Section 3.4. In addition these features are generalized in three ways using POS tags, with the word X  X ord pair replaced with word X  and POS  X  POS . Table 2 gives some examples.
 distance features encode the dependency relation and the word associated with the lexical category (but not the argument word), plus some measure of distance between the two dependent words. We use three distance measures which count the following: the number of intervening words, with four possible values 0, 1, 2, or more; the number of intervening punctuation marks, with four possible values 0, 1, 2, or more; and the number of intervening verbs (determined by POS tag), with three possible values 0, 1, or more. Each of these features is again generalized by replacing the word associated with the lexical category with its POS tag. fining dependency features in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. in three ways using POS tags, as shown in Table 3. There are also the three distance measures which encode the distance between the two head words of the combining cat-egories, as for the dependency model. Here the distance feature encodes the combining categories, the result category, the head of the result category (either as a word or tag), and the distance between the two head words.
 that is, a feature had to occur at least twice in the gold-standard normal-form deriva-tions to be included in the model. The same cutoff was applied to the features in the dependency model, except for the rule instantiation feature types. For these features the counting was done across all derivations licensed by the gold-standard lexical category sequences and a frequency cutoff of 10 was applied. The larger cutoff was used because the productivity of the grammar can lead to very large numbers of these features. We 520 also only included those features which had a nonzero empirical count, that is, those features which occured on at least one correct derivation. These feature types and frequency cutoffs led to 475,537 features for the normal-form model and 632,591 features for the dependency model. 8. The Supertagger
Parsing with lexicalized grammar formalisms such as CCG is a two-step process: first, elementary syntactic structures X  X n CCG  X  X  case lexical categories X  X re assigned to each word in the sentence, and then the parser combines the structures together. The first step can be performed by simply assigning to each word all lexical categories the word and unknown words (such as assigning the complete lexical category set; Hockenmaier 2003a). Because the number of lexical categories assigned to a word can be high, some strategy is needed to make parsing practical; Hockenmaier, for example, uses a beam search to discard chart entries with low scores.
 which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word (Ratnaparkhi 1996). The features used in the models are the words and POS tags in the five-word window, plus the two previously assigned lexical categories to the left. The conditional probability of a sequence of lexical categories, given a sentence, is then defined as the product of the individual probabilities for each category.
 of the Viterbi algorithm for HMM taggers. We restrict the categories which can be word in the data. For words seen less than k times, an alternative based on the word X  X  POS tag is used: The tagger can only assign categories which have been seen with the
POS tag in the data. We have found the tag dictionary to be beneficial in terms of both efficiency and accuracy. A value of k = 20 was used in the experiments described in this article.
 (2004a) and Curran, Clark, and Vadas (2006). It includes all lexical catgeories which ap-pear at least 10 times in Sections 02 X 21 of CCGbank, resulting in a set of 425 categories. The Clark and Curran paper shows this set to have very high coverage on unseen data. accuracy of 36.8%. Sentence accuracy is the percentage of sentences whose words are category is simply the punctuation mark itself, and are obtained using gold standard POS tags. With automatically assigned POS tags, using the
Clark (2003), the accuracies drop to 91.5% and 32.5%. An accuracy of 91 X 92% may ap-pear reasonable given the large lexical category set; however, the low sentence accuracy suggests that the supertagger may not be accurate enough to serve as a front-end to a parser. Clark (2002) reports that a significant loss in coverage results if the supertagger is used as a front-end to the parser of Hockenmaier and Steedman (2002b). In order to increase the number of words assigned the correct category, we develop a multitagger, which is able to assign more than one category to each word.
Here y i is to be thought of as a constant category, whereas y possible categories for word j . In words, the probability of category y sentence, is the sum of the probabilities of all sequences containing y be calculated efficiently using a variant of the forward X  X ackward algorithm. For each word in the sentence, the multitagger then assigns all those categories whose probability according to Equation (32) is within some factor,  X  , of the highest probability category for that word. In the implementation used here the forward X  X ackward sum is limited to those sequences allowed by the tag dictionary. For efficiency purposes, an extra pruning strategy is also used to discard low probability sub-sequences before the forward X  backward algorithm is run. This uses a second variable-width beam of 0 . 1  X  . levels of category ambiguity, together with the average number of categories per word.
The SENT column gives the percentage of sentences whose words are all supertagged correctly. The set of categories assigned to a word is considered correct if it contains the correct category. The table gives results when using gold standard the final two columns, when using POS tags automatically assigned by the described in Curran and Clark (2003). The drop in accuracy is expected, given the importance of POS tags as features.
 the number of categories in the tag dictionary X  X  entry for the word is is 45. However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. ,the supertagger correctly assigns one category to is for all values of  X  .
 not used to estimate the probability in Equation (32). Curran, Clark, and Vadas (2006) investigate the improvement obtained from using the forward X  X ackward algorithm, and also address the drop in supertagger accuracy when using automatically assigned
POS tags. We show how to maintain some POS ambiguity through to the supertagging phase, using a multi-POS tagger, and also how POS tag probabilities can be encoded as real-valued features in the supertagger. The drop in supertagging accuracy when 522 moving from gold to automatically assigned POS tags is reduced by roughly 50% across the various values of  X  . 9. Parsing in Practice 9.1 Combining the Supertagger and the Parser The philosophy in earlier work which combined the supertagger and parser (Clark,
Hockenmaier, and Steedman 2002; Clark and Curran 2003) was to use an unrestrictive setting of the supertagger, but still allow a reasonable compromise between speed and accuracy. The idea was to give the parser the greatest possibility of finding the correct reasonable efficiency. However, for some sentences, the number of categories in the chart gets extremely large with this approach, and parsing is unacceptably slow. Hence a limit was applied to the number of categories in the chart, and a more restrictive setting of the supertagger was reverted to if the limit was exceeded.
 of the supertagger, and only assign more categories if the parser cannot find an analysis supertagger. In effect, the parser is using the grammar to decide if the categories pro-vided by the supertagger are acceptable, and if not the parser requests more categories.
The advantage of this adaptive supertagging approach is that parsing speeds are much higher, without any corresponding loss in accuracy. Section 10.3 gives results for the speed of the parser. 9.2 Chart Parsing Algorithm The algorithm used to build the packed charts is the (Kasami 1965; Younger 1967) described in Steedman (2000). The naturally to CCG because the grammar is binary. It builds the chart bottom-up, starting with constituents spanning a single word, incrementally increasing the span until the whole sentence is covered. Because the constituents are built in order of span size, at any point in the process all the sub-constituents which could be used to create a particular new constituent must be present in the chart. Hence dynamic programming can be used to prevent the need for backtracking during the parsing process. 9.3 Grammar Implementation
There is a trade-off between the size and coverage of the grammar and the efficiency of the parser. One of our main goals in this work has been to develop a parser which can provide analyses for the vast majority of linguistic constructions in CCGbank, but is also efficient enough for large-scale NLP applications. In this section we describe some of the decisions we made when implementing the grammar, with this trade-off in mind. tions 02 X 21 of CCGbank. Applying a frequency cutoff of 10 results in a set of 425 lexical and is a manageable size for adding the head and dependency information, and also mapping to grammatical relations for evaluation purposes (Section 11). scribed in Section 10.2.1, two types of contraints on the grammar rules are used. Sec-tion 3 described the Eisner constraints, in which any constituent which is the result of a forward composition cannot serve as the primary (left) functor in another forward composition or forward application; an analogous constraint applies for backward composition. The second type of constraint only allows two categories to combine if permits category combinations seen in Sections 02 X 21 of CCGbank, we have found that it is detrimental to neither parser accuracy nor coverage.
 (Clark and Curran 2004a). The constraints are also useful for training. Section 10 shows that having a less restrictive setting on the supertagger, when creating charts for dis-criminative training, can lead to more accurate models. However, the optimal setting applied, because otherwise the memory requirements are prohibitive.
 composition (for all models): The Y category in (7) cannot be an N or NP category. We also place a similar constraint on backward composition. Both constraints reduce the size of the charts considerably with no impact on coverage or accuracy.

It is implemented by adding one of three fixed sets of categories to the chart whenever an NP , PP ,or S [ adj ] \ NP is present. Appendix A gives the category sets. Each category transformation is an instance of the following two rule schemata:
Curran 2004b, 2004a, 2006), mainly because the improvement in the supertagger since the earlier work means that we can now use a larger grammar but still maintain highly efficient parsing. 10. Experiments
The statistics relating to model estimation were obtained using Sections 02 X 21 of CCG-00 as development data and Section 23 as the final test data. The results for parsing speed were obtained using Section 23. There are various hyperparameters in the parsing system, for example the frequency cutoff for features, the  X  parameter in the Gaussian experimentally using Section 00 as development data. 10.1 Model Estimation
The gold standard for the normal-form model consists of the normal-form derivations in CCGbank. For the dependency model, the gold-standard dependency structures are 524 produced by running our CCG parser over the normal-form derivations. It is essential form model this means that our parser must be able to produce the gold-standard derivation from the gold-standard lexical category sequence; and for the dependency standard dependency structure. Not all rule instantiations in CCGbank can be produced by our parser, because some are not instances of combinatory rules, and others are very rare punctuation and type-changing rules which we have not implemented. Hence it is not possible for the parser to produce the gold standard for every sentence in Sec-tions 02 X 21, for either the normal-form or the dependency model. These sentences are not used in the training process.
 set assigned to each word. (We do not do this when parsing the test data.) The average number of categories assigned to each word is determined by the  X  parameter in the supertagger. A category is assigned to a word if the category X  X  probability is within  X  of the highest probability category for that word. Hence the value of  X  has a direct effect on the size of the packed charts: Smaller  X  values lead to larger charts.
 will be used for each sentence for the discriminative training algorithm. We have found that the  X  parameter can have a large impact on the accuracy of the resulting models:
If the  X  value is too large, then the training algorithm does not have enough incorrect derivations to  X  X iscriminate against X ; if the  X  value is too small, then this introduces too many incorrect derivations into the training process, and can lead to impractical memory requirements.
 approach we adopt for training differs from that used for testing and follows the original approach of Clark, Hockenmaier, and Steedman (2002): If the size of the chart exceeds some threshold, the value of  X  is increased, reducing ambiguity, and the sentence is supertagged and parsed again. The threshold which limits the size of the charts was set at 300,000 individual entries. (This is the threshold used for training; a higher value was used for testing.) For a small number of long sentences the threshold is exceeded even at the largest  X  value; these sentences are not used for training.
 of Sections 02 X 21) and for the dependency model 35,889 sentences (90.6%). Table 5 gives training statistics for the normal-form and dependency models (and a hybrid model described in Section 10.2.1), for various sequences of  X  values, when the training algorithm is run to convergence on an 18-node cluster. The training algorithm is defined to have converged when the percentage change in the objective function is less than 0.0001%. The  X  value in Equation (16), which was determined experimentally using the development data, was set at 1.3 for all the experiments in this article.
 faster than the dependency model is that, for the normal-form model, we applied the two types of normal-form restriction described in Section 9.3: First, categories can only combine if they appear together in a rule instantiation in Sections 2 X 21 of CCGbank; and second, we applied the Eisner constraints described in Section 3.
 that we are able to perform the discriminative estimation at all; without it the memory requirements would be prohibitive, even when using the cluster. 10.2 Parsing Accuracy
This section gives accuracy figures on the predicate X  X rgument dependencies in CCG-in Clark, Hockenmaier, and Steedman (2002). Because the purpose of this article is to demonstrate the feasibility of wide-coverage parsing with evaluation targeted specifically at long-range dependencies; such an evaluation was presented in Clark, Steedman, and Curran (2004).
 was set at 1,000,000 individual entries. This value was chosen to maximize the coverage possible. This was also the threshold parameter used for the speed experiments in Section 10.3.
 development data. The final test result, showing the performance of the best performing model, was obtained using Section 23. Evaluation was performed by comparing the dependency output of the parser against the predicate X  X rgument dependencies in CCG-bank. We report precision, recall, and F-scores for labeled and unlabeled dependencies, and also category accuracy. The category accuracy is the percentage of words assigned the correct lexical category by the parser (including punctuation). The labeled depen-dency scores take into account the lexical category containing the dependency relation, the argument slot, the word associated with the lexical category, and the argument the two dependent words are considered. The F-score is the balanced harmonic mean of which were parsed successfully. We also give coverage values showing the percentage of sentences which were parsed successfully.
 work, in which we generated our own gold standard by running the parser over the derivations in CCGbank and outputting the dependencies. In this article we wanted to use a gold standard which is easily accessible to other researchers. However, there are some differences between the dependency scheme used by our parser and CCGbank. For example, our parser outputs some coordination dependencies which are not in
CCGbank; also, because the parser currently encodes every argument slot in each lexical category as a dependency relation, there are some relations, such as the subject of to in a to -infinitival construction, which are not in CCGbank either. In order to provide a fair evaluation, we ignore those dependency relations. This still leaves some minor differences. We can measure the remaining differences as follows: Comparing the CCG-bank dependencies in Section 00 against those generated by running our parser over the derivations in 00 gives labeled precision and recall values of 99.80% and 99.18%, 526 respectively. Thus there are a small number of dependencies in CCGbank which the current version of the parser can never get right. 10.2.1 Dependency Model vs. Normal-Form Model. Table 6 shows the results for the normal-form and dependency models evaluated against the predicate X  X rgument dependen-cies in CCGbank. Gold standard POS tags were used; the LF( labeled F-score with automatically assigned POS tags for comparison. Decoding with the dependency model involves finding the maximum-recall dependency structure, and decoding with the normal-form model involves finding the most probable derivation, as described in Section 6. The  X  value refers to the setting of the supertagger used for training and is the first in the sequence of  X  s from Table 5. The  X  values used during the testing are those in Table 4 and the new, efficient supertagging strategy of taking the highest  X  value first was used.
 model are slightly higher than for the normal-form model. However, the coverage of the normal-form model is higher (because the use of the normal-form constraints mean that there are less sentences which exceed the chart-size threshold). One clear result from the table is that increasing the chart size used for training, by using smaller  X  values, can significantly improve the results, in this case around 1.5% F-score for the normal-form model.
 the cluster. However, it is possible to use smaller  X  values for training the dependency model if we also apply the two types of normal-form restriction used by the normal-form model. This hybrid model still uses the features from the dependency model; still performed using the maximum-recall algorithm; the only difference is that the derivations in the charts are restricted by the normal-form constraints (both for training and testing). Table 5 gives the training statistics for this model, compared to the de-pendency and normal-form models. The number of sentences we were able to use for training this model was 36,345 (91.8% of Sections 02 X 21). The accuracy of this hybrid dependency model is given in Table 7. These are the highest results we have obtained to date on Section 00. We also give the results for the normal-form model from Table 6 for comparison.
 type, using the same relations given in Clark, Hockenmaier, and Steedman (2002).
Automatically assigned POS tags were used. 10.2.2 Final Test Results. Table 9 gives the final test results on Section 23 for the hybrid dependency model. The coverage for these results is 99.63% (for gold-standard analysis. When using automatically assigned POS tags, the coverage is slightly lower: 99.58%. We used version 1.2 of CCGbank to obtain these results. Results are also given for Hockenmaier X  X  parser (Hockenmaier 2003a) which used an earlier, slightly different version of the treebank. We wanted to use the latest version to enable other researchers to compare with our results. 528 10.3 Parse Times
The results in this section were obtained using a 3.2 GH z Intel Xeon P4. Table 10 gives parse times for the 2,407 sentences in Section 23 of CCGbank. In order not to optimize speed by compromising accuracy, we used the hybrid dependency model, together with both kinds of normal-form constraints, and the maximum-recall decoder. Times are given for both automatically assigned POS tags and gold-standard
The sents and words columns give the number of sentences, and the number of words, parsed per second. For all of the figures reported on Section 23, unless stated otherwise, we chose settings for the various parameters which resulted in a coverage of 99.6%. It is possible to obtain an analysis for the remaining 0.4%, but at a significant loss in speed.
The parse times and speeds include the failed sentences, and include the time taken by the supertagger, but not the POS tagger; however, the POS taking less than 4 seconds to supertag Section 23, most of which consists of load time for the Maximum Entropy model.
 restrictive setting of the supertagger. The first value of  X  is 0.005; if the parser cannot find a spanning analysis, this is changed to  X  = 0 . 001 k = 150 number of categories assigned to a word by decreasing  X  and increasing the tag-dictionary parameter. If the node limit is exceeded at  X  = 0 . 005 (for these experiments  X  is changed to 0.03, and finally to 0.075.
 setting of the supertagger (  X  = 0 . 075), and moving through the settings if the parser cannot find a spanning analysis. The table shows that the new strategy has a significant impact on parsing speed, increasing it by a factor of 3 over the earlier approach (given the parameter settings used in these experiments).
 0 . 075; the parser ignores the sentence if it cannot get an analysis at this level. The per-centage of sentences without an analysis is now over 6% (with automatically assigned
POS tags), but the parser is extremely fast, processing over 30 sentences per second. This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for example, for which large amounts of data are required. The oracle row showing the speeds which could be achieved given the perfect supertagger.
 level, for both the new and old parsing strategies. The results show that, for the old supertagger (  X  = 0 . 005); conversely, for the new approach, most of the sentences are pertagger at the  X  = 0 . 075 level needs to be improved, without increasing the number of categories assigned on average.
 parser can be made to run faster, for example by changing the beam parameter in the Collins (2003) parser, but that any increase in speed is typically associated with a reduction in accuracy. For the CCG parser, the accuracy did not degrade when using the new adaptive parsing strategy. Thus the accuracy and efficiency of the parser were not tuned separately: The configuration used to obtain the speed results was also used to obtain the accuracy results in Sections 10.2 and 11.

Table 12 gives the parse times on Section 23 for a number of well-known parsers. Sagae and Lavie (2005) is a classifier-based linear time parser. The times for the Sagae, Collins, and Charniak parsers were taken from the Sagae and Lavie paper, and were obtained using a 1.8 GH z P4, compared to a 3.2 GH zP4forthe CCG numbers. Comparing parser speeds is especially problematic because of implementation differences and the fact that the accuracy of the parsers is not being controlled. Thus we are not making any strong claims about the efficiency of parsing with CCG compared to other formalisms.
However, the results in Table 12 add considerable weight to one of our main claims in this article, namely, that highly efficient parsing is possible with scale processing is possible with linguistically motivated grammars. 11. Cross-Formalism Comparison
An obvious question is how well the CCG parser compares with parsers using different grammar formalisms. One question we are often asked is whether the 530 output by the parser could be converted to Penn Treebank X  X tyle trees to enable a com-parison with, for example, the Collins and Charniak parsers. The difficulty is that derivations often have a different shape to the Penn Treebank analyses (coordination being a prime example) and reversing the mapping used by Hockenmaier to create CCGbank is a far from trivial task.

Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003). Cahill et al. (2004) evaluate an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations ( GR s) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GR s of the Buchholz parser, are mapped into the required grammatical relations, with the result that the most accurate.
 parser, the Collins parser is at an immediate disadvantage. This is especially true if the alternative output is significantly different from the Penn Treebank trees and if the in-formation required to produce the alternative output is hard to extract. One could argue that the relative lack of grammatical information in the output of the Collins parser is a weakness and any evaluation should measure that. However, we feel that the onus of mapping into another formalism should ideally lie with the researchers making claims about their own particular parser. The second difficulty is that some constructions may be analyzed differently across formalisms, and even apparently trivial differences such as tokenization can complicate the comparison (Crouch et al. 2002).
 CCG parser. For the gold standard we chose the version of DepBank reannotated by
Briscoe and Carroll (2006) (hereafter B &amp; C ), consisting of 700 sentences from Section 23 of the Penn Treebank. The B &amp; C scheme is similar to the original DepBank scheme in many respects, but overall contains less grammatical detail. Briscoe and Carroll (2006) describe the differences between the two schemes.
 other researchers to compare against our results; the share some similarities with the predicate X  X rgument dependencies output by the parser; and we can directly compare our parser against a non-RASP parser X  X nd because we are converting the CCG output into the format used by RASP the CCG parser is not at an unfair advantage. There is also the standard (Carroll, Briscoe, and Sanfilippo 1998), on which the but we chose not to use this for evaluation. This earlier dencies output by the CCG parser, and the comparison would be complicated further by fact that, unlike CCGbank, the SUSANNE corpus is not based on the Penn Treebank. Carroll, and Watson (2006). Table 13 contains the complete list of evaluation, with examples taken from Briscoe. The CCG formed into GR s in two stages. The first stage was to create a mapping between the
CCG dependencies and the GR s. This involved mapping each argument slot in the 425 lexical categories in the CCG lexicon onto a GR . In the second stage, the for a particular sentence X  X y applying the mapping to the parser output X  X ere passed through a Python script designed to correct some of the obvious remaining differences between the CCG and GR representations.
 problem: Without looking at examples it was difficult to create the mapping and im-possible to know whether the two representations were converging. Briscoe, Carroll, 532 and Watson (2006) split the 700 sentences in DepBank into a test and development set, but the latter only consists of 140 sentences which we found was not enough to reliably create the transformation. There are some development files in the provide examples of the GR s, which we used when possible, but these only cover a subset of the CCG lexical categories.

CCGbank into GR s and use these to develop the transformation. So we did inspect the annotation in DepBank, and compared it to the transformed only the gold-standard CCG dependencies. Thus the parser output was never used dur-ing this process. We also ensured that the dependency mapping and the post-processing examples of the dependency mapping. Because the number of sentences annotated with
GR s is so small, the only other option would have been to guess at various DepBank analyses, which would have made the the evaluation even more biased against the parser.
 standard CCG dependencies with the gold-standard GR s, we can measure how close the CCG representation is to the GR s. This provides some indication of how difficult it is to perform the transformation, and also provides an upper bound on the accuracy of the parser on DepBank. This method would be useful when converting the output of the Collins parser into an alternative representation (Kaplan et al. 2004): Applying the transformation to the gold-standard Penn Treebank trees and comparing with DepBank would provide an upper bound on the performance of the Collins parser and give some indication of the effectiveness of the transformation. 11.1 Mapping the CCG Dependencies to GR s
Table 14 gives some examples of the mapping. In our notation, %l indicates the word argument slot. For many of the CCG dependencies, the mapping into ward. For example, the first two rows of Table 14 show the mapping for the transitive transitive verb category, Kim is the subject, and oranges is the head of the constituent filling the object slot, leading to the following GR s: (ncsubj likes Kim ) and (dobj likes oranges) . The third row shows an example of a modifier: ( fies a verb phrase to the right. Note that, in this example, the order of the lexical category (%l) and filler (%f) is switched compared to the previous example to match the DepBank annotation.
 more difficult than these examples suggest. The first problem is that the mapping from
CCG dependencies to GR s is many-to-many. For example, the transitive verb category
Imperial Savings &amp; Loan . With the default annotation the relation between is and parent would be dobj , whereas in DepBank the argument of the copular is analyzed as an
The constraint in the first example means that, whenever the word associated with the transitive verb category is a form of be , the second argument is xcomp , otherwise the default case applies (in this case dobj ). There are a number of categories with similar constraints, checking whether the word associated with the category is a form of be . category of the word filling the argument slot. In this example, if the lexical category of the preposition is PP / NP , then the second argument of ( S [ dcl ] thus in The loss stems from several factors the relation between the verb and preposition is (iobj stems from) . If the lexical category of the preposition is PP / ( S [ ng ] then the GR is xcomp ;thusin The future depends on building cooperation the relation between the verb and preposition is (xcomp depends on) . There are a number of
CCG dependencies with similar constraints, many of them covering the iobj / xcomp distinction.
 relations, whereas the CCG dependencies are all binary. The primary example of this is to-infinitival constructions. For example, in the sentence The company wants to wean itself away from expensive gimmicks ,the CCG parser produces two dependencies relating wants , to and wean , whereas there is only one GR : (xcomp to wants wean) .Thefinalrowof 534
Table 15 gives an example. We implement this constraint by introducing a %k variable into the GR template which denotes the argument of the category in the constraint column (which, as before, is the lexical category of the word filling the argument slot). In the example, the current category is ( S [ dcl ] \ NP 1 ) / ( S [ to ] wants ; this combines with ( S [ to ] \ NP ) / ( S [ b ] \ of the current category when creating the GR s.
 ventions differ between DepBank and CCGbank. By  X  X ead passing X  we mean the mechanism which determines the heads of constituents and the mechanism by which words become arguments of long-range dependencies. For example, in the sentence
The group said it would consider withholding royalty payments , the DepBank and CCGbank annotations create a dependency between said and the following clause. However, in
DepBank the relation is between said and consider , whereas in CCGbank the relation is between said and would . We fixed this problem by changing the head of would consider to be consider rather than would . In practice this means changing the annotation of all the relevant lexical categories in the markedup file. 8 The majority of the categories to which this applies are those creating aux relations.
 lations in CCGbank than DepBank. In the previous example, CCGbank has a subject relation between it and consider ,andalso it and would , whereas DepBank only has the relation between it and consider . In practice this means ignoring a number of the subject dependencies output by the CCG parser, which is implemented by annotating the relevant lexical categories plus argument slot in the markedup filewithan X  X gnore X  marker.
 ment of relative pronouns. For example, in Sen. Mitchell, who had proposed the stream-lining , the subject of proposed is Mitchell in CCGbank but who in DepBank. Again, we implemented this change by fixing the head annotation in the lexical categories which apply to relative pronouns.
 bring the dependency annotations of CCGbank and DepBank closer together. The major types of changes have been described here, but not all the details. 11.2 Post-Processing of the GR Output
Despite the considerable changes made to the parser output described in the previous section, there were still significant differences between the dependencies and the DepBank GR s. To obtain some idea of whether the schemes were converging, we performed the following oracle experiment. We took the derivations from CCGbank corresponding to the sentences in DepBank, and ran the parser over the gold-standard derivations, outputting the newly created the DepBank GR s as a gold standard, and comparing these with the CCGbank gave precision and recall scores of only 76.23% and 79.56%, respectively. Thus given the current mapping, the perfect CCGbank parser would achieve an F-score of only 77.86% when evaluated against DepBank.
 applied to bring the schemes closer together, which we implemented as a Python post-processing script. We now provide a description of some of the major changes, to give an indication of the kinds of rules we implemented. We tried to keep the changes as handling of monetary amounts, are genre-specific. We decided to include these rules because they are trivial to implement and significantly affect the score, and we felt that, without these changes, the CCG parser would be unfairly penalized.

DepBank and CCGbank is the treatment of coordinations as arguments. Consider the example The president and chief executive officer said the loss stems from several factors. In both CCGbank and DepBank there are two conj GR s arising from the coordination: (conj and president) and (conj and officer) . 10 The difference arises in the subject of said : in DepBank the subject is and : (ncsubj said and ) , whereas in CCGbank there are two subjects: (ncsubj said president ) and (ncsubj said officer ) .Wedeal with this problem by replacing any pairs of GR s which differ only in their arguments, and where the arguments are coordinated items, with a single dination term as the argument. Two arguments are coordinated if they appear in conj relations with the same coordinating term, where  X  X ame term X  is determined by both the word and sentence position.

DepBank, but the GR for S / S is ncmod .Soany ncmod whose modifier X  X  lexical category is S / S , and whose POS tag is CC , is changed to conj .
 example, the CCGbank analysis of Standard &amp; Poor X  X  index assigns the lexical category
N / N to both Standard and &amp; , treating them as modifiers of Poor , whereas DepBank treats &amp; as a coordinating term. We fixed this by creating conj GR s between any words on either side; removing the modifier GR between the two words; and replacing any GR s in which the words on either side of the &amp; are arguments with a single which &amp; is the argument.
 2006), is difficult to assign correctly to the parser output. The simple punctuation rules used by the parser, and derived from CCGbank, do not contain enough information to distinguish between the various cases of ta . Thus the only rule we have implemented, which is somewhat specific to the newspaper genre, is to replace (cmod say arg) with (ta quote arg say) , where say can be any of
This rule applies to only a small subset of the ta cases but has high enough precision to be worthy of inclusion.
 surprising given the difficulty that human annotators have in distinguishing arguments and adjuncts. There are many cases where an argument in DepBank is an adjunct in CCGbank, and vice versa. The only change we have made is to turn all ncmod 536 of as the modifier into iobj GR s(unlessthe ncmod is a partitive predeterminer). This was found to have high precision and applies to a significant number of cases. ples include any dependencies in which a punctuation mark is one of the arguments, and so we removed these from the output of the parser.
 slot specifies additional information about the GR ; examples include the value obj in a passive ncsubj , indicating that the subject is an underlying object; the value num in ncmod , indicating a numerical quantity; and prt in ncmod to indicate a verb particle.
The passive case is identified as follows: Any lexical category which starts S [ pss ] indicates a passive verb, and we also mark any verbs POS tagged the lexical category N / N as passive. Both these rules have high precision, but still leave many of the cases in DepBank unidentified. Many of those remaining are modifiers, so we did not attempt to extend these rules further. The numerical case is identified using two rules: the num subtype is added if any argument in a the lexical category N / N [ num ], and if any of the arguments in an ncmod is CD . prt is added to an ncmod if the modifiee has a modifier has POS tag RP .
 from which the grammar is extracted, so that it could be integrated into the parser in a principled way. However, in order that the parser evaluation be as fair and informative as possible, it is important that the parser output conform as closely to the gold standard as possible. Thus it is appropriate to use any general transformation rules, as long as they are simple and not specific to the test set, to achieve this.

CCGbank dependencies when compared with DepBank; the simple post-processing rules have increased the F-score from 77.86% to 84.76%. However, note that this F-score provides an upper bound on the performance of the against CCGbank. Section 11.4 contains more discussion of this issue. 11.3 Results
The results in Table 16 were obtained by parsing the sentences from CCGbank corre-(2006). We used the CCGbank sentences because these differ in some ways from the original Penn Treebank sentences (there are no quotation marks in CCGbank, for ex-ample) and the parser has been trained on CCGbank. Even here we experienced some unexpected difficulties, because some of the tokenization is different between DepBank and CCGbank (even though both resources are based on the Penn Treebank), and there are some sentences in DepBank which have been significantly shortened (for no apparent reason) compared to the original Penn Treebank sentences. We modified the
CCGbank sentences X  X nd the CCGbank analyses because these were used for the oracle experiments X  X o be as close to the DepBank sentences as possible. All the results were obtained using the RASP evaluation scripts, with the results for the from Briscoe, Carroll, and Watson (2006). The results for CCGbank were obtained using the oracle method described previously.
Curran and Clark (2003) tagger. For the parser we used the hybrid dependency model and the maximum recall decoder, because this obtained the highest accuracy on CCGbank, with the same parser and supertagger parameter settings as described in
RASP parser is also 100%: 84% of the analyses are complete parses rooted in S and the rest are obtained using a robustness technique based on fragmentary analyses (Briscoe and Carroll 2006). The coverage for the oracle experiments is less than 100% (around 95%) since there are some gold-standard derivations in CCGbank which the parser is unable to follow exactly, because the grammar rules used by the parser are a subset of those in CCGbank. The oracle figures are based only on those sentences for which there is a gold-standard analysis, because we wanted to measure how close the two resources are and provide an approximate upper bound for the parser. (But, to repeat, the accuracy figures for the parser are based on the complete test set.) 538 #
GR s is the number of GR s in DepBank. For a GR in the parser output to be correct, it has to match the gold-standard GR exactly, including any subtype slots; however, it is possible for a GR to be incorrect at one level but correct at a subsuming level. For example, if an ncmod GR is incorrectly labeled with xmod , but is otherwise correct, it will be correct for all levels which subsume both ncmod and xmod , for example mod . Thus the scores at the most general level in the GR hierarchy ( dependent ) correspond to unlabeled accuracy scores. The micro-averaged scores are calculated by aggregating the counts for all the relations in the hierarchy, whereas the macro-averaged scores are the mean of the individual scores for each relation (Briscoe, Carroll, and Watson 2006).
 overall, and also higher on the majority of GR types. Relations on which the performs particularly well, relative to RASP ,are conj , det , ncmod , cmod , ncsubj , dobj , obj2 ,and ccomp . The relations for which the CCG parser performs poorly are some of the less frequent relations: ta , pmod , xsubj , csubj ,and pcomp ; in fact pmod and pcomp are not in the current CCG dependencies to GR s mapping. The overall F-score for the parser, 81.14%, is only 3.6 points below that for CCGbank, which provides an upper bound for the CCG parser.
 (Kaplan et al. 2004) on DepBank, obtaining similar results overall, but acknowledging that the results are not strictly comparable because of the different annotation schemes used. 11.4 Discussion
We might expect the CCG parser to perform better than RASP is not tuned to newspaper text and uses an unlexicalized parsing model. On the other hand the relatively low upper bound for the CCG parser on DepBank demonstrates the considerable disadvantage of evaluating on a resource which uses a different annotation scheme to the parser. Our feeling is that the overall F-score on DepBank understates the accuracy of the CCG parser, because of the information lost in the translation. evaluation is the set of labeled dependencies used. In CCGbank there are many more labeled dependencies than GR s in DepBank, because a dependency is defined as a lexical category-argument slot pair. In CCGbank there is a distinction between the direct object of a transitive verb and ditransitive verb, for example, whereas in DepBank these would both be dobj . In other words, to get a dependency correct in the CCGbank evaluation, the lexical category X  X ypically a subcategorization frame X  X as to be correct. In a final experiment we used the GR s generated by transforming CCGbank as a gold standard, against which we compared the GR s from the transformed parser output. The resulting F-score of 89.60% shows the increase obtained from using gold-standard from CCGbank rather than the CCGbank dependencies themselves (for which the F-score was 85.20%).
 ually corrected, whereas CCGbank, including the test sections, has been produced semi-automatically from the Penn Treebank. There are some constructions in CCGbank X  noun compounds being a prominent example X  X hich are often incorrectly analyzed, simply because the required information is not in the Penn Treebank. Thus the evalua-tion on CCGbank overstates the accuracy of the parser, because it is tuned to produce the output in CCGbank, including constructions where the analysis is incorrect. A similar comment would apply to other parsers evaluated on, and using grammars extracted from, the Penn Treebank.
 cross-formalism parser comparisons. Note that the difficulties are not unique to and many would apply to any cross-formalism comparison, especially with parsers using automatically extracted grammars. Parser evaluation has improved on the origi-nal PARSEVAL measures (Carroll, Briscoe, and Sanfilippo 1998), but the challenge still remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms. 12. Future Work
One of the key questions currently facing researchers in statistical parsing is how to adapt existing parsers to new domains. There is some experimental evidence showing that, perhaps not surprisingly, the performance of parsers trained on the
Treebank drops significantly when the parser is applied to domains outside of news-extract a CCG grammar from the Penn Treebank, together with the preprocessing of the Penn Treebank which was required, took a number of years; and developing the Penn Treebank itself also took a number of years.
 questions from the TREC Question Answering ( QA ) track. Because of the small number of questions in the Penn Treebank, the performance of the parser was extremely poor X  well below that required for a working QA system. The novel idea in Clark, Steedman, and Curran was to create new training data from questions, but to annotate at the lexical category level only, rather than annotate with full derivations. The idea is that, because lexical categories contain so much syntactic information, adapting just the supertagger to the new domain, by training on the new question data, may be enough to obtain good parsing performance. This technique assumes that annotation at the lexical category level can be done relatively quickly, allowing rapid porting of the supertagger. We were able to annotate approximately 1, 000 questions in around a week, which led to an accurate supertagger and, combined with the Penn Treebank parsing model, an accurate parser of questions.
 have developed a method for training the dependency model which requires lexical category data only (Clark and Curran 2006). Partial dependency structures are extracted from the lexical category sequences, and the training algorithm for the dependency model is extended to deal with partial data. Remarkably, the accuracy of the depen-dency model trained on data derived from lexical category sequences alone is only 1.3% labeled F-score less than the full data model. This result demonstrates the significant amount of syntactic information encoded in the lexical categories. Future work will look at applying this method to biomedical text.
 of the supertagger and parser. In Curran, Clark, and Vadas (2006) we investigate using the multi-tagging techniques developed for the supertagger at the idea is to maintain some POS tag ambiguity for later parts of the parsing process, using the tag probabilities to decide which tags to maintain. We were able to reduce the drop 540 in supertagger accuracy by roughly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage.
 supertagger as part of the parse selection process. These scores could be incorporated We would also like to investigate using the generative model of Hockenmaier and a discriminative framework has been beneficial for reranking approaches (Collins and log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking.
 large amounts of additional training data, by taking the lexical categories chosen by the parser as gold-standard training data. If enough unlabeled data is parsed, then the large volume can overcome the noise in the data (Steedman et al. 2002; Prins and van Noord 2003). We plan to investigate this idea in the context of our own parsing system. 13. Conclusion
This article has shown how to estimate a log-linear parsing model for an automat-ically extracted CCG grammar, on a very large scale. The techniques that we have developed, including the use of a supertagger to limit the size of the charts and the use of parallel estimation, could be applied to log-linear parsing models using other grammar formalisms. Despite memory requirements of up to 25 how a parallelized version of the estimation process can limit the estimation time to under three hours, resulting in a practical framework for parser development. One of the problems with modeling approaches which require very long estimation times is that it is difficult to test different configurations of the system, for example different feature sets. It may also not be possible to train or run the system on anything other than short sentences (Taskar et al. 2004).
 the charts considerably compared with naive methods for assigning lexical categories, pertagger and parser enables highly efficient as well as accurate parsing. The parser is significantly faster than comparable parsers in the NLP literature. The supertagger we have developed can be applied to other lexicalized grammar formalisms.
 for CCG . In particular, we have shown how to define a exploits all derivations, including nonstandard derivations. These nonstandard deriva-whether efficent estimation and parsing algorithms can be defined for models which use these derivations. We have also defined a new parsing algorithm for maximizes expected recall of predicate X  X rgument dependencies. This algorithm, when combined with normal-form constraints, gives the highest parsing accuracy to date on CCGbank. We have also given competitive results on DepBank, outperforming a non-
CCG parser ( RASP ), despite the considerable difficulties involved in evaluating on a gold standard which uses a different annotation scheme to the parser.

CCG is necessarily ineffficient because of CCG  X  X   X  X purious X  ambiguity. We have efficient parsing is practical with CCG . Linguistically motivated grammars can now be used for large-scale NLP applications. 12 Appendix A The following rules were selected primarily on the basis of frequency of occurrence in Sections 02 X 21 of CCGbank.
 Type-Raising Categories for NP , PP and S [ adj ] \ NP
S / ( S \ NP ) ( S \ NP ) \ (( S \ NP ) / NP ) (( S \ NP ) / NP ) \ ((( S \ NP ) / NP ) / NP ) (( S \ NP ) / ( S [ to ] \ NP )) \ ((( S \ NP ) / ( S [ to ] (( S \ NP ) / PP ) \ ((( S \ NP ) / PP ) / NP ) (( S \ NP ) / ( S [ adj ] \ NP )) \ ((( S \ NP ) / ( S [ adj ] ( S \ NP ) \ (( S \ NP ) / PP ) ( S \ NP ) \ (( S \ NP ) / ( S [ adj ] \ NP )) Unary Type-Changing Rules The category on the left of the rule is rewritten bottom-up as the category on the right. 542 Punctuation Rules
A number of categories absorb a comma to the left, implementing the following schema: The categories are as follows, where S [  X  ] matches an S category with any or no feature: N , NP , S [  X  ], N / N , NP \ NP , PP \ PP , S / S , S \ ( S \ NP ) / ( S \ NP ), (( S \ NP ) \ ( S \ NP )) \ (( S \ NP )
Similarly, a number of categories absorb a comma to the right, implementing the fol-lowing schema: The categories are as follows:
N , NP , PP , S [ dcl ], N / N , NP \ NP , S / S , S \ S , S [ ( S [ dcl ] \ S [ dcl ]) \ NP ,( S [ dcl ] \ NP ) / NP ,( S [ dcl ] ( S \ NP ) \ ( S \ NP ), ( S \ NP ) / ( S \ NP )
These are the categories which absorb a colon or semicolon to the left, in the same way as for the comma:
N , NP , S [ dcl ], NP \ NP , S [  X  ] \ NP ,( S \ NP ) \ ( S These are the categories which absorb a colon or semicolon to the right:
N , NP , PP , S [ dcl ], NP \ NP , S / S , S [  X  ] \ NP ,( S [ dcl ] ( S \ NP ) / ( S \ NP ) These are the categories which absorb a period to the right:
N , NP , S [  X  ], PP , NP \ NP , S \ S , S [  X  ] \ NP , S [ These are the categories which absorb a round bracket to the left: N , NP , S [ dcl ], NP \ NP ,( S \ NP ) \ ( S \ NP ) These are the categories which absorb a round bracket to the right: N , NP , S [ dcl ], N \ N , N / N , NP \ NP , S [ dcl ] \ ( S \ NP ) \ ( S \ NP ), ( S \ NP ) / ( S \ NP )
There are some binary type-changing rules involving commas, where the two categories on the left are rewritten bottom-up as the category on the right:
Finally, there is a comma coordination rule, and a semicolon coordination rule, repre-sented by the following two schema: The categories which instantiate the comma schema are as follows:
N , NP , S [  X  ], N / N , NP \ NP , S [  X  ] \ NP ,( S \ NP ) The categories which instantiate the semicolon schema are as follows:
NP , S [  X  ], S [  X  ] \ NP 544 Other Rules
There are two rules for combining sequences of noun phrases and sequences of declar-ative sentences:
Finally, there are some coordination constructions in the original Penn Treebank which were difficult to convert into CCGbank analyses, for which the following rule is used: Appendix B
The annotation in the markedup file for some of the most frequent categories in CCG-bank is shown in Section 11.1. The annotation provides information about heads and dependencies, and also the mapping from CCG dependencies to the relations plus the category annotated with head and dependency information. Variables in curly brackets indicate heads, with  X   X  used to denote the word associated with the lexical category. For example, if the word buys is assigned the transitive verb category
Co-indexing of variables allows head passing; for example, in the relative pronoun cat-egory (( NP { Y }\ NP { Y } 1 ) { } / ( S [ dcl ] { Z } 2
NP is taken from the NP which is modified to the left, and this head also becomes the subject of the verb phrase to the right. So in the man who owns the company , the subject of owns is man .
 nominal modifier category ( N { Y } / N { Y } 1 ) { } , there is one dependency between the modifier and the modifiee. Long-range dependencies are indicated by marking head variables with  X  .The  X  in the relative pronoun category indicates that when the Y vari-able unifies with a lexical item, this creates a long-range subject dependency. ! .ThisisusedtoproducetheDepBank GR s. For example, the relative pronoun category has a second annotation which results in who being the subject of owns in the man who owns the company , rather than man , because this is consistent with DepBank. The first annotation is consistent with CCGbank.
 described in Section 11.1.
 546 Acknowledgments 548 550
