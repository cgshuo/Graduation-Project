 In this paper, we study collaborative filtering (CF) in an interactive setting, in which a recommender system contin-uously recommends items to individual users and receives interactive feedback. Whilst users enjoy sequential recom-mendations, the recommendation predictions are constant-ly refined using up-to-date feedback on the recommended items. Bringing the interactive mechanism back to the CF process is fundamental because the ultimate goal for a rec-ommender system is about the discovery of interesting items for individual users and yet users X  personal preferences and contexts evolve over time during the interactions with the system. This requires us not to distinguish between the stages of collecting information to construct the user profile and making recommendations, but to seamlessly integrate these stages together during the interactive process, with the goal of maximizing the overall recommendation accuracy throughout the interactions. This mechanism naturally ad-dresses the cold-start problem as any user can immediately receive sequential recommendations without providing rat-ings beforehand. We formulate the interactive CF with the probabilistic matrix factorization (PMF) framework, and leverage several exploitation-exploration algorithms to se-lect items, including the empirical Thompson sampling and upper confidence bound based algorithms. We conduct our experiment on cold-start users as well as warm-start users with drifting taste. Results show that the proposed methods have significant improvements over several strong baselines for the MovieLens, EachMovie and Netflix datasets. H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Information Filtering Interactive Collaborative Filtering, Exploitation-Exploration, Personalization, Recommender Systems  X  The first two authors have equal contribution to this work.
An increasing number of online services have adopted rec-ommender systems to help users discover information; users can either browse, search or simply receive recommendations to satisfy their needs. Amazon reported 35% of purchases o-riginated from recommended items in 2006 [23], and Google News improved its traffic by 38% via its personalized rec-ommender system [11]. Despite providing a more intelligent way to discover relevant items, the use of traditional rec-ommender systems has been limited and they are usually positioned as a complement to searching and browsing. More recently, as a standalone Web service, Interactive Recommender Systems have emerged, by which 100% vol-ume of their services is comprised of direct user interactions with the recommender system over time. In many such sys-tems, there is no need for users to actively search for content. Instead, items, such as webpages ( StumbleUpon.com ), songs ( Pandora.com ) or deals ( Groupon.com ), can be sequentially recommended to individual users, whilst feedback on rec-ommended items is continuously observed. During the in-teractions, the recommender system is continuously refined by receiving feedback on delivered items and the user can enjoy sequential recommendations. Figure 1(a) provides s-napshots of the StumbleUpon Web discovery service and the Pandora radio streaming service. Such interactive recom-mendation services, despite easy to use, pose a  X  X hicken-or-the-egg X  problem in providing accurate personalized recom-mendations. Successful personalized recommendation pre-diction requires adequate observations of user X  X  preferences. However, as illustrated in Figure 1(b), the preference ob-servation is restricted only to the items that have been rec-ommended. Therefore a critical problem is: how to rapidly learn a new user X  X  interest while not compromising his/her recommendation experience? Or, from the modelling per-spective, how to balance between the goals of learning the user profile and providing accurate predictions?
In such interactive recommendations, the first series of interactions with a new user are especially important. It is usually called the cold-start problem in the literature. Ex-isting techniques address the problem by using a two-phase process, i.e., to use active learning [21, 29] or an interview process (preference elicitation) [41, 16] to first learn the us-er profile, and then to make recommendations based on the established profile. The two-phase solution solves the prob-lem to a certain extent, but may not provide a complete time-consuming and sometimes a burden for him/her; the user may have already left the service before his/her profile has been established. Recommendations would preferably be provided right from the very beginning and interests ac-quired by employing a less intrusive and implicit method of (a) Two examples of Interactive Recommender Systems. Figure 1: Interactive Recommendation: a  X  X hicken-or-the-egg X  problem. Recommendation requires feedback, whereas the feedback is only on the rec-ommended items. The scenario is also different from common relevance feedback in information retrieval, which normally handles the feedback in one or two iterations and lacks an established way to balance the exploration and exploitation [34]. gradually learning user profiles. A successful algorithm for interactive recommendation should not distinguish between the two phases, but continuously detect/learn the user pro-file while trying to satisfy the user at the same time.
The two inter-connected objectives mentioned above are closely related to the Exploitation-Exploration (EE) prob-lem [6, 15, 25]. It is the dilemma whether, for each inter-action, we should try to satisfy the user X  X  interest with the best-guessed item based on current knowledge, or whether we should try some sub-optimal yet discriminative items to gain more knowledge about the user. The EE problem has been heavily studied in the machine learning and statis-tics communities, and multi-armed bandits are the gener-ic setting of an EE problem. Many algorithms have been proposed under the independent-arm assumption, such as probability-based methods, e.g., -greedy [6], epoch-greedy [24], Exp3 and Exp4 [7], and index-based methods, e.g., Git-tins Index [15] and the Upper-Confidence Bound [5, 6].
The EE principle has been applied for personalized con-tent recommendation. For example, in news article delivery tasks [25], a contextual-bandit algorithm has been proposed on the basis of the content features (texts) of the news ar-ticles and the browsing contexts of the users. The idea was to approximate the news articles X  profile features and max-imize the overall delivery performance across news articles using a linear prediction model.

However, it is unclear as to how to model the interaction in pure collaborative filtering settings where there is no content data to represent users and items and the only observations are ratings. In this paper, in order to naturally integrate with existing collaborative filtering approaches, we address the Interactive Collaborative Filtering (ICF) problem under the popular matrix factorization framework, which has been proven to be effective in various recommendation competi-tions [22]. Specifically, we extend the probabilistic matrix factorization (PMF) [33] to build the probabilistic model of the user-item ratings over time. Our intention is to max-imize the users X  overall satisfaction throughout the recom-mendation journey. By modeling the rating with PMF, the uncertainty of a predicted rating comes from both the user feature vectors and the item feature vectors. While many existing multi-armed-bandit algorithms have addressed the problem by assuming arms to be independent [9, 24, 6], some others have tackled special cases where arms are structured in linear forms [5, 25]. In this paper, to consider the uncer-tainty in both the user and item feature vectors, we adopt empirical effective algorithms such as Thompson sampling [8] and construct a sampling process from their distribu-tions. Furthermore, in some cases, the item feature vectors are well-learnt and thus the item-side uncertainties could be disregarded. The problem then falls into a linear form in the item feature vectors, and thus various linear bandit al-gorithms [5] are applicable, including a variation of -greedy algorithm, linear and generalized linear upper-confidence-bound algorithms. Experimental results on both cold-start and warm-start (with drifting taste) users show significant improvements over several strong baselines for the Movie-Lens, EachMovie and Netflix datasets.

The rest of this paper is organized as follows. Section 2 discusses the related work. Our solution is formally present-ed in Section 3. The experiment is described in Section 4, and we conclude this paper in Section 5.
The CF research has been traditionally focused on pre-dicting the unknown ratings of a target user as accurately as possible from a collection of user profiles. Major solutions are categorized into two classes: The memory-based meth-ods make the recommendation by explicitly modelling the user or item similarities [19, 12] or combining them together [38], while the model-based methods provide recommenda-tions by developing a  X  X odel X  of user ratings. For instance, latent factor models have become quite popular during the recent years [20], while the matrix factorization techniques [22] have shown their effectiveness in various settings such as the Netflix and Yahoo! music competitions.

A key challenge in CF is to effectively predict preferences for new users, a problem generally referred to as the us-er cold-start problem [2, 35]. A straightforward method to tackle the problem is to interview the user to provide addi-tional information (e.g. favorite genres) or to ask the user to rate a set of items in order to provide enough data for the recommender system. The items used in the interview can be selected based on measures such as popularity, entropy and coverage [30, 31], or a decision tree to partition the users [41, 17]. Active learning is also deployed in order to identify the most informative set of training examples (items) with respect to some selection criterion, such as the expected in-formation gain [18, 21], with the target of minimizing the number of interactions. In summary, all those methods first explicitly figure out the user profile and then use the estab-lished user profile to make further recommendations. How-ever, asking users to take interviews is still time-consuming and sometimes a hurdle for them to overcome even if the effort has been kept minimum. By contrast, ICF that we propose does not distinguish between the stages of learn-ing user profiles and making satisfactory recommendations, but naturally integrates them together. As such, any user, whether a new user or not, can immediately receive sequen-tial recommendations and explore items without overcoming the hassle of providing ratings before hand.
 In the machine learning and statistics communities, the EE problem has been well studied by considering the multi-armed bandit settings [9, 24, 5]. Under the assumption that rewards of arms are independent to each other, -greedy is a straightforward algorithm that adds the probability of random exploration [6] into a greedy algorithm. The epoch-greedy method generalizes it for the case that the total time T is unknown so that the exploitation and exploration take place alternatively in each epoch to minimize the regret (i.e., the cumulative loss compared to the optimal one) [24], and it achieves a regret of e O ( T 2 / 3 ) with a high probabili-ty. The confidence bound algorithms seek to find a region that bounds the expected payoff with a high probability, and within the region, the arm with highest upper confi-dence bound is selected: the EXP4 approach [7] achieves an e O (  X  approaches lies in that when the number of arms is huge, ex-ploration becomes difficult. As such, the underlying struc-ture of the arms should be considered. A well-known sce-nario is that the expected reward is a linear function with respect to the features of them, where the linear bandit algo-rithm is proposed [5, 10, 1]. On the other hand, in the con-textual bandit model with side information, the structure is modelled by positioning each arm into a feature space [13], while a more general linear setting is discussed in [14]. Be-sides the above algorithms, Thompson sampling yet provides a more flexible way to tackle the multi-armed bandit prob-lems [8] which does not restrict the reward function forms to a linear one. Therefore, when we model the interaction, we propose to use Thompson sampling [8], which provides an empirical solution where the uncertainties of both the learnt feature vectors of the user and the item are considered.
The contextual bandit models have been applied to mod-el news article recommendations [25] and online advertising [27], where in each timestep a context (in the form of a fea-ture vector) is revealed, and an arm (either a piece of news or an ad) is selected based on the context. The contextual bandit approaches can be naturally applied to both cases because in both cases the content features such as the user X  X  demography and location information, and the item X  X  tex-tual descriptions, preexist and can be immediately used to represent the  X  X ontext X . In our domain-free scenario where users or items are presented only by ratings, it is, however, essential to derive a sensible representation for the correlat-ed arms (items) and the contexts (users) and combine them together. Suppose the system has N items and M users in record. The ratings between them are recorded in the preference ma-trix R M  X  N in which each element r ui is the observed rating from the user u to the item i . Without loss of generality, we consider the following process in discrete timesteps. Sup-pose the target user is now denoted simply by u . At each timestep t  X  [1 , 2 ,...,T ], the system delivers (recommends) an item to the target user. The user will then give feedback in the form of ratings, or  X  X ike X  X  and  X  X islike X  X , or ignore the recommendation ( X  X nknown X  X ). In either way, we denote the feedback as r u,i ( t ) , the rating collected by the system from user u to the recommended item i ( t ) at timestep t . In other words, r u,i ( t ) is the  X  X eward X  collected by the system from this target user. After receiving feedback, the system up-dates its model and decides which item to recommend next.
Let X  X  denote H ( t ) as the available information at t the system has for the target user The item is selected according to a strategy  X  , which is defined as a function from the current information to the selected item: The optimal strategy should maximize the cumulated ex-pected reward during T timesteps, recommender systems, here we use reward rather than regret to express the objective function, and maximizing cumula-tive reward is equivalent to minimizing regret. Here we con-sider the quality of recommendations at different timesteps as equally important, and summarize the user X  X  overall sat-isfaction at a given period T . In our experiments, we show that a higher level of exploration is required in order to achieve a longer-term cumulative reward.

This objective falls into the target of the multi-armed ban-dit problem, where we regard each item as each arm of the bandit. The next questions are how to estimate the reward and how to optimize the objective function. Using the la-tent factor model [20], the rating is a product of user and item feature vectors p u and q i . This is widely used in many CF algorithms: where  X   X  N (0 , X  2 ) is the observation noise. The objective function is then re-formulated as follows: The question now is how to optimize the objective function.
Both p u and q i are random variables following certain sample an item based on its probability of being optimal [8], p ( i ( t ) = i ) = where I is the indicator function; it is 1 when the equali-ty holds (i.e., when item i has the highest expected rating given p u and q i ); otherwise 0. Thus, integrating p u and q out gives the probability of being optimal at t for item i . The integration is computational expensive, but in practice, no need to compute it explicitly. In this paper, a sampling approach, called Thompson sampling, is leveraged to ap-proximate the integration in Eq. (6) [8]. A nice property of Thompson sampling is that the integration is circumvented by sampling both the user and item feature vectors togeth-er from their distributions (consider the uncertainty from both aspects) and picking the item that leads to the largest expectation of the reward: where  X  p u and  X  q i mean the sampled user and item feature vectors, which will be described in the next section.
In this section, we adopt the PMF model [33] to build the distributions for the user and the item feature vectors, which are then used to generate the samples. Specifically, the con-ditional probability distribution of the rating given the user and item feature vectors follows a Gaussian distribution
We denote P ( Q ) as the user (item) feature vector ma-trix, where each row vector represents a user (item) feature vector ( P = [ p 1 , p 2 ,..., p M ] 0 , Q = [ q 1 , q 2 ,..., q distribution of the preference matrix R given P and Q is then the joint probability, i.e., where  X  ij = 1 if user u rated item i and  X  ij = 0 otherwise.
Similar to the PMF model [33], we define the prior dis-tributions of the user and item feature vectors as Gaussian with prior variances  X  2 p and  X  2 q
By observing the ratings R , we can obtain the posteri-or distributions for the user and item feature vectors [33]. Here we focus on the conditional distribution of the user (item) feature vectors, given the current item (user) feature vectors to implement Markov chain Monte Carlo and Gibbs Sampling (MCMC-Gibbs): p ( P | R,Q, X  2 , X  2 p , X  2 q )  X  p ( R | P,Q, X  2 )  X  p ( P |  X   X 
Y  X 
Y  X 
Y  X 
Y
It means for each user its feature vector follows a Gaussian distribution given item feature vectors:
Here, D u is the observational matrix for the user, each row of which is the feature vectors of the user rated items sampled from their posteriors. r u denotes the vector of cor-responding ratings of these items for the given user u , and  X  Algorithm 1 Thompson sampling Require: parameters for the item feature vector distribu-tions  X  = { (  X  1 ,  X  1 ) ,..., (  X  N ,  X  N ) } ,  X  ,  X  p
Initialization: A  X   X  p I b  X  0 for t = 1 , 2 , 3 ,...,T do end for
Similarly, the posterior distribution for the item feature vector q i conditioned on the sampled user feature vectors can be obtained as and B i is the observational matrix with each row the sam-pled user feature vector.

The distributions converge by alternatively sampling the item and the user feature vectors according to the condi-tional distributions for them. Then both the expected user and item feature vectors (  X  u and  X  i ) and their uncertainties ( X  u and  X  i ) are obtained.
Thompson sampling can be implemented according to the distributions while they are updated online whenever new ratings are collected by the system. However, in the ICF scenario, the distribution of the target user X  X  feature vec-tor is much more sensitive to his/her feedback on the items. On the item side, since each item has usually collected rel-feature vector immediately after receiving each rating from the target user, and we choose to periodically retrain them. Therefore, we simply use the notation  X  q i to express sam-pled item feature vector from the presently calculated item feature vector distribution. For the target user, its observa-tional matrix grows each time, and its distribution can be described similarly conditioned on the observations: where
Similarly, D u,t is the observational matrix with each row is the recommended item feature vector, and  X  u,t is the uncertainty of the user feature vector at time t . From Eq. (7), the Thompson sampling method with the PMF modeling suggests to choose the item with the highest value of the inner product of the sampled values, and Eq. (7) can be approximated as: where  X  p u,t is sampled from the estimated distribution in Eq. (19). The algorithm is described in Algorithm 1.

Thompson sampling enables exploration through the  X  X idth X  of the distributions of the inner product of the user and Algorithm 2 Linear UCB Require: MAP solution of item feature vectors Q = {  X  1 ,...,  X  N } ,  X  ,  X  p , and  X   X  R +
Initialization: A  X   X  p I b  X  0 for t = 1 , 2 , 3 ,...,T do end for item feature vectors. The  X  X idth X  further comes from both the uncertainties of the user and item feature vectors. By this approach described above, the uncertainties of the user and item feature vectors are considered on the same foot-ing. However, considering ICF as a user-centric scenario (Figure 1), the obtained knowledge on the target user side may be much more important than that on the item side, especially when items have collected many ratings and thus are always already well-learnt. Therefore, in the following part, we adopt a biased view so that the item feature vectors are assumed to be well-learnt as the maximum a posterior (MAP) solution  X  i from the distributions obtained by PMF, and only the user feature vector distributions are maintained for the sampling process.
With the item feature vectors known and fixed, the re-ward in Eq. (4) tends to be a linear form with the item feature vectors as coefficients, and the essence of the EE is to approach the user feature vector. Therefore, such prob-lem falls into the framework of linear bandits [5]. Linear Upper-Confidence-Bound (UCB) algorithm, and its varia-tions are widely used for such problems. In this way, we take the MAP estimation of the item feature vectors  X  i as the representatives of the items and assume them to be fixed.
In the following, linear and generalized linear UCB algo-rithms are presented for our problem respectively. A varia-tion of -greedy algorithm is also provided for comparison.
As mentioned above, assuming the item feature vectors as fixed, the reward function reduces to be linear in the item feature vectors, and the objective function in Eq. (4) is further written as i  X  (  X  ) = arg max where E p u [ p 0 u | t ] can be estimated according to Eq. (20).
The expected user feature vector can be obtained accord-ing to Eq. (20). Now the uncertainty of the reward can be obtained as the estimated variance of the inner product of the user and item feature vectors p 0 u  X  i , which comes from the uncertainty of the estimation in the user feature vector. The estimated variance is the 2-norm based on  X  u,t (accord-ing to Eq. (21), but note that here the observational matrix is made up of the posterior feature vectors of the items): Algorithm 3 GLM-UCB Require: MAP solution of item feature vectors Q = {  X  1 ,...,  X  N } ,  X  ,  X  p , and c  X  R +
Initialization: A  X   X  p I for t = 1 , 2 , 3 ,...,T do end for Algorithm 4 Linear -greedy Require: MAP solution of item feature vectors Q = {  X  1 ,...,  X  N } ,  X  p and  X  [0 , 1]
Initialization: A  X   X  p I b  X  0 for t = 1 , 2 , 3 ,...,T do end for
According to [37], with the item feature vectors known and fixed, the expectation of the reward by choosing item i is bounded in the interval  X  i,t with probability at least 1  X   X  where  X  = 1+ p ln (2 / X  ) / 2. The bounded interval motivates an UCB bandit algorithm, i.e., at each timestep, choose the item with the highest upper confidence bound:
The algorithm is given in Algorithm 2. The algorithm is proven to have a very tight regret bound of  X  O (
As defined in Eq. (21), matrix  X  u,t is a regularized fish-er information matrix, measuring how much  X  X nformation X  is known about the user feature vector from the previously rec-ommended items, given the item feature vectors are known already. That is, to recommend an item that maximizes ||  X  i || 2 ,  X  u,t is to recommend an item that has been the least represented (understood) by the perviously recommended items.
The problem can be also linked to the generalized lin-ear bandit problem in [14], which gives a general solution Generalized Linear Model Bandit-Upper Confidence Bound (GLM-UCB) if we assume the reward takes the following form where  X  is a monotonically increasing function which takes a linear or nonlinear form. Here we give two options of function  X  , a linear form suggested in Eq. (4), and a sigmoid form
Similar to the derivations in LinUCB, here the item fea-ture vectors q i are approximated by the maximum a pos-terior (MAP) solution  X  i . On the other hand, we need to estimate the user feature vector according to the generalized linear model, which here we denote as  X  p u,t (note that here the solution  X  p u,t is no longer the MAP solution in Eq. (20) due to the nonlinear function  X  ). In general, according to [14], the quasi-likelihood estimator  X  p u,t of Eq. (27) is the solution of
Specifically, for a sigmoid form, it is estimated as
For a linear form, the estimate is the same as the max-imum posterior estimation of the user feature vectors Eq. (20).

The GLM-UCB algorithm follows a similar process as Lin-ear UCB, i.e., firstly  X  p u,t is estimated, and the choice of the item is based on the estimated  X  p u,t but with exploration part added which is 2-norm based on  X  u,t (Eq. (24)) multiplied by a factor c i  X  ( t ) gl = arg max The GLM-UCB algorithm is illustrated in Algorithm 3. Note that exploration term  X  is time-dependent: where c is a constant with respect to t [14]. With term c  X  that the exploration level is maintained to some extent. Us-ing the conclusion from [14], GLM-UCB has a regret bound of  X 
O (
Just like the other index-based EE algorithms [5], the algorithms have a low computational complexity, which is O ( T 3 + K 2 N ).
The linear -greedy algorithm is based on the greedy s-trategy under our setting, which can be described as
Because  X  u,t and  X  i can be seen as the MAP solutions for the PMF model, which can also be referred as the solutions by singular vector decomposition (SVD). We refer to the greedy strategy as greedy SVD, or simply SVD.

The greedy strategy is the myopic strategy that always picks the item leading to the highest expected reward based on current knowledge. Linear -greedy we adopt here is the naive algorithm which chooses the greedy strategy with probability 1  X  and explores into random items with prob-ability . The algorithm is described in Algorithm 4.
For the above algorithms, two factors contribute to the selection of the item: the exploitation factor suggested by the greedy algorithm Eq. (33), and the exploration factor The detailed form of the bound is looser than that of Lin-UCB but it is more general.
 which is controlled by parameters  X  , c and respectively. For each of the three algorithms, the larger the parameter is, the more emphasis is put onto the exploration effort accordingly.
Experiments are carefully designed to answer the follow-ing questions: (1) How can EE algorithms outperform my-opic CF algorithms for the cold-start users? (2) Among the EE algorithms, which one is the most effective and why? (3) Are the algorithms also effective for warm-start users, espe-cially those whose interests change over time? (4) Consid-ering top-n recommendation over time, can the algorithms still be effective? We base our experiments on three popular datasets Movie-Lens (100k), EachMovie and Netflix. The basic information of the datasets is summarized in Table 1.

Due to the interactive nature of our problem, an online experiment with true interactions from users would be ide-al, but it is not always possible [25]. Instead, we follow an unbiased offline evaluation scheme for contextual-bandit al-gorithms in [26]. In our setting, we assume that the ratings recorded in the datasets are users X  instinctive actions, not biased by the recommendations provided by the system. In this way, the records can be treated as unbiased to represent the feedback in an interactive setting [41].

We normalize the ratings into the range [  X  1 , 1] and s-plit the data into two user-disjoint sets: the training users and their ratings are used to train the parameters for the item distributions, as required in Thompson Sampling, and to obtain the MAP solutions of the item feature vectors, as required in UCB-based algorithms (Section 3.2.1). The item feature vector information is maintained as unchanged during the test phase when the test users go through the interactive recommendation process during T timesteps be-cause the collected ratings from the target user have trivial effect on the item feature vector distributions. According to the purpose (whether to test the performance on cold-start users, or on warm-start users), we select test users based on different criteria, detailed in each subsection. The baselines include:
Random. In each interaction, randomly chooses an item from the entire item set to recommend to the target user.
Popularity-based (Pop). The system picks the most popular items to recommend to the target user.
 Greedy SVD (SVD). This algorithm is built upon the SVD approach. We regard it as the myopic algorithm in ICF. For each target user, the system needs to retrain the SVD model after each interaction.

Active Learning (AL). Active learning methods have been proposed for the cold-start problem [18]. The idea is to minimize the uncertainty in the model, so that the item with highest uncertainty is selected [32].

Interview Process (Interview). The interview pro-cess first constructs the user profile with a number of most discriminative items, and then shifts to the greedy recom-mendation strategy. Here we follow the work of [17] and set the interview question number as 5.
 Our EE algorithms include the following variations: used to tune this model.
 function  X  as a linear function, i.e., GLM-Lin , and a sig-moid function, i.e., GLM-Sig . c is used to tune this model. tuning parameter is used to control the balance between the exploitation and exploration.

In addition, we add a constraint for all the algorithms that the same item should not be repeatedly recommended as suggested in most previous ranking-oriented recommen-dation settings [28, 40]. Three evaluation metrics are used:
Cumulative Precision@ T . A straightforward evalua-tion measure is the number of the positive ratings collected during the total T interactions: For both datasets, we define  X  hit = 1 if the rating is no less than 4, and 0 otherwise, similar to the definition of positive ratings in previous work [3].

Cumulative Recall@ T . We can also check for the recall during T timesteps of the interactions.
Cumulative nDCG@ n . For the case that multiple item-s are shown in one interaction, the ranking of the item listed is also important: it is more useful to have the highly rel-evant items appear earlier in the ranking list. We use the normalized discounted cumulative gain (nDCG@ n ) as the Figure 2: Cumulative precision against parameter turning on EachMovie. ranking measure where r j is the real rating of the item shown at ranking posi-tion j . Z is the normalization factor making the score of the optimal ranking to 1 such that 0  X  nDCG@ n  X  1. Similar to the cumulative precision and recall, here the cumulative nDCG@ n should also take sum over T and average on users.
In order to test the system X  X  performance on cold-start users, we first select users with sufficient numbers of record-ed ratings in order to test the performance. Here we ran-domly select 200 users with more than 120 ratings because up to T = 120 interactions are studied. Then the parame-ters of item feature vector distributions are trained without these user X  X  ratings according to Section 3.2.1.
Performances of proposed algorithms and the baselines for cold-start users are compared and summarized in Table 2. Optimally tuned parameters have been adopted for each T = 10 , 20 , 40 , 80 , 120. The effect of parameters will be dis-cussed in Section 4.4.3. The best-performing algorithm is shown in boldface with  X  marking significant improvements (by Wilcoxon signed-rank test). The row of improvement shows the increases brought by the best-performing algo-rithm compared to the greedy SVD strategy.

The observations can be summarized into the following points: (1) The Thompson sampling algorithm generally works better than the greedy SVD, or the -greedy algorith-m. In most cases, Thompson sampling also exceeds other baseline algorithms (according to the cumulative precision). It means that the exploration by considering the uncertain-ties of the user and items according to their probability dis-tributions, is more promising than randomly conducting ex-plorations. Nevertheless, the Thompson sampling fails to outperform the LinUCB or GLM algorithms. (2) In almost all cases, the UCB-based algorithms perform better than the baselines. In MovieLens and Netflix, the three UCB-based algorithms have close performances while in Each-Movie GLM-Lin outperforms all the baselines. The increase by the proposed EE algorithm compared to SVD is up to 7 . 7% on MovieLens, 24 . 7% on Eachmovie, and 24 . 4% on Netflix (according to the cumulative precision). All of the improvements (except one) are statistically significant. (3) Among all the proposed EE algorithms, linear -greedy per-forms worst, but still better than the greedy SVD. It sug-gests that adding some level of exploration can always im-prove the pure exploitation strategy. (4) The greedy SVD outperforms the popularity-based strategy, and obviously the random strategy performs the worst. (5) The interview strategy performs better than the active learning strategy in the long run, because it shifts to exploitation after learn-ing the user by exploration (5 timesteps). For all the three datasets, however, the interview strategy is not as good as the algorithms which are proposed based on ICF framework.
There are two possible reasons that UCB-based algorithm-s outperform the Thompson sampling method. First, the user uncertainties may play a much more important role in the ICF scenario. Consideration on item-side uncertainties may be helpful for learning the item feature vectors in the long run, but in this user-centric system, it may hamper the user experience. Second, compared with the UCB-based al-gorithms which explicitly pursue the highest possible perfor-mance for each item as their exploration strategy, Thomp-son sampling involves considerations on both the positive and negative possible performances for each item. In ad-dition, the sampling process itself imports the exploration instability. However, the UCB-based algorithms are built on the assumption that the item feature vectors are well-learnt. In the case of very limited available data and thus underestimated item feature vectors, it may be necessary to consider the uncertainty of item feature vectors. We leave this problem as our future work.
The algorithm-dependent parameters  X  , c , are used to balance between the exploitation and exploration. Here we focus on the cumulative precision as the measure of the per-formance, and investigate how the performance depends on the parameters, with respect to two horizons T = 20 and T = 120, shown in Figure 2. 2 We only show the impact of  X  for LinUCB and for linear -greedy due to the page limit whereas other cases display the similar trends.
 We observe that when either  X  or (for either the case of T = 20 or T = 120) increases, the performance first increas-es, and then falls down. The peak performance corresponds to the optimal parameter which for T = 20 is smaller than that for T = 120. This is intuitively correct because more exploration is needed when a longer-term satisfaction is tar-geted. In practice, for the decision of T , we can make use of the statistics from the system record, such as the average life-time of the users.
Through this experiment, we aim to answer the question whether the algorithms are also applicable on warm-start users to follow up their interests throughout the interaction-s, especially when their tastes are changing over time. To do this, we first divide the rating records of the users (whose ratings are more than 120) into two periods (set 1 and set 2). Then, we employ the genre information of the items as an indication of the user interest [36]. That is, we calcu-late the cosine similarity between the genre vectors of the two periods. We choose the users with the smallest cosine similarity as an indication that they have significant interest drift across the two time periods. All the other users with Empirically, we do not necessarily restrict  X  according to Eq. (25). D-Drama, R-Romance, S-Scientific Fiction, T-Thriller, W-War. their ratings compose the training set. We only conduct ex-periments on MovieLens and EachMovie datasets, as there is no movie genre information for Netflix dataset.
In order to test how the system can catch the users X  taste drift, we conduct the empirical experiment as follows: for each user, in the first period with 60 interactions, we use set 1 as the ground truth of the test users; and then, from the 61st interaction, the ground truth is changed from set 1 to set 2 to simulate the process of his/her taste drift. Table 3 presents the results of our proposed algorithms compared to the baselines on the datasets, respectively. Because we focus on the performance when the user has changed the interest, only the results for T  X  60 are shown.

From the results, it can be seen that the proposed al-gorithms outperform the baselines for both datasets. When compared with the greedy SVD method, the improvement is up to 7 . 6% on MovieLens dataset, and 17 . 5% on EachMovie dataset. Among the algorithms, the UCB-based algorithms perform better than Thompson sampling method, which is similar to the results for the cold-start experiments.
We also conduct an experiment with multiple item slots at each interaction. The ranking-aware measure nDCG is used to test the performance. The test users are the same as the ones in the cold-start setting. The only difference is that the number of interactions is reduced since the number of recommended items in each interaction increases. The results are shown in Table 4.

A similar trend is shown compared to the case of one item at each timestep: on MovieLens, either LinUCB or GLM-Lin performs the best, and on EachMovie, GLM-Lin always performs best. The results indicate that the algorithms still outperform the baselines in the multiple item setting. In addition, the performance on nDCG measure suggests that our proposed algorithms are also capable of the CF ranking problems.
In order to better illustrate why the algorithms outperfor-m greedy SVD, we present two case studies for a cold-start user and a taste-drift user respectively.
In Table 5, we present the first 15 sequentially recom-mended movies to a typical user #454 on Movielens by greedy SVD and LinUCB, and the corresponding feedback. From the results we can see that (i) LinUCB earns more  X  X ike X  feedback and less  X  X islike X  and  X  X nknown X  feedback; (ii) After the first three  X  X ikes X , SVD keeps recommending ac-tion, crime and thriller movies, which is somewhat myopic; (iii) For LinUCB, after receiving the positive and negative feedback on action, war, thriller, and science fiction movies, it tries different genres such as drama, comedy and anima-tion. After the next five interactions, LinUCB discovers the other interest in drama movies.
In Figure 3, we show a typical taste-drift case of user #833 on Movielens. Specifically, 7 typical movie genres (out of 18) are involved here. The black bars show the user X  X  taste drift by calculating the percentage difference of the normalized distributions on each genre between two time periods as in Section 4.5.1. The blue and orange bars show the percent-age difference on each genre of the recommended items by LinUCB and greedy SVD respectively. We see that LinUCB captures the user X  X  taste drift in a better way than greedy SVD: (i) For these genres, LinUCB captures the drift di-rection. For example, the user X  X  interest in Comedy movies decreases (-4.3% 3 ) between the two periods. LinUCB also recommends fewer (-3.2%) Action movies but greedy SVD recommends more (+1.1%) Action movies to the user. (i-i) For most genres, LinUCB to some extent captures the drift degree, e.g., the user has a 0.8% interest decrease on The percentage measures the difference of the proportion of Comedy movies the user watches between the two periods. Figure 3: A case study on handling taste drift.
 Romance movies and LinUCB also recommend 1.1% fewer Romance movies, but greedy SVD dramatically decreases this type of movies to the extent of 4.4%.
In this paper, we have introduced an interactive collabo-rative filtering framework. Within the framework, a prob-abilistic matrix factorization model is leveraged to capture the distributions of user and item feature vectors. And based on that, the Thompson sampling and several UCB-based algorithms are adopted to balance between the exploitation and exploration for the interactive CF problem. The experi-ments were conducted in three situations: when a cold-start user joins the system, when a warm-start user has taste drift, and when multiple items are recommended in each in-teraction. Throughout the experiments, we demonstrated that our proposed algorithms outperformed several strong baselines including the greedy SVD algorithm, the active learning and the interview approaches.

Our future work involves several possible directions. First, we will investigate the roles that the user and item feature vector uncertainties in the case of limited available data. Second, we are interested in the item cold-start problem: how to target new items to a set of existing users so that the long-term feedback collected by the item is maximized. Fi-nally, we would like to extend our work to interactive search [4] and consider the diversity of top-N ranking [39, 36], and compare our work with other interaction-based approaches.
