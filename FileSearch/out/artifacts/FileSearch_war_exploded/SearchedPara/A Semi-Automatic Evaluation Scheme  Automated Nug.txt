 In many natural language processing (NLP) tasks, we are faced w ith the problem of determining the appropriate granularity level for information units. Most commonly, we use sentences to model indi-vidual pi eces of informat ion. However, more NLP applications require us to define text units smaller than sentences, essentially decomposing sentences into a collection of phrases. Each phrase carries an independent pi ece of informat ion that can be used as a standalone unit. These finer-grained informa-tion units are usually referred to as nuggets . for redundancy and/or relevancy judgments, with-out a precise and consistent breakdown of nuggets we can only rely on rudimentary n -gram segmenta-tions of sentences to form nuggets and perform subsequent n -gram-wise text comparison. This is not satisfactory for a variety of reasons. For exam-ple, one n -gram window may contain several sepa-rate pieces of information, while another of the same length may not contain even one complete piece of informat ion. nuggets in a relatively straightforward fashion. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), ma-chine-generated summaries were compared with human-written ones at the nugget level. However, automatic creation of the nuggets is not trivial. Hamly et al. (2005) explore the enumeration and combination of all words in a sentence to create the set of all possible nuggets. Their automation proc-ess still requires nuggets to be manually created a priori for reference summaries before any sum-mary comparison takes place. This human in-volv ement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in ques-tion-answering (QA) evaluations, gold-standard answers use manually created nuggets and com-pare them against system-produced answers bro-ken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the s ame words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is per-formed over an extended period of time, the con-sistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. this paper: First, how do we define nuggets so that they are consistent in definition? Secondly, how do we utilize automatically extracted nuggets for vari-ous evaluation purposes? Based on our manual analysis and computational modeling of nuggets, we define them as follows: Definition: The anchor is either: mation associated with the anchor. Each anchor may have several separate contents. definition is applied recursively. Figure 1 shows an example. Anchors are marked with square brack-ets. If the anchor is a verb, then its entity attach-ment is marked with curly brackets. If the sentence in question is a compound and/or complex sen-tence, then this definition is applied recursively to allow decomposition. For example, in Figure 1, without recursive decomposition, only two nuggets are formed: 1)  X  X girl] working at the bookstore in Hollywood X , and 2)  X  X girl} [talked] to the diplo-mat living in Britain X . In this example, recursive decomposition produces nuggets with labels 1-a, 1-b, 2-a, and 2-b. 2.1 Nugget Extraction We use syntactic parse trees produced by the Collins parser (Collins, 1999) to obtain the struc-tural representation of sentences. Nuggets are ex-tracted by identifying subtrees that are descriptions for entities and events. For entity nuggets, we ex-amine subtrees headed by  X  X P X ; for event nuggets, subtrees headed by  X  X P X  are examined and their corresponding subjects (siblings headed by  X  X P X ) are treated as entity attachments for the verb phrases. In recent QA and summarizat ion evaluation exer-cises, manually created nuggets play a determinate role in judging system qualities. Although the two task evaluations are similar, the text comparison task in summarizat ion evaluation is more complex because systems are required to produce long re-sponses and thus it is hard to yield high agr eement if manual annotations are performed. The follow-ing experiments are conducted in the realm of summarization evaluation. 3.1 Manually Created Nuggets During the recent two Document Understanding Confereces (DUC-05 and DUC-06) (NIST, 2002 X  2007), the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. In this framework, human annotators select and highlight portions of reference summa-ries to form a pyramid of summary content units (SCUs) for each docset. A pyramid is constructed from SCUs and their corresponding popularity scores X  X he number of reference summaries they appeared in individually. SCUs carrying the same information do not n ecessarily have the same sur-face-level words. Annotators need to make the de-cisions based on semantic equivalence among various SCUs. To evaluate a peer summary from a particular docset, annotators highlight portions of text in the peer summary that convey the same in-formation as those SCUs in previously constructed pyramids. 3.2 Automatically Created Nuggets We envisage the nuggetization process being automated and nugget comparison and aggregation being performed by humans. It is crucial to involve humans in the evaluation process because recog-nizing semantically eq uivalent units is not a trivial task computationally. In addition, since nuggets are system-produced and can be imperfect, annotators are allowed to reject and re-create them. We per-form record-keeping in the background on which nugget or nugget groups are edited so that further improvements can be made for nuggetization. 3.3 Consistency in Human Involvement The process of creating nuggets has been auto-mated and we can assume a certain level of consis-tency based on the usage of the syntactic parser. However, a more important issue emerges. When given the same set of nuggets, would human anno-tators agree on nugget group selections and their corresponding contributing nuggets? What levels of agreement and disagreement should be ex-pected? Two annotators, one familiar with the no-tion of nuggetization (C1) and one not (C2), participated in the following experiments. reference summaries. After two rounds of individ-ual annotations and consolidations and one final round of conflict resolution, a set of gold-standard nugget groups is created for each docset and will be subsequently used in peer su mmary an notations. The first round of annotation is needed since one of the annotators, C2, is not familiar with nuggeti-zation. After the initial introduction of the task, concerns and questions arisen can be addressed. Then the annotators proceed to the second round of annotation. Naturally, some differences and con-flicts remain. A nnotators must resolve these prob-lems during the final round of conflict resolution and create the agreed-upon gold-standard data. Previous manual nugget annotation has used one annotator as the primary nugget creator and an-other annotator as an inspector (Nenkova and Pas-sonneau, 2004). In our annotation experiment, we encourage both annotators to play equally active roles. Conflicts between annotators resulting from ideology, comprehension, and interpretation differ-ences helped us to understand that complete agreement between annotators is not realistic and not achievable, unless one annotator is dominant over the other. We should expect a 5-10% annota-tion variation. from first to second round. The x -axis shows the nugget groups that C1 and C2 have agreed on. The y -axis shows the popularity score a particular nug-get group received. Select ing from three reference summaries, a score of three for a nugget group in-dicates it was created from nuggets in all three 
Figure 2. Reference annotation and gold-standard data creation. summaries. The first round initially appears suc-cessful because the two an notators had 100% agreement on nugget groups and their correspond-ing scores. However, C2, the novice nuggetizer, was much more conservative than C1, because only 10 nugget groups were created. The geometric mean of agreement on all nugget group assignment is merely 0.4786. During the second round, differ-ences in group-score allocations emerge, 0.9192, because C2 is creat ing more nugget groups. The geometric mean of agreement on all nugget group assignment has been improved to 0.7465. gold-standard data was created. Since all conflicts must be resolved, annotators have to either con-vince or be convinced by the other. How much change is there between an annotator X  X  second-round annotation and the gold-standard? Geomet-ric mean of agreement on all nugget group assign-ment for C1 is 0.7543 and for C2 is 0.8099. Agreement on nugget group score allocation for C1 is 0.9681 and for C2 is 0.9333. From these fig-ures, we see that while C2 contributed more to the gold-standard X  X  nugget group creations, C1 had more accuracy in finding the correct number of nugget occurrences in reference summaries. This confirms that both annotators played an active role. Using the gold-standard nugget groups, the annota-tors performed 4 peer summary annotations. The agreement among peer summary annotations is quite high, at approximately 0.95. Among the four, annotations on one peer su mmary from the two annotators are completely identical. In this paper we have given a concrete definition for information nuggets and provided a syst ematic implementat ion of them. Our main goal is to use these machine-generated nuggets in a semi-automatic evaluation environment for various NLP applications. We took a close look at how this can be accomplished for summary evaluat ion, using nuggets created from reference summaries to grade peer summaries. Inter-annotator agreements are measured to insure the qual ity of the gold-standard data created. And the agreements are very high by following a meticulous procedure. We are cur-rently preparing to deploy our design into full-scale evaluation exercises. 
