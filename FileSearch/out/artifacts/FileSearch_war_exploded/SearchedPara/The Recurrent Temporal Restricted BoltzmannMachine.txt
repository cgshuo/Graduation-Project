 [13], as well as complete and denoise such sequences. principle.
 convey more information through its hidden-to-hidden conn ections. shorthand for visible and hidden) by the equation where b and W is the matrix of connection weights. The quantity Z = Z ( b values; one way of achieving this is by the definition to a multivariate Gaussian with parameters N ( b generalizations.
 The gradient of the average log probability given a dataset S , L = 1 / | S | P following simple form: where  X  P ( V ) = 1 / | S | P concentrated at x ), and h f ( X ) i the exact values of the expectations hi the RBM).
 Divergence [3] learning procedure, CD Algorithm 1 (CD Models learned by CD continued with CD X tion P ( V T is that of an RBM, whose biases for H where b to The RBM X  X  partition function depends on h at time t depend on the value of the random variable H by an equation very similar to Eq. 5, except that the (undefine d) term W  X  h term b for P Boltzmann Machines has been considered in [7]).
 respect to the TRBM X  X  parameters is P T of the log likelihood of an RBM, can be approximated using CD Inference in a TRBM model, because even computing the probability P ( H ( j ) distribution P ( H by sampling from the distribution Q T Q P Figure 2: The graphical structure of the RTRBM, Q . The variables H variables H  X  and will work directly with the distribution Q ( V affect H h an RTRBM. Note that the outcome of the operation  X  P ( H An RTRBM, Q ( V T The terms appearing in this equation will be defined shortly. described by the following algorithm.
 Algorithm 2 (for sampling from the TRBM) : for 1  X  t  X  T : which involves running a Markov chain.
 assume that P and Q have identical parameters, which are W, W  X  , b algorithm samples from the RTRBM Q under this assumption. Algorithm 3 (for sampling from the RTRBM) for 1  X  t  X  T : We can infer that Q ( V sistent with the equation given in figure 2 where H  X  h seen that H 4.1 Inference in RTRBMs Inference in RTRBMs given v T probability. Suppose, for example, that the value of V the end of step 1 in Algorithm 3. Since step 2, the determinist ic operation h been executed, the only value h other value for h probability 0. In addition, by executing this operation, we can recover h  X  Once h computed by h the same reasoning is repeated t times, then all of h t when V t strength that the knowledge of V T The resulting inference algorithm is simple: Algorithm 4 (inference in RTRBMs) for 1  X  t  X  T : Let h ( v ) T described by 4.2 Learning in RTRBMs the equation  X  log Q ( v T the gradient  X  log Q ( v T continuous function of W . Thus, the gradient has to be computed differently. Notice that the RTRBM X  X  log probability satisfies log Q ( v T try computing the sum  X  P T feasible is the equation where h ( v ) tion holds because Q ( v posterior distribution Q ( H from Eq. 7.
 The equality Q ( V of variables at each timestep, { ( v hidden variables (all of which are deterministic). The hidd ens r T where W  X  r 1 = h ( v ) T 1 history using the marginal distribution of the RBM Q ( V defined to be log Q ( v of an RNN whose cumulative objective for the sequence v T where Q ( v the equality log Q ( v which is the log probability of the corresponding RTRBM. This means that  X  O =  X  log Q ( v T Divergence. 4.3 Details of the backpropagation through time algorithm a term  X  X / X  X  by the equation where a.b denotes component-wise multiplication, the term r the logistic function s  X  ( x ) = s ( x ) . (1  X  s ( x )) , and  X  log Q ( v equations Each term of the form  X  log Q ( v Q gradient of the RBM X  X  log probability. comparison between the RTRBM and the TRBM uninteresting. the TRBM and the RTRBM. We provide the code for our experiment s in [URL]. 5.1 Videos of bouncing balls (e.g., by convolutional weight matrices).
 do not have a strong self connection (i.e., W  X  5.2 Motion capture data 5.3 Details of the learning procedures first 1000 weight updates, which is then switched to CD initialized with static CD the motion capture data it was 0.005. short term memory RNN [6].
 Acknowledgments from http://people.csail.mit.edu/ehsu/work/sig05stf/ . For Matlab playback toolbox ( http://www.dcs.shef.ac.uk/  X  neil/mocap/ ).
