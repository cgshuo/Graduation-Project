 Word sense distributions are typically skewed and WSD systems do best when they exploit this ten-dency. This is usually done by estimating the most frequent sense ( MFS ) for each word from a training corpus and using that sense as a back-off strategy for a word when there is no convincing evidence from the context. This is known as the MFS heuristic 1 and is very powerful since sense distributions are usually skewed. The heuristic becomes particularly hard to beat for words with highly skewed sense dis-tributions (Yarowsky and Florian, 2002). Although the MFS can be estimated from tagged corpora, there are always cases where there is insufficient data, or where the data is inappropriate, for example because it comes from a very different domain. This has mo-tivated some recent work attempting to estimate the distributions automatically (McCarthy et al., 2004; Lapata and Keller, 2007). This paper examines the case for determining the skew of a word sense distri-bution by estimating entropy and then using this to increase the precision of an unsupervised first sense heuristic by restricting application to those words where the system can automatically detect that it has the most chance. We use a method based on that proposed by McCarthy et al. (2004) as this approach does not require hand-labelled corpora. The method could easily be adapted to other methods for predic-ing predominant sense. Given a listing of senses from an inventory, the method proposed by McCarthy et al. (2004) pro-vides a prevalence ranking score to produce a MFS heuristic. We make a slight modification to Mc-Carthy et al. X  X  prevalence score and use it to es-timate the probability distribution over the senses of a word. We use the same resources as Mc-Carthy et al. (2004): a distributional similarity the-saurus and a WordNet semantic similarity measure. The thesaurus was produced using the metric de-scribed by Lin (1998) with input from the gram-matical relation data extracted using the 90 mil-lion words of written English from the British Na-tional Corpus ( BNC ) (Leech, 1992) using the RASP parser (Briscoe and Carroll, 2002). The thesaurus consists of entries for each word ( w ) with the top 50  X  X earest neighbours X  to w , where the neighbours are words ranked by the distributional similarity that they share with w . The WordNet similarity score is obtained with the jcn measure (Jiang and Con-rath, 1997) using the WordNet Similarity Package 0.05 (Patwardhan and Pedersen, 2003) and WordNet version 1.6. The jcn measure needs word frequency information, which we obtained from the BNC . 2.1 Estimates of Predominance, Probability Following McCarthy et al. (2004), we calculate prevalence of each sense of the word ( w ) using a weighted sum of the distributional similarity scores of the top 50 neighbours of w . The sense of w that has the highest value is the automatically detected MFS (predominant sense). The weights are deter-mined by the WordNet similarity between the sense in question and the neighbour. We make a modi-fication to the original method by multiplying the weight by the inverse rank of the neighbour from the list of 50 neighbours. This modification magni-fies the contribution to each sense depending on the rank of the neighbour while still allowing a neigh-bour to contribute to all senses that it relates too. We verified the effect of this change compared to the original ranking score by measuring cross-entropy. 2
Let N w = n 1 , n 2 ... n k denote the ordered set of the top k = 50 neighbours of w according to the distri-butional similarity thesaurus, senses ( w ) is the set of senses of w and dss ( w , n j ) is the distributional sim-ilarity score of a word w and its j th neighbour. Let ws i be a sense of w then wnss ( ws i , n j ) is the maxi-mum WordNet similarity score between ws i and the WordNet sense of the neighbour ( n j ) that maximises this score. The prevalence score is calculated as fol-lows with 1 et al.
 Prevalence Score ( ws i ) =  X  n To turn this score into a probability estimate we sum the scores over all senses of a word and the proba-bility for a sense is the original score divided by this sum: To smooth the data, we evenly distribute 1/10 of the smallest prevalence score to all senses with a unde-fined prevalence score values. Entropy is measured as:
H ( senses ( w )) =  X   X  using our estimate (  X  p ) for the probability distribu-tion p over the senses of w . We conducted two experiments to evaluate the ben-efit of using our estimate of entropy to restrict appli-cation of the MFS heuristic. The two experiments are conducted on the polysemous nouns in SemCor and the nouns in the S ENSEVAL -2 English all words task (we will refer to this as SE 2-EAW ). 3.1 SemCor For this experiment we used all the polysemous nouns in Semcor 1.6 (excluding multiwords and proper nouns). We depart slightly from (McCarthy et al., 2004) in including all polysemous nouns whereas they limited the experiment to those with a frequency in SemCor of 3 or more and where there is one sense with a higher frequency than the others. Table 1 shows the precision of finding the predomi-nant sense using equation 1 with respect to different entropy thresholds. At each threshold, the MFS in Semcor provides the upper-bound ( UB ). The random baseline ( RBL ) is computed by selecting one of the senses of the target word randomly as the predomi-nant sense. As we hypothesized, precision is higher when the entropy of the sense distribution is lower, which is an encouraging result given that the entropy is automatically estimated. The performance of the random baseline is higher at lower entropy which shows that the task is easier and involves a lower de-gree of polysemy of the target words. However, the gains over the random baseline are greater at lower entropy levels indicating that the merits of detect-ing the skew of the distribution cannot all be due to lower polysemy levels.
We also conducted a frequency and polysemy analysis shown in Table 2 to demonstrate that the increase in precision is not all due to frequency or polysemy. This is important, since both frequency and polysemy level (assuming a predefined sense in-ventory) could be obtained without the need for au-tomatic estimation. As we can see, while precision is higher for lower polysemy, the automatic estimate of entropy can provide a greater increase in preci-sion than polysemy, and frequency does not seem to be strongly correlated with precision. 3.2 S ENSEVAL -2 English All Words Dataset The SE 2-EAW task provides a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II (Palmer et al., 2001). Again, we examine whether precision of the MFS heuristic can be increased by restricting application depending on entropy. We use the same resources as for the SemCor experiment. 3 Table 3 gives the re-sults. The most frequent sense ( MFS ) from SE 2-EAW itself provides the upper-bound ( UB ). We also com-pare performance with the Semcor MFS ( SC ). Per-formance is close to the Semcor MFS while not re-lying on any manual tagging. As before, precision increases significantly for words with low estimated entropy, and the gains over the random baseline are higher compared to the gains including all words. There is promising related work on determining the predominant sense for a MFS heuristic (Lapata and Keller, 2007; Mohammad and Hirst, 2006) but our work is the first to use the ranking score to estimate entropy and apply it to determine the confidence in the MFS heuristic. It is likely that these methods would also have increased precision if the ranking scores were used to estimate entropy. We leave such investigations for further work.

Chan and Ng (2005) estimate word sense distri-butions and demonstrate that sense distribution esti-mation improves a supervised WSD classifier. They use three sense distribution methods, including that of McCarthy et al. (2004). While the other two methods outperform the McCarthy et al. method, they rely on parallel training data and are not appli-cable on 9.6% of the test data for which there are no training examples. Our method does not require parallel training data.

Agirre and Mart  X   X nez (2004) show that sense dis-tribution estimation is very important for both super-vised and unsupervised WSD . They acquire tagged examples on a large scale by querying Google with monosemous synonyms of the word senses in ques-tion. They show that the method of McCarthy et al. (2004) can be used to produce a better sampling technique than relying on the bias from web data or randomly selecting the same number of exam-ples for each sense. Our work similarly shows that the automatic MFS is an unsupervised alternative to SemCor but our work does not focus on sampling but on an estimation of confidence in an automatic MFS heuristic. We demonstrate that our variation of the McCarthy et al. (2004) method for finding a MFS heuristic can be used for estimating the entropy of a sense dis-tribution which can be exploited to boost precision. Words which are estimated as having lower entropy in general get higher precision. This suggests that automatic estimation of entropy is a good criterion for getting higher precision. This is in agreement with Kilgarriff and Rosenzweig (2000) who demon-strate that entropy is a good measure of the difficulty of WSD tasks, though their measure of entropy was taken from the gold-standard distribution itself.
As future work, we want to compare this approach of estimating entropy with other methods for es-timating sense distributions which do not require hand-labelled data or parallel texts. Currently, we disregard local context. We wish to couple the con-fidence in the MFS with contextual evidence and in-vestigate application on coarse-grained datasets.
