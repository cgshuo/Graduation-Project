 Shapelets are discriminative sub-sequences of time series that best predict the target variable. For this reason, shapelet discovery has recently attracted considerable interest within the time-series re-search community. Currently shapelets are found by evaluating the prediction qualities of numerous candidates extracted from the se-ries segments. In contrast to the state-of-the-art, this paper pro-poses a novel perspective in terms of learning shapelets. A new mathematical formalization of the task via a classification objec-tive function is proposed and a tailored stochastic gradient learning algorithm is applied. The proposed method enables learning near-to-optimal shapelets directly without the need to try out lots of can-didates. Furthermore, our method can learn true top-K shapelets by capturing their interaction. Extensive experimentation demon-strates statistically significant improvement in terms of wins and ranks against 13 baselines over 28 time-series datasets.
Time-series research has attracted significant interest within the data mining community, due to the fact that series data are present in a wide range of real-life domains. Time-series data often exhibit inter-class differences in terms of small sub-sequencies rather than the full series structure [17]. A recently introduced concept, named shapelet , represents a maximally discriminative sub-sequence of time series data. Stated more directly, shapelets identify short dis-criminative series segments [17, 11]. Apart from their high predic-tion accuracy, shapelets also offer interpretable features to domain experts. Moreover, discovering shapelets has been a hot topic in the time-series domain during the last five years [17, 11, 10, 19, 8, 12, 9].

State-of-the-art methods discover shapelets by trying a pool of candidate sub-sequences from all possible series segments [17, 10] and then sorting the top performing segments according to their target prediction qualities. Distances between series and shapelets represent shapelet-transformed [10] classification features for a se-ries of segregation metrics, such as information gain [17, 11], F-Stat [8] or Kruskall-Wallis [9]. The brute-force candidates search approach, based on an exhaustive search of candidates, suffers from a high runtime complexity, therefore several speed-up techniques have aimed at reducing the discovery time of shapelets [11, 12, 1]. In terms of classification performance, the shapelet-transformation method constructs qualitative predictors for standard classifiers and has recently shown improvements with respect to prediction accu-racy [10, 8].

This paper proposes an entirely new perspective on time-series shapelets. For the first time, we propose a mathematical formu-lation of the shapelet learning task as an optimization of a classi-fication objective function. Furthermore, we propose a learning method that learns (not searches for) the shapelets which opti-mize the objective function. Concretely, we learn shapelets whose distances to series can linearly separate the time series instances by their targets, as shown in Figure 1. In comparison to existing approaches, our method can learn near-to-optimal shapelets and true top-K shapelet interactions. In a large pool of 28 datasets we demonstrate that the proposed method yields a large and statis-tically significant improvement over 13 baselines.
Shapelets were first proposed by [17] as time-series segments that maximally predict the target variable. All possible segments were considered as potential candidates, while the minimum dis-tances of a candidate to all training series were used as a predictor feature for ranking the information gain accuracy of that candidate on the target variable. Other quality metrics have been proposed for evaluating the prediction accuracy of a shapelet candidate such as F-Stats [10], Kruskall-Wallis or Mood X  X  median [8]. In addition, the minimum distance of a set of shapelets to time series can be perceived as a data transformation [10], while standard classifiers have achieved high accuracy over the shapelet-transformed repre-sentation [8].

Due to the high number of candidates, the runtime of brute-force shapelet discovery is not feasible. Therefore, a series of speed-up techniques such as early abandoning of distance computations and entropy pruning of the information gain metric have been proposed [17]. Other speed-ups rely on the reuse of computations and prun-ing of the search space [11], as well as exploiting projections on the SAX representation [12]. Alternatively, the training time has been reduced by elaborating the usage of infrequent shapelet candidates [7]. Moreover, hardware-based optimization have assisted the dis-covery of shapelets using GPUs [1]. Shapelets have been applied in a series of real-life applications. Unsupervised shapelets have also been utilized for clustering time series [19]. Shapelets have been found useful for identifying humans through their gait data [13]. Gesture recognition is another application domain that has benefited from the discovery of shapelets [5, 6]. In the domain of medical and health informatics, interpretable shapelets have been used to enable efficient early classification of time-series [16, 15].
In comparison to the state-or-the-art methods, we propose a novel method that learns near-to-optimal shapelets directly, without the need to search exhaustively among a pool of candidates extracted from time-series segments.
A time-series dataset is composed of I training instances and for notation ease we assume that each series contains Q -many or-dered values, even though our method can operate on variable se-ries lengths. The dataset is defined as T I  X  Q , while the series target is a nominal variable Y  X  X  1 ,...,C } I having C categories.
A sliding window segment of length L is an ordered sub-sequence of a series. Concretely, the segment starting at time j inside the i -th series is defined as ( T i,j ,...,T i,j + L  X  1 ) . There are totally J := Q  X  L + 1 segments in a time series provided the starting index of the sliding window is incremented by one.
A shapelet of length L is simply an ordered sequence of val-ues from a data structure perspective. Nevertheless, shapelets se-mantically represent intelligence on how to discriminate the target variable of a series dataset. The K -most informative shapelets are denoted as S  X  R K  X  L .
The distance between the i -th series T i and the k -th shapelet S is defined as the minimum distance M i,k (shown in Equation 1) among the distances between the shapelet S k and each segment j of T i [17, 18]. Informally speaking, it is the distance of a shapelet to the most similar series segment, as shown in Figure 1.
Minimum distances to shapelets can be characterized as a trans-formation of the time-series data T  X  R I  X  Q into a new represen-tation M  X  R I  X  K [10]. Such a transformation reduces the di-mensionality of the original time-series, because typically K &lt; Q . General purpose classifiers (e.g.: SVMs, Bayesian Network, . . . ) have been recently shown to achieve high prediction accuracy over the new representation M [8].
The logistic sigmoid function is an S shaped instance of the lo-gistic function and is defined as  X  ( Y ) = 1 + e  X  Y  X  1 going to use the sigmoid function for the prediction of target vari-ables via a logistic regression loss.
In this paper we propose a novel principle in learning time-series shapelets. Instead of searching among possible shapelet candidates from the series segments [17, 10], we propose a formal method that can directly learn optimal shapelets without needing to explore all possible candidates. Our principle can be summarized in two steps: (i) Start with rough initial guesses for the shapelets, (ii) Iter-atively learn/optimize the shapelets by minimizing a classification loss function. In order to conduct the shapelet optimization, we de-fine a novel classification model that is differentiable with respect to shapelets. Therefore, shapelets can be updated in a stochastic gradient descent optimization fashion, by taking steps towards the minimum of the classification loss function (i.e. towards maximal prediction accuracy).
For the sake of simplicity, the model introduced in this Section will be focused only on binary targets Y  X  { 0 , 1 } I and a fixed shapelet length L . A general version of the model, with extended properties, is described Section 5.
Since the minimum distances M are the new predictors in the transformed shapelets space, a linear learning model can predict approximate target values  X  Y  X  R I  X  K via the predictors M and linear weights W  X  R K (plus bias W 0  X  R ), as shown in Equa-tion 2.
In this paper we are going to exploit the logistic regression classi-fication model, because it provides an option to interpret predicted binary targets as probabilistic confidences. Such a probabilistic in-terpretation will ensure extending our approach to the multi-class case in Section 5. The logistic regression operates by minimizing the logistic loss, defined in Equation 3, between true targets Y and estimated ones  X  Y .

The logistic loss function together with regularization terms rep-resent the regularized objective function, denoted as F in Equa-tion 4. The idea of this paper is to jointly learn the optimal shapelets S and the optimal linear hyper-plane W that minimize the clas-sification objective F . argmin
In order to compute the derivative of the objective function, all the involved functions of the model need to be differentiable. Un-fortunately, the minimum function of Equation 1 is not differen-tiable and the partial derivative  X  X   X  X  is not defined. A differentiable approximation to the minimum function is introduced in this sec-tion.

For the sake of organizational clarity we will introduce the dis-tance between the j -th segment of series i and the k -th shapelet as D i,k,j and define it in Equation 5.

A differentiable approximation of the minimum function is the popular Soft Minimum function that is depicted in Equation 6. A parameter  X  controls the precision of the function and the soft min-imum approaches the true minimum for  X   X  X  X  X  .

Please note that the soft minimum has the shapelets as the only varying input, which appear embedded inside the distance defini-tion D . We would like to explain the operating principle of the soft minimum with the aid of Figure 2.

A series from the FaceFour dataset and a shapelet are depicted in the upper plot of Figure 2. The shapelet is a slightly distorted variant of the series segment starting at time index 51. If we slide the shapelet over all the series segments and record the distance of shapelets to segments (i.e. Equation 5), then the Euclidean dis-tances X  plot in blue is achieved. Two plots in red (bottommost) il-lustrate the operation of the soft minimum function. Each point j of area under the soft-minimum plots sums up to the true minimum distance between the shapelet and the series (i.e. Equation 1). It is important to realize in the third plot (  X  =  X  20 ) that the amount by which a segment distance impacts the overall minimum is directly related to how small is that segment X  X  distance compared to other segment distances. As can be seen in the bottom plot, if  X  =  X  100 , then only the true minimum segment distance is allowed to con-tribute to the grand total minimum. We found out that  X  =  X  100 is small enough to make the soft minumum yield exactly the same Figure 2: Illustration of the soft minimum between a shapelet (green) and all the segments of a series (black) from the FaceFour dataset results as the true minimum. Therefore, we kept this value fixed throughout all our experiments.
The optimization we will adopt in this paper is a stochastic gra-dient descent approach that remedies the classification error caused by one instance at a time. The Equation 7 demonstrates the de-composed objective function F i , which corresponds to a division of the objective of Equation 4 into per-instance losses for each time series.

The learning algorithm requires the definition of the gradients of the objective function with respect to the shapelets. The gradient of point l in shapelet k with respect to the objective of the i -th time series is defined in Equation 8 and is derived through the chain rule of derivation.
Furthermore, the gradient of the loss with respect to the predicted target is defined in Equation 9, while the gradient of the minimum distances with respect to the estimated target is shown in Equa-tion 10.
In addition, the gradient of the overall minimum distance with respect to a segment distance is presented in Equation 11 and the gradient of a segment distance with respect to a shapelet point is derived in Equation 12. Algorithm 1 Learning Time-Series Shapelets Require: T  X  R I  X  Q , Number of Shapelets K , Length of a Ensure: Shapelets S  X  R K  X  L , Classification weights W  X  R 1: for iteration= N maxIter 1 do 2: for i = 1 ,...,I do 3: for k = 1 ,...,K do 5: f or L = 1 ,...,L do 7: end for 8: end for 10: end for 11: end for 12: return S,W,W 0
The hyper-plane weights W can also be learned to minimize the classification objective via stochastic gradient descent. Equation 13 shows the partial gradient of updating each weight W k and Equa-tion 14 presents the bias term W 0 .
After having derived the gradients of the shapelets and the weights, we can introduce the overall learning algorithm. Our approach it-erates in a series of epochs and updates the values of the shapelets and weights in the negative direction of the derivative with respect to the classification objective of each training instance.
The steps of the learning process are shown in Algorithm 1. The pseudo-code iterates over all training instances I and updates all K shapelets S and the weights W,W 0 by a learning rate  X  .
The convergence of Algorithm 1 depends on two parameters, the learning rate  X  and the maximum number of iterations. High values for the learning rate can minimize the objective in less iterations, but pose the risk of divergence, while small learning rates require more iterations. Subsequently, the learning rate and the number of iterations should be learned via cross-validation from the training data.

For instance Figure 3 illustrates the convergence of the learning algorithm on the Coffee dataset for 57 shapelets. Both the training and the testing loss converge very smoothly for  X  = 0 . 01 . As a consequence of optimizing the training loss, the train and test errors also decrease simultaneously. In addition, the regularization Figure 3: Convergence of the Coffee dataset, Parameters: K = 57 , L = 143 ,  X  = 0 . 01 ,  X  W = 0 . 001 ,  X  =  X  100 weight  X  W = 0 . 001 ensures that the train and test loss have small differences, which can be interpreted as a generalization quality without any over-fitting effect.
Equation 4 is a non-convex function in terms of S and W be-cause both are variables that need to be learned. Gradient descent techniques do not theoretically guarantee the discovery of global optima in non-linear functions. Unfortunately, non-convex opti-mization techniques are very slow for data mining problems, there-fore gradient based approaches are often selected as a compromise between feasibility and optimality [14].

Gradient descent optimization requires a good initialization of the parameters when applied to non-convex functions. In other words, if the initialization starts the learning around a region where the global optimum is located, then the gradient can update the pa-rameters to the exact location of the optimum.

Initialization can influence a gradient based technique signifi-cantly. We are going to illustrate the sensitivity of shapelets ini-tialization through an experiment shown in Figure 4. For the sake of two-dimensional illustration, we initialized one shapelet ( S ) in the Gun-Point dataset using two values. The first 15 points of a 30 points long shapelet ( S 1:15 ) were given a fixed initial value, while the other half points of the shapelet S 16:30 were initialized with an-other fixed value. Figure 4 demonstrate that different initial values of the shapelet can result in different loss values and error rates over the training instances.
 Figure 4: Sensitivity of Shapelet Initialization, Gun-Point dataset, P arameters: L = 30 ,  X  = 0 . 01 ,  X  W = 0 . 01 , Iterations = 3000 ,  X  =  X  100 In order to robustify the initialization guesses we use the K-Means centroids of all segments as initial values for the shapelets. Since centroids represent typical patterns of the data, they offer a good variety of shapes for initializing shapelets and help our method achieve high prediction accuracy. The initialization is con-ducted before Algorithm 1 starts, while W is also initialized ran-domly around 0. An illustration of the learning algorithm is depicted in Figure 5. Two shapelets of length 40 are learned from the Gun-Point dataset. Sub-figure a ) demonstrates the initialization values of the shapelets and the arrangement of the minimum values of the time series to shapelets. As can be seen, a linear hyper-plane W cannot eas-ily separate the two classes. After 400 iterations of our method, the shapelets are updated as shown in sub-figure b ) . In addition, the shapelet transformed data representation M becomes almost linearly separable with few exceptions. Finally, the algorithm ap-proaches convergence in sub-figure c ) after 800 iterations. The lin-ear hyper-plane W separates the shapelet-transformed instances of the binary dataset with just a single error (in red).
The baseline method which exhaustively tries candidates from series segments [17, 10] requires O ( I 2 Q 3 ) running time for dis-covering the best shapelet of a particular length Q . On the other hand, our method requires O ( IQ 2  X  maxIter ) , therefore our al-gorithm finds the best shapelet in a faster time, given that usually maxIter &lt;&lt; IQ .
The optimal solution of Equation 4 gives the optimal shapelets, while a gradient descent approach can find a near-to-optimal mini-mum given an appropriate initialization.

The baseline approaches, on the other hand, provide no guaran-tee of optimal solutions for two primary reasons. First of all, the baselines are bound to shapelet candidates from the pool of series segments and cannot explore candidates which do not appear liter-ally as segments. Secondly, minimizing the classification objective through candidate guesses has no guarantee of optimality, while a gradient-based optimization guarantees at least near-to-optimal minima.
The baselines find the score of each shapelet independently and then sort the individual quality of each shapelet, in order to select the top performers. However, such an approach does not take into account interactions among patterns. In other words, two shapelets can be individually sub-optimal, but when combined together they can improve the results. In fact, this problem is well known in data mining and referred to as variable subset selection [2]. Figure 6: Interactions among Shapelets Enable Individually Unsuc-cessful Shapelets (left plots) to Excel in Cooperation (right plot)
For instance, Figure 6 demonstrates an example on how interac-tions among shapelets can become a game changing factor. On the left plots, we show the minimum distances of series to two shapelets. As can be observed, the individual discriminative quality of the shapelets is poor. On the other hand, a simple 2-dimensional interaction of exactly the same distances M 1 ,M 2 can yield drasti-cally improved results, as shown on the right plot. When combined together, the distances to those shapelets can create a linearly sep-arable discrimination, i.e. a perfect classification accuracy.
If the baseline X  X  exhaustive discovery approach would attempt to select the true top-K interaction of shapelets out of I ( Q  X  L + 1) candidates, then it will need to check the interaction of: I ( Q  X  L + 1) candidates. For instance finding the true top 100 shapelets of length 30 from the Adiac dataset with I = 390 and Q = 176 requires checking 3 . 42  X  10 317 combination trials using the baseline X  X  ap-proach. Clearly, the exhaustive search baseline cannot find true top-K shapelet interactions within a feasible time-frame. On the contrary, our method can find the interactions at a simple linear scale K , due to the property of jointly learning the shapelets and their interactions.
Our method relies on more hyper-parameters than the baselines, such as the learning rate  X  , the number of iterations, the regulariza-tion parameter  X  W and the soft-min precision  X  . However, given the high prediction accuracy that will be demonstrated in Section 6, we argue that the very high accuracy by far out-weights the model X  X  learning efforts.

The time needed for the baselines to compute the top-K shapelets is not significantly large with respect to the time needed to find a single shapelet. Such a behavior comes from the fact that the qual-ity of each candidate is known and ready in the end of the discov-ery. On the other hand, our method needs K -many time units for K shapelets, w.r.t. learning one shapelet. However, such a disadvan-tage in time for large K is well spent in terms of accuracy, because our method can learn true top-K shapelet interactions and signif-icantly improve the classification accuracy. Moreover, we believe that our method may yield to further improvements in efficiency by exploiting "early abandoning" and caching of partial results (as in [7]). For brevity we ignore such issues here and focus on forcefully demonstrating the improvements in accuracy.
The model presented in Section 3.3 can be generalized to multi-class labels and multi-size shapelets. Basically the model is ex-tended to classify multi-class targets and capture interactions among shapelets of various sizes. In order to learn from multi-class targets Y  X  X  1 ,...,C } C categories, we will convert the problem into C -many one-vs-all sub-problems. Each sub-problem will discriminate one class against all the others. The one-vs-all binary targets Y b are defined in Equation 15.
 Y
In fact, the conversion to one-vs-all sub-problems will be useful for the operation of the logistic regression classifier. The output of the logistic regression for a binary problem can be perceived as a confidence probability. Therefore, the index of the most confident among the C -many classifiers is selected as the predicted categori-cal value of a test instance.
Capturing interactions among shapelets having various lengths is another aspect of the extended method. Our generalized model learns R different scales of shapelet lengths starting at a minimum L min as { L min , 2 L min ,...,RL min } . The shapelets therefore will be sent K -many shapelets for each scale R , i.e. totally KR shapelets. The length of a shapelet at scale r  X  { 1 ,...,R } is r  X  L Consequently, the number of segments in a time series depends on the scale of the shapelet X  X  length to be matched against and is J ( r ) = Q  X  r  X  L min + 1 . The objective function of the generalized model is presented in Equation 16, which is a regularized logistic regression loss between the true targets and the predicted ones shown in Equation 17. The notation M r,i,k identifies the minimum distance of the i -th series to the k -th shapelet of scale r , i.e. to S r,k  X  R r  X  L the weight W c,r,k identifies the class c classifier and the weight of the k -th shapelet at scale r .
Once the model is learned, a test instance indexed t is classified as the one-vs-all classifier which yields maximum confidence, as presented in Equation 18. The algorithmic complexity of classify-ing a test instance is O ( CRKJ (  X  )) , but since C,R,K are asymp-totically smaller values than J (  X  ) , the big-O notation complexity is O ( J (  X  )) .
The soft minimum function can be trivially generalized to in-clude the notation for the scales r as shown in Equation 19. The distance between the k -th shapelet at scale r and the j -th segment of time series i is denoted as D r,i,k,j in Equation 20.
The objective function can be split per each instance i and the loss of each one-vs-all classifier c and denoted in Equation 21 as F
The derivative of the per-cell objective F i,c with respect to each shapelet S r,k,l is shown in Equation 22.
Moreover, the derivative of the minimum distances with respect to the generalized shapelets is defined in Equations 23-24.
The gradients of the per-cell objective with respect to the gener-alized weights and the bias terms are presented in Equations 25-26. Algorithm 2 summarizes all the steps of the learning process. The first section of the procedure pre-computes terms which are used frequently in the gradients of the shapelets, such as  X ,D, X , X  . The pre-computations boost the learning time and avoid computing the same terms repeatedly. The second part of the algorithm up-dates the weights and the shapelets using the defined gradients and the precomputed terms.
For the sake of equivalent comparison, we selected exactly the same set of datasets as the closest baselines [10, 8]. A large pool of 28 datasets consisting of time-series datasets having various num-bers of instances, lengths and number of classes is selected and details are shown in Table 1. In order to ensure a fair comparison with the baselines, we used the default train and test data splits, same as the baselines [10, 8]. The datasets are available through the UCR 1 and UEA 2 websites. Our method (hereafter denoted as LTS, for L earning T ime-Series S hapelets) requires the tuning of a series of hyper-parameters, which were found through a grid search approach using cross-validation over the training data. The number of shapelets was searched in a range of K  X  { 0 . 05 , 0 . 15 , 0 . 3 } , which is a fraction of the series length, e.g. K = 0 . 3 means 30% of Q . Similarly, L min  X  { 0 . 025 , 0 . 075 , 0 . 125 , 0 . 175 , 0 . 2 }  X  100% of Q , while three scales of shapelet lengths were searched from R  X  { 1 , 2 , 3 } . The regular-ization parameter was one of  X  W  X  { 0 . 01 , 0 . 1 , 1 } . The learning rate was kept fixed at a small value of  X  = 0 . 01 , while the number of iterations is selected from maxIter  X  { 2000 , 5000 , 10000 } . All our method X  parameters for all datasets are shown in Table 1. The authors are devoted to promote reproducibility, therefore the source code, datasets and instructions are made publicly avail-able 3 . http://www.cs.ucr.edu/~eamonn/time_series_ data/ http://www.uea.ac.uk/computing/ machine-learning/shapelets/shapelet-data http://fs.ismll.de/publicspace/ LearningShapelets/ Algorithm 2 Generalized Shapelets Learning Require: T ime series T  X  R I  X  Q , Binary labels Y b  X  R Ensure: Shapelets S  X  R R  X  K  X  X  , Classification weights W  X  1: Initialize S,W,W 0 2: for iteration= { 1 ,..., maxIter } do 3: for i = { 1 ,...,I } do 4: {Pre-compute Terms} 5: for r = { 1 ,...,R } , k = { 1 ,...,K } do 6: for j = { 1 ,...,J ( r ) } do 9: end for 12: end for 13: for c = { 1 ,...,C } do 16: end for 17: {Learn Shapelets and Classification Weights} 18: for c = { 1 ,...,C } do 19: for r = { 1 ,...,R } , k = { 1 ,...,K } do 21: for j = { 1 ,...,J ( r ) } do 23: for l = { 1 ,...,r  X  L min } do 25: end for 26: end for 27: end for 29: end for 30: end for 31: end for 32: return S,W,W 0
Thirteen different baselines were compared against, which are grouped into the following four clusters. min R  X  W maxIter F-Stat (Sec) LTS (Sec)
We compared our method of learning shapelets (denoted as LTS) against the selected baselines in terms of classification accuracy ratio (fraction of correct classifications) as shown in Table 2. The best method per dataset is highlighted in bold .
 Our method LTS has a very large superiority in terms of Absolute Wins (17.28 absolute wins in 28 datasets against 13 baselines) and 1-to-1 wins, as indicated by the respected rows in the end of the table. Each dataset awards one point, which is split into fractions in case of draws. In addition, we compared the ranks of the classifiers and found out that LTS has a significantly better rank of 1 . 946  X  0 . 536 against the closest baseline X  X  (SVM) rank 4 . 554  X  1 . 180 . In order to bullet-proof our claim, we ran the well-known Wilcoxon signed ranks test [3] against all baselines and found out that all results are statistically significant at p &lt; 0 . 05 , as can be deduced by the p-values of the most bottom row.
Since the idea of this paper is entirely novel, our first priority is to evaluate its prediction accuracy rather than elaborating on speed-up techniques, as in the fast shapelets approach [12]. Neverthe-less, we would like to show that our method is indeed feasible and competitive in terms of running time and faster than the exhaustive candidate search approach [10, 8]. We compared the time needed to find the best shapelet of each dataset against the F-Stat metric, which is the fastest quality metric [10, 8]. The best shapelet run-time comparison is advocated by our baseline [8], in order to ensure that methods can process the same number of candidates. As can be seen from Table 1, our method can learn the shapelet within a faster time (57 times faster in average) compared to the baseline, which is an indication that our method is practically feasible in terms of running time. Each execution of our method searched over five dif-the other parameters were set to  X  = 0 . 01 , maxIter = 3000 and  X 
W = 0 . 001 .
In this study we introduced a novel perspective into learning time-series shapelets. In contrast to related work which searches for top shapelets from a pool of candidates, we propose a novel math-ematical formulation of the task via a classification objective func-tion. In addition, we introduced a learning algorithm which learns near-to-optimal shapelets by exploring shapelet interactions. An extensive experimentation on 28 time-series datasets and 13 base-Table 2: Accuracy Ratios involving 28 Time Series Datasets and 13 Baselines lines is conducted. Our method outperforms all the baselines with statistically significant margins in terms of both wins and ranks. This study was partially co-funded by the Seventh Framework Programme (FP7) of the European Commission, through project REDUCTION 4 (#288254). [1] K.-W. Chang, B. Deka, W. mei W. Hwu, and D. Roth.
 [2] A. Das and D. Kempe. Algorithms for subset selection in [3] J. Dem X ar. Statistical comparisons of classifiers over multiple [4] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, and E. J. [5] B. Hartmann and N. Link. Gesture recognition with inertial [6] B. Hartmann, I. Schwab, and N. Link. Prototype [7] Q. He, F. Zhuang, T. Shang, Z. Shi, et al. Fast time series [8] J. Hills, J. Lines, E. Baranauskas, J. Mapp, and A. Bagnall. www.reduction-project.eu [9] J. Lines and A. Bagnall. Alternative quality measures for [10] J. Lines, L. Davis, J. Hills, and A. Bagnall. A shapelet [11] A. Mueen, E. Keogh, and N. Young. Logical-shapelets: an [12] T. Rakthanmanon and E. Keogh. Fast shapelets: A scalable [13] P. Sivakumar and T. Shajina. Human gait recognition and [14] E. W. Wild. Optimization-based Machine Learning and Data [15] Z. Xing, J. Pei, and P. Yu. Early classification on time series. [16] Z. Xing, J. Pei, P. Yu, and K. Wang. Extracting interpretable [17] L. Ye and E. Keogh. Time series shapelets: a new primitive [18] L. Ye and E. Keogh. Time series shapelets: a novel technique [19] J. Zakaria, A. Mueen, and E. Keogh. Clustering time series
