 In this paper, we give out a two-stage approach for domain adaptation problem in sentiment classification. In the first stage, based on our observation that cust omers often use different words to comment on the similar topics in the different domains, we regard these common topics as the bridge to link the different domain-specific features. We propose a novel topic model named Transfer-PLSA to extract the to pic knowledge between different domains. Through these common topics , the features in the source domain are corresponded to the targ et features, so that those domain-specific knowledge can be transferred across different labeled examples in the sour ce domain to pick up some informative examples in the target domain. Then we retrain the classifier on these selected examples, so that the classifier is adapted for the target domain. E xperimental results on sentiment classification in four different domains indicate that our method outperforms other traditional methods. I.2.7 [ Artificial Intelligence ]: Natural language processing algorithms, experimentation Domain adaptation, Sentiment Classification, Feature translation. In this paper, we investigate the domain adaptation problem in sentiment classification, where only the instances in source domain are labeled and no labele d data in target domain are available. We expect the sentiment classifier trained only by labeled data in source domain has good performance on the data in the target domain. Thus, for sentiment classification, we mainly consider two characteristics of domain adaptation as follows. 1) The instances in the source and the target domain are represented using different features. In different domains, people use different words to express th eir opinions, which makes their feature space to be not same. For example,. when expressing positive sentiment in car domain, people often use  X  X aster, powerful, safe, ... X  frequently, wh ile in the movie domain, they often use  X  X mpressive, fun, pr edictable, ... X . Those domain-specific features, occurring only in the source domain, are not useful for the testing data in the target domain because they aren X  X  observed in the target domain. 2) The distribution of data from th e different domain is different. Even though two datasets from di fferent domains share the same feature space, the frequences of the same feature in different domains are often different. Th at causes the distribution difference between different domains. Therefore, that will make the decision hyper-plane traine d on the source domain is not correct for classify the data in the target domain. the dark stars denote the positive and negative instances in the source domain, respectively. A nd the blank triangles and the blank stars denote the positive and negative instances in the target domain. As our observation, the instances from different domains share the same feature space but the different distribution. If we use the source dataset to train a sentiment classifier, we can obtain a decision hyper-plane denoted as a line, which cannot correctly separate the negative instances and the positive instances in the target domain. In this paper, when performing cross-domain sentiment classification, we propose a two-stage method, where each stage gives a solution for each characteristic, respectively. feature space that the different domains can share. We construct a w in the source domain to a t w in the target domain. In this way, the learned classifier treat those domain-specific features similarly, so that those domain-specific knowledge can be transferred across different domains. To construct this feature translator, Our intuitive idea is to regard the common topic knowledge as the bridge between the features fro m different domains. Although the topics that customers comment on may be different, there are some similar topics that are shared between different domains. If two different features describe the similar topics, they can be linked together through these topics. For example: These two sentences come from different domains. They both express positive emotions on the product X  X  appearance. We can customers use  X  noble  X  rather than  X  exquisite  X  to appraise the product X  X  appearance. Therefore, these domain-specific features make the feature spaces of each domain to be different. We use the common topic (the product X  X  appearance) as a bridge to correspond these two domain-specific features, so the classifier will treat them similarly. In othe r words, knowledge is transferred across domains. Figure 2 shows our idea for identifying the correspondence between different feat ures. In this figure, triangles denote the common topics betw een two domains, and circles denote the features. We can see that the domain-specific features ( X  X xquisite X  in the cell-phone doma in,  X  X oble X  in the car domain) are connected by a comm on topic ( X  X ppearance X ). Figure 2. Connecting Different Features through Common In the second stage, the aim of our method is to resolve the second characteristic. We use the source instances to train a classifier and classify the unlabeled instances in the target domain. Then we select some informative target instances,. and use them to retrain a new classifier, which can revise the original decision hyper-plain. As shown in the right graph in figur e 1, our classifier retrained by the informative instances in the target domain can more accurately classify the testing data than the classifier trained only in the source domain. In our problem, each instance s x in the source domain space. Each instance t x in the target domain can be represented by () p  X  , where i N tt wW  X  and t W is the target feature space. Both datasets have the same label space Y . Only the instances in are labeled, which have different feature representation with the testing instances in t D . As mentioned in the first section, to identify the correspondence between the features of two different domain, a feature translator ww  X  is constructed. We believe that (,) ( | ) w denotes the features in the target domain. To compute (|) ts p ww , we regard the topics as the bridge to connect different features from different domains. So that pww pwzpzw pwzpwzpz = X   X   X  (1) where z is the topic of the document. This process is similar to the process of machine translation. The features in the source domain are translated into the features in the target domain through the topic knowledge. In our algorithm, we use the topic model PLSA [5] to compute () pz and (|) pw z in formula (1). Since the aim of us is to find the common topics between the doc uments from the different domains, we must use a joint proba bilistic model rather than two separated PLSA models. Furthermor e, since the distribution of the datasets from the different domains are different, if we perform standard PLSA on it, the perfo rmance will drop. Thus, we proposed a new topic model named Transfer-PLSA to find the topic distribution between two different datasets. Since the sentiment classification is performed in the target domain, so the likelihood must be estimated u nder the target distribution () The likelihood L of the standard PLSA can be modified as the following formula: 
L nd w p d w nd w p d w where (, ) nd w denotes the term frequency in document d . And (, ) t Pdw can be obtained using th e following formula: To find the optimal solution of the object L , EM algorithm is used steps as follows E-Step : M-Step : In formula (5), (| ,) ti p wDz denotes the probability of generating a word w by a topic z in domain i D , and (| ) probability distribution of the topic z in domain i D . We simply use (| ,) i pw D z and (| ) i pz D to approximate (| ,) (| ) p zD separately. They can be estimated as follows: It X  X  worth noting that () understood as a kind of releva nce metric between dataset the target domain. We set () ()1 estimate () pD  X  = X  in our method. When 00.5  X  &lt;&lt; , it means that our algorithm will put more weight on the topic distribution in the weight on the source domain. Our intuitive idea is that the common topics are the bridge between two different domains Therefore, the common topics must be extracted first after () | For each topic z , the KL distance between different domains is used as the criterion for extracting common topics as follows. smaller () Score z as the common topics. Although these common topics are re garded as the bridge between the different feature spaces, we could not ignore other uncommon topics completely. We set different weights on the common topics and the uncommon topics to compute () | where [0,1]  X   X  . In this way, different features are linked through the common topics and we obtain the transl ating probability from a source feature correlated with the same common topics in a similar way, they will have higher degree of co rrespondence. Based on the matrix of () | p ww , the feature translator (,) s t ww  X  can be constructed. Then we can construct a linear projection  X  from the source ww  X  . In the source feature space, for the common feature s st wW W  X  X  X  , feature mapping will not be performed. For the domain-specific features s st wW W  X  X  X  , they will be translated to represented as an augmented featur e vector which contains all the common features and the mapped domain-specific features so that the domain-specific features, whatever in source domain or documents in the target domain. After the first stage, all data can be represented in a unique feature space through feature translation. In the second stage of our method, we add the unlabeled instance into the training process to make our classifier to be cl oser to the target domain. Our method is similar to [6]. At first we use the classifier trained on the transformed source labeled data to select l informative instances in the target domai n. Then, we will use these informative instances to retrain a new classifier. We believe this classifier will be more closer to the target domain than the original classifier. We use SVM as our classifier and select the top l instances which is the farthest to the decision hyper-plane as the informative examples, because the example is more far to the hyper-plain, it has more confid ence to belong to some class. The dataset which we used is as same as [1], which contains four domains. For the evaluation of our method, we select na X ve Bayes (NB) and Support Vector Machin e (SVM) as the baselines, we also select some semi-supervised approaches as the baselines, such as EM based na X ve Bayes (EMNB), Transductive Support Vector Machine (TSVM) [4]. Furthermore, we design some baselines for comparison. We list them as follows: ONLY-FIRST : We only perform the first stage for cross-domain sentiment classification. And the second stage is not performed. In ONLY-FIRST we set 4 k = , 0.3  X  = ,0.2  X  = and the number of the topics be 20. ONLY-SECOND : In this method, we refer to the method [6], which is also a transfer l earning method. ONLY-SECOND only performs the second stage of our method when training the classifier, but doesn X  X  perform the first stage. TWO-STAGE : In this method, we use the proposed two-stage method to train our classifier. Both stages are used to transfer knowledge across different domains. The setting of each stage is the same as ONLY-FIRST and ONLY-SECOND, respectively. SPLSA : SPLSA is similar to the first stage of our algorithm. The difference between them is that SPLSA use standard PLSA to associate words with topics but not transfer PLSA. Other setting is as same as ONLY-FIRST. All the methods mentioned above use unigrams as features. And we use 2,000 instances in one domain (source domain) as the training set and 2,000 instance s in the other domain (target domain) as the testing set. The sentiment classification results in four domains are presented in table 1. The first column in Table 1 shows that the dataset which we use, where the first letter indicates the source domain and the second letter indicates the target domain. For example,  X  X  and test it on the  X  X ook X  domain. From the results, we can see that the ONLY-FIRST outperforms not only supervised method, but also semi-supervised method in all datasets. It proves the effectiveness of our first stage, which is more adaptive to the target domain than the traditional learning methods. We believe the reason is that our method can connect different domain-specific featur es between different domains according their semantic relationship, so that the domain-specific knowledge can be transferred across different domains. Comparing the results of ONLY-SECOND with that of other baselines, we can find the strategy in the second stage in our method can effectively improve the performance, which also prove the validity of our method. We can obtain the conclusion that the new classifier trained by the selected informative examples can be more adaptive to the target domain than the original classifier trained on the source domain. Furthermore, we can see TWO-STAGE further increase the classification performance of ONLY-FIRST and ONLY-SECOND. We believe the reason is that the domain adaptation in the first stage of our approach can obtain better performance on the target domain than tr aditional methods, so that it is helpful for selecting more informative examples to retrain a new classifier. At the same time, those selected informative examples in the target domain can effective revise the original classifier because their distribution is similar to the distribution of the target domain. In Table 1, we can see that the performance of ONLY-FIRST is better than that of SPLSA methods. That indicates that Transfer-PLSA proposed in this paper can mo re accurately extract the topic knowledge across different domain s. Through these topics, the semantic relationship between f eatures across different domains can be constructed more accurately. The standard PLSA doesn X  X  distinguish the distribution di fference between domains. In contrast, our algorithm can set diffe rent weights to the instances in the different domains through  X  . If we can find the appropriate  X  , the performance of the sentimental classifier for domain adaptation can be promoted. Figure 3. Performance for Different Number of the Selected In the second stage of our met hod, the number of the selected informative examples has great influence on the final performance. In this experiment, we conduct the experiment on  X  X itchen X  domain to empirically analyze how classification accuracy evolves when the number of the selected informative examples changes from 200 to 1800. The e xperimental results of TWO-STAGE are shown in figure 3. From results, we can see that the best performance is obtained when the number of the informative examples is set between 1200 to 1600 .When the size of selected examples is too small, we have no sufficient knowledge to train the classifier. So the performance decreases, even lower than the original classifier trained on the labeled data in the source domain. However, if we select appropriate the number of th e informative examples, the performance of TWO-STAGE w ill be improved too much. Therefore, the results prove that the second stage in our method is useful and effective for improving the performance. In this paper, we propose a novel method for domain adaptation in sentiment classification, which contains two stages. In the first stage, we proposed a new approach based on feature translation, which can find a unique feature representation between different domains. In the second stage of our method, we use transformed labeled instances in the source domain to select some informative examples in the target domain and use them to retrain a new classifier, so that the hyper-plane can be revised to be more closer to the distribution of the target domain. The work is supported by the National High Technology Development 863 Program of China under Grants no. 2006AA01Z144, and the National Natural Science Foundation of China under Grants no. 60673042 and 60875041. [1] John Blitzer, Mark Dredze and Fernando Pereira. 2007. [2] Hal Daum X  III and Daniel Marcu. 2006. Domain Adaptation [3] Wenyuan Dai, Yuqiang Chen , Gui-Rong Xue, Qiang Yang [4] T. Joachims, 1999, Making large-Scale SVM Learning [5] Thomas Hofmann. Probabi listic Latent Semantic [6] Songbo Tan, Gaowei Wu, Huife ng Tang and Zueqi Cheng. [7] Gui-Rong Xue, Wenyuan Dai Qiang Yang and Yong Yu. 
