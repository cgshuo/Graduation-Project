 With the wide applications of information technology in biomedical field, biomedical technology has developed very rapidly. This in turn produces a large amount of biomed-ical data such as human gene bank. Consequently, biomedical literature available from the Web has experienced unprecedented growth over the past few years. The amount of literature in MEDLINE grows by nearly 400,000 citations each year. To mine infor-mation from the biomedical databases, a helpful and useful pre-processing step is to extract the valuable biomedical named entity. In other words, this step needs to identify some names from scientific text that is not st ructured as traditional databases and clas-sify these different names. As a result, bi omedical named entity recognition (BioNER) becomes one of the most important issues in automatic text extraction system. Many popular classification algorithms have been applied to this bioNER problem. These algorithms include Support Vector Machin e (SVM) [1,18,19], Conditional Random Fields (CRFs) [3], the Hidden Markov Model (HMM) [5], the Maximum Entropy (ME) [15], decision tree [16], and so on. While successful, each classifier has its own short-comings and none of them could consistently perform well over all different datasets. To overcome the shortcomings of individual methods, ensemble method has been sug-gested as a promising alternative.

Ensemble method is more attractive than individual classification algorithm in that it is an effective approach for improving the pre diction accuracy of a single classification algorithm. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (typically by weighted or unweighted voting) to classify new examples [8,11]. One of the most active areas of research in supervised learning has been to study methods for constructing good ensembles of classifiers. The most impor-tant property of successful ensemble methods is if the individual classifiers have error rate below 0.5 when classifying sample data w hile these errors are uncorrelated at least in some extent. That is, a necessary and sufficient condition for an ensemble of classi-fiers over its individual members is that the c lassifiers are accurate and diverse. Several recent studies indicate that the ensemble l earning could improve the performance of a single classifier in many real world text classification [6,7,9,10,12,13,14,23,24].
In this paper, we propose a generic genetic classifier-ensemble approach, which em-ploys multi-objective genetic algorithm and SVM based classifiers to construct an en-semble classifier. Each SVM based classifier is trained on a different feature subset and used as the classification committee. Th e rest of the paper is organized as follows: Section 2 discusses the generic genetic classifier-ensemble approach in detail. Experi-mental results and analysis are provided in Section 3. Conclusions and future work are presented in Section 4. Classifier-ensemble is a popular technique in pattern recognition domain. It reflects the generalization accura cy if an ensemble depends not only on the performances of the individual classifier but also on the diversity among the classifiers [6,8,10,7,12,22]. Therefore, a classifier-ensemble system is usually made up of two major components: the classifiers forming the ensemble members and the combination scheme. In order to achieve this goal, we develop a generic genetic classifier-ensemble algorithm. In the proposed approach, SVM is used as the basic classifier and the genetic algorithm was used to search the optimal solution o f weighted classifier combination. 2.1 Feature Set and SVM Based Classifier Since the main issue using machine learning method for BioNER task is to design a proper feature set, choosing the suitable feature is very important for improving the performance of the system. Here various types of features have been considered for bioNER task in different combinations (see Table 1).
Next, due to the fact that support vector machines(SVMs) are powerful methods for learning a classifier and have been applied successfully to many NLP tasks, SVMs construct the base classifier in BioNER. The general-purpose text chunker named Yet Another Multipurpose Chunk Annotator-Yamcha 2 uses TinySVM 3 for learning the clas-sifiers. Yamcha is utilized to transform the input data into feature vectors usable by TinySVM [18,19]. Table 3 shows the Yamcha p arameters. Accordingly, each classifier is unique in at least one of the following properties: window size, degree of the poly-nomial kernel, parsing direction as well as feature set. Consequently, this constructs 46 individual SVM classifier committees [17,20,21]. 2.2 Generic Genetic Classifier-Ensemble Algorithm The genetic algorithm (GA) was developed in the 1970s by Holland as an effective evolutionary optimization method [25]. In GA the two core elements are chromosome and fitness. Chromosome is used to encode representation of the optimal solution to the classifier-ensemble problem. Fitness is des igned to measure the chromosome X  X  perfor-mance.
 Genetic Classifier-Ensemble-I. The basic idea behind the gen etic classifier-ensemble-I is that different classes in each classifier differ with contributing degrees of prediction classes. In other words, each class in each cl assifier has been assigned a weight which corresponds with the contributing degree of prediction class. To use genetic algorithm, we first need to represent the problem domai n as a chromosome. Here, we want to find an optimal set of weight for classifier ensemble scheme shown in Figure 1. Assume that there are totally N tags (classes) co rresponding to the named entities considered in the BioNER task. Set the total number of available classifiers denoted by M. The optimal weight solution of the classifier ensemble scheme is encoded in the form of a weight chromosome,which has N*M genes. First N genes belong to the first classifier and the next N genes the second classifier and so on. The encoding of a chromosome is illustrated in Figure 1. Each value of gene in the chromosome is initialized to a small random number, said within the range[0,1]. Thus, we obtain a chromosome.

The second step is to define a fitness function for evaluating the chromosome X  X  per-formance. This function must estimate the pe rformance of a given classifier-ensemble problem with weights. We define the fitness of a chromosome as the full object F-score provided by the weighted majority voting type decision combination rule [12,17,22]. In this rule, the class receiving the maxim um combined score is selected as the joint decision. By the definition of the combined score of a particular class, we obtain the fitness as follows: where M denotes the total number of classifiers and F m denotes the full object F-score of m th classifier. w(m,i) is assigned to a weight value in the gene of i th class of m th classifier in the chromosome.

The third step is to choose the genetic operators-crossover and mutation. A crossover operator takes two parent chromosomes and c reates two children with genetic material from both parents. In the proposed approach, either uniform or two point crossover method is randomly selected with equal pr obability. The selected operator is applied with a probability p cross to generate two offspring. A mutation operator randomly se-lects a gene in offspring chromosomes with a probability p mut and adds a small ran-dom number within the range[0,1] to each weight in the gene. In addition, we still need to specify the tournament size,elitism, popula tion size and the number of generations. Tournament size is used in tournament sel ection during the repr oduction. Elitism is ap-plied at the end of each iteration where the best elit size% of the original population are used to replace those in the offspring producing the lowest fitness.
 Genetic Classifier-Ensemble-II. The basic principle behind the genetic classifier-ensemble-II is that different classifiers have different contributing degrees of prediction of classes. In other words, each classifie r can be assigned a weight which corresponds with the contributing degree of predictio n of class. Suppose each chromosome is en-coded as a weight string havi ng M genes, one for each classifier(see Figure 2). If the value of a gene is w m , this means that the contributing degree of the m th classifier in this ensemble is w m . Accordingly, the combined score of a given class can be redefined as:
At the same time, all parameters of this algorithm described above including pop-ulation size, the number of generations, crossover and mutation rate etc. are kept the same.
 Genetic Classifier-Ensemble-III. Based on the above consideration in both subsec-tions 2.2.1 and 2.2.2, not only contributing degrees of prediction classes among different classes in the same classifier are different, but also contributing degrees of prediction classes among different classifiers differ. Thus, the chromosome is made up of the chromosome in genetic classifier-ensemble-I and the chromosome in genetic classifier-ensemble-II, and has (N+1)*M genes (see Figure 3). Therefore, the combined score of a given class is determined as: Similarly, all the other parameters are kept the same.

After given the definition of chromosome and fitness as well as all parameters, the complete genetic classifier-ensemble algorithm can be described in the following steps: Figure 4 presents the flow of the proposed generic genetic classifier-ensemble algorithm.

The overall system architecture is illustrated in Figure 5. The best-fitting solution of weighted classifier-ensemble is obtained by using the classifier outputs generated through three-fold cross-validation on the training data. In our proposed algorithm, the training data is initially partitioned into three parts. Each classifier is trained using two parts and then tested with the remaining part . This procedure is repeated three times and the whole set of training data is used for computing the best-fitting solution. Multi-class SVM is used for all individual classifier. The major differences among the individual classifiers are in their modeling parameter values and feature sets. Each classifier is different from the rest in at least one mode ling parameter or the feature set. During testing, the outputs of the individual classifiers are combined by using the computed best-fitting solution of weight classifier-ensemble.
 To conduct the experiment, we use the latest GENIA 4 version 3.02 corpus provided by the shared task in COLING 2004 JNLPBA. The corpus includes the training dataset and the testing dataset. The training dataset consists of 2000 MEDLINE abstracts of the GENIA corpus with named entities in IOB2 format. The testing dataset consists of 404 abstracts. There are 18546 sentences and 492551 words in the training dataset and 3856 sentences and 101039 words in the testing dataset. Each word is tagged with  X  X -X X ,  X  X -X X , or  X  X  X  to indicate that the word is at the  X  X eginning X (B) or  X  X nside X (I) of a named entity of type X, or  X  X utside X (O) of a named entity. For BioNER task, the named entity types are DNA, RNA, cell line, cell type, and protein. Table 4 shows the number of 5 different biomedical named entities in this corpus. For each entity, two different tags(classes) result in 10 tags for the named entities and one additional tag for all non-named entities called class. Acco rdingly, this translate to a total of N =11 classes. Besides, we present M =46 single SVM base classifier committees on the basis of different combination within feature set and Yamcha parameter. The experimental performance is evaluated by the standard measures, namely precision, recall and F-score which is the harmonic mean of precision and recall.

In the simulation experiments, The tournament size, crossover probability, mutation probability and elitism ratio are empirically computed as 40, 0.7, 0.02, and 20%, respec-tively. The population size of the generic genetic classifier-ensemble algorithm is set to 100. This means that one hundred different ensemble candidates evolve simultaneously. The algorithm is run for 10000 iterations. The weight classifier-ensemble correspond-ing to the chromosome with the highest fitness value in the last generation is selected as the optimal solution. We perform simulation experiments repeatedly by changing the weight values of these chromosomes and selected the weight genes of the chromosome providing the best performance of BioNER on the training data. In the testing, the test data is measured by using the optimal solution. This solution provides the best-fitting ensemble parameter with weights in the simulation experiments.

Table 5 shows the performance of the proposed three genetic classifier-ensemble scheme on precision, recall, and Fscore for BioNER. In this table, the genetic classifier-ensemble-III gets the better results compared with the genetic classifier-ensemble-I and genetic classifier-ensemble-II, where the performance of precision, recall and Fsore reach 75.65%, 78.52%, and 77.85% respectively.

It can be seen that in Table 6 we compare our best result with those of the recent work that employ support vector machines as classifier. The individual best SVM-classifier has the full feature set and optimal setting parameters[20,21]. Dimililer et al. used a vote-based classifier selection approach to construct a classifier ensemble and effec-tive post-processing techniques for biomedi cal named entity recognition task[17,20,21]. Compared with the individual best SVM-classifier and SVM-classifier ensemble, our method outperforms them. It means that our generic genetic classifier-ensemble ap-proach which searched the best-fitting ensemble parameter with weights can be power-ful and efficient to combine orderly individual SVM base classifier with their strengths through giving the corresponding weights and to avoid individual classifier X  X  weakness.
Table 7 shows that the best result of our e xperiment outperforms that of other indi-vidual classifier algorithms [26]. Their approaches include the Hidden Markov Model (HMM) [5], the Maximum Entropy Markov Model (MEMM) [4] and the Conditional Random Field (CRF) [3], which use deep knowledge resources with extra costs in pre-processing and post-processing. For instance, Zhou and Su [1] used name alias resolution, cascaded entity name resolution, abbreviation resolution and an open dic-tionary (around 700,000 entries). Finkel et al. used gazetteers and web-querying [2]. Settles used 17 lexicons that include Greek letters, amino acids, and so forth [3]. In contrast, our system did not include these similar processing. We proposed a generic genetic classifier-ensemble approach to recognizing the biomed-ical named entities. The contributions of this paper are that a novel genetic classifier-ensemble algorithm with weights is provided to deal with bioNER task and improve the BioNER performance compared with both of S VM-based classifiers as well as other individual machine learning algorithms. In the future, we will incorporate much more effective features and more classifiers usin g different machine learning algorithms in our ensemble approach, and include some post-processing techniques and comparison of computational cost.

