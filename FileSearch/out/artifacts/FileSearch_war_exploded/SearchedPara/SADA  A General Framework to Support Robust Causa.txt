 Ruichu Cai cairuichu@gmail.com Technology, Nanjing University, P.R. China Zhenjie Zhang zhenjie@adsc.com.sg Zhifeng Hao zfhao@gdut.edu.cn Causality discovery plays an important role on a va-riety of scientific domains. Different from the main-stream statistical learning approaches, causality learn-ing tries to understand the data generation procedure, rather than characterizing the joint distribution of the observed variables only. It turns out that understand-ing causality in such procedures is essential to predict the consequences of interventions, which is the key to a large number of applications, such as genetic therapy, advertising campaign design, etc.
 From computational perspective, causation discovery is usually formulated with a graphical probabilistic model on the variables ( Pearl , 2009 ), such that di-rected edges between variables indicate causation rela-tionships. When it is unlikely to manipulate the sam-ples in experiments, conditional independence test-ing is commonly employed to detect local causalities among the variables ( Pearl , 2009 ; Spirtes et al. , 2001 ). Despite of the successes of these approaches on small problem domains and large sample bases, they usu-ally fail to find true causalities, when huge equivalent classes over the graphical probabilistic models render exactly the same conditional independence.
 To tackle the difficulties in the problem of causal structure learning under non-experimental setting, re-searchers are recently resorting to asymmetrical rela-tionship between the cause and effect variables un-der assumptions on the generation process. The dis-covery ability is dramatically improved, by exploit-ing linear assumption ( Zscheischler et al. , 2012 ), linear non-Gaussian assumption ( Shimizu et al. , 2006 ; 2011 ), nonlinear non-Gaussian assumption ( Hoyer et al. , 2008 ), discrete property ( Peters et al. , 2010 ), and so on. When the variables are correlated under linear re-lationships and the noises follow non-Gaussian distri-butions, for example, LiNGAM ( Shimizu et al. , 2006 ) and its variants ( Shimizu et al. , 2011 ) are known as the best causality inference algorithms. However, the scalability of LiNGAM and its variants is still ques-tionable, since they heavily depend on the independent component analysis (ICA) during the computation. To return robust results from ICA, it is necessary to feed a large bulk of samples, which are expected to be no smaller than the number of variables.
 Motivated by the common observations on the spar-sity of causal structures, i.e. each variable usually only depends on a small number of parent variables, we de-rive a new general framework in this paper. The new framework helps the existing causation algorithms to get rid of difficulties on small sample cardinality in practice. Well designed conditional independence test-ings are conducted first, to partition the problem do-main into small subproblems. With the same number of samples, existing causation algorithms could gener-ate more robust and accurate results on these small subproblems. Partial results from all subproblems are finally merged together, to return a complete picture of causalities among all the variables. This framework is theoretically solid, as it always returns correct and complete result under the optimal setting. Our exper-iments on synthetic and real datasets verify the supe-rior scalability and effectiveness of our proposal, when applied together with two mainstream causation anal-ysis algorithms. Causality Bayesian network (CBN) is part of the theo-retical background of this work. CBN is a special case of Bayesian network, whose edge direction presents the causality relations among the nodes ( Pearl , 2009 ). CBN has been used to model the causal structure in many real-world applications, for example, the gene regulatory network ( Friedman et al. , 2000 ; Kim et al. , 2004 ), causal feature selection ( Cai et al. , 2011 ). Most of existing work try to explore the struc-ture learning approach to learning the CBN, e.g. the well known PC algorithm ( Spirtes et al. , 2001 ; Kalisch &amp; B  X uhlmann , 2007 ), Markov Blanket discov-ery methods ( Zhu et al. , 2007 ). These methods pro-vide the skeleton of causal structures, i.e. parent-child pairs and Markov Blanket. However, these methods usually cannot distinguish causes from consequences, thus unable to output exact causalities.
 Pearl is the pioneer of the causality analysis the-ory ( Pearl , 2009 ). Since Pearl X  X  Inductive Causality ( Pearl &amp; Verma , 1991 ), a large number of extensions are proposed. Most causality inference algorithms as-sume the acquisition of a sufficiently large sample base ( Aliferis et al. , 2010 ). Though there are studies aiming at the inference problem under small sample cardinal-ity ( Bromberg &amp; Margaritis , 2009 ), the actual number of the samples used in their empirical evaluations re-mains significantly larger than the number of variables. Cai X  X  study ( Cai et al. , 2013 ) is another attempt under this category to extend the method to the high dimen-sional gene expression data by exploring the local sub-structures. However, all these approaches, based on independence conditional testings, cannot distinguish two causality structures if they come from a so-called Markov equivalence class ( Pearl , 2009 ), in which ex-pensive intervention experiments were previously con-sidered essential ( He &amp; Geng , 2008 ).
 Recently, Additive Noise Model is proposed to break the limitation of the class of method purely under conditional independence testings, by exploiting the asymmetric property of the noises in the generative progress, which brings a gleam of dawn to resolve the causal equivalence problem. The Additive Noise Model highly depends on the type of noise and the form of causality. Existing studies on this line can be categorized based on the adopted assumption on the noise type and data generation mechanism. LiNGAM and its variants ( Shimizu et al. , 2006 ; 2011 ), for ex-ample, assume that the data generating process is lin-ear and the noise distributions are non-Gaussian. The nonlinear non-Gaussian method ( Hoyer et al. , 2008 ) works when the data generating process is nonlin-ear. And discrete model ( Peters et al. , 2010 ) is pro-posed for the causal inference on domains with only discrete variables. There are other research stud-ies on this topic, such as explaining the underly-ing theoretical foundation behind additive noise mode ( Janzing et al. , 2012 ), regression-based model infer-ence ( Mooij et al. , 2009 ) and kernel independence test ( Zhang et al. , 2012 ). To the best of our knowledge, there is no existing work to address the problem of sample cardinality under Additive Noise Model. 3.1. Preliminaries Assume that all samples from the problem domain contain information on m different variable, i.e. V = { v observation sample set. Each sample x i is denoted by a vector x i = ( x i 1 , x i 2 , . . . , x im , y i ), where x dicates the value of the sample x i on variable v j and y i is the target variable under investigation. If P is a distribution over the domain of variables in V , we assume that there exists a causal Bayesian network N faithful to the distribution P . The network N in-cludes a directed acyclic graph G , each edge in which indicates a conditional (in)dependent relationship be-tween two variable nodes. Each edge is also associated with a conditional probability function which simu-lates conditional probability distribution of each vari-able given the values of the parent variables. Following the common assumption of existing studies, we only consider problem domain with the Faithfulness Con-dition ( Koller &amp; Friedman , 2009 ). Specifically, P and N are faithful to each other, iff every conditional inde-pendence entailed by N corresponds to some Markov condition present in P .
 Due to the probabilistic nature, it is likely to find a huge number of equivalent Bayesian networks. Two different Bayesian Networks, N 1 and N 2 , are indepen-dence (or Markov) equivalent, if N 1 and N 2 entail exactly the same conditional independence relations among the variables. In all these Bayesian networks, Causal Bayesian network (CBN) is a special one in which each edge is interpreted as a direct causal re-lationship between a parent variable node and a child variable node.
 Generally speaking, it is difficult to distinguish CBN from independence equivalent Bayesian networks, un-less additional assumptions are made. When the vari-ables are correlated under linear relationships and the noises follow non-Gaussian distributions, LiNGAM and its variants ( Shimizu et al. , 2006 ; 2011 ) are known to return more accurate causations from the uncontrol-lable samples. In particular, such assumption can be formulated by an equation, such that every variable v = P v the parent variables of v i , A ij is the linear dependence weight w.r.t. v i and its parent v j , and e i is an non-Gaussian noise over v i . Assume that the variables in V are organized based on a topological order in the causal graph. The generation procedure of a sample could be written as V = A  X  V + E . LiNGAM aims to find such a topological order and reconstructs the ma-trix A by exploiting independence component analysis (ICA) over the sample.
 When assuming non-linear generation procedure ( Hoyer et al. , 2008 ) and discrete data domain ( Peters et al. , 2010 ), additive noise model provides an-other approach to utilize the asymmetric relation be-tween causal variables and consequence variables. A regression model v i = f ( v j )+ e i is trained for each pair of variables v i and v j . If the noise variable e i is inde-pendent of v j , variable v j is returned as the cause of variable v i . Note that algorithms under discrete addi-tive noise model are usually run over pairs of variables independently.
 A common observation on the CBNs in real-world domains is the sparsity on the causal relationships. Specifically, a variable usually only has a small num-ber of parental causal variables in the CBN, regard-less of the underlying true generative procedure. This property, however, is not fully exploited by the existing causation algorithms. 3.2. Framework Let G = ( V, E ) denote a directed graph on the variable set V . A variable set C  X  V forms a causal cut set over G , iff C deduces three non-overlapping variable subsets V 1 , V 2 and C of V such that (1) V 1  X  V 2  X  C = V ; (2) there is no edge between V 1 and V 2 in E . Intuitively, variables in C block all paths between the variables in V 1 and V 2 . For each directed edge u  X  v in E , one of the two following cases must hold: (1) intra-causality: u, v  X  V 1 , u, v  X  V 2 or u, v  X  C ; and (2) inter-causality: ( u  X  V 1  X  V 2 and v  X  C ), or ( u  X  C and v  X  V 1  X  V 2 ).
 In Figure 1 , for example, C = { v 4 } is a valid causal cut, which separates the variables into V 1 = { v graph G , there could be different valid cuts satis-fying the above conditions. In the example graph, C  X  = { v 3 , v 4 , v 5 } is another valid causal cut with V cut may not lead to d-separation, e.g. C  X  in the exam-Algorithm 1 SADA ple does not d-separate the other variables.
 Given a causal cut ( C, V 1 , V 2 ) on variable set V , we are able to transfer the causation inference problem on V into two smaller causation inference problems over variables V 1  X  C and V 2  X  C respectively. This partitioning operation could be recursively called, un-til the number of variables involved in the subproblem is below a specified threshold T . The complete pseu-docodes are available in Algorithm 1 . The inputs of SADA include the sample set D , the variables V , a threshold  X  and an underlying causation algorithm A . Here,  X  is used to terminate the recursive partitioning when the variable set is sufficiently small, and A is an arbitrary causation algorithm invoked to find the ac-tual causal graph on the subset of variables. In the rest of the section, we will discuss how to effectively and efficiently find causal cuts on a variable set V . We will also present the details of the merging operator, which tackles the problem of inconsistency and redundancy on the partial results from the subproblems. 3.3. Finding Causal Cuts The searching of the causal cuts is crucial to the par-titioning operation in SADA. To identify potential causal cuts, our algorithm resorts to conditional in-dependence relation between variables in the Bayesian network. The following lemma formalizes the connec-tion.
 Lemma 1 ( C, V 1 , V 2 ) is a valid causal cut over V , iff (1) V 1  X  V 2  X  C = V ; and (2)  X  u  X  V 1 and  X  v  X  V 2 , there exists a variable set C uv  X  C such that u  X  v | C uv Proof:  X   X   X  Based on the definition of causal cut, con-dition (1) always holds. Since there is no directed edge between V 1 and V 2 , for any u  X  V 1 and v  X  V 2 , u and v are d -separated by C , implying the validity of con-dition (2).  X   X   X  For all pairs of variables ( u, v ) that u  X  V 1 and v  X  V 2 are d -separated by C , there is no directed edge between V 1 and V 2 . Therefore, V 1 , V 2 and C must Algorithm 2 Finding Causal Cut satisfy the definition of causal cut.
 The details of the algorithms are listed in Algorithm 2 . The algorithm runs with k different initial variable pairs. For each pair of { u, v } conditional independent of each other in term of other variables, the algorithm greedily adds other variables into C , V 1 and V 2 . After completing all assignment, the algorithm also tries to move the variables from C to V 1 or V 2 to maximize the partitioning effect. Finally, the causal cut with largest min {| V 1 | , | V 2 |} are returned as final result. We leave the discussion on the parameters k and  X  to next section. Please note that the sample size needed in the cut algorithm highly depends on the local connec-tivity of the causal structure but not on the number of variables. This is an important advantage of the algorithm to applications in large scale sparse causal inference problems. 3.4. Merging Partial Results As is shown in Algorithm 1 , two partial results G 1 and G 2 are combined as a single casual graph as on variables in V . Since G 1 and G 2 are calculated inde-pendently, the merging operation is carefully designed to handle conflicts and redundancies.
 The general form of a conflict is a cycle of directed edges among a group of variables. Given two nodes v 1 and v 2 , there are two paths co-existing, such as v 1  X  X  X  X  X  X  v 2 and v 1  X  v 2 . To resolve such conflicts, we simply remove the least significant edge, whenever a cycle is found.
 Redundancy incurs under the following observation: given two variables v 1 and v 2 , if both v 1  X   X  X  X   X  v 2 and v 1  X  v 2 are discovered, v 1  X  v 2 may be redun-dant. Since the dependency relation v 1  X  v 2 could be blocked by certain variables in the variable set Path ( v 1  X  v 2 ), where Path ( v 1  X  v 2 ) includes all variables involved in v 1  X   X  X  X   X  v 2 . Such redun-dancy raises when the following two conditions are satisfied: 1) the source and destination variables are both in the causal cut, i.e. v 1 , v 2  X  C , 2) there is an-other variable v 3  X  V 1 , such that v 1  X  v 3  X  v 2 . If the above two conditions are met, one path v 1  X  v 2 will be returned from the subproblem with V 1  X  C , while another path v 1  X  v 2 turns up from the other subproblem with V 2  X  C . To tackle this problem, our merging algorithm runs the following conditional inde-pendence test to verify if  X  V  X   X  Path ( v 1  X  v 2 ) that v  X  v 2 | Path ( v 1  X  v 2 ).
 To summarize, the merging operation works as follows. Firstly, all directed edges from both solutions are sim-ply added into a single edge set. Secondly, Edges are ranked according to the associated significance mea-sure, calculated by the underlying causation algorithm A used by SADA. Thirdly, a sequential check on the edges are run based on the order of the significance. An edge is removed if it is conflicted with any of the previous edge. Finally, the redundancy edges are dis-covered and removed based on results of the condi-tional independence testings. A complete description is available in Algorithm 3 .
 Algorithm 3 Merge Results In this section, we study the theoretical properties of SADA, especially on the effectiveness on problem scale reduction, consistency on causal results and interpre-tation with independent component analysis. 4.1. Effectiveness In this part of the section, we aim to verify the effec-tiveness of the causal cut search algorithm. In partic-ular, we try to prove that the scale of the subproblem is significantly reduced when applying the randomized search algorithm.
 Theorem 1 If every variable has no more than c parental variables in CBN, by setting k = (2 c + 2) 2 , Algorithm 2 returns a causal cut ( C, V 1 , V 2 ) with prob-ability at least 0.5, such that Proof Sketch: Since the causal graph in CBN must be a DAG, there is at least one topological order on the variables, i.e. V = { v 1 , v 2 , . . . , v | V | } , such that v parental variables are ahead of v i in the order. When randomly picking up variable pairs in V , i.e. u and v from V , we will first show that u and v generate a at least 1 / (2 c + 2) 2 .
 With out loss of generality, we assume n = | V | and the variable u is behind v in the topological order over V . With probability  X  , u is one of the vari-ply put all these variables in V 1 , and put all parental variables of V 1 , denoted by P ( V 1 ). and all variables behind v (0 . 5+  X  ) n into C . The rest of the variables are inserted into V 2 . It is easy to prove that these con-figuration { C, V 1 , V 2 } is a valid causal cut. Moreover, | V 1 | =  X n and | V 2 |  X  n 2  X   X cn . By picking  X  = 1 2 c +2 min {| V 1 | , | V 2 |}  X  n 2 c +2 . When v is selected in V gorithm 2 must converge to a solution better than the artificial configuration above. This happens with prob-By running the randomized search algorithm k = (2 c + 2) 2 times, since (1  X  e  X   X  )  X   X  e  X  1 when  X  is sufficiently large, the probability of finding a causal cut with min {| V 1 | , | V 2 |} X  n 2 c +2 is at least 1/2. The last theorem implies that the causal cut is effective on reducing the scale of the subproblems. Another implication is on the selection of the parameter  X  . To guarantee there is a reduction on problem size, the parameter  X  should be no smaller than 2 c + 2, since such theta ensuring that  X  2 c +2  X  1. 4.2. Correctness and Completeness The effectiveness of SADA is guaranteed based on the conclusion of the following theorem.
 Theorem 2 Assume D is a set of data samples gen-erated from the causal structure G defined on the vari-ables in V . If the causation algorithm A and condi-tional independence test used in SADA are both reli-able, SADA always finds the true causal structure G . Proof: Assume G  X  is the causal structure discovered by SADA. We only need to prove the correctness and completeness of G  X  . The correctness and completeness are equivalent to  X  ( v 1  X  v 2 )  X  G  X  , ( v 1  X  v 2 )  X  G , and  X  ( v 1  X  v 2 )  X  G , ( v 1  X  v 2 )  X  G  X  , respectively. The details of the proof are given as follows: Completeness: Assume ( v 1  X  v 2 )  X  G , firstly, ac-cording to the causal cut step, both v 1 and v 2 must be in one subproblem, V 1  X  C or V 2  X  C , but not acrose the two subproblems. Otherwise, v 1 and v 2 is condi-tional independent of each other given some subset of C , conflicts with the condition v 1  X  v 2  X  G and the assumption that the conditional independence test is reliable. Secondly, according to the following two con-ditions:  X  v 1 and v 2 are in the same subproblem X  and  X  X asic causal solver is reliable X , v 1  X  v 2  X  G  X  will be discovered in one of the subproblems. Finally, the edge v 1  X  v 2 won X  X  be removed in the merge step, because if the edge is removed by either conflict or redundancy reason, it will conflict with the condition v 1  X  v 2  X  G and the assumption that the condition independence test is reliable. Thus, v 1  X  v 2 must be contained in the result of SADA, in anther word, v 1  X  v 2  X  G  X  . Correctness: Assume ( v 1  X  v 2 )  X  G  X  , firstly we will show v 1  X  v 2 is the correct result of the subproblems. According to the framework of SADA, v 1 and v 2 must be discovered in one of the subproblem V 1  X  C and V 2  X  C . Without loss of generality, assume v 1  X  v 2 is discovered in the subproblem V 1  X  C by the basic causal solver. According to the condition that the ba-sic causal solver is reliable, v 1  X  v 2 must be the cor-rect result for the subproblem V 1  X  C . Secondly, we will show ( v 1  X  v 2 )  X  G . If v 1  X  v 2 is the correct re-sult of V 1  X  C but not contained in G , then there must appears some variable set V  X   X  V satisfies v 1  X  v 2 | V Thus, there must be a path v 1  X  X  X  X  X  X  v 2 which con-tains V  X  as intermediate nodes. If such path exists, according to the Merge step, v 1  X  v 2 will be removed from the result set G  X  , and conflict with the condition that v 1  X  v 2  X  G  X  . Thus, v 1  X  v 2  X  G .
 The previous theorem is based on an optimal setting on the reliability of the causation algorithms and con-ditional independence testings. In practice, such reli-ability may not be achieved, due to the noises on the samples and limited accuracy of the statistical num-bers. In the experiments, we show that our approach remains effective, even when the condition of reliability is not fully satisfied. 4.3. Connection between SADA and ICA In this part of the section, we discuss the connection between SADA and ICA, under the assumption of lin-ear correlation between variables and non-Gaussian noises. By running SADA, the variables are parti-tioned into (possibly) overlapping subsets, such that each two subsets are independent of each other, in the sense that there is no causal edge across them. On the other hand, running ICA on the samples could be interpreted based on the causal graph represented in matrix form.
 In Figure 2 , we present a simple example with four v 3 = v 1 + e 3 and v 4 = v 2 + e 4 . An entry in the ma-trix indicates if there is an causal edge between two variables. Due to the sample generation rule, there are only two 1s in the matrix, i.e. ( v 1 , v 3 ) and ( v The ICA approach tries to find a permutation over the complete variable set, such that triangle sub-matrices with zeros on all top right entries are identified, as is shown in Figure 2 . Similarly, SADA returns two sub-problems based on the causal cut C =  X  , V 1 = { v 1 , v 3 and V 2 = { v 2 , v 4 } , by spending a much smaller com-putation cost. In a more complicated case with the causal structure in Figure 1 , the causal relationships are modeled in Fig-ure 3 . Since it X  X  impossible to find a permutation as in Figure 2 to perfectly divide the variables into triangle sub-matrices, a duplicate variable on v 4 must be intro-duced to satisfy the requirements of ICA. Our SADA framework easily finds the causal cut with C = { v 4 } , V 1 = { v 1 , v 3 , v 6 , v 7 } and V 2 = { v 2 , v 5 , v 8 generates subproblems consistent with ICA X  X  results. However, the computational cost of SADA is signifi-cantly smaller than that of running ICA on the com-plete variable set. This is the main advantage of SADA, since much less samples are needed to find a robust causal cut and each subproblem is much easier to solve.
 We evaluate our proposal on datasets generated by different real-world Bayesian network structures 1 , un-der linear non-Gaussian model and discrete additive noise model. It generally covers a variety of applica-tions, including, medicine ( Alarm dataset), weather forecasting ( Hailfinder dataset), printer troubleshoot-ing( Win95pts dataset), pedigree of breeding pigs ( Pigs dataset) and linkage among genes ( Link dataset). The structural statistics of these Bayesian networks are summarized in Table 1 . In all the Bayesian networks, the maximal degrees, i.e. the maximal number of parental variables in the networks, are no larger than 6, regardless of the total number of variables. This ver-ifies the correctness of our sparsity assumption. On all datasets, SADA stops the partitioning when the subproblem reaches the size  X  = 10. The recursive partitioning is also terminated when Algorithm 2 fails to find any valid causal cut.
 In all the experiments, we report results on Causal Cut Error . We use N to denote the number of causal variable pairs in the specific Bayesian network, and use N e to denote the number of causal variable pairs wrongly divided into subproblems after running divi-sion operations in SADA. The causal cut error is the ratio N e /N . We also report the recall , precision and F1 score on the result causal relationships returned by SADA and baseline approaches. Specifically, F1 score is calculated as 2 P  X  R P + R , which R and P are recall and precision respectively. The experiments are compiled and run with Matlab 2009a on a windows PC equipped with a dual-core 2.93GHz CPU and 2GB RAM. 5.1. On Linear Non-Gaussian Model Under the assumption of linear non-Gaussian model, the samples are generated based on linear functions as v = P v these linear functions, we restrict that P P ( v and the the variance V ar ( e i ) = 1 for every variable v We employ the conditional independence test follow-ing the method proposed in ( Baba et al. , 2004 ), with threshold at 95%. LiNGAM ( Shimizu et al. , 2006 ) is appointed as the basic causation algorithm A after SADA reaches the minimal scale threshold  X  at sub-problems. LiNGAM without applying any division is also used as the baseline approach, denoted by BL, when reporting recall, precision and F1 score. The causal cut errors are reported in Figure 4 , on vary-ing the number of samples generated by the Bayesian networks. Even when the samples size is 2 | V | , the highest causal cut error is within 0.12. Moreover, the causal cut error consistently decreases with the growth of sample size. These results reveal the fundamental advantage of SADA, such that the sufficient number of samples only depends on the sparsity of the causal structure but not the number of variables. Note that the baseline approach LiNGAM does not work when the number of samples are as small as 2 | V | . In the following experiments, we compare SADA against the baseline approach by fixing the sample size at 2 | V | . As shown in Table 2 , SADA achieves signifi-cantly better F1 score on all of the five datasets. SADA is particularly doing well on precision, i.e. returning more accurate causality relationships. SADA X  X  divi-sion strategy is the main reason behind the improve-ment of precision on SADA. Specifically, the division on variables allows SADA to remove a large number of candidate variable pairs if they are assigned to V 1 and V 2 . The basic causality sovler, LiNGAM in this case, is run on subproblem of much smaller scale, thus generating more reliable results. The Recall of SADA is comparable to the baseline approach on four of the datasets, and slightly worse on the other one. This shows that the unavoidable causal cut error does not affect the recall under linear non-Guassian models. 5.2. On Discrete Additive Noise Model The generation process of the discrete data follows the method used in ( Peters et al. , 2011 ) under Addi-tive Noise Model(ANM) for causal inference on dis-crete data. Each variable is restricted to 3 different value and values are randomly generated based on conditional probability tables. The implementation of SADA for discrete domain is slightly different from that for continuous domain. G 2 test ( Spirtes et al. , 2001 ) is employed as the conditional independence test, with the threshold at 95%. The causation algo-rithm A called by SADA is a brute force method to find all causalities on problems of small scaled. Again, the brute-force method without variable division is also employed as a baseline approach, denoted by (BL) in these results. In particular, the algorithm checks ev-ery possible pair of variables following the method pro-posed in ( Peters et al. , 2011 ).
 The causal cut error of SADA on the discrete data is presented in Figure 5 , which shows similar property of the result on linear non-Gaussian models. This fur-ther verifies the generality of SADA on different data domains.
 In this group of experiments, we fix the sample size at 2000, and report recall, precision and F1 score in Table 3 . Note that the baseline approach is only applicable to domain with small number of variables. This leads to difficulties for baseline to finish the computation on Pigs and Link in one week. This proves the improve-ment of SADA on scalability in terms of the variables. Generally speaking, the results in the table also veri-fies the effectiveness of SADA, especially on dramatic enhancement on precision and F1 score.
 As a conclusion, SADA shows excellent performance on 5 different domains with real-world Bayesian net-works. SADA returns accurate causal relations when combined with two well known causal inference algo-rithms. The causal cut used to partition the problem does incur certain error on incorrect partitioning. De-spite of the errors, SADA still outperforms the baseline algorithms without partitioning on almost all settings. In this paper, we present a general and scalable frame-work, called SADA, to support causal structure in-ference, using a split-and-merge strategy. In SADA, causal inference problem on a large variable set is partitioned into subproblems with overlapping sub-sets of variables, utilizing the concept of causal cut. Our proposal facilitates existing causation algorithms to handle problem domains with more variables and less samples, which are considered impossible in the past. Strong theoretical analysis proves the effective-ness, correctness and completeness guarantee of SADA under a general setting. Experimental results fur-ther verifies the usefulness of the new framework with two mainstream causation algorithms on linear non-Gaussian model and discrete additive noise model. Cai and Hao are supported by NSF of China (61070033, 61100148,61202269), NSF of Guangdong Province (S2011040004804), Opening Project of the State Key Laboratory for Novel Software Technology (KFKT2011B19), Foundation for Distinguished Young Talents in Higher Education of Guangdong, China (LYM11060).
 Aliferis, Constantin F., Statnikov, Alexander, Tsamardinos, Ioannis, Mani, Subramani, and
Koutsoukos, Xenofon D. Local causal and markov blanket induction for causal discovery and feature selection for classification. J. Mach. Learn. Res. , 11:171 X 234, 2010.
 Baba, Kunihiro, Shibata, Ritei, and Sibuya, Masaaki.
Partial correlation and conditional correlation as measures of conditional independence. Australian &amp; New Zealand Journal of Statistics , 46(4):657 X 664, 2004.
 Bromberg, Facundo and Margaritis, Dimitris. Improv-ing the reliability of causal discovery from small data sets using argumentation. J. Mach. Learn. Res. , 10: 301 X 340, 2009.
 Cai, Ruichu, Zhang, Zhenjie, and Hao, Zhifeng. Bas-sum: A bayesian semi-supervised method for clas-sification feature selection. Pattern Recognition , 44 (4):811 X 820, 2011.
 Cai, Ruichu, Zhang, Zhenjie, and Hao, Zhifeng. Causal gene identification using combinatorial v-structure search. Neural Networks , 2013. doi: 10.1016/j. neunet.2013.01.025.
 Friedman, Nir, Linial, Michal, Nachman, Iftach, and
Pe X  X r, Dana. Using bayesian networks to analyze expression data. In RECOMB , pp. 127 X 135, 2000. He, Y. and Geng, Z. Active learning of causal networks with intervention experiments and optimal designs. J. Mach. Learn. Res. , 9:2523C2547, 2008.
 Hoyer, Patrik O., Janzing, Dominik, Mooij, Joris M.,
Peters, Jonas, and Sch  X olkopf, Bernhard. Nonlin-ear causal discovery with additive noise models. In NIPS , pp. 689 X 696, 2008.
 Janzing, Dominik, Mooij, Joris M., Zhang, Kun,
Lemeire, Jan, Zscheischler, Jakob, Daniusis, Povi-las, Steudel, Bastian, and Sch  X olkopf, Bernhard.
Information-geometric approach to inferring causal directions. Artif. Intell. , 182-183:1 X 31, 2012. Kalisch, M. and B  X uhlmann, P. Estimating high-dimensional directed acyclic graphs with the pc-algorithm. The J. Mach. Learn. Res. , 8:613 X 636, 2007.
 Kim, Sunyong, Imoto, Seiya, and Miyano, Satoru. Dy-namic bayesian network and nonparametric regres-sion for nonlinear modeling of gene networks from time series gene expression data. Biosystems , 75(1-3):57 X 65, 2004.
 Koller, Daphne and Friedman, Nir. Probabilistic Graphical Model: Principles and Techniques . The MIT Press, 2 edition, 2009.
 Mooij, Joris M., Janzing, Dominik, Peters, Jonas, and
Sch  X olkopf, Bernhard. Regression by dependence minimization and its application to causal inference in additive noise models. In ICML , pp. 94, 2009. Pearl, Judea. Causality: models, reasoning and infer-ence . Cambridge Univ. Press, 2 edition, 2009. Pearl, Judea and Verma, Thomas. A theory of inferred causation. In Proceedings of the 2nd International
Conference on Principles of Knowledge Representa-tion and Reasoning , pp. 441 X 452, 1991.
 Peters, Jonas, Janzing, Dominik, and Sch  X olkopf, Bern-hard. Identifying cause and effect on discrete data using additive noise models. In AIStats , pp. 597 X  604, 2010.
 Peters, Jonas, Janzing, Dominik, and Sch  X olkopf, Bern-hard. Causal inference on discrete data using addi-tive noise models. IEEE Trans. Pattern Anal. Mach. Intell. , 33(12):2436  X  2450, 2011.
 Shimizu, Shohei, Hoyer, Patrik O., Hyv  X arinen, Aapo, and Kerminen, Antti J. A linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res. , 7:2003 X 2030, 2006.
 Shimizu, Shohei, Inazumi, Takanori, Sogawa, Ya-suhiro, Hyv  X arinen, Aapo, Kawahara, Yoshinobu,
Washio, Takashi, Hoyer, Patrik O., and Bollen, Ken-neth. Directlingam: A direct method for learning a linear non-gaussian structural equation model. J. Mach. Learn. Res. , 12:1225 X 1248, 2011.
 Spirtes, Peter, Glymour, Clark, and Scheines, Richard.
Causation, Prediction, and Search . The MIT Press, 2 edition, 2001.
 Zhang, Kun, Peters, Jonas, Janzing, Dominik, and
Sch  X olkopf, Bernhard. Kernel-based conditional in-dependence test and application in causal discovery. CoRR , abs/1202.3775, 2012.
 Zhu, Z., Ong, Y.S., and Dash, M. Markov blanket-embedded genetic algorithm for gene selection. Pat-tern Recognition , 40(11):3236 X 3248, 2007.
 Zscheischler, Jakob, Janzing, Dominik, and Zhang, Kun. Testing whether linear equations are causal:
A free probability theory approach. CoRR ,
