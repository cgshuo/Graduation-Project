 The majority of the current information retrieval models weight the query concepts (e.g., terms or phrases) in an unsupervised manner, based solely on the collection statis-tics. In this paper, we go beyond the unsupervised estima-tion of concept weights, and propose a parameterized con-cept weighting model. In our model, the weight of each query concept is determined using a parameterized com-bination of diverse importance features. Unlike the exist-ing supervised ranking methods, our model learns impor-tance weights not only for the explicit query concepts, but also for the latent concepts that are associated with the query through pseudo-relevance feedback. The experimen-tal results on both newswire and web TREC corpora show that our model consistently and significantly outperforms a wide range of state-of-the-art retrieval models. In addi-tion, our model significantly reduces the number of latent concepts used for query expansion compared to the non-parameterized pseudo-relevance feedback based models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Parameterized concept weighting, query expansion
Term weighting is a classical information retrieval prob-lem that has been studied for decades. However, despite significant advances, a majority of the most widely used in-formation retrieval models, including language modeling [29] and BM25 [30], still weight the importance of query concepts (e.g., terms or phrases) in a simple, unsupervised manner. Such unsupervised estimates of concept importance tend to be based solely on global collection statistics, as evidenced by the standard inverse document frequency (IDF) measure of a term X  X  importance.

However, such unsupervised estimates of concept impor-tance have two major shortcomings that can degrade re-trieval effectiveness. First, these estimates are rigid and in-flexible, due to their dependency on a single global statistic (e.g., IDF). Such dependency does not take into account the influence of a wide range of factors beyond document frequency on the importance of a query concept. Flexible query term weighting is particularly important for verbose queries that often contain a mix of key and complementary concepts [5, 14].

Second, most of the importance weighting research has been applied to single terms (i.e., unigrams). Relatively little research has been done to investigate the appropri-ateness of IDF and other unsupervised importance weights for multiple-term concepts, including bigrams, phrases, and other proximity expressions. In fact, recent work by Mac-donald and Ounis suggests that global statistics do not play as important of a role for such concepts [22]. This research highlights the need for improved understanding of possible alternatives for estimating the importance of multiple-term concepts.

To overcome the shortcomings of unsupervised concept weighting, a number of researchers have recently proposed to incorporate flexible supervised concept importance weight-ing into the Markov Random Field model [18, 6, 23]. In particular, the weighted sequential dependence model ( WSD ) proposed by Bendersky et al. [6] models the importance weight of the query concepts (including query terms, ex-act phrases and proximity matches) as a linear combina-tion of document-independent features, such as the number of times the concept occurred as a Wikipedia title, its fre-quency within a query log, and its global frequency within a large web crawl. The WSD model provides a flexible way of estimating importance and can be effectively optimized using existing learning to rank approaches.

The main shortcoming of the WSD model [6], and its vari-ants [31, 37], is that the weighting is performed exclusively on the concepts that explicitly occur within the query and disregards the latent concepts associated with the informa-tion need underlying the query (e.g., the concepts distilled by state-of-the-art query expansion approaches such as rel-evance model [16] or latent concept expansion [24]). There-fore, the question of how to seamlessly and effectively in-Table 1: Explicit and latent concepts with the high-e st importance weight for the query  X  X hat is the current role of the civil air patrol and what training do participants receive? X . tegrate these latent concepts within a supervised concept weighting model still remains open.

To address this question, in this paper, we propose a novel parameterized query expansion model. The proposed model provides an effective alternative to the standard unsuper-vised weighting for both single terms and multiple-term con-cepts. In addition, the model generalizes the current super-vised concept weighting approaches [6, 18, 31, 35, 37] and provides a unified framework for weighting both explicit and latent query concepts.

As an illustrative example of the parameterized query ex-pansion in action, consider the verbose query Table 1 shows the most important explicit query concepts (i.e., the query terms and bigrams) and the most important latent concepts (i.e., the expansion terms) learned by our model. Note that the weights assigned by our model are different from the weights that would be assigned by IDF alone. For instance, while the term air has higher IDF than the term training , it is deemed less important for the query. In addition, while the term air is not important on its own, it is significant in the context of the bigram air patrol .
In the case of the query in Table 1, the parameterized query expansion model improves the retrieval effectiveness by 64% over the standard query-likelihood model [29], by 21% over the WSD model [6], and by 8% over the latent concept expansion model [24]. As the evaluation in Sec. 5 demonstrates, these gains in retrieval effectiveness are con-sistent across queries and collections.

This paper has three primary contributions. First, we describe a novel parameterized query expansion model. Pa-rameterized query expansion provides a flexible framework for modeling the importance of both explicit and latent query concepts. As we show, this framework is a gener-alization and unification of current state-of-the-art concept weighting [6, 18, 31] and query expansion [24, 15] models. Second, we describe a novel two-stage optimization tech-nique for parameterized query expansion. This technique leverages learning to rank approaches for effective estima-tion of the explicit and latent concepts importance weights. Finally, we carry out a detailed empirical evaluation that demonstrates the state-of-the-art effectiveness of the pro-posed model. Our evaluation shows that the approach is particularly beneficial for verbose queries, but is also highly effective for short keyword queries.
 The remainder of this paper is laid out as follows. First, Sec. 2 details the process of concept weight parameteriza-tion. Then, Sec. 3 describes query expansion with parame-terized concept weights. Related work is covered in Sec. 4. Experimental evaluation is provided in Sec. 5. Finally, Sec. 6 concludes the paper and describes directions for future work.
Given a query Q , we assume that there exists a set of concepts related to the underlying information need. In this paper, we use a very broad definition of a concept. A concept is defined as any syntactic expression that can be matched within a document .

Clearly, there are a multitude of concept types that can potentially be associated with the information need: indi-vidual words, exact phrases, unordered phrases, etc. It is important to note that these concept types can be either ex-plicitly present in the query (e.g., query terms or phrases), or latent (e.g., concepts that are associated with the infor-mation need via the process of query expansion).

Formally, we use T to denote the set of all possible concept types (explicit and latent) associated with the information need underlying the query Q . Then, to assign a score to document D in response to the query Q , we use a linear weighted combination of matches in document D of all con-cepts types in T as follows:
This ranking function consists of two components: a con-cept matching function f (  X , D ) that computes how related  X  is to D , and a concept importance weight  X   X  that indi-cates the importance of  X  . We now discuss how these two components are computed.
The concept matching function f (  X , D ) assigns a score to the matches of concept  X  in the document D . The function can take various forms, but in information retrieval appli-cations it is commonly a monotonic function, i.e., its value increases with the number of times concept  X  matches doc-ument D .

Throughout the remainder of this paper we assume that the matching function f (  X  , D ) takes the form where tf  X ,D and tf  X , C are the number of concept occurrences in the document and the collection, respectively;  X  is a free parameter; | D | is the number of terms in D , and |C| is the total number of terms in the collection.

The matching function in Eq. 2 is exactly the log of the language modeling estimate for concept  X  with Dirichlet smoothing [39]. We use the language modeling estimate as a concept matching function since it is convenient and effi-cient to compute and exhibits state-of-the-art retrieval per-formance in other concept-based retrieval models [23, 24]. However, Eq. 1 can also be implemented using other match-ing functions such as BM25 [30] or DFR [3].
Parameter  X   X  assigns a document-independent importance weight to each concept  X  associated with the information need. Based on Eq. 1, the score for document D increases as it matches more of the important concepts associated with the information need. Therefore, correct estimation of the concept importance weight  X   X  is a crucial aspect of the rank-ing function in Eq. 1. There are several possible approaches f or determining concept importance.

One common approach is tying the weights  X   X  of all the concepts of the same type T . This approach is commonly used in the bag-of-words models [29, 3, 30], as well as models that incorporate multiple concept types [23, 28, 10]. Con-cept weight tying is equivalent to the assumption that all the concepts of the same type are equally important for express-ing the query intent. Such an assumption can be potentially detrimental for retrieval performance, especially for complex verbose queries, which combine a large number of concepts of different types [5].

Accordingly, we would like to relax the uniform impor-tance assumption and estimate the weights  X   X  . Clearly, separately estimating a single weight for each concept  X  is infeasible, since the number of possible concepts is exponen-tial in the size of the vocabulary.

Instead, we parameterize a concept of type T using a set of importance features  X  T , which is associated with each concept of the type T . Thus, a concept weight,  X   X  , can be represented as a linear weighted combination of importance features
Although a linear form of the importance weighting func-tion is used to simplify parameter estimation (see Sec. 3.3), there is no reason why more complex functional forms could not be used instead.
Having specified the concept matching function and the parameterized concept weight, we are now ready to derive the final parameterized form of our ranking function. Plug-ging the parameterized concept weight  X   X  from Eq. 3 into Eq. 1, we get
From Eq. 4 it is evident that sc ( Q, D ) is in fact linear in w  X  . This observation simplifies the optimization of Eq. 4, since many current learning to rank techniques [19] can be readily applied to efficiently and effectively optimize a linear ranking function.

Recently, there has been some work on ranking with pa-rameterized concept weights [5, 6, 18, 35]. There are two main limitations in the existing work that we address in this paper.

First, the previous work does not take the holistic view presented in this paper, and optimizes weights for a pre-determined concept type. For instance, Bendersky and Croft [5] apply parameterized weights only to the noun phrase concepts, Lease [18] applies it only to the query terms, and Svore et al. [35] only to the query term spans [33].
Second, the aforementioned previous work focuses on the surface form of the query, rather than its underlying infor-mation need. Therefore, it is only applicable to the concepts that are explicitly present in the query, and not to the la-tent concepts that are obtained through query expansion. In contrast, in this paper we propose a novel parameterized query expansion model that applies parameterized concept weighting to both the explicit and the latent query concepts.
In this section, we provide a detailed description of the parameterized query expansion retrieval model. This model is an implementation of the general parameterized concept weighting model defined in Sec. 2. It jointly weights several types of both explicit and latent concepts associated with the query. To fully describe the parameterized query expansion model (also referred to as PQE ), in Sec. 3.1 we detail the concept types used by the model; in Sec. 3.2 we describe the set of features that determine the concept importance; finally, in Sec. 3.3 we outline the process of optimizing the retrieval effectiveness of the parameterized ranking function.
In this section, we define the set of concept types T used by the parameterized query expansion retrieval model. Re-call that the choice of T will determine the structure of the parameterized ranking function in Eq. 4. PQE draws from two sources of evidence for deriving the concepts in T . The first source is the set of words ( q 1 , q 2 , . . . , q | Q | the query Q . The second source is the set of latent concepts  X  concepts that are associated with the query Q through the process of pseudo-relevance feedback. Overall, T consists of the following concept types. (1) QT -concepts . The query term ( QT ) concepts are simply the individual query words q i . This is the most common concept type in information retrieval, which is used both in bag-of-words models [29, 30] and models that incorporate multiple concept types [23, 27, 10]. (2) PH -concepts . The phrase ( PH ) concepts are combinations of query terms that are matched as exact phrases in the doc-ument. Exact phrase matching has often been used for im-proving the performance of retrieval methods [9, 38]. Most recently, it has been shown that using query bigrams for exact phrase matching is a simple and efficient method for improving the retrieval performance in large scale web col-lections [23, 27, 28]. Following this finding, we define the PH -concepts as adjacent query word pairs ( q i q i +1 ). (3) PR -concepts . Similarly to the PH -concepts, the proxim-ity ( PR ) concepts are defined over adjacent query word pairs ( q q i +1 ). In order to match the document, both of the indi-vidual words in a PR -concept must occur in any order within a window of fixed length . In this paper, we fix the window size to 8 words, following some previous work on proximity matching [23, 24, 28]. (4) ET -concepts . The expansion ( ET ) concepts are defined as the top-K terms associated with the query through the pro-cess of pseudo-relevance feedback. There is an abundance of literature on query expansion using pseudo-relevance feed-back, most recent of which includes, among many others, work by Lavrenko and Croft [16], Tao and Zhai [36], Cao et al. [8], and Lv an Zhai [21]. In this paper we use the latent concept expansion (LCE) technique, first proposed by Metzler and Croft [24]. This technique has several impor-tant advantages, including state-of-the art performance [24, 15] and the ability to leverage information about arbitrary concepts to improve the quality of query expansion.
To obtain the initial set of ET -concepts using LCE, we first rank documents in the collection using Eq. 4 includ-ing only the concept types manifested in the query itself ( QT -concepts, PH -concepts and PR -concepts). Then, all the terms in the pseudo-relevant set of documents R (top ranked d ocuments) are weighted by w LCE (  X  ) = X where  X  i  X  X  are free parameters. Finally, K terms with the highest w LCE weights, are added to the set of ET -concepts.
As evident from Eq. 5, w LCE combines three key fea-tures to rank a concept: document relevance (manifested by the document score sc ( Q , D )), score of the concept in the pseudo-relevance set R (manifested by the matching function f (  X  , D )), and the inverse collection frequency (ICF) of the concept (  X  log tf  X , C |C| ) . The ICF factor dampens the weights of very common words, thereby improving the qual-ity of the initial set of ET -concepts.

Latent concept expansion can be adopted to include any arbitrary concept type for query expansion. However, in this paper we limit the expansion to individual terms. First, this focus improves the overall efficiency of the PQE model. Second, previous work found no significant benefits when additional types of latent concepts (such as bigrams) were associated with the query in addition to terms alone [24].
As described in Sec. 2, in order to derive the ranking func-tion sc ( Q, D ) (Eq. 4), we associate a separate set of impor-tance features  X  T with each concept type T = ( QT , PH , PR , ET ). As these features depend only on the concept itself, they can leverage the statistics of the underlying document collection as well as the statistics of external data sources to achieve a potentially more accurate concept weighting.

Following previous work [5, 6, 37], in this paper we use three such external data sources: (i) a large collection of web n-grams, (ii) a sample of a search log, and (iii) Wikipedia. Some of these data sources provide better coverage of terms, while others provide more focused sources of information for determining concept importance. Although there are nu-merous additional data sources that could be potentially used, we intentionally limit our attention to these three sources as they are available for research purposes, and can be used to reproduce the reported results.
 the frequency counts of English n-grams generated from ap-proximately 1 trillion word tokens. We expect these counts to provide a more accurate frequency estimator, especially for smaller corpora, where some concept frequencies may be underestimated due to the collection size.

In addition, we use a large sample of a search log con-data source to estimate how often a concept occurs in user queries. Intuitively, we assume a positive correlation be-tween an importance of a concept for retrieval and the fre-quency with which it occurs in queries formulated by the search engine users.
 Finally, our third external data source is a snapshot of high diversity of topics covered by Wikipedia, we assume that important concepts will often appear in its article titles.
For each concept type, we derive five simple frequency fea-tures based on these three external sources, as well as the underlying document collection (see Table 2). Note that the parameterized concept weighting in Eq. 3 does not restrict us to this particular set of features, and any additional im-portance features can be associated with each concept type. However, in this paper, we limit our attention to these five features, since they can be efficiently computed and cached even for large-scale web collections [37], and are suitable for operational retrieval systems.

In addition to the frequency features, Table 2 lists a sixth feature, AP (  X  ), which is an a priori concept weight (a weight assigned to the concept by default). AP (  X  ) is set to 1 for query-based concept types ( QT , PH , PR ), and to w LCE (Eq. 5) for the ET -concepts.

The features in Table 2 are computed for each of the four concept types, resulting in 24 features overall. For each such feature  X  , we need to estimate the parameter w  X  (see Eq. 4). This estimation process is described in the next section.
To estimate the free parameters w  X  associated with the concept importance features in the ranking function in Eq. 4, we rely on a large and growing body of literature on the learning to rank methods for information retrieval (see Liu [19] for a survey). In this section, we first discuss the co-ordinate ascent (CA) algorithm [25], which we choose as a base optimization algorithm (Sec. 3.3.1). Then, in Sec. 3.3.2 we discuss the adaption of the CA algorithm for the param-eterized query expansion.
As previously discussed, the ranking function in Eq. 4 is linear w.r.t. w  X  . Therefore, as a base algorithm for optimiz-ing the parameters in Eq. 4 we make use of the coordinate ascent (CA) algorithm proposed by Metzler and Croft [25].
The CA algorithm iteratively optimizes a target metric (in our case, retrieval metric such as MAP) by performing a series of one-dimensional line searches. It repeatedly cy-cles through each of the parameters w  X  , holding all other parameters fixed while optimizing it. This process is per-formed iteratively over all parameters until the gain in the target metric is below a certain threshold. Although we use the CA algorithm primarily for its simplicity, efficiency and effectiveness, any other learning to rank approach that esti-mates the parameters for linear models such as RankSVM [13] or RankNet [7] can be adopted as well.
In contrast to most other learning to rank approaches, which usually consider only the concepts that explicitly oc-cur in the query, the parameterized query expansion com-bines evidence from the query itself and the pseudo-relevance feedback in response to the query. Therefore, the setting of 3 A vailable at http://download.wikimedia.org/enwiki/ Figure 1: Algorithms for (a) training and (b) testing phases of the parameterized query expansion model. the importance feature weights associated with the query-based concept types ( QT , PH , PR ), will have a direct effect on the quality of the ET -concepts set, obtained through pseudo-relevance feedback.

To address this challenge, we propose a novel two-stage optimization technique for estimating the set of free param-eters W  X  in the PQE retrieval model. While simple, this two-stage technique is effective for learning robust weights for both explicit and latent query concepts, as well as im-proving the quality of the set of ET -concepts.

The algorithm in Fig. 1 provides a schematic overview of this two-stage optimization. At the first stage of the training phase (Fig. 1 (a)), we include only the explicit concept types for optimizing the weights W  X  (line a3 ) and ranking with Eq. 4 (line a4 ). This initial ranking is used to obtain a large initial pool of ET -concepts 4 using latent concept expansion, as described in Eq. 5 (line a5 ).

At the second stage of the training phase, we include both explicit and latent concepts for ranking with Eq. 4 (line a6 ). A second round of the CA algorithm is then performed in order to re-estimate the weights W  X  for all concept types (line a7 ). To make the optimization process more efficient, at each iteration of the CA algorithm, we use only a small set of the top-K ET -concepts from the initial large pool 5 . At each iteration, the top-K ET -concepts are updated based on the current setting of the W  X  .

The training phase concludes after the second round of the CA algorithm is completed. At this point, the weights W
 X  are optimized (in terms of the target retrieval metric) for the training queries. We then use a held-out set of test queries to evaluate the performance of the optimized weights W
 X  (Fig. 1 (b)).
Importance weighting of query concepts is one of the key challenges of information retrieval research. However, the majority of commonly used bag-of-words retrieval models (including, among many others, BM25 [30] and language modeling [11, 29, 39]) still use unsupervised term weight-ing based on global collection statistics, which resembles the term weighting proposed by Luhn [20] in 1958.

Recently, researchers began investigating techniques for supervised weighting of the terms and concepts in the query [5, 6, 18, 31, 35]. However, these investigations mostly focus only on assigning importance weights to a subset of pre-determined concept types. Table 3: Summary of TREC collections and topics u sed for evaluation in Sec. 5. Figure 2: An example of h t itle i and h desc i portions of a TREC topic #752.

For instance, Lease [18] focuses on term weighting, Ben-dersky and Croft [5] on noun phrases, and Svore et al. [35] on query term spans. In addition, these supervised techniques take into account only the explicit query concepts and dis-regard the latent concepts that can be associated with the query via expansion. The parameterized query expansion method proposed in this paper addresses these limitations.
Another field of research which is relevant to this paper is pseudo-relevance feedback. While there is a large number of successful pseudo-relevance feedback based retrieval models (e.g., [8, 16, 24, 21, 38]), most of them employ unsupervised weighting for both explicit and latent concepts. A notable exception is the work by Cao et al. [8] which uses binary classification to determine the importance of the expansion terms. Unlike Cao et al. [8], the proposed parameterized query expansion method takes a more holistic approach, and assigns importance weights to both explicit and latent con-cepts.
This section describes the details of our experimental eval-uation. First, in Sec. 5.1, we describe the experimental setup used for the evaluation. Then, in Sec. 5.2, we com-pare the performance of the parameterized query expansion ( PQE ) method to the performance of several standard non-parameterized retrieval methods. Further analysis, compar-isons and in-depth discussion of the results are provided in Sec. 5.3.
The retrieval experiments described in this paper are im-plemented using Indri, an open-source search engine [34]. The structured query language implemented in the Indri search engine natively supports multiple concept types, in-cluding exact phrases and proximity matches. The Indri query language also supports custom term weighting schemes. As a result, Indri provides a flexible and convenient platform for evaluating the performance of our method.

Table 3 presents a summary of the TREC corpora used in our experiments. The corpora vary both by type (RO-BUST04 is a newswire collection, while WT10g and GOV2 are web collections), number of documents, and number of available topics, thereby providing a diverse experimental setup for assessing the robustness of the proposed retrieval method.

During indexing and retrieval, both documents and queries are stemmed using the Porter stemmer. Stopword removal is performed on both documents and queries using the stan-Table 4: Summary of the evaluated retrieval meth-o ds. Each cell indicates whether the weights for the concept type are parameterized according to Eq. 3 ( P ) or not ( N ). An empty cell indicates that the concept type is not used by the retrieval method. dard INQUERY stopword list [2]. The free parameter  X  in the concept matching function f (  X  , D ) (see Eq. 2) is set to 2 , 500, according to the default Indri configuration of the Dirichlet smoothing parameter.

The optimization of the PQE method is done using 3-fold cross-validation with mean average precision (MAP) as the target metric of the CA algorithm (see Sec. 3.3.2). The statistical significance of differences in the performance of PQE with respect to other retrieval methods is determined using a two-sided Fisher X  X  randomization test with 50,000 permutations and  X  &lt; 0 . 05.

As was shown in previous work [4, 5, 6, 18], the impact of concept weighting techniques varies significantly across queries of different length. In general, more verbose queries are expected to benefit more from concept weighting, since they are more likely to contain concepts of varying impor-tance. Thus, to test the performance of the proposed meth-ods across multiple query lengths, we treat the h title i and the h desc i portions of TREC topics as two separate sets of queries in our experiments. The h title i and the h desc i query convey the same information need for the same topic, but differ in their structure. The h title i query is a short keyword query, while the h desc i query is a verbose natural language description of the information need. Fig. 2 shows an exam-ple of h title i and h desc i queries for TREC topic #752.
Our initial evaluation compares the retrieval performance of the parameterized query expansion ( PQE ) retrieval method (described in Sec. 3) to the performance of several standard baseline methods that do not employ concept weight param-eterization.

First, we compare the retrieval performance of the PQE method to the performance of the query likelihood ( QL ) [29] and sequential dependence model ( SD ) [23] retrieval meth-ods. These baselines do not perform query expansion, and differ in the choice of the query-based concept types that they use. QL is a standard bag-of-words method. In contrast, the SD method uses, in addition to query terms, both PH -concepts and PR -concepts (which are described in Sec. 3.1). The SD method has consistently demonstrated state-of-the-art retrieval effectiveness in a variety of search tasks, and especially for search over large web collections [23]. Top per-forming submissions at several TREC tracks have used SD : Terabyte Track 2004-2006 [26], Million Query Track 2007-2008 [1] and Web Track 2009 [32].

Second, we compare the performance of PQE to the per-formance of two retrieval methods that perform pseudo-relevance feedback (PRF) for query expansion: the RM3 variant of the relevance model ( RM ) [16] and Latent Concept Expansion ( LCE ) [24]. Both of these methods are known to improve retrieval performance over methods that do not em-ploy query expansion. Analogously to the QL and SD meth-ods, the RM and LCE methods differ in their choice of the query-based concept types. RM is a bag-of-words model, while LCE uses both PH -concepts and PR -concepts. Both PRF-based methods use individual terms (i.e., unigrams) for query expansion.

Both RM and LCE exhibit highly competitive retrieval per-formance. Particularly, the LCE method is among the most effective PRF-based methods for large-scale web collections [23, 15]. Lease [17] has recently affirmed its effectiveness at the TREC Relevance Feedback track.

To ensure competitive baseline performance, the free pa-rameters in the PRF-based methods  X  such as the number of documents used for pseudo-relevance feedback, the query weight in the RM method and the  X  parameters in the LCE method (see Eq. 5)  X  are set using 3-fold cross-validation, analogously to the PQE method. To maintain reasonable ef-ficiency, especially for the large web collection GOV2, we limit the number of expansion terms to 10 for all the PRF-based methods presented in Table 5.

Overall, the four baselines described above differ in their choice of concept types. In contrast to the PQE method, they do not parameterize the concept weights. Table 4 summa-rizes the choice of concepts and weight parameterization by these methods (as well as two additional methods that will be discussed in Sec. 5.3). For instance, we can see from Ta-ble 4 that LCE and PQE share the same concept types, but differ in the parameterization of the concept weights.
Table 5 compares the retrieval effectiveness of the four baselines to the retrieval effectiveness of PQE , both for h title i and h desc i queries. Effectiveness is measured using both an early precision metric (prec@20), and the mean average precision of the entire ranked list of 1,000 documents (MAP).
First, it is clear from Table 5 that methods that use multi-ple concept types ( SD , LCE , PQE ) are superior to the methods that use terms alone ( QL , RM ). This result holds for all the collections, for both prec@20 and MAP.

Second, the LCE method, which uses both multiple ex-plicit query concept types and latent expansion concepts, outperforms the SD method, which uses the query concepts alone. This result is consistent with previous work [24], and demonstrates the positive effect of query expansion, even when multiple query concept types are used.

Finally, we compare the proposed method, PQE , to the four baselines. In all 12 comparisons (three collections, two met-rics and two query types), our method outperforms all the baselines, in most cases to a statistically significant degree. There are two key elements that contribute to the success of the PQE retrieval method.

First, similarly to LCE , PQE combines multiple explicit con-cept types with expansion concepts. This combination leads to a very substantial improvement over the standard bag-of-words methods. For instance, for h desc i queries on the GOV2 collection, PQE achieves 24% and 17% improvement in MAP over QL and RM , respectively. 21 . 1 9 qs r (+2.7/+0.50) 56 . 6 6 q r 34 . 8 4 q 48 . 4 2 27 . 1 1 q 22 . 1 7 qs (+11.8/+4.3) 54 . 7 0 qs rl 31 . 6 8 respectively.
 Second, unlike all the standard baselines in Table 5, the PQE method applies parameterized concept weighting to both explicit and latent concepts. The parameterized concept weighting leads to significant improvements over all the non-parameterized baselines, including those that use multiple concept types. In most cases, these improvements are sta-tistically significant for both metrics, and they are consistent across different collections.
 On average, for h desc i queries, there is a 12 . 2% gain in MAP over SD , and 4 . 1% gain over LCE . Note that when com-paring PQE and LCE methods, the concept weight parameter-ization is the only factor that contributes to the effectiveness gain, since these two methods share the same concept types (as demonstrated in Table 4).
In Table 5 we have shown that the PQE method signifi-cantly improves the overall performance compared to two state-of-the-art PRF-based methods ( RM and LCE ). In this section, we analyze the robustness of PQE , compared to these two methods. Following previous work [24], we define the robustness of the method as the number of queries improved or hurt (and by how much  X  in terms of MAP) as the result of the application of the method. A highly robust expansion technique will significantly improve many queries and only minimally hurt a few.

Fig. 3 provides an analysis of the robustness of RM , LCE and PQE for the h desc i queries 6 . The histograms in Fig. 3 show, for various ranges of relative decreases/increases in MAP, the number of queries that were hurt/improved with respect to the QL baseline.

Fig. 3 unequivocally demonstrates that PQE is more ro-bust compared to the other two methods. For instance, for the GOV2 collection, PQE improves the performance of 75% of the queries w.r.t. QL , compared to 64% and 68% of the queries improved by RM and LCE respectively. Similar im-provements are observed for the other two collections.
In addition, the PQE method is much less likely to signif-icantly hurt the performance, compared to the other two 6 T he robustness of these methods for the h title i queries is similar, and is omitted due to space constraints. methods. For instance, for the ROBUST04 collection, PQE decreases performance by more than 25% for only 24 (out of 250) queries, compared to 33 and 34 queries with such a decrease for the RM and LCE methods, respectively.
In Table 5 and Fig. 3, we have evaluated the retrieval methods using binary relevance judgments  X  documents are either assumed relevant or non-relevant. However, graded relevance judgments (i.e., categorical judgments with more than two degrees of relevance) are becoming more widely used, especially for evaluating web search tasks. Accord-ingly, in addition to the binary metrics, we use the nor-malized discounted cumulative gain (nDCG) metric, which takes into account multiple relevance grades, to evaluate the retrieval effectiveness for the GOV2 collection, which has graded relevance judgments available.

Table 6 shows the nDCG at the top 20 results, as well as the nDCG of the entire ranked list for the PQE method along with the two most effective baselines from Table 5 ( SD and LCE ). The results in Table 6 are in agreement with the results for the binary metrics in Table 5.

PQE is the most effective among the three methods, in most cases to a statistically significant degree. It is interesting to note that while it is often the case that query expansion methods do not have a significant positive effect on early precision [21, 24], PQE shows significant improvements over the SD method for prec@20 and nDCG@20 metrics for both h title i and h desc i queries. 5.2.4 h title i and h desc i Queries
While most of the previous concept weighting techniques specifically target verbose natural language queries [5, 12, 14, 18], the parameterized concept weighting described in Sec. 2 is more general, and can be applied to any type of query. Table 5 and Table 6 show that PQE outperforms the other methods for both h title i and h desc i queries.
Intuitively, however, we expect that the parameterized concept weighting will be more beneficial for longer, more complex queries, which may contain more concepts of vary-ing importance. Overall, the results in Table 5 and Table 6 h d esc i GOV2 SD 41.15 54.45 LCE[10] 41.66 56.41 PQE[10] 43 . 3 0 s l (+5.2/+3.9) 57 . 6 0 s l (+5.8/+2.1) improvement over SD and LCE methods, respectively. confirm this intuition. The gains attained by the parame-terized concept weighting for h desc i queries are, on average, higher than those attained for h title i queries. For instance, the average gain in MAP of the PQE method over the LCE method is 1 . 7% for the h title i queries, compared to 4 . 1% for the h desc i queries.

It is important to note, however, that the effectiveness gains achieved by the PQE method are consistent, and in many cases statistically significant, for both h title i and h desc i queries. This showcases the applicability of the parameter-ized concept weighting employed by the PQE method for a variety of search scenarios, apart from verbose natural lan-guage queries.
In the remainder of this section, we provide a deeper anal-ysis of the various aspects of the PQE method. In Sec. 5.3.1 we compare the performance of PQE to the performance of some previously proposed parameterized retrieval methods. In Sec. 5.3.2 we compare the effectiveness of PQE with a small number of expansion concepts to the effectiveness of non-parameterized PRF-based methods that use an increasingly large number of expansion concepts. Finally, in Sec. 5.3.3 we compare PQE with two recently published methods for query expansion that employ concept weighting.
In Sec. 5.2 we have compared the performance of the PQE method to the performance of four standard retrieval meth-ods that do not perform any parameterized concept weight-ing. In this section, we compare PQE with two additional retrieval methods, both of which employ parameterized con-cept weighting. These parameterized retrieval methods dif-fer in their choice of the concept types.

The first method is WSD , proposed by Bendersky et al. [6]. This method is a parameterized version of the standard SD method. The second method, WRM is the parameterized version of the RM method. It is conceptually similar to the PQE method, however it only uses unigram concepts ( QT and ET -concepts).

Table 4 summarizes the concepts and the parameteriza-tion of the WSD and WRM methods. We compare the effec-tiveness of these two methods to the effectiveness of the PQE method in Table 7.

First, we note that the three parameterized retrieval meth-ods WSD , WRM , and PQE outperform their non-parameterized counterparts ( SD , RM and LCE , respectively). In most cases these performance gains are statistically significant. For in-stance, WSD attains a 5 . 5% gain over SD , and WRM attains a 10 . 3% gain over RM for the h desc i queries. Moreover, analo-gously to the PQE method, these gains, while consistent for all queries, are, on average, larger for the h desc i queries.
Second, we note that PQE is, overall, the best-performing parameterized retrieval method. The only exception is the WT10g collection, where WSD and PQE are statistically in-distinguishable. On average, PQE attains 4 . 7% gain over WSD and 3 . 5% gain over WRM for the h desc i queries (and much higher gains for the ROBUST04 and GOV2 collec-tions, specifically).
In Table 5, we have limited ourselves to the efficient set-ting of using solely the top ten ET -concepts for query ex-pansion. In this section, we study the effect of increasing Table 7: Retrieval effectiveness (MAP) of the pa-r ameterized retrieval methods. Statistically signifi-cant differences are marked using the second letter in the title of the retrieval method under compari-son. Best result per column is marked by boldface. the number of expansion concepts. Particularly, we are in-terested in addressing the question of whether the addition of expansion concepts in the non-parameterized PRF-based methods (e.g., LCE ) can compensate for their lack of accurate concept weighting.

Fig. 4 shows the effect of increasing the number of ex-pansion concepts used by the LCE retrieval method on its retrieval effectiveness for the h desc i queries 7 for all test col-lections. Fig. 4 demonstrates that adding expansion terms improves the effectiveness of LCE in some cases (but worsens it in the case of WT10g). However, adding more ET -concepts to LCE is still inferior to the fixed setting of using the top ten ET -concepts in the PQE method, while significantly increasing the query latency.

Fig. 4 demonstrates the importance of parameterized con-cept weighting for creating both effective and efficient re-trieval methods that can scale to large web collections. Com-pared to the non-parameterized retrieval methods, PQE pro-vides a more accurate estimate of concept importance. More-over, since the features that are used to determine the im-portance of a concept can be pre-computed and cached (see Sec. 3.2), PQE effectively and efficiently filters out the less 7 T he results for the h title i queries are similar, and are omit-ted due to space constraints.
 Table 8: Additional comparisons with: (a) Cao et al. [ 8]; (b) Lv and Zhai [21]. Best result per comparison is marked by boldface. important expansion concepts and minimizes the query ex-ecution time.
In this section, we compare the performance of the PQE re-trieval method to the performance of two recently proposed query expansion methods that employ concept weighting and proximity information. The first method was proposed by Cao et al. [8], and uses binary classification to weight ex-pansion terms. The second method was proposed by Lv and Zhai [21], and leverages term proximities for expansion term weighting. While less general than the approach proposed here, these two methods also focus on concept weighting, and hence we briefly compare their performance to PQE .
For comparison, we use the MAP results reported in the papers by Cao et al. [8] and Lv and Zhai [21], for a sub-set of topics overlapping with our evaluation. The reported results are for the h title i queries only, since these queries are also used in the papers under consideration. Table 8 re-ports the comparison between the PQE method and these two methods. While we cannot draw statistical significance con-clusions, since we have no information on individual query performance, we can see from Table 8 that PQE is the best performing method in both comparisons.

In all the cases in Table 8 similar query and document pro-cessing was applied (Porter stemming, INQUERY stopwords removal, setting of smoothing parameters and number of ex-pansion terms), and similar baselines were reported. Hence, we can confidently attribute the performance gains to the effectiveness of our method, even when compared to other state-of-the-art query expansion methods that use concept weighting and proximity information.
I n this paper, we introduced a novel framework for query expansion with parameterized concept weighting. Parame-terized query expansion generalizes and unifies several of the current state-of-the-art concept weighting and query expan-sion approaches.

Unlike many common retrieval models that use unsuper-vised concept weighting based on a single global statistic, pa-rameterized query expansion leverages a number of publicly available sources such as Wikipedia and a large collection of web n-grams, to achieve a more accurate concept importance weighting. This importance weighting is applied to both ex-plicit query concepts (terms, exact phrases and proximity matches) as well as latent concepts, which are associated with the query using pseudo-relevance feedback.

An empirical evaluation on newswire and web TREC cor-pora unequivocally demonstrates the state-of-the-art effec-tiveness of the parameterized query expansion. Our method consistently outperforms a number of strong baseline meth-ods, which use term dependencies and pseudo-relevance feed-back with a larger number of latent concepts. It also achieves significant gains over methods that use parameterized con-cept weighting, but do not perform query expansion. The highest effectiveness gains are demonstrated for verbose nat-ural language queries, but parameterized query expansion is beneficial for the keyword queries as well.

Overall, our findings demonstrate that the parameterized query expansion is an effective and flexible framework that can seamlessly incorporate multiple concept types. Accord-ingly, in future work, we intend to introduce additional types of concepts into the parameterized query expansion frame-work, including multiple-term expansion concepts, named entities, and non-adjacent query term pairs.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or rec-ommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor.
