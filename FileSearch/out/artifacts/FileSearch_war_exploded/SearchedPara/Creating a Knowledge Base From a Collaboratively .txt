 The last decade has seen statistical techniques for Natural Language Processing (NLP) gaining the status of standard approaches to most NLP tasks. While advances towards robust statistical inference methods (cf. e.g. Domingos et al. (2006) and Pun-yakanok et al. (2006)) will certainly improve the computational modelling of natural language, we believe that crucial advances will also come from re-discovering the use of symbolic knowledge, i.e. the deployment of large scale knowledge bases.

Arguments for the necessity of symbolically en-coded knowledge for AI and NLP date back at least to McCarthy (1959). Symbolic approaches using knowledge bases, however, are expensive and time-consuming to maintain. They also have a limited and arbitrary coverage. In our work we try to over-come such problems by relying on a wide coverage on-line encyclopedia developed by a large amount of users, namely Wikipedia. That is, we are interested in whether and how Wikipedia can be integrated into NLP applications as a knowledge base. The motiva-tion comes from the necessity to overcome the brit-tleness and knowledge acquisition bottlenecks that NLP applications suffer. Ponzetto &amp; Strube (2006) and Strube &amp; Ponzetto (2006) aimed at showing that  X  X he encyclopedia that anyone can edit X  can be indeed used as a semantic resource for research in NLP. In particular, we as-sumed its category tree to represent a semantic net-work modelling relations between concepts, and we computed measures of semantic relatedness from it. We did not show only that Wikipedia-based mea-sures of semantic relatedness are competitive with the ones computed from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system dealing with corefer-ence resolution is in fact beneficial. 2.1 WikiRelate! Computing Semantic Semantic relatedness measures have been proven to be useful in many NLP applications such as word sense disambiguation (Kohomban &amp; Lee, 2005; Pat-wardhan et al., 2005), information retrieval (Finkel-stein et al., 2002), information extraction pattern induction (Stevenson &amp; Greenwood, 2005), inter-pretation of noun compounds (Kim &amp; Baldwin, 2005), paraphrase detection (Mihalcea et al., 2006) and spelling correction (Budanitsky &amp; Hirst, 2006). Approaches to measuring semantic relatedness that use lexical resources transform that resource into a network or graph and compute relatedness us-traverse MeSH, a term hierarchy for indexing arti-cles in Medline, and compute semantic relatedness as the edge distance between terms in the hierar-chy. Jarmasz &amp; Szpakowicz (2003) use the same approach with Roget X  X  Thesaurus while Hirst &amp; St-Onge (1998) apply a similar strategy to WordNet.
The novel idea presented in Strube &amp; Ponzetto (2006) was to induce a semantic network from the Wikipedia categorization graph to compute mea-sures of semantic relatedness. Wikipedia, a multi-lingual Web-based free-content encyclopedia, al-lows for structured access by means of categories : the encyclopedia articles can be assigned one or more categories, which are further categorized to provide a so-called  X  X ategory tree X . Though not de-signed as a strict hierarchy or tree, the categories form a graph which can be used as a taxonomy to compute semantic relatedness. We showed (1) how to retrieve Wikipedia articles from textual queries and resolve ambiguous queries based on the arti-cles X  link structure; (2) compute semantic related-ness as a function of the articles found and the paths between them along the categorization graph (Fig-ure 1). We evaluated the Wikipedia-based measures against the ones computed from WordNet on bench-marking datasets from the literature (e.g. Miller and Charles X  (1991) list of 30 noun pairs) and found Wikipedia to be competitive with WordNet. 2.2 Semantic Knowledge Sources for Evaluating measures of semantic relatedness on word pair datasets poses non-trivial problems, i.e. all available datasets are small in size, and it is not always clear which linguistic notion (i.e. similar-ity vs. relatedness) underlies them. Accordingly, in Ponzetto &amp; Strube (2006) we used a machine learn-ing based coreference resolution system to provide an extrinsic evaluation of the utility of WordNet and Wikipedia relatedness measures for NLP applica-tions. We started with the machine learning based baseline system from Soon et al. (2001), and an-alyzed the performance variations given by includ-ing the relatedness measures in the feature set (Fig-ure 2). The results showed that coreference resolu-tion benefits from information mined from seman-tic knowledge sources and also, that using features induced from Wikipedia gives a performance only slightly worse than when using WordNet. Our results so far suggest that Wikipedia can be con-sidered a semantic resource in its own right. Un-fortunately, the Wikipedia categorization still suf-fers from some limitations: it cannot be considered an ontology, as the relations between categories are not semantically-typed, i.e. the links between cate-gories do not have an explicit semantics such as is-a , part-of , etc. Work in the near future will accordingly concentrate on automatically inducing the semantics of the relations between Wikipedia categories. This aims at transforming the unlabeled graph in Figure 3(a) into the semantic network in Figure 3(b), where the links between categories are augmented with a clearly defined semantics.

The availability of explicit semantic relations would allow to compute semantic similarity rather than semantic relatedness (Budanitsky &amp; Hirst, 2006), which is more suitable for coreference res-olution. That is, we assume that the availability of hyponymic/hyperonymic relations will allow us to compute lexical semantic measures which will further increase the performance of our coreference resolution system, as well as further bringing for-ward Wikipedia as a direct competitor of manually-designed resources such as WordNet.

In order to make the task feasible, we are currently concentrating on inducing is-a vs. not-is-a semantic relations. This simplifies the task, but still allows us to compute measures of semantic similarity. As we made limited use of the large amount of text in Wikipedia, we are now trying to integrate text and categorization. This includes extracting semantic re-lations expressed in the encyclopedic definitions by means of Hearst patterns (Hearst, 1992), detection of semantic variations (Morin &amp; Jacquemin, 1999) between category labels, as well as using the cat-egorized pages as bag-of-words to compute scores of idf-based semantic overlap (Monz &amp; de Rijke, 2001) between categories. Further work will then concentrate on making this information available to our coreference resolution system, e.g. via semantic similarity computation.

Finally, since Wikipedia is available in many lan-guages, we believe it is worth performing experi-ments in a multilingual setting. Accordingly, we are lect word relatedness judgements from native speak-ers of German, French and Italian, in order to trans-late the semantic relatedness dataset from Finkel-stein et al. (2002) and test our methodology with languages other than English. In this paper we presented our previous efforts on us-ing Wikipedia as a semantic knowledge source. We aim in the future to induce an ontology from its col-laboratively generated categorization graph. We be-lieve that our work opens up exciting new challenges for the AI and NLP research community, e.g. how to handle the noise included in such knowledge bases and how to fully structure the information given in the form of only partially structured text and rela-tions between knowledge base entries.
 Acknowledgements: This work has been funded by the Klaus Tschira Foundation, Heidelberg, Ger-many. The author has been supported by a KTF grant (09.003.2004).

