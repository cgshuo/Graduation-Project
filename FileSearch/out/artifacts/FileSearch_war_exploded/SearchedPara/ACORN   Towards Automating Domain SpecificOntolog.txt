 The task of automatically acquiring knowledge from data is a critical research topic and a key component in many business. The importance of this endeav-our has become more apparent recently w ith the advent of the Internet and the increasing amount of large and complex d ata being transacted online. However, without the provision of any external knowledge, it is difficult to automatically distinguish the underlying conceptual differences of natural text in data. More-over, given a particular domain, ther e are various ways to represent common knowledge, which can create ambiguity between parties exchanging information.
To overcome the above difficulties (and fo r many other reasons), ontologies can be constructed, which arrange domain specific concepts and their relationships in a structured format. These ontologies then can serve as common repositories of knowledge for users and agents, which can be inquired and reasoned with to clarify the uncertain analysis of data and help valuable information to be extracted.
However, currently the construction of o ntologies is predominantly performed by human domain experts. This manual p rocess obviously requires much time and effort and for developing larger ontologies (i.e. SNOMED [4]), it is not un-common to involve many contributors over a number of years. Although several research works have attempted at automating its construction, many are limited to just extracting concepts and defining relationships between them remains as a difficult task. In this paper, we would like to propose a new tool called ACORN 1 , which includes the following novel features :  X  Extracting domain specific concepts : Given an input corpus of documents,  X  Identifying concept-to-concept relat ionships : We explore two techniques to
This paper is organized as follows. In the next section, we explore some of other relevant work which automates the ontology construction task. In section 3, we describe our tool and its components in details. We provide the results of our experiments in section 4 to demonstrate the benefits of using ACORN and finally we propose extensions to our tool for future work in section 5. We note here that the automatic construction of a complete and comprehensive ontology is an extremely difficult task. Ontologies often capture relationships, constraints and rules between concep ts are specific to domain, which may not be directly attainable from input corpus. Commonly, many tools present a semi-automatic environment which provides a  X  X keleton X  ontology (i.e concepts and some relationships defined) upon which human experts can include further de-tails regarding its domain. With this in mind, here we describe some of the related work.

In [7], authors introduce  X  X ntoGen X  which uses singular value decomposition, k -means clustering algorithm and a SVM classifier to extract domain specific concepts from a document corpus. Once concepts are found, a human expert must identify relationships between these concepts through OntoGen X  X  GUI. Although, the concepts extracted from this system are reasonable and its outlier detection feature can fur ther enhance the quality of concepts, OntoGen still requires significant effort and time by human. In [2], COBWEB [6] algorithm was proposed as a tool to automatically generate ontologies. COBWEB algorithm is well known for extracting hidden concepts from data using its category utility function. However, results in [2] do not sufficiently support the direct adoption of COBWEB as an ontology construction tool and it was even found in [11] that other clustering techniqu es can outperform COBWEB.
In [14], a fuzzy formal concept analy sis was used in order to extract key concepts from documents, which are subs equently clustered to form a hierarchi-cal structure. However, from experim ents, we have found that key concepts of higher quality can be extracted using simpler and common TFIDF weighting than using fuzzy formal concept analysis. Moreover, the technique is currently limited to work with only document citation information. On the other hand in [3], the author uses rough set theory to determine the relationships between concepts and implements a link-based clustering for further refining the con-cept groups. The technique, however, assumes that there is already a high level document clusters available and a number of initial thresholds, which need to be specified a priori, can vary the outcome. Finally in [13], OntoEdit is pre-sented as a framework for managing ontology construction and maintenance. In relation to building an ontology,  X  X ntoEdit X  uses its own lexical database to first extract concepts then uses asso ciation rule algorithm to build rela-tionships between them. The problem her e is that the lexical database itself may not contain the domain specific keywords, while the association rule al-gorithm works very similarly to simpler technique such as term co-occurrence measure [3].

From all the available methods, we observe that the ontology construction task involves two critical components -ex tracting concepts and identifying re-lationships between them. While many related works have included the former component, the latter is acknowledged as difficult and largely left to the experts. In ACORN we propose two different methods, which aim to identify meaningful relationships and we describe its process in detail in the following section. We illustrate the overall process of ACORN in figure 1. Each of these components will now be described in detail.
 Document Preparation. This stage processes the input corpus in such a way that appropriate data can be prepared for ontology construction. This requires first performing  X  X ocument cleansing X  by removing all stop-words and applying stemming algorithm (i.e. Porter Stemmer) to prune unwanted words. Each term is then weighted with TFIDF score as described in [16]. From each document, we then selected the top k number of keywords 2 having the highest TFIDF scores as its representatives. Effectively, the coll ection of all the representative keywords from the documents serves as the keywords found from the input domain.
Additionally, we also calculated the co-occurrence frequency between every pair of keywords in the corpus as described in [12]. This is achieved by first detecting all the sentences in the cor pus, which is considered to be a set of words separated by a stop mark ( X . X ,  X ? X  or  X ! X ) and counting the number of times a pair of keywords occurs together in all sentences. The aim of finding the co-occurrence frequency is to determin e the keywords, which are semantically similar (since they appear together frequently in sentences). We use this value in the later stages of ACORN to extract concepts and also to build relationships between them.
 Concept Extraction. Once all the keywords are found, we would like to group together those which are conceptually similar. These groups of keywords will then form a particular concept present in the current domain. We define such similarity as below.
 Definition 1 KeywordSim(A,B). For a pair of words A and B ,lettheco-occurrence frequency value be given by COOF ( A, B ) and JCn ( A, B ) be the JCn [8] similarity measure according to the WordNet lexical database [5]. The overall conceptual similarity between a pair of words then can be given as below. In the above definition, the  X  is the normalization factor which ensures that the sum of COOF ( A, B )and JCn ( A, B ) gives a value between 0 and 1. The value of  X  was set to 0.5 in ACORN. The main emphasis with the equation 1 is that we are able to leverage the positive features of both measures. While JCn measure provides the general conceptual similarity between two words, the COOF can identify the similarity, specific to the given domain. Moreover, the WordNet may not contain all the terms extracted, whereas COOF will not be sufficient to detect conceptually sim ilar, words which do not occur in the same sentences.

Given these keywords and their pairwise similarity values, we can present them in a graph G =( V, E ), called  X  X oncep t lattice X  where V = { v 1 ,v 2 , .., v n } is a set of vertices containing each keyword, and E = { e 1 ,e 2 , .., e m } is a set of edges between vertices. Eac h edge connecting vertices v i and v j is weighted with the similarity and is expressed as w ij = KeywordSim ( v i ,v j ). The illustration is given in figure 2.

Once the keywords and the pairwise similarity values are represented in the lattice , we can merge closely related k eywords together to form concepts. We achieve this by setting a similarity threshold t where 0  X  t  X  1 and combine the keywords (where the w ij &gt;t ) and form a cluster of concepts (concept node). We show some of the concepts generated from the  X  X pace X  newsgroup, when t =0 . 3 in figure 3.

In section 4, we show the concepts ex tracted from ACORN and evaluate their quality. The next stage involves developing relationships between the key concepts, and as aforementioned, we p rovide two techniques -cluster mapping and generality ordering -to build a hierarchical domain ontology. 3.1 Ontology Construction Through Cluster Mapping Although a number of research works [13,7,2] have applied some type of cluster-ing in their ontology construction process, its use has been limited to keyword extraction and grouping them together for forming concepts. However, clus-ters generated from the input corpus, especially hierarchical clusters (or dendro-grams) by agglomerative clustering algorithms, can provide valuable information for establishing relation ships between concepts.

Although it could be argued that the resultant dendrogram can be directly used as an ontology, such work has had a limited success [10] and it does not appropriately capture the conceptual taxonomy information contained in the domain. Instead, in this section, we introduce a  X  X luster mapping X  technique, in which the document cluster hierarchy generated by the proposed algorithm aids concepts to be mapped to its approximate position in the ontology. The mapping is achieved by comparing the keywords in each concept group discovered in the previous step to the labels representin g clusters. The assumption here is that the document clusters will place general topics towards the top of the hierarchy, while more specific topics are positioned near the bottom. The algorithm is shown in algorithm 1.

The clustering algorithm we implement is a single-linkage algorithm [9], which is a popular agglomerative clustering method and has been tested in [11] to be the best performing algorithm for producing a meaningful dendrogram. In this Algorithm 1. Cluster Mapping algorithm, each document is initially assigned to a single cluster (line 2) and a pair of most similar clusters are merged (lines 3 to 5). This process is continued until only one cluster remains. The similarity between clusters is calculated by the cosine angle between closest documen ts from each cluster (line 4) since each document is represented by k number of TFIDF-weighted words (as mentioned in section 3 and therefore can be project ed onto a vector space. Additionally, after each cluster merge, we recalculate the TFIDF weights of terms in the new cluster and reselect the top k terms as cluster labels. We also insert each merged cluster into a tree data structure T for mapping concepts (line 9).

Once the hierarchy is built, we would like to map each concept k i in K to a node n j in T (line 11 to 13). We achieve this by traversing through the tree T and calculating the similarity between co ncepts and nodes. This similarity can be defined as below.
 Definition 2 ConceptNodeSim(k,n). Let ConceptN odeSim ( k, n ) be a func-tion, which calculates the similarity between a concept k andanode n . It is given by the average similarity between every pair of terms in the concept and terms inthetreenode(labelsofclusternode).
 where | k | and | n | indicate the number of terms contained in the concept and labels respectively If more than one concept is mapped to a node n ,wecreateananotherchild node to the parent node of n (effectively creating a sibling node) to assign extra concepts. On the other hand, all cluster nodes which do not have concepts assigned will be pruned away. The resultant tree forms an ontology, which will then only show cluster nodes with the concepts assigned. 3.2 Ontology Construction Through Generality Ordering The second technique we introduce in ACORN for identifying the hierarchical re-lationships between concep ts is through  X  X enerality ordering X . In this technique, we quantitatively calculate the  X  X ener ality X  of a concept, which effectively de-termines how general (or conversely, sp ecific) the concept is within the given domain. Ordering the concepts by this generality score will provide information on their positions in the ontology. We define this generality as below. Definition 3 ConceptGenerality. Given a concept k = { w 1 ,w 2 , .., w p } ,which is a set of semantically related keywords, ConceptGenerality ( k ) is a function, which averages the inverse document frequency (IDF) weights of each keyword w i in k , given as follows.

The IDF weighting scheme has been widely used for determining the  X  X eneral-ity/specificity X  of a term within a corpus [15] and in equation 3, we calculate the average IDF value from all the keywords in the concept. The overall algorithm for generality ordering techni que is described in algorithm 2.

After calculating and ordering concep ts by the generality score (line 1 to 4), we also calculate concept-to-concept similarity (line 5-9), which is defined identically as the equation 2, except w e are comparing keywords between two concepts. Since the linearly ordered co ncepts does not reflect the hierarchical structure of the final ontology, the con cept-to-concept similarity can serve as additional information for specifying the exact location of a concept, which is achieved in the third step of the algor ithm (line 10 to 16). Here, a concept k i is inserted into the tree T under the node which contains the concept which is more general than k i and also shares the highest similarity value (line 14). This ensures that each concept belongs to the most similar yet more general parent concept. The steps 2 and 3 in algorithm 2 are illustrated in figures 4 and 5 respectively. 3.3 Ontology Generation The two techniques introduced above ret urn a tree structure containing various domain specific concept nodes, which ca n be directly viewed as an ontology and Algorithm 2. Ontology Construction through generality ordering we have described how ACORN is able to automate this process. As aforemen-tioned, however, constructing a full featu red ontology (i.e. including constraints and hidden rules between concepts) is a difficult task without any other external knowledge. Therefore, in ACORN, we repr esent its results as  X  X emplates X  and display them through a graphical user interface, from which the domain experts can make further enhancements. For our experiments, we have used 20 newsgroup dataset available from [17]. This dataset comprises of 20,000 messages from 20 newsgroups (2000 per each group). We have applied each newsgroup articles as input data to generate an ontology specific to the given newsgroup domain. The dataset was applied through the ACORN process described in figure 1. Due to the space constraints, we are not able to show all the experimental results.
 Concept extraction. In table 4, we present the concepts extracted by ACORN and compare them to those generated by OntoGen [7]. From the table, it is quite clear that the concepts from ACORN is more meaningful than those from OntoGen. For example, OntoGen makes inappropriate groups of keywords such as (IBM, speak, kind) and (era, total, field), which do not represent a specific concept for the given domain. On the other hand, ACORN is able to find many of domain specific concepts such as (I slam, Muslims, Fatwa) where Fatwa is an Arabic term used in Islamic religion and (Lankford, pinch, hitter), which describes a player who was a  X  X inch hitter X  in the baseball league.
 Discovery of concept-to-concept relationships : Figures 6 and 7 display the parts of domain specific ontologies generated by both cluster mapping and generality ordering technique. 2 $ 
As it can be observed, all four ontologies are able to establish meaningful relationships between majority of concep ts. For instance, the generality order-ing technique was able to join  X  X tatement X  as a parent and  X  X heism X ,  X  X uran X  and  X  X ible X  as its children. Such relationship particularly referred to the religious statements being made within the newsgroup documents. Furthermore, cluster mapping also correctly connected  X  X layers  X  as a parent and  X  X itchers X ,  X  X atchers X  and  X  X lack X  as its children, indicating that the documents in the  X  X aseball X  news-group categorizes players into their fielding positions (pitchers and catchers), but also by their races (black and hispanic). The key feature to note here is that we are creating ontologies, which are specifically relevant to the domain suggested by the input corpus.

We can also notice that the two techniques work slightly differently. First, we found that the cluster mapping technique tends to produce more shallow ontologies than the generality orderin g technique. This is due to some cluster node labels not necessarily matching closely with some of the concepts while multiple concepts were assigned to one n ode. On the other hand, the generality score ordering ensures that all concepts have different scores and determining their position of level in the hierarchy is calculated by similarity between the nodes. This allows less number of concepts to be assigned to the same node, hence creating a taller tree.
 Both techniques also showed some lim itations when conn ecting concepts. Firstly, some concepts were assigned to the wrong places in the hierarchy. For cluster mapping technique, terms like  X  X exts X  in figure 6(a) does not seem to belong near the top. The term  X  X theist X  in the figure 6(b) for the generality ordering technique can be also repositioned. Lastly, concepts such as  X  X et,find X  (figure 6(a)) and  X  X tarting X  (figure 7(b)) are ambiguous and seem redundant to be included in the ontology.

Overall, the generality ord ering technique construct ed slightly better ontolo-gies than the cluster mapping technique. We have observed that the cluster mapping technique is heavily dependent on the quality of document clusters for generating valid ontologies, which did not occur for all experiments.
Nevertheless, the experimental results above show that through two novel techniques, ACORN is able to extract domain specific ontologies, which can be further enhanced by the domain experts but at the same time provides a shortcut in constructing ontologies. The experimental results highlighted the need for high quality document clus-ters when using cluster mapping technique. We plan to explore a wider range of document clustering algorithms in the future. Moreover, applying alternate clustering algorithms like COALA [1] may provide a different technique for map-ping concepts as it generates multiple clusterings from the same dataset. Finally, we aim to include filtering techniques in ACORN in order to prune ambiguous and redundant concepts from ontologies using techniques such as singular value decomposition. In this paper, we have identified the importance of automating the ontology con-struction process and introduced a new tool called ACORN, which implements techniques for extracting concepts and building relationships between them. Through experimental results, we fo und that ACORN discovers domain specific concepts of high quality. Moreover, it was shown that both  X  X luster mapping X  and  X  X enerality ordering X  tec hniques are useful in defining relationships between con-cepts. Through the evaluation of our results, we have also identified the limitations of our methods and highlighted the areas to improve for future work.
