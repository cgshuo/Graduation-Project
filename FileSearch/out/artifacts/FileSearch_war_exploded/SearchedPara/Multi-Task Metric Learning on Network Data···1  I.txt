 jointly and simultaneously over multiple, different but related tasks. Compared to single-task learning (STL), which learns a model for each task independently using only task specific data, MTL leverages all available data and shares knowl-edge among tasks, thereby resulting in better model generalization and predic-tion performance. The underlying principle of MTL is that highly correlated tasks can benefit from each other via joint training, but additional care should be taken to respect the distinct nature of each task, i.e., it is usually inappro-priate to pool all available data and learn a single model for all tasks. for tasks on i.i.d. data. Standard examples include phoneme recognition [ 14 ]and image recognition [ 19 ]. Explicitly correlated data, often represented in the form of a network, is widely available, such as social network, citation network and influence network. It provides a rich source of new application contexts to MTL. Due to the diversity and variation in networks (e.g., multi-relational links or multi-category entities/nodes), various tasks can be performed and often a rich correlation exists between them. In the following, we give two common scenarios where there is abundant correlation between tasks and it is beneficial to apply MTL to exploit it. (These scenarios are also the settings for the experiments using real-world data that we present in Section 4).
 Scenario 1: Article Citation Prediction The citation prediction problem has been studied extensively [ 1 , 8  X  10 , 18 ]. People either build a predictive model for a unified network [ 10 ] (i.e., a citation network that contains papers across all subject areas) or build predictive models for each area independently [ 16 ]. Since article content and citation pattern varies across different areas, the former methodology ignores the difference between areas. However, some areas, while labeled as different are still related, in the sense of both content and citation pattern. Thus the latter methodology fails to exploit the correlation among subject areas. For example, computer science and elec-trical engineering articles may be classified or tagged as different areas, but in many cases they may still have much in common, or at least have significant sim-ilarity or overlap. In this case, to build predictive models for citations, a learning algorithm that is capable of utilizing these overlaps and explicit commonalities has advantages over traditional methods.
 Scenario 2: Social Circle Prediction Members of online social networks tend to categorize their links to followers/ followees. For example, many social networking platforms enable coarse-scale categorizations such as  X  X amily members, X  or  X  X riends and colleagues. X  Finer gradations allow for categorizations such as colleagues at particular companies or classmates at specific schools. A person X  X  social circle , studied in [ 11 ], is the ego network of a social network user (or  X  X go X ). This is the (star-shaped) sub-graph on  X  X go X  and all of ego X  X  followers comprising all the links joining ego to ego X  X  followers that belong to the same category. Given a friend or stranger, the goal of social circle prediction is to assign him/her to appropriate social circles. Because some social circles are related to each other (e.g., family mem-bers and childhood friends may share some common informative features such as geographical proximity), advantages may very well accrue if the relatedness of the entities is used for the various predictions, instead of building a predictive assignment model for each social circle independently.
 As these scenarios suggest, correlations commonly exist among tasks on net-work data and there should be significant advantages to developing methods that can leverage it. Different from i.i.d. data, network data not only has attributes (metadata) associated with each entity (node), but also rich structural informa-tion, mainly encoded in the links. Therefore, we employ structural learning to exploit both attributes and structure of networks. Specifically, we adopt structure preserving metric learning (SPML) [ 16 ], which was originally developed for single-task learning on networks. Our proposed method, MT-SPML, empowers SPML with the ability of doing MTL over multiple tasks and networks. SPML learns a single Mahalanobis distance metric on node attributes for a single task by using network structure as supervision, so that the learned distance function encodes the structure. Our method learns Mahalanobis distance metrics jointly over all tasks. More precisely, it learns a common metric for all tasks and one metric for each individual task. The common metric construction follows the methodology of shared intermediate parameterization [ 7 , 12 ], which allows sharing knowledge between tasks. While a task specific metric alone captures task specific informa-tion, when combined they work together to preserve the connectivity structure of the corresponding network. The learned metrics of SPML and MT-SPML are useful to many tasks on network, one of which is predicting future link pattern. We further show that as in the case of SPML, MT-SPML can be optimized with efficient online methods similar to OASIS [ 4 ] and PEGASOS [ 15 ] via stochastic gradient descent. Finally, MT-SPML is designed for general networks, thus can be applied extensively in a wide variety of problems. In experiments, in order to demonstrate the advantages of MTL on network data, we apply MT-SPML to two common real-world prediction problems (citation prediction and social circle pre-diction), and achieve promising results for link prediction. MTL is a popular research topic and has been studied extensively and systemat-ically for i.i.d. data. To name a few, Yu et al. [ 21 ] applied hierarchical Bayesian modeling for text categorization. Evgeniou et al. [ 7 ] extended Support Vector Machines (SVMs) to MTL via parameter sharing. Following [ 7 ], Parameswaran et al. [ 12 ] proposed the multi-task version of large margin nearest-neighbor met-ric learning [ 20 ]. However, there have been only few works focusing on MTL on relational data [ 5 , 17 , 22 ]. Of greatest relevance for our work is [ 13 ] wherein Qi et al. carefully designed a mechanism to sample across networks to predict missing links in a target network. Our paper differs from it in several ways. First, we aim at improving prediction performance of all networks, while [ 13 ] targets at a specific network and uses other networks as additional sources. Second, MT-SPML learns a joint embedding of both attribute features and net-work topological structure. Thus, the learned metrics can predict link patterns solely from node attributes while [ 13 ] tries to combine linearly attribute features with hand-constructed local structure information such as the number of shared neighbors between nodes. This suffers from the well-known  X  X old start X  problem when structure information is limited (e.g. new nodes). In this section, we first cover the technical details of SPML and then those of MT-SPML. 3.1 Notations and Preliminaries Given a network on n nodes we represent it as a pair G =( X , A ), where X  X  R d  X  n represents the node attributes and A  X  R n  X  n cency matrix, whose entry A ij indicates the linkage information between node i and node j . Recall that a Mahalanobis distance is parameterized by a positive semidefinite (PSD) matrix M  X  R d  X  d , where M 0. The corresponding distance function is defined as d M ( x i ,x j )=( x i  X  x j ) M ( x the existence of a linear transformation matrix L on the feature space such that M = L L . Given a metric M , to predict the structure pattern of X we adopt a simple k -nearest neighbor algorithm, which is denoted as is connected with its top-k nearest neighbors under the defined metric. Mathe-matically, we say M is structure preserving or that it preserves A ,if closely approximates A .
 Let G = { G 1 , G 2 ,..., G Q } denote a set of networks. Each individual network G has its own X q and A q .Weuse q to index the network so that A for element ( i, j )in A q . Similarly, x qi represents the feature of node i in X algorithms, we will use a superscript to index over iteration, e.g., M the k -th iteration of M under the relevant iterative process. 3.2 SPML The goal of SPML is to learn M from a network G =( X , A ), such that M preserves A . This problem has a semidefinite max margin learning formulation, subject to the following constraints: In Eq.(1) ||  X  || F denotes the Frobenius norm and it takes on the role as a regu-larizer on M with  X  representing the corresponding weight parameter. The key piece for achieving structure preserving is the set of linear constraints in Eq.(2). This essentially enforces that from node i , the distances to all disconnected nodes must be larger than the distance to the furthest connected node. Thus, when the constraints in Eq.(2) are all satisfied, C ( X , M ) will exactly reproduce A . Furthermore, to allow for violation (with penalty), the slack variable  X  is introduced.
 With the many constraints in Eq.(2), optimizing Eq.(1) becomes unfeasible when the network has even a few hundred nodes. But a rewriting of the problem as follows makes possible the use of stochastic subgradient descent (see Algorithm 1): where  X  M ( x i ,x j ,x l )= d M ( x i ,x l )  X  d M ( x i 0 } . Thus, inclusion of the triplet ( i, j, l ) means that there is a link between node i and node l , but not between i and j . The subgradient of Eq.(3) can be calculated as where S + is the set of triplets whose hinge losses are positive. At every itera-tion t of Algorithm 1, B triplets are randomly sampled and the corresponding stochastic subgradient is calculated with regard to the current metric M these triplets. Since Algorithm 1 is a variant of PEGASOS [ 15 ], its complexity does not depend on the training set size n , but on the feature dimensionality d . For the number of iterations T needed to reach convergence, as proved by [ 15 , 16 ] it depends on the parameter  X  and the optimization error, which measures how close the final objective value is to the global optimal objective value. Notice that after updating M , it is optional to project the current M to be positive semidef-inite (PSD). Experiments in [ 16 ] show that delaying this operation to the end of the algorithm works well in practice and reduces computational complexity. 3.3 MT-SPML In this section, we explain how MT-SPML extends SPML to the multi-task setting. The input is a set of networks G = { G 1 , G 2 ,..., G each network is G q =( X q , A q ). Our approach is a general method. It works in settings for which there either are or are not nodes overlapping between networks. Note that the nodes of all networks are assumed to have a common feature space. MT-SPML treats each network as a task. It follows the idea of shared intermediate parametrization [ 12 ] to enable knowledge transfer between tasks. The goal is to learn jointly over G a task specific metric M a common metric M 0 , through which knowledge transfers among tasks, so that the combined metric ( M 0 + M q ) respects the structure of G The distance between two nodes x qi ,x qj  X  G q is defined as d x qj ) ( M 0 + M q )( x qi  X  x qj ). And MT-SPML is formulated as the solution to the regularized learning problem subject to the following constraints: In order to solve this we rewrite it by incorporating the constraints where  X  q ( x qi ,x qj ,x ql )= d q ( x qi ,x ql )  X  d q ( x unknown variables than Eq.(3), with respect to each unknown, it is in the same form as Eq.(3). Therefore, Eq.(7) can be solved with the same stochastic sub-gradient descent method using partial subgradient. The partial subgradients of Eq.(7) with respect to M 0 and M q are and The optimization algorithm outlined in Algorithm 2 runs for T iterations. Within each iteration, it does two things: (1) Randomly samples B triplets for each task, then calculates the partial subgradient and updates the corresponding unknowns; (2) Calculates the partial subgradient of the common metric M using the Q  X  B triplets already sampled. Optionally, the metric matrices can be projected to be PSD. The analysis of Algorithm 1 still holds for Algorithm 2. Thus it scales with regard to feature dimensionality, optimization error and the parameters  X  q , but not the training set size.
 Algorithm 1 . Optimization of SPML In this section, we present experimental results on real-world data and we adopt link pattern prediction as the performance measurement. We apply MT-SPML to the two scenarios mentioned in Section 1: article citation prediction and social circle prediction. We show that in both cases, MT-SPML significantly improves performance and has various advantages. 1 4.1 Citation Prediction on Wikipedia The data is obtained from [ 16 ]. The articles of the following three areas were crawled from Wikipedia: search engine, graph theory and philosophy, each of which has 269, 223 and 303 articles respectively. The citations between articles within each area are also crawled. The number of citations within each area are 332, 917 and 921. The goal is, given an article, to predict the referencing of other articles within its area solely from its content. Therefore, at test time, no reference information from the test article is made available at all. The challenge of this problem is the fact that: (1) there is little node overlap between networks (i.e., an article belongs to only one area), thus the marginal distribution of node attributes P ( X q ) may vary dramatically from area to area, which poses difficulty for knowledge transfer; (2) the conditional probability of structure on attributes P ( A | X ) may also vary, because some words are informative and indicative for some areas, but not for others. Bag-of-words (i.e., word frequency) is used to capture article content and the dimensionality is 6695. The high dimensionality reduces the need to learn full matrices. Therefore, we choose to learn diagonal metric matrices. This further reduces computational complexity. We split the dataset 80%/20% as training and testing respectively, then fix the testing part and vary the size of the training set by sampling from the training part. We end up sampling 20% , 40% , 60% , 80% , and finally 100% of the training part. Model selection is carried out on the sampled training set via 5-fold cross-validation. At test time, the goal is to predict links between testing nodes from attributes. For every test example our algorithm ranks other articles for citation according to their distances. We build the receiver operator characteristic (ROC) curve for every test article, and use the average area under the curve (AUC) of the entire test set as performance measurement. We compare our results with two families of methods: SVM Methods. We apply SVM-based methods as part of our baselines. Since SVM-based methods do not model network structure, we need to construct fea-tures to encode this information. The training examples are constructed by tak-ing the pairwise difference of the attributes between two nodes. The training labels are binary, with 1 representing the existence of a link between two nodes and 0 the absence. For a given edge, we measure its distance/length using the output of the classification score, which represents the confidence of having a link. Although the classification score is inversely proportional to the notion of distance, a simple conversion can make the two variables proportional. Thus ROC and AUC can be calculated. The following specific methods are included: (1) ST-SVM : This is the normal single-task SVM. An SVM is trained for each network independently. It does not explore the correlation between tasks. (2) U-SVM : We train one SVM for all networks by pooling all data together. We use the capital letter  X  X  X  to denote the naive strategy of data pooling. This ignores the fact that training examples are from different tasks and treats it as single task learning. (3) MT-SVM : This is the multi-task SVM in [ 7 ]. It jointly learns a common decision boundary for all and a specific boundary for each task. At test time, the common and task specific decision boundary together form the final model for each task. This method exploits task correlations via intermediate parameter sharing, but does not use network structure at the model level.
 SPML methods : We apply three methods that are based on SPML. Com-pared to SVM-based methods, these methods explicitly model the network struc-ture information. Therefore, the feature used here is simply the node attributes and links become linear constraints. Given an edge, its distance is just the Maha-lanobis distance defined by learned metrics. The following methods are included: (1) ST-SPML : This is the single-task SPML [ 16 ]. A metric is learned for each network independently. It does not model task correlations. (2) U-SPML :  X  X  X  means data pooling. Training examples from all tasks are pooled together and the learning procedure is simply ST-SPML. This is a naive way of sharing knowledge between tasks, but it does not respect the differences between and distinctiveness of tasks. Thus we expect inferior results, particularly for less related tasks. (3) MT-SPML : This is our proposed method. Comparison with these other methods demonstrates the fact that MT-SPML exploits relatedness missed by these other methods while respecting the distinctive nature of the individual tasks.
 Finally, we also compare to the direct use of the original feature vector, i.e., using Euclidean distance. Other methods in link prediction literature, such as Adamic-Adar [ 10 ], typically heavily rely on local structure of test nodes, thus will suffer from  X  X old start X  prescribed in our experimental framework. The results are reported in Fig.1. The first thing we see is that SVM-based methods perform the worst when there are fewer training examples while the SPML family achieves good results in all settings, due to its ability to model structure information. We also find that among the SPML methods, MT-SPML consistently outperforms the others, which implies that MT-SPML is better at exploiting task correlations. The least amount of improvement from MT-SPML is found for philosophy articles. This is in line with intuition as papers related to search engines and graph (network) theory should have more in common with each other than either has with philosophy papers.
 We also show the convergence behavior of MT-SPML by plotting the value of | S triplets, for every task in each iteration. The fewer the number of violated constraints, the better the new metric respects the network structure. In experi-ments we set B , the time of random sampling, to be 10. In order to make a clearer demonstration, in Fig.2 we set B to be 100. As Fig.2 shows, the numbers of vio-lated constraints of all tasks drop quickly within the first 1000 iterations and stabilizes after 4000 iterations. This is in accordance with the previous analysis and experiments of convergence and efficiency of SPML [ 16 ]. 4.2 Social Circle Prediction on Google+ Every member of an online social network (e.g., Google+) is the ego of his/her (sub-)network and tends  X  or may be forced  X  to categorize his/her relationships (e.g. family members, college friends or childhood friends). For each type of rela-tionship, there is a sub-network associate with it, the social circle (SC), which is directly formalized in the online structures of Google+ (see [ 11 ]). In this section, given a Google+ social network user (the ego) and his/her friends, we want to pre-dict his/her SC, namely the type of relationships between ego and ego X  X  friends based on profile information. We are only interested in the ego network, mean-ing that we do not predict the links between friends. A similar topic is studied by McAuley et al. [ 11 ], where the setup is very different from ours. They assume the observation of an entire ego network, including node attributes and structure, but not any SC labels, and the goal is to assign SC labels to links in an unsupervised manner. Our problem uses a supervised learning setting, where we observe only parts of the network and the corresponding SC labels. For the prediction of each social circle, we treat it as link prediction. However, as mentioned in Section 1, the correlation between social circles should be exploited. Thus, we treat the pre-diction of each social circle as a task, and MT-SPML is applied to learn metrics jointly over the underlying ego networks of all social circles. Note that, as reported in [ 11 ], SCs largely overlap with each other, which implies strong correlations and MTL is thus likely to achieve a more significant performance gain. We obtain data from [ 11 ], which was from Google+ users and information is anonymous. We ran-domly pick one user and his/her social circles for our experiment. The entire ego network has 4402 nodes and 5 social circles. The profile of all nodes is also pre-served. There are 6 types of feature: gender, institution, job title, last name, place, and university. We build a bag-of-words feature for all feature types and concate-nate them all, resulting in a feature vector of 2969 dimensions. The data are split 80%/20% as non-overlapping training and testing. In this experiment, we adopt a slightly different procedure for clear demon-stration and a more detailed analysis. We index the SCs from 1 to 5. We start by using ST-SPML to learn a metric for each SC independently. Then, for com-parison we run MT-SPML on various numbers of SCs. There are 26 nontrivial combinations of SCs, so for reasons of space and clarity we explore in detail a single sequence of combinations. Similar results were achieved with the other combinations. We begin by running on { 1,2 } and add one more SC at a time in order, resulting in the following four combinations: { { 1,2,3,4,5 } . We use these in later experiments as well. In this way, we can com-pare the behavior of the algorithms as more tasks join the process. In Fig.3, we compare ST-SPML to MT-SPML on the four combinations of SCs. Note that, because of the inferior performance of SVM based methods on Wikipedia article data, we omit them in this experiment. There two clear observations from Fig.3: (1) All SCs benefit from MTL and the improvement is significant; (2) Perfor-mance continues to improve as more tasks are involved, which demonstrates the superior ability of joint learning. One exception is SC 2, where the performance gain is small. We speculate that SC 2 is not closely related to other circles (e.g., in terms of the number of overlapping nodes). We will discuss the case of SC 2 later.
 and estimates a model for all tasks. Both MT-SPML and U-SPML are applied to the four combinations of SCs. As shown by Fig.4, MT-SPML consistently and significantly outperforms U-SPML at all locations.
 in Table 1, where we show the percentage of node overlapping between SCs. The overlap is defined as the intersection of nodes over the union. As we can see, some circles largely overlap (e.g., SC 1 and 3 have 81.9% nodes in common), while SC 2 barely overlaps with the others. Although overlapping is not the only quantitative measurement of correlations between social circles, a substantial set of common nodes suggests that there are some shared semantics between two relationships. Thus Table 1 supports our earlier speculation as to why SC 2 does not benefit from joint learning as much as the others.
 showing the results on a pair of tasks that are less correlated to each other. We choose SCs 1 and 2, since they have only 1.1% nodes in common. In Fig.5, MT-SPML is jointly learned on { 1,2 } , U-SPML is learned via data pooling, and ST-SPML is trained on 1 and 2 independently. The prediction performances of two tasks are reported in the two groups of bars respectively. As shown in Fig.5, MT-SPML still gets 2%-5% performance improvement over ST-SPML (bars with circles on top). However, the naive data pooling strategy of U-SPML (bars with down pointing triangles) produces results even worse than ST-SPML. This observation suggests that on difficult cases where tasks are not as related, MTL is still able to utilize useful correlations, while respecting the boundaries between tasks. In this paper, we deal with MTL on general network data. We first show that correlation widely exist between tasks on network data by giving two common scenarios where it is beneficial to employ MTL. Then we proposed MT-SPML, a multi-task structural metric learning method. It learns task specific metrics as well as a common distance metric. By combining them, the final metric preserves the structure of the networks. We applied MT-SPML to citation network and social network, and measure the performance in link prediction. Improvements were achieved and detailed analysis was provided. Moreover, its SGD implemen-tation is easy and efficient with good convergence behaviour, thus the proposed method can scale up to larger problems and be a strong baseline approach to future research works.

