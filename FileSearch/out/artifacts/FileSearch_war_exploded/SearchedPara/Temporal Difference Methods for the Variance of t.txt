 Aviv Tamar avivt@tx.technion.ac.il Dotan Di Castro dot@tx.technion.ac.il Shie Mannor shie@ee.technion.ac.il In sequential decision making within the Markov De-cision Process (MDP) framework, policy evaluation refers to the process of mapping each state of the sys-tem to some statistical property of its long-term out-come, most commonly its expected reward to go . In the fields of Reinforcement Learning (RL; Bertsekas ning in MDPs (Puterman, 1994), policy evaluation is a fundamental step in many policy improvement algo-rithms. Yet in domains where policies are mostly hand designed, for example in clinical decision making, pol-icy evaluation is also important, for a prudent choice of strategy must depend on it (Shortreed et al., 2011). A principal challenge in policy evaluation arises when the state space is large, or continuous, necessitating some means of approximation for the process to be tractable. This difficulty is even more pronounced when a model of the process is not available, and the evaluation has to be estimated from a limited amount of samples. Fortunately, for the case of the expected reward to go, also known as the value function and de-noted by J , the sequential nature of the problem may be exploited to overcome these difficulties. Temporal Difference methods (TD; Sutton, 1988) employ func-tion approximation to represent J in a lower dimen-sional subspace, and tune the approximation param-eters efficiently from data. Enjoying both theoretical guarantees (Bertsekas, 2012; Lazaric et al., 2010) and empirical success (Tesauro, 1995), these methods are considered the state of the art in policy evaluation. However, when it comes to evaluating additional statistics of the reward to go, such as its variance, little is known. This is due to the fact that the expectation plays a key role in the Bellman equation, which drives TD algorithms.
 Yet, the incentives to evaluate such statistics are ex-tensive. In the context of RL and planning, incor-porating such statistics into the performance evalu-ation criteria leads to risk sensitive optimization, a topic that has gained significant interest recently (Fi-lar et al., 1995; Mihatsch &amp; Neuneier, 2002; Geibel &amp; Wysotzki, 2005; Mannor &amp; Tsitsiklis, 2011). In a more general context, uncertainty in a policy X  X  long-term outcome is critical for decision making in many areas, such as financial, process control, and clinical domains. In these domains, considering the variance of the total reward is particularly important, as it is both common-practice and intuitive to understand (Sharpe, 1966; Shortreed et al., 2011).
 In this paper we present a TD framework for estimat-ing the variance of the reward to go , denoted by V , using function approximation, in problems where a model is not available. To our knowledge, this is the first work that addresses the challenge of large state spaces, by considering an approximation scheme for V . Our approach is based on the following observa-tion: the second moment of the reward to go, denoted by M , together with the value function J , obey a lin-ear  X  X ellman-like X  equation. By extending TD meth-ods to jointly estimate J and M with linear function approximation, we obtain a solution for estimating the variance, using the relation V = M  X  J 2 .
 We propose both a variant of Least Squares Temporal Difference (LSTD) (Boyan, 2002) and of TD(0) (Sut-ton &amp; Barto, 1998) for jointly estimating J and M with a linear function approximation. For these algorithms, we provide convergence guarantees and error bounds. In addition, we introduce a novel method for enforc-ing the approximate variance to be positive, through a constrained TD equation. An empirical evaluation on a challenging continuous maze problem demonstrates the applicability of our approach to large domains, and highlights the importance of the variance function in understanding the risk of a policy.
 A previous study by Sato et al. (2001) suggested TD equations for J and V , without function approxima-tion. Their approach relied on a non-linear equation for V , and it is not clear how it may be extended to handle large state spaces. More recently, Morimura et al. (2012) proposed TD learning rules for a paramet-ric distribution of the return, albeit without function approximation nor formal guarantees. In the Bayesian GPTD framework of Engel et al. (2005), the reward-to-go is assumed to have a Gaussian posterior dis-tribution, and its mean and variance are estimated. However, the resulting variance is a product of both stochastic transitions and model uncertainty, and is thus different than the variance considered here. We consider a Stochastic Shortest Path (SSP) prob-lem 1,2 (Bertsekas, 2012), where the environment is modeled by an MDP in discrete time with a finite state space X , { 1 ,...,n } and a terminal state x  X  . A fixed policy  X  determines, for each x  X  X , a stochastic transition to a subsequent state x 0  X  { X  X  x  X  } with probability P ( x 0 | x ). We consider a deterministic and bounded reward function r : X  X  R , and assume zero reward at the terminal state. We denote by x k the state at time k , where k = 0 , 1 , 2 ,... .
 A policy is said to be proper (Bertsekas, 2012) if there is a positive probability that the terminal state x  X  will be reached after at most n transitions, from any initial state. In this paper we make the following assumption Assumption 1. The policy  X  is proper. Let  X  , min { k &gt; 0 | x k = x  X  } denote the first visit time to the terminal state, and let the random variable B denote the accumulated reward along the trajectory until that time In this work, we are interested in the mean-variance tradeoff in B , represented by the value function and the variance of the reward to go We will find it convenient to define also the second moment of the reward to go Our goal is to estimate J ( x ) and V ( x ) from trajectories obtained by simulating the MDP with policy  X  . In this section we derive a projected equation method for approximating J ( x ) and M ( x ) using linear func-tion approximation. The estimation of V ( x ) will then follow from the relation V ( x ) = M ( x )  X  J ( x ) 2 . Our starting point is a system of equations for J ( x ) and M ( x ), first derived by Sobel (1982) for a dis-counted infinite horizon case, and extended here to the SSP case. The equation for J is the well known Bellman equation for a fixed policy, and independent of the equation for M .
 Proposition 2. The following equations hold for x  X  X
J ( x ) = r ( x ) + X M ( x ) = r ( x ) 2 + 2 r ( x ) X Furthermore, under Assumption 1 a unique solution to (1) exists.
 A straightforward proof is given in Appendix A. At this point the reader may wonder why an equation for V is not presented. While such an equation may be derived, as was done by Tamar et al. (2012), it is not linear. The linearity of (1) in J and M is the key to our approach. As we show in the next subsection, the solution to (1) may be expressed as the fixed point of a linear mapping in the joint space of J and M . We will then show that a projection of this mapping onto a linear feature space is contracting, thus allowing us to use existing TD theory to derive estimation algorithms for J and M . 3.1. A Projected Fixed Point Equation in the For the sequel, we introduce the following vector no-tations. We denote by P  X  R n  X  n and r  X  R n the SSP transition matrix and reward vector, i.e., P x,x 0 = P ( x 0 | x ) and r x = r ( x ), where x,x 0  X  X . Also, we define the diagonal matrix R , diag ( r ).
 For a vector z  X  R 2 n we let z J  X  R n and z M  X  R n denote its leading and ending n components, respec-tively. Thus, such a vector belongs to the joint space of J and M .
 We define the mapping T : R 2 n  X  R 2 n by It may easily be verified that a fixed point of T is a solution to (1), and by Proposition 2 such a fixed point exists and is unique.
 When the state space X is large, a direct solution of (1) is not feasible, even if P may be accurately obtained. A popular approach in this case is to approximate J ( x ) by restricting it to a lower dimensional subspace, and use simulation based TD algorithms to adjust the ap-proximation parameters (Bertsekas, 2012). In this pa-per we extend this approach to the approximation of M ( x ) as well.
 We consider a linear approximation architecture of the form where w J  X  R l and w M  X  R m are the approximation parameter vectors,  X  J ( x )  X  R l and  X  M ( x )  X  R m are state dependent features, and (  X  ) &gt; denotes the trans-pose of a vector. The low dimensional subspaces are therefore where  X  J and  X  M are matrices whose rows are  X  J ( x ) &gt; and  X  M ( x ) &gt; , respectively. We make the following standard independence assumption on the features Assumption 3. The matrix  X  J has rank l and the matrix  X  M has rank m .
 As outlined earlier, our goal is to estimate w J and w M from simulated trajectories of the MDP. Thus, it is constructive to consider projections onto S J and S M with respect to a norm that is weighted according to the state occupancy in these trajectories.
 For a trajectory x 0 ,...,x  X   X  1 , where x 0 is drawn from a fixed distribution  X  0 ( x ), and the states evolve ac-cording to the MDP with policy  X  , define the state occupancy probabilities and let We make the following assumption on the policy  X  and initial distribution  X  0 Assumption 4. Each state has a positive probability of being visited, namely, q ( x ) &gt; 0 for all x  X  X . For vectors in R n , we introduce the weighted Euclidean norm and we denote by  X  J and  X  M the projections from R n onto the subspaces S J and S M , respectively, with respect to this norm. For z  X  R 2 n we denote by  X  the projection of z J onto S J and z M onto S M , namely 3 We are now ready to fully describe our approximation scheme. We consider the projected fixed point equation and, letting z  X  denote its solution, propose the approx-imate value function  X  J = z  X  J  X  S J and second moment function  X  M = z  X  M  X  S M .
 We proceed to derive some properties of the projected fixed point equation (4). We begin by stating a well known result regarding the contraction properties of the projected Bellman operator  X  J T J , where T J y = r + Py . A proof can be found at (Bertsekas, 2012), proposition 7.1.1. Lemma 5. Let Assumptions 1, 3, and 4 hold. Then, there exists some norm k X k J and some  X  J &lt; 1 such that Similarly, there exists some norm k  X  k M and some  X 
M &lt; 1 such that Next, we define a weighted norm on R 2 n Definition 6. For a vector z  X  R 2 n and a scalar 0 &lt;  X  &lt; 1 , the  X  -weighted norm is where k X k J and k X k M are defined in Lemma 5.
 Our main result of this section is given in the following proposition, where we show that the projected opera-tor  X  T is a contraction with respect to the  X  -weighted norm.
 Proposition 7. Let Assumptions 1, 3, and 4 hold. Then, there exists some 0 &lt;  X  &lt; 1 and some  X  &lt; 1 such that  X  T is a  X  -contraction with respect to the  X  -weighted norm, i.e., k  X  Tz 1  X   X  Tz 2 k  X   X   X  k z 1  X  z 2 k  X  ,  X  z 1 ,z 2  X  R Proof. First, using (2) and (3) we have that k  X  Tz 1  X   X  Tz 2 k  X  = k  X  P ( z 1  X  z 2 ) k  X  , where Thus, it suffices to show that for all z  X  R 2 n We will now show that k  X  P z k  X  may be separated into two terms which may be bounded by Lemma 5, and an additional cross term. By balancing  X  and  X  , this term may be contained to yield the required contraction. We have k  X  P z k  X  =  X  k  X  J Pz J k J where the equality is by definition of the  X  weighted norm (5), the first inequality is from the triangle in-equality, and the second inequality is by Lemma 5. Now, we claim that there exists some finite C such that To see this, note that since R n is a finite dimensional real vector space, all vector norms are equivalent (Horn &amp; Johnson, 1985) therefore there exist finite C 1 and C such that for all y  X  R n
C 1 k 2 X  M RPy k 2  X k 2 X  M RPy k M  X  C 2 k 2 X  M RPy k 2 , where k X k 2 denotes the Euclidean norm. Let  X  denote the spectral norm of the matrix 2 X  M RP , which is fi-nite since all the matrix elements are finite. We have that Using again the fact that all vector norms are equiva-lent, there exists a finite C 3 such that Setting C = C 2  X C 3 we get the desired bound. Let  X   X  = max {  X  J , X  M } &lt; 1, and choose &gt; 0 such that Now, choose  X  such that  X  = C + C . We have that and plugging this into (7) yields We now return to (6), where we have  X  X  J k z J k J + (1  X   X  )  X  M k z M k M + (1  X   X  ) k 2 X  M  X   X  X  J k z J k J + (1  X   X  )  X  M k z M k M +  X  k z J k J  X  (  X   X  + ) (  X  k z J k J + (1  X   X  ) k z M k M ) , where the first inequality is by (8), and the second is by the definition of  X   X  . We have thus shown that Finally, choose  X  =  X   X  + .
 Proposition 7 guarantees that the projected operator  X  T has a unique fixed point. Let us denote this fixed point by z  X  , and let w  X  J ,w  X  M denote the corresponding weights, which are unique due to Assumption 3 In the next proposition we provide a bound on the approximation error. The proof is in Appendix B. Proposition 8. Let Assumptions 1, 3, and 4 hold. Denote by z true  X  R 2 n the true value and second mo-ment functions, i.e., [ z true ] J = J, and [ z true ] M Then, with  X  and  X  defined in Proposition 7. In this section we propose algorithms that estimate  X  J and  X  M from sampled trajectories of the MDP, based on the approximation architecture of the previous section. We begin by writing the projected equation (9) in ma-trix form. First, let us write the equation explicitly as
 X  M ( Rr + 2 RP  X  J w  X  J + P  X  M w  X  M ) =  X  M w  X  M . Projecting a vector y onto  X  w satisfies the following orthogonality condition we therefore have
 X 
M Q ( X  M w which can be written as with A =  X  &gt; J Q ( I  X  P )  X  J , b =  X  &gt; J Qr, C =  X  &gt; M Q ( I  X  P )  X  M , d =  X  &gt; M QR r + 2 P  X  and the matrices A and C are invertible since Proposi-tion 7 guarantees a unique solution to (9) and Assump-tion 3 guarantees the unique weights of its projection. 4.1. A Least Squares TD Algorithm Our first simulation-based algorithm is an extension of the Least Squares Temporal Difference (LSTD) al-gorithm (Boyan, 2002). We simulate N trajectories of the MDP with the policy  X  and initial state dis-k = 0 , 1 ,...,N , denote the state sequence and visit times to the terminal state within these trajectories, respectively. We now use these trajectories to form the following estimates of the terms in (12) A b C d where E N denotes an empirical average over trajecto-approximation is given by The next theorem shows that LSTD converges.
 Theorem 9. Let Assumptions 1, 3, and 4 hold. Then  X  w
J  X  w The proof involves a straightforward application of the law of large numbers and is described in Appendix C. Convergence rates for regular LSTD were derived by Konda (2002) and Lazaric et al. (2010), and may be extended to the algorithm presented here. This issue is deferred to the full version of this paper. 4.2. An Online TD(0) Algorithm Our second estimation algorithm is an extension of the well known TD(0) algorithm (Sutton &amp; Barto, 1998). Again, we simulate trajectories of the MDP corresponding to the policy  X  and initial state distri-bution  X  0 , and we iteratively update our estimates at every visit to the terminal state 4 . For some 0  X  t &lt;  X  and weights w J ,w M , we introduce the TD terms  X 
J ( t,w J ,w M ) = r ( x  X 
M ( t,w J ,w M ) = r Note that  X  k J is the standard TD error (Sutton &amp; Barto, 1998). For the intuition behind  X  k M , observe that M in (1) is equivalent to the value function of an x  X  P ( x 0 | x ).  X  k M is then the equivalent TD error, with  X  ( x 0 ) &gt; w J substituting J ( x 0 ). The TD(0) algorithm is given by where {  X  k } are positive step sizes.
 The next theorem shows that TD(0) converges.
 Theorem 10. Let Assumptions 1, 3, and 4 hold, and let the step sizes satisfy probability 1.
 The proof, provided in Appendix D, is based on rep-resenting the algorithm as a stochastic approximation, and using a result of Borkar (2008) to show that the iterates asymptotically track a certain ordinary differ-ential equation (ODE). This ODE is then shown to have a unique asymptotically stable equilibrium ex-actly at w  X  J ,w  X  M . Convergence rates for TD(0) may be derived along the lines of Konda (2002), with the details deferred to the full version of this paper. 4.3. Multistep LSTD(  X  ) Algorithms A common method in value function approximation is to replace the single step mapping T J with a multistep version of the form with 0 &lt;  X  &lt; 1. The projected equation (10) then may write a multistep equation for M where and Note the difference between T M  X  and [ T ] M defined ear-lier; We are no longer working on the joint space of J and M but instead we have an independent equation for approximating J , and its solution w  X  (  X  ) J is part of Equation (14) for approximating M . By Proposition 7.1.1 of Bertsekas (2012) both  X  J T (  X  ) J and  X  M T (  X  ) contractions with respect to the weighted norm k X k q , therefore both multistep projected equations admit a unique solution. In a similar manner to the single step version, the projected equations may be written in ma-trix form where A C d sions above may be obtained by using eligibility traces, as described by Bertsekas (2012), and the LSTD(  X  ) approximation is then given by  X  w  X  (  X  ) J = ( A (  X  ) d (  X  ) , a similar procedure may be used to derive esti-mates C (  X  ) N and d (  X  ) N , and to obtain the LSTD(  X  ) ap-sult similar to Theorem 9 may also be obtained. Due to the similarity to the LSTD procedure in (13), the exact details are omitted. The TD algorithms of the preceding section approx-imated J and M by the solution to the fixed point equation (9). While Proposition 8 shows that the ap-proximation errors of  X  J and  X  M are bounded, it does not guarantee that the approximated variance  X  V , given edy is to set all negative values of  X  V to zero; however, by such we lose all information in these states. In this section we propose an alternative method, based on modifying the fixed point equation (9) to include constraints for variance non-negativeness. We thus ob-tain a different approximation architecture, in which a non-negative variance is inherent. We now present the constrained equation and discuss how its solution may be computed.
 First, let us write the multistep equation for the second moment weights (14) with the projection operator as an explicit minimization with  X  Observe that a non-negative variance in some state x may be written as a linear inequality in w  X  (  X  ) M We now propose to add such inequality constraints to the projection operator. Let { x 1 ,...,x s } denote a set of states in which we demand that the variance be non-negative. Let H  X  R s  X  m denote a matrix with the features  X   X  &gt; M ( x i ) as its rows, and let g  X  R s denote a vector with elements  X  (  X  J ( x i ) &gt; w  X  (  X  ) J the non-negative-variance projected equation for the second moment as Here, w + M denotes the weights of  X  M in the modified approximation architecture. We now discuss whether a solution to (16) exists, and how it may be obtained. Let us assume that the constraints in (16) admit a feasible solution: Assumption 11. There exists w such that Hw &lt; g . Note that a trivial way to satisfy Assumption 11 is to have some feature vector that is positive for all states. Equation (16) is a form of projected equation studied by Bertsekas (2011), the solution of which exists, and may be obtained by the following iterative procedure where  X  is an arbitrary positive definite matrix, and  X 
 X  ,  X  W M denotes a projection onto the convex set { w | Hw  X  g } with respect to the  X  weighted Euclidean norm. The following lemma, which is based on a con-vergence result of Bertsekas (2011), guarantees that algorithm (17) converges.
 Lemma 12. Assume  X  &gt; 0 , and let Assumption 11 hold. Then (16) admits a unique solution w + M , and there exists  X   X  &gt; 0 such that  X   X   X  (0 ,  X   X  ) and  X  w the algorithm (17) converges at a linear rate to w + M . Proof. This is a direct application of the convergence result of Bertsekas (2011). The only nontrivial as-sumption that needs to be verified is that T (  X  ) contraction in the k X k q norm (Proposition 1 in Bert-sekas, 2011). For  X  &gt; 0 Proposition 7.1.1. of Bertsekas (2012) guarantees that T (  X  ) M is indeed contracting in the k X k q norm.
 vance, and should be replaced in (17) with their simu-in the previous section. The convergence of these esti-mates, together with the result of Lemma 12, lead to the following convergence result, which is given with-out proof.
 Theorem 13. Consider the algorithm in (17) with C respectively, and with k ( N ) replacing k for a specific N . Also, let the assumptions in Lemma 12 hold, and let  X   X  (0 ,  X   X  ) , with  X   X  defined in Lemma 12. Then w An in-depth study of the approximation architecture (16) is deferred to the full version of this paper. How-ever, an illustration on a toy problem is provided in Appendix E. In this section we present numerical simulations of pol-icy evaluation on a challenging continuous maze do-main. The goal of this presentation is threefold; first, we show that the variance of the reward-to-go may be estimated successfully on a large state space. Sec-ond, the intuitive maze domain highlights the insight that may be gleaned from this variance, and third, we show that in terms of sample efficiency, our LSTD(  X  ) algorithm significantly outperforms the current state-of-the-art. We begin by describing the domain and then present our policy evaluation results.
 The Pinball Domain (Konidaris &amp; Barto, 2009) is a continuous 2-dimensional maze where a small ball needs to be maneuvered between obstacles to reach some target area, as depicted in Figure 1A. The ball is controlled by applying a constant force in one of the 4 directions at each time step, which causes accelera-tion in the respective direction. In addition, the ball X  X  velocity is susceptible to additive Gaussian noise (zero mean, standard deviation 0.03) and friction (drag co-efficient 0.995). The obstacles are sharply shaped, and collisions are fully elastic. The state of the ball is thus with 4 available controls. The reward is -1 for all states until reaching the target. A Java implementation of the pinball domain used by Konidaris &amp; Barto (2009) is available on-line 5 and was used for our simulations as well, with the addition of noise to the velocity. A near-optimal policy  X  was obtained using SARSA (Sutton &amp; Barto, 1998) with radial basis function fea-tures. The value J and standard deviation of the reward-to-go 1(B;C), for 1816 equally spaced states between the ob-stacles with zero velocity. These plots were obtained by Monte Carlo (MC) estimation of the mean and vari-ance, using over 2 million trajectories starting from these states. To our knowledge, MC is the current state-of-the-art technique for obtaining such variance estimates. As should be expected, the value is ap-proximately a linear function of the distance to the target. In contrast, the standard deviation is clearly not linear in the distance, and in some places not even monotone. Furthermore, we see that an area in the top part of the maze before the first turn is very risky, even more than the farthest point from the target. We stress that this information cannot be gleaned from in-specting the value function alone.
 Figure 1D shows the approximate standard deviation p  X 
V obtained by the LSTD(  X  ) algorithm of Section 4.3. We used uniform tile features for  X  J and  X  M (50  X  50 non-overlapping tiles in x and y without dependence on velocity, for the same resolution as the MC esti-mate), and set  X  = 0 . 9. To emphasize the efficiency of our method, we used only one sample trajectory per each state in the MC evaluation  X  a total of N = 1816 trajectories, with uniformly distributed initial states. Clearly, a single sample for each evaluation point is insufficient for a meaningful MC variance estimate. However, by exploiting relations between states (1), LSTD provides a reasonable approximation.
 We further explore LSTD(  X  ) in Figure 1E, where we show the RMS error of p  X  V (compared to the MC estimate) for different values of  X  and N . As in regular LSTD,  X  trades off estimation bias and variance. This work presented a novel framework for policy eval-uation in RL with respect to the variance of the reward to go. We presented both formal guarantees and em-pirical evidence that this approach is useful in prob-lems with a large state space. To the best of our knowl-edge, such problems are beyond the capabilities of pre-vious approaches.
 A requirement of variance evaluation is that it be non-negative. We approached this issue by adding con-straints to the second moment approximation. An alternative is through the choice of features. Inter-estingly, in our experiments we found that using non-overlapping tile features produces a non-negative ap-proximate variance. For this choice of features (identi-cal for J and M ), we can show that the direct approx-imation is always non-negative, i.e.,  X  M  X  ( X  J ) 2  X  0, where the square is element-wise. Whether this holds also for the fixed-point approximation, and if there are other features with this property, is an open question. We conclude with a discussion on policy optimiza-tion with respect to a mean-variance tradeoff. While a naive variance-penalized policy iteration algorithm may be easily conceived, its usefulness should be ques-tioned, as it was shown to be problematic for the standard deviation adjusted reward (Sobel, 1982) and the variance constrained reward (Mannor &amp; Tsitsiklis, 2011). Perhaps a wiser approach would be to consider gradient based updates. Tamar et al. (2012) proposed policy gradient algorithms for a class of variance re-lated criteria, and showed their convergence to local optima. These algorithms may be extended to use the variance function in an actor-critic type scheme. Such a study is left for future research.
 The research leading to these results has received funding from the European Research Counsel under the European Union X  X  Seventh Framework Program (FP7/2007-2013) / ERC Grant Agreement No 306638. Bertsekas, D. P. Temporal difference methods for gen-eral projected equations. IEEE Trans. Auto. Con-trol , 56(9):2128 X 2139, 2011.
 Bertsekas, D. P. Dynamic Programming and Optimal
Control, Vol II . Athena Scientific, fourth edition, 2012.
 Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Programming . Athena Scientific, 1996.
 Borkar, V. S. Stochastic Approximation: A Dynamical Systems Viewpoint . Cambridge Univ Press, 2008. Boyan, J. A. Technical update: Least-squares tem-poral difference learning. Machine Learning , 49(2): 233 X 246, 2002.
 Engel, Y., Mannor, S., and Meir, R. Reinforcement learning with Gaussian processes. In ICML , 2005. Filar, J. A., Krass, D., and Ross, K. W. Percentile performance criteria for limiting average Markov de-cision processes. IEEE Trans. Auto. Control , 40(1): 2 X 10, 1995.
 Geibel, P. and Wysotzki, F. Risk-sensitive reinforce-ment learning applied to control under constraints. JAIR , 24(1):81 X 108, 2005.
 Horn, R. A. and Johnson, C. R. Matrix Analysis . Cam-bridge University Press, 1985.
 Konda, V. Actor-Critic Algorithms . PhD thesis, Dept.
Comput. Sci. Elect. Eng., MIT, Cambridge, MA, 2002.
 Konidaris, G. D. and Barto, A. G. Skill discovery in continuous reinforcement learning domains using skill chaining. In NIPS , 2009.
 Lazaric, A., Ghavamzadeh, M., and Munos, R. Finite-sample analysis of LSTD. In ICML , 2010.
 Mannor, S. and Tsitsiklis, J. N. Mean-variance opti-mization in Markov decision processes. In ICML , 2011.
 Mihatsch, O. and Neuneier, R. Risk-sensitive rein-forcement learning. Machine Learning , 49(2):267 X  290, 2002.
 Morimura, T., Sugiyama, M., Kashima, H., Hachiya,
H., and Tanaka, T. Parametric return density es-timation for reinforcement learning. arXiv preprint arXiv:1203.3497 , 2012.
 Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming . John Wiley &amp; Sons, Inc., 1994.
 Sato, M., Kimura, H., and Kobayashi, S. TD algorithm for the variance of return and mean-variance rein-forcement learning. Transactions of the Japanese Society for Artificial Intelligence , 16:353 X 362, 2001. Sharpe, W. F. Mutual fund performance. The Journal of Business , 39(1):119 X 138, 1966.
 Shortreed, S. M., Laber, E., Lizotte, D. J., Stroup,
T. S., Pineau, J., and Murphy, S. A. Informing se-quential clinical decision-making through reinforce-ment learning: an empirical study. Machine learn-ing , 84(1):109 X 136, 2011.
 Sobel, M. J. The variance of discounted Markov deci-sion processes. J. Applied Probability , pp. 794 X 802, 1982.
 Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning , 3(1):9 X 44, 1988.
 Sutton, R. S. and Barto, A. G. Reinforcement Learn-ing . MIT Press, 1998.
 Tamar, A., Di Castro, D., and Mannor, S. Policy gra-dients with variance related risk criteria. In ICML , 2012.
 Tesauro, G. Temporal difference learning and TD-gammon. Communications of the ACM , 38(3):58 X 
