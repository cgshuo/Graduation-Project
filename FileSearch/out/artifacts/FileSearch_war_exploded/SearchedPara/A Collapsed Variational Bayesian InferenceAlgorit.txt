 Gatsby Computational Neuroscience Unit 17 Queen Square, London WC1N 3AR, UK such models.
 (VB) inference [2], expectation propagation (EP) [7] to col lapsed Gibbs sampling [5]. inference procedures.
 space X  X here the parameters are marginalized out X  X ixes muc h better than a Gibbs sampler that (CVB) inference algorithm.
 accurate than standard VB. mixing proportion  X  the i th word in the document, a topic z word x symmetric Dirichlet prior with parameter  X  is placed on the topic parameters  X  joint distribution over all parameters and variables is: where n n Given the observed words x = { x distribution over the latent topic indices z = { z parameters  X  = {  X  propagation [7] and collapsed Gibbs sampling [5]. We review the VB and collapsed Gibbs sam-combines advantages of both. 2.1 Variational Bayes variational free energy: with  X  q ( z ,  X  ,  X  ) an approximate posterior, H (  X  q ( z ,  X  ,  X  )) = E tropy, and  X  q ( z ,  X  ,  X  ) assumed to be fully factorized:  X   X  p ( 2.2 Collapsed Gibbs Sampling Given the current state of all but one variable z where the superscript  X  ij means the corresponding variables or counts with x and the denominator is just a normalization. The conditiona l distribution of z any particular other variable z number of samples may be required to reduce sampling noise.
 when mean field algorithms can be expected to be accurate [9]. The counts n  X  ij fields through which z p ( algorithm of the next section. algorithm collapsed variational Bayesian (CVB) inference .
 independent, thus we approximate the posterior as: where  X  q ( z p (  X  ,  X  | x , z ,  X ,  X  ) , and the variational free energy simplifies to: variational parameters  X   X  Plugging in (7), expanding log  X (  X  + n ) n , and cancelling terms appearing both in the numerator and de nominator, we get 3.1 Gaussian approximation for CVB Inference requires minimal computational costs.
 In this section we describe the Gaussian approximation appl ied to E two expectation terms are similarly computed. Assume that n P with mean parameter  X   X  We further approximate the function log(  X  + n  X  ij E [ n  X  ij jk  X  ] , and evaluate its expectation under the Gaussian approxima tion: Because E even when n the variance of n  X  ij Finally, plugging (17) into (15), we have our CVB updates: by replacing the fields n  X  ij exponentiated terms are correction factors accounting for the variance in the fields. By keeping track of the mean and variance of n variance of the corresponding Bernoulli variables wheneve r we require the terms with x moved, the computational cost scales only as O ( K ) for each update to  X  q ( z current sample of z collapsed Gibbs sampling is significantly smaller than thos e for VB and CVB. average 136 words per document. Second is  X  X IPS X  (books.nips.cc) with J = 1675 documents, a NIPS. We ran each algorithm on each dataset 50 times with diff erent random initializations. probabilities are computed as: Note that we used estimated mean values of  X  S samples from the posterior, we used: approximation becomes dubious in that regime. found to be so accurate that there is never a need for exact sum mation. Dirichlet processes, and hierarchical Dirichlet processe s.
 { in the marginalize out parameters and assume latent variabl es are independent cell. We can compute the expectation terms in (15) exactly as follo ws. Consider E which requires computing  X  q ( n  X  ij n parameter  X   X  the convolution of all v be the ( m +1) st entry in v  X  ij This exact implementation requires an impractical O ( n 2 vectors v but with minimal implementation complexity and computatio nal cost. YWT was previously at NUS SoC and supported by the Lee Kuan Yew Endowment Fund. MW was supported by ONR under grant no. N00014-06-1-0734 and by NSF under grant no. 0535278.
