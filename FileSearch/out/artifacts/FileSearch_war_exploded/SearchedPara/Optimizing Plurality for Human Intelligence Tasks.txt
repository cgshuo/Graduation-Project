  X   X  In a crowdsourcing system, Human Intelligence Tasks ( HITs ) (e.g., translating sentences, matching photos, tagging videos with key-words) can be conveniently specified. HITs are made available to a large pool of workers , who are paid upon completing the HITs they have selected. Since workers may have different capabilities, some difficult HITs may not be satisfactorily performed by a sin-gle worker. If more workers are employed to perform a HIT, the quality of the HIT X  X  answer could be statistically improved. Given a set of HITs and a fixed  X  X udget X , we address the important prob-lem of determining the number of workers (or plurality ) of each HIT so that the overall answer quality is optimized. We propose a dynamic programming (DP) algorithm for solving the plurality assignment problem (PAP). We identify two interesting properties, namely, monotonicity and diminishing return , which are satisfied by a HIT if the quality of the HIT X  X  answer increases monotoni-cally at a decreasing rate with its plurality. We show for HITs that satisfy the two properties (e.g., multiple-choice-question HITs), the PAP is approximable. We propose an efficient greedy algorithm for such case. We conduct extensive experiments on synthetic and real datasets to evaluate our algorithms. Our experiments show that our greedy algorithm provides close-to-optimal solutions in practice. H.2.8 [ Database Management ]: Database Applications Crowdsourcing, Data Quality
Recently, there has been a rise of interest in crowdsourcing sys-tems , such as the Amazon Mechanical Turk (AMT) 1 and Crowd-Flower 2 . These Internet-based systems harness human effort to https://www.mturk.com http://crowdflower.com solve problems that are easy for human beings but difficult for com-puters [3, 6 X 8]. Typically, a requester announces a set of Human Intelligent Tasks (or HITs ). Through a user interface, a worker se-lects her preferred HIT(s) and submits her answers to the system. If the requester is satisfied about the answers, the worker is duly rewarded. Example HITs include question answering [3, 8, 17], en-tity resolution [6, 20, 21], sorting and join [7, 9], data filtering [16], and image tagging [22].

For HIT answers to be useful, the workers involved need to per-form the tasks well. In practice, however, workers could be casual Internet users and their answers are hardly perfect [3, 6 X 8]; they may make careless mistakes or misinterpret the HIT requirements. To improve the quality of a HIT X  X  answer, a requester is suggested to assign a sufficient number of workers to the HIT [1, 3, 6 X 8]. In AMT, for instance, a requester is asked to specify the plurality of a HIT, which is the number of workers required to perform that HIT. With multiple workers, their answers can be combined to derive a higher-quality one for a HIT. We call the combined answer the result of the HIT. For example, given a HIT of a binary question (such as True or False), the result of the HIT could be taken as the most frequent answer given by the workers of the HIT. Statistically speaking, more workers working on a HIT gives a higher-quality result of the HIT because the impact of wrong answers are much reduced by the correct ones, if the latter outnumbers the former. In fact, worker multiplicity is advised by AMT [1] as well as in many other previous works [3, 6 X 8].

In practice, plurality has to be limited because: (1) a HIT is as-sociated with a cost for paying the worker and the crowdsourcing system; (2) a requester may only have a limited budget for reward-ing workers; and (3) a requester has to spend some time to ver-ify the HIT results. An interesting question is thus about how one could wisely assign just the right pluralities to HITs of various for-mats, difficulties, and costs, to achieve overall high-quality results, subject to a budget constraint. We call this problem the plurality assignment problem (PAP). As crowdsourcing systems are becom-ing more popular, larger in scale, and with a richer variety of HITs, we anticipate that plurality assignment will become an essential component of crowdsourcing systems. So far, however, few works (e.g., [2, 3, 8, 15]) have addressed the problem.

Manually assigning pluralities to HITs is tedious if not infea-sible. As an example, we inspected the HITs that are available on AMT on 28th October 2012. (Let X  X  call a requester a  X  X eavy requester X  if she submits a large number of HITs.) We observed that the top-10 heaviest requesters together submitted about 90,000 HITs. It is thus not uncommon that a requester has to handle thou-sands of HITs. Our goal is to develop algorithms for automating the process of plurality assignment, considering the various properties of HITs, such as their costs and difficulties, under a given budget constraint.

Among the various types of HITs, multiple choice questions (MCQs) are the most popular ones. For example, among all the HITs on AMT on 28th October 2012, more than three quarters are MCQs. Given an MCQ, workers are asked to make a choice among a set of given ones. For instance, a sentiment-analysis HIT contains a sen-tence, and a worker needs to indicate her opinion ( positive , neutral , or negative ) about it. Other examples include categorizing objects (e.g., choosing the best category for an image) and assigning rat-ing scores to items. One way to combine workers X  answers to an MCQ to obtain a result is half voting . Specifically, the choice that is selected by more than half of the workers is taken as the MCQ X  X  result. Due to its popularity, we illustrate our solution framework using MCQ as an example. We remark that our framework can be extended to address other kinds of HITs.

Recall that the objective of plurality assignment is to maximize the overall quality of a set of HITs X  results. We therefore need a quantitative measure of a result X  X  quality. For an MCQ, we measure its result X  X  quality (or simply the MCQ X  X  quality) by the likelihood that the result is correct (see Section 3.2). We note that an MCQ X  X  quality generally improves with (1) its plurality and (2) the accu-racies of its workers. That is, more and better workers improves the MCQ X  X  quality. As a result, we formulate an MCQ X  X  quality as a function of the above two factors. To tackle the plurality assign-ment problem (PAP), we develop a dynamic programming algo-rithm DP . Given a set of MCQs, DP takes the HITs X  quality func-tions and a budget as input and determines the optimal plurality assignment for the MCQs.

Although DP gives optimal solutions to PAP, it is not very effi-cient especially for HIT sets that contain thousands of HITs. For example, for a set of 60,000 HITs extracted from AMT, DP takes over 10 hours to execute. In a large crowdsourcing system (e.g., AMT) that manages HITs from many heavy requesters, efficiency becomes an important issue. We have made two interesting obser-vations of a crowdsourcing system which lead to significant speedups in solving PAP. First, we found that an MCQ X  X  quality function possesses two interesting properties: (1) monotonicity : the quality function increases with plurality; and (2) diminishing return : the rate of quality improvement drops with plurality. We make use of these characteristics of MCQs to design an approximation algo-rithm Greedy , which is much more efficient than DP . We show the theoretical approximation ratio of Greedy . We also show that Greedy  X  X  solution is very close to the optimal solution in prac-tice. Second, we observe that many HITs submitted by the same requester are given the same cost and that these HITs are of very similar nature. For example, a requester performing sentiment anal-ysis may submit numerous HITs, each one being a tweet to be la-beled with an opinion (positive, negative, neutral). It is reasonable to assume that these HITs share the same quality function. If we group HITs of the same cost and quality function as a group, then each HIT in the group should be given more or less the same plu-rality. We exploit this observation and use a  X  X rouping technique X  to effectively reduce the problem size of PAP. This technique can be applied to both DP and Greedy to improve their efficiency.
We have performed extensive experiments to evaluate our ap-proaches on a synthetic dataset. We examine the effectiveness and the efficiency of our algorithms over a large number of HITs and budget values. (By effectiveness, we refer to the HITs X  quality as a result of a given plurality assignment computed by an algorithm.) We have also developed a system to collect real data from human users. We found that both DP and Greedy are on average 20% more effective than other simple approaches (e.g., evenly assign-ing budget to HITs). Moreover, Greedy is about a thousand times faster than DP , and the group-based versions of DP and Greedy are about ten times faster than their non-group-based counterparts.
Figure 1 shows the framework of our approach. (Step 1) A re-quester sends her HITs and budget information to the quality man-ager , which decides the plurality of the HITs. (Step 2) The manager invokes the quality estimator , which keeps track of the accuracy statistics of a pool of workers. (Step 3) Our algorithm, which com-putes the plurality of each HIT, is then invoked. (Step 4) The HITs are installed in the crowdsourcing system. In this paper we focus on the design of plurality assignment algorithms.

The rest of the paper is as organized as follows. Section 2 dis-cusses the related works. In Section 3 we describe the plurality assignment problem. Section 4 describes various plurality assign-ment algorithms. We show our experimental results in Section 5. Section 6 discusses how our framework can be extended to address other kinds of HITs. Section 7 concludes the paper.
Databases for crowdsourcing. To meet the needs of manag-ing large amounts of data collected from crowdsourcing systems, database prototypes such as CrowdDB [6] and Qurk [10, 11] have been developed recently. These systems provide native language support for crowdsourcing, where query operators can be invoked to collect information from workers. In [9], the authors study a database that utilizes workers to compare or to rate database items, and to perform sort and join operations. It is interesting to exam-ine how these systems should operate under the various resource constraints (e.g., limited requester budget, number of workers, and their accuracy). Our work on plurality assignment can very well be applied to a crowdsourcing database to tackle this problem.
Designing HIT questions. A few works have recently addressed the management of resource constraints in crowdsourcing systems. In [17], the authors study human-assisted graph search, and pro-pose algorithms to generate the optimal set of questions. In [20,21], algorithms are developed to derive the minimum set of questions for supporting entity resolution. In [7], the authors study how to generate a set of paired-entity-comparison questions that opti-mally identify the entity with the maximum value under a limited budget. [16] studies the problem of filtering data using human re-sources, and designs effective strategies to optimize expected cost and error. In [15], the problem of deciding which items to label, with the aim of improving the model used for active learning, is studied. Notice that these works focus on the design of HITs (i.e., what questions to ask). On the other hand, we focus on how to ob-tain high-quality results for HITs under a limited budget by running a plurality assignment algorithm.

Determining plurality. Although the problem of determining the pluralities of HITs has recently attracted some attention, there are only a few works that have been done for specific types of HITs. These include MCQs [2, 8], binary questions [3], and labeling [15]. In [2, 8], the authors show how to determine the minimum plu-rality of an MCQ such that a user-given quality threshold can be achieved. Our work differs from [2, 8] in two ways. First, while they deal with a quality threshold as a constraint, we deal with a budget constraint. In some cases, it is more natural and is easier for a requester to specify a budget (in dollar amount) than to specify a quality threshold (in probability). Our work thus complement that of [2,8]. Second, [8] assumes that all MCQs are answered correctly with the same probability, and hence the same plurality is assigned to all the MCQs. We do not make that assumption and consider assigning (different) pluralities to a set of different MCQs.
In [3], the authors address binary question (or BQ ), which con-tains two possible answers only. An example BQ is similarity com-parison, where two images are shown, and a worker is asked to give a yes/no answer to state whether the two images refer to the same person. [3] examines the problem of allocating manpower to work on BQ-type HITs based on the workers X  accuracy. Their solution, which assigns specific workers to each HIT, may not be applica-ble in existing systems. This is because in common crowdsourcing platforms like AMT, workers are allowed to freely choose the HITs they wish to do. Our algorithms only decide plurality values and do not force specific workers to perform specific tasks. Moreover, we study MCQs, which are generalized forms of BQs.

In [15], the authors study the problem of assigning pluralities to HITs that assign keyword labels to Internet resources (e.g., im-ages). To determine pluralities, they propose a simple dynamic-programming-based algorithm. Although their solution share some similar properties as our DP algorithm, [15] did not further elabo-rate on the efficiency and effectiveness of their algorithm. We study some properties of MCQ (Section 3), and the relationship between HIT cost and quality (Section 4.5). We use these observations to develop solutions that are much faster than DP .
We now describe our data and quality models (Sections 3.1 and 3.2). We focus on MCQ as an illustration of our solution frame-work. We then discuss two properties of MCQ in Section 3.3. We formally define the plurality assignment problem in Section 3.4. Although we focus on MCQ to simplify our discussion, our solu-tion framework can be extended to cover other HIT types. We will briefly discuss other HIT types in Section 6. The data model we employed here is based on the AMT system. In particular, a requester submits a set of one or more HITs to the crowdsourcing system, where each HIT contains a single MCQ. For each MCQ, the requester specifies the question to be asked, as well as all their possible choices. The requester also associates an amount of reward to each MCQ to indicate the number of monetary units that will be paid to a worker for successfully finishing a HIT.
Notice that the number of MCQs contributed by a requester can be enormous; for instance, on 28 October 2012, the top-10 heaviest requesters contributed over 90,000 questions in total. To enable better HIT management, and to allow workers to choose HITs more easily, requesters may label the HITs and classify them based on themes and other properties.

Now, let T = { t 1 ,t 2 ,...,t n } be n MCQs submitted by a re-quester. We use k i , a non-negative integer, to denote the plurality
In the sequel, when we mention MCQ, we refer to a HIT that contains a single MCQ.
 of t i (i.e., k i workers are needed for t i ). Let c i be the cost of t the amount of reward given to a worker for completing t i summaries the symbols used. As discussed before, we determine the pluralities for a set of MCQs to optimize their overall quality . We now explain how to compute the quality of an MCQ t i . Intuitively, this quantity cap-tures the confidence of the answers given by workers on t pends on the answers X  values, as well as the performances of work-ers on t i . To model a worker X  X  performance, the worker X  X  accuracy model was proposed in [8]. We first describe this model and then explain how to use it to define an MCQ X  X  quality.

Worker X  X  accuracy [8]. Every HIT t i is associated with a real value p i called worker X  X  accuracy , or accuracy in short. This is the probability that a randomly-chosen worker provides a correct answer for t i . A way to estimate p i is to collect information from other HITs whose true answers are known, and whose features are similar to those of p i . For example, consider a set of HITs, each of which consists of a sentence; a worker is asked to express her senti-ment about it. As discussed in [4], the readability of a sentence, and hence the worker X  X  ability to comment it correctly, depends on its length. We can thus cluster these HITs according to the lengths of the sentences associated with them. Given a HIT t i of a cluster, [8] presents a method to find p i : 1. Some  X  X ample HITs X  are collected from the cluster, whose true answers are known. 2. Each worker is asked to do these HITs, after which her score is obtained (e.g., 90% of the questions are correctly answered). 3. The average score of these workers is taken as p i .

As we will show in our experiments (Section 5), a small frac-tion of questions extracted from each cluster suffices to derive an accurate value of p i .

It is worth noting that the information of  X  X IT clusters X  may be provided by requesters themselves. For instance, requesters in AMT often classify HITs according to features like themes, cate-gories, time, and locations, in order to assist a worker to choose HITs. On 28 October 2012, among the top-70 heaviest requesters, 45% of them provided more than one cluster of HITs, and 17% of them separated their HITs into four or more clusters. These clusters could be used to find p i , by using the method described above.
MCQ quality. Now, let  X  i ( k ) , a real-valued function, be the quality function of t i , which describes the  X  X xpected quality score X  of t i after it has been performed by k workers. The exact form of  X  ( k ) was recently proposed in [8], as  X  i ( k ) =
Figure 2: Monotonocity of  X  ( k ) ( p i = 0.6).
C l k is the notation for the binomial coefficient k l . Note that  X  ( k ) is not defined for any positive even number k . When k (the plurality) is zero,  X  i (0) has the lowest value of zero, since no work-ers have worked on t i . When p i = 1 , we have full confidence about the workers, and  X  i ( k ) attains the highest value of one. For other cases,  X  i ( k ) is given by: Essentially, it is the probability that, out of the k workers answer-ing t i , l  X  k 2 of them give a correct answer. In other words, more than half of the workers provide the correct answer for this question [8]. We require k to be an odd number, so that an unam-biguous decision can be made out of the k answers provided. This requirement is also adopted by previous works (e.g., [3, 8]). We remark that BQ is a special case of MCQ (where only two possible answers are provided), and so  X  i ( k ) can also be applied to BQ.
Before we go on, we would like to discuss the effect of p  X  ( k ) . As we will elaborate in Section 3.3,  X  i ( k ) increases mono-tonically with k if and only if p i  X  0 . 5 . Intuitively, if it is more likely that a worker gives a correct answer than an incorrect one, then the quality of t i increases monotonically with the number of answers obtained. In our experiments with real human workers, we observed that their accuracies were generally well above 0.5. We thus assume that p i  X  0 . 5 in our subsequent discussions. 4 We next define the total and average qualities of T .

Definition 1. Let ~ k = ( k 1 ,k 2 ,...,k n ) , where k i of t i . The total quality of T , or Q t ( T , ~ k ) , is: Definition 2. The average quality of T , or Q a ( T , ~ k ) , is: We will explain later how these two quality measures can be opti-mized by assigning pluralities.
We now discuss two interesting properties of MCQ X  X  quality function. As we will show later, these properties are useful in de-veloping plurality assignment algorithms.
If p i &lt; 0 . 5 , the plurality k i should be set to zero, since  X  not improve with the number of workers. (1) Monotonicity. We first observe that the MCQ quality, or  X  ( k ) , increases with plurality k . An example is illustrated in Fig-ure 2, which plots  X  i ( k ) over different (odd) values of k , for p 0 . 6 . We see that when one worker is assigned to t i ,  X  with three workers, the quality increases to  X  i (3) = 0.648. Theo-rem 1 formalizes this phenonmenon.

T HEOREM 1.  X  i ( k ) increases monotonically with k , if and only if p i  X  [0 . 5 , 1] .
 The proof can be found in [14]. We also note that this theorem is consistent with the observation that the more workers are assigned to an MCQ, the more confident we are about its result. In fact, the AMT Best Practices Guide also suggests a HIT to be assigned higher plurality, in order to yield better results [1]. (2) Diminishing Return (DR). The second property of  X  i ( k ) is that its rate of change drops with k . As an example, Figure 3 shows that the difference between  X  i ( k + 2) and  X  i ( k ) decreases with k , for p = 0 . 6 . This concept, also known as Diminishing Return (DR) in economics literature [12], can be formalized by marginal return :
Definition 3. The marginal return of  X  i ( k ) , denoted by  X   X  is: where  X  k = 1 if k = 0 ; otherwise,  X  k = 2 .
 Essentially,  X   X  i ( k ) describes the rate of change of  X  that  X   X  i ( k )  X  0 , since its nominator is always non-negative due to Theorem 1. We now present Theorem 2, which describes the DR property of  X  i ( k ) .

T HEOREM 2.  X   X  i ( k ) decreases monotonically with plurality k , if and only if p i  X  [0 . 5 , 1] .
 To prove this theorem, we express  X   X  i ( k + 2) and  X   X  i ing Equation 2. Then we show that  X   X  i ( k + 2)  X   X   X  i detailed proof can be found in [14]. Intuitively, the higher the value of k , the smaller is the change of  X  i ( k ) . These two properties can be useful to develop efficient algorithms to solve the plurality as-signment problem.
Let B be the budget of a requester. That is, a maximum of B cost units is awarded to workers upon successful completion of the MCQs in T . Our goal is to solve the plurality assignment prob-lem (denoted by P ( B, T ) ), as defined below.
Our aim is to find ~ k = { k 1 ,k 2 ,...,k n } such that Q is maximized. Since n is constant, optimizing Q t is the same as optimizing Q a (c.f., Definitions 1 and 2). For simplicity, we use Q as our objective function (Equation 7). 5 Also notice that the above problem has two constraints. First, since each t i is associated with a cost of k i c i , the total cost of all the MCQs in T must not exceed B (Equation 8). Second, k i has to be a positive odd number or 0 (Equation 9) according to the domain of  X  i ( k ) .
We use Q a as an effective measure in our experiments. In this section we present several plurality assignment algorithms. We first discuss two heuristics in Section 4.1. We then present an optimal solution and its fast version in Sections 4.2 and 4.3, respec-tively. We propose a greedy algorithm in Section 4.4, and discuss how to enhance its performance in Section 4.5.
Consider a requester with a budget B and n MCQs. Without any information about the workers X  accuracies, how would the re-quester assign her budget? Here we consider two heuristic plurality assignment methods which might be adopted by an  X  X ninformed X  requester. 1. Random : We arbitrarily pick an MCQ to increase its plurality and repeat until the budget is exhausted. Specifically, let b be the currently available budget. Initially, b is equal to B , and all k equal to zero. We execute two steps: (Step 1) We randomly pick an MCQ t j such that b is large enough to increase t j  X  X  plurality to the next odd number. (Step 2) We decrease b by c j (if k 2 c j (if k j &gt; 1 ). These two steps are repeated until no MCQ can be picked in Step 1. 2. Even : We divide the budget evenly across all the MCQs. Specif-ically, let b be the amount of budget allocated for each MCQ, we have b = B/n . Since the cost consumed for each MCQ should not exceed b , i.e., k i c i  X  b , we set k i to be the largest odd number that does not exceed b b c
These heuristics, which ignore the worker X  X  performance infor-mation ( p i  X  X ), may not yield an optimal plurality assignment. For example, if p i is high, then just a few workers are enough to yield a high quality for MCQ t i . However, these heuristics may still assign an excessively large k i . We thus treat these two heuristics as our baseline algorithms. We next investigate better solutions. Let us now present an optimal solution, called DP , to solve PAP. We observe that problem P ( B, T ) exhibits optimal substructure , which enables dynamic programming. The detailed proof of this property can be found in Appendix A. Now, let T l be a subset of T , which contains the MCQs with l smallest indices, i.e., T { t 1 ,...,t l } . Let Q ( B,n ) be the optimal total quality of T for P ( B, T ) , and Q ( b,l )(0  X  b  X  B, 0  X  l  X  n ) be the optimal total quality of T l for the subproblem P ( b, T l ) . Since the objective function of P ( B, T ) is P n i =1  X  i ( k i ) (Equation 3), we can compute Q ( B,n ) recursively:
To compute Q ( b,l ) , we enumerate all possible values of k find the optimal one. Let k  X  l be the value of k l that yields the max-imal value of Q ( b,l ) . Then, k  X  l is also the optimal plurality for t which is guaranteed by the optimal substructure of P ( b, T details of DP can be found in Appendix B.

Discussions. Let M = min i =1 ,...,n { c i } . Let F ( t time for computing  X  i ( k i ) . Then, the complexities of F and DP are quently ( nB 2 M times), if any of n , B , or F is large, DP will be very slow. In our experiments, it takes a few hours to compute the plu-rality of a set of 60,000 HITs! An efficient plurality assignment al-gorithm is therefore essential for large crowdsourcing systems that manage numerous requesters, or when the plurality assignment has to be recomputed frequently to reflect any changes in worker statis-tics and HITs. Next, we study how to reduce the time needed to compute  X  i ( k ) .
We discuss an alternative way to compute an MCQ X  X  quality function,  X  i ( k ) . This result can be used to develop a faster ver-sion of DP , which we call DP-inc . The mathematical derivations of the equations shown in this section can be found in our technical report [14].

Our main idea is to express  X  i ( k ) in terms of  X  i ( k k is some number smaller than k . Particularly, let d i ( k ) be the difference between  X  i ( k 00 ) and  X  i ( k ) , where k 00 immediately larger than k . In other words, We can then rewrite  X  i ( k ) in terms of d i ( k 0 ) : Further, we have: For k &gt; 1 , given the value of d i ( k  X  2) , we can obtain d Equation 13, in constant times. We can then  X  X ncrementally X  obtain  X  ( k + 2) , since  X  i ( k + 2) =  X  i ( k ) + d i ( k ) by Equation 11.
In DP , the possible plurality values for an MCQ, represented by k in Equation 10, are enumerated in ascending order. Hence, we can use the solution above to compute  X  i ( k ) based on the d values. The corresponding complexity of computing  X  i ( k ) , or F , drops from O ( B M ) to O (1) , and the running time of DP is improved to O ( nB 2 M ) . We name this modified version of DP as DP-inc .
Our Greedy algorithm adopts the framework of the greedy al-gorithm in [13], which was developed to solve the 0-1 knapsack problem. In each iteration, we select the  X  X est X  MCQ according to some measures, and increase its plurality accordingly. These steps are repeated, until the budget B is exhausted. The criterion that we use to choose the MCQ is the marginal gain , as defined below:
Definition 4. The marginal gain of t i , denoted by h i ( k ) , is: In essence, h i ( k ) is the marginal return (Definition 3) per unit cost after k workers have been assigned to t i .

Details. The implementation of Greedy is shown in Algo-rithm 1. We let b be the currently available budget, which is ini-tially equal to B (Line 3). We use a priority queue structure, Q , to store the IDs of MCQs. An MCQ with a larger marginal gain has a higher priority in Q . We also initialize the plurality of every MCQ to zero (Lines 4 to 6).

Next, in every iteration, we select an MCQ with the largest marginal Specifically, the MCQ t i with the largest h i ( k i ) in Q is popped (Line 8). If the budget b is sufficient for h i ( k i ) to be increased by  X  k , we update the values of k i and b (Line 12). We then recompute Algorithm 1 Greedy 1: 2: 3: 4: 7: the new h i ( k i ) value, and push t i to Q (Lines 13 to 14). This pro-cess is repeated until (1) the budget is exhausted or (2) Q is empty (Line 7). In Line 15, Greedy returns ~ k as the final answer.
Complexity. Notice that h i ( k i ) is a function of d d ( k ) can be incrementally computed in O (1) times (using Equa-tion 13). In Greedy , we compute h i ( k i ) in ascending order of k therefore, the time complexity of computing each h i ( k i Updating Q costs O (log n ) time, and the maximum value of k cannot exceed B/M . Hence, the time and space complexities for Greedy are respectively O (( B M + n ) log n ) and O ( n ) .
Accuracy . We next sketch the proof that Greedy is a 0.5-approximation algorithm. First, we show that our plurality opti-mization problem P is just a variant of the 0-1 knapsack problem P 0 [13]. (Specifically, P is equivalent to P 0 with some additional constraint.) Second, by using the monotonicity and DR properties of  X  i ( k ) , we prove that the optimal solutions of P and P the same objective value. Third, we consider KPGreedy , which solves P 0 with a worst-case effectiveness ratio of 0 . 5 [13]. We show that Greedy is the same as KPGreedy , and is thus a 0.5-approximation algorithm. For more details, please consult [14]. In practice, KPGreedy has a close-to-optimal effectiveness [18]. This can also be said for Greedy . In fact, our experiments also show that the accuracy of Greedy is close to that of DP .
For the algorithms presented so far (i.e., DP , DP-inc , and Greedy ), there is one thing in common: the plurality k i of each MCQ t determined independently . Computing k i involves a cost; for DP and DP-inc , their running times can be large when the budget B is big. If n , the number of MCQs under consideration, is large too, then the execution times of our algorithms could be substantial. Unfortunately, B and n can be huge in reality: the MCQs submit-ted by a requester in AMT is often in thousands. For example, we extracted over 60,000 MCQs from AMT for our experiments. Even with a small budget for these MCQs, say B = 60,000 (i.e., each HIT has an average plurality of one), DP-inc does not terminate within ten hours! Hence, it is important to improve the performance of our solutions in order to handle large HIT sets.

Our idea for enhancing the algorithms is not complicated. Also, consider two MCQs, t i and t j , which have the same cost and worker accuracy. If budget allows, they must be allocated the same plural-ity by DP , DP-inc , and Greedy . This follows from the fact that these algorithms only consider cost and accuracy of MCQs, and so they cannot treat t i and t j differently. If we have determined the plurality k i , then k j can be immediately deduced to be equal to k This can be faster than computing k i and k j independently.
Based on this intuition, we first partition the set of MCQs into groups ; the MCQs that belong to each group possess the same cost and accuracy. For each group, we select a  X  X epresentative MCQ X  and evaluate its plurality. Given a sufficient budget size, this plural-ity is assigned to other MCQs in the same group. This is faster than computing the plurality of every MCQ in the group individually.
Due to the limitation of space, we do not show all the details of how group information is incorporated in our algorithms. An im-plementation issue that we want to discuss is that the given budget may not be enough to enable all the MCQs in the same group to be assigned the same plurality. Let us consider Line 12 of Greedy (Algorithm 1), which increases k i by  X  k if the remaining budget, b , is sufficient (Line 11). In the  X  X roup-based version X  of Greedy , we treat t i as the representative of a group g . In Line 11, we calcu-late the maximum number m of MCQs that can have their plurality increased by  X  k with budget b . In Line 12, we arbitrarily select m MCQs from g , and increase their plurality values accordingly. Then g is disregarded, since we can no longer increase the plurality of the MCQs in g . More details about how we consider group infor-mation in our algorithms can be found in our technical report [14].
We also remark that the MCQ set used in our experiments con-sists of only a few  X  X ig X  groups. That is to say, each group contains a large number of HITs. Consequently, the algorithms that make use of group information significantly outperform those that handle MCQs individually. Next we explain these results in more detail.
We now present the experiment results of DP , DP-inc , Greedy , and the baseline heuristics, Even and Random . Results on syn-thetic data are reported in Section 5.1. We also examine our solu-tions on the data collected from our system prototype (Section 5.2).
In this section, DP , DP-inc , and Greedy refer to their variants that consider the grouping of MCQs discussed in Section 4.5. We use DP-NG , DP-inc-NG , and Greedy-NG to denote the versions of algorithms that manage MCQs independently (i.e., no grouping). In the sequel, we focus on DP , DP-inc , and Greedy . 6 Each data point is the average result of 100 runs. All our experiments are implemented with C++ on a 64 -bit Ubuntu system with 8 G memory and an Intel i 5 processor. (a) Experiment setup. We use the MCQs provided by an AMT requester called  X  X rowdSource X  on 28 October 2012 as the basis of the synthetic dataset. This requester submits a total of 67 , 075 MCQs. These MCQs have been classified into N groups by the requester, where N = 12 . Recall that every MCQ in the same group has the same cost and worker accuracy. For convenience, we label each group as g j , where j = 1 ,...,N . Each g MCQs. For each MCQ in g j , its cost and accuracy are  X  j respectively. In these experiments,  X  j is randomly selected from the range [0 . 5 , 1] . We use the average quality, Q a tion 2), to measure the effectiveness of our algorithms. The costs of MCQs vary from $0 . 08 to $0 . 24 , with an average of $0 . 12 . The requester X  X  budget is $20 K by default. If plurality is not consid-ered, this budget allows each MCQ to be worked by an average of 3 workers. Table 2 shows the detailed setup of our experiments. (b) Effect of budget B . Figure 4(a) shows the effectiveness of DP-inc , Greedy , Even and Random under different values of
The use of groups does not change the effectiveness of the algo-rithms being considered. Moreover, they can be much faster than if groups are not used. Hence, we use the  X  X roup-based algorithms X  as our default. are true values extracted from AMT). budget B . Since DP-inc , a faster version of DP , has the same effectiveness as DP , we do not show the results of DP here. Also, as shown later in our performance experiments, DP-inc runs very slowly under a large budget, and so we only present its effectiveness for B  X  $30 K . For all the algorithms shown, Q a increases with B . This means that in general a larger budget gives better quality result. However, the increase rate of Q a drops with B ; increasing its value constantly does not give a significant improvement of Q Hence, it is not necessary to use a very large value of B to obtain high-quality results.
 We also see that DP-inc attains the highest effectiveness, and Greedy comes very close to it. On average, the difference be-tween these two algorithms is 1% or less. At B = $20 K , DP-inc and Greedy both have Q a = 81% , which is 5% higher than that of Even ( 77% ) and 20% higher than that of Random ( 64% ). Un-der a smaller budget (say, $10 K ), the gap is even wider: the ef-fectiveness of DP-inc and Greedy are about 76% , while that of Even and Random is both less than 50% . This difference is due to the fact that neither Even nor Random considers accuracy in-formation in setting plurality values. As a result, MCQs with lower accuracy may not receive enough number of answers, while the highly accurate MCQs may be associated with unnecessarily high plurality values. Since DP-inc considers group accuracy informa-tion and produces an optimal solution, its effectiveness is the best. The effectiveness of Greedy is almost as good as DP-inc . Also, although Even performs better than Random in most of cases, it performs the worst under a small budget (of 10 K or less). In Even , expensive MCQs may have a zero plurality, rendering poor effec-tiveness. Another point is that both DP-inc and Greedy need a budget of $20 K to achieve Q a = 80% , while Even and Random will respectively use more than $30 K and $45 K to achieve the same effectiveness. Hence, if we want to attain a specified value of Q a , DP-inc and Greedy only needs about 67% of the cost of Even and 44% of the cost of Random . (c) Effect of number of HITs n . Figure 4(b) examines the ef-fect of the number of HITs ( n ), obtained by varying the number of groups. We observe that the effectiveness of all the algorithms drop with the increase of n . Under a fixed budget, with more MCQs, the plurality assigned to each MCQ tends to be smaller. Hence, Q a drops. Another observation is that the quality drop rates of DP-inc and Greedy are both slower than those of Even and Random . When the number of MCQs is small, all the algo-rithms concerned have few choices, and so their effectiveness val-ues are similar. When the number of MCQs increases, DP-inc and Greedy allocate plurality to MCQs with higher accuracy and lower cost, and so they perform better than both Even and Random . (d) Effect of worker X  X  accuracy  X  j . Figures 4(c) and (d) study the impact of accuracy of an MCQ of each group (i.e.,  X  j algorithms X  effectiveness. For Figure 4(c),  X  j is uniformly dis-tributed between [ u  X  0 . 05 ,u + 0 . 05] , and we vary the value of u . Observe that with a higher MCQ accuracy, the effectiveness of all the algorithms increases; in other words, better MCQ results can be obtained. On average, DP-inc and Greedy are 5% and 18% more effective than Even and Random , respectively.

Figure 4(d) examines the effect of the range of  X  j on Q a the values of  X  j are uniformly distributed between [0 . 75  X  ] . When r increases, the difference in  X  j values among different groups increases. Notice that the effectiveness of all the algorithms is stable over a wide range of r . Also, DP-inc and Greedy are 5% and 18% better than Even and Random respectively. (e) Performance. Figure 4(e) illustrates the performance of the plurality-setting algorithms under different budget size B . Since DP and DP-inc are very slow, we only show their results for small values of B . We see that DP-inc is about 100 times faster than DP . This is because DP-inc computes the MCQ quality function more efficiently than DP . Unfortunately, DP-inc is still very slow com-pared to heuristics (i.e., Random and Even ) and our approximate solution (i.e., Greedy ). At B = 20 K , for example, DP-inc , which completes in 6000s, is 1,000 times slower than Random , which finishes in less than 1s, and is four orders of magnitude poorer than Even and Greedy , whose running times are less than 10ms. Moreover, while the execution time of DP-inc increases sharply with B , the performance of other solutions is relatively stable. We conclude that the performance of DP and DP-inc is significantly inferior to other solutions.

Grouping. On the same graph, we display Greedy-NG , which does not use any grouping information. We see that it is 20 times slower than Greedy . Recall that the MCQs tested here are parti-tioned into 12 groups (Table 2). Also, Greedy-NG has to compute the plurality independently for every MCQ. Since the number of MCQs is tremendous ( 67 K ), its performance is much worse than Greedy , which evaluates the plurality for only 12 group represen-tatives (and assigns these values to other MCQs). We have also tested DP-NG and DP-inc-NG . Since they are extremely ineffi-cient, we do not show all their results here. Under a mild budget of 5 K (i.e., each MCQ has an average plurality of one), they cannot finish in 10 hours. By using group information, DP and DP-inc complete in 4 hours and 10 minutes respectively. Hence, the use of grouping information significantly improves the performance of DP , DP-inc , and Greedy .

Figure 4(f) shows the performance of the plurality-setting algo-rithms under different number of groups N . Similar to (e), DP-inc performs much worse than other solutions. For example, at N = 8 , DP-inc needs 3,500 seconds to complete, while the finishing time of Greedy is only 10ms. Moreover, while the execution time of DP-inc increases sharply with N , the completion time of other solutions increases much more slowly with N . We skip the results for DP-NG and DP-inc-NG here, since they are extremely slow. (f) Summary. By assigning plurality to MCQs based on their ac-curacy and cost information, DP-inc and Greedy achieve higher effectiveness than simple heuristics (e.g., Even and Random ). The effectiveness of DP-inc and Greedy is stable over a wide range of worker X  X  accuracy values. The use of grouping techniques sig-nificantly improves the performance of these algorithms without sacrificing effectiveness. The best algorithm is the Greedy ; while its effectiveness is close to optimal, it is efficient even under large values of B and n . (a) Experiment setup. We have created 100 MCQs for collect-ing the sentiment information of a comment. These 100 comments are extracted from ten popular Youtube videos. For each MCQ, a worker is asked to select an answer from { positive , neutral , nega-tive }. As discussed in [4], a longer comment is usually harder to interpret. We thus separate these MCQs into three groups of dif-ferent lengths. We also associate more incentives to groups with longer comments, as shown in Table 3. We have implemented a web application for workers to perform these MCQs (Figure 5). The worker can view the comments and submit her choice. Help messages, displayed at the bottom of the screen, assist the worker to use the system. We have employed 25 graduate students, whose second language is English, to work on these MCQs.

We compute the worker X  X  accuracy values based on their input, based on the method described in Section 3.2. First, we manually check each comment to generate our ground truth. Then, we ran-domly sample s = 10% MCQs from each group, and ask all our workers to do these selected questions. (For convenience, we call s the sampling rate .) By comparing their answers with the ground truth, we can estimate their performance of doing the MCQs in each group. Specifically, let S be the set of sampled MCQs, and W be the set of workers. For each worker w  X  W , let A ( w ) be the set of answers provided by w for questions in S , and a be an answer of A ( w ) . We then compute the effectiveness of each worker p ( w ) for questions in S , where The accuracy of an MCQ for each group is estimated as the average values. We can see that the longer the comments, the lower is the accuracy.

In the sequel, we present our results on the effectiveness of the plurality assignment algorithms, as well as the actual amount of time needed by workers to answer the MCQs. We have also exam-ined the performance of our algorithms. Their relative performance is similar to that of synthetic data. Due to space constraints, we do not report their results here. (b) Effect of budget B . Figure 6 (a) shows the average qual-ity, Q a , over different values of B . We can see that the trend of ure 6(a), where DP-inc and Greedy are always better than Even and Random . At B = $20 , DP-inc and Greedy are 26 . 3% and 42 . 1% more effective than Even and Random , respectively. Moreover, to achieve Q a = 80% , DP-inc and Greedy require a budget of $40 , which is 30% and 55% smaller compared to the budget needed by Even and Random , respectively. (c) Effect of sampling rate s . Figures 6(b) shows the effect of sampling rate on Q a . Here, each line represents the effectiveness of Greedy based on a fraction s of the MCQs. Observe that s does not have significant influence on Q a . For example, at B = $30 , the quality difference between the two cases { s = 10% , s = 100% } is only 0 . 49% . This shows that a low sampling rate (e.g., 10% ) is sufficient to yield high effectiveness.

Let us further investigate why the values of s tested do not have a big impact at Q a . We first compute the estimated accuracy val-ues ~ X  = {  X  s 1 , X  s 2 ,..., X  s N } for G = { g 1 ,g 2 ,...,g denotes the sampling rate used for estimating  X  i .

We then define the sampling error as:
Figure 6(c) shows err ( ~ X  ) over different values of s . We can see that err ( ~ X  ) is quite stable (less than 4%) when s  X  20% . Hence, the effectiveness of Greedy is not very sensitive to s . (d) Real quality. In this experiment, we analyze the answers obtained from the workers, after the number of answers provided for each MCQ has reached the assigned plurality. We would like to compute the goodness of these results, which we call real qual-ity , and compare it with Q a . Specifically, for every MCQ t plurality k i , we obtain a set A ( t i ) of k i worker X  X  answers for t Then, we use half-voting to decide the result of this MCQ. Let the Q , as: Essentially, Q r is the fraction of the MCQs, whose half-voting re-sult is same as the ground truth.

Figure 6(d) shows the effect of budget on the real quality. No-tice that the graph is highly similar to Figure 6(a) (which plots Q against B ). This indicates that Q a reflects the number of MCQs correctly answered. To study the relationship between Q a we plot Figure 6(e), where each point ( x i ,y i ) corresponds to a plu-rality assignment ~ k based on some budget and plurality assignment algorithm. Specifically, x i = Q a ( T , ~ k ) and y i = Q observe a strong correlation between Q a and Q r . We further com-pute the correlation [5] of the two sets of { x i } and { y find that the correlation is over 99 . 8% . Hence, in this experiment, our average quality measure is a good indicator of the correctness of workers X  results. (e) Completion time. We next analyze the average completion time of workers to work on an MCQ. Specifically, we record the time a particular worker uses to give an answer to MCQ t i is defined as the completion time of this worker on t summarizes the average completion time for MCQs in each group. We observe that for a longer comment, workers may consume a longer time to read it and decide its sentiment. Similar to the ex-periment on real quality, for every MCQ t i with plurality k obtain k i workers, and also their corresponding completion time. We then sum them up to get the total completion time for all these k workers on t i , denoted by T i . The average completion time of T , denoted by T T , is Figure 6(f) shows the effect of budget on T T . We observe that T of all algorithms increases with budget since more plurality is allo-cated to each MCQ as budget increases. Besides, T T for DP-inc (21.52s) and Greedy (21.89s) are both about 5% higher than that of Random (20.5s), and 9% higher than that of Even (19.88s). This is because in this real dataset, MCQs with lower accuracy re-quire more completion time, and both DP-inc and Greedy allo-cate higher plurality to MCQs with lower accuracy. Consequently, the average completion time of DP-inc and Greedy are slightly higher than others. This increase in completion time, however, is justified, because the answers produced by DP-inc and Greedy have a higher quality than those of Even and Random .
We have focused our discussion so far on MCQ, which is the most popular HIT type on AMT, as an illustration of our solution framework. We remark that other kinds of HITs can also be sim-ilarly handled. In this section we further illustrate our framework by briefly discussing how plurality assignment can be done on two other kinds of HITs, namely, Enumeration Query (EQ) and Tagging Query (TQ).
 First, we note that the key component of our framework (see Figure 1) is the quality manager, which consists of a quality esti-mator and a plurality assignment algorithm . Basically, the job of the quality estimator is to derive the quality function  X  t after the HIT has received k answers. For MCQ, we model  X  by Equation 1. Note that this model has one parameter p job of the quality estimator is thus to estimate the model parameter p , which is done through a sampling process (Section 3.2). Given the quality functions  X  i ( k ) of all the HITs as input, the plurality assignment algorithm outputs an assignment. The choice of the as-signment algorithm depends on the properties of the quality func-tion  X  DP is applicable for all quality functions, while Greedy is applicable as long as the quality function observes monotonicity and diminishing return. So, for any other types of HITs, to apply our plurality assignment framework, we only need (1) a model of the quality function  X  i ( k ) and a way to estimate the model X  X  pa-rameters (such as p i for MCQ) and (2) analyze the quality function and see if it satisfies the two conditions required by Greedy (and if so, we apply Greedy ; otherwise, we apply DP ). Let us illustrate this process using two other HIT types, EQ and TQ.

The objective of an EQ is to obtain the complete set of distinct elements for a set query. (For example,  X  Name a state in the US . X  with the purpose of getting all the states X  names.) From [19], we can model the quality function  X  i ( k ) by  X  f 0 1  X  1  X  The variables  X  f 0 and  X  C are two model parameters that captures the information of the total number of distinct answers, and the per-centage of distinct answers collected so far, respectively, and k is the number of answers given by workers so far. This quality function estimates the number of distinct answers obtained by k workers. In [19], it is shown how the model parameters can be esti-mated by statistical methods. It is easy to verify that the above qual-ity function satisfies monotonicity and diminishing return. Hence, PAP on EQ can be efficiently solved by our Greedy algorithm.
As another example, we consider Tagging Query (TQ). The ob-jective of a TQ is to obtain keywords (or tags) that best describe an object (such as an image or a web page). TQ has been recently studied in [22], which proposes a quality metric for measuring the quality of tags collected through workers X  answers. Although not yet proven, the empirical results presented in [22] indicates that the quality metric proposed exhibits a power-law relationship with the number of workers X  answers. We can thus model our quality func-tion  X  i ( k ) for TQ using some power-law models. Since power-law models satisfy monotonicity and diminishing return, Greedy is again applicable to TQ. As a future work, we are studying how to effectively estimate the power-law model parameters for TQ.
We study the problem of setting plurality for HITs in crowd-sourcing environments. We develop a solution that enables an opti-mal assignment of plurality for MCQs under a limited budget. We show that the quality of MCQs demonstrate monotonicity and di-minishing return. We use these properties to develop effective and efficient plurality algorithms. We have performed extensive exper-iments on real and synthetic data. We conclude that the greedy algorithm is highly effective and efficient. We briefly discuss how our framework can be extended to support other kinds of HITs (i.e., EQ and TQ). We plan to study these extensions in more detail. This research is partly supported by Hong Kong Research Grants Council grant HKU711309E and HKU712712E. We would like to thank the anonymous reviewers for their insightful comments. [1] Amazon Web Services LLC. Amazon Mechanical Turk: Best [2] D. W. Barowy et al. Automan: A platform for integrating [3] C. Cao and J. She et al. Whom to ask? Jury selection for [4] E. Dale and J. S. Chall. A formula for predicting readability. [5] S. Dowdy. Statistics for Research . Wiley, 1983. [6] M. J. Franklin et al. CrowdDB: answering queries with [7] S. Guo and A. Parameswaran et al. So who won? dynamic [8] X. Liu, M. Lu, B. Ooi, Y. Shen, S. Wu, and M. Zhang. [9] A. Marcus, E. Wu, D. Karger, S. Madden, and R. Miller. [10] A. Marcus, E. Wu, S. Madden, and R. Miller. Crowdsourced [11] A. Marcus and E. Wu et al. Demonstration of qurk: A query [12] A. Marshall. 1920. Principles of economics , 8, 1890. [13] S. Martello and P. Toth. Knapsack problems: algorithms and [14] L. Mo, R. Cheng, B. Kao, and X. Yang et al. Optimizing [15] B. Mozafari et al. Active learning for crowd-sourced [16] A. Parameswaran and H. Garcia-Molina et al. Crowdscreen: [17] A. Parameswaran and A. Sarma et al. Human-assisted graph [18] T. Saaty. Mathematical methods of operations research . [19] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar. [20] J. Wang, T. Kraska, M. Franklin, and J. Feng. Crowder: [21] S. Whang et al. Question selection for crowd entity [22] X. Yang et al. On incentive-based tagging. ICDE , 2013.
In this section, we show that P ( B, T ) exhibits optimal sub-structure , and so Bellman X  X  optimality principle is hold. We let ( Q,K ) to be the optimal solution to P ( B, T ) , where Q is the op-timal value and K = { k i | i = 1 , 2 ,...,n } is the plurality of k to t i in the optimal solution. Consider the subproblem P ( B  X  k c n , T n  X  1 ) . We claim that ( Q  X   X  n ( k n ) ,K  X  X  k mal solution to this subproblem. Suppose that this is not true. Let ( Q 0 ,K 0 ) , where K 0 = { k 0 1 ,...,k 0 n  X  1 } , be the optimal solution to P ( B  X  k n c n , T n  X  1 ) , and Q 0 &gt; Q  X   X  n ( k n ) . Let ( Q ( Q 0 +  X  n ( k n ) , K 0  X  { k n } ) . Since P n  X  1 ( Q 00 ,K 00 ) is a feasible solution to P ( B, T ) . However, Q Q 0 +  X  n ( k n ) &gt; Q , which violates the assumption that Q is the optimal value to P ( B, T ) . Therefore, ( Q  X   X  n ( k n must be the optimal solution to the subproblem.
 Algorithm 2 DP 1: 2: 3: 4:
Our bottom-up, dynamic programming algorithm is shown in Al-gorithm 2. Q [ b,l ] and y [ b,l ] are both B  X  n arrays that store the optimal value of P ( b, T l ) and the optimal plurality k subproblem. So the optimal value of P ( B, T ) is Q [ B,n ] .
We first deal with the boundary case where l = 0 . Then for each subproblem P ( b, T l ) , we enumerate all the possible settings to k and find out the one at which the optimal value is achieved. Q [ b,l ] and y [ b,l ] are updated accordingly(Step 4 to 10). Thus, the optimal value Q  X  = Q [ B,n ] .
 Step 11 to 13 are used to recover the optimal plurality assignment K  X  based on optimal plurality of k l for each subproblem.
As we can see, all subproblems are scanned once during the pro-cess and each time all possible values of k l are checked once from 0 to calculating the specific value of the quality function requires O ( F ) time. Therefore, the time and space complexities of this algorithm are O ( nB 2 F M ) and O ( nB ) , respectively.
