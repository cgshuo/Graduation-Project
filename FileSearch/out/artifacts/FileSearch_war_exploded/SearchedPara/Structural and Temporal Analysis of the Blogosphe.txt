 The blogosphere has unique structural and temporal prop-erties since blogs are typically used as communication medi a among human individuals. In this paper, we propose a novel technique that captures the structure and temporal dynam-ics of blog communities. In our framework, a community is a set of blogs that communicate with each other triggered by some events (such as a news article). The community is represented by its structure and temporal dynamics: a community graph indicates how often one blog communi-cates with another, and a community intensity indicates the activity level of the community that varies over time. Our method, community factorization , extracts such com-munities from the blogosphere, where the communication among blogs is observed as a set of subgraphs (i.e., threads of discussion). This community extraction is formulated as a factorization problem in the framework of constrained op-timization, in which the objective is to best explain the ob-served interactions in the blogosphere over time. We furthe r provide a scalable algorithm for computing solutions to the constrained optimization problems. Extensive experiment al studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communi-ties that are not detectable by traditional methods. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Experimentation, Measurement, Theory Blog, Blogosphere, Community Factorization, Non-negativ e Matrix Factorization, Regularization, Iterative Search
Blog is self-publishing media on the Web that has been growing quickly and becoming more and more important. At the time of this writing, Technorati, a well-known blog search engine, is tracking 57 million blogs, with 1.3 millio n new entries generated everyday. Furthermore, those num-The blogosphere , the universe of blogs, provides a lot of ap-plications in the areas of economics and finance (e.g., virus marketing and opinion extraction), social relations (e.g. , on-line friendships), politics (e.g., as media outlets for pol itical candidates), and so on. The blogosphere raises interesting research challenges since blogs have various distinct feat ures compared with general Web pages.

A key difference between the blogosphere and the Web is the lifetime of their contents (i.e., pages and links). Note that a blog is typically used as a tool for communication and it consists of a temporal sequence of entries. Driven by an event (such as a news), blogs publish their entries that refer to each other. Most of such entries quickly become obsolete and will never be referred to by later entries. Thus , links among blog entries have significant temporal locality , resulting in a number of dense subgraphs (each of which is a thread of discussion) with a short lifetime. On the other hand, the content of the Web has longer lifetime, and it is common that a new page refers to a very old page (such as an authoritative page). When a new page appears with links to an old page in a subgraph, the subgraph grows. Thus, the structure of the subgraph gradually and incrementally changes over time.

Given this unique structure of the blogosphere, the anal-ysis of temporal dynamics in blog communities should be different from the traditional web analysis. In the general web analysis, a dense subgraph of web pages is often consid-ered as a community, and its temporal dynamics is captured by observing how the subgraph grows over time. However, in the blogosphere, a dense subgraph grows only within a short time span. The traditional analysis can only capture dynam-ics within a short-term activity (such as a single thread of discussion).

Alternatively, we may accumulate such links over a very long period, generating a graph of blog that represents how much they communicate with each other (i.e., a social net-work of blogs). Here, we can apply similar analysis to the Web pages since the graph is relatively static and changes incrementally. However, it misses more detailed temporal behavior. For instance, assume that there are two commu-nities, a politics community and an economics community, and that one community becomes inactive while the other community becomes active during the observation period due to change of trends in the real world. For example, the politics community becomes inactive after a particular po-litical issue is addressed, then people start worrying abou t its impact to economics. Some blogs are interested both in politics and economics, moving from one community to an-other. Since the two communities have overlap, they will be seen as a single community in the aggregated graph.
In this paper, we propose a novel technique, community factorization , to extract communities and their temporal dynamics in the blogosphere. Our technique identifies the longer-term graph structure (e.g. the politics community and the economic community) from a series of short-term subgraphs (i.e., threads of discussion among blogs).
In our framework, a community is a set of blogs that communicates with each other in a synchronized manner, i.e., communication is triggered by some events (such as a news article), resulting in a number of short-term subgraph s. A community has its structure, called a community graph , which represents how much one blog communicates with an-other. By observing a number of short-term subgraphs, we estimate the structure of a community graph. A commu-nity also has its temporal aspect: It can become active and inactive over time. We introduce a community intensity to represent the activity level of a community at a particular time. Then the blogosphere at a particular time should be represented as mixture of community graphs weighted by their intensities. Our technique formulates this as a prob-lem of non-negative matrix factorization.

Our main contributions in the paper can be summarized as follows. (1) We provide a new model of community that has both (2) We pose and solve the problem of extracting communi-
Experimental results on both synthetic data and two real-life data show that our algorithm is able to detect significan t communities with temporal dynamics while many of these communities are not detectable by traditional methods on aggregated blog graphs.

The rest of the paper is organized as follows. We discuss background information and related work in Section 2. In Section 3 we present our algorithm for community factor-ization. In Section 4 we provide a computational procedure to solving the community factorization problem. We show results of experimental studies in Section 5 and give conclu -sion and future directions in Section 6.
We introduce some mathematical notations that will be used in later sections. We denote scalars by lower-case let-sors by calligraphic letters ( A , B ,. . . ). In addition, for a ma-trix A we use A ( j ) to denote the j -th column of A , and we use both a ij and ( A ) ij to represent the element at the i -th row and j -th column of A . We use R to represent the set of real numbers and R + for non-negative real numbers. A non-negative matrix is a matrix whose elements are in R + For a general matrix A we use A + to represent A  X  X  posi-tive part and A  X  to represent A  X  X  negative part. That is, A + = ( A + | A | ) / 2 and A  X  = ( | A | X  A ) / 2 where both A A  X  are non-negative matrices. For a tensor A  X  R m  X  n  X  p mally, we can say the k -th  X  X lice X  of B is a linear combina-tion of all  X  X lices X  of A where the coefficients are the k -th column of U . Finally, unless stated otherwise, all matrix (tensor) norms |||| in this paper means Frobenius norm, i.e., || A || = q P i,j a 2 ij .
Community extraction and analysis has been studied ex-tensively and is mainly studied as a graph problem. Flake et al. [7] defined communities as dense subgraphs and pro-posed algorithms for identifying communities by using a maximum flow/minimum cut framework. Ino et al. [12] pro-posed different algorithms based on ideas similar to that of Flake. These studies extract communities from a static (ag-gregated) graph and miss the details on the dynamic behav-ior about the communities.

There is also a large body of work on analyzing online social networks. Kumar et al. [13] studied the bursty evolu-tion of links among different communities in the blogosphere by using a Markov chain model. Gruhl et al. [9] proposed a generative model X  X ransmission graph X  X o model the in-formation diffusion in the blogosphere in a way similar to modeling disease-propagation in epidemic studies. Leskov ec et al. [16] studied the patterns of growth for graphs in vario us fields and proposed generators that produce graphs exhibit-ing the discovered patterns. Kumar et al. [14] analyzed the temporal dynamics of the structures of the social networks of Flickr and Yahoo! 360. Most of these studies, however, only address high-level macro statistics of the online net-works such as their size, density, degree distribution as we ll as the evolution of these macro statistics. In comparison, our algorithm can analyze the micro structure and tempo-ral trends down to the level of individual communities.
Recently, there have been many research works on min-ing temporal patterns from a collection of documents with time stamps. Mei et al. [17] proposed a model that takes time and location of blogs are into consideration and mines spatio-temporal theme patterns by using a probabilistic ap -proach. Wang et al. [23] proposed the Topic over Time (TOT) model to capture temporal topical trends by extend-ing the well-known Latent Dirichlet Allocation (LDA) model with a temporal component. However, in these studies, doc-uments in the collection are treated as separated observa-tions that are generated independently and therefore the interactions among communities are ignored.

There are some other recent studies that are closely re-tion of 3-mode product, U is multiplied from right , which is different from that defined in [5].
 lated to our work. Backstrom et al. [1] analyzed two large-scale time-resolved social networks and studied the dynami c community formation. However, in their study, the commu-nity memberships are explicitly available in the data sets. Chakrabarti et al. [2] proposed a novel evolutionary clus-tering algorithm in which the current clusters are affected by the historic cluster memberships. Our algorithm has similar effect to Chakrabarti X  X  algorithm, i.e., consisten cy of community structures over time is emphasized. How-ever, instead of discovering clusters at each time window as Chakrabarti X  X  algorithm did, our algorithm extracts com-munities that exist over all the time history. Falkowski et al. [21] proposed to cluster communities obtained in each time window in order to visualize the evolution of commu-nities and their clustering is based on a heuristic similar-ity between communities at different time windows. Qamra et al. [19] proposed a community-based temporal cluster-ing using the Chinese Restaurant Process. Unfortunately, the inference of their model requires Gibbs sampling, which is notoriously slow for large scale data. In [3], we applied the high-order singular value decomposition (HOSVD) to extract dynamic structural changes of the blogosphere for a given query keyword. There are two weak points in that work: First, only some high-level signatures (e.g., hub and authority scores) of the communities were extracted. Sec-ond, because HOSVD is an orthogonal decomposition ([5]), most community structures and temporal trends discovered in [3] contain negative values and they do not map directly to communities and temporal trends in real world.
In this section, we propose a new technique, called com-munity factorization , to extract communities from the blo-gosphere.
As discussed in Section 1, we want to extract a community as a structure that lasts for a longer time period whereas the blogosphere consists of a number of short-term subgraphs.
We define a community as a set of blogs that communi-cates with each other in a synchronized manner, i.e., com-munication among the community members is triggered by some events (such as a news article). Such communication is observed as a number of dense subgraphs, each of which is a short-term thread of discussion.

Finding a community is to identify its structure and tem-poral dynamics: a community graph that represents how much one blog communicates with another, and a commu-nity intensity that represents the activity level of the com-munity varying over time.

Since the communication in a community is observed as dense subgraphs, the structure of such subgraphs should re-flect the community graph structure. However, the structure of a single dense subgraph does not necessarily reflect the entire structure of a community: Since a member does not always participate in a thread of discussion, a community may appear as smaller pieces of disconnected subgraphs at a particular time. Moreover, it is possible that members of different communities participate in a single subgraph.
Our idea here is to represent a community structure as a combination of the observed subgraphs. Then the problem is how to find coefficients for such combination as well as the values of the community intensity over time.
Community factorization is a technique to find such pa-rameters that give the best explanation of the observed data : i.e., if community graphs are combined together with their intensities as weighting factors, the combined graph shoul d approximate the observed data. In the rest of this section, we formalize this community factorization as a factorizati on problem in the framework of constrained optimization.
In this section we formalize the above idea on community factorization.

Assume that there are n blogs b i ( i = 1 , . . . , n ) in the blogosphere. The linking activity in the blogosphere is ag-gregated as a graph structure A s  X  R n  X  n + for each time window s ( s = 1 , . . . , t ). These graphs are then stacked as a tensor A . From the observed graph A s , dense subgraphs are extracted. These graphs are also stacked as another ten-sor B . Given A and B , we will define a set of community graphs { C l } ( l = 1 , . . . , k ) and community intensities { v ( s = 1 , . . . , t ). Each graph C l  X  X  n  X  n + represents how blogs communicate to each other within the community. The in-tensity { v sl } indicates how much the l -th community con-tributes at time s . We want to find k communities such that their community graphs and intensities best explain the ob-served data A .
In our analysis, the data representing the blogosphere over time is given as a tensor A , which we call data tensor .
We define a link from b i to b j at time s ( i 6 = j ) when b publishes an entry at time s that has a hyperlink pointing to any content of b j . These links are counted for each time window (say a day) and represented as an adjacency matrix A s where ( A s ) ij is the count of links from b i to b j in the s -th time window. The adjacency matrix A s can be seen as a snapshot of activity in the blogosphere at time s , which we call a snapshot graph .

Stacking the adjacency matrices for the n time windows together, we have a 3-dimensional tensor where the first two dimensions of A are blog indices (of citing blogs and cited blogs), and the third dimension of A is time window index.
For each snapshot graph A s , we want to identify dense subgraphs where blogs communicate with each other. We can apply a graph partitioning algorithm, such as Shi X  X  nor-malized cut [20] or Newman X  X  optimal modularity [18, 24]. After removing insignificant subgraphs (e.g., a subgraph wi th only a couple of nodes), we have m s graphs B s 1 , . . . , B We call them basis subgraphs since we will define a commu-nity as a linear combination of these subgraphs.
For the t time windows, we have in total m = P t s =1 m s basis subgraphs. Stacking these basis subgraphs together, we get another 3-dimensional tensor and we call B the basis tensor . Essentially, the basis in the basis tensor capture all significant subgraphs at all tim e windows.

Since a community is established through communication, i.e., basis subgraphs, let us define a community graph C l ( l = 1 , . . . , k ) as a linear combination of the basis subgraphs: where u pl is a weight that indicates how important the p -th basis subgraph is to the l -th community. In other words, u indicates to what level a basis subgraph B p (at some time window) belongs to the l -th community. The coefficients { u pl } are parameters we will need to estimate.

At time s , we see a snapshot graph A s in the data. Note that multiple communities may have their communications at the same time since communities behave concurrently. Thus, multiple community graphs can affect the structure of A s . We introduce the community intensity v sl of the l -th community at time s such that represents the observed data A s . Then our problem is for-mulated as minimization of the following error where a tensor C l is given as
Plugging Equations (1) and (4) into Equation (3), we can show that this formulation can be posed as the optimization problem to minimize the following objective function subjected to U  X  R m  X  k + , V  X  R t  X  k + . In the above formula,  X  3 represents the 3-mode multiplication of a tensor by a matrix.

The solution of this optimization is given next.
For the objective function (5), because only the 3-mode tensor product is involved, it can be equivalently written i n a matrix form as for A  X  R n 2  X  t + and B  X  R n 2  X  m + . In the above objective function, A is obtained from the data tensor A in the fol-lowing way: each column A ( s ) of A is obtained by stacking the columns of the blog graph A s into an n 2  X  1 vector. B is obtained in a similar way from the basis tensor B . There-fore, A and B in Equation (6) are given while our task is to search for non-negative matrices U and V that minimize J 1
Once we have the solutions U and V to Equation (6), we can derive the j -th community from columns U ( j ) and V ( j ). Recall that a column U ( j ) of U is a vector of weights on the basis subgraphs. Therefore, the community graph of the j -th community is given as the 3-mode multiplication of B by U ( j ). At the same time, V ( j ) represents the change of interaction intensity within the j -th community over time.
With U and V constrained to be non-negative matrices, this optimization problem falls into the category of non-negative matrix factorization (NMF) [15].

NMF has many advantages over general matrix factoriza-tion like principal component analysis (PCA). First, NMF decomposes data into the addition of several non-negative components. Such a nature of additive only (no subtractive) makes results of NMF easy to interpret. For example, Lee et al. [15] showed that, whereas traditional PCA decomposes the pictures of human faces into eigen-faces whose physical meaning is not clear, NMF is able to decompose the pictures into individual human parts such as eyes, nose, and mouth. Second, outputs of NMF can be directly used as the results for tasks such as document clustering. In comparison, one usually has to apply further post-processing steps, such as k-means, to the outputs of general matrix factorization. As demonstrated by Xu et al. [26], such post-processing steps are nonintuitive and sometimes do not give as good perfor-mance as directly using the outputs of NMF. Third, another feature of NMF is that it does not require orthogonality on outcomes. In comparison, for PCA, U and V must both have orthogonal columns. This feature of NMF is very ap-pealing in our case because in our solution, the columns of V represent the temporal trends of different communities and it will be unreasonable to force these temporal trends to be orthogonal to each other. Similarly, a blog may have mem-bership in multiple communities and it will be unreasonable to force the columns of U to be orthogonal.
Let us contrast our formulation against direct application of traditional NMF formula. The original NMF minimizes the objective function || A  X  UV T || 2 as described in [15, 26] (which we call the standard NMF). Another form of NMF (which we call the convex NMF) with a different objective function || A  X  AUV T || 2 has been proposed independently by Xu et al. [25] and Ding et al. [6]. By re-writing the objective function for the standard NMF as || A  X  IUV T || 2 , where I represents the identity matrix, we can generalize a family of NMF as || A  X  XUV T || 2 for various X . Our community factorization chooses B instead of A or I .
 For an application in information retrieval, the standard NMF and the convex NMF are meaningful in the following ways. The data is a term-document matrix A where the j -th column of A represents the terms occurred in the j -th document and the i -th row of A represents the documents that contain the i -th term. In the standard NMF, a concept is represented by a column of U , i.e., by a term vector. In the convex NMF, a concept is represented by a column of AU , i.e., by an additive combination of the documents themselves.

Because our data A represents graph structure, neither the standard NMF nor the convex NMF is directly appli-cable to our application. If we use standard NMF, then a community comprises arbitrary links and this ignores the relationship among links (e.g., e 1 and e 2 point to the same node); if we use the convex NMF, then a community must be a combination of snapshot graphs, which is obviously incor-rect  X  a snapshot graph most likely contains communica-tions of different communities at the same time. In contrast, in our setting, the links belonging to the same basis graph must be assigned together to a community graph.

It is a common technique to incorporate prior knowl-edge into the objective function by introducing regulariza -tion terms. For this, we generalize Equation (6) by intro-ducing Tikhonov regularization terms ( cf [22]) as
J 2 = 1 2 || A  X  BUV T || 2 + 1 2  X  1 || R 1 U || 2 + 1 2 where  X  1 and  X  2 are user defined parameters. In this pa-per, we set  X  1 to be 1 and R 1 to be the identity matrix to regularize U . For V , we apply a simple piece of intuitive prior knowledge X  X he temporal trends, i.e., the column of V , should be smooth. That is, the value difference between two consecutive (in temporal order) elements in the same column of V should be small. For this purpose, we set R 2 to be a difference matrix We will later use experimental studies to demonstrate the effect of tuning  X  2 on the smoothness of the results. Of course, we can also choose more sophisticate spline functio ns [11] instead of the above R 2 . As can be seen, in general the matrices R 1 and R 2 (or any other regularization matrices) are not necessarily non-negative.

In general, regularization would be useful to incorporate other prior knowledge in our framework. For example, from the profile of a blogger we may know if she is a Democrat or Republican and this information can help determine the community preference of the blogger. As another example, we may be interested in communities that are very active during a specific period of time. That is, we have some pref-erence for certain temporal trends. However, it is out of scope of this paper to demonstrate such general regulariza-tion.
In our algorithm, we cut data into snapshots according time windows and then apply graph partition algorithms on each snapshot to extract the basis subgraphs. Two practi-cal issues arise: how to choose the size of time window and how to choose the size of basis subgraphs. As a matter of fact, given data over enoughly long period, our algorithm is not very sensitive to these two sizes, as long as they are not overly large. For example, when extracting communi-ties from the blogosphere, we can aggregate blog linkage data by days or by weeks and for data in each time win-dow, we can choose different numbers of basis subgraphs. No matter what window size we choose (the window size is not even necessarily uniform), as long as it is not too long, the fact that members of the same community behave in synchronization will be detected by our algorithm. For each snapshot, even if we cut a true community into small pieces (of course, not as small as a pair of nodes) and therefore put them as different basis subgraphs in the basis tensor, as long as these small pieces frequently co-occur in the data tensor , they will be picked up together by our algorithm to recon-struct the original true community. However, we should be cautious about overly long time window sizes and basis sub-graphs sizes because they aggregate out the details about community behavior.
Determining the right cluster number k is a difficult prob-lem in clustering research that in many cases has no clear an-swers. In spectral clustering, one commonly used method is to sort the eigenvalues of the corresponding eigen-decompo sition in an decreasing order and then pick the cluster number k where there is a large gap between the k -th and the (k+1) -th eigenvalues. The reasoning is that the (k+1) -th eigenvalues reflect the error introduced by keeping the top-k compo-nents in the eigen-decomposition [8]. In our case, because the non-linearity of NMF, we are not able to use this eigen-decomposition argument. However, we still can try different k  X  X  to compare the reconstruction error and then choose one that is reasonably small and at the same time explains data reasonably well.
We now give an algorithm to compute a solution to the optimization problem whose objective function is given by Equation (7), analyze the complexity of the algorithm, and prove the correctness of the algorithm. To solve for U and V in Equation (7), we start by setting U and V to some random non-negative matrices and then iteratively update U and V :
Theorem 1. The following multiplicative updating rules will converge to non-negative solutions to the optimizatio n problem whose objective function is given by Equation (7) where ( R T 1 R 1 ) + , ( R T 2 R 2 ) + , ( R T 1 R 1 )  X  non-negative matrices that represent the positive and nega -tive parts of R T 1 R 1 and R T 2 R 2 , respectively.
Now we prove that the updating rules given in Equa-tions (8) and (9) converge to the solutions to U and V in Equation (7). We start with the general optimization on quadratic form. Assuming we want to find the non-negative solution ~y to minimize the following quadratic form Ding et al. [6] showed the following lemma.

Lemma 1 (Ding). The following iterative update pro-cedure will converge to the solution ~y Equipped with this lemma, now we prove Theorem 1.
Proof of Theorem 1. We first rewrite the objective func-tion as
Defining the vectorization, ~u, ~v , of U, V by stacking the columns of U and V , and ignoring the constant term 1 2 T r ( A we can rewrite J 2 as the quadratic form for ~u and for ~v , re-spectively. Then it can be easily shown that for ~u where  X  is the Kronecker product and vec is the vectoriza-tion operation (see, e.g., [10]). Plugging the above forms into Equation (11), we can get the updating rules in Equa-tions (8) and (9).

It is worth noting that in Equations (8) and (9) we assume that A and B are non-negative, which is true in our applica-tion. However, even if this assumption is not true, because the updating rule (11) applies to general quadratic forms (i.e., when C and ~ d are not necessarily non-negative), we still can derive from Equation (11) multiplicative updatin g rules that are similar to (only slightly different from) Equa -tions (8) and (9), which converge to non-negative solutions U and V .
For an algorithm to be applicable to real problems in the blogosphere, it should be scalable to hundreds of millions o f blogs. We now discuss some aspects of our proposed algo-rithm.

First, our algorithm requires the basis tensor B to be com-puted. This should not be a bottleneck, for there exist very efficient algorithms for graph partitioning. As an example, there exists a greedy algorithm for optimal modularity [4] that is essentially linear in the size of the graph. Second, in the updating equations (8) and (9), the most time and memory consuming parts are B T A and B T B . However, this should not be a serious problem either. First, both A and B are extremely sparse matrices X  X he number of non-zero elements in A is the same as the number of links in the orig-inal data and the number of non-zero elements in B is less than that in A (because B is a partition of A ). In addi-tion, although A and B are relatively large, B T A and B T are relatively small. Furthermore, we only have to compute B
T A and B T B once and then use the results throughout all the iterations.

The algorithm is an iterative one. The overall complex-ity is the number of iteration times the complexity of one iteration. The number of iterations largely depends on the numerical precision. The complexity of one iteration can be analyzed as follows. Let l be the number of links in the graph. Based on the model, we have l  X  m  X  t  X  k . Be-cause of the sparsity of matrices A and B , the complexity of A T B and B T A is O ( lmt ) and the complexity of B T O ( lm 2 ). After computing these terms, the rest of the matrix multiplication are all of lower dimensionality, therefore the complexity is covered by O ( lm 2 ). Thus, the total computa-tion complexity of one iteration is O ( lm 2 ). Therefore, our algorithm should be scalable to large number of blogs in the blogosphere.
In this section, we perform experimental studies by apply-ing our technique to discovering communities from three set s of data. We first use a synthetic data and controlled stud-ies to illustrate some good properties of our technique. The second data set is a blog data set obtained by an in-house crawler developed at NEC Laboratories America. This data set contains a small number of blogs among which there have been intensive interactions over a very long period of time. We will show that our technique discovers many interesting communities that are not detectable by traditional meth-ods. Finally, we study our technique by using a large scale benchmark blog data set. On one hand we demonstrate that our technique is scalable to such a large data set, and on the other hand, we reveal a weak point of our technique when the time period is extremely short.
We design this experiment to demonstrate that our algo-rithm can separate two overlapped communities that have different temporal trends. The data set contains 150 blogs that belong to two communities. Each community contains 100 blogs and therefore there are 50 blogs that participate in both of the two communities. We let the intensities of interaction within the two communities vary following si-nusoid trends with different phases (as will be revealed in Figure 2), over 100 consecutive time windows. Figures 1(a) and 1(b) show the adjacency matrix of the blog graph ag-gregated over all the 100 time windows in a 3D plot and a 2D plot. Figures 1(c) and 1(d) show the adjacency matrices of two communities discovered by applying Shi X  X  Normal-ized Cut algorithm to the aggregated blog graph. As can be seen, the community result is incorrect.
 Figure 1: Synthetic data: (a) and (b), the aggre-gated blog graph; (c) and (d), two communities dis-covered by the Normalized Cut algorithm Figure 2: Synthetic data: two communities together with their trends extracted by using our algorithm
Figure 2 shows the two communities discovered by our algorithm. The communities are given as weighted graphs that have overlap in the membership (see Figure 3 for the ac-tual weights in one of the discovered communities). Figure 2 also shows the temporal trends obtained by using  X  2 = 1 and  X  2 = 1000, respectively. As can be seen, both the commu-nity structure and the temporal trends are discovered accu-rately. Also can be seen from the figure, when  X  2 is set to 1000 and therefore the regularization term has more weight, the obtained temporal trends become smoother as expected. When  X  2 is increased from 1 to 1000, the factorization er-ror, || A  X  BUV T || , did not increase much (from 252 to 259), meaning that the introduction of the regularization term di d not cause negative effects on the factorization quality. Figure 3: Synthetic data: soft community member-ship
Next, we perform an experiment to demonstrate that our algorithm is not too sensitive to the variation of window size. We aggregate data in the previous study in the fol-lowing way  X  in each step, we generate a random number p between 1 and 3 and aggregate the next p time windows into a single one; we repeat this process until all 100 time windows are used. Then we apply our algorithm on this aggregated data. Figure 4 shows the results (with trends V unwrapped according to the aggregation). Compared with Figure 2, we can see that the main communities and trends are both successfully recovered, although with a little wor se quality.
At NEC Laboratories America, we have built a focused crawler on blogs. Given seeds of technology-related blogs, the crawler discovered blogs that are densely connected wit h the seeds, resulting in an expanded set of blogs that com-Figure 4: Synthetic data: two communities together with their trends extracted by using our algorithm, with varying time window sizes municate with each other. The crawler then continued mon-itoring for new entries over a long time period. From this data set, we use 407 English blogs that have 274,679 en-tries in 441 days (63 weeks) between July 10th in 2005 and September 23rd in 2006. These entries are connected with 148,681 links. Expanded from the seed set on technology, the data set actually contain roughly two groups of blogs  X  one with technology focus and another with politics fo-cus. Figure 5 shows the blog graph for this NEC data set, where the layout is determined by the Pajek software using the Kamada-Kawai algorithm. Here we can see two groups: blogs with technology focus are placed as circles in the left region of the graph, and blogs with politics focus are placed as triangles in the right region of the graph.

We first try extracting communities from the aggregated blog graph by using traditional algorithms. Given the graph in Figure 5, we could imagine two large static communi-ties on technology and politics. However, it is hard to see more details as well as the temporal dynamics. By using the Normalized Cut algorithm, we split the aggregated blog graph into 50 clusters (communities). Then for the tempo-ral trends of the communities, we report the number of links among each community every day. Figure 6 shows some of the representative results. We did not find any useful infor-mation in this result.

We now apply our algorithm on this data set to discover the same number of 50 communities. The number 50 has been set rather arbitrarily, although we have tested other numbers such as 20 and 100 and obtained similar results. Figure 6: Temporal trends of communities extracted by a traditional method In Figures 7-10, we show some representative communities that have been discovered. To visualize each community, we display the community graph on the left side and the community intensity over the 441 days on the right side.
In the community graph, the size of a node and the width of a link reflect how important that blog and the interaction between that pair of blogs are in the community. The size of a node is determined by the corresponding row sum in the community graph C l as defined in Equation (1) and the width of a link is determined by the corresponding entry in C l . In addition, to give readers a high-level idea on the types of community members, we fix the coordinates of all the blogs in the graphs to the same position as they are in Figure 5 (i.e., technology blogs in the left region and polit ics blogs in the right region).

In addition, we use the content of the blog entries to val-idate our discovered communities. Note that our algorithm depends purely on structural (hyperlink) information, not on content of blogs. However, intuitively, a valid commu-nity should have coherent topics discussed and consistent vocabulary used among community members. Therefore in this experiment, we extract top keywords from each commu-nity to see if they form a coherent set. Keywords are ranked with the frequency within the community divided by the one within the entire data set.

Now we illustrate several sample communities that are discovered by our algorithm. Please note that in almost all the 50 communities discovered by our algorithm, we are able to detect meaningful trends and coherent topics. In the following discussion, we only show several more well-known communities.

In the first community, which is shown in Figure 7, the main community members include several well-known po-not have any major spike. Instead, the intensity fluctuates as political events happen over the whole period of study. Note that Michelle Malkin is the name of a famous political blogger.

The second community, which is shown in Figure 8, is formed around an authoritative blog by David Sifry, the CEO of one of the top blog search engines  X  Technorati. The peak in the temporal trend happened on October 17, 2005. On that day, David Sifry posted in his blog site a com-http://michellemalkin.com/ http://ezraklein.typepad.com/blog/ Figure 7: Community related to politics in general which includes the number of blogs and entries tracked by Technorati, the speed of growth of the blogosphere, the spam blogs in the blogosphere, the comparison of blogs with mainstream media, and so on. This report was one of the most authoritative studies on the blogosphere and it has been cited and discussed extensively immediately after it had been released. Figure 8: Community related to the status of the blogosphere
The peak in the temporal trend of the third community, as shown in Figure 9, happened on February 1, 2006, when Figure 9: Community related to the Muhammad cartoons controversy the newspapers in some European countries republished the controversial Muhammad cartoons and therefore triggered widespread protests all over the world. As can be seen, the main members of this community belong to the political group of blogs.

The forth community, as shown in Figure 10, is related to the hurricane Katrina, which happened at the end of Au-gust 2005. As can be seen, due to the severe aftermath of the hurricane and due to its high political impact, the com-munity members are more diversified  X  they are distributed all over the blogosphere. Figure 10: Community related to hurricane Katrina
In summary, for this NEC data set, our algorithm is able to detect meaningful communities together with their tem-poral trends. Top representative keywords in each of the discovered community reveal coherent topics.
We apply our community extraction algorithm on the benchmark data set provided by the organizers of the WWW 2006 Workshop on the Weblogging Ecosystem. Compared with the NEC data set, this workshop dataset is of much larger scale. It contains 8.37 million entries from 1.43 mil -lion different blog sites during a 3-week period between July 4th and July 24th, 2005. Because our community extrac-tion algorithm is link-based, instead of the whole data set, we studied the subset of blogs that contain at least one link. By this restriction, we are able to narrow down the number of blogs to around 141K, where the number of links among this set of blogs are 1.62 million.

We use this data set to illustrate two points about our algorithm. First, we show that our algorithm is scalable to data set with large number of blogs and links. As will be reported in the next subsection, our algorithm can handle this large data set very efficiently by using a modest desktop PC running Matlab codes.

More importantly, we want to use this data set to demon-strate a weak point of our algorithm. That is, in order to discover communities that show consistency over time, we need data for a long period. For this benchmark data, there are hundreds of thousands of blogs, but there are only 21 time windows. As a result the community number cannot be too large. We set the number of community to be 10 and run our algorithm on the benchmark data. In Figure 11 we show two sample communities that are discovered whereas all other eight communities follow similar patterns. As can be seen from the figure, the temporal trends for the com-munities are just local spikes at different time windows. In the lower panels of the figure, we show the U vectors for the two communities. As can be seen, each community consists most of local basis subgraphs in the time window where the community peaks. In other words, each community essen-tially picks the whole snapshot graph of one time window. The temporal trends also tell us that the snapshot graphs at consecutive time windows have some correlation. However, other than this trivial information, our algorithm is not ab le to discover meaningful communities. Figure 11: Two sample communities extracted from the benchmark data: (a) and (b), temporal trends of the two communities; (c) and (d), the columns of U corresponding to the two communities
When the number of blogs n is large and the number of time windows t is small, it is difficult for our algorithm to extract meaningful communities. Similar weak points exist in applying NMFs to traditional information retrieval area . That is, when the number of documents is too few, a better explanation to all the documents is to take each individual document as a concept. In our case, instead of individual groups of blogs that form interactive communities, a better explanation to the observed data (in terms of low reconstruc -tion error) is to take some snapshot graphs as communities. In conclusion, our method should be applied to data over a longer time period (i.e., t/n is large), and the traditional approach with aggregated graphs should be applied to data within a short time period (i.e., t/n is small).
In Table 1 we report the running time of our algorithm on the three data sets. Our algorithm is implemented in Matlab and runs on a PC of Pentium IV processor with 2G Hz CPU and 2GB memory. The codes are implemented in Matlab. The reported running time is in second per itera-tion . When the criterion for convergence is set to be that | J ( t )  X  J 2 ( t  X  1) | &lt; 10  X  6 , the algorithm can converge with in 1000 iterations for all the three cases. As can be seen from the running time, our algorithm scales nicely to the size of the data both in terms of the number of blogs and the number of links.

The blogosphere has a unique structure: a series of short-term subgraphs representing the communication among blogs . In this paper, we proposed a novel technique that capture the structure and temporal dynamics of communities from the blogosphere. In our framework, a community is a set of blogs that communicates with each other in a synchronized manner, triggered by some events (such as a news article). The community is represented by a community graph , which indicates how often one blog communicates with another, and community intensities , which indicate the activity level of the community over time. Our method, community fac-torization , extracts such communities from the communica-tion among blogs that is observed as subgraphs (i.e., thread s of discussion). We formalized this as a factorization probl em in the framework of constrained optimization.

Experimental studies were conducted on both synthetic and real blog data. With synthetic data, we demonstrated that our technique is able to identify two overlapping com-munities that have different temporal dynamics. In NEC data set, our technique was able to discover meaningful communities that are not detectable by traditional meth-ods. Through the WWW Workshop benchmark data set, we pointed out that our technique is not appropriate for data with a very short time period. Finally, we demonstrated that our algorithm scales nicely to handle a large number of blogs.

We plan to extend our research in several directions. First, the technique proposed in this paper is based on link anal-ysis only and not combined with content analysis. We plan to incorporate content information into our framework by extending the tensors with keywords as one more dimen-sion so that it can capture multiple topics of interaction among blogs. Second, our technique adopted Shi et al. X  X  normalized cut algorithm for partitioning local blog graph s as undirected graphs. We are also interested in using di-rected graph partitioning algorithms since the direction of links in the blogosphere is important. [1] L. Backstrom, D. Huttenlocher, J. Kleinberg, and [2] D. Chakrabarti, R. Kumar, and A. Tomkins.
 [3] Y. Chi, B. L. Tseng, and J. Tatemura. Eigen-trend: [4] A. Clauset, M. E. J. Newman, and C. Moore. Finding [5] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [6] C. Ding, T. Li, and M. Jordan. Convex and [7] G. Flake, S. Lawrence, and C. Giles. Efficient [8] G. Golub and C. V. Loan. Matrix Computations . [9] D. Gruhl, R. Guha, D. Liben-Nowell, and A. Tomkins. [10] D. A. Harville. Matrix Algebra From a Statistician X  X  [11] T. Hastie, R. Tibshirani, and J. H. Friedman. The [12] H. Ino, M. Kudo, and A. Nakamura. Partitioning of [13] R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. [14] R. Kumar, J. Novak, and A. Tomkins. Structure and [15] D. D. Lee and H. S. Seung. Learning the parts of [16] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs [17] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic [18] M. E. J. Newman. Modularity and community [19] A. Qamra, B. L. Tseng, and E. Y. Chang. Mining blog [20] J. Shi and J. Malik. Normalized cuts and image [21] J. B. T. Falkowski and M. Spiliopoulou. Mining and [22] G. Wahba. Spline Models for Observational Data . [23] X. Wang and A. McCallum. Topics over time: A [24] S. White and P. Smyth. A spectral clustering approach [25] W. Xu and Y. Gong. Document clustering by concept [26] W. Xu, X. Liu, and Y. Gong. Document clustering
