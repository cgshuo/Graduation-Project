 Recognition]: Applications  X  Text processing General Terms : Experimentation, Performance Keywords : Text Classification, IDF, Term Weighting 
Text classification is the task of automatically assigning unlabeled documents into predefined categories. In text classification, text representa tion transforms the content of textural documents into a compact format so that the documents can be recognized and classified by a classifier. In the vector space model, a document is represented as a vector in the term spaces, ),...,( || V wwd 1  X  , where |V| is the size of vocabulary. The value of w i between [0,1] represents how much the term w i contributes to the semantics of the document d . Text classification has borrowed the traditional term weighting schemes from information retrieval field, such as tf, tf.idf [1] and its variants. 
This research starts with a question,  X  X an we make a better term weighting scheme than ones of information retrieval for text classification? X  We believe that text classification should utilize class information better than information retrieval because supervised learning based text cl assification has labeled training data. However, inverse document frequency, idf , is a measure of general importance of a term; there is no use of class information. Therefore, this paper focuses on how we can improve text classification by effectively applying class information to a term weighting scheme. 
Recently, researchers have attempted to use this prior information, class information, for term weighting. There are two representative term weighting schemes: rf (relevance frequency) [2] and Delta tf.idf [3,4]. The former was proposed to use the ratio of term occurrences of the positive class and the negative class for calculating term weights. However, they did not discuss how they make the representation of test documents. Since a test document does not have any prior class inform ation, it is hard to represent the test document using class in formation. The latter provided a solution to use class information for sentiment classification by localizing the estimation of idf to the documents of one or the other class and subtracting th e two values. However, this approach is limited to classi fication problems with only two classes just like sentiment classification. 
This paper proposes new term weighting schemes for multi-class text classification, which include term weighting methods for test documents. Then it was compared to the previous studies [2,3,4] and tf.idf . As a result, the proposed schemes achieved the best performance on all of da ta sets and classifiers. 1. Word Max ( W-Max ): the term weight of each word is chosen 2. Document Max ( D-Max ): the sum of all term weights in 3. Documents Two Max ( D-TMax ): the sum of all term weights 
Two widely-used data sets were used as the benchmark data sets: Reuters and 20 Newsgroups data sets, and two promising learning algorithms, which have shown better performance than other algorithms, were chosen in the experiments: k NN and SVM. The one-against-the-rest method was used for setting up positive examples and negative examples for each class. 
The Reuters 21578 data set ( Reuters ) was split as training and test data according to the standard ModApte split and the top 10 largest categories were used in the experiments. The 20 Newsgroups data set ( NG ) is a collection of approximate 20,000 newsgroup data evenly divided among 20 discussion groups. For fair evaluation, we used five-fold cross validation. 
These data sets have quite different characteristics. Reuters has a skewed class distribution and many documents have two or more class labels. On the other hand, NG has uniform class distribution and its documents ha ve only one class label. Thus, two different measures were us ed to evaluate various term weighting schemes on tw o classifiers for our experiments. For Reuters , we used the micro-averaging Break Even Point (BEP) measure which is a standard information retrieval measure for binary classification and, for NG , the performances are reported by the micro-averaging F1 measure. 
First of all, the proposed term weighting schemes ( TRR ) are compared with the traditional idf scheme. All of these schemes did not use any tf information. As a result, the proposed schemes achieved better performances than the idf scheme. We here need to discuss the results of TRR because TRR has two different estimation methods ( Cat-MLE by equation (2) and Doc-MLE by equation (3)) and they showed different performance aspects in each data set; Cat-MLE obtained the best performances in Reuters while Doc-MLE the best performances in NG . It can be caused by the skewed distribution of the document length in Reuters ; many documents in Reuters consist of small number of sentences such as just two or three. It could make a bad probability estimation in the Doc-MLE scheme. We can also observe the same results in the following experiments; Cat-MLE is better in Reuters while Doc-MLE in NG . 
Table 1. Comparison of TRR (Cat-MLE &amp; Doc-MLE) and idf
Reuters SVM 94.11 94.69 94.29 
