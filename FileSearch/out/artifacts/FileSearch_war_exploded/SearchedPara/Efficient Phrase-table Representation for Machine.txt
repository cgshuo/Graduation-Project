 In phrase-based statistical machine translation, a huge number of source and target phrase pairs is memorized in the so-called phrase-table. For medium sized tasks and phrase lengths, these phrase-tables already require several GBs of mem-ory or even do not fit at all. If the source text, which is to be translated, is known in advance, a common trick is to filter the phrase-table and keep a phrase pair only if the source phrase occurs in the text. This filtering is a time-consuming task, as we have to go over the whole phrase-table. Furthermore, we have to repeat this filtering step whenever we want to translate a new source text.

To address these problems, we will use an ef-ficient representation of the phrase-table with two key properties: on-demand loading and a prefix tree structure for the source phrases. The prefix tree structure exploits the redundancy among the source phrases. Using on-demand loading, we will load only a small fraction of the overall phrase-table into memory. The majority will remain on disk.

The on-demand loading is employed on a per sen-tence basis, i.e. we load only the phrase pairs that are required for one sentence into memory. There-fore, the memory requirements are low, e.g. less than 20 MB for the Chin.-Eng. NIST task. Another ad-vantage of the on-demand loading is that we are able to translate new source sentences without filtering.
A potential problem is that this on-demand load-ing might be too slow. To overcome this, we use a binary format which is a memory map of the internal representation used during decoding. Additionally, we load coherent chunks of the tree structure instead of individual phrases, i.e. we have only few disk ac-cess operations. In our experiments, the on-demand loading is not slower than the traditional approach.
As pointed out in (Mathias and Byrne, 2006), one problem in speech translation is that we have to match the phrases of our phrase-table against a word graph representing the alternative ASR tran-scriptions. We will present a phrase matching algo-rithm that effectively solves this combinatorial prob-lem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of sig-nificantly larger input word graphs in a more effi-cient way resulting in improved translation quality.
The remaining part is structured as follows: we will first discuss related work in Sec. 2. Then, in Sec. 3, we will describe the phrase-table represen-tation. Afterwards, we will present applications in speech translation and online MT in Sec. 4 and 5, respectively. Experimental results will be presented in Sec. 6 followed by the conclusions in Sec. 7. (Callison-Burch et al., 2005) and (Zhang and Vogel, 2005) presented data structures for a compact rep-resentation of the word-aligned bilingual data, such that on-the-fly extraction of long phrases is possi-ble. The motivation in (Callison-Burch et al., 2005) is that there are some long source phrases in the test data that also occur in the training data. How-ever, the more interesting question is if these long phrases really help to improve the translation qual-ity. We have investigated this and our results are in line with (Koehn et al., 2003) showing that the trans-lation quality does not improve if we utilize phrases beyond a certain length. Furthermore, the suffix ar-ray data structure of (Callison-Burch et al., 2005) re-quires a fair amount of memory, about 2 GB in their example, whereas our implementation will use only a tiny amount of memory, e.g. less than 20 MB for the large Chinese-English NIST task. In this section, we will describe the proposed rep-resentation of the phrase-table. A prefix tree, also called trie, is an ordered tree data structure used to store an associative array where the keys are symbol sequences. In the case of phrase-based MT, the keys are source phrases, i.e. sequences of source words and the associated values are the possible transla-tions of these source phrases. In a prefix tree, all descendants of any node have a common prefix, namely the source phrase associated with that node. The root node is associated with the empty phrase.
The prefix tree data structure is quite common in automatic speech translation. There, the lexicon, i.e. the mapping of phoneme sequences to words, is usu-ally organized as a prefix tree (Ney et al., 1992).
We convert the list of source phrases into a pre-fix tree and, thus, exploit that many of them share the same prefix. This is illustrated in Fig. 1 (left). Within each node of the tree, we store a sorted ar-ray of possible successor words along with pointers to the corresponding successor nodes. Additionally, we store a pointer to the possible translations.
One property of the tree structure is that we can efficiently access the successor words of a given pre-fix. This will be a key point to achieve an efficient phrase matching algorithm in Sec. 4. When looking for a specific successor word, we perform a binary search in the sorted array. Alternatively, we could use hashing to speed up this lookup. We have chosen an array representation as this can be read very fast from disk. Additionally, with the exception of the root node, the branching factor of the tree is small, i.e. the potential benefit from hashing is limited. At the root node, however, the branching factor is close to the vocabulary size of the source language, which can be large. As we store the words internally as in-tegers and virtually all words occur as the first word of some phrase, we can use the integers directly as the position in the array of the root node. Hence, the search for the successors at the root node is a simple table lookup with direct access, i.e. in O (1) .
If not filtered for a specific test set, the phrase-table becomes huge even for medium-sized tasks. Therefore, we store the tree structure on disk and load only the required parts into memory on-demand. This is illustrated in Fig. 1 (right). Here, we show the matching phrases for the source sen-tence  X  X  a a c X , where the matching phrases are set in bold and the phrases that are loaded into memory are set in italics . The dashed part of the tree structure is not loaded into memory. Note that some nodes of the tree are loaded even if there is no matching phrase in that node. These are required to actually verify that there is no matching phrase. An example is the  X  X c X  node in the lower right part of the figure. This node is loaded to check if the phrase  X  X  a a X  occurs in the phrase-table. The translations, however, are loaded only for matching source phrases.

In the following sections, we will describe appli-cations of this phrase-table representation for speech translation and online MT. In speech translation, the input to the MT system is not a sentence, but a word graph representing alter-Figure 2: Illustration for graph G : node j with suc-native ASR transcriptions. As pointed out in (Math-ias and Byrne, 2006), one problem in speech trans-lation is that we have to match the phrases of our phrase-table against the input word graph. This re-sults in a combinatorial problem as the number of phrases in a word graph increases exponentially with the phrase length. 4.1 Problem Definition In this section, we will introduce the notation and state the problem of matching source phrases of an input graph G and the phrase-table, represented as prefix tree T . The input graph G has nodes 1 ,...,j,...,J . The outgoing edges of a graph node j are numbered with 1 ,...,n,...,N j , i.e. an edge in the input graph is identified by a pair ( j,n ) . The source word labeling the n th outgoing edge of graph node j is denoted as f G j,n and the successor node of this edge is denoted as s G j,n  X  { 1 ,...,J } . This nota-tion is illustrated in Fig. 2.

We use a similar notation for the prefix tree T with nodes 1 ,...,k,...,K . The outgoing edges of a tree node k are numbered with 1 ,...,m,...,M k , i.e. an edge in the prefix tree is identified by a pair ( k,m ) . The source word labeling the m th outgoing edge of tree node k is denoted as f T k,m and the successor node of this edge is denoted as s T k,m  X  { 1 ,...,K } . Due to the tree structure, the successor nodes of a tree node k are all distinct: Let k 0 denote the root node of the prefix tree and let  X  f k denote the prefix that leads to tree node k . Furthermore, we define E ( k ) as the set of possible translations of the source phrase  X  f k . These are the entries of the phrase-table, i.e.
 We will need similar symbols for the input graph. Therefore, we define F ( j 0 ,j ) as the set of source phrases of all paths from graph node j 0 to node j , or formally: F ( j 0 ,j ) =  X  f  X  ( j i ,n i ) I i =1 :  X  f = f G j Here, the conditions ensure that the edge sequence ( j i ,n i ) I i =1 is a proper path from node j in the input graph and that the corresponding source phrase is  X  f = f G j be expressed in a recursive way; the idea is to extend the phrases of the predecessor nodes by one word: F ( j 0 ,j ) = [
Here, the set is expressed as a union over all in-bound edges ( j 00 ,n ) of node j . We concatenate each source phrase  X  f that ends at the start node of such an edge, i.e.  X  f  X  F ( j 0 ,j 00 ) , with the corresponding edge label f G j 00 ,n . Additionally, we define E ( j 0 ,j ) as the set of possible translations of all paths from graph node j 0 to graph node j , or formally: E ( j 0 ,j ) =  X  e  X   X  f  X  F ( j 0 ,j ) : p (  X  e |  X 
Here, the definition was first rewritten using Eq. 2 and then using Eq. 3. Again, the set is expressed recursively as a union over the inbound edges. For each inbound edge ( j 00 ,n ) , the inner union verifies that there exists a corresponding edge ( k,m ) in the prefix tree with the same label, i.e. f G j 00 ,n = f T k,m
Our goal is to find all non-empty sets of trans-lation options E ( j 0 ,j ) . The naive approach would be to enumerate all paths in the input graph from node j 0 to node j , then lookup the corresponding source phrase in the phrase-table and add the trans-lations, if there are any, to the set of translation options E ( j 0 ,j ) . This solution has some obvious weaknesses: the number of paths between two nodes is typically huge and the majority of the correspond-ing source phrases do not occur in the phrase-table.
We omitted the probabilities for notational conve-nience. The extensions are straightforward. Note that we store only the target phrases  X  e in the set of possible translations E ( j 0 ,j ) and not the source phrases  X  f . This is based on the assumption that the models which are conditioned on the source phrase  X  f are independent of the context outside the phrase pair (  X  f,  X  e ) . This assumption holds for the standard phrase and word translation models. Thus, we have to keep only the target phrase with the highest prob-ability. It might be violated by lexicalized distor-tion models (dependent on the configuration); in that case we have to store the source phrase along with the target phrase and the probability, which is again straightforward. 4.2 Algorithm The algorithm for matching the source phrases of the input graph G and the prefix tree T is presented in Figure 3: Algorithm phrase-match for match-ing source phrases of input graph G and prefix tree T . Input: graph G , prefix tree T , translation options E ( k ) for all tree nodes k ; output: translation options E ( j 0 ,j ) for all graph nodes j 0 and j . 0 FOR j 0 = 1 TO J DO 1 stack.push ( j 0 ,k 0 ) 2 WHILE not stack.empty() DO 3 ( j,k ) = stack.pop() 4 E ( j 0 ,j ) = E ( j 0 ,j )  X  E ( k ) 5 FOR n = 1 TO N j DO 6 IF ( f G j,n = ) 7 THEN stack.push ( s G j,n ,k ) 8 ELSE IF (  X  m : f G j,n = f T k,m ) 9 THEN stack.push ( s G j,n ,s T k,m ) Fig. 3. Starting from a graph node j 0 , we explore the part of the graph which corresponds to known source phrase prefixes and generate the sets E ( j 0 ,j ) incre-mentally based on Eq. 6. The intermediate states are represented as pairs ( j,k ) meaning that there ex-ists a path in the input graph from node j 0 to node j which is labeled with the source phrase  X  f k , i.e. the source phrase that leads to node k in the prefix tree. These intermediate states are stored on a stack. After the initialization in line 1, the main loop starts. We take one item from the stack and update the transla-tion options E ( j 0 ,j ) in line 4. Then, we loop over all outgoing edges of the current graph node j . For each edge, we first check if the edge is labeled with an in line 6. In this special case, we go to the suc-cessor node in the input graph s G j,n , but remain in the current node k of the prefix tree. In the regular case, i.e. the graph edge label is a regular word, we check in line 8 if the current prefix tree node k has an out-going edge labeled with that word. If such an edge is found, we put a new state on the stack with the two successor nodes in the input graph s G j,n and the prefix tree s T k,m , respectively. 4.3 Computational Complexity In this section, we will analyze the computational complexity of the algorithm. The computational complexity of lines 5-9 is in O ( N j log M k ) , i.e. it depends on the branching factors of the input graph and the prefix tree. Both are typically small. An ex-ception is the branching factor of the root node k 0 of the prefix tree, which can be rather large, typically it is the vocabulary size of the source language. But, as described in Sec. 3, we can access the successor nodes of the root node of the prefix tree in O (1) , i.e. in constant time. So, if we are at the root node of the prefix tree, the computational complexity of lines 5-9 is in O ( N j ) . Using hashing at the interior nodes of the prefix tree would result in a constant time lookup at these nodes as well. Nevertheless, the sorted ar-ray implementation that we chose has the advantage of faster loading from disk which seems to be more important in practice.

An alternative interpretation of lines 5-9 is that we have to compute the intersection of the two sets f G j and f T k , with Assuming both sets are sorted, this could be done in linear time, i.e. in O ( N j + M k ) . In our case, only the edges in the prefix tree are sorted. Obviously, we could sort the edges in the input graph and then ap-ply the linear algorithm, resulting in an overall com-plexity of O ( N j log N j + M k ) . As the algorithm visits nodes multiple times, we could do even better by sorting all edges of the graph during the initial-ization. Then, we could always apply the linear time method. On the other hand, it is unclear if this pays off in practice and an experimental comparison has to be done which we will leave for future work.
The overall complexity of the algorithm depends on how many phrases of the input graph occur in the phrase-table. In the worst case, i.e. if all phrases oc-cur in the phrase-table, the described algorithm is not more efficient than the naive algorithm which simply enumerates all phrases. Nevertheless, this does not happen in practice and we observe an ex-ponential speed up compared to the naive algorithm, as will be shown in Sec. 6.3. Beside speech translation, the presented phrase-table data structure has other interesting applica-tions. One of them is online MT, i.e. an MT sys-tem that is able to translate unseen sentences with-out significant delay. These online MT systems are typically required if there is some interaction with human users, e.g. if the MT system acts as an in-terpreter in a conversation, or in real-time systems. This situation is different from the usual research environment where typically a fair amount of time is spent to prepare the MT system to translate a cer-tain set of source sentences. In the research scenario, Train Sentence pairs 7 M Test 2002 Sentences 878 3 512 this preparation usually pays off as the same set of sentences is translated multiple times. In contrast, an online MT system translates each sentence just once. One of the more time-consuming parts of this preparation is the filtering of the phrase-table. Us-ing the on-demand loading technique we described in Sec. 3, we can avoid the filtering step and di-rectly translate the source sentence. An additional advantage is that we load only small parts of the full phrase-table into memory. This reduces the mem-ory requirements significantly, e.g. for the Chinese X  English NIST task, the memory requirement of the phrase-table is reduced to less than 20 MB using on-demand loading. This makes the MT system usable on devices with limited hardware resources. 6.1 Translation System For the experiments, we use a state-of-the-art phrase-based statistical machine translation system as described in (Zens and Ney, 2004). We use a log-linear combination of several models: a four-gram language model, phrase-based and word-based translation models, word, phrase and distortion penalty and a lexicalized distortion model. The model scaling factors are optimized using minimum error rate training (Och, 2003). 6.2 Empirical Analysis for a Large Data Task In this section, we present an empirical analysis of the described data structure for the large data track of the Chinese-English NIST task. The corpus statis-tics are shown in Tab. 1.

The translation quality is measured using two ac-curacy measures: the BLEU and the NIST score. Additionally, we use the two error rates: the word error rate (WER) and the position-independent word error rate (PER). These evaluation criteria are com-puted with respect to four reference translations.
In Tab. 2, we present the translation quality as a function of the maximum source phrase length. We observe a large improvement when going beyond length 1, but this flattens out very fast. Using phrases of lengths larger than 4 or 5 does not result in fur-ther improvement. Note that the minor differences in the evaluation results for length 4 and beyond are merely statistical noise. Even a length limit of 3, as proposed by (Koehn et al., 2003), would result in almost optimal translation quality. In the following experiments on this task, we will use a limit of 5 for the source phrase length.

In Tab. 3, we present statistics about the extracted phrase pairs for the Chinese X  X nglish NIST task as a function of the source phrase length, in this case for length 1-5. The phrases are not limited to a spe-cific test set. We show the number of distinct source phrases, the number of distinct source-target phrase pairs and the average number of target phrases (or translation candidates) per source phrase. In the ex-periments, we limit the number of translation can-didates per source phrase to 200. We store a to-tal of almost 90 million distinct source phrases and more than 225 million distinct source-target phrase pairs in the described data structure. Obviously, it would be infeasible to load this huge phrase-table completely into memory. Nevertheless, using on-demand loading, we are able to utilize all these phrase pairs with minimal memory usage.

In Fig. 4, we show the memory usage of the de-scribed phrase-table data structure per sentence for Figure 4: NIST task: phrase-table memory usage per sentence (sorted). the NIST 2002 test set. The sentences were sorted according to the memory usage. The maximum amount of memory for the phrase-table is 19 MB; for more than 95% of the sentences no more than 15 MB are required. Storing all phrase pairs for this test set in memory requires about 1.7 GB of mem-ory, i.e. using the described data structures, we not only avoid the limitation to a specific test set, but we also reduce the memory requirements by about two orders of a magnitude.

Another important aspect that should be consid-ered is translation speed. In our experiments, the described data structure is not slower than the tradi-tional approach. We attribute this to the fact that we use a binary format that is a memory map of the data structure used internally and that we load the data in rather large, coherent chunks. Additionally, there is virtually no initialization time for the phrase-table which decreases the overhead of parallelization and therefore speeds up the development cycle. 6.3 Speech Translation The experiments for speech translation were con-ducted on the European Parliament Plenary Sessions (EPPS) task. This is a Spanish-English speech-to-speech translation task collected within the TC-Star project. The training corpus statistics are presented in Tab. 4. The phrase-tables for this task were kindly provided by ITC-IRST.

We evaluate the phrase-match algorithm in the context of confusion network (CN) decoding (Bertoldi and Federico, 2005), which is one ap-proach to speech translation. CNs (Mangu et al., 2000) are interesting for MT because the reordering can be done similar to single best input. For more details on CN decoding, please refer to (Bertoldi et al., 2007). Note that the phrase-match algo-rithm is not limited to CNs, but can work on arbi-trary word graphs.
 Statistics of the CNs are also presented in Tab. 4. We distinguish between the full CNs and pruned CNs. The pruning parameters were chosen such that the resulting CNs are similar in size to the largest ones in (Bertoldi and Federico, 2005). The average depth of the full CNs, i.e. the average number of al-ternatives per position, is about 2.7 words whereas the maximum is as high as 136 alternatives.

In Fig. 5, we present the average number of phrase-table look-ups for the full EPPS CNs as a function of the source phrase length. The curve  X  X N total X  represents the total number of source phrases in the CNs for a given length. This is the number of phrase-table look-ups using the naive algorithm. Note the exponential growth with increasing phrase length. Therefore, the naive algorithm is only appli-cable for very short phrases and heavily pruned CNs, as e.g. in (Bertoldi and Federico, 2005).

The curve  X  X N explored X  is the number of phrase-table look-ups using the phrase-match algo-rithm described in Fig. 3. We do not observe the exponential explosion as for the naive algorithm. Thus, the presented algorithm effectively solves the combinatorial problem of matching phrases of the input CNs and the phrase-table. For comparison, we plotted also the number of look-ups using the phrase-match algorithm in the case of single-Figure 5: EPPS task: avg. number of phrase-table look-ups per sentence as a function of the source phrase length.
 Table 5: EPPS task: translation quality and time for different input conditions (CN=confusion network, time in seconds per sentence).
 best input, labeled  X  X ingle-best explored X . The maxi-mum phrase length for these experiments is seven. For CN input, this length can be exceeded as the CNs may contain -transitions.

In Tab. 5, we present the translation results and the translation times for different input conditions. We observe a significant improvement in translation quality as more ASR alternatives are taken into ac-count. The best results are achieved for the full CNs. On the other hand, the decoding time in-creases only moderately. Using the new algorithm, the ratio of the time for decoding the CNs and the time for decoding the single best input is 3.4 for the full CNs and 1.8 for the pruned CNs. In previous work (Bertoldi and Federico, 2005), the ratio for the pruned CNs was about 25 and the full CNs could not be handled.

To summarize, the presented algorithm has two main advantages for speech translation: first, it enables us to utilize large CNs, which was pro-hibitively expensive beforehand and second, the ef-ficiency is improved significantly.

Whereas the previous approaches required care-ful pruning of the CNs, we are able to utilize the un-pruned CNs. Experiments on other tasks have shown that even larger CNs are unproblematic. We proposed an efficient phrase-table data structure which has two key properties: 1. On-demand loading.
 2. Prefix tree data structure.

We have shown that this data structure scales very well to large data tasks like the Chinese-English NIST task. The implementation of the described data structure as well as the phrase-match al-gorithm for confusion networks is available as open source software in the MOSES toolkit 1 .

Not only standard phrase-based systems can ben-efit from this data structure. It should be rather straightforward to apply this data structure as well as the phrase-match algorithm to the hierarchical approach of (Chiang, 2005). As the number of rules in this approach is typically larger than the number of phrases in a standard phrase-based system, the gains should be even larger.

The language model is another model with high memory requirements. It would be interesting to in-vestigate if the described techniques and data struc-tures are applicable for reducing the memory re-quirements of language models.

Some aspects of the phrase-match algorithm are similar to the composition of finite-state au-tomata. An efficient implementation of on-demand loading (not only on-demand computation) for a finite-state toolkit would make the whole range of finite-state operations applicable to large data tasks.
