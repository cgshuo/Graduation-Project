 Ontologies represent data relationships as hierarchies of pos-sibly overlapping classes. Ontologies are closely related to clustering hierarchies, and in this article we explore this re-lationship in depth. In particular, we examine the space of ontologies that can be generated by pairwise dissimilar-ity matrices. We demonstrate that classical clustering algo-rithms, which take dissimilarity matrices as inputs, do not incorporate all available information. In fact, only special types of dissimilarity matrices can be exactly preserved by previous clustering methods. We model ontologies as a par-tially ordered set (poset) over the subset relation. In this paper, we propose a new clustering algorithm, that gener-ates a partially ordered set of clusters from a dissimilarity matrix.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms
Keywords: PoCluster, Poset, Dissimilarity, Clustering
Classification hierarchies are natural ways of organizing data. Such hierarchies can range from taxonomies, where all subclasses are disjoint subsets of the parent class, to ontolo-gies that allow arbitrary overlaps between subclasses as well as allowing any subclass to have multiple parents. Typically, classification hierarchies are designed by domain experts. In this article, we address the problem of constructing ontolo-gies automatically by computational means. Moreover, we attempt to derive both categories and their subclass relation-ships when given only pairwise relationships, dissimilarities, between elements. As a result, we treat ontology construc-tion as a data clustering generalization where the set of ob-jects is grouped into clusters, and the clusters are partially ordered by the subset relation.

Dissimilarity is a common intermediary used by clustering methods to classify data. Applications range from analyz-ing microarray gene expression levels collected under mul-tiple conditions[15], to analyzing word usage statistics from a corpus of documents[6]. Dissimilarities represent relative pairwise relationships between data objects. Often, they are Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Figure 1: Frequencies of nodes with multiple parents in three GO files (biological process, cellular compo-nent, and molecular function). derived, by various means, fro m data features. Dissimilar-ity matrices simplify some of the problems associated with clustering high-dimensional datasets, since their size is only a function of the number of objects ( O ( |N| 2 )), and indepen-dent of the objects X  dimensions. Many clustering approaches have been developed that take dissimilarities as inputs, and generate hierarchies of clusters. Such hierarchies can be viewed as categorizations or taxonomies if the clusters form a hierarchy of data partitions.
 Classification ontologies are an important tool in biology. Biological ontologies, such as Gene Ontology[1] (GO), are carefully curated and encapsulate both functional knowledge and important relationships between genes. The class-subclass relationships in GO are neither a simple tree, nor a lat-tice structure. Instead, it is a directed acyclic graph, where any child can have multiple parents. The frequency of cate-gories with multiple parents in GO is illustrated in Figure 1. Ontologies are a rich source of information for comparing functions and relationships between various subsets of genes. Clearly, genes in the same category are expected to be simi-lar. Likewise, genes whose categories share a common parent-age would also be expected to exhibit some similarity, albeit to a lesser extent than members of a common category. Re-cent efforts have tried to extract the functional relationships of the expressed genes seen in microarray studies based on their classification in GO[11].

Ideally, one would expect that the categorical similarities and dissimilarities derived from a domain expert X  X  knowledge of a gene X  X  function could be used as the basis for extracting biologically meaningful clusters[15]. We explore the potential for extracting such meaningful clusters directly from pairwise dissimilarity measures. Classic clustering algorithms are typ-ically (with some exceptions noted later) either flat, or hier-archical, data partitions, whereas the categories of an ontol-ogy allow objects to be members of multiple categories or clusters, and allow clusters to have multiple parents.
Before proceeding, we examine the specific classification notion assumed in this paper. Mathematically, a general classification system, namely, ontology, is a partially ordered set[1], or poset. It represents the relationships among mul-tiple categories of objects. For our discussion, we consider a category as a set of objects and the more  X  X pecific X  relation-ship between a parent and a child as a  X  X ubset X  relationship. Therefore, a poset contains the sets of objects as the elements ordered according to their subset relationships. A poset can be constructed from any combination of subsets taken from the set X  X  power set. Therefore, the set of posets has a maxi-mal cardinality of 2 2 |N| ,where N is the object set.
A poset generalizes hierarchical clustering structures by al-lowing overlaps. This generalization poses a challenge to tra-ditional clustering methods. Classical clustering algorithms generate disjoint subsets, such as graph-theoretical cluster-ing, density-based clustering and k-means type clustering, etc. Even agglomerative hierarchical clustering methods main-tain the invariant that child subsets of a common parent are disjoint.In this paper, we focus on incorporating all of the available information from a given dissimilarity matrix into a clustering algorithm, and derive partially ordered sets from it.

From an application standpoint, the goal of our paper is to derive plausible ontology-like categorizations of objects from a pairwise dissimilarity matrix via a clustering algorithm. We adopt the natural definition of the cluster in graph the-ory, maximal clique. A clique cluster is a maximal subset of objects whose maximal pair-wise dissimilarity is below a certain threshold. The PoCluster is a collection of maximal cliques arrived at by smoothly varying the threshold from 0 to the maximum dissimilarity within the dataset.

In order to construct the PoCluster from a dissimilarity matrix, we map our problem to a dual graph problem. We start with a graph with no edges and gradually insert the edges in the increasing order according to their dissimilar-ity values. After all edges less-than-or-equal to a given dis-similarity threshol d are inserted, the graph is searched for cliques . These cliques represent potential categories (sub-sets). In subsequent passes the threshold is increased and the process repeats until all objects are combined into a sin-gle clique. As a result, the series of cliques form a PoCluster. Our experiments on real data have shown effectiveness and efficiency compared with conventional hierarchical and pyra-midal clustering algorithms.

The remainder of this paper is organized as follows. Sec-tion 2 addresses related work in clustering, automated tax-onomy construction, and dissimilarity measures appropriate for taxonomies. Section 3 defines PoCluster and its proper-ties. Section 4 constructs the poset from the dissimilarity data. A performance study is reported in Section 5. Section 6 concludes the paper and discusses some future work.
Many clustering algorithms assume that the input is given as a dissimilarity matrix. However, relatively few investiga-tions have been conducted in establishing the relationship between a dissimilarity matrix input and the clustering re-sult. In this section, we review previous studies on clustering algorithms that have known relationships to special classes of dissimilarity matrices.
Both hierarchical[10, 3] and pyramidal clustering [7, 4] gen-erate clusters that have bijections to some special sub-classes of dissimilarity matrices.

Hierarchical clustering[10, 3] refers to the formation of a recursive clustering of data objects: a partition into two clus-ters, each of which is itself hierarchically clustered. It is often represented by a dendrogram , that is, a binary tree with the objects at its leaves and a root corresponding to the uni-versal set (of all objects). The heights of the internal nodes represent the maximal dissimilarities between the descendant leaves. It has been proven that a bijection exists between hi-erarchical clustering and an ultrametric [7]  X  a special type of metric in which the dissimilarities satisfy the ultramet-ric triangle inequality D ( a, c ) &lt;max { D ( a, b ) ,D ( b, c ) equivalent way of defining an ultrametric is that there exists a linear order of all objects such that their dissimilarities are the distances between them.

Pyramidal clustering[7, 4] allows for a more general model than hierarchical clustering. A child cluster may have up to two parent clusters. Two clusters may overlap by sharing a common child cluster. The structure can be represented by a directed acyclic graph. It is known that a bijection ex-ists between pyramidal clustering and dissimilarity matrices that are Robinson matrices. A matrix is a Robinson matrix if there exists an ordering among all objects such that the dissimilarities in the rows and columns do not decrease when moving horizontally or vertically away from the main diag-onal. An ultrametric matrix is a special case of Robinson matrix and hierarchical clustering is a special case of pyra-midal clustering. Note that a dissimilarity matrix may not always be a Robinson matrix, and in such case, neither hi-erarchical clustering nor pyramidal clustering is able to gen-erate clustering from which the original dissimilarity matrix can be re-derived. That is, the bijection no longer exists. Information of dissimilarity will be lost during the clustering procedure.

Consider the example shown in Figure 2. Figure 2(A) shows 6 points on a circle in a 2D space. The non-overlapping property of hierarchical clustering (Figure 2(B)) prohibits the clustering of (-1, 0) with (-0.5, -.87) once it is clustered with (-0.5, 0.87), although it has the same distance to both (-0.5, -.87) and (-0.5, 0.87). Pyramidal clustering (Figure 2(C)) al-leviates this problem by allowing (-1, 0) to be clustered with both (-0.5, -0.87) and (-0.5, 0.87). However, a strict ordering of points based on Robinson matrix criterion is impossible in this case. With the optimized ordering (  X  0 . 5 , 0 . 87), (-1, 0), (-0.5, -0.87), (0.5, -0.87), (1, 0), (0.5, 0 . 87) ,points(-0.5, 0.87) and (0.5, 0.87) are not connected although they have the minimum distance. Our method (Figure 2(D)) considers only the different dissimilarities in the data. In this example, they are { 1 , 1 . 74 , 2 } . For each dissimilarity d ,welookforthe maximal sets of points whose maximum pair-wise dissimilar-ity is less than or equal to d .When d = 1, the set of clusters are those intermediate clusters in the first level shown in Fig-ure 2(D).
Similarities or dissimilarities among objects organized in a hierarchical structure are often easy to compute than those of the objects in a DAG(Directed Acyclic Graph), such as wordNet and GO. Semantic similarity[6] was introduced to measure the similarity between two concepts in the Word-Net. Similar measures were also applied to determine the dissimilarity between a pair of genes in Gene Ontology[11, 15].

The following are a few widely adopted measures. Guided by the intuition that the similarity between a pair of con-cepts may be assessed by  X  X he extent to which they share information X , Resnik defined the dissimilarity between two concepts c 1 and c 2 as the information content of their low-est subsumer(which is measured by a probability p ), i.e., sim ( c 1 ,c 2) =  X  log p ( ls ( c 1 ,c 2)). Leacock and Chodorow proposed a very different similarity measure that relies on the length len ( c 1 ,c 2) of the shortest path between two con-cepts. However, they limit their attention to specific links and scale the path length by the overall depth D of the tax-ters derived from these dissimila rities relate to the original ontology. In Section 5, we provide a comparison of those al-gorithms in how suitable they are when used for recovering the original ontology. Jiang et. al [6] and Lin[6] also devel-oped other two alternative similarity measures, which are a variation of Resnik X  X  method.
In the following discussion, we assume a universal set of objects denoted by N .A pair in N refers to an object pair { x, y } ,where x, y  X  X  . Given a set S  X  X  ,thesetofpairs in S is denoted by S  X  S or S 2 .

A dissimilarity matrix describes the pair-wise relationships between objects. It is a mapping D from ( N X N )toareal nonnegative value. A dissimilarity matrix has the following two properties (1) reflectivity:  X  x , D ( x, x )=0;(2)sym-metry:  X  x, y , D ( x, y )= D ( y, x ) . A dissimilarity matrix can be directly mapped to an undirected weighted graph G = V, E, W , where each node in V corresponds to an object in N , and each edge e = x, y with weight w depicts the dissimilarity D ( x, y ) between the two objects it connects. We denote the graph implied by the dissimilarity D as G ( D ).
Example : Figure 3 (B) shows a dissimilarity matrix of ob-ject set { A, B, C, D, E } . It satisfies both reflectivity(0 diago-Figure 3: A running example. (A) shows a PoClus-nal) and symmetry. This dissimilarity matrix can be mapped to the undirected weighted graph in Figure 4 ( d =4). Each node in graph corresponds to an object, each edge corre-sponds to a pair and the weight of the edge is the dissimilarity between the pairs of objects.

A clique is a fully connected subgraph in an undirected graph.The diameter of a clique is the maximum edge weight within the clique. A clique cluster is defined as a maximal clique with a diameter d . A diameter indicates the level of dissimilarity of the set of objects in the clique cluster.
Definition 3.1. (Clique Cluster). Let G ( D ) be an undi-rected weighted graph of a dissimilarity matrix D .Aclique cluster C = S, d is a maximal clique S with diameter d in graph G ( D ) .

When there are multiple cliques within the graph with the same diameter d , we denote this set of clique clusters as cliqueset( d ).

Example: Given the dissimilarity matrix shown in Fig-ure 3(B), ABCD forms a clique with maximum edge weight 2, as shown in Figure 4. Therefore, ABCD is a clique cluster with diameter 2. So is BCE . We denote them as cliqueset(2)= { ABCD, BCE } .
 The notion of clique cluster is not new. The intermediate clusters generated by hierarchical clustering using a complete linkage criterion is similar to clique clusters in spirit, since they both look for a cluster with minimum diameter. How-ever, when two clusters are merged in hierarchical clustering, the relationship(linkage) between two clusters to be merged solely depends on the maximum or minimum dissimilarity be-tween a pair of objects within two clusters. This oversimpli-fied similarity measure ignores many pair-wise relationships between objects in the two clusters. To best explore and preserve the information carried by a dissimilarity matrix, in this paper, we present PoCluster. PoCluster reveals clique clusters with all possible diameters present in the dissimi-larity matrix. The non-disjoint feature allows us to explore richer and deeper relationships among objects. We formally define PoCluster in Definition 3.2.

Definition 3.2. (PoCluster) Let D be a dissimilarity ma-trix, a PoCluster P of D is defined as which is the collection of clique clusters of all possible diam-eters in diameter set W ( D ) .

Example: The dissimilarity matrix in Figure 3(B) con-sists of 4 possible diameters, they are { 1 , 2 , 3 , 4 } diameter, we map them into an undirected graph, namely, di-ameter graph, where there exists an edge between two nodes only if their dissimilarities are smaller than or equal to the diameter. For each graph, there exists a set of cliques in it. For example, in Figure 4, when d = 1, there are four 2-cliques in the graph. The set of the clique clusters generated in each of the diameter graph is shown as poClusters in Figure 3(A).
Now, we examine the properties of PoClusters. Similar to hierarchical clustering, PoCluster also includes the set containing all the objects. This set has the maximum dis-similarity in D as its diameter. PoCluster does not ignore dissimilarity measures as hierarchical clusters do since each pair-wise dissimilarity is covered by at least one clique cluster whose diameter equals to the pair-wise dissimilarity. In ad-dition, the maximal clique cluster insures that if one cluster is a subset of the other, one X  X  diameter will be strictly lower than the other. This property generates a partial order of the clusters in the PoCluster as shown in Figure 3(A).
Property 3.1. Let D be a dissimilarity matrix of object set N ,Let P be a PoCluster of dissimilarity matrix D , P has the following properties. 1. N  X  P 2.  X  C 1 ,C 2  X  P ,if C 1  X  C 2 ,thendiam ( C 1 ) &lt; diam ( C 3.  X  x, y  X  N , there exists a cluster C  X  P , such that 4.  X  x  X  N , { x } X  P . Table 1: PoCluster generated based on dissimilarity ma-
Given a dissimilarity matrix D , the corresponding PoClus-ter P (i.e., P = { cliqueset( d ) | X  d  X  W ( D ) } ) can be found by repeating a simple procedure. One needs only to find all cliques in a subgraph of G ( D ) that includes only those edges corresponding to the pair-wise dissimilarities less than or equal to a threshold d as the threshold varies from the smallest to the largest dissimilarity in D . Finding a clique of size k in a graph is one of the original NP-complete prob-lems identified in Karp X  X  seminal paper [12]. The k-clique problem can be reduced in polynomial time to a PoCluster-ing problem, hence the PoClustering problem is NP-hard. In order to make a PoCluster reconstruction practical for large datasets, we present an incremental algorithm which takes advantage of already constructed clique clusters to generate their subsequent parent clusters.
 Algorithm 1 gen poCluster( E ) Input E:an ordered list of edges.
 Output P :aPoCluster 1: t =0; G = N,  X  . 2: E 0 = E 3: while E =  X  do 4: e  X  min ( E t ); E t +1  X  E t  X  e 5: C t +1  X  gen clique clusters( C t ,e ) 6: P  X  P  X  C t +1 ; t  X  t +1 7: end while 8: return P The incremental algorithm only computes cliques that are af-fected by the introduction of new edges. The algorithm keeps a pool of all cliques in the previous graph. Given the next graph with more edges, the pool of cliques can be updated as follows: First, find all the cliques in the pool that share a vertex with the new edges. Second, if a clique in the pool can be extended by adding one or more of the new edges, the extended maximal cliques are added into the pool, and the cliques in the old pool that are subgraphs of the newly added cliques are removed. The parent-child relationships can be established between new cliques and removed cliques.
Let x  X  V be a node in graph G ,andlet  X  ( x )denoteall the cliques containing x . lemma 4.1. Let G =( V, E ) be an undirected graph. Let C be the cliques contained in G ,andlet e =( x, y ) be the edge added to G . The cliques in the new graph G can be obtained based on the cliques in the graph G .
When a new edge is inserted, a new set of cliques are gen-erated based on the previous graph. Algorithm 2 implements the second part of the Lemma 4.1 while removing cliques in G t  X  1) from which the new cliques are derived.
 Algorithm 2 gen clique clusters( C, e ) Input e : an new edge and C : current cliques.
 Output C new : cliques after adding e . 1: ( x, y )  X  get vertices( e ) 3: for all c 1 ,c 2  X   X  ( x )  X   X  ( y ) do 4: c  X  c 1  X  c 2 5: if { c = c 1 } X  X  c = c 2 } then 6: C  X  C  X  X  c } 7: else if c  X  C new ,c  X  X  x, y } X  c then 8: C new  X  C new  X  X  c  X  X  x, y }} 9: end if 10: end for 11: C new = C new \{ c | c  X  c , c, c  X  C new } 12: return C new = C  X  C new
Our experiments are done on a subset of gene function cat-egories obtained from Gene Ontology. We compare hierar-chical clustering(Hierarchy), py ramidal clustering(Pyramid), and incremental optimal poCluster algorithm, to evaluate the their capabilities in preserving the structures.
The match score is used to measure the approximation of the recovered poset to the original poset. We take each poset as a set of sets. Given P 1 and P 2 ,thematchscoreof P 2 P 1 is computed as:
Our experiment compares the quality of the three clus-tering algorithms given a real ontology categorization. We also evaluate our method for deriving dissimilarities against previous approaches reviewed in Section 2.

We use 799 genes which are the most active and cell cy-cle co-regulated in the yeast cell cycle data of Spellman et al.(1998)[14]. We consider three GO files on biological pro-cess (BP), cellular component (CC), and molecular function (MF), from the Gene Ontology database. We extract all GO categories that contain at least two genes and remove dupli-cate categories.

The remaining GO categories are taken to generate dis-similarity matrices as the input to the clustering algorithms. Table 3: Statistics for the three GO files. MF: Molec-ular Function, CC: Cellular Component; BP: Biolog-ical Process Table 3 presents size and overlapping statistics of our data. The statistics of the three GO files are listed in Table 3.
We first compare the two possible similarity measures in-cluding Resnik, and Leacock and Chodorow (LC) [6]. These methods are applied to generate dissimilarity matrices of genes based on the structure of categories in GO. We de-rive two dissimilarity matrices, one by each method. We then apply the PoCluster algorithm to the dissimilarity ma-trices in order to reconstruct the categorization structure of GO. The matching scores of three different files are shown in Table 4. The result shows that Resnik method renders consistently higher recoverability result than LC method for all three GO files. Therefore, in the rest of experiment, we take Resnik method to measure the affinity of genes based on the GO categorization.
 Table 4: Reconstructed poset match score to original GO based on various similarity measures
We then apply the three algorithms: PoCluster, hierar-chical clustering and pyramidal clustering to reconstruct the original categorization. Among them, PoCluster performs the best in recovering the GO categories, while hierarchical and pyramidal clustering seems to miss many of the cate-gories. In this case, two additional measurements are used to evaluate the relationships between a reconstructed poset( P ) and GO files( go ). They are recovery rate and accuracy. The recovery rate is the percentage of GO categories recovered; the accuracy is the percentage of the clusters discovered that truly appear in GO categories. The two measurements pro-vide more information about those clusters. The recovery rate in the second column of Table 2 is more than 50% and even closer to 79%(MF) of the categories that cannot be properly discovered by hierarchical and pyramidal clustering. According to the third column in the same table, over 96% of the discovered clusters by PoCluster are actual matches to the GO categories. In comparison, the spurious clusters in pyramidal clustering and hierarchical clustering may exceed 50%, which is unacceptable in real applications.
We have presented a new clustering algorithm for the auto-matical generation a set of partially ordered clusters(PoCluster) based on the pairwise relationships between objects. The structure of a PoCluster is analogous to that of the classifica-tion ontology such as Gene Ontology by allowing overlapping between sibling categories and allowing one child category to have multiple parent categories.

PoClustering is a generalization of both hierarchical clus-tering and pyramid clustering. PoCluster provides both ho-mogeneity within a cluster, as measured by the cluster X  X  di-ameter, and separation between clusters. Different from dis-joint clustering algorithms, PoClusters allow overlaps be-and P is the reconstructed poset tween clusters in a meaningful way. However, given an ar-bitrary poset containing the whole object set and single-ton set, it might not be a valid PoCluster. For example, given a set of objects { A, B, C } ,aposetofthisobjectset as shown in Figure 5, is not a valid PoCluster. Assume the poset shown is a valid poset, according the second property listed in Property 3.1, we have diam( AB ) &lt; diam( ABC ), diam( AC ) &lt; diam( ABC ), and diam( BC ) &lt; diam( ABC ). However, the three conditions cannot be true at the same time since diam( ABC )=max(diam( AB ), diam( BC ), diam ( AC )). Therefore, this poset is not a valid PoCluster. One of the interesting questions that is worth further investigation is how to identify an arbitrary poset as a valid poCluster. Figure 5: An example of poset which is not a valid
As we know, the hierarchical clustering only preserves the information of the ultrametric dissimilarity matrix. By pre-serving, we mean there exists one to one correspondence be-tween the set of hierarchical clusters and the set of ultramet-ric dissimilarity matrices. Similarly, the pyramidal cluster-ing, which is an extension of hierarchical clustering preserves another special type of matrices, namely, the Robinson ma-trices. Applying pyramidal clustering on other matrices may cause information contained in the dissimilarity matrices to be ignored.

This inspires us to pursue the following questions: Is PoClus-ter able to fully preserve the information provided in a dis-similarity matrix? If so, what are the types of dissimilarity matrix? And how should we derive it from a PoCluster? In addition, how do they relate to the Robinson matrices or ultrametric matrices? The answers to these theoretical ques-tions may lead to deeper understanding of the relationships between the sets of PoClusters and the set of dissimilarity matrices.

The formal definition of PoCluster is primarily of theoret-ical interest, since computing the exact solution is likely to be intractable for large problems. In order to address the challenge, it is important to investigate an approximation algorithm which is scalable to large datasets in our future work. [1] M.Ashburner,CA.Ball,JA.Blake,D.Botstein,H.
 [2] Applications of the pyramidal clustering method to [3] P. Berkhin. Survey of clustering data mining techniques [4] P. Bertrand and M. F. Janowitz. Pyramids and weak [5] C. Bron and J. Kerbosch, Algorithm 457: Finding all [6] Budanitsky, A., and G. Hirst,  X  X emantic Distance in [7] E. Diday, Orders and overlapping clusters in pyramids. [8] R.O.Duda,P.E.Hart,andD.G.Stork,Pattern [9] L. K. Hua, Introduction to Number Theory.
 [10] A. JAIN and R. Dubes. Algorithms for clustering data. [11] J. L. Sevilla, V.Segura, A. Podhorski, E. Guruceaga, J. [12] R.M. Karp Reducibility among combinatorial [13] P.W. Lord, R. Stevens, A. Brass, and C.A.Goble. [14] P.T. Spellman, G.Sherlock, M.Q.Zhang, V.R.Lyer, [15] H. Wang, F. Azuaje, O. Bodenreider. An
