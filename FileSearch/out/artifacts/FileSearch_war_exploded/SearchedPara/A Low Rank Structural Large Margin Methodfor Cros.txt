 Cross-modal retrieval is a classic research topic in multime-dia information retrieval. The traditional approaches study the problem as a pairwise similarity function problem. In this paper, we consider this problem from a new perspec-tive as a listwise ranking problem and propose a general cross-modal ranking algorithm to optimize the listwise rank-ing loss with a low rank embedding, which we call Latent Semantic Cross-Modal Ranking (LSCMR). The latent low-rank embedding space is discriminatively learned by struc-tural large margin learning to optimize for certain ranking criteria directly. We evaluate LSCMR on the Wikipedia and NUS-WIDE dataset. Experimental results show that this method obtains significant improvements over the state-of-the-art methods.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Theory, Experimentation Cross-Modal Retrieval; Ranking; Low Rank Embedding
Nowadays many real-world applications involve multi-modal data. The ranking of cross-modal retrieval is imperative to many applications of the practical interest, such as finding relevant textual documents of a tourist spot that best match a given image of the spot or finding a set of images that vi-sually best illustrate a given text description. Therefore, it is desirable to support the ranking of multi-modal data, e.g., identifying the most relevant textual documents in response to a query image or vice versa. It is obvious that a good performance of ranking multi-modal data hinges upon an appropriate learning of the similarity across different modal-ities. The heterogeneity-gap between multi-modal data has been widely understood as a fundamental barrier for the cross-modal metric learning.

In recent years, there has been a great deal of research de-voted to the development of algorithms for learning an op-timal direct similarity metric between different modalities. Among existing research of cross-modal metric learning, one kind of popular approaches is to map the data with multiple modalities into a common (or shared) space such that the distance between two similar objects is minimized, while the distance between two dissimilar objects is maximized. These approaches (e.g., Canonical Correlation Analysis (CCA) [12] and [33]) usually assume a training set of strictly paired data and exploit the symbiosis of multiple-modality data which is common to describe the rich literal and visual semantics, such as a web image with loosely related narrative text de-scriptions, and a news report with collateral text and images. Another kind of approaches for multi-modal metric learning is the extensions of Latent Dirichlet Allocation (LDA) [5] which are conducted on two or more collections of multiple-modality data in an unsupervised manner. The LDA-based approaches tend to model correlations among multi-modal documents at a latent semantic ( topic ) level across different modalities, e.g. correspondence LDA [4].

We are particularly interested in ranking the multi-modal data (i.e. cross-modal ranking ) in this paper. Different from the two aforementioned categories of approaches which do not maximize a criterion related to the final ranking perfor-mance, recent years have witnessed the efforts in learning an optimal similarity (ranking) function between different modalities by using learning to rank techniques. These ap-proaches (e.g. [11, 23, 2]) are supervised but do not enforce a strict assumption that the trained multi-modal data must be paired (e.g., one image is in pair-correspondence with its collateral text). They in fact need some lists of ranked data related to the queries for training where the training exam-ples can be easily obtained from the abundance of users X  clickthrough data with little overhead [18, 10]. In this way, the learned metric for ranking multi-modal data is generally optimized for a ranking-based loss function (evaluation cri-terion) to preserve the orders of the relevance instead of the purely absolute values of similarity (dis-similarity) between multi-modal data.

A discriminative kernel-based method PAMIR proposed in [11] solves the problem of cross-modal ranking by adapt-ing the Passive-Aggressive algorithm [8]. However, PAMIR is inherently a pairwise cross-modal ranking approach; its ranking performance is limited by the distribution of the pairs of items and skewed data may even deteriorates the ranking result. These are also the same problems for another pairwise approach SSI [2] proposed by Bai et al. Moreover in the real world, a long search query (e.g., a whole docu-ment) is beneficial because users X  intents can be described in more detail [31]. Therefore, it is appropriate to support long textual documents as queries in cross-modal ranking. It is obvious that PAMIR cannot easily expedite the long queries and restrains itself from a more flexible application of cross-modal ranking in the setting of uncontrolled multi-modal data involved.

This paper aims to bridge the gap between learning a la-tent space and ranking for multi-modal data. We consider the problem of cross-modal retrieval from a new perspec-tive as a listwise ranking problem in this paper, and pro-pose a general cross-modal ranking algorithm to optimize the listwise ranking loss meanwhile considering a low rank embedding, called Latent Semantic Cross-Modal Ranking (LSCMR). LSCMR employs the structural SVM [29] to sup-port the optimizations of various ranking evaluation mea-sures (e.g, MAP [32] and NDCG [7]) under a unified algo-rithmic framework. LSCMR also incorporates a low rank embedding in the learning procedure in which the latent as-pect space is induced to address the curse of dimensionality and discover the correlations between different modalities.
It is worthwhile to highlight the main motivations of the proposed method. We would like the method to benefit both from the low rank embedding and the most recent advances in learning to rank techniques:
We show experimental results on the ranking of cross-modal data obtained from two real-world datasets. The proposed LSCMR outperforms other cross-modal ranking approaches. LSCMR is particularly appropriate for cross-modal ranking due to its structural large margin and low-rank listwise ranking pursuing.

The rest of this paper is organized as follows. In Section 2, we describe the method in detail and show its feasibility. Section 3 discusses the existing work. We compare the pro-posed LSCMR with other cross-modal ranking approaches on two real-world datasets in Section 4. Conclusions are given finally.
The proposed method LSCMR is a general cross-modal retrieval framework in the sense that it can be applied in both directions of image-query-text retrieval and text-query-image retrieval. Consequently, a query here may be either an image or a text document. Similarly, a retrieved document can either be an image or a text document. For a clear articulation in the rest of this section, the algorithm is only derived in the case of text-query-image retrieval. We report the experiments in both scenarios.
All vectors are assumed to be column vectors and a super-script T denotes the transpose of a matrix or vector. Denote the text query as q  X  R m and the retrieved image as d  X  R where m is the dimension of the text space (e.g., vocabu-lary size of bag-of-words (BoW)) and n is the dimension of the image space (e.g., vocabulary size of bag-of-visual-words (BoVW) which is quantized by clustering from low-level vi-sual features such as SIFT [22]). We are given a training set of N samples, in which each contains a text query q i ( i = 1 , . . . , N ) as well as a set of corresponding retrieved images d i with their true rankings y  X  i  X  X  , where Y denotes the set of all possible permutations (rankings). We formu-late a ranking as a matrix of pair orderings as [32] does, Y  X  { X  1 , 0 , +1 } | d | X | d | where the operator || denotes the number of elements in a set. For any y  X  Y , y ij = +1 if document d i is ranked ahead of document d j , and y ij =  X  1 if d j is ranked ahead d i , and y ij = 0 if d i and d j have equal rank. We consider only matrices with correspond to valid rankings (i.e., obeying antisymmetry and transitivity). Moreover, assume that the true ranking is a weak ranking with two rank values ( relevant and irrelevant ). For any tex-tual query q i , let d + i and d  X  i denote the set of relevant and irrelevant images in d i , respectively. For simplicity, we omit the subscript i of q i and d i when it is clear from the context.
We consider that learning scores for ranking from a super-vised manner, in which the ranking of images corresponding to a given textual query is available for training. Unlike the uni-modal data ranking, cross-modal ranking attempts to learn a similarity function f ( q, d ) between a text query q and an image d according to a pre-defined ranking loss. The learned function f maps each text-image pair to a ranking score based on their semantic relevance.
Given a text query q  X  R m and an image d  X  R n , we tend to learn a linear scoring function to measure the relevance of d given q : where f ( q, d ) is the score (or similarity) between the query q and the image d , and the only parameter W  X  R m  X  n in the linear model captures the correspondence of the two differ-ent modalities of data as a weighting matrix: W ij weights the correlation between the i th dimension of the text space and the j th dimension of the image space. Note that both positive values and negative values in W ij are allowed in which a negative value represents a negative correlation.
Motivated by the idea of low rank embedding, a low rank prior is introduced into matrix W with W = U T V in equa-tion (1), which results in a low rank ranking model [2]: where U  X  R k  X  m and V  X  R k  X  n . U refers to map the query text q from the m -dimensional text space to the k -dimensional latent space by a liner mapping, and V refers to map the retrieved image d from the n -dimensional image space to the k -dimensional latent space. Therefore, the text query and the retrieved image are mapped to a common k -dimensional latent aspect space, and then their similarity is measured by a dot product of the two vectors in the k -dimensional space, which is commonly used to measure the matching between textual vectors [1].

Intuitively, the low rank model in equation (2) helps us deal with the problem of textual/visual synonymy and poly-semy which particularly occur in cross-modal retrieval. Note that Latent Semantic Indexing (LSI) [9] takes into account of the correlations between textual words (synonym and poly-semy) in a single modality in an unsupervised manner, while the low rank model in equation (2) attempts to capture the correlation across two different modalities from a supervised manner. By constraining the form of W with a low rank form, the benefits are similar to LSI: U and V not only in-duce a k -dimensional latent aspect space but are also faster to compute and lead to much smaller storage requirements by representing the image documents in k dimensions in-stead of the original n dimensions ( k is chosen much smaller than m or n ). Besides, from the viewpoint of statistical learning theory, fewer parameters ( k ( m + n )  X  mn ) lead to a better stability and generalization in performance.
Similar to [2], here U and V are different and there is no assumption that the query texts and the target images should be embedded to the latent space in the same way. This is appealing to cross-modal ranking since the distribu-tions of the query texts and the target images are inherently different due to the heterogeneity-gap.

We can obtain a prediction y for each input query q and its corresponding ranked target images d by sorting the f ( q, d ) in a descending order. The rest is to learn U and V . We aim to obtain the values of U and V by the minimization of the following empirical risk, where the non-negative loss function  X  : Y X Y  X  R quanti-fies the penalty for making prediction y i if the correct output is y  X  i , which is typically bounded in [0 , 1]. For example, we define the loss function  X  with the average precision (AP, detailed definition in Equation (14)) loss as follows: and then to minimize the empirical risk is to maximize the Mean Average Precision (MAP). Note that one can con-struct different ranking objective problems by considering different ranking loss function  X .
In this section, we present the formulation of LSCMR in details. The proposed LSCMR is inspired by the structural SVM framework [29], especially SVM map [32] for optimiz-ing the average precision. The algorithmic illustration of LSCMR is presented in Figure 1.

The motivation of LSCMR is to learn a cross-modal rank-ing function h : X  X  Y between an input space X (a text query q as well as all possible target images) and output space Y (rankings over the image set). Similar to SVM, we can derive a prediction by finding the ranking y that maxi-mizes the following discriminant function h : where F is considered as a compatibility function param-eterized by U, V that measures how compatible the triple ( q, d , y ) are.

By adapting the most commonly used partial order com-bined feature representation in [19] to the cross-modal rank-ing, we define F as: where y ij = +1 if image d i is more preferred (more relevant to query q ) than image d j , and y ij =  X  1 otherwise since we assume that the predicted rankings are complete.
 One attractive property of F is that for the fixed U and V , the ranking y which maximizes function F (then the predicted ranking) is simply sorted by descending f ( q, d ) = ( Uq ) T V d . To see this, we note that F is a summation over the differences of all relevant/irrelevant document pairs since we assume weak rankings with two rank values. Since F de-composes linearly over the pairwise representation, we can maximize F by optimizing each y ij individually: if ( Uq ) T V d is the same procedure as sorting documents by descending f ( q, d ). More details can be obtained from [19]. We note that this simple prediction rule establishes a connection be-tween the compatibility function F and the aforementioned low rank ranking model.

Since U and V are independent to the summation in equa-tion (5), we rewrite F as a linear function of U T V : where Here the combined feature function  X ( q, d , y ) is a summa-tion over the vector differences of all the relevant/irrelevant image pairs. By representing the scoring F as a Frobenius inner product of U T V and  X , we see that it is straightfor-ward to extend the idea of the structural SVM to learn the cross-modal ranking function F .

For the purpose of learning to rank, the structural SVM takes a set of vector-valued features which characterize the relationship between the input query and a set of target documents as the input, and returns a ranking list y  X  Y of the target documents. The structural SVM is applied to maximize the margins between the true ranking list y  X  and all the other possible lists y . In this paper, LSCMR takes cross-modal ranking into consideration, for i = 1 , . . . , N : where for compactness, we define
Since we assume that the query texts and the target im-ages are embedded into a common latent space, respectively, LSCMR adapts the original structural SVM to learn the op-timal U  X  and V  X  which maximize the margins between the true ranking and all the other possible rankings of the target images for each text query. Hence, we replace the standard kk F denotes the Frobenius norm. Intuitively, this extension simplifies the model complexity, thereby promoting a better generalization performance.
 The optimization problem is then presented as follows:
Optimization Problem 1.
For each triple ( q i , d i , y i ) in the training set, a set of con-straints (10) are added to the optimization problem. To see how these constraints indeed work, note that during the pre-diction the model chooses the ranking  X  y i which maximizes F ( q i , d i , y ) given the fixed U and V . If the predicted ranking is an incorrect ranking  X  y , i.e., F ( q i , d i ,  X  y where y  X  i is the true ranking, the corresponding slack vari-able  X  i must be at least  X ( y  X  i ,  X  y i ) to satisfy the constraint. of slacks (i.e., 1 N P N i =1  X  i ) upper bounds the empirical risk R  X  ( f ) defined in Equation (3). This is stated formally in Proposition 1.

Proposition 1. Denote by  X   X  ( U, V ) the optimal solution of the slack variables in Optimization Problem 1 for the given parameters U and V . Then 1 N P N i =1  X   X  i is an upper bound on the empirical risk R  X  ( f ) .

Similar to SVM, to avoid overfitting, the objective func-tion (9) to be minimized is a tradeoff between the model complexity, and a hinge loss relaxation of  X  loss. A pre-chosen value of parameter  X  controls this tradeoff and can be tuned to achieve a good performance via the cross vali-dation procedure.

Note that by exploring the low rank property, the opti-mization problem is not convex. The well-known kernel trick is difficult to be applied to (9), while kernel trick is consid-ered as one of the main benefits of the traditional support vector machine. Fortunately, a linear-SVM without using kernels has been shown to give competitive performances for textual documents classification [13]. On the other hand, ac-cording to the cross-modal retrieval approach PAMIR [11], a linear mapping of BoVW yeilds the highest performance of the other kernel mapping methods. As a result, with the multi-modal data under a certain feature representa-tion, we argue that the model can indeed capture the linear structures of the multi-modal data to learn a cross-media semantic representation.
Since |Y| is super-exponential in the size of the train-ing set, our algorithm for learning U and V is adapted from the 1-slack margin-rescaling cutting-plane algorithm of Joachims et al [20]. The algorithm alternates between two steps, one optimizing the model parameters ( U and V in our case) and the other updating the constraints set with a new batch of rankings (  X  y 1 , . . . ,  X  y N ) (  X  y i one query sample, i = 1 , . . . , N ) which most violate the cur-rent constraints. Once reaching a stopping criterion based on the accuracy of the empirical risk (the new constraint batch X  X  empirical risk is no more than that of the current set of constraints within a tolerance  X  &gt; 0), the algorithm terminates.

The general optimization procedure of LSCMR is listed in Algorithm 1. The code is implemented in MATLAB. The proof of the correctness can be easily extended from [20]. Algorithm 1 Latent Semantic Cross-Modal Ranking (LSCMR).
 Output: mapping parameters U and V , slack variable  X   X  1: W  X  X  X  2: repeat 3: Solve for the optimal U , V and slack  X  : 4: for i = 1 to N do 5:  X  y i  X  argmax 6: end for 7: W  X  X  X  (  X  y 1 , . . . ,  X  y N ) 8: until 9: return U, V,  X  ;
To solve the optimization problem in Algorithm 1, there are two key issues to be resolved. One is searching for the most violated constraints, the so-called separation oracle , in Step 5. For different loss functions  X ( y  X  , y ), different methods are proposed to address this issue, for example, [19] for AUC loss (defined as 1  X  AUC( y  X  , y )) and [32] for MAP loss. Recalling that F is the Frobenius inner product of U T V and  X , their work [19, 32] can be easily applied to this algorithm with a minor modification in implementation to reduce the computational complexity.

The other key issue is how to solve the optimization prob-lem in Step 3. Since the problem is not a convex problem, the parameters U and V are initialized with their previ-ous (local) optimal values while in the beginning they are randomly initialized using a normal distribution with mean zero and standard deviation one. We have implemented a subgradient descent solver adapted from Pegasos algorithm [27] originally proposed for solving a traditional support vec-tor machine. The Pegasos algorithm is a simple iterative algorithm which alternates between stochastic subgradient descent and projection steps, and is shown to be effective to solve the primal problem of SVM. In the problem, the subgradient descent is performed by iteratively picking the most violated ranking tuple (  X  y 1 ,  X  y 2 , . . . ,  X  y to minimize the slack variable.
 On iteration t , the update for U is given by: where  X  t is the learning rate on iteration t which is adjustable and  X   X ( q i , d i ,  X  y i ) ,  X ( q i , d i , y  X  i )  X   X ( q obtained by projecting U t + 1 (see [27]): The update for V can be derived similarly except for the most violated ranking tuple which is computed using the updated U t +1 . The update is calculated exactly as given by: followed by the projection step (12).

Moreover, our problem is a bit different from Pegasos [27]: control the model perplexity. It should be noted that the optimal U and V must satisfy the condition k U k F = k V k since the prediction rule uses the product U T V only. Thus after each subgradient descent, the updated U and V are forced to be multiplied with a constant respectively to en-sure k U k F = k V k F while keeping k U T V k F fixed. Let  X  = p k U k F k V k F , The experiments show that this strategy yields a much faster convergence rate. For fixing tolerance  X  = 0 . 01, the loop in Algorithm 1 usually terminates within 200 iterations.
There has been a great deal of research devoted to the de-velopment of algorithms for learning the similarity between the data with different modalities in order to perform cross-modal retrieval. Most of cross-modal metric learning ap-proaches tend to project multi-modal data into a common (or shared) subspace so that the correlation between multi-modal data is preserved or maximized. As one of the most popular approaches to finding a pair of linear transforma-tions to maximize the correlations between two variables, Canonical Correlation Analysis (CCA) [15] and its exten-sions are applied in cross-modal similarity learning. For example, after the maximally correlated subspaces of text and image features are obtained by CCA, logistic regression is employed to cross-modal retrieval in [26]. As a super-vised kernelizable extension of CCA, Generalized Multiview Analysis [28] is conducted to map data in different modality spaces to a single (non)linear subspace. Motivated by the fact that dictionary learning (DL) methods have the intrinsic power of capturing the heterogeneous features by generat-ing different dictionaries for multi-modal data, multi-modal dictionary learning is recently applied to cross-modal metric learning [16, 24]. Following the the seminal work of Blei et al. [5], LDA has been extended to learn the joint distribution of multi-modal data (e.g., text and imagery) such as corre-spondence LDA [4], topic-regression multi-modal LDA [25], Multi-modal Document Random Field [17] and hierarchical Dirichlet process (HDP)-based LDA [30].

The aforementioned approaches, either optimizing the sim-ilarity (distance) between pairs of samples or optimizing the likelihood of the topic models, do not optimize for the final ranking performance directly. While bearing a resemblance to multi-modal metric learning which aims at learning the similarity or the distance measure from multi-modal data, the multi-modal ranking function is generally optimized by an evaluation criterion or a loss function defined over the permutation space induced by the scoring function over the target documents.

Traditionally, algorithms of learning to rank can be cat-egorized into the pointwise approaches, the pairwise ap-proaches, and the listwise approaches. The main differences among these three categories of approaches actually lie in the input representations and the loss functions employed in training. It is observed that the listwise and pairwise ap-proaches usually outperform the pointwise approaches [21].
In [18] Joachims et al. trained a Ranking Support Vec-tor Machine (RankSVM) to learn the weights of the hand-designed features in which the training set was a set of doc-uments preference pairs obtained through the clickthrough data from the query-log of a search engine. The retrieval function is automatically learned by taking a support vector machine. The goal of RankSVM is to minimize the aver-age number of the inversions in ranking; thus the method is considered as a pairwise preference satisfaction approach.
Unlike the pairwise approaches, Cao et al. [6] first noticed the fact that ranking was a prediction task on a list of docu-ments and took the ranking lists as training instances. They trained two probabilistic models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning.

Yue et al. [32] proposed another listwise approach SVM map to solve the problem of learning to rank in a discriminative way. The method uses the structural SVM framework [29] with the loss function defined as MAP (mean average preci-sion) loss that globally optimizes a hinge-loss relaxation of MAP loss. This method simplifies the process of obtaining ranking functions with a high MAP performance by avoiding the additional intermediate steps and heuristics.
Chakrabarti et al. [7] proposed almost-linear-time algo-rithms to optimize MRR (mean reciprocal rank) and NDCG (normalized discounted cumulative gain). Further, they folded multiple ranking loss functions into a multi-criteria max-margin optimization problem to develop a single, robust ranking model with close to the best accuracy of the learners trained on individual criterions.

Different from the aforementioned uni -modal learning to rank techniques, to the best of our knowledge, Passive Ag-gressive Model for Image Retrieval (PAMIR) is the first at-tempt to address the problem of ranking images by text query directly [11]. PAMIR formulates the cross-modal re-trieval problem similar to RankSVM and derives an efficient training procedure by adapting the Passive-Aggressive algo-rithm.

The authors of [23] studied metric learning as a prob-lem of learning to rank. They presented a general metric learning algorithm based on the structural SVM, to learn a metric such that the ranking of data induced by the distance from a query can be optimized again various ranking mea-sures. Different from LSCMR, they focused on learning a intra-modality metric which restricts a positive semi-define matrix W (see Eq. (1)) to learn a valid metric and does not introduce a low rank embedding.
 The text and imagery are usually represented as BoW and BoVW in a high-dimensional vector space. However, the high-dimensional vector space representation suffers from its inability to cope with two classic problems, i.e., synonymy and polysemy. To capture the latent semantic associations of data and to address these problems, embedding words in a low-dimensional latent space to capture the semantics is a classic approach in text retrieval such as Latent Seman-tic Indexing (LSI) [9] and pLSA [14]. The idea of low rank embedding is introduced into Supervised Semantic Indexing (SSI) for cross-lingual retrieval [2]. SSI defines a set of lin-ear low rank models to take account of correlations between words (synonymy and polysemy). Related to SSI, Polyno-mial Semantic Indexing (PSI) [3] generalizes and extends the SSI approach to general polynomial models which could be used to capture the higher order relationships among words.
The main goal of the experiments is to evaluate the ef-fectiveness of the proposed LSCMR approach. To show its competitive performance, LSCMR is compared with other three state-of-the-art approaches (CCA, PAMIR and SSI) for cross-modal ranking.

These comparative methods are elaborately chosen for the fair comparisons. Comparing with the classical CCA method aims to test LSCMR X  X  ability to learn a useful la-tent space; PAMIR has been shown to outperform pLSA and SVM [11]; however it does not consider a latent space; SSI introduces the low rank parameterizations while it min-imizes a pairwise ranking loss and lacks of parameter reg-ularization. Since LSCMR can generate low rank matrices U and V (see Eq. (2)), in the experiments, we demonstrate that the learned model also discovers the latent correlations between textual words and topics.
Two public real-world datasets are used in the compara-tive experiments. They are the largest available multimodal datasets that are fully paired and labeled (tagged), to the best of our knowledge. Both datasets are bi-modal with the image and the associated text modalities. The statistics of the two datasets are summarized in Table 1.

The first dataset, Wikipedia feature articles 1 , consists of 2,866 images, each with a short paragraph describing the http://www.svcl.ucsd.edu/projects/crossmodal/ BoVW vocabulary size 1000 500 BoW vocabulary size 5000 1000 Avg. # of words/image 117.5 7.73 a Partitions are ordered by training/validation/test. image. The images are labeled with exactly one of the 10 different semantic classes, such as art and geography. In the originally provided dataset, the text comes with a 10 di-mensional feature vector representing the probabilistic dis-tributions over the 10 topics, which are derived from a La-tent Dirichlet Allocation (LDA) model [5]. We note that LSCMR and the comparative methods all resort to the raw low-level features rather than the high-level semantic fea-tures. For the training text, we extract 5,000-dimensional feature vectors using the bag of words (BoW) representation with the TF-IDF weighting scheme. For images, we first ex-tract SIFT points from each images in the dataset. The randomly selected SIFT points are clustered by k -means to generate 1,000 centers as the visual dictionary. Then each image is quantized into a 1,000 dimensional histogram fea-ture vector using the bag-of-visual-words (BoVW) model.
The second dataset, NUS-WIDE 2 , contains 133,208 im-ages with 1,000 tags and 81 concepts, which are pruned from the NUS dataset by keeping the images that have at least one tag and one concept. For the feature representation, we use the publicly available 1,000 dimensional text feature vec-tor (namely tags) and 500 dimensional image feature vector based on SIFT BoVW kindly provided by the authors.
Another reason why we choose these two datasets is due to the large difference on the average number of the tex-tual words per image and the dimensionality of the text space. In Wiki dataset the textual descriptions are based on Wikipedia surrounding paragraphs which yield a 5,000 dimension text space and in average there are 117.5 sur-rounding words per image. The NUS dataset, on the other hand, is based on Flickr user-provided tags which yield a 1,000 dimension text space and in average there are 7.73 words (tags) per image. A manual examination reveals that the synonymy and polysemy problem may occur more fre-quently in the Wiki dataset than in the NUS dataset. For their difference, first we want to examine our algorithm X  X  ability to learn a latent space for the Wiki dataset and sec-ond we want to see whether our algorithm decays rapidly with the NUS dataset.

Note that the two datasets are both presented by pairs of text and imagery where CCA can be trained by this setting. For the other three methods (PAMIR, SSI and LSCMR), the restriction of paired correspondence of a text document and an image is not needed. On the contrary, the queries and the corresponding ranking lists are needed as training exam-ples of the three methods. These training examples originate from both direction of the text-query-image retrieval and the image-query-text retrieval. For this purpose, we first define the relevance assessment. For the Wiki dataset, we define http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm that a target document d is relevant to a query q if d and q belong to the same semantic class. Similarly, for the NUS dataset, a target document d is relevant if it shares at least one concept with query q . The ranking examples are gen-erated as follows: for each text (image) query, we randomly selected 40 images (text documents) in the other modal-ity in the training set as candidates and then the selected target documents are automatically labeled as relevant or irrelevant to form a ranking example.

For all the 2,866 generated ranking examples in the Wiki dataset, we randomly sample 1,500 examples to form the training set, of which 500 examples form the validation set. The rest are used to form the testing set. For NUS, 2,664 ranking examples are randomly selected to be the training samples and 2,000 to be validation samples (see Table 1). To be fair, all the comparative approaches are trained and tested on the same training set and testing set respectively.
For both datasets, performance evaluations are conducted using standard information retrieval metrics. We use Mean Average Precision (MAP) as the performance measures. Let p  X  = rank( y  X  ) (true ranking with two rank value +1 and  X  1) and p = rank( y ) (predicted ranking with a total order). Given a query and a set of R retrieved target documents, the Average Precision (AP) is defined as where L is the number of the relevant documents in the retrieved set, P rec ( j ) is the percentage of the relevant doc-uments in the top j documents in predicted ranking p and Rel ( j ) is an indicator function equaling 1 if the item at rank j in predicted ranking p is a relevant document, zero other-wise. We then average the AP values from all the queries in the query set to obtain the MAP score. The larger the MAP, the better the performance. In the experiments, R is the number of the retrieved documents to be examined, where we set R = 50 or R = all for all the retrieved docu-ments. Recalling that our model can be optimized for var-ious ranking measures, we implement the greedy algorithm for optimizing the average precision proposed in [32].
We report the performance results on both directions of ranking images from text queries ( text-query-image ) and ranking text documents from image queries ( image-query-text ). Besides, to give an pictorial demonstration of an al-gorithm X  X  performance, the Precision-Recall curves are also reported on all the approaches.
Table 2 reports the performance of LSCMR and the other comparative models on the testing set of the Wiki dataset, showing that LSCMR outperforms all the comparative meth-ods on both directions of the retrieval tasks. Compared to the best comparative methods, the minimum relative im-provement is 9.6 percent gained by LSCMR for the image-query-text retrieval with R = 50 and the maximum is 25.3 percent also for the image-query-text retrieval with R = all . This improvement is due to the latent semantic space. To verify this, we note that the low-rank based SSI also outperforms PAMIR in the image-query-text retrieval while PAMIR even controls the model complexity by optimizing an adapted cross-modal RankSVM model. Further, LSCMR outperforms SSI due to the structural large margin that reg-(c) NUS Text Query Table 2: The performance comparison in terms of MAP@ R scores on the Wiki dataset. Each text doc-ument is represented as 5000-D BoW and each im-age is presented as 1000-D BoVW. Both directions of ranking tasks are reported. The results shown in boldface are the best results.
 Query Image Query ularizes the model and optimizes for MAP ranking loss di-rectly. The Precision-Recall curves on both directions are re-ported in Figure 2(a) and 2(b). The Precision-Recall curves further validate the superiority of LSCMR for the cross-modal ranking.

Recall that the CCA model is trained by the pairs of data objects with different modalities and learns a unified model for the retrieval tasks of both directions. Hence, CCA has achieved nearly the same performance in both image-query-text retrieval and text-query-image retrieval. On the other hand, the performances of other three approaches (including LSCMR) are noticeably different in the corresponding both directions of the retrieval.
The improvement of LSCMR on the NUS dataset is not as significant as that on the Wiki dataset. The MAP scores of all the methods are shown in Table 3 and the Precision-Recall curves are reported in Figure 2(c) and Figure 2(d). For text-query-image retrieval, LSCMR outperforms the other comparative methods again while for image-query-text re-trieval LSCMR outperforms all the comparative methods in all the cases except for the case of R = 50 where PAMIR has a slightly better overall performance than LSCMR.
Recall that in the NUS-WIDE dataset, one image is as-sociated with about seven annotated words in average. The low rank embedding does not help much for querying short texts in the task of image-query-text retrieval. PAMIR and LSCMR both train a regularized model, and therefore the performances of PAMIR and LSCMR is undoubtedly super to SSI in the image-query-text retrieval.

Once again, it is observed that CCA has a very similar performance in both directions of the retrieval.
 Table 3: The performance comparison in terms of MAP@ R scores on the NUS dataset. Each text doc-ument is represented as 1000-D BoW and each im-age is represented as 500-D BovW. Both directions of ranking tasks are reported. The results shown in boldface are the best results.
 Query Image Query
It is noted that LSCMR has a better overall performance for text-query-image retrieval than for image-query-text re-trieval in the Wiki dataset and a better overall performance for image-query-text retrieval than for text-query-image re-trieval in the NUS dataset. The reason exactly lies in the low-rank embedding of LSCMR that is capable of discern-ing the latent aspect space and consequently supports rich-semantic queries. For the Wiki dataset, each document has over one hundred words in average, resulting in a long text query which is presumably much richer in semantics than the case in the NUS dataset where each image has only about seven text words in given as the annotation that is much shorter when posed as a query. On the other hand, the overall image quality in the NUS dataset is much richer and more diverse in semantics than that in the Wiki dataset, and thus resulting in a better overall performance for image-query-text-retrieval.

As we have stated that for both datasets CCA achieves very similar performances in both directions of the retrieval tasks. The reason is that CCA learns a unified model from paired multi-modal data in which the pair-correspondence of images and text documents ensure an equal contribution to the learned metric between both modalities. However, a unified model like CCA does not give a good performance which is verified in both datasets. Our explanation is as fol-lows: it is not the problem of unified models but the problem of taking strictly paired data as training instances; in such settings CCA can not capture the complete ranking informa-tion (e.g. dissimilarity between data in different modalities). Nevertheless, learning a unified model with ranking lists is still our interests for future work.

Overall, the results on the Wiki dataset and NUS dataset demonstrate that the use of LSCMR is advantageous for rich-semantic queries and has a superior performance in both Figure 3: The performance (MAP@50) comparisons for the Wiki dataset when the dimensions of latent space are set to different values. The dimensional-ity of 10 is reported to be the best for the overall performance of text query and image query.
 Table 4: Exemplar words along with their top 9 neighboring words from the Wiki dataset.
 Fiction Stories Manuscript Tales Language Theatre Football Teams Player NHL Goal Conference Compete DVD Moves Brand Broadcasting Movies Victims Airport Passenger Oregon Democratic Communities directions of the cross-modal retrieval than that of the peer methods from the state-of-the-art literature. The results over the Wiki dataset outline the advantage of LSCMR over the comparative solutions, especially PAMIR. This observation is certainly due to the low rank embedding used in LSCMR. Now we look into the learned embedding latent aspect space for the Wiki dataset. Since the BoVW features used in our experiments originate from the SIFT points which are difficult to illustrate, we only demonstrate the latent space by textual words.

First, the dimensionality of the subspace (also the size of U and V ) is a parameter to be determined before solv-ing Optimization Problem 1. We tune this parameter via a validation set and the MAP@50 performance over a candi-date set of the chosen dimensionalities is reported in Figure 3. It is observed that LSCMR performs the best with the optimal subspace dimensionality of 10. When the chosen dimensionality of the latent subspace is larger (even much larger) than 10, the performances do not decay rapidly at the same time it is observed that the topics overlap. But we find something interesting that though some topics overlap, some smaller but more precise topics are discovered (see the two  X  X iology X  topics in Table 5).

Second, consider the mapping of textual words into the latent space in LSCMR. For a text query q , Uq maps q into the latent space. Note that U  X  R k  X  m where k represents the dimensionality of the latent space. For each row in U , the row weights the contribution of all words to the corre-sponding  X  X opic X  in the latent space. The larger the number U ij , the more positive correlation between topic i and word j . We then sort every row by which the most relevant words are ranked ahead. We present some topic examples in Table 5. The columns in U acts a similar way like the rows except for that each column represents the relevance between the corresponding word and all topics. We define two words are neighbors if their relevance with all topics are similar. Some examples on the neighboring words are shown in Table 4.
In this work, we have presented a new approach to solving the problem of cross-modal retrieval by casting the problem as a problem of learning to rank in a supervised manner with the idea of low rank embedding. We have demonstrated the effectiveness of our proposed method LSCMR and have shown significant improvements over the comparative meth-ods especially on two datasets. We have also investigated the interpretability of the leaned low rank model by show-ing some examples on the textual topics and the neighboring words. This work is supported by 973 Program (No. 2012CB316400), NSFC (61070068, 90920303), 863 program (2012AA012505), Chinese Knowledge Center of Engineering Science and Tech-nology (CKCEST) and China Academic Digital Associative Library (CADAL). Zhongfei Zhang is also supported by US NSF (IIS-0812114, CCF-1017828). [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] B. Bai, J. Weston, D. Grangier, R. Collobert, [3] B. Bai, J. Weston, D. Grangier, R. Collobert, [4] D. Blei and M. Jordan. Modeling annotated data. In [5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [6] Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. Learning [7] S. Chakrabarti, R. Khanna, U. Sawant, and [8] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, [10] J. Gao, W. Yuan, X. Li, K. Deng, and J. Nie. [11] D. Grangier and S. Bengio. A discriminative [12] D. Hardoon, S. Szedmak, and J. Shawe-Taylor. [13] C. Ho and C. Lin. Large-scale linear support vector [14] T. Hofmann. Probabilistic latent semantic indexing. In [15] H. Hotelling. Relations between two sets of variates. [16] Y. Jia, M. Salzmann, and T. Darrell. Factorized latent [17] Y. Jia, M. Salzmann, and T. Darrell. Learning [18] T. Joachims. Optimizing search engines using [19] T. Joachims. A support vector method for [20] T. Joachims, T. Finley, and C. Yu. Cutting-plane [21] H. Li. Learning to rank for information retrieval and [22] D. Lowe. Distinctive image features from [23] B. McFee and G. Lanckriet. Metric learning to rank. [24] G. Monaci, P. Jost, P. Vandergheynst, B. Mailhe, [25] D. Putthividhy, H. Attias, and S. Nagarajan. Topic [26] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, [27] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [28] A. Sharma, A. Kumar, H. Daume, and D. Jacobs. [29] I. Tsochantaridis, T. Joachims, T. Hofmann, and [30] S. Virtanen, Y. Jia, A. Klami, and T. Darrell. [31] Y. Yang, N. Bansal, W. Dakka, P. Ipeirotis, [32] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [33] Y.-T. Zhuang, Y. Yang, and F. Wu. Mining semantic
