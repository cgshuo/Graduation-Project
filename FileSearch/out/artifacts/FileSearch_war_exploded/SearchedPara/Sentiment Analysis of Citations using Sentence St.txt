 Sentiment analysis is the task of identifying positive and negative opinions, sentiments, emotions and at-titudes expressed in text. Although there has been in the past few years a growing interest in this field for different text genres such as newspaper text, reviews and narrative text, relatively less emphasis has been placed on extraction of opinions from scientific liter-ature, more specifically, citations. Analysis of cita-tion sentiment would open up many exciting new ap-plications in bibliographic search and in bibliomet-rics, i.e., the automatic evaluation the influence and impact of individuals and journals via citations.
Existing bibliometric measures like H-Index (Hirsch, 2005) and adapted graph ranking algo-rithms like PageRank (Radev et al., 2009) treat all ci-tations as equal. However, Bonzi (1982) argued that if a cited work is criticised, it should consequently carry lower or even negative weight for bibliometric measures. Automatic citation sentiment detection is a prerequisite for such a treatment.

Moreover, citation sentiment detection can also help researchers during search, by detecting prob-lems with a particular approach. It can be used as a first step to scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions.

For other genres a rich literature on sentiment de-tection exists and researchers have used a number of features such as n -grams, presence of adjectives, adverbs and other parts-of-speech (POS), negation, grammatical and dependency relations as well as specialised lexicons in order to detect sentiments from phrases, words, sentences and documents. State-of-the-art systems report around 85-90% ac-curacy for different genres of text (Nakagawa et al., 2010; Yessenalina et al., 2010; T  X  ackstr  X  om and Mc-Donald, 2011).

Given such good results, one might think that a sentence-based sentiment detection system trained on a different genre could be used equally well to classify citations. We argue that this might not be the case; our citation sentiment recogniser uses spe-cialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the fol-lowing observations:  X  Sentiment in citations is often hidden. This might be because of the general strategy to avoid overt criticism due to the sociological aspect of cit-ing (MacRoberts and MacRoberts, 1984; Thomp-son and Yiyun, 1991). Ziman (1968) states that many works are cited out of  X  X oliteness, policy or piety X . Negative sentiment, while still present and detectable for humans, is expressed in subtle ways and might be hedged, especially when it cannot be quantitatively justified (Hyland, 1995).
  X  Citation sentences are often neutral with respect to sentiment, either because they describe an al-gorithm, approach or methodology objectively, or because they are used to support a fact or state-ment.

This gives rise to a far higher proportion of objec-tive sentences than in other genres.  X  Negative polarity is often expressed in contrastive terms, e.g. in evaluation sections. Although the sentiment is indirect in these cases, its negativity is implied by the fact that the authors X  own work is clearly evaluated positively in comparison.  X  There is also much variation between scientific texts and other genres concerning the lexical items chosen to convey sentiment. Sentiment car-rying science-specific terms exist and are rela-tively frequent, which motivates the use of a sen-timent lexicon specialised to science.
  X  Technical terms play a large role overall in scien-tific text (Justeson and Katz, 1995). Some of these carry sentiment as well.

For this reason, using higher order n -grams might prove to be useful in sentiment detection.  X  The scope of influence of citations varies widely from a single clause (as in the example below) to several paragraphs:
This affects lexical features directly since there could be  X  X entiment overlap X  associated with neighbouring citations. Ritchie et al. (2008) showed that assuming larger citation scopes has a positive effect in retrieval. We will test the op-posite direction here, i.e., we assume short scopes and use a parser to split sentences, so that the fea-tures associated with the clauses not directly con-nected to the citation are disregarded.

We created a new sentiment-annotated corpus of scientific text in the form of a sentence-based col-lection of over 8700 citations. Our experiments use a supervised classifier with the state-of-the-art features from the literature, as well as new fea-tures based on the observations above. Our results show that the most successful feature combination includes dependency features and n -grams longer than for other genres ( n =3 ), but the assumption of a smaller scope (sentence splitting) decreased re-sults. We manually annotated 8736 citations from 310 re-search papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network 1 (Radev et al., 2009) was used. We identified the actual text of the citations by regular expressions and replaced it with a special token &lt; CIT &gt; in order to remove any lexical bias associated with proper names of researchers. We la-belled each sentence as positive, negative or objec-tive, and separated 1472 citations for development and training. The rest were used as the test set con-taining 244 negative, 743 positive and 6277 objec-tive citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. We represent each citation as a feature set in a Sup-port Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use uni-grams and bigrams as features and also add 3 -grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and ap-pending all tags at the end of the sentence. This may help in distinguishing between homonyms with dif-ferent POS tags and signalling the presence of ad-jectives (e.g., JJ) respectively. Name of the primary author of the cited paper is also used as a feature.
A science-specific sentiment lexicon is also added to the feature set. This lexicon consists of 83 polar phrases which have been manually extracted from the development set of 736 citations. Some of the most frequently occurring polar phrases in this set consists of adjectives such as efficient , popular , suc-cessful , state-of-the-art and effective . 3.2 Contextual Polarity Features Features previously found to be useful for detect-ing phrase-level contextual polarity (Wilson et al., 2009) are also included. Since the task at hand is sentence-based, we use only the sentence-based fea-tures from the literature e.g., presence of subjectiv-ity clues which have been compiled from several sources 2 along with the number of adjectives, ad-verbs, pronouns, modals and cardinals.

To handle negation, we include the count of nega-tion phrases found within the citation sentence. Sim-ilarly, the number of valance shifters (Polanyi and Zaenen, 2006) in the sentence are also used. The polarity shifter and negation phrase lists have been taken from the OpinionFinder system (Wilson et al., 2005). 3.3 Sentence Structure Based Features We explore three different feature sets which focus on the lexical and grammatical structure of a sen-tence and have not been explored previously for the task of sentiment analysis of scientific text. 3.3.1 Dependency Structures
The first set of these features include typed depen-dency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships be-tween words. We aim to capture the long distance relationships between words. For instance in the sentence below, the relationship between results and competitive will be missed by trigrams but the de-pendency representation captures it in a single fea-ture nsubj competitive results .
A variation we experimented with, but gave up on as it did not show any improvements, concerns backing-off the dependent and governor to their POS tags (Joshi and Penstein-Ros  X  e, 2009). 3.3.2 Sentence Splitting
Removing irrelevant polar phrases around a ci-tation might improve results. For this purpose, we split each sentence by trimming its parse tree. Walk-ing from the citation node ( &lt; CIT &gt; ) towards the root, we select the subtree rooted at the first sentence node ( S ) and ignore the rest. For example, in Figure 1, the cited paper is not included in the scope of the discarded polar phrase significant improvements . 3.3.3 Negation
Dependencies and parse trees attach negation nodes, such as not , to the clause subtree and this shows no interaction with other nodes with respect to valence shifting. To handle this effect, we take a simple window-based inversion approach. All words inside a k -word window of any negation term are suffixed with a token neg to distinguish them from their non-polar versions. For example, a 2-word negation window inverts the polarity of the positive phrase work well in the sentence below. The negation term list has been taken from the OpinionFinder system. Khan (2007) has shown that this approach produces results comparable to gram-matical relations based negation models. Because of our skewed dataset, we report both the macro-F and the micro-F scores using 10-fold cross-validation (Lewis, 1991). The bold values in Table 1 show the best results.

The selection of the features is on the basis of im-provements over a baseline of 1-3 grams i.e. if a feature (e.g. scilex) did not shown any improvement, it is has been excluded from the subsequent experi-ments.

The results show that contextual polarity features do not work well on citation text. Adding a science-specific lexicon does not help either. This may indi-cate that n -grams are sufficient to capture discrim-inating lexical structures. We find that word level and contextual polarity features are surpassed by de-pendency features. Sentence splitting does not help, possibly due to longer citation scope. Adding a negation window ( k =15) improves the performance but the improvement was not found to be statistically significant. This might be due to skewed class dis-tribution and a larger dataset may prove to be useful. While different schemes have been proposed for annotating citations according to their function (Spiegel-R  X  osing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), there have been no at-tempts on citation sentiment detection in a large cor-pus.

Teufel et al. (2006) worked on a 2829 sentence ci-tation corpus using a 12-class classification scheme. However, this corpus has been annotated for the task of determining the author X  X  reason for citing a given paper and is thus built on top of sentiment of cita-tion. It considers usage, modification and similar-ity with a cited paper as positive even when there is no sentiment attributed to it. Moreover, contrast be-tween two cited methods (CoCoXY) is categorized as objective in the annotation scheme even if the text indicates that one method performs better than the other. For example, the sentence below talks about a positive attribute but is marked as neutral in the scheme.

Using this corpus is thus more likely to lead to inconsistent representation of sentiment in any sys-tem which relies on lexical features. Teufel et al. (2006) group the 12 categories into 3 in an at-tempt to perform a rough approximation of senti-ment analysis over the classifications and report a 0.710 macro-F score. Unfortunately, we have ac-cess to only a subset 3 of this citation function cor-pus. We have extracted 1-3 grams, dependencies and negation features from the reduced citation function dataset and used them in our system with 10-fold cross-validation. This results in an improved macro-F score of 0.797 for the subset. This shows that our system is comparable to Teufel et al. (2006). When this subset is used to test the system trained on our newly annotated corpus, a low macro-F score of 0.484 is achieved. This indicates that there is a mis-match in the annotated class labels. Therefore, we can infer that citation sentiment classification is dif-ferent from citation function classification.
Other approaches to citation annotation and clas-sification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, cer-tainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers.
Different dependency relations have been ex-plored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nak-agawa et al. (2010) report that using dependencies on conditional random fields with lexicon based po-larity reversal results in improvements over n -grams for news and reviews corpora.

A common approach is to use a sentiment la-belled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzi-vassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (En-gstr  X  om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel fea-tures, including n -grams, scientific lexicon, depen-dency relations and sentence splitting. Our results show that 3 -grams and dependencies perform best in this task; they outperform the scientific lexicon and the sentence splitting features. Future direc-tions include trying to improve the performance by modelling negations using a more sophisticated ap-proach. New techniques for detection of the nega-tion scope such as the one proposed by Councill et al. (2010) might also be helpful in citations. Explor-ing longer citation scopes by including citation con-texts might also improve citation sentiment detec-tion.

