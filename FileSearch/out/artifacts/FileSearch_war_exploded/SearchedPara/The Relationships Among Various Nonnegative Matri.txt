 shown recently to be useful for clustering. Various exten-sions of NMF have also been proposed. In this paper we present an overview and theoretically analyze the relation -ships among them. In addition, we clarify previously unad-dressed issues, such as NMF normalization, cluster poste-rior probabilty, and NMF algoritm convergence rate. Ex-periments are also conducted to empirically evaluate and compare various factorization methods.
 ing, NMF normalization, NMF convergence rate shown recently to be useful for many applications in en-vironment, pattern recognition, multimedia, text mining, and DNA gene expressions [9, 26, 29, 32]. NMF can be traced back to 1970s (Notes from G. Golub) and is stud-ied extensively by Paatero [29]. The work of Lee and Seung [24, 25] brought much attention to NMF in ma-chine learning and data mining fields. A very recent the-oretical analysis [12] shows the equivalence of NMF and spectral clustering and K -means clustering. Various ex-tensions and variations of NMF have been proposed re-cently [13, 14, 15, 23, 27, 3, 30, 33].
 tempts have been made to establish the connections between various factorization methods while highlighting their di f-ferences. In this paper, we aim to provide a comparative study on matrix factorization for clustering. We present an overview and summary on various matrix factorization al-gorithms and theoretically analyze the relationships amon g them. Experiments were also conducted to empirically eval-uate and compare various factorization methods. In particu -lar, our study tries to address the following important ques -tions for matrix factorizations:  X  What are the available forms of matrix factorizations  X  What are the relations among the matrix factorizations  X  How to interpret the cluster posterior obtained from  X  What are the benefits of simultaneous clustering?  X  How to evaluate simultaneous clustering?  X  How to choose different factorization methods?
We expect our study would provide insightful guidance on matrix factorization research for clustering. The rest o f the paper is organized as follows: Section 2 summarizes various matrix factorizations; Section 3 illustrates the d if-ferences among various factorizations using examples; Sec -tion 4 introduces the computation algorithms for various fa c-torization methods; Section 5 studies the relationships of various matrix factorization methods; Section 6 discusses the normalization method for eliminating the uncertainty i n NMF solutions; Section 7 explains when and why the si-multaneous clustering is preferred and presents strategie s for evaluating simultaneous clustering; Section 8 shows the ex -perimental results for empirically comparing various matr ix factorization methods; and finally Section 9 concludes.
In general, matrix factorization algorithms attempt to find the subspace in which the majority of the data points lie.
Let the input data matrix X = ( x 1 , , x n ) contain the col-lection of n data column vectors. Generally, we factorize X into two matrices, where X  X  R p  X  n , F  X  R p  X  k and G  X  R n  X  k . Generally, the rank of matrices F , G is much lower than the rank of X (i.e., k  X  min ( p , n ) ). Here we provide an overview on related matrix factorization methods: 1. SVD: The classic matrix factorization is Principal 2. NMF: When the input data is nonnegative, and we re-3. Semi-NMF: When the input data has mixed signs, 4. Convex-NMF: In general, the basis vectors 5. Tri-Factorization: To simultaneously cluster the rows 6. Kernel NMF: Consider a mapping
In summary, various factorizations differ by the restric-tions on the matrix forms and signs, we write them collec-tively as follows: ence among various NMF methods. the differences in NMF and Tri-Factorization.
  X   X   X   X   X   X  0 . 185 0 . 326 0 . 761 2 . 799 2 . 375 2 . 0 . 508 0 . 380 0 . 884 2 . 134 2 . 374 2 . 0 . 452 0 . 887 0 . 457 2 . 065 2 . 484 2 . 1 . 486 1 . 843 1 . 858 0 . 566 0 . 103 0 . 1 . 496 1 . 806 1 . 610 0 . 612 0 . 158 0 . One can see that the first 3 columns should be one cluster and the last 4 columns should be another cluster.
 Factorization are:
Basis vectors are normalized to 1 in L 2 -norm (norms are given at the bottom line) for comparison purpose. We can see that F leads to the correct row clustering results: the first three row are in one cluster and the bottom 2 rows are in another cluster.
 as follows: G 0 . 234 0 . 287 0 . 259 0 . 080 0 . 012 0 . 0 . 006 0 . 014 0 . 040 0 . 223 0 . 238 0 . G 0 . 270 0 . 335 0 . 333 0 . 034 0 . 000 0 . 0 . 000 0 . 000 0 . 000 0 . 239 0 . 248 0 . results: the first three columns are in one cluster and the remaining columns are in another cluster. Note that
It absorbs the different scales of X , F Tri  X  f actor G the attribute clusters).
In this section, we give an example to illustrate the differ-ences in SVD, semi-NMF and convex-NMF. The input data matrix is
One can see that the first 3 columns should be one cluster and the last 4 columns should be another cluster.
 The computed basis vectors F are:
Basis vectors are normalized to 1 in L 2 -norm (norms are given at the bottom line) for comparison purpose.
 The matrix G are listed as follows:
Both semi-NMF and convex-NMF give the correct clus-tering results. However, convex-NMF gives sharper clus-ter indicators, while semi-NMF gives a soft clustering. The residual values, the level of low-rank approximations, are for SVD, semi-NMF, and convex-NMF respectively. We see that semi-NMF has a good quality approximation close to SVD.
The algorithms for matrix factorizations are generally it-erative updating procedures: updating one factor while fix-ing the other factors. The algorithms for various matrix
G = X G ( G T G )  X  1 G r r  X  h  X  ( X ) T  X  ( X ) i in Convex-NMF factorizations are summarized in Table 1. In the table, we separate the positive and negative parts of a matrix A as ik = ( | A ik | + A ik ) / 2 , A  X  ik = ( | A ik | X  A ik ) / Lee-Seung algorithm converge to a local minima. However, it is easy to show that at convergence, the solution satisfy t he well-known KKT complementarity condition in the theory of constrained optimization, which is, ( X G  X  FG T G ) ik F ik = 0 , ( X T F  X  GF T F ) jk G jk for the objective J = || X  X  F G T || 2 . For example, at con-to first condition in Eq.(4). Therefore, Lee-Seung algorith m does converge to a local minima according to KKT theory. It has been proved that at convergence, solutions of all algo -rithms listed in Table 1 satisfy the KKT conditions in their respective cases.
 The objectives for all cases in Table 1 have been proved to Jordan [31], we expand 2  X   X  M (  X   X  ) + (  X  M /  X  X  )(  X   X  Therefore, under appropriate matrix norm. In general,  X  M /  X  X  Thus these updating algorithms have first order convergence rate, same as the EM algorithm [31].
In this section, we theoretically analyze the relationship s among various matrix factorization methods. Lee and Seung [24] emphasizes the difference between NMF and vector quantization (which is K -means clustering). Later experiments [22, 26] empirically show that NMF has clear clustering effects. Theoretically, NMF is inherentl y related to kernel K-means clustering.
 Theorem 1 . Orthogonal NMF, is equivalent to K-means clustering.
 This theorem has been previously proved [12] with addi-tional normalization conditions. Here we give a more gen-eral proof, which will generalize to bi-orthogonality. Proof . We write J = || X  X  F G T || 2 = Tr ( X T X  X  2 F T X G F T F ) . The zero gradient condition  X  J /  X  F =  X  2 X G 0 gives F = X G . Thus J = Tr ( X T X  X  G T X T X G ) . Tr ( X T X ) is a constant, the optimization problem becomes According to Theorem 2 below, this is identical to K-means clustering.

We note that Theorem 1 holds even if X and F are not nonnegative, i.e., X and F have mixed-sign entries. Theorem 2 [11, 34]. The K -means clustering where f k is the cluster centroid of the k -th cluster, and more generally, the Kernel K-means with mapping x i  X   X  ( x i ) where  X   X  k is the centroid in the feature space. This can be solved via the optimization problem where G are the cluster indicators and W i j =  X  ( x i ) T the kernel. For K -means,  X  ( x i ) = x i , W i j = x T i
NMF has clustering capabilities which is generally better than the K-means. In fact, PCA is effectively doing K -means clustering [11, 34]. Let G be the cluster indicators for the k clusters then (1) GG T  X  V V T ; (ii) the principal directions, UU T , project data points into the subspace spanned by the k cluster centroids. all have K -means clustering interpretations when the factor G is orthogonal. Being orthogonal and nonnegative, implies each row of G has only one nonnegative elements, i.e., G is a bona fide cluster indicator. We have Theorem 3 . G -orthogonal NMF, semi-NMF, convex-NMF and Kernel-NMF is identical to relaxed K -means clustering. Proof . For NMF, semi-NMF and convex-NMF, we first eliminate F . The objective is J = k X  X  F G T k 2 = Tr ( 2 X T F G T + F F T ) . Setting  X  J /  X  F = 0 , we obtain F Thus we obtain For Kernel-NMF, we have J = k  X  ( X )  X   X  ( X ) W G T k 2 = Tr ( K  X  G T KW + W T where K is the kernel. Setting  X  J /  X  W = 0, we have KG KW . Thus In all the above cases, the first term are constant and are ignored. The minimization problem thus becomes where K is either a linear kernel X T X or h  X  ( X ) ,  X  ( known [34] that this is identical to (kernel-) K -means clus-tering.  X   X  is not restricted to be orthogonal; these NMF varieties are soft versions of K -means clustering. From NMF/semi-NMF, and to convex-NMF, the successive restrictions make them different levels of soft clustering.

This situation is similar to the mixture of Gaussian gen-eralization of K -means . K -means is a mixture of spherical Gaussians with same variance. The first step is to generalize to spherical Gaussians with individual variance. The secon d step is to generalize to Gaussians with individual full co-variance matrix, etc. Each generalization have more model parameters and fits the data better. First, we emphasize the role of orthogonality in Tri-Factorization 3 Considering the unconstrained 3-factor NMF we note that this 3-factor NMF can be reduced to the uncon-strained 2-factor NMF by mapping F  X  F S . Another way to say this is that the degree of freedom of F SG T is the same as F G T .

Therefore, 3-factor NMF is interesting only when it can not be transformed into 2-factor NMF. This happens when certain constraints are applied to the 3-factor NMF. How-ever, not all constrained 3-factor NMF differ from their 2-factor NMF counterpart. For example, the following 1-sided orthogonal 3-factor NMF is no different from its 2-factor counterpart, because the mapping F  X  F S reduces one to another. It is clear that has no corresponding 2-factor counterpart. This is a genuin e new factorization, which we call 3-factor NMF. The update rules are given in Table 1.

An important special case is that the input X contains a matrix of pairwise similarities: X = X T = W . In this case, F = G = H . We optimize the symmetric NMF: When the orthogonality of H T H = I is enforced, we can use the update rules of Tri-Factorization of Eq.(12) with appro -priate substitutions. When H T H = I is not enforced, the update rules are : where 0 &lt;  X   X  1. In practice, we find  X  = 1 / 2 is a good choice. unsupervised learning method: Probabilistic Latent Seman -tic Indexing (PLSI). So far, the cost function we used for computing NMF is the sum of squared errors, || X  X  F G T || Another cost function KL divergence: J supervised learning method based on statistical latent cla ss models and has been successfully applied to document clus-tering [21]. (PLSI is further developed into a more compre-hensive Latent Dirichlet Allocation model [5].) where the joint occurrence probability is factorized (i.e. , pa-rameterized or approximated ) as assuming that w i and d j are conditionally independent given z .
 Proposition 1 . Objective function of PLSI is identical to the objective function of NMF, i.e., J (
F G T ) i j = P ( w i , d j ) . Therefore, the NMF update algorithm and the EM algorithm in training PLSI are alternative meth-ods to optimize the same objective function [14]. obtained form matrix factorization.
 assumed that G is the cluster posterior and thus G ik gives the posterior probability that x i belongs to the k -th column clus-ter. However, the NMF solutions are not unique. Suppose (
F , G ) is solution of NMF. There exist many matrices ( A , such that AB T = I , FA  X  0 , GB  X  0 . Thus ( FA , GB ) is also the solution with the same residue k X  X  FG T k .

A way to resolve this is to assume NMF follows a certain distribution. We can think of the rectangular input data X as a word-document matrix and perform a PLSI type prob-abilistic decomposition. as in Eq. 18, where z k is the latent cluster variable, and the probability factors follow the pr ob-ability normalization We assume the data is normalized such that  X  i j X i j = 1. With this, the cluster posterior probability for column d j is then Translating to F , G , the equivalent probabilistic decomposi-tion is where D F = diag ( e T F ) , D G = diag ( e T G ) , and
Thus for standard NMF, the cluster posterior probability for column x i is For Convex-NMF, the centroid interpretation of F = XW im-plies W should have a L 1 normalization. Thus we write Therefore, the cluster posterior probability for column x Convex-NMF: p ( z k | x i )  X  ( GD W ) ik , D W = diag ( e Semi-NMF does not have a probability interpretation be-cause F could be negative signs. For this reason, the L normalization is most natural. Let F = ( f 1 , , f k ) Z F = diag ( || f 1 || , , || f k || ) . We write Thus the cluster posterior probability is Consider the nonnegative Tri-Factorization X  X  F SG T . For the objective of the function approximation, we optimiz e We note X  X  R p  X  n This allows the number of row cluster ( k ) differ from the number of column cluster (  X  ). In most cases, we set k =  X  This form gives a good framework for simultaneously clus-tering the rows and columns of X . Recently, simultaneous clustering has been extensively studied [10, 8, 2, 7, 28]. However, two questions are still largely unaddressed in the literature:  X  Why do we prefer the simultaneous clustering to single- X  How to evaluate the simultaneous clustering? In this section, we attempt to provide our insights for the above questions. tions in high dimensional spaces. Most clustering algorith ms do not work efficiently in high dimensional spaces due to the curse of dimensionality . It has been shown that in a high di-mensional space, the distance between every pair of points i s almost the same for a wide variety of data distributions and distance functions [4]. Many feature selection techniques have been applied to reduce the dimensionality of the space. However, as demonstrated in [1], the correlations among the dimensions are often specific to data locality; in other word s, some data points are correlated with a given set of features and others are correlated with respect to different feature s. As pointed out in [20], all methods that overcome the di-mensionality problems use a metric for measuring neighbor-hoods, which is often implicit and/or adaptive. Simultane-ous clustering performs an implicit feature selection at ea ch iteration and provides an adaptive metric for measuring the neighborhood.
 is an association relationship between the data and the fea-tures (i.e., the columns and the rows). A case is the binary data. A distinctive characteristic of the binary data is tha t the features (attributes) they include have the same nature as the data they intend to account for: both are binary. Another case is block diagonal clustering where both data points and features have the same number of clusters. In this case, afte r appropriate permutation of the rows and columns, the cluste r structure takes the form of a block diagonal matrix [18]. be interpreted using a probabilistic view similar to the PLS I model. Instead of assuming that the variables w i and d j conditionally independent given z k in Eq. 18, we assume that the variable w i only depends on its cluster variable f k the variable d j only depends on its cluster variable g l probabilistic model of simultaneous clustering. Therefor e we have Here, P ( w i | f k ) corresponds to F , P ( d j | g l ) to G and P to S .
The example is based on a simple dataset which contains six system log messages from two different situations: Start and Create .

After removing stop words and words only appear once, we get the binary document-term matrix as shown in Table 2. For this example, using one-side clustering, e.g., k-means , it usually does get perfect clustering results. However, usin g simultaneous clustering, we could correctly obtain the mes -sage clusters. The reason is that using simultaneous clus-tering, in the iteration process, we could adaptively measu re the distance between the data points: if the words have sim-ilar distributions across multiple clusters, it can be trea ted as outliers and does not contribute to the distance computatio n. In the example, Term 3 and 4 (i.e., column 3 and 4) can be thought as feature noises as they have similar distribution s across multiple clusters.

Table 2. Log message example: The 6 terms are start , application , version , service , create , tem-porary respectively.
Simultaneous clustering performs clustering of row and column clustering simultaneously, where the factor F is the cluster indicator for words (i.e., rows). Quantitatively, we can view the i -th row of the cluster indicator F as the pos-terior probability that word i belongs to each of the K word clusters. We can assign a word to the cluster that has the largest probability value. However, row clustering has no clear a prior labels to compare with. For example, for doc-ument clustering, we usually have labels for each document class and we have no label information about word clusters. uating the clustering of rows (i.e.,words). Let this row of F be ( p 1 , , p k ) , which has been normalized to  X  k p Suppose a word has a posterior distribution of it is obvious that this word is cleanly clustered into one clu s-ter. We say this word has a 1-peak distribution. Suppose another word has a posterior distribution of obviously this word is clustered into two clusters. We say this word has a 2-peak distribution. In general, we wish to characterize each word as belonging to 1-peak, 2-peak, 3-peak etc. For K word clusters, we set K prototype distribu-tions: For each word, we assign it to the closest prototype distribu -tion based on the Euclidean distance, allowing all possible permutations of the clusters. For example, ( 1 , 0 , 0 , , such that the components decrease from the left to the right, and then assign it to the closest prototype. Generally speak -ing, the less peaks of the posterior distribution of the word , the more unique content of the word has. This multi-peak distribution approach provides the capability of evaluati ng row (e.g., word) clusterings and enables the systematic ana l-ysis of word content. compare the clustering results of various NMF algorithms. In our experiments, documents are represented using the bi-nary vector-space model where each document is a binary vector in the term space. Our comparative experimental study includes the following six methods: K -means , NMF, Semi-NMF, Convex-NMF, Tri-Factorization, and PLSI. used in the information retrieval research. Table 3 summa-rizes the characteristics of the datasets.
 cal reports (TRs) published in the Department of Computer Science at a research university. The dataset contained 476 abstracts, which were divided into four research areas: Nat -ural Language Processing(NLP), Robotics/Vision, Systems , and Theory.

WebKB The WebKB dataset contains webpages gath-ered from university computer science departments. There are about 8280 documents and they are divided into 7 cat-egories: student, faculty, staff, course, project, depart ment and other. The raw text is about 27MB. Among these 7 cat-egories, student, faculty, course and project are four most populous entity-representing categories. The associated sub-set is typically called WebKB4 .

Reuters The Reuters-21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987. It is a standard text categorization bench -mark and contains 135 categories. In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10 .

WebACE The K-dataset was from WebACE project and has been used for document clustering [6, 19]. The K-dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997. These documents are divided into 20 classes.

Log The log data used in our experiments are collected from several different machines with different operating s ys-tems using logdump2td (NT data collection tool) developed at IBM T.J. Watson Research Center. The data in the log files describe the status of each component and record sys-tem operational changes, such as the starting and stopping of services, detection of network applications, software c on-figuration modifications, and software execution errors.
To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels.
The above document datasets are standard labeled cor-pora widely used in the information retrieval literature. W e view the labels of the datasets as the objective knowledge on the structure of the datasets. We use accuracy as the cluster -ing performance measure. Accuracy discovers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. It sums up the whole matching de-gree between all pair class-clusters. Accuracy can be repre -sented as: where C k denotes the k -th cluster, and L m is the m -th class. T (
C k , L m ) is the number of entities which belong to class m are assigned to cluster k . Accuracy computes the maximum sum of T ( C k , L m ) for all pairs of clusters and classes, and these pairs have no overlaps. The greater accuracy means the better clustering performance.

The experimental results are shown in Table 4 and Fig-ure 1. From the experimental comparisons, we observe that:  X  NMF-like algorithms generally outperform K-mean  X  On most of the datasets, NMF gives somewhat better  X  The experimental comparisons empirically verify the  X  Tri-Factorization generally is better than K-means and  X  As we discussed in Section 7, Tri-Factorization enables
In this paper we provide a comparative study on (non-negative) matrix factorization for clustering. Attempts h ave been made to establish the relations among various matrix factorization methods while highlighting their differenc e. Previously unaddressed yet important questions such as the interpretation and normalization of cluster posterior, co nver-gence issues, and the benefits and evaluation of simultane-ous clustering have also been studied. We expect our study could provide insightful guidances on matrix factorizatio n research for clustering. In particular, the extensive rese arch and experiments show that NMF provides a new paradigm for unsupervised learning.
 Acknowledgments . We would like to Dr. Shenghuo Zhu for useful comments and discussion. T. Li is partially supporte d by a IBM Faculty Research Award, and the NSF CAREER Award IIS-0546280. C. Ding is partially supported by the US Dept of Energy, Office of Science.

