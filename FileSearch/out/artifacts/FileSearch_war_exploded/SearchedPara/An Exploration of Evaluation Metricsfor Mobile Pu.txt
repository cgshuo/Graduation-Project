 How do we evaluate systems that filter social media streams and send users updates via push notifications on their mo-bile phones? Such notifications must be relevant, timely, and novel. In this paper, we explore various evaluation metrics for this task, focusing specifically on measuring relevance. We begin with an analysis of metrics deployed at the TREC 2015 Microblog evaluations. A simple change to the metrics, reflecting a different assumption, dramatically alters system rankings. Applying another metric, previously used in the TREC Microblog evaluations, again yields different system rankings. We find little correlation between a number of  X  X easonable X  evaluation metrics, which suggests that system effectiveness depends on how you measure it X  X n undesir-able state in IR evaluation. However, we argue that exist-ing evaluation metrics can be generalized into a framework that uses the same underlying contingency table, but places different weights and penalties. Although we stop short of proposing the  X  X ne true metric X , this framework can guide the future development of a family of metrics that more ac-curately models user needs.
This paper explores the problem of evaluating push no-tification techniques on social media streams in a filtering application. We assume an infinite stream of social media posts such as Twitter, against which the user issues an arbi-trary number of standing queries representing  X  X nterest pro-files X , analogous to topics in traditional ad hoc retrieval. For example, the user might be interested in poll results for the 2016 U.S. presidential elections and wishes to be notified whenever new results are published. The system X  X  task is to identify relevant tweets from the stream and send those updates directly to the user X  X  mobile phone via push noti-fications. Since such notifications are often associated with an auditory or visual cue upon arrival, each imposes a non-trivial cognitive burden on the user (even if ignored). Thus, careful control of the volume of notifications is critical to successful push strategies. This paper explores evaluation metrics for such a task.

At a high level, push notifications should be relevant, timely (provide updates as soon after the actual event oc-currence as possible), and novel (users should not be pushed multiple notifications that say the same thing). Accordingly, an evaluation metric should reward systems for updates that satisfy these three main criteria. As the design space is vast, in this short paper we focus on relevance, adopting existing notions of novelty and timeliness.
 We start with an analysis of metrics from the TREC 2015 Microblog track, which operationalized such a push notifica-tion task, and then re-assess submitted runs after making a minor tweak to reflect a different assumption about the user model. We then re-assess submitted runs using variants of a metric that has been applied in previous iterations of the same evaluation. Using score and rank correlations, we com-pare system effectiveness as measured by each metric. Our results are surprising: we find little correlation between the different metrics. This means that the answer to  X  X hich system is better X  depends on how you measure it, which is undesirable from an evaluation perspective.

The contribution of this paper is twofold. First, we present the novel and surprising finding discussed above: any num-ber of reasonable evaluation metrics give rise to significantly different system rankings. We discuss and analyze why, trac-ing the issue to the handling of days for which there are no relevant tweets. Second, we argue that the different exist-ing evaluation metrics we applied can be generalized into a framework that uses the same underlying contingency table, but places different weights and penalties. Although we do not propose the  X  X ne true metric X , we believe this framework can guide the future development of an evaluation metric that more accurately models user needs.
The application described in the introduction was opera-tionalized in the TREC 2015 Microblog track as the so-called  X  X cenario A X  variant of the real-time filtering task [3]. Over the official evaluation period, which spanned ten days during July 2015, participating systems  X  X istened X  to Twitter X  X  live tweet sample stream to identify relevant tweets with respect to 225 topics, 51 of which were later assessed. Each system identified up to ten tweets per day, which were putatively delivered to hypothetical users. In total, 14 groups submit-ted 37 runs to the evaluation. Data from this evaluation provides the starting point for our analysis.

The assessment workflow for the track was as follows: first, tweets returned by the systems were assessed for rel-evance using a traditional pooling process. Relevant docu-ments were then semantically clustered into groups contain-ing tweets that share substantively similar information. We refer the reader to previous papers for more details [5].
The two metrics used to evaluate system runs were ex-pected latency-discounted gain (ELG) and normalized cu-mulative gain (nCG). These two metrics are computed for each topic for each day in the evaluation period (explained in detail below). The score for a topic is the average of the daily scores in the evaluation period. The score of a system run is the average of the scores across all topics.
Expected latency-discounted gain (ELG) was adapted from the TREC Temporal Summarization track [2]: where N is the number of tweets returned and G( t ) is the gain of each tweet: non-relevant tweets receive a gain of 0, relevant tweets receive a gain of 0.5, and highly-relevant tweets receive a gain of 1.0.

A key aspect of this metric is its handling of redundancy and timeliness: a system only receives credit for returning one tweet from each cluster. Furthermore, a latency penalty is applied to all tweets, computed as MAX(0 , (100  X  d ) / 100), where the delay d is the time elapsed (in minutes, rounded down) between the tweet creation time and the putative time the tweet was delivered. That is, if the system delivers a relevant tweet within a minute of the tweet being posted, the system receives full credit. Otherwise, credit decays linearly such that after 100 minutes, the system receives no credit even if the tweet was relevant.

The second metric is normalized cumulative gain (nCG): where Z is the maximum possible gain (given the ten tweet per day limit). The gain of each individual tweet is com-puted as above (with the latency penalty). Note that gain is not discounted (as in nDCG) because the notion of docu-ment ranks is not meaningful in this context.

Due to the setup of the task and the nature of interest pro-files, it is possible (and indeed observed empirically) that for some days, no relevant tweets appear in the judgment pool. In terms of evaluation metrics, a system should be rewarded for correctly identifying these cases and not pushing non-relevant content. If there are no relevant tweets for a par-ticular day and the system returns zero tweets, it receives a score of one (i.e., perfect score) for that day; otherwise, the system receives a score of zero for that day. This applies to both ELG and nCG.

It is worth mentioning that despite superficial similari-ties, our task is very different from document filtering in the context of topic detection and tracking (TDT) [1]. TDT is concerned with identifying all documents related to a par-ticular event X  X ith an intelligence analyst in mind X  X hich requires keeping track of false alarms and missed detections. In contrast, we are focused on identifying a small set of the most relevant updates to push to users, grounded in inter-actions with mobile devices. Furthermore, in TDT, systems must make online decisions as soon as documents arrive, whereas in our case systems can choose to push older con-tent (subjected to the latency penalty), thus giving rise to the possibility of algorithms operating on bounded buffers. For these various reasons, TDT evaluation tools such as the decision error tradeoff (DET) curve and derivative metrics provide inspiration, but are not directly applicable.
In Figure 1 (left), we show a scatterplot of the official ELG scores (which we call ELG-1 for reasons that will become clear shortly) vs. nCG (specifically, nCG-1, for the same reasons). Although there is an overall correlation between ELG-1 and nCG-1 across all submitted runs, we do note that in particular cases ELG-1 and nCG-1 are capturing different aspects of effectiveness: for example, the top three runs in terms of ELG-1 (circled in blue) exhibit relatively large differences in nCG-1. There are also cases in which systems achieve high nCG-1 relative to their ELG-1 scores (the runs circled in red).

One interesting aspect of ELG-1 and nCG-1 is their han-dling of days in which there are no relevant documents: for rhetorical convenience, we call days in which there are no relevant tweets for a particular topic (in the pool)  X  X ilent days X , in contrast to  X  X ventful days X  (where there are rele-vant tweets). In both ELG-1 and nCG-1, for a  X  X ilent day X , the only two possible scores are one (if the system remained silent) or zero (if the system pushed any tweet). This means that an empty run (a system that never returns anything) may have a non-zero score based on how many silent days there are in each topic. As it turns out, an empty run will score 0.2471 in ELG-1 and nCG-1, shown as dotted lines in Figure 1 (left). Since this was the first year of this TREC evaluation, systems achieved high scores by simply returning few results, in many cases for totally idiosyncratic reasons X  for example, the misconfiguration of a score threshold.
As an alternative, what if we did not reward systems for remaining silent? That is, on a silent day, all systems re-ceive a zero score, no matter what they did. We call these variant metrics ELG-0 and nCG-0 (in contrast to ELG-1 and nCG-1). We can justify this from the user perspective in that for a silent day, the user does not obtain any rele-vant information regardless of system output (since there are no relevant documents). In this case, how would the user know to  X  X eward X  a system for remaining silent? That is, properly determining a silent day requires global knowledge (e.g., from pooling), which no individual user has access to.
In Figure 1, we show scatterplots of ELG-1 vs. ELG-0 (middle) and nCG-1 vs. nCG-0 (right). We see no dis-cernible relationship between each pair of metrics, which suggests that the handling of silent days is the most critical part of each metric , in that different (reasonable) formula-tions yield dramatically different results and system rank-ings. In fact, we would go as far as saying that effectiveness under ELG-1 and nCG-1 is primarily dominated by a sys-tem X  X  ability to identify the silent days. Under both metrics, systems do well by learning when to  X  X hut up X .

This observation is further illustrated by the scatterplots in Figure 2, where we show silence precision vs. ELG-1 (left), silence recall vs. ELG-1 (middle), and silence precision vs. silence recall (right) for all runs. Silence precision and re-call follow the usual definitions of precision and recall, but with respect to identifying the silent days. Since each topic has an equal number of days, there is no difference between micro-and macro-averaging. Across all the topics, 24.7% of all days are completely silent, while another 6.7% have rele-vant but redundant material. We see that there is a slightly positive (but very weak) correlation between ELG-1 and si-lence precision. The middle graph, in effect, shows that sys-tems achieve a high ELG-1 score by achieving a high silence recall X  X .e., getting a good score is dominated by a system knowing when to  X  X hut up X . Although systems with com-parable silence recall can differ substantially in ELG-1, we were surprised by how much of the variance in the official metric can be explained by silence recall alone.

The right graph in Figure 2 shows the tradeoffs systems make with respect to precision and recall. On the right edge of the plot are cases where the systems are almost always quiet, achieving nearly perfect recall; in the left lower corner is a system that never  X  X huts up X , and hence its precision and recall are both zero. It is interesting to note that some systems perform poorly in both precision and recall X  X hey don X  X  push content when there X  X  relevant content and don X  X   X  X hut up X  when there X  X  no relevant content.

To our knowledge, we are the first to make this observation about the huge impact of silent vs. eventful days in the cur-rent evaluation of push notification. However, we withhold judgment as to whether the current TREC metrics repre-sent the  X  X ight X  approach: from the user perspective, since push notifications are associated with high cognitive effort (because they may interrupt the user), perhaps we should force systems to focus on learning when to  X  X hut up X . On the other hand, having such highly binarized scores on the silent days creates many issues for system tuning, since it creates discontinuities in the objective. We observe similar issues when trying to optimize a metric such as precision at rank one for question answering.
What are other reasonable ways in which we can eval-uate the push notification task? A simple and intuitive utility-based metric would be to reward  X  X ain X  based on de-livery of relevant information and to deduct  X  X ain X  based on delivery of non-relevant information. In fact, the TREC 2012 Microblog track employed exactly such a metric, called T11U [4], itself derived from the linear utility metrics used in the TREC filtering tracks.

We adopt a slightly different but mathematically equiva-lent formulation as follows: where G is total gain, N x is the number of non-relevant doc-uments pushed, and  X  controls the relative weight of gain vs. pain. Note that in T11U, the total gain factors in different relevance grades, the latency penalty, and the treatment of redundant tweets in exactly the same way as ELG.

In Figure 3, we show a scatterplot of ELG-1 vs. T11U with  X  = 0 . 66, which was the value used in TREC 2012. This value can be understood as setting the gain of a relevant no-tification (highest relevance grade, no temporal penalty, not redundant) equal to the pain of returning two non-relevant updates. With this setting, we do see reasonable positive correlation between ELG-1 and T11U overall, but this cor-relation is highly misleading. According to T11U, very few systems achieve positive utility overall X  X hat is, the gain from pushing relevant content is not sufficient to offset the pain from pushing non-relevant content. Furthermore, the relatively large cluster of runs which score around zero in T11U vary widely in ELG-1, from around 0.2 to over 0.3, with the highest T11U score being somewhere in the mid-dle of this band. In other words, poorly-performing systems Figure 4: Kendall X  X   X  between T11U and ELG-1 for different values of  X  . score low in both ELG-1 and T11U, but beyond that, T11U and ELG-1 exhibit a weak correlation at best.

Of course, absolute scores and relative system rankings depend on the  X  parameter that balances gain vs. pain, and the setting of  X  = 0 . 66 was arbitrary. What would the evalu-ation results look like for different settings of  X  ? The answer is shown in Figure 4, where we sweep the  X  parameter and compute Kendall X  X   X  with respect to ELG-1 for each set-ting. The results show that Kendall X  X   X  varies substantially, ranging from moderate correlation to non-existent and even slightly negative correlation. We have shown above that high correlations can be misleading, and this plot shows that  X  = 0 . 66 is around the highest Kendall X  X   X  we can obtain regardless. These results suggest that T11U and ELG-1 are measuring quite different aspects of effectiveness.
Let us take stock of our findings so far: we have evaluated runs from the TREC 2015 Microblog track using official as well as alternative metrics (ELG-1, ELG-0, nCG-1, nCG-0, and T11U). In comparing the metrics, we observe many in-consistencies in terms of both system scores and relative rankings. In other words,  X  X hich system is better X  depends on what measure we use. From an evaluation perspective, this is not desirable because researchers lack consistent guid-ance for algorithm development.

To address this issue, we propose a general evaluation framework built around the contingency table shown in Ta-System action  X  X ventful days X   X  X ilent days X  Pushed relevant + G E -Pushed not-relevant  X  P E  X  P 0 Stayed silent  X  S E + S 0 Table 1: The contingency table for a general evalu-ation framework for push notifications. ble 1. At the core, our framework is utility-based in that gain is rewarded for pushing relevant content (+ G E ) and pain is deducted for pushing non-relevant content (  X  P E and  X  P However, a key insight is the explicit separation of eventful and silent days, which our ELG-1 vs. ELG-0 experiments have shown to be critical in system evaluations.

Our evaluation framework is general in that the metrics we have examined in this paper can be viewed as specific instantiations of the parameters in Table 1. For example, T11U sets G E and P E (based on  X  ) but ignores the final row, and furthermore does not make the distinction between eventful and silent days. ELG-1 and ELG-0 make different choices on S 0 , how systems should be rewarded for staying silent on silent days, but both set P E and P 0 to zero. That is, no pain is deducted for pushing non-relevant content.
Using the framework presented in Table 1 as a guide, we can imagine a family of metrics beyond those already pre-sented. For example, we might augment T11U by creating a distinction between eventful and silent days, thus arriving at a metric that is closer to ELG-1 or ELG-0. We might set P E differently from P 0 to create more nuanced distinc-tions in a T11U-like metric. Different ratios between these weights also give rise to emphasis on different aspects of the push notification problem.

The question remains on how to properly set the gain and pain weights in the contingency table X  X nd we presently pro-vide no concrete answer, expect to say that further studies in user modeling are necessary. For example, we have pre-sented two plausible scenarios (ELG-1 and ELG-0) for the treatment of silent systems on silent days: a user study is necessary to decide which alternative (or neither) matches user preferences. Although the development of the  X  X ne true metric X  is beyond the limited scope of this short paper, our framework contributes to a step toward that goal.
 Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS-1218043 and CNS-1405688 and the Natural Sciences and En-gineering Research Council of Canada (NSERC). Any opin-ions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.
