 Personalization techniques have been widely adopted in many recommender systems. However, experiments on real-world datasets show that for some users in certain contexts, per-sonalized recommendations do not necessarily perform bet-ter than recommendations that rely purely on popularity. Broadly, this can be interpreted by the fact that the pa-rameters of a personalization model are usually estimated from sparse data; the resulting personalized prediction, de-spite of its low bias, is often volatile. In this paper, we study the problem further by investigating into the ranking of recommendation lists. From a risk management and port-folio retrieval perspective, there is no difference between the popularity-based and the personalized ranking as both of the recommendation outputs can be represented as the trade-off between expected relevance (reward) and associated uncer-tainty (risk). Through our analysis, we discover common scenarios and provide a technique to predict whether per-sonalization will fail. Besides the theoretical understanding, our experimental results show that the resulting switch al-gorithm, which decides whether or not to personalize, out-performs the mainstream recommendation algorithms.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Information Filtering Personalization; Collaborative Filtering; Recommender Sys-tems; Portfolio Theory
Personalization techniques have been widely adopted in many recommender systems. Personalized recommendation-s are generally derived by distinguishing individuals and their contexts. Example applications such as Amazon [17] and YouTube [7] show personalized recommendations have been well acknowledged and implemented by some of the most successful recommender systems. In particular, Col-laborative Filtering (CF) techniques, which make use of the aggregated user preference data to make personalized inter-est predictions, have proven to be both efficient and effec-tive, as evidenced by a series of recommendation algorithm competitions [1, 10].

Nevertheless, it is somewhat unclear as to whether person-alization will always outperform the non-personalized. The fact is, in some cases non-personalized recommendations are able to provide more relevant (or interesting) items, while in other cases the personalized may fail to catch the user-s X  current personal interest and result in low quality rec-ommendations. The work in [5] found surprisingly good performance from popularity-based recommendations. Fur-thermore, Figure 1 illustrates a performance comparison be-tween the Bayesian personalized ranking ( BPR ) [22], a state-of-the-art ranking-based CF algorithm, and the popularity-based (non-personalized) algorithm on the MovieLens dataset. It illustrates that, although the personalized algorithm work-s better overall, on each evaluation measure, there are a large proportion of cases where the popularity-based recommen-dation outperforms the Bayesian personalized ranking.
From a machine learning perspective, the personalized recommendation can reduce the bias but may increase the variance of prediction for user X  X  preference [9]. More impor-tantly, such bias and variance are different across differen-t users and in different recommendation contexts. There-fore, compared with non-personalized recommendation, the overall performance of the personalized recommendation is usually higher but it does not guarantee to be better in each of the cases. In some research work, regularization and cross validation are adopted for model selection and the bias-variance balance of the model [12]. Thus, it is of great interest to study in what situations the personalization fails and develop a switch to judge which model should be adopt-ed to provide recommendations to the target user.

In this paper, we analyze different characteristics of per-sonalized and non-personalized recommendations. As a by-product, we provide effective switch algorithms in order to decide whether to personalize or not for each user. Specifi-cally, we introduce the concept of risk borrowed from finance [15] into the preference prediction model. We then leverage modern portfolio theory [11] to optimize the expected re-ward and risk of the recommendations from both of the per-sonalized and non-personalized algorithms. Finally, a series of switch algorithms are proposed both by heuristics and learning-based methods. As an empirical study, we conduc-t our experiments on two collaborative filtering benchmark datasets: MovieLens and Netflix. The experimental results support our idea of introducing risk into the preference es-timation and our proposed mean-variance-aware switch al-gorithms .

The rest of this paper is organized as follows. Related work is discussed in Section 2. Next, we introduce the con-cepts and methods of risk management in item recommen-dation, and provide an analysis on whether to personalize Figure 1: Performance comparison of personalized and non-personalized recommendations. or not in a risk management perspective. Motivated by our analysis, we propose our methodology in Section 4. The ex-periments are described in Section 5, and finally, this paper is concluded in Section 6.
Collaborative Filtering has been widely applied and re-searched [23, 26]. It refines user-specific profiles by analyzing the historic behavior from a large corpus of users and makes the recommendation based on that. There are two major catergories for CF algorithms: the memory-based method-s explicitly figure out the user or item similarities [13, 8] or combine them together [30] and make recommendation-s directly based on the target user X  X  past behavior. The model-based methods learn a  X  X odel X  of each user X  X  behav-ior and use it to provide recommendations. The latent factor models [14] implemented by matrix factorization [16, 25, 21] have become quite popular during recent years. Today both the memory and model-based methods are widely used in commercial personalized recommender systems [7, 6].
While many research efforts focus on the personalization algorithms for recommender systems, there are, to the best of our knowledge, very few work that investigates which situations the users need personalized recommendation or not. Such idea has been proposed in Web search scenario [29], where the variation of user search intent is modelled by characterizing the queries with a variety of features and the search results. Even that work is still essentially differ-ent from our scenario where there is no any query and the recommender system needs to judge the user X  X  preference on whether to personalize at each recommendation request. Another work [28] studies the relationship between recom-mendation accuracy and the item popularity but it does not address the problem of whether to personalize or not.
Risk management and modelling are a significant compo-nent for financial institutions, where the companies need to evaluate the risk (i.e, the possibility of failing) for each new candidate investment and how it fits in their previous portfo-lio [15]. Recently, this idea has been introduced to the infor-mation retrieval area: risk-sensitive models are proposed to measure the uncertainty of document ranking performance [34, 33]; optimization algorithms are introduced to improve the robustness of ranking models [2, 33]; the modern port-folio theory is applied to the ranking model (as a document recombination process) to reduce the risk [31]. In addition, a risk-sensitive retrieval task is added in TREC Web track 2013 to maintain the good robustness of IR models [3]. For recommender systems, the risk of delivering an item or a list of items is actually much significant. However, quite few work explicitly models the risk to improve the performance of recommender systems. In [27], the idea of portfolio theory for ranking documents is borrowed to collaborative filtering to adaptively diversify the recommended items. Compared with our work, it mainly focuses diversification of recom-mended items instead of carefully modelling the individual item risk and the decision making of whether to personalize.
In this section, we make use of the concepts of portfo-lio retrieval [31, 20] to conduct our analysis. To make this paper self-contained, we first present the basic concepts of portfolio retrieval in the preliminaries section, and the de-tails are referred to [31, 20]. We then analyze the (non-) personalized item portfolios using the expected reward and its associated risk and study their relationships in the risk-reward diagram.
Definition 1 (Item Portfolio). An item portfolio p is a collection of n items with a weight w i assigned to each item i :
As will be detailed later, the weight is relevant to the im-portance of each item, which will finally determine the rank-ing of it. In our proposal, the weight w i can be any positive or negative value in R , and the larger the weight the more important the item is. Such setting is different from [31] but this is a more general case of portfolio weight, just as the asset short-selling in finance [18]. Without loss of general-ity, we assume the portfolio includes n items with indices i = 1 ...n . Suppose the user u  X  X  preference on item i is a random variable, denoted as r u,i , and its mean and variance are  X  u,i and  X  2 u,i respectively, then the user u  X  X  preference for an item portfolio p , denoted as R u,p , is calculated as a linear function over the item weights:
The expected reward and variance of R u,p are where we have already used vector representations with w as weight vector for the items;  X  = (  X  u, 1 ... X  u,n the covariance matrix about the user X  X  preference on items in p . The element of  X  is  X  ij =  X  ij  X  u,i  X  u,j [18, 31], where  X  ij is the correlation between item i and j . With the above definition and the calculation of the mean and variance of the user X  X  preference on an item portfolio, Figure 2(a) gives an illustration of the mean and standard deviation of three individual items, as well as the different portfolios based on these items.

Definition 2 (Item Portfolio Domination). For a user u , an item portfolio p 1 is said to dominate another item portfolio p 2 , denoted as p 1 p 2 , if and only if it satisfies Figure 2: An example of portfolio mean-variance, domination and efficient frontier.

An example of item portfolio domination is given in Figure 2(b) where portfolio 1 dominates portfolio 2. Obviously, in the  X  - X  plot, left-upper points dominate the right-lower ones as the former is with larger expectation and smaller standard deviation than that of the latter.

Definition 3 (Efficient Frontier). Given a set of items with their individual means, variances, and correla-tions, the efficient frontier is a set of item portfolios, each of which is dominated by none of other possible item portfo-lios.

Given an expected relevance  X  p , the corresponding point on the efficient frontier can be calculated by minimizing the risk; examples include the well-known Markowitz X  X  Objec-tive [18]. Its variant for web search diversification was also proposed in [20]. More specifically, we have The closed form solution of Eq. (7) is w
With such setting of  X  p and w M , the variance of p is which is a quadratic polynomial of  X  p where and x = 1 T  X   X  1 1 , y = 1 T  X   X  1  X  =  X  T  X   X  1 1 , and z =  X 
T  X   X  1  X  . From Eq. (10) we can see  X  p is a squared root of a quadratic polynomial of  X  p and it forms part of a hyper-bola in the  X  - X  plot. Figure 2(b) gives an illustration of the corresponding frontier curve.

Up to this point, we have introduced the basic building blocks of portfolio theory for recommendation tasks, such as item portfolio, item portfolio domination and efficient fron-tier. The detailed information on such concepts in finance can be found in [11, 15]. Next, we shall introduce our initial risk analysis of two recommendation portfolios.
We are now ready to analyze the expected reward and its risk on two item portfolios (one from the personalized and one from the non-personalized). Without loss of generali-ty, in this paper we consider two well-known recommenda-tions algorithms. For the personalized recommendation al-gorithm, we consider Bayesian personalized ranking ( BPR ) [22], one of the state-of-the-art ranking-based CF algorithm-s. And for the non-personalized recommendation algorithm, we consider the popularity-based recommendation ( POP ), which simply recommends the items with the highest num-ber of positive feedbacks.

The relationship between the personalized recommenda-tion and non-personalized recommendation can be illustrat-ed by four typical cases in the risk-reward diagram, as shown in Figure 3. For each case, the three bar diagrams show the actual ranking performance comparison between BPR and POP after portfolio optimization, which picks an optimal point on the efficient frontier. The right  X  - X  plot gives the mean (expected reward) and the standard deviation (risk) for two sets of individual items, two efficient frontiers, and two optimized item portfolios. In this section, we focus the analysis on whether to personalize or not, given these two item portfolios optimized independently. The detailed dis-cussion of estimating the individual item mean and standard deviation and optimizing the item portfolio for reranking will be presented in the following sections.

Figure 3(a) and 3(b) show two cases that one item port-folio dominates the other. We can see the dominating one always obtains higher ranking performance than the dom-inated one. In both cases, the selection (switch) criterion is straightforward: given two item portfolios, the one with higher expected reward and lower risk (i.e., the dominating portfolio) will be chosen.

Figure 3(c) and 3(d) present another two cases of no direct dominance, where one portfolio has higher expected reward while the other has lower risk. The item portfolio cannot be easily selected as the cases of direct dominance. For exam-ple, as shown in Figure 3(c), the portfolio with lower risk fails to outperform the one with higher expected reward. A different result is exhibited in Figure 3(d), where the item portfolio with higher expected reward is outperformed by the one with lower risk. In such situations, a switch algo-rithm that considers how to select an optimal portfolio from those of no direct dominance is needed.

Based on previous analysis, given two recommended item lists from personalized and non-personalized recommenda-tion algorithms, what we need to do is threefold: Item Level -Estimate the expected reward and risk for each individual item. That is to obtain the individual item points on  X  - X  plots in Figure 2 and 3. In our paper, we lever-age Bayesian linear regression based on latent factor model to estimate the mean and variance of the preference value of user u on each candidate item i , which will be briefly dis-cussed in Section 4.1.
 Portfolio Level -Given the mean and variance of each item and their correlations, find the optimal weighting com-bination of them. That is to obtain the optimal weighting points on the efficient frontiers in Figure 3. We propose our portfolio optimization algorithm in Section 4.2.
 Two-Portfolio Level -Given the two optimized item port-portfolios. Data source: MovieLens. folios from personalized and non-personalized recommenda-tions respectively, design the mean-variance-aware switch al-gorithms to choose whether to personalize or not. We pro-pose a series of switch algorithms in Section 4.3.
On the item level, we focus on the estimation of  X  u,i ,  X  for each item. Here we leverage the Bayesian latent factor model to estimate them, inspired by [25, 12] 1 . Latent factor model is a typical well-known personalized recommendation algorithm. Considering the probability distribution of the user X  X  rating, we can use the probabilistic matrix factoriza-tion (PMF) [25] to model, that is PMF suggests to provide prior distributions for U and V and then the maximum a posteriori (MAP) solutions for U and V for the given rating data can be directly obtained. Here we relate the uncertainty in the user-item rating r u,i with the uncertainty from the user factors [27], while taking the item factors as learnt and fixed. Therefore, here we fix the item latent factors V from this MAP solution [25], and the distribution of U in a form of Bayesian inference in linear regression can be obtained.

Denote the user u  X  X  behavior observation as D u , where each row is the latent factor V T i of the item i that he/she has rated, and denote the corresponding rating vector as r . Suppose the prior distribution of u  X  X  latent factor U is a Gaussian p ( U u ) = N ( U u | U 0 ,  X  0 ). Then its posterior distribution is still a Gaussian where
Note that other alternatives to estimate these parameters can also be adopted [24, 35].
 Therefore, the Gaussian distribution of the rating r u,i can be written as where the mean and variance are
We also need to estimate the mean values and variances of the items from a non-personalized perspective. To do this, we assume an  X  X verage user X  who has an averaged rating be-havior across the whole user set. The posterior distribution of latent factors of the  X  X verage user X  is calculated by taking average across the parameters of the posterior distributions of all users. Then the mean and variance of individual items are estimated by Eq. (16) 2 .

In addition, for the item correlation  X  i,j , a routine solution is to leverage the item-item rating similarity in item-based k NN algorithms [8]. Note that the Pearson correlation of item latent factors can also be adopted here.
In this section, we focus on the item portfolio weighting optimization analysis. Investors in finance always adjust their shares in an investment portfolio in order to have better expected reward as well as to reduce risk [15], here in the recommendation scenario, it is preferred to provide highly relevant items while the risk of the user dissatisfaction being controlled. This target can be achieved by refining the item ranking list according to the weights of the items.
In [31], Wang et al. provided a general form of the objec-tive function in practice of information retrieval. Here the parameter b indicates the risk-averse level of the user. Thus the contour line of objective function forms a
Due to the rating prediction model of PMF, mean values of some items are possibly higher than 5 (rating upper bound), as shown in Figure 3. Figure 4: Contour lines of objective functions and the different optimal solution for different objective functions. Figure 5: An example of rankings and their cor-responding regions on the efficient frontier. The covariances are cov (1 , 2) =  X  0 . 5 , cov (1 , 3) =  X  0 . 7 , cov (2 , 3) =  X  0 . 4 . quadratic curve on the  X  - X  plot. To illustrate, the left panel of Figure 4 shows the efficient frontier of three individual items, and two contour lines with two different risk-averse levels ( b = 2 and b = 10). The right panel of the figure shows two portfolios and two contour lines with the same risk-averse level tangent with them.

The optimal weight point is the point of tangency between the objective quadratic curve and the efficient frontier. The solution can be obtained by the following equations: where the intercept t of the contour line  X  p = t + b X  2 p objective value in Eq. (17),  X  i  X  X  are the same as in Eq. (11). The solution of  X  p is and the analytic solution for the optimal weight w  X  can be further obtained by Eq. (8).
Compared to the portfolio weighting in finance, here the weight w i for each item indicates how much attention the recommender system wants the target user to pay on the item i . Therefore, according to the position bias [4], the final delivered items should be ranked by the descending order of their weights.

Figure 5 gives an illustration of the relationship between item rankings and their corresponding regions on the effi-cient frontier. We can see not all possible item rankings nec-essarily occur on the efficient frontier (no 2-1-3 and 2-3-1 ). Specifically, the mean is more emphasized in the portfolios with ranking 3-2-1 while the variance is more emphasized in the portfolios with ranking 1-2-3 . The other two rankings try to strike a balance between mean and variance.
From previous sections, we can now use the proposed risk management model to calculate the expected reward and risk for any item portfolio to the target user, and then find the optimal portfolio weighting strategy to rerank the recommended items. Now we aim to propose switch algo-rithms, which make decisions between personalized and non-personalized recommendation portfolios to get the highest utility, based on the expected reward and risk.
Choosing whether to personalize or not in recommenda-tion is essentially a switch decision problem. Given the information of two item portfolios (from personalized and non-personalized recommendation respectively), the switch policy  X  chooses one to deliver, denoted as  X  ( u,p 1 ,p 2
Suppose there is a utility function r ( u,p ) which estimates the utility of recommending an item portfolio p to user u . Then the corresponding optimal policy can be defined ac-cording to the utility as follows:
Here we propose some heuristic and learning-based switch algorithms.
 Mean prior ( Mean ) -Select the portfolio with higher ex-pected reward. Variance prior ( Var ) -Select the portfolio with lower risk. Linear risk target ( Lin ) -Select the portfolio with higher  X   X  b X  value. Quadratic risk target ( Quad ) -Select the portfolio with higher  X   X  b X  2 value. Dominance motivated ( Dom ) -Select the personalized portfolio by default, unless it is dominated by the non-personalized one. Golden Switch ( Golden ) -In order to check the potential of these switch algorithms, here we provide an upper bound algorithm, which exactly switches to the better one. Adaptive risk parameter ( X-A ) 3 -Different users have different risk-averse levels. For example, some users expect the recommender system could provide some serendipitous items for them to discover new interest, while some users may just need the recommendations to be consistent with their previous interest. This risk-averse level parameter is indeed the b in Eq. (17), denoted as b u here.

In order to adaptively learn this parameter for each indi-vidual user, we conduct a cross validation on the training data to empirically tune the parameter b u for each user u and pick the optimal value. Then during the portfolio opti-mization for each user u , we adopt this adaptive parameter b . And during the latter switch decision process, for the Lin and Quad algorithm, we also adopt b u to make decision.
Note that the calculations on item level, portfolio level, and two-portfolio level are all independent from (non-) per-sonalized recommendation algorithms, which indicates that our work can be seamlessly generalized on other (non-) per-sonalized recommendation algorithms.
In this section, we intend to answer the following question-s from the experiments: (i) Will the portfolio optimization models improve the ranking performance in both personal-ized and non-personalized cases? (ii) Given the two opti-mized ranking lists, which switch algorithm works the best and why? Datasets -We base our experiments on the two popular datasets MovieLens-100k 4 and Netflix. MovieLens contains 100K ratings from 943 users and 1,682 items, while Netflix contains 100M ratings from 480,189 users and 17,770 items. Following the experimental setting of [19] we pick the 4 or 5-star ratings as positive feedbacks and give a 4:1 random splitting for the training and test data.
 Compared Algorithms -We compare the portfolio-based reranking algorithms and switch algorithms to the base rec-ommendation algorithms. Also, we explore and discuss the combination of the three types of algorithms.

Base recommendation algorithms retrieve top-N items for each target user. Here we choose BPR and POP as the per-sonalized and non-personalized algorithms respectively.
Portfolio-based reranking algorithms take the retrieved top-N items (from based recommendation algorithms) and the target user u X  X  profile as inputs. It evaluates the ex-pected reward and risk for each item and then performs the portfolio optimization to obtain the optimal weighting for the combination of items, which finally will determine the ranking of the items. Specifically, we propose to directly solve out the optimal weighting solution (denoted as Opt ) as in Section 4.2.1 and then rerank the items by the de-scending order of the optimal weighting. Furthermore, the Bayesian latent factor model discussed in Section 4.1 is lever-aged to estimate the individual item expected reward and risk. For comparison, a portfolio-based greedy ranking algo-rithm proposed in [31, 27] is used (denoted as Greedy ) which sequentially chooses the item to maximize the incremental target in Eq. (17), and the expected reward and risk are calculated accordingly [27].

Switch algorithms take the expected reward and risk of the two optimized item portfolios ( Opt-BPR and Opt-POP ) along with the user X  X  risk-averse level parameter b as inputs, and choose between the two portfolios and finally return the
Here X is one of the previously introduced algorithms.
This small dataset is chosen because of its convenience of providing case illustrations.
 Table 1: Item reranking performance on MovieLens Table 2: Item reranking performance on Netflix corresponding item ranking list as the output. The com-pared switch algorithms are listed in Section 4.3, denoted as Mean , Var , Lin , Quad , Dom , and Golden respectively. Note that  X  -A  X  denotes that the algorithm is based on the adaptive risk parameter setting, such as Opt-BPR-A and Opt-Mean-A . Evaluation Measures -Three ranking measures are used in our experiments: precision (P@ n ), normalized discounted cumulative gain (NDCG@ n ) and mean reciprocal rank (M-RR). Besides the absolute performance values, the improve-ment of the best algorithm against the baseline algorithm is also calculated. In addition, Wilcoxon signed-rank tests are conducted and statistically significant improvements are marked with *.
In this section, we first compare portfolio-based rerank-ing algorithms and then we test the effectiveness of different switch algorithms. Finally, we provide case studies to fur-ther illustrate the effectiveness of our models.
We present the performance for the two portfolio-based r-eranking algorithms ( Opt and Greedy ) based on the two base recommendation algorithms ( BPR and POP ). In addition, we add the algorithms with adaptive risk parameter ( Opt-X-A ) into the comparison. The overall results on MovieLens and Netflix datasets are shown in Table 1 and 2 respectively. The improvement is calculated between the best algorithm and the baseline BPR .

From the overall results we can have the following obser-vations. (i) On all evaluation measures, the two portfolio-based reranking algorithms Greedy and Opt bring improve-ment on both of the output of base recommendation algo-rithms BPR and POP . (ii) Compared with Greedy , Opt brings a higher improvement, which verifies the effectiveness of our method that tries to find the optimal portfolio weighting for items and ranks them by the weight. (iii) Furthermore, Opt-X-A with the setting of adaptive learning on parameter b u for each target user leads a further high improvement. This indicates different users have different risk-averse level-s, which can be traced through analyzing each user X  X  history data. (iv) Among the three evaluation measures, the im-provements on MRR are the most significant, which is con-sistent with the findings in [31, 32] that reciprocal ranking metric is optimized when diversifying the ranking list.
To see the impact of the risk-averse level parameter b , we trace the performance of Opt-BPR and Greedy-BPR against b in comparison with non-b BPR and adaptively tuned-b Opt-Figure 6: Performance of different item reranking algorithms against b -MovieLens. Figure 7: Performance of different item reranking algorithms against b -Netflix.
 BPR-A . The results are shown in Figure 6 and 7. The cor-responding results on POP are much similar. We can see that the performance of Greedy is much sensitive against b and will drop even lower than the original BPR when b gets larger than some value. This is mainly caused by the my-opic picking strategy of Greedy . On the other hand, as has been shown in Figure 5, as the risk parameter b changes, the item ranking from Opt will not change frequently which is consistent with the results in Figure 6 and 7.
Based on the portfolio-based reranking algorithm, for each user, consider there are two item recommendation lists (port-folios), from Opt-BPR (or Opt-BPR-A ) and Opt-POP (or Opt-POP-A ), the switch algorithm chooses one list (port-folio) to recommend. The results on two datasets are pre-sented in Table 3 and 4 respectively. The switch/total im-provements are calculated between the best algorithm and BPR-Opt-A / BPR , respectively.

From the results we can observe: (i) the baseline switch algorithm Mean usually results in a performance drop from Opt-BPR , which means that purely selecting a list by the higher expected reward is not sufficiently effective. (ii) The risk-aware baseline switch algorithm Var leads to improve-ment on MovieLens while bringing some decreases of NDCG and MRR on Netflix. Given the item lists from portfolio-based reranking algorithms, choosing the list with less risk is somewhat reasonable but the performance drop on Netflix indicates the instability of this algorithm across different datasets. (iii) Compared with Mean and Var , the mean-variance-aware algorithms Lin and Quad bring improvement of all the measures on two datasets, exhibiting the stable ef-fectiveness of such switch algorithms. Compared with each other, there is no obvious difference of the performance be-tween Lin and Quad . (iv) The dominance-based switch al-gorithm Dom , although not as effective as Lin or Quad , still improves the performance. Hence the dominating portfolio should always be selected whenever it exists. (v) Equipped with the adaptive risk parameter setting, all the switch al-gorithms bring further improvements. On MovieLens, Opt-Lin-A obtains the best performance, while on Netflix, the best performance is brought by Opt-Quad-A in Precision and NDCG, and Opt-Lin-A in MRR.
Figure 8 provides two typical MovieLens cases with the users X  historic item information, mean-variance-aware analy-sis for (non-) personalized recommendations, and their rank-ing performances.

User-82 is a 50 years old male programmer. He has rated 86 movies, most of which are of relatively low popularity. It is typical that the mainstream movies do not match the interest of people of such an age. On the other hand, the 86 movies he has rated provide sufficient information for the recommender system to learn his profile and to catch his personal interest. In the mean-variance-aware analysis, the portfolio of the three recommended movies from per-sonalized algorithm dominates that from non-personalized algorithm. Thus all the switch algorithms will choose the personalized recommendation. And the final ranking per-formances support such analysis.
 User-247 is a 28 years old male engineer. Compared to User-82 , he has only 15 feedbacks, most of which are pop-ular ones. It is yet another common case as young people tend to find their interests in the mainstream movies. Al-so the relatively low number of feedbacks do not support a sufficiently learned preference profile for him. In the mean-variance-aware analysis, it shows the dominance of the non-personalized portfolio on the personalized one. Therefore, the non-personalized recommendation should be chosen for him. Again, the ranking performances are consistent with the analysis.
In this paper, the portfolio retrieval theory [31, 20] is used to study the relationships between personalized and non-personalized recommendations. Under a single risk-reward diagram, we studied the efficient frontiers for both recom-mendation solutions. In either case, diversifying a ranking list would reduce the risk of a recommended item list. We found that the personalized recommendation does not al-ways dominate the non-personalized one. For each target user, we developed several strategies to determine whether or not to personalize the recommendation. The experimen-tal results demostrated the effectiveness of the proposed portfolio-based reranking algorithms and the switch algo-rithms. For the future work, we plan to generalize the study in this paper and deal with the blending of two or multiple item lists from different (non-) personalized recommenda-tion algorithms.
