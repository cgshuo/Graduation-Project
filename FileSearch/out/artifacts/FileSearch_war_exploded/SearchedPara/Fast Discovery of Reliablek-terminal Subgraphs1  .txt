 Graphs and networks are powerful means of representing information in various domains such as biology, sociology, and communications. However, large graphs are difficult to understand and use by humans. Given that the user is interested in some particular nodes and their connectivity, a large fraction of the original graph is often irrelevant. Subgraph extraction addresses this problem.
As an example application, consider Biomine , a biological graph consisting roughly of a million nodes and eight million edges [1]. One form of a query to Biomine is to specify a small number of query nodes, such as a gene and a disease, and extract a small subgraph that maximally connects the gene to the disease. A subgraph of few dozens of nodes typically already gives a good picture of the connectivity X  X ot only the best paths, but a subgraph describing the network that connects the given nodes. At the same time, almost all of the millions of edges and nodes are irrelevant to how the gene is related to the disease.
In the most reliable subgraph problem [2], the user gives query nodes (also called terminals) and a budget, and the task is to extract a subgraph maximally relevant with respect to the given query nodes, but with a size within the given budget. The problem is defined for simple (Bernoulli) random graphs, where edge weights are interpreted as probabilities of the edges, and where a natural definition for  X  X elevance X  is network reliability (see Section 2).

In this paper, we propose a novel, efficient algorithm for extracting a reliable subgraph given an arbitrary number of query nodes. Previous work on the most reliable subgraph problem suffers either from a limitation to exactly two query nodes, or from computational complexit y. Our work builds on a recent, efficient method for two query nodes, called Path Covering [3]. We define the problem of finding the most reliable k -terminal subgraph, loosely following conventions and notations from previous work [4]. Let G =( V, E )be an undirected graph where V is the set of nodes and E the set of edges. G is a Bernoulli random graph where each edge e has an associated probability p e . The interpretation is that edge e  X  E exists with probability p e , and conversely e does not exist, or is not true with probability 1  X  p e . Given edge probabilities, the states of edges are mutually independent. Nodes are static.

Given a set Q  X  V of nodes, or terminals ,the network reliability R( G, Q )of G is defined as the probability that Q is connected, i.e., that any node in Q can be reached from any other node in Q [5]. In the most reliable subgraph problem we are looking for a subgraph H  X  G connecting the terminals in Q , such that H has at most B edges and a maximal reliability with respect to the terminals, i.e., find H  X  =argmax H  X  G, || H || X  B R( H, Q ). Although the problem can be defined for directed graphs as well [2], we focus on undirected graphs in this paper. This problem, like reliability problems in general [6], is inherently difficult: efficient solutions are available only for restrict ed classes of graphs, but cases on general graphs are most likely intractable [2].
 We now introduce some additional notation used in the later sections. Given a graph G , V ( G ) is the node set of G and E ( G )theedgesetof G . Given a set of edges S  X  E ,wesay S induces agraph G ( S )=( V ,S ), where V consists of the endpoints of the edges in S .

The union between two graphs G 1 =( V 1 ,E 1 )and G 2 =( V 2 ,E 2 )isanew graph H = G 1  X  G 2 =( V 1  X  V 2 ,E 1  X  E 2 ). Other set operations for graphs are defined analogously. For notational convenience, we treat paths, trees and edges as (induced) graphs when there is no risk of confusion. This makes it notationally easy, e.g., to add a path P to a graph G by writing simply G  X  P instead of G  X  G ( P ), or to denote the edges of a tree T as E ( T ) instead of E ( G ( T )). Finally, a path with endpoints u and v is said to be a u  X  v -path.
 Related Work. The most reliable subgraph proble m was introduced recently [2], but algorithms were given only for the two-terminal case. We are aware of two previous solutions for the g eneral case. One, proposed by Kroese et al., is based on the cross-entropy method [7]. De Raedt et al. give other solution to the general case in the setting of theory compression for ProbLog [8,9]. Unfortunately, these methods do not scale well to large databases, where input graphs may have hundreds or thousands of edges. Other closely related work includes connection subgraphs [10], center-piece subgraphs [11], and proximity graphs [12].
Recently, a novel Monte-Carlo samplin g based algorithm Path Covering (PC) has been proposed for the two-terminal case [3]. The method proposed in this paper is based on the ideas of PC, so we briefly review its principles. The al-gorithm has two phases: a path sampling phase and a subgraph construction phase. In the path sampling phase, the goal is to identify a small set C of paths that have high probabilities and are relatively independent of each other. This is achieved by approximately maximizing the probability that at least one path P  X  C is true. We denote this probability by Pr( C )=Pr(
In the subgraph construction phase, PC chooses a set of solution paths S  X  C such that the probability Pr( S )=Pr( where || G || denotes the number of edges in G . PC does not maximize R( G ( S )) directly, but works on its lower bound Pr( S ) instead. Concisely put, PC gen-erates S iteratively by choosing at each iteration the path P  X  which gives the maximal per-edge increase to the (estimated) probability Pr( S ), that is where H = G ( S ) is the result subgraph being constructed. At each iteration, paths that become included into H are removed from C . To satisfy the budget The algorithm stops when || H || = B or C \ S =  X  , and returns the subgraph H . We propose a novel, efficient algorithm for the problem of extracting a reliable k -terminal subgraph from an undirected graph. The proposed algorithm is a generalization of the Path Covering (PC) method [3] (see Section 2 for a brief overview) to more than two query nodes. T he basic principles remain the same: the two phases, use of Monte Carlo simulations, as well as many subtle details. However, whereas PC uses paths connect ing the two given query nodes as its building blocks (set C ) in the subgraph construction phase, here we consider spanning trees connecting the k query nodes, with 2  X  k  X | V | . Similarly, set S in the objective function (1) consists of spanning trees instead of paths as in PC.
In the first phase, the algorithm extracts a set of trees from the original graph G . Each of the trees connects the given k query nodes; by construction, they are spanning trees having the query nodes as leaves. In the second phase, these trees are used as building blocks t o construct the result of the algorithm just like PC uses paths as its building blocks. We focus on the novel aspects of the proposed algorithm, the ones that allow solving the k -terminal problem. For brevity, we omit technical details sh ared with Path Covering and described in depth elsewhere [3]. We begin by presenting the general aspects of the new algorithm and then proceed to more detailed description.
 Input and output data. The first phase of the algorithm (Algorithm 1) takes a random graph G and a set Q  X  V of query nodes as its input. The algorithm outputs a set C of trees such that each tree connects all the query nodes. We call these trees candidate trees . C is used as an input in the second phase of the algorithm (Algorithm 2).
 Producing candidate trees. At the first iteration, ( | Q | 2  X  X  Q | ) / 2 new can-didate trees are generated (Lines 2 X 3) . Each tree connects one pair of query nodes and each query node pair is conn ected by one tree. Later, as the algo-rithm proceeds, new trees are added one by one; each of the later trees is also initially formed as a path between two query nodes. During the algorithm, indi-vidual trees are created and grown iterativ ely. In each iteration, either a branch isaddedtoanexisting incomplete tree (a tree that does not yet connect all query nodes) so that a new query node is conn ected to the tree, or a new initial tree is generated. At the end of the algorithm we output only complete trees (trees that connect all query nodes).
 Edge sampling. The algorithm is stochastic. At each iteration it randomly decides, according to the probabilities p e , which edges exist and which do not (Line 5). Only edges that are included in at least one candidate tree are decided. All other edges are considered to exist. The next step is to determine if any of the previous candidate trees exist in the current graph realization (Line 9). If one does not exist a new candidate tree is generated (Lines 14 X 15). If a previously discovered tree exists, the first such tree is taken into examination (Line 10). If the tree is complete, the algorithm pro ceeds directly to the next iteration. Oth-erwise the tree is extended (Lines 17 X 21) before continuing to the next iteration. Tree construction. A new tree is formed by the best path connecting two query nodes (Line 15). A prev iously established incomplete tree is extended by connecting a new query node to it wit h the best path between some node in the tree and the new query node (Line s 18 X 20). The pr obabilities of all edges in the tree are set to 1 prior to the search of the best path (Line 17), while the probabilities of other edges remain the same. As a result the new branch is formed by the best path between the new query node and the tree. Edges that do not exist at the iteration are not used. All edge weights are set to their original values before proceedin g to the next iteration (Line 21).
 Choosing query nodes. When a new tree is formed, the algorithm decides randomly which two query nodes are included in the initial tree. Later on when an incomplete tree is extended, the algorithm again randomly selects the new query node to connect to the tree. This is to avoid unnecessary complexity: in our experiments this solution produced better results and shorter running times than selecting the node to be added based on its distance from the tree (results not shown).
 Discovering strong trees. The collection C of candidate trees is organized as a queue, i.e., the oldest candidate trees are always considered first. This drives the algorithm to complete some trees first (the oldest ones) rather than extending them in random order and not necessarily up to a completion. On the other hand, the stochasticity of the algorithm favors strong trees: they are more likely to be true at any given iteration and thus more likely to be extended. The algorithm also has a tendency to avoid similar trees: when two (partial) trees are true at the same time, only the oldest one is potentially extended.
 Algorithm 1. Sample trees Stopping condition. We use the number | C | of complete trees generated as the stopping condition for the first phase. The number of iterations would be another alternative (see Section 4). Using the nu mber of trees seems a better choice than using the number of iterations, since the minimum number of iterations needed to produce a single complete tr ee increases when the number of query nodes increase. We have implemented and experimentally evaluated the proposed algorithm on random graphs derived from public biological databases. In this section, we ex-amine the usefulness of k -terminal subgraphs: do they maintain the k -terminal reliability of input graphs, and what is the amount of random variance in the results? We demonstrate the efficiency and scalability of our algorithm for large input graphs. Finding a suitable stopping criterion for Algorithm 1 is difficult; we also address this issue. Finally, we compare the algorithm against a baseline method based on enumerating the best paths between the query nodes.
 Algorithm 2. Select trees 4.1 Test Set-Up Test data and query nodes. We use Biomine database [1] as our data source. Biomine is a large index of various interlinked public biological databases, such as EntrezGene, UniProt, InterPro, GO, and STRING. Biomine offers a uniform view to these databases by representing th eir contents as a large, heterogeneous biological graph. Nodes in this graph represent biological entities (records) in the original databases, and edges represent their annotated relationships. Edges have weights, interpreted as probabilities [1]. We evaluated the proposed method using six source graphs of varying sizes (Table 1) and a set of up to ten query nodes. They were obtained as follows.

First, the largest subgraph, consisting of approximately 5000 edges and 1500 nodes, was retrieved from the Biomine database using Crawler , a subgraph re-trieval component proprietary to Biomine. For this initial retrieval, we used eight randomly selected query nodes.

Second, a set of ten query nodes to be used in the experiments was de-fined by randomly picking nodes from the subgraph of 5000 edges. The query node identifiers are EntrezGene:348, EntrezGene:29244, EntrezGene:6376, En-trezGene:4137, UniProt:P51149, UniProt:Q91ZX7, EntrezGene:14810, UniProt: P49769, EntrezGene:11810, and UniProt:P98156.

Third, smaller subgraphs were retrieved with Crawler by a sequence of sub-graph retrievals, always extracting the next smaller subgraph from the previous subgraph, using the ten query nodes given above.

We also used two additional source graphs when evaluating the scalability of the algorithm to large source graphs. The smaller one consisted of 51,448 edges and 15,862 nodes. The size of the larger graph was 130,761 edges and 66,990 nodes. The smaller graph was a subgraph of the larger one and all other source graphs used in the experiments were subgraphs of both.
 Biomine Crawler. The subgraph retrieval component of the Biomine system,  X  X rawler X , was used to extract the source graphs, and it will also be used below in a comparative experiment to assess the effectiveness of the proposed algorithm. Crawler is currently undocumented. Given a source graph, a set of query nodes, and a node budget, it works roughly as follows. It first finds a large set of best paths between all pairs of query nodes. I t then picks those paths sequentially between the node pairs, until the subgraph induced by the chosen paths reaches the specified number of nodes. The method is somewhat similar to the BPI algorithm [4], but works for multiple query nodes. Even though the Crawler works with random graphs, it does not try to optimize the k -terminal reliability. Stopping condition. We used the number of complete candidate trees generated as the stopping condition for Algorithm 1. Another obvious alternative would be the number of iterations. Neither condition is perfect: for instance, the number of query nodes has a strong effect on the number of trees needed to find a good subgraph. On the other hand, the number of query nodes has also a strong effect on the number of iterations needed to produce a sufficient amount of trees. A single fixed number of candidate paths is a suitable stopping condition for the two-terminal case [3] but it is problematic in the k-terminal case where the building blocks are trees consisting of multiple branches. For the current experiments, we believe a fixed number of candidate trees gives a fair impression of the performance of the method.
 Parameters. The experiments have been performed using the following param-eter values; the default values we have used throughout the experiments, unless otherwise indicated, are given in boldface.  X  Size of source graph G (Table 1): || G || = 400, 500 , 700, 1000, 2000, 5000  X  Number of query nodes: | Q | =1,2,3, 4 , ... ,10  X  Size of extracted subgraph: || H || = 10, 20, 30, ... , 60 , ... , 100, 150, 200  X  Number of complete trees (stopping condition of Algorithm 1): | C | = 10, 20, To control random variation, the values reported below are averages over 10 independent runs. 4.2 Results Let us first take a look at how well the method manages to extract a reliable subgraph (Fig. 1). For three to four query nodes ( | Q | ), a subgraph of only 20 X 30 edges manages to capture 80% of the reliability of the source graph of 500 edges. For a large number of query nodes, the problem seems much more challenging. It seems obvious, that larger subgraphs are needed for a larger number of query nodes, if the reliability should be preserved.

The number | C | of candidate trees produced in the first phase of the al-gorithm has an effect on the relia-bility of the extracted subgraph, but we discovered that sampling a rela-tively small number of trees is enough to produce good subgraphs (approxi-mately 50 trees for four query nodes; results not shown). An experimental analysis of the running time indicates that the method scales linearly with respect to the number of candidate trees generated.

The scalability of the new algo-rithm to large source graphs (Fig. 2, left) is clearly superior to previous methods (see Section 1). Source graphs of thousands of edges are handled within a second or two. Scalability is close to linear, which is expected: the running time of the algorithm is dominated by Monte-Carlo simulation, whose complexity grows linearly with respect to the input graph size and the number of iterations. Running times for the two ad-ditional large source graphs (51,448 edges and 130,761 edges) are not shown in the figure, but the average running times over ten independent runs are approx-imately 16 (standard deviation 0.57) and 53 seconds (standard deviation 2.8), respectively. Limiting the length of tree branches might shorten running times in some cases.
The right panel of Fig. 2 indicates the original reliability of the growing source graph, as well as the reliability of the extracted subgraph (of a fixed size of 60 edges). The relative difference in reliability is less than 20% in all cases, emphasizing the ability of the algorithm to preserve strong connectivity between the query nodes. These results suggest that the algorithm is competitive for interactive visualization.
Finally, we compare the new algo-rithm to the Biomine Crawler (Fig. 3), as it is the only available method for comparison on this scale. The com-parison is not completely fair, as the Crawler does not aim to optimize the k -terminal reliability, but the general goal of extracting a subgraph connect-ing the query nodes is the same. In the experiments, the proposed algo-rithm produces significantly more re-liable subgraphs, especially when the extracted subgraphs are small. Both methods converge towards the relia-bility of the source graph. However, where the new method reaches 80% of the original reliability with only 30 edges, the Crawler needs 60 edges for the same. We gave an efficient algorithm for solving the most reliable subgraph extraction problem for more than two query nodes. This is a significant improvement over state-of-the-art that could efficiently only handle exactly two query nodes.
Experimental results with real biological data indicate that the problem and the proposed method have some very useful properties. First, reliable k -terminal subgraphs of fairly small sizes seem to ca pture essential connectivity well. Sec-ond, the proposed method extracts a reliable subgraph in a matter of seconds, even from a source graph of thousands of edges; the time complexity seems to be linear with respect to the size of the original graph.

There are many possible variants of the approaches described in this paper that could be explored to find better solutions. For instance, how to choose which partial tree to expand and how to expand it, or how to efficiently use partial trees also in the second phase? Another interesting approach could be using (approximated) Steiner trees as spanning trees.

Future experiments include systematic tests to find out robust sets of param-eters that perform reliably over wide range of input graphs and query nodes, and more extensive comparisons with related methods. Possible extension of the proposed algorithm for directed variant of the problem is an open question. Acknowledgements. We would like to thank the Biomine team and espe-cially Lauri Eronen for providing the Biomine Crawler. This work has been supported by the Algorithmic Data Analys is (Algodan) Centre of Excellence of the Academy of Finland (Grant 118653).

