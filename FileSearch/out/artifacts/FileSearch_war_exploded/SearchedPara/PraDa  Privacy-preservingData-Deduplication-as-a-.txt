 The data-cleaning-as-a-service ( DCaS ) paradigm enables users to outsource their data and data cleaning needs to com-putationally powerful third-party service providers. It raises several security issues. One of the issues is how the client can protect the private information in the outsourced data. In this paper, we focus on data deduplication as the main data cleaning task, and design two efficient privacy-preserving data-deduplication methods for the DCaS paradigm. We analyze the robustness of our two methods against the at-tacks that exploit the auxiliary frequency distribution and the knowledge of the encoding algorithms. Our empirical study demonstrates the efficiency and effectiveness of our privacy preserving approaches.
 H.2.0 [ General ]: Security, integrity, and protection Data-cleaning-as-a-service, data deduplication, outsourcing, security, privacy-preserving
It has been well known that real-world datasets, partic-ularly those from multiple sources, tend to be dirty -inac-curate, incomplete, and inconsistent. According to a recent survey [8], around 40% of companies have suffered losses, problems, or costs due to poor quality data. Dirty data are commonly present in both single data collections (e.g., due to misspellings during data entry, missing information or other invalid data) or multiple data sources. Many data management and decision making applications, e.g., data warehouses and web-based information systems, require ex-tensive support for data cleaning.
 Data cleaning is a labor-intensive and complex process [3]. Providing a data cleaning solution that is powerful, reliable, and easy to use is extremely challenging. With corporate data exploding in volume and variety, cleaning of large scale data has grown difficult. However, building in-house tools for data cleaning is highly labor-intensive, and may create maintenance problems down the road. Alternatively, pur-chasing expensive proprietary data cleaning solutions may not be acceptable by those medium-and small-size organi-zations that have limited budgets. A possible solution to resolve this dilemma is to outsource the data to a third-party data cleaning service provider for efficient data clean-ing. This motivates the data-cleaning-as-a-service (DCaS) paradigm that enables organizations with limited computa-tional resources to outsource their data cleaning needs to a third-party service provider. Recently, several academic and industrial organizations have started investigating and developing technologies and infrastructure for cloud-based data cleaning services (e.g., OpenRefine [1]).

In this paper, we focus on data deduplication , one of the important data cleaning problems, that aims to identify the records that refer to the same entity. Outsourcing data deduplication to a third-party service provider raises a few issues. For instance, when the outsourced data involves any personal information (e.g., medical or financial details of in-dividuals), the privacy of those information needs to be care-fully protected. Indeed, personal information is commonly required for data deduplication to match records from differ-ent databases [5]. It is therefore paramount to protect data privacy when outsourcing the data to the DCaS provider.
A concept that is highly related to data deduplication is record linkage , which shares the same goal as data dedupli-cation as finding similar records in the datasets. There has been extensive study of the privacy-preserving record link-age ( P P RL ) problem that enables linking large databases across organizations while preserving the privacy of the en-tities stored in these databases. The existing P P RL solu-tions consider either the 2-party that consists of two pri-vate database owners (e.g., [2, 19]) or 3-party that compro-mises of a trusted third-party besides the two data owners (e.g., [12, 21]). Unlike these existing P P RL techniques, we consider an asymmetric paradigm that consists of two par-ties where one is computationally weak and the other one is much more powerful. We call the weak party client and the other party server in the rest of the paper.

We assume the server is untrusted. Therefore, to pro-tect the sensitive information in the outsourced dataset, the client encodes its data and sends the encoded data to the server. The attacker (i.e., the server) may try to decode the received dataset to infer the private information. We assume that the attacker has (either exact or approximate) frequ ency distribution information of the outsourced data, and will launch the frequency analysis attack by exploiting the frequency knowledge to break the encoding. Further-more, we assume that the attacker knows the details of the encoding scheme; he can exploit such knowledge to launch the known-scheme attack , trying to recover the original data.
In general, the client can use salt , a random value that is different for each tuple, as part of the encoding scheme to prevent the frequency analysis attack. However, adding salt may disable to find duplicated records from the encoded data. On the other hand, using hash values [5], Bloom filters [22], and embedding techniques [21] still enable one-to-one mapping between the original and encoded data values, thus cannot defend against the frequency analysis attack. Contributions. In this paper, we design the Pr ivacy-pres erving Da ta-D eduplication-a s-a-Service ( P raD a ) system that enables the client to outsource her data as well as data deduplication needs to a potentially untrusted server. P raDa enables the server to discover near duplicated records from the encoded data, while providing provable privacy guarantee against both the frequency analysis and the known-scheme attacks. To the best of our knowledge, we are the first to design privacy-preserving techniques for the DCaS paradigm against the frequency analysis attack. In particu-lar, our contributions include the following.

First, we design the locality-sensitive hashing based ( LSHB ) scheme that utilizes locality-sensitive hashing ( LSH ) to map strings to hash values such that similar strings share the same value with high probability [15]. To defend against the frequency analysis attack, we design the split-and-duplication method to ensure that the LSH values always behave a uni-form frequency distribution. We analyze the privacy guar-antee of the LSHB approach against both the frequency analysis and the known-scheme attacks. The LSHB ap-proach can deal with a number of distance measurement metrics for categorical data.

Second, we design the embedding and homophonic sub-stitution ( EHS ) scheme. EHS first maps string values to a Euclidean space in the way that similar strings are mapped to close Euclidean points. To defend against the frequency analysis attack, all identical Euclidean points are encoded as different but close points. The frequency distribution of the encoded Euclidean points is always flattened, regardless of the frequency distribution of original data. We analyze the robustness of EHS against both the frequency analy-sis and the known-scheme attacks. The EHS approach can deal with both categorical and numerical data.
 Finally, we evaluate and compare the performance of both LSHB and EHS in terms of time complexity and data deduplication accuracy, on real datasets. Our results show that both approaches guarantee high recall (i.e., the percent-age of original duplicated records that are returned by the server). In particular, EHS always guarantees 100% recall. LSHB delivers lower recall (around 70%), but it has bet-ter time performance than EHS at both client and server sides. For both approaches, the client X  X  computational effort is much cheaper than the server as well as executing data deduplication locally.

The rest of the paper is organized as follows. Section 2 describes the preliminaries. Section 3 and 4 discuss our LSHB and EHS methods respectively. Section 5 presents the post-processing procedure at the client side. Section 6 provides the experiment results. Section 7 discusses the related work. Section 8 concludes the paper.
Record linkage techniques are used to link together records which relate to the same entity (e.g. patient or customer) in one or more data sets where a unique identifier is not available. The key of the most record linkage techniques is to measure the similarity of records. Normally two records S and S 2 are  X  -similar , denoted as S 1  X  S 2 , if their similarity according to some distance measurement metrics is no less than a given threshold  X  . In this paper, we consider the Jaccard similarity based on q -grams. But we also discuss how to extend our methods to other distance metrics such as edit distance and hamming distance.

We consider a client-server framework that contains a data owner as the client, and the service provider as the server. The client has a private dataset D and would like to find out all record pairs in D that are near-duplicates. To pro-tect the private information in D , she transforms D to D and sends D  X  together with the similarity threshold to the server. When the server receives D  X  , it executes the data deduplication on D  X  to find all data pairs that are similar (e.g., their distance is less than the client X  X  threshold), and returns the results to the client.
Since the server is potentially untrusted, the client will send the encoded records to the server for data deduplica-tion. We assume the server is semi-honest , meaning that it follows the protocols honestly but it is curious to extract private information. In this paper, we consider two spe-cific attacks, namely the frequency analysis attack and the known-scheme attack .
 Frequency analysis attack. We assume the attacker may possess the knowledge of the domain values of the outsourced data, as well as the frequency distribution of the data val-ues at attribute level. As shown previously [22, 7], such adversary knowledge can be easily obtained in real-world applications. We assume that the attacker has the exact frequency occurrence of attribute values. The attacker can launch the frequency analysis attack by matching the en-coded data with the original ones of the same frequency. We assume that the attacker has no prior knowledge about the correlations of data values.
 Known-scheme attack. We assume the attacker knows the details of the encoding methods that are used by the client. He may try to break the encoding scheme by utilizing the knowledge of the encoding methods.
We define precision and recall to measure the impact of privacy-preserving techniques to the accuracy of record link-age. Formally, given a dataset D , let R O be the similar strings in D , and R E be the pairs of similar strings (possibly in encrypted format) returned by the service provider. The precision is measured as P re = | R E  X  R O | | R sures the fraction of near-duplicates by the encoding method that are correct, while the recall measures the fraction of original near-duplicates that are preserved by the encoding methods. We aim at designing the privacy-preserving tech-niques that achieve provable privacy guarantee with high prec ision/recall.
In this section, we propose two locality-sensitive hash-ing based ( LSHB ) approaches, namely the duplication ap-proach (Section 3.1) and the split-and-duplication approach (Section 3.2). The key idea of the two LSHB methods is to map the strings to locality-sensitive hashing ( LSH ) values. LSH is a set of hash functions that map objects into several buckets such that similar objects share a bucket with high probability, while dissimilar ones do not. Formally, let O be the domain of objects, and dist () be the distance measure between two objects. Then,
Definition 3.1. [( d 1 , d 2 , p 1 , p 2 )-sensitive hashing] A func-objects o 1 , o 2  X  O : (1) If dist ( o 1 , o 2 ) &lt; d 1 h ( o 2 )]  X  p 1 ; (2) If dist ( o 1 , o 2 ) &gt; d 2 , then P r h ( o 2 )]  X  p 2 , where 0  X  d 1 &lt; d 2  X  1, and 0  X  p 1 A family is interesting when p 1 &gt; p 2 . Intuitively, any two similar objects will have the same LSH value with high prob-ability, while any two dissimilar objects will have the same LSH value with a low probability. To date, several LSH fam-ilies have been discovered for different distance metrics. In this paper, we consider the q-gram based Jaccard metric and use the MinHash[6] as the LSH function family. The family of MinHash functions is a ( d 1 , d 2 , 1  X  d 1 , 1  X  d 2 ily for any d 1 and d 2 , where 0  X  d 1 &lt; d 2  X  1. Our method can be applied to other distance metrics such as Hamming distance and edit distance.

Intuitively, sending LSH values guarantees privacy as it does not involve any data. However, the LSH values are vulnerable against the frequency analysis attack by which the attacker tries to map the LSH values to strings based on their frequency. Formally, given n unique strings S = { S 1 , . . . , S n } and m  X  n LSH values of these strings, let f and l i be the frequency of the string S i (1  X  i  X  n ) and the LSH value L i (1  X  i  X  m ) respectively. Then for each LSH value L i (1  X  i  X  m ), the attacker tries to find its matching candidates S  X   X  S s.t. (1) for all S i , S j  X  S  X  , S i and (2) P S to find those similar strings that are mapped to the same LSH value based on their frequency. Then the attacker X  X  probability that the LSH value L i (1  X  i  X  m ) is mapped to a specific string S j  X  S  X  is prob ( L i  X  S j ) = 1 |S  X  | LSH value that has few matching candidates (e.g., those strings have few similar others), it can be mapped to the correct string with high probability (possibly 100%). To defend against the frequency analysis attack on the LSH-based approach, we define  X  -privacy to quantify the desired privacy guarantee: Definition 3.2. [  X  -privacy] Given a set of unique strings S { S 1 , . . . , S n } and their LSH values L { L 1 , . . . , L L is  X  -private if for each L i  X  L , the probability that maps it to string S j satisfies that prob ( L i  X  S j )  X   X  . Intuitively, lower  X  provides higher privacy guarantee.
Next, we present our duplication-based approach that con-structs  X  -private LSH values. It consists of two steps: Step 1: Grouping. First, we construct the LSH values of all strings in S . Second, we sort LSH values by their fre-quency in descending order. Third, starting from the most frequent LSH value, we repeat grouping k = [ 1  X  ] ad jacent values together into one group (called an  X  -group), until all LSH values are grouped. The last group, if less than k in size, is merged with its previous group.
 Step 2: Frequency homogenization. For each group, let l max be the maximum frequency of its LSH values. Then for each LSH value L i in the group, we add l max  X  l i copies of L i , where l i is the frequency of L i .

After the two steps, all LSH values in the same group have the same frequency, regardless their frequency in the original dataset. Frequency analysis attack. The duplication-based ap-proach guarantees that there are always at least [ 1  X  ] LSH v alues that are of the same frequency. Therefore, even for the worst case that these values have no other similar strings, the probability of associating any LSH value with its orig-inal string based on their frequency is at most  X  . In other words, the duplication approach guarantees  X  -privacy. Known-scheme attack. When the attacker knows the de-tails of the duplication approach, he can apply the following two-step attack trying to find the mapping between LSH val-ues and strings. First, the attacker tries to find the mapping between string groups and LSH groups. He can execute the grouping step of the duplication approach over the string values to get string groups; the string groups are expected to be similar to the LSH groups of the strings. Then for each string group, he will try to find out the corresponding LSH values based on their frequency. From the knowledge of the encoding algorithm, he knows that with high probability all LSH values of the same frequency belong to the same group. Furthermore, for any string S and its LSH value L , it must be true that f req ( S )  X  f req ( L ). Based on these informa-tion, he can map LSH groups to the string groups based on their frequency. Note that all LSH values in the same group have the same frequency, while the strings in the same group do not have to. Apparently, for any string groups of k unique and dissimilar strings, its LSH group must contain exactly k unique LSH values. Furthermore, since it is highly likely that a string group only contains dissimilar strings, the max-imum frequency of the LSH group must be no larger than the maximum frequency of its string group. The attacker can use these knowledge to map LSH groups to string groups. Under the worst case, there is only one such mapping. Sec-ond, the attacker tries to find the mapping between specific strings and LSH values based on the group mapping result. Consider a mapping that consists of k strings and k LSH val-ues. The attacker constructs all possible mappings between k LSH values and k strings, with each LSH value mapped to exactly one string. There are k ! such mappings. Among these mappings, ( k  X  1)! mappings map a specific LSH value L (1  X  j  X  k ) to its correct string S i (1  X  i  X  k ) correctly. Therefore, the probability prob ( L j  X  S i ) = ( k  X  1)! Sinc e k = [ 1  X  ], th e duplication-approach is  X  -private against the known-scheme attack.
The duplication approach requires O ( n ) to group and add duplicates at the client side, where n is the number of unique strings in D . The complexity of data deduplication at the server side is O (( | D | + p ) 2 ), where | D | is the total num-ber of strings in D ( | D |  X  n ), and p is the total num-ber of duplicated LSH values. In particular, the number of duplicated LSH values that are added to the string S i r = ( f max  X  f i ). The total number p of duplicated LSH val-ues added to D is p = the datasets of skewed frequency distribution. This will hurt the performance of data deduplication in terms of both accu-racy and time performance. Next, we discuss our split-and-duplication approach that can improve the duplication-based approach by reducing the number of added LSH copies, while still providing provable guarantee against both the fre-quency analysis attack and the known-scheme attack.
The main idea of the split-and-duplication ( SD ) approach is to add a split step between the grouping step and the frequency homogenization step of the duplication-based ap-proach. The aim of the split step is to convert those LSH values of large frequency to several different ones of smaller frequency, so that the number of added LSH values by the frequency homogenization step can be reduced. Apparently the split copies of the same string cannot have the same LSH values. A naive approach is, for each LSH value L of high frequency, we make several copies of L , so that the frequency of these copies accumulate to the same frequency. Then for each copy of L , we randomly modify a number of its bits, so that all split copies of L will be different after modification. Though simple, this approach may lead to high amounts of accuracy lost due to the fact that the LSH values of similar strings may not share the same bucket anymore. Therefore, instead of making split copies of LSH values, we propose to make split copies of the strings, and construct LSH values of these string split copies. We ensure that for each similar string pair, at least one pair of their split copies share the same LSH value with high probability. Therefore, the server still can identify the similar strings from the received LSH values. Next, we present the details of the split step. Our split step is based on the permutation mechanism. Formally, let S and S  X  be equal-size strings. We say S is a permutation of S  X  if there exists a bijection  X  such that for each i , S [ i ] =  X  ( S  X  [ i ]). For instance, the string  X  X acb X  is a permutation of the string  X  X bac X  as there exists the bi-jection  X  ( a ) = c ,  X  ( b ) = a , and  X  ( c ) = b . We call  X  a permutation function. For any given string S of frequency f , the client defines g unique permutation functions, and constructs g unique split copies of S by applying the g per-mutation functions on S . Each permutation generates a split copy of frequency [ f g ]. C ontinuing our example, con-sider the string  X  X bac X , and two permutation functions  X  1 and  X  2 such that  X  1 ( a ) = c ,  X  1 ( b ) = a , and  X  1 copies  X  X acb X  and  X  X cba X . We have the following theorem to show that permutation preserves Jaccard similarity.
Lemma 3.1. Given any two strings S 1 and S 2 , let  X  be a permutation function. Let S  X  1 and S  X  2 be the strings after applying  X  on S 1 and S 2 respectively. Then it must be true that jaccard ( S 1 , S 2 ) = jaccard ( S  X  1 , S  X  2 ).
The proof of Lemma 3.1 can be found in Appendix. Fol-lowing Lemma 3.1, we have the following theorem to show that for any two (dis)similar strings, their permutations us-ing the same permutation function are always (dis)similar. Indeed, this applies to not only Jaccard similarity measure-ment but also other distance metrics such as Hamming dis-tance and edit distance.
 Theorem 3.1. Given any two strings S 1 and S 2 s.t. S 1  X  S 2 ( S 1 6 X  S 2 resp.). Let S  X  1 and S  X  2 be the strings after apply-ing the permutation function  X  on S 1 and S 2 respectively. Then it must be true that S  X  1  X  S  X  2 ( S  X  1 6 X  S  X  2 resp.). Theorem 3.1 shows that the permutation-based split proce-dure will lead to neither false negatives (i.e., the split copies of two similar strings turn dissimilar) nor false positives (i.e., the split copies of two dissimilar strings become similar).
Next, we discuss how to decide the total number of split copies. Our goal is to ensure that the total amounts of du-plicated LSH values by the SD approach is no larger than that of the duplication approach. First, we calculate the total number of the duplicated values that are to be added by the SD approach. For each group that consists of k strings { S 1 , . . . , S k } , let f i be the frequency of the string S (1  X  i  X  k ). Assume that the string S i is split into g i copies. Apparently, the frequency of each split copy of S copies in the group. Then for each string S i , its number  X  r of duplicated LSH values is  X  r i = g i ( f max  X  [ f i g when the SD approach incurs smaller amounts of duplicate values than the duplication approach, we compare r i and  X  r Apparently  X  r i = g i ( f max  X  [ f i g with r i = f max  X  f i , the SD approach wins the duplication approach if f max &gt; g i f max . Note that the split copy of the s tring that has f max ma y not be the same string of f max Therefore, for each string, we always pick g i , the number of split copies, in the way that ensures f max &gt; g i f max Frequency analysis attack. The SD approach guarantees that for each LSH value, there are always at least [ 1  X  ]  X  1 other LSH values that are of the same frequency. Thus, it guarantees  X  -privacy against the frequency analysis attack. Known-scheme attack. If the attacker knows the details of the SD approach, he can launch the following two-step attack trying to decode the received LSH values. By the first step, the attacker tries to find mapping between string and LSH groups based on the frequency of strings and LSH values. From the analysis of the SD algorithm, he knows that all LSH values of the same frequency belong to the same group with high probability. Furthermore, for any string S and its split copy S  X  , f req ( S )  X  f req ( S parently, for any string groups of k unique strings, its LSH groups must contain at least k unique LSH values. Further-more, the maximum frequency of the LSH group must be no larger than the maximum frequency of its string group. The attacker can use these two facts to map LSH groups with string groups. In the worst case, there is only one such mapping. Then by the second step, the attacker tries to find the mapping between LSH values and strings. Consider a mapping that consists of k strings and  X   X  k LSH val-ues, in which LSH values are of the same frequency while the strings may have different frequency. The attacker tries to construct all possible mappings of  X  LSH values to k strings. This is equivalent to the problem of distributing  X  distinguishable balls into k distinguishable boxes where  X  &gt; k and no box is empty. The number of such map-ping equals: M (  X , k ) = P k  X  1 i =0 (  X  1) i k i ( k  X  i ) mappings, there are M (  X   X  1 , k ) + M (  X   X  1 , k  X  1) correct mappings between a LSH value L j (1  X  j  X   X  ) a nd its string S (1  X  i  X  k ). Therefore, the probability of finding the correct mapping L j  X  S i is Sinc e k  X  1  X  , the S D approach can provide  X  -privacy against the known-scheme attack. Interestingly, varying the number of split copies will not impact the privacy guarantee.
The SD approach requires O ( ns ) to construct all split copies at the client side, where n is the number of unique strings in D , and s is the average number of split copies of all strings. The complexity of the data deduplication at the server side is O (( | D | + p ) 2 ), where | D | is the total number of strings in D , and p is the total number of duplicates. Note that | D |  X  n . Thus the computational effort at the client side is much cheaper than that at the server side.
In this section, we present a different approach called Em-bedding &amp; Homophonic Substitution ( EHS ) that constructs the encoding of the input data without adding any addi-tional data. In a nutshell, EHS has two steps: (1) con-version of categorical data to Euclidean space, and (2) 1-to-many homophonic substitution of Euclidean points. The EHS approach can be easily adapted to numerical data for which the conversion step is not necessary, while the substi-tution step will be applied on the original data.
For the first step, many mapping techniques (e.g., F astM ap [9] and StringM ap [16]) can be used to map strings into a multidimensional Euclidean space. The mapping functions guarantee that the similar strings are mapped to close Eu-clidean points. In this paper, we use F astM ap [9] for string conversion. By F astM ap , the string similarity threshold  X  will be adjusted to a new similarity threshold  X  E for Eu-clidean points. F astM ap is not reversible [9], thus the origi-nal strings cannot be reconstructed from the embedded Eu-clidean points. The complexity of the embedding step is O ( d 2 | D | ), where d is the number of dimensions of the Eu-clidean points, and | D | is the number of strings in D . We omit the details of the F astM ap algorithm for space issue.
F astM ap embedding is one-to-one, simply mapping strings to Euclidean points fails to defend against the frequency analysis attack. Therefore, the second step of EHS is to apply a homophonic substitution encoding scheme on the Euclidean points to defend against the frequency analysis attack. Homophonic substitution schemes are one-to-many, meaning that multiple encoded symbols can map to one plaintext symbol. Next, we will first discuss our basic ho-mophonic substitution encoding scheme and its weakness against both the frequency analysis and the known-scheme attacks (Section 4.1). We then present our grouping-based homophonic substitution ( GHS ) scheme that can defend against these attacks with provable guarantee (Section 4.2).
By the basic approach, any Euclidean point P of frequency f is transformed to f unique points E 1 , . . . , E f in the same Euclidean space, each point of frequency 1. Therefore, the frequency distribution of the Euclidean points is always uni-form, regardless of the frequency distribution of the original data. The substitution scheme is one-to-many as one string maps to multiple Euclidean points. To preserve similarity, for each point P in the d -dimension Euclidean space, we con-struct a d -sphere H r of radius r centered at P , where r is a user-specified parameter. Let f be the frequency of P . Then we pick f points from H r in a uniformly random manner, with each point of frequency 1. These f points are the en-coded versions of P . The distance between the constructed Euclidean points satisfies the following theorem. Theorem 4.1. Given any two original Euclidean points P 1 and P 2 , let { E 1 1 , . . . , E k 1 } and { E 1 2 , . . . , E clidean points of P 1 and P 2 constructed by the above pro-cedure ( t and k are not necessarily the same). Then for any pair of Euclidean points E i 1 and E j 2 ( i  X  [1 , k ] , j  X  [1 , t ]), Euclidean distance function, and r 1 , r 2 are the length of the radius of the corresponding d -spheres of P 1 and P 2 .
The proof of Theorem 4.1 is included in Appendix. Based on Theorem 4.1, we adjust the similarity threshold of the encoded Euclidean points accordingly. We say two encoded values E i and E j are similar if dist ( E i , E j )  X   X  E where  X  E is the distance threshold after embedding, r i and r are the length of the radius of spheres of P i and P j , the corresponding original Euclidean point of E i and E j . Frequency analysis attack. Given n strings, each of fre-quency f i , and m encoded Euclidean points, each of fre-quency f ( f = 1 for the basic approach), we define sf = P i =1 f i . With further computation, the probability prob that the encoded point E j is mapped to its string S i by the frequency analysis attack is A s an example, consider two strings S 1 and S 2 of frequency 10000 and 10. These two strings are encoded as 10000 and 10 Euclidean points respectively by the basic approach. Then for any encoded Euclidean point E j of S 1 , prob F ( E j S Known-s cheme attack. If the attacker possesses the de-tails of the basic approach, he knows that all Euclidean points of the same string must fall in a circle of a small radius in the Euclidean space. Given n unique strings and m  X  n Euclidean points, he can apply the following 2-step attack to find the mapping between the Euclidean points and the strings: By the first step, it uses a clustering algorithm (e.g., k -means clustering) to group m Euclidean points to n groups based on their closeness, so that the groups are of low intra-group distances and high inter-group distances. By the second step, it maps n clusters of Euclidean points to n unique strings based on their similarities. In particular, the attacker maps the n cluster centroids to n strings in the way that for any three centroids c i , c j and c k , their corresponding strings S i , S j and S k satisfy that if dist ( c i , c j then jaccard ( S i , S j )  X  jaccard ( S i , S k ). After the two-step attack, some Euclidean points of the same string may be cov-ered by multiple clusters (i.e., mapped to multiple strings). Given n unique strings and m  X  n Euclidean points by the basic approach, the probability prob S that the encoded point E j is mapped to its corresponding string S i by the known-scheme attack is w here t is the number of clusters that include E j . If t = 1, the mapping E j  X  S i is broken with 100% certainty.
Now we are ready to define the privacy model that can defend against both the frequency analysis attack and the known-scheme attack.

Definition 4.1. [(  X  1 ,  X  2 )-privacy] Given n strings S { S 1 , . . . , S n } and m encoded Euclidean points E { E 1 , . . . , E we say E satisfies (  X  1 ,  X  2 ) -privacy if for any Euclidean point E i  X  E (whose corresponding string is S j  X  S ), prob F ( E S )  X   X  1 , and prob S ( P i  X  S i )  X   X  2 , where prob F and prob are measured by Eqn. 1 and Eqn. 2 respectively.
 As we have shown, the basic approach may fail to meet the (  X  1 ,  X  2 )-privacy requirement.
To address the privacy weakness of the basic approach, we design the grouping-based homophonic substitution encod-ing scheme ( GHS ) that satisfies (  X  1 ,  X  2 )-privacy. The GHS takes n Euclidean points that are transformed from the orig-inal n strings and outputs m ( m  X  n ) transformed Euclidean points, each of frequency 1. It satisfies that P n i =1 f where P n i =1 f i is the sum of the frequency of all Euclidean points. Before we explain the details of GHS method, we first define (  X  1 ,  X  2 ) -bounded clusters .

Definition 4.2. [(  X  1 ,  X  2 )-bounded clusters] For any given cluster C and two user-specified thresholds  X  1 ,  X  2 , C is (  X  bounded if it satisfies: (1) for each point P  X  C , f P P  X  , wh ere f P ( f X , resp.) is the frequency of point P (point X , resp); and (2) C contains at least [ 1  X  The GHS procedure consists of 2 steps. First, the Euclidean points are grouped into (  X  1 ,  X  2 )-bounded clusters. For each cluster C , a d -sphere H is constructed to cover all points of C , where d is the dimension of the Euclidean space. Sec-ond, for each sphere H that covers the points { P 1 , , P f + + f k points in total are constructed within H in a uni-form random fashion, where f i (1  X  i  X  l ) is the frequency of the point P i . Each constructed point is of frequency 1. The-orem 4.2 below shows how to update the similarity threshold for the Euclidean points constructed by the GHS approach. Theorem 4.2. Given any two original Euclidean points P 1 and P 2 that are similar according to the threshold  X   X  let { E 1 1 , . . . , E k 1 } and { E 1 2 , . . . , E t 2 of P 1 and P 2 constructed by the above procedure ( t and k are not necessarily the same), and r 1 and r 2 be the top-2 maximum radius length of all spheres. Then for any pair of Euclidean points E i 1 and E j 2 ( i  X  [1 , k ] , j  X  [1 , t ]), distance function.

The proof of Theorem 4.2 is in Appendix. Note that dif-ferent from Theorem 4.1 that considers the distance between the centers of spheres, Theorem 4.2 considers the distance between any two points within the spheres. Following The-orem 4.2, the new similarity threshold for the points con-structed by GHS approach is  X  new =  X  E + 2 r 1 + 2 r 2 . Next, we discuss how to construct (  X  1 ,  X  2 )-bounded clusters.
Our algorithm consists of the following steps. First, each point is initialized as a cluster. Second, for each cluster C i that is not (  X  1 ,  X  2 )-bounded, we look for other clus-ter(s) to merge with C i . In particular, for each cluster candidate C j , we compute the radius length of the sphere by merging C j with C i , and pick the candidate that leads to the smallest radius length. We design an efficient algo-rithm that computes the approximate radius length in O (1) time. In particular, given two clusters C i , C j of centers c and c i respectively, let r i and r j be the radius length of the spheres of C i and C j , and c i [ x ] be the value of the x -th dimension of the vector c i . The new center c new the sphere that covers all points in C i  X  C j is computed as c new [ x ] = number of points of C i and C j , and d is the dimension of the Euclidean space. Then the approximate radius length of the sphere that covers all points in C i  X  C j is computed as r new = max ( dist ( c i , c new ) + r i , dist ( c j , c look for the cluster C j whose merge with C i will lead to the smallest r new . After that, we check whether C i  X  C a (  X  1 ,  X  2 )-bounded cluster. If not, we repeat the merging procedure, until all clusters are (  X  1 ,  X  2 )-bounded.
The GHS approach provides (  X  1 ,  X  2 )-privacy against the frequency analysis attack, as each point belongs to a (  X  cluster. Next, we discuss the robustness against the known-scheme attack. If the attacker knows the details of the GHS method, he can group the received Euclidean points into clusters of high intra-cluster similarity. Next, he tries to map Euclidean points to the strings based on their frequency. In particular, a cluster of f points is highly likely to be mapped the attacker can find a unique mapping between the clusters and the strings. After that, for each cluster C i that con-sists of  X  Euclidean points { E 1 , . . . , E  X  } (each of frequency f = 1), assume C i is mapped to k  X   X  strings { S 1 , . . . , S He can apply the frequency analysis attack (Section 4.1.1) on C i . Since each cluster is (  X  1 ,  X  2 )-bounded, the probabil-ity prob F that the encoded point E j is mapped to its corre-sponding string S i by the frequency analysis attack equals to prob F ( E j  X  S i ) = f i f / mf f = f i mf  X   X  1 . Th e attacker also computes the probability of mapping each Euclidean point to a specific string by trying all possibilities. The reasoning of the attack probability is similar to the attack as described in Section 3.2.1, where M (  X , k ) = P k  X  1 i =0 (  X  1) i k k  X  [ 1  X  approach can provide (  X  1 ,  X  2 )-privacy guarantee.
The complexity of constructing (  X  1 ,  X  2 )-bounded clusters is O ( ny ), where n is the number of Euclidean points, and y is the number of required iterations. Our empirical study shows that y is normally a small portion of n (at most half of n ). The complexity of constructing the spheres for each cluster is O (1), and the complexity of constructing all trans-formed Euclidean points is O ( n ). Thus the total complexity of GHS is O ( ny ). The complexity of data deduplication at the server side is O ( | D | 2 ), where | D | is the total num-ber of strings of D . Since identical string values are always mapped to the same Euclidean point, | D |  X  n . Therefore, the computations at the client side are much cheaper than that at the server side.
After the server returns the near-duplicates, the client maps the received results to strings, then eliminates the false positives by measuring the similarity of all near-duplicate pairs. Assume that the similarity of each near-duplicate pair can be measured in constant time. Then the complexity of the post-processing is O ( | R S | ), where | R S | is the number of near-duplicates that are returned by the server. Compared with the complexity of data deduplication of the original dataset D (quadratic to | D | ), the post-processing is much cheaper than running the data deduplication locally (note that | R S |  X  | D | ). Experiment environment. We implemented both of our LSHB and EHS approaches in Java. All experiments were executed on a PC with a 2.4GHz Intel Core i5 CPU and 8GB memory running Linux.
 Datasets. We use two real datasets 1 from US Census Bu-reau in our experiments: (1) the female first name dataset ( FEMALE for short) that contains 4275 unique female first names and their frequencies, and (2) the last name dataset ( LAST for short) that contains 88799 last names and their frequencies. We also use subsets of LAST dataset of various sizes for scalability measurement. To study the impact of pairwise distance distribution to the performance, we also prepared two types of datasets: (1) the thick dataset in which a large potion of strings are similar to each other; and (2) the sparse dataset in which most strings are dissimilar. Parameters. We use q = 2 for q -gram setting and set the jaccard similarity threshold to be 0.4 for both FEMALE and LAST datasets. For LSHB approach, we apply 100 random permutations to generate the MinHash signatures.
First, we measure the impact of various  X  values (for  X  -privacy) to the time performance of LSHB approach (Fig-ure 1 (a)). We measure preparation time at the client side, data deduplication time at the server side, and post-processing time at the client side. First, the preparation time at the client side is stable. This is because the complexity of en-coding is decided by the number of unique strings, which is not affected by  X  value. The post-processing time at client side is also stable, because different  X  values do not change the number of similar string pairs. Second, the time per-formance at the server side is also stable. The reason is the complexity at the server side only relies on the num-ber of unique LSH values (and thus the number of unique strings), thus changing  X  values does not affect the running ava ilable at http://goo.gl/cjbN . time at server side. Third, the computational efforts at the server side dominates the whole data deduplication process (including both at the client and the server side). This ob-servation consolidates our claim that the LSHB approach suits the outsourcing paradigm very well.

Second, we measure the scalability of LSHB approach on datasets of various sizes (Figure 1 (b)). We observe that the preparation time at the client side increases slowly with the growth of the data size, while the post-processing time at the client side and the time performance at the server side increases sharply. This is because the complexity of preparing LSH values at the client side is linear to the data size, while the complexity of deduplication and data post-processing is quadratic to data size. Second, compared to the time performance at the server side, the post-processing time at the client side is very small, as the server has fil-tered many dissimilar string pairs. We also observe that the time performance of preparation at the client side is much cheaper than that of the server side. Indeed, the LSH con-struction time at client side never exceeds 2 seconds. We also observe that the difference between the performance at the client and the server sides becomes more significant with the growth of data size. This is because the complexity of preparation at the client side is linear to the data size, while the complexity at the server side is quadratic to the data size. This proves the advantage of the outsourcing paradigm that handles large datasets most of the time.

Third, we measure the impact of LSHB encoding to the time performance of data deduplication. We compare the time of the LSHB approach (including the time at the client and the server side) with the time of the data deduplica-tion on the original data (Figure 1 (c)). We observe that the time performance of both scenarios increases with the growth of data size. However, the time performance of the LSHB encoding is always cheaper than that of the original data. This speed-up is mainly due to the use of LSH val-ues. This shows that although the privacy-preserving meth-ods may bring computational overhead in general, picking a right encoding method (e.g., LSHs) may enable faster data deduplication than the brute force approach of measuring the pairwise similarity of all string pairs.
Storage overhead. We measure the amounts of the added LSH copies by using duplication ratio . We define the duplication ratio as | D  X  | | D | , wh ere | D | and | D number of strings in original D and after applying LSHB respectively. In Figure 2 (a), the ratio increases with the decrease of  X  (i.e., higher privacy requirement). The reason is that smaller  X  requires larger groups, and thus more in-serted copies to make their frequency homogenized. This is the price we need to pay for higher privacy.

Precision/recall We compare the precision/recall of ap-plying our LSHB approach with the precision/recall of di-rectly applying LSH hashing over the original data (Figure 2 (b)). In all the experiments, the precision varies around 75%, and the recall is around 80%. Our LSHB approach is able to provide comparable accuracy to LSH . This con-vinces us the good utility of the LSHB approach. We note that after post-processing, we can ensure 100% precision.
First, we measure the time performance of EHS encod-ings for various  X  1 and  X  2 settings of (  X  1 ,  X  2 )-privacy (Fig-ure 3 (a)). We use the similarity threshold  X  E = 0 . 2 and various  X  values (  X  =  X  1 =  X  2 ). We measure: (1) the preparation time at the client side, (2) the data dedupli-cation time at the server side, and (3) the post-processing time at the client side. First, we observe that the prepa-ration time decreases when  X  increases. The reason is that larger  X  value requires smaller clusters and points of lower frequency, leading to less merge during the (  X  1 ,  X  2 )-bounded cluster. Second, the data deduplication time is stable for all  X  values, because the complexity at the server side is decided by the dataset size, which is unchanged for these settings. Third, the post-processing time decreases when  X  increases. That is when  X  value is larger, the clusters get smaller, which leads to more tightened similarity threshold and thus fewer false positives. For all experiments, the post-processing time is always less than 0.6 seconds. Similar to LSHB approach, we observe that the server X  X  computation efforts are much higher than that of the client.

Second, we measure the scalability of EHS approach for various data sizes. As shown in Figure 3 (b), the prepara-tion time increases with the growth of the data size. That is because larger dataset requires more time to construct (  X  1 ,  X  2 )-bounded clusters. On the other hand, the post-processing time increases because the size of R S increases with the size of data. We also compare the time performance at both the client and the server side. We observe that the client requires much less time than the server. When the size of the dataset increases, the running time at the server side grows faster than the client side. This observation shows that our EHS approach suits the outsourcing paradigm well.
Third, we compare the time performance of data dedupli-cation by our EHS encoding with that on the original data (Figure 3 (c)). For EHS encoding, we add up the prepara-tion time, the post-processing time, and the data dedupli-cation time. We observe that the processing time of both cases increases with the size of the data. However, applying EHS encoding only adds a small potion of time overhead compared with the performance of the original data.
We measure the precision and recall of our EHS approach for various  X  values (  X  =  X  1 =  X  2 ) and various  X  E values (Figure 4). First, we observe that recall is always 100%. In other words, all the near-duplicates in the original dataset are returned. Second, from Figure 4 (a), we observe that precision increases with  X  value. This is because larger  X  value leads to smaller clusters, which yield more tightened  X  new . One interesting observation is, the precision on the sparse dataset increases much slower than that of the thick dataset. That is because the  X  new value of the sparse dataset is large; thus the change of  X  new (from 1.00 to 0.89) makes less impact than the change of  X  new on the thick dataset (from 0.23 to 0.21). As shown in Figure 4 (b), the preci-sion raises with  X  E , because large  X  E results in more similar pairs. Thus, the ratio of false positive decreases. In all ex-periments, we observe that the precision of the thick dataset (at least 0.4) is much better than the sparse dataset. To un-derstand why, we measured the average pair-wise distances of both datasets. It turned out that the average pair-wise distance of the thick and the sparse datasets are 0.27 and 0.31 respectively. We use 0.2 as the value of  X  E , and mea-sure the new similarity threshold  X  new . For the thick dataset,  X  new is less than 0.23, which is less than 15% change of the similarity threshold. While for the sparse dataset,  X  new changed to be between 0.9 and 1, which is too loose com-pared with the original threshold and thus concludes almost all pairs as similar. Therefore, the (  X  1 ,  X  2 )-bounded clus-tering method introduces fewer false positives on the thick dataset than the sparse dataset.
First, we compare the time performance of encoding at client side for both approaches (Figure 5 (a)). We observe that the LSHB approach is much faster than the EHS ap-proach. This is because constructing LSHs is much faster than the F astM ap embedding and (  X  1 ,  X  2 )-bounded clus-tering. Second, we compare the time performance of data deduplication at the server side. The result (Figure 5 (b)) shows that although LSHB adds additional values to the outsourced dataset, it is again faster than EHS , due to the fact that dealing with LSH values is much faster than the computation of Euclidean distances. Third, we compare the precision and recall of the two approaches (Figure 5 (c)). We observe that LSHB yields comparable precision with EHS but EHS yields much higher recall (100%). In summary, there exists the trade-off between the time performance of the privacy-preserving encoding methods and the accuracy of data deduplication on encoded data. The use of LSH val-ues enables faster time performance at both the client and the server sides, but it introduces lower recall rate, while EHS guarantees 100% recall.
Data deduplication is highly related to the record linkage problem. Privacy-preserving record linkage has been paid much attention recently. The existing approaches can be classified based on the privacy-preserving techniques they use. We classify these techniques into the following types: (1) encoding and embedding, (2) differential privacy, (3) se-cure multi-party computation (SMC).
 Substitution encoding and embedding. Churches et al. [5] propose a three-party protocol based on hash val-ues of q -grams. In particular, for each string, its power set (i.e. the subsets of the original bigram set) is constructed. Each subset of the power set is hashed using a common se-cret key shared among database owner A and B . Party A and B form tuples containing the hash values, the number of q-grams in the hashed subset and the total number of q-grams and an encryption of the identifiers to a third party C . Party C computes the similarity based on the hash values. To prevent the frequency analysis attack, Churches et al. [5] propose to: (1) use an additional trusted party, thereby extending the number of parties to four; and (2) recommend to use chaffing and winnowing [20] to insert dummy records to obfuscate the frequency information in the hash values. A trusted party may not exist in practice. Furthermore, besides the substantial increase of computational and com-munication costs, inserting dummy records is still prone to frequency attacks on the hashes of the q-gram subsets with just one q-gram [24]. Schnell et al. [22] and Durham et al. [7] propose to store the q-grams in the Bloom Filters (BFs); the BFs of two similar strings (i.e., many q-grams in common) have a large number of identical bit positions set to 1. However, Schnell et al. [22] observed that the BFs are still open to the frequency attack. [18] formalized the frequency attack on the Bloom filters as a constraint satisfaction problem, and pointed out that the probability of mapping BF values to original strings is high even when only the frequency of samples is used for the attack. To fix this problem, [7] propose composite BFs by which the attribute-level bloom filter values are to be combined into a single bloom filter for the entire record. Compared with our work, Kuzu et al. [18] consider the attack that tries to map BF bits back to the attributes from which they were con-structed by utilizing the frequency of bits, while we consider the attack that maps the encoded values to their original strings based on their frequency. Storer et al. [23] con-sider the duplicates as exact identical contents. They use convergent encryption that uses a function of the hash of the plaintext of a chunk as the encryption key, so that any identical plaintext values will be encrypted to identical ci-phertext values. This one-to-one encryption cannot defend against the frequency analysis attack.

Scannapieco et al. [21] focus on the three-party scenario and use embedding techniques to transform original strings into a Euclidean space by using the SparseM ap method [11]. Since the mapping between each distinct string and each distinct Euclidean point is still one-to-one, the embedded Euclidean points can be easily mapped to their strings easily based on their frequency.
 Differential privacy. Bonomi et al. [4] propose a new transformation method that integrates embedding methods with differential privacy. Its embedding strategy projects the original data on a base formed by a set of frequent vari-able length grams. The privacy of the gram construction procedure is guaranteed to satisfy differential privacy. How-ever, as the embedding is still one-to-one, it cannot defend against the frequency-based attack. Inan et al. [12, 13, 14] propose a three-party protocol that first runs the blocking step by utilizing anonymized data sets to accurately match or mismatch a large portion of record pairs. Then the block-ing step is followed by the SMC step, where unlabeled record pairs are labeled using cryptographic techniques. However, the SMC step still involves high computational cost [4]. Secure multi-party (SMC) computations. Most of the aforementioned protocols involve 3 parties. Atallah et al. [2] design a SMC protocol based on homomorphic encryption for record linkage based on edit distance similarity. Raviku-mar et al. [19] propose a SMC protocol for two-party record matching using TF-IDF distance and Euclidean distance. Since we consider a client-server framework, we aim to de-sign privacy-preserving encoding methods that are more ef-ficient than SM C computations.
In this paper, we designed two data encoding methods for privacy-preserving data-deduplication-as-a-service ( DDaS ) paradigm. Our two methods can defend against the fre-quency analysis attack and the known-scheme attack with provable guarantee, while enabling to find near-duplicates from the encoded data. For the future work, we are in-terested in extending our work to other similarity measure-ment metrics, e.g., edit distance. We also plan to investigate other security issues related to DDaS , e.g., efficient result integrity verification of the returned near-duplicates.
Proof. Let  X  be the alphabet. Since  X  is a bijection, for any
Proof. Given any two points P 1 and P 2 , we pick a point A Figure 7: Similarit y Preservation of Euclidean Points
Proof. Given any two Euclidean points P 1 and P 2 , assume
