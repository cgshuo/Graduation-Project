 Recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, tar get mark eting, netw ork intrusion detection, etc. Con ventional kno wl-edge disco very tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper , we propose a general frame work for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, nai ve Bayesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected clas-sification accurac y on the test data under the time-e volving environ-ment. Thus, the ensemble approach impro ves both the efficienc y in learning the model and the accurac y in performing classification. Our empirical study sho ws that the proposed methods have sub-stantial adv antage over single-classifier approaches in prediction accurac y, and the ensemble frame work is effecti ve for a variety of classification models.
 H.2.8 [ Database Management ]: Database Applications X  data min-ing ; I.2.6 [ Artificial Intelligence ]: Learning X  concept learning ; I.5.2 [ Patter n Recognition ]: Design Methodology X  classifier de-sign and evaluation classifier , classifier ensemble, data streams, concept drift
The scalability of data mining methods is constantly being chal-lenged by real-time production systems that generate tremendous amount of data at unprecedented rates. Examples of such data streams include netw ork event logs, telephone call records, credit card transactional flo ws, sensoring and surv eillance video streams, etc. Other than the huge data volume, streaming data are also char -acterized by their drifting concepts. In other words, the underlying Cop yright 2003 ACM 1-58113-737-0/03/0008 ... $ 5.00. data generating mechanism, or the concept that we try to learn from the data, is constantly evolving. Kno wledge disco very on streaming data is a research topic of gro wing interest [1, 4, 7, 19]. The funda-mental problem we need to solv e is the follo wing: given an infinite amount of continuous measurements, how do we model them in or-der to capture time-e volving trends and patterns in the stream, and mak e time-critical predictions?
Huge data volume and drifting concepts are not unf amiliar to the data mining community . One of the goals of traditional data mining algorithms is to learn models from lar ge databases with bounded-memory . It has been achie ved by several classification methods, including Sprint [21], BO AT [14], etc. Ne vertheless, the fact that these algorithms require multiple scans of the training data mak es them inappropriate in the streaming environment where examples are coming in at a higher rate than the y can be repeatedly analyzed.
Incremental or online data mining methods [25, 14] are another option for mining data streams. These methods continuously revise and refine a model by incorporating new data as the y arri ve. Ho w-ever, in order to guarantee that the model trained incrementally is identical to the model trained in the batch mode, most online algo-rithms rely on a costly model updating procedure, which sometimes mak es the learning even slo wer than it is in batch mode. Recently , an efficient incremental decision tree algorithm called VFDT is in-troduced by Domingos et al [7]. For streams made up of discrete type of data, Hoef fding bounds guarantee that the output model of VFDT is asymptotically nearly identical to that of a batch learner .
The abo ve mentioned algorithms, including incremental and on-line methods such as VFDT , all produce a single model that repre-sents the entire data stream. It suf fers in prediction accurac y in the presence of concept drifts. This is because the streaming data are not generated by a stationary stochastic process, indeed, the future examples we need to classify may have a very dif ferent distrib ution from the historical data.

In order to mak e time-critical predictions, the model learned from the streaming data must be able to capture transient patterns in the stream. To do this, as we revise the model by incorporating new examples, we must also eliminate the effects of examples represent-ing outdated concepts. This is a non-tri vial task. The challenge of maintaining an accurate and up-to-date classifier for infinite data streams with concept drifts including the follo wing:
In light of these challenges, we propose using weighted classi-fier ensembles to mine streaming data with concept drifts. Instead of continuously revising a single model, we train an ensemble of classifiers from sequential data chunks in the stream. Maintaining a most up-to-date classifier is not necessarily the ideal choice, be-cause potentially valuable information may be wasted by discard-ing results of pre viously-trained less-accurate classifiers. We sho w that, in order to avoid overfitting and the problems of conflicting concepts, the expiration of old data must rely on data X  s distrib ution instead of only their arri val time. The ensemble approach offers this capability by giving each classifier a weight based on its ex-pected prediction accurac y on the current test examples. Another benefit of the ensemble approach is its efficienc y and ease-of-use. In this paper , we also consider issues in cost-sensiti ve learning, and present an instance-based ensemble pruning method that sho ws in a cost-sensiti ve scenario a pruned ensemble deli vers the same level of benefits as the entire set of classifiers.
 Paper Or ganization. The rest of the paper is organized as fol-lows. In Section 2 we discuss the data expiration problem in mining concept-drifting data streams. Section 3 pro ves the error reduction property of the classifier ensemble in the presence of concept drifts. Section 4 outlines an algorithm frame work for solving the problem. In Section 5, we present a method that allo ws us to greatly reduce the number of classifiers in an ensemble with little loss. Experi-ments and related work are sho wn in Section 6 and Section 7.
The fundamental problem in learning drifting concepts is how to identify in a timely manner those data in the training set that are no longer consistent with the current concepts. These data must be discarded. A straightforw ard solution, which is used in man y cur -rent approaches, discards data indiscriminately after the y become old, that is, after a fix ed period of time T has passed since their arri val. Although this solution is conceptually simple, it tends to complicate the logic of the learning algorithm. More importantly , it creates the follo wing dilemma which mak es it vulnerable to unpre-dictable conceptual changes in the data: if T is lar ge, the training set is lik ely to contain outdated concepts, which reduces classifica-tion accurac y; if T is small, the training set may not have enough data, and as a result, the learned model will lik ely carry a lar ge variance due to overfitting.

We use a simple example to illustrate the problem. Assume a stream of 2-dimensional data is partitioned into sequential chunks based on their arri val time. Let S i be the data that came in between time t i and t i +1 . Figure 1 sho ws the distrib ution of the data and the optimum decision boundary during each time interv al.

The problem is: after the arri val of S 2 at time t 3 , what part of the training data should still remain influential in the current model so that the data arri ving after t 3 can be most accurately classified?
On one hand, in order to reduce the influence of old data that may represent a dif ferent concept, we shall use nothing but the most recent data in the stream as the training set. For instance, use the training set consisting of S 2 only (i.e., T = t 3 t 2 , data discarded). Ho we ver, as sho wn in Figure 1(c), the learned model may carry a significant variance since S 2  X  X  insuf ficient amount of data are very lik ely to be overfitted.
The inclusion of more historical data in training, on the other hand, may also reduce classification accurac y. In Figure 2(a), where S [ S 1 (i.e., T = t 3 t 1 ) is used as the training set, we can see that the discrepanc y between the underlying concepts of S becomes the cause of the problem. Using a training set consisting of S 2 [ S 1 [ S 0 (i.e., T = t 3 t 0 ) will not solv e the problem either . Thus, there may not exists an optimum T to avoid problems arising from overfitting and conflicting concepts.

We should not discard data that may still pro vide useful infor -mation to classify the current test examples. Figure 2(c) sho ws that the combination of S 2 and S 0 creates a classifier with less over-fitting or conflicting-concept concerns. The reason is that S 0 have similar class distrib ution. Thus, instead of discarding data using the criteria based solely on their arri val time, we shall mak e decisions based on their class distrib ution. Historical data whose class distrib utions are similar to that of current data can reduce the variance of the current model and increase classification accurac y.
Ho we ver, it is a non-tri vial task to select training examples based on their class distrib ution. In this paper , we sho w that a weighted classifier ensemble enables us to achie ve this goal. We first pro ve, in the next section, a carefully weighted classifier ensemble built on a set of data partitions S 1 ; S 2 ; ; S n is more accurate than a single classifier built on S 1 [ S 2 [[ S n . Then, we discuss how the classifiers are weighted.
Given a test example y , a classifier outputs f c ( y ) , the probability of y being an instance of class c . A classifier ensemble pools the outputs of several classifiers before a decision is made. The most popular way of combining multiple classifiers is via averaging [24], in which case the probability output of the ensemble is given by: where f i ensemble.

The outputs of a well trained classifier are expected to approx-imate the a posterior class distrib ution. In addition to the Bayes error , the remaining error of the classifier can be decomposed into bias and variance [15, 6]. More specifically , given a test example y , the probability output of classifier C i can be expressed as: where p ( c j y ) is the a posterior probability distrib ution of class given input y , i given input y . In the follo wing discussion, we assume the error consists of variance only , as our major goal is to reduce the error caused by the discrepancies among the classifiers trained on dif fer -ent data chunks.

Assume an incoming data stream is partitioned into sequential chunks of fix ed size, S 1 ; S 2 ; ; S n , with S n being the most recent chunk. Let C i , G k , and E k denote the follo wing models.
In the concept-drifting environment, models learned up-stream may carry significant variances when the y are applied to the cur -rent test cases (Figure 3). Thus, instead of averaging the outputs of classifiers in the ensemble, we use the weighted approach. We assign each classifier C i a weight w i , such that w i is reversely pro-portional to C i  X  X  expected error (when applied to the current test cases). In Section 4, we introduce a method of generating such weights based on estimated classification errors. Here, assuming each classifier is so weighted, we pro ve the follo wing property .
E k produces a smaller classification err or than G k , if classifier s in
E k are weighted by their expected classification accur acy on the test data.
 Figur e 4: Err or regions associated with appr oximating the a posteriori probabilities [24].

We pro ve this property through bias-v ariance decomposition based on Tumer X  s work [24]. The Bayes optimum decision assigns ure 4, the Bayes optimum boundary is the loci of all points that p ( c i j x ) = p ( c j j x ) , where j = argmax The decision boundary of our classifier may vary from the optimum boundary . In Figure 4, b = x b x denotes the amount by which the boundary of the classifier dif fers from the optimum boundary . In other words, patterns corresponding to the darkly shaded region are erroneously classified by the classifier . The classifier thus intro-duces an expected error Err in addition to the error of the Bayes optimum decision: where A ( b ) is the area of the darkly shaded region, and density function for b . Tumer et al [24] pro ves that the expected added error can be expressed by: where s = p 0 ( c j j x ) p 0 ( c i j x ) is independent of the trained
Thus, given a test example y , the probability output of the single classifier G k can be expressed as:
Assuming each partition S i is of the same size, we study 2 the variance of g that is, there is no conceptual drift, then the single classifier which is learned from k partitions, can reduce the average variance by a factor of k . With the presence of conceptual drifts, we have:
For the ensemble approach, we use weighted averaging to com-bine outputs of the classifiers in the ensemble. The probability out-put of the ensemble is given by: where w i is the weight of the i -th classifier , which is assumed to be reversely proportional to Err i ( c is a constant): The probability output E k (4) can also be expressed as: where Assuming the variances of dif ferent classifiers are independent, we deri ve the variance of E We use the reverse proportional assumption of (5) to simplify (6) to the follo wing: It is easy to pro ve: or equi valently: which based on (3) and (7) means: and thus, we have pro ved:
This means, compared with the single classifier G k , which is learned from the examples in the entire windo w of k chunks, the classifier ensemble approach is capable of reducing classification error through a weighting scheme where a classifier X  s weight is re-versely proportional to its expected error .

Note that the abo ve property does not guarantee that E k cept drifts between the partitions are so dramatic that S represent totally conflicting concepts, then adding C n 1 making will only raise classification error . A weighting scheme should assign classifiers representing totally conflicting concepts near -zero weights. We discuss how to tune weights in detail in Section 4.
The proof of the error reduction property in Section 3 sho wed that a classifier ensemble can outperform a single classifier in the presence of concept drifts. To apply it to real-w orld problems we need to assign an actual weight to each classifier that reflects its predicti ve accurac y on the current testing data. The incoming data stream is partitioned into sequential chunks, S ; S 2 ; ; S n , with S n being the most up-to-date chunk, and each chunk is of the same size, or ChunkSize . We learn a classifier for each S i , i 1 .
 According to the error reduction property , given test examples we should give each classifier C i a weight reversely proportional to the expected error of C i in classifying T . To do this, we need to kno w the actual function being learned, which is una vailable.
We deri ve the weight of classifier C i by estimating its expected prediction error on the test examples. We assume the class distri-bution of S n , the most recent training data, is closest to the class distrib ution of the current test data. Thus, the weights of the clas-sifiers can be approximated by computing their classification error on
More specifically , assume that S n consists of records in the form of ( x; c ) , where c is the true label of the record. C i  X  X  classification error of example ( x; c ) is 1 f i given by C i that x is an instance of class c . Thus, the mean square error of classifier C i can be expressed by: The weight of classifier C i should be reversely proportional to MSE On the other hand, a classifier predicts randomly (that is, the proba-bility of x being classified as class c equals to c  X  X  class distrib utions p ( c ) ) will have mean square error: For instance, if c 2f 0 ; 1 g and the class distrib ution is uniform, we have MSE r = : 25 . Since a random model does not contain useful kno wledge about the data, we use MSE r , the error rate of the ran-dom classifier as a threshold in weighting the classifiers. That is, we discard classifiers whose error is equal to or lar ger than MSE thermore, to mak e computation easy , we use the follo wing weight w for classifier C i : For cost-sensiti ve applications such as credit card fraud detection, we use the benefits (e.g., total fraud amount detected) achie ved by classifier C i on the most recent training data S n as its weight.
Assume the benefit of classifying transaction x of actual class as a case of class c 0 is b in Table 1 (where t ( x ) is the transaction amount, and cost fraud investigation cost), the total benefits achie ved by and we assign the follo wing weight to C i : where b r is the benefits achie ved by a classifier that predicts ran-domly . Also, we discard classifiers with 0 or negati ve weights.
Since we are handling infinite incoming data flo ws, we will learn an infinite number of classifiers over the time. It is impossible and unnecessary to keep and use all the classifiers for prediction. In-stead, we only keep the top K classifiers with the highest predic-tion accurac y on the current training data. In Section 5, we dis-cuss ensemble pruning in more detail and present a technique for instance-based pruning.

Algorithm 1 gives an outline of the classifier ensemble approach for mining concept-drifting data streams. Whene ver a new chunk of data has arri ved, we build a classifier from the data, and use the data to tune the weights of the pre vious classifiers. Usually , ChunkSize is small (our experiments use chunks of size ranging from 1,000 to 25,000 records), and the entire chunk can be held in memory with ease.

The algorithm for classification is straightforw ard, and it is omit-ted here. Basically , given a test case y , each of the K applied on y , and their outputs are combined through weighted av-eraging.
 Input : S : a dataset of ChunkSize from the incoming stream
Output : C : a set of K classifiers with updated weights train classifier C 0 from S ; compute error rate / benefits of C 0 via cross validation on deri ve weight w 0 for C 0 using (8) or (9); for eac h classifier C i 2C do
C K of the top weighted classifiers in C [f C 0 g ; return C ; Algorithm 1: A classifier ensemble approach for mining concept-drifting data streams
Assume the comple xity for building a classifier on a data set of size s is f ( s ) . The comple xity to classify a test data set in order to tune its weight is linear in the size of the test data set. Suppose the entire data stream is divided into a set of n partitions, then the comple xity of Algorithm 1 is O ( n f ( s=n ) + Ks ) , where K . On the other hand, building a single classifier on s requires means the ensemble approach is more efficient.
A classifier ensemble combines the probability or the benefit out-put of a set of classifiers. Given a test example y , we need to consult every classifier in the ensemble, which is often time consuming in an online streaming environment.
In man y applications, the combined result of the classifiers usu-ally con verges to the final value well before all classifiers are con-sulted. The goal of pruning is to identify a subset of classifiers that achie ves the same level of total benefits as the entire ensemble.
Traditional pruning is based on classifiers X  overall performances (e.g., average error rate, average benefits, etc.). Several criteria can be used in pruning. The first criterion is mean square error . The goal is to find a set of n classifiers that has the minimum mean square error . The second approach favors classifier diversity , as diversity is the major factor in error reduction. KL-distance, or relati ve entrop y, is a widely used measure for dif ference between two distrib utions. The KL-distance between two distrib utions q is defined as D ( p jj q ) = P and q are the class distrib utions given by two classifiers. The goal is then to find the set of classifiers S that maximizes mutual KL-distances.

It is, howe ver, impractical to search for the optimal set of clas-sifiers based on the MSE criterion or the KL-distance criterion. Ev en greedy methods are time consuming: the comple xities of the greedy methods of the two approaches are O ( j T j N K ) O ( j T j N K 2 ) respecti vely , where N is the total number of avail-able classifiers.

Besides the comple xity issue, the abo ve two approaches do not apply to cost-sensiti ve applications. Moreo ver, the applicability of the KL-distance criterion is limited to streams with no or mild con-cept drifting only , since concept drifts also enlar ge the KL-distance.
In this paper , we apply the instance-based pruning technique [11] to data streams with conceptual drifts.
Cost-sensiti ve applications usually pro vide higher error toler -ance. For instance, in credit card fraud detection, the decision threshold of whether to launch an investigation or not is: where t ( y ) is the amount of transaction y . In other words, as long as no matter what the exact value of p ( fraud j y ) is. For example, as-suming t ( y ) = $900 , cost = $90 , both p ( fraud j y ) = 0 : 2 p ( fraud j y ) = 0 : 4 result in the same prediction. This property helps reduce the  X  X xpected X  number of classifiers needed in prediction.
We use the follo wing approach for instance based ensemble prun-ing [11]. For a given ensemble S consisting of K classifiers, we first order the K classifiers by their decreasing weight into a  X  X ipeline X . (The weights are tuned for the most-recent training set.) To classify a test example y , the classifier with the highest weight is consulted first, follo wed by other classifiers in the pipeline. This pipeline procedure stops as soon as a  X  X onfident prediction X  can be made or there are no more classifiers in the pipeline.

More specifically , assume that C 1 ; ; C K are the classifiers in the pipeline, with C 1 having the highest weight. After consulting the first k classifiers C 1 ; ; C k , we deri ve the current weighted probability as: The final weighted probability , deri ved after all K classifiers are consulted, is F K ( x ) . Let k ( x ) = F k ( x ) F K ( x ) at stage k . The question is, if we ignore k ( x ) and use decide whether to launch a fraud investigation or not, how much confidence do we have that using F K ( x ) would have reached the same decision?
Algorithm 2 estimates the confidence. We compute the mean and the variance of k ( x ) , assuming that k ( x ) follo ws normal distri-bution. The mean and variance statistics can be obtained by eval-uating F k ( : ) on the current training set for every classifier reduce the possible error range, we study the distrib ution under a finer grain. We divide [0 ; 1] , the range of F k ( ) , into bins. An example x is put into bin i if F k ( x ) 2 [ i ; i +1 ) . We then com-training examples in bin i at stage k .

Algorithm 3 outlines the procedure of classifying an unkno wn instance y . We use the follo wing decision rules after we have ap-plied the first k classifiers in the pipeline on instance where i is the bin y belongs to, and t is a confidence interv al param-eter . Under the assumption of normal distrib ution, t = 3 a confidence of 99.7%, and t = 2 of 95%. When the prediction is uncertain, that is, the instance falls out of the t sigma region, the next classifier in the pipeline, C k +1 , is emplo yed, and the rules are applied again. If there are no classifiers left in the pipeline, the cur -rent prediction is returned. As a result, an example does not need to use all classifiers in the pipeline to compute a confident prediction. The  X  X xpected X  number of classifiers can be reduced.
 Input : S : a dataset of ChunkSize from the incoming stream
Output : C : a set of K classifiers with updated weights train classifier C 0 from S ; compute error rate / benefits of C 0 via cross validation on deri ve weight w 0 for C 0 using (8) or (9); for eac h classifier C k 2C do
C K of the top weighted classifiers in C[fC 0 g ; for eac h y 2 S do Algorithm 2: Obtaining k;i and k;i during ensemble con-struction
Algorithm 3 outlines instance based pruning. To classify a dataset of size s , the worst case comple xity is O ( Ks ) . In the experiments, we sho w that the actual number of classifiers can be reduced dra-matically without affecting the classification performance.
The cost of instance based pruning mainly comes from updating k;i and 2 k;i for each k and i . These statistics are obtained during training time. The procedure sho wn in Algorithm 2 is an impro ved version of Algorithm 1. The comple xity of Algorithm 2 remains O ( n f ( s=n )+ Ks ) (updating of the statistics costs O ( Ks ) s is the size of the data stream, and n is the number of partitions of the data stream.

Input : y : a test example Output : prediction of y  X  X  class Let C = fC 1 ; ; C n g with w i w j for i &lt; j ;
F 0 ( y ) 0 ; w 0 ; for k = f 1 ; ; K g do if F K ( y ) &gt; cost=t ( y ) then else
Algorithm 3: Classification with Instance Based Pruning
We conducted extensi ve experiments on both synthetic and real life data streams. Our goals are to demonstrate the error reduction effects of weighted classifier ensembles, to evaluate the impact of the frequenc y and magnitude of the concept drifts on prediction accurac y, and to analyze the adv antage of our approach over al-ternati ve methods such as incremental learning. The base models used in our tests are C4.5 [20], the RIPPER rule learner [5], and the Nai ve Bayesian method. The tests are conducted on a Linux machine with a 770 MHz CPU and 256 MB main memory .
We denote a classifier ensemble with a capacity of K classifiers as E K . Each classifier is trained by a data set of size ChunkSize . We compare with algorithms that rely on a single classifier for min-ing streaming data. We assume the classifier is continuously being revised by the data that have just arri ved and the data being faded out. We call it a windo w classifier , since only the data in the most recent windo w have influence on the model. We denote such a clas-sifier by G K , where K is the number of data chunks in the windo w, and the total number of the records in the windo w is K ChunkSize Thus, ensemble E K and G K are trained from the same amount of data. Particularly , we have E 1 = G 1 . We also use G 0 the classifier built on the entire historical data starting from the be-ginning of the data stream up to now. For instance, BO AT [14] and VFDT [7] are G 0 classifiers, while CVFDT [19] is a G K classifier . Synthetic Data. We create synthetic data with drifting concepts based on a mo ving hyperplane. A hyperplane in d -dimensional space is denoted by equation: We label examples satisfying P d amples satisfying P d been used to simulate time-changing concepts because the orienta-tion and the position of the hyperplane can be changed in a smooth manner by changing the magnitude of the weights [19].

We generate random examples uniformly distrib uted in multi-dimensional space [0 ; 1] d . Weights a i ( 1 i d ) in (11) are initialized randomly in the range of [0 ; 1] . We choose the value of a so that the hyperplane cuts the multi-dimensional space in two parts of the same volume, that is, a 0 = 1 half of the examples are positi ve, and the other half negati ve. Noise is introduced by randomly switching the labels of p % of the exam-ples. In our experiments, the noise level p % is set to 5%
We simulate concept drifts by a series of parameters. Parame-ter k specifies the total number of dimensions whose weights are changing. Parameter t 2 R specifies the magnitude of the change (every N examples) for weights a 1 ; ; a k , and s i 2 f 1 ; 1 g specifies the direction of change for each weight a i , 1 i k Weights change continuously , i.e., a i is adjusted by s ter each example is generated. Furthermore, there is a possibility of 10% that the change would reverse direction after every amples are generated, that is, s i is replaced by s i with probabil-ity 10%. Also, each time the weights are updated, we recompute a = 1 2 P d i =1 a i so that the class distrib ution is not disturbed. Cr edit Car d Fraud Data. We use real life credit card transac-tion flo ws for cost-sensiti ve mining. The data set is sampled from credit card transaction records within a one year period and con-tains a total of 5 million transactions. Features of the data include the time of the transaction, the merchant type, the merchant loca-tion, past payments, the summary of transaction history , etc. A detailed description of this data set can be found in [22]. We use the benefit matrix sho wn in Table 1 with the cost of disputing and investigating a fraud transaction fix ed at cost = $90 .
The total benefit is the sum of reco vered amount of fraudulent transactions less the investigation cost. To study the impact of con-cept drifts on the benefits, we deri ve two streams from the dataset. Records in the 1st stream are ordered by transaction time, and records in the 2nd stream by transaction amount. Time Analysis. We study the time comple xity of the ensemble approach. We generate synthetic data streams and train single deci-sion tree classifiers and ensembles with varied ChunkSize . Con-sider a windo w of K = 100 chunks in the data stream. Figure 5 sho ws that the ensemble approach E K is much more efficient than the corresponding single-classifier G K in training.

Smaller ChunkSize offers better training performance. Ho w-ever, ChunkSize also affects classification error . Figure 5 sho ws the relationship between error rate (of E 10 , e.g.) and ChunkSize . The dataset is generated with certain concept drifts (weights of 20% of the dimensions change t = 0 : 1 per N = 1000 records), lar ge chunks produce higher error rates because the ensemble cannot de-tect the concept drifts occurring inside the chunk. Small chunks can also dri ve up error rates if the number of classifiers in an en-semble is not lar ge enough. This is because when ChunkSize is small, each indi vidual classifier in the ensemble is not supported by enough amount of training data.
 Pruning Effects. Pruning impro ves classification efficienc y. We examine the effects of instance based pruning using the credit card fraud data. In Figure 6(a), we sho w the total benefits achie ved by ensemble classifiers with and without instance-based pruning. The X-axis represents the number of the classifiers in the ensemble, which ranges from 1 to 32. When instance-based pruning is in ef-fect, the actual number of classifiers to be consulted is reduced. In the figure, we overload the meaning of the X-axis to represent the average number of classifiers used under instance-based pruning. For E 32 , pruning reduces the average number of classifiers to a reduction of 79%. Still, it achie ves a benefit of $811,838, which is just a 0.1% drop from $812,732  X  the benefit achie ved by which uses all 32 classifiers.

Figure 6(b) studies the same phenomena using 256 classifiers (
K = 256 ). Instead of dynamic pruning, we use the top K classi-fiers, and the Y-axis sho ws the benefits impro vement ratio. The top rank ed classifiers in the pipeline outperform E 256 in almost all the cases except if only the 1st classifier in the pipeline is used. Err or Analysis. We use C4.5 as our base model, and compare the error rates of the single classifier approach and the ensemble approach. The results are sho wn in Figure 7 and Table 2. The syn-thetic datasets used in this study have 10 dimensions ( d = 10 Figure 7 sho ws the averaged outcome of tests on data streams gen-erated with varied concept drifts (the number of dimensions with changing weights ranges from 2 to 8, and the magnitude of the change t ranges from 0.10 to 1.00 for every 1000 records).
First, we study the impact of ensemble size (total number of clas-sifiers in the ensemble) on classification accurac y. Each classifier is trained from a dataset of size ranging from 250 records to 1000 records, and their averaged error rates are sho wn in Figure 7(a). Apparently , when the number of classifiers increases, due to the in-crease of diversity of the ensemble, the error rate of E k nificantly . The single classifier , G k , trained from the same amount of the data, has a much higher error rate due to the changing con-cepts in the data stream. In Figure 7(b), we vary the chunk size and average the error rates on dif ferent K ranging from 2 to 8. It sho ws that the error rate of the ensemble approach is about 20% lower than the single-classifier approach in all the cases. A detailed compar -ison between single-and ensemble-classifiers is given in Table 2, where G 0 represents the global classifier trained by the entire his-tory data, and we use bold font to indicate the better result of and E k for K = 2 ; 4 ; 6 ; 8 .

We also tested the Nai ve Bayesian and the RIPPER classifier un-der the same setting. The results are sho wn in Table 3 and Table 4. Although C4.5, Nai ve Bayesian, and RIPPER deli ver dif ferent ac-curac y rates, the y confirmed that, with a reasonable amount of clas-sifiers ( K ) in the ensemble, the ensemble approach outperforms the single classifier approach. Concept Drifts. Figure 8 studies the impact of the magnitude of the concept drifts on classification error . Concept drifts are con-trolled by two parameters in the synthetic data: i) the number of dimensions whose weights are changing, and ii) the magnitude of weight change per dimension. Figure 8 sho ws that the ensemble approach outperform the single-classifier approach under all cir -cumstances. Figure 8(a) sho ws the classification error of E k (averaged over dif ferent K ) when 4, 8, 16, and 32 dimen-sions X  weights are changing (the change per dimension is fix ed at t = 0 : 10 ). Figure 8(b) sho ws the increase of classification error when the dimensionality of dataset increases. In the datasets, 40% dimensions X  weights are changing at 0 : 10 per 1000 records. An interesting phenomenon arises when the weights change monoton-ically (weights of some dimensions are constantly increasing, and others constantly decreasing). In Figure 8(c), classification error drops when the change rate increases. This is because of the follo w-ing. Initially , all the weights are in the range of [0 ; 1] changes cause some attrib utes to become more and more  X  X mpor -tant X , which mak es the model easier to learn.
 aim at maximizing benefits. In Figure 9(a), we compare the sin-gle classifier approach with the ensemble approach using the credit card transaction stream. The benefits are averaged from multiple runs with dif ferent chunk size (ranging from 3000 to 12000 trans-actions per chunk). Starting from K = 2 , the adv antage of the ensemble approach becomes obvious.

In Figure 9(b), we average the benefits of E k and G k ( K = 2 ; ; 8 ) for each fix ed chunk size. The benefits increase as the chunk size does, as more fraudulent transactions are disco vered in the chunk. Again, the ensemble approach outperforms the single classifier approach.

To study the impact of concept drifts of dif ferent magnitude, we deri ve data streams from the credit card transactions. The simulated stream is obtained by sorting the original 5 million transactions by their transaction amount. We perform the same test on the simu-lated stream, and the results are sho wn in Figure 9(c) and 9(d).
Detailed results of the abo ve tests are given in Table 6 and 5.
Data stream processing has recently become a very important re-search domain. Much work has been done on modeling [1], query-ing [2, 13, 16], and mining data streams, for instance, several pa-pers have been published on classification [7, 19, 23], regression analysis [4], and clustering [17].

Traditional data mining algorithms are challenged by two char -acteristic features of data streams: the infinite data flo w and the drifting concepts. As methods that require multiple scans of the datasets [21] can not handle infinite data flo ws, several incremental algorithms [14, 7] that refine models by continuously incorporating new data from the stream have been proposed. In order to handle drifting concepts, these methods are revised again to achie ve the goal that effects of old examples are eliminated at a certain rate. In terms of an incremental decision tree classifier , this means we have to discard, re-gro w sub trees, or build alternati ve subtrees under a node [19]. The resulting algorithm is often complicated, which indicates substantial efforts are required to adapt state-of-the-art learning methods to the infinite, concept-drifting streaming envi-ronment. Aside from this undesirable aspect, incremental methods are also hindered by their prediction accurac y. Since old examples are discarded at a fix ed rate (no matter if the y represent the changed concept or not), the learned model is supported only by the current snapshot  X  a relati vely small amount of data. This usually results in lar ger prediction variances.

Classifier ensembles are increasingly gaining acceptance in the data mining community . The popular approaches to creating en-sembles include changing the instances used for training through techniques such as Bagging [3] and Boosting [12]. The classifier ensembles have several adv antages over single model classifiers. First, classifier ensembles offer a significant impro vement in pre-diction accurac y [12, 24]. Second, building a classifier ensemble is more efficient than building a single model, since most model con-struction algorithms have super -linear comple xity . Third, the na-ture of classifier ensembles lend themselv es to scalable paralleliza-tion [18] and on-line classification of lar ge databases. Pre viously , we used averaging ensemble for scalable learning over very-lar ge datasets [10]. We sho w that a model X  s performance can be esti-mated before it is completely learned [8, 9]. In this work, we use weighted ensemble classifiers on concept-drifting data streams. It combines multiple classifiers weighted by their expected predic-tion accurac y on the current test data. Compared with incremental models trained by data in the most recent windo w, our approach combines talents of set of experts based on their credibility and adjusts much nicely to the underlying concept drifts. Also, we in-troduced the dynamic classification technique [11] to the concept-drifting streaming environment, and our results sho w that it enables us to dynamically select a subset of classifiers in the ensemble for prediction without loss in accurac y. [1] B. Babcock, S. Bab u, M. Datar , R. Mota wani, and J. Widom. [2] S. Bab u and J. Widom. Continuous queries over data [3] Eric Bauer and Ron Koha vi. An empirical comparison of [4] Y. Chen, G. Dong, J. Han, B. W. Wah, and J. Wang.
 [5] William Cohen. Fast effecti ve rule induction. In Int X  X  Conf . [6] P. Domingos. A unified bias-v ariance decomposition and its [7] P. Domingos and G. Hulten. Mining high-speed data streams. [8] W. Fan, H. Wang, P. Yu, and S. Lo. Progressi ve modeling. In [9] W. Fan, H. Wang, P. Yu, and S. Lo. Inducti ve learning in less [10] W. Fan, H. Wang, P. Yu, and S. Stolfo. A frame work for [11] Wei Fan, Fang Chu, Haixun Wang, and Philip S. Yu. Pruning [12] Yoav Freund and Robert E. Schapire. Experiments with a [13] L. Gao and X. Wang. Continually evaluating similarity-based [14] J. Gehrk e, V. Ganti, R. Ramakrishnan, and W. Loh. BO AT X  [15] S. Geman, E. Bienenstock, and R. Doursat. Neural netw orks [16] M. Greenw ald and S. Khanna. Space-ef ficient online [17] S. Guha, N. Milshra, R. Motw ani, and L. O X  X allaghan. [18] L. Hall, K. Bo wyer , W. Kegelme yer , T. Moore, and C. Chao. [19] G. Hulten, L. Spencer , and P. Domingos. Mining [20] J. Ross Quinlan. C4.5: Programs for Mac hine Learning . [21] C. Shafer , R. Agra wal, and M. Mehta. Sprint: A scalable [22] S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. [23] W. Nick Street and YongSeog Kim. A streaming ensemble [24] Kagan Tumer and Joydeep Ghosh. Error correlation and [25] P. E. Utgof f. Incremental induction of decision trees.
