 Masashi Sugiyama SUGI @ CS . TITECH . AC . JP A good metric for input data is a key factor for many ma-chine learning algorithms. Classical metric learning meth-ods fall into three types: (a) Supervised type requiring class labels (e.g., Sugiyama, 2007); (b) Supervised type requir-ing weak labels, i.e., { X  1 } -valued labels that indicate the similarity/dissimilarity of data pairs (e.g., Weinberger et al., 2005; Davis et al., 2007); (c) Unsupervised type requiring no label information (e.g., Belkin &amp; Niyogi, 2001). Types (a) and (b) have a strict limitation for real-world applica-tions since they need lots of labels. Based on the belief that preserving the geometric structure in an unsupervised man-ner can be better than relying on the limited labels, semi-supervised metric learning has emerged. To the best of our knowledge, all semi-supervised extensions employ off-the-shelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or man-ifold embedding (Hoi et al., 2008; Baghshah &amp; Shouraki, 2009; Liu et al., 2010). They can be regarded as propagat-ing labels along an assistant metric by some unsupervised techniques and learning a target metric implicitly in a su-pervised manner.
 However, the target and assistant metrics assume different forms, one Mahalanobis distance defined over a Euclidean space and one geodesic distance over a curved space or a Riemannian manifold. The two metrics also share slightly different goals: the target metric tries to learn a metric so that data in the same class are close and data from differ-ent classes are far apart (e.g., Fisher discriminant analysis (Fisher, 1936)), and the assistant one tries to identify and preserve the intrinsic geometric structure (e.g., Laplacian eigenmaps (Belkin &amp; Niyogi, 2001)). Simply putting them together works in practice, but the paradigm is conceptu-ally neither natural nor unified.
 In this paper, we propose a semi-supervised metric learn-ing approach S ERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) as an information-theoretic alternative to the manifold-based methods. Our idea is to optimize a metric by optimizing a conditional probability parameterized by that metric. Following entropy regular-ization (Grandvalet &amp; Bengio, 2004), we maximize the en-tropy of that probability on labeled data, and minimize it on unlabeled data, which can achieve the sparsity of the posterior distribution (Grac  X a et al., 2009), i.e., the low un-certainty/entropy of unobserved weak labels. Furthermore, we employ mixed-norm regularization (Ying et al., 2009) to encourage the sparsity of the projection matrix, i.e., the low rank of the projection matrix induced from the metric. Unifying the posterior sparsity and the projection sparsity brings us to the hyper-sparsity . Thanks to this property, the metric learned by S ERAPH possesses high discriminability even under a noisy environment.
 Our contributions can be summarized as follows. First, we formulate the supervised metric learning problem as an in-stance of the generalized maximum entropy distribution es-timation (Dud  X   X k &amp; Schapire, 2006). Second, we propose a semi-supervised extension of the above estimation follow-ing entropy regularization (Grandvalet &amp; Bengio, 2004). Notice that our extension is compatible with the manifold-based extension, which means that S ERAPH could adopt an additional manifold regularization term. In this section, we first formulate the model of S ERAPH and then develop the EM-like algorithm to solve the model. 2.1. Notations Suppose we have a training set X = { x i | x i  X  R m } n i =1 that contains n points each with m features. Let the sets of similar and dissimilar data pairs be With some abuse of terminology, we refer to S  X  X  as the labeled data and as the unlabeled data. A weak label y i,j = 1 is assigned to ( x i ,x j )  X  S , or y i,j =  X  1 to ( x i ,x j )  X  D . We abbrevi-P
U and P y . Consider learning a Mahalanobis distance metric for x,x 0  X  R m of the form where &gt; is the transpose operator and A  X  R m  X  m is a sym-metric and positive semi-definite matrix to be learned 2 . The probability p A ( y | x,x 0 ) of labeling ( x,x 0 )  X  R m  X  with y =  X  1 is parameterized by the matrix A . When ap-plying p A ( y | x,x 0 ) to ( x i ,x j ) , it is abbreviated as p 2.2. Basic model To begin with, we derive a probabilistic model to investi-gate the conditional probability of y =  X  1 given ( x,x 0 R m  X  R m . We resort to a parametric form of p A ( y | x,x and will focus on it for the out-of-sample ability. The maximum entropy principle (Jaynes, 1957) suggests that we should choose the probability distribution with the maximum entropy out of all distributions that match the data moments. Let 3 be the entropy of the conditional probability p A i,j ( y ) , and be a feature function that is convex with respect to A . The constrained optimization problem is where  X  is a slack variable and  X  &gt; 0 is a regularization parameter. The penalty presumes the Gaussian prior of the expected data moments from the empirical data moments, which is essentially consistent in spirit with the generalized maximum entropy principle (Dud  X   X k &amp; Schapire, 2006) (see Appendix B.1).
 Theorem 1. The primal solution p  X  A is given in terms of the dual solution ( A  X  , X   X  ) by where Z ( x,x 0 ; A, X  ) = P y 0 exp(  X f ( x,x 0 ,y 0 ; A )) , and ( A  X  , X   X  ) can be obtained by solving the dual problem Define the regularized log-likelihood function on labeled data (i.e., on observed weak labels) as Then, for supervised metric learning, the regularized max-imum log-likelihood estimation and the generalized maxi-mum entropy estimation are equivalent. 4 When considering f ( x,x 0 ,y ; A ) that should take moments about the metric information into account, we propose where  X  &gt; 0 is a hyperparameter used as the threshold to separate the sets S and D under the target metric d ( x,x Now the probabilistic model (2) becomes For the optimal solution ( p  X  A ,A  X  , X   X  ) , we hope for so there must be  X   X  &lt; 0 .
 Although we use Eq.(4) as our feature function, other op-tions are available. Please see Appendix C.1 for details. 2.3. Regularization In this subsection, we extend L 1 ( A, X  ) by entropy regular-ization to semi-supervised learning. Moreover, we regular-ize our objective by trace-norm regularization.
 Our unsupervised part does not rely upon the manifold as-sumption and is not in the paradigm of smoothing the pro-jected training data. In order to be integrated with the su-pervised part more naturally in philosophy, we follow the minimum entropy principle (Grandvalet &amp; Bengio, 2004), and hence p A i,j should have low entropy or uncertainty for ( x i ,x j )  X  U . Roughly speaking, the resultant discrimina-tive models prefer peaked distributions on unlabeled data, which carries out a probabilistic low-density separation . Subsequently, according to Grandvalet &amp; Bengio (2004), our optimization becomes where  X   X  0 is a regularization parameter.
 In addition, we hope for the dimensionality reduction abil-ity by encouraging a low-rank projection induced from A . This is helpful in dealing with corrupted data or data dis-tributed intrinsically in a low-dimensional subspace. It is known that the trace is a convex relaxation of the rank for a matrix, so we revise our optimization problem into max where tr( A ) is the trace of A , and  X   X  0 is a regularization parameter.
 Optimization (6) is the final model of S ERAPH , and we say that it is equipped with the hyper-sparsity when both  X  and  X  are positive. S ERAPH possesses standard kernel and man-ifold extensions. For more information, please refer to Ap-pendix C.2 and C.3. 2.4. Algorithm From now on we will simplify the model (6) and derive a practical algorithm. First, we eliminate  X  from (6), thanks to the fact that we use a simple feature function (4) in (1). Theorem 2. Define the simplified optimization problem as 5 max where the simplified probabilistic model is Let  X  A and ( A  X  , X   X  ) be the optimal solutions to (7) and (6) , respectively. Then, there exist well-defined hyperparame-ters  X   X  and  X   X  , such that  X  A is equivalent to A  X  with respect to d ( x,x 0 ) , and the resulting  X  p A ( y | x,x 0 ) parameterized by  X 
A and  X   X  is identical to the original p A ( y | x,x 0 ) param-eterized by A  X  ,  X   X  and  X  .
 Remark 1 . After the simplification,  X  is dropped,  X  and  X  are modified, but the regularization parameter  X  remains the same, which means that the tradeoff between the super-vised and unsupervised parts has not been affected. Optimization (7) could be directly solved by the gradient projection method (Polyak, 1967), even though it is non-convex. Nevertheless, we would like to pose it as an EM-like iterative scheme to access the derandomization by the initial solution, the stability for the gradient update, and the insensitivity to the step size, just to name a few of the gained algorithmic properties.
 The EM-like algorithm runs as follows. In the beginning, we initialize a nonparametric probability q ( y | x i ,x j then the M-Step and the E-Step get executed repeatedly un-til the stopping conditions are satisfied.
 At the t -th E-Step, similarly to Grac  X a et al. (2009), we have for each pair ( x i ,x j )  X  X  that where KL is the Kullback-Leibler divergence, and p A i,j is parameterized by the metric A ( t ) found at the last M-Step. Optimization (9) can be solved analytically. Theorem 3. The solution to (9) is given by On the other hand, at the t -th M-Step, we find new metric A ( t ) through the probability q ( y | x i ,x j ) which is gener-ated in the last E-Step and only defined for ( x i ,x j )  X  X  : max It could be solved by the gradient projection method with-out worry about local maxima using the calculation of  X  X  given by  X  X  ( A ) =  X  X the feature function f ( x,x 0 ,y ; A ) with respect to A implies the convexity of the objective F ( A ) .
 A remarkable property of F ( A ) is that its gradient is uni-formly bounded, regardless of the scale of A , i.e., the mag-nitude of tr( A ) .
 Theorem 4. The objective F ( A ) is Lipschitz continuous, and the best Lipschitz constant Lip k X k the Frobenius norm k X k F satisfies where diam( X ) = max x i ,x j  X  X  k x i  X  x j k 2 is the diameter of X , and # measures the cardinality of a set.
 In our current implementation, the initial solution is q (  X  1 | x ,x j ) = 1 , which means that we treat all unlabeled pairs as dissimilar pairs. The overall asymptotic time complex-ity is O ( n 2 m + m 3 ) in which the stopping criteria of the M-Step and the whole EM-like iteration are ignored. Dis-cussions about the computational complexity and the fast implementation can be found in Appendix D. In this section, we discuss the sparsity issues, namely, we can obtain the posterior sparsity (Grac  X a et al., 2009) by en-tropy regularization and the projection sparsity (Ying et al., 2009) by trace-norm regularization.
 By a  X  X parse X  posterior distribution, we mean that the un-certainty (i.e., the entropy or variance) is low. See Figure 1 as an example. Recall that supervised metric learning aims at a metric under which data in the same class are close and data from different classes are far apart. This results in the metric which ignores the horizontal feature and focuses on the vertical feature. However, the vertical feature is impor-tant, and taking care of the posterior sparsity would lead to a better metric as illustrated in (e) and (f). Therefore, we prefer taking the posterior sparsity into account in addition to the aforementioned goal, and then the risk of overfitting weakly labeled data can be significantly reduced.
 We can rewrite L 2 ( A, X  ) as a soft posterior regularization (PR) objective (Grac  X a et al., 2009). Let the auxiliary feature function be g ( x,x 0 ,y ) =  X  ln p A ( y | x,x 0 ) , then maximiz-ing L 2 ( A, X  ) is equivalent to On the other hand, according to optimization (7) of Grac  X a et al. (2009), the soft PR objective should take a form as where  X  i,j are slack variables. Since q is unconstrained, we can optimize it with respect to fixed A and  X  . It is easy to see that q should be p A (restricted on U ), so the KL term is zero and the expectation term is the entropy, which implies the equivalence of optimizations (13) and (14).
 Besides the above posterior sparsity, we also hope for the projection sparsity, which may guide the learned metric to better generalization performance. See Figure 2 as an ex-ample of its effectiveness, where the horizontal feature is informative and the vertical feature is useless.
 The underlying technique is the mixed-norm regularization (Argyriou et al., 2006). Denote the ` (2 , 1) -norm of a sym-Similarly to Ying et al. (2009), let P  X  R m  X  m be a pro-jection, and W = P &gt; P be the metric induced from P . Let the i -th column of P and W be P i and W i . If P i is iden-tically zero, the i -th component of x has no contribution to z = Px . Since the column-wise sparsity of W and P are equivalent, we can penalize k W k (2 , 1) to reach the column-wise sparsity of P .
 Nevertheless, this is feature selection rather than dimen-sionality reduction. Recall that the goal is to select a few most representative directions of input data which are not restricted to the coordinate axes. The solution is to pick an extra transformation V  X  X  m to rotate x before the projec-tion where O m is the set of orthonormal matrices of size m , and add V to the optimization variables. Consequently, we penalize k W k (2 , 1) , project x to z = PV x , and since A = ( PV ) &gt; ( PV ) = V &gt; WV , we arrive at The equivalence of optimizations (6) and (15) is guaranteed by Lemma 1 of Ying et al. (2009).
 Moreover, there is another justification based on the infor-mation maximization principle (Gomes et al., 2010). Please see Appendix B.2 for details. Xing et al. (2002) initiated metric learning based on pair-wise similarity/dissimilarity constraints by global distance metric learning (G DM ). Several excellent metric learning methods have been developed in the last decade, including neighborhood component analysis (N CA ; Goldberger et al., 2004), large margin nearest neighbor classification (L MNN Weinberger et al., 2005), and information-theoretic metric learning (I TML ; Davis et al., 2007).
 Both I TML and S ERAPH are information-theoretic, but the ideas and models are quite different. I TML defines a gen-erative model p A ( x ) = exp(  X  1 2 k x  X   X  k 2 A ) /Z , where  X  is unknown mean value and Z is a normalizing constant. Compared with G DM , I TML regularizes the KL-divergence between p A 0 ( x ) and p A ( x ) , and transforms this term to a Log-Det regularization. By specifying A 0 = 1 n I m , it be-comes the maximum entropy estimation of p A ( x ) . Thus, it prefers the metric close to the Euclidean distance. S APH also follows the maximum entropy principle, but the probabilistic model p A ( y | x,x 0 ) is discriminative. A probabilistic G DM was designed intuitively as a baseline in the experimental part of Yang et al. (2006). It is a special case of our supervised part. In fact, S ERAPH is much more general. Please refer to Section 2.2 for details.
 Subsequently, local distance metric learning (L DM ; Yang et al., 2006) is the pioneer of semi-supervised metric learn-ing, which assumes that the eigenvectors of A are the prin-cipal components of training data. Hoi et al. (2008) com-bines manifold regularization to the min-max principle of G
DM based on Belkin &amp; Niyogi (2001), and Baghshah &amp; Shouraki (2009) shows that Roweis &amp; Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al. (2008). The manifold extension described in Appendix C.3 can be attached to all metric learning methods, whereas our unsu-pervised part applies to probabilistic methods only. How-ever, any probabilistic method with an explicit expression of the posterior distribution adopts two semi-supervised ex-tensions, while deterministic methods such as L MNN can-not benefit from entropy regularization.
 Due to limited space, we leave out sparse metric learning and robust metric learning. Instead, we recommend Huang et al. (2009) and Huang et al. (2010) for the latest reviews of sparse and robust metric learning respectively. 5.1. Setup We compared S ERAPH with the Euclidean distance, four famous supervised and two representative semi-supervised metric learning methods 6 : global distance metric learning (G
DM ; Xing et al., 2002), neighborhood component anal-ysis (N CA ; Goldberger et al., 2004), large margin nearest neighbor classification (L MNN ; Weinberger et al., 2005), information-theoretic metric learning (I TML ; Davis et al., 2007), local distance metric learning (L DM ; Yang et al., 2006), and manifold Fisher discriminant analysis (M FDA ; Baghshah &amp; Shouraki, 2009).
 Table 1 describes the specification of the data sets used in our experiments. The top six data sets (i.e., iris, wine, iono-sphere, balance, breast cancer, and diabetes) come from the UCI machine learning repository 7 , while the USPS and MNIST come from the homepage of the late Sam Roweis 8 . Gray-scale images of handwritten digits are downsampled to 8  X  8 and 14  X  14 pixel resolution resulting in 64-and 196-dimensional vectors for USPS and MNIST. The sym-bol USPS 1  X  5 , 20 means 20 training data from each of the first 5 classes, USPS 1  X  10 , 40 means 40 training data from each of all 10 classes, MNIST 1 , 7 means digits 1 versus 7, and so forth. Note that in the last two tasks, the dimension-ality of data is greater than the size of all training data. In our experiments, all methods were repeatedly run on 50 random samplings. For each random sampling, class labels of the first few data were revealed, and the sets S and D were constructed according to these revealed class labels. The sizes of S , D and U were fixed for all samplings of USPS and MNIST but random for the samplings of UCI data sets. We measured the performance of the one-nearest-neighbor classifiers based on the learned metrics as well as the computation time for learning the metrics.
 Four settings of S ERAPH were included in our experiments (except on two artificial data sets): S ERAPH none stands for S  X  = #( S X  X  ) # U and  X  = 1 . We fixed  X  = 1 for simplicity. There was no cross-validation for each random sampling, otherwise the learned metrics would be highly dependent upon the final classifier, and also because of the large vari-ance of the classification performance given the limited su-pervised information. The hyperparameters of other meth-ods, e.g., the number of reduced dimensions, the number of nearest neighbors, and the percentage of principal compo-nents, were selected as the best value based on another 10 random samplings if default values or heuristics were not provided by the original authors. 5.2. Results Figures 1 and 2 had previously displayed the visually com-prehensive results of the sparsity regularization on two ar-tificial data sets respectively. Subfigures (c) and (d) in both E G N L I L M S S S S E G N L I L M S S S S figures were obtained by G DM , while (e) and (f) were gen-erated by S ERAPH with  X  = 10  X  #( S X  X  ) # U , X  = 0 in Figure 1 and  X  = 0 , X  = 300 in Figure 2. We can see from Figures 1 and 2 that S ERAPH improved supervised global metric learning dramatically by the sparsity regularization. The experimental results of the one-nearest-neighbor clas-sification are reported in Table 2 (G DM was sometimes very slow and excluded from the comparison). S ERAPH is fairly promising, especially with the hyper-sparsity (  X  = #( S X  X  ) and  X  = 1 ). It was best or tie over all tasks, and often statis-tically significantly better than others on UCI data sets ex-cept I TML . It was better than all other methods statistically significantly on USPS, and S ERAPH hyper outperformed both S ERAPH post and S ERAPH proj . Moreover, it improved the accuracy even on the ill-posed MNIST tasks, though the improvement was insignificant on MNIST 3 , 5 , 8 . In a word, S
ERAPH can reduce the risk of overfitting weakly labeled data with the help of unlabeled data, and hence our sparsity regularization would be reasonable and practical.
 In vivid contrast with S ERAPH that exhibited nice general-ization capability, supervised methods might learn a metric even worse than the Euclidean distance due to overfitting problems, especially N CA that optimized the leave-one-out performance based on such limited label information. The powerful L MNN did not behave satisfyingly, since it was hardly fulfilled to find a lot of neighbors belonging to the same class within labeled data. I TML was the second best method though it can only access weakly labeled data, but it became less useful for difficult tasks. On the other hand, we observed that L DM might fail when the principal com-ponents of training data were not close to the eigenvectors of the target matrix, and M FDA might fail if the amount of training data cannot recover the underlying manifold well. An observation is that the global metric learning often out-performed the local one, if the supervised information was insufficient. This phenomenon indicates that the local met-ric learning tends to fit the local neighborhood information exceedingly and then suffers from overfitting problems. Finally, we report in Figure 3 the computation time of each algorithm on each task (excluding G DM ). Generally speak-ing, S ERAPH was the second fastest method, and the fastest M
FDA involves only some matrix multiplication and a sin-gle eigen-decomposition. Improvements may be expected if we program in Matlab with C/C++.
 In this paper, we proposed an information-theoretic semi-supervised metric learning approach S ERAPH as an alterna-tive to the manifold-based methods. The generalized maxi-mum entropy estimation for supervised metric learning was our foundation. Then a semi-supervised extension that can achieve the posterior sparsity was obtained via entropy reg-ularization. Moreover, we enforced a trace-norm regular-ization that can reach the projection sparsity. The resulting optimization was solved by an EM-like scheme with sev-eral nice algorithmic properties, and the learned metric had high discriminability even under a noisy environment. Experiments on benchmark data sets showed that S ERAPH often outperformed state-of-the-art fully-/semi-supervised metric learning methods given only limited supervised in-formation. A final note is that in our experiments the pos-terior and projection sparsity were demonstrated to be very helpful for high-dimensional data if and only if they were combined with each other, i.e., integrated into the hyper-sparsity. An in-depth study of this interaction is left as our future work.
 The authors would like to thank anonymous reviewers for helpful comments. GN is supported by the MEXT schol-arship No.103250, MY is supported by the JST PRESTO program, and MS is supported by the FIRST program.
