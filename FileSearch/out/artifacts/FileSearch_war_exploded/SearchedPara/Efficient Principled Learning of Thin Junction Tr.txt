 models (PGMs) have been successful as compact representati ons for probability distributions. provides probably approximately correct (PAC) learnabili ty guarantees. Let C = { C a set of edges connecting pairs of cliques such that ( T, C ) is a tree. A set S junction tree in Fig. 1, variable x the treewidth of that junction tree is 2.
 arator S pendent of B given C by ( A  X  B | C ) . Let C i without using edge ( i  X  j ) , and denote these reachable variables by V i For example, in Fig. 1, S Definition 2. P ( V ) factors according to junction tree ( T, C ) iff  X  ( i  X  j )  X  T , V i is k -JT representable . In this case, a projection P ( T, C ) of P on ( T, C ) , defined as of Definition 3. ( T, C ) is an  X  -junction tree for P ( V ) iff  X  ( i  X  j )  X  T : I V i tractable principled approximation P learning structure of such junction tree from data (samples from P ).  X  a part of problem input. The complexity of our approach is exp onential in k . Let us initially assume that we have an ora-cle I ( , | ) that can compute the mutual in-formation I ( A, B | C ) exactly for any disjoint subsets A, B, C  X  V . This is a very strict re-quirement, which we address in the next sec-tion. Using the oracle I , a na  X   X ve approach possible Q, S  X  V s.t. | S | = k and record all pairs ( S, Q ) with I ( Q, V -list L . We will say that a junction tree ( T, C ) is consistent with a list L iff for every separa-tor S n 3.1 Global independence assertions from local tests need to call the oracle exponentially many times ( 2 n  X  k  X  1 , once for every Q  X  V -can limit ourselves to computing mutual information over sm all subsets of variables: Lemma 4. Let P ( V ) be a k -JT  X  -representable distribution. Let S  X  V , A  X  V -s.t. | X | X  k + 1 , it holds that I ( A  X  X, V -We can thus compute an upper bound on I ( A, V -also bounds the quality of approximation of P by a projection on any junction tree ( T, C ) : Corollary 5. If conditions of Lemma 4 hold for P ( V ) with S = S S 3.2 Partitioning algorithm for weak conditional independe ncies Algorithm 3 : Efficient approach to struc-ture learning
Input : V , oracle I ( , | ) , treewidth k , for S  X  V s.t. | S | = k do return FindConsistentTreeDPGreedy( L ) is submodular : F algorithm with divide-and-conquer approach to partition V -complexity of their approach is still exponential in n , in general. 2 any  X  -junction tree that includes S as a separator. Notice also that separator, it follows that their container partitions Q LTCI merges Q Proposition 6. The time complexity of LTCI with | S | = k is O n where J MI ponents of V i Suppose ( T, C ) is an  X  -junction tree for P ( V ) , and Q S either Q  X  V i side of a separator. We will say that an algorithm is  X  -weak iff  X  Q  X  Q  X  3-5 in Alg. 1, satisfies the first requirement and a relaxed ver sion of the second: Lemma 7. LTCI, for q  X  k + 1 , is correct and n (  X  + ( k  X  1)  X  ) -weak. 3.3 Implementing FindConsistentTree using dynamic programming [13] for the same purpose. We briefly review the intuition; se e [2] for details. Consider a junction tree ( T, C ) . Let S reachable from C cliques from C i are each connected to C Formally, we require the following property: Definition 8. ( S, Q )  X  L is L -decomposable iff  X  D =  X  The set { ( S cover problem because of the requirement Q polynomial, we use a simple greedy approach: for every x  X  Q decomposition D , add ( S We call the resulting procedure FindConsistentTreeDPGreedy .
 Combining Alg. 2 and FindConsistentTreeDPGreedy , we arrive at Alg. 3. Overall complexity of Alg. 3 is dominated by Alg. 2 and is equal to O ( n 2 k +3 J MI guaranteed to find a junction tree. Intuitively, we require t hat for every ( S tree ( T, C ) , Alg. 2 adds all the components from decomposition of ( S tree are sufficiently strongly interdependent (have a certa in level of mutual information): for every separator S , clique C  X  C , min If we employ this oracle in our algorithms, the performance g uarantee becomes probabilistic: Alg. 3, called with  X  =  X  + X  and  X  I ( , , ) based on Thm. 11, using U  X  F ( k, R,  X  ,  X  close to one, a junction tree that approximates P arbitrarily well in time polynomial in 1 random variables with domain size R , for any P ( V ) , S, x it holds that I ( x, V -separator). Thus, we can restrict binary search to range  X   X  [0 , log R ] . value of  X  . It is possible, however, to find the optimal  X  while only checking min partitions Q and a hyper-edge connecting all variables from A whenever min dependent sets) may increase. More specifically, a graph Q S . For all S, A add a hyper-edge connecting all variables in A annotated with strength be remove hyperedge component Q  X  Q hyper-edge A which is contained in a connected component. However, as  X  increases, a component may become disconnected, because such an edge was not added. Therefore, we may have more Q [17] (denoted OBS). All experiments were run on a Pentium D 3. 4 GHz, with runtimes capped to 10 hours. The necessary entropies were cached in advance.
 ALARM . This discrete-valued data was sampled from a known Bayesia n network with treewidth 4. but worse than the OBS of [17] and Chow-Liu. The implementati on of OBS was the only one to a structure found by LPACJT for ALARM data. LPACJT only misse d 3 edges of the true model. TRAFFIC . This dataset contains traffic flow information measured eve ry 5 minutes in 8K loca-regularized algorithms, including LPACJT, give results of essentially the same quality. and effective structure learning heuristics. N000140710747. C. Guestrin was also supported in part by an A lfred P. Sloan Fellowship. We thank Nathan Srebro for helpful discussions, and Josep Rour e, Ajit Singh, CMU AUTON lab, Mark Teyssier, Daphne Koller, Percy Liang and Nathan Srebro for s haring their source code.
