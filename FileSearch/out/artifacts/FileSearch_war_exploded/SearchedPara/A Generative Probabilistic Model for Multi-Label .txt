
Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the sce-nario of multi-label classification, most of the classifiers simply assume the predefined classes are independently dis-tributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative proba-bilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a varia-tional procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes pa-rameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.
In the traditional definition of classification, classes are mutually exclusive:
However, in most of the real situation, data may asso-ciate with multi-classes simultaneously. For example, in the text classification task, a scientific article might be also concerning about the economy and in the scene categoriza-tion domain, an image may belong to the semantic concept beach and sunset together, yielding multiple labels [20]. In that case, a suitable definition for this kind of classifi-cation should be the multi-label classification, by modify-ing y in the original definition to be a subset of Y rather than a single one, and thus the optimal classifier should be H : x  X  y ,x  X  X, y  X  Y to optimize some specific eval-uation metric. We should note that, in most cases, there are intrinsic latent correlations between the classes. For exam-ple, a document concerning about politics is more likely to be also talking about the economy (positive correlation) but less likely talking about the pop stars (negative correlation). Unfortunately, most of the classification algorithms for the multi-labeling problem simply assume the classes are inde-pendently distributed, so that they failed to directly model the correlation between the classes.
 tion is to map the problem to a one-versus-the rest man-ner [15], which constructs a set of binary classifiers ob-tained by training on each possible class versus all the rest and assigns a real value for each class to indicate the class membership. But the deficiency of this simple mapping is obvious: the rough separation strategy ignores the correla-tion between the classes; moreover, the traditional discrim-inative classifiers make little attempt to uncover the proba-bilistic structure within both input and output spaces. it from different perspectives. Matthew et al. suggested several ways to utilize the multi-label samples for train-ing with binary classifiers and different strategies to predict the class membership [20]. Zhang adapted the traditional KNN lazy learning algorithm for multi-label data by utiliz-ing the statistical information gained from the unseen sam-ple X  X  neighborhood [25]. Schapire et al. advanced BoosT-exter [21], an extended AdaBoost algorithm, to address the multi-label text classification problem. In their work, they transformed the multi-labeling issue into a document-class pair ranking problem. BoosTexter employed various base classifiers to evaluate every document-class pair and ranked the separate predictions according to the weight settings. However, they noted that it was an open issue to control the model complexity to avoid over-fitting. McCallum pro-posed a Bayesian mixture model to select the most proba-ble set of classes from the power set of all the classes and used some heuristics to reduce the associated computational complexity [19]. The proposed model tried to capture the relationship between the classes and word occurrences, but it did not consider the correlation within the classes.
Nowadays, in the machine learning community, the gen-erative topic model is receiving more and more attentions. Latent Dirichlet Allocation ( LDA ) [11] is one of the most typical models. It reduces the complex process of produc-ing a document into a small number of simple probabilistic steps and thus specifies a probability distribution over all possible documents. Using standard statistical techniques, one can invert the process and infer the set of latent topics responsible for generating a given set of documents [22]. An important contribution of LDA is that, it explicitly mod-els the heterogeneity in the grouped data that exhibits mul-tiple latent patterns.

Recent work has employed LDA as a building block to address particular modeling problems. Fei-Fei Li advanced a hierarchical generative model to classify natural scene in an unsupervised manner [17]; Blei proposed an image-caption model to capture the correlation between image re-gions and caption words [9]; Griffiths modeled the docu-ments with both short-range syntactic and long-range se-mantic dependencies [14].

However, the LDA model failed to directly formulate the correlation between topics because of the dependence assumption implicit in the Dirichlet distribution on the topic proportions, which are nearly independent. Several other generative topic models have been recently proposed to capture the correlation between topics, such as Hier-archical Dirichlet Process Model (HDP) [23], Correlated Topic Model (CTM) [10] and Pachinko Allocation Model (PAM) [18].

The advantages of the generative topic model are obvi-ous: 1) it would be easy to postulate complex latent struc-tures responsible for a set of observations; 2) the correlation between the different factors could be easily exploited by introducing the latent variables.

In this paper, to capture the correlations within differ-ent classes and words in the multi-label classification, we propose a hierachical generative probabilistic model to for-mulate the generation of the multi-labeled documents. We model these documents as a finite mixture over the classes and words: different classes exhibit different proportions of latent topics, which are represented by distributions of words over a fixed vocabulary, and the observed words are governed by the latent topic factors accordingly. By this model, we would be able to model the correlations within the classes and words simultaneously.
 guage of text classification in the following expatiation is just for intuitive understanding and interpretation about the notions. It is important to note that the proposed model is not narrowly restricted to the text classification field: it could be feasibly applied to any multi-label classification or annotation problem such as scene categorization in image processing and gene function annotation in bioinformatics. descriptions for the proposed model are presented and we will discuss the inference and parameter estimation proce-dures for the proposed model in Section 3; in Section 4, extensive experiments are performed in different perspec-tives to validate the model; we would conclude the work in this paper and demonstrate our contributions in Section 5. Model) to address the multi-label classification issue. The graphical representation of the CoL model is depicted in Figure 1. Following the standard graphical model formal-ism [12], nodes represent the random variables, edges in-dicate the possible dependence and boxes with number N means the unit in this box is repeated N times. Shaded nodes are observed random variables, unshaded nodes are latent random variables. The joint distribution can be ob-tained from the graph by taking the product of the condi-tional distribution of nodes given their parents, see Eq(2). process that, to generate a document, we should first select a set of classes (e.g. themes of a document), then select different topics under the classes (e.g. aspects about the themes), and finally employ specific words to build up the contents of the document. C classes and V words, and a given document consists of M classes and N words. To simplify the model, we have assumed the topic size k is known and fixed on the whole corpus. In the given document d , we denote  X  as the document-specific distribution of classes;  X  as the distribu-tion of topics under each class; z = { z 1 ,z 2 ,z 3 ,...,z N } as the particular discrete topic assignment for each word and y = { y 1 ,y 2 ,y 3 ,...,y N } as an indexing variable to indi-cate which class generates the corresponding topic. These are the latent variables. c is a C -dimensional vector with c i =1 to imply document d is associating with class i and w = { w 1 ,w 2 ,w 3 ,...,w N } are the observed words in d . Besides,  X  and  X  are the mean and covariance parame-ters of a multivariate Normal distribution to formulate the class distribution;  X  are Ck -dimensional Dirichlet param-eters to characterize the topic prior distribution under each class; and  X  are kV -dimensional Multinomial parameters to represent the word distribution under topics. These are the model parameters.
 Conditioned on the model parameters (  X ,  X  , X , X  ) ,the CoL model assumes the following generative process of the classes and words in the document: 1. Sample  X  from the Normal distribution:  X   X  N (  X ,  X ) 2. For each class c m ,m  X  X  1,2,3,...,M } : 3. For each word w n ,n  X  X  1,2,3,...,N } : where l (  X  ) maps the natural parameter of the class propor-tions to the mean parameter by logistic Normal [6]:
Note that, the CoL model employs a multivariate Nor-mal distribution N(  X ,  X  ) to capture the correlation between the classes: for each document, it draws a real valued ran-dom vector from N(  X ,  X  ) and then maps it to a C -1 dimen-sional simplex to obtain a Multinomial parameter for the document-specific distribution of classes. The mapping is implemented by the logistic Normal l (  X  ) , see Eq(1). The covariance matrix  X  induces the dependencies between the components, allowing for a general pattern of variability be-tween its components. Following the general settings of LDA model, we assume the topic proportion  X  is drawn from the Dirichlet distribution and each topic is represented by a Multinomial distribution of words on a fixed vocab-ulary. Furthermore, we assume such proportion varies be-tween different classes observed in the documents. Besides, since the relationship between the classes and topics is un-derlying, we use the indexing variable y to indicate the la-tent structure between them.

The joint probability on the words, classes and the latent variables in one document is thus given by: p (  X ,  X , y , z , c , w |  X ,  X  , X , X  )= (2)
From the notion behind the proposed model, we can find obvious distinction between the proposed CoL model and the LDA model: the CoL model is supervised while the LDA model is unsupervised. CoL model aims to capture the in-formation conveyed in the class membership, to exploit the in-depth relation between the classes and words, and to pre-dict the potential classes in an unseen document. The LDA model is not capable to directly formulate such class mem-bership, so that some other regression or classification tech-niques have to be employed to perform the prediction [11]. Besides, the LDA model assumes the proportion of topics is identical in the whole corpus, while in the CoL model the mixture is depending on the classes which the document belongs to. In this sense, the CoL model can overcome the deficiency in the LDA model stems from the strong inde-pendence assumptions.

An intuitive interpretation for the proposed CoL model is illustrated in Figure 2. In the traditional approach for the multi-label classification (the left panel in Figure 2), the em-ployed classifiers simply assume the predefined classes are independent between each other. When one class conveys information about another, the traditional classifiers would fail to capture this. Furthermore, those classification algo-rithms assume all the words are independent when given the observed classes, thus it would ignore to model the latent patterns among the different words under particular classes either.
 On the contrary, the CoL model (the right panel in Figure 2) formulates the relationship between words and classes within a more throughout consideration: in each document, the classes are drawn from a correlated prior distribution, in our case the multivariate Normal distribu-tion with a non-diagonal covariance, each class exhibits
Figure 2. Comparison between the traditional multi-labeling approach and the CoL model.

In the above representation, c denotes the class labels associating with the document, w denotes the observed words and t in the right panel denotes the latent topic factors. different proportion of the topics, and different topics gov-ern dissimilar word occurrences, embedding the correlation among different words.

In the above intuitive representation of the CoL model, it is obvious that the correlation between the classes and words is not modeled as an one-to-one mapping, but in a more general manner: via the latent topic factors, words are treated as finite mixtures under a set of classes, so that they are not restricted to any particular classes and multiple words could contribute to the same class. Efficient dimen-sional decomposition could be explicitly implemented: V -dimensional word space is mapped into the k -dimensional topic space, in which it will be easier to reveal the latent correlations between the classes and the variant word distri-butions.
In order to utilize the CoL model, the key inferential problem is to compute the posterior distribution of the classes in a given document, that is: p ( c , X , X , y , z | w , X ,  X  , X , X  )=
Unfortunately, this posterior distribution is intractable: the couples between  X  and  X  ,  X  and  X  induce a combi-natorial number of terms and make it impossible to effi-ciently get the exact inference result. Different from the LDA model, where the conjugacy between the Dirichlet and Multinomial distribution provides nice computational con-venience; in the CoL model, a non-conjugate Normal dis-tribution is employed to capture the correlations between the classes, which does not enjoy the same convenience. Thus we cannot analytically compute the integrals of each term. And the non-conjugacy further precludes most of the Markov chain Monte Carlo (MCMC) [7] sampling tech-niques, especially for the Gibbs Sampling, which makes use of the conjugacy to compute the analytical coordinate-wise posteriors. In this case, we develop a variational proce-dure [8] (in particular, the mean filed approximation) to ap-proximate the desired posterior distribution, which provides nice computational convenience and intuitive interpretation about the middle results.

In particular, we define the following fully factorized dis-tribution on the latent factors: q (  X ,  X , y , z |  X ,  X ,  X ,  X ,  X  )= (3)
In the above variational distribution, the document-specific class distribution  X  is governed by a C dimensional multivariate Normal distribution N (  X ,  X  ) . Since the vari-ational parameters are fit within a single document, there is no advantage to introduce a non-diagonal covariance. The variational topic distribution  X  is specified by Mk -dimensional Dirichlet parameters  X  , the class-topic indica-tor y is conditioned on NM -dimensional Multinomial pa-rameters  X  and the discrete topic assignment z is controlled by Nk -dimensional Multinomial parameters  X  .

The meaning of this variational distribution is obvious: we release the dependence among the latent variables by assuming they are independently drawn from the respective distribution. Thus the aim of the variational inference is to find the optimal variational parameters which would maxi-mize the likelihood on the given documents.

By Jensen X  X  inequality, we could estimate the lower bound of the log likelihood as follows (we omit the param-eters for simplicity): log p ( c , w ) = log  X  = E q [log p (  X ,  X , y , z , c , w )]  X  E q [log q (  X ,  X , y , z )]
It is easy to verify that the difference between two sides of the above inequality is the Kullback-Leibler di-vergence between the variational posterior probability and the true posterior probability. We denote the right side of the above inequality as L (  X ,  X ,  X ,  X ,  X  ;  X ,  X  , X , X  ) to repre-sent the lower bound of log likelihood. Thus, to maximize L (  X ,  X ,  X ,  X ,  X  ;  X ,  X  , X , X  ) is equivalent to minimize the KL divergence between the variational posterior probability and the true posterior probability.

Following the general recipe for variational approxima-tion, we take derivatives of the expectation likelihood func-tion L (  X ,  X ,  X ,  X ,  X  ;  X ,  X  , X , X  ) with respect to the varia-tional parameters and obtain the following iterative varia-tional parameter estimation equations: 1. Dirichlet parameter  X  : 2. Multinomial parameter  X  : 3. Multinomial parameter  X  : 4. Optimize the Normal parameter  X  and  X  2 by the
These estimations have appealing intuitive interpreta-tions. Because the Multinomial distribution is conjugated with the Dirichlet distribution , estimations (4)  X  (6) are the posterior updating given the expected observations (suffi-cient statistics) taken under the variational distribution. But the non-conjugacy between the Multinomial and Normal distribution prevents us to analytically get the update equa-tions, therefore we employ the Conjugate Gradient algo-rithm to find the optimal parameters in (7) and (8).
The only problem left for the inference procedure is that, when we are in the testing phase, we could not know ex-actly which classes are assigned to the given document in advance. Without the specific classes, we are not able to tell where the words and topics come from. To solve this problem, we appeal to the maximum a posteriori ( MAP )cri-terion to retrieval the most probable classes associating with the given document: where c i is the subset from the power set of all the possible classes.

Unfortunately, it is unfeasible when the number of classes is large. To simplify the computation complexity, we simply estimate the posterior probability of every single class in the given document and use a pre-estimated thresh-old to retrieve the most probable ones.
Following the same procedure in the variational infer-ence, in this section, we utilize an empirical Bayesian method to estimate the parameters of the CoL model. To maximize the likelihood on the training data, we look for the optimal parameters to tighten the lower bound of L (  X ,  X ,  X ,  X  ;  X ,  X  , X , X  ) estimated by the variational infer-ence. By taking derivatives of L (  X ,  X ,  X ,  X  ;  X ,  X  , X , X  ) with respect to the model parameters (  X ,  X  , X , X  ), we obtain the following update equations: 1. Update the mean parameter  X  and covariance matrix 2. Update the Dirichlet parameter  X  by the Newton-3. Update the Multinomial parameter  X  :
These update equations correspond to find the maximum likelihood estimation with the expected sufficient statistics for each document taken under the variational posterior. We employed an alternating EM procedure to find the optimal parameters as follows: 1. (E-Step) For each document in the training corpus, op-2. (M-Step) Maximizing the resulting lower bound on the
We collect two different types of data with multi-label annotations from: scientific publications and news reports to evaluate the capability of the proposed model in man-aging various applications. The macro-precision , macro-recall and macro-fscore [13] are employed to evaluate the performance in average.
Biological literature. In the molecular biology domain, biologists would employ various experiment methods to confirm their findings; and a single document may con-tain multiple methods simultaneously. It is important to annotate these experiment methods since each method has an implicit degree of reliability. We collect 5319 full text documents from the public biological database PubMed [5] with method annotations from another public annotation databases MINT [3] and IntAct [1]. One thing we should emphasize is that, this collection is heavily unbalanced: the whole corpus consists of 115 unique methods, and each document is labeled with 1.99 different methods in aver-age; unfortunately, there are 5 dominate methods taking up nearly 59.3% occurrences and 86.1% (99 out of 115) of the methods are observed in less than 10% training data.
Reuters-21578. Documents in this collection are col-lected from the Reuters financial newswire service in 1987 [16]. It is a well-studied benchmark corpus for many text classification algorithms. There are 90 classes and 10,788 documents in the original corpus and the collection is pre-partitioned into a training set of 7769 documents and a testing set of 3019 documents. To get a more balanced data set, we remove the minor classes with less than 50 doc-uments and build up a collection consisting of 36 classes and 10449 documents with 7543 training documents and 2906 testing documents. In this collection, each document is associated with 1.3 classes in average and about 13.9% documents contain multiple labels.

These two data sets are quite different from each other and represent the typical sources in the real text processing task. We perform simple pre-processions on each data set: 1) remove a standard list of 400 stop words, punctuations, and the terms occur less than 50 times; 2) stems the words to original form.
We first use the perplexity as the criterion to evaluate the effect of the number of topic factors, which is the only arbitrary parameter in the CoL model. The perplexity on a set of testing samples is calculated as follows: perplexity =exp
Eq(15) is equivalent algebraically to the inverse of the geometric mean per-class likelihood and the better gener-alization capability is indicated by a lower perplexity over the held-out testing samples. We evaluate the perplexity on both data sets respectively. In the Bio-Literature data set, we held out 20% of collection for the test purpose and used the remaining 80% to train the model, in accordance with 5-fold cross-validation.
Figure 3. Class perplexity on the number of topics. The left panel illustrates the perplex-ity on the Bio-Literature data set and the right panel illustrates the perplexity on the Reuters data set.

Figure 3 demonstrates that the generalization power of the CoL model gets improved with more topic factors. Since with more topic factors the documents could be partitioned into finer segments, more precise correlations between the classes and words could be captured. But as the number of topics exceeds a limit, the model becomes too specific (higher perplexity). Therefore we could conclude that the topic factors could be treated as the discriminate granular-ity of the model and it operates as a tradeoff between the generality and specificity. Besides, as the number of topic factors increase, there will be more parameters to be esti-mated (linearly increase with the number of topics), so that more training data is needed to obtain the reliable parame-ters. In this sense, when the number of topic factors exceeds a limit, the quality of the estimated parameters decreases and hampers the prediction power.

Besides understanding the impact of the number of topic factors on the generalization capability, we would be more interested in their explicit effect on the classification perfor-mance. Here, we evaluate the precision and recall perfor-mances under different number of topic factors on the two data sets. We use the same corpus partition as in Figure 3.
Figure 4. Classification performance on the number of topics.

Figure 4 demonstrates that, both the precision and recall performances get improved as the number of topic factors increase. We could discover that the classification perfor-mance peaks close to the place where the perplexity reaches the minimum. And from the results on the Reuters data set (since the Bio-Literature data set is unbalanced), we can see that with a smaller number of topics the model behaves with nice recall performance; while with more topics, the preci-sion performance improves fast. This is consistent with the foregoing perplexity result.
We employ Na  X   X ve Bayes , KNN and SVM models as the baseline methods to evaluate the capability of the CoL model. We choose Na  X   X ve Bayes because it is the sim-plest generative model with complete independence as-sumptions, and KNN model could exploit the correlation between classes among similar documents. These are the two basic notions in the CoL model. Besides, SVM model is one of the most powerful discriminative model for classi-fication task [15]. All the baseline models are operating on the same feature set as the CoL model employs.

In Na  X   X ve Bayes model, we estimate the posterior prob-ability of the classes in a given document by Eq(16). We use a pre-estimated threshold to retrieval the most probable classes.
In KNN model, we make the prediction by ranking the candidate classes in the union of the unlabeled sample X  X  k -nearest labeled neighbors, and weight the candidate labels by the similarity between the desired unlabeled sample and its neighbors. This strategy is similar with the ML-kNN pro-posed by Zhang in [25].

In SVM model, we follow Boutell X  X  strategy [20] to train a set of binary classifiers for each class and predict the unknown classes by the classifiers X  output. We use SV M light [24] toolkit to implement a linear kernel SVM model with the default parameters.

We first perform the comparison on different proportions of data used for training on the Bio-Literature data set. In this comparison, we set the size of topics in the CoL model to be 250 and k in KNN model to be 37.
Figure 5. Comparisons with the baseline models on the Bio-Literature data set.

We could discover from the above results that, as the training set increases, the performance of the CoL model improves rapidly. The reason for this phenomenon is that in the CoL model, there are C ( C +1)+ k ( C + V ) parameters to be estimated, when the training set is not large enough, most of the parameters cannot be fully estimated, and it di-rectly hinders the capability of the model.

One thing we should note is that, since the Bio-Literature data set is unbalanced, we should attend the performance on the minor classes as well. In the class-level evaluation, the baseline models only retrieve most of the major classes (e.g. the top 5 methods) but ignoring the other minor ones, while the CoL model exhibits superior retrieve power. We demon-strate the coverage performance of each model on the same settings as in Figure 5 to compare their retrieve capability. ter retrieval capability than all the baseline methods when the training set is large enough.
 Reuters data set and compare the result with two reported approaches on this data set [21, 19]. This time, we set the size of topics in the CoL model to be 300 and k in the KNN model to be 37.
 demonstrated in Table 1. We can see that the CoL model achieved the best F-Score performance and both its preci-sion and recall performances are promising.
 McCallum X  X  EM [21] 0.839 --largest classes and reported precision performance of 0.839. Schapire classified the original data set with all the classes and reported F-Score performance of 0.851. To compare with their achieved performances, we run the CoL model on the same training and testing data set as they did respec-tively. As a result, the CoL model achieves competitive per-formances, illustrated in Table 2.

From the detailed comparisons on these two data sets, we can discover the proposed CoL model possesses nice preci-sion and comparative recall performance. We contribute the improvement to the information exploited from the correla-tion between different classes: the model captures the rela-tionship between the classes from the training set and filters out the false positive combinations in the testing phrase.
With the CoL model, we formulate the correlation be-tween different classes via the latent topic factors, which enable us to analyze the relationship between the classes in the latent space. Meanwhile, in the biological domain, there is a well-defined language describing the relationship among the biological concepts, named ontology and orga-nized in a directed acyclic graph (DAG) . The Molecular In-teraction ( MI ) ontology [4] is such a concept hierarchy in the molecular interaction domain, which includes the terms describing the molecular interaction types and the experi-ment detection methods.

To represent the given detection methods in the latent topic space, we average the variational posterior Dirichlet parameters over all documents associating with method i : where Z is a normalization factor to normalize the varia-tional parameter  X  in each document, D is the document set associating with category c i . Recall that, the variational parameter  X  i is approximate to the posterior topic distribu-tion under category c i in the given document. By averaging it over all the relevant documents, we can approximate the posterior distribution of classes over the latent topic factors.
Based on this approximate representation, we employ the accumulative clustering algorithm to perform hierarchi-cal clustering and utilize a visualization tool gCluto [2] to demonstrate the captured  X  X edigree X  tree. (We only illus-trate part of the clustering result because of the page limit.)
From the clustering result in Figure 7, we can discover that most of the sibling nodes defined in the MI ontology are successfully clustered with the correct hierarchy and high confidence (red circles mean the correct clusters). The promising result confirms that the correlation between dif-ferent classes exploited by the CoL model is reasonable and the model does capture the in-depth semantic relations.
It would be interesting to investigate the words poste-rior distribution under the given classes. Especially in the biological domain, particular terms and phrases convey cru-cial domain dependent information. To mine relevant words within a given class from the corpus, we utilize a class-specific distribution over words by the conditional distribu-tion p ( w | c ) to retrieval the most relevant terms under each desired class by the following evaluation function: where D is the document set associating with the desired class c .
 We collect top 15 terms for 5 different methods from the Bio-Literature data set in Table 3 and top 15 terms for 5 dif-ferent classes from the Reuters data set in Table 4. We can see from Table 3, most of the terms are appropriately gath-ered with the given classes. For example  X  X rystal X  ,  X  X e-lix X  ,  X  X tructure X  are gathered to x-ray , and  X  X wo-hybrid X  ,  X  X east X  ,  X  X ite X  are gathered to two hybrid , which are the informative phrases in the MI ontology definition of these methods. And in Table 4, terms are also properly clustered to the desired classes. For example,  X  X asoline X  ,  X  X il X  ,  X  X n-ergy X  are gathered to gas , and  X  X urplus X  ,  X  X eficit X  ,  X  X ur-rency X  are gathered to bop (balance of payments). The rea-sonable word distribution under classes confirms that the CoL model captures the proper correlation not only between the different classes but also between classes and words. model, the CoL model, to formulate the correlation between the different classes, and exploit the in-depth semantic re-lationship within the classes and word occurrences. By applying the model on various data sets, we achieved en-couraging results comparing to the traditional classification algorithms. The experiment results confirm that it is nec-essary to model the correlation among different classes in the multi-label classification issue, and the proposed model properly modeled the latent correlations which benefit the classification performance.
 LDA model is that, the CoL model performs supervised learning while the LDA model is unsupervised. In this sense, the CoL model is capable to capture the information conveyed by the class labels while the LDA model fails to do so. Besides, because the CoL model assumes the topic proportion is governed by the classes the document belongs to, it can overcome the deficiency in the LDA model stem-ming from the strong independence assumption introduced by the Dirichlet distribution.
 restricted to the text classification task; instead, it could be feasibly applied to various application areas such as scene categorization, opinion mining and gene function annota-tion.
 ing the correlation among classes for the multi-label clas-sification problem, which is ignored by most of previ-ous approaches; 2) proposing a generative probabilistic model with proper underlying probabilistic semantics for the multi-labeling issue, which can be feasibly adopted to various applications. Foundation under grant No. 60572084 and 60621062, as well as Tsinghua Basic Research Foundation under grant No. 052220205 and No. 053220002.

