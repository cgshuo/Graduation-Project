 Music information retrieval (MIR) holds great promise as a technology for managing large music archives. One of the key components of MIR that has been actively researched into is music tagging. While significant progress has been achieved, most of the existing systems still adopt a simple classification approach, and apply machine learning classi-fiers directly on low level acoustic features. Consequently, they suffer the shortcomings of (1) poor accuracy, (2) lack of comprehensive evaluation results and the associated analy-sis based on large scale datasets, and (3) incomplete content representation, arising from the lack of multimodal and tem-poral information integration.

In this paper, we introduce a novel system called MMTag-ger that effectively integrates both multimodal and tempo-ral information in the representation of music signal. The carefully designed multilayer architecture of the proposed classification framework seamlessly combines Multiple Gaus-sian Mixture Models (GMMs) and Support Vector Machine (SVM) into a single framework. The structure preserves more discriminative information, leading to more accurate and robust tagging. Experiment results obtained with two large music collections highlight the various advantages of our multilayer framework over state of the art techniques. H.3.3 [ Information Search and Retrieval ]: Retrieval, Search process; I.2.m [ Computing Methodologies ]: Arti-ficial Intelligence; H.5.5 [ Sound and Music Computing ]: Systems Algorithms, Design, Experimentation, Human Factors Music Information Retrieval, Tagging, Browsing, Search
Music is an unique art form created by human to repre-sent emotion, cultural background, social context and time. To facilitate music information retrieval (MIR) from large music collections, it is necessary to annotate the music doc-uments with comprehensive textual information [2, 16, 24]. Existing music tagging approaches generally perform musi-cal feature extraction, followed by applying machine learning methods to model the relationship between text labels and music features. The approaches hinge on two interrelated issues: (1) the extraction of high quality acoustic features to represent multiple music characteristics, and (2) the ju-dicious application of statistical model(s) for classification and tagging. The effectiveness of the approaches should be demonstrated through a proper evaluation process involving large test collections and appropriate benchmarking metrics.
There has been a long history of using low level acoustic features extracted from audio objects as content descrip-tors [13, 3, 10, 9]. Unfortunately, how to effectively de-rive high-level semantic concepts (such as genre and mood) from the physical features still remains an extremely diffi-cult problem. There are several reasons for this. First, there is a gulf between high level concepts and low level acoustic characteristics, as evident by the mismatch in semantic sim-ilarity in the search results produced by systems that rely solely on low level features [5]. Second, the content of music is rich and complex, spanning a wide range of features like timbral texture, harmony, rhythm structure and pitch [21, 27, 12, 19, 20, 30]. It is thus imperative to employ a content representation that captures these features comprehensively, and to determine which features to use for what purpose. In view of the challenges, it is not surprising that existing mu-sic tagging systems that adopt a simple approach of apply-ing machine learning classifiers directly on low level acoustic features do not deliver good performance.

In this work, we propose a framework called MMTagger (Multifeature based Music Tagger) that combines advanced feature extraction techniques and high level semantic con-cept modeling for effective annotation of music documents. The basic idea for the proposed scheme is to model music information (text based description) with hierarchical struc-ture and relationship between tags and concepts. It tries to map sound documents to a representation in the so-called latent musical concept space , where relevance between docu-ments and tags can be more accurately modeled than in the acoustic feature space. The MMTagger X  X  architecture com-prises three interconnected functionality layers. The techni-cal design of the first layer aims at not only providing high quality feature combination but also to incorporate tempo-ral information. The latter is motivated by the observation that music documents belonging to the same category gener-ally share certain temporal patterns. The second layer of the proposed system is for discriminative musical concept mod-eling, and is intended to bridge the  X  X emantic gap X  between the low level music features in the first layer, and the music tags in the third layer. Here, we utilize multiple Gaussian Mixture Models (GMMs) to represent different concepts [15, 7]. Since a semantic concept could be relevant to many dif-ferent keywords, the third layer contains multiple support vector machines (SVM), each trained to derive the likelihood score of a tag from its association strength with the various music concepts. We have conducted a comprehensive ex-periment study with two large test collections. The results indicate that our solution achieves substantial performance improvement in accuracy and robustness in annotating mu-sic documents.

The rest of the article is structured as below: Section 2 gives a brief overview of related work in the area of music tagging, including their assumptions and limitations. In Sec-tion 3, we provide details on our proposed architecture and introduce the structure of each system component module and its learning algorithms. Section 4 reports on our ex-periment configuration while Sections 5 presents empirical evaluation results. Finally, our conclusions and directions for future research are summarized in Section 6.
Automated music tagging is an important research prob-lem with numerous applications such as music search and music recommendation. This area has received considerable attention and many related techniques have been developed in recent years. Among the earliest of such systems, Whit-man and Rifkin [29, 28] proposed a novel Regularized Least-Squares Classification (RLSC) based approach. The goal is to derive non-linear relationship between text captions and acoustic features in an efficient way. For performance evalu-ation, 255 songs from 51 perform ers are separated into train-ing and testing sets with roughly equal size. Using the SVM classifier, accuracy achieved ranges from 0.0% to 38.9% de-pending on the terms used for evaluation process. In [23], Turnbull et al. applied a supervised multiclass na  X   X ve Bayes model to estimate relationship between musical sound and words. The features considered by this system can be classi-fied into two categories -textual features and audio features (e.g., dMFCC and auditory filter-bank temporal envelope features). The test collection contains totally 2,131 songs and their song reviews. Using dMFCC feature, precision and recall rates achieved by the system is 0.072 and 0.119 for the annotation task. To facilitate effective music retrieval with semantic description, Turnbull et al. developed a mu-sic labeling scheme based on the supervised multi-class la-beling model (SML) [26, 25] 1 . In this approach, sound doc-uments are modeled as a GMM distribution over a set of predefined terms (corpus). The distance between the multi-nomial distributions of keyword query and a music feature can be estimated with the Kullback-Leibler (KL) divergence In this paper, we use MSML to denote this system. for the purpose of ranking search results. The acoustic fea-ture considered in this system is MFCC. Using the CAL500 dataset, they achieved a nice performance improvement in retrieval and annotation accuracy. More recently, Duan et al. designed an interesting approach for collective annota-tion of music data [6]. It assumes that there are certain levels of correlation between different tags. Studying the re-lationship is useful for improving annotation performance. They employed two different statistical models -GMMs and Conditional Random Field to exploit the label correlation. Experiment results demonstra te a small but consistent per-formance gain. In addition, Bertin-Mahieux et al. pro-posed Autotagger system using advanced ensemble learning schemes to combine discriminative power of different clas-sifiers [8, 4]. Those schemes include AdaBoost and Filter-Boost. Acoustic features considered by the scheme include 20 MFCC Coefficients, 176 autocorrelation coefficients, and 85 spectrogram coefficients. Experiment results based on the CAL500 dataset and another large test collection demon-strate that Autotagger performs better than MSML. It is currently the most advanced technique for music tagging.
This section presents a novel scheme to facilitate effec-tive automated tagging over large music collections. As il-lustrated in Figure 1, the architecture of our system con-sists of three functionality layers: music preprocessing layer (MPL) for music sequence segmentation and acoustic feature extraction, discriminative concept modeling layer (DCML) and SVM based tag refinement layer (TRL). Similar to an Artificial Neural Network [7], each layer is fully connected with each other. The first layer MPL aims to extract four different features including timb ral feature, spectral feature, rhythm feature and melody feature. Using those features, a set of statistical models based on GMMs are constructed to characterize high level musical concepts in database, one GMMs per concept (e.g., genre, singer, mood). Those con-cepts can be treated as the most fundamental component of latent musical concept space . The output of this layer is a set of likelihood scores, which serve as input to TRL. The TRL contains a collection of SVM based tag classifier, each is trained to generate relevance score for a tag. Based on the relevance scores, we can finally rank each tag and pick the top N tags to annotate the input music. The follow-ing sections elaborate on each of the layers and give a full description of the associated algorithms.
The function of this layer is to preprocess raw audio sig-nal and compute music features. The related process com-prises two steps: music segmentation and feature extraction. When an audio signal is received, it is first partitioned into several short fixed length time-frames. For this study, we set the length of each frame to be 0.5 second. Distinguished from previous tagging schemes, we introduce a temporal descriptor in the music content representation. Its main advantage is better content description capability through combining both acoustic information and temporal informa-tion. For a segment with starting time ts s and end time te the corresponding temporal musical descriptor is defined as, where td f ( m i ,ms s ) denotes the feature f calculated from the segment ms s =( ts s ,te s ) of music file m i and extra an extraction function for feature f . Each music document is treated as a composite of different kinds of acoustic fea-ture vectors with temporal information. The motivation de-rives from the observation that discriminative characteristics are often embodied within local temporal acoustic features. Thus the proposed temporal based feature enjoys greatest potential to provide more comp rehensive summarization for the purpose of classification. The MMTagger system con-siders four different kinds of music features:
Accordingly, the final content representation includes four different groups of musical features (local content informa-tion) and time information (temporal information). Total dimensionality of the feature set considered in this study is 100.
For the second layer of the proposed MMTagger system, multiple GMMs are trained to statistically model the rela-tionship between each high level concept and various acous-tic features. Those high level concepts constitute the latent musical concept space . Eachhighlevelmusicconceptcor-responds to one GMMs. GMMs is among the most widely applied statistical analysis methods due to its flexibility of representing different kinds of distributions. However, gain-ing an accurate estimation of the distribution of the music features associated with to a high level concept goes beyond a straightforward application of the GMM method. While the distance between two music concepts can be estimated via the KL divergence between their GMMs, accurate result cannot be expected in general. There are two main reasons:
To solve those problems, we develop a two-step adaptation approach to construct the GMMs based on adaptive learn-ing [1]. It includes generative adaptation and discriminative music concept adaptation. Generative adaptation has been widely explored in many tasks such as speaker identification and image categorization [11, 18]. It tries to use all learning samples in the training process and then obtain the Uni-versal Background Model (UBM), which is the GMMs op-timized by the principle of Maximum a Posteriori (MAP). In the second step, discriminative music concept adaptation is designed to adjust the mean vectors of each GMMs to achieve the targets of 1) keeping the music documents be-longing the same semantic concept closer, and 2) separating out those with different labels. In this way, the obtained GMMs exhibit better classification capability. The UBM obtained in the initial phase of training the GMMs is denoted as, where w k ,  X  k and  X  k are the weight, mean and covariance matrix of the k th Gaussian component, respectively. x is input feature vector. K is the total number of Gaussian components and the probabilistic density is calculated as a weighted combination of K Gaussian densities,
The parameters of UBM are estimated using the tradi-tional EM algorithm. In the E-step, the posterior probabil-ity is calculated via where n k = vectors via
When EM iteration stops, the resulting GMMs is the Uni-versal Background Model (UBM).
The purpose of discriminative concept adaptation is to es-timate the GMMs parameter belonging to a certain music concept from a UBM. After it, a series of GMMs { G 1 , ..., G = { P 1 ( x |  X  1 ) ,P 2 ( x |  X  2 ) , ..., P C ( x |  X  C ) each GMMs approximating distribution over the audio fea-ture space of a high level concept. A special transformation matrix M on the mean vectors of the GMMs is designed to enhance classification capability further. Since each high level music concept is represented by a GMMs, the distance between two concepts can be measured using the KL di-vergence between their GMMs. However, since the KL di-vergence of GMMs is not analytically tractable, we use the upper bound of the divergence for calculating distance. It can be proved that where  X  a k and  X  b k respectively denote the mean of the k th component from music concept a and b . The current model cannot yet achieve optimal classification performance be-cause inter-class and intra-class distances are not taken into account. To improve its effectiveness, Neighborhood Com-ponent Analysis (NCA) is performed to derive transforma-tion matrix M and apply it on D ( G a || G b ). NCA is a learn-ing method, which constructs a distance metric optimizing leave-one-out (LOO) performance based on training data. An infinitesimal change in A may change the neighbor graph and thus lift LOO discrimination power by a finite amount. NCA adopts a more well behaved measure of nearest neigh-bor performance by introducing a differentiable cost function based on stochastic neighbor assignment in the transformed space. Each point i in multidimensional feature space selects another point j as its neighbor with certain probability p and inherits its class label from the selected point. p ij defined as where D ij = || Mx i  X  Mx j || 2 . The objective is to maximize the expected number of samples correctly classified. Thus, we have, where C i denotes the set of samples in the same class as i -th sample. After carrying out diffe rentiation with respect to the transformation matrix M , we can obtain, where
The optimization problem above can be easily solved with a gradient descent process. After the training process, the GMMs for each high level music concept estimates the likeli-hood score  X  c and this score is used to quantify the distance between raw feature input and concept label. At the same time, the output of DCML is a vector  X  that models prob-abilistic relationship between different music concepts and audio input, where  X  =[  X  1 , X  2 , ....,  X  C ]. It serves as input for tag refinement layer, the last layer in the MMTagger framework. The SVM based computational nodes constitute the Tag Refinement Layer (TRL) of the MMTagger framework. Each of them is designed to estimate the probability of a partic-ular tag based on input from DCML. In the current imple-mentation of MMTagger system, SVM is selected as a tag classifier due to its effectiveness [22].

Since traditional SVM can be used only for binary classi-fication, the method proposed by Hastie and Tibshirani [17] is employed to derive numeric value for the probability that an unknown sample belongs to a certain tag. Its key idea is to adopt Gaussian distribution to model tag-conditional densities p t (  X  | y =1)and p t (  X  | y =  X  1). Using Bayes X  rule, the relevance score (posterior probability) r t for each given tag t can be computed via: where  X  =[  X  1 , X  2 , ....,  X  C ] is a vector generated by DCML. It contains a set of likelihood values describing the probability that an input music belongs to music concept c ,where c = 1 , ..., C . Using equation 11, a set of relevance scores r = [ r ,r 2 , ..., r T ] are obtained for ranking tags. Eventually the top k tags are selected as annotation for input music, with value of k being predefined by the user.
This section introduces the experiment configuration for our performance evaluation. We report details on two test datasets, performance metrics, competitors and evaluation methodology. All tagging methods evaluated in this study have been fully implemented and tested on a Pentium (R) D, 3.20GHz, 1.98 GB RAM PC running the Windows XP operating system.
Test collections play a very important role in empirical study. Two test collections are used in this evaluation. The first one (TS1) is the Computer Audition Lab 500-Song (CAL 500) data set developed by the CAL group [25, 26]. This collection contains 500 modern western music docu-ments performed by 500 different artists. Altogether, there are 174 tags categorized into six different semantic groups including instrumentation, vocal characteristics, genre, emo-tions, solo and usage terms. For this dataset, we use those six groups as high level musical concepts to train our statis-tical model.

Since the size of the CAL500 data set is relatively small, we developed the second test collection called TS2. It con-tains 4000 popular music items downloaded from Youtube. They are performed by 110 different singers including 55 fe-males and 55 males. The music documents are converted to 22050Hz, 16-bit, mono audio files. 12 amateur musicians, who are familiar with various music taxonomy and concepts, were hired to create ground truth about this collection. The ground truth information was generated by attaching a tag to a music item if at least three people assigned the tag to the song. In the case that an agreement on tag assignment between different respondents can not be reached, a simi-lar resolution used in generating CAL500 is applied. At the end of the process, we obtain totally 250 tags, belonging to 8 different categories. They are instrumentation, emotions, country, time, genre, vocal characteristics, solo and usage terms. Consequently, our evaluation with TS2, involves 8 high level concepts. The size of the vocabulary | V | is 174.
Textual information generated by music tagging systems can be used for many different MIR applications. To validate the performance of different tagging schemes, we select two MIR tasks:
Here, three different evaluation metrics are used for music annotation. They are mean per-tag precision and recall, and the F-score. Based on the methodology used by Turnbull et al. [26], the top 10 tags generated by the models are used for comparison and thus annotation length A is 10. Per-tag recall and per-tag precision is formally defined as where | t GT | is the number of songs annotated with the tags in the human-generated  X  X round truth X  annotation and t TP is the number of songs annotated correctly with the tags. Based on per-tag precision and per-tag recall, the F-score is defined as
To measure the performance of different approaches for music search, the mean average precision (MeanAP) and the area under the receiver operating characteristic curve (AROC) are adopted as assessment metrics. Given a query tag, the focus of MeanAP is on finding the most relevant songs, while AROC emphasizes whether relevant songs are ranked higher than irrelevant ones. We also apply  X  -fold cross validation to ensure the stability and robustness of the empirical results.  X  is predefined to be 5.

In this study, we compare the performance of our system against two state-of-the-art approaches including Autotag-ger [8, 4] and MSML [26, 25]. Acoustic feature considered by MSML is Mel-frequency cepstral coefficient (MFCC). Au-totagger is evaluated based on three feature sets including MFCC delta, afeats and bfeats 2 For MMTagger, we consider five low level feature configurations (timber features denoted by TF, rhythm features denoted by RF, spectral features de-noted by SF, melody features denoted by MF and timber fea-tures+rhythm features+spect ral features+melody features denoted by ALL.). Autotagger(MFCC delta), Autotagger(afeats) and Autotagger(bfeats) denote Autotagger with MFCC delta, afeats and bfeats respectively. MMTagger(TF), MMTag-ger(SF), MMTagger(MF), MMTagger(RF), MMTagger(ALL) denote our proposed model with timbral features, spectral features, rhythmic features, melody features and the combi-nation of all four musical features.
This section presents an experiment study to evaluate the competing techniques on the task of music annotation, re-trieval as well as music annotation in noisy environment.
We report a comparative study of the various tagging sys-tems on music annotation processing. Table 2 and 3 sum-marize the empirical results for three systems with various configurations on the two test collections. The size of tag set is set to 10. Here, MMTagger is tested with five different fea-ture settings is tested. The bottom four rows of both tables present the accuracy of our proposed system with just one acoustic feature. In comparison to MMTagger(ALL), they suffer from lower accuracy. In fact, the multiple feature combination achieves significant effectiveness gain ranging from 5% to 15%. The empirical results points clearly to the importance of combining features intelligently to tagging ef-fectiveness. The experimental results also demonstrate that the MMTagger significantly outperforms the existing ap-proaches. For example, in Table 2, comparing to Autotag-ger(bfeats), MMTagger(ALL) improves the precision from 0.291 to 0.351 for the CAL500 dataset, and from 0.268 to 0.327 for the TS2 collection. Similar observations can be made on the other two evaluation metrics over different test collections. We thus conclude that MMTagger emerges as the most effective music tagging scheme.
 Autotagger(MFCC delta) 0.281 0.131 0.179 Table 2: Tagging accuracy on test collection CAL500(TS1).
Detail information about those feature sets can be found in [26].
 Autotagger(MFCC delta) 0.257 0.102 0.101
Table 3: Tagging accuracy on test collection TS2.
With the wide availability of large music collections, ac-curate music search is mandatory to achieve usability. This section presents empirical results to compare the accuracy of music retrieval facilitated by our proposed scheme and the two competitors. Experimental methodology is that given a keyword query kw q in vocabulary V , a test set of songs are ranked. The metrics MeanAP and MeanAROC of each ranking are calculated for performance comparison. Tables 4 and 5 summarize the experiment results with CAL500(TS1) and TS2. Clearly, the proposed MMTagger(ALL) signifi-cantly outperforms the other approaches.

In particular, the results shows that relative to Autotag-ger, MMTagger enjoys at least 10% MeanAP increase on both test collections. Although a nice improvement over Autotagger can be observed, we find that there is more sig-nificant gain over MSML. Averagely around 21% lift in term of accuracy can be found for the two datasets. In addition, we can summarize that for our proposed system, a proper integration of multiple music features can bring substantial improvement for search and annotation effectiveness. This observation corroborates other researchers X  finding that ac-curate MIR can not be achieved with a single type of music feature and development of effective acoustic feature com-bination scheme plays important role for tagging system X  X  performance enhancement.
 Table 4: Music retrieval accuracy on test collection CAL500(TS1). Table 5: Music retrieval accuracy on test collection TS2.
Modern MIR systems often need to work robustly in pres-ence of ambient noise (e.g. raw music signal recorded from live concerts or other outdoor environments). However, ex-isting schemes might not perform effectively when handling noisy audio input. Thus, it is important to evaluate the ro-bustness of different music annotation schemes against music sources containing different kinds of audio distortion. In this work, we study how different types of noise in the query mu-sic affect annotation accuracy. We use the TS1 (CAL500) as evaluation data and apply the same set of test music as those used in the music annotation experiment. Before the test, various kinds of audio distortion are injected into each query music item. The distortion cases include 50% vol-ume amplification, 50% volume deamplification, 10 second cropping, 35dB SNR mean background noise and 35db SNR white background noise 3 .
 Tables 6-10 illustrate the noise robustness performance of MMTagger and its competitors for the different distortion cases. In general, the results show that when the music input is polluted by a certain kind of noise, annotation accuracy of all the systems suffers. Comparing to the other approaches, MMTagger demonstrates high resilience and stable perfor-mance. Specifically, MMTagger using single kind of acoustic feature suffers greater performance degradation than MM-Tagger with all four acoustic features. Moreover, crosscheck-ing the results from this section and Section 5.1 reveals that MMTagger demonstrates more stable performance and en-joys less accuracy degradation than Autotagger and MSML under noi sy circumstances. For e xample, MMTagger X  X  pre-cision decreases about 7% when annotating inputs with 50% volume amplification. Whereas, Autotagger and MSML suf-fer about 15% and 17% drop on average. We thus conclude that MMTagger is robust to different kinds of noise.
As a key enabling technology for music information re-trieval, tagging has received a lot of research attentions in recent years. However, the performance of existing systems is far from satisfactory. In this paper, we describe a new mu-sic annotation scheme based on advanced feature extraction and multilayer structure. We have applied our method to two large test collections. Theoretical analysis and empiri-cal results indicate that our approach achieves substantially
SNR dB =10 log 10 S N is the equation used to calculate the signal-to-noise ratio, where S denotes the signal power, and N denotes the noise power in dB Autotagger(MFCC delta) 0.241 0.112 0.154 Table 6: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type -50% vol-ume amplification.
 Autotagger(MFCC delta) 0.239 0.117 0.167 Table 7: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type -50% vol-ume deamplification.
 Autotagger(MFCC delta) 0.237 0.120 0.150 Table 8: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type -10 second cropping.
 Autotagger(MFCC delta) 0.240 0.110 0.160 Table 9: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type -35dB SNR mean background noise Autotagger(MFCC delta) 0.242 0.108 0.156 Table 10: Tagging accuracy in noisy environment on test collection CAL500(TS1). Noise type -35dB SNR white background noise higher accuracy in tagging music documents comparing to existing techniques. Moreover, our method demonstrates su-perior robustness against different kinds of audio distortion.
This work can be extended in several directions: At this stage, we have only tested our method on audio data. It would be very interesting to apply the method to data from other application domains (e.g. image and video retrieval) and investigate its performance characteristics. In addition, we plan to integrate more acoustic features into our frame-work. A natural question arises as to what kinds of feature combination is best in terms of effectiveness and robust-ness enhancement. Finally, designing a robust and effective evaluation methodology is also very important for further investigation and fair performance comparison. Shuichang Yan is partially supported by AcRF Tier-1 Grant of R-263-000-464-112, Singapore.
