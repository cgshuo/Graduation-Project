 Over the past several years, there has been much in-terest in the task of multi-document summarization. In the common Document Understanding Confer-ence (DUC) formulation of the task, a system takes as input a document set as well as a short descrip-tion of desired summary focus and outputs a word length limited summary. 1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content. 2
There are several approaches to modeling docu-ment content: simple word frequency-based meth-ods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically mo-tivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum  X  e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, lit-tle has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization.

In this work we examine a series of content models for multi-document summarization and ar-gue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summariza-tion quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see sec-tion 5.2). We also contend that they provide con-venient building blocks for adding more structure to a summarization model. In particular, we uti-lize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific  X  X ub-topics X  within a document set. The resulting model,
IER S UM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics. The task we will consider is extractive multi-document summarization. In this task we assume a document collection D consisting of documents D 1 ,...,D n describing the same (or closely related) set of events. Our task will be to propose a sum-mary S consisting of sentences in D totaling at most L words. 3 Here as in much extractive summariza-tion, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1). The most prevalent example of this data setting is document clusters found on news aggregator sites. 2.1 Automated Evaluation For model development we will utilize the DUC 2006 evaluation set 4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5).
 Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system-generated summary against a set of human-generated peer summaries. 5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries. In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams) 6 . We present R-2 without stop words in the running text, but full development results are pre-sented in table 1. Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004). In addition to presenting automated results, we also present a user evaluation in section 5.2. We present a progression of models for multi-document summarization. Inference details are given in section 4. 3.1 SumBasic The S UM B ASIC algorithm, introduced in Nenkova and Vanderwende (2005), is a simple effective pro-cedure for multi-document extractive summariza-tion. Its design is motivated by the observation that the relative frequency of a non-stop word in a doc-ument set is a good predictor of a word appearing in a human summary. In S UM B ASIC , each sentence S is assigned a score reflecting how many high-frequency words it contains, where P D (  X  ) initially reflects the observed unigram probabilities obtained from the document collection D . A summary S is progressively built by adding the highest scoring sentence according to (1). 7
In order to discourage redundancy, the words in the selected sentence are updated P new P til the summary word limit has been reached.
Despite its simplicity, S UM B ASIC yields 5.3 R-2 without stop words on DUC 2006 (see table 1). 8 By comparison, the highest-performing ROUGE sys-tem at the DUC 2006 evaluation, S UM F OCUS , was built on top of S UM B ASIC and yielded a 6.0, which is not a statistically significant improvement (Van-derwende et al., 2007). 9
Intuitively, S UM B ASIC is trying to select a sum-mary which has sentences where most words have high likelihood under the document set unigram dis-tribution. One conceptual problem with this objec-tive is that it inherently favors repetition of frequent non-stop words despite the  X  X quaring X  update. Ide-ally, a summarization criterion should be more recall oriented, penalizing summaries which omit moder-ately frequent document set words and quickly di-minishing the reward for repeated use of word.
Another more subtle shortcoming is the use of the raw empirical unigram distribution to represent con-tent significance. For instance, there is no distinc-tion between a word which occurs many times in the same document or the same number of times across several documents. Intuitively, the latter word is more indicative of significant document set content. 3.2 KLSum The KLS UM algorithm introduces a criterion for se-lecting a summary S given document collection D , where P S is the empirical unigram distribution of the candidate summary S and KL ( P k Q ) repre-sents the Kullback-Lieber (KL) divergence given by divergence between the true distribution P (here the document set unigram distribution) and the approx-imating distribution Q (the summary distribution). This criterion casts summarization as finding a set of summary sentences which closely match the doc-ument set unigram distribution. Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems. We ad-dress optimizing equation (2) as well as summary sentence ordering in section 4.

KLS UM yields 6.0 R-2 without stop words, beat-ing S UM B ASIC but not with statistical significance. It is worth noting however that KLS UM  X  X  performance matches S UM F OCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006. 3.3 TopicSum As mentioned in section 3.2, the raw unigram dis-tribution P D (  X  ) may not best reflect the content of D for the purpose of summary extraction. We propose T OPIC S UM , which uses a simple LDA-like topic model (Blei et al., 2003) similar to Daum  X  e III and Marcu (2006) to estimate a content distribu-tion for summary extraction. 11 We extract summary sentences as before using the KLS UM criterion (see equation (2)), plugging in a learned content distribu-tion in place of the raw unigram distribution.
First, we describe our topic model (see figure 1) which generates a collection of document sets. We assume a fixed vocabulary V : 12 1. Draw a background vocabulary distribution  X  B 2. For each document set D , we draw a content 3. For each document D in D , we draw a  X  4. For each sentence S of each document Our intent is that  X  C represents the core con-tent of a document set. Intuitively,  X  C does not include words which are common amongst several document collections (modeled with the
ACKGROUND topic), or words which don X  X  appear across many documents (modeled with the D OC S PE -CIFIC topic). Also, because topics are tied together at the sentence level, words which frequently occur with other content words are more likely to be con-sidered content words.

We ran our topic model over the DUC 2006 document collections and estimated the distribution  X  a summary using the KLS UM criterion with our es-timated  X  C in place of the the raw unigram distribu-tion. Doing so yielded 6.3 R-2 without stop words (see T OPIC S UM in table 1); while not a statistically significant improvement over KLS UM , it is our first model which outperforms S UM B ASIC with statistical significance.

Daum  X  e III and Marcu (2006) explore a topic model similar to ours for query-focused multi-document summarization. 16 Crucially however, Daum  X  e III and Marcu (2006) selected sentences with the highest expected number of C ONTENT words. 17 We found that in our model using this extraction criterion yielded 5.3 R-2 without stop words, sig-nificantly underperforming our T OPIC S UM model. One reason for this may be that Daum  X  e III and Marcu (2006) X  X  criterion encourages selecting sen-tences which have words that are confidently gener-ated by the C ONTENT distribution, but not necessar-ily sentences which contain a plurality of it X  X  mass. Previous sections have treated the content of a doc-ument set as a single (perhaps learned) unigram dis-tribution. However, as Barzilay and Lee (2004) ob-serve, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. For concreteness consider the DUC 2006 docu-ment collection describing the opening of Star Wars: Episode 1 (see figure 2(a)).

While there are words which indicate the general content of this document collection (e.g. star , wars ), there are several sub-stories with their own specific vocabulary. For instance, several documents in this collection spend a paragraph or two talking about the financial aspect of the film X  X  opening and use a specific vocabulary there (e.g. $ , million , record ). A user may be interested in general content of a docu-ment collection or, depending on his or her interests, one or more of the sub-stories. We choose to adapt our topic modeling approach to allow modeling this aspect of document set content.

Rather than drawing a single C ONTENT distribu-tion  X  C for a document collection, we now draw a general content distribution  X  C LET ( V ,  X  G ) as well as specific content distribu-tions  X  C LET ( V ,  X  S ). 18 Our intent is that  X  C 0 represents the general content of the document collection and each  X  C i represents specific sub-stories.

As with T OPIC S UM , each sentence has a distribution  X  T over topics ( the model works exactly as in T OPIC S UM . However when the C ONTENT topic is drawn, we must decide whether to emit a general content word (from  X  C or from one of the specific content distributions (from one of  X  C story of T OPIC S UM is altered as follows in this case:  X  General or Specific? We must first decide  X  What Specific Topic? If  X  G decides we are
Our intent is that the general content distribution  X 
C 0 now prefers words which not only appear in many documents, but also words which appear con-sistently throughout a document rather than being concentrated in a small number of sentences. Each specific content distribution  X  C topics which are used in several documents but tend to be used in concentrated locations.

H IER S UM can be used to extract several kinds of summaries. It can extract a general summary by plugging  X  C also produce topical summaries for the learned spe-cific topics by extracting a summary over each  X  C distribution; this might be appropriate for a user who wants to know more about a particular sub-story. While we found the general content distribu-tion (from  X  C we experimented with utilizing topical summaries for other summarization tasks (see section 6.1). The resulting system, H IER S UM yielded 6.4 R-2 without stop words. While not a statistically significant im-provement in ROUGE over T OPIC S UM , we found the summaries to be noticeably improved. Since globally optimizing the KLS UM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we opted instead for a simple approximation where sen-tences are greedily added to a summary so long as they decrease KL-divergence. We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains. All summary sentence order-ing was determined as follows: each sentence in the proposed summary was assigned a number in [0 , 1] reflecting its relative sentence position in its source document, and sorted by this quantity.

All topic models utilize Gibbs sampling for in-ference (Griffiths, 2002; Blei et al., 2004). In gen-eral for concentration parameters, the more specific a distribution is meant to be, the smaller its con-centration parameter. Accordingly for T OPIC S UM ,  X  used  X  G = 0 . 1 and  X  S = 0 . 01 . These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended. We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words 22 which will be eval-uated automatically and manually in order to simu-late as much as possible the DUC evaluation envi-ronment. 23 DUC 2007 consists of 45 document sets, each consisting of 25 documents and 4 human refer-ence summaries.

We primarily evaluate the H IER S UM model, ex-tracting a single summary from the general con-tent distribution using the KLS UM criterion (see sec-tion 3.2). Although the differences in ROUGE be-found H IER S UM summary quality to be stronger.
In order to provide a reference for ROUGE and manual evaluation results, we compare against
YTHY , a state-of-the-art supervised sentence ex-traction summarization system. P YTHY uses human-generated summaries in order to train a sentence ranking system which discriminatively maximizes ROUGE scores. P YTHY uses several features to rank sentences including several variations of the UM B ASIC score (see section 3.1). At DUC 2007,
YTHY was ranked first overall in automatic ROUGE evaluation and fifth in manual content judgments. As P YTHY utilizes a sentence simplification com-ponent, which we do not, we also compare against
YTHY without sentence simplification. 5.1 ROUGE Evaluation ROUGE results comparing variants of H IER S UM and as described in section 3.4 yields 7.3 R-2 without stop words, falling significantly short of the 8.7 that
YTHY without simplification yields. Note that R-2 is a measure of bigram recall and H IER S UM does not represent bigrams whereas P YTHY includes several bigram and higher order n-gram statistics.
 footing with respect to R-2, we instead ran H IER -
UM with each sentence consisting of a bag of bi-grams instead of unigrams. 24 All the details of the model remain the same. Once a general content distribution over bigrams has been determined by hierarchical topic modeling, the KLS UM criterion is used as before to extract a summary. This sys-tem, labeled H IER S UM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperform-ing H IER S UM unigram. This model outperforms
YTHY with and without sentence simplification, but not with statistical significance. We conclude that both P YTHY variants and H IER S UM bigram are com-parable with respect to ROUGE performance. 5.2 Manual Evaluation In order to obtain a more accurate measure of sum-mary quality, we performed a simple user study. For each document set in the DUC 2007 collection, a user was given a reference summary, a P YTHY sum-mary, and a H IER S UM summary; 25 note that the orig-inal documents in the set were not provided to the user, only a reference summary. For this experiment we use the bigram variant of H IER S UM and compare it to P YTHY without simplification so both systems have the same set of possible output summaries.
The reference summary for each document set was selected according to highest R-2 without stop words against the remaining peer summaries. Users were presented with 4 questions drawn from the DUC manual evaluation guidelines: 26 (1) Overall quality: Which summary was better overall? (2) Non-Redundancy: Which summary was less redun-dant? (3) Coherence: Which summary was more coherent? (4) Focus: Which summary was more focused in its content, not conveying irrelevant de-tails? The study had 16 users and each was asked to compare five summary pairs, although some did fewer. A total of 69 preferences were solicited. Doc-ument collections presented to users were randomly selected from those evaluated fewest.
 As seen in table 5.2, H IER S UM outperforms
YTHY under all questions. All results are statis-tically significant as judged by a simple pairwise t-test with 95% confidence. It is safe to conclude that users in this study strongly preferred the H IER -
UM summaries over the P YTHY summaries. While it is difficult to qualitatively compare one summarization system over another, we can broadly characterize H IER S UM summaries compared to some of the other systems discussed. For example out-put from H IER S UM and P YTHY see table 2. On the whole, H IER S UM summaries appear to be signifi-cantly less redundant than P YTHY and moderately less redundant than S UM B ASIC . The reason for this might be that P YTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy. Another tendency is for H IER S UM to se-lect longer sentences typically chosen from an early sentence in a document. As discussed in section 3.4,
IER S UM is biased to consider early sentences in documents have a higher proportion of general con-tent words and so this tendency is to be expected. 6.1 Content Navigation A common concern in multi-document summariza-tion is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collec-tion in order to propose a query.

As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries rel-evant to a user X  X  interests. We may use H IER S UM in order to facilitate content discovery via presenting a user with salient words or phrases from the spe-cific content topics parametrized by  X  C (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies.
 In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as mea-sured by automatic and manual metrics.
 Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments.
