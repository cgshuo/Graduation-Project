 retr i eval (MIR) . Instead of retr i ev i ng mus i c w i th metadata, such as t i tle, performer, and composer, i t i s des i rable to locate mus i c by s i mply humm i ng or s i ng i ng a p i ece of tune to the system . Th i s concept has been extens i vely stud i ed i n var i ous content-based mus i c retr i eval research [1-7], collect i vely called query-by-humm i ng or query-by-s i ng i ng . One i s the symbol i c mus i c represented by mus i cal scores, e . g . , MIDI and Humdrum . The second category relates to those conta i n i ng acoust i c s i gnals recorded from real wh i ch many notes may be played s i multaneously, i n contrast to monophon i c mus i c, i n wh i ch at most one note i s played at any g i ve t i me . Consequently, extract i ng the ma i n melody d i rectly from a polyphon i c mus i c proves to be a very challeng i ng task [6-10], compared to deal i ng w i th the MIDI mus i c, wh i ch i s easy to acqu i re the ma i n melody by select i ng one of the symbol i c tracks .

On the other hand, the development of a query-by-s i ng i ng MIR system rel i es on an effect i ve melody s i m i lar i ty compar i son . S i nce most users are not profess i onal s i ngers, a sung query may conta i n i nev i table tempo errors, note dropout errors, note i nsert i on errors, etc . To handle these errors, var i ous approx i mate match i ng methods, such as dynam i c t i me warp i ng (DTW) [5][11-12], h i dden Markov model [13], and N-gram model [8][10], have been stud i ed, w i th DTW be i ng the most popular . However, due to the cons i derable t i me consumpt i on for DTW, another key i ssue on des i gn i ng a query-by-s i ng i ng MIR system i s how to speed up the s i m i lar i ty compar i son, so that a large scale mus i c database can be searched eff i c i ently [5][14-15] .
 In th i s study, we focus on a sort of mus i c data called Karaoke . It stems from Japanese popular enterta i nment, wh i ch prov i des prerecorded accompan i ments to e i ther VCD or DVD format . Each p i ece of Karaoke track comes w i th two aud i o channels : one i s a m i x of vocal and accompan i ment, and the other i s composed of accompan i ment only . The mus i c i n the accompan i ment-only channel i s usually very methods are proposed to extract vocal  X  s melody from accompan i ed Karaoke tracks by reduc i ng the i nterference from the background accompan i ments . In parallel, we apply Bayes i an Informat i on Cr i ter i on (BIC) [16] to detect the onset t i me of each phrase i n the accompan i ed vocal channel, wh i ch enables the subsequent DTW-based s i m i lar i ty compar i son to be performed more eff i c i ently . The proposed system further uses mult i ple-level mult i ple-pass DTW to i mprove the retr i eval eff i c i ency and accuracy . We evaluate our approaches on a Karaoke database cons i st i ng of 1,017 songs . The 
The rema i nder of th i s paper i s organ i zed as follows . Sect i on 2 i ntroduces the conf i gurat i on of our Karaoke mus i c retr i eval system . Sect i on 3 presents the methods for background mus i c reduct i on and ma i n melody extract i on . Sect i on 4 descr i bes the phrase onset detect i on . In Sect i on 5, we d i scuss the s i m i lar i ty compar i son module and presents our exper i mental results, and Sect i on 7 concludes th i s study . Our Karaoke mus i c retr i eval system i s des i gned to take as i nput an aud i o query sung by a user, and to produce as output the song conta i n i ng the most s i m i lar melody to the sung query . As shown i n F i g . 1, i t operates i n two phases : i ndex i ng and search i ng . 2 . 1 Index i ng The i ndex i ng phase cons i sts of two components : ma i n melody extract i on and phrase S i nce the channel conta i n i ng vocal s i gnals also encompasses accompan i ments, the melody extracted from raw aud i o data may not be the tune performed by a s i nger, but the i nstruments i nstead . To reduce the i nterference from the background accompan i ments to ma i n melody extract i on, we propose explo i t i ng the s i gnal of accompan i ment-only channel to approx i mate the vocal  X  s background mus i c . The the fundamental frequenc i es of the vocal s i gnals are est i mated, whereby convert i ng the waveform representat i on i nto a sequence of mus i cal note symbols .

The phrase onset detect i on a i ms to locate the expected beg i nn i ng of a query that users would l i ke to s i ng to the system . In v i ew of the fact that the length of a popular of a sentence of lyr i cs . For i nstance, a user may query the system by s i ng i ng a p i ece There ' s a shadow hang i ng over me . X  In contrast, a sung query l i ke  X  I used to be . Therefore, pre-locat i ng the phrase onsets could not only match users  X  quer i es better, but also i mprove the eff i c i ency of the system i n the search i ng phase .

After i ndex i ng, the database i s composed of the note-based sequences and the labels of phrase onset t i mes for each i nd i v i dual song .
 2 . 2 Search i ng In the search i ng phase, the system determ i nes the song that a user looks for based on what he/she i s s i ng i ng . It i s assumed that a user  X  s sung query can be e i ther a complete commences w i th the end-po i nt detect i on that records the s i ng i ng vo i ce and marks the sal i ent pauses w i th i n the s i ng i ng waveform . Next, the s i ng i ng waveform i s converted i nto a sequence of note symbols v i a the note sequence generat i on module as used i n the i ndex i ng phase . Accord i ngly, the retr i eval task i s narrowed down to a problem of documents  X  note sequences . The song assoc i ated w i th the note sequence most s i m i lar to the query  X  s note sequence i s regarded as relevant and presented to the user . 3 . 1 Background Mus i c Reduct i on contrast to the retr i eval of MIDI mus i c, wh i ch i s easy to acqu i re the ma i n melody by select i ng one of the symbol i c tracks, retr i ev i ng a polyphon i c ob j ect i n CD or Karaoke format requ i res to extract the ma i n melody d i rectly from the accompan i ed s i ng i ng background mus i c from the accompan i ed s i ng i ng s i gnals .

The i ndex i ng phase beg i ns w i th the extract i on of aud i o data from Karaoke VCD  X  s accompan i ment, and the other i s composed of accompan i ment only . F i g . 2 shows an example waveform for a Karaoke mus i c p i ece . It i s observed that the accompan i ments i n the two channels often resemble each other, but are not i dent i cal . Th i s mot i vates us accompan i ment-only channel . To do th i s, the s i gnals i n both channels are f i rst d i v i ded s background mus i c . Usually, m  X  t  X  m t , s i nce the accompan i ment s i gnals i n one channel may be d i fferent from another i n terms of ampl i tude, phase, etc . , where the phase d i fference reflects the asynchron i sm between two channels  X  accompan i ments . As a d i st i ll i ng the des i red vocal . To handle th i s problem better, we assume that the accompan i ed vocal i s of the form c t = s t + a m t + b , where m t+b i s the b -th frame next to m est i mat i on error, i. e . ,  X  | c s channel after background mus i c reduct i on . We can see that the accompan i ment i n the accompan i ed vocal channel i s largely reduced .
 3 . 2 Note Sequence Generat i on After reduc i ng the undes i red background accompan i ments, the next step i s to convert each record i ng from i ts waveform representat i on i nto a sequence of mus i cal notes . Follow i ng [7], the convert i ng method beg i ns by comput i ng the short-term Fast determ i ned by where C i s a pre-set number of harmon i cs concerned, h i s a pos i t i ve value less than 1 est i mated by and where  X  X  i s a floor operator, F ( j ) i s the correspond i ng frequency of FFT i ndex j , and U (  X  ) represents a convers i on between the FFT i nd i ces and the MIDI note numbers . 3 . 3 Note Sequence Smooth i ng The result i ng note sequence may be ref i ned by i dent i fy i ng and correct i ng the abnormal notes ar i s i ng from the res i dual background mus i c . The abnormal i ty i n a note sequence can be d i v i ded i nto two types of errors : short-term error and long-term wh i ch replaces each note w i th the local med i an of notes of i ts ne i ghbor i ng frames . On the other hand, the long-term error i s concerned w i th a success i on of the est i mated notes not produced by a s i nger . These success i ve wrong notes are very l i kely several octaves above or below the true sung notes, wh i ch could result i n the range of the est i mated notes w i th i n a sequence be i ng w i der than that of the true sung note sequence . As reported i n [7], the sung notes w i th i n a verse or chorus sect i on usually vary no more than 22 sem i tones . Therefore, we may ad j ust the suspect notes by o { o  X  1 , o where R i s the normal vary i ng range of the sung notes i n a sequence, say 22, and o i s cons i dered as a wrong note and needs to be ad j usted i f i t i s too far away from o , i. e . , | o  X  o | &gt; R /2 . The ad j ustment i s done by sh i ft i ng the wrong note  X  ( o  X  ( o t  X  o  X  R /2)/12  X  octaves . In general, the structure of a popular song i nvolves f i ve sect i ons : i ntro , verse , chorus , that a verse or chorus i s the favor i te that people go away humm i ng when they hear a good song, and hence i s often the query that a user may hum or s i ng to a mus i c retr i eval system .
 each phrase can be detected before the DTW compar i son i s performed, i t i s expected that both the search eff i c i ency and the retr i eval accuracy can be i mproved .

Our strategy for detect i ng the phrase onsets i s to locate the boundar i es that the s i gnal i n the accompan i ed vocal channel i s changed from accompan i ment-only to a m i x of vocal and accompan i ment . As ment i oned earl i er, the accompan i ment of one channel i n a Karaoke track often resembles that of the other channel . Thus, i f no vocal i s performed i n a certa i n passage, the d i fference of s i gnal spectrum between the two channels i s t i ny . In contrast, i f a certa i n passage conta i ns vocal s i gnals, there must be could exam i ne the d i fference of s i gnal spectrum between the two channels, thereby [16] i s appl i ed to character i ze the level of spectrum d i fference . 4 . 1 The Bayes i an Informat i on Cr i ter i on (BIC) for model H k i s def i ned as : number of free parameters i n H k . The select i on cr i ter i on favors the model hav i ng the largest value of BIC . Obv i ously, the larger the value of  X  BIC , the more l i kely segments X and Y are from d i fferent acoust i c classes, and v i ce versa . We can therefore set a threshold of  X  BIC to determ i ne i f two aud i o segments belong to the same acoust i c class .

Assume that we have two audio segmen ts represented by feature vectors, X = { x 1 , x ,..., x N } and Y = { y 1 , y 2 ,..., y N }, respectively. If it is desired to determine whether X and Y belong to the same acoustic class, then we have two hypotheses to consider: one is  X  X es X , and the other is  X  X o X . Pr ovided that hypotheses  X  X es X  and  X  X o X  are characterized by a certain stochastic models H 1 and H 2 , respectively, our aim will be single Gaussian distribution N ( P , 6 ), where P and 6 are the sample mean and covariance 6 x are estimated using vectors { x 1 , x 2 ,..., x N }, and the sample mean P y and covariance 6 y are estimated using vectors { y 1 , y 2 ,..., y N }. Then, the problem of judging which model is better can be solved by computing a difference value of BIC between BIC ( H 1 ) and BIC ( H 2 ), i.e., 4 . 2 Phrase Onset Detect i on V i a BIC j udge whether the s i gnals i n the two channels belong to the same acoust i c class dur i ng a certa i n t i me i nterval, where one class represents accompan i ment only, and the other represents vocal over accompan i ment . Thus, by cons i der i ng X and Y as two concurrent channels  X  s i gnals,  X  BIC can be computed along the ent i re record i ng, and phrase onset detect i on . F i gs . 3(a) and 3(b) are the waveforms i n the accompan i ed vocal channel and the accompan i ment-only channel, respect i vely . The phrase onset detect i on beg i ns by chopp i ng the waveform i n each of the channels i nto non-overlapp i ng frames of 20ms . Each frame i s represented as 12 Mel-scale Frequency Cepstral Coeff i c i ents (MFCCs) . Then, the  X  BIC value between each pa i r of one-second segments i n the two channels i s computed frame by frame, thereby form i ng a  X  that the frame of the accompan i ed vocal channel conta i ns vocals but the concurrent frame i n the accompan i ment-only channel does not . In contrast, the negat i ve value of  X 
BIC i nd i cates that both frames conta i n accompan i ments only . Therefore, the i ntervals where pos i t i ve values of  X  BIC appear are i dent i f i ed as vocal sect i ons .
In the example shown i n F i g . 3(c), two vocal sect i ons are i dent i f i ed, each m i n i mums on the  X  BIC curve can be further located as phrase onsets because the frame corresponds to rest or breath i ng between phrases usually y i elds a small  X  BIC value . F i g . 3 (d) dep i cts the detected phrase onsets i n th i s way . Note that the trade-off number of detected phrase onsets . In general, the larger the number of the l i kely phrase onsets i s detected, the h i gher the retr i eval accuracy can be ach i eved . However, i ncreas i ng the number of the cand i date phrase onsets often decreases the retr i eval eff i c i ency drast i cally . G i ven a user  X  s sung query and a set of mus i c documents, each of wh i ch i s represented by a note sequence, the task here i s to f i nd a mus i c document whose part i al note sequence i s most s i m i lar to the query  X  s note sequence . 5 . 1 Dynam i c T i me Warp i ng Framework user  X  s query and a part i cular mus i c segment to be compared, respect i vely . As the temporal mapp i ng between q and u by Dynam i c T i me Warp i ng (DTW) . computed us i ng : and where  X  i s a small constant that favors the mapp i ng between notes q t and u A , g i ven the d i stance between note sequences { q 1 , q 2 , ... , q t -1 } and { u 1 , u 2 , ... , u A -1 } .
To compensate for the i nev i table errors ar i s i ng from the phrase onset detect i on, we assume that the true phrase onset t i me assoc i ated w i th the automat i cally detected Though the DTW recurs i on i n Eq . (9) i nd i cates that the best path ex i sts only when the sequence ( i. e . , between T /2 and 2 T ), we prefer to l i m i t the best mapp i ng length of u to q to be between T /2 and kT , where k i s a value between 1 and 2, so that the mapp i ng can be more prec i sely . In other words, the tempo of the query i s allowed to be 1/ k i mplementat i on, we set the new phrase onset t i me t  X  os as t os -r /2, clone the subsequence def i ne the boundary cond i t i ons for the DTW recurs i on as, evaluated by 5 . 2 Mult i ple-Pass DTW to Improve Retr i eval Accuracy S i nce a query may be sung i n a d i fferent key or reg i ster than the target mus i c and the document could be rather d i fferent . Th i s problem can be allev i ated by sh i ft i ng the query  X  s note sequence upward or downward several sem i tones, so that the mean of the sh i fted query  X  s note sequence can equal that of the document to be compared . part i al sung query, we further perform mult i ple DTW s i m i lar i ty compar i sons by then def i ned as, where q ( v ) denotes the query sequence obta i ned by sh i ft i ng q upward or downward v costs, because the s i m i lar i ty compar i son requ i res two extra DTW operat i ons whenever the value of V i s i ncreased by one . Thus, an econom i c value of V = 1, i. e . , three-pass DTW, i s adopted i n th i s work .

In add i t i on to the d i fference of key and tempo ex i st i ng between quer i es and documents, another problem to be addressed i s the ex i stence of vo i celess reg i ons i n a sung query . The vo i celess reg i ons, wh i ch may ar i se from the rest, pause, etc . , result i n some notes be i ng tagged w i th  X  0  X  i n the query note sequence . However, the correspond i ng non-vocal reg i ons i n the document are usually not tagged w i th  X  0  X  , because there are accompan i ments i n those reg i ons . Although the vo i celess reg i ons i n a sung query can be detected by s i mply us i ng the energy i nformat i on, the accurate detect i on of non-vocal reg i ons i n a mus i c document rema i ns a very d i ff i cult problem . To s i destep th i s problem, we mod i fy the computat i on of d ( t , A ) i n Eq . (10) to vo i celess reg i ons of a query . 5 . 3 Mult i ple-Level DTW to Improve Retr i eval Eff i c i ency necessary d i stance computat i on D (  X  ) for construct i ng a T  X  L table i s As ment i oned earl i er, s i nce L i s usually set to be kT , where 2 2 / 1  X   X  k , the computat i onal complex i ty can be rewr i tten as Although the DTW recurs i on allows a document sequence w i th i n half to tw i ce the length of the query sequence, emp i r i cal ev i dence shows that the document length can degrad i ng the retr i eval performance . Hence, by subst i tut i ng k = 1 . 2 i nto Eq . (16), the phrase onset tolerance i n the DTW w i ll i ncrease the computat i onal complex i ty by rT . computat i onal complex i ty i s tw i ce that of the typ i cal DTW .

S i nce the complex i ty i s O ( T 2 ), the most prom i s i ng way to speed up the search i ng Aggregate Approx i mat i on (PAA) [14], we propose a d i mens i onal i ty reduct i on techn i que, called Mult i -Level Data Abstract i on (MLDA) . Unl i ke PAA, wh i ch d i v i des a t i me ser i es i nto equal-length frames, and then calculates the mean value of the data step manner . In MLDA, the compress i on rate of data i s power of two at each level . For example, i f we go through the note sequence and p i ck one note every two notes, the note sequence i s reduced to half length, wh i le the computat i on i s reduced to mus i c documents . If the or i g i nal computat i onal complex i ty i s CC and the prun i ng rate i s R pr , then the total computat i onal complex i ty of the two-level DTW i s [1/4+(1-R )] CC . If a three-level DTW i s appl i ed, the complex i ty i s reduced to [(1/4) 2 +(1/4)(1-R )+(1-R pr ) 2 ] CC . In th i s way, when the prun i ng rate R pr i s set at 0 . 75, the complex i ty of the two-level DTW and three-level DTW i s 1/2 and 3/16 that of the s i ngle-level DTW, respect i vely . 6 . 1 Mus i c Database The mus i c database used i n th i s study cons i sted of 1,071 songs extracted from Karaoke VCDs . The extracted waveform s i gnals were down-sampled from the sampl i ng rate of 44 . 1 kHz to 22 . 05 kHz . The database was d i v i ded i nto two subsets . The f i rst subset cons i sted of 95 songs, denoted as DB-1 . The second subset cons i sted of 976 songs, denoted as DB-2 . To compare the system performances ach i eved w i th automat i c and manual detect i on of phrase onset, we manually labeled the phrase onsets of the songs i n DB-1 . There were 775 phrase onsets marked .

We collected 90 quer i es from 9 male and 4 female users . The durat i on of each query ranged from 15 seconds to 45 seconds, but only the f i rst 8-second port i on was i nput to the system . The performance was measured on the bas i s of song accuracy def i ned as, documents can be prov i ded for user  X  s cho i ces, we also computed the Top-N accuracy def i ned as the percentage of the quer i es whose target songs are among Top-N . The overall system performance was evaluated on both DB-1 and DB-2 .
 6 . 2 Exper i mental Results The f i rst exper i ment was conducted to evaluate the effect i veness of the background mus i c reduct i on us i ng DB-1 . Here, the phrase onsets were labeled manually . The reduct i on i mproves the retr i eval performance .

The second exper i ment was conducted to compare the retr i eval performance obta i ned w i th the manual phrase onset label i ng and the automat i c phrase onset detect i on . The automat i c phrase onset detect i on approach marked 2,719 phrase onsets i n the 95 songs i n DB-1, wh i ch i s about 3 . 5 t i mes that of hand-labeled phrase onsets . The retr i eval performance i s g i ven i n Table 2 . We observe that the retr i eval accuracy only decreases sl i ghtly when the way to mark the phrase onsets was changed from manual to automat i c .
 Lastly, we evaluated the performance of the Karaoke retr i eval system us i ng both DB-1 and DB-2, w i th the same 90 quer i es . The system automat i cally marked 27,397 phrase onsets i n the 1,017 songs i n DB-1 and DB-2 . The retr i eval performance i s g i ven i n Table 3 . We observe that the Top-1 accuracy drops from 70 . 00% to 51 . 11% as the database expands from 95 songs to 1,071 songs, wh i le the Top-10 accuracy only sl i ghtly drops from 77 . 78% to 70 . 00% . To speed up the search i ng, the four-level DTW, w i th var i ous prun i ng rates, R pr , was i mplemented . We observe from Table 3 that the search i ng t i me for the mult i ple-level DTW can be greatly reduced at a small cost of retr i eval accuracy degradat i on . We have presented a Karaoke mus i c retr i eval system that allows users to locate the i r accompan i ments are m i xed together i n an accompan i ed vocal channel, we proposed a method to reduce the accompan i ments i n the accompan i ed vocal channel so that the accuracy of ma i n melody extract i on could be i mproved . In add i t i on, we appl i ed wh i ch reflects the most l i kely beg i nn i ng of a sung query . The phrase onset detect i on, compar i son, enables an eff i c i ent and effect i ve search for a large mus i c database . The exper i ments conducted on a mus i c database cons i st i ng of 1,017 songs conf i rmed the feas i b i l i ty of our retr i eval system .
 Acknowledgments . Th i s work was supported i n part by the Nat i onal Sc i ence Counc i l, Ta i wan, under Grants : NSC94-2422-H-001-007 and NSC95-2422-H-001-008 .

