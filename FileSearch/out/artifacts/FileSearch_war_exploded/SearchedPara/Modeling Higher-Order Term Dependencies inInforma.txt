 Many of the recent, and more effective, retrieval models have incorporated dependencies between the terms in the query. In this paper, we advance this query representation one step further, and propose a retrieval framework that models higher-order term dependencies, i.e., dependencies between arbitrary query concepts rather than just query terms. In order to model higher-order term dependencies, we repre-sent a query using a hypergraph structure  X  a generalization of a graph, where a (hyper)edge connects an arbitrary sub-set of vertices. A vertex in a query hypergraph corresponds to an individual query concept, and a dependency between a subset of these vertices is modeled through a hyperedge. An extensive empirical evaluation using both newswire and web corpora demonstrates that query representation using hypergraphs is highly beneficial for verbose natural language queries. For these queries, query hypergraphs significantly improve the retrieval effectiveness of several state-of-the-art models that do not employ higher-order term dependencies. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Query hypergraphs, query representation, retrieval models
Over the past decade, information retrieval research has undergone a gradual shift of focus from retrieval models that use bag-of-words query representations to retrieval models that incorporate term dependencies. Some recent exam-ples of retrieval models that incorporate term dependencies include, among others, Markov random fields [27], linear discriminant model [14], dependence language model [13], quasi-synchronous dependence model [30], and positional language model [26].

In this paper, we propose a novel retrieval framework that takes a further step toward a more accurate modeling of the dependencies between the query terms. Rather than mod-eling the dependencies between the individual query terms , our framework models dependencies between arbitrary con-cepts in the query.

We broadly define a query concept as a syntactic expres-sion that models a dependency between a subset of query terms. Query concepts may model a variety of linguistic phe-nomena, including n-grams, term proximities, noun phrases, and named entities. Therefore, a dependency between query concepts represents a dependency between term dependen-cies , i.e., a higher-order term dependency 1 .

To the best of our knowledge, there is little prior work on modeling this type of higher-order term dependencies for information retrieval. Most retrieval models limit their at-tention to either pairwise term dependencies [11, 26] or, at most, dependencies between multiple terms [2, 27]. In con-trast, the retrieval framework proposed in this paper can model dependencies between arbitrary concepts, e.g., a de-pendency between a phrase and a term. We hypothesize that an accurate modeling of concept dependencies is espe-cially important for verbose natural language queries. This is due to the fact that the grammatical complexity of these queries often challenges the capabilities of the current re-trieval models [2, 20].
 As an example, consider the natural language query in Figure 1:
Figure 1(a) shows an excerpt from the top document re-trieved by a sequential dependence model [27], a state-of-the-art retrieval model that incorporates term dependencies. As evident from this excerpt, the top-retrieved document is non -relevant with respect to the query. Even though it con-tains many instances of the phrase  X  X aw enforcement X  as well as the terms provided and information it does not mention the use of dogs.

On the other hand, an excerpt from the document in Fig-ure 1(b) clearly indicates the relevance of the top document ...Simi Valley, West Covina and Los Angeles police d epartments were among the first law enforcement agencies to receive money through the forfeiture pro-gram....a narcotics-sniffing dog in a Simi Valley police investigation...led to the largest seizure of cocaine ever by authorities from Ventura County... dog  X  X  efforts are expected to yield a substantial amount of money...for the 21-officer department... retrieved by our method with respect to the query. Even though this excerpt matches less of the query terms than the excerpt in Figure 1(a), it contains a relationship between the term dog and the phrase  X  X aw enforcement X  , which is highly indicative of its relevance. This relationship cannot be mod-eled without accounting for higher-order term dependencies.
As Figure 1 shows, the evidence of the concepts co-occurring within a passage of text is a strong indicator of their depen-dency. This is somewhat akin to term dependencies, which are often modeled based on the frequency of the terms co-occurring next (or close) to each other in the document [27, 39, 26].

In the case of concept dependency, however, instead of re-lying on the entire document, we only examine a single doc-ument passage that is deemed to be the most relevant with respect to the query. This focused evidence can distinguish between relevant documents and documents which simply contain many repeated concept instances, as in Figure 1(a). This approach is reminiscent of the passage retrieval models that often make use of the evidence from the highest-scoring document passage [3, 9, 8, 16, 41].

In contrast to the approach presented in this work, most passage retrieval methods are based on a conjunctive re-trieval model and treat a query as a bag of words. How-ever, as the excerpts in Figure 1 demonstrate, such a simple conjunctive retrieval model is not sufficient, especially for verbose, natural language queries.

Instead, the proposed retrieval framework distinguishes between the concepts and the dependencies that are cru-cial for conveying the query intent, and the concepts and the dependencies of lesser importance. For instance, in the case of the query in Figure 1, the dependency ( dog ,  X  X aw enforcement X  ) in Figure 1(b) is crucial for expressing the query intent, while the dependency ( information and  X  X aw enforcement X  ) in Figure 1(a) is not.

To summarize, unlike any of the current retrieval models, the retrieval framework proposed in this paper integrates three main characteristics that we believe are crucial for im-proving the effectiveness of retrieval with verbose queries. First, it models arbitrary term dependencies as concepts. Second, it uses passage-level evidence to model the depen-dencies between these concepts. Finally, it assigns weights to both concepts and concept dependencies, proportionate to the estimate of their importance for expressing the query intent. In this paper, we show that by integrating these characteristics, the proposed retrieval framework can signif-icantly improve the effectiveness of several current state-of-the-art retrieval models.
 Table 1: Examples of the possible structures and the c oncepts they might contain for the query X  X embers of the rock group nirvana X  (stopwords removed).

The proposed retrieval framework is based on a query rep-resentation using a hypergraph structure  X  a generalization of a graph, where an edge can connect more than two vertices. A vertex in a query hypergraph corresponds to an individ-ual query concept. The vertices are grouped by structures, which model various linguistic phenomena. For instance, as shown in Table 1, a structure can group together terms, n-grams or noun phrases. Finally, any subset (rather than just a pair as in a standard graph) of vertices can be connected via a hyperedge , which models concept dependencies.
We use the query hypergraph representation to derive a ranking function that incorporates concepts and concept de-pendencies in a principled manner, based on the factoriza-tion of the hypergraph. We then propose two approaches for the parameterization of the ranking function. The first pa-rameterization approach assigns weights to the concepts and the concept dependencies based on their respective struc-tures. The second parameterization approach assigns weights based on a set of importance features associated with each concept and concept dependency.

The remainder of this paper is organized as follows. First, in Section 2 we provide the theoretical underpinnings of the query hypergraph representation and ranking with query hypergraphs. Then, in Section 3 we describe the related work and its connection to query representation using hy-pergraphs. In Section 4 we report the details of the empirical evaluation of the proposed framework. Section 5 concludes the paper.
In this paper, we base the query representation on two modeling assumptions. First, we assume that given a query Q , we can model it using a set of linguistic structures The structures in the set  X  Q are both complete and disjoint . The completeness of the structure implies that it can be used as an autonomous query representation. The disjointness of the structures means that there is no overlap in the linguistic phenomena modeled by the different structures. In other words, each structure groups together concepts of a single type (e.g., terms, bigrams, noun phrases, etc.).

Second, within each structure, arbitrary term dependen-cies can be modeled as concepts. In other words, each struc-ture  X  i  X   X  Q is represented by a set of concepts Each such concept is considered to be an atomic unit for the purpose of query representation. In addition, for conve-nience, we adopt the notation to refer to the union of all the query concepts, regardless of their respective structures.

These modeling assumptions, while conceptually simple, create an expressive formalism for hierarchical query rep-resentation. This formalism is flexible enough to specify a wide range of specific instantiations. Table 1 shows that it can model a wide spectrum of linguistic phenomena that are often encountered in natural language processing and information retrieval applications.

For instance, as we can see in Table 1, a structure can contain single terms as concepts, resulting in a bag-of-words query representation. A structure can also contain adjacent bigrams or noun phrases. Concepts need not be defined over contiguous query terms, as is demonstrated by the last struc-ture in Table 1, which models a set of linguistic dependency links between the query terms.

For the purpose of information retrieval, we are primarily interested in using the resulting hierarchical query represen-tation to model the relationship between a query Q and a document D in the retrieval corpus. Specifically, given a set of query structures  X  Q and a document D , we construct a hypergraph H ( X  Q , D ) 3 .

A hypergraph is a generalization of a graph where an edge can connect an arbitrary set of vertices. A hypergraph H is represented by a tuple h V, E i , where V is a set of elements or vertices and E is a set of non-empty subsets of V , called hy-peredges . In other words, the set E  X  PS ( V ) of hyperedges is a subset of the powerset of V [18].

Specifically for the scenario of document retrieval, we de-fine the hypergraph H over the document D and the set of query concepts K Q as
Figure 2 demonstrates an example of a hypergraph H for the search query X  X nternational art crime X . In this particular example, we have two structures. The first structure con-tains the query terms denoted i , a , and c , respectively. The second structure contains a single phrase, ac . Over these concepts, we can define a set of five hyperedges  X  four hy-peredges connecting document D and each of the concepts, and one hyperedge connecting D and all of the concepts.
Formally, for the hypergarph H in Figure 2, the vertices and the hyperedges are defined as follows Note that this hypergraph configuration is just one possible choice. In fact, any subset of query terms can serve as a query concept, and similarly, any subset of query concepts can serve as a hyperedge, as shown by Equation 1.
In the previous section, we defined the query representa-tion using a hypergraph H = h V, E i . In this section, we define a global function over this hypergraph, which assigns a relevance score to document D in response to query Q . This relevance score is used to rank the documents in the retrieval corpus.

A factor graph, a form of hypergraph representation which is often used in statistical machine learning [6], associates a factor  X  e with a hyperedge e  X  E . Therefore, most generally, a relevance score of document D in response to query Q represented by a hypergraph H is given by
It is interesting to note that Equation 2 is reminiscent of t he recently proposed log-linear retrieval models, including the Markov random field model [27] and the linear discrim-inant model [14]. Similarly to these models, Equation 2 scores a document using a log-linear combination of factors  X  ( k e , D ).

However, an important difference from these retrieval mod-els is related to the fact that the factors  X  e ( k e , D ) in Equa-tion 2 are defined over concept sets , rather than single con-cepts , as in previous work [14, 27]. This definition enables the modeling of higher-order dependencies between query terms. Higher-order term dependencies cannot be easily modeled by the existing retrieval models that incorporate term dependencies [4, 14, 26, 27, 30, 39].

Thus far, we have provided only the most abstract defi-nition of the query representation and ranking with query hypergraphs. In the remainder of this section, we provide an in-depth discussion of the query hypergraph induction and a detailed derivation of the ranking function.

First, in Section 2.3, we fully specify the structures, con-cepts, and hyperedges in the query hypergraph H . Then, in Section 2.4, we instantiate the factors  X  e ( k e , D ) in the ranking function in Equation 2 using these specifications. Finally, in Section 2.5, we examine the different parameter-izations of the ranking function.
There are many potential ways in which we could define the set of structures  X  Q in the query hypergraph. In this work, we focus on three types of structures that are success-fully used in previous work on modeling term dependencies for information retrieval [4, 5, 27, 32]. We leave a further ex-ploration of other possible hypergraph structures to future work. (1) QT -structure . The query term ( QT ) structure contains the individual query words t i as concepts. Terms are the most commonly used concepts in information retrieval, both in bag-of-words models [33, 34] and models that incorporate term dependencies [27, 29, 14]. (2) PH -structure . The phrase ( PH ) structure contains the combinations of query terms that are matched as exact phrases in the document. Exact phrase matching has often been used for improving the performance of retrieval methods [12, 42]. Most recently, it has been shown that using query bi-grams for exact phrase matching is a simple and efficient method for improving the retrieval performance in large scale web collections [4, 5, 27, 29, 32]. Following this find-ing, we define the concepts in the PH -structure as adjacent query word pairs ( t i t i +1 ). (3) PR -structure . Unlike the PH -structure, the proximity ( PR ) structure can contain arbitrary subsets of query terms of the form { t : t  X  Q } as concepts. The PR -structure also dif-fers from the PH -structure in the way the concepts in the structure are matched in the document. In order to match the document, the individual terms in a concept in the PR -structure may occur in any order within a window of fixed length . In this paper, we fix the window size to 4 | t | terms, where | t | is the number of terms in the concept. This ap-proach follows the definition of term proximity as defined by Metzler and Croft [27].
As described in Section 2.1, a na  X   X ve induction approach may result in an exponential number of hyperedges in a query hypergraph. Instead, for the purpose of this paper, we limit our attention to two types of hyperedges, which have an intuitive appeal from an information retrieval perspec-tive. (1) Local hyperedges. For each concept  X   X  K Q , we define a hyperedge ( {  X  } , D ). This local edge 4 represents the con-tribution of the concept  X  to the total document relevance score, regardless of the other query concepts. As we show in the next section, the factors defined over the local edges are akin to the functions that are usually employed in the existing log-linear retrieval models [14, 27]. (2) Global hyperedge. In addition to the local edges, we define a single global hyperedge ( K Q , D ) over the entire set of query concepts K Q . This global hyperedge provides the evidence about the contribution of each concept  X   X  K Q given its dependency on the entire set of query concepts K
Q . Unlike in the case of local edges, the factors defined over the global hyperedge cannot be easily expressed using the existing log-linear retrieval models.

Figure 2 provides a simple example of these two types of hyperedges. The hyperedges at the bottom of the hyper-graph in Figure 2 are the local edges, while the hyperedge at the top is the global hyperedge. Following the hyperedge induction process described in Section 2.3.2, in this section we define two types of factors. The local factors  X  corresponding to the local edges  X  are defined in Section 2.4.1; the global factor  X  corresponding to the global hyperedge  X  is defined in Section 2.4.2.
Both local and global factors incorporate a matching func-tion f (  X , X ), which assigns a score to the occurrences of the concept  X  in a text fragment X . As a matching function, following some previous work on log-linear retrieval models [4, 14, 27], we use a log of the language modeling estimate for concept  X  with Dirichlet smoothing [45], i.e. where tf (  X , X ) and tf (  X , C ) are the number of occurrences of the concept  X  in the text fragment and the collection, respectively;  X  is a free parameter; | X | is the number of terms in X , and |C| is the total number of terms in the collection. The local factors are defined over the local edges ( {  X  } , D ). A local factor assigns a score to the occurrences of concept  X  in the document D , regardless of the other query concepts. Therefore, a local factor is defined similarly to the previously proposed log-linear retrieval models [4, 14, 27] where  X  (  X  ) is an importance weight assigned to the concept  X  , and f (  X , D ) is a matching function between the concept  X  and the document D .
The global hyperedge ( K Q , D ) described in Section 2.3.2, represents a dependency between the entire set of query con-cepts. In this section, we present a global factor that is defined over this hyperedge.

A common way to estimate a dependency between query terms is using a measure of their proximity in a retrieved document [11, 26, 27, 39]. Analogously, we may simply choose to estimate a dependency between query concepts using similar proximity measures. However, there are two notable difficulties that impede an application of this ap-proach to concept dependency.

First, the existing term proximity measures usually cap-ture close, sentence-level, co-occurrences of the query terms in a retrieved document [27, 32, 39]. The dependency range is much longer for concept dependencies. For instance, in the example in Figure 1(b), the concepts dog and law enforce-ment do not ever appear in the same sentence. However, the dependency between them is revealed when examining their co-occurrences in a larger text passage.

Second, since concepts can be arbitrarily complex syntac-tic expressions, the probability of observing a concept co-occurrence is much lower than the probability of observing a term co-occurrence , even in large collections. For instance, most documents in the retrieved list for the query in Fig-ure 1, do not contain both of the concepts dog and law en-forcement in a context of a single passage.

Therefore, instead of estimating the dependency between query concepts using the standard proximity measures, we leverage a long history of research on passage retrieval [3, 8, 9, 25, 16, 40, 41] for the derivation of the global factor.
In the passage retrieval literature, a document is often segmented into overlapping passages of text of fixed size [16, 17]. The document is then scored using some combi-nation of document-level and passage-level scores. One of the most successful and frequently-used score combinations is the Max-Psg combination, which uses the highest scoring passage to assign a score to the document [3, 8, 16, 24, 41].
Similarly to the Max-Psg retrieval model, we define the global factor using a passage  X  , which receives the highest score among the set  X  D of passages extracted from the doc-ument D . Formally, where  X  (  X , K Q ) is the importance weight of the concept  X  in the context of the entire set of query concepts K Q , and f (  X  ,  X  ) is a matching function between the concept  X  and a passage  X   X   X  D .

Intuitively, the global factor in Equation 5 assigns a higher relevance score to a document that contains many important concepts in the confines of a single passage. Note that the importance weight  X  (  X , K Q ) of a concept in the global factor is determined not only by the concept itself  X  as in the case of the importance weights  X  (  X , D ) in the local factors  X  but also by the concepts that co-occur together with the concept in the passage  X  .

In the previous section, we introduced two types of con-cept weights that parameterize the ranking function in Equa-tion 2. First, there are the independent importance weights  X  (  X  ) that parameterize the local factors (see Equation 4). Second, there are the importance weights  X  (  X , K Q ) that as-sign weight to a concept, while taking into account the rest of the concepts in the query (see Equation 5).

In this section, we consider two possible parameteriza-tion schemes for these concept weights. In Section 2.5.1, we consider parameterization by structure. Conversely, in Section 2.5.2, we examine parameterization by concept.
A simple way to parameterize the importance weights  X  (  X  ) and  X  (  X , K Q ), is to make the assumption that the weights of all the concepts in the same structure are tied. Formally:
This assumption has the benefit of significantly reducing the number of free parameters in the retrieval model, thereby greatly simplifying the estimation process. Due to its sim-plicity, parameterization by structure is often used in the log-linear retrieval models [14, 27, 32].

Using parameterization by structure and the definitions of local and global factors in Section 2.4, we can explicitly rewrite the ranking function in Equation 2 as
The main drawback of parameterization by structure is the fact that it implies that all the concepts in the same structure are equally important for expressing the query in-tent. This implication is not always true, especially for more verbose, natural language queries, which may benefit from assigning varying concept weights [2, 4, 22].

Therefore, we may wish to remove the restriction im-posed in the previous section, and parameterize the concept weights based on the concepts themselves rather than their respective structures. Assigning a single weight to each con-cept is clearly infeasible, since the number of concepts is exponential in the size of the vocabulary. Therefore, we take a parameterization approach proposed in recent work on query modeling [2, 4, 5, 22, 35, 38], and represent each concept using a combination of importance features ,  X , de-scribed in Table 2. These importance features are based on concept frequencies, and can be efficiently computed and c ached, even for large-scale collections.

Using these importance features, we can explicitly rewrite the ranking function in Equation 2 as sc ( Q, D ) = X To estimate the free parameters  X  ( ) in Equation 6 and Equation 7, we rely on a large and growing body of literature on the learning to rank methods for information retrieval (see Liu [23] for a survey). As a base algorithm for parameter optimization we make use of the coordinate ascent (CA) algorithm proposed by Metzler and Croft [28].

The CA algorithm iteratively optimizes a target metric (in our case, retrieval metric such as MAP) by performing a series of one-dimensional line searches. It repeatedly cycles through each of the parameters  X  ( ), while holding all other parameters fixed. This process is performed iteratively over all parameters until the gain in the target metric is below a certain threshold.

We use the CA algorithm primarily for its simplicity, ef-ficiency and effectiveness, as demonstrated by the previous work [4, 5, 27]. However, any other learning to rank ap-proach that estimates the parameters for linear models such as RankSVM [15] or RankNet [7] can be adopted as well.
To ensure the scalability of our retrieval model, we com-pute the global factor (Equation 5) only for the top thou-sand documents retrieved by the local factors (Equation 4). Therefore, the setting of the importance weights  X  (  X  ) will affect the document ranking, which, in turn, will affect the choice of the highest-scoring passages and subsequently the setting of the importance weights  X  (  X , K Q ).
 Accordingly, we perform the optimization in two stages. We decompose sc ( Q, D ) into its local and global components. First, we optimize the local component (i.e., the weights  X  (  X  )). Then, we fix the weights of the local component, and optimize the global component (i.e., the weights  X  (  X , K Each of these optimizations is done using the standard CA algorithm.
In this paper we describe a general retrieval framework that models dependencies between arbitrary query concepts using a query hypergraph. It is important, therefore, to examine the connections between some of the well known retrieval models and query hypergraphs.
As Zobel and Mofat [46] point out, the majority of the standard bag-of-words models in IR can be generally ex-pressed by the following summation: where  X  ( t, Q ) and f ( t, D ) are some arbitrary functions (which may include normalization constants) defined over a query term t and its occurrences in the query and the document, respectively. Examples of such models include, among oth-ers, the query likelihood model [33], BM25 [34] and diver-gence from randomness [1].

Therefore, it is easy to show that all of these bag-of-words models can be straightforwardly modeled using a query hy-pergraph. To induce such a hypergraph, we simply need to define a single QT -structure  X  t = { t 1 , t 2 , . . . } , and a set of local edges
There is a long history of passage-based retrieval models in information retrieval [3, 8, 9, 16, 41, 40, 24]. These retrieval models are typically defined using vector space models [9, 16, 17] or language models [2, 24, 40], and employ a simple bag-of-words query representation. One of the most com-mon passage retrieval techniques is Max-Psg , which uses the passage with the highest score for document score derivation [3, 8, 16, 24, 41].

Max-Psg with the bag-of-words query representation is a special case of the general query hypergraph described in this paper. Our model combines the recent advances in re-trieval models that go beyond the bag-of-words query rep-resentations with passage retrieval models.

In addition, it is important to mention some recent work on query expansion [21] and query reformulation [43] us-ing passage-based evidence, which uses hierarchical graphi-cal representation of the query, similar to the one presented in this paper. This work is orthogonal to ours, as it uses passage evidence to augment the query with new concepts, rather than to model the query and the retrieval function. Combining this work on query expansion and reformulation with the retrieval models based on query hypergraphs is a promising direction for future work.
The advent of large-scale web corpora encouraged the de-velopment of retrieval models that employ phrases and prox-imity matches to model term dependencies [27, 29, 39, 14, 26, 32]. Most of these retrieval models take a log-linear form, and can be modeled using a query hypergraph with the structures described in Section 2.3.1, but without the inclusion of the global hyperedege.

Retrieval models that employ term dependencies usually resort to parameterization by structure [27, 14, 39, 32] (as described in Section 2.5.1). While this assumption signifi-cantly reduces the number of the free parameters in the re-trieval model, it may be detrimental to the performance of verbose natural language queries that may contain concepts of variable importance.

Recently, researchers started to examine retrieval models that employ parameterization by concept. To avoid learning a separate weight for each concept, these models represent a concept using a set of features [4, 5, 22, 35, 38]. This ap-proach significantly outperforms parameterization by struc-ture, especially for verbose natural language queries. Ac-cordingly, we also employ parameterization by concept in the retrieval with query hypergraphs (see Section 2.5.2).
To the best of our knowledge, there is very little prior work on retrieval with higher-order term dependencies (i.e., de-pendencies between arbitrary concepts rather than terms). O ne notable exception is an early work on generalized term dependencies by Yu et al. [44], which derives higher-order dependencies from pairwise term dependencies. However, the model proposed by Yu et al. [44] is infeasible for large-scale collections, since it requires an explicit computation of the probability of relevance for each individual query term, as well as pairs and triples of query terms.

A more recent retrieval model that attempts to incorpo-rate higher-order term dependencies is the Full Dependence ( FD ) variant of the Markov random field model proposed by Metzler and Croft [27]. The FD model, however, is only able to capture dependencies between multiple terms, rather than multiple concepts. For instance, it can model a dependency between the terms in the triple (dog, law, enforcement) , but it cannot model a dependency between the pair of concepts (dog,  X  X aw enforcement X ) .
All the empirical evaluation described in this section is implemented using Indri, an open-source search engine [37]. The structured query language implemented in Indri na-tively supports multiple types of concepts, including exact phrases and proximity matches, as well as customizable con-cept weighting schemes. As a result, Indri provides a flexible and convenient platform for evaluating the retrieval perfor-mance of query hypergraphs.

Table 4 presents a summary of the TREC corpora used in our experiments. The corpora vary both by type ( Ro-bust04 is a newswire collection, Gov2 is a crawl of the .gov domain, and ClueWeb-B is a set of pages with the highest crawl priority derived from a large web corpus), number of documents, and number of available topics, thereby provid-ing a diverse experimental setup for assessing the robustness of retrieval with query hypergraphs.
 Table 4: Summary of the TREC collections and top-i cs used for evaluation.

For the Robust04 and Gov2 collections, a standard Porter stemmer is used. In contrast, the ClueWeb-B collection is stemmed using the Krovetz stemmer, which is a X  X ight X  X tem-mer, as it makes use of inflectional linguistic morphology [19] and is especially suitable for web collections where aggressive stemming can decrease precision at top ranks [31]. Stopword removal is performed on both documents and queries using the standard INQUERY stopword list. The free parameter  X  in the concept matching function f (  X , X ) (see Equation 3) is set according to the default Indri configuration of the Dirich-let smoothing parameter.

Since query hypergraphs attempt to capture complex de-pendencies between query concepts, we apply them to the description portions of the TREC topics. TREC topic de-scriptions express the information needs behind the topics using verbose natural language queries. For instance, a de-scription portion of the TREC topic entitled  X  X ydrogen en-ergy X  is a question  X  X hat is the status of research on hy-drogen as a feasible energy source? X . As shown by previous work, these queries are more likely to benefit from complex representation and weighting schemes than their keyword counterparts [2, 20, 22].

In order to compute the global factor (Equation 5), we seg-ment each document into semi-overlapping passages of 150 words (i.e., the overlap between the passages is 75 words). As shown in previous work on passage retrieval [3, 9, 8, 16], this passage configuration leads to improved effectiveness on most TREC collections.

The optimization of the free parameters for all the base-lines and the proposed retrieval methods is done using three-fold cross-validation with mean average precision (MAP) as the target metric. In addition to MAP, we also report ERR@20, an early precision metric that was adopted as the official retrieval performance metric at the TREC 2010 Web Track [10]. The statistical significance of differences in the performance of the proposed retrieval methods with respect to their respective baselines is determined using a two-sided Fisher X  X  randomization test [36] with 25,000 permutations and  X  &lt; 0 . 05.
In this section, we compare the performance of the re-trieval with query hypergraphs to a number of state-of-the-art baselines that incorporate exact phrase matches, proxim-ities, and concept weight parameterization. These baselines do not, however, incorporate concept dependencies.
The query hypergraph representation, proposed in this pa-per, further extends each of these baselines with higher-order term dependencies via the inclusion of the global hyperedge and the corresponding global factor  X  ( K Q , D ) (see Equa-tion 5). In the remainder of this section, we examine the improvements in the retrieval performance (or lack thereof) of these baselines when they are extended with the query hypergraph representation.
Query likelihood [33] is a popular retrieval method that employs a bag-of-words query representation. In this sec-tion, we juxtapose the retrieval performance of the query likelihood baseline (denoted QL ) to the performance of a query hypergraph that includes a single QT -structure (struc-ture that contains the individual query terms as concepts). We denote this hypergraph representation H -QL . This juxta-position demonstrates the contribution of the global factor  X  ( K Q , D ) (see Equation 5) to the retrieval performance. Table 3(a) shows the comparison between the QL and the H -QL methods. The results in Table 3(a) demonstrate that the addition of the global factor  X  ( K Q , D ) into a bag-of-words representation significantly improves its retrieval ef-fectiveness in all the cases.

Note that the H -QL method is equivalent to the Max-Psg method proposed in the previous work [3, 8, 9, 16, 41], which ranks the documents in the collection by a combination of the document score and the score of its highest-scoring pas-sage. The improvements in retrieval performance shown in Table 3(a) are in line with the improvements attained by the Max-Psg method reported in this previous work.
Markov random fields for information retrieval (MRF-IR) is a state-of-the-art retrieval framework that incorporates QL 11.44 24.24 15.06 25.66 7.32 12.75
H -Q L 11.66 25.49 q ( +5.2%) 15.33 27.24 ( +6.2%) 7.63 13.07 q ( +2.5%) SD 11.76 25.62 15.73 27.97 7.58 12.99
H -S D 11.93 26.65 s ( +4.0%) 15.93 28.63 ( +2.4%) 7.78 13.08 ( +0.7%) FD 11.87 25.69 16.10 28.25 8.21 13.28
H -F D 11.94 26.50 f ( +3.1%) 16.02 28.70 ( +1.6%) 8.15 13.35 ( +0.5%) WSD 12.04 27.41 16.52 29.36 8.58 14.56
H -W SD 12.34 w 27.79 w ( +1.4%) 16.56 29.82 ( +1.6%) 8.31 14.68 ( +0.8%) the baseline. term dependencies. It was first proposed by Metzler and Croft [27], and was shown to be highly effective, especially for large-scale web collections.

Metzler and Croft propose two instantiations of the gen-eral MRF-IR framework. The first instantiation is the se-quential dependence model (denoted SD ), which incorporates only dependencies between adjacent query terms. The sec-ond instantiation is the full dependence model ( FD ), which incorporates dependencies between all query term subsets 5
The SD and FD baselines can be extended with a respec-tive hypergraph that includes three structures: QT , PR and PH (refer to Section 2.3.1 for the exact definitions of these structures). We denote these hypergraph representations H -SD and H -FD , respectively. These hypergraphs are parame-terized by structure, and their ranking functions are derived according to Equation 6.

Table 3(b) compares the performance of the sequential dependence baseline ( SD ) and its corresponding hypergraph H -SD . As evident from Table 3(b), in most cases (except for the ClueWeb-B collection) the retrieval effectiveness (in terms of MAP) is significantly improved by the hypergraph extension. However, these improvements are smaller than in the case of the QL baseline.

Similarly, Table 3(c) compares the performance of the full dependence baseline ( FD ) and its corresponding hypergraph H -FD . Comparing Table 3(b) and Table 3(c), we can see that in most cases the FD baseline slightly outperforms the SD baseline. However, these differences were not found to be statistically significant.

When comparing the performance of the FD baseline and its corresponding hypergraph H -FD , Table 3(c) demonstrates that the inclusion of the global factor results in an im-proved retrieval effectiveness (in terms of MAP) for all col-lections, and in statistically significant improvements for the Robust04 and Gov2 collections.

In addition, we can compare between the retrieval perfor-mance of the hypergraphs H -SD and H -FD . Similarly to the case of the baselines SD and FD , no statistically significant differences were found in the performance of these hyper-graphs. H -FD is slightly more effective for the ClueWeb-B and the Gov2 collections, while being slightly less effective for the Robust04 collection.
A major drawback of the SD and the FD baselines is that they use the parameterization-by-structure approach (see Section 2.5.1), which ties the importance weights  X  ( ) of all the concepts that belong to the same structure (i.e., all the terms, phrases and proximities get the same respective weights). This parameterization can be detrimental, espe-cially for longer, more verbose queries that may mix concepts of differing importance.

Recently, Bendersky et al. [4] proposed a weighted vari-ant of the sequential dependence mode (denoted WSD ) that overcomes this drawback. The concept weights in the WSD method are parameterized using a set of importance fea-tures, associated with each concept based on its respective structure, as described in Section 2.5.2.
 We extend the WSD baseline with a query hypergraph H -WSD . The H -WSD includes the global factor  X  ( K Q , D ), which is also parameterized by concept. The ranking function for the H -WSD hypergraph is presented in Equation 7.
Table 3(d) compares the retrieval performance of the WSD baseline and its corresponding hypergraph H -WSD . While the retrieval improvements that stem from this hypergraph ex-tensions are not as pronounced as in the cases of the QL , SD and F D baselines, the addition of the global factor to the WSD baseline still results in effectiveness gains for all the collec-tions and most of the metrics. For instance, for the Gov2 collection, the H -WSD method improves the performance (in terms of MAP) for 60% of the queries compared to the WSD baseline, while hurting only 30% of the queries. For 7% of the queries MAP is improved by more than 25%, while there is a 25% drop in performance for only 2% of the queries.
In addition to comparing each individual query hyper-graph model to its respective baseline, some general trends can be observed in Table 3. First, it is interesting to compare the relative differences in gains across the baselines, when the global factor is added. The gains are the largest for the QL baseline, which does not include any term dependencies, and decrease as more term dependencies are added by the SD and the FD baselines. As an example, for the Gov2 col-lection, the effectiveness gain as a result of the global factor inclusion decreases from 6.2% for the QL baseline to 1.6% for the FD baseline.

These diminishing returns demonstrate that there is some degree of overlap between the effect of term dependencies and higher-order term dependencies on the retrieval effec-tiveness. The overlap is not complete, however, since the addition of the global factor still has a statistically signifi-cant impact on the retrieval performance in most cases. This is true even for the FD baseline, which includes term depen-dencies between all query term pairs and triples.
Finally, we note that the parameterization of the ranking function by concept (as in the WSD baseline) (a) significantly improves the retrieval performance of the ranking function parameterized by structure (as in the SD baseline), and (b) further diminishes the gains obtained through the inclusion of the global factor. While H -WSD is the best-performing retrieval method (in terms of MAP) in Table 3, its aver-age effectiveness gain over the WSD baseline is only 1.3%. For comparison, the average effectiveness gain of the H -QL method over the QL baseline is 4.7%.
In this section we analyze the parameterization of the query hypergraph. We examine both parameterization-by-structure and parameterization-by-concept regimes, which are described in detail in Section 2.5.1 and Section 2.5.2, respectively.

Recall that the parameters of the query hypergraph are optimized using the coordinate ascent algorithm such that the ranking function is decomposed into local and global factors (see Section 2.5.3). In this section, due to the space constraints, we focus our attention on the resulting param-eterization for the Robust04 collection. We choose this col-lection, since it has the largest number of queries, and the learned parameterization is stable across all folds. However, it is important to note that the findings in this section hold for the other two collections as well.
Table 5 shows the hypergraph parameters for the local fac-tors (  X  (  X  )) and the global factor (  X  (  X  ,  X  Q )), averaged across folds, when the parameterization-by-structure approach is used (see Equation 6). These parameters correspond to the H -SD model, the results for which are shown in Table 3(b). Table 5: Query hypergraph parameterization by s tructure ( Robust04 collection). Table 6: Query hypergraph parameterization by c oncept ( Robust04 collection).

Note that both for the local and the global factors the weights assigned to the term structure ( QT ) are the highest, which is in line with other models that incorporate term dependencies [27]. This demonstrates that despite the im-portance of term dependencies, individual term occurrences are still the most important indicators of relevance.
In addition, in Table 5, the parameters of the local factors are weighted higher than the parameters of the global fac-tor. Recall that the global factor is defined over the highest-scoring passage in the document. Thus, the lower weight of the global factor parameters is in line with previous work, where passage evidence is typically weighted lower than the document evidence [3, 41, 16].

Finally, note the negative weight assigned to the proxim-ity ( PR ) structure in the global factor. While small, this negative weight is consistent across folds, as well as in the other collections. Intuitively, this negative weight indicates that in the highest-scoring passage of the relevant document we expect to encounter exact phrase concepts, rather than unordered proximity concepts.
Table 6 shows the hypergraph parameters for the local factors (  X  (  X ,  X  )) and the global factor (  X  (  X ,  X  ,  X  eraged across folds, when the parameterization-by-concept approach is used (see Equation 7). These parameters corre-spond to the H -WSD model, the results for which are shown in Table 3(d). For the convenience of presentation and to reduce weight sparsity, we combine the weights of the PH and PR structures in the PR + PH column.

Note that a priory constant importance feature AP gener-ally receives the highest weight. This is due to the fact that setting all the other feature weights to zero yields exactly the parameterization-by-structure approach.

Features such as document frequency ( DF ), collection fre-quency ( CF ) and Google frequency ( GF ) receive, as expected, negative weights in most cases. In contrast, the query fre-quency ( QF ) and the Wikipedia title frequency ( WF ) features get positive weights, which indicates that the appearance of the concept in page title or in a search query is positively correlated to the concept importance.
T he retrieval framework proposed in this paper represents a query by means of a hypergraph. In the query hypergraph, each vertex corresponds to a concept, and these concepts are grouped into disjoint structures. A hyperedge in the query hypergraph represents a concept dependency. We describe a principled derivation of a ranking function based on the factorization of the query hypergraph. We then propose two parameterization regimes for the derived ranking function, based on either structures or concepts.

The proposed retrieval framework exhibits three impor-tant characteristics. First, it models term dependencies as concepts. Second, it models dependencies between these concepts (i.e., higher-order term dependencies). Finally, it assigns weights to concepts and concept dependencies, pro-portionate to their importance for expressing the query in-tent. For verbose natural queries, the proposed retrieval framework significantly improves the retrieval effectiveness of several state-of-the-art retrieval methods that do not in-corporate higher-order term dependencies.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or rec-ommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
