 Most of the widely-used methods for documents retrieval utilize word-level in-formation. A representative algorithm is TF-IDF [8,9], it directly compares the word-count vectors of documents to measure their similarity, which only con-siders individual weighted words; Latent Semantic Analysis (LSA) [4] evaluates the similarity based on the latent semantic between the words of the documents, which takes into consideration semantic content. PLSA [6] and LDA [3] are two probabilistic models that characterize each document with a set of topics(words). Recently, a RBMs-based model [7], which c onsiders more semantic information, is designed to do semantic hashing on documents.

In this paper, we propose a new deep learning model that uses a 4-layer stacked autoencoders [2] with specific non-linear transformation functions to do semantic hashing [7] on documents. With representing documents by word-count vectors, our model takes the vectors as input and further learns compact binary codes to represent the semantic of documents. Further more, the codes learned by our model has an elegant property that semantically similar documents have similar codes, which facilitates the comparison and retrieval. We adopt pre-training and fine-tuning procedures to train our model.

The experimental results show that comp ared with some traditional methods, our model achieves better performance in terms of document retrieval, and the learned compact binary codes successfully encode the semantic of documents such that they can brilliantly represent the meanings of documents. In this section, we describe our model X  X  architecture and the techniques used to do pre-training and fine-tuning. We first preprocess all documents through removing stop words and performing word stemming, then each document is represented as a normalized 2,000-dimensional word-count vector, which is fed to our semantic hashing model.

The architecture of our stacked autoencoders model is 2,000-500-300-128. We gradually decrease the layer X  X  dimension to make the model learn more and more abstract features from the input, and by treating the final 128 dimensional binary codes as memory addresses, we can find semantically similar documents in a time that is independent of the size of document collection [7].
In pre-training process, we greedily train each layer (except the input layer) of the stacked autoencoders model as an individual autoencoder through re-constructing its input as a target. We use sigmoid function as the non-liner activation function for the encoder and decoder parts of the first auto-encoder. The sigmoid function is a bounded differentiable function defined for all real values. It maps all input values to the interval of (0,1), which is much similar to the range of our normalized input values. Thus, this function can appropriately reconstruct the input vectors.

For any node j on the hidden layer of the first autoencoder, the learned feature h j is: where  X  ( x )=1 / (1 + e  X  x ), b j is the bias on node j , w jk is the weight between node k (on previous layer) and node j ,and v k is the output value of node k . Then for any node i out the output layer, the value computed on it is: where  X  is the same sigmoid function as  X  . The objective of the autoencoder is to use output y reconstruct input v by minimizing the loss function J w.r.t all training data S (consists of m examples):
We apply stochastic gradient descent (SGD) [1] method to update parameters (W, b) to optimize the autoencoder.

Using the same method, we further pre-train the second and third autoen-coders, and this makes our model find a good region of the parameter space, starting from this region, we then fine-tune the parameters to produce a much better model [5].

We add a prediction layer on the top of the network to do further supervised fine-tuning. For the data which is labeled with category or other classification information, the predication layer aims to predicate the input data X  X  label and we use softmax as the activation function. For any node i on the predication layer, the computed value is: Where a is the sum of all values coming into node i . The output vector y can be regarded as a probability distribution over the labels w.r.t that input data. The target t is a binary vector, wherein only one element, which corresponds to the true category, is set to 1. We use cross-entropy error as loss function J :
Using gradient descent method, we iteratively train the model until we find a good local minima of the loss function. We first do information retrieval task on the 20Newsgroups data set. Fig. 1 shows the precision-recall curves achi eved by our model and the TF-IDF, LSA and RMBs-based model. We can see that, our binary codes (auto-encoder based 128d-codes) get the best accuracy at all recall levels. While LSA 128d-codes get the worst result, for RBMs based method and TF-IDF method, they perform better than LSA and achieve similar retrieval accuracy.

We visualize the 128-dimensional codes of all documents from 6 categories obtained by our model (AE-based mod el) and the RMBs-based model. Fig. 2 and Fig. 3 show that both codes preserve the category structure of documents, but obviously, our learned codes are much better in capturing the structure in-formation so that it significantly cluster the documents into different categories.
We then encode 0.54M Wikipedia articles into the binary codes space. Due to the lack of specific label information of Wikipedia articles, we use the prediction layer to reconstruct the input vector as the objective. Table 1 shows the 5 nearest neighbors of 5 randomly chosen queries (i.e. Wikipedia items). The semantic of the neighbor items are very closely related to the queries X  semantic, and this illustrates that our binary codes indeed capture the semantic information of Wikipedia articles and facilitate the fast retrieval of similar Wikipedia items. We proposed a new deep learning model using stacked autoencoders to encode document semantic into binary codes space, where the codes have a property that semantically similar documents have similar codes, and thus, we can simply measure the similarity according to the Hamming distance between two docu-ments X  codes. Future work would analyze this model on short text understanding and also concentrate on the mechanism to enrich short text using a probabilistic knowledge, known as Probase [10].
 Acknowledgement. The first author is supported by NSFC61232006.

