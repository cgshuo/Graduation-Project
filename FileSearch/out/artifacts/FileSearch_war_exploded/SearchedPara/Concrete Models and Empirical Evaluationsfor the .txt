 Google DeepMind Queen Mary University of London (2008) and Coecke, Sadrzadeh, and Clark (2010) provides a solution by unifying a categorial grammar and a distributional model of meaning. It takes into account syntactic relations during semantic vector composition operations. But the setting is abstract: It has not been evaluated on empirical data and applied to any language tasks. We generate concrete models the abstract parameters using empirical data. We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task. Our results show the implementation of this experiments. 1 1. Introduction
The distributional approach to the semantic modeling of natural language, inspired by the notion X  X resented by Firth (1957) and Harris (1968) X  X hat the meaning of a of semantic representation. It draws from the frequent use of vector-based document models in information retrieval, modeling the meaning of words as vectors based on the distribution of co-occurring terms within the context of a word.
 distributional semantic models (DSMs) are used for a variety of NLP tasks, from automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay marking (Landauer and Dumais 1997). The broader connection to information retrieval and its applications is also discussed by Manning, Raghavan, and Sch  X  utze (2011). The success of DSMs in essentially word-based tasks such as thesaurus extraction and con-struction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can be applied to NLP and information retrieval ( IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors. However, the problem of compositionality in DSMs X  X f how to go from word to sentence and beyond X  X as proved to be non-trivial.

Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) reconciles nature of formal semantic models. This framework is abstract; its theoretical predictions have not been evaluated on real data, and its applications to empirical natural language processing tasks have not been studied.
 which fill this gap in the DisCoCat literature; in it, we develop a concrete model and an unsupervised learning algorithm to instant iate the abstract vectors, linear maps, and language processing experiments and data sets and implement our algorithm on large equations; and we evaluate the model on these experiments and compare the results with other competing unsupervised models. F urthermore, we provide a linear algebraic analysis of the algorithm of Grefenstette and Sadrzadeh (2011a) and present an in-depth study of the better performance of the method of Grefenstette and Sadrzadeh (2011b). compositional distributional models. We br iefly introduce two approaches to semantic modeling: formal semantic models and distrib utional semantic models. We discuss their differences, and relative advantages and disadvantages. We then present and critique various approaches to bridging the gap betw een these models, and their limitations. In Section 3, we summarize the categorical co mpositional distributional framework of
Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) and pro-vide the theoretical background necessary to understand it; we also sketch the road map of the literature leading to the development of this setting and outline the contributions framework, and introduce learning algorithms used to build semantic representations evaluate this implementation against other un supervised distributional compositional models. Finally, in Section 6 we discuss these results, and posit future directions for this research area. 2. Background
Compositional formal semantic models represent words as parts of logical expressions, and compose them according to grammatical structure. They stem from classical ideas sentence is a function of the meaning of its parts (Frege 1892). These models relate to well-known and robust logical formalisms, hence offering a scalable theory of meaning that can be used to reason about language using logical tools of proof and inference.
Distributional models are a more recent approach to semantic modeling, representing 72 the meaning of words as vectors learned emp irically from corpora. They have found their way into real-world applications such as thesaurus extraction (Grefenstette 1994;
Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have connections to semantically motivated information retrieval (Manning, Raghavan, and
Sch  X  utze 2011). This two-sortedness of defining properties of meaning:  X  X ogical form X  versus  X  X ontextual use, X  has left the quest for  X  X hat is the foundational structure of language X  X ven more of a challenge.
 natural language semantics, and providing a non-exhaustive list of some approaches to compositional distributional semantics. For a more complete review of the topic, we encourage the reader to consult Turney (2012) or Clark (2013). 2.1 Montague Semantics
Formal semantic models provide methods for translating sentences of natural language reason about them (Alshawi 1992).
 words must interact with one another. In formal semanti cs, this further interaction is represented as a function derived from the gr ammatical structure of the sentence. Such models consist of a pairing of syntactic analysis rules (in the form of a grammar) with semantic interpretation rules, as exemplifi ed by the simple model presented on the left of Figure 1.
 formulae, which can be combined with one an other to form well-formed logical expres-sions. The function | X  X  : L  X  M maps elements of the lexicon (e.g., predicates, relations, dom ain objects) in the logical model typically just logical atoms, whereas adjec tives and verbs and other relational words are interpreted as predicates and relations. The parse of a sentence such as  X  X ats like milk, X  represented here as a binarized parse tree, is used to produce its semantic in-terpretation by substituting semantic representations for their grammatical constituents and applying  X  -reduction where needed. Such a derivation is shown on the right of Figure 1.
 meaning of a sentence if given a logical model and domain, as well as verify whether or not one sentence entails another according to the rules of logical consequence and deduction.
 topic of expressions beyond their truth-conditions and which models satisfy these truth conditions. Hence, formal semantic approaches to modeling language meaning do not perform well on language tasks where the notion of similarity is not strictly based on truth conditions, such as document retrieval, topic classification, and so forth. Further-more, an underlying domain of objects and a valuation function must be provided, language using such a model, rather than just use it. 2.2 Distributional Semantics butional hypothesis of Harris (1968), who postulated that the meaning of a word was keeps. X  This view of semantics has furthermore been associated (Grefenstette 2009;
Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use.
 at what other words occur with it within a certain context , and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient because vectors are a familiar str ucture with a rich set of ways of computing vector distance, allowing us to experiment wi th different word similarity metrics. The geometric nature of this representation entails that we can not only compare individual words X  meanings with various levels of granularity (e.g., we might, for example, be able to show that cats are closer to kittens than to dogs, but that all three are mutually closer than cats and steam engines), but also apply m ethods frequently called upon in IR tasks, such as those described by Manning, Raghavan, and Sch  X  utze (2011), to group concepts by topic, sentiment, and so on.
 vectors of which are dictated by the contex t. In simple models, the basis vectors will the inner product of any one basis vector with another [other than itself] is zero). The vectors: where { is the weight associated with basis vector word n i associated with basis vector each occurrence of the word for which we are constructing the vector. This count is then typically adjusted according to a weighting scheme (e.g., TF-IDF). The  X  X ontext X  of a word can be something as simple as the oth er words occurring in the same sentence as 74 the word or within k words of it, or something more complex, such as using dependency relations (Pad  X  o and Lapata 2007) or other syntactic features.
 cosine measure, which is the sum of the product of the basis weights of the vectors: where c a i and c b i are the basis weights for weighting scheme.
 semantics are invited to consult Curran (2004), which contains an extensive overview of implementation options for distributional models of word meaning. 2.3 Compositionality and Vector Space Models have seen that DSMs are a rich and tractable way of learning word meaning from a corpus, and obtaining a measure of semantic similarity of words or groups of words.
However, it should be fairly obvious tha t the same method cannot be applied to sen-tences, whereby the meaning of a sentence w ould be given by the distribution of other sentences with which it occurs.
 substantial and informative distributions cannot be created in this manner. More im-portantly, human ability to understand new sentences is a compositional mechanism:
We understand sentences we have never seen before because we can generate sentence meaning from the words used, and how they are put into relation .Togofromword vectors to sentence vectors, we must provide a composition operation allowing us to con-struct a sentence vector from a collection of word vectors. In this section, we will discuss several approaches to solving this problem, t heir advantages, and their limitations. 2.3.1 Additive Models. The simplest composition operation that comes to mind is straight-forward vector addition, such that:
Conceptually speaking, if we view word vect ors as semantic information distributed crude, this approach is computationally cheap, and appears sufficient for certain NLP tasks: Landauer and Dumais (1997) show it to be sufficient for automated essay marking tasks, and Grefenstette (2009) shows it to perform better than a collection of other simple similarity metrics for summarization, sent ence paraphrase, and document paraphrase detection tasks.
 vector addition is commutative, therefore,  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X 
Wine drank John, and thus vector addition ign ores syntactic structure completely; and second, vector addition sums the information contained in the vectors, effectively jum-bling the meaning of words together as sentence length grows.
 leads them to equate the representation of se ntences with patently different meanings.
Mitchell and Lapata (2008) propose to add some degree of syntactic sensitivity X  X amely, accounting for word order X  X y weighting word vectors according to their order of appearance in a sentence as follows: where  X  ,  X   X  R .Consequently, not have the same representation as syntactic complexity. Guevara (2010) suggests using machine-learning methods such as partial least squares regression to determ ine the weights empirically, but states that this approach enjoys little success beyond minor composition such as adjective-noun or noun-verb composition, and that there i s a dearth of metrics by which to evaluate such machine learning X  X ased systems, s tunting their growth and development. we construct sentences, rather than decrease in ambiguity as we would expect from giving words a context; and for this reason Mitchell and Lapata (2008) suggest replacing additive models with multiplicative models a s discussed in Section 2.3.2, or combining them with multiplicative models to form mix ture models as discussed in Section 2.3.3. 2.3.2 Multiplicative Models. The multiplicative model of Mitchell and Lapata (2008) is an attempt to solve the ambiguity problem discussed in Section 2.3.1 and provide implicit disambiguation during compositi on. The composition operation proposed is weighted sum of their basis vectors, and t he weight of the basis vectors of the com-posed vector is the product of the we ights of the original vectors; for  X   X  b =
Such multiplicative models are shown by Mitchell and Lapata (2008) to perform better at verb disambiguation tasks than additive models for noun-verb composition, against a baseline set by the original verb vectors. The experiment they use to show this will also serve to evaluate our own models, and form the basis for further experiments, as discussed in Section 5.

First, component-wise multiplication is still commutative and hence word order is not accounted for; second, rather than  X  X iluting  X  information during large compositions 76 and creating ambiguity, it may remove too much through the  X  X iltering X  effect of component-wise multiplication.
 additive models, because both scalar multip lication and component-wise multiplication are commutative and hence  X  taken into account using scalar weights.
 well with sentence length, we look at the str ucture of component-wise multiplication again: for any composition, the number of non-zero basis weights of the produced vector is less than or equal to the number of non-zero basis weights of the original vectors: At each composition step information is filtered out (or preserved, but never increased).
Hence, as the number of vectors to be co mposed grows, the number of non-zero basis weights of the product vector stays the same or X  X ore realistically X  X ecreases.
Therefore, for any composition of the form for any i , multiplicative models alone are not apt as a single mode of composition beyond noun-verb (or adjective-noun) composition operations.
 would be to introduce some smoothing factor s  X  R + such that filtered out. Seeing how the problem of syntact ic insensitivity still stands in the way of full-blown compositionality for multiplicat ive models, we leave it to those interested in salvaging purely multiplicative models to determine whether some suitable value of s can be determined. 2.3.3 Mixture Models. The problems faced by multiplicative models presented in Sec-tion 2.3.2 are acknowledged in passing by Mitchell and Lapata (2008), who propose mixing additive and multiplicative models in the hope of leveraging the advantage of each while doing away with their pitfalls. This is simply expressed as the weighted sum of additive and multiplicative models: where  X  ,  X  ,and  X  are predetermined scalar weights.
 weights are to be obtained still needs to be determined. Mitchell and Lapata (2008) con-mixture models is that the lack of scalar weights removes the need to optimize the scalar weights for particular tasks (at the cost of not accounting for syntactic structure), and avoids the methodological concerns accompanying this requirement.
 to more syntactically rich expressions must be addressed. Using scalar weights to sentence vectors for sentences such as The dog bit the man and The man was bitten by weights could be given to particular syntactic classes (such as nouns) to introduce a more complex syntactic element into vector c omposition, but it is clear that this alone is not a solution, as the weight for nouns dog and man would be the same, allowing for the same commutative degeneracy obse rved in non-weighted additive models, in which systems accounting for both word order and syntactic roles may be a solution, but it is not only ad hoc but also arguably only partially reflects the syntactic structure of the sentence.
 mixture models perform better at verb disambiguation tasks than additive models and weighted additive models, they perform equi valently to purely multiplicative models with the added burden of requiring parame tric optimization of the scalar weights. tive models while avoiding their problems, they are only partly successful in achieving the latter goal, and demonstrably do little better in achieving the former. 2.3.4 Tensor-Based Models. From Sections 2.3.1 X 2.3.3 we observe that the need for incor-porating syntactic information into DSMs to achieve true compositionality is pressing, if only to develop a non-commutative compos ition operation that can take into account word order without the need for adhoc weighting schemes, and hopefully richer syn-tactic information as well.

Legendre 2006) to use linear algebraic tensors as a composition operation solves the problem of finding non-commutative vector co mposition operators. The composition of two vectors is their tensor product, sometimes called the kronecker product when ap-plied to vectors rather than vector spaces. For we have:
The composition operation takes the or iginal vectors and maps them to a vector in a
Here the second instance of  X  is not a recursive application of the kronecker product, but rather the pairing of basis elements of V and W to form a basis element of V
The shared notation and occasional conflation of kronecker and tensor products may seem confusing, but is fairly standard in multilinear algebra.
 not live in the same spaces but can be composed nonetheless. This allows us to repre-sent vectors for different word classes (topics , syntactic roles, etc.) in different spaces with different bases, which was not possible under additive or multiplicative models.
Second, because the product vector lives in a larger space, we obtain the intuitive notion that the information of the whole is richer and more complex than the information of the parts.

Dimensionality Problems. However, as observed and commented upon by Smolensky based models. The first is computational: The size of the product vector space is the product of the size of the original vector sp aces. If we assume that all words live in the 78 same space N of dimensionality dim ( N ), then the dimensionality of an n -word sentence there are lexemes in our vocabulary (e.g., approximately 170k in English of our sentence vectors quickly reaches magnitudes for which vector comparison (or even storage) are computationally intractable. 3 Even if, as most DSM implementations do, we restrict the basis vectors of word semantic spaces to the k (e.g., k sentence length, and the implementation problems remain.
 spaces, and if we assign different vector spaces to different word types (e.g., syntactic classes), then sentences of different syntactic structure live in different vector spaces, and hence cannot be compared directly using inner product or cosine measures, leaving us with no obvious mode of semantic comparison for sentence vectors. If any model reducing the dimensionality of product vectors to some common vector space so that they may be directly compared.
 general are the holographic reduced representations proposed by Plate (1991). The and one of the original vectors produces a no isy version of the other original vector.
The noisy vector can be used to recover the c lean original vector by comparing it with a predefined set of candidates (e.g., the set of word vectors if our original vectors are word meanings). Traces can be summed to form new traces effectively containing several vec-tor pairs from which original vectors can be recovered. Using this encoding/decoding mechanism, the tensor product of sets of vectors can be encoded in a space of smaller dimensionality, and then recovered for computation without ever having to fully repre-sent or store the full tensor product, as discussed by Widdows (2008).
 to store information for sentences of variab le word length without having to directly represent the tensored sentence vector, setting an upper bound to the number of vectors that can be composed in this manner limits the length of the sentences we can represent compositionally using this method.
 distributed such that the mean Euclidean length of each vector is 1. Such conditions are unlikely to be met in word semantic vectors obtained from a corpus; and as the duced representations are not prima facie usable for compositional DSMs, although it is important to note that Widdows (2008) considers possible application areas where they may be of use, although once again these mostly involve noun-verb and adjective-noun compositionality rather than full blown sentence vector construction. We retain from Plate (1991) the importance of finding methods by which to project the tensored sentence vectors into a common space for direct comparison, as will be discussed further in Section 3.

Syntactic Expressivity. An additional problem of a more conceptual nature is that using the tensor product as a composition operation simply preserves word order. As we discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We need to have some means by which to incorpor ate syntactic analysis into composition operations.
 butions for word vectors are collected from the corpus, thereby embedding syntac-
Smolensky, who used sums of pairs of vector representations and their roles, obtained by taking their tensor products, to obtain a vector representation for a compound. The application of these ideas to DSMs was studied by Clark and Pulman (2007), who representations of words as the vectors. For example, in the sentence Simon loves red wine . Hence, from the dependency tree with loves as root node, its subject and object as children, and their adjectival descriptors (if any) as their children, we read the following structure: innerproductsoftensorproducts: having to actually represent the tensored sentence vector:
This example shows that this formalism allo ws for sentence comparison of sentences with identical dependency trees to be broken down to term-to-term comparison without the need for the tensor products to ever be computed or stored, reducing computation to inner product calculations.
 works well in the given example, this model suffers from the same problems as the dimensionality and thus cannot be directly compared. Hence we cannot use this to measure the similarity between even small variations in sentence structure, such as the pair  X  X imon likes red wine X  and  X  X imon likes wine. X  representations discussed in Section 2.3.4 is applied differently in the structured vector 80 space model presented by Erk and Pad  X  o (2008). They propose to represent word mean-ings not as simple vectors, but as triplets: where v is the word vector, constructed as in any other DSM, R and R preferences, and take the form of R  X  D maps where R relations, and D is the set of word vectors. Selectio nal preferences are used to encode the lemmas that w is typically the parent of in the dependency trees of the corpus in the case of R , and typically the child of in the case of R  X 
Let a = ( v a , R a , R  X  1 a )and b = ( v b , R b , R  X  1 dependency relation linking a to b . The vector update procedure is as follows: where a , b are the updated word meanings, and is whichever vector composition are being composed with expects to bear relation r to, and this relation between the composed words a and b is considered to be used and hence removed from the domain of the selectional preference functions used in composition.
 disambiguation mechanism discussed by Mitchell and Lapata (2008) in that the com-bination of words filters the meaning of t he original vectors that may be ambiguous (e.g., if we have one vector for bank ); but contrary to Mitchell and Lapata (2008), the information of the original vectors is modifi ed but essentially preserved, allowing for further combination with other terms, rather than directly producing a joint vector for the composed words. The added fact that R and R  X  1 are partial functions associated with specific lemmas forces grammaticality during composition, since if a holds a dependency relation r to b which it never expects to hold (for example a verb having as its subject another verb, rather than the reverse), then R and the update fails. However, there are some problems with this approach if our goal is true compositionality.
 because of the update of R and R  X  1 . For example, we expect that an adjective composed with a noun produces something like a noun in order to be further composed with a verb or even another adjective. However, here, because the relation adj would be removed from R  X  1 b for some noun b composed with an adjective a , this new representation b would not have the properties of a noun in th at it would no longer expect composition with an adjective, rendering representat ions of simple expressions like  X  X he new red car  X  impossible. Of course, we could remove the update of the selectional preference functions from the compositional mechanism, but then we would lose this attractive feature of grammaticality enforcement through the partial functionality of R and R is expected during composition, rather than actually provide a full blown compositional model. The inability of this system to provide a novel mechanism by which to obtain a joint vector for two composed lemmas X  X hereby building towards sentence vectors X  entails that this system provides no means by which to obtain semantic representations of larger syntactic structures that can be compared by inner product or cosine measure compositional models presented in Sections 2.3.1 X 2.3.3 to produce sentence vectors, but whereas some syntactic sensitivity would have been obtained, the word ordering and other problems of the aforementioned models would still remain, and little progress would have been made towards true compositionality.
 information obtained from syntactic dependency relations is important for proper dis-ambiguation, and that having some mechanism by which the grammaticality of the expression being composed is a precondition for its composition is a desirable feature for any compositional mechanism. 2.4 Matrix-Based Compositionality matrix-based models.
 Generic Additive Model. The first is the Generic Additive Model of Zanzotto et al. (2010).
In this model, rather than multiplying lexical vectors by fixed parameters before adding them to form the representation of their combination, they are instead the arguments of matrix multiplication by square matrices A and B :
Here, A and B represent the added information provided by putting two words into relation.
 (  X   X  a , of the combination of labeled data for linear regression to be performed. Zanzotto et al. (2010) suggest several sources for this labeled data, such as dictionary definitions and word etymologies. act as linear maps on the vectors they take as  X  X rguments, X  and thus can encode more subtle syntactic or semantic relations. Howev er, this model treats all word combinations as the same operation X  X or example, treating the combination of an adjective with its argument and a verb with its subject as the same sort of composition. Because of the diverse ways there are of training such sup ervised models, we leave it to those who wish to further develop this specific line of research to perform such evaluations. Adjective Matrices. The second approach is the matrix-composition model of Baroni and
Zamparelli (2010), which they develop only for the case of adjective-noun composition, although their approach can seamlessly be used for any other predicate-argument com-position. Contrary to most of the earlier approaches proposed, which aim to combine two lexical vectors to form a lexical vector for their combination, Baroni and Zamparelli suggest giving different semantic representa tions to different types, or more specifically to adjectives and nouns. 82 a view of adjectives that is more in line with formal semantics than with distributional semantics, they model adjectives as linear m aps taking lexical vectors as input and pro-ducing lexical vectors as output. Such linear maps can be encoded as square matrices, and applied to their arguments by matri x multiplication. Concretely, let M matrix encoding the adjective X  X  linear map, and a noun; their combination is simply by linear regression over a set of pairs ( semantic vectors for the arguments of the adjective in a corpus, and vector corresponding to the expected output of the composition of the adjective with that noun.
 are needed. However, Baroni and Zamparelli (2010) work around this constraint by noun compound as a single token, and learning its vector using the same distributional learning methods they used to learn the vectors for nouns. This same approach can be extended to other unary relations without change and, using the general framework reader to Grefenstette et al. (2013).
 Recursive Matrix-Vector Model. The third approach is the recently developed Recursive
Matrix-Vector Model (MV-RNN) of Socher et al. (2012), which claims the two matrix-based models described here as special cases. In MV-RNN, words are represented as a pairing of a lexical semantic vector given the parse of a sentence in the form of a binarized tree, the semantic representation (  X   X  c , C ) of each non-terminal node in the tree is produced by performing the following two operations on its children ( child to the vector of the other, and vice versa , and then projecting both of the products back into the same vector space as the ch ild vectors using a projection matrix W ,which must also be learned: into the same space, using a projection matrix W M , which must also be learned:
The pairing ( of the phrase falling under the scope of the segment of the parse tree below that node. relational words differently through their op eration matrices, and allowing for recursive composition, as the output of each composition operation is of the same type of object as its inputs. This approach is highly genera l and has excellent coverage of different syntactic types, while leaving much room for parametric optimization.
 subsequently is that composition in MV-RNN is always a binary operation; for exam-subject. The framework we discuss in this article allows for the construction of larger representations for relations of larger ariti es, permitting the simultaneous composition of a verb with its subject and object. Whether or not this theoretical difference leads to source of learning error during training, which is easy to measure in the case of label paraphrase detection where no objective label exists. A direct comparison to MV-RNN methods within the context of experiments similar to those presented in this article has been produced by Blacoe and Lapata (2012), showing that simple operations perform on par with the earlier complex deep learning architectures produced by Socher and colleagues; we leave direct comparisons to future work. Early work has shown that the addition of a hidden layer with non-linearities to these simple models will improve the results. 2.5 Some Other Approaches to Distributional Semantics
Domains and Functions. In recent work, Turney (2012) suggests modeling word repre-sentations not as a single semantic vector, but as a pair of vectors: one containing the information of the word relative to its domain (the other words that are ontologically similar), and another containing information relating to its function .Theformervector is learned by looking at what nouns a word co-occurs with, and the latter is learned by looking at what verb-base d patterns the word occurs in. Similarity between sets of words is not determined by a single similari ty function, but rather through a combi-nation of comparisons of the domain components of words X  representations with the function components of the words X  representations. Such combinations are designed composition of the sort we explore in this art icle, Turney shows that similarity measures can be designed for tasks similar to those pres ented here. The particular limitation of his approach, which Turney discusses, is that similarity measures must be specified for each task, whereas most of the compositional models described herein produce repre-sentations that can be compared in a task-independent manner (e.g., through cosine similarity). Nonetheless, this approach is i nnovative, and will merit further attention in future work in this area.
 Language as Algebra. Atheoreticalmodelof meaning as context has been proposed in
Clarke (2009, 2012). In that model, the meaning of any string of words is a vector built 84 distributional models from words to strings of words: in that model, one builds vectors for strings of words in exactly the same way as one does for words. The main problem, appear repeatedly in a document, but strings of words, especially for longer strings, rarely do so; for instance, it hardly happens that an exact same sentence appears more than once in a document. To overcome this problem, the model is based on the hypo-thetical concept of an infinite corpus, an assu mption that prevents it from being applied to real-world corpora and experimented within natural language processing tasks. On the positive side, the model provides a theoretical study of the abstract properties of a general bilinear associative composition op erator; in particular, it is shown that this operator encompasses other composition ope rators, such as addition, multiplication, and even tensor product. 3. DisCoCat
In Section 2, we discussed lexical DSMs and t he problems faced by attempts to provide a vector composition operation that would a llow us to form distributional sentence rep-resentations as a function of word meaning. In this section, we will present an existing formalism aimed at solving this composition ality problem, as well as the mathematical background required to understand it and further extensions, building on the features and failures of previously discussed attempts at syntactically sensitive compositionality. propose adapting a category theoretic model , inspired by the categorical compositional vector space model of quantum protocols (Abramsky and Coecke 2004), to the task grammars X  X  categorial grammar X  X s given categorical semantics in order to be repre-sented as a compact closed category P (a concept explained subsequently), the objects of which are syntactic types and the morphisms of which are the reductions forming the basis of syntactic analysis. This syntactic category is then mapped onto the semantic compact closed category FVect of finite dimensional vector spaces and linear maps. The mapping is done in the product category FVect  X  P via the following procedure.
Each syntactic type is interpreted as a vector space in which semantic vectors for words with that particular syntactic type live; th e reductions between the syntactic types are interpreted as linear maps between the interpreted vector spaces of the syntactic types. way different mathematical formalisms that have similar structures, even if the original formalisms belong in different branches of mathematics. In this context, it has enabled us to relate syntactic types and reductions to vector spaces and linear maps and obtain a mechanism by which syntactic analysis guides semantic composition operations . syntactically driven semantic composition i n the form of inner-products provide the im-plicit disambiguation features of the compositional models of Erk and Pad  X  o (2008) and
Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). defined over Boolean values to obtain gramma tically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008).
Some logical operators can be emulated in th is setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be defined.
 notions of pregroup grammars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars
Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras because of their well-studied algebraic struc ture, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; t his logic is sometimes referred to as Bi-Compact
Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of other categorial grammars, for instance, those of the combinatory categorial grammar (CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist of atomic grammatical types that combin e to form compound types. A series of CCG-like application rules allow for type-reduction s, forming the basis of syntactic analysis.
As our first step, we show how this syntactic analysis formalism works by presenting an introduction to pregroup algebras.

Pregroups. A pregroup is an algebraic structure of the form ( P , elements are defined as follows: 86 cd and call this a reduction , omitting the identity wherever it might appear. Monoid multiplication is associative, so parentheses may be added or removed for notational clarity without changing the meaning of the expression as long as they are not directly under the scope of an adjoint operator.

We note here that the reduction order is not always unique, as we could have reduced as of reductions a  X  ...  X  b we may simply write a  X  b (in virtue of the transitivity of partial ordering relations). Hence in our giv en example, we can express both reduction paths as aa r bc l c  X  b .

Pregroups and Syntactic Analysis. Pregroups are used for grammatical analysis by freely generating the set P of a pregroup from the basic syntactic types n , s , n stands for the type of both a noun and a noun phrase and s for that of a sentence.
The conflation of nouns and noun phrases suggested here is done to keep the work discussed in this article as simple as possible, but we could of course model them as different types in a more sophisticated version of this pregroup grammar. As in any categorial grammar, words of the lexicon are assigned one or more possible types (corresponding to different syntactic roles) in a predefined type dictionary ,andthe grammaticality of a sentence is verified by d emonstrating the existence of a reduction from the combination of the types of words w ithin the sentence to the sentence type s . the transitive verbs will have the compound type n r sn l a transitive verb that it is the type of a word which  X  X xpects X  a noun phrase on its left and a noun phrase on its right, in order to produce a sentence. A sample reduction of
John loves cake with John and cake being noun phrases of type n and loves being a verb of type n r sn l is as follows:
We see that the transitive verb has combined with the subject and object to reduce to a sentence. Because the combination of the types of the words in the string John loves cake reduces to s , we say that this string of words is a grammatical sentence. As for more examples, we recall that intransitive verbs can be given the type n would be analyzed in terms of the reduction n ( n r s )  X  type nn l such that red round rubber ball would be analyzed by ( nn so on and so forth for other syntactic classes.
 with a richer set of types than presented here. This grammar is hand-constructed and iteratively extended by expanding the typ e assignments as more sophisticated gram-such types of assignments for larger fragments (e.g., as seen in empirical data). Pregroup grammars have been prov en to be learnable by B  X  echet, Foret, and Tellier (2007), who also discuss the difficulty of this task and the nontractability of the procedure. Because of these constraints and lack of a workable pregroup parser, the pregroup grammars we will use in our categorical formalism are derived from CCG types, as we explain in the following.

Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with other categorial grammars such as CCG, do not yet have a large set of tools for parsing available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier X  X  CCG lexicon and treebank (Hockenmaier 2003;
Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran X  X  parser, or Hockenmaier X  X 
CCGs, such as those used in the aforementioned tools, are strongly context-free and thus expressively equivalent to pregroup grammars. In order to be able to apply the parsing tools for CCGs to our setting, we use a translation mechanism from CCG types to pre-group types based on the Lambek-calculus-to-pregroup-grammar translation originally presented in Lambek (1999). In this mechanism, each atomic CCG type X is assigned a unique pregroup type x ;forany X / Y in CCG we have xy l in the pregroup grammar; and for any X \ Y in CCG we have y r x in pregroup grammar. Therefore, by assigning NP to n and S to s we could, for example, translate the CCG transitive verb type ( S the pregroup type n r sn l , which corresponds to the pregroup type we used for transitive verbs in Section 3.1. Wherever type replacement (e.g., N  X  set an ordering relation in the pregroup grammar (e.g.,  X  n type associated with N ). Because forward and backward slash  X  X perators X  in CCG are not associative whereas monoid multiplication in pregroups is, it is evident that some information is lost during the translation p rocess. But because the translation we need is one-way, we may ignore this problem and use CCG parsing tools to obtain pregroup parses. Another concern lies with CCG X  X  cro ssed composition and substitution rules. as pregroups are a simplification of the Lambek Calculus and these rules did not hold in the Lambek Calculus either, as shown in Moortgat (1997), for example. However, for the phenomena modeled in this paper, the CCG rules without the backward cross rules will suffice. In general for the case of English, one can avoid the use of these rules by overloading the lexicon and using addition al categories. To deal with languages that have cross dependancies, such as Dutch, various solutions have been suggested (e.g., see Genkin, Francez, and Kaminski 2010; Preller 2010). 3.2 Categories
Category theory is a branch of pure mathema tics that allows for a general and uniform formulation of various different mathematical formalisms in terms of their main struc-tural properties using a few abstract concepts such as objects, arrows, and combinations and compositions of these. This uniform co nceptual language allows for derivation of new properties of existing formalisms an d for relating these to properties of other formalisms, if they bear similar categorical representation. In this function, it has been 88 at the center of recent work in unifying two or thogonal models of meaning, a qualitative categorial grammar model and a quantitative distributional model (Clark, Coecke, and
Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categori-cal structures at work here were inspired by the ones used in the foundations of physics and the modeling of quantum information flow, as presented in Abramsky and Coecke (2004), where they relate the logical structure of quantum protocols to their state-based vector spaces data. The connection between the mathematics used for this branch of several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). categories, and compact closed categorie s. The focus will be on defining enough ba-and the modeling of information flow, as seve ral excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categories-in-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010).

The Basics of Category Theory. A basic category C is defined in terms of the following elements:
For dom ( f ) = A and codom ( f ) = B we abbreviate these definitions as f : A morphisms, and we should not treat them a priori as functions.
 verify that these axioms hold for them. For example, category Set with sets as objects and functions as morphisms, or category Rel with sets as objects and relations as morphisms, category Pos with posets as objects and order-preserving maps as morphisms, and category Group with groups as objects and group homomorphisms as morphisms, to name a few.
 objects, where A  X  ob ( C )and B  X  ob ( D ). There exists a morphism ( f , g ):( A , B ) in C  X  D if and only if there exists f : A  X  C  X  hom ( C )and g : B categories allow us to relate objects and morphisms of one mathematical formalism to those in another, in this example those of C to D .
Compact Closed Categories. A monoidal category C is a basic category to which we add a monoidal tensor  X  such that:
The product category of two monoidal categories has a monodial tensor, defined point-wisely by ( a , A )  X  ( b , B ): = ( a  X  b , A  X  B ).
 axioms: veyed in Selinger (2010). These calculi visualize and simplify the axiomatic reasoning (2008) and Coecke, Sadrzadeh, and Clark (2010) show how they depict the pregroup grammatical reductions and visualize the flow of information in composing meanings of single words and forming meanings for sentences. Although useful at an abstract level, these calculi do not play the same simplifying role when it comes to the concrete and empirical computations; therefore we will not discuss them in this article.
Elements of a pregroup are objects of the category, the partial ordering relation provides the morphisms, 1 is I , and monoidal multiplication is the monoidal tensor. spaces and linear maps over R  X  X hat is, vector spaces over of finite dimension, and an inner product operation vector space A .Theobjectsof FVect are vector spaces, and the morphisms are linear 90 maps between vector spaces. The unit object is R and the monoidal tensor is the linear algebraic tensor product. FVect is degenerate in its adjoints, in that for any vector space
A ,wehave A l = A r = A  X  ,where A  X  is the dual space of A . Moreover, by fixing a basis we obtain that A  X   X  = A . As such, we can effectively do away with adjoints in this category, and  X  X ollapse X  l , r ,  X  l ,and  X  r maps into  X  X djoint-free X  and the maps are inner product operations, A : A  X  A  X  R ,andthe generate maximally entangled states, also referred to as Bell-states . 3.3 A Categorical Passage from Grammar to Semantics
In Section 3.2 we showed how a pregroup algebra and vector spaces can be modeled as compact closed categories and how product categories allow us to relate the objects and morphisms of one category to those of another. In this section, we will present how
Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) suggest building on this by using categories to relate semantic composition to syntactic analysis in order to achieve syntax-sensitive composition in DSMs. 3.3.1 Syntax Guides Semantics. The product category FVect where A is a vector space and a is a pregroup grammatical type, and as morphism pairs ( f ,  X  )where f is a linear map and  X  a pregroup ordering relation. By the definition of partial ordering a  X  b . If we view these pairings as the association of syntactic types with vector spaces containing semantic vectors for words of that type, this restriction category if a reduces to b .
 by considering the pairs of unit objects and structural morphisms from the separate categories: I is now ( R , 1), and the structural morphisms are ( (from the definition of product categories): verbs of type n l sn r live in some space N  X  S  X  N , then there must be some structural morphism of the form:
We can read from this morphism the functions required to compose a sentence with a noun, a transitive verb, and an object to ob tain a vector living in some sentence space S , namely, ( N  X  1 S  X  N ).
 space associated with it. The structural morphisms of the product category guarantee that for every syntactic reduction there is a semantic composition morphism provided by the product category: syntactic analysis guides semantic composition . 3.3.2 Example. To give an example, we give syntactic type n to nouns, and n sitive verbs. The grammatical reduction for kittens sleep ,namely, nn to the morphism r n  X  1 s in P . The syntactic types dictate that the noun in some vector space N , and the intransitive verb phism gives us the composition morphism ( N  X  1 S ), which we can apply to  X   X   X  X  X   X  sleep.
 us expand composition as follows: consolidated the sums by virtue of distribut ivity of linear algebraic tensor product over addition, we have applied the tensored linear maps to the vector components (as the weights are scalars), and finally, we have simplified the indices since  X   X  n =  X   X  n sentence space S .
 92 to the composition morphism, namely,  X   X  X  X  X   X  kittens  X  treat the tensor products here as commas s eparating function arguments, thereby avoiding the dimensionality problems presented by earlier tensor-based approaches to compositionality. 3.4 This Article and the DisCoCat Literature tors with syntactic types was proposed in Clark and Pulman (2007). The setting of a
DisCoCat generalized this by making the mean ing derivation process rely on a syntactic type system, hence overcoming its central problem whereby the vector representations preliminary version of a DisCoCat was de veloped in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exemplified meanings of words were sets of their denota tions and meanings of sentences were their
Clark (2013), where a pla usibility truth-theoretic model for sentence spaces was worked branch and developed a toy example where neither words nor sentence spaces were
Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and co mplex types and the sentences containing them; then applied the algorithm to a disa mbiguation task performed on the British verbs and showed how a number of single operations may optimize the performance.
We discuss these developments in detail in the following sections. 4. Concrete Semantic Spaces steps to semantic composition operations. The structure of our syntactic representation dictates the structure of our semantic spaces, but in exchange, we are provided with composition functions by the syntactic ana lysis, rather than having to stipulate them ad hoc. Whereas the approaches to compositional DSMs presented in Section 2 either failed to take syntax into account during composition, or did so at the cost of not being able to compare sentences of different structure in a common space, this categorical approach projects all sentences into a common sentence space where they can be directly compared. However, this alone does not give us a compositional DSM.
 with syntactic types. We therefore cannot construct vectors for different syntactic types in the same way, as they live in spaces of diffe rent structure and dimensionality. Further-more, nothing has yet been said about the structure of the sentence space S into which expressions reducing to type s are projected. If we wish to have a compositional DSM that leverages all the benefits of lexical DSM s and ports them to sentence-level distribu-tional representations, we must specify a new sort of vector construction procedure. (2008) and Coecke, Sadrzadeh, and Clark (2010), examples of how such a compositional
DSM could be used for logical evaluation are presented, where S is defined as a Boolean space with True and False as basis vectors. However, the word vectors used are hand-written and specified model-theoretically, a s the authors leave it for future research to determine how such vectors might be obtained from a corpus. In this section, we will discuss a new way of constructing vectors for compositional DSMs, and of defining applicability and flexibility of sta ndard distributional models. 4.1 Defining Sentence Space
Assume the following sentences are all true: 1. The dogs chased the cats. 2. The dogs annoyed the cats. 3. The puppies followed the kittens. 4. The train left the station. 5. The president followed his agenda.
 pair (1) and (3), and perhaps to a lesser degree (1) and (2), and (2) and (3). Sentences (4) and (5) obviously speak of a different state of the world as the other sentences. distinctions. If we compare these by lexical similarity, (1) and (2) seem to be a closer relation such as  X  X opic, X  (5) might end up closer to (3) than (1). What, then, might cause us to pair (1) and (3)? relation by similar verbs are themselves similar. Abstracting away from tokens to some notion of property, we might say that both (1) and (3) express the fact that something concepts (word meanings) as  X  X essy bundles of properties, X  it seems only natural to have the way these properties are acted upo n, qualified, and related as the basis for sentence-level distributional representati ons. In this respect, we here suggest that the how the properties of the semantic objects within are qualified or brought into relation by verbs, adjectives, and other predicates and relations.
 namely, S I  X  = N for sentences with intransitive verbs and S with transitive verbs. These definitions mean that the sentence space X  X  dimensions are commensurate with either those of N ,orthoseof N  X  N . These are by no means the only options, but as we will discuss here, they offer practical benefits.
 hence, 94 unique ordered pairs of elements from N , for example,  X   X  X  X  X   X  ( n 1 , n 2 ), and so on, following the same matrix flattening structure used in the proof of the equal cardinality of N and Q . Because of the isomorphism between S will use the notations ways of representing the basis elements of su ch a space. To propagate this distinction respectively.
 work of Coecke, Sadrzadeh, and Clark (2010), namely, the result that all sentence vectors live in the same sentence space. A mathematical solution to this two-space problem was suggested in Grefenstette et al. (2011), and a variant of the models presented in this article permitting the non-problematic projection into a single sentence space ( S has been presented by Grefenstette et al. (2013). Keeping this separation allows us to deal with both the transitive and intransi tive cases in a simpler manner, and because sentences, and transitive sentences with transitive sentences, we will not address the issue of unification here. 4.2 Noun-Oriented Types
While Lambek X  X  pregroup types presented in Lambek (2008) include a rich array of basic types and hand-designed compound types in order to capture specific grammatic for experimental purposes, similar to s ome common types found in the CCG-bank (Steedman 2001).
 for their semantic representations. Furthermore, we will treat noun-phrases as nouns, assigning to them the same pregroup type and semantic space.
 return a sentence, and transitive verbs as functions ( NP phraseandreturnanintransitiveverbfun ction, which in turn consumes a noun phrase and returns a sentence. Using our distinction between intransitive sentences, we give intransitive verbs the type n r s I associated with the semantic space N verbs the type n r s T n l associated with the semantic space N returning a noun phrase; and hence we give them the type nn space N  X  N .
 can use these types to construct sentence vector representations for simple intransitive verb X  X ased and transitive verb X  X ased sentences, with and without adjectives applied to subjects and objects. 4.3 Learning Procedures
Tobegin,weconstructthesemanticspace N for all nouns in our lexicon (typically lim-ited by the words available in the corpus used). Any distributional semantic model can be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors X  X s measured by metrics such as the WordSim353 test of Finkelstein et al. (2001) X  X ill produce better relational vectors from the procedure underlying assumption in most of the curre nt literature on the subject (Erk and Pad  X  o 2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010).
 semantic representations for relational words. In pregroup grammars (or other com-binatorial grammars such as CCG), we can view such words as functions, taking as arguments those types present as adjoints in the compound pregroup type, and return-ing a syntactic object whose type is that of the corresponding reduction. For example, an adjective nn l takes a noun or noun phrase n and returns a noun phrase n from the reduction ( nn l ) n  X  n . It can also compose with another adjective to return an adjective ( nn l )( nn l )  X  nn l . We wish for our semantic representations to be viewed in the same way, such that the composition of an adjective with a noun (1 be viewed as the application of a function f : N  X  N to its argument of type N . be characterized by the properties that thei r arguments hold in the corpus, rather than an example, rather than learning what the adjective angry means by observing that it co-occurs with words such as fighting , aggressive ,or mean , we learn its meaning by (i.e., co-occur with words such as) fighting , aggressive ,and mean . Whereas in the lexical semantic case, such associations might only rarely occur in the corpus, in this indirect method we learn what properties the adjective relates to even if they do not co-occur with it directly.
 argument. If, indeed, angry is characterized by arguments that have high basis weights for basis elements corresponding to the concepts (or context words) fighting , aggressive , and mean , and relatively low counts for semant ically different concepts such as passive , peaceful ,and loves , then when we apply angry to dog the vector for the compound angry dog should contain some of the information found in the vector for dog .Butthis vector should also have higher values for the basis weights of fighting , aggressive ,and mean , and correspondingly lower values for the basis weights of passive , peaceful ,and loves .
 sentation for relations of any arity, as first presented in Grefenstette et al. (2011), we examine how we would deal with this for binary relations such as transitive verbs. If a transitive verb of semantic type N  X  S T  X  N is viewed as a function f : N that expresses the extent to which the properties of subject and object are brought into relation by the verb, we learn the meaning o f the verb by looking at what properties are brought into relation by its arguments in a corpus. Recall that the vector for a verb v ,  X   X  v  X  N  X  S { (  X   X  X  X  X   X 
SUBJ l , 96 is some value of j such that s j = ( n i , n k ), we define otherwise. Using all of this, we define the calculation of each basis weight c would involve computing size ( arg v )  X  dim ( N ) 2  X  dim ( S ) ucts. However, using the decomposition of basis elements of S into pairs of basis elements of N (effectively basis elements of N  X  N ), we can remove the of indices would be 0. Hence we simplify: vector in which all the basis weights where s j = ( n i , n of the dim ( N ) 4 values are non-zero. In short, the vector weights for learning algorithm, entirely characterized by the values of a dim ( N )by dim ( N )matrix, indices.
 late a compact expression of  X   X  u , discarded relative to the previous definition of factor of dim ( N ) 2 simply discards all basis elements for which the basis weight was 0 by default. used in the compositional mechanism prese nted in Section 3.3, as the dimensions of  X   X  However, a solution can be devised if we r eturn to the sample calculation shown in
Section 3.3.2 of the composition of a transitive verb with its arguments. The composition reduces as follows: where the verb v is represented in its non-compact f orm. If we introduce the compact-ness given to us by our isomorphism S T  X  = N  X  N we can express this as where v is represented in its compact form. Furt hermore, by introducing the component wise multiplication operation : we can show the general form of transitive verb composition with the reduced verb representation to be as follows: 1. We have treated them as functions, taking two nouns and returning a 2. We have built them by counting what properties of subject and object 3. We have assumed that output properties were a function of input properties 4. We have shown that this leads to a compact representation 5. We have shown that the composition operations 98 choice of S T  X  = N  X  N as output type for N  X  N as an input type (the pair of arguments the verb takes), justifying our choice of a tra nsitive sentence space. In the intransitive case, the same phenomenon can be observed, since such a verb takes as argument a vector in N and produces a vector in S I  X  = N . Furthermore, our choice to make all other types dependent on one base type X  X amely, n (with associated semantic space N ) X  yields the same property for every relation al word we wish to learn: The output type is the same as the concatenated (on the syntacti c level) or tensored (on the semantic level) input types. It is this symmetry between input and output types that guarantees that any m -ary relation, expressed in the original fo rmulation as an element of tensor space
N  X  ...  X  N of the reduced representation stands for the degree to which the i th element of the input vector affects the i th element of the output vector.
 resentations, first presented in Grefenstette and Sadrzadeh (2011a), which is as follows.
Each relational word P with grammatical type  X  and m adjoint types coded as an ( r  X  ...  X  r ) multi-dimensional array with m degrees of freedom (i.e., a rank m tensor). Because our vector space N has a fixed basis, each such array is represented in vector form as follows:
This vector lives in the tensor space N  X  N  X  X  X  X  X  X  N to the procedure described in Figure 2.

Linear algebraically, this procedure corresponds to computing the following arg following operations: proach to any sentence for which we have a pregroup parse (e.g., as might be obtained by our translation mechanism from CCG). For example, determiners would have a type nn l , allowing us to model them as matrices in N  X  N ;adverbswouldbeoftype ( n s ) r ( n r s ), and hence be elements of S  X  N  X  N  X  procedure defined earlier, although for wor ds like determiners and conjunctions, it is unclear whether we would want to learn such logical words or design them by hand, as was done in Coecke, Sadrzadeh, and Clark (2010) for negation. Nonetheless, the pro-cedure given here allows us to generalize the wo rk presented in this article to sentences of any length or structure based on the pregroup types of the words they contain. 4.4 Example
To conclude this section with an example taken from Grefenstette and Sadrzadeh (2011a), we demonstrate how the meaning of the word show might be learned from a corpus and then composed.

The vector of show is
TF/IDF-weighted values for vectors of selected nouns (built from the BNC) are as shown in Table 1.
 Part of the matrix compact representation of show is presented in Table 2. 100
As a sample computation, the weight c 11 for vector ( by multiplying weights of table and result on and location on total weight 79 . 24. Similarly, the weight c 21 for vector ( computed by multiplying the weight of (27  X  7), then multiplying the weight of (7 . 4  X  5 . 9), and then adding these (189 + 43 . 66) to obtain the total weight of 232 omitting the determiner and possessive for sim plicity, as we have left open the question as to whether or not we would want to learn them using the generalized procedure from Section 4.3 or specify them by design due to their logical structure. The calculation according to the vectors in Table 1 and the matrix in Table 2 will be: namely, the sentence vector in S T for [the] master showed [his] dog : 5. Experiments to evaluate how well the compositional process works; second, we also are trying to determine how useful the final representation X  X he output of the composition X  X s, relative to our needs. models, from bag-of-words approaches in in formation retrieval to logic-based formal semantic models, via language models used for machine translation. It is heavily task dependent, in that a representation that is suitable for machine translation may not be appropriate for textual inference tasks, and one that is appropriate for IR may not be ideal for paraphrase de tection. Therefore this aspect of semantic model evaluation ide-ally should take the form of application-orie nted testing. For instance, to test semantic representations designed for machine tran slation purposes, we should use a machine translation evaluation task.
 and Clark 2010), described in Section 3, allows for the composition of any words of any syntactic type. The general learning algo rithm presented in Section 4.3 technically can be applied to learn and model relations of any semantic type. However, many open questions remain, such as how to deal with logical words, determiners, and with transitive and intransitive sentences. W e will leave these questions for future work, briefly discussed in Section 6. In the meantime, we concretely are left with a way of satisfactorily modeling only simple sente nces without having to answer these bigger questions.
 evaluating how well various models of sema ntic vector composition perform (along with the one described in Section 4) in a phrase similarity comparison task. This task aims to test the quality of a compositional p rocess by determining how well it forms a clear joint meaning for potentially ambiguous words. The intuition here is that tokens, process X  X hrough giving them context  X  X hat we understand their specific meaning. For example, bank itself could (among other meanings) mean a river bank or a financial are talking about the financial institution.
 disambiguation occurs as a byproduct of composition. We begin by describing the first with subject and object). In Section 5.3, we discuss a new data set, based around short transitive-verb phrases where the subject and object are qualified by adjectives. We leave discussion of these results for Section 6. 5.1 First Experiment the degree to which an ambiguous intransitive verb (e.g., draws ) is disambiguated by combination with its subject.

Data set Description. The data set 4 comprises 120 pairs of intransitive sentences, each of the form NOUN VERB. These sentence pairs are generated according to the following 102 procedure, which will be the basis for the cons truction of the other data sets discussed subsequently: 1. A number of ambiguous intransitive verbs (15, in this case) are selected 2. For each verb V , two mutually exclusive synonyms V 1 and V 3. For each pair of verb pairs ( V , V 1 )and( V , V 2 ), two frequently occurring 4. By combining the nouns with the verb pairs, we form two high similarity pairs. In Mitchell and Lapata (2008), eight triplets are generated for each pair of verb its HIGH or LOW classification (based on t he choice of noun for the verb pair) is an entry in the data set, and can be read as a pair of sentences: ( V , V intransitive sentences NV and NV i .
 notators. These annotators are asked to ra te the similarity of meaning of the pairs of sentences in each entry on a scale of 1 (low similarity) to 7 (high similarity). The final form of the data set is a set of lines each containing:
Evaluation Methodology. This data set is used to compare various compositional distribu-tional semantic models, according to the following procedure: 1. For each entry in the data set, the representation of the two sentences 2. The similarity of these sentences according to the model X  X  semantic 3. The rank correlation of entry model scores against entry annotator scores (perfect correlation). The higher the  X  score, the higher the compositional model can be said to produce sentence representations th at match human understanding of sentence meaning, when it comes to comparing the meaning of sentences. As such, we will rank the models evaluated using the task by decreasing order of rank-based: It does not require models X  se mantic similarity metrics to be normalized rank correlation with human scores, but producing model scores on a small scale (e.g., values between 0 . 5and0 . 6), will obtain a higher  X  score than a model producing model scores on a larger scale (e.g., between 0 and 1) but with less perfect rank correlation.
If we wished to then use the former model in a task requiring some greater degree of numerical separation (let us say 0 for non-similar sentences and 1 for completely similar sentences), we could simply renormalize the model scores to fit the scale. By eschewing score normalization as an evaluation factor, we minimize the risk of erroneously ranking one model over another.
 scores and annotator scores, Mitchell and Lapata (2008) calculate the mean model reported in their paper as additional means for model comparison. However, for the same reason we considered Spearman X  X   X  to be a fair means of model comparison X  namely, in that it required no model score normalization procedure and thus was
HIGH/LOW means to be inadequate grounds for comparison, precisely because it requires normalized model scores for comparison to be meaningful. As such, we will not include these mean scores in the presentation of this, or any further experiments in this article.

Models Compared. In this experiment, we compare the best and worst performing models for all words in the corpus, using standard distributional semantic model construction procedures ( n -word window) with the parameters of Mitchell and Lapata (2008). These parameters are as follows: The basis elements of this vector space are the 2,000 most frequently occurring words in the BNC, excluding common stop words. For our evalu-ation, the corpus was lemmatized using Carroll X  X  Morpha (Minnen, Carroll, and Pearce 2001), which was applied as a byproduct of our parsing of the BNC with C&amp;C Tools (Curran, Clark, and Bos 2007). 104 on either side. After the vector for each word w was produced, the basis weights c associated with context word b i were normalized by the following ratio of probabilities weighting scheme:
Let
It should be evident that there is no significant difference in using W for nouns in lieu of building a separate vector space N strictly for noun vectors.
 in constructing the sentence representation, effectively comparing the semantic content of the verbs: strictly unsupervised (i.e., no free parame ters for composition). These are the additive model Add ,wherein and the multiplicative model Multiply ,wherein
Other models with parameters that must be optimized against a held-out section of the data set are presented in Mitchell and Lapata (2008). We omitted them here principally because they do not perform as well as Multiply , but also because the need to optimize the free parameters for this and other data se ts makes fair comparison with completely unsupervised models more difficult, and less fair.
 where according to the procedure described in Section 4.3. It should be noted that for the case of intransitive verbs, this composition opera tion is mathematically equivalent to that of the Multiply model (as component-wise multiplication is a commutative operation), the difference being the learning procedure for the verb vector.
 taken to be the cosine similarity of their vectors, defined in Section 2.2: an upper-bound. The additional baseline, Bigram Baseline , is a bigram-based language model trained on the BNC with SRILM (Stolcke 2002), using the standard language model settings for computing log-probab ilities of bigrams. To determine the semantic similarity of sentences s 1 = noun verb 1 and s 2 = noun verb mutually conditionally independent prope rties, and computed the joint probability:
The reasoning behind this baseline is as follows. The sentence formed by combining the first verb with its arguments is, by the design of this data set, a semantically coherent sentence (e.g., the head bowed and the government bowed both make sense). We therefore than bigrams that are not semantically cohe rent sentences (and therefore unlikely to be observed in the corpus). A similar (rela tive) high probability is to be expected when the sentence formed by taking the second verb in each entry and combining it with acting as a normalizing factor. To summarize, while this bigram-based measure only tracks a tangential aspect of semantic similarity, it can be one which plays an artificially important role in experiments with a predetermined structure such as the one described in this section. For this reason, we use thi s bigram baseline for this experiment and all that follow.
 scores, using Spearman X  X   X  .

Results. The results 5 of the first experiment are shown in Table 4. As expected from the fact that Multiply and Categorical differ only in how the verb vector is learned, the results of these two models are virtually identical, outperforming both baselines and the Additive model by a significant margin. However, the distance from these models to the upper bound is even greater, demonstrating that there is still a lot of progress to be made. 5.2 Second Experiment
The first experiment was followed by a se cond similar experiment, in Mitchell and Lapata (2010), covering different sorts of composition operations for binary combinations of syntactic types (adjective-noun, noun-noun, verb-object). Such further experiments are interesting, but rather than continue down this binary road, we now turn to the development of our second experiment, involving sentences with larger syntactic structures, to examine how well various compositional models cope with more complex syntactic and semantic relations. 106
Sadrzadeh (2011a), is an extension of the first in the case of sentences centered around transitive verbs, composed with a subject an d an object. The results of the first experi-ment did not demonstrate any difference betw een the multiplicative model, which takes into account no syntactic information or wor d ordering, and our syntactically motivated categorical compositional model. By running the same experiment over a new data set, where the relations expressed by the verb have a higher arity than in the first, we hope to demonstrate that added structure lead s to better results for our syntax-sensitive model.

Data Set Description. The construction procedure for this data set for the first data set, with the following differences: identical sentence pairs and rejected annota tors who did not score these gold standard sentences with a high score of 6 or 7. 7 Lemmatized sentences from sample entries of this data set and our HIGH-LOW tags for them are shown in Table 5.
 for the previous data set. The annotators were shown, for each entry, a pair of sentences created by adding the in front of the subject and object nouns and putting the verbs in the past tense, 8 and were instructed to score each pair of sentences on the same scale of 1 (not similar in meaning) to 7 (similar in meaning), based on how similar in meaning they believed the sentence pair was.

Evaluation Methodology. The methodology for this experiment is exactly that of the previous experiment. Models compositionally construct sentence representations, and compare them using a distance metric (all vector-based models once again used cosine similarity). The rank correlation of models scores with annotator scores is calculated using Spearman X  X   X  , which is in turn used to rank models.

Models Compared. The models compared in this experiment are those of the first experi-ment, with the addition of an extra trigram -based baseline (trained with SRILM, using the addition of log-probability of the sentence as a similarity metric), and a variation on our categorical model, presented subsequently. With W as the distributional semantic space for all words in the corpus, trained using the same parameters as in the first exper-iment, and respectively, and with verb cat as the compact representation of a transitive verb learned using the algorithm presented in this pape r, we have the following compositional methods: 108
We first presented this new addition in Grefenstette and Sadrzadeh (2011b), where we observed that the compact representation of a verb in the DisCoCat framework, under the assumptions presented in Section 4, can be viewed as dim ( N ) matrices in N  X  N . We considered alternatives to the algorithm presented earlier for the construction of such matrices, and wer e surprised by the results of the Kronecker method, wherein we replaced the matrix learned by our algorithm with the Kronecker product of the lexical semantic vectors for the verb. Further analysis performed since the publication of that paper can help to understand why this method might work. Using the following property, for any vectors we can see that the Kronecker model X  X  composition operation can be expressed as
Bearing in mind that the cosine measure we are using as a similarity metric is equivalent to the inner product of two vectors norm alized by the product of their length and the following property of the inner product of kronecker products we finally observe that comparing two sentences under Kronecker corresponds to the following computation: cosine ( where  X  is the normalization factor
We note here that the Kronecker is effectively a parallel application of the Multiply context of this task, the comparison of two sentences boils down to how well each verb combines with the subject multiplied by how well it combines with the object, with the joint product forming the overall model score. In short, it more or less constitutes the introduction of some mild syntactic sensitivi ty into the multiplicative model of Mitchell and Lapata (2008), although it remains to be seen how well this scales with syntactic complexity (e.g., extended to ditransitive verbs, or other relations of higher arity). puting the pairwise alignment of each annotator with every other, and averaging the resulting rank correlation coefficients.

Results. The results of the second experiment are shown in Table 6. The baseline scores Trigram Baseline , the difference between both models no t being statistically significant.
The additive model Add performs on par with the first ex periment. The multiplicative model Multiply and our Categorical model perform on par with the version used for the intransitive experiment, but obtain a score comparable to t he baselines. The best performing model here is our newly introduced Kronecker model, which leads the pack by a steady margin, with a score of 0 . 26. The inter-annotator agreement UpperBound is much higher in this experiment than in the p revious experiment, indicating even more room for improvement.
 the multiplicative model, and that the Kronecker model provides the highest results, while being as simple in its construction as th e multiplicative model, requiring only the learning of lexical vectors for the verb. 5.3 Third Experiment
The third and final experiment we present is a modified version of the second data set presented earlier, where the nouns in each entry are under the scope of adjectives ap-plied to them.The intuition behind the data sets presented in Section 5.1 and Section 5.2 was that ambiguous verbs are disambiguated through composition with nouns. These nouns themselves may also be ambiguous, and a good compositional model will be capable of separating the noise produced by o ther meanings through its compositional mechanism to produce unambiguous phrase rep resentations. The intuition behind this biguation of the nouns they apply to, but also additional semantic noise. Therefore, a 110 good model will also be able to separate the useful information of the adjective from its semantic noise when composing it with its argument, in addition to doing this when composing the noun phrases with the verb.

Data Set Description. The construction procedure for this data set was to take the data set from Section 5.2, and, for each entry, add a pair of adjectives from those most frequently occurring in the corpus. The first adjecti ve from the pair is applied to the first noun (subject) of the entry when forming the sentences, and the second adjective is applied to the second noun (object). For each entry, we chose adjectives which best preserved the meaning of the phrase constructed by combining the first verb with its subject and object.

Amazon X  X  Mechanical Turk service. The annotators were shown, for each entry, a pair of sentences created by adding the in front of the subject and object noun phrases and putting the verbs in the past tense, 10 and asked to give each sentence pair a meaning similarity score between 1 and 7, as for the previous data sets. We applied the same quality control mechanism as in the second experiment. Some 94 users returned anno-tations, of which we kept 50 according to our gold standard tests. We are unaware of can only lead to the production of higher quality annotations.

Evaluation Methodology. The evaluation methodology in this experiment is identical to that of the previous experiments.

Models Compared. In this experiment, in lieu of simply comparing compositional models  X  X cross the board X  (e.g., using the multiplicative model for both adjective-noun compo-sition and verb-argument compositio n), we experimented with different combinations of models . This evaluation procedure was chosen b ecause we believe that adjective-noun composition need not necessarily be the same kind of compositional process as subject-see what model mixtures work well together. Naturally, this is not a viable approach to selecting composition operations in general, as we will not have the luxury of try-ing every combination of composition oper ations for every combination of syntactic types, but it is worthwhile performing these t ests to at least verify the hypothesis that operation-specific composition operations (or parameters) is a good thing. Notably, this idea has been explored very recently, within the context of deep learning networks, by Chen et al. (2013).
 a adjective-noun composition model. For v erb-argument composition, we used the three best models from the previous experiment, namely, Multiply , Categorical ,and noun composition. With in our distributional l exical semantic space W (built using the same procedure as the previous experiments) and model, built according to the algorithm from Section 4.3, we have the following models:
The third model, AdjNoun , is a holistic (non-compositional) model, wherein the adjective-noun compound was treated as a single token, as its semantic vector  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  ( adjective noun ) lex  X  W was learned from the corpus using the same learning procedure applied to construct other vectors in W . Hence, the model defines adjective-noun  X  X omposition X  as:
Bigram Baseline ,and Trigram Baseline . As in previous experiments, the verb baseline words. The bigram and trigram baselines are c alculated from the same language model as used in the second experiment. In both case s, the log-probability of each sentence is calculated using SRLIM, and the sum of log-probabilities of two sentences is used as a similarity measure.

Results. The results for the third experiment are shown in Table 8. The best performing adjective-noun combination operations for each verb-argument combination operation are shown in bold. Going through the combined models, we notice that in most cases the results stay the same whether the a djective-noun combination method is AdjMult or
CategoricalAdj . This is because, as was shown in the first experiment, composition of a unary-relation such as an adjective or intransitive verb with its sole argument, under verb vector is constructed. We note, however, that with Categorical as a verb-argument composition method, the CategoricalAdj outperforms AdjMult by a non-negligible margin (0 . 19 vs. 0 . 14), indicating that the difference in learning procedure can lead to different results depending on wha t other models it is combined with. 112
CategoricalAdj+Kronecker (  X  = 0 . 27). Combinations of the adjective composition methods with other composition methods at be st matches the best-performing baseline,
Verb Baseline . In all cases, the holistic model AdjNoun provides the worst results. 6. Discussion
In this article, we presented various approa ches to compositionality in distributional semantics. We discussed what mathematical operations could underlie vector combina-tion, and several different ways of including syntactic information into the combinatory process. We reviewed an existing compositio nal framework that leverages the ability to communicate information across mathematical structures provided by category theory in order to define a general way of linking syntactic structure to syntax-sensitive com-position operations. We presented concrete ways to apply this framework to linguistic tasks and evaluate it, and developed algorith ms to construct semantic representations for words and relations within this framew ork. In this section, we first briefly comment upon the combined results of all three experiments, and then conclude by discussing what aspects of compositionality require further attention, and how experiment design should adapt towards this goal.

Results Commentary. We evaluated this framework against other unsupervised compo-sitional distributional models, using non-compositional models and n -gram language models as baselines, within the context of three experiments. These experiments show that the concrete categorical model develop ed here, and the Kronecker-based variant presented alongside it, outperform all other models in each experiment save the first, where they perform on par with what was the leading model at the time this experiment was performed (namely, 2011). As the experiments involved progressively more syntac-tically complicated sentences, the increased reliability of our categorical approaches rel-ative to competing models as sentence complexity rises seems to indicate that both the categorical and Kronecker m odel successfully leverage the added information provided by additional terms and syntactic structures.
 composition operations, depending on the syntactic type of the terms being combined, the adjective-noun combination models AdjMult and CategoricalAdj , despite their mathematical similarity, produce noticeably different results when combined with the categorical verb-argument composition o peration, while they perform equally with most other verb-argument composition operations. We can conclude that different mod-els combine different semantic aspects more prominently than others, and that through combination we can find better performance by assuming that different kinds of compo-sition play on different semantic properties. For example, predicates such as intersective adjectives add information to their argument (a red ball is a ball that is also red). This raises the question of how to design models of composition that systematically select which operations will match the semantic as pects of the words being combined based investigation.
 early part of this article, and those evaluated in the later part. First, we have shown that a linguistically motivated and mathem atically sound framework can be imple-mented and learned. Second, we showed that simple methods such as the Kronecker model perform well, especially when combin ed with the multiplicative model (effec-composition.

Future Work. These experiments showcased the abilit y of various compositional models to produce sentence representations that were less ambiguous than the words that formed them. They also more generally demonstrated that concrete models could be built from the general categorical fra mework and perform adequately in simple paraphrase detection tasks. However, var ious aspects of compositionality were not actual capacity of a model to use the syntactic structure was not tested. For instance, models that ignore word order and syntax altogether were not particularly penalized, as might be done if some sentences were simply the reverse of another sentence they are paired with (where syntax insensitive mo dels would erroneously give such sentence pairs a high similarity score). One way of testing word order while expanding the data such as Mitchell and Lapata X  X , discussed herein, creates a new entry where one of the sentences has the order of its words reversed, and is assigned an artificial annotator 114 as guitar played man and man played guitar , where the subjects and objects of verbs are indeed reversed but the two sentences do not necessarily express opposite mean-ings; we will be looking into expanding su ch data sets to ones where both sentences make sense and have opposite meanings, such as in the pair man bites dog and dog bites man .
 words are aligned. Models that might indirectly benefit from this alignment, such as
Kronecker , may have been given an advantage due to this, as opposed to models such as Categorical , which are designed to compare sentences of different length or syntactic structure, when resolving the difference between intransitive and transitive sentence spaces as has been done in Grefenstette et al. (2011). Future experiments should aim to do away with this automated alignment, and in clude sentence comparisons that would penalize models which do not leverage syntactic information.
 the head verb. However, sentences with dif ferent sorts of verbs should be able to be directly compared. Not only do several models, both non-syntax sensitive (additive, multiplicative) and syntax-sensitive (Baroni and Zamparelli 2010; Socher et al. 2012), framework the concrete categorical models were derived from does not commit us to different sentence spaces either. The topic of how to solve this problem for the concrete models developed here is beyond the scope o f this article, but various options exist, such as the projection of S T into S I , the embedding of S give this degree of convenience to the models being evaluated (and their designers), by comparing sentences of different types, lengths, and structure so as to more fairly evaluate the capacity of models to produce meaningful semantic representations in a single space, which can be directly compared regardless of syntactic structure and verb-type.
 application-oriented. The class of experimen ts presented in this article specifically see how well disambiguation occurs as a byprodu ct of composition. We presented concrete models that would offer ways of combining r elations underlying verbs and adjectives, and tested how well these relations disambiguated their arguments and were in turn disambiguated by their arguments. We did not address other aspects of language, such sentences, and many other linguistic aspects of spoken and written language that have complex syntactic structure and comp licated semantic roles. Addressing these sorts of problems requires determining how negation, conjunction, and other logical relations should be modeled within composit ional distributional formalisms, a problem we share with other similar approaches. The development of newer models within the categorical framework we based our work on, and the further de velopment of other approaches mentioned here, must be driven by new experimental goals different from those offered by the task and experiments d iscussed in this article. Thinking not only about how to address these aspects of language within compositional models, but how this field. Acknowledgments References 116
