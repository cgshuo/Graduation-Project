 Query formulation is one of the most difficult and important aspects of information seeking and retrieval. Two techniques, term relevance feedback and query suggestion, provide methods to help users formulate queries, but each is limited in different ways. In this research we combine these two techniques by automatically creating query suggestions using term relevance feedback techniques. To evaluate our approach, we conducted an interactive information retrieval study with 55 subjects and 20 topics. Each subject completed four topics, half with a term suggestion system and half with a query suggestion system. We also investigated the source of the suggestions: approximately half of all subjects were provided with system-generated suggestions, while half were provided with user-generated suggestions. Results show th at subjects used more query suggestions than term suggesti ons and saved more documents with these suggestions, even though there were no significant differences in performance. Subjects preferred the query suggestion system and rate d it higher along a number of dimensions including its ability to help them think of new approaches to searching. Qualitative data provided insight into subjects X  usage and ratings, and i ndicated that subjects often used the suggestions even when th ey did not click on them. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -query formulation, search process.
 Performance, Design, Human Factors. Query suggestion, term sugge stion, relevance feedback, interactive searching. Many information retrieval (IR) techniques have been developed to assist users formulate and reformulate queries, most notably relevance feedback (RF) [17]. Examples of RF include users adding terms suggested by a system to their queries (term RF) or indicating to a system passages or documents that are relevant to were not adopted by many users. Recently, however, there has been a revival of interest in query and term suggestion features by major search engine companies and researchers. Smyth, et al. [18] describe the I-SPY search engine, which incorporates a collaborative ranking function based on similar query-document pairs and suggest s related queries to users. Results of several evaluations , including ones with subjects, demonstrated that the technique s were effective at improving retrieval. Freyne, et al. [6] describe the integration of I-SPY with a social navigation component and reported that subjects found query suggestions useful during browsing. White, et al. [19] compared the effectiveness and usability of a system that suggested queries with one that s uggested destinations (or pages). White, et al. studied two types of tasks: exploratory and known-item, and found an interaction effect according to task type. Subjects provided mixed reviews about the suggestion features, but for known-item tasks the query suggestion feature was rated most positively. Furthermore, subjects who rated the query suggestion system more positively indicated they did so because the system saved them from typing queries and helped them generate new ideas for query re formulation. Those who rated it unfavorably stated they felt the suggested queries were not relevant. Overall, the evidence indicates query suggestion can be a useful feature at least for some types of tasks and users. A necessary condition for query suggestion to occur is that a set of queries that are similar to the current query exists and that the similarity of these queries to one another (and to the current query) can be determined. A num ber of techniques have been proposed to determine query simila rity, including the use of term overlap [18], query hitting time [14] and the examination and comparison of results retrieved in response to queries [4, 5, 15]. Query similarity has been used as a method for identifying terms for automatic query expansion [4, 5] and query rewriting [11]. However, all of these techniques need a sufficiently large number of existing query and/or query-document pairs to work, and their effectiveness at generating query suggestions for user consumption is unclear. Moreover, many queries are unique and occur infrequently. Techniques that rely on the existence of previous queries are unlikely to work well in these situations. The work proposed here seeks to address the problems outlined above by combining research in these two areas  X  term RF and query recommendation. To summarize, the major problems related to term RF features are that users are often reluctant to make use of such features, and often have a difficult time selecting good terms from the list of candidates. The major problems related to query reco mmendation are that systems do not always have available a set of similar queries and determining the similarity of these queries can be difficult. As a tool for query reformul ation, query recommendation may have an advantage over traditional term RF interfaces because of its form and the kind of interaction required by users: users simply click on a query to view new results, rather than selecting one or more terms to add to their queries and re-running retrieval. Overall, the costs involved with selecting a recommended query are less than that of selecting terms to add to one X  X  query. Furthermore, query recommendati on provides term context; terms are not presented in isolation but within the context of other query terms. This might help users di sambiguate query terms, and more quickly and easily recognize whether a suggestion is relevant to ordered by descending weight in the query X  X  language model. We keep the top N terms from this set, where N is equal to the number of terms within the user X  X  original query plus 2. These term sets are then used to populate both que ry and term suggestions. To create query suggestions, for each cluster, we order terms from left to right by descending scor e and append these terms to the user X  X  original query; hence, each query is produced by terms which are part of a common clus ter and terms from the user X  X  original query. For term suggesti on, we use all terms to populate the suggested term list, pruni ng duplicates and terms which appear within the original query. We return the top 100 results initially retrieved by this method to users. This approach addresses the two PRF problems identified previously. Since terms within suggested queries are generated based on clustered documents, rather than the top N documents, their semantic similarity tends to be stronger and leads to more coherent queries. Second, since we select clusters based on their size (rather than the rank of their documents) many queries are generated based on documents wh ich appear at relatively high ranks within the result list. We ran a series of experiments to evaluate our technique for generating queries, although results of these experiments are not reported in this paper because of space limitations. For each user query, about 13 suggested queries per query were produced by our approach. These suggested que ries varied according to the number of relevant documents th ey produced when issued. The results of the best suggested query (i.e., that which produced the most relevant documents), routinely outperformed pure PRF. Interestingly, the top performi ng query was often generated from a cluster of documents whose average rank in the result list for the user query was quite high (e.g., 50-100). On the other hand, the worst suggested query regularly performed worse than standard PRF. Ultimately, the performance our approach depends on which queries the user selects. On average, subjects received about 13.10 suggestions per query entered. We also investigated source of suggestions . Half of the subjects were presented with suggestions which were generated automatically for each query they entered using the techniques described above ( system-generated suggestions ). The other half of the subjects were presented with suggestions that had been generated by a separate group of subjects (n=35) who participated in a remote evaluation of the automatic techniques ( user-generated suggestions ) [13]. The procedures for the remote experiment were the same as those used in the experiment described in this paper except that subjects completed the study remotely and all subjects received system-generated suggestions. The purposes of the remote study we re to generate queries for use in the current study and to compare differences in study mode. We do not report results of this study here because the focus of the study was slightly different from the one reported here. To identify which user queries from the remote study we would use to populate our user-generated suggestion system, we divided subjects X  queries (n=669) into th ree stages: those entered during the first third of the search ( early), those entered during the second third (mid) and those entered during the final third (late) and then sampled queries equally from the mid and late groups. The reason for doing this was because we believed that these queries would be more likely to be unique and helpful  X  at the start of the search, users entered similar queries, but as they progressed through the search, their queries became more unique. We randomly selected queries from each of these bins for each select topics, we examined the performances of queries written by subjects from another study that used all 50 topics from this collection (424 queries contributed by 61 subjects) [12]. We computed normalized discounted cumulated gain (nDCG) [8] at depth 50 for each query, averaged these values for each topic and sorted topics into four quartiles according to nDCG: easy, medium, moderate and difficult. We then manually selected five topics from each bin by considering the number of relevant documents in the corpus for each topic and whether we thought the topic would be of interest to our target subjects (undergraduate students). Each s ubject completed one topic from each difficulty bin (4 total). The basic objective of the search task was to find documents relevant to the information described by the topic. Subjects were limited to 15 minutes per topic. Topics were rotated and counter-balanced across subjects and systems. Subjects X  interactions with the system were logged. The log was examined to identify use of sugge stions, or the number of clicks users made on suggested queries and terms. Two performance measures were computed using documents subjects X  saved: number save d and session-based discounted cumulated gain (sDCG) [9]. To compute sDCG, first DCG is computed for each query [8]. sDCG is a session-based measure that accommodates interactive search situations where a user enters multiple queries during a single session (where a session is considered in this case a temporal instance of a user s earching for information about a single topic). sDCG includes a discount function for each query issued by a user that is based on the sequence number of the query within the session. Essentially, th e total value of search results returned by queries issued later in a search session is worth less than those returned by queries issued earlier in the session. sDCG for a query is defined as: where bq is the logarithm base for the query discount, q is the position of the query in the session and DCG is the vector for the search results returned by the query. We normalized DCG scores, so the resulting sDCG also represents a normalized value (snDCG). J X rvelin, et al. [9] describe bq as a bounded parameter (1 &lt; bq &lt; 1000) which can be set to model varying types of users (e.g., bq =2 for an impatient user and bq =10 for a patient user). We set bq to 10 since our users were experimental subjects who were tasked with evaluating the system. It is also necessary to set two parameters for the nDCG vector: b which is the logarithm base for the result discount and d which is the depth of the search results examined. We set b = 2 and d = 100. Once we computed snDCG for each query in a session, we averaged these values to arrive at a composite measure for each session, which represents a subject X  X  performance on a particular topic. Subjects evaluated the topics and the systems according to effectiveness, satisfaction and preference using an Exit Questionnaire. Results regarding the topic evaluations are not reported here. The main part of the questionnaire presented the systems together (query and term suggestion) and asked subjects to rate the two along several items (see Table 4 for items). We presented the systems together to facilitate relative comparisons per topic, while those who receive d system-generated suggestions submitted around 6 per topic. Overall, the majority of queries originated with the subject. S ubjects who received user-generated suggestions used significantly more suggestions than those who received system-generated suggestions [ F (1, 218)=11.60, p &lt;.01]. Of these suggestions, subjects us ed significantly more of the query suggestions than the term suggestions [ F (1, 218)=10.45, p &lt;.01]. There were no significant interaction effects between suggestion type and source of terms. Subject Generated 4.13 (2.88) 3.80 (2.48) 4.87 (3.68) 6.00 (4.73) System Assisted 0.73 (1.21) 0.76 (1.12) 1.04 (2.02) 1.22 (2.12) Total Queries Issued Table 1 displays the mean number of queries submitted according to topic difficulty. This includes the mean number of queries that originated with subjects and the mean number that were created with the help of a suggestion. This latter category is further broken down into mean number of query suggestions taken and mean number of queries created us ing term suggestions. Overall, subjects issued the most queries for hard topics and the second most for moderate topics. Subjects entered slightly more queries for easy topics than medium topics. One-way ANOVA showed that there was a significant difference in these means [ F (3, 219)=4.60, p &lt;.01]. Bonferroni post-hoc tests showed that the difference was between hard and easy topics and hard and medium topics. Results also showed that as topics became more difficult, subjects used more suggestions. However, the differences in these means was not significant [ F (2, 219)=1.06, p =.368]. Of the two, subjects used more query suggestions than term suggestions when issuing ne w queries, and in general, the number of each of these increased as topic difficulty increased. Subjects X  mean responses to th e perceived effectiveness and usability items from the Exit Questionnaire are presented in Table 4. Table 5 shows results of ANOVA tests of the differences between these means. Overall, s ubjects rated the query suggestion system higher than the term s uggestion system on 7 of the 11 items. Two of the items concer ned the suggestions X  abilities to help the subject think of new approaches to searching and better understand the topics. For both of these items, query suggestions 
Questionnaire Item TS QS SGS UGS Table 5. Results of ANOVA tests of the differences in means For four items, subjects rated th e term suggestion system higher than the query suggestion system, although this difference was only significant for one item. Tw o of these items were related to the execution of the suggestions : one asked subjects to rate how helpful the suggestions were in modifying their queries and another asked about how easy the s uggestions were to use. The responses to the first item might be an artifact of the item itself and how subjects X  perceived of the suggestions  X  the terms allowed subjects to modify their current queries, while the queries replaced their original queries. Although both actions could be considered query modi fication, it seems that subjects distinguished between modifying and issuing a new query. The latter item perhaps gets at the idea that the terms can be used independently, while the queries need to be used as unit. Subjects X  responses to anothe r of these items where term suggestion was rated higher than query suggestion were also suggestions selected this opti on. Very few subjects said the suggestions were equally usef ul. With respect to ease of operating and integration into s earching, again, more subjects picked query suggestions over te rm suggestions and this was relatively consistent regardless of whether they received system-generated or user-generated suggestions. These results are somewhat inconsistent with subjects X  numeric responses to Item 5, where term suggestion was ra ted higher (these differences were not statistically significant). Overall, more subjects preferred query suggestion, although the preference difference was most pronounced for subjects who received user-generated suggestions. Subjects were nearly equally likely to pick term, query or no preference in the system-generated condition. Despite some differences in subjects X  responses to these three questions, Chi-square tests show ed no significant differences in the distributions. The general message seems to be that subjects prefer query suggestions and that those w ho received system-generated suggestions did not always pref er one type of suggestion over the other. Follow-up responses provided insight into subjects X  preferences. Generally, subjects X  comments who received system-generated suggestions did not differ much from those who received user-generated s uggestions. However, there was more mention of random suggesti ons in the system-generated group. Several subjects mentioned that they preferred their own ideas to suggestions for either system. Those who preferred the query suggestions liked the all in one approach and the ability to click once to get results. They also liked the specificity and focus of the suggestions, and commented that the queries presented whole ideas. A subject commented,  X  X uery suggestion was faster, eas ier and provided a better understanding of how I should perform my search. There may be a slight bias because I X  X  not used to term suggestion but I found it a bit uncomfortable to use. X  Many subjects saw the terms as being jumbled together or taking too much effort to execute . Those who preferred the term suggestions thought that the terms provided more flexibility and that the suggested queries were too similar. They liked being able to refine their existing queri es instead of starting new ones. Most subjects seemed to view the terms as a way to modify their existing queries, although it was possible to create new queries with the terms. A subject stated,  X  X ith the terms, if you used them, it added them on to your already listed search which manipulated, this would give s ubjects greater control over query formulation and allow them to be more discriminating and use their judgment to guide the assistance. We proposed and evaluated a feature which used pseudo-relevance feedback and clustering to generate query suggestions. We compared the query suggestions to term suggestions and examined diffe rences between automatically generated suggestions and thos e generated by humans in a previous experiment. We found that subjects did not use the suggestion features that often, but when they did, more query suggestions were used. We also found that subjects submitted more queries and used more suggestions for difficult topics. Subjects who received user-gener ated suggestions saved more documents found through suggestions; the most were saved for query suggestions. snDCG show ed that the best performance was achieved by those who received user-generated query suggestions and that there appear ed to be an interaction with source of suggestions and sugge stion type: those who received user-generated suggestions did better with query suggestions, while those with system-generated terms did better with term suggestions. Query suggestions were rated higher for the majority of the perceived effectiveness and satisfaction items and more subjects preferred them to the term suggestions. There was no clear preference for user-generated suggestions over system-generated system which was encouraging since these results show that our automatic techniques were perceived to be at least as good as user-generated suggestions. Future work will investigate alternative techniques for automatically creating query suggestions, examine wh en and how users integrate suggestions into their searches and experiment with hybrid query and term suggestion features that help users effortlessly learn more about their topics while not taking away control. [1] Beaulieu, M. &amp; Jones, S. (1998). Interactive searching and [2] Beerferman, D., &amp; Berger, A. (2000). Agglomerative [3] Belkin, N. J., Cool, C., Kelly, D., Lin, S. J., Park, S. Y., [4] Billerbeck, B., Scholer, F., Williams, H. E., &amp; Zobel, J. [5] Fitzpatrick, L., &amp; Dent, M. (1997). Automatic feedback [6] Freyne, J., Farzan, R., Brusilovsky, P., Smyth, B., &amp; [7] Hsieh-Yee, I. (1993).Effects of search experience and 
