 We describe the use of the social reference management website CiteULike for recommending scientific articles to users, based on their reference library. We test three different collaborative filter-ing algorithms, and find that user-based filtering performs best. A temporal analysis of the data indexed by CiteULike shows that it takes about two years for the cold-start problem to disappear and recommendation performance to improve.
 H.3 [ Information Storage and Retrieval ]: H.3.4 Systems and Software; H.3.5 Online Information Services; H.3.7 Digital Li-braries Algorithms, Measurement, Performance, Experimentation
One of the trends within the Web 2.0 paradigm is a shift in infor-mation access from local and solitary, to global and collaborative. Instead of storing, managing, and accessing personal information on only one specific computer or browser, personal information management and access has been moving more and more to the Web. Social bookmarking websites are clear cases in point: in-stead of keeping a local copy of pointers to favorite URLs, users can instead store and access their bookmarks online through a Web interface. The underlying application then makes all stored infor-mation sharable among users, allowing for improved searching and generating recommendations between users with similar interests.
A special kind of social bookmarking services X  X nd the focus of our paper X  X re social reference managers such as CiteULike, Con-notea, Bibsonomy, and 2Collab 1 that aid users in managing their reference collection. All of these services allow users to bookmark any Web page or reference they choose, and in addition they offer
Availabl eat http://www.citeulike.org , http:// www.connotea.org , http://www.bibsonomy.org ,and http://www.2collab.co m respect ively.
 Copyright 2008 ACM 978-1-60558-093-7/08/10 ... $ 5.00. special functionality for certain academic resources, such as linking to online versions of papers and special access to metadata specific to academic resources.

All four mentioned bibliographical reference managers encour-age users to organize their references with one or more tags, or keywords. These in turn enable users to view all references, from any user, associated with a chosen tag, as well as information about the popularity of a reference. This same linking is also applied to the author level, so that users can browse other users who added ref-erences to publications written by a specific author. These features can help users to better cope with the information overload that is as overwhelming in the academic community as it is on the Web, with an ever-increasing number of journals, books, and conference proceedings being published every year. This overload makes it hard to keep up with interesting new work, or to get a complete overview of relevant literature on specific topics.

For these features to be effective, active use of the online sys-tem on the part of the user (searching, browsing) is needed. Our interest lies in using recommender systems to relieve this burden and automatically find interesting and related reading material for the user. A recommender system is a type of personalized infor-mation filtering technology used to identify a sets of items that are likely to be of interest to a certain user. One particular class of rec-ommendation algorithms is collaborative filtering (CF), that base recommendations on the opinions or actions of other like-minded users. The motivation here is that a user will be more satisfied with recommended items that are liked by like-minded users, than by items that are picked randomly or based on overall popularity.
In this paper, we focus on using one of these social reference managers, CiteULike, to generate reading lists for scientific arti-cles based on a user X  X  online reference library. We describe the construction of a test collection based on the services offered by CiteULike and apply three different CF algorithms to our data. We also analyze the data across its temporal dimension: we use pub-licly available activity logs to determine how recommendation per-formance changes as the website grows over time.
 The paper is structured as follows. We discuss related work in Section 2. We discuss CiteULike, how our test collection was cre-ated, and what issues we ran into in greater detail in Section 3. In the following Section 4 we describe our experimental setup and evaluation, followed by the results in Section 5. Section 6 contains the results of our temporal analysis of the different algorithms. We conclude in Section 7 and highlight possible future work.
Most of the work related to recommending interesting informa-tion with respect to the user X  X  current interest or task has focused on creating information management agents. Maes (1997) was among the first to signal the need for information filtering agents that can reduce overload [ 8]. Since then, several types of agents have been prototyped and developed for many different fields, such as the Web, music, and academic writing. See Montaner et al. (2003) for a comprehensive overview of agents available on the Web.
Yet there have been only a handful of approaches to recommend-ing interesting academic articles to users, McNee et al. (2006) ar-guably being the most prominent one. McNee frames the interac-tion between users and recommender systems, focusing on recom-mending interesting research papers from a user-centric perspec-tive. He identifies the different tasks a recommender system could perform to assist the user, such as finding a starting point for re-search on a particular topic, and maintaining awareness of a re-search field. Recommendations are generated on the basis of cita-tions in scientific papers [ 10].

Basu et al. (2001) focus on the related problem of recommending conference paper submissions to reviewing committee members [1]. They use a content-based approach to paper recommendation, using the Vector Space model with tf  X  idf weighting. Another re-lated area of research is the development of recommender systems that employ folksonomies. Most of the work so far has focused on recommending tags for bookmarks. J  X  aschke et al. (2007), for in-stance, compared two different CF algorithms with a graph-based algorithm for recommending tags in Bibsonomy. They found that the graph-based algorithm outperforms the CF algorithms only for the top 3 ranks [ 6]. Mishne (2006) performs similar experiments when predicting tags associated with blog posts [ 11]. In our ex-periments we focus on CiteULike as the social reference manager. Capocci et al. analyze the small-world properties of the CiteULike folksonomy [ 2].
CiteULike is a website that offers a  X  X  free service to help you to store, organise, and share the scholarly papers you are reading X  It allows its users to add their academic reference library to their online profile on the CiteULike website. At the time of writing, CiteULike contains around 885,310 unique items, annotated by 27,489 users with 174,322 unique tags. Articles can be stored with their metadata (in various formats), abstracts, and links to the pa-pers at the publishers X  websites. Users can also add reading prior-ities, personal comments, and tags to their papers. CiteULike also offers the possibility of users setting up and joining groups that con-nect users sharing academic or topical interests. These group pages report on recent activity, and offer the possibility of maintaining discussion fora or blogs. The full text of articles is not accessible from CiteULike, although links to online articles can be added.
CiteULike offers daily dumps of their core database 2 . We used the dump of November 2, 2007 as the basis for our experiments. A dump contains all information on which articles were posted by whom, with which tags, and at what point in time. It does not, however, contain any of the other metadata described above, so we crawled this metadata ourselves from the CiteULike website using the article IDs. We collected the following five types of metadata: Topic-related metadata including all metadata descriptive of the Person-related metadata such as the authors of the article as well See http://www.citeulike.org/faq/data.adp .
 Temporal metadata such as the year and, if available, month of Miscellaneous metadata such as the article type. The extracted User-specific metadata including the tags assigned by each user,
As CiteULike offers the possibility of users setting up groups that connect users that share similar academic and topical interests, for each group we collected the group name, a short textual descrip-tion, and a list of its members.
After crawling and data clean-up, our collection contained a total of 1,012,898 different postings, where we define a posting as a user-item pair in the database, i.e. an item that was added to a CiteULike user profile. These postings comprised 803,521 unique articles posted by 25,375 unique users using 232,937 unique tags. Meta-data was available for 543,433 of the 803,521 articles 3 . CiteULike contained 1,243 different groups with 2,301 different users being a member of one or more groups, corresponding to 9.1% of all users. We did not crawl the full text of publications, but 33.7% of the articles included the abstract in their metadata.
McNee identifies eight different tasks that a recommender sys-tem could fulfill in a digital library environment [ 10]. Not all of these tasks are equally applicable in the CiteULike environment, and not all of them can be fulfilled using the collection we created. However, a social reference manager could arguably fulfill addi-tional, new tasks not applicable in a digital library environment. In this paper we focus on the task of generating list of related pa-pers based on a user X  X  reference library. This task corresponds most closely to McNee X  X  tasks of Fill Out Reference Lists and Maintain Awareness [10]. In contrast to McNee X  X  approach of using cita-tions, we use the direct user-item preference relations to generate our recommendations from. In order to evaluate different recommender algorithms on the CiteULike data and to compare the usefulness of the different infor-mation we have available, we need a proper framework for experi-mentation and evaluation. Recommender systems evaluation X  X nd the differences with IR evaluation X  X ave been addressed by, among others, Herlocker et al. [ 4, 5], the latter identifying six discernible recommendation tasks. The recommendation task we evaluate here is the  X  X ind Good Items X  task 4 , where users are provided with a ranked list of recommended items, based on their personal profile.
Following common practice in recommender system evaluation [4, 5, 10], to ensure that we would be able to generate reliable rec-ommendations, we select a realistic subset of the CiteULike data set by only keeping the users who have added 20 items or more to the personal profile. In addition, we filter out all articles that oc-cur only once, since these items do not contain sufficiently reliable ties to the rest of the data set, and thus would only introduce noise
The overwhelming majority of the articles with missing metadata were spam articles. How we detected this is beyond the focus of this paper.
Also known as Top-N recommendation. for our CF algorithms. This procedure generates a set of 258,701 user-item pairs, with 2,539 unique users, and 87,908 unique items.
When generating predictions, we withhold 10 randomly selected items from each user, and generate predictions by using the remain-ing data as training material. As retrieval algorithms, recommender algorithms tend to be controlled by several parameters. One way of optimizing these parameters is by maximizing performance on a given data set. Such tuning, however, tends to overestimate the ex-pected performance of the system, so in order to prevent this kind of overfitting we used 10-fold cross-validation [ 9 ].

We first divided our data set into a training and a test set by ran-domly selecting 10% of the users to be in our test set. Final per-formance was evaluated on this 10% by withholding 10 items from each user, and using the remaining profile items together with the training set to generate the recommendations for those 10%. In order to properly optimize parameters we divided our training set (containing 90% of the users) by randomly dividing the users over 10 folds, each containing 10% of the training users. Each fold is used as a validation set once, with 10 items being withheld for each validation fold user. The remaining profile items and the data in the other 9 folds are then used to generate the predictions. The final values for our evaluation metrics on the withheld items were then averaged over the 10 folds.
In our evaluation, we adopt an IR perspective by treating each of the users as a separate query or topic. The 10 withheld items for each user make up the relevant items for which we have relevance judgments. For each user, a ranked list of items is produced and evaluated on whether these withheld items show up in the result list. While it is certainly possible and very likely that the recom-mendation lists contain other recommendations that the user would find relevant or interesting, we cannot know this without the user judging them. This means that because our relevance judgments correspond to items added to a user X  X  profile, we can never have any items judged as being not relevant without user intervention.
Herlocker et al. [ 5 ] assesses the usefulness of different metrics for each of the six recommendation tasks they identified. For our  X  X ind Good Items X  task, they find that metrics taking into account the ranking of the items are most appropriate. We therefore evalu-ated our recommender system using Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision @ 10 (P@10). In addition to these IR metrics, we also measured coverage: certain recommendation algorithms need enough data on a user or item for them to be able to reliably generate recommendations. Not all algo-rithms will be able to cover every user or item. We therefore mea-sure user coverage (UCOV), which is the percentage of all users for which we were able to generate any predictions at all.
In the preliminary experiments described in this paper we com-pare three different CF algorithms. The term collaborative filtering was first used by Goldberg et al. [ 3 ] and describes a class of al-gorithms that, instead of looking at the content, use data about all users X  preferences for items to generate recommendations for the so-called  X  X ctive X  user. We use and briefly describe the two most simple variants: user-based filtering and item-based filtering user-based filtering, the active user is matched against the database to find the neighboring users that the active user has a history of agreeing with. Once this neighborhood has been identified, all ob-jects in the neighbors X  profiles unknown to the active user are con-sidered as possible recommendations and sorted by their frequency
See [ 14 ] for an overview of more sophisticated CF algorithms. in that neighborhood. A weighted aggregate of these frequencies is used to generate the recommendations [ 4 ]. Item-based filtering turns this around by matching items against the database to find the neighborhood of similar items [ 13 ].

We test three different algorithms implemented in the freely avail-able Suggest recommendation engine [ 7 ]. The first model is an item-based filtering approach that uses the cosine similarity met-ric to determine the similarity between items. Model 2 is an item-based algorithm that calculates similarity based on conditional prob-ability. Finally, model 3 is a user-based algorithm that uses the co-sine similarity metric to determine similarity between users. Item-based filtering tends to perform well when there are more users than items in the database whereas user-based filtering works better in the reverse situation. We therefore expect the user-based filtering algorithm to outperform the other two algorithms.
The CF algorithms we employ in our study have one impor-tant parameter that can be tuned: the neighborhood size k , i.e. the number of similar users used to generate the predictions. We tune this parameter for each of the models, using our 10-fold cross-validation setup described in Section 4.1 , varying k between 1 and 500, evaluating performance at each step. Figure 1 displays the effect of neighborhood size on the MAP scores of the three algo-rithms for k up to 140; MAP values stay the same for k &gt; other metrics show a similar trend over the different k values. Figure 1: The effect of neighborhood size k on MAP for the three algorithms.

The two item-based algorithms, models 1 and 2, both have an optimal k of 500. Performance increases for Models 1 and 2 as k increases, but the recommender implementation ran out of mem-ory when we tried increasing k beyond 500. The optimal value of k for Model 3 lies between 4 and 8, with none of the differences in this range being statistically significantly different. We there-fore picked a k of 5 as the optimal value for Model 3. However, for all values of k the user-based filtering algorithm significantly outperforms the item-based filtering algorithm ( p &lt; 10 nal performance of the three models on the test set is displayed in Table 1 . Model 3 outperforms the other models significantly on all measures ( p &lt; 10  X  6 ) and shows acceptable performance for a preliminary approach.
CiteULike X  X  daily database dumps offer us the unique opportu-nity of analyzing the influence of growth of a social bookmarking Table 1: Results of our three CF algorithms on the data set. service such as CiteULike on recommendation performance. The database dumps contain the time stamps for each posting; we used these to construct growth curves of the different algorithms from a temporal perspective. We gradually increased the size of our data set of user-item pairs by a month at a time and used the same setup and optimal parameters at each temporal step. The first item was added to CiteULike on November 4, 2004, giving us 37 months of data. We repeated these steps 10 times and calculated the average MAP scores.

Figure 2 shows the MAP scores for the three different algorithms plotted against the months in the CiteULike data set. Throughout the 37 months, the user-based filtering method always outperforms the other two item-based models. All CF algorithms achieve rel-atively high MAP scores in the initial months, after which perfor-mance remains low and erratic for the first two years. The last months of 2006 mark a turning point where a critical mass seems to have been reached. From that point on, recommendation perfor-mance starts to climb and even triples in the span of one year for all three algorithms. With the exception of the outliers in the first few months, this seems to be a clear illustration of the cold-start prob-lem CF algorithms suffer from at a system-wide level: it is hard to generate recommendations for new items in the start-up phase, when there is not enough usage data about new items to make reli-able correlations with other items [ 5 ].
Figure 2: Performance on the CiteULike data set over time.
In this paper we demonstrated how a social reference manager can be used as a test collection for recommending research pa-pers. The data we collected represent many different aspects of both users and their annotations, and offers many opportunities for testing and combining different representations and recommender algorithms. We found that a user-based filtering algorithm yields the best performance. The likely explanation for this is that the data set contains a magnitude more items than users, which makes it both harder and slower to perform item-based filtering. Another interesting observation is that the optimal neighborhood size for item-based filtering is five users. This corresponds rather well to the average group size of 4.8 on CiteULike.

We also performed a temporal analysis on the data, and identified a clear cold-start problem for CF algorithms on the CiteULike. The results show that it took about two years for CF performance to start improving to a useful level. An interesting question is whether this two year period is specific to CiteULike or that a similar pattern would emerge when repeating these experiments on other social bookmarking websites. This is an important and interesting point for future work.

In other future work, we plan to experiment with and combine several different recommender algorithms, based on the different contextual and metadata information present in the CiteULike data set. Content-based algorithms should be able to take advantage of the full text of the documents and the metadata, while activity logs can be used to take recency effects into account. The CiteULike folksonomy offers another promising avenue of contextual recom-mendation material. The different algorithms based on the different contextual data will then be combined to determine the optimal rec-ommendation process. Our end goal is to test our methods on users for one or two of the tasks identified by McNee et al. [ 10 ].
The work of Toine Bogers is funded by the IOP-MMI program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the ` A Propos project. Antal van den Bosch is funded by NWO, the Netherlands Organization for Scientific Research.
