 Google, Mountain View, CA 94043 Shai Shalev-Shwartz SHAI @ TTI -C . ORG Toyota Technological Institute, Chicago, IL, 60637 Google, Mountain View, CA 94043 A prevalent machine learning approach for decision and prediction problems is to cast the learning task as penal-ized convex optimization. In penalized convex optimiza-vector w , which minimizes a convex objective function in w with an additional penalty term that assesses the com-plexity of w . Two commonly used penalties are the 1-norm and the square of the 2-norm of w . An alternative but mathematically equivalent approach is to cast the prob-lem as a constrained optimization problem. In this setting we seek a minimizer of the objective function while con-straining the solution to have a bounded norm. Many re-cent advances in statistical machine learning and related fields can be explained as convex optimization subject to a 1-norm constraint on the vector of parameters w . Im-posing an  X  encourages sparse solutions, i.e a solution for which many components of w are zero. When the original dimension of w is very high, a sparse solution enables easier inter-pretation of the problem in a lower dimension space. For the usage of  X  ing see for example (Tibshirani, 1996) and the references therein. Donoho (2006b) provided sufficient conditions for obtaining an optimal  X  cent work on compressed sensing (Candes, 2006; Donoho, 2006a) further explores how  X  recovering a sparse signal sampled below the Nyquist rate. The second motivation for using  X  learning problems is that in some cases it leads to improved generalization bounds. For example, Ng (2004) examined the task of PAC learning a sparse predictor and analyzed cases in which an  X  than an  X  In this paper we re-examine the task of minimizing a con-vex function subject to an  X  the solution. We are particularly interested in cases where the convex function is the average loss over a training set of m examples where each example is represented as a high-dimensional vector as well. Recent work on constrained optimization for machine learning indicates that gradient-related projection algorithms are more effi-cient in approaching a solution of good generalization than second-order algorithms when the number of examples and the dimension are large. For instance, Shalev-Shwartz et al. (2007) give recent state-of-the-art methods for solv -ing large scale support vector machines. Adapting these recent results to projection methods onto the  X  algorithmic challenges. While projections onto  X  straightforward to implement in linear time with the ap-propriate data structures, projection onto an  X  more involved task. The main contribution of this paper is the derivation of gradient projections with  X  projection with  X  Our starting point is an efficient method for projection onto the probabilistic simplex. The basic idea is to show that, after sorting the vector we need to project, it is possible to calculate the projection exactly in linear time. This idea was rediscovered multiple times. It was first described in an abstract and somewhat opaque form in the work of Gafni and Bertsekas (1984) and Bertsekas (1999). Crammer and Singer (2002) rediscovered a similar projection algorithm as a tool for solving the dual of multiclass SVM. Hazan (2006) essentially reuses the same algorithm in the con-text of online convex programming. Our starting point is another derivation of Euclidean projection onto the sim-plex that paves the way to a few generalizations. First we show that the same technique can also be used for project-ing onto the  X  components of the vector to be projected and thus requires O ( n log( n )) time. We next present an improvement of the algorithm that replaces sorting with a procedure resemblin g median-search whose expected time complexity is O ( n ) . In many applications, however, the dimension of the feature space is very high yet the number of features which attain non-zero values for an example may be very small. For in-stance, in our experiments with text classification in Sec. 7 , the dimension is two million (the bigram dictionary size) while each example has on average one-thousand non-zero features (the number of unique tokens in a document). Ap-plications where the dimensionality is high yet the number of  X  X n X  features in each example is small render our second algorithm useless in some cases. We therefore shift gears and describe a more complex algorithm that employs red-black trees to obtain a linear dependence on the number of non-zero features in an example and only logarithmic dependence on the full dimension. The key to our con-struction lies in the fact that we project vectors that are th e sum of a vector in the  X   X  X lmost X  in the  X  In conclusion to the paper we present experimental results that demonstrate the merits of our algorithms. We compare our algorithms with several specialized interior point (IP ) methods as well as general methods from the literature for solving  X  data (the MNIST handwritten digit dataset and the Reuters RCV1 corpus) for batch and online learning. Our projec-tion based methods outperform competing algorithms in terms of sparsity, and they exhibit faster convergence and lower regret than previous methods. We start by establishing the notation used throughout the paper. The set of integers 1 through n is denoted by [ n ] lower case bold face letters. We use the notation w  X  b to designate that all of the components of w are greater than b . We use kk as a shorthand for the Euclidean norm kk 2 . The other norm we use throughout the paper is the 1 norm of the vector, k v k this paper. To that end, we let v statistic of v , that is, v In the setting considered in this paper we are provided with a convex function L : R n  X  R . Our goal is to find the minimum of L ( w ) subject to an  X  Formally, the problem we need to solve is Our focus is on variants of the projected subgradient method for convex optimization (Bertsekas, 1999). Pro-jected subgradient methods minimize a function L ( w ) sub-ject to the constraint that w  X  X , for X convex, by gener-ating the sequence { w ( t ) } via X } is Euclidean projection of x onto X . In the rest of the paper, the main algorithmic focus is on the projection step (computing an unbiased estimate of the gradient of L ( w ) straightforward in the applications considered in this pap er, as is the modification of w ( t ) by  X  ( t ) ). For clarity, we begin with the task of performing Euclidean projection onto the positive simplex; our derivation natu-rally builds to the more efficient algorithms. As such, the most basic projection task we consider can be formally de-scribed as the following optimization problem, minimize When z = 1 the above is projection onto the probabilistic simplex. The Lagrangian of the problem in Eq. (3) is
L ( w ,  X  ) = where  X   X  R is a Lagrange multiplier and  X   X  R n vector of non-negative Lagrange multipliers. Differenti-ating with respect to w The complementary slackness KKT condition implies that whenever w w i &gt; 0 All the non-negative elements of the vector w are tied via a single variable, so knowing the indices of these elements gives a much simpler problem. Upon first inspection, find-ing these indices seems difficult, but the following lemma (Shalev-Shwartz &amp; Singer, 2006) provides a key tool in de-riving our procedure for identifying non-zero elements. Lemma 1. Let w be the optimal solution to the minimiza-tion problem in Eq. (3). Let s and j be two indices such that v Denoting by I the set of indices of the non-zero compo-nents of the sorted optimal solution, I = { i  X  [ n ] : v 0 } , we see that Lemma 1 implies that I = [  X  ] for some 1  X   X   X  n . Had we known  X  we could have simply used Eq. (4) to obtain that and therefore Given  X  we can characterize the optimal solution for w as We are left with the problem of finding the optimal  X  , and the following lemma (Shalev-Shwartz &amp; Singer, 2006) pro-vides a simple solution once we sort v in descending order. Lemma 2. Let w be the optimal solution to the minimiza-tion problem given in Eq. (3). Let  X  denote the vector ob-tained by sorting v in a descending order. Then, the num-ber of strictly positive elements in w is  X  ( z,  X  ) = max .
 The pseudo-code describing the O ( n log n ) procedure for solving Eq. (3) is given in Fig. 1.
 I NPUT : A vector v  X  R n and a scalar z &gt; 0 We next modify the algorithm to handle the more general  X  -norm constraint, which gives the minimization problem We do so by presenting a reduction to the problem of pro-jecting onto the simplex given in Eq. (3). First, we note that if k v k Therefore, from now on we assume that k v k case, the optimal solution must be on the boundary of the constraint set and thus we can replace the inequality con-straint k w k Having done so, the sole difference between the problem have an additional set of constraints, w  X  0 . The follow-ing lemma indicates that each non-zero component of the optimal solution w shares the sign of its counterpart in v Lemma 3. Let w be an optimal solution of Eq. (7). Then, for all i , w Proof. Assume by contradiction that the claim does not hold. Thus, there exists i for which w be a vector such that  X  w  X  w  X  w is a feasible solution. In addition, k w  X  v k 2 2  X  X   X  w  X  v k 2 2 = ( w i  X  v i ) 2  X  (0  X  v i We thus constructed a feasible solution  X  w which attains an objective value smaller than that of w . This leads us to the desired contradiction.
 Based on the above lemma and the symmetry of the ob-jective, we are ready to present our reduction. Let u be a vector obtained by taking the absolute value of each com-ponent of v , u minimize Once we obtain the solution for the problem above we con-struct the optimal of Eq. (7) by setting w In this section we describe a more efficient algorithm for performing projections. To keep our presentation simple and easy to follow, we describe the projection algorithm onto the simplex. The generalization to the  X  straightforwardly incorporated into the efficient algorit hm by the results from the previous section (we simply work in the algorithm with a vector of the absolute values of v replacing the solution X  X  components w For correctness of the following discussion, we add an-other component to v (the vector to be projected), which we set to 0 , thus v start by examining again Lemma 2. The lemma implies v manipulations the above can be rewritten in the following somewhat simpler form:
X Given  X  and v The task of projection can thus be distilled to the task of finding  X  , which in turn reduces to the task of finding  X  the pivot element v task of finding an order statistic with an additional compli-cating factor stemming from the need to compute summa-tions (while searching) of the form given by Eq. (9). Our efficient projection algorithm is based on a modification of the randomized median finding algorithm (Cormen et al., 2001). The algorithm computes partial sums just-in-time and has expected linear time complexity.
 The algorithm identifies  X  and the pivot value v sorting the vector v by using a divide and conquer proce-dure. The procedure works in rounds and on each round either eliminates elements shown to be strictly smaller tha n v the algorithm maintains a set of unprocessed elements of v . This set contains the components of v whose relation-ship to v On each round of the algorithm we pick at random an in-dex k from the set U . Next, we partition the set U into two subsets G and L . G contains all the indices j  X  U whose components v that v current summation of entries in v greater than the hypoth-esized v Eq. (9), v ements in G participate in the sum defining  X  as given by Eq. (9). We can discard G and set U to be L as we still need to further identify the remaining elements in P that v v set L and v ends when U is empty.
 Along the process we also keep track of the sum and the number of elements in v that we have found thus far to be no smaller than v recalculate partial sums. The pseudo-code describing the keep the set of elements found to be greater than v implicitly . Formally, at each iteration of the algorithm we maintain a variable s , which is the sum of the elements in the set { v ignate the cardinality of the this set throughout the algo-rithm. Thus, when the algorithms exits its main while loop,  X  is the maximizer defined in Lemma 1. Once the while loop terminates, we are left with the task of calculating using Eq. (10) and performing the actual projection. Since P simply set  X  to be ( s  X  z ) / X  and perform the projection as prescribed by Eq. (6).
 Though omitted here for lack of space, we can also extend the algorithms to handle the more general constraint that P a i | w i | X  z for a i  X  0 . Before we dive into developing a new algorithm, we re-mind the reader of the iterations the minimization algo-rithm takes from Eq. (2): we generate a sequence { w ( t ) by iterating projection onto this set.
 In many applications the dimension of the feature space is very high yet the number of features which attain a non-zero value for each example is very small (see for in-stance our experiments on text documents in Sec. 7). It is straightforward to implement the gradient-related update s in time which is proportional to the number of non-zero rithm described in the previous section is linear in the di-mension. Therefore, using the algorithm verbatim could be prohibitively expensive in applications where the dimen-sion is high yet the number of features which are  X  X n X  in each example is small. In this section we describe a pro-scales linearly in the number of non-zero entries of g ( t ) and non-zeros in w ( t ) ).
 feature spaces is to represent the projected vector as a  X  X aw  X  vector v by incorporating a global shift that is applied to each non-zero component. Specifically, each projection step amounts to deducting  X  from each component of v and thresholding the result at zero. Let us denote by  X  shift value used on the t th iteration of the algorithm and by  X  The representation we employ enables us to perform the step in which we deduct  X  vector implicitly , adhering to the goal of performing a sub-linear number of operations. As before, we assume that the goal is to project onto the simplex. Equipped with these variables, the j th component of the projected vector after projected gradient steps can be written as max { v The second substantial modification to the core algorithm is to keep only the non-zero components of the weight vector in a red-black tree (Cormen et al., 2001). The red-black tree facilitates an efficient search for the pivot element ( v time which is logarithmic in the dimension, as we describe in the sequel. Once the pivot element is found we implic-itly deduct  X  vector by updating  X  that are less than v efficient and requires only logarithmic time (Tarjan, 1983) . The course of the algorithm is as follows. After t projected gradient iterations we have a vector v ( t ) whose non-zero el-ements are stored in a red-black tree T and a global deduc-tion value  X  just-in-time, i.e. when needed. Therefore, each non-zero weight is accessed as v zero elements of the vector. When updating v with a gradi-based vector g ( t ) with k non-zero components. This update is done using k deletions (removing v g i 6 = 0 O (log( n )) time the value of  X  t . Fig. 3 contains the algo-rithm for this step; it is explained in the sequel. The last step removes all elements of the new raw vector v ( t ) + which become zero due to the projection. This step is dis-cussed at the end of this section.
 In contrast to standard tree-based search procedure, to find  X  we need to find a pair of consecutive values in v that correspond to v of the smallest element that satisfies the left hand side of Eq. (9) while searching based on the condition given on the right hand side of the same equation. T is keyed on the val-ues of the un-shifted vector v t . Thus, all the children in the left (right) sub-tree of a node v represent values in v t are smaller (larger) than v . In order to efficiently find keep at each node the following information: (a) The value of the component, simply denoted as v . (b) The number of elements in the right sub-tree rooted at v , denoted r ( v ) cluding the node v . (c) The sum of the elements in the right sub-tree rooted at v , denoted  X  ( v ) , including the value itself. Our goal is to identify the pivot element v index  X  . In the previous section we described a simple con-dition for checking whether an element in v is greater or smaller than the pivot value. We now rewrite this expres-sion yet one more time. A component with value v is not smaller than the pivot iff the following holds: The variables in the red-black tree form the infrastructure for performing efficient recursive computation of Eq. (11). Note also that the condition expressed in Eq. (11) still hold s when we do not deduct  X  The search algorithm maintains recursively the number  X  and the sum s of the elements that have been shown to be greater or equal to the pivot. We start the search with the root node of T , and thus initially  X  = 0 and s = 0 . Upon entering a new node v , the algorithm checks whether the condition given by Eq. (11) holds for v . Since  X  and s were computed for the parent of v , we need to incorporate the number and the sum of the elements that are larger than v itself. By construction, these variables are r ( v ) and which we store at the node v itself. We let  X   X  =  X  + r ( v ) and  X  s = s +  X  ( v ) , and with these variables handy, Eq. (11) we know that v is either larger than the pivot or it may be the pivot itself. We thus update our current hypothesis for searching the left sub-tree (left ements smaller than v . If inequality  X  s &lt; v  X   X  + z hold, we know that v &lt; subtree (right naturally terminates once we reach a leaf, where we can also calculate the correct value of  X  using Eq. (10). Once we find  X   X  T smaller than  X  algorithm for splitting a red-black tree. This step is log-arithmic in the total number of non-zero elements of v t . Thus, as the additional variables in the tree can be updated in constant time as a function of a node X  X  child nodes in T , each of the operations previously described can be per-formed in logarthmic time (Cormen et al., 2001), giving us a total update time of O ( k log( n )) . We now present experimental results demonstrating the ef-fectiveness of the projection algorithms. We first report re -sults for experiments with synthetic data and then move to experiments with high dimensional natural datasets. In our experiment with synthetic data, we compared vari- X  -regularized least squares and  X  gression. We compared our methods to a specialized coordinate-descent solver for the least squares problem du e methods for both least squares and logistic regression (Koh et al., 2007; Kim et al., 2007). The algorithms we use are batch projected gradient, stochastic projected subgradie nt, and batch projected gradient augmented with a backtrack-ing line search (Koh et al., 2007). The IP and coordinate-wise methods both solve regularized loss functions of the form f ( w ) = L ( w ) +  X  k w k domain constraint, so our objectives are not directly com-parable. To surmount this difficulty, we first minimize L ( w )+  X  k w k 1 and use the 1-norm of the resulting solution w  X  as the constraint for our methods.
 To generate the data for the least squares problem setting, we chose a w with entries distributed normally with 0 mean and unit variance and randomly zeroed 50% of the vector. The data matrix X  X  R m  X  n was random with entries also normally distributed. To generate target values for the lea st squares problem, we set y = X w +  X  , where the com-ponents of  X  were also distributed normally at random. In the case of logistic regression, we generated data X and the vector w identically, but the targets y sign ( w x i ) with probability 90% and to  X  sign ( w x i otherwise. We ran two sets of experiments, one each for n = 800 and n = 4000 . We also set the number of ex-amples m to be equal to n . For the subgradient methods in these experiments and throughout the remainder, we set  X  =  X  0 / (  X  method are not descent directions; the noise will quickly disappear because the step sizes are proportional to 1 /  X  Fig. 4 and Fig. 5 contain the results of these experiments floating point operations. From the figures, we see that the projected subgradient methods are generally very fast at th e quickly, but their rate of convergence slows over time. The fast projection algorithms we have developed, however, al-low projected-subgradient methods to be very competitive with specialized methods, even on these relatively small problem sizes. On higher-dimension data sets interior poin t methods are infeasible or very slow. The rightmost graphs floating point operations for least squares and logistic re-gression with dimension n = 4000 . These results indicate that in high dimensional feature spaces, the asymptoticall y faster convergence of IP methods is counteracted by their quadratic dependence on the dimension of the space. We also ran a series of experiments on two real datasets with high dimensionality: the Reuters RCV1 Cor-pus (Lewis et al., 2004) and the MNIST handwritten digits database. The Reuters Corpus has 804,414 examples; with simple stemming and stop-wording, there are 112,919 uni-gram features and 1,946,684 bigram features. With our pre-processing, the unigrams have a sparsity of 1.2% and the bi-grams have sparsity of .26%. We performed  X  binary logistic regression on the CCAT category from RCV1 (classifying a document as corporate/industrial) us-ing unigrams in a batch setting and bigrams in an online set-ting. The MNIST dataset consists of 60,000 training exam-ples and a 10,000 example test set and has 10-classes; each image is a gray-scale 28  X  28 image, which we represent as we learned weights w j using the following Kernel-based  X  X imilarity X  function for each class j  X  X  1 , . . . , 10 } k ( x , j ) = X In the above, K is a Gaussian kernel function, so that K ( x , y ) = exp(  X  X  x  X  y k 2 / 25) , and S is a 2766 element support set. We put an  X  the following multiclass objective with dimension 27,660: minimize w 1 As a comparison to our projected subgradient methods on real data, we used a method known in the literature as either entropic descent, a special case of mirror descent (Beck &amp; Teboulle, 2003), or exponentiated gradient (EG) (Kivinen &amp; Warmuth, 1997). EG maintains a weight vector w sub-ject to the constraint that P easily be extended to work with negative weights under a 1-norm constraint by maintaining two vectors w + and w  X  . We compare against EG since it works well in very high di-mensional spaces, and it very quickly identifies and shrinks weights for irrelevant features (Kivinen &amp; Warmuth, 1997). At every step of EG we update where Z to be minimized. EG can actually be viewed as a pro-jected subgradient method using generalized relative en-tropy ( D ( x k y ) = P function for projections (Beck &amp; Teboulle, 2003). We can replace  X  of the gradient of f , to get stochastic EG. A step size  X  1 /  X  each experiment with EG, however, we experimented with learning rates proportional to 1 /t , 1 /  X  t , and constant, as well as different initial step-sizes; to make EG as competi-tive as possible, we chose the step-size and rate for which EG performed best on each individual test..
 Results for our batch experiments learning a logistic class i-fier for CCAT on the Reuters corpus can be seen in Fig. 6. The figure plots the binary logistic loss of the different al-gorithms minus the optimal log loss as a function of CPU time. On the left side Fig. 6, we used projected gradient descent and stochastic gradient descent using 25% of the training data to estimate the gradient, and we used the al-gorithm of Fig. 2 for the projection steps. We see that  X  projections outperform EG both in terms of convergence speed and empirical log-loss. On the right side of the fig-ure, we performed stochastic descent using only 1 training example or 100 training examples to estimate the gradient, using Fig. 3 to project. When the gradient is sparse, up-dates for EG are O ( k ) (where k is the number of non-zeros in the gradient), so EG has a run-time advantage over  X  projections when the gradient is very sparse. This advan-tage can be seen in the right side of Fig. 6.
 For MNIST, with dense features, we ran a similar series of tests to those we ran on the Reuters Corpus. We plot the multiclass logistic loss from Eq. (12) over time (as a function of the number gradient evaluations) in Fig. 7. The left side of Fig. 7 compares EG and gradient descent using the true gradient while the right figure compares stochas-tic EG and stochastic gradient descent using only 1% of the training set to estimate the gradient. On top of outper-forming EG in terms of convergence rate and loss, the  X  projection methods also gave sparsity, zeroing out between 10 and 50% of the components of each class vector w j in the MNIST experiments, while EG gives no sparsity. the RCV1 dataset using bigram features, comparing  X  projections to using decreasing step sizes given by Zinke-vich (2003) to exponential gradient updates. The  X  projections are computationally feasible because of algo-rithm 3, as the dimension of our feature space is nearly 2 million (using the expected linear-time algorithm of Fig. 2 sparse updates in online learning). We selected the bound left figure plots the cumulative log-loss for the CCAT and ECAT binary prediction problems as a function of the num-ber of training examples, while the right hand figure plots the sparsity of the  X  function of the dimension and as a function of the number of features actually seen. The  X  tained an active set with only about 5% non-zero compo-nents; the EG updates have no sparsity whatsoever. Our on-line  X  online regret (cumulative log-loss), and the  X  over all the examples on the CCAT task and 14.9% on ECAT (versus more than 15% and 20% respectively for EG).
 We thank the anonymous reviewers for their helpful and insightful comments.

