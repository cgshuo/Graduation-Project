 Information diffusion in social networks has been an active research field in about a decade. It is a natural phenomenon that information propagates from nodes to nodes over a social network, which acts like an epidemic. There are many applications on information diffusion, such as promoting an idea more effectively in a network. A well-known problem therein is called influence maximization , formulated by Kempe et al. [ 10 ]. The problem of influence maximization is to find a group of target nodes to be convinced of an idea initially to maximize the spread size, i.e. the number of nodes adopting the idea, on a given diffusion model.
 To model how information diffuses in a network, researchers have proposed various diffusion models from different aspects. In these diffusion models, the Independent Cascading (IC) model and the Linear Threshold (LT) Model [ 10 ] have been widely employed in many applications and have several variants [ 1 ]. Both models consider the influence strength of a neighbor. The key difference is that, for a node turning to adopt an idea, IC considers only the influence from exactly one activated neighbor with uncertainty, while LT considers the collaborative influence contribution from all activated neighbors. In other words, IC places importance on which neighbor tries to affect the node to adopt the idea whereas LT thinks highly on the overall influence contribution from neighbors. Nevertheless, the real world is so complicated that a simple concern is hard to capture such complexity. Many factors probably affect the decision of adoption. For example, an idea that has been adopted by most people will have more chance to influence somebody [ 9 ] and an idea is harder to be adopted by someone as time passes. Moreover, a person may have different strength of interests in different topics [ 1 ]. However, the factors considered by previous diffusion models in social networks are all fixed. For a different scenario of diffusion with new factors, previous diffusion models may not model the diffusion well, or are not applicable at all. Therefore, one usually has to propose a new diffusion model for modeling diffusion of a specific scenario better by considering new factors. corresponding algorithms, e.g. parameters learning and influence maximization, both become tedious. In this work, we aim to design a diffusion model which can consider multiple factors flexibly and further propose a framework of learning parameters of the model, related to information transmission likelihood between nodes and adoption prediction of a node. To the best of our knowledge, no exist-ing work has the same sight. Specifically, we propose a Multiple-Factors Aware Diffusion (MFAD) model which is able to consider multiple factors flexibly that may affect adoption behaviors. MFAD is a two-stage propagation model. In the first stage, called influence transmission , an activated node u tries to influence its inactivated neighbor v with a probability. If the influence of u is successfully transmitted to v , in the second stage, called adoption decision , v decides whether it becomes activated based on its considerations, predicted via its related classi-fication model trained on historical adoption information. Unfortunately, due to the limitation of observation in the real world, only positive instances are avail-able to train classifiers , which is hard to achieve good performance. We further design a mechanism to get unlabeled instances to help train nodes X  classifiers and propose the learning framework to learn the classifiers and transmission probabilities between nodes. Our contributions are summarized as follows. 1. Our proposed MFAD model is flexible to extend and change factors since 2. Our proposed learning framework is independent of factors considered and 3. Due to the limitation of observation on diffusion in the real world, to predict The remaining of the paper is organized as follows. We next review the related work in Section 2 . We introduce our diffusion model in Section 3 and how to learn the model in Section 4 . In Section 5 , we conduct experiments on a Twitter dataset. Finally, we conclude in Section 6 . In this section, we briefly review the related works on diffusion models in social networks and learning parameters of diffusion models.
 Diffusion models interpret how information spreads within a network. As mentioned above, IC and LT are two classical models and have been widely employed since they were connected to the influence maximization problem [ 10 ]. Recently, more factors of diffusion are explored in the literature like [ 1 ], [ 9 ]and [ 14 ], to name a few. N. Barbieri et al. [ 1 ] extend the IC model to consider topic distribution of items. T.-A. Hoang and E.-P. Lim [ 9 ] propose a model considering three factors, user virality, user susceptibility and item virality. Moreover, S.A. Myers et al. [ 14 ] explore not only internal influence from activated nodes in a network, but also external influence outside the network. However, as discussed above, the factors considered by these diffusion models are all fixed. For a dif-ferent scenario of diffusion with new factors, these models may not model the diffusion well, or are not applicable at all.
 Although diffusion models in social networks have been proposed for a long time, algorithms to learn parameters of a diffusion model are proposed recently. For example, K. Saito et al. [ 16 ] first propose a learning method for the IC model. The following up works mainly propose learning methods for their own diffusion models [ 1 ]. Moreover, A. Goyal et al. [ 7 ] propose the Credit Distribution model that directly estimates spread size from diffusion data without learning influence probabilities between nodes. Since we aim to design a learning framework that is independent of the diffusion factors considered, the above results do not apply to our scenario. In the work, our aim is to design a diffusion model which considers multiple factors flexibly for information propagation. We propose the Multiple Factors-Aware Diffusion (MFAD) model in the section.
 Given a social graph G =( V, E ), where V is the node set and E is the edge set composed of directed edges without multiple edges and self-loops, let p denote the probability of node u to successfully transmit influence to node v  X  u  X  X  out-neighbors N out ( u )after u is activated by an item i and let f denote a probabilistic classifier for node v where f v ( x ) considers multiple prede-fined features, i.e. factors, that affect the tendency of u to adopt item i after v is exposed to i and x is the feature vector of the exposure. Note that a nonprob-abilistic classification model, the outputs of which can be transformed to the probabilistic outputs [ 15 ], is applicable to f v ( x ), e.g. SVM with Platt scaling. The Multiple Factors-Aware Diffusion (MFAD) model is defined as follows. Definition 1. Multiple Factors-Aware Diffusion (MFAD) model. The propaga-tion of diffusion starts with initial seed nodes S 0  X  V which have adopted item i . In the first timestamp t 1 ,each u  X  S 0 tries to influence u  X  X  out-neighbors which are not activated by item i . The successful activation probability for v is calculated as p u,v f v ( x ) . The activated nodes in t tamp t 2 , new activated nodes in S 1 try to activate their inactivated out-neighbors in the same process as the above. The process runs iteratively. If S timestamp t j , the diffusion terminates. Note that the spread size can be expressed as | X  0  X  i  X  j  X  1 S i | and when a node becomes activated, it n ever turns to be inac-tivated.
 activation process in MFAD is shown as Figure 1 . In the first stage, called influ-ence transmission ,anode u tries to influence each v  X  N out activated with probability p u,v in timestamp t i  X  1 , where u is activated in times-tamp t i  X  2 . However, the influence successfully transmitted over the edge does not directly activate a node. Our model has the following stage, called adoption decision . In the second stage, if the influence of u is successfully transmitted with probability p u,v to v previously, v receives this influence and decides whether it becomes activated based on its considerations, predicted via its classification model f v ( x ). If v is activated, v will try to influence its neighbors in the next timestamp t i . Consider a news diffusing in an online social network, e.g. Face-book and Twitter. A user u posted a message about the news recently. Due to the ranking mechanism designed by Facebook or since there are too many messages, a friend v of u may not see the message, which is modeled by influence transmis-sion. Moreover, even if the friend v reads the message, v may consider whether to share or reply to the message based on several concerns, e.g. v  X  X  interests, the importance of the news, which is modeled by adoption decision. In contrast to the traditional diffusion models, MFAD is flexible to consider multiple factors and considers more in a microscopic view for information propagation. In this section, we propose a two-stage learning framework for MFAD since MFAD is a two-stage propagation model. In the first stage, the classifier of each node is trained, which corresponds to adoption decision, while the transmission probability between two connected nodes is estimated in the second stage, which corresponds to influence transmission. We first introduce the observed data. Observed Data. A propagation trace consists of activation records. Each record { u, v, i, t } represents that node u adopts an item i at timestamp t and the adop-tion is caused by u  X  X  in-neighbor v .If u is actually a seed of the item in the observed data, v does not exist and is set to be NIL . In reality, we usually do not observe that a node fails to influence others. In other words, we would only have positive instances for training classifiers directly from the propagation trace. We next discuss how to learn classifiers of nodes in such a situation. 4.1 Learning Classifiers of Nodes Due to the limitation of observation in the real world, only positive instances are available to train binary classifiers of each nodes. However, a classifier trained on only positive instances is hard to achieve good performance. In fact, unlabeled instances can provide more information for learning, e.g. feature distribution, and can be generated via observing that an inactivated out-neighbor of an activated node does not turn activated in the next timestamp. We use the term unlabeled instead of negative since an unlabeled instance may be positive or negative due to the limitation of observation. Thus, the task of the first-stage learning becomes training classifiers by using positive and unlabeled instances. In the literature [ 6 , 12 ], the problem is called positive and unlabeled learning . Among previous work on positive and unlabeled learning, C. Elkan and K. Noto [ 6 ] provide a principled way to assigning weights to unlabeled instances. Based on their work [ 6 ], we construct a framework to learn nodes X  classifiers for MFAD. In the follow-ing, we first describe how to obtain unlabeled instances from observed positive records and then describe how to train a node X  X  classifier based on positive and unlabeled instances.
 Obtaining Unlabeled Instances. With observed positive records, we can analyze the whole propagation trace to get positive instances for training nodes X  classifiers with ease. However, negative instances are hard to obtain due to two main reasons: (1) an item does not successfully be exposed to a node from its in-neighbor; (2) the observation window for a node is not long enough. Fortunately, we can generate unlabeled instances to help train a node X  X  classifier. Assume that we have the complete propagation trace which consists of posi-tive records in the format of ( u, s, i, t, o = 1) where the binary variable o indicates whether a record is an observed positive record in the trace or not. If a node u adopts an item i from s at timestamp t , we can observe u  X  X  out-neighbors who haven X  X  adopted i from timestamp t . For an out-neighbor v of u ,if v does not adopt i in the complete propagation trace, we generate an unlabeled record ( v, u, i, t ,o = 0), where t is the end observation time of propagation trace. How-ever, the approach is with high cost and does not work if the size of the trace is extremely large. For a program to sequentially trace the positive records in Algorithm 1. Unlabeled Record Generation chronological order, it has to memorize all items that a node has not adopted in order to generate unlabeled records at the end of tracing, which is impossi-ble for a single machine with limited memory size. Otherwise, multiple scans are needed, which incurs many disk I/O operations and therefore is time-consuming. generate unlabeled records. Let T be the time window to observe whether an out-neighbor v of u , for a positive record ( u, s, i, t, 1), adopts i before t + T . The main idea of the algorithm is that if v does not adopt i before t + T ,the algorithm generates an unlabeled record ( v, u, i, t, 0). Although the pseudo code of Algorithm 1 is written in a batch way, it is easy to adapt it to process positive records coming in a streaming way. Note that a feature vector x , i.e. an instance, is generated at the same time when a positive record is traced or an unlabeled record is generated in order to capture the state of an exposure.
 Training a Node X  X  Classifier. With the above approach, for a node u ,we can obtain positive instances P ( u ) and unlabeled instances u  X  X  classifier in order to predict the adoption tendency of an instance. Given an instance x , the goal is to predict p ( a =1 | x ), where a is a binary random variable to indicate whether the instance is positive ( a = 1) or negative ( a = 0). Recall that o is a binary variable to indicate an instance is observed ( o =1)or unlabeled ( o = 0). In the lemma derived in [ 6 ], p ( a =1 c = p ( o =1 | a = 1) is a constant value 1 . Based on the lemma, they [ 6 ] further reach the result on how to give weights to instances rigorously as the following. The weight of a positive instance is still unit, while an unlabeled instance have two copies, where one copy is a positive instance with weight p ( a =1 and the other copy is a negative instance with weight 1  X  subtasks: (1) to learn p ( o =1 | x ), (2) estimate c and (3) learn p ( a =1 Specifically, for a node u , (1) we first train a nontraditional classifier g p ( o =1 | x )on P ( u )  X  V ( u )and U ( u ), where the instances in selected from P ( u ), which is reserved as a validation set to estimate c . (2) Next, positive instances P ( u ) and negative instances N ( u ) to train a traditional classi-fier f u ( x )= p u ( a =1 | x ) for the node u . P ( u ) contains the instances in copies from the instances in U ( u ) with each weight p u sists of copies from the instances in U ( u ) with each weight 1 In the experiments, we use the logistic regression to train both g since its output probability is well-calibrated [ 6 ] by applying the above way. 4.2 Learning the Transmission Probability With the above P ( v ), U ( v ) and the trained classifier f we now describe how to learn transmission probability between two connected nodes. Let D = v  X  V ( P ( v )  X  U ( v )) denote the dataset for learning transmission probabilities. An instance x  X  D is in the format of ( f 1 u is the node that tries to activate v by item i before time t ( x t ( x = 0), o is a binary variable to indicate whether v is activated during the observation in Algorithm 1 and f 1 , f 2 , ..., f m are factors of adoption, calculated in the same time of running Algorithm 1 for the exposure. An instance x unlabeled if x o = 0; otherwise, x is positive.
 We train the MFAD model via maximizing the likelihood of D model. Let D s denote the data of node s in D , i.e. D s = and let  X  denote all parameters of the MFAD model to learn, i.e. all transmis-between node s and its in-neighbors N in ( s ). Assuming adoptions between nodes are independent, the complete data log-likelihood can be expressed as follows. Note that since we want to learn transmission probability between two connected nodes, we exclude an instance x , x u of which is NIL ,from f ( x ) for each node s has trained in the above, the data likelihood of each only related to  X  s . To maximize Eq.( 1 ) is equal to maximizing each In reality, the diffusion happens in a continuous time space, while MFAD is a discrete time-based diffusion model. We include time constraints  X  to decide the validity of an instance. Let D + = { x  X  D | D | x =0 } . We define D + s and D  X  s as follows.
 s = { x  X  X  s | x o =1  X  X  X  y  X  X  + ( y v = x u  X  y i = x i  X 
D  X  s = { x  X  X  s | x o =0  X  X  X  y  X  X  + ( y v = x u  X  y i = x is v  X  X  in-neighbor set. The data likelihood of node s is then defined as where log L (  X  s ; D s )= To find p q,s by maximizing the above log likelihood, let rithm [ 2 ]. The Brent X  X  algorithm uses a combination of golden section search and successive parabolic interpolation. For an initial good guess p converge fast, we apply the first order Taylor series to approximate at p q,s =0as  X  f s ( x )  X  f s ( x ) 2 p q,s .Thus,theEq.( 8 ) becomes and by some mathematical manipulation we get  X  p q,s =  X  C be a real number greater than 0 and obviously, B and C are non-negative real numbers. In some situation,  X  p q,s will not be a valid probability value, the value In the section, we conduct experiments on a Twitter dataset to evaluate the effectiveness on activation prediction. We first describe the setup. 5.1 Setup Dataset. We use the real dataset collected from Twitter by L. Weng et al. [ 17 ]. We use standard preprocessing steps, similar to the steps used in [ 1 ]to clean diffusion data. However, in order to obtain enough size of training data for training nodes X  classifiers, we allow multiple activation records of the same item for a node. After the preprocessing, each node has at least 20 activation records, i.e. retweets and tweets with hashtags in Twitter, and each item, i.e. hashtags, are adopted by at least 20 nodes. Moreover, there is no isolated node left. The remaining social graph consists of 24 , 045 nodes and 871 , 745 directed edges. The remaining activation records contain 8 , 427 different items and the number of all activation records is 1 , 105 , 316. The dataset spans from March 23 to April 25 in 2012, approximately one month long.
 Factors. We first define some notations. Let deg in ( v )and deg in-degree and out-degree in the graph. For an item i ,weuse t the earliest time in which i is adopted by some node in the data and t to denote the earliest time in which i is exposed to v by an in-neighbor of v that time t and node loc ( i, t, v ) to represent in-neighbors of v which are activated by item i before time t . For a directed edge from u to v ,weuse ratio to denote the ratio that v  X  X  adoptions are caused by u and ratio denote the ratio that v  X  X  adopted items are the same as u  X  X  adopted items before time t .
 For an instance x =( f 1 , f 2 , ..., f m ) [ u,v,o,t,i ] item i before time t ( x o = 1) or at time t ( x o = 0), the features composed of three types, structure-based, time-based and history-based features. Structure-based features include deg in ( u ), deg out ( u ), deg number of common neighbors between u and v . Time-based features are t t the item and if it is unavailable in the dataset, we assume t ( i, u ) first two are able to reflect global and local freshness. The last one is to measure the adoption latency. History-based features are | node glo ratio from ( v, u, t )and ratio same ( v, u, t ). Thus, there are m = 12 features in total for training a node X  X  classifier in the experiment.
 Instances. We use the approach introduced in Section 4.1 to generate positive and unlabeled instances from the dataset with different T for the most active 100 nodes, measured by the number of positive records in the whole dataset. The statistics of training and testing instances are summarized as Table 1 . In both training and testing sets, we exclude instance x , x u of which is NIL , since we want to learn transmission probability between two connected nodes for diffusion models. Note that the positive instances for testing consist of positive instances and unlabeled positive instances, while the negative instances for testing consist of unlabeled negative instances. For each T , we use the earliest 20% instances as the training set. From the latest 80% instances, the testing set only contains instances related to p u,v that is trained in the training data for MFAD. Thus, the satisfied testing instances are not too many. Moreover, the number of unlabeled instances for training is much more than the labeled positive instances since the earliest 20% time (  X  6 . 6 days) is relative short and when a node u adopts an item i at time t ( i, u ) but u  X  X  in-neighbors all adopt i before t ( i, u ) unlabeled positive instances will be generated.
 Methods. We include the following three methods to predict activations of nodes: (1) the logistic regression directly trained on positive and unlabeled instances (LOGIST), which is the classical approach, (2) our proposed learn-ing framework for the MFAD model (MFAD) and (3) the independent cascading model (IC). Note that only MFAD and IC are diffusion models, while LOGIST is a classification algorithm only and cannot be applied to other applications on diffusion, e.g. influence maximization [ 10 ]. We select the IC model instead of the LT model since IC is also a probabilistic diffusion model. The parameters of IC are inferred by the maximum-likelihood estimation conducted in the same app-roach of Section 4.2 . For two connected nodes u and v , the influence probability p and MFAD, the class imbalance problem is encountered, i.e. skewed class dis-tribution. We use SMOTE[ 4 ], which doubles the size of the minority class, and then apply SpreadSubsample [ 8 ] to undersample instances of the majority class to balance the class distribution. Moreover, we set time constraints  X  in Eq. ( 3 ) and ( 4 ) as the same value of T . All methods are implemented in Java with Weka [ 8 ] and executed in a PC with an Intel i7 3 . 4GHz CPU. The running time of a run of MFAD for the same T does not exceed 2 hours, including time for sampling, training 100 nodes X  classifiers and learning related transmission probabilities.
 Metrics. We use four metrics, precision, recall, F-Measure and accuracy, to measure the results of activation prediction, based on true positive ( TP ), false positive ( FP ), true negative ( TN ) and false negative ( FN ) instances. Precision 5.2 Results The overall results of activation prediction are shown in Figure 2 . Our MFAD outperforms the other two methods significantly in the overall metrics, F-Measure and accuracy. F-Measure in Fig. 2(a) concerns mainly on true positive, false pos-itive and false negative instances, while accuracy in Fig. 2(b) takes true negative instances into consideration. F-Measure is suitable for the scenario of retrieval of activated nodes whereas accuracy is more suitable for the scenario of spread estimation. MFAD works great for both scenarios. For the components of F-Measure, MFAD is very effective in precision as shown in Fig. 3(a) , which means the size of false positive instances is much smaller than those of LOGIST and IC. The recalls of MFAD and LOGIST are close to each other but much better than that of IC as shown in Fig. 3(b) . Moreover, as T increases, F-Measure and accuracy become better for all methods since the positive unlabeled instances are fewer and thus more positive instances are available for training classifiers. In summary, MFAD is the best method to predict activation of nodes among three methods. Most importantly, MFAD is a diffusion model and therefore can simulate how information diffuses whereas LOGIST cannot. IC is also a diffusion model, but it cannot reflect the state of an exposure precisely and thus do not model the diffusion well. Although there is an extension of IC in [ 1 ], called TIC, to consider the topic factor, the dataset does not have the detailed textual information of tweets and hence we do not include TIC in the experiment. Nevertheless, our MFAD model can consider the topic factor by defining new features for nodes X  classifiers with ease, which does not require the modifications of the learning framework.
 In this work, we propose the model of Multiple-Factors Aware Diffusion Model (MFAD) which explicitly models influence transmission and adoption decision and considers multiple factors flexibly that may affect adoption behaviors. The learning framework of MFAD is independent of factors considered and is effective as shown in the experiment. Therefore, MFAD has more flexibility and can be applied to different scenarios for different purposes with ease. In the future, we will design influence maximization algorithms for MFAD.

