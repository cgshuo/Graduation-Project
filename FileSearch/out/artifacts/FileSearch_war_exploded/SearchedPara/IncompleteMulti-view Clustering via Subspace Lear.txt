 Multi-view clustering, which explores complementary infor-mation between multiple distinct feature sets for better clus-tering, has a wide range of applications, e.g., knowledge management and information retrieval. Traditional multi-view clustering methods usually assume that all examples have complete feature sets. However, in real applications, it is often the case that some examples lose some feature sets, which results in incomplete multi-view data and notable performance degeneration. In this paper, a novel incomplete multi-view clustering method is therefore devel-oped, which learns unified latent representations and pro-jection matrices for the incomplete multi-view data. To ap-proximate the high level scaled indicator matrix defined to represent class label matrix, the latent representations are expected to be non-negative and column orthogonal. Be-sides, since data are often with high dimensional and noisy features, the projection matrices are enforced to be sparse so as to select relevant features when learning the latent space. Furthermore, the inter-view and intra-view data structure is preserved to further enhance the clustering performance. To these ends, an objective is developed with efficient opti-mization strategy and convergence analysis. Extensive ex-periments demonstrate that our model performs better than the state-of-the-art multi-view clustering methods in various settings.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering ; I.5.3 [ Pattern Recogni-tion ]: Clustering X  Algorithms Multi-view clustering; Incomplete multi-view data; Feature selection; Subspace learning; Graph regularization c  X  Fi gure 1: The overview of the proposed method.
 X 1 and X 2 are incomplete multi-view data with the red rectangular indicating examples with com-plete feature sets. We learn projection matri-ces U (1) and U (2) and the unified latent repre-sentations F (non-negative and column orthogonal ) fo r multi-view data. Here we constrain the projection matrix to be sparse so as to select rele-vant features from the possibly high dimensional and noisy feature sets. Besides, the inter-view and intra-view data structure is preserved whe n learning the latent space. Finally, the clustering results are ob-tained by performing the k -means algorithm on F .
Various kinds of real-world data appear in multiple modal-ities or come from multiple channels. For example, a web page can be described by both images and texts, and an image can be encoded by different visual features such as SIFT and GIST. We call such data multi-view data with each view representing a type of feature set. Usually, mul-tiple views provide complementary information for the se-mantically same data, which leads to the development of multi-view learning. By exploiting the complementary char-acteristics between multi-view data, multi-view learning can obtain better performance of learning tasks than relying on just one single view [27]. Till now, multi-view learning has been widely studied in a variety of areas, such as knowledge management, data mining, multimedia and information re-trieval [18, 27].

Multi-view clustering, as one of basic tasks of multi-view learning, provides a natural way to cluster multi-view dataset-s. Generally, the main challenge lies in the mining of the co mplementary information among multiple sources of in-formation. Fortunately, a number of promising approaches have been proposed, which can be roughly classified into four categories [27, 31, 8]. Methods in the first category are subspace based ones [5, 15, 16, 29, 28], which learn a latent space so that different views are comparable in that space. On the other hand, some methods are co-training based al-gorithms [1, 14, 33], which obtain the clustering results in an iterative clustering manner. The third category is called late fusion [3, 10, 13], which combines the clustering results of different views by voting or other fusion strategies. The last category learns a unified similarity matrix among multi-view data [32, 19, 30], which serves as an affinity matrix for final clustering. For more details on multi-view clustering, please refer to Section 2.

It should be noted that previous multi-view clustering methods usually assume that all the examples have complete information of all views, i.e., each example in the database has complete feature sets. However, in real applications, it is often the case that some views suffer from missing infor-mation. For example, in image clustering based on visual and textual features, some images have only visual or tex-tual information with only a part of the data sharing both feature sets. We call such dataset incomplete multi-view data. When traditional multi-view clustering methods con-front the above scenarios, a naive approach may remove the data examples that are incomplete. However, this strategy is contradicting with our goal that groups all data examples into their corresponding clusters.

Recently, a few attempts have been made concentrating on multi-view clustering with incomplete views, which may be classified into two major categories. The first strategy pre-processes incomplete views by filling missing information. Piyush et al. [23] and Shao et al. [24] provided to complete the kernel matrices of the incomplete views and then used kernel-based clustering methods for final clustering. Howev-er, these two methods can only deal with the kernel-based multi-view clustering algorithms, which greatly limit their extension to more widely used subspace-based multi-view clustering methods. Recently, Li et al. [34] claimed that the methods in the first category are not a good choice for incom-plete multi-view clustering and accordingly proposed a non-negative matrix factorization based method (PVC), which proved to be effective for document clustering. But, there are also some limitations about the PVC method. Firstly, since data are now often with high dimensional and noisy features, it becomes urgent to select relevant and discrimi-native features when performing clustering. Secondly, PVC utilizes nonnegative matrix factorization to learn latent rep-resentations of data, which cannot well deal with data with negative feature representations.

In this paper, we propose a novel incomplete multi-view clustering method based on joint feature selection and sub-space learning (as shown in Figure 1). Firstly, we utilize a regression-like objective to learn a subspace, in which da-ta examples from different views can be compared irrespec-tive of the heterogeneity between feature sets. To directly explore the complementary characteristics among differen-t views, the latent representations of data examples with complete views are expected to be the same. Besides, since the features for different views may be high dimensional and even noisy, feature selection is performed to select relevant features for latent space learning. At last, a graph regu-larization is utilized to further explore the inter-view and intra-view relationship of the data examples. To these end-s, we develop an objective to achieve all the above goals, and accordingly propose an alternating minimization algo-rithm to find an efficient solution. Extensive experiments demonstrate that our method outperforms the state-of-the-art multi-view clustering methods.

Main contributions: 1) We propose a novel incomplete multi-view clustering method, which incorporates feature selection, subspace learning and inter-view and intra-view similarity preserving into a unified objective. 2) We devel-op an iterative optimization algorithm to efficiently solve the proposed objective, and theoretical analysis is provided to guarantee its convergence. 3) We validate our proposed method with extensive experiments under two settings on four databases, achieving better performance than the state-of-the-art methods.

The rest of the paper is organized as follows. In Section 2, we briefly review multi-view clustering methods. Then our incomplete multi-view clustering algorithm is elaborated in Section 3. Section 4 shows experimental results and analysis. Finally, Section 5 concludes the paper.
Multi-view learning deals with data represented by mul-tiple distinct feature sets and aims at boosting the learning performance [27]. Till now, plenty of methods have been developed with sound theories and multi-view learning has become a hot topic with widespread applicability [20, 26]. For example, the co-training method [2], one of the most fa-mous multi-view learning frameworks, has been widely ap-plied for webpage classification. When multi-view learning meets the unsupervised clustering task, multi-view cluster-ing is accordingly developed to extend traditional single view clustering to the multi-view case.

Generally, multi-view clustering can be roughly classified into four categories. Algorithms in the first category find a unified low-dimensional space, in which the learned embed-ding of data can well explore the complementary information among different views [5, 15, 16, 29, 11]. These methods ob-tain final clustering results through a single view clustering method performed on the learned embedding. Kamalika et al. [5] obtained the low-dimensional subspace of multi-view data through the widely used canonical correlation analysis technique. Kumar et al. [15] proposed two objectives to reg-ularize the Laplacian embeddings between different views to be similar and spectral analysis is employed for parameter learning. Liu et al. [16] developed a multi-view non-negative matrix factorization based method to gain a consensus em-bedding of the original data, which is further developed by He et al. [12] using various co-regularization forms. Re-cently, Wang et al. [29] proposed a regression-like objective, which conducts multi-view clustering and feature selection at the same time. Tang et al. [28] utilized unsupervised fea-ture selection to cluster multi-view social media, and Qian and Zhai [22] also resorted to the above technique to obtain a low dimensional embedding of multi-view web news data.
Methods in the second category integrate multiple sources of information in the clustering process. Typical examples are the co-training and co-EM based multi-view clustering methods [1, 14, 33]. Kumar et al. [14] resorted to co-training, a popular semi-supervised tool, to develop the first co-training based multi-view clustering algorithm. Further-mo re, Zhan et al. [33] proposed a more sophisticated multi-view clustering algorithm by combining LDA, k -means and the co-training framework. The third category is late fu-sion, which integrates the clustering results obtained from each view by voting or other fusion strategies [3, 10, 13]. Long et al. [17] proposed to learn the best clusters by fus-ing the clusters from each view through mapping functions. Greene et al. [10] utilized the matrix factorization based method to obtain optimal clusters. The last category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [32, 19, 30, 4]. Muthukrishnan et al. [19] combined multiple similarity matrices by using a regularization framework to obtain a better similarity graph. Furthermore, Yin et al. [32] resort-ed to subspace clustering to obtain comparable similarity matrices through pairwise co-regularization.

The existing multi-view clustering methods mainly focus on the data with complete views, i.e., each data example has complete feature sets. However, in real applications, some data examples possibly lose some views. To handle this sce-nario, a few works have been developed [23, 24, 34]. Piyush et al. [23] proposed a spectral-based multi-view clustering method, which can deal with the scenario that at least one view is complete. They use the similarity matrix of the complete view to fill the kernel matrices of incomplete views through Laplacian regularization. Furthermore, Shao et al. [24] improved [23] by dealing with situations where no views are complete. They collectively fill all the kernel matrices by optimizing the alignment of shared data examples in the database. To sum up, both methods are based on the k-ernel matrices and can only adapt to kernel-based multi-view clustering. Recently, Li et al. [34] proved that the above methods are not a good choice for incomplete multi-view clustering and proposed a subspace based method using nonnegative matrix factorization (PVC). However, PVC has some limitations restricting its applications. Firstly, multi-view data are often high dimensional and noisy, and it may be necessary to select discriminative features when learning the latent subspace. Secondly, PVC utilizes nonnegative ma-trix factorization to learn latent representations of the data, which limits it applications to data with negative feature sets.
For the sake of introducing our model, we discuss a dataset with two views and it is straight-forward to extend our model to the dataset with more views. Assume the two views of data are represented as X (1) and X (2) respectively. In the traditional multi-view clustering setting, a complete given, where N is the number of data examples. Howev-er, in the incomplete view setting, we are given data  X  X = {  X   X  X c + m +1 , ..., c + m + n } represent data examples having com-plete views, only the first view and only the second view with the number of examples being c , m and n respectively. In total, we have c + m + n examples in the database. having both views with d 1 and d 2 being the dimensionality of the two feature sets. Then X (1) c and  X  X (1) consist of the ex-amples in the first view, as denoted as  X  X (1) = [ X (1) R 1  X  ( c + m ) . Similarly, we have the examples of the second view represented as  X  X (2) = [ X (2) c ,  X  X (2) ]  X  R d task is to group the incomplete multi-view data into their corresponding groups.
Generally, multi-view data consist of heterogeneous fea-ture sets representing the same object, and therefore they share the same class labels. We denote Y = [ Y 1 , ..., Y  X  X  0 , 1 } ( c + m + n )  X  k the class index of the incomplete multi-view database, where Y i  X  X  0 , 1 } k  X  1 is the class indica-tor vector for the i -th example and k is the number of clusters. Then the scaled indicator matrix F is defined as ty F T F = I k , where I k is an identity matrix with a size of k .

In our objective, we aim to find a F satisfying the above properties for multi-view clustering and the advantages are listed as follows. Firstly, F reflects the class indicator of the multi-view data, which is a higher level semantic representa-tion of data. Even though data consist of multiple heteroge-neous features, they should share the same semantic infor-mation. By introducing this semantic space, we construct a bridge for different heterogeneous feature sets. Furthermore, using such an indictor matrix, we can learn the projection matrix for each view and perform feature selection in a su-pervised manner, which will be described later.

To learn the indictor matrix, we learn a projection ma-trix for each view to project their original space to such a semantic space. The objective is then formulated as: trices for the two views. F c  X  R c  X  k ,  X  F (1)  X  R m  X   X  F (2)  X  R n  X  k are the learned latent representations for da-ta examples with complete views, only the first view and only the second view, respectively. It can be seen that we explore the relationship between the two views by enforc-ing the data examples with complete feature sets to have the same latent representation. || U (1) || 21 = where U (1) ( i, :) is the i -th row of U (1) . The  X  21-norms im-posed on the projection matrices result in relevant features being selected for each view as always done by supervised feature selection [21].  X  is a regularization parameter con-trolling the degree of the sparsity of projection matrices. When  X  is big, only a small subset of features will be se-lected, otherwise a large subset of features will be selected. resentations for all the data examples, and F T F = I k and F  X  0 are used to constrain the latent representation to be consistent with the indicator matrix of the database.
In Equation 1, we project different feature sets into the same latent space and the relationship between differen-t views is explored on such space in a direct manner. In the following part, we add extra regularization constraints on the projection matrices to further dig the relationship between data examples in each view and between the two views to model the structure of the multi-view data. More sp ecifically, we hope to preserve the intra-view similarity and the inter-view similarity relationships in the dataset. Their details are listed as follows. 1) Intra-view similarity relationship: to preserve the local structure of data examples in each view, we constrain the neighborhood relationship between data points under each view also hold in the learned latent space. Generally, the neighborhood structure can be obtained by using a Gaussian based kernel matrix. we denote the matrices as W (1) and W (2) for the two views respectively and the entities in the matrix indicate the similarity between two data examples under a specific view. The detailed formulation is: W where z ( t ) ij is the Euclidean distance between data examples X i and X nearest neighbors of X ( t ) i . 2) Inter-view similarity relationship: although differen-t views of data have different feature sets, they share the same semantics if they represent the same content or top-ic. To preserve such inter-view similarity when learning the projection matrices, we construct similarity matrices W (12) and W (21) for view 1 to view 2 and view 2 to view 1 respec-tively. Under the incomplete view setting W (12) = ( W (21) and they are defined as:
Using the inter-view and intra-view similarities, we define the overall similarity matrix W as: Based on this similarity, we define the regularization on the projection matrices as : and it is further rewritten as: where L = D  X  W is the Laplacian matrix and D is a di-agonal matrix with its i -th diagonal element defined as the sum of the i -th row of W . T r is the trace of a matrx.
Adding this regularization constraint to Equation 1, we obtain the final objective as: In our objective, we have three terms: using the projec-tion matrix to project each incomplete view to the latent space defined by F ; feature selection for each view using the  X  21-norm based constraint and the inter-view and intra-view similarity preserving term defined by the Laplacian matrix. Besides, the constraints imposed on F guarantee that each example only belongs to one group.
In this section, we propose to optimize the objective as described in Equation 7. Since the variables, such as the projection matrix and the latent representation, are coupled together, it may be difficult to optimize them at the same time. Hence, we propose to alternatively optimize the vari-ables to obtain a local solution. 1) Optimize F with fixed U : the constraints on F in Equation 7 make the optimization not an easy problem, especially different views only have part of all the latent representations, i.e., [ F c ;  X  F (1) ] and [ F c ;  X  F (2) F . To handle this, we optimize F c ,  X  F (1) and  X  F (2) and relax the constraints to the following form: Even though the orthogonal constraint on F c may not be rigorous when data examples with complete feature sets do not have all kinds of class labels. We ignore this slight influ-ence. In turn, it makes our optimization very compact. As for  X  F ( k ) , ( k = 1 , 2), since examples in the same view share the same projection matrix and these examples follow the same data distribution,  X  F ( k ) will have similar characteristic with F c . In summary, the relaxed constraints will have al-most the same effect with that of the original ones and can make the optimization more succinct.

We denote the objective in Equation 7 as O . Then mini-mizing O over F c ,  X  F (1) and  X  F (2) are simplified as: To optimize F c , we bring in Lagrangian function as: where  X  and  X  are Lagrangian multipliers of the above func-tion and A i = ( X ( i ) c ) T U ( i ) . Applying KKT condition, i.e.,  X ( s, t ) F c ( s, t ) = 0, we obtain: and we can obtain the following updating rule for F c :
F c ( s, t ) = F c ( s, t ) C  X  ( s, t ) = ( | C ( s, t ) | X  C ( s, t )) / 2 and C = C +  X  fo r  X , its diagonal elements are obtained by summing s :  X  ( s, s ) = elements of  X  are approximated by ignoring the non-negative values of F c :  X ( s, t ) = mary,  X  is calculated by  X  =
To optimize  X  F (1) and  X  F (2) , we directly obtain their gradi-ents and the updating rule is: 2) Optimize U with fixed F : Minimizing the objective O in Equation 7 with respect to U (1) and U (2) are rewritten as: min trix and the latent representation for one view as described before. They consist of the data examples with both feature sets and only with the s -th feature set.

Differentiating the objective function in Equation 15 with respect to U ( s ) and setting it to zero, we have the following equation: where D ( s ) is a diagonal matrix with its i -th diagonal ele-ment calculated as D ( s ) ( i, i ) = 1 / (2 || U ( s ) ( i, :) by where  X  is a smoothing term, which is usually set to be a small positive value.

Then Equation 16 is further written as: The objective can be optimized using the following equation: Algorithm 1 gives the overall optimization for equation 7. In Step 3, we calculate the latent representation for the in-complete multi-view dataset. In Steps 4 and 5, we optimize the projection matrices U ( s ) , ( s = 1 , 2). Finally Steps 3, 4 and 5 are repeated until convergence. Based on the latent representation, the final clustering results can be obtained by using regular clustering algorithms, e.g., k -means. The overall clustering algorithm is summarized in Algorithm 2. vergence of the algorithm. Similar to [9], we add a smoothing term as in Equation 17.
 Al gorithm 1 Solving Equation 7 to obtain the latent rep-resentation of the incomplete multi-view dataset Inpu t: 1: t = 1. Initialize U ( s ) , ( s = 1 , 2) and F randomly; 2: while not converge do 3: Calculate F c ,  X  F (1) and  X  F (2) using Equation 13 and 14 4: Solve D ( s ) , ( s = 1 , 2) using Equation 17; 5: Calculate U ( s ) , ( s = 1 , 2) using Equation 19 respec-6: end while Output: Al gorithm 2 Clustering procedure for the incomplete multi-view dataset Inpu t: 1: Obtain the latent representation F of all the data by 2: Perform k -means clustering on F to obtain the clustering Output:
In this section, we prove that Algorithm 1 converges to a local minima.

Theorem 1. The proposed iterative optimization strate-gy in Algorithm 1 will monotonically decrease the objective function in Equation 7 in each iteration until convergence. a) In Step 3 of Algorithm 1, we will resort to auxiliary function approach [7] to validate that the updating rule for F c will monotonically decrease the objective value. As for the updating rule for  X  F (1) and  X  F (2) , it is easy to verify that their objectives are convex and their optimization methods can decrease the objective function monotonically. Let and it is further rewritten as:
H ( F c ) = T r ( Then the following function ia an auxiliary function of H ( F c ). Besides, it is easy to verify that the Hessian matrix of h ( F c ,  X  F c ) is a positive definite matrix, thus, h ( F c ,  X  F c ) is convex and its global minimum is obtained as in Equation 13.

Through the definition of the auxiliary function and the above derivation, we can obtain the following inequality: Thus, the updating rule for F c will monotonically decrease the objective value. b) In Step 5 of Algorithm 1, we will prove that the up-dating rule in Equation 19 for U ( s ) , ( s = 1 , 2) will decrease the objective monotonically.
 Taking U (1) as an example, we can derive that: and Equation 19 is the analytic solution of the above func-tion. Then we have: L where Substituting D t +1 (1) into the above inequality, we have: Here we introduce a function f ( x ) = x  X  x 2 / (2 a ), which satisfies { X  x  X  R, f ( x )  X  f ( a ) | a &gt; 0 } . Then we make x and following inequality: Add both sides of the above inequality to Equation 27, we obtain the following inequality: Thus the updating rule for U will decrease the objective function monotonically.

Combining the above derivation, we prove that Algorithm 1 converges to a local minimum.

Complexity analysis: We briefly discuss the computa-tional complexity of our algorithm. As for the optimization of F , the main computation lies in the updating for F c as in Equation 13, which mainly consists of some matrix multipli-cation operations. When optimizing U , we need to compute the overall multi-view similarity matrix, whose complexity is about O ( d m N 2 m ), where d m N 2 m being the product of the dimensionality and the square of the number of examples for the m -th view is the largest one among all views. However, it is a constant matrix and can be computed before the opti-mization of the variables. Besides, we need to use Equation 19 to calculate U , which solves an inverse problem. Instead, we can update the projection matrices by solving a linear system for O (  X  d 2 )(  X  d = max( d 1 , d 2 )).
We report experiments on four widely used multi-view datasets and their descriptions are summarized in Table 1. Da taset # size # view # cluster # feature size US PS 2,0 00 2 1 0 7 6+216 W ebKB 1,0 51 2 2 1, 840+3,000 T able 1: Information of the multi-view datasets. # feature size means the dimensionality of the two fea-ture sets of the database.

UCI Handwritten Digit Dataset 2 It consists of fea-ture sets of handwritten numerals (0-9) extracted from Dutch utility maps. The database has 2,000 examples even-distributed in ten categories and is represented in terms of six visual fea-tures. Being same in [15], we use the 76 Fourier coefficients of the character shapes and the 216 profile correlations as two views.

Cora Dataset 3 It contains 2,708 scientific publications divided into 7 classes (Neural N etworks, Rule Learn ing, Re-inforcement Lea rning, Probabilistic Me thods, Theory, Ge-netic Al gorithms, Case B ased). Two heterogeneous feature sets, i.e., citations and content are utilized here for exper-iments, where the content feature is represented by 0/1-valued word vector indicating the absence/presence of the corresponding word from the 1,433 words constructed dic-tionary.

BBC Dataset 4 It is a synthetic multi-view text database, which is constructed using single view BBC and BBCSport corpora. In total, it consists of 2,012 data examples cat-egorized into 5 classes. The two views used here are the segments representations of the same document with the dimensions being 6,838 and 6,790 respectively. We use prin-cipal component analysis (PCA) to preprocess the data and the dimension is selected based on the eigenvalues of the covariance matrix obtained from the data.

WebKB Datasets 5 It is a webpage dataset from the computer science departments of four universities. The dataset consists of two categories, i.e., course and non-course with t-wo heterogeneous feature sets, namely the textual content of the webpage and the link representation. Here the link rep-resentation is the anchortext on links in the other webpages linking to the current webpage.
To simulate the incomplete multi-view datasets, we ran-domly select part of examples to have only one single feature set. Similar to [34], two different settings are considered and listed as follows.

As described in Section 3, we denote m and n the numbers of examples appearing only in the first view and only in the second view respectively. 1) the first setting : m &gt; 0 , n &gt; 0, namely both views do not contain all the examples in the database. 2) the second setting : either m = 0 or n = 0, namely at least one view is complete.

For the above two settings, we randomly select 10% to 90% of the total examples, with 20% as interval, to have only one feature set. And this process is repeated 10 times with the average to be reported. Besides, as for the first setting, we evenly distribute the number of examples for the two views for simplicity.
We compare our algorithm with several representative multi-view clustering methods, which consist of three subspace learning based methods and three kernel matrix based meth-ods and their modifications.

SingleV1, SingleV2: We run spectral clustering [25] on the two views under the condition that all views have complete data examples.

CCA: We use canonical correlation analysis to obtain the latent representation of multi-view data and then apply k -means on the obtained representation.

PairwiseSC, CentroidSC: The multi-view spectral clus-tering methods based on two regularization frameworks de-veloped by Kumar et al. [15].

MultiCF: Wang et al. [29] proposed a structure sparsity based unsupervised feature selection method for the task of multi-view clustering.

RMSC: Xia et al. [30] developed a multi-view spectral clustering method, which is based on low rank and sparse decomposition of the transition matrix.

PVC: Li et al. [34] proposed probably the only incom-plete multi-view clustering method without filling the miss-ing information.

PairwiseSC++, CentroidSC++, RMSC++: For the kernel based multi-view clustering algorithms, Piyush et al. [23] proposed to fill the kernel matrix of the view with in-complete examples using the kernel matrix of the view with complete examples. So in our second setting that one view is complete, we can use this method to fill the incomplete kernel matrix. Then the modified PairwiseSC, CentroidSC and RMSC methods may obtain better clustering results. Moreover, Shao et al. [24] proposed to fill the kernel matri-ces even there are no views with complete examples. Then in our first setting, we may promote PairwiseSC, Centroid-SC and RMSC methods using this method. We denote the PairwiseSC, CentroidSC and RMSC methods with the pre-processing of the kernel matrix under the two settings as PairwiseSC++, CentroidSC++, RMSC++ respectively.

For the compared methods that are not designed for in-complete multi-view clustering, i.e., CCA, PairwiseSC, Cen-troidSC, MultiCF and RMSC, we just use zeros to replace incomplete feature sets. This may be a little arbitrary, but we find possibly no methods can well fill various types of features at the same time, e.g., visual features and textual features. Besides, it may be fair enough since our method do not preprocess the data at all. For PairwiseSC, Cen-troidSC, RMSC and PVC methods, we use the codes the authors have released to achieve their best performance and the method CCA is achieved using the LSCCA package 6 . As for MultiCF, we implement the method and follow the au-thors X  suggestions to achieve the clustering results. For our method, we use KNN based Gaussian kernel to construct the intra-view similarity matrix and the number of the KN-N neighbors and the width parameter for Gaussian kernel are empirically selected as ten percent of the total examples of the database and one respectively in all the experiments. As for the trade off parameters  X  and  X  , they are empirically selected to achieve the best clustering results. We will test their effects in the parameter study part. Since k -means is used in all the experiments, it is run 20 times with random initialization and the mean value is reported.

Finally, by following [34], the normalized mutual informa-tion (NMI), as one of the most famous clustering evaluation measures, is utilized. Users can refer to [6] for more details on its definition.
Figure 2 shows the clustering results of all the methods under the first setting, i.e., both views suffer from infor-mation loss. IER (incomplete example ratio) indicates the percentage of examples having only one feature set. Besides, the results of all methods with IER being zero are also re-ported as the upper bound of each method. Overall, it can be seen that our method performs better than all the com-dex.html peting methods under different incomplete example ratios on the four databases.

Compared with the results of SingleV1 and SingleV2 meth-ods, our method performs better even with up to 50% of ex-amples only appearing in one feature set on the USPS, Cora and WebKB datasets. This is an inspiring result, which indicates that our method can well explore the multi-view complementary information even in relatively large incom-plete example ratios.

As for PairwiseSC, CentroidSC and RMSC, we utilize the method proposed in [24] to fill the kernel matrices of the incomplete views and accordingly PairwiseSC++, Centroid-SC++, RMSC++ are developed. From Figure 2, they per-form better in some databases and the performance gain seems not very considerable especially when IER being large. In summary, our method performs better although these k-ernel based multi-view clustering methods preprocess to fill the lost information.

As for PVC, it uses non-negative matrix factorization to find a unified low dimensional space and constrains the ex-amples with complete views sharing the same representa-tions to deal with the incomplete multi-view data. Com-pared with it, we also apply feature selection to select rele-vant features when learning the low dimensional subspace, which works confronting the high dimensional and noisy fea-tures. Besides, the multi-view data structure is also explored in the proposed method. Thus our method performs better than PVC.
 One of the major differences between our method and the MultiCF method under complete views is the constraint im-posed on the learned latent representation. We add the non-negative constraint, which is more reasonable to approach the normalized indictor matrix and this may be the rea-son that our method performs better when the incomplete example ratio is zero. Since MultiCF is not designed for in-complete multi-view data, our method also outperforms it when IER is greater than zero.
Figures 3 and 4 display the clustering performance under the second setting with the first and second view suffering from incomplete examples respectively. It should be noted that we apply the method in [23] to fill the kernel matrix of the incomplete view using that of the complete view for PairwiseSC, CentroidSC and RMSC to obtain the Pairwis-eSC++, CentroidSC++, RMSC++ methods.

It can be seen that similar results are obtained as in Fig-ure 2 except that all the methods obtain relatively better performance compared with that in the first setting under the same incomplete example ratio. This may be because there exists one complete view to aid the multi-view cluster-ing and it may be more useful compared with the scenario of no complete views. Overall, our method still obtains the best clustering performance almost on all the datasets under this setting.
In our proposed model as in Equation 7, there are two pa-rameters  X  and  X  balancing the effect of feature projection term,  X  21-norm based feature selection term and graph reg-ularization based structure preserving term. In this section, we investigate how the performance varies with the changes o f the above two parameters. Due to space limitation, we conduct experiments on the four databases under the first setting and the incomplete example ratios are selected as 0 and 0.3 respectively. It should be noted that similar results can be obtained under the second setting. The results are shown in Figure 5.
 Fi gure 5: The NMI results on the four databases under the first setting with the incomplete example ratio being 0 and 0.3 respectively.  X  controls the sparsity of the projection matrices. When it is small, the constraint will lose the effect of feature selec-tion. In the case when  X  is too big, the sparse characteristic will lead to the loss of useful features and harm the learned latent representations.  X  is the weight for the graph regular-ization term, which keeps the inter-view and intra-view data structure of the original spaces in the learned space. When it is too big, it may rely on too much of the neighborhood re-lationship obtained using the similarity metric and this may harm the intrinsical data structure because of the possible inaccuracy of the calculated similarity matrix. In summary,  X  and  X  should be carefully selected and [0.001,0.01] is an optimal interval when the multi-view data is normalized.
As discussed in Section 3.4, the optimization strategy con-verges to a local minima. In this section, we give the conver-gence and the corresponding NMI curves with the varying updating iterations. Due to space limitation, we only give the results under the first setting with incomplete example ratio being 30% and similar results can be achieved under the second setting. From Figure 6, it can be seen that the objective function converges fast, and the clustering perfor-mance needs about 100 iterations to reach the best results. This may because the initial values of the variables in Al-gorithm 1 are randomly set. In the future, we may consider a nice initialization method to reduce the number of itera-tions. Fi gure 6: Convergence and the corresponding NMI curves for the four databases under the first setting with incomplete example ratio being 0.3.
In this paper, we have proposed a novel incomplete multi-view clustering algorithm to cluster incomplete multi-view data. In our model, we learn a latent representation of the data examples, which serves as an approximation of the nor-malized indictor matrix. Besides, the complementary in-formation between different views is explored by enforcing examples with complete views sharing the same represen-tations. Through the  X  21-norm based constraint, relevant features are selected for the projection to the latent space. Furthermore, we add a graph regularization term to pre-serve the inter-view and intra-view data structure, which further promotes the clustering performance. Extensive ex-periments have validated the effectiveness of the proposed method compared with the state-of-the-art methods. Since it is practical to obtain partial label or must-link and cannot-link information between data examples, we may consider adding such information to promote clustering in the future. This work is jointly supported by National Basic Research Program of China (2012CB316300), and National Natural Science Foundation of China (61175003, 61420106015, U1435221, 61403390). [1] S. Bickel and T. Scheffer. Multi-view clustering. [2] A. Blum and T. Mitchell. Combining labeled and [3] E. Bruno and S. Marchand-Maillet. Multiview [4] X. Cao, C. Zhang, H. Fu, S. Liu, and H. Zhang. [5] K. Chaudhuri, S. M. Kakade, K. Livescu, and [6] W.-Y. Chen, Y. Song, H. Bai, C.-J. Lin, and E. Y. [7] C. Ding, T. Li, and M. I. Jordan. Convex and [8] X. Dong, P. Frossard, P. Vandergheynst, and [9] I. F. Gorodnitsky and B. D. Rao. Sparse signal [10] D. Greene and P. Cunningham. A matrix factorization [11] Y. Guo. Convex subspace representation learning from [12] X. He, M.-Y. Kan, P. Xie, and X. Chen.
 [13] S. F. Hussain, M. Mushtaq, and Z. Halim. Multi-view [14] A. Kumar and H. Daum  X e. A co-training approach for [15] A. Kumar, P. Rai, and H. Daume. Co-regularized [16] J. Liu, C. Wang, J. Gao, and J. Han. Multi-view [17] B. Long, S. Y. Philip, and Z. M. Zhang. A general [18] E. Muller, S. Gunnemann, I. Farber, and T. Seidl. [19] P. Muthukrishnan, D. Radev, and Q. Mei. Edge [20] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and [21] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient [22] M. Qian and C. Zhai. Unsupervised feature selection [23] P. Rai, A. Trivedi, H. Daum  X e III, and S. L. DuVall. [24] W. Shao, X. Shi, and P. S. Yu. Clustering on multiple [25] J. Shi and J. Malik. Normalized cuts and image [26] N. Srivastava and R. R. Salakhutdinov. Multimodal [27] S. Sun. A survey of multi-view machine learning. [28] J. Tang, X. Hu, H. Gao, and H. Liu. Unsupervised [29] H. Wang, F. Nie, and H. Huang. Multi-view clustering [30] R. Xia, Y. Pan, L. Du, and J. Yin. Robust multi-view [31] C. Xu, D. Tao, and C. Xu. A survey on multi-view [32] Q. Yin, S. Wu, R. He, and L. Wang. Multi-view [33] X. Zhao, N. Evans, and J.-L. Dugelay. A subspace [34] S.-Y. L. Y. J. Zhi and H. Zhou. Partial multi-view
