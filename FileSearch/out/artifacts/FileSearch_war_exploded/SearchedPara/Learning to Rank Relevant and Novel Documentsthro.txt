 We consider the problem of learning to rank relevant and novel documents so as to directly maximize a performance metric called Expected Global Utility (EGU), which has several desirable properties: (i) It measures retrieval per-formance in terms of relevant as well as novel information, (ii) gives more importance to top ranks to reflect common browsing behavior of users, as opposed to existing objective functions based on set-coverage, (iii) accommodates different levels of tolerance towards redundancy, which is not taken into account by existing evaluation measures, and (iv) ex-tends naturally to the evaluation of session-based retrieval comprising multiple ranked lists. Our ground truth is de-fined in terms of  X  X nformation nuggets X , which are obviously not known to the retrieval system when processing a new user query. Therefore, our approach uses observable query and document features (words and named entities) as surro-gates for nuggets, whose weights are learned based on user feedback in an iterative search session. The ranked list is produced to maximize the weighted coverage of these sur-rogate nuggets. The optimization of such coverage-based metrics is known to be NP-hard. Therefore, we use a greedy algorithm and show that it guarantees good performance due to the submodularity of the objective function. Our ex-periments on Topic Detection and Tracking data show that the proposed approach represents an efficient and effective retrieval strategy for maximizing EGU, as compared to a purely-relevance based ranking approach that uses Indri, as well as a MMR-based approach for non-redundant ranking. H.3.3 [ Information Search and Retrieval ]: Relevance Feedback Algorithms, Experimentation, Measurement, Performance novelty-based ranked retrieval, nuggets, user feedback
There has been growing interest in building and optimiz-ing retrieval systems with respect to multiple criteria like relevance, novelty, and diversity of information [30, 6, 22, 29]. However, each of the current approaches is based on its own objective function that does not fully capture all the factors that are essential for realistic evaluation and opti-mization of systems with respect to relevance and novelty.
One of the most common modes of interaction with re-trieval systems ( e.g. , search engines) is ranked retrieval, where the system produces a list of documents ordered by decreas-ing relevance. However, so far novelty detection has not received much attention in a ranked retrieval setting. For instance, the TREC novelty track [24], which is represen-tative of the research on novelty detection, assumed a fixed chronological order of documents, and the system X  X  task was to merely detect the relevant and novelty sentences, without re-ordering them. While such a setting helped in isolating the novelty detection task and greatly simplified the cre-ation of ground truth ( i.e. , pre-judged relevant and novel sentences), it obviously does not reflect how users interact with today X  X  retrieval systems. Similarly, diversity-based re-trieval has been treated as a set retrieval problem: The ob-jective function is reduced to the set-covering problem [30, 1], which does not differentiate between orderings of docu-ments. This leads to an undesirable gap between set-based objective functions and the ranked-based evaluation metrics that are of ultimate interest.

Moreover, users often have different tolerances towards re-dundancy, as was also noted by [1, 4, 8]. While some users only want to see previously unseen documents, other users might desire a certain level of redundancy in the ranked list for various reasons like corroboration of information, or as-sessing the consensus or opinions on a single topic or product based on different news sources, reviewers, or blogs. How-ever, none of the existing approaches to novelty or diversity-based ranking take this into account.

Accounting for redundancy in a principled manner is espe-cially important when dealing with multiple ranked lists, for example, in an interactive session with a Web search engine, where the user goes through multiple rounds of query refor-mulation and feedback. It is not clear how to model novelty across multiple ranked lists. Simple strategies exist, e.g. , pe-nalizing or removing documents that were already presented, or are similar to already-presented documents. Such an ap-proach is reasonable in an adaptive filtering setting [32], but not for ranked retrieval: Users generally do not read all doc-uments presented to them in a ranked list (especially in long ranked lists produced by search engines). They are more likely to read the top-ranked documents, and stop at some position based on their patience or satisfaction. A possible solution would be to assume a fixed position where users stop in each ranked list, e.g. , assuming that all users read the top ten documents in each ranked list [14]. However, any single stopping position would be a crude approximation of the dynamic behavior of real users and would completely ig-nore the documents below the cut-off rank for the purpose of evaluation. Instead, a probabilistic user model is desirable to model the browsing behavior of a population of users: A document that was presented at a very low rank in a previ-ous ranked list has a smaller likelihood of being read by the user, and hence, should be discounted appropriately for the purpose of estimating the novelty of subsequent documents, thus leading to a probabilistic notion of novelty that extends naturally to multiple ranked lists in a search session.
Finally, the non-independent nature of novelty raises new challenges for learning from user feedback, which might be available in explicit ( e.g. ,  X  X ike X / X  X islike X  buttons) or implicit ( e.g. , clicks) form. Since the utility of each document de-pends on other documents shown to the user, the feedback provided by the user should also be interpreted with respect to previously seen documents. In other words, user feed-back is an indicator of the marginal utility of documents, instead of its absolute usefulness with respect to the user X  X  information need. However, the latter assumption has been commonly used for learning from relevance feedback, e.g. , through regression [16, 28] or language modeling [31].
To summarize, accurate evaluation and optimization of retrieval systems must be based on a performance measure with the following properties: (i) It should take both rele-vance and novelty into account, (ii) give more importance to top ranks to reflect common browsing behavior of users, (iii) accommodate different levels of tolerance towards re-dundancy, and (iv) extend naturally to the evaluation of session-based retrieval comprising multiple ranked lists. In this paper, we develop a retrieval strategy for optimizing a recently proposed performance measure called Expected Global Utility ( EGU ) [27] that satisfies the above-mentioned criteria. We also propose a logistic regression based ap-proach for learning from user feedback that takes the non-independent nature of document utility into account.
Our approach is based on maximizing a recently-proposed metric called Expected Global Utility ( EGU ) [27] that com-bines all the above-mentioned criteria in a principled man-ner. Relevance and novelty are modeled in terms of  X  X nfor-mation nuggets X . The gain received from relevant nuggets follows a diminishing returns property to account for the re-duced utility of seeing repeated information. However, how to directly optimize a retrieval system with respect to such a metric remains an open challenge, which we aim to address in this paper.  X  X nformation nuggets X , or simply  X  X uggets X , is a concept borrowed from question answering evaluation [10]. A nugget is an atomic piece of information that is either present or ab-sent from a given document. Thus, the answer keys for each user query can be defined in terms of the nuggets that sat-isfy the query. For example, the query  X  X P oil spill X  would have the following nuggets:  X  X ire started on April 20, 2010 X ,  X  X ine crew members and two engineers died X ,  X  X op hat at-tempt X ,  X  X op kill attempt X , and so on 1 . A document X  X  utility ( i.e. , relevance and novelty) depends on the nuggets it con-tains, and whether these nuggets have been seen by the user in previously displayed documents. The goal of a retrieval system is to generate ranked lists that would maximize the utility received by the user.

There are several advantages to using nuggets as retrieval units. Nuggets can be used as answer keys to create reusable test collections for relevance and novelty-based retrieval eval-uations. Previous novelty-based evaluations ( e.g. , TREC [24] and TDT [2]) used documents or sentences as retrieval units and depended on a fixed (chronological) order of re-trieval, which is clearly unrealistic for ranked retrieval. More-over, the use of nuggets allows a more natural definition of novelty where a given document can be deemed as redundant based on two or more previously seen documents, instead of a single near-duplicate document seen by the user in the past. Since entire documents are rarely redundant with re-spect to each other, nuggets provide a finer granularity for relevance and novelty-based evaluation.

Next, we describe the Expected Global Utility ( EGU ) in detail, followed by our proposed approach for optimizing retrieval systems with respect to this performance measure.
Utility. EGU is based on the notion of  X  X tility X  of each document returned by the system, defined as the difference between the gain and cost that would be accrued by the user when he or she reads that document.

The cost of reading the document represents the time and effort expended by the user in going through the system X  X  output. The simplest definition would be unit cost per doc-ument, which explicitly penalizes longer ranked lists 2 . More sophisticated definitions of cost can be based on length of the document, its language, and so on.

The gain from reading the document is defined in terms of its nuggets, whose contribution is discounted based on how many times they have been seen by the user previously: where  X  is a nugget, w  X  is the weight of  X  ,  X  (  X ,i  X  1) is the number of times  X  appears in the ranked list up to rank i  X  1.  X  is a pre-specified parameter that reflects the user X  X  tolerance for redundancy. Thus, the gain received from each
Note that nuggets are defined at a conceptual level, e.g. , a nugget like  X  X arack Obama X  acts as a placeholder for all possible ways of referring to the same person, e.g. ,  X  X bama X ,  X  X resident Obama X , etc. Therefore, nuggets must be some-how matched to their various possible surface-level mani-festations. To solve this problem, we use the dataset and nugget-matching rules that are made publicly available by [28]. (See Section 4.1 for description of the dataset).
Such a penalization can be used to evaluate retrieval sys-tems that are expected to limit the amount of information shown to the user, but is irrelevant for evaluating search en-gines, which produce very long ranked lists that are never fully browsed by the user. successive presentation of the same nugget is discounted by a factor  X  . If  X  = 1, no discounting takes place: The user is as-sumed to be fully tolerant to redundancy, and the evaluation reduces to be relevance-based only. At the other extreme of  X  = 0, reading a nugget after the first time is assumed to be completely useless to the user (interpreting 0 n = 1 if n = 0, and 0 for n &gt; 0).
 The gain of an entire ranked list can thus be defined as: where  X   X  ( L ) is the number of times nugget  X  appears in the ranked list L , and  X  q is the set of all nuggets that are relevant to the query q .

Expected Utility . The current definition of gain of a ranked list does not favor early retrieval of useful informa-tion. We must take the typical browsing behavior of users into account: Users are more likely to browse ranked lists in a top-down manner and stop at some position due to various reasons like satisfaction or frustration. To capture this be-havior, the utility of a ranked list is interpreted as a function of the user X  X  stopping position s , which is assumed to be a random variable. By defining a probability distribution over s , say, Pr( s | p ) with parameter p , we can obtain the expected utility of a ranked list: We assume Pr( s ) to be a geometric distribution with pa-rameter p to model the common observation that users are more likely to stop at early positions in a ranked list.
Multiple Ranked Lists The above definition extends naturally to session-based retrieval with multiple ranked lists by extending the definition of stopping distribution as well as utility to multiple ranked lists 3 . Specifically, let Pr( s ) = ing stopping positions in the K ranked lists. Similarly, util-ity can be defined as a function of stopping positions in all ranked lists by considering only the nuggets that appear in the respective top s 1 ..s K documents in each ranked list. Since the expectation is calculated by summing over all stop-ping positions in each ranked list, we can obtain a more ac-curate estimate of the utility of multiple interrelated ranked lists without having to assume a hard reading cut-off in each ranked list 4 .

Next, we focus on the main problem of system optimiza-tion, i.e. , designing a retrieval strategy to maximize EGU .
As mentioned earlier, the ground truth is defined in terms of nuggets. However, a retrieval system would obviously not have access to the true nuggets for an unseen query.
Since the multiple ranked lists are assumed to be part of a single coherent session, their utility cannot be calculated independently of each other due to the non-independent na-ture of novelty.
Summing over all stopping positions in multiple ranked lists can be computationally prohibitive; See Section A.1 for so-lutions.
 Therefore, it must depend on observable features of docu-ments ( e.g. , words) to rank them with respect to a query. Several approaches have been used for estimating the rel-evance of documents in terms of the query and document words, e.g. , hand-crafted scoring functions like BM25 [23], or probabilistic approaches like language modeling [17]. Sim-ilarly, novelty of documents has been measured in terms of word-level similarities between documents, e.g. , cosine sim-ilarities [4, 27]. However, such approaches would only pro-vide an indirect way of optimizing EGU without taking into account its various nuances like user X  X  tolerance for redun-dancy (  X  , see Eq. 3) and browsing persistence ( p , see Eq. 4). Moreover, such approaches measure relevance and novelty independently and then combine them to score each docu-ment ( e.g. , using the MMR strategy [4]). That is, relevance and novelty are treated as compensatory: High novelty can compensate for low relevance. Hence, it is possible for such methods to favor a document that is highly novel but ir-relevant to the given query. However, users generally treat relevance as a pre-condition for usefulness of information [19, 12]. Therefore, it is more appropriate to directly target the retrieval of relevant and novel information.
 Nuggets directly capture relevant and novel information. Therefore, we argue that the system X  X  model of relevance and novelty should also be based on nuggets. Again, since the true nuggets are unknown to the system, it must use observable features ( e.g. , words or named entities) as sur-rogates for the true nuggets. However, the main challenge is that not all features are equally important: e.g. , certain frequently occurring words known as  X  X topwords X  carry neg-ligible information. Moreover, the importance of a feature depends on the user X  X  query: e.g. , for a query like  X  X P oil spill X , the system should focus on the coverage of words and named entities that denote the occurrence, consequences, and containment efforts related to the oil spill. Further-more, for broad or ambiguous queries, the importance of the features might depend on the intention or focus of the particular user, which can be determined based on explicit or implicit feedback.

Next, we will illustrate our retrieval approach and describe how it addresses the above-mentioned problems.

Proposed Retrieval Approach. Given a query, rank-ing documents is a multi-step process: 1. Obtain a candidate set of documents using a standard 2. Identify and assign weights to all features that appear 3. Re-rank the documents to maximize the coverage of 4. Update weights of features based on user feedback. Re-
Let us look at each of these steps in detail.
This step is accomplished using an off-the-shelf retrieval system, and serves to limit the number of documents that need to considered for creating the final ranking. In our experiments, we use a state-of-the-art retrieval engine, Indri [26], to retrieve the initial set of documents.
The candidate set of documents is used to extract features that will act as surrogates for the true nuggets that the user is interested in. We use the following features as surrogates for nuggets:
Each nugget ( i.e. , word or named entity) is assigned a weight based on two factors: 1. Its TF X  X DF (Term Frequency X  X nverse Document Fre-2. The scores of the documents (as assigned by the ini-
That is, the weight w  X  assigned to the nugget  X  is: where  X  s (  X  ) is the average score of the documents in which  X  appears.

Note that this is only an initial assignment of weights and is updated based on user feedback, as described in Sec-tion 2.2.4.
Once weights are assigned to features, we must rank the documents so as to maximize the weighted coverage of the features at the top ranks, so that a typical user who browses the ranked list in a top-down manner is likely to come across the most number of (surrogate) nuggets. It is already known that finding the optimal set of documents that will maxi-mize the coverage of any discrete elements (aspects, nuggets, categories, etc.) is an NP-hard problem [30, 1, 5]. The NP-hardness also extends to the ranking problem. In Sec-tion A.2, we show that maximizing EGU can be reduced to Algorithm 1 Greedy algorithm ( Greedy ) 1: /* Input: Documents to rank D = { d 1 ,d 2 ,...,d k } */ 2: /* Output: Ranked list L */ 3: L  X  X  X  4: for i = 1 to k do 5: j  X  Choose-Next-Doc ( L,D ) 6: L  X  L  X  d j 7: D  X  D \ d j 8: end for 9: 10: sub Choose-Next-Doc ( L,D ) do 11: /* Input: Current ranked list L , remaining docu-12: /* Output: Index of next document from D to 13: for i = 1 to | D | do 14: s 0 ( i ) = Marginal-Utility ( d i ,L ) 15: end for 16: return argmax i s 0 ( i ) 17: end sub the Maximum Coverage Problem, and that a simple greedy algorithm guarantees good performance due to the submod-ularity of EGU . Here, we only focus on the greedy algorithm. Algorithm 1 shows the steps involved in the greedy algo-rithm for ranking. At each step, the algorithm picks the document with the highest marginal utility , which in turn depends on the expected number of times each nugget in the document has already appeared in documents already included in the ranked list L :
The definition of marginal utility can be extended to mul-tiple ranked lists by replacing  X   X  ( L ) by the expected number of times each nugget appeared in all previously displayed ranked lists (Eq. 14).
The initial weights obtained in Section 2.2.2 are further tuned to adapt to the user X  X  information needs by leverag-ing feedback obtained from the user. In a deployed sys-tem, such feedback could be available in an explicit ( e.g. ,  X  X ike X / X  X islike X  buttons in the interface) or implicit ( e.g. , clicks on documents) form. For the purpose of this paper, we will simulate the presence of positive and negative feed-back on documents returned by the system. We aim to use this feedback to automatically infer the user X  X  interest in particular pieces of information, i.e. , nuggets.
 Learning from user feedback poses two main challenges: First, user feedback is generally available at the document level. However, we need to learn the concept of usefulness at a much lower granularity of nuggets. Second, unlike tra-ditional retrieval setup, where the relevance of each docu-ment is assumed to be independent of other documents in the ranked list, the usefulness of each document depends on other documents presented in the ranked list. Therefore, the user X  X  feedback on a document can no longer be assumed to be independent of what he or she has seen before the current document. In other words, the user X  X  feedback is an indica-tor of the marginal utility of a document, not its absolute utility.
To solve both these problems, we use a learning approach based on logistic regression, which models the user X  X  feed-back as a function of the marginal gain provided by each document (as opposed to its absolute gain). Specifically, the log-odds of receiving a positive feedback on a document is modeled as a linear combination of the marginal gain of each nugget in the document: where f i is the feedback on the i th document, and g i ( w ) is the corresponding marginal gain interpreted as a function of the weights w of the nuggets: where w  X  is the weight of nugget  X  , and E n  X  ( d 1: i  X  1 expected number of times nugget  X  has been seen by the user before the current document.

Thus, each document in a ranked list is a training in-stance with label equal to the user X  X  feedback (+1 or  X 1), and predictors equal to the marginal gain of each nugget. The goal of logistic regression is to find the weights for nuggets that best explain the observed user feedback. The optimal weights w  X  are found through maximum a-posteriori (MAP) estimation, using a Normal prior whose mean is equal to the current estimate of the weights: where  X  controls the strength of the prior. The use of a prior allows the system to adapt to the user X  X  interests in an incremental manner by using the previous iteration X  X  weights as the prior for the current step.

The optimal weights maximize the log-likelihood over all documents on which feedback is received: ` ( w ) =  X  X which can be solved efficiently using conjugate gradient as-cent [18].
Let us estimate the time complexity of the proposed ap-proach and compare it against the MMR strategy for novelty-based ranking. Assume an initial candidate set of documents (cf. Section 2.2.1) of size C . Since retrieval of this candidate set is common to both re-ranking approaches (and also not the focus of this paper), we will ignore it from the time com-plexity calculation. The goal is to create a new ranked list of length k . Assume that each document contains W words on average.

The proposed approach first processes the candidate set of documents to assign initial weights to all surrogate nuggets ( e.g. , words), which requires O( CW ) operations. Then, the ranked list is built in k steps: At each step, score the remain-ing O( C ) documents based on their marginal utility, which requires O( W ) operations per document. This step leads to a time complexity of O( kCW ). Hence, the total time complexity of the proposed approach is O( CW + kCW ).
The MMR-based approach is also based on iteratively building the ranked list: At each of the k steps, score the Figure 1: Time complexity of the proposed nugget-based re-ranking vs. that of MMR-based re-ranking approach. remaining O( C ) documents by computing their cosine sim-ilarity with each of the O( k ) documents already selected in the ranked list. A single cosine similarity computation takes O( W ) operations. This leads to a time complexity of O( k 2 CW ).

Since MMR is based on computing similarities with all previously displayed documents, its computation is quadratic in the length of the ranked lists, which can be prohibitive for long ranked lists, or multiple ranked lists in a search ses-sion. On the other hand, the proposed approach is based on marginal utilities of documents, which can be computed using a running total of the number of times each nugget has been previously displayed to the user. This acts as a succinct representation of the user X  X  browsing history and leads to the linear time complexity with respect to ranked list length. Figure 1 shows the difference in time complex-ities of the two approaches as a function of the ranked list length ( k ), assuming that the average document length ( W ) is 50 words and the initially retrieved set of candidate docu-ments is three times the number of documents in the target ranked list, i.e. , C = 3  X  k .
We use the Topic Detection and Tracking dataset (de-scribed below) to compare our proposed approach against the following baselines: (i) Indri [26], which is a state-of-the-art retrieval engine, and represents a purely relevance based approach, and (ii) Indri+MMR: Indri is used to create an initial set of documents for each query, which are re-ranked according to the Maximal Marginal Relevance (MMR) [4] criterion. This represents a baseline for novelty-based rank-ing that uses cosine similarities between document vectors to measure redundancy, which is then combined with the relevance score in a linear fashion 5 .

We include four variants of the proposed approach to un-
MMR involves a parameter  X  (see [4] for details) that con-trols the trade-off between relevance and novelty. We used a validation set to choose the best value of  X  in all experi-ments. derstand its behavior: (i) Indri+W, which re-ranks an ini-tial set of documents returned by Indri by using words as surrogates for nuggets, but makes no use of feedback, (ii) Indri+W+F, which again uses words as surrogates but up-dates their weights based on user feedback, (iii) Indri+NE+F, which uses named entities as surrogates and leverages user feedback as above, and (iv) Indri+W+NE+F, which uses both words and named entities as surrogates and also lever-ages user feedback.

To assess the behavior and performance of the greedy al-gorithm for novelty-based ranking, we generate synthetic ranked lists and evaluate the ranked list returned by the greedy algorithm against the true ideal ranked list obtained using exhaustive search.
Topic Detection and Tracking (TDT) Data. TDT4 was a benchmark corpus used in Topic Detection and Track-ing ( TDT2002 and TDT2003 ) evaluations. It consists of over 90,000 articles from various news sources published between October 2000 and January 2001. This corpus was extended for novelty-based evaluations by creating 120 queries with corresponding nuggets and nugget-matching rules as described in [13, 28]. To simulate session-based retrieval with multi-ple rounds of retrieval and user feedback, we divided the 4-month span of the corpus into 25 chunks, each compris-ing approximately 5 consecutive days. A retrieval system is expected to produce a ranked list of documents at the end of each chunk, receive feedback from the user, and then produce a new ranked list for the next chunk, and so on. This setup simulates a user who is following an evolving news event over an extended period of time X  X xpecting the retrieval system to return a personalized ranked list of rele-vant and novel documents after every 5 days.
We used EGU with three different values of  X  : 0.0, 0.1, and 1.0, to simulate different tolerances towards redundancy. 0.0 corresponds to no tolerance, 0.1 corresponds to some toler-ance, and 1.0 corresponds to full tolerance for redundancy, i.e. , the traditional relevance-only based retrieval setup. The user X  X  stopping probability p was set to 0.1, which corre-sponds to an average reading length of 10 documents. Since we are mainly interested in the ability of the system to re-turn relevant and novel documents, and do not care about the lengths of the ranked lists (as is common in ranked re-trieval evaluation), we use zero cost in EGU (see Section 2.1).
Main Results. Table 1 shows the performance obtained by the baselines and different settings of the proposed ap-proaches 6 . The use of words and named entities as sur-rogates for re-ranking, with weights updated through user feedback (Indri+W+NE+F) performs the best. The per-formance of words-only with feedback (Indri+W+F) and named entities-only with feedback (Indri+NE+F) is close in all cases, but their combination leads to the best perfor-mance.

When novelty is a factor of consideration ( i.e. ,  X  = 0 . 0 or 0 . 1), the proposed approach using words as surrogates but
The symbols  X  and  X  indicate statistically significant dif-ferences ( p &lt; 0 . 01 for sign test with paired queries) with respect to the two baselines, respectively.
 Table 1: EGU scores of different systems for three values of  X  .
 without feedback (Indri+W) performs better than the base-lines, which demonstrates its ability to model novelty effec-tively. However, it is disappointing that the feedback-based variants do not exhibit a substantial improvement over the no-feedback variant. To some extent, this can be explained by the fact that the benefits of feedback are nullified by the demand for novelty: Through feedback, the user indicates interest in specific items, but at the same time, expects the system to not retrieve the same (or very similar) items in the future, but instead, other items that are relevant and novel. Our evaluation merely shows that the benefits of user feed-back may be overestimated if novelty (or redundancy) is not taken into account.

On the other hand, when the user has full tolerance for redundancy ( i.e. ,  X  = 1 . 0), the proposed approach without any feedback (Indri+W) performs worse than the baseline, which shows that the proposed re-ranking approach is less effective for relevance-based ranking unless feedback is pro-vided. Hence, there is an opportunity to improve the re-ranking approach itself (possibly through the use of more sophisticated surrogates for nuggets), which would lead to further improvements across all settings. When feedback is provided, the proposed method indeed performs substan-tially better.

For  X  = 0 . 0 and 0 . 1, MMR-based retrieval (Indri+MMR) performs better than the baseline of relevance-only ranking (Indri), as expected. Although the improvement in perfor-mance of the proposed approaches over MMR is not substan-tial (especially if feedback is not considered), the proposed approaches provide a computationally efficient alternative without requiring parameter tuning. In the MMR approach, the parameter  X  must be re-tuned for different tolerances for redundancy (  X  ), whereas the proposed approaches directly take this into account through the definition of marginal util-ity (Eq. 7), which can enable a user to dynamically change his or her redundancy tolerance in a deployed system. More-over, as mentioned in Section 3, the proposed approach can be much more computationally efficient compared to the MMR approach for longer (or multiple) ranked lists. For  X  = 1 . 0, the MMR approach obtains the same performance as the Indri baseline because the optimal value of  X  (see Section 4) was found to be zero as expected, i.e. , no novelty component in the document scoring.

Performance of the greedy algorithm. An impor-tant step in our approach is the use of the greedy algorithm Figure 2: Approximation factors achieved by the greedy algorithm on synthetic ranked lists for dif-ferent redundancy tolerances  X  . to rank the documents so as to maximize the coverage of nuggets at the top ranks. Therefore, we wish to understand the performance characteristics of the greedy algorithm, and assess whether its use is justified for optimizing EGU . While the greedy algorithm has been shown to perform well for solving Max-Cover and Set-Cover problems [5], EGU is a more general form of Max-Cover due to the expectation taken over all possible rank positions (thus, leading to a probabilistic version of Max-Cover ) as well as the notion of diminishing returns adjustable using the  X  parameter (thus, leading to a notion of  X  X oft coverage X ).

We use 1000 randomly generated ranked lists to assess the effect of  X  (user X  X  redundancy tolerance) on the behav-ior of the greedy algorithm. Figure 2 shows the performance obtained by the greedy algorithm for different values of  X  . We have plotted the mean (flanked by minimum and max-imum) approximation factors 7 , i.e. , the score of the greedy algorithm divided by the best possible score obtained us-ing exhaustive search. Notice that the approximation fac-of  X  increases. This is intuitive, since  X  = 1 corresponds to relevance-based ranking, which does not penalize redun-dancy, and hence, a simple greedy approach of ranking by decreasing number of nuggets is provably optimal. At the other extreme,  X  = 0 corresponds to the  X  X ard X  notion of coverage where every nugget is only counted once, which corresponds to the standard Max-Cover problem.
One of the earliest works on novelty and diversity-based ranking is the Maximal Marginal Relevance (MMR) method [4], which proposed a greedy algorithm that incrementally builds the ranked list by choosing the next document with the highest  X  X arginal X  relevance, i.e. , high relevance to the query, and low similarity to already selected documents in the ranked list. However, as mentioned in Section 2.2, MMR can lead to sub-optimal performance due to the independent treatment of relevance and novelty. Our proposed approach is based on a unified model of relevance and novelty in terms of nuggets.
Only those ranked lists where the greedy algorithm led to sub-optimal performance were included.

Zhai et al. [30] proposed an approach for diversity-based retrieval, where diversity is defined in terms of the num-ber of sub-topics covered by the retrieved documents for a given search topic. To measure the quality of diversity-based rankings, they extended the traditional measures of recall and precision for sub-topic retrieval and defined two new measures: S-recall and S-precision. However, the au-thors point out the challenges in defining a single summary measure that combines relevance and novelty. As a compro-mise, they measure aspect coverage at a few arbitrarily de-fined recall levels. For system optimization, the authors use language modeling based scores for relevance and novelty in an MMR-like formulation. However, it is evident from the definition of S-recall and S-precision that their framework does not allow different tolerances towards redundancy: A sub-topic is either covered or uncovered, and subsequent pre-sentations or the same sub-topic receive no credit. In other words, S -recall and S -precision are inflexible in that they assume no tolerance towards redundancy for all users.
Agrawal et al. [1] proposed an approach for maximiz-ing the coverage of different categories of documents in the ranked list to deal with the inherent ambiguity associated with certain user queries. Their goal was to maximize the probability that the user will find at least one document rel-evant to his or her true intent. However, the authors admit that such a criterion might be too conservative when serv-ing users who desire a certain level of redundancy. More-over, their objective function is set-based, i.e. , it does not differentiate between different permutations of the selected documents. In reality, the ranking of documents plays an important role in the perceived utility of the system. The top-down browsing behavior of users is not explicitly mod-eled by the objective function, and only manifests as a side-effect of the greedy ranking algorithm.

In the above-mentioned approaches, there is a disconnect between the evaluation metric (e.g., S-recall, S-precision in [30], and IA-NDCG and IA-MAP in [1]), and the objective function used by the system. In contrast, this paper repre-sents the first framework where the evaluation metric and objective function coincide for the optimization of relevance and novelty-based retrieval.
 Clarke at el. [8] proposed  X  -NDCG as a variation of NDCG to model relevance and novelty in terms of nuggets. However,  X  -NDCG is not based on an explicit model of user browsing behavior, i.e. , the likelihood of user stopping at various ranks. Therefore, it does not naturally extend to multiple ranked lists since it is not clear which nuggets in documents from previous ranked lists would be deemed as read by the user for the purpose of evaluating the current ranked list. On the other hand, EGU is based on a proba-bilistic model of user behavior, and hence, is a more general optimization criterion for retrieval performance over one or more ranked lists.

El-Arini et al. [11] proposed an approach for learning from user X  X  feedback to provide a personalized set of diverse blogs to the user. They address the problem of non-independent feedback. However, their objective function is set-based, and hence, does not take ranking performance into account. Also, similar to the other approaches for diversity-based retrieval, different tolerances towards redundancy are not taken into account.

Radlinski et al. [22] proposed the use of click-through data to improve the document rankings produced by the retrieval system. Since real users implicitly take all pertinent factors (relevance and novelty with respect to previously seen doc-uments) into account when clicking on documents, such an approach can optimize for novelty without explicitly mod-eling it. However, such approaches are expensive since they require multiple interactions with real users to collect click-through patterns on different variations of ranked lists for each query. Therefore, such approaches do not provide an efficient means for offline evaluation and tuning of new and potentially risky algorithms.
Expected Global Utility ( EGU ) is a recently proposed met-ric that has several desirable characteristics for measuring the performance of novelty-based ranking systems. We present the first approach for directly optimizing retrieval systems with respect to this performance measure. EGU models rel-evance and novelty in terms of  X  X uggets X ; Since a retrieval system does not have access to the true nuggets for a given query, our approach is based on the use of observable fea-tures like words and named entities as surrogates for the true nuggets, whose weights are updated based on user feedback in an iterative search session. We show that the ranking problem is NP-hard, and use mathematical as well as em-pirical analysis to demonstrate that a simple greedy algo-rithm achieves good performance with respect to EGU . Our experiments on a nugget-based data collection indicate that the proposed approach can successfully optimize the perfor-mance in terms of EGU , compared to a purely relevance based approach (Indri) as well as an MMR-based approach, which is computationally expensive for longer (or multiple) ranked lists. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and [3] A. Caputo, P. Basile, and G. Semeraro. Boosting a [4] J. Carbonell and J. Goldstein. The use of MMR, [5] B. Carterette. An Analysis of NP-Completeness in [6] H. Chen and D. Karger. Less is more: probabilistic [7] R. Church and C. ReVelle. The maximal covering [8] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, [9] G. Cornuejols, M. Fisher, and G. Nemhauser.
 [10] H. Dang, J. Lin, and D. Kelly. Overview of the TREC [11] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. [12] H. Greisdorf. Relevance thresholds: a multi-stage [13] D. He, P. Brusilovsky, J. Ahn, J. Grady, R. Farzan, [14] K. J  X  arvelin, S. Price, L. Delcambre, and M. Nielsen. [15] G. Kumaran and J. Allan. Text classification and [16] A. Lad and Y. Yang. Generalizing from relevance [17] J. Lafferty and C. Zhai. Probabilistic relevance models [18] T. Minka. A comparison of numerical optimizers for [19] S. Mizzaro. Relevance: The whole history. Historical [20] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis [21] J. Prager, E. Brown, A. Coden, and D. Radev.
 [22] F. Radlinski, R. Kleinberg, and T. Joachims. Learning [23] S. Robertson and S. Walker. Some simple effective [24] I. Soboroff. Overview of the TREC 2004 novelty track. [25] R. Srihari and W. Li. A question answering system [26] T. Strohman, D. Metzler, H. Turtle, and W. Croft. [27] Y. Yang and A. Lad. Modeling Expected Utility of [28] Y. Yang, A. Lad, N. Lao, A. Harpale, B. Kisiel, and [29] Y. Yue and T. Joachims. Predicting diverse subsets [30] C. Zhai, W. Cohen, and J. Lafferty. Beyond [31] C. Zhai and J. Lafferty. Model-based feedback in the [32] Y. Zhang, J. Callan, and T. Minka. Novelty and
The computation of EGU becomes intractable with increas-ing number and lengths of ranked lists. Specifically, the com-putation of expected gain requires summing over all combi-nations of stopping positions in all ranked lists. Expected cost is easy to calculate for the linear definition of cost used in this paper. Therefore, we will only focus on the com-putation of expected gain over multiple ranked lists. First, let us re-write expected utility as the sum of expected gain obtained for each nugget: where nugget counts  X   X  ( s ) depend on the stopping positions s = s 1 ...s K in the K ranked lists, respectively. We can approximate EGU by moving the expectation operator inside: That is, instead of calculating the expected gain with respect to different browsing patterns, we compute the gain obtained by the expected number of times each nugget will be read from all ranked lists, i.e. , E [  X  ( s )]. This quantity can be efficiently calculated by summing over the expected nugget counts in each ranked list: Thus, the approximate computation requires a sum over O ( | L 1 | + | L 2 | + ... + | L K | ) terms, instead of the O ( | L | L 2 |  X  ...  X  | L K | ) terms in the original calculation, which must consider all combinations of stopping positions in the K ranked lists.

Let us focus on a particular parameterization of  X  = 0 i.e. , no tolerance towards redundancy, and P ( s = k ) = 1, i.e. , the user reads all documents from the top down and stops at a given rank, say k . Given a set of documents, all nuggets that appear in at least one of these documents are said to be covered by the set of documents. Then, finding a set of k documents that cover the most number of nuggets is exactly equivalent to the Maximum Coverage Problem , which is known to be NP-hard [7].

Maximum Coverage Problem ( Max-Cover ): Given a collection of sets S = S 1 ,S 2 ,...,S m , each containing a subset of elements, i.e. , S i  X  X  e 1 ,e 2 ,...,e n } , find the subset S  X   X  S of size K such that the number of covered elements is maximized:
Our ranking problem can be reduced to Max-Cover by mapping documents to sets and nuggets to elements.
Submodularity. Our objective function admits addi-tional structure that allows approximation algorithms to guarantee good performance. Specifically, the gain func-tion of EGU is submodular . Submodularity formalizes the intuitive property of diminishing returns, and is defined as follows [20]: A set function F is called submodular if and only if for all A  X  B  X  V and s  X  V \ B it holds that F ( A  X  X  s } )  X  F ( A )  X  F ( B  X  X  s } )  X  F ( B ).
The submodularity of the gain function follows directly from its concavity with respect to nugget counts. Intuitively, the increase in gain obtained by adding a document d i to a ranked list L can never be larger than the increase obtained by adding d i to a subset of L .

A classic result shows that the simple greedy algorithm of incrementally building the list of documents based on de-creasing marginal utilities guarantees a constant approxi-mation ratio: For any monotonic submodular function, the greedy algorithm achieves an approximation ratio of (1  X  1 /e ) [20].

However, this lower-bound can be further improved by taking the special structure of EGU into account, as we show in Section A.3.

Here, we develop a tighter bound on the performance of the greedy algorithm for the optimization of EGU . The lower-bound for Max-Cover is 1  X  1 /e , which is approximately 0 . 63. The main idea for deriving the new bound is that the lower bound of 1  X  1 /e is too conservative: It is guaranteed irrespective of the size k of the covering problem, whereas EGU involves an expectation of Max-Cover problems of size k = 1 , 2 ,... , i.e. , all stopping positions.

The lower bound for Max-Cover , as a function of k is [9]: where g k is the greedy solution and I k is the ideal solution. The bound of 1  X  1 /e arises because: which approaches equality for large k . However, for smaller values of k , the gap is large. For instance, for k = 1, the expression (1  X  1 /k ) k is equal to zero, which corresponds to the fact that the solution of size 1 is always optimal. In other words, approximation factors better than 0.63 can be guaranteed for covering problems of smaller size. Since the total gain, say G , is calculated by taking an expectation over all stopping positions, i.e. , k = 1 , 2 ,... , we can therefore derive a tighter bound by taking the size-dependent bound into account. Specifically, the total gain of the ideal solution, say I , is equal to: But due to Eq. (16), we have: Therefore, the desired bound is: Figure 3: Comparison of the original (1  X  1 /e ) and the improved bounds for various values of stopping probability p .

We compared this bound against the original bound of (1  X  1 /e ) by running the greedy algorithm on 1000 synthetic ranked lists. Figure 3 shows the bounds obtained for vari-ous values of the stopping probability p (note that Pr(  X | p ) is a geometric distribution with parameter p ). Note that the improved bound is dependent on the greedy scores as well as the stopping probability, which is evident in Eq. (20). Also, the bound improves (get closer to 1) with increasing values of p , which is expected behavior because higher values of p correspond to higher likelihood of the user to stop at one of the top ranks, where the greedy algorithm guarantees bet-ter worst-case performance: In the extreme case of p = 1, i.e. , the user only reads the first document, the greedy strat-egy of choosing the document with most number of nuggets (breaking ties arbitrarily) is provably optimal.
