 Robert C. Williamson BOB . WILLIAMSON @ ANU . EDU . AU Tsinghua University, Beijing 100084, China We study multiclass proper composite losses which are the composition of a proper loss and and invertible link (both defined formally below). This representation makes the un-derstanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the nu-merical ( Vernet et al. , 2011 ). The statistical properties are controlled by the proper loss. The link function is essen-tially just a parametrisation. Choice of a suitable link can help  X  for example, a nonconvex proper loss can be made convex (and thus more amenable to numerical optimisa-tion) by choice of the appropriate link. In this paper we show how this is possible, when a composite loss is con-vex, and how to convexify an arbitrary proper multiclass loss. The results extend the results on binary composite losses due to Reid &amp; Williamson ( 2010 ). 1.1. Previous Work Proper losses are the natural losses to use for probability estimation. The key property of a proper loss (see  X  2.1 be-low) is that its expected value is always minimised by the distribution defining the expectation. They have been stud-ied in detail when n = 2 (the  X  X inary case X ) where there Raftery , 2007 ; Reid &amp; Williamson , 2011 ), and characteri-zation ( Reid &amp; Williamson , 2010 ) when differentiable. The theory of loss functions makes it clear how one ide-ally chooses a loss  X  one takes account of one X  X  util-ity concerning various incorrect predictions ( Kiefer , 1987 ), ( Berger , 1985 , Section 2.4). The practice rarely involves such a step. There is little guidance in the literature con-cerning how to choose a loss function; typically heuristic arguments are used for the choice  X  confer e.g. ( Ighodaro et al. , 1982 ; Nayak &amp; Naik , 1989 ). Early approaches to multiclass losses used a simple reduction to binary ( Diet-terich &amp; Bakiri , 1995 ). More recently, other approaches to the design of losses for multiclass prediction have received 2009 ), although none of these papers developed the con-nection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry condi-tions). Zou et al. ( 2005 ) proposed a multiclass generali-sation of  X  X dmissible losses X  (their name for classification calibration) for multiclass margin classification. Liu ( 2007 ) considered several multiclass generalisations of hinge loss (suitable for multiclass SVMs) and showed some of them were and others not Fisher consistent. When they were not it was shown how the training algorithm could be modified to make the losses behave consistently. Multiclass losses have also been considered in the development of multiclass 2011 ; Wu &amp; Lange , 2010 ). 1.2. Key Contribution and Significance The key point of the paper is as follows. Multiclass losses are necessary in many problems. To date they have typi-cally been constructed as margin losses via a convex func-tion f applied to a generalised notion of  X  X argin X . This is unsatisfactory from a design perspective because it con-founds two distinct issues: the decision theoretic notion of a loss (that captures what it is that is important to the end user ( Berger , 1985 , confer)) and issues associated with the ease of numerical optimisation. Furthermore, margin losses are not particularly well suited to the non-symmetric treatment of different classes, as is necessary in many ap-plications. Fortunately there is a way of neatly separat-ing these two concerns through the use of a composite loss trolled by the choice of proper loss, and the optimisation properties via the link. This leads to the natural question: suppose one fixes a proper loss (and hence the statistical properties), how should one choose a link to ensure con-vexity of the overall loss? The answer is not obvious and not trivial, and is the key technical contribution of the pa-per (Theorem 5 ). The result opens up the possibility of a more systematic approach to the design of multiclass losses which previously has been approached in a rather ad hoc manner. Suppose X is some set and Y = { 1 ,..., n } =[ n ] is a set of labels. We suppose we are given data S = * x i , y i + such that y i 2 Y is the label corresponding to x i 2 X . These data follow a joint distribution P X , Y on X  X  [ n We denote by E X , Y and E Y | X respectively, the expecta-tion and the conditional expectation with respect to P X , Y Given a new observation x we want to predict the probabil-Multiclass classification requires the learner to predict the most likely class of x ; that is to find  X  y = argmax i 2 2.1. Losses A loss measures the quality of prediction. Let D n : = { ( p 1 ,..., p n ) :  X  i 2 [ n ] p i = 1 , and 0  X  p i  X  1 , 8 note the n-simplex . For multiclass probability estima-tion, ` : D n ! R n R ( ` ing q 2 D n when y = i . Throughout the paper, A 0 denotes transpose of a matrix A , except when applied to a real-valued function where it denotes derivative. We denote the matrix multiplication of compatible matrices A and B by A  X 
B , so the inner product of two vectors x , y 2 R n is x 0 The conditional risk L : D n  X  D n ! R + associated with a loss ` is the function where Y  X  p means Y is drawn according to a multinomial distribution with parameter p 2 D n . In a typical learning problem one will make an estimate q : X ! D n . The full risk is L ( q )= E X E Y | X ` Y ( q ( X )) .
 Minimizing L ( q ) over q : X ! D n is equivalent to mini-mizing L ( p ( x ) , q ( x )) over q ( x ) 2 D n for all x p ( x )=( p 1 ( x ) ,..., p n ( x )) 0 , and p i ( x )= P ( Y = Thus when there is no restriction on the hypothesis class, it suffices to only consider the conditional risk; confer ( Reid &amp; Williamson , 2011 ).
 If one is interested in estimating probabilities ( ` : D n it is natural to require the associated conditional risk is minimized when estimating the true underlying probability. Such a loss is called proper (formally: if L ( p , p )  X  L 8 p , q 2 D n ). It is strictly proper if the inequality is strict when p 6 = q (so it is uniquely minimised by predicting the correct probability). The conditional Bayes risk This function is always concave ( Gneiting &amp; Raftery , 2007 ). If ` is proper, then L ( p )= L ( p , p )= p 0  X  ` ( Strictly proper losses induce Fisher consistent estimators of probabilities: if ` is strictly proper, p = argmin q L The losses above are defined on the simplex D n since the argument (an estimator) represents a probability vector. However it is sometimes desirable to use another set V of predictions. One can consider losses ` : V ! R n there exists an invertible function y : D n ! V . Then ` can be written as a composition of a loss l defined on the sim-plex with y 1 . That is, ` ( v )= l y ( v ) : = l ( y 1 ( a function l y is a composite loss . If l is proper, we say a proper composite loss , with associated proper loss l and link y . Binary proper composite losses have been studied by ( Reid &amp; Williamson , 2010 ). 2.2. Matrix Differential Calculus In order to differentiate the losses we project the n -simplex into a subset of R n 1 . Let  X  n : = n 1. Let denote the  X  X ottom X  of the n-simplex . We denote by
P D : D n 3 p =( p 1 ,..., p n ) 0 7!  X  p =( p 1 ,..., p the projection of the D n , and its inverse.
 We use the following notation. The k th unit vector e k is the n vector with all components zero except the k th which is 1. f is denoted D f and its Hessian H f . The (relative) interior of the simplex is  X  D n : = { ( p 1 ,..., p n ) :  X  i 2 [ p &lt; 1 , 8 i 2 [ n ] } and the boundary is  X  D n : = D n \ of A stacked on top of each other. The Kronecker product of an m  X  n matrix A with a p  X  q matrix B is the mp  X  nq matrix We use the following properties of Kronecker prod-ucts (see Chapter 2 of Magnus &amp; Neudecker ( 1999 )): (
A  X  B )( C  X  D )=( AC  X  BD ) for all appropriately sized A , B , C , D , and I 1  X  A = A .
 If f : R n ! R m is differentiable at c then the partial deriva-tive of f i with respect to the j th coordinate at c is denoted D j f i ( c ) The m  X  n matrix of partial derivatives of f is the Jacobian of f and denoted If F is a matrix valued function D F ( X ) : = D f ( vec X where f ( X )= vec F ( X ) .
 We will require the product rule for matrix valued functions ( Vetter , 1970 ; Fackler , 2005 ): Suppose f : R n ! R m  X  p g : R n ! R p  X  q so that ( f  X  g ) : R n ! R m  X  q . Then
D ( f  X  g )( x )=( g ( x ) 0  X  I m )  X  D f ( x )+( I q  X  f The Hessian at x 2 X  X  R n of a real-valued function f : X ! R is the n  X  n real, symmetric matrix of second derivatives at x Note that the derivative D k , j is in row j , column k . It is easy to establish that the Jacobian of the transpose of the Jacobian of f is the Hessian of f . That is, ( Magnus &amp; Neudecker , 1999 ). If f : X ! R m for X  X  is a vector valued function then the Hessian of f at x 2 X is the mn  X  n matrix that consists of the Hessians of the functions f i stacked vertically: If A and B are square matrices, A &lt; B if A B is positive semidefinite. In order to establish the convexity and other properties of composite losses we start by proving some identities for their first and second derivatives.
 Suppose ` = l y 1 is composed of the proper loss l : to simplify matters, derivatives for the function ` : V ! we will assume the set V is a flat, ( n 1 ) -dimensional, convex subset of R n trary manifold the extra definitions required to make sense of convexity ( e.g. , in terms of geodesics) and derivatives on manifolds would obscure the thrust of the results below. Furthermore, little is lost either practically or theoretically by assuming a simple V . In practice, predictions are usu-ally vectors in R n a parametrisation of V in terms of some simpler space U and redefine the link via composition with that parametri-sation. Alternatively, since links must be invertible, a com-posite loss could be defined by a choice of loss and choice of inverse link y 1 : V ! D n for a V assumed to be flat, etc.
 Let v 2 V fixed but arbitrary with corresponding  X  p =  X  y 1 ( v ) where  X  y (  X  p ) : = y (  X  p 1 ,...,  X  p  X  rule and the inverse function theorem the derivatives for each of the partial losses ` i satisfy Let us write e n i as the i th n -dimensional unit vector, e ( 0 ,..., 0 , 1 , 0 ,..., 0 ) 0 when i 2 [ n ] , and define e i &gt; n . We can now write D l i (  X  p ) in terms of the n trix D l (  X  p ) using D l i (  X  p )=( e n i ) 0  X  D l ( ( D  X  l (  X  p ) 0 , D l n (  X  p ) 0 ) 0 , where  X  l (  X  p )=( so Furthermore, since l is proper, Lemma 5 by van Erven et al. ( 2011 ) means we can use the relationship between a proper loss and its projected Bayes risk  X  L : = L P 1 write where W (  X  p ) : = I  X  n  X  n  X   X  p 0 and where y (  X  p and p n (  X  p ) : = 1  X  i 2 [  X  n ] p i .
 Thus, combining ( 4  X  6 ) we have for all i 2 [  X  n ] and be merged and combined with ( 3 ) to obtain the following proposition.
 Proposition 1 For all i 2 [ n ] ,  X  p 2  X   X  D n , and v where k (  X  p ) : = H  X  L (  X  p )[ D  X  y (  X  p )] 1 . Using the definition of the Hessian H ` i = D [ D ` 0 i ] product rule ( 1 ) gives
D where D v is used to indicate that the derivative is with re-spect to v even when the terms inside the derivative are ex-pressed using  X  p . We have now established the following proposition.
 Proposition 2 For all i 2 [ n ] ,  X  p 2  X   X  D n , and v where k (  X  p ) : = H  X  L (  X  p )  X  [ D  X  y (  X  p )] The product k (  X  p ) : = H  X  L (  X  p )[ D  X  y (  X  p both propositions above can be interpreted as the curvature the link function  X  y . When the link function is the identity proper loss case) the expressions for the derivative and Hes-sian of each ` i simplify to The form of k as the product of H  X  L and D  X  y suggests an-other simplification. The canonical link function for a loss l with Bayes risk L is defined by the relationship for all  X  p . (We will show in section 5.1 that this is guaran-teed to be a legitimate link.) We see the term k simplifies to k (  X  p )= I  X  n since D  X  y (  X  p )= D ( D  X  L ( this choice of link function, the first and second derivatives become considerably simpler.
 Proposition 3 If l : D n ! R n its associated canonical link then, for all i 2 [ n ] ,  X  p The simplified form of the Hessian above is established by noting that since k (  X  p )= I  X  n we have D [ k (  X  y 1 all v 2 V in Proposition 2 .
 While the above propositions hold for any number of classes n , it is instructive (both here and later in the pa-per) to examine the binary case where n = 2. In this case, Proposition 1 and Proposition 2 reduce to Convexity of a loss is desirable for the ease of numerical optimisation of an empirical risk. We will now consider when multiclass proper losses are convex, and give a char-acterisation in terms of the corresponding Bayes risk which as we have seen is the natural way to parametrise a loss. The results in this section are the multiclass generalisation of the characterisation of convexity of binary proper losses ( Reid &amp; Williamson , 2010 ). In fact we obtain more general results even in the binary case because here we consider strongly convex losses. We will also show how any non-convex proper loss can be made convex by suitable choice of a link function, specifically: the canonical link. We define a loss ` : D n ! R n loss is convex if, under and distribution p over outcomes i see that ` is convex if and only if ` i : D n ! R + is convex for all i 2 [ n ] . (The  X  X f X  part follows since a sum of convex functions is convex; the  X  X nly if X  follows by considering p = e i , for i 2 [ n ] .) Definition 4 Suppose C  X  R n is convex. A function f : C ! R is strongly convex on C with modulus c 0 if for all x , x 0 2 C, 8 a 2 ( 0 , 1 ) , When c = 0 in the above definition, f is convex. The function f is strongly convex on C with modulus c if and Lemar  X  echal , 2001 , page 73). Therefore, the maps v 7! ` are c -strongly convex if and only if H ` i ( v ) &lt; cI ing Proposition 2 we obtain the following characterisation of the c -strong convexity of the loss ` .
 Theorem 5 A proper composite loss ` = l y 1 is strongly convex with modulus c 2 [ 0 , 1 ] if and only if for all  X  p 2  X   X  D n and for all i 2 [ n ] e  X  n i  X  p  X  I  X  n  X  D k  X  y 1 ( v ) 4 k (  X  p ) 0  X  [ D We now consider the implications of Theorem 5 in two spe-cial cases: in the multiclass case with canonical link, and in the binary case with the identity link. 4.1. Implications for Canonical Links Recall that the canonical link  X  y ` is chosen so that  X  y so D k (  X  p )= 0. In this case, equation ( 17 ) reduces to the following corollary.
 Corollary 6 If ` = l y 1 is defined so that  X  y = D  X  L then each map v 7! ` i ( v ) is c-strongly convex if and only if  X 
H  X  L (  X  p ) An immediate consequence of this result is obtained by observing the definiteness constraint is always met when c = 0 since  X  L is always a concave function. Thus, using a canonical link guarantees a composite loss is convex . There is a analogous upper definiteness condition to strong convexity that has implications for optimisation rates. In twice differentiable function f : X ! R satisfies for all x 2 X  X  R n then the value M m is an upper bound on the condition number of H f , that is, the ratio of maximum to minimum eigenvalue of H f . This value measures the eccentricity of the sublevel sets of f and controls the rate at which optima of f are approached.
 Applying this result to the Hessian of a composite loss ` with a canonical link shows that the condition number bound is controlled by the Hessian of the Bayes risk of ` Specifically, if the condition number is to be no more than M M = m and the condition number is 1, the only Hessian the Bayes risk surface for square loss. Thus, square loss is the only canonical composite loss for which a condition number of 1 is possible . 4.2. Implications for Binary Losses In the binary case, when n = 2, ( 15 ) and ( 16 ) and the posi-tivity of  X  y 0 simplify ( 17 ) to two conditions: Further assuming that  X  y is the identity link (  X  y ( v letting w ( p ) : =  X  L ( p ) gives The last equivalence is achieved by dividing through by w ( p ) c which must necessarily be positive since if it were not the final pair of inequalities would imply 1 p 1 1 p ,a to ( Reid &amp; Williamson , 2010 , Corollary 26) for c = Observe that if g ( p ) : = log ( w ( p ) c ) then g 0 ( p is the middle term in ( 18 ). This allows a simplification of the inequality. Specifically, if we assume w ( 1 2 )= 1 then ) , log ( q ) log ( 2 ) Q g ( q ) log ( 1 c ) (19) , which gives the following proposition purely in terms of w ( p ) , rather than w ( p ) and its derivative.
 w ( 1 / 2 )= 1 . A proper binary loss ` : D 2 ! R 2 convex with modulus c 2 [ 0 , 1 ] only if where Q denotes  X  for p 1 2 and denotes for p  X  1 2 . When c = 0 (corresponding to ` being convex) this is equiv-alent to an expression by Reid &amp; Williamson ( 2010 , Equa-tion 31). Equation 20 is illustrated in Figure 1 . The above proposition only gives a necessary condition for strong convexity. (In addition to w belonging to the speci-fied region, w 0 ( p ) also needs to be suitably controlled). A sufficient condition is useful for designing strongly convex proper losses. Observe that if where u : [ 0 , 1 ] ! R and K , c 2 R , then  X   X  p log ( w u ( p ) . We require w ( 1 / 2 )= 1 thus exp c = 1, so e K = 1 c and satisfies ( 18 ) if and hence the loss with weight function w is strongly con-vex with modulus c . Thus by choosing u and constructing w via ( 21 ) one can design strongly convex proper binary losses.
 One can ask whether equation ( 17 ) can be simplified in the n &gt; 2 case by using a matrix version of the logarithmic 1991 , Section 6.6.19) but it requires that ( H  X  L (  X  p the case. The theory developed above suggests that each choice of proper loss l and link function y results in an overall loss function with properties ( e.g. , convexity) that depend en-tirely on their relationship to each other. Given these two  X  X nobs X  for parameterising a loss function, we can begin to ask what kind of practical trade-offs are involved when se-lecting a composite loss as a surrogate loss for a particular problem.
 We now propose a simple scheme for constructing fami-lies of losses with the same Bayes risk. This is achieved by fixing a choice of proper loss l and creating a param-eterised family (described below) of link functions y a for parameters a 2 A . Since the Bayes risk is entirely deter-mined by l any composite loss l y 1 a for a 2 A will have Bayes risk L ( p )= p 0 l ( p ) . Thus, we are able to examine the effect different choices of composite loss can have on a problem without changing the essential underlying prob-lem . 1 Through some simple experiments we validate that, at least in the context of boosting, the choice of link can have a significant affect on the convergence and robustness of learning. 5.1. Parameterised Links In order to construct a parametric family of links we first choose some set of inverse link functions B = { 1 ,..., V ! D n for a common n and V . This collection will be called the basis set of link functions. We then take the convex hull of B to form a set of inverse link functions Y 1 = conv ( B ) . Each y 1 2 Y 1 is then identified with the unique a 2 A = D B such that  X  B b this construction to be valid, it it necessary to show that ev-ery such y 1 2 Y 1 is indeed an inverse link function, that is, it is invertible.
 The following proposition shows that it suffices to assume that all of the basis functions are strictly monotonic .A function f : V ! R n is monotone if for all distinct u , v ( f ( u ) f ( v )) 0 ( u v ) 0. Strict monotonicity holds when the inequality is strict.
 Proposition 8 Every function y 1 in the set Y 1 = conv ( B ) is invertible whenever each basis function in B is strictly monotone.
 This result is a consequence of: 1) strict monotonicity being preserved under convex combination; and 2) strict monotonicity implies invertibility. The first claim is es-tablished by considering strictly monotonic f and g and some a 2 [ 0 , 1 ] and noting that if h = a f +( 1 a ) g then ( h ( u ) h ( v )) 0 ( u v )= a ( f ( u ) f ( v )) 0 a )( g ( u ) g ( v )) 0 ( u v ) &gt; 0. A strictly monotone func-tion f that is not invertible is impossible since if we have ( f ( u ) f ( v )) 0 ( u v ) &gt; 0 for all u , v then a u 6 = f ( u )= f ( v ) would lead to a contradiction.
 Strictly monotone basis functions are easily obtained via canonical links for strictly proper losses. By definition, a canonical link satisfies  X  y = D  X  L for some Bayes risk function. Strict properness guarantees  X  L is strictly concave ( Vernet et al. , 2011 ) and Kachurovskii X  X  theorem ( Showal-monotonic if and only if the function is (strictly) con-vex. Since ( f ( f 1 ( u )) f ( f 1 ( v ))) 0 ( f 1 ( u ) ( u v ) 0 ( f 1 ( u ) f 1 ( v )) we see that strictly monotone functions have strictly monotone inverses and we have es-tablished the following proposition.
 Proposition 9 If l is a strictly proper loss then its canon-ical link  X  y l = D  X  L has a strictly monotone inverse. This result means that a set of basis links can be defined via a choice of strictly concave Bayes risk functions. As an example, the class of Fisher-consistent margin losses proposed by Zou et al. ( 2008 ) provides a flexible start-ing point for designing sets of link functions as described above. They give explicit formulae for the inverse link for a composite loss defined by a choice of convex function f : R ! R . Specifically, if the loss for predicting v 2 V = { verse link is y 1 f ( v )= 1 normalises the vector to lie in D n . Each choice of strictly convex f gives a valid inverse link which can be used as a basis function. 5.2. Experiments In order to test the impact the choice of link has on the convergence rate we ran a simple experiment using a basic multiclass boosting algorithm, much like the L K -TreeBoost method ( Friedman , 2001 ) for trees with two ter-minal nodes. In this experiment l was fixed to be the log loss ( i.e. , l i ( p )= log p i ), and two basis links y sq correspond to choosing in the preceeding subsection tively. Inverse link functions y 1 a = ay 1 exp +( 1 a ) y 1 composite losses ` a = l y 1 a . For each loss, boosting was performed on data generated by i.i.d. sampling from three 2-dimensional Gaussians at (0,0), (2,2), and (-2,2) with identity covariance. 4,800 training and 1,200 test sam-ples were used with equal class proportions in both sets. The results shown in Figure 2 clearly indicate the impor-tance of careful link selection. Composite multiclass losses are a natural family of losses for multiclass probability estimation and classification which provide a seperation of concerns between the statisit-ical performance ( l ) and the parametrisation ( y ). We have shown that the requirement that the loss decompose into a proper loss and an inverse link gives enough structure to obtain simple expressions for the gradient and derivative of these losses, especially in the binary case or under the ad-ditional assumption that the link be canonical for the loss. We used these results to provide sufficient conditions for the convexity and condition numbers for composite losses and a general scheme for designing families of multiclass losses with the same Bayes risk. Preliminary experiments show that there are trade-offs inherent when designing mul-ticlass losses that require further investigation. This work was supported by the Australian Research Coun-cil (ARC). NICTA is funded by the Australian Government as represented by the Department of Broadband, Commu-nications and the Digital Economy and the ARC through the ICT Centre of Excellence program. Peng Sun was a visitor at ANU and NICTA while working on this paper.
