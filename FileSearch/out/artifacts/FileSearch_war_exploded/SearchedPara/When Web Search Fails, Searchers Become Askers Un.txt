 While Web search has become increasingly effective over the last decade, for many users X  needs the required answers may be spread across many documents, or may not exist on the Web at all. Yet, many of these needs could be addressed by asking people via popular Community Question Answer-ing (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. In this paper, we perform the first large-scale anal-ysis of how searchers become askers. For this, we study the logs of a major web search engine to trace the transformation of a large number of failed searches into questions posted on a popular CQA site. Specifically, we analyze the character-istics of the queries, and of the patterns of search behavior that precede posting a question; the relationship between the content of the attempted queries and of the posted questions; and the subsequent actions the user performs on the CQA site. Our work develops novel insights into searcher intent and behavior that lead to asking questions to the commu-nity, providing a foundation for more effective integration of automated web search and social information seeking. H.3.3 [ Information Systems ]: Information Storage and Retrieval query analysis, community question answering
While Web search engines have significantly progressed in effectiveness and efficiency over the last decade, there still exist certain user needs that cannot be satisfied. This could be due to a number of reasons, such as the difficulty of ex-pressing a complex need as a short search query, the lack of existing relevant content on the Web (e.g., for unique or  X  X ail X  needs that keep appearing), and for more  X  X ocial X  needs, for which the user prefers to interact with a real hu-man.

In fact, Hitwise in August 2011 reported that only 66-80% of the searches are successful 1 , and Hassan et al. [15] obtained a similar success rate of search goals (73%) via hu-man labeling. We argue that many such unsatisfied searches could be addressed by asking people via Community Ques-tion Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. It already happens in practice. For example, we have observed that about 2% of web search sessions performed by users who are also members of the Yahoo! Answers community, lead to a question posted to the community. Consider Figure 1a, which depicts a sample search submitted to a major search engine. The searcher is not satisfied with the results, and eventually posts a related question on the Yahoo! Answers site, which is then answered to the searcher X  X  satisfaction. Understanding and improving the synergy between searching and community question an-swering is at the heart of this work.

Specifically, our goal is to better understand the behav-ior of these users, as well as characterize the types of Web searches that could be effectively handled by CQA sites. Insights acquired during such analysis should bring multi-ple benefits to both search engines and CQA systems. On one hand, search engines always need to better understand when searchers are unsatisfied by the returned results. More specifically, Web search engines would find value in analyz-ing the search session patterns of such unsuccessful queries, the associated underlying query intents, and possibly reflect these findings in search effectiveness metrics. One can even imagine new search experiences that would allow users to turn to the community for certain types of needs better ad-dressed by people than by traditional Web search. Addi-tionally, CQA systems could potentially improve the asking experience by taking advantage of the context provided by unsuccessful queries preceding a posted question. One can imagine several ways to leverage this context, such as au-tomatically giving examples of irrelevant answers to clarify the question to the community.

To the best of our knowledge, our work is the first to per-form a large-scale study of the transformation of searchers into askers. That is, we start our analysis with web search sessions, trace the searcher through her visit to a CQA site, and analyze the resulting questions posted for the commu-nity.
 We focus on one of the most visited, and more mature, CQA systems existing today, namely Yahoo! Answers, which www.hitwise.com/us/about-us/ press-center/press-releases/ experian-hitwise-reports-google-share-of-searche/ with a satisfactory answer from the community (b). with more than 1 billion posted answers 2 is highly visible in most search engines result pages. We built a corpus of query-to-question transitions and studied it in order to un-derstand when and why searchers become askers. A privacy-preserving subset of the data has recently been made pub-licly available through Yahoo X  X  Webscope program 3 .
More specifically, our study is organized around the fol-lowing three research questions, each associated with a set of hypotheses: Research Question 1: When do searchers turn to CQA for answers? Research Question 2: How do search queries relate to the associated questions posted on CQA sites? http://yanswersblog.com/index.php/archives/2010/ 05/03/1-billion-answers-served/ http://webscope.sandbox.yahoo.com/ Research Question 3: How do searchers behave after transferring to the CQA site?
The rest of this paper is dedicated to answering the above questions and verifying the associated hypotheses.
In order to understand how searchers become askers, we collected a dataset that contains both the search session part and asking session part of each user who conducted a search session that resulted in posting a question. Our dataset is derived from joining a sample of the query logs of the Yahoo! search engine and the Yahoo! Answers question logs, both for June 2011.

To create this dataset, we first created a mapping of users between the two logs, based on the clicks on the same Yahoo! Answers question page following the same query on the same time frame, as they appear in both logs. Then, we extracted user actions from the query and question logs, e.g. posting queries and clicking on results from query logs, as well as posting questions and re-viewing them from the question logs. We constructed search sessions from these extracted actions, with a 30 minutes timeout as a session boundary. Question sessions have no temporal boundary, since every action in the session unambiguously refers to the question posted by the asker.
Once we had search sessions and question sessions, and mapping between some of them, we created two datasets. The first, termed SearchAsk dataset, contains search ses-sions that turned into question sessions. We only kept such sessions that resulted in posting one and only one question for simplicity of analysis later on. In addition, we only kept sessions in which the posted question is  X  X elevant X  to a pre-viously issued query (if the query and the question share at least one non-stopword, they were considered relevant). By observing that some users actually searched for Yahoo! Answers to navigate to its home page before they posted a question there, we deleted such special navigational clicks and corresponding queries from the user action sequences. The second dataset, termed SearchOnly dataset, consists of search sessions that did not turn into question sessions. In both datasets, we only kept sessions for users that posted at least once in Yahoo! Answers, since these users are aware of the site and know how to post a question there, thus remov-ing the potential investment of effort for newcomers to join the site, and filtering our the users that simply do not know where to ask questions.
 Table 1 reports the statistics of the datasets we obtained. As shown in the table, 95.8% of all the search sessions are SearchOnly sessions, while SearchAsk sessions account for 1.65%. Despite the sparsity of the SearchAsk sessions, we still believe that understanding how searchers become askers in such sessions can be helpful for improving the search ex-perience of these users and perhaps more users. Indeed, the two datasets allow us to investigate the differences be-tween sessions in which users posted a question following attempted searches, mainly due to search failure or searcher frustration, and sessions in which users that have experience of asking questions on Yahoo! Answers did not bother or did not need to ask questions at that time. Recall, that the users in these datasets satisfy two conditions: (1) having clicked at least a Yahoo! Answers question page within this month; and (2) having asked at least one question on Yahoo! An-swers within the month. Yet, we believe that such users still represent the general, though somewhat experienced, web searchers.
As a first step, we study the characteristics of queries lead-ing to a question post on Yahoo! Answers (Section 3.1), and the characteristics of searcher behavior before question ask-ing (Section 3.2).
The first interesting question is which queries are more likely to be unsuccessful for automated search, but instead are more amenable to be answered by a CQA site. To get such queries, we examine each SearchAsk session, and SearchAsk queries 6.5 2.4 28% 5.1
SearchOnly queries 3.4 0.72 11% 6.0 extract the queries that are issued before the question is posted, and are relevant to the question. We call such queries SearchAsk queries . For comparison, we also ex-tract the queries in each SearchOnly session which are called SearchOnly queries . In the following, we explore how SearchAsk queries are different from SearchOnly queries in terms of length, words, frequency, and results.

Figure 2 compares the distribution of query length (in terms of number of words in the query) for the SearchAsk and SearchOnly queries. We can see that SearchAsk queries tend to be longer than SearchOnly queries, as 85% of the SearchOnly queries contain at most 5 words, while about 50% of the SearchAsk queries contain more than 5 words. Therefore, searchers issuing longer queries are more likely to turn to Yahoo! Answers to post a relevant question.
Table 2 compares the average word length per query and the average number of stopwords for the SearchAsk queries and SearchOnly queries. We can see that, on average, queries turning to questions tend to contain more words (but shorter words) than queries that do not turn to questions. The main reason could be that SearchAsk queries contain more stop-words (which are often short) than SearchOnly queries. In-deed, the percentage of stopwords in SearchAsk queries is over 2.5 times higher than in SearchOnly queries.
To better understand the difference between the content of SearchAsk queries and SearchOnly queries, we compare their word distributions and show the main difference in Ta-ble 3. We can see that SearchOnly queries are more likely to be navigational, e.g., to reach websites like Facebook or YouTube, or to find information related to the searcher X  X  common tasks such as looking up the weather, hunting for coupons, or finding a cooking recipe. In contrast, SearchAsk quries are more likely to start with question words (e.g.,  X  X ow X ,  X  X hat X ), and tend to use more verbose natural lan-guage to express the needs of the searchers (e.g.,  X  X ant X ,  X  X o X ,  X  X now X ) rather than using only keywords.

To verify the hypothesis from the above word distribu-tion analysis that SearchAsk queries are more likely to be unique, we compute the frequency 4 of SearchAsk queries and SearchOnly queries in our 1-month query log. Figure 3 shows the results. We can see that over 90% of SearchAsk queries are tail (actually unique) queries, indicating the va-riety of the needs of searchers and the ways to express them. In contrast, SearchOnly queries contain more popular queries, e.g., around 20% of SearchOnly queries occur in more than 100 search sessions.

To better understand user needs behind SearchAsk queries, we further examine the results returned in their search en-gine result pages (SERPs). We found a significant differ-ence between SearchAsk and SearchOnly queries based on whether a SERP contains a Yahoo! Answers question page. As shown in Figure 4, a Yahoo! Answers question page oc-curs in the SERPs for half of the queries that eventually turn to questions, but for only 13% of SearchOnly queries. It is clear that SearchAsk queries are more likely to have a Yahoo! Answers question page in the SERP. This is not surprising. First, having a Yahoo! Answers question page in search results indicates that the query could be relevant to an existing Yahoo! Answers question. Therefore, answers from a human might be more suitable to address the need behind the query, encouraging the searcher to post a ques-tion on Yahoo! Answers. Second, more impressions often leads to more clicks. After landing on the Yahoo! Answers
The frequency of a query in this paper is computed as the number of search sessions containing the query.
 site, the searcher might realize that a community might be able to answer her information need, and try posting a ques-tion.

As a summary of the above analysis, we conclude that queries that are more likely to fail in search and lead to a question post on Yahoo! Answers tend to be longer, and use more verbose natural language to express the searchers X  needs. The needs behind such queries tend to be more unique and complex than those associated with SearchOnly queries.
To understand how searchers become askers, we analyze the searcher behavior in search sessions, with an associated question posted by the same user on Yahoo! Answers.
First, we examine what searchers do right before they start question asking, i.e., we examine the last user action prior to a question being posted. We found that the last search action before question asking is a click on Yahoo! Answers question result in 47.8% of the sessions, a click on other result in 31.2% of the sessions, and a query in 17.4% of the sessions. We notice that in about half of the ses-sions, the searcher posts a question right after clicking on a Yahoo! Answers question page from the search engine re-sults. There may be several reasons for this. First, such a click indicates that the query is relevant to the clicked question, and therefore it probably carries an information need that would benefit from a human response. Second, when the clicked Yahoo! Answers question page cannot sat-isfy the search need, it encourages the user to post a new question on Yahoo! Answers. Of course, it is also possible that a searcher had already decided to post a question when seeing the original SERP, and she then clicked on a Yahoo! Answers question result simply to navigate to the Yahoo! Answers site.
 To better understand the effects of clicking on a Yahoo! Answers question result on the transformation of searchers into askers, we compute and compare the likelihood of such clicks in SearchAsk and SearchOnly sessions. Figure 5 shows the results. First, 21% of SearchOnly sessions and 81% of SearchAsk sessions contain a Yahoo! Answers question page in the SERPs. Next, after seeing a Yahoo! Answers question page in the SERPs, 81% of the searchers who turned to askers had clicked on a Yahoo! Answers question result while 19% of them hadn X  X ; in contrast, 43% of the searchers in SearchOnly sessions seeing a Yahoo! Answers question result clicked on it while 57% of them didn X  X . Therefore, users in SearchAsk sessions are about twice as likely as in SearchOnly sessions to click on a Yahoo! Answers question page in the search results once seeing it. This indicates that searchers are more likely to post a question once clicking on a Yahoo! Answers question result.

To better understand searcher actions, we further com-pute the probability of transitions between actions in SearchAsk and SearchOnly sessions respectively, and compare them in Figure 6. The transition probability between two ac-tions a i and a j in SearchAsk (SearchOnly) sessions is com-puted using Maximum Likelihood estimation: P ( a i , a N action a i to action a j in all SearchAsk (SearchOnly) sessions, and N a i = P a are shown in red before the slash symbol, while SearchOnly transition probabilities are shown in black after the slash symbol. If we look at the transitions for SearchOnly sessions from the figure, we can see that after issuing a query, the searcher is very likely to click on other result, then with per-haps more queries and clicks on other result, and then ends the session. Clicking on a Yahoo! Answers question result is very unlikely. However, in SearchAsk sessions, the searcher has a higher probability on clicking a Yahoo! Answers ques-tion result. After the click, the searcher in SearchAsk ses-sions would post a question on Yahoo! Answers for around half of the time.
 Figure 6: Transition probabilities for actions in SearchAsk (in red, before the slash symbol) and SearchOnly (in black, after the slash symbol) ses-sions. Note that two other actions (Pagination and Click interface) are ignored for simplicity.

To better understand how searchers become askers, we examine the user action sequences in SearchAsk sessions be-fore the question post, and compare them with action se-quences in SearchOnly sessions. Table 4 shows a sample of top frequent user action sequences. The top frequent path in SearchOnly sessions indicates navigational needs of the searchers, i.e., they issue a query, click on a search result and leave the session. Such navigational cases account for 30% of total SearchOnly sessions. In contrast, the top frequent path in SearchAsk sessions indicates more  X  X ocial X  needs of the searchers, i.e., they issue a query, click on a search result of Yahoo! Answers question page, and then ask a question on Yahoo! Answers. Yet, the path distribution is more bal-anced for SearchAsk sessions. Moreover, clicks on Yahoo! Answers question results are common in the paths.
 Finally, we compare the distribution of session sizes for SearchAsk and SearchOnly sessions. Session size can be measured in several ways, e.g., by the number of (unique) queries issued by the searcher in the session, by the num-ber of actions performed in the session, or by the duration that the session lasts. We use the first option in this paper. The results are shown in Figure 7. While only one query Table 4: Top frequent user action sequences in SearchAsk sessions and SearchOnly sessions (B: Be-gin a session, Q: Query, C qr : Click on a Yahoo! An-swers question result, C or : Click on other result, A: Ask a question, E: End a session) is issued in the half of SearchOnly sessions, at least three different queries are issued in the half of SearchAsk sessions. The average session size is 2.5 for SearchOnly sessions and 3.8 for SearchAsk sessions. This shows that searchers tend to issue more queries in SearchAsk sessions, possibly be-cause SearchOnly sessions contain more navigational needs, while SearchAsk sessions are associated with more difficult or complex needs, and thus require more effort in finding answers.
After discovering the unique attributes of queries that lead to asking a question, we next want to understand better the process of turning a search session, as captured by a query, into a question posted on Yahoo! Answers.

The most expected difference between queries and ques-tions is their length. Table 5 shows these differences. From the table we can see that a question has 66 more words than its associated query on average. This indicates two things: first, as expected, questions are much more verbose, being natural language expressions, compared to the con-cise queries; second, since Yahoo! Answers questions are not Table 5: Statistics of length difference between a query and its associated question (number of words). Table 6: Overlap of content words (CW) between a query and its associated question.
Figure 8: Word distributions over question words limited in length, additional knowledge of the problem to be solved is added. Interestingly, the subject of the question is very close in length to the query, which shows that searchers still think in search-style writing for the subject. However, the content part of the question is significantly longer, and much more information is added in this question part.
We next look at word distribution differences, since they may point at the lexical gap between queries leading to ques-tions and their associated posted questions. Figure 8 de-picts the word occurrence distribution over word ranking by frequency for search-related questions. The most no-table difference between the two distributions is that ques-tions tend to be more personal and verbose, as captured by the abundant usage of the pronouns such as  X  I  X ,  X  me  X ,  X  it  X  and  X  this  X , connectives such as  X  but  X ,  X  because  X ,  X  recently  X , and  X  just  X , as well as sentiment indicators such as  X  help  X ,  X  please  X , and  X  thanks  X . Queries, on the other hand, tend to focus more on the things or actions that are searched for, with content question words like  X  how  X ,  X  why  X  and  X  what  X  occurring more frequently than in the associated questions corpus. Inter-estingly, one to four digit figures, such as car model years, also appear more in question-related queries than in their associated questions, probably since they capture much of the essence of the target information need.

To further understand the semantic difference between composing a query and its related question, we measured the distribution of query-question pairs in which the same words are used for both query and question, the pairs in which one is included in the other, and those pairs in which each contains words that do not occur in the other. Table 6 presents these statistics, while Table 7 provides examples of such pairs, annotated with the type of context added when switching from query to question, as been classified by [23], i.e., task, situation, attribute, limit, and thought.
Some interesting question composition patterns are evi-dent from this analysis. First, in the majority of pairs (66%), both queries and questions contain unique words that do not occur in the other. This is somewhat surprising, since we would expect more complete inclusion of the query terms in the question. However, it seems that with the freedom of writing a free text question, searchers tend to rephrase some of the terms they used in their queries. For example, abbre-viations and short terms are turned into their more complete forms, e.g.  X  AZ  X  into  X  Arizona  X  and  X  newborn  X  into  X  new baby  X  (see example 7 and 8 in Table 7). In addition, while 31% of the pairs do show complete inclusion of the query terms in the question, many times the query terms do not all ap-pear in the question X  X  subject or content, but spread in both question parts. Table 7 shows that most of the extensions of the query into a question include additional details that are related to the search task. Yet, many times details of the personal situation are added, such as the state of mind, e.g.  X  after watching a special on TV  X  (example 6 in Table 7).
One interesting future research is to automatically gener-ate questions from queries [25][26]. However, adding context information to the question, such as the situation or limit is a difficult challenge. Still, expanding the query expression to an explicit question form may be possible for many cases, e.g. examples 1 and 3 in Table 7.
As our final analysis, we are interested in discovering unique activity patterns in Yahoo! Answers that searchers posting a question have, compared to typical asker behavior in Ya-hoo! Answers. Specifically, we first examine the differences in lexicon, that is whether different words are used when composing a question (Section 5.1). Then, we analyze the difference in asker behavior after posting such questions, in terms of  X  X raditional X  CQA activities (Section 5.2).
As expected, we find that there is a large difference be-tween the word distribution for the corpus of all questions posted in June 2011 and the distribution of the corpus of Table 8: Categories with largest differences in assignment probability between questions coming from search and general questions questions posted by searchers. In addition, the entropy of generating a word from the search-related question corpus is much lower, showing a more focused vocabulary. But what are the reasons for this large difference? It turned out to be mainly topical.

To measure this topical difference between the two types of questions, we looked at the distribution of categories to which the questions in the two compared corpora are as-signed. Table 8 shows the categories with largest differences in assignment probability, those that are preferred more in the general question corpus and in the search-related ques-tion corpus respectively. These lists show that searchers tend to ask informational questions [14] to get fact-or advice-oriented answers, such as how to fix the car or maintain one X  X  garden, how to bake cookies, but also questions related to Yahoo products, such as Yahoo! Mail. On the other hand, regular askers are more likely to ask conversational questions [14] with a social flavor, such as discussions around music or sports events, politics and religions, and opinions on possible baby names. We manually labeled 100 questions randomly sampled from the search-related question corpus, and found none are conversational, showing a very different distribu-tion compared to that 38% of Yahoo! Answers questions are conversational as reported in [14]. We conjecture that this is because searchers usually turn to search engines to find in-formation instead of starting conversations. Another kind of questions that are less likely searched first over the web are personal questions, in which the asker is interested in adding very personal details. These include topics such as diet and fitness advices, dating and style opinions. Finally, there are questions that are too complex, for which the asker knows the answer cannot be found on the web. A good example are Math questions, such as example 4 in Table 7.

To further investigate the differences between the two question types, we removed the strong bias caused by the different category distributions within the two corpora by sampling questions from the general question corpus based on the category distribution of the search-related question corpus. By comparing the word distribution between the sampled corpus and the search-related question corpus, we found that hardly no topical differences remained. That is, the topical variation in the two corpora is more or less com-pletely captured by the level of assigned categories, without Table 9: Statistics of words in SearchAsk questions and sampled general questions # words 78.3 73.7 # words per sentence 13.5 13.7 # sentences 5.8 5.4 % stopwords 66.3 65.0 word length 4.22 4.17 Table 10: Statistics about user follow-up activities around their posted questions.
 more subtle topical differences evident. Still, there may be stylish variations in question composition between searchers and typical askers. Table 9 provides the stylish statistics for the general-sampled and search-related question corpora. The significant difference between the two corpora is the number of words per question: for the same topics, gen-eral questions contain 6% more words compared to search-related questions. Yet, interestingly, this attribute is due to more sentences that are written on average per general question, while if we look at the number of words per sen-tence, we see that surprisingly search-related questions have slightly more words in each sentence. This could be related to more information-focused nature of the questions posted after a search session, and suggests further investigation.
As our final question behavior analysis, we wanted to test whether a searcher interacts more or less with Yahoo! An-swers after posting the question. To that end, we measured both the number of actions that both searchers and regular askers perform around a specific question they posted, as well as the duration of this set of actions. Follow-up actions after a posted question include: browsing the question page ( e.g. checking for new answers), adding more details to the question, selecting a best answer, reporting abusive answers, voting for answers, and deleting the question.

Table 10 provides the average statistics of these actions, while Figures 9 and 10 depict the distribution of number of actions and their duration for searchers and regular askers. From the table we can see that searchers perform fewer yet similar number of actions as typical askers do, but in a much shorter duration. As can be seen by Figure 9, in terms of number of actions, the difference of about one more action on average for regular askers is small though constant. For the duration of the interaction, regular askers spend about 7% more time on average around the question, but looking at the median, the difference is substantially larger, with half the searchers spending 2.2 hours or less while the typi-cal askers tend to spend about 68% more time, or 3.7 hours, at the median. As an interesting comparison, we also mea-sured the average time the searchers spent searching before asking questions, to show the substantial difference between
Figure 10: Duration of user actions in questions an interactive search session and an offline asking session, a difference that is clear to the searchers, since they are will-ing to spend several hours waiting for an answer to arrive, compared to a few minutes actively searching.

In summary, we showed that searchers are expecting a faster response time for their questions, which often aim to address practical problem solving tasks. On the other hand, general Yahoo! Answers askers are willing to put more effort in following up their questions. One possible reason for this behavior is that Yahoo! Answers site is often viewed by users from a more social perspective, as indicated by many users asking socially-focused (e.g., conversational) questions.
As we study the transformation of unsatisfied searches into questions posted on a popular CQA site, our work is related to the work on query log analysis, searcher behavior and satisfaction prediction, and CQA question analysis.
On the search side, significant research has been done on analysis of queries and searcher behavior based on query logs. For example, understanding query intent and user goals has attracted much research effort [22][6]. Difficult queries [9][8], long and tail queries [4][5], and question-like queries [21] have also received special research attention. Be-sides, searcher satisfaction and frustration [11][15][2][16] has also been actively studied, which utilized query log informa-tion for satisfaction prediction, such as relevance measures, as well as user behavior during the search session, including mouse clicks and time spent between user actions.

Donato et al. [10] identified the research missions that often associate with complex information needs and require collecting information from many pages. In our work we focused on studying the types of queries that arguably are difficult for a web search engine to satisfy, often require hu-man to answer [19][20][18], and thus could be better handled by CQA sites. Liu et al. [18] argued that some of these needs can be satisfied with existing answers from CQA archives by harnessing the unique structure of such archives for detect-ing web searcher satisfaction. Our work in this paper further observed that many searchers not satisfied with search re-sults finally posted a related question on a CQA site, which inspired our analysis of how searchers become askers.
White and Dumais [24][12] studied search engine switch-ing behavior and developed models to predict the switching and its rationale. Although different types of searchers are focused on (they focused on searchers who turn to another search engine and issue more queries, while we focused on searchers who turn to CQA sites and post questions), we are both interested in characterizing the types of queries and searcher behavior that lead to the switchings. Our analysis shows both similar (e.g. longer sessions are more likely to involve a switch) and different characteristics (e.g. different last action before switching) compared to their study.
On the CQA side, there is also research effort devoted to question analysis, e.g. distinguishing conversational and in-formational questions [14], identifying high quality questions [3], and investigating the effects of contexts in questions on answer quality [23]. In our work, we use their classification of contextual factors to analyze the semantic difference be-tween the query and question posted by the same user for the same need. There is also some previous work related to asker behavior analysis. For example, Adamic et al. [1] analyzed the content properties and user interaction pat-terns across different Yahoo! Answers categories. Gyongyi et al. [13] studied several aspects of user behavior in Yahoo! Answers, including users X  activity levels, interests, and rep-utation. Yet, they did not study the effort that askers spend in tracing their posted questions as we studied in this paper.
Web search needs are becoming increasingly sophisticated, and the expectations have grown accordingly. As a result, quite a few search sessions end in posting a question in a Community Question Answering service, as the searcher re-alizes that such a service could better answer her need.
This work studies the unique properties of SearchAsk ses-sions: search sessions that turn into question composition. To the best of our knowledge, this paper presents the first large-scale analysis of the user transition from searching to asking. What makes our work unique is the study of the explicit connection between the search query and the cor-responding question from the same user for the same need . It provides insights into some specific needs that searchers try to express on search engines, yet are not satisfied by search results, and turn to human answerers instead. We analyzed the various aspects of SearchAsk sessions, includ-ing the differences between general search-engine queries and those belonging to a SearchAsk session, the transformation of a query into a natural language question and the ques-tion composition patterns, as well as other asking behavior of searchers, compared to general askers in a CQA service.
Our findings may contribute both to search-engine opti-mization, as well as to better user experience in CQA sites. For example, we found out that searchers are not as patient as regular askers when waiting for answers to their questions. This finding may influence CQA sites to promote questions coming from searchers, if they want to retain their engage-ment. As another example, our analysis of the transitions between user actions in SearchAsk sessions, and especially the fact that question asking is typically preceded by viewing a CQA page, may help search engines. They might decide to detect such cases and explicitly promote the option of ask-ing a question to the searcher, even before she resorts into doing it on her own. Furthermore, as this paper demon-strates, modeling the transformation of a query meant for an automated search engine into a fully specified question meant for human, provides a valuable tool for query intent and satisfaction analysis.

In future work, we intend to develop some of the direc-tions mentioned above. One of the most intriguing ones in our view is for search engines to automatically trigger a dia-log for posting questions in the right CQA forum, whenever a SearchAsk need is detected. This is just one application made possible by our study, which lays a foundation for more effective integration of automated web search and social in-formation seeking.
This work was supported by the National Science Foun-dation grant IIS-1018321 and by the Yahoo! Faculty Re-search and Engagement Program. The authors also would like to thank Elad Yom-Tov for the helpful discussion and the anonymous reviewers for their valuable comments. [1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S.
 [2] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find [3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [4] P. Bailey, R. W. White, H. Liu, and G. Kumaran. [5] M. Bendersky and W. B. Croft. Analysis of long [6] A. Broder. A taxonomy of web search. SIGIR Forum , [7] G. Buscher, R. W. White, S. Dumais, and J. Huang. [8] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. [9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.
 [10] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you [11] H. A. Feild, J. Allan, and R. Jones. Predicting [12] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and [13] Z. Gyongyi, G. Koutrika, J. Pedersen, and [14] F. M. Harper, D. Moy, and J. A. Konstan. Facts or [15] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: [16] S. B. Huffman and M. Hochster. How well does result [17] R. Jones and K. L. Klinkner. Beyond the session [18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, [19] M. R. Morris, J. Teevan, and K. Panovich. A [20] M. R. Morris, J. Teevan, and K. Panovich. What do [21] B. Pang and R. Kumar. Search in the lost sense of [22] D. E. Rose and D. Levinson. Understanding user goals [23] S. Suzuki, S. Nakayama, and H. Joho. Formulating [24] R. W. White and S. T. Dumais. Characterizing and [25] S. Zhao, H. Wang, C. Li, T. Liu, and Y. Guan. [26] Z. Zheng, X. Si, E. Y. Chang, and X. Zhu. K2q:
