 Semi-sup ervise d support vector machine ( S3VM ) attempts to learn a decision boundary that traverses through low data densit y regions by maximizing the margin over labeled and unlab eled examples. Traditionally , S3VM is form ulated as a non-c onvex integer programming problem and is thus di X -cult to solve. In this paper, we prop ose the cutting plane semi-sup ervise d support vector machine ( CutS3VM ) algo-rithm, to solve the S3VM problem. Speci X cally , we con-struct a nested sequence of successiv ely tighter relaxations of the original S3VM problem, and each optimization problem in this sequence could be e X cien tly solved using the con-strained concave-c onvex procedure ( CCCP ). Moreo ver, we prove theoretically that the CutS3VM algorithm takes time O ( sn ) to converge with guaran teed accuracy , where n is the total number of samples in the dataset and s is the aver-age number of non-zero features, i.e. the sparsit y. Experi-mental evaluations on several real world datasets show that CutS3VM performs better than existing S3VM metho ds, both in e X ciency and accuracy .
 I.2.6 [ Arti X cial Intelligence ]: Learning Algorithms, Performance Semi-Sup ervised Supp ort Vector Machine, Cutting Plane, Constrained Conca ve Convex Procedure
In many practical applications of pattern classi X cation and data mining, one often faces a lack of su X cien t labeled data, since labeling often requires expensiv e human labor and much time. However, in many cases, large number of unlab eled data can be far easier to obtain. For exam-ple, in text classi X cation, one may have an easy access to a large database of documen ts ( e.g. by crawling the web), but only a small part of them are classi X ed by hand. Con-sequen tly, Semi-Sup ervise d Learning (SSL) metho ds, which aim to learn from partially labeled data, are prop osed[5, 23].
Motiv ated by the success of large margin metho ds in su-pervised learning, [19] prop osed a semi-sup ervised exten-sion of Supp ort Vector Machine (SVM) , which treats the unkno wn data labels as additional optimization variables in the standard SVM problem. Speci X cally , by maximizing the margin over labeled and unlab eled data, the metho d targets to learn a decision boundary that traverses through low data densit y regions while respecting labels in the input space [7]. The idea was  X rst prop osed in [19] under the name Trans-ductive SVM , but since it learns an inductiv e rule de X ned over the entire space, we refer to this approac h as Semi-Supervise d SVM (S3VM) in this paper.

Several attempts have been made to solve the non-con vex optimization problem associated with S3VM , e.g., local com-binatorial searc h [11], branc h-and-b ound algorithms [2, 6], gradien t descen t [7], semi-de X nite programming [5, 20, 21, 22], continuation techniques [4], non-di X eren tiable metho ds [1], conca ve-con vex procedure [10, 9], and deterministic an-nealing [17]. However, the time complexit y of these meth-ods scales at least quadratically with the dataset size, which renders them quite time consuming on large scale real world applications. Therefore, how to e X cien tly solve the S3VM problem to make it capable of handling large scale datasets is a very challenging researc h topic.
 Joachims prop oses in [12] a fast training metho d for linear SVM based on cutting plane algorithm [13]. Although the algorithm in [12] greatly reduces the training time for linear SVM , it could only handle the supervised training case. Un-like supervised large margin metho ds which are usually for-mulated as convex optimization problems, semi-sup ervise d SVM involves a non-c onvex integer optimization problem, which is much more di X cult to solve. In this paper, we apply the cutting plane algorithm to semi-sup ervised SVM train-ing, and prop ose the cutting plane semi-sup ervise d support vector machine algorithm CutS3VM . Speci X cally , we con-struct a nested sequence of successiv ely tighter relaxations [12] of the original S3VM problem, and each optimization problem in this sequence could be e X cien tly solved using the constr ained concave-c onvex procedure ( CCCP ). More-over, we prove theoretically that the CutS3VM algorithm takes time O ( sn ) to converge with guaran teed accuracy , where n is the total number of samples in the dataset and s is the average number of non-zero features, i.e. the spar-sity. Our experimen tal evaluations on several real world datasets show that CutS3VM converges much faster than existing S3VM metho ds with guaran teed accuracy , and can thus handle larger datasets e X cien tly.

The rest of this paper is organized as follows. The cutting plane semi-sup ervise d SVM algorithm is presen ted in detail in section 2. In section 3, we provide theoretical analysis on the accuracy and time complexit y of CutS3VM . Experi-mental results on several real world datasets are provided in section 4, followed by the conclusions in section 5.
In this section, we  X rst introduce a slightly di X eren t for-mulation of the semi-sup ervise d support vector machine that will be used throughout this paper and show that it is equiv-alent to the conventional S3VM form ulation. Then we presen t the main procedure of the cutting plane semi-sup ervise d sup-port vector machine ( CutS3VM ) algorithm.
Given a point set X = f x 1 ;  X  X  X  ; x l ; x l +1 ;  X  X  X  ; x the  X rst l points in X are labeled as y i 2f X  1 ; +1 g 1 and the remaining u = n  X  l points are unlab eled. Conventionally , semi-sup ervise d support vector machine could be form ulated as the following optimization problem ture how C l and C u scale with the dataset size. Moreo ver,  X  (  X  ) maps the data samples in X into a high (possibly in- X nite) dimensional feature space, and by using the kernel trick, this mapping could be done implicitly . However, in those cases where the kernel trick cannot be applied, if we still want to use a nonlinear kernel, it is possible to com-pute the coordinates of each sample in the kernel PCA basis [16] according to the kernel K . More directly , as stated in [7], one can also compute the Cholesky decomp osition of the kernel matrix K = ^ X ^ X T , and set  X  ( x i ) = ( ^ X i; 1 Furthermore, in problem (1), the losses over labeled and un-labeled samples are weighted by two parameters, C l and C , which re X  X ct con X dence in labels and in the cluster as-sumption respectiv ely. In general, C l and C u need to be set at di X eren t values for optimal generalization performance [7]. The di X cult y with problem (1) lies in the fact that we have to minimize the objectiv e function w.r.t. the labels y l +1 ; : : : ; y n , in addition to w , b and  X  .

Finally , to avoid unbalanced solutions, [7] introduces the following class balance constrain t by enforcing that certain fraction r of the unlab eled data should be assigned to the
Datasets with more than two classes can be handled with the one-v ersus-rest approac h. positiv e class. However, as the true class ratio for the unlab eled data is unkno wn, r can be estimated from the class ratio on the labeled set, or from prior knowledge of the classi X cation problem. For a given r , an easy way to enforce the class balance constrain t (2) is to translate all the points such that P j = l +1  X  ( x i ) = 0. Then, by  X xing b = 2 r  X  1, we have an unconstrained optimization problem on w [7]. We assume that the  X  ( x i ) are translated and b is  X xed in this manner for the rest of this paper.
In this section, we will reform ulate problem (1) to reduce the number of variables. Speci X cally ,
Theorem 1. Problem (1) is equivalent to where the labels y j ; j = l + 1 ; : : : ; n are calculate d as y sign( w T  X  ( x j ) + b ) .
 For simplicit y, unless noted otherwise, the index i runs over above theorem is simple and we omit it due to lack of space. By reform ulating problem (1) as problem (3), the number of variables involved in the S3VM problem is reduced by u , but there are still n slack variables in problem (3). To further reduce the number of variables involved in the optimization problem, we have the following theorem
Theorem 2. Problem (3) can be equivalently formulate d as min s:t: and any solution w  X  to problem (4) is also a solution to problem (3) (vice versa), with  X   X  = C l n P l i =1  X   X  i
Proof. We will show that problem (3) and problem (4) have the same objectiv e value and an equiv alent set of con-strain ts. Speci X cally , we will prove that for every w , the and  X  in problem (4) are equal. This means, with w  X xed, and (4) respectiv ely, and they result in the same objectiv e function value.
For any given w , the  X  i and  X  j in problem (3) can be optimized individually and the optim um is achieved as Similarly for problem (4), the optimal  X  is Since each c i ; c j are indep enden t in Eq.(7), they can be op-timized individually . Therefore, Hence, for any w , the objectiv e functions for problem (3) and problem (4) have the same value given the optimal  X  i and  X  . Therefore, the optima of the two optimization prob-lems are the same.

Although problem (4) has 2 n constrain ts, one for each slack variable  X  that is shared across all constrain ts, thus, the number of variables is further reduced by n  X  1. Each constrain t in this form ulation corresp onds to the sum of a subset of constrain ts from problem (3), and the vector c selects the subset [12]. Putting theorem 1 and theorem 2 together, we could therefore solve problem (4) instead to  X nd the same maxim um margin classifying hyperplane.
On the other hand, the number of constrains is increased from n to 2 n . The algorithm we prop ose in this paper tar-gets to  X nd a small subset of constrain ts from the whole set of constrain ts in problem (4) that ensures a su X cien tly accurate solution. Speci X cally , we emplo y an adaptation of the cutting plane algorithm [13] to solve the S3VM training problem, where we construct a nested sequence of succes-sively tighter relaxations of problem (4) [12]. Moreo ver, we will prove theoretically in section 3 that we can always  X nd a polynomially sized subset of constrain ts, with which the solution of the relaxed problem ful X lls all constrain ts from problem (4) up to a precision of  X  . That is to say, the remain-ing exponen tial number of constrain ts are guaran teed to be violated by no more than  X  , without the need for explicitly adding them to the optimization problem [12]. Speci X cally , the CutS3VM algorithm keeps a subset  X  of working con-strain ts and computes the optimal solution to problem (4) subject to the constrain ts in  X . The algorithm then adds the most violated constrain t in problem (4) into  X . In this way, a successiv ely strengthening appro ximation of the original S3VM problem is constructed by a cutting plane that cuts o X  the curren t optimal solution from the feasible set [13]. The algorithm stops when no constrain t in (4) is violated by more than  X  . Here, the feasibilit y of a constrain t is measured by the corresp onding value of  X  , therefore, the most violated constrain t is the one that would result in the largest  X  . Since each constrain t in problem (4) is represen ted by a vector c , then we have
Theorem 3. The most violate d constr aint could be com-puted as follows
Proof. As stated above, the most violated constrain t is the one that would result in the largest  X  . In order to ful X ll all constrain ts in problem (4), the slack variable  X   X  could be calculated as follows Therefore, the most violated constrain t c that results in the largest  X   X  could be calculated as in Eq. (9) and (10).
The CutS3VM algorithm iterativ ely selects the most vi-olated constrain t under the curren t hyperplane parameter and adds it into the working constrain t set  X  until no vio-lation of constrain t is detected. Moreo ver, since in problem (4), there is a direct corresp ondence between  X  and the fea-all constrain ts up to precision  X  , i.e. then the point ( w ; b;  X  +  X  ) is feasible. Furthermore, as in the objectiv e function of problem (4), there is a single slack variable  X  that measures the training loss. Hence, we could simply select the stopping criterion as all samples satisfying the inequalit y (12). Then, the appro ximation accuracy  X  of this appro ximate solution is directly related to the training loss. Assume the curren t working constrain t set is  X , S3VM could be form ulated as the following optimization problem min s:t: 8 c 2  X : where b = 2 r  X  1. Before getting into details of solving problem (13), we  X rst presen t the outline of our CutS3VM algorithm in Algorithm 1.

Algorithm 1: Cutting Plane Semi-Sup ervised SVM 1. Initialization. Set the values for C l , C u , r and  X  , and 2. Solve optimization problem (13) under the curren t 3. Select the most violated constrain t c under the 4. If the selected constrain t is violated by no more than 5. Output. Return the labels for unlab eled samples as
In each iteration of CutS3VM , we need to solve problem (13) to obtain the optimal classifying hyperplane under the curren t working constrain t set  X . Although the objectiv e function in (13) is convex, the constrain ts are not, and this makes problem (13) di X cult to solve. Fortunately , the con-strained concave-c onvex procedure (CCCP) is designed to solve those optimization problems with a conca ve-con vex ob-ically , the objectiv e function in problem (13) is quadratic. Moreo ver, the constrain t as shown in Eq. (14) is, though non-con vex, a di X erence between two convex functions. 8 c 2  X : 1 Hence, we can solve problem (13) with the CCCP . Notice smooth function of w . To use the CCCP , we need to replace the gradien t by the subgradient [8]: Given an initial point w 0 , the CCCP computes w t +1 from w with its  X rst-order Taylor expansion at w t , i.e. By substituting the above  X rst-order Taylor expansion (16) into problem (13), we obtain the following quadr atic pro-gramming (QP) problem: min s:t: 8 c 2  X : and the above QP problem could be solved in polynomial time. Following the CCCP , the obtained solution w from this QP problem is then used as w t +1 and the iteration continues until convergence and Smola et al . [18] proved that the CCCP is guaran teed to converge.

We will show in the theoretical analysis section that the dual problem of (17) has desirable sparseness prop erties. For simplicit y, de X ne the following variables where k runs over 1 ; : : : ; j  X  j . The dual problem of (17) is max s:t: The above optimization problem is a QP problem with j  X  j variables, where j  X  j denotes the total number of constrain ts in the subset  X .

Note that in successiv e iterations of the CutS3VM algo-rithm, the optimization problem (13) di X ers only by a single constrain t. Therefore, we can emplo y the solution in last iteration of the CutS3VM algorithm as the initial point for the CCCP , which greatly reduces the runtime. Putting ev-erything together, according to the form ulation of the CCCP [18], we solve problem (13) with Algorithm 2, where we set the stopping criterion in CCCP as the di X erence between
Algorithm 2: Solve problem (13) using CCCP 1. Initialize ( w 0 ; b 0 ) with the output of the last 2. Find ( w t +1 ; b t +1 ) as the solution to the 3. If convergence criterion satis X ed, return ( w t ; b t ) two iterations less than  X  % and set  X  % = 0 : 01, which means the curren t objectiv e function is larger than 1  X   X  % of the objectiv e function in last iteration, since CCCP decreases the objectiv e function monotonically .
In this section, we will provide detailed theoretical analy-sis of the CutS3VM algorithm, including its correctness and time complexit y.

Speci X cally , the following theorem characterizes the accu-racy of the solution computed by CutS3VM .

Theorem 4. For any dataset X = ( x 1 ; : : : ; x n ) and any  X  &gt; 0 , the CutS3VM algorithm returns a point ( w ; b;  X  ) for which ( w ; b;  X  +  X  ) is feasible in problem (4).
Proof. In step 3 of our CutS3VM algorithm, the most violated constrain t c , which leads to the largest value of  X  , is selected using Eq.(10). According to the outline of the CutS3VM algorithm, it terminates only when the newly selected constrain t c satis X es the following inequalit y If the above relation holds, since the newly selected con-strain t is the most violated one, all other constrain ts will satisfy the above inequalit y relation. Therefore, if ( w ; b;  X  ) is the solution returned by our CutS3VM algorithm, then ( w ; b;  X  +  X  ) will be a feasible solution to problem (4).
Based on the above theorem,  X  indicates how close one wants to be to the error rate of the best classifying hyper-plane and can thus be used as the stopping criterion [12]. We next analyze the time complexit y of CutS3VM . For the high-dimensional (say, d-dimensional) sparse data commonly en-coun tered in applications like text mining, web log analysis and bioinformatics, we assume each data sample has only s  X  d non-zero features, i.e. , s implies the sparsit y, while for non-sparse data, by simply setting s = d , all our theo-rems still hold.

Theorem 5. Each iteration of CutS3VM takes time O ( sn ) for a constant working set size j  X  j .

Proof. In steps 3 and 4 of the CutS3VM algorithm, we need to compute n inner products between w and  X  ( x i ). Each inner product takes time O ( s ) when using sparse vec-tor algebra, and totally n inner products will be computed in O ( sn ) time. To solve the CCCP problem in step 2, we will need to solve a series of quadr atic programming (QP) problems. Setting up the dual problem (19) is dominated by computing the j  X  j 2 elemen ts ( z l k + z u k ) T ( z this can be done in time O ( j  X  j 2 sn ) after  X rst computing z volved in the QP problem (19) is j  X  j and (19) can be solved in polynomial time, the time required for solving the dual problem is then indep enden t of n and s . Therefore, each iteration in the CCCP takes time O ( j  X  j 2 sn ). Moreo ver, in numerical analyses, we observ ed in each round of the CutS3VM algorithm, less than 10 iterations is required for solving problem (13), even for large scale datasets. More-over, the number of iterations required is indep enden t of n and s . Therefore, the time complexit y for each iteration of our CutS3VM algorithm is O ( sn ), which scales linearly with n and s .
 De X ne ~ w = [ w ; b ] and ~  X  ( x i ) = [  X  ( x i ) ; 1], then ~ w w
T  X  ( x i ) + b . With b  X xed at 2 r  X  1, arg min ~ w 1 arg min w 1 2 w T w . Therefore, problem (4) could be equiv a-lently form ulated as min
Theorem 6. For any  X  &gt; 0 , C l &gt; 0 , C u &gt; 0 , l  X  0 , and any dataset X = f x 1 ; : : : ; x n g , the CutS3VM algorithm where R is a constant numb er indep endent of n and s . Proof. Note that w = 0,  X  = lC l + uC u n =  X C l + (1  X   X  ) C u is a feasible solution to problem (4), where  X  = l denotes the fraction of labeled samples in the whole dataset X . Therefore, the objectiv e function of the solution of (4) is upper bounded by  X C l +(1  X   X  ) C u . We will prove that in each iteration of the CutS3VM algorithm, by adding the most violated constrain t, the increase of the objectiv e function is at least a constan t number [12]. Due to the fact that the objectiv e function of the solution is non-negativ e and has upper bound  X C l +(1  X   X  ) C u , the total number of iterations will be upper bounded.

To compute the increase brough t up by adding one con-strain t into the working set  X , we will  X rst need to presen t the dual problem of (4). The di X cult y involved in obtain-ing this dual problem comes from the abstracts in the con-strain ts. Therefore, we  X rst need to replace the constrain ts in (4) with the following the Lagrangian dual function can be obtained as follows = inf = inf = 1 satisfying the following constrain ts 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; : where we de X ne L = I  X  2 P n j = l +1  X  j D j . The CutS3VM al-gorithm selects the most violated constrain t c 0 and continues if the following inequalit y holds 1 n 2 4 C l Since  X   X  0, the newly added constrain t c 0 satis X es 1 n 2 4 C l value of the Lagrangian dual function subject to  X  k +1 =  X  k [f c 0 g . The addition of a new constrain t to the primal problem is equiv alent to adding a new variable  X  k +1 into the dual problem.
 = max Substituting Eq.(26) into inequalit y (28) and according to the constrain t  X  k +1  X  0, we have 1 n  X  Substituting the above inequalit y into (29), we get the lower de X ne  X  ( k ) i as the value of  X  i that results in the largest L (  X ;  X ;  X ;  X  ). Since for semi-sup ervised scenario, l  X  u al-ways holds, the optimal value for  X  ( k ) i can be appro ximately obtained by solving the following optimization problem max By maximizing the above appro ximate Lagrangian dual func-tion ~ L (  X ;  X ;  X  ),  X  ( k ) could be obtained as follows =arg max =arg max subject to the following equation The only constrain t on  X  j is  X  j  X  0, therefore, to maximize P i =1 (  X  j  X   X  j ), the optimal value for  X  j is 0. Hence, Thus, n X  ( k ) j is a constan t number indep enden t of n . More-ments in the constrain t vector c 0 , and therefore is a constan t only related to the newly added constrain t, and prop ortional indep enden t of n . Moreo ver,  X  = C is a constan t number indep enden t of n , the matrix 1 2 number indep enden t of n and s , and we denote it with Q Moreo ver, we de X ne R = max k f Q k g as the maxim um of Q k through the whole CutS3VM process. Therefore, the increase of the objectiv e function of the Lagrangian dual problem after adding the most violated constrain t c 0 is at least  X  2 R . Furthermore, denote with G k the value of the ob-jectiv e function in problem (4) subject to  X  k after adding k constrain ts. Due to the weak dualit y [3], at the opti- X C l +(1  X   X  ) C u . Since the Lagrangian dual function is upper bounded by  X C l +(1  X   X  ) C u , the CutS3VM algorithm termi-
It is true that the number of constrain ts can potentially explo de for small values of  X  , however, experience with the CutS3VM algorithm shows that relativ ely large values of  X  are su X cien t without loss of accuracy . Note that the ob-jectiv e function of problem (4) with the scaled C l n and instead of C l and C u is essen tial for this theorem. Putting everything together, we arriv e at the following theorem re-garding the time complexit y of CutS3VM .

Theorem 7. For any dataset X = f x 1 ; : : : ; x n g with n samples and sparsity of s , and any  X xed value of C l &gt; 0 , C u &gt; 0 , l  X  0 and  X  &gt; 0 , CutS3VM takes time O ( sn ) .
Proof. Since theorem 6 bounds the number of iterations in our CutS3VM algorithm to a constan t (  X C l +(1  X   X  ) C which is indep enden t of n and s . Moreo ver, each iteration of the algorithm takes time O ( sn ). The CutS3VM algorithm has time complexit y O ( sn ).
In this section, we will validate the accuracy and e X ciency of CutS3VM on several real world datasets. Moreo ver, we will also analyze the scaling behavior of CutS3VM with the dataset size and its sensitivit y to  X  , C l and C u , both in accuracy and e X ciency . All the experimen ts are performed on a 1.66GHZ Intel Core TM 2 Duo PC running Windo ws XP with 1.5GB main memory .
We use 10 datasets in our experimen ts 2 , which are selected to cover a wide range of prop erties: g50c, uspst, text1 and coil20 from [7], Ionosphere and Sonar from the UCI repository , 20 newsgroup , WebKB , Cora [15] and RCVI [14]. For the 20 newsgroup dataset, we choose the topic rec which contains autos, motor cycles, baseball and hockey from the version 20-news-18828. For WebKB , we select a subset consists of about 6000 web pages from computer science de-partmen ts of four schools (Cornell, Texas, Washington, and Wisconsin). For Cora , we select a subset containing the re-searc h paper of sub X eld data structure (DS), hardw are and architecture (HA), machine learning (ML), operating system (OS) and programming language (PL). For RCVI , we use the data samples with the highest two topic codes (CCA T and GCA T) in the \Topic Codes" hierarc hy in the training set.

Besides our CutS3VM algorithm, we also implemen t some other comp etitiv e algorithms and presen t their results for comparison. Speci X cally , we use the conventional SVM al-gorithm as baseline, and also compare with four state-of-the-art metho ds: TSVM-Ligh t (TSVML) [11], Gradi-ent Descen t TSVM (LDS) [7], the Conca ve Convex Procedure (CCCP) [9] and Deterministic Annealing (DA) [17]. Moreo ver, we also report the 5-fold cross valida-tion results ( SVM-5cv in table 2) of an SVM trained on the whole dataset using the labels of the unlab eled points. Mul-ticlass datasets are learned with a one-v ersus-rest approac h. For each dataset, classi X cation accuracy averaged over 20 indep enden t trials is reported. In each trial, the training set contains at least one labeled point for each class, and the remaining data are used as the unlab eled (test) data. Moreo ver, for CutS3VM , linear kernel is used. For other
All datasets used in this paper could be found on http://binzhao02.go oglepages.com/ to determine how small  X  should be to guaran tee su X cien t accuracy . We presen t in  X gure 2 how classi X cation accuracy and computational time scale with  X  . According to  X gure 2,  X  = 0 : 1 is small enough to guaran tee classi X cation accuracy . The log-log plot in  X gure 2 veri X es that the CPU-time of CutS3VM decreases as  X  increases. Moreo ver, the empirical better than O ( 1  X  2 ) in theorem 6.
Besides  X  , C l and C u are also crucial in CutS3VM as they adjust the tradeo X  between the margin and the training loss. Therefore, we study their e X ects on the accuracy and speed of CutS3VM and presen t in  X gure 3 and  X gure 4 how classi- X cation accuracy and computational time scale with C l and C . Figures 3 and 4 show that the classi X cation accuracy Figure 3: Classi X cation accuracy and CPU-time (seconds) of CutS3VM vs. C l . Figure 4: Classi X cation accuracy and CPU-time (seconds) of CutS3VM vs. C u . increases if we increase C l or decrease C u . Moreo ver, the computational time scales roughly linearly with C l and C which coincides with our theoretical analysis in section 3.
We prop ose the cutting plane semi-sup ervise d SVM algo-rithm in this paper, to e X cien tly classify data samples with the maxim um margin hyperplane in the semi-sup ervised sce-nario. Detailed theoretical analysis of the algorithm is pro-vided, where we prove that the computational time of our metho d scales linearly with the sample size n and sparsit y s with guaran teed accuracy . Moreo ver, experimen tal eval-uations on several real world datasets show that CutS3VM performs better than existing S3VM metho ds, both in e X -ciency and accuracy . This work is supp orted by the National Natural Science Foundation of China, Gran t No. 60675009, and the National 863 project, Gran t No. 2006AA01Z121.
