 Relevance models provide an important approach for estimating probabilities of words in the relevant class. However, the associated bag-of -words assumption breaks dependencies between words, especially between those within a phrase. If such dependencies could be preserved, it would permit matching the que ry terms with document terms having the same dependencies. Additionally, during the estimation of relevance, relevance models are unable to distinguish relevant and non-relevant information in a feedback document, and hence take the entire document into account, which potentially hurts the accuracy of estimation. In this paper, we define the notion of  X  X oncept X , and design a concept-based information retrieval framework. Using this framework, we transform documents and queries from term space into concept space, and propose a concept-based relevance model for improved estimation of relevance. Our approach has three advantages. First, this approach only assumes independence between concepts, so is able to keep the strong dependencies between the words of a concept. Second, it unifies synonyms or different surface forms of a concept, leading to reduced dimensionality of the space, increased sample size of a concept, and consequently more accurate and reliable estimates of the relevance. Third , when knowledge bases are available, our approach enables the semantic analysis of query concepts, and thus identifies concepts related to the query, from which a more accurate distribution of relevance can be estimated. This work is aligned with semantic search methods. 
We apply our concept-based relevance model to information retrieval in the medical domain, where concepts are abundant and their variations are numerous. We compare with relevance models, BM25 with pseudo relevance feedback , and the state of the art conceptual language models, on several data collections. The proposed model demonstrates consistent and statistically significant improvements across collections, outperforming top benchmark conceptual language models by at least 9 % and up to 20 % on a number of metrics.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Information retrieval; concepts; term dependency; relevance; concept relevance models; semantic search; ontology; UMLS 
Relevance has always been fundamental to information retrieval. In the research community, however, there are many different views of relevance. Jones, Robertson, et al. argued that relevance should be explicitly recognized in any formal models [1], since it is the key notion in retrieval. The classical Probabilistic Model, which is derived from the Probabilistic Ranking Principle, does this straightforwardly [2]. The Vector Space Model assumes that some of the points in the multi-dimensional information space of a retrieval system represent relevant documents. However, the early Language Modeling (LM) approach works differently. Instead of modeling relevance directly, LM presumes that if it is highly probable that the document generates the query, then the content of the document is more likely to be relevant to the information need underlying the user X  X  request. Therefore, it uses as the ranking function the probability that a language model that generated the document would generate the query [3]. To answer the question  X  X here X  X  th e relevance? X  in LM, Laffety and Zhai [4] provided a theoretical justification for LM that the language modeling approach and the traditional probabilistic model can be interpreted within the same probabilistic framework based on a generative relevance model, but these two models are not equivalent from a statistical point of view, since the factorization and estimation are different. To bridge the gap between relevance and LM, Lavrenko and Croft [5] proposed the Relevance-based Language Models (or Relevance Models, RM), which seek to estimate a more generalized notion of relevance. This approach assumes that there exists an ideal, accurate model of relevance, and both the relevant documents and the query are random samples from it. This relevance model can be estimated from relevant documents, or in a pseudo relevance feedback fashion, if training documents are unavailable. RM proved very effective in the field. It achieved the best results on the TREC 2005 and 2006 Terabyte tracks, respectively [8, 12]. Recent research trends show that the relevance model formalism has established a foothold in the research in IR community, based on which new models and algorithms are continuously built and developed [6, 7, 8, 12]. 
Like most statistical retrieval models, RM assumes words are independent of each other in a document. This is a compromise due to the infeasibility of estimating the general term dependencies from sparse data. As we all know, for example, when we read the phrase  X  X nformation retrieval and knowled ge management X  in a CIKM proceeding, the association between  X  X nformation X  and  X  X etrieval X , or between  X  X nowledge X  and  X  X anagement X  is much stronger than that between  X  X etrieval X  and  X  X nd X , or  X  X nd X  and  X  X nowledge X . In this circumstance, if modeling all the dependencies is infeasible, at least one should attempt to keep the highly dependent words such as  X  X nformation retrieval X  or  X  X nowledge management X  together. Towards this end, we make our initial effort by proposing and modeling the notion of concept. We define a concept as a word or a multi-word phrase that represents a single but complete meaning in context. We argue that the dependencies within the words of a concept (if more than one), should be preserved; therefore, given a document, independence only exists between concepts; and it would be beneficial to conduct the estimation of model components in concept space, rather than in term space. In this paper, we will show how concept-based relevance can be estimated without the bag-of -words assumption, and how the relevance model can be developed to the Concept-based Relevance Model (CRM). There are three primary advantages of our model. 
First, CRM replaces the bag-of -words assumption of RM with the bag-of -concepts assumption, in order to retain term dependencies within a concept. Usually, term dependencies inside a concept are stronger than those outside of it. Preserving inside dependencies will provide retrieval systems extra information to better match query concepts and document concepts with the same dependencies. Following the above example, when users query  X  X nowledge management X  in a system, matching the entire concept could enable a retrieval system to rank the text  X  X nformation retrieval and knowledge management X  higher than  X  X omain knowledge in project management X . In order to present more relevant documents to users, such association among query terms should be considered. In the meanwhile, we assume independence between concepts, so the joint probability of concepts can be decomposed and estimated conveniently. 
Next, CRM provides an advanced approach to estimating relevance. It improves estimation from two perspectives. During the concept extraction, with the aid of information extraction and natural language processing (NLP) , the synonyms and other different surface forms of a concept (e.g.  X  X eart attack X ,  X  X yocardial infarction X ,  X  X I X ,  X  X nfarction of heart X , and  X  X ardiac infarction X ) will be identified and unified under the same concept. This consequently reduces the dimensionality of the concept space and increases the sample size of a concept to be estimated, essentially helping solve the synonym problem and the data sparsity issue in information retrieval. The estimation performed in such a denser space is more accurate and reliable. In fact, previous work has shown that increasing density helps improve the estimation. Diaz and Metzler [16] accomplished this by incorporating more data from large external corpora, while in this study we seek to find an alternative by better mining the data itself. In addition, some noisy text and non-relevant information, such as author X  X  affiliations , privacy notices, and advertisements, will be pruned during concept extraction, which makes the content transformed to concept space more relevant and consistent. 
Finally, if knowledge base is available, CRM allows semantic analysis to further improve relevance estimation. By modeling semantic types of query concepts, CRM is able to find concepts that are semantically related to query concepts in feedback documents. Estimating on these relevant concepts yields a more accurate distribution of relevance. No te this could be termed semantic search, since it is aligned with methods in that area. 
The remainder of the paper is structured as follows. In Section 2, we briefly survey the IR models considering term dependencies and using concepts. In Section 3, we construct and estimate the simple concept-based relevance models. Section 4 shows how we develop a full concept-based relevance models by semantic analysis and modeling concept semantic types. We elaborate the experimental setup and results in Section 5, discuss the reason for significant improvements in Section 6, and conclude in Section 7. 
Many recent studies in IR consider query terms associated with each other. Most of them have focused on the term proximity. N-gram models consider n word sequences to expand the radius of matching [15], but it is yet hard to determine the optimal n , and the complexity usually grows exponentially with the growth of n . Some work integrates word proximity or term location information into probabilistic weighting models in a heuristic manner [7, 18]. Other models have been proposed to capture the exact dependencies between more than two terms. Despite the more complex nature, they have been shown to produce little or no improvements in effectiveness. As mentioned in [23, 24], most previous term dependence models have failed to show robust, significant improvements over baseline bag-of -words models, with only a few exceptions such as the dependence language models [25] and the Markov random fields (MRF) model [24, 27]. The dependence language models require computing a link structure for each query, which is not straightforward. As a query expansion technique, the Latent Concept Expansion (LCE) derived from MRF shows significant improvements over relevance models on MAP across different data sets, but only small, insignificant improvements on precision at 5, 10, and 20 [27]. One problem of LCE is that it adds new words to a concept based on word statistics regardless of the semantics and syntax of the expanded concepts. This results in expanded concepts inconsistent with common language usage and unnatural to human understanding. For example,  X  X wo hubble space X  and  X  X elescope shuttle X  will be generated from the query  X  X ubble telescope achievements X . Learning fr om the above studies, we do not plan to model term dependencies directly in this paper. Instead, we intend to extract concepts that are naturally expressed in documents and queries, so that the strong term dependencies will be preserved in the concept. In fact, as the authors pointed out, the inability to improve performance by expanding two-word concepts using LCE indicates that two-word concepts already consist of two highly dependent terms [27], so it is difficult to find additional terms that would make the concept more effective to retrieval. This finding potentially supports our hypothesis of using the stronger dependencies inside of natural concepts and skipping the weaker dependencies outside. 
For concept-based IR, some empirical studies show that retrieval models using only concepts have inconsistent results, since not all documents and queries could be effectively represented using concepts. Different methods are investigated to combine relevance scores from both word and concept retrievals in [9, 10, 20]. Explicit Semantic Analysis (ESA) represents and analyzes text as vectors in a high-dimensional space of natural concepts derived from Wikipedia [21]. In this model, a concept is actually represented by a document, the Wikipedia article. An inverted index is built to store the tf-idf word-concept association. Then the text-concept strength (the entry of the above vector) can be calculated by the weighted sum of tf-idf scores of all words occurring in the text. Ranking these strengths yields the mos t related concepts of that text. According to its nature of characterizing information by concept vectors, ESA is a vector space retrieval model.  X  : A term in term space  X  : A document in term space  X  = X  1 ... X   X  : A query in term space, composed of  X  terms  X   X  : A concept relevance model  X  : A concept in  X   X   X  : A relevant document in concept space  X   X  = X   X 1 X   X   X  X  X  : The query  X  in concept space, composed of k concepts  X   X 1 X   X   X  X  X   X  =  X   X 1 , X   X 2 ,... X   X  X  X  : The collection of  X  relevant documents 
In the LM framework, Conceptual Language Models (CLM) have been proposed to enrich the representation of a textual query with information carried by concepts [22]. This model does not extract concepts from text, but uses metadata or keywords assigned to abstracts of scientific papers (only 11 human-annotated concepts per document on average), so the concepts used by the model are limited in number and do not represent the content of documents, while our model will recognize hundreds to even thousands of concepts from the entire document for estimation. Besides, concepts in CLM are merely used as a pivot language to help estimate textual terms from pseudo relevant documents, but the word expression of concepts is not used in estimation or retrieval. The final expanded query is still in the bag-of -words representation, leaving the term dependency issue unresolved. However, CLM would be an appropriate baseline to compare with our models, considering the same LM framework, but different approaches to dealing with concepts. 
To obviate the limitations described above in related work, we model and use concepts in a novel, simple, and effective manner . We propose a concept-based relevance model for information retrieval. Figure 1 shows the role and steps of our novel CRM in a retrieval process . A more detailed workflow of CRM retrieval algorithm is described later in Figure 4 . CRM is derived from the idea of Lavrenko X  X  RM, but unique in four respects (shown as shaded boxes in Figure 1): (1) W e perform concept extraction and estimation in concept space, while performing query expansion in term space, with appropriate weights for both regular query terms and the word expression of estimated concepts; (2) We make a new assumption concerning the generative process for relevance, which is more pertinent to the criteria used by judges to assess relevance, and subsequently validate this new assumption; (3) We develop the model by proposing semantic analysis to identify concepts relevant to queries; and (4) We propose a new estimation method of document models using such relevant concepts. In this section, we mainly focus on extracting concepts and developing a simple CRM. The full CRM with semantic analysis and new estimation method will be presented in Section 4. 
In our current study, we set the retrieval environment to the medical domain, because there are abundantly many concepts and numerous concept variations in medical documents [17], and rich domain resources available to exploit. There are many existing algorithms and tools for medical concept extraction. Our research focus in this paper is information retrieval, rather than the investigation of high accuracy information extraction algorithms; the influence of concept extraction on retrieval performance may comprise our future work. Instead, we adopt an unsupervised, ontology-based extraction method to identify concepts in medical map biomedical text to concepts defined in the Unified Medical over 1 million biomedical concepts and 5 million concept names, from over 130 lexicons and thesauri in clinical care, public health, and epidemiology. It is produced and updated by the National Library of Medicine with the purpose of assisting computer systems to  X  X nderstand X  the language of biomedicine and health. For a particular concept, the UMLS provides a list of synonyms, concept semantic types, and their semantic relationships to other concepts and types. In our work, we use MetaMap to interact with the UMLS to discover concepts referred to in text. The NLP and computational linguistic techniques used in MetaMap, such as shallow parsing, noun phrase chunking, and lexicon lookup, enable it to achieve a 0.909 precision and a 0.554 recall in medical concept extraction task [28], which are fairly good results for unsupervised algorithms. 
Once we extract concepts from documents and queries, the entire modeling process can be transformed into a concept space. In this space, queries, documents, user X  X  information need, and any other type of information are represented in the form of concepts (see Table 1 for the definition of random variables) . We assume that there is an underlying concept relevance model  X  every information need, which is a multinomial distribution over concepts. Assume that a concept  X  in  X   X  is sampled from  X  the probability  X ( X  X  X   X  ) , and a query  X   X  is sampled from  X  the probability  X ( X   X  | X   X  ) . Both the  X   X  and the query  X  random samples from the distribution  X ( X  X  X   X  ) , but they do not necessarily follow the same sampling process. 
Considering a query  X   X  = X   X 1 X   X   X  X  X  which provides the only available samples observed about the unknown process  X  best guess for the probability of observing  X  from  X  related to the conditional probability of observing  X  given  X 
The challenge then becomes estimating the joint probability of observing the concept  X  together with query concepts  X  Lavrenko and Croft proposed two methods for estimating the joint probability of RM [5]. The first method assumes that the query words and the words in relevant documents are sampled identically and independently from a unigram distribution. This is a simple but very strong independence assumption so that makes the estimation not as good as the other method [5]. We choose the other method, the conditional sampling, to estimate our model. 
Given a specific concept  X  , we pick a document  X  according to  X ( X   X  | X ) , then sample the query concept  X  with the probability  X ( X   X  X  X  | X   X  ) , and repeat this process  X  times to generate the whole query  X   X  . This sampling strategy assumes the query concepts  X   X 1 ...  X   X  X  X  are independent of each other, but keep their dependence on  X  . Here  X   X  is a set of documents that are relevant or pseudo relevant to the initial query  X  . In the pseudo relevant case, they are the top ranked documents for query  X  . Following this sampling process, the joint distribution can be expressed as below:  X  (  X , X   X 1 ...  X   X  X  X  ) = X  (  X  )  X  X  X  (  X   X  X  X  |  X  )
The prior  X  (  X  ) can be estimated from the concept in every relevant document in  X   X  . 
The conditional probability of picking  X   X  given the concept  X  can be expressed as below using Bayes X  rule:
The document prior  X ( X   X  ) is assumed to be uniform over  X  Then, the only unknown term is the critical document model  X  (  X  |  X   X  ) . We employ RM X  X  estimation method for it in this section, but will propose a much more effective method later. where  X  (  X , X   X  ) is the frequency of  X  in  X   X  , |  X  number of concepts in  X   X  ,  X ( X  X  X  X  X  X  X  X ) is the frequency of  X  in the collection divided by the total number of concepts in the collection, and  X  X  X [0,1] is the smoothing parameter. 
We are able to calculate the joint probability by plugging Eq. (3)-(5) into Eq. (2), and then estimate  X  (  X  |  X   X  approach can be seen as an extension of RM in concept space, therefore we name it the Simple Concept-based Relevance Model (s-CRM). We will show how we incorporat e semantic information to develop the full CRM in the next section. 
The process of concept extraction not only annotates medical concepts in documents, but also provides semantic types of concepts, and semantic relations between them. Such additional information motivate s us to improve the estimation of  X  (  X  |  X  under a new assumption. 
Th e assumption in Section 3.2 is equivalent to that used in RM, based on which RM generates the query and the entire relevant document . However, we are aware that this assumption is not consistent with the prevailing TREC standard used to create relevance judgments, which was described by Voorhees in [13]:  X  To define relevance for the assessors, the assessors are told to assume that they are writing a report on the subject of the topic statement. If they would use any information contained in the document in the report, then the (entire) document should be marked relevant, otherwise it should be marked irrelevant. X 
We can see that a document is relevant to a topic if part of it is relevant. This actually coincides with van Rijsbergen X  X  definition of relevance judgment [14]:  X  A document is relevant to an information need if and only if it contains at least one sentence which is relevant to that need . X 
These definitions can be seen as a practical guideline to make the process of assigning relevance judgments more executable . But apparently it results in obstacles to the estimation of relevance. Now that a relevant document may contain only a small piece of information that is really pertinent to the information need, using the entire document to estimate relevance appears inefficient, sometimes even ineffective. 
For this reason, we are motivated to make a new assumption that  X   X  generates the query  X   X  but only the relevant concepts in a a relevant document comprises two subsets: the relevant concepts (with regard to the query) and the non-relevant concepts, where only the relevant concepts are generated from  X  relationships are represented in Figure 2 left side. We argue that this assumption is more consistent with the prevailing standard of relevance judgments, though it may not be the only valid assumption or even optimal. 
Such a change in the assumption is very important, because it directly reformulates the scope of information we will use to estimate the model, and eventually makes a considerable impact on the accuracy of estimated relevance . The right side of Figure 2 briefly shows the process we intend to perform through semantic analysis, which will be detailed next. 
Table 2: Notation of random variables on semantic types  X  : A semantic type  X  : The semantic type of concept  X   X  ={ X   X   X  (  X , X   X  ) : if a semantic relation between two semantic types has been defined in the Sematic Network,  X  (  X , X   X  ) =1 ; otherwise 0. 
In its Semantic Network, the UMLS defines: (1) a set of broad subject categories, or semantic types, that provide a consistent categorization of all concepts represented in it; (2) a set of useful and important relationships, or semantic relations, that exist between semantic types. The semantic types are the nodes in the network, and the semantic relations between them are the links. In current version, the Semantic Network contains 133 semantic types and 54 relations. A small portion of concepts may have more than one semantic type. During concept extraction, MetaMap will determine which one fits such a concept best in the context. Table 2 shows the definition of random variables on semantic types and relations. 
From the semantic point of view, it is reasonable to assume that a concept a is unlikely to be relevant to concept b , if the category of a does not interact from any perspective with the category of b on a hyper semantic level. On the other hand, if a and b are related in certain manner,  X   X  and  X   X  should have been linked in the Sematic Network. Based on this assumption, we define the concepts that are relevant to query  X   X  as: T is actually the set of all semantic types that are related to query semantic types. 
Th e identification of relevant concepts can be illustrated with an example in Figure 3 . We extract and present all concepts in the cloud. To determine the relevant concepts of  X  X car tissue X , we map all concepts to their semantic types (rectangular boxes) in the semantic space. If there is a defined relation (solid edge) between a semantic type and  X  X athologic Function X , the semantic type of  X  X car tissue X , all concepts mapped to that type will be treated as relevant to  X  X car tissue X . To make it clearer, we mark yellow all boxes that have certain relation with  X  X athologic Function X , and then mark red all concepts mapped to these yellow boxes. They are the relevant concepts of  X  X car tissue X , from which a more accurate relevance model of  X  X car tissue X  will be estimated. On the contrary, the concepts detected as non-relevant are some spatial concepts, quantitative concepts, and conceptual entities. They are probably not what users want to know, therefore will deteriorate the estimation if not filtered out. If a query contains multiple concepts, we will repeat this process for every concept. 
It is worth mentioning that semantic relations are inheritable from parent nodes. In spite of no direct edge between  X  X athologic Function X  and  X  X ody Substance X , since  X  X athologic Function X  is a child node of  X  X iologic Function X  (defined by  X  X s a X  relation), it inherits the  X  X roduces X  relation between  X  X iologic Function X  and  X  X ody Substance X . If  X  X athologic Function X  has child nodes, this relation is further transitive to them as well . Figure 3: The process of semantic analysis to identify concepts relevant to the query  X  X car tissue X .
We assume that there is a multinomial distribution of concepts for any semantic type in a document . We model the document as a mixture of concept distributions of different semantic types. Given a concept  X  and a relevant document  X   X  , we first pick a semantic type  X  by the probability  X ( X  X  X   X  ) , and then sample the concept  X  from  X  by the probability  X  (  X  |  X , X   X  ) . Following this process, we propose a new method to estimate the document model  X  (  X  |  X   X  ) , instead of using Eq. (5) in RM framework. where  X  (  X  X  X   X  ) can be treated as the importance of a semantic type in  X   X  , and  X  (  X  |  X , X   X  ) is the concept distribution of this semantic type in  X   X  . 
We are able to estimate  X  (  X  |  X , X   X  ) by the maximum likelihood estimation of  X  in this semantic type, and then smoothed by the collection background model.  X  (  X  |  X , X   X  ) = where I is the indicator function and  X  (  X  |  X , X  X  X  X  X  X  ) is 
For  X  (  X  X  X   X  ) , ideally, users could specify what types of information and how much of them they would like to see in a retrieved document. For example, the document should mainly present the  X  Disease or Syndrome  X  description , the  X  X hera peutic Procedure X  , or a certain combination of both. However, without user input , we are unable to obtain such prior information. In this situation, the best guess for user X  X  need on semantic types is naturally related to estimating it from pseudo relevant documents. Intuitively, the weight of a semantic type depends on the frequency of concepts of this semantic type in the document. On the other hand, according to our new assumption , only the semantic types belonging to  X  should be taken into account in relevance estimation . Consequently, we propose the following formula to estimate  X  (  X  X  X   X  ) . 
In this formula, the weight of a semantic type from  X  is proportional to th e occurrence of all concepts in this type in the document. Semantic types that are absent from  X  will receive a null weight, which makes concepts in such semantic types have a null  X  (  X  |  X   X  ) , and eventually a null  X  (  X  |  X  will be composed of relevant concepts shown in Eq. (6) only. We name the  X  (  X  |  X   X  ) estimated using this approach the Full Concept-based Relevance Model (f-CRM). Both s-CRM and f-CRM may be referred to as CRM or CRMs later. We describe the workflow of CRM retrieval algorithm in Figure 4. Estimating CRM requires obtaining  X   X  , a set of pseudo relevant documents. We employ the query likelihood model with a Dirichlet prior for this purpose. Its ranking function is where  X ( X , X ) is the frequency of  X  in  X ,  X ( X | X ) is estimated by the maximum likelihood of  X  in  X  with Dirichlet smoothing,  X ( X | X  X  X  X  X  X ) is the frequency of  X  in the collection divided by the total number of terms in the collection, and  X  is the smoothing parameter from Dirichlet prior. We set  X  to the average document length for each individual data collection [22]. The top  X  documents ranked by the query likelihood model will be incorporated in  X   X  . 
Relevance models capture the behavior of returned documents but may drift from the original query. In practice, they are usually employed for query expansion to achieve better retrieval results [9, 10, 19]. We carry out the similar procedure for CRM by linearly interpolating it with the maximum likelihood query estimate. Then we obtain the final expanded query model  X  where  X  X  [0,1] controls the weight of concepts from CRM. For computational efficiency, we truncate and normalize using the concepts with the highest probabilities [10, 22]. Concepts are sorted in the descending order of  X  (  X  |  X   X  ) , the top  X  concepts are selected, and their probabilities are normalized to sum to one. 
We utilize the Indri retrieval model for document retrieval in our work. This model combines the language modeling and inference network approaches, and enables modeling structured and is therefore able to evaluate structured queries derived based on Eq. (7). We will discuss the specific approach to formulating structured queries using the word expression of estimated concepts in Section 5.1.3. 
Figure 4: The workflow of the f-CRM retrieval algorithm. express constraints between the words in the retrieved documents, such as Boolean and proximity operators [26]. In order to better understand the strengths and weaknesses of CRM , we carefully pick three data collections for evaluation, TREC Genomics 2006, CLEF2013, and OHSUMED. All of them are designed for medical information retrieval, but have quite different characteristics (full-text articles, web pages, and article abstracts). Table 3 provides a summary of these. 
The document collection for the TREC 2006 Genomics ad-hoc search task [ 29] consists of full-text scientific papers from 49 biomedical journals. Its original topics follow the pre-defined templates, such as  X  X hat is the role of [ gene ] in [ disease ]? X  We derive the topics used in our experiments by only extracting bold-faced terms and discarding the remainder of the template [22]. The relevance is assessed at the document, passage, and aspect level. We only use the judgments at the document level. Two topics without any relevant documents are not used in evaluation. 
The CLEF2013 collection comes from the information retrieval task of the eHealth Challenge in the Conference and Labs of the Evaluation Forum 2013 [ 30 ]. It is a large web collection covering a broad range of health topics from several online sources, including websites certified by Health on the Net Foundation, as well as other commonly used health and medicine websites (e.g. ClinicalTrial.gov, Diagnosia, and Drugbank). 50 topics are created based on questions that patients realistically posed after reading their discharge summaries. We preprocess the HTML files by extracting textual content using Boilerpipe library. 
Apart from the above two collections with relatively long documents, we are also interested in the performance of CRM on short documents, which are assumed to challenge the model by providing a limited amount of data points and decreased sample size of concepts. For this reason, we choose OHSUMED , a collection of titles and abstracts of MEDLINE articles over a five-year period [31] . Its relevance judgments are definitely relevant, partially relevant, or irrelevant. We count both the definitely relevant and the partially relevant as relevant to be consistent with other studies. Five topics without any relevant documents are not used in evaluation. PubMed stopwords list and Porter stemming are applied to all three collections. In all experiments, only the title portion from the original topics is used to construct queries. 
We use the query likelihood model to acquire pseudo relevant documents. The top 100 documents of each query are gathered and sent to MetaMap for concept extraction. To avoid losing any information during the transformation from  X  to  X   X  term (other than stopwords) cannot be mapped to a concept, we will manually extract it as a concept and add to MetaMap, so that such term will be identified by MetaMap in pseudo relevant documents. We find nine such terms in all queries. Most of extracted concepts contain two words (47%) , then one word (35%), three words (14%), and less than 5% over three words. In Table 4, w e show the statistics of feedback document subsets, and compare them between term space and concept space. We use the Indri retrieval system from the Lemur project for indexing and retrieval. Indri has flexible query language and provides multiple operators for us to express concepts in structured queries. The  X #1(...) X  operator restricts the words of a concept to appear in the exact ord er. The  X #weight(...) X  operator assigns different weights to expressions in it. The  X #syn(...) X  operator treats all expressions in it as synonyms. The expanded query in Eq. (7) can be implemented by creating a structured query of the form:  X  X  X  X  1 =# X  X  X  X  X  X  X  X  X  (  X  (  X   X 1 |  X   X  ) #1 (  X   X 1 ) ... X  (  X   X  X   X  X  X  X  2 =# X  X  X  X  X  X  X  X  X  (  X  (  X  1 |  X   X  ) #1 (  X  1 ) ... X  (  X   X  |  X 
If a concept  X   X  has  X  surface forms  X   X  ={ X   X  X  X  , X =1... X  X , we will replace #1 (  X   X  ) above by # X  X  X  X  ( #1( X   X 1 ) ...#1 (  X  include all synonyms. We now investigate the effectiveness of our models in practice. Two CRMs (s-CRM and f-CRM) are compared with relevance models (RM-3 version) from language modeling framework, and Okapi BM25 with pseudo relevance feedback (BM25-FB) from probabilistic ranking framework, and the state of the art Conceptual Language Models (CLM, the baseline ) [22] . Both CRMs and CLM will use the same  X   X  from the initial retrieval. We follow the same setting of parameters in [22] to estimate the parsimonious language model in CLM. Retrieval performance is evaluated using a variety of metrics, including precision at 5 and 10, nDCG at 5 and 10, and MAP (cutoff at 1000) . In order to obtain a fair comparison, we pursue five-fold cross-validation on all models, and then compare their cross-validation performance. We randomize and separate queries into five partitions as evenly parameters to optimize MAP for the other four sets, and use such parameters to test on the k -th set of queries. The cross-validation performance is the average performance on the five test query sets. 
The free parameters trained in cross-validation include: (1) for all models, the number of pseudo relevant documents and the number of expansion terms/concepts in [1,50], with 1 increment in [1, 30] and 5 in [ 30 ,50], and the expansion weight in [0,1] with 0.1 increment; (2) for CLM, RM , and CRMs, smoothing parameter in [0,1] with 0.1 increment ; (3) for CLM, | X  X  X  X 1, 100] , with 1 increment in [1,30] and 5 in [30,100]; (4) for BM25-FB,  X   X  X 1,5] with 0.1 increment and  X   X  X 0,1] with 0.05 increment. 
Table 5 shows the experimental results of different models on three data collections. The percentage changes over the baseline CLM are listed in the second column. We also conduct one million times randomization tests to compute the significance of the improvements [11] . The superscripts  X  ,  X  ,  X  , and  X  indicate statistically significant improvements over CLM, RM , BM25-FB, and s-CRM for p &lt;0.05 . The best result under each metric is bold. 
From the results, two CRM retrieval algorithms consistently outperform CLM, RM and BM25-FB algorithms on all metrics across three data sets. And the improvements are all statistically significant over these three models, except on P@5 from TREC2006 and few metrics from OHSUMED. In general, BM25-FB performs worse than RM, which is caused by its high sensitivity to parameter selection. We will address this problem in next section. CLM performs better than RM on TREC2006 and CLEF2013, while not as well as RM on OHSUMED because of the small average document length. This observation is consistent with the experiment results on other data sets of short documents reported in [22]. In all five models, f-CRM achieves the best results, substantially improving P@5 by 10%, P@10 by 11%, nDCG@5 by 9%, nDCG@10 by 10%, and MAP by 10%, at least. 
CLM in our experiments is able to use hundreds of concepts extracted from the entire document for model estimation, which is quite different from the environment in [22] that only a limited number of concepts assigned to the abstract are available. We compare the results on TREC2006 reported in [22] with those in our experiments (experiment settings are slightly different though). U nsurprisingly, using more concepts for model estimation improves retrieval performance substantially. This confirms the positive effect of concept extraction in our work. 
Comparing f-CRM with s-CRM, we find that the full CRM with semantic information consistently produces better results, with an extra 5% improvement over s-CRM on average. In addition, such improvement is statistically significant on most metrics. This validates our new assumption on relevance generation and the approach to modeling semantic types. 
We also compare our results with the officia l best results from data collection campaigns. For CLEF2013, there is no single best mo del dominating on every metric [30] , but f-CRM improves over the best reported results by 8%, 10%, 21%, 26%, and 18% on P@5, P@10, nDCG@5, nDCG@10, and MAP. On TREC2006, the best model reported a 0.5439 document-level MAP, but it was actually calculated using top ranked passages instead of documents [29], which is different from the standard evaluation method we used, and thus makes the results incomparable. However, this best TREC2006 model successfully conducted fine-grained preprocessing [32], such as Greek letter conversion, conditional stemming, and gene symbol handling, to customize algorithms for this particular gene data set, which can also be incorporated in our CRMs for further improvement. 
We explore the empirical settings of four parameters: the smoothing parameter  X  , the interpolation parameter  X  , the number of feedback documents  X  , and the number of concepts used in query  X  . In addition to the cross-validation parameters, we run models over all queries and search for their global optimal parameters that yield the best MAP. We investigate parameter sensitivities, and why some models perform well and some not. 
Smoothing is a critical component of any system based on language modeling. The smoothing parameter  X  in Eq. (9) controls how much of the background estimate will be used in  X  (  X  |  X , X   X  ) . The interpolation parameter  X  in Eq. (13 ) controls how much weight will be assigned to CRM concepts in the interpolation with the original query. We plot the performance simplex and contour s of the f-CRM in Figure 5 to show the sensitivity of  X  and  X  on TREC2006. Figures on other collections show similar patterns, so we skip them in the interest of space. In general,  X  is not sensitive and has less impact to MAP. [0.2, 0.6 ] appears a safe interval. However,  X  influences the performance significantly. Choosing a small value will relegate CRM to the query likelihood model, while choosing a large value will underestimate the original query. On all data sets,  X  in [0.2, 0.8 ] could yield results better than RM, and in [0.4, 0.6] yields satisfactory results.  X  is the number of pseudo relevant documents in  X   X  . Setting  X  too small leads to insufficient relevant information for estimation; setting  X  too large brings in more interference from non-relevant content.  X  is the number of top-ranked concepts used for retrieval. Ideally, we would like to use all of them; however running thousands of concepts in a query is computationally very expensive. We plot the performance simplex and contours of the f-CRM in Figure 6 to show the sensitivity of  X  and  X  on TREC2006 . Typically, these two parameters have less sensitivity than the interpolation parameter  X  , though very small values of  X  and  X  will hurt the performance. For  X  , the empirical recommendation [6,15] produce satisfactory results on all data sets. For  X  , initially, the more concepts are added, the better the results become; however, once reaching certain level, the results stay stable. This can be explained by the insight that the concepts added later have a low probability  X  (  X  |  X   X  ) , which results in a negligible impact on the entire query once it is small enough. The optimal value of  X  varies from 20 to 50 for different data sets. However, when examining  X  (  X  |  X   X  ) of the N -th concept, we find that it is around 0.0047 on every data set. In practice, we can truncate  X   X  with  X  (  X  |  X   X  ) &gt;0.0047 for retrieval. 
For CRMs, all five sets of parameters trained from cross-validation are very consistent with the ir global optimal parameters, with more than 80% identical and the rest very close. Therefore, CRMs could achieve relatively high cross-validation performance (e.g. MAP 0.473 vs. the global optimal 0.484). The similar trend on parameters can be observed from CLM and RM as well. We attribute this to the reduced parameter sensitivity and good generalization capability of language modeling framework. By way of contrast, BM25-FB appears more sensitive to parameters. Its parameters obtained from each partition in cross-validation are quite different from each other, and the model performs worse on test data using trained parameters. In this section we aim to develop a deeper understanding why CRM achieves such significant improvements . First, let us review Table 4 more closely. There are obvious differences on data representation between term space and concept space. For the same data set, the number of unique concepts is much smaller than the number of unique words. This indicates that the dimensionality of concept space is significantly reduced. Besides, the average concept frequency is higher than the average word frequency. Higher concept frequency means that more samples are available for concept estimation. From above observations we conclude that working on concepts yields a denser space for relevance estimation, which should theoretically produce better estimates. 
Then, we would like to examine the quality of the empirical distribution of relevance estimated by CRM. Table 6 lists the top10 relevant words or concepts and their probabilities to a given query. We observe that: (1) RM and CLM could obtain some relevant words, but also includes quite a few noisy and non-relevant words such as  X  X t X ,  X  X l X ,  X  X sa X , and  X  X bout.com X , which can be pruned by CRM; (2) RM and CLM are unable to keep necessary term dependencies, so that it estimates fuzzy relevance, which comprises of individual words without complete meanings; for example, despite  X  X hite X ,  X  X lood X , and  X  X ells X  are captured by RM, the meaning they convey is not as precise as the entire concept  X  X hite blood cells X ; using such relevance for retrieval is prone to erroneously matching documents containing  X  X hite people X  and  X  X ed blood cells X ; (3) CRM is able to deliver explicit relevance with natural concepts, which conforms to human language and comprehension; (4) CRM is able to deliver more accurate relevance, by providing more concepts closely related to the query; (5) CRM assigns higher probabilities to relevant concepts, due to the increased sample size and the reduced dimensionality of the space; (6) semantic analysis further refines concepts by removing those not directly related to the query, e.g.  X  X atient X ,  X  X ospital X , and  X  X ealth X . All these observations empirically demonstrate CRM indeed produces better relevance. Query expansion resulting from such relevance will better represent user X  X  underlying search intent. And retrieval using these expanded concepts would precisely match document terms with the same dependencies. All these factors together eventually improve the retrieval performance by a significant degree. 
Despite both CLM and CRM are LM-based models that are built on concepts, they have major differences in modeling concept s. In CLM, concepts serve as an intermediate form to connect pseudo relevant documents, document terms, and queries, where the important information such as word expression of concepts and term dependencies within concepts is ignored. However, in our work, CRM is proposed to model and utilize such information from the beginning, and factors in the semantic information in the domain . Experimental results demonstrate that all this information is critical to retrieval results, and should be formally included by future concept-based retrieval models. 
Another interesting finding is that, CRMs perform even better on TREC2006 and CLEF2013 than on OHSUMED, which is in accordance with our conjecture and the observation on CLM. The documents in the first two sets are full-text articles and web pages, much longer than the abstracts in OHSUMED, so they provide sufficient data points for accurate statistical estimation. This again supports our motivation of modeling relevance by constructing a denser, more relevant concept space. It is encouraging to observe that CRMs also work effectively on short documents. 
We have presented a concept-based relevance model for improved estimation of relevance. Our method models natural concepts, so is able to keep the strong dependencies inside of a concept, generate the explicit expression of relevance, and precisely match terms with the same dependencies . The empirical relevance distribution estimated by CRMs confirms that CRMs unify synonyms or different surface forms of a concept, leading to reduced dimensionality of the space, increased sample size of a concept, and eventually more accurate and reliable estimates of the relevance. We show that, when semantic analysis is enabled, f-CRM is able to further improve relevance estimation by modeling semantic types and relevant concepts . 
We evaluate the effectiveness of CRMs for information retrieval on different types of data sets. Experimental results demonstrate that the models are highly effective. They completely and consistently dominate conceptual language models, relevance models, and BM25 with pseudo relevance feedback across data sets. In fact, f-CRM achieves statistically significant improvement over conceptual language models by at least 9% on any metric of precision@5, precision@10, nDCG@5, nDCG@10, and MAP. 
In future work, we would like to experiment CRM on generic or other domain data sets. It would be interesting to use the publicly available Wikipedia concepts in CRM, where ample contextual information is available for new exploration. A potential improvement can be made by estimating a query-specific  X  , so that concept weights will be customized for each query. We are also interested in expanding our research on CRM to the probabilistic ranking framework. [1] K. S. Jones, S. Robertson, D. Hiemstra, and H. Zaragoza. [2] K. S . Jones, S. Walker, and S. Robertson, A probabilistic [3] J. M. Ponte and W. B. Croft. A language modeling approach [4] L. Lafferty and C. Zhai. Probabalistic relevance models [5] V. Lavrenko and W. B. Croft. Relevance-based language [6] S. Vargas, P. Castells, and D. Vallet. Explicit relevance [7] Y. Lv and C. Zhai. Positional relevance model for pseudo-[8] D. Metzler, T. Strohman, Y. Zhou, and W.B. Croft. Indri at [9] D. Trieschnigg, D. Hiemstra,F. de Jong, and W. Kraaij . A [10] N. Limsopatham, C. MacDonald, and I. Ounis. Learning to [11] M. D. Smucker, J. Allan, and B. Carterette. A comparison of [12] J. Li and H. Yan. Peking University at the TREC 2006 [13] E. Voorhees. Overview of TREC 2003. In Proceedings of [14] C.J. van Rijsbergen. Information Retrieval. Butterworths, [15] F. Ahmed and A. Nurnberger. Evaluation of n-gram [16] F. Diaz and D. Metzler. Improving the estimation of [17] C. Wang and R. Akella. A hybrid approach to extracting [18] B. He, J.X. Huang, and X. Zhou. Modeling term proximity [19] A.-J. Nasreen, J. Allan, W.B. Croft, et al. UMass at TREC [20] P. Castells, M. Fernandez, and D. Vallet. An adaptation of [21] O. Egozi, S. Markovitch, and,E. Gabrilovich. Concept-based [22] E. Meij, D. Trieschnigg, M. de Rijke, and W. Kraaij. [23] T. Tao and C. Zhai. An exploration of proximity measures in [24] D. Metzler. Automatic feature selection in the Markov [25] J. Gao, J.-Y. Nie, et al. Dependence language model for [26] D. Metzler and W.B. Croft. Combining the language model [27] D. Metzler and W. B. Croft. Latent concept expansion using [28] J. Osborne, B. Gyawali, and T. Solorio. Evaluation of YTEX [29] W. Hersh, A. Cohen, P. Roberts, et al. TREC 2006 genomics [30] H. Suominen, S. Salantera, et al. Overview of the [31] W. Hersh, C. Buckley, T. Leone, et al. OHSUMED: An [32] W. Zhou, C. Yu, et al. A concept-based framework for 
