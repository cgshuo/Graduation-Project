 Ranking is a main research issue in IR-styled keyword search over a set of documents. In this paper, we study a new keyword search problem, called context-sensitive document ranking, which is to rank documents with an additional context that provides additional information about the application domain where the documents are to be searched and ranked. The work is motivated by the fact that additional information associated with the documents can possi-bly assist users to find more relevant documents when they are un-able to find the needed documents from the documents alone. In this paper, a context is a multi-attribute graph, which can represent any information maintained in a relational database. The context-sensitive ranking is related to several research issues, how to score documents, how to evaluate the additional information obtained in the context that may contribute the document ranking, how to rank the documents by combining the scores / costs from the docu-ments and the context. More importantly, the relationships between documents and the information stored in a relational database may be uncertain, because they are from di ff erent data sources and the relationships are determined systematically using similarity match which causes uncertainty. In this paper, we concentrate ourselves on these research issues, and provide our solution on how to rank the documents in a context where there exist uncertainty between the documents and the context. We confirm the e ff ectiveness of our approaches by conducting extensive experimental studies using real datasets.
 H.3.3 [ Information Search and Retrieval ]: Search process Design, Management
Keyword search is an important issue in the information retrieval area [3] and has been widely and extensively studied over decades. Most of the existing keyword search approaches return a set of doc-uments with ranking that is most important and relevant to a set of user-specified keywords. However, in many real applications, the documents may not be the only data source, and there are other forms of interrelated data for the same application domain. In this paper, we study a new ranking problem that is to rank documents, in particular, when users cannot find any documents that contain all the given keywords from the set of documents itself, or when users can possibly find more relevant documents by exploring the docu-ments as well as the interrelated data. In other words, we consider how to e ff ectively use the additional relevant information, that may be maintained in a database, to rank documents. We explain our motivation using an example.

Fig. 1 shows a collection of reviews (documents) on the left side about movies. There are three movie reviews with review titles, namely,  X  X cean X  X  Twelve X ,  X  X even X , and  X  X welve Monkeys X . In the movie review entitled  X  X cean X  X  Twelve X , it wrote  X ... those things make the film truly great ... X . These movie reviews are about movies which can be maintained as tuples in a multi-attribute ta-ble, consisting of the movie title and other possible attribute val-ues as illustrated in Fig. 1 on the right side (refer to IMDB http: //www.imdb.com/interfaces ). The names of directors or actors or actresses may not appear in the movie reviews, but they appear in some tuples in some tables in the IMDB movie database that are linked to the movie titles via relationships such as movies di-rected by directors, and movies played by actors / actresses, based on the primary keys and foreign key references. We call such in-terrelated information maintained in multi-attribute tables a con-text to which the movie review is directly / indirectly related. The relationship between a movie review (document) and a movie ti-tle (or simply movie) maintained in a multi-attribute table is with uncertainty. Such uncertainty may be caused by missing titles, or mismatching of the discussions in the body of movie review, or di ff erent movies with the same title. For example, the movie re-view entitled  X  X cean X  X  Twelve X  may not exactly match the movie title  X  X cean X  X  12 X  maintained in the multi-attribute table. In Fig. 1, there is a link between a movie review and a movie in a table, where such uncertainty is indicated as a probability.

Suppose a user wants to know something  X  X reat X  about the actor  X  X rad Pitt X , by issuing a 2-keyword query,  X  X reat X  and  X  X rad Pitt X , against the collection of movie reviews. The movie review enti-tled  X  X even X  will have the highest score, followed by movie review on  X  X welve Monkeys X  and  X  X cean X  X  Twelve X . Suppose that the user issues a keyword query with two additional keywords,  X  X att Damon X  and  X  X avid Mills X , the movie reviews and their rank will remain the same, since none of the reviews contain the additional two keywords. As observed from Fig. 1, there is information about  X  X att Damon X  and / or  X  X avid Mills X  indirectly linked to the movies and the movie reviews, as a part of structural information, which are not e ff ectively used to answer users X  keyword queries.
The main issue we study in this paper is how to rank documents using a set of keywords, given a context that is associated with the documents. The main contributions of this paper are summarized below. First, we study a new ranking problem to rank documents for a given set of keywords. The uniqueness of the problem is that the documents to be ranked are associated with sets of interrelated multi-attribute tuples called context, which contains additional in-formation that assist users to rank the relevant documents. Second, we model the problem using a graph G with two kinds of nodes (document nodes and multi-attribute nodes), and discuss its score function, cost function, and ranking with uncertainty. Third, we propose new algorithms to rank documents that are most related to the user-given keywords by integrating the context information. We also conducted extensive experimental studies to show the ef-fectiveness of our approaches using a real dataset.
We consider a set of documents, D A = { d 1 , d 2 , } . In addition, we assume there exists a context, which is considered as a multi-attribute graph G R ( V , E ) that specifies the knowledge about docu-ments. A multi-attribute graph G R is capable of representing all the tuples in a relational database, where a node represents a tuple, and an edge between two nodes represents a foreign-key reference between the two corresponding tuples. All nodes (tuples) in the same relation are said to have the same type. There exists a set of specific A -typed nodes in G R ( V , E ), V A (  X  V ) which are explicitly linked to the documents in D A . A document may be linked to sev-eral A -typed nodes, and an A -typed node may be related to several documents. To integrate the set of documents, D A , and the multi-attribute graph, G R ( V , E ), we consider a weighted graph G ( V Here, V = V  X  D A , and E = E  X  E D where E D is a set of pairs, ( d , v j ), if d i is related to an A -typed node v j . We call a node in D document node, and a node in G R a multi-attribute node. We assign edge-weights to edges in the subgraph G R ( V , E ) of G based on [11, 6]. We assign the edges in E D with a non-zero weight which is a normalized similarity score computed as given in [2]. We con-sider the weight assigned to ( d i , v j )  X  E D as the probability that d is related to v j . A document node, d i , may be related to several dif-ferent multi-attribute nodes, v j and v k , with probability distribution, must satisfy P v Example 2.1: Fig. 2(a) shows a graph G ( V , E ). There are two documents in D O = { d 1 , d 2 } , and a multi-attribute graph G illustrated in the dashed rectangle. There are two types of nodes in V , namely O and V , where the former consists of { o 1 and the latter consists of { v 1 , v 2 , v 3 , v 4 } . The edges between two nodes in G R show how the multi-attribute nodes are interrelated. The edge ( d 1 , o 1 ) and ( d 1 , o 2 ) show that d 1 is related to o with probability distribution, 0.5 and 0.4, respectively. Problem Statement : Finding top-k documents for an l -keyword query, Q = { w 1 , w 2 , , w l } , against graph G . (a) A Sample Graph
Given an l -keyword query Q = { w 1 , w 2 , , w l } , we first discuss a concept, called a local context, for a document node d i possibly ranked. A local context is a connected tree in G , denoted as T ( V , E ). The local context contains one document node, d can also link to a connected rooted tree,  X  ( V , E ), as a subgraph in G . Note that G R  X  G . We say that the local context T supports the document d i . A connected tree  X  ( V , E ) specifies an interrelated tuple structure among tuples that is related to the document node d . The root of  X  ( V , E ) is an A -typed node, say v j , and the proba-bility for d i to link to v j is prob ( d i , v j ). A keyword w is contained in T ( V , E ), if it appears in the document node d i or in any multi-attribute node in the corresponding  X  ( V , E ).

In the following we use T and t i as local contexts interchange-ably. Here, T means a general local context, whereas t i means a local context for a specific document node. We denote the proba-bility of a local context, t  X  for a document d i as prob ( t  X  ). It is equal it is prob ( d i ) = 1  X  P v Example 3.1: Consider Fig. 2(a), given a 3-keyword query, Q = { a , b , c } , we show the six local contexts, t i , for 1  X  i  X  6, in Fig. 2(b). Among them, the local contexts, t 1 , t 2 , and t document d 1 , and the local contexts, t 3 , t 4 , and t 6 Score Function for Local Context : We discuss score functions to score a local context for a document below, and will discuss ranking documents in local contexts, in the next section, which is based on the scores and probabilities of having such scores.

Recall that a local context t  X  consists of a document node d which may link to a connected tree  X  j rooted at a multi-attribute node v j . There are two main components in our score function to score a local context t  X  . One is related to the keywords it contains. The other is how to evaluate the cost of using a connected tree  X  The connected tree  X  j is needed if it contains some keywords that are missing in the document node d i . However, there is a cost to use such a connected tree  X  j to include more keywords. It is because that there may be a  X  j which is very relevant to d i , and there may be a  X  j which is not very relevant to d i .

First, for the first component, we consider a local context T as a virtual document [12], which can be scored using an IR-style rele-vance score, with respect to the l -keyword query Q = { w It is defined below [12], Here, w  X  T  X  Q indicates the subset of keywords of Q appearing in T . t f w ( T ) is the term frequency; id f w = N + 1 d f is the document frequency, and N is the total number of the vir-tual documents; (1  X  s ) + s dl T avdl is the length normalization, where dl
T is the document length of the local context T , avdl is the aver-age document length among the whole virtual document collection, and s is a parameter. We also consider a completeness coe ffi cient for score IR ( T , Q ), denoted as complete ( T , Q ), which specifies how important for a local context to contain a keyword. The more key-words contained the better. Here, I ( w i , T ) is an indicator function, it equals to 1 if and only if local context T contains keyword w i and 0 otherwise.

Second, for the second component, we define it to be the total distance between the document node d i and the nodes in  X  contain the keywords in a local context T [7]. where V (  X  j ) denotes the set of nodes in T that appear in  X  denotes the node in  X  j that contains the keyword w , and dis ( a is the distance between two nodes in the local context based on the edge weights.

Combining the two components, the score of a local context, T , with respect to a query Q = { w 1 , w 2 , , w l } is given as follows. where  X  is a parameter in the range of [0 , 1], that specifies the rel-ative importance of the two factors. A local context, T , is ranked higher if its score score ( T , Q ) is larger, which implies that either its textual component score score IR ( T , Q ) is larger or its structural component cost cost st ( T , Q ) is smaller.
Given an l -keyword query, Q , against a graph G , we first gener-ate the set of local contexts, T = { t 1 , t 2 , } , where a local context t , scored score ( t x , Q ), contains a unique document node, d has a probability prob ( T ), which means that the local context t supports that document d i has a score socre ( t x , Q ) with probability prob ( T ). Each local context supports one document. Given the set of local contexts, the score of documents is a probability distribu-tion. After getting the local contexts, documents are ranked based on the probability score distribution. We integrate the information to rank top-k documents.

We adopt ranking with uncertainty probability as discussed in [9, 5], where x-Relation model is used. In the x-Relation model [1], there is a set of independent x-tuples (called generation rules in [14, 9]), where an x-tuple consists of a set of mutually exclusive tuples (called alternatives), represents a discrete probability distribution of the possible tuples it may take in a randomly instantiated data. Each alternative t has a score and a probability, where Pr( t ) represents its existence probability over possible instances.

The x-Relation, X , is a probability distribution over a set of pos-sible instances { I 1 , , I n } . A possible instance, I j or one alternative for each x-tuple  X   X  X  . The probability of an in-stance I j , is the probability that x-tuples makes exactly the choice of alternatives as in I j , specifically, Pr( I j ) = Q t  X  I Pr(  X  )), where  X  &lt; I j means x-tuple  X  takes no alternative in I Pr(  X  ) = P t  X   X  Pr( t ). The entire possible worlds of an x-Relation, X , denoted as pwd ( X ), is the set of all the subset of I  X  X with probability greater than 0 (Pr( I ) &gt; 0).

In our problem setting, the local contexts confirm the x-Relation model. The set of local contexts that support the same document forms an x-tuple, and an x-tuple specifies that the local contexts belong to it are mutually exclusive. Intuitively, an x-tuple supports one document with mutually exclusive evidences. And the local contexts that support di ff erent documents are independent.
Before we discuss the possible worlds based ranking semantics, we first study a straightforward strategy of getting score for doc-uments deterministically. As we have shown, an x-tuple specifies a probability distribution on the score for documents. It is natural to define the score of a document as the expected score of the lo-cal contexts that support it. The expected score of a document is score ( T , Q ) is the score of local context T against query Q (refer to Eq. (4)), and prob ( T ) is the probability of local context T . How-ever, one drawback of the expected score is that the expected score is sensitive to the score values, high score value with low probabil-ity can result in high expected score.

The top-k probability of a local context, T , denoted as tk p ( T ), is the marginal probability that it ranks top-k in the possible worlds [9], the local context is ranked as one of the top-k local contexts in the instance I . The top-k probability of a document is defined as
Expected rank semantic is proposed in [5]. The rank of a local context T in a possible world I is defined to be the number of local contexts whose score is lager than score ( T , Q ), i.e., rank |{ T i  X  I | score ( T i , Q ) &gt; score ( T , Q ) }| . The expected rank of Pr( I ). The expected rank of a document is defined as ERank ( d P
T : doc ( T ) = d i ERank ( T ). It means the expected rank position for the document d i in a randomly generated possible world.
We have implemented six algorithms, based on di ff erent ranking semantics, to find the top-k documents with the help of a multi-attribute graph G . The six algorithms are denoted as: ERank , tk p , ES core , OptProb , OptS core , and DocOnly . The first three are ranking based on the expected rank, top-k probability, and ex-pected score as discussed in Section 4, respectively. OptProb and OptS core are two heuristics to choose the score of a document as the score of the local context in the corresponding supporting set (x-tuple) with the largest probability and score, respectively. DocOnly is to rank the documents based on the IR-score only, with-out a multi-attribute graph. We conducted all the experiments on a 2.8GHz CPU and 2GB memory PC running XP.

For testing the algorithms, we use the real datasets, for both doc-uments and multi-attribute graph. For the documents, we use the movie review dataset, and we crawled 330 , 201 reviews about dif-ferent movies from the Amazon website ( http://www.amazon. com ). For the multi-attribute graph, we use an IMDB movie database ( http://www.imdb.com/interfaces ). The multi-attribute graph is created in the same way as used in the literature of keyword search in RDB [6]. That is, we create a node for each tuple in the database, and there is a directed edge from node u to v if and only if there is a foreign key reference from u to v . In our experiments, we use 6 tables from IMDB database using the follow schema: Movie(Mid , Name), Genre(Mid, Genre), Director(Did , Name), Di-rect(Mid, Did), Actor(Aid , Name), and Play(Aid, Mid, Character), where primary keys are underlined. The numbers of tuples of the 6 tables are: 110 K , 148 K , 62 K , 113 K , 577 K , and 1410 K , respec-tively. The similarities between the matchings of movies in IMDB and movie reviews are calculated using the methods given in [13].
We evaluate the ranking accuracy for two keyword queries, q {  X  X aptain X ,  X  X xcellent X ,  X  X usic X ,  X  X ea X  } , and q 2 = {  X  X dventure X ,  X  X ebster X ,  X  X antastic X ,  X  X obert X  } . For query q 1 , we expect to find some reviews regarding excellent musical movies which are about sea and a captain. Query q 2 intents to find the reviews for fantastic movies, either the genre of the movie is adventure or the review categorizes it as an adventure movie, and  X  X ebster X  and  X  X obert X  are either character names or actor / director names. We evaluate the ranking accuracy of our algorithms using a (discounted) cumulative gain measure [10] based on human judgements. The (discounted) cumulative gain measure is widely used in measuring ranking ac-curacy of top-k queries [4]. For each query, we run all the six algo-rithms for a top-30 query, and assign relevance values (0-4) to all the documents returned by the algorithms.

Fig. 3(a) and Fig. 3(b) shows the cumulative gain and discounted cumulative gain for the top-10 documents returned by these algo-rithms for query q 1 . The three algorithms: ERank , tk p , ES core have very similar cumulative and discounted cumulative gains, and consistently perform better than the other algorithms. Fig. 3(c) shows the discounted cumulative gain for query q 2 . One di ff erent characteristic of q 2 from q 1 is that, while the relevant document for q contains a lot of keywords in q 1 , the keywords of q 2 mostly ap-pear in the multi-attribute graph. tk p performs better than ERank and ES core , because most part of the score of a local context comes from the multi-attribute graph for q 6 . The probability of a local context has more impacts on the documents ranking. The uncer-tainty aware algorithms ERank , tk p , and ES core perform better than other algorithms.
Fig. 4 shows the top-1 result returned by ERank , tk p , and ES core for query q 2 . In this case, the document contains only one keyword  X  X antastic X . The multi-attribute graph provides other keywords for the documents, and the relationships between these keywords and the document to be ranked are very tight. This top-1 result can not be found by DocOnly , as it contains only one keyword. OptProb and OptS core can not find this top-1 result either, because there exists other local contexts with a larger score but lower probability.
The problem studied in this paper is related to three aspects of works in the literature. First is fuzzy object matching, which is to reconcile objects from di ff erent collections that have used dif-ferent naming conventions. Fuzzy object matching is also known as record linkage, de-duplication or merge-purge [2, 8]. Our work interpret the object linkage as probabilities. Second is keyword search in RDBs, where the RDB is materialized as a graph. Most of the works concentrate on finding minimal connected tuple trees that contain all the keywords [11, 6, 7]. In our work, the context is also materialized as a multi-attribute graph. Third is ranking queries in uncertain data. Most of them represent the uncertainty as a probability value, also called probabilistic data. Trio system [1] is a frequently used uncertain database model. Possible worlds based query ranking determines the top-k result by the interplay between score and probability. In [14, 9], they get the top-k tuples in an x-relation [1], where each tuple has both a score and a probability. U-TopK and U-kRanks queries are first proposed in [14]. Hua et al. in [9] define the PT-k query, and proposed three approaches to answer the PT-k query. Cormode et al. [5] propose the expected rank query, which ranks tuples based on their expected rank values.
In this paper, we focus on a new keyword search problem, which is to rank documents in a context (a multi-attribute graph), for a user-given l -keyword query. There are two main data sources in the problem. One is a set of documents, and the other is a relational database which the multi-attribute graph represents, the relation-ships between documents and the tuples in the relational database are with uncertainty, which is caused by similarity match. We dis-cuss how to model using a graph G , and how to score and rank. Our work involves two di ff erent issues: ranking and uncertainty, and the unique issue is that the ranking needs to take uncertainty into consideration because a result with a higher rank may be with a lower probability to occur. We present our score function with two components (the IR-styled score and the structural cost). We confirmed the e ff ectiveness of our approach using real Acknowledgment: This work was supported by a grant of RGC (No. 419008, No. 419109), Hong Kong SAR, China.
