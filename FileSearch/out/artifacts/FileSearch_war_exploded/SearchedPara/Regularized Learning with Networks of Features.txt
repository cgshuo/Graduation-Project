 For many important problems in machine learning, we have a li mited amount of labeled training data and a very high-dimensional feature space. A common app roach to alleviating the difficulty of learning in these settings is to regularize a model by pena lizing a norm of its parameter vector. The most commonly used norms in classification, L parameters [1]. However, we often have access to informatio n about dependencies between param-eters. For example, with spatio-temporal data, we usually k now which measurements were taken at points nearby in space and time. And in natural language proc essing, digital lexicons such as Word-Net can indicate which words are synonyms or antonyms [2]. Fo r the biomedical domain, databases such as KEGG and DIP list putative protein interactions [3, 4 ]. And in the case of semi-supervised learning, dependencies can be inferred from unlabeled data [5, 6]. Consequently, we should be able to learn models more effectively if we can incorporate depen dency structure directly into the norm used for regularization.
 Here we introduce regularized learning with networks of fea tures, a framework for constructing cus-tomized norms on the parameters of a model when we have prior k nowledge about which parameters feature weights in a linear classifier. The prior knowledge i s encoded as a network or graph whose nodes represent features and whose edges represent similar ities between the features in terms of how likely they are to have similar weights. During learning, ea ch feature X  X  weight is penalized by the amount it differs from the average weight of its neighbors. T his regularization objective is closely connected to the unsupervised dimensionality reduction me thod, locally linear embedding (LLE), proposed by Roweis and Saul [7]. In LLE, each data instance is assumed to be a linear combina-tion of its nearest neighbors on a low dimensional manifold. In this work, each feature X  X  weight is preferred (though not required) to be a linear combination o f the weights of its neighbors. Similar to other recent methods for incorporating prior kno wledge in learning, our framework can be viewed as constructing a Gaussian prior with non-diagona l covariance matrix on the model pa-rameters [6, 8]. However, instead of constructing the covar iance matrix directly, it is induced from a network. The network is typically sparse in that each featu re has only a small number of neigh-bors. However, the induced covariance matrix is generally d ense. Consequently, we can implicitly construct rich and dense covariance matrices over large fea ture spaces without incurring the space explicitly.
 large amounts of unlabeled data. We show that regularizatio n with feature-networks derived from word co-occurrence statistics outperforms manifold regul arization and another, more recent, semi-supervised learning approach [5] on the task of text classifi cation. Feature network based regu-larization also supports extensions which provide flexibil ity in modeling parameter dependencies, allowing for feature dissimilarities and the introduction of feature classes whose weights have com-mon but unknown means. We demonstrate that these extensions improve classification accuracy on the task of classifying product reviews in terms of how fav orable they are to the products in question [11]. Finally, we contrast our approach with relat ed regularization methods. We assume a standard supervised learning framework in which we are given a training set of in-stances T = { ( x classifier parameterized by weight vector w  X  R d by minimizing a convex loss function l ( x , y ; w ) over the training instances, ( x number of labeled instances, n . Therefore, it is important to impose some constraints on w . Here features of our model and whose edges link features whose wei ghts are believed to be similar. The edges of G are non-negative with larger weights indicating greater si milarity. Conversely, a weight of zero means that two features are not believed a priori to be similar. As has been shown elsewhere statistics computed on unlabeled data. For the time being we assume that G is given and defer its construction until section 4, experimental work.
 The weights of G are encoded by a matrix, P , where P from vertex i to vertex j . We constrain the out-degree of each vertex to sum to one, P that no feature  X  X ominates X  the graph. Because the semantic s of the graph are that linked features should have similar weights, we penalize each feature X  X  wei ght by the squared amount it differs from the weighted average of its neighbors. This gives us the foll owing criterion to optimize in learning: where we have added a ridge term to make the loss strictly conv ex. The hyperparameters  X  and  X  specify the amount of network and ridge regularization resp ectively. The regularization penalty can be rewritten as w  X  M w where M =  X  ( I  X  P )  X  ( I  X  P )+  X  I . The matrix M is symmetric positive definite, and therefore our criterion possesses a Bayesian i nterpretation in which the weight vector, w , is a priori normally distributed with mean zero and covariance matrix 2 M  X  1 .
 Minimizing equation (1) is equivalent to finding the MAP esti mate for w . The gradient of (1) with respect to w is  X  our experiments X  X .e., it has only kd entries for k  X  d  X  X hen the matrix multiply is O ( d ) . Thus will typically be dense even though P is sparse, showing that we can construct dense covariance structures over w without incurring storage and computation costs. 2.1 Relationship to Locally Linear Embedding Locally linear embedding (LLE) is an unsupervised learning method for embedding high dimen-sional data in a low dimensional vector space. The data { ~ X sional manifold of dimension c within a high dimensional vector space of dimension d with c  X  d . Since the data lies on a manifold, each point is approximatel y a convex combination of its nearest neighbors on the manifold. That is, ~ X lie close to i on the manifold. As above, the matrix P has non-negative entries and its rows sum to one. The set of low dimensional coordinates, { ~ Y of squares cost: subject to the constraint that the { ~ Y to equation (2) is found by performing eigen-decomposition on the matrix ( I  X  P )  X  ( I  X  P ) = U  X  U  X  where U is the matrix of eigenvectors and  X  is the diagonal matrix of eigenvalues. The LLE coordinates are obtained from the eigenvectors, u smallest 1 by setting ~ Y clear that our feature network regularization penalty is id entical to LLE except that the embedding is found for the feature weights rather than data instances. However, there is a deeper connection. If we let L ( Y, X w ) denote the unregularized loss over the training set where X is the n  X  d matrix of instances and Y is the n -vector of class labels, we can express equation (1) in matri x form as we can view feature network regularization as: 1) finding an e mbedding for the features using LLE in which all of the eigenvectors are used and scaled by the inv erse square-roots of their eigenvalues (plus a smoothing term,  X I , that makes the inverse well-defined); 2) projecting the dat a instances onto these coordinates; and 3) learning a ridge-penalized m odel for the new representation. In using all of the eigenvectors, the dimensionality of the feature e mbedding is not reduced. However, in in the network regularized problem become the directions of maximum variance in the associated the effective dimensionality of the learning problem is red uced to the extent that the distribution of inverted eigenvalues is sharply peaked. When the best repr esentation for classification has high dimensional problems of section 4, we find that regularizati on with feature networks outperforms LLE-based regression. In this section, we pose a number of extensions and alternati ves to feature network regularization as formulated in section 2, including the modeling of classes o f features whose weights are believed to share the same unknown means, the incorporation of featur e dissimilarities, and two alternative regularization criteria based on the graph Laplacian. 3.1 Regularizing with Classes of Features In machine learning, features can often be grouped into clas ses, such that all the weights of the features in a given class are drawn from the same underlying d istribution. For example, words can be grouped by part of speech, by meaning (as in WordNet X  X  syns ets), or by clustering based on the words they co-occur with or the documents they occur in. Usin g an appropriately constructed feature graph, we can model the case in which the underlying distribu tions are believed to be Gaussians with known, identical variances but with unknown means. That is, the case in which there are k disjoint classes of features { C known and shared across all classes.
 The straight-forward approach to modeling this scenario mi ght seem to be to link all the features within a class to each other, forming a clique, but this does n ot lead to the desired interpretation. features, f as the estimates for the true but unknown means,  X  each feature to the virtual feature for its class with an edge of weight one. The virtual features, themselves, do not possess any out-going links.
 their weights are free to take on whatever values minimize th e network regularization cost in (1), in particular the estimates of the class means,  X  regularization penalty maximizes the log-likelihood for t he intended scenario. We can extend this construction to model the case in which the feature weights a re drawn from a mixture of Gaussians by connecting each feature to a number of virtual features wi th edge weights that sum to one. 3.2 Incorporating Feature Dissimilarities Feature network regularization can also be extended to indu ce features to have opposing weights. Such feature  X  X issimilarities X  can be useful in tasks such a s sentiment prediction where we would like weights for words such as  X  X reat X  or  X  X antastic X  to have opposite signs from their negated bigram counterparts  X  X ot great X  and  X  X ot fantastic, X  and from thei r antonyms. To model dissimilarities, we construct a separate graph whose edges represent anti-corr elations between features. Regularizing boring weights. To do this, we encode the dissimilarity grap h using a matrix Q , defined analogously to the matrix P , and add the term P like its similarity graph counterpart. Goldberg et al. [12] use a similar construction with the graph Laplacian in order to incorporate dissimilarities between instances in manifold learning. 3.3 Regularizing Features with the Graph Laplacian feature weights using a penalty derived from the graph Lapla cian [13]. Here, the feature graph X  X  edge weights are given by a symmetric matrix, W , whose entries, W between features i and j . The Laplacian penalty is 1 w  X  ( D  X  W ) w , where D = diag( W 1 ) is the vertex degree matrix. The main difference between the Laplacian penalty and the network penalty in equation (1) is that the Laplacian penalizes each edge equally (modulo the edge weights) whereas the network penal ty penalizes each feature equally. In graphs where there are large differences in vertex degree, t he Laplacian penalty will therefore focus most of the regularization cost on features with many neighb ors. Experiments in section 4 show that the criterion in (1) outperforms the Laplacian penalty as well as a related penalty derived from penalty assumes that p D should have similar weights. We evaluated logistic regression augmented with feature ne twork regularization on two natural lan-well-known document classification benchmark. The second w as sentiment classification of prod-uct reviews, the task of classifying user-written reviews a ccording to whether they are favorable or unfavorable to the product under review based on the review t ext [11]. Feature graphs for the two tasks were constructed using different information. For do cument classification, the feature graph was constructed using feature co-occurrence statistics gl eaned from unlabeled data. In sentiment prediction, both co-occurrence statistics and prior domai n knowledge were used. 4.1 Experiments on 20 Newsgroups We evaluated feature network based regularization on the 20 newsgroups classification task using all twenty classes. The feature set was restricted to the 11, 376 words which occurred in at least 20 documents, not counting stop-words. Word counts were trans formed by adding one and taking logs. To construct the feature graph, each feature (word) was repr esented by a binary vector denoting its presence/absence in each of the 20,000 documents of the data set. To measure similarity between features, we computed cosines between these binary vectors . Each feature was linked to the 25 other features with highest cosine scores, provided that th e scores were above a minimum threshold constructed by normalizing each vertex X  X  out-degree to sum to one.
 Figure 1 (left) shows feature network regularization compa red against five other baselines: logis-tic regression with an L each instance was projected onto the largest 200 right singu lar vectors of the n  X  d matrix, X ; LLE-logistic regression in which each instance was projected on to the smallest 200 eigenvectors of the and unnormalized graph Laplacians described in section 3.3 . Results at each training set size are averages of five trials with training sets sampled to contain an equal number of documents per class. For ridge, the amount of L Similarly, for feature network regularization and the Lapl acian regularizers, the hyperparameters  X  and  X  were chosen through cross validation on the training set usi ng a simple grid search. The ratio of  X  to  X  tended to be around 100:1. For PCR and LLE-logistic regressi on, the number of eigenvec-tors used was chosen to give good performance on the test set a t both large and small training set sizes. All models were trained using L-BFGS with a maximum of 200 iterations. Learning a sin-gle model took between between 30 seconds and two minutes, wi th convergence typically achieved before the full 200 iterations. The results in figure 1 show that feature network regularizat ion with a graph constructed from unla-beled data outperforms all baselines and increases accurac y by 4%-17% over the plain ridge penalty, an error reduction of 17%-30%. Additionally, it outperform s the related LLE regression. We conjec-ture this is because in tuning the hyperparameters, we can ad aptively tune the dimensionality of the underlying data representation. Additionally, by scaling the eigenvectors by their eigenvalues, fea-ture network regularization keeps more information about t he directions of least cost in weight space than does LLE regression, which does not rescale the eigenve ctors but simply keeps or discards them (i.e. scales them by 1 or 0).
 Figure 1 (right) compares feature network regularization a gainst two external approaches that lever-age unlabeled data: a multi-task learning approach called a lternating structure optimization (ASO), and our reimplementation of a manifold learning method whic h we refer to as  X  X ocal/global consis-tency X  [5, 10]. To make a fair comparison against the reporte d results for ASO, training sets were sampled so as not to necessarily contain an equal number of do cuments per class. Accuracies are given for the highest and lowest performing variants of ASO r eported in [5]. Our reimplementation of local/global consistency used the same document preproc essing described in [10]. However, the graph was constructed so that each document had only K = 10 neighbors (the authors in [10] use a fully connected graph which does not fit in memory for the ent ire 20 newsgroups dataset). Clas-sification accuracy of local/global consistency did not var y much with K and up to 500 neighbors were tried for each document. Here we see that feature networ k regularization is competitive with the other semi-supervised methods and performs best at all b ut the smallest training set size. 4.2 Sentiment Classification For sentiment prediction, we obtained the product review da tasets used in [11]. Each dataset con-sists of reviews downloaded from Amazon.com for one of four different product domains: books, DVDs, electronics, and kitchen appliances. The reviews hav e an associated number of  X  X tars, X  rang-ing from 0 to 5, rating the quality of a product. The goal of the task is to predict whether a review review. We performed two sets of experiments in which prior d omain knowledge was incorporated using feature networks. In both, we used a list of sentimenta lly-charged words obtained from the SentiWordNet database [14], a database which associates po sitive and negative sentiment scores to each word in WordNet. In the first experiment, we constructed a set of feature classes in the manner co-occurrence X  X  with the sentimentally charged words.
 From SentiWordNet we extracted a list of roughly 200 words wi th high positive and negative sen-timent scores that also occurred in the product reviews at le ast 100 times. Words to which Senti-WordNet gave a high  X  X ositive X  score were placed in a  X  X ositi ve words X  cluster and words given a high  X  X egative X  score were placed in a  X  X egative words X  clu ster. As described in section 3.1, all words in the positive cluster were attached to a virtual feat ure representing the mean feature weight representing the mean weight of the negative cluster words. We also added a dissimilarity edge (de-classes of features to have opposite means. As shown in figure 2, imposing feature clusters on the two classes of words improves performance noticeably while the addition of the feature dissimilarity edge does not yield much benefit. When it helps, it is only for th e smallest training set sizes. This simple set of experiments demonstrated the applicabil ity of feature classes for inducing groups of features to have similar means, and that the words extract ed from SentiWordNet were relatively helpful in determining the sentiment of a review. However, t he number of features used in these experiments was too small to yield reasonable performance i n an applied setting. Thus we extended the feature sets to include all unigram and bigram word-feat ures which occurred in ten or more reviews. The total number of reviews and size of the feature s ets is given in table 1. The method used to construct the feature graph in the 20 newsgroups experiments was not well suited for sentiment prediction since plain feature co-occurrence statistics tended to find groups of words that showed up in reviews for products of the same type, e.g., digital cameras or laptops. While such similarities are useful in predicting what type of product is being reviewed, they are of little help in determining whether a review is favorable or un-favorable. Thus, to align features along dimensions of  X  X entiment, X  we computed the correlations of all feature s with the SentiWordNet features so that each word was represented as a 200 dimensional vector of corr elations with these highly charged sentiment words. Distances between these correlation vect ors were computed in order to determine which features should be linked. We next computed each featu re X  X  100 nearest neighbors. Two fea-tures were linked if both were in the other X  X  set of nearest 10 0 neighbors. For simplicity, the edge weights were set to one and the graph weight matrix was then ro w-normalized in order to construct the matrix P . The number of edges in each feature graph is given in table 1.
 The  X  X itchen X  dataset was used as a development dataset in or der to arrive at the method for con-structing the feature graph and for choosing the hyperparam eter values:  X  = 9 . 9 and  X  = 0 . 1 . stances. The results show that linking features which are si milarly correlated with sentiment-loaded words yields improvements on every dataset and at every trai ning set size. Most similar to the work presented here is that of the fused la sso (Tibshirani et al. [15]) which can be interpreted as using the graph Laplacian regularizer but with an L residuals of weight differences: P discuss, an L vector of weight differences is sparse. L equal, but in many settings, features are near copies of one another whose weights should be similar rather than identical. Thus in these settings, penalizing s quared differences rather than absolute ones is more appropriate. Optimizing L optimization problem, making it less applicable in large sc ale learning. Li and Li [13] regularize feature weights using the normalized graph Laplacian in the ir work on biomedical prediction tasks. As shown, this criterion does not work as well on the text pred iction problems considered here. Krupka and Tishby [8] proposed a method for inducing feature -weight covariance matrices using distances in a  X  X eta-feature X  space. Under their framework , two features positively covary if they are close in this space and approach independence as they gro w distant. The authors represent each feature i as a vector of meta-features, u matrix, C graph or metric space, is application dependent. However, i t is less obvious how to incorporate a nearest neighbors graph, the induced covariance matrix, C , need not be sparse. Thus, working with entries of this covariance matrix are learned jointly with a regression model for predicting feature weight covariances as a function of meta-features. However , since their approach explicitly predicts each entry of the covariance matrix, they are restricted to l earning smaller models, consisting of hundreds rather than tens of thousands of features. We have presented regularized learning with networks of fea tures, a simple and flexible framework are modeled using a feature graph and the weight of each featu re is preferred to be close to the average of its neighbors. On the task of document classificat ion, feature network regularization is superior to several related criteria, as well as to a manif old learning approach where the graph models similarities between instances rather than between features. Extensions for modeling feature classes, as well as feature dissimilarities, yielded benefi ts on the problem of sentiment prediction.
