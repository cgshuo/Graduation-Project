 REGULAR PAPER Ta o L i  X  Shenghuo Zhu  X  Mitsunori Ogihara Abstract Many supervised machine learning tasks can be cast as multi-class clas-sification problems. Support vector machines (SVMs) excel at binary classifi-cation problems, but the elegant theory behind large-margin hyperplane cannot be easily extended to their multi-class counterparts. On the other hand, it was shown that the decision hyperplanes for binary classification obtained by SVMs are equivalent to the solutions obtained by Fisher X  X  linear discriminant on the set of support vectors. Discriminant analysis approaches are well known to learn dis-criminative feature transformations in the statistical pattern recognition literature and can be easily extend to multi-class cases. The use of discriminant analysis, however, has not been fully experimented in the data mining literature. In this paper, we explore the use of discriminant analysis for multi-class classification problems. We evaluate the performance of discriminant analysis on a large col-lection of benchmark datasets and investigate its usage in text categorization. Our experiments suggest that discriminant analysis provides a fast, efficient yet accu-rate alternative for general multi-class classification problems.
 Keywords Multi-class classification  X  Discriminant analysis 1 Introduction Classification tasks aim to assign a predefined class to each instance. It can help to understand existing data and be used to predict how new instances will behave. The problem can be treated as a regression problem which be formally defined as follows: given a set of training samples, in the form of x i , y i ,the goal is to learn an approximate function  X  f of the underlying function f , such that y = f ( x tween the real targets, where numeric values, 1 and 0, represent logic values, true and false, respectively, and the estimated values, which is the score of being true. Though the distribution of the errors is neither normal nor individually indepen-dent, we treat them as noise for simplicity. Generally each sample is represented as a multi-dimensional vector and the function f takes values from a discrete set of  X  X lass labels X : { c 1 , c 2 ,..., c n } . In cases when n = 2, i.e., there are only two possible values for f , the classification problems are referred as binary classifi-cation problems. We can use the n estimated scores to decide which one is real true case. Most machine learning algorithms were devised first for binary classi-fication problems. On the other hand, many real-word problems have more than two classes to deal with. Typical examples include optical character recognition (OCR), object and gesture recognition, part-of-speech tagging, text categorization and microarray data analysis.
 nary classification tasks. They are accurate, robust and quick to apply to test in-stances. However, the elegant theory behind the use of large-margin hyperplanes cannot be easily extended to multi-class classification problems. A number of reduction approaches such as one-versus-the-rest method [ 6 ], pairwise compar-ison [ 25 ], direct graph traversal [ 44 ], error-correcting output coding [ 1 , 12 ], and multi-class objective functions [ 56 ] have been proposed to first reduce a multi-class problem to a collection of binary-class problems and then combine their predictions in various ways. In practice, the choice of reduction method from multi-class to binary is problem-dependent and not a trivial task since each re-details, these reduction techniques are not well suited for classification problems with a large number of categories because SVMs, while accurate and fast to ap-time of a new instance also increases significantly when the number of classes becomes larger. Hence, despite the theoretical elegance and superiority of SVMs, the training/prediction time requirement and scaling are great concerns. ture transformations in the statistical pattern recognition literature and have been successfully used in many recognition tasks [ 17 ]. Fisher discriminant analysis [ 15 ] finds a discriminative feature transform as eigenvectors of matrix T =  X  where  X  w is the intra-class covariance matrix and  X  b is the inter-class covariance matrix. Basically T captures both compactness of each class and separations be-tween classes and hence eigenvectors corresponding to largest eigenvalues of T would constitute a discriminative feature transform. Shashua [ 52 ] showed that the decision hyperplanes for binary classification obtained by SVMs is equivalent to the solution obtained by Fisher X  X  linear discriminant on the set of support vectors. For the multi-class cases, such equivalence is not clear. Liner discriminant analysis approach is similar to Gaussian Processes [ 3 , 20 ] in the way of inference from the covariance matrix of training data. Also, Gallinari et al. [ 18 ] showed that neural network classifiers are equivalent to discriminant analysis. Fisher X  X  discriminant analysis was first described for two-class cases [ 15 ], and can be easily extended to multi-class cases via multiple discriminant analysis [ 30 ]. In fact, discriminant analysis has been widely used in face recognition [ 17 ]. These observations hint that discriminant analysis could be very promising for multi-class classification tasks. However, there is little investigation on the benchmark datasets in machine learning and data mining.
 criminant analysis for multi-class classification. We evaluate the performance of discriminant analysis on a large collection of benchmark datasets and investigate its usage in text categorization. Our study shows that discriminant analysis has several favorable properties: first, it is simple and can be easily implemented; sec-ond, it is efficient and most of our experiments only took a few seconds; last but not the least, it also has comparable accuracy in performance on most of the datasets we experimented. The rest of the paper is organized as follows: Section 2 reviews the related work on multi-class classification. Section 3 gives a brief overview of Linear Discriminant Analysis (LDA). Section 4 discusses some of the issues in discriminant analysis. Section 5 shows our experimental results on a variety of benchmark data sets. Section 6 presents the case study of using LDA for text categorization, and finally Sect. 7 provides our conclusions. 2 Related work Generally speaking, multi-class classification approaches can be roughly parti-tioned into two groups. The first group consists of those algorithms that can be naturally extended to handle multi-class cases. This group contains such algorithm as nearest neighborhoods [ 26 ], regression and decision trees including C4.5 [ 45 ] and CART [ 7 ]. The second group consists of methods that involve reduction of multi-class classification problems to binary ones. Depending on the reduction technique that is used, the group can be further divided into one-versus-the-rest error-correcting output coding [ 1 , 12 ], multi-class objective functions [ 56 ]. sifier, first construct a set of binary classifiers C 1 , C 2 ,..., C K . Each binary clas-sifier is first trained to separate one class from the rest and then the multi-class classification is carried out according to the maximal output of the binary classi-fiers. Since the binary classifiers are obtained by training on different binary clas-sification problems, it is unclear whether their real-valued outputs (before thresh-olding) are on comparable scales [ 50 ]. In practice, however, situations often arise where several binary classifiers assign the same instance to their respective class (or where none does). In addition, binary one-versus-the-rest classifiers has been criticized for dealing with rather asymmetric problems [ 50 ].
 For K classes, this results in ( K  X  1 ) K / 2 binary classifiers. Given a new instance, the multi-class classification is then executed by evaluating all ( K  X  1 ) K / 2in-dividual classifiers and assigning the instance to the class which gets the highest number of votes. Basically the individual classifiers used in pairwise comparison have smaller training sets comparing with the one-versus-the-rest method. Also the individual classifiers are usually easier to be learned since the classes have less overlap. However, pairwise comparison implies a large number of individual classifiers, especially for datasets with lots of classes.
 training phase of the direct graph traversal is the same as that of pairwise com-parison by building ( K  X  1 ) K / 2 individual classifiers. To classify new instances, however, direct graph traversal method uses a rooted binary directed acyclic graph which has ( K  X  1 ) K / 2 internal nodes and K leaves. Each classification run then corresponds to a directed traversal of the graph and classification can be much faster. Hsu and Lin [ 27 ] compared the performance of three methods for multi-class support vector machine: one-versus-the-rest, pairwise comparison and direct graph traversal. Their experiments indicated that the performance of three meth-ods are very similar and no one method is statistically better the others. Bakiri [ 12 ]. In a nutshell, the idea here is to generate a number of binary clas-sification problems by smartly splitting the original set of classes into two sets. In other words, each class is assigned a unique binary strings of length l (these strings are regarded to codewords). Then l classifiers are trained to predict each bit of the string. For new instances, the predicted class is the one whose codeword is the closest (in Hamming distance) to the codeword produced by the classifiers. Allwein et al. [ 1 ] extended ECOC and presented a general framework that unifies methods of reducing multi-class to binary including one-versus-the-rest, pairwise comparison and ECOC. Allwein et al. [ 1 ] also gave the loss-based coding scheme which takes margins into consideration and is more sophisticated and efficient than Hamming coding. In addition, it presented experimental results with a vari-ety of multi-to-binary-class reductions and demonstrated that although loss-based coding is better than Hamming coding in most cases, the best method seems to be problem-dependent.
 classification by solving one single optimization problem. The idea is to directly modify the objective function of support vector machine (SVM) in such a way that it allows simultaneous computation of a multi-class classifier. In terms of accuracy, the results obtained by this approach are comparable to those obtained by the widely used one-versus-the-rest method. But, the multi-class objective function has to deal with all the support vectors at the same time and hence lead to long training time.
 problem-dependent and not a trivial task. Crammer and Singer [ 10 ] discussed the problem of designing output codes for multi-class classification problems. Dis-criminant analysis is a direct method for multi-class classification and it does not require reducing multi-class to binary.
 veloped recently. Park et al. [ 43 ] presented an algorithm for dimensional reduction for text representation based on cluster structure preserving projection using gen-eralized singular value decomposition (GSVD). Godbole et al. [ 21 ] presented a new technique for multi-class classification by exploiting the accuracy of SVMs and the speed of Naive Bayes (NB) classifiers. The new technique first utilized a NB classifier to quickly compute a confusion matrix which is used to reduce the number and complexity of the two-class SVMs that are built in the second stage. More literature on multi-class classification and its applications can be found in [ 11 , 19 , 23 , 33 , 46 , 48 , 60 ].
 multi-class approach generally outperforms the others. For practical problems, the choice of approach will depend on constraints on hand such as required accuracy, the time available for development and training and the nature of the classification problem. The simple, efficient and accurate discriminant analysis provides a good choice for practical multi-class classification problems. 3 Linear discriminant analysis (LDA) In this paper, we focus on linear transformation since linear discriminant analysis frequently achieves good performances in the tasks of face and object recogni-tion, even though the assumptions of common covariance matrix among groups and normality are often violated [ 13 ]. In addition, kernel tricks can be used with linear discriminant analysis for non-linear transformation [ 39 ]. The basic idea of LDA is to find a linear transformation that best discriminate among classes and the classification is then performed in the transformed space based on some met-ric such as Euclidean distance. Mathematically a typical LDA implementation is carried out via scatter matrix analysis [ 17 ]. 3.1 Two-class LDA Fisher [ 15 ] first introduced LDA for two classes and its idea was to transform the multivariate observations x to univariate observations y such that the y  X  X  derived from the two classes were separated as much as possible. Suppose that we have asetof mp -dimensional samples x 1 , x 2 ,..., x m (where x i = ( x i 1 ,..., x ip ) ) belonging to two different classes, namely c 1 and c 2 . For the two classes, the scatter matrices are given as where  X  x i = 1 m intra-class scatter matrix is given by The inter-class scatter matrix is given by Fisher X  X  criterion suggested the linear transformation to maximize the so-called Rayleigh coefficient , that is, the ratio of the determinant of the inter-class scatter matrix of the projected samples to the intra-class scatter matrix of the projected samples: If  X  w is non-singular, Eq. ( 3 ) can be solved as a conventional eigenvalue problem and is given by the eigenvectors of matrix  X   X  1 w  X  b . 3.2 Multi-class LDA If the number of classes are more than two, then a natural extension of Fisher Lin-ear discriminant exists using multiple discriminant analysis [ 30 ]. As in two-class case, the projection is from high dimensional space to a low dimensional space and the transformation suggested still maximize the ratio of intra-class scatter to the inter-class scatter. But unlike the two-class case, the maximization should be done among several competing classes.
 ilar to Eq. ( 1 ): The inter-class scatter matrix slightly differs in computation and is given by where m i is the number of training samples for each class,  X  x i is the mean for each and  X  w , the linear transformation we want should still maximize Eq. ( 3 ). It can be shown that the transformation can be obtained by solving the generalized eigenvalue problem: tively m  X  n and n  X  1. Multiple discriminant analysis provides an elegant way for classification using discriminant features. 3.3 Classification Once the transformation is given, the classification is then performed in the transformed space based on some distance metric, such as Euclidean distance d ( x , y ) = i ( x i  X  y i ) 2 and cosine measure d ( x , y ) = 1  X  i x i y i upon the arrival of the new instance z , it is classified to where  X  x k is the centroid of k -th class.
 4 Discussions on LDA 4.1 Fisher criterion and its non-optimality Although practical evidences have been shown that discriminant analysis is ef-fective (and also will be demonstrated in our experimental study in Sect. 5 ), it should be pointed out that a significant separation does not necessarily imply a good classification. Multi-class discriminant analysis is concerned with the search for a linear transformation that reduces the dimension of a given p -dimensional statistical model, consisting of n classes, to n  X  1 dimensions while preserving a maximum amount of discriminant information in the lower-dimensional model. It is in general too complicated to use the Bayes error directly as a criterion and Fisher X  X  criterion is just a suboptimal criterion and easy to optimize. The solu-tion of optimizing Fisher X  X  criterion is obtained by an eigenvalue decomposition of  X   X  1 w  X  b and taking the rows of the transformation matrix to equal to the n  X  1 eigenvectors corresponding the n  X  1 largest eigenvalues. It has been shown that, however, for multi-class problem, the fisher X  X  criterion is actually maximizing the mean squared distance between the classes in the lower-dimensional space and is clearly different from minimizing classification error [ 35 ]. In maximizing the squared distances, pairs of classes, between which there are large distances, com-pletely dominate the eigenvalue decomposition. The resulting transformation pre-serves the distances of already well separated classes. As a consequence, there is a large overlap among the remaining classes, leading to an overall low and subop-timal classification rate. 4.2 When the inner scatter is singular There are at most n  X  1 nonzero generalized eigenvector of  X   X  1 w  X  b , and so an up-per bound of dimensions d in the transformed space is n  X  1. At least p + n samples are required to guarantee that  X  w does not become singular. In practice, especially in text categorization and pattern recognition, where the number of samples m is small and/or the dimensionality p is large,  X  w is usually singular. To deal with the singularity of  X  w , several methods have been proposed:  X  Regularization . In regularization method [ 38 , 61 ], the matrix  X  w is regularized  X  Subspace . The subspace method uses a non-singular intermediate space of  X  w  X  Null space method . Instead of discarding the null space of  X  w , null space 4.3 Computational complexity The computational complexity of training time consists of evaluating the inner and between covariance matrices, eigenvalue decomposition, and selecting discrimi-nating features. The computational complexity of evaluating the covariance ma-trices is mp 2 . If we use SVD to decompose the matrices, we can directly use the feature vectors and avoid the multiplications in evaluating covariance matrices. The computational complexity of eigenvector decomposition is mp min ( m , p ), where m and p are the number of rows and the number of columns of the given matrix. In our case, m and p are the number of instances and the number of fea-tures respectively. The time of selecting discriminating features is almost linear to p . Therefore, the total time is about O ( mp min ( m , p )) . Since we only need the largest n  X  1 eigenvalues and corresponding eigenvectors, the complexity could be even lower if the feature vectors are sparse. 5 Experiments on benchmark datasets 5.1 Data description We used a wide range of data sets in our experiments as summarized in Table 1 . The number of classes ranges from 3 to 100, the number of samples ranges from 72 to 581,012 and the number of attributes ranges from 8 to 12,558. In addition, theses data sets represent applications from different domains such as image pro-cessing, gene expression data and pattern recognition. We anticipated that these data sets provide us enough insights on the behavior of LDA.
 dataset contains measurements corresponding to ALL (B-cell and T-cell) and AML samples from Bone Marrow and Peripheral Bloodies Blood. 1 It was first studied in [ 22 ] for binary class classification. ALL (acute lymphoblastic leukemia) dataset is used to classify subtypes of pediatric acute lymphoblastic leukemia and the data has been divided into six diagnostic groups. 2 DNA dataset is used to rec-ognize, given a sequence of DNA, the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA se-quence that are spliced out). 3 Coil-100 dataset consists of color images of 100 objects where the images of objects that were taken at a pose interval of 5  X  , i.e., 72 poses per object. 4 All the other datasets are from UCI repository [ 4 ]. test sets. For the other datasets, if not specified, we used 10-fold cross validation method to evaluate results.
 the singularity of inner scatter matrix. For regularization method, we set  X  equal to 2 times the average of the diagonal components of the inner scatter matrix [ 31 ]. When the inner matrix is singular, the results reported to be the best one obtained by the three methods. 5.2 Data preprocessing There are four types of data values, continuous, binary, ordered, and categorical. Discriminant analysis algorithms are originally designed for continuous valued data sets. However, with simple preprocessing, it can be used on any type of data sets too. Values of a binary valued attribute can be translated into 0 and 1. Values of an ordered valued attribute can be translated into natural numbers according to their order.
 attributes as its cardinality, each of which represents whether a value belongs to the corresponding category of the original attribute. For example, suppose that an named A , B ,and C . Attribute A takes value 1 if the original attribute takes value A and 0 otherwise. This translation scheme was used in our experiments on Audi-ology dataset. It is possible to use a conversion with one dimension less than this conversion. We prefer this conversion because this conversion is symmetrical to all categorical values, and it is not as efficient to represent all categorical values in linear transformations without the additional dimension as in logic combination. Discriminant analysis can be easily extended to handle such datasets. For our ex-periments, in the training phase, a missing value was imputed with the mean of the existent attribute values in the same class. In predicting phase, the attributes with missing values were ignored. In other words, an attribute with missing value was imputed with the attribute mean value of each class when comparing with the class. 5.3 Results analysis In this section, we present and discuss our experimental results. All of our ex-periments were performed on a P4 2 GHz machine with 512 M memory running Linux 2.4.9 X 31.
 sented in [ 1 ] since most of them were regarded to be state-of-the-art. If we can not find corresponding results from [ 1 ], we compared our results with the docu-mented usage or other reported results. Finally, if both means are not available, we then compared the results on LDA with those obtained using our available implementations of classification methods.
 bean datasets, we compared our experimental results with those presented in [ 1 ] using the support vector machine algorithm as the base binary learner. There are two decoding schemes used: Hamming decoding and loss-based decoding. The authors pointed out, and it actually demonstrated by their experiments, that loss-based decoding is almost always better than Hamming coding. So our comparison was against the results of loss-based decoding. For loss-based decoding, the pa-per gave the results of five different types of output codes: one-versus-the-rest, pairwise comparison, complete code (in which there is one column for every pos-sible non-trivial split of the classes) and two types of random codes (dense code and sparse code). Although one-versus-the-rest is generally not as good as other four codes, it is the most widely used method. For all other four codes, there is no clear winners. However, because complete and pairwise comparison codes are not suitable for large datasets and in fact, the paper did not provide the results on these two codes for some datasets. Hence, for presentation purpose, we include only the results of sparse code and one-versus-the-rest for comparison. For more details on these codes, refer to [ 1 ]. For Isolet, Letter, Audiology and Segmenta-tion, [ 1 ] did not have the results using SVM as binary classifier since their SVM implementation could not handle those datasets. However, they gave the results on these datasets using AdaBoost as base binary learner. We compared our results on these datasets with those using AdaBoost and loss-based decoding.
 evaluated several state-of-the-art methods for constructing ensembles of classi-fiers with stacking. Stacking with Multi-response Model Tree (SMM5) achieved the best performance in their reported experiments. Therefore, we referred their results with SMM5 for comparison.
 results in [ 51 ]. For Covertype and Optdigits datasets, we compared our results with those documented results in [ 4 ]. For DNA, we compared our results with that from [ 41 ]. For Coil-100 dataset, we used the same experiment strategy as [ 47 ]. In this paper, we only present the result of an experiment using 1 / 9 of all the images for training, the rest for testing. For ALL-AML, Page-blocks, All, Zoo and water datasets, we compared the results on LDA with those obtained using our available implementations of SVM methods with one-against-all reductions. The detailed results are presented in Table 2 .
 Coil-100, outperform their counterparts. For Car, Heart, Segmentation, Covertype, Pendigits and Letter, LDA results are inferior to their counterparts. The main rea-son is that the number of attributes of these datasets is relatively low with respect to the size of large training sets. One possible solution is to increase dimensional-ity of datasets, such as using kernel functions. We will discuss some related issues in Sect. 7 . Other results are generally comparable.
 than a second. Table 3 gave the running time for experiments on the large datasets. In summary, the extensive experimental results on benchmark datasets have clearly demonstrated the efficiency and effectiveness of LDA. 6 LDA in text categorization Automated text categorization is a multi-class classification problem, defined as assigning pre-defined category labels to new documents based on the likelihood suggested by the training set of labeled documents. Naive Bayes has been a very successful practical learning method in text categorization despite its impracti-cal and simplified conditional independence assumptions [ 40 ]. Fisher linear dis-criminant analysis can be derived by starting with the theoretically optimal Bayes classifiers and assuming normal distribution for classes [ 26 ]. This suggests the applicability of LDA in text categorization.
 ysis domain. One reason is the extremely high dimensions of the document representation. Usually, a document collection would have thousands of terms. The intra-class covariance matrix is thus usually singular. In addition, the large document-term matrices incur considerable computation cost (eigen-analysis, in-verse computation, etc.) and hence restrict the usage of discriminant. Fortunately, a simple remedy exists: Feature Selection . It has been shown that feature selection via information gain can remove up to 90% or more of the unique terms without significant performance degrade [ 59 ]. With the feature selection, we then could explore the use of LDA in text categorization. 6.1 Text data description We used a wide range of text datasets in our experiments. Most of them are well-known in information retrieval literature. The number of classes ranges from 4 to 105 and the number of documents ranges from 476 to 20,000. ticles evenly divided among 20 Usenet newsgroups. The raw text takes up 26 MB. All words were stemmed using a porter stemmer, all HTML tags were skipped and all header fields except subject and organization of the posted article were ignored.
 computer science departments. There are about 8300 documents and they are di-vided into seven categories: student, faculty, staff, course, project, department and other. The raw text is about 27MB. Among these seven categories, student, faculty, course and project are four most populous entity-representing categories. The as-sociated subset is typically called WebKB4 . In this paper, we did experiments on both seven-category classification and four-category classification. In either case, we did not use stemming or a stoplist.
 by Market Guide Inc. consists of company homepages classified in a hierarchy of industry sectors. 6 In our experiments, we did not take the hierarchy into account and used a flattened version of the dataset. There are 9637 documents in the dataset divided into 105 classes. In tokenizing the data, we skipped all MIME and HTML headers, used a standard stoplist and did not perform stemming.
 uments collected from the Reuters newswire in 1987. It is a standard text catego-rization benchmark and contains 135 categories. In our experiments, we used two subsets of the data collection. The first one include the 10 most frequent categories among the 135 topics and we call it Reuters-top10. the second one contains the documents which have unique topic, i.e, the documents that have multiple class assignments were ignored, and we call it Reuters-2. There are about 9000 docu-ments and 50 categories.
 corpus version 3.2 released in December 6, 1999 [ 54 ]. The TDT2 corpus con-tains news data collected daily from nine news sources in two languages (Amer-ican English and Mandarin Chinese), over a period of six months (January X  X une, 1998). In our experiment, we used only the English news which were collected from New York Times Newswire Service, Associated Press Worldstream Service, Cable News Network, Voice of America, American Broadcasting Company and Public Radio International. The documents were judged relevant to any of 96 tar-get topics by manual annotation. We selected the documents having annotated topics and removed the brief news data. We finally got 7980 documents.
 in [ 5 ] for document clustering. The K-dataset contains 2340 documents consist-ing news articles from Reuters new service via the Web in October 1997. These documents were divided into 20 classes. The documents were processed by elim-inating stop words and HTML tags, stemming the remaining words using Porter X  X  suffix-stripping algorithm.
 lished by the computer science department at university of Rochester from year 1991 to 2002. 7 The dataset contains 476 abstracts that are divided into four differ-ent research areas: Symbolic-AI, Spatial-AI, Systems, and Theory. We processed the abstracts by removing stop words and applying stemming operations on the remaining words.
 6.2 Text data preprocessing In all our experiments, we randomly chose 70% of the documents for training and the remaining 30% for testing. As suggested in [ 59 ], information gain can be effective in term removal and it can remove up to 90% or more of the unique terms without performance degrade. Hence, we first selected the top 1000 words by information gain with class labels. 8 The feature selection is done with the rainbow package [ 37 ].
 Different measures such as precision-recall graphs and F 1 measure [ 58 ] have been used in literature. However, since the document datasets used in the experiments misclassification error and high separation between different classes on a test set, accuracy is a good performance for our purposes [ 49 ]. All of our experiments were performed on a P4 2 GHz machine with 512 M memory running Linux 2.4.9-31. 6.3 Experimental results on text datasets We compared LDA with Naive Bayes and Support Vector Machine on the exactly same datasets with the same training and test settings. SVMTorch [ 9 ] 9 is used for experiments involving SVMs. SVMTorch handles multi-class classification using one-versus-the-rest decomposition. The Naive Bayes classifier is built on the Bow (A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering) University. 10 For SVM, we used the linear kernel.
 datasets. SVM achieves the highest performance on WebKB4, WebKB, Reuters-2, K-dataset and TDT2. LDA achieves the best performance on 20Newsgroups and Industry Sector while NB has the highest performance on On Reuters-top 10 and CSTR. On 20Newsgroups, the performance of LDA is 93 . 90%, which is about 8% higher than that of Naive Bayes and 2% higher than SVM. The results of LDA and SVM are quite close to each other on WebKB4, WebKB, Industry Sector, Reuters-2 and TDT2. On Reuters-top 10 and K-dataset, LDA is beaten by Naive Bayes and SVM by about 10%. The comparisons show that, although there is no single winner on all the datasets, LDA is a viable and competitive algorithm in text categorization domain.
 Ta b l e 6 and Fig. 2 summarize the running time of all the experiments of LDA and SVM. The time saving of LDA is obvious. 7 Conclusions and future work In this paper, we investigate the use discriminant analysis for multi-class classi-fication. Our experiments have shown that LDA is a simple efficient yet accurate approach for multi-class classification problems. The precision of LDA approach is comparable to other approaches such as SVM, however, the time consumption of LDA approach is much less than other approaches.
 in the experiments, LDA performed poorly on several datasets, such as Letter, owing to the low dimensionality of the datasets. One possible solution to increase the dimensionality is introducing kernel functions.
 (GSVD) [ 34 ] to solve generalized eigenvalue problems. A Generalized Singular Value Decomposition (GSVD) is an SVD of a sequence of matrices and it brings several favorable computation properties such as stability for computation [ 2 ]. One promising direction is to explore the use of GSVD to improve the perfor-mance of LDA.
 transforms the original data into a low dimensional space determined by the n  X  1 different eigenvectors where n is the number of different classes. Understanding the interpretations of the LDA basis vectors could potentially lead to efficient dimension reduction. For example, it would be very interesting to study what are the original genes that the discriminating features correspond to in ALL-AML and ALL datasets.
 performance correlates with different feature selection methods and the number of words selected when applying LDA for text categorization.
 dom projection to reduce the dimensionality before applying discriminant anal-ysis [ 42 ].
 References Author Biographies
