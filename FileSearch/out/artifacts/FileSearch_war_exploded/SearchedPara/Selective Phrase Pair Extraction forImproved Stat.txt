 Phrase translation tables are the heart of phrase-based statistical machine translation (SMT) systems. They provide pairs of phrases that are used to con-struct a large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set. 1
The most widely used method for building phrase translation tables (Koehn et al., 2003) selects, from a word alignment of a parallel bilingual training cor-pus, all pairs of phrases (up to a given length) that are consistent with the alignment. This procedure typically generates many phrase pairs that are not re-motely reasonable translation candidates. 2 To avoid creating translations that use these pairs, a set of fea-tures is computed for each pair. These features are used to train a translation model, and phrase pairs that produce low scoring translations are avoided. In practice, it is often assumed that current translation models are good enough to avoid building transla-tions with these unreasonable phrase pairs.
In this paper, we question this assumption by in-vestigating methods for pruning low quality phrase pairs. We present a simple procedure that reduces the overall phrase translation table size while in-creasing translation quality. The basic idea is to initially gather the phrase pairs and train an trans-lation model as usual, but to then select a subset of the overall phrases that performs the best, prune the others, and retrain the translation model. In experi-ments, this approach reduced the size of the phrase tranlsation table by half, and improved the BLEU score of the resulting translations by up to 1.5 points. As a baseline, we present a relatively standard SMT approach, following Koehn et al. (2003). Potential translations are scored using a linear model where the best translation is computed as where s is the input sentence, t is the output sen-tence, and a is a phrasal alignment that specifies how t is constructed from s . The weights  X  i associated with each feature f i are tuned to maximize the qual-ity of the translations.

The training procedure starts by computing a word alignment for each sentence pair in the train-ing corpus. A word alignment is a relation between the words in two sentences where, intuitively, words are aligned to their translation in the other language. In this work, we use a discriminatively trained word aligner (Moore et al., 2006) that has state of the art performance. Figure 1 presents a high quality align-ment produced by this aligner.

Given a word aligned corpus, the second step is to extract a phrase translation table. Each entry in this table contains a source language phrase s , a target language phrase t , and a list of feature values  X  ( s,t ) . It is usual to extract every phrase pair, up to a cer-tain phrase length, that is consistent with the word alignment that is annotated in the corpus. Each con-sistent pair must have at least one word alignment between words within the phrases and no words in either phrase can be aligned any words outside of the phrases. For example, Figure 2 shows some of the phrase pairs that would be extracted from the word-aligned sentence pair in Figure 1. A full list using phrases of up to three words would include 28 pairs.
For each extracted phrase pair ( s,t ) , feature val-computed. The first two features, the log translation and inverse translation probabilities, are estimated by counting phrase cooccurrences, following Koehn et al. (2003). The third feature is the logarithm of a lexical score l ( s,t ) that provides a simple form of smoothing by weighting a phrase pair based on how likely individual words within the phrases are to be translations of each other. We use a version from Foster et al. (2006), modified from (Koehn et al., 2003), which is an average of pairwise word transla-tion probabilities.

In phrase-based SMT, the decoder produces trans-lations by dividing the source sentence into a se-quence of phrases, choosing a target language phrase Figure 2: Phrase pairs consistent with the word alignment in Figure 1. as a translation for each source language phrase, and ordering the target language phrases to build the fi-nal translated sentence. Each potential translation is scored according to a weighted linear model. We use the three features from the phrase translation ta-ble, summing their values for each phrase pair used in the translation. We also use four additional fea-tures: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above.

Finally, to estimate the parameters  X  i of the weighted linear model, we adopt the popular min-imum error rate training procedure (Och, 2003) which directly optimizes translation quality as mea-sured by the BLEU metric. In order to improve performance, it is important to select high quality phrase pairs for the phrase trans-lation table. We use two key ideas to guide selection:  X  Preferential Scoring: Phrase pairs are selected  X  Redundancy Constraints: Our intuition is
Selecting phrases that a translation model prefers and eliminating at least some of the ambiguity that comes with extracting multiple translations for a sin-gle phrase occurrence creates a smaller phrase trans-lation table with higher quality entries.

The ideal scoring metric would give high scores to phrase pairs that lead to high-quality translations and low scores to those that would decrease transla-tion quality. The best such metric we have available is provided by the overall translation model. Our scoring metric q ( s,t ) is therefore computed by first extracting a full phrase translation table, then train-ing a full translation model, and finally using a sub-part of the model to score individual phrase pairs in isolation. Because the scoring is tied to a model that is optimized to maximize translation quality, more desirable phrase pairs should be given higher scores.
More specifically, q ( s,t )=  X  ( s,t )  X   X  where  X  ( s,t ) is the length three vector that contains the feature values stored with the phrase pair ( s,t ) in the phrase translation table, and  X  is a vector of the three parameter values that were learned for these features by the full translation model. The rest of the features are ignored because they are either constant or de-pend on the target language sentence which is fixed during phrase extraction. In essence, we are using the subpart of a full translation model that looks at phrase pair identity and scoring the pair based on how the full model would like it.

This scoring metric is used in a phrase pair se-lection algorithm inspired by competitive linking for word alignment (Melamed, 2000). Local com-petitive linking extracts high scoring phrase pairs while enforcing a redundancy constraint that mini-mizes the number of phrase pairs that share a com-mon phrase. For each sentence pair in the training set, this algorithm marks the highest scoring phrase pair, according to q ( s,t ) , containing each source language phrase and the highest scoring phrase pair containing each target language phrase. Each of these marked phrase pairs is selected and the phrase translation table is rebuilt. This is a soft redundancy constraint because a phrase pair will only be ex-cluded if there is a higher scoring pair that shares its source language phrase and a higher scoring pair that shares its target language phrase. For example, consider again the phrase pairs in Figure 2 and as-sume they are sorted by their scores. Local compet-itive linking will select every phrase pair except for 27 and 28. All other pairs are the highest scoring options for at least one of their phrases.

Selective phrase extraction with competitive link-ing can be seen as a Viterbi reestimation algorithm. Because we are extracting fewer phrase pairs, the features associated with each phrase pair will differ. If the removed phrases were not real translations of each other in the first place, the translation features p ( s | t ) and p ( t | s ) should be better estimates because the high quality phrases that remain will be given the probability mass that was assigned to the pruned phrase pairs. Although we are running it in a purely discriminative setting, it has a similar feel to an EM algorithm. First, a full phrase translation table and parameter estimate is computed. Then, based on that estimate, a subset of the phrases is selected which, in turn, supplies a new estimate for the parameters. One question is how many times to run this reestima-tion procedure. We found, on the development set, that it never helped to run more than one iteration. Perhaps because of the hard nature of the algorithm, repeated iterations caused slight decreases in phrase translation table size and overall performance. In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT-NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). Phrase pairs are extracted from 500,000 word-aligned French-English sen-tence pairs. Translation quality is evaluated accord-ing to the BLEU metric (with one reference trans-lation). Three additional disjoint data sets (from the same source) were used, one with 500 sentence pairs for minimum error rate training, another with 1000 pairs for development testing, and a final set of 2000 sentence pairs for the final test. For each experiment, we trained the full translation model as described in Section 2. Each trial varied only in the phrase trans-lation table that was used. 3
One important question is what the maximum phrase length should be for extraction. To inves-tigate this issue, we ran experiments on the devel-opment set. Figure 3 shows a comparison of the full phrase table to local competitive linking as the maximum phrase length is varied. Local competi-tive linking consistently outperforms the full table and the difference in BLEU score seems to increase with the length. The growth in the size of the phrase translation table seems to be linear with maximum phrase length in both cases, with the table size grow-ing at a slower rate under local competitive linking.
To verify these results, we tested the model trained with the full phrase translation table against the model trained with the table selected by local competitive linking on the heldout test data. Both ta-bles included phrases up to length 7 and the models were tested on a set of 2000 unseen sentence pairs. The results matched the development experiments. The full system scored 26.78 while the local linking achieved 28.30, a difference of 1.52 BLEU points. The most closely related work attempts to create higher quality phrase translation tables by learning a generative model that directly incorporates phrase pair selection. The original approach (Marcu and Wong, 2002) was limited due to computational con-straints but recent work (DeNero et al., 2006; Birch et al., 2006) has improved the efficiency by using word alignments as constraints on the set of possible phrase pairs. The best results from this line of work allow for a significantly smaller phrase translation table, but never improve translation performance.
In this paper, we presented an algorithm that improves translation quality by selecting a smaller phrase translation table. We hope that this work highlights the need to think carefully about the qual-ity of the phrase translation table, which is the cen-tral knowledge source for most modern statistical machine translation systems. The methods used in the experiments are so simple that we believe that there is significant potential for improvement by us-ing better methods for scoring phrase pairs and se-lecting phrase pairs based those scores.

