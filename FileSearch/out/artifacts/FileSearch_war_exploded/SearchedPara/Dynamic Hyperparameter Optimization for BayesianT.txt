 This paper presents a new Bayesian topical trend analysis. We regard the parameters of topic Dirichlet priors in latent Dirichlet allocation as a function of document timestamps and optimize the parameters by a gradient-based algorithm. Since our method gives similar hyperparameters to the doc-uments having similar timestamps, topic assignment in col-lapsed Gibbs sampling is affected by timestamp similarities. We compute TFIDF-based document similarities by using a result of collapsed Gibbs sampling and evaluate our proposal by link detection task of Topic Detection and Tracking. Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning; H.3.3 [Information Storage and Re-trieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: Temporal Analysis, Topic Modeling, Topic De-tection
Bayesian approach is an outstanding trend in data min-ing. Especially, latent Dirichlet allocation (LDA) [4] leads to challenges in various fields [5][7][8][14]. In this paper, we focus on topical trend analysis and propose a new ap-proach, called Latent dYNnamically-parameterized Dirich-let Allocation (LYNDA), by regarding the parameters of topic Dirichlet priors in LDA as a function of document timestamps. In LYNDA, a topic multinomial for each doc-ument is drawn from a Dirichlet prior whose parameters are an instantiation of a per-topic function defined over document timestamps. However, if topic Dirichlet priors are markedly different among documents, we cannot fully take advantage of Bayesian approach, because too differ-ent per-document topic multinomials result in overfitting to word co-occurrence patterns local to each document. There-fore, we regard the parameters of topic Dirichlet priors as a smooth function of continuous timestamps. Consequently, documents having similar timestamps are generated based on similar Dirichlet priors and exhibit similar topic mixture.
LYNDA has an advantage in its simplicity. We can use collapsed Gibbs sampling (CGS) for LDA [6] as is and can estimate hyperparameters in the course of CGS by using an existing gradient-based method such as L-BFGS [11] [9].
CGS for LDA provides a topic assignment to all word to-kens and induces predictive word probabilities conditional on each document [16]. Since document similarities can be computed based on these probabilities, we compare LYNDA with competing methods by link detection task of Topic De-tection and Tracking (TDT), where better document simi-larity leads to a better result. Note that our approach may be applied to other probabilistic models where Dirichlet pri-ors are prepared for timestamped data.
In this paper, we focus on probabilistic topical trend anal-ysis. Dynamic Topic Models (DTM) [3] and its continuous time version (cDTM) [13] utilize transitions of the parame-ters of per-topic word multinomials for modeling document temporality, where the vectors drawn from time-dependent Gaussians are used to obtain multinomial parameters. How-ever, Gaussian is not a conjugate to multinomial. There-fore, the authors of [10] discuss that inference becomes com-plicated and propose Multiscale Topic Tomography Models (MTTM) based on a completely different idea. In MTTM, the time interval is segmented into two pieces recursively to obtain a binary tree representing the inclusion among subin-tervals. Further, each leaf node is associated with a Poisson distribution for word generation. However, MTTM cannot be applied to continuous timestamps. When compared with these works, LYNDA is remarkable in two features below.
First, inference is easy to implement. While LYNDA re-quires additional computations for hyperparameter estima-tion, we can use CGS for LDA as is and can adopt an exist-ing gradient-based method for hyperparameter estimation. In contrast, the above works require heavily customised im-plementation. Second, LYNDA can be applied to continuous timestamps. While cDTM has this feature, a special imple-mentation technique is required for efficient memory usage.
More appropriately, we can compare LYNDA with Top-ics over Time (TOT) [15], because TOT shares both above features. TOT extends LDA just by adding per-topic Beta distributions, which are defined over continuous timestamps.
However, these Beta distributions cause the following prob-lem. The full conditional probability that topic k is as-signed to i th token of document j in TOT is proportional  X (  X  ) denotes the standard gamma function, and  X  j is the timestamp of document j . The first half corresponds to the full conditional in LDA [6]. The latter corresponds to the likelihood of Beta distribution and means that the same topic is likely to be assigned to the word tokens appearing in the documents having similar timestamps. Our preliminary experiments reveal that this likelihood grows unboundedly as CGS proceeds, and that the first half comes to play only a marginal role. To solve this problem, we should keep the likelihood comparable with the first half. In our evaluation experiment, we multiply all Beta parameters a k , b k by the same constant and keep them less than a specific limit.
In contrast, LYNDA controls time-dependency of topic assignment not by augmenting LDA with probability dis-tributions without priors, but by making hyperparameters time-dependent to achieve a moderate fit to timestamp data. In LYNDA, we make topic Dirichlet priors time-dependent by defining p (  X  j | f 1 , . . . , f K )  X  j , where f k (  X  ) are per-topic differentiable functions defined over continuous timestamps. In this paper, we adopt unnor-malized Gaussian density and define f k (  X  )  X   X  k exp { X  (  X   X  m k ) 2 / (2 s 2 k ) } . We think that Gaussian density is appropriate to represent the dynamism of topical trends. Consequently, each topic Dirichlet prior is determined by the three pa-rameters. m k specifies the position of the peak popular-ity of topic k , s k tells how wide the popularity stretches, and  X  k represents the intensity of popularity. We introduce  X  to automatically rescale the unnormalized Gaussian den-sity separately for each topic k . These parameters are es-timated by empirical Bayes method. The log likelihood of a topic assignment can be obtained as log and s k with L-BFGS [11][9]. However, in the preliminary ex-periments, m k showed no significant change after optimiza-tion. Therefore, we fix m k to ( k  X  1) / ( K  X  1) by normalizing the timestamp domain to [0 , 1]. As a result, we can put an equal interval between the peaks of neighboring Gaussians and thus can cover the entire time interval impartially.
We compare LYNDA also with LDA where hyperparam-eters are optimized straightforwardly. The log likelihood of a topic assignment in LDA is obtained as log imizing the log likelihood. LDA with this straightforward hyperparameter estimation is denoted by HypLDA. We use TDT4 dataset [1] for our link detection evaluation. This dataset is accompanied with the on-topic document sets for 40 topics of TDT 2002 competition and those for 40 topics of TDT 2003 competition. From now, we say  X  X DT-topic X  to indicate the topics prepared for these TDT com-petitions. While an off-topic document set is also prepared for each TDT-topic, we regard all documents other than on-topic documents as off-topic to make evaluation reliable by using as many documents as possible. The entire dataset is used both to train probabilistic models and to compute doc-ument similarities. A similar strategy is taken when LDA-based models are evaluated in ad-hoc retrieval tasks [16]. If we split the dataset into a training and a test sets, we will face a difficulty in constructing new on-topic document sets based on this split, because distributions of on-topic docu-ments along time axis may be heavily modified. The dataset consists of J = 96 , 259 documents, W = 196 , 131 unique un-stemmed words, and 17,638,946 word tokens after removing stop words. We use the dates as document timestamps.
Our baseline method is TFIDF, because it is widely known that TFIDF is effective in TDT tasks [2][12]. We define the weight of word w in document j as n jw { log( J/J w ) +  X  log( n jw /n j ) } , where J w is the document frequency of word w , n jw is the term frequency of word w in document j , and n j is the length of document j . When  X  = 0 , we obtain the conventional TFIDF. While our weighting schema looks ad-hoc, only this could give results better than the conven-tional TFIDF. We set  X  = 0 . 4 and regard this as our baseline TFIDF, because other values give comparable or weaker re-sults. We adopt cosine measure for document similarity.
When we use a result of CGS, the above weighting schema is modified as n jw { log( J/J w )+  X  log p ( w | j )+  X  log( n where p ( w | j ) denotes the predictive probability of word w given document j . Based on preliminary experiments, we set  X  = 0 . 2 and  X  = 0 . 2. For LDA and HypLDA, we have p ( w | j ) =
P  X 
P where a normalization is required. In Section 2, we discuss that TOT suffers unbounded increase of the likelihood of per-topic Beta distributions. Therefore, for all Beta param-eters a k , b k , we put two types of limits: a k , b k  X  2 and a , b k  X  5. Correspondingly, we present evaluation results under the tags TOT2 and TOT5. Since the peak of Beta density is higher for larger parameters, TOT5 is more af-fected by timestamp data. We use these two limits, because smaller limits make TOT indistinguishable from LDA, and larger limits heavily degrade the overall performance. Fi-nally, for LYNDA, p ( w | j ) =
We compare LYNDA with five methods: TFIDF, LDA with fixed hyperparameters (  X  k = 0 . 5 for all k,  X  = 0 . 01), HypLDA, TOT2, and TOT5. The number of topics K is set to 100. While we tested K = 50 and 200, we only ob-tained worse results for K = 50 and comparable results for K = 200 . We prepare 30 results of CGS from different random initializations. The number of iterations of CGS is 500 for LDA, TOT2, and TOT5. In the course of CGS for HypLDA and LYNDA, we estimate hyperparameters by L-BFGS once for every 10 iterations. HypLDA and LYNDA require 1,000 iterations, because convergence is slow due to the incorporation of hyperparameter estimation.

Our evaluation is conducted as follows. Assume that the number of on-topic documents for a TDT-topic t is T t . By computing the similarities between each on-topic document and all documents, we have similarities for T t  X  J docu-ment pairs. Among these pairs, we call T t  X  T t pairs of two on-topic documents  X  X orrect X  and the rest  X  X ncorrect. X  We would like to approximate this ideal split by devising a doc-ument similarity and then by setting a similarity threshold to obtain a split. We introduce two functions of similar-ity threshold  X  . R t (  X  ) is the number of correct pairs whose similarities are larger than  X  . A t (  X  ) is the number of incor-rect pairs whose similarities are larger than  X  . Now we can define two evaluation measures: miss probability P Miss t  X  1  X  R t (  X  ) / ( T t  X  T t ) and false alarms probability P  X  A t (  X  ) / { T t  X  ( J  X  T t ) } . We also use Normalized Detection Cost (NDC) defined as P Miss t (  X  ) + 4 . 9  X  P FA t (  X  ) based on an intuition that false alarms are more unfavorable [2][12].
Figure 1 presents NDCs for the similarity thresholds from 0.040 to 0.055 with 0.001 step. The top panel gives NDCs averaged over 40 TDT-topics of TDT 2002, and further aver-aged over 30 results of CGS except for TFIDF. The bottom panel gives the results for TDT 2003. Based on Figure 1, we choose  X  = 0 . 04, because smaller values are advanta-geous to TDT 2002, and larger values are to TDT 2003. Figure 1 shows that TFIDF is efficient for TDT 2003. A detailed inspection reveals that TFIDF is likely to work for the TDT-topics whose topic explication includes character-istic words, e.g. for TDT-topic 41016 having an explication where the words  X  X asque X  and  X  X TA X  appear, and for 41021 whose explication includes the words X  X hana X  X nd X  X ufuor. X 
Based on Figure 1, we can conclude that hyperparameter estimation improves LDA, because both LYNDA and Hy-Figure 2: TDT-topics where our approach succeeds. pLDA work better than LDA. However, there seem no signif-icant differences between LYNDA and HypLDA. The differ-ences will later be revealed by inspecting several TDT-topics separately. Further, Figure 1 shows that TOT results in a weak performance. However, if we inspect TDT-topics sep-arately, we will know that TOT is comparable with LYNDA under a specific condition, and nevertheless that LYNDA is more robust with respect to overfitting to timestamp data.
In each of Figures 2 and 3, we select four TDT-topics and clarify detailed differences. Figure 2 presents the results for the TDT-topics where LYNDA succeeds, and Figure 3 for the TDT-topics where LYNDA fails. Each scatter graph is tagged with a TDT-topic ID and includes markers each of which corresponds to a pair of a false alarms probability (horizontal axis) and a miss probability (vertical axis). One cross marker is given for TFIDF, and 30 markers are for other methods, because we have 30 results of CGS except for TFIDF. The line graph on the left side of each scatter graph shows the number of on-topic documents at each date ranging from December 1 in 2000 to January 31 in 2001, where horizontal grids are put at unit interval.
First, we compare LYNDA with HypLDA and LDA. Fig-ures 2 and 3 provide the following observations. In these figures, we give the TDT-topics whose on-topic documents exhibit a distribution with one distinguished peak along time axis. However, smaller peaks are also observed. When the height of smaller peaks is far less than that of the largest peak, LYNDA succeeds (e.g. 40007 and 41026 in Figure 2). However, even when the height of smaller peaks is consider-ably less than that of the largest peak, LYNDA fails as long as some smaller peaks are placed far from the largest peak (e.g. 40026 and 41025 in Figure 3). When the height of smaller peaks is comparable with that of the largest peak, LYNDA fails (e.g. 40003 and 41014 in Figure 3). How-ever, even when the height of smaller peaks is comparable with that of the largest peak, LYNDA succeeds as long as smaller peaks are located near the largest peak (e.g. 40004 and 40038 in Figure 2). Based on these observations, we can draw a claim: For a pair of documents having similar timestamps, LYNDA works.

Second, to compare LYNDA with TOT2 and TOT5, we observe from Figure 2 that TOT achieves the results better than or comparable with LYNDA for many of the presented TDT-topics, e.g. 40007 and 41026. That is, TOT also works for a pair of documents having similar timestamps. How-ever, recall that Figure 1 shows weaker results for TOT, where NDCs are averaged over all TDT-topics. This means that TOT gives poor results for the TDT-topics which are not included in Figure 2, i.e., the TDT-topics whose on-topic documents do not show a concentrated timestamp distribu-tion. Therefore, we can draw another claim: For a pair of documents located far from each other along time axis, TOT is likely to give the results worse than other methods, though LYNDA can give the results at least comparable with others.
We can combine these two claims as follows: While TOT excessively favors the TDT-topics whose on-topic documents show a concentrated timestamp distribution, LYNDA receives such TDT-topics with moderate favor. In this sense, LYNDA helps us to prevent from overfitting to timestamp data.
In this paper, we give a new approach for using document timestamps in LDA-based document modeling. We clarify when our approach succeeds by discussing the correlation between timestamp similarities and document similarities by comparing the results of various competing methods. How-ever, we reveal differences between the methods only with respect to link detection task. Further investigation will be required to confirm the efficiency of our approach.
Figure 3: TDT-topics where our approach fails.
