 Visual category recognition is a vital thread in computer vi sion research. The recognition problem remains challenging because of the wide variation in appear ance a single class typically exhibits, as well as differences in viewpoint, illumination, and clutte r. Methods are usually most reliable when good training sets are available, i.e., when labeled image e xamples are provided for each class, and where those training examples are adequately representati ve of the distribution to be encountered at test time. The extent of an image labeling can range from a flag telling whether the object of interest is present or absent, to a full segmentation specifying the o bject boundary. In practice, accuracy often improves with larger quantities of training examples and/or more elaborate annotations. Unfortunately, substantial human effort is required to gat her such training sets, making it unclear how the traditional protocol for visual category learning c an truly scale. Recent work has begun to explore ways to mitigate the burden of supervision [1 X 8]. Whi le the results are encouraging, exist-ing techniques fail to address two key insights about low-su pervision recognition: 1) the division of labor between the machine learner and the human labelers o ught to respect any cues regarding which annotations would be easy (or hard) for either party to provide, and 2) to use a fixed amount of manual effort most effectively may call for a combination of annotations at multiple levels (e.g., a full segmentation on some images and a present/absent flag o n others). Humans ought to be re-sponsible for answering the hardest questions, while patte rn recognition techniques ought to absorb and propagate that information and answer the easier ones. M eanwhile, the learning algorithm must be able to accommodate the multiple levels of granularity th at may occur in provided image annota-tions, and to compute which item at which of those levels appears to be most fruitful to have labeled next (see Figure 1).
 To address this challenge, we propose a method that actively targets the learner X  X  requests for su-pervision so as to maximize the expected benefit to the catego ry models. Our method constructs an initial classifier from limited labeled data, and then consi ders all remaining unlabeled and weakly labeled examples to determine what annotation seems most in formative to obtain. Since the varying levels of annotation demand varying degrees of manual effor t, our active selection process weighs the value of the information gain against the cost of actuall y obtaining any given annotation. After each request, the current classifier is incrementally updat ed, and the process repeats. Our approach accounts for the fact that image annotations ca n exist at multiple levels of granularity: both the classifier and active selection objectives are form ulated to accommodate dual-layer labels. To achieve this duality for the classifier, we express the pro blem in the multiple instance learning (MIL) setting [9], where training examples are specified as b ags of the finer granularity instances, and positive bags may contain an arbitrary number of negativ es. To achieve the duality for the active selection, we design a decision-theoretic criterion that b alances the variable costs associated with each type of annotation with the expected gain in informatio n. Essentially this allows the learner to automatically predict when the extra effort of a more precis e annotation is warranted.
 The main contribution of this work is a unified framework to ac tively learn categories from a mixture of weakly and strongly labeled examples. We are the first to id entify and address the problem of active visual category learning with multi-level annotati ons. In our experiments we demonstrate two applications of the framework for visual learning (as hi ghlighted in Figure 1). Not only does our active strategy learn more quickly than a random selection b aseline, but for a fixed amount of manual resources, it yields more accurate models than conventiona l single-layer active selection strategies. The recognition community is well-aware of the expense of re quiring well-annotated image datasets. Recent methods have shown the possibility of learning visua l patterns from unlabeled [3, 2] image collections, while other techniques aim to share or re-use k nowledge across categories [10, 4]. Sev-eral authors have successfully leveraged the free but noisy images on the Web [5, 6, 11]. Using weakly labeled images to learn categories was proposed in [1 ], and several researchers have shown that MIL can accommodate the weak or noisy supervision often available for image data [11 X 14]. Working in the other direction, some research seeks to facil itate the manual labor of image annota-tion, tempting users with games or nice datasets [7, 8].
 However, when faced with a distribution of unlabeled images , almost all existing methods for vi-sual category learning are essentially passive, selecting points at random to label. Active learning strategies introduced in the machine learning literature g enerally select points so as to minimize the model entropy or reduce classification error (e.g., [15, 16] ). Decision-theoretic measures for tradi-tional (single-instance) learning have been explored in [1 7, 18], where they were applied to classify synthetic data and voicemail. Our active selection procedu re is in part inspired by this work, as it also seeks to balance the cost and utility tradeoff. Recent w ork has considered active learning with Gaussian Process classifiers [19], and relevance feedback f or video annotations [20].
 In contrast, we show how to form active multiple-instance le arners, where constraints or labels must be sought at multiple levels of granularity. Further, we int roduce the notion of predicting when to  X  X nvest X  the labor of more expensive image annotations so as to ultimately yield bigger benefits to the classifier. Unlike any previous work, our method continu ally guides the annotation process to the appropriate level of supervision. While an active criter ion for instance-level queries is suggested in [21] and applied within an MI learner, it cannot actively s elect positive bags or unlabeled bags, and does not consider the cost of obtaining the labels reques ted. In contrast, we formulate a gen-eral selection function that handles the full MIL paradigm a nd adapts according to the label costs. Experiments show this functionality to be critical for effic ient learning from few images. The goal of this work is to learn to recognize an object or cate gory with minimal human intervention. The key idea is to actively determine which annotations a use r should be asked to provide, and in what order. We consider image collections consisting of a va riety of supervisory information: some images are labeled as containing the category of interest (o r not), some have both a class label and a foreground segmentation, while others have no annotat ions at all. We derive an active learning criterion function that predicts how informative further a nnotation on any particular unlabeled image or region would be, while accounting for the variable expens e associated with different annotation types. As long as the information expected from further anno tations outweighs the cost of obtaining them, our algorithm will request the next valuable label, re -train the classifier, and repeat. In the following we outline the MIL paradigm and discuss its a pplicability for two important image classification scenarios. Then, we describe our decision-t heoretic approach to actively request useful annotations. Finally, we discuss how to attribute costs and risks for multi-level annotations. 3.1 Multiple-Instance Visual Category Learning Traditional binary supervised classification assumes the l earner is provided a collection of labeled data patterns, and must learn a function to predict labels on new instances. However, the fact that image annotations can exist at multiple levels of granulari ty demands a learning algorithm that can encode any known labels at the levels they occur, and so MIL [9 ] is more applicable. In MIL, the learner is instead provided with sets (bags) of patterns rather than individual patterns, and is o nly told that at least one member of any positive bag is truly positive, while every member of any negative bag is guaranteed to be negative. The goal of MIL is to induce the f unction that will accurately label individual instances such as the ones within the training ba gs.
 MIL is well-suited for the following two image classificatio n scenarios: In both cases, an instance-level decision is desirable, but bag-level labels are easier to obtain. While it has been established that MIL is valuable in such cases, pr evious methods do not consider how to determine what labels would be most beneficial to obtain.
 We integrate our active selection method with the SVM-based MIL approach given in [22], which uses a Normalized Set Kernel (NSK) to describe bags based on t he average representation of in-stances within them. Following [23], we use the NSK mapping f or positive bags only; all instances in a negative bag are treated individually as negative. We ch ose this classifier since it performs well in practice [24] and allows incremental updates [25]; f urther, by virtue of being a kernel-based algorithm, it gives us flexibility in our choices of features and kernels. However, alternative MIL techniques that provide probabilitistic outputs could eas ily be swapped in (e.g. [26, 24, 23]). 3.2 Multi-Level Active Selection of Image Annotations Given the current MIL classifier, our objective is to select w hat annotation should be requested next. Whereas active selection criteria for traditional supervis ed classifiers need only identify the best instance to label next, in the MIL domain we have a more comple x choice. There are three possible types of request: the system can ask for a label on an instance , a label on an unlabeled bag, or for a joint labeling of all instances within a positive bag. So, w e must design a selection criterion that simultaneously determines which type of annotation to requ est, and for which example to request it. Adding to the challenge, the selection process must also account for the variable costs associated with each level of annotation (e.g., it will take the annotat or less time to detect whether the class of interest is present or not, while a full segmentation will be more expensive).
 We extend the value of information (VOI) strategy proposed in [18] to enable active MIL selecti on, and derive a generalized value function that can accept both instances and bags. This allows us to predict the information gain in a joint labeling of multiple instances at once, and thereby actively choose when it is worthwhile to expend more or less manual eff ort in the training process. Our method continually re-evaluates the expected significance of knowing more about any unlabeled or partially labeled example, as quantified by the predicted re duction in misclassification risk plus the cost of obtaining the label.
 We consider a collection of unlabeled data X U , and labeled data X L composed of a set of positive bags X p and a set of negative instances  X  X n . Recall that positively labeled bags contain instances whose labels are unknown, since they contain an unknown mix o f positive and negative instances. Let r p denote the user-specified risk associated with misclassify ing a positive example as negative, and r n denote the risk of misclassifying a negative. The risk assoc iated with the labeled data is: where x i denotes an instance and X i denotes a bag. Here p ( x ) denotes the probability that a given input is classified as positive: p ( x ) = Pr( sgn ( w X  ( x ) + b ) = +1 | x ) for the SVM hyperplane pa-rameters w and b . We compute these values using the mapping suggested in [27] , which essentially fits a sigmoid to map the SVM outputs to posterior probabiliti es. Note that here a positive bag X i is first transformed according to the NSK before computing its p robability. The corresponding risk for unlabeled data is: where y i is the true label for unlabeled example x i . The value of Pr( y = +1 | x ) is not directly computable for unlabeled data; following [18], we approxim ate it as Pr( y = +1 | x )  X  p ( x ) . This simplifies the risk for the unlabeled data to: Risk ( X U ) = P x again we transform unlabeled bags according to the NSK befor e computing the posterior.
 The total cost T ( X L , X U ) associated with the data is the total misclassification risk , plus the cost of obtaining all labeled data thus far: where the function C (  X  ) returns the cost of obtaining an annotation for its input, an d will be defined in more detail below.
 To measure the expected utility of obtaining any particular new annotation, we want to predict the change in total cost that would result from its addition to X L . Thus, the value of obtaining an annotation for input z is: where z ( t ) denotes that the input z has been merged into the labeled set with its true label t , and X
U r z denotes that it has been removed from the set of unlabeled dat a. If the VOI is high for a given input, then the total cost would be decreased by adding its annotation; similarly, low values indicate minor gains, and negative values indicate an annot ation that costs more to obtain than it is worth. Thus at each iteration, the active learner surveys al l remaining unlabeled and weakly labeled examples, computes their VOI, and requests the label for the example with the maximal value. However, there are two important remaining technical issue s. First, for this to be useful we must be able to estimate the empirical risk for inputs before thei r labels are known. Secondly, for active selection to proceed at multiple levels, the VOI must act as a n overloaded function: we need to be able to evaluate the VOI when z is an unlabeled instance or an unlabeled bag or a weakly labeled example, i.e., a positive bag containing an unknown number o f negative instances.
 To estimate the total risk induced by incorporating a newly a nnotated example z into X L be-fore actually obtaining its true label t , we estimate the updated risk term with its expected value: hand for the expected value expression preceding it. If z is an unlabeled instance, then computing the expectation is straightforward: where L = { +1 ,  X  1 } is the set of all possible label assignments for z . The value Pr( sgn ( w X  ( z ) + b ) = l | z ) is obtained by evaluating the current classifier on z and mapping the output to the associ-ated posterior, and risk is computed based on the (temporari ly) modified classifier with z ( l ) inserted into the labeled set. Similarly, if z is an unlabeled bag, the label assignment can only be positiv e or negative, and we compute the probability of either label via the NSK mapping.
 If z is a positive bag containing M = | z | instances, however, there are 2 M possible labelings: L = { +1 ,  X  1 } M . For even moderately sized bags, this makes a direct computa tion of the expectation impractical. Instead, we use Gibbs sampling to draw samples of the label assignment from the joint distribution over the M instances X  descriptors. Let z = { z 1 , . . . , z M } be the positive bag X  X  instances, { +1 ,  X  1 } . To sample from the conditional distribution of one instanc e X  X  label given the rest X  X he basic procedure required by Gibbs sampling X  X e re-train the M IL classifier with the given labels added, and then draw the remaining label according to a j  X  Pr( sgn ( w X  ( z j ) + b ) = +1 | z j ) , where z denotes the one instance currently under consideration. Fo r positive bag z , the expected total risk is then the average risk computed over all S generated samples: where k indexes the S samples. To compute the risk on X L for each fixed sample we simply re-move the weakly labeled positive bag z , and insert its instances as labeled positives and negative s, as dictated by the sample X  X  label assignment. Computing the VOI values for all unlabeled data, espe-cially for the positive bags, requires repeatedly solving t he classifier objective function with slightly different inputs; to make this manageable we employ increme ntal SVM updates [25].
 To complete our active selection function, we must define the cost function C ( z ) , which maps an input to the amount of effort required to annotate it. This fu nction is problem-dependent. In the visual categorization scenarios we have set forth, we define the cost function in terms of the type of annotation required for the input z ; we charge equal cost to label an instance or an unlabeled bag , and proportionally greater cost to label all instances in a p ositive bag, as determined empirically with labeling experiments with human users. This reflects th at outlining an object contour is more expensive than naming an object, or sorting through an entir e page of Web search returns is more work than labeling just one.
 We can now actively select which examples and what type of ann otation to request, so as to maxi-mize the expected benefit to the category model relative to th e manual effort expended. After each annotation is added and the classifier is revised accordingl y, the VOI is evaluated on the remaining unlabeled and weakly labeled data in order to choose the next annotation. This process repeats ei-ther until the available amount of manual resources is exhau sted, or, alternatively, until the maximum VOI is negative, indicating further annotations are not wor th the effort. In this section we demonstrate our approach to actively lear n visual categories. We test with two distinct publicly available datasets that illustrate the t wo learning scenarios above: (1) the SIVAL dataset 1 of 25 objects in cluttered backgrounds, and (2) a Google data set ([5]) of seven categories downloaded from the Web. In both, the classification task is t o say whether each unseen image contains the object of interest or not. We provide compariso ns with single-level active learning (with both the method of [21], and where the same VOI function is use d but is restricted to actively label only instances), as well as passive learning. For the passiv e baseline, we consider random selections from amongst both single-level and multi-level annotation s, in order to verify that our approach does not simply benefit from having access to more informative pos sible labels. 2 To determine how much more labeling a positive bag costs rela tive to labeling an instance, we performed user studies for both of the scenarios evaluated. For the first scenario, users were shown oversegmented images and had to click on all the segments bel onging to the object of interest. In the second, users were shown a page of downloaded Web images and h ad to click on only those images containing the object of interest. For both datasets, their baseline task was to provide a present/absent flag on the images. For segmentation, obtaining labels on all positive segments took users on average four times as much time as setting a flag. For the Web images, it took 6.3 times as long to identify all positives within bags of 25 noisy images. Thus we set the c ost of labeling a positive bag to 4 and 6.3 for the SIVAL and Google data, respectively. These value s agree with the average sparsity of the two datasets: the Google set contains about 30% true positiv e images while the SIVAL set contains 10% positive segments per image. The users who took part in th e experiment were untrained but still produced consistent results. 4.1 Actively Learning Visual Objects and their Foreground R egions from Cluttered Images The SIVAL dataset [21] contains 1500 images, each labeled wi th one of 25 class labels. The clut-tered images contain objects in a variety of positions, orie ntations, locations, and lighting conditions. The images have been oversegmented into about 30 regions (in stances) each, each of which is rep-resented by a 30-d feature describing its color and texture. Thus each image is a bag containing both positive and negative instances (segments). Labels on the t raining data specify whether the object of interest is present or not, but the segments themselves are u nlabeled (though the dataset does provide ground truth segment labels for evaluation purposes).
 The initial training set is comprised of 10 positive and 10 ne gative images per class, selected at random. Our active learning method must choose its queries f rom among 10 positive bags (com-plete segmentations), 300 unlabeled instances (individua l segments), and about 150 unlabeled bags (present/absent flag on the image). We use a quadratic kernel with a coefficient of 10  X  6 , and average results over five random training partitions.
 Figure 2(a) shows representative (best and worst) learning curves for our method and the three baselines, all of which use the same MIL classifier (NSK-SVM) . Note that the curves are plotted against the cumulative cost of obtaining labels X  X s opposed to the number of queried insta nces X  since our algorithm may choose a sequence of queries with non -uniform cost. All methods are given a fixed amount of manual effort (40 cost units) and are allowed to make a sequence of choices until that cost is used up. Recall that a cost of 40 could correspond , for example, to obtaining labels on 1 = 40 instances or the learning curves for all categories, in terms of the avera ge improvement at a fixed point midway through the active learning phase.
 All four methods steadily improve upon the initial classifie r, but at different rates with respect to the cost. (All methods fail to do better than chance on the  X  X irty glove X  class, which we attribute to the lack of distinctive texture or color on that object.) In gene ral, a steeper learning curve indicates that a method is learning most effectively from the supplied labe ls. Our multi-level approach shows the most significant gains at a lower cost, meaning that it is best suited for building accurate classifiers with minimal manual effort on this dataset. As we would expec t, single-level active selections are better than random, but still fall short of our multi-level a pproach. This is because single-level active selection can only make a sequence of greedy choices while ou r approach can jointly select bags of instances to query. Interestingly, multi-and single-leve l random selections perform quite similarly on this dataset (see boxplots in (b)), which indicates that h aving more informative labels alone does not directly lead to better classifiers unless the right inst ances are queried.
 The table in Figure 3 compares our results to those reported i n [21], in which the authors train an initial classifier with multiple-instance logistic regression , and then use the MI Uncertainty (MIU) to actively choose instances to label. Following [21], we repo rt the average gains in the AUROC over all categories at fixed points on the learning curve, averagi ng results over 20 trials and with the same initial training set of 20 positive and negative images. Sin ce the accuracy of the base classifiers used by the two methods varies, it is difficult to directly compare the gains in the AUROC. The NSK-SVM we use consistently outperforms the logistic regressio n approach using only the initial training set; even before active learning our average accuracy is 68. 84, compared to 52.21 in [21]. There-fore, to aid in comparison, we also report the percentage gai n relative to random selection, for both classifiers. The results show that our approach yields much s tronger relative improvements, again illustrating the value of allowing active choices at multip le levels. For both methods, the percent gains decrease with increasing cost; this makes sense, sinc e eventually (for enough manual effort) a passive learner can begin to catch up to an active learner. 4.2 Actively Learning Visual Categories from Web Images Next we evaluate the scenario where each positive bag is a col lection of images, among which only a portion are actually positive instances for the class of in terest. Bags are formed from the Google-downloaded images provided in [5]. This set contains on aver age 600 examples for each of the seven categories. Naturally, the number of true positives for eac h class are sparse: on average 30% contain a  X  X ood X  view of the class of interest, 20% are of  X  X k X  quality (occlusions, noise, cartoons, etc.), and 50% are  X  X unk X . Previous methods have shown how to learn from noisy Web images, with results rivaling state-of-the-art supervised techniques [11, 5, 6 ]. We show how to boost accuracy with these types of learners while leveraging minimal manual annotati on effort.
 To re-use the publicly available dataset from [5], we random ly group Google images into bags of size 25 to simulate multiple searches as in [11], yielding ab out 30 bags per category. We randomly select 10 positive and 10 negative bags (from all other categ ories) to serve as the initial training data for each class. The rest of the positive bags of a class are use d to construct the test sets. All results are averaged over five random partitions. We represent each i mage as a bag of  X  X isual words X , and compare examples with a linear kernel. Our method makes acti ve queries among 10 positive bags (complete labels) and about 250 unlabeled instances (image s). There are no unlabeled bags in this scenario, since every downloaded batch is associated with a keyword. Figure 4 shows the learning curves and a summary of our active learner X  X  performance. Our multi-level approach again shows more significant gains at a lower c ost relative to all baselines, improving accuracy with as few as ten labeled instances. On this datase t, random selection with multi-level annotations actually outperforms random selection on sing le-level annotations (see the boxplots). We attribute this to the distribution of bags/instances: on average more positive bags were randomly chosen, and each addition led to a larger increase in the AURO C. Our approach addresses a new problem: how to actively choose not only which instance to label, but also what type of image annotation to acquire in a cost-effec tive way. Our method is general enough to accept other types of annotations or classifiers, as long a s the cost and risk functions can be appro-priately defined. Comparisons with passive learning method s and single-level active learning show that our multi-level method is better-suited for building c lassifiers with minimal human intervention. In future work, we will consider look-ahead scenarios with m ore far-sighted choices. We are also pursuing ways to alleviate the VOI computation cost, which a s implemented involves processing all unlabeled data prior to making a decision. Finally, we hope t o incorporate our approach within an existing system with many real users, like Labelme [8].

