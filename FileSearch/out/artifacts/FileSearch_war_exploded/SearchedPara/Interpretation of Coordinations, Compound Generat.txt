 We investigate interpreting coordinations (e.g. word sequences con-nected with coordinating conjunctions such as  X  X nd X  and  X  X r X  ) as logical disjunctions of terms to generate a set of disjunction-free query variants for information retrieval (IR) queries. In addi-tion, so-called hyphen coordinations are resolved by generating full compound forms and rephrasing the original query, e.g.  X  X ice im-and export X  is transformed into  X  X ice import and export X  . Query variants are then processed separately and retrieval results are merged using a standard data fusion technique. We evaluate the approach on German standard IR benchmarking data. The results show that: i) Our proposed approach to generate compounds from hyphen co-ordinations produces the correct results for all test topics. ii) Our proposed heuristics to identify coordinations and generate query variants based on shallow natural language processing (NLP) tech-niques is highly accurate on the topics and does not rely on parsing or part-of-speech tagging. iii) Using query variants to produce mul-tiple retrieval results and merging the results decreases precision at top ranks. However, in combination with blind relevance feedback (BRF), this approach can show significant improvement over the standard BRF baseline using the original queries.
 H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval X  Query formulation ; H.3.1 [ INFOR-MATION STORAGE AND RETRIEVAL ]: Content Analysis and Indexing X  Linguistic processing Query structure, Query variants, Compounds, Result set fusion
Information retrieval (IR) approaches typically treat queries as a  X  X ag of words X , disregarding any syntactic or logical structure. We investigate a novel approach to generate query variants by in-terpreting syntactic coordinations of clauses or chunks, indicated by coordinating conjunctions such as  X  X nd X  and  X  X r X  , as logical disjunctions. In addition, we propose a knowledge-light approach to resolve hyphen coordinations to generate full compounds from short (hyphenated) words used in these coordinations. Query vari-ants are generated by selecting only one conjunct per query variant (subquery). We merge results from processing query variants sep-arately using a standard data fusion technique.

The general idea for the work presented in this paper is to avoid a topic shift when a query puts too much emphasis on all conjuncts, which can occur when a query contains coordinations with many elements or many words. We hypothesize that firstly, resolving hy-phenated coordinations generates new compounds that better match actual index terms. For example, in the coordination  X  X eisim-und -export X  (rice im-and export),  X  X m- X  is not a proper word or might be recognized as a stopword. Our proposed approach to compound generation will rewrite this query into  X  X ice import and export X  . Secondly, processing subqueries focuses on retrieving relevant doc-uments for one aspect of the query. Coordination in queries may shift the focus of retrieved results, either if many less important words are enumerated (e.g.  X  X inde Artikel, Reporte, Daten und Dokumente  X ber Bildung X  (Find articles, reports, data, and docu-ments on education)), or when the coordination elements otherwise dominate other key aspects in a query. For example, the query  X  X n-formationen  X ber US-Beziehungen mit Brasilien, Russland, Indien und China X  (Information on US relations with Brazil, Russia, In-dia, and China) would aim for documents describing the relations between the US and other countries, but not the relations between China and Brazil . However, the bag-of-words approach disregards the structural information. Our proposed approach generates four subqueries containing the word pairs { US, Brazil }, { US, Russia }, { US, India }, and { US, China }. Results for the subqueries are then merged using a standard result fusion technique to retrieve docu-ments relevant to more than one aspect of the original query (i.e. more than one subquery) at top ranks. Thus, the final data fusion of results retrieved for the query variants would still result in ranking documents covering more than one aspect higher.

The rest of this paper is organized as follows: Section 2 dis-cusses related work. Section 3 introduces our proposed approaches to generate compounds and query variants. Section 4 presents our experimental setup and experimental results. Section 5 presents and analyzes the results, before concluding in Section 6.
The query understanding workshop at SIGIR 2010 1 is one exam-ple of an effort to investigate the structural analysis of IR queries.
Xue and Croft [14] propose to represent queries as sets of distri-butions, allowing to transform queries, which are associated with h ttp://ciir.cs.umass.edu/sigir2010/qru/ variations generated by applying operations such as adding o r re-placing words or detecting phrase structures, into a distribution of query reformulations. In comparison, the approaches described in this paper focus on symbolic query manipulation.

Jones et al. [8] investigate query substitutions, a method for gen-erating query suggestions. They propose to replace the original query using information about similar queries extracted from query logs. In contrast, our proposed approach does not rely on additional external query logs or on resources not freely available.
Data fusion of results typically involves multiple unrelated re-trieval approaches or retrieval models (see, for example [4]). The idea dates back to Fox and Shaw [5], who conducted experiments on TREC 2 data. Savoy [13] performed extensive experiments on CLEF 3 data and showed that combining results from different re-trieval models improves IR performance significantly. In contrast, the experiments in this paper are based on generating subqueries from the same original query and use a single retrieval model.
Compound analysis in IR traditionally focuses on decompound-ing words and has been shown to improve IR effectiveness for com-pounding languages such as Finnish, Dutch, or German [1, 2]. For decompounding words, we follow techniques as described by [10, 13], but extend the decomposition procedure to handle additional compound linking elements between compound constituents.
Hyphen coordination has been briefly described by Neumann et al. as part of a shallow NLP engine [11], but it typically requires deeper NLP techniques. Hartrumpf et al. [6] explore question de-composition for geographic questions. Their technique is based on a syntactic-semantic analysis which produces semantic networks. These are split up to generate subquestions, which are then pro-cessed separately and answers retrieved for subquestions are used to reformulate the original question.

Huston and Croft [7] explore reducing the length of verbose queries. Our proposed approach to identify coordinated compounds by resolving hyphenated coordinations adds new compounds to a query, but also transforms the original query into simpler and shorter subqueries. The approach proposed in this paper is com-pletely novel and requires  X  in contrast to the solutions above  X  no syntactic or semantic parsing or part-of-speech tagging. Only case information, stopwords, and part-of-speech information for closed word categories are employed. Our experiments focus on process-ing German, but, as the translated examples show, similar prob-lems occur in English and our proposed solution could be applied to other languages as well. The techniques proposed in this paper have  X  to the best of our knowledge  X  not been investigated for IR.
In this section, we briefly describe the approaches proposed to identify and process coordinations. In linguistics, a coordination is a syntactic structure that links together two or more elements (conjuncts). It is usually indicated by a coordinating conjunction (coordinator). The totality of coordinator(s) and conjuncts forming an instance of coordination is called a coordinate structure. Splitting Compounds. For decomposition of German words, we employ the general process proposed by Koehn and Knight [10]. This process considers each position in a word as a candidate split, where the probability of a split is based on the term frequency of the resulting candidate constituents in a document collection. The de-composition with the maximum probability is selected and the pro-cess is applied recursively to the constituents. In contrast to simpler approaches to decompounding (e.g. [3]), which handle only the h ttp://trec.nist.gov/ http://www.clef-initiative.eu/ most frequent linking elements such as  X +s X  , our compound split-ter allows a larger set of linking elements and also allows combina-tions of elisions and linking elements (e.g.  X -e+s X  :  X  X ietshaus X  (rental house) =  X  X iet(-e)+s+haus X  ). The minimum word length for compound constituents is 3. Table 1 shows examples for the linking elements and elisions handled by our compounds splitter. Table 2: Frequency distribution of hyphenated coordination s. Generating Compounds. I n German, common nouns and proper nouns are capitalized, which make them easy to distinguish from adjectives or verbs. For simplicity, we assume that coordinators are expressed as a single word. We employ a small set of coordinators for our experiments (e.g.  X  X nd X  (and),  X  X der X  (or),  X  X owie X  (as well as),  X / X  (/)). Other coordinators occur only rarely in the topics and are also likely to express contrast or negation (e.g.  X  X eder ... noch X  (neither ... nor)) and should thus be treated differently.
Queries are tokenized and tokens encompassing a coordinate struc-ture are returned as triples ( w l , w c , w r ), where w (e.g.  X  X nd X  ,  X  X r X  ) and w r , the word immediately right of w with a hyphen (type 1), or w l , the word immediately left of w with a hyphen (type 2), or both (type 3; see Table 2).

The compound generation process works as follows: The word not starting or ending with a hyphen is expected to be a compound and is split into its constituent parts. If this word can not be split, the triple is discarded. A new compound is generated by concatenating the hyphenated form (without hyphen) with the first or last con-stituent of the compound. For example,  X  X eisimport X  (rice import) is split into  X  X eis X  (rice) and  X  X mport X  (import). Concatenating the first constituent with the hyphenated form  X -export X  (export) yields  X  X eisexport X  (rice export), The combination of both types (type 3) is possible, but occurs very rarely (e.g.  X  X einan-und -abbau X  (wine growing and harvest)).

As an initial experiment, we analyzed the frequency of hyphen coordinations in a large German corpus of news articles from the Leipzig corpora collection 4 . We used corpus of 16M sentences h ttp://corpora.uni-leipzig.de/ from news articles from 1995-2010. Table 2 shows the frequenc y and examples of hyphenated coordinations. 1.5% of all sentences contain hyphen coordination, where 248,859 coordinations are of type 1, 3,186 of type 2, and only 311 of type 3. In contrast, 13 topics out of 300 queries (150 topic titles and descriptions each) in our test data exhibit hyphenated coordinations, all of type 1. As coordinations of type 3 are very rare, we chose to not extend our algorithm to handle this case. Our proposed algorithm handles all 13 cases correctly.
 Identifying Coordinations. For simplicity, we do not consider coordination of full sentences or complex phrases as these could express unrelated aspects of a query and seldomly occur in short queries. The main idea of our proposed approach is to identify the coordinate structures and recombine conjunct words to model correct syntactic attachment.

Our proposed approach aims at simplicity and speed and thus, does not rely on parsing or part-of-speech tagging. Let w ..., w n be the tokenized text. For each position i where w coordination, follow the algorithm outlined below.
In the GIRT 2003-2008 topics, there are 123 coordinations, 30 in the topics titles and 93 in the topic descriptions. Topics from 2006 have 23 or more cases per 25 topics (2006: 26, 2007: 23, 2008: 28), topics before 2006 have 17 cases or less (2003: 14, 2005: 17, 2006: 15). Thus, the effect of coordination processing on IR effectiveness should be higher for topics from 2006. A brief post-analysis of the results showed that a few cases in the topics were not handled correctly, due to ambiguity, syntactic complexity (e.g. nested coordinations), or coordination of full sentences. Generating Query Variants. Query variants are generated by ap-plying (in this order) hyphen coordination resolution, coordination resolution, and compound splitting on a query. For example, the query  X  X apans Reisim-und export X  (Japan X  X  rice im-and export) is first transformed into query  X  X apans Reisimport und Reisexport X  (Japan X  X  rice import and export) and processed into two query vari-ants  X  X apans Reisimport X  (Japan X  X  rice import) and  X  X apans Reis-export X  ( Japan X  X  rice export). For our experiments, we also add the original queries (title and description) as variants.
 Merging Results. To merge results retrieved for query variants, we employ the combMNZ approach [5] on document scores from retrieval results R 1 , ..., R k , after MinMax normalization. This is a standard data fusion technique used in previous IR research [4, 13].
We perform IR experiments on the German data for the GIRT-4 domain-specific task at CLEF (see, for example [9]), because we assumed that coordination occurs frequently in these topics. Our analysis later confirmed this assumption and showed that coordina-tions occur in 41% of queries generated from title and description (see Table 3). We used data from 2003 to 2008, which comprises 25 topics and their corresponding relevance assessments per year. The GIRT document collection contains more than 150,000 documents from the social sciences.

Queries are preprocessed by case folding, stopword removal, and stemming, using a light German stemmer [13]. Queries are trans-formed into query variants as described. We used the GIRT topic titles and description fields (TD) as queries. We employ the BM25 retrieval model with default parameters ( k 1 = 1 . 2 , b = 0 . 75 , k = 1000 ). We use standard blind relevance feedback (BRF) [12] with 10 feedback terms and 20 feedback documents, which corre-sponds to a conservative setting for BRF for this task. The BRF experiment serves as our baseline.

We perform four experiments per topic set, using standard BM25, its combination with BRF, and the corresponding experiments us-ing query variants obtained from interpreting coordinations as dis-junctions (QV). In addition, we performed a set of experiments using only queries containing at least one coordinator. We re-port mean average precision (MAP), GMAP (geometric MAP), the number of retrieved and relevant results (rel_ret), and precision at a cut-off of N (P@N). Results are shown in Table 4. Significance tests are based on the paired Wilcoxon test with 95% confidence level and compare results to the standard BRF to obtain a strong baseline. Significant improvements are indicated by  X * X  .
Compared to the official results of participants in the GIRT task at CLEF, our results rank among the top three for all years. The better performing systems in this task usually employed a much more complex system setup, e.g. by combining results from differ-ent retrieval models, or by using additional knowledge for domain adaptation. In contrast, we did not employ any additional domain Table 4: Evaluation results for German retrieval on TD fields. adaptation for this domain-specific task, as this paper focus es on processing coordination in queries.

Hyphen coordination is frequent in the topics and occurs in 41% of all tested topic titles and descriptions. Interestingly, the preci-sion and MAP of the initial retrieval run is is typically considerably lower when using query variants compared to the standard initial retrieval. However, the results for the subsequent BRF can outper-form results for the standard BRF. This indicates that the generated query variants are better at selecting better (not necessarily rele-vant) feedback documents. Using the combination of query vari-ants and BRF can produce significantly higher MAP compared to standard BRF, especially when the topics contain many coordina-tions (e.g. for the 2007 and 2008 topic set).

In addition, coordination might not correspond to logical dis-junction in all cases, but this view helps to generate shorter queries. For example, so-called twin pairs such as  X  X ag und Nacht X  (night and day), fixed expressions such as  X  X ehr oder weniger X  (more or less) and short title queries  X  X ewalt und Schule X  (violence and schools) should probably not be treated as disjunctions.
Coordinations are little researched in IR. We propose a simple method to detect coordinations and the corresponding chunks and transform them into query variants. Our results for query variants show a lower precision at top ranks. However, we showed that interpreting coordinations as logical disjunctions in combination with BRF can improve IR performance significantly compared to standard BRF.

The proposed method relies on case information to distinguish between adjectives and nouns which is available in German. To adapt this method to languages which do not capitalize nouns (such as English) or to languages which do not have cases at all (such as Chinese), more complex natural language processing such as part-of-speech tagging is required. The method can then be adapted to work with adjectives (taking the role of lowercase words) and nouns (uppercase words). For English, the GIRT topics showed a similar number of cases with coordinations which illustrates the importance of coordinations in other languages.

As part of future work, we want to apply coordination detec-tion to machine translation. Coordinations can span long distances, which cannot easily be captured by standard MT n -gram models. We expect that reformulating and simplifying the MT input and combining the translation results will increase MT quality. This research is supported by the Science Foundation of Ireland (grant 07/CE/I1142) as part of the Centre for Next Generation Lo-calisation ( http://www.cngl.ie/ ). [1] E. Airio. Word normalization and decompounding in mono-[2] M. Braschler and B. Ripplinger. How effective is stemming [3] A. Chen and F. C. Gey. Multilingual information retrieval [4] W. B. Croft. Combining approaches to information retrieval. [5] J. A. Fox and E. A. Shaw. Combination of multiple searches. [6] S. Hartrumpf and J. Leveling. Recursive question [7] S. Huston and W. B. Croft. Evaluating verbose query [8] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating [9] M. Kluck. The domain-specific track in CLEF 2004: [10] P. Koehn and K. Knight. Empirical methods for compound [11] G. Neumann and J. Piskorski. A shallow text processing core [12] S. E. Robertson, S. Walker, S. Jones, M. M.
 [13] J. Savoy. Report on CLEF-2003 monolingual tracks: fusion [14] X. Xue and W. B. Croft. Representing queries as
