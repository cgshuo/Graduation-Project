 Motivated by the insufficiency of the existing quasi-identifier/sensitive-attribute (QI-SA) framework on modeling real-world privacy re-quirements for data publishing, we propose a novel versatile pub-lishing scheme with which privacy requirements can be specified as an arbitrary set of privacy rules over attributes in the microdata table. To enable versatile publishing, we introduce the Guardian Normal Form (GNF), a novel method of publishing multiple sub-tables such that each sub-table is anonymized by an existing QI-SA publishing algorithm, while the combination of all published tables guarantees all privacy rules. We devise two algorithms, Guardian Decomposition (GD) and Utility-aware Decomposition (UAD), for decomposing a microdata table into GNF, and present extensive ex-periments over real-world datasets to demonstrate the effectiveness of both algorithms.
 H.2.8 [ Database Applications ]: Data Mining Security, Algorithms, Performance Privacy preservation, Versatile publishing, Guardian normal form, Decomposition
Privacy-preserving data publishing (PPDP) aims to publish a mi-crodata table for research and statistical analysis, without disclos-ing sensitive information at the individual level. A large number of such microdata tables are published regularly, especially in the healthcare industry. For example, the Texas Department of State  X 
Table 1: A microdata table of hospital patient discharge data Health Services publishes every year a table of patients discharged from more than 450 state-licensed hospitals [34]. Table 1 depicts an example of such a microdata table. We selected 6 out of 260 at-tributes for display. Attribute hospital denotes the ID of the hospital which reports a tuple. ICD-9-CM denotes the diagnostic code.
Traditionally, an attribute is called a sensitive attribute (SA) if it carries sensitive information at the individual level -e.g., dis-ease, salary, etc. There is another class of attributes called quasi-identifier (QI). Typical QI attributes include age, zipcode, etc. These attributes may be linked (by an adversary) with external data sources (e.g., voter registry [32]) to re-identify the SA of an individual from the published table, thereby raising privacy concerns.
Most of the existing work on PPDP [20, 23 X 25, 39, 44] can be considered as enforcing a single privacy rule QI SA -i.e., to ensure that an adversary with knowledge of QI cannot infer the SA of a tuple (beyond a pre-defined privacy guarantee such as -diversity [24]), where QI and SA are two disjoint sets of attributes in the microdata table.

The main motivation for this paper is an observation that the en-forcement of a single privacy rule does not always suffice for speci-fying the complex privacy requirements of real-world applications. For example, the following three rules are selected out of nine from the user manual of the aforementioned Texas inpatient dis-charge data [34]. All rules must be properly enforced before the Texas Department of State Health Services publishes the dataset. Rule I : If a hospital has fewer than five discharges of a particular gender, then suppress 2 the zipcode of its patients of that gender. Rule II : If the ICD-9-CM of a patient indicates HIV or alcohol/drug abuse, then suppress the gender and zipcode of that patient. Rule III : If a hospital has fewer than ten discharges of a race, then for all of its patients of that race, change race to other .
From the perspective of PPDP, the above three statements can be interpreted as the following rules 3 . Each rule has its own QI, i.e., left-hand-side (LHS) attributes, and SA, i.e., right-hand-side (RHS) attribute. Rule I, for example, states that an adversary should not be able to infer a patient X  X  zipcode from the published data even given knowledge of the hospital and gender of that patient.
 Rule I : hospital, gender zipcode Rule II : ICD-9-CM gender Rule III : hospital race
One can see from this interpretation that a single privacy rule is insufficient for describing the complex privacy requirement of the Texas Health Service -instead, the published data must satisfy multiple privacy rules. Moreover, an attribute in the microdata table is not restricted to be either QI or SA -e.g., gender is treated as both QI in the first rule and SA in the second -an attribute can also be  X  X eutral X  (i.e., neither QI nor SA) as shown in the existing work [3].
The requirement of multiple privacy rules can be identified from many other real-world applications -e.g., the US Cancer Statistics Data published by the Center for Disease Control and Prevention is required to satisfy the following two statements [4] which we can again translate into two different privacy rules: Rule I : Suppress count and rate when a cell has fewer than 16 indi-viduals.
 Rule II : Suppress count and rate when a cell has race attribute value limited to  X  X ther races combined X  .
To properly model a real-world privacy requirement, we define versatile publishing , a novel framework which specifies the pri-vacy requirement of publishing a microdata table as an arbitrary set of privacy rules . Each rule { Q 1 ,...,Q p } { S 1 ,...,S sures that an adversary with knowledge of all the LHS attributes Q ,...,Q p cannot learn the RHS attributes S 1 ,...,S r beyond a pre-defined privacy guarantee 4 such as -diveristy [24], (  X , k ) -anonymity [38], t -closeness [20],  X  1 -to- X  2 breach [25, 33], etc. With this definition, most existing work on PPDP can be consid-ered as special cases, each enforcing only one privacy rule.
For the ease of understanding, throughout the paper we use Ta-ble 1 along with the following three privacy rules as a simple run-ning example -we shall consider more attributes and privacy rules from the Texas dataset in the experiments.
 Rule 1 : age, ICD-9-CM race Rule 2 : gender, ICD-9-CM zipcode Rule 3 : hospital, race zipcode
The axioms used to infer between sets of privacy rules are sub-tle -which we shall discuss in  X 3 along with the proof of sound-ness and completeness. It is important to recognize that our work is transparent and orthogonal to studies on defining and achieving privacy guarantees. Indeed, any existing guarantee defined for the QI-SA framework can be readily used to define a privacy rule in versatile publishing.
One seemingly simple solution to versatile publishing is the di-rect application of the single-table multiple-SA publishing [24] method.
Table 2: An example of publishing multiple (bucketized) tables For brevity, we use the term multi-SA publishing to refer to this ap-proach. With multi-SA publishing, one defines as SA all attributes that appear on the RHS of at least one privacy rule, and QI as the set of all other attributes. In the running example, zipcode and race will be treated as SA, while QI will consist of age, gender, hospital and ICD-9-CM. If such a table can be properly anonymized, then the result satisfies all three privacy rules.

Nonetheless, a well-understood drawback of multi-SA publish-ing is that it might overly reduce the utility of the published ta-ble, especially when the number of SAs increases [24]. To satisfy -diversity, for example, each anonymous group (e.g., a group of QI-indistinguishable tuples after generalization [24]) must contain at least m tuples, where m is the number of SA attributes (e.g., m = 2 for the running example) -leading to a rapidly decreasing utility of the published data when multiple privacy rules have to be satisfied. We shall verify this observation in the experiments.
Another seemingly simple solution is to decompose the original table into a set of smaller ones for publishing, such that each small table satisfies an individual privacy rule. Table 2 shows an example of publishing three 2 -diversity tables (bucketized by anatomy [39]) to satisfy Rules 1-3, respectively. The problem with this solution is that the published tables may be vulnerable to the following inter-section attack [11,29,36]: First, by joining Tables 2(a) and (b), an adversary can learn that either ( asian , 71000 )or( asian , 72000 ) ap-pears in the microdata table. Whereas, it is known from Table 2(c) that asian should be associated with 71000 or 73000 .By intersect-ing the two conjectures, one can infer an original tuple ( 111111 , asian , 71000 ). This violates Rule 3: {hospital, race zipcode}. Similarly, there are 3 other tuples that can be compromised through this intersection attack: ( 222222 , black , 73000 ), ( 333333 , black , 72000 ) and ( 333333 , white , 71000 ). Such a vulnerability remains when generalization [32] is used instead of bucketization.
The intersection attack could be easily dismissed by single-attribute publishing -i.e., an extreme decomposition which publishes each attribute as an individual table. Table 1, for example, can be de-composed into 6 disjoint sub-tables. However, such a publishing method yields low utility because it destroys any correlation infor-mation between different attributes.

In summary, to the best of our knowledge, no existing or sim-ple solution to versatile publishing can produce satisfiable results by enforcing multiple privacy rules while maintaining a reasonable level of utility for the published table(s).
To enable versatile publishing, we consider a decomposition of the original microdata table into multiple sub-tables with possibly overlapping attributes, such that at most one privacy rule applies to each sub-table, allowing the sub-table to be anonymized by existing PPDP algorithms under the QI-SA framework (e.g., [13,18,19,39]).
To avoid the intersection attack and to provide criteria for de-termining whether a privacy rule is satisfied over the multiple pub-lished tables, we develop the Guardian Normal Form (GNF), a nor-mal form for the schema of published tables which guarantees that all privacy rules are satisfied over the collection of all published ta-bles. The essence of GNF is the existence of a guardian table in the published schema for each privacy rule that needs to be satisfied.
Our GNF is close in spirit to normal forms in relational database theory. In particular, like normal forms, GNF is defined over the schema of published tables, rather than the tuples in them. As a result, GNF is generic to a variety of privacy guarantees (e.g., -diversity [24] and its variants [38, 44], t -closeness [20],  X  breach [9,33]).

Similar to database normalization, there are many different ways to decompose a microdata table into GNF. The selection of a proper decomposition should be made with the utility of the published ta-bles under consideration. In terms of how to use the multiple pub-lished tables in GNF (e.g., for data mining), we follow the same idea as the marginal publishing technique [16] proposed for inject-ing utility into PPDP. In particular, each published sub-table rep-resents a duplicate-preserving view (i.e., marginal) of the original table. To use the published tables, one needs to combine informa-tion from all published marginals to estimate a multinomial model over the original table. As in [16], we use the theory of log-linear modeling to generate a maximum likelihood estimate according to constraints given by all published marginals. A key difference be-tween our work and [16], however, is that we publish marginals for the purpose of satisfying multiple privacy constraints, while the objective of [16] is to increase the utility of published data.
We prove that the optimization of utility for decomposing a ta-ble into GNF is NP-hard (through reduction from MIN-VERTEX-COLORING). Then, we develop two carefully designed heuristics for GNF decomposition. The first, Guardian Decomposition (GD), is similar to the normalization of a relational schema (e.g., into BCNF) -it identifies a privacy rule which violates GNF, and then decomposes the corresponding table into two to eliminate the vio-lation. While the GD algorithm itself is simple and efficient, opti-mizing its utility can be computationally very expensive for a mi-crodata table with a large number of attributes. To address such high-dimensional cases, we devise Utility-Aware Decomposition (UAD), an efficient decomposition algorithm based on a vertex-coloring heuristic.  X  We define the novel problem of versatile publishing which cap- X  We derive the sound and complete set of inference axioms for  X  We define guardian normal form (GNF) which guarantees a set  X  For decomposing a table into GNF, we prove the utility opti- X  We conduct a comprehensive set of experiments over two real-
The rest of the paper is organized as follows.  X 2 introduces pre-liminary notions and defines versatile publishing. The inference axioms between privacy rules are derived in  X 3. We describe GNF in  X 4 and develop the corresponding decomposition algorithms GD and UAD in  X 5. The experimental results are presented in  X 6, fol-lowed by related work in  X 7 and final remarks in  X 8.
We begin with essential notations. Let T = { t 1 ,...,t n crodata table of n tuples and m attributes A ={ A 1 ,A 2 ,...,A Let t j [ A i ] be the value of attribute A i of tuple t j existing work in PPDP, we assume all attributes to be discrete, and leave the optimal discretization of continuous attributes as a sepa-rate problem for future work.

A (deterministic or randomized) privacy-preserving data pub-lishing algorithm perturbs T to one or more published tables based on user-specified privacy rules. We denote the set of d published ta-bles by T  X  ={ T  X  1 ,T  X  2 ,...,T  X  d }. In general, we use the calligraphic font (e.g., T  X  or A ) to represent a set (of tables or attributes).
Privacy requirements can be defined in various ways. A baseline definition is a single-SA privacy rule which specifies one attribute S as the sensitive attribute (SA). The privacy rule states that, for all tuples t  X  T , given the published table and the QI attribute values of t , no adversary is capable of inferring t [ S ] beyond a pre-defined guarantee. We shall discuss further details of this guarantee in the next subsection where we define the privacy measure used by versatile publishing.

In this paper, we consider a more flexible specification of privacy requirements by allowing one to define multiple privacy rules -e.g., a set of privacy rules Q S where Q X  X  , S  X  X  and S  X  Q . Each privacy rule specifies its LHS (i.e., Q ) and RHS (i.e., S ) attributes, and requires that for all tuples t  X  T , given the published table and the LHS attribute values of t (i.e., t [ Q ] ), no adversary is capable of inferring the RHS attribute value of t (i.e., t [ S ] ) beyond a pre-defined guarantee.

More generally, we can allow multiple attributes to be included in the RHS of a privacy rule. This way, a privacy rule Q S (
Q X  X  , S X  X  , Q X  X  =  X  ) states that  X  t  X  T , given the published table and t [ Q ] , the composition of the RHS attributes of t cannot be inferred beyond a pre-defined guarantee. For exam-ple, when -diversity is used, the privacy rule requires at least well-represented value combinations for the RHS attributes. Note that such a multiple-RHS-attribute rule is weaker than the same rule with a subset of the RHS attributes -i.e., a publishing method which satisfies the latter automatically satisfies the former. This stands in contrast to the semantics of multiple RHS attributes in the multi-SA publishing method [24], which requires all SA attributes to be protected, making it stronger than the case with a subset of SA attributes. Our choice here is made for the sake of complete-ness -note that the stronger multi-SA rule can be specified as single-SA rules where |S| is the number of attributes in S attribute A i  X  X  is protected by a privacy rule A\ A i A i the other hand, the weaker privacy rules cannot be specified as a combination of multiple single-RHS-attribute rules.

Nonetheless, we do recognize that real-world privacy require-ments rarely specify the composition of multiple attributes as the RHS of a privacy rule. For example, none of the privacy rules for the Texas inpatient data includes more than one RHS attributes. Hence, this paper focuses on the cases where each privacy rule has a single RHS attribute. As mentioned above, this covers the multi-SA case in the traditional sense, as the privacy requirement there can be specified as a set of single-RHS-attribute rules.
As discussed above, a privacy rule Q S requires that S be protected from an adversary with knowledge of Q and the pub-lished tables. Such a rule needs to be instantiated by a privacy guarantee with a threshold on the degree of disclosure of S . Popular existing measures that may be used include -diversity [24], (  X , k ) -anonymity [38], t -closeness [20], and  X  1 -to- X  2 breach [9,33], etc.
Note that our focus in this paper is not to study a specific pri-vacy measure, but to address the change from enforcing one single privacy rule to enforcing multiple rules. For this purpose, we con-sider a generic privacy measure defined by Kifer [6], and assume all privacy rules to adopt the same privacy guarantee (though over different attributes). Nevertheless, our work can be easily extended to support different privacy guarantees specified in one privacy rule from another. Kifer X  X  measure captures the difference between an adversary X  X  belief before and after observing the published table(s). In the rest of this paper, unless specified otherwise, we use Kifer X  X  generic privacy measure.

D EFINITION 1. (Kifer X  X  Generic Privacy Measure [6]) To sat-isfy a privacy rule Q S , for any tuple t  X  T , an attacker X  X  prior and posterior belief about t [ S ] , i.e., P ( t [ S ] | t [ ) , must satisfy  X  ( P ( t [ S ] | t [ Q ]) ,P ( t [ S ] |  X  (  X  ,  X  ) is a distance function between two probability distributions, T  X  is published table(s) and b is a data-publisher-specified thresh-old.

In summary, the objective of versatile publishing is to publish (multiple) tables from the original microdata, such that the set of privacy rules defined by the data publisher can be satisfied simulta-neously. As discussed in  X 1, we can consider traditional PPDP to be a special case of our setting with which the privacy requirement is specified as one privacy rule QI SA.
We now consider the inference axioms for privacy rules. A pri-vacy rule r can be inferred from a set of privacy rules R always satisfied when all privacy rules in R are satisfied. Some-what surprisingly, we find that the only possible inference is the trivial one, that is, a privacy rule r 1 can be inferred from r LHS attributes of r 1 is a subset of that of r 2 . The following theorem shows the completeness of such a trivial inference axiom.
T HEOREM 3.1. (Completeness of Privacy Rule Inference) A pri-vacy rule Q S can be inferred from a set of privacy rules there exists a rule Q S in R such that Q X  X  .

P ROOF . The correctness of the inference axiom directly follows from the definition of privacy rule. For the completeness of the axiom, we prove by contradiction. Note that we only need to con-struct such a contradiction for one specific instantiation of the generic privacy measure defined in Definition 1 because if an(other) axiom does not hold for this specific instantiation, then it clearly does not hold for the generic definition. In the proof, we consider the privacy measure for every rule to be -diversity with = 2 .
 Suppose Q S can be inferred from a set of privacy rules R which contains no rule of the form Q S where Q X  X  .
 Without loss of generality, let Q = { A 1 ,...,A k } ( k  X  and S = A m .

In the following, we reach a contradiction by constructing a ta-ble T which satisfies every possible privacy rule Q 0 S 0 Q  X  X  0 and S = S 0 . The existence of T shows that, if any other inference axiom (which cannot be derived from the trivial one) ex-isted, then Q S would be satisfied -this contradicts the fact that T violates Q S .

We construct such a table T as follows: Let the domain of at-tribute A i be  X  i = { 0 , 1 ,a,b } when i  X  [1 ,k ] and  X  when i  X  [ k +1 ,m ] . There are 2 m + k tuples in the table. The projection of these tuples on A 1 ,...,A m  X  1 enumerate all possi-ble value combinations for these attributes (note that the Cartesian product of A 1 ,...,A m  X  1 has size 4 k  X  2 m  X  k = 2 m + be where xor is the exclusive-OR function and for all tuples in the table.
 We use three steps to prove that T satisfies every privacy rule Q 0 S 0 unless Q  X  X  0 and S = S 0 . These three steps address three disjoint and exhaustive subsets of such privacy rules respec-tively. First, consider a privacy rule in which A m does not appear. Since the values of A 1 ,...,A m  X  1 are essentially independent (i.e., they enumerate all possible value combinations), every privacy rule not involving A m must satisfy the 2 -diversity privacy guarantee.
Second, consider a privacy rule Q 0 S 0 in which S 0 = A m Since Q  X  X  0 , at least one of A 1 ,...,A k must be absent from the LHS. According to (1), given any value combination of the LHS, there must be equal number of 0 s and 1 s for A m . Thus, such a privacy rule must also satisfy the 2 -diversity privacy guarantee.
Finally, consider a privacy rule Q 0 S 0 in which A m ap-pears on the LHS. If S 0  X  X  A k +1 ,...,A m  X  1 } , 2 -diversity must be satisfied because S is independent of all LHS attributes. If S 0  X  X  A 1 ,...,A k } , since Q  X  X  0 , given any value combina-tion for the LHS attributes, there must be equal number of tuples with S 0 =0 (resp. 1 ) and S 0 = a (resp. b ). Thus, such a privacy rule must also satisfy 2 -diversity.

In fact, we can derive a similar theorem for privacy rules with more than one RHS attributes. Again, the only possible inference rules are the trivial ones:
C OROLLARY 3.1.1. A privacy rule Q S can be inferred from a set of privacy rules R iff there exists a rule Q S such that Q X  X  and S  X  X  .

Since our focus is on privacy rules with a single RHS attribute, we omit the proof in this paper.

Based on the theorem and the corollary, we can define an irre-ducible set of privacy rules as follows.

D EFINITION 2. (Irreducibility of Privacy Rule Set) A set of pri-vacy rules  X  is irreducible iff it does not contain two privacy rules Q S and Q S such that Q X  X  and S  X  X  .

For example, { A 1 A 3 , { A 1 ,A 2 } A 3 } is not irreducible because it can be reduced to { A 1 ,A 2 } A 3 , which is an irre-ducible set. Without loss of generality, we focus on an irreducible set of privacy rules for the rest of this paper.
 Connection with Functional Dependencies: Intuitively, the def-inition of privacy rules somewhat resembles functional dependen-cies (FDs), though we have shown their reduction rules to be quite different. The connections between these two concepts do not end with their definitions. In particular, when an adversary can learn certain functional dependencies through external knowledge [5,25], the privacy rules may have to be expanded to protect against addi-tional privacy disclosure. The following theorem illustrates the case with the -diversity privacy guarantee.

T HEOREM 3.2. (Functional Dependencies in External Knowl-edge) For the -diversity privacy guarantee, a set of published ta-bles satisfies Q S only if, for any FD X X  S in adversarial knowledge, the published tables satisfy Q X . Table 3: A GNF example of publishing multiple (bucketized) tables Table 4: A non-GNF example of publishing multiple tables. (a) Published table schemas (left) and enforced privacy rules (right). (b) The graphic representation of T  X  = { T  X  1 ,T  X  2 ,T
P ROOF . If the published tables T  X  violate Q X , then there must exist a tuple t  X  T and a value v X in the domain of that Pr { t [ X ]= v X | t [ Q ] , T  X  } &gt; 1 / . Since there must exist a value v S in the domain of S such that Pr v | t [ X ]= v X } =1 . Thus, Pr { t [ S ]= v S | t [ Q ] , v | t [ Q ] , T  X  } &gt; 1 / . One can see that Q S is violated.
While the theorem illustrates that privacy rules should be ex-panded based on FDs that are available through external knowl-edge, it does not offer a complete solution: In particular, it is un-known from the theorem how Q S can be satisfied if Q X  S happens to be a FD, because it is impossible to enforce Q Q without changing the values of Q .

Unfortunately, this is a price we have to pay for using the exist-ing privacy guarantees such as -diversity to instantiate the privacy rules. The reason can be seen from the following example which shows that it may not be possible to eliminate an FD-incurred dis-closure by adding additional privacy rules: Let there be a table of two ternary attributes A 1 ,A 2  X  X  0 , 1 , 2 } over which a privacy rule A 1 A 2 needs to be enforced with -diversity guarantee and =2 . Consider the publishing of each attribute as an individ-ual table: A 1 : { 0 , 0 , 0 , 1 , 1 , 2 } and A 2 : { 0 , 2 , 1 , 2 , 2 , 1 see that this publishing method satisfies all 2 -diversity-based pri-vacy rules that can be specified. Nonetheless, an adversary which knows that A 1  X  A 2 is a FD can still infer that 0 , 2 , 1 , 1 and 2 , 0 must be the original tuple values. This violates A 1 We leave the defense against such an FD-incurred disclosure as an open problem for future work.
To enable versatile publishing, we consider the generation of multiple sub-tables with possibly overlapping attributes, such that each sub-table only addresses (at most) one privacy rule and there-fore can be processed by existing PPDP algorithms designed for the QI-SA framework. This section is focused on how to design the schema of published tables such that each privacy rule remains in effect over the collection of all published tables. We defer to  X 5 for discussions about the utility of published tables.
It is important to understand the implications of publishing mul-tiple tables on the enforcement of privacy rules: Due to the intersec-tion attack [11,29,36], a privacy rule satisfied by two anonymized tables individually may be broken by the combination of both ta-bles (recall the example in Table 2). Yet a privacy rule addressed by none of the published tables may be automatically satisfied due to the separation of attributes across published tables (recall the single-attribute publishing technique discussed in  X 1).
Thus, it is necessary to provide criteria for determining whether a privacy rule is satisfied over a collection of published tables. We define such criteria as GNF, a normal form for the schema of pub-lished tables. In the following, we discuss the basic ideas of GNF in terms of two ways for a privacy rule to be satisfied: 1) a singular case of Q ,S non-reachability; 2) a generic case of the existence of a guardian table. In the next subsection, we shall combine these two scenarios to form the definition of GNF.
 Singular Case (Non-Reachability): For the ease of understand-ing, we use an undirected graph to represent the schema of pub-lished tables. The graph is constructed as follows: Each vertex cor-responds to an attribute in the original microdata table. An edge ex-ists between two vertices iff there is at least one published table that contains both attributes. For example, suppose that Tables 3(a) and (b) are published for our running example. Table 3(c) depicts the graph representation. One can see that if two vertices (e.g., age and race) are not reachable from each other (i.e., no path between them exists) in the graph, then the two corresponding attributes are in-dependent given the published tables -i.e., no correlation between them is disclosed. Thus, if every attribute in Q is not reachable from S , then a privacy rule Q S is satisfied.
 Formally, we have the following definition and lemma:
D EFINITION 3. (Reachablility) Two attributes A i and A j reachable iff there exists a sequence of published tables T such that T  X  1 contains A i , T  X  h contains A j , and T one common attribute with T  X  i +1 for all i  X  [1 ,h  X  1] . L EMMA 1. The published tables satisfy Q S if  X  A i  X  X  , A i and S are not reachable.
 Due to limited space, please refer to [14] for the proof of Lemma 1. Consider Rules 1-2: {age, ICD-9-CM race} and {gender, ICD-9-CM zipcode} in the running example. According to Lemma 1, both rules are satisfied over Tables 3(a) and (b).
 General Case (Guardian Table): In the nonsingular case where Q and S are reachable from each other, at least one of the pub-lished tables that link Q and S together may have to be properly anonymized in order to satisfy Q S . We define the guardian table of a privacy rule as follows:
D EFINITION 4. (Guardian Table) A published table T  X  i with attributes A  X  i is said to be the guardian table for a privacy rule Q S iff (i) T  X  i contains S , and (ii) T  X  i by itself satisfies ( Q X  X   X  i )  X  X   X  S where Q  X  (iii) After removing S from T  X  i , S (if it also occurs in the other
When Q and S are reachable from each other, the existence of a guardian table also ensures its uniqueness due to Condition (iii). The implication of Condition (iii) indicates that all paths from (any attribute in) Q to S must pass through edges defined by the guardian table for Q S . As a result, the enforcement of the privacy rule over the guardian table also guarantees it over the collection of all published tables, as shown by the following lemma:
L EMMA 2. The published tables satisfy Q S if there exists a guardian table for Q S .

P ROOF . (Sketch) Let T  X  i be the guardian table for Q S . Re-T i satisfies where Q  X  = { A j | A j  X  X   X  i \ S and A j is reachable from at least one attribute in Q given T  X  \ T  X  i } . Let W =( Q X  X   X  i following, we prove that the published tables T  X  satisfy S . Note that due to the trivial inference rule, this implies
Consider attributes in Q\W . Due to the definition of W , these attributes much be unreachable from S . According to the proof of Lemma 1 in [14], they must be conditionally independent with S given the published tables (denoted by ( Q\W )  X  S |T  X  ). Thus,
Since S is not reachable from Q after removing T  X  i from the published tables, S must be unreachable from W as well after the removal of T  X  i . Thus, That is, t [ S ]  X  ( T\ T  X  i ) | T  X  i ,t [ W ] . Thus, ( -i.e.,
According to (6), P ( t [ S ] | t [ W X  X  ] , T  X  )= P ( t [ S ] Since W S is satisfied by T  X  i , ( W X  X  ) S must be satisfied by T  X  , the set of all published tables.

In the running example of publishing Tables 3(a) and (b), one can see that Table 3(b) serves as the guardian table for Rule 3: {hospital, race zipcode} where ( Q X  X   X  i )  X  X   X  = {race}  X   X  and S = zipcode. As another example, Table 4(a) illustrates the schema of four published tables, each enforcing at most one privacy rule (specified on the right). In this case, one can verify according to Definition 4 that T  X  3 is the guardian table for Rule 1: {age, ICD-9-CM race} where ( Q X  X   X  i )  X  X   X  = {age}  X  {hos-pital, gender} and S = race. On the other hand, no published table can serve as the guardian table for Rule 2: {gender, ICD-9-CM zipcode}. Consider T  X  2 and T  X  4 , the only two tables satisfying Con-dition (i). Either removing zipcode from T  X  2 or from T  X  same graphic representation of the four tables (after the removal) as in Table 4(b), where zipcode is still reachable from both gender and ICD-9-CM. Thus, the published tables satisfy Rule 1 but may vio-late Rule 2. We are now ready to combine both scenarios to define GNF:
D EFINITION 5. (GNF) For a given set of privacy rules R , a set of published tables is in GNF iff for any privacy rule Q S in either Q and S are not reachable from each other, or there exists a published table that is the guardian table for Q S .
 The following theorem follows directly from Lemmas 1 and 2.
T HEOREM 4.1. For a given set of privacy rules, if the published tables are in GNF, then all privacy rules are satisfied.
For the running example, the published tables specified by Ta-ble 3 are in GNF while those in Table 4 are not.
Given the definition of GNF, we now consider how to decompose a microdata table into GNF. Since GNF guarantees the satisfaction of all privacy rules, the focus here is on optimizing the utility of published tables. In particular, we first discuss how to utilize the multiple published tables in applications such as data mining. We then establish the hardness of utility optimization and devise GD and UAD, two decomposition algorithms on heuristic.
A main goal of PPDP is to enable analytical applications such as data mining. While the specific need of these applications can be quite different and hard to align [21], a general requirement is the knowledge of the probability distribution of the original data. In terms of the publishing method, most existing solutions in the QI-SA framework publish one anonymized view of the original data while our decomposition approach publishes multiple views. But in terms of the ultimate usage of the published data, both publishing methods produce the same -i.e., an estimation of the original data distribution based on constraints defined by the published view(s).
Thus, we adopt as the utility measure the Kullback-Leibler diver-gence (KL-divergence) between the original data distribution and the maximum likelihood estimation from the published views [16]. In particular, let v i = v i 1 ,...,v i m be a possible tuple value in the multi-dimensional domain of the original data, and p i the probability for a tuple to take the value of v i given the original and the estimated distributions, respectively. The KL-divergence is defined as which is non-negative and takes the value of 0 iff p  X   X  p . Since KL-divergence measures the difference in log-likelihood between the two distributions, the smaller it is, the better utility the published data is able to provide.

The problem of estimating the original distribution based on the anonymized views was studied in [16] -we follow the solution in this paper. Specifically, log-linear modeling, a popular statistics tool to model attribute associations based on contingency tables (e.g., anonymized views), is used to estimate parameters for the original multinomial distribution. Please refer to [16] for the algo-rithmic details. One subtle point is that the graphic representation (i.e., interaction graph [16]) of all published tables in GNF must be triangulated [17] -i.e., there are no induced cycles of length 4 or more. Otherwise, this contradicts Condition (iii) in Definition 4. Therefore, publishing schemas generated by all decomposition al-gorithms in this paper are decomposable [16], whereby they sup-port a closed-form solution for the maximum likelihood estimate of the log-linear model. The hardness of utility optimization comes from two sources. First, since we use the existing algorithms for the QI-SA framework to anonymize each decomposed table, the overall utility is subject to the non-optimality of these algorithms. From this perspective, the utility optimization for achieving single-table -diversity has been proved to be NP-hard [26,41]. The second source of hardness comes from the optimization of utility during the decomposition process, as shown by the following theorem.

T HEOREM 5.1. The utility optimization for decomposing a mi-crodata table into GNF is NP-hard.

Please refer to [14] for the detailed proof. The main idea of the proof is a reduction from MIN-VERTEX-COLORING [12].
Similar in spirit to the database normalization algorithm which decomposes a relation into BCNF (see Algorithm 11.3 in [8]), Al-gorithm GD follows a simple idea: find a privacy rule which vi-olates GNF, decompose the existing sub-tables to address the pri-vacy rule, and continue until no more offending privacy rule exists. Specifically, if Q S violates GNF, then we remove all occur-rences of S from the existing tables, and then insert a new sub-table which consists of attributes in Q X  X  S } and enforces Q S . The details are depicted by Steps 1 to 4 in Algorithm 1.

Such a simple decomposition does not lose any attribute, but may not produce the optimal utility either -e.g., additional attributes might be added to the decomposed tables without violating GNF. To remedy this deficiency, Step 5 onwards in Algorithm 1 uses a greedy method to add attributes back to the decomposed tables. In-tuitively, it inserts attributes in decreasing order of their  X  X ffective-ness X  on reducing the KL-divergence. While this method produces good utility over microdata tables with a small number of attributes, as we shall show in the experiments, it does not scale well when the number of attributes is large, mainly because the computation of KL-divergence involves a time-consuming process of constructing log-linear models across multiple tables. In addition, this method cannot change the number of published tables which is determined by the decomposition step -if initially the microdata table is de-composed into many small pieces, and then one may not be able to add back many attributes without violating GNF.
 Algorithm 1 : Guardian Decomposition (GD) Input : The microdata table T and privacy rules Output : Published tables T  X 
Choose an arbitrary privacy rule and anonymize T to T  X  in order to enforce it; T  X   X  X  T  X  } ;
Find a privacy rule Q S which violates GNF over T  X  ;
Remove S from all tables in T  X  ;
Create anonymized table T  X  1 which has attributes Q X  X  S } enforces Q S ; Add T  X  1 to T  X  ; Goto 2; foreach pair of attribute A i in table T and T  X   X  X   X  such that T  X  remains in GNF after adding A i to T  X  do
Find A i ,T  X  j with the smallest KL-divergence value; Add A to T  X  j ;
Goto 5 until no such pair exists;
To address the problems of GD, we develop UAD by leverag-ing the link between utility optimization and the MIN-VERTEX-COLORING problem, as discovered in the hardness proof. In par-ticular, consider a graph in which each vertex corresponds to an attribute in the microdata table, and an edge exists between two vertices iff the two corresponding attributes appear on two differ-ent sides of a privacy rule 5 . The key idea of UAD stems from the following observation: according to GNF, if two attributes A A j reside on two different sides of a privacy rule (i.e., connected in the graph), then A i and A j cannot appear in the same decomposed table unless the table enforces a privacy rule with either A on the RHS. Thus, the set of LHS attributes in each decomposed table forms an independent set of the graph.
 Algorithm 2 : Utility-Aware Decomposition (UAD) Input : The microdata table T and privacy rules Output : Published tables T  X 
Construct a directed graph G = V , E where each vertex v i  X  X  corresponds to an attribute A i  X  T ; Add an edge and RHS, respectively;
Use DSATUR to color the undirected version of G ;
Find V max as the maximum set of nodes with the same color. Break tie arbitrarily;
Construct a table T  X  with attributes V max ; Add T  X  to T  X 
Find v  X  X \V max which has no outgoing edge to, and most incoming edge from, vertices in V max ; Break tie arbitrarily; Goto 7 if no such v exists;
V max  X  X  max  X  X  v } . Add v to T  X  and anonymize it to enforce V max v where V max is the subset of V max that have edges to v ;
Remove V max and all associated edges from V ;
Goto 2 until V is empty;
Since decomposition removes from the published data correla-tion information between attributes in different decomposed tables, we aim to generate as few such independent sets as possible. There-fore, we call DSATUR [2], a well-known heuristic algorithm for MIN-VERTEX-COLORING, to group all vertices into a small num-ber of independent sets (i.e., colors). Then, we pick all attributes in the largest color group ( V max in Line 3) to construct a decom-posed table. We anonymize the table by adding (and enforcing a privacy rule over) a sensitive attribute v . The only condition on v is that it never appears on the LHS of a privacy rule in which an attribute in V max appears on the RHS (Line 5). The construction of decomposed tables is repeated until all attributes are published.
One can easily verify that tables published by UAD are in GNF and thus satisfy all privacy rules. While the algorithm is designed with utility under consideration, it is difficult to theoretically an-alyze the utility of published tables due to its dependency on the original data distribution. Thus, we leave utility analysis to experi-mental evaluations in  X 6.

The time complexity of UAD depends on the underlying algo-rithm used to anonymize each published table. When anatomy [39] is used, anonymizing a table with n tuples to -diversity takes O ( n (log  X  + )) , where  X  is the maximum domain size of an attribute. Given the complexity of DSATUR [2] being O ( m where m is the number of attributes, UAD terminates in O ( m n (log  X  + )) . Since n m for most microdata tables, the time complexity of UAD is linear to the number of tuples in the table.
In this section, we evaluate the performance of GD and UAD for versatile publishing on two real-world datasets (i.e., Adult [1] and Texas inpatient discharge data [34]). In particular, we compare our (a) Adult dataset algorithms against two baseline techniques discussed in  X 1: multi-SA publishing and single-attribute publishing . Please refer to [14] for details of our implementation of multi-SA publishing by using Binary Integer Programming (BIP). Hardware: All our experiments were conducted on a 2.6GHz Intel Core 2 Duo machine with 2GB RAM and Windows XP OS. All algorithms were implemented using C++.
 Dataset 1 (Adult): We used a well-known benchmark Adult dataset [1] which contains 7 attributes and 45 , 222 tuples after removing all tuples with missing values. Since our BIP implementation of multi-SA publishing is extremely time-consuming for datasets with a large number of tuples, for the purpose of comparing with multi-SA publishing, we sampled 10 , 000 tuples without replacement as our testing bed, and left the scalability testing to experiments con-ducted on the Texas dataset.

We used two steps to pre-process the Adult dataset in order to test multi-SA publishing within a reasonable amount of time: First, we reduced the number of tuples to 10 , 000 by sampling without replacement, and 2) we reduced the number of possible values of each attribute (i.e., domain size) through generalization. In partic-ular, similar to the generalization hierarchy used in [10], we gen-eralized attribute age into 6 values: { 17 -24 } , { 25 -34 { 45 -54 } , { 55 -64 } and { 64 -90 } ; attribute country into 2 values: US and non _ US ; attribute education into 7 values: preschool , elementary , secondary , some _ colloeage , associted _ degree , university , post _ graduate ; and attribute occupation into 3 val-ues: white _ collar , blue _ collar and others . Table 5a summarizes the domain sizes of all attributes after generalization. Dataset 2 (Texas [34]): We also tested the aforementioned Texas patient discharge data [34]. In particular, we selected 18 attributes involved in the privacy rules specified in the user manual, removed all tuples with missing values, and used the remaining 177 , 148 tuples as our testbed. Table 5b describes the domain sizes of at-tributes in the Texas dataset.
 Privacy Guarantee: We used -diversity [24] as the privacy guar-antee in the experiments. Thus, a published table T  X  satisfies a privacy rule Q S iff Pr { t [ S ] | t [ Q ] , T  X  } X  1 / for all t 13, 37, 39]. We implemented the bucketization-based anatomy al-gorithm [39] as the single-table anonymization subroutine invoked by GD and UAD.
 Generating Privacy Rules: We randomly generated irreducible sets of privacy rules (see definition in  X 3) while varying two pa-rameters: the number of privacy rules c and the number of LHS attributes lhs (recall that there is always one RHS attribute) in each privacy rule. To compare with single-table publishing, we excluded a privacy rule if it cannot even be satisfied by publishing its RHS attribute as a separate table, because such a privacy rule cannot be satisfied by single-table publishing.
 Utility Measure: For the purpose of providing an intuitive ob-servation of the utility of published tables, in addition to the KL-divergence measure discussed in  X 5, we also tested another com-mon utility measure, relative error [39]. Relative error measures the accuracy of answering a workload of queries of the form: SELECT COUNT(*) FROM Dataset
WHERE pred ( A 1 ) ,...,pred ( A qd ) where qd is the query dimension and pred ( A i ) denotes a predicate of A i belonging to a set of randomly selected domain values. The size of the set is captured by a parameter called value percentage p (i.e., the percentage of all domain values in the set). Let Act and Est be the query answer over the microdata table T and the published tables T  X  , respectively. The relative error is defined as For each experiment, we ran a workload of 1 , 000 randomly gen-erated queries and calculated the average relative error. We varied both parameters qd and p during the generation.
With the Adult dataset, we compared the utility and efficiency of all four algorithms: multi-SA, single-attribute, GD, and UAD. Utility vs. Number of Privacy Rules: We varied the number of privacy rules c from 1 to 9 while fixing =2 and lhs =3 for gen-erating the privacy rules. We ran each algorithm for 50 times. For the relative error measure, we set qd =3 and p = 40% . Figures 1 and 2 depict the average KL-divergence and relative error for each algorithm, respectively. Note that when c =1 , multi-SA publish-ing, GD and UAD are all equivalent. For c&gt; 1 , our two algorithms, GD and UAD, provide nearly identical utility which outperforms both multi-SA and single-attribute publishing, according to both measures. Also note that the utility of multi-SA publishing dete-riorates significantly when the number of privacy rules increases. This is because when more attributes are added as SA, multi-SA publishing has to produce buckets (or QI-indistinguishable groups) with extremely large sizes, reducing the utility of published tables. Efficiency: Following the same setting as Figures 1 and 2, Figure 3 depicts the efficiency of UAD, GD, and multi-SA publishing over the Adult dataset. Note that the figure also depicts the efficiency of UAD over the Texas dataset to demonstrate its scalability to a much larger dataset. Single-attribute publishing has essentially no over-head because it simply splits each attribute into a different table. One can see that for the same (Adult) dataset, the computational overhead of multi-SA publishing is orders of magnitude greater than that of UAD and GD. Also note that UAD is more efficient than GD, as we explained in  X 5.
With the Texas dataset, we focused on testing the utility of UAD and single-attribute publishing because both multi-SA publishing and GD require a matter of weeks to complete computations on the large Texas dataset.
 Utility vs. Number of Privacy Rules: We set =10 and lhs = 3 for generating privacy rules and varied the number of privacy rules c from 1 to 9 . For testing the relative error measure, we set qd =4 and p = 40% . Figure 4 depicts the KL-divergence and relative error of UAD and single-attribute publishing. One can see that UAD significantly outperforms single-attribute publishing for both measures.
 Relative Error with Various Settings: To thoroughly test the rel-ative error measure, we varied its settings parameters qd from 2 to 10 and p from 0 . 3 to 0 . 7 . Note that we omitted qd =1 be-cause both UAD and single-attribute produce can precisely answer 1 -dimensional queries. For generating privacy rules, we followed
Figure 1: Adult dataset, vary c
Figure 5: Texas dataset, vary qd the same parameters as above except that the number of privacy rules was fixed as c =5 . Figures 5 and 6 depict the relative error of UAD and single-attribute publishing given the varying qd and p , respectively. Again, UAD produces smaller relative errors than single-attribute publishing under all settings.
 Utility vs. Number of LHS attributes: We investigated the effect of the number of LHS attributes in each privacy rule on the utility of published tables. All other settings were kept the same as in Figure 4 with c =5 and lhs varies from 1 to 9 . Figure 7 depicts the results. One can see that lhs has barely any influence on the utility of UAD when lhs  X  3 . To understand why, consider adding an attribute A i to the LHS of a privacy rule Q S . Given the tables published by UAD, the only situation which requires changes to the tables is when A i and S both appear as the LHS attributes of a published table. Nonetheless, UAD is less likely to assign S to the LHS of any table, especially one that has a large number of (other) LHS attributes, because doing so prevents Q from being included in the table. Hence, the number of LHS attributes in a privacy rule is not a dominating factor for the utility of UAD.
 Utility vs. : The utility of published tables is determined by two factors: (1) the decomposition of the original table, and (2) the anonymization of decomposed tables. To identify the dominating factor, we tested UAD with varying from 5 to 20 because do-ing so has significant impact on anonymization but no impact on decomposition. Figure 8 shows that the utility of UAD is not sig-nificantly affected by the value of . This indicates that the utility of publishing multiple tables is mainly determined by decomposition. While this paper initiates the study of decomposition design with two heuristic algorithms, we believe such importance of decompo-sition calls for further studies of its design in the future work.
Sweeney and Samarati [31, 32] first defined the k -anonymity guarantee for PPDP. After that, motivated by different sensitive in-formation and adversarial knowledge, many other privacy guaran-tees have been proposed -e.g., -diversity [24], (  X , k ) -anonymity [38], t -closeness [20],  X  -presence [28], skyline privacy [5],  X  to- X  2 breach [9, 33], m -confidentiality [37], -privacy [23], etc. Meanwhile, various anonymization techniques, e.g., generalization and/or suppression [10, 13, 18, 19, 32], bucketization [39, 44], ran-domization [7,30,33,40], etc, were developed to achieve these pri-vacy guarantees. Our work is related but orthogonal to the study of these anonymization techniques. In particular, while the existing anonymization techniques mostly address PPDP at the tuple level, we approach the problem at the schema level. As a result, our def-initions of versatile publishing and GNF are transparent to the un-derlying privacy guarantees (e.g., -diversity). Our work is also orthogonal to the study of differential privacy [7] which, instead of providing absolute privacy guarantees on the values of sensi-tive attribute (like -diversity), aims to ensure minimum difference between the cases where an individual is or is not present in the database.

It is important to note that some recent work studied the algorithm-based disclosure of some existing anonymization techniques -i.e., disclosure that happens once an adversary learns the mechanism of the underlying annoymization algorithm [15, 22, 37, 43]. Note that such a vulnerability is transparent to our decomposition-based algorithms: Since we use the existing anonymization techniques as primitive operations, we can eliminate the threat of algorithm-based disclosure by using anonymization techniques that have been proved to be free of algorithm-based disclosure (e.g., anatomy [39], SP-Hilb [15]).

Other closely related work to ours includes the study of anonymiza-tion without explicit QI and SA attributes [36] and the protection of inference rules [35] by suppression. The generation of multiple views for optimizing utility was studied in [16], and the anonymiza-tion of a given set of views in [27, 42]. Nonetheless, both studies were designed specifically for achieving the k -anonymity guaran-tee. [29] addressed the publishing of a given microdata table to mul-tiple users with different QI. While this problem can be considered as satisfying more than one privacy rules, it does not support the specification of arbitrary privacy rules with different RHS attributes as in our versatile publishing framework.
In this paper, we addressed the versatility problem of PPDP by defining versatile publishing which allows the specification of mul-tiple privacy rules with arbitrary QI/SA combinations. To enable versatile publishing, we developed GNF, a normal form for publish-ing multiple sub-tables, each processed by an existing anonymiza-tion algorithm for a single privacy rule, to satisfy all privacy rules over the combination of all published sub-tables. To decompose a given microdata table into GNF, we devised two decomposition algorithms, GD and UAD, evaluated their performance over real-world datasets, and demonstrated their superiority over a number of baseline techniques for versatile publishing.
 Acknowledgements: The authors would like to thank Johannes Gehrke (Cornell University) for his many insightful and helpful comments on this work, especially on the definition of privacy rules, the link with functional dependencies, and the utilization of tables published with GNF. The authors are also grateful to the anonymous reviewers for their constructive suggestions. [1] A. Asuncion and D. Newman. UCI machine learning [2] D. Brelaz. New methods to color the vertices of a graph. [3] J. Brickell and V. Shmatikov. The cost of privacy: [4] Centers for Disease Control and Prevention. United states [5] B. Chen, R. Ramakrishnan, and K. LeFevre. Privacy skyline: [6] K. Daniel. Attacks on privacy and de finetti X  X  theorem. In [7] C. Dwork. Differntial privacy. In ICALP , 2006. [8] R. Elmasri and S. B. Navathe. Fundamentals of Database [9] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting privacy [10] B. C. M. Fung, K. Wang, and P. S. Yu. Top-down [11] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith.
 [12] M. Garey and D. Jonson. Computers and Instractability: A [13] G. Ghinita, P. Karras, P. Kalnis, and N. Mamoulis. Fast data [14] X. Jin, M. Zhang, N. Zhang, and G. Das. Versatile publishing [15] X. Jin, N. Zhang, and G. Das. Algorithm-safe [16] D. Kifer and J. Gehrke. Injecting uility into anonymized [17] S. L. Lauritzen. Graphical Models . Clarendon Press, 1996. [18] K. LeFevre, D. DeWitt, and R. Ramakrishnan. Mondrian: [19] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito: [20] N. Li, T. Li, and S. Venkatasubramanian. t -closeness: [21] T. Li and N. Li. On the tradeoff between privacy and utility [22] W. Liu, L. Wang, and L. Zhang. k -jump strategy for privacy [23] A. Machanavajjhala, J. Gehrke, and M. G X tz. Data [24] A. Machanavajjhala, D. Kifer, J. Gehrke, and [25] D. J. Martin, D. Kifer, A. Machanavajjhala, J. Gehrke, and [26] A. Meyerson and R. Williams. On the complexity of optimal [27] M. Nergiz, C. Clifton, and A. Nergiz. Multirelational [28] M. E. Nergiz, M. Atzori, and C. Clifton. Hiding the presence [29] J. Pei, Y. Tao, J. Li, and X. Xiao. Privacy preserving [30] V. Rastogi, S. Hong, and D. Suciu. The boundary between [31] P. Samarati. Protecting respondents X  identities in microdata [32] P. Samarati and L. Sweeney. Protecting privacy when [33] Y. Tao, X. Xiao, J. Li, and D. Zhang. On anti-corruption [34] Texas Department of State Health Services. User manual of [35] K. Wang, B. C. M. Fung, and P. S. Yu. Template-based [36] K. Wang, Y. Xu, A. Fu, and R. Wong. Ff-anonymity: When [37] R. C. Wong, A. W. Fu, K. Wang, and J. Pei. Minimality [38] R. C. Wong, J. Li, A. W. Fu, and K. Wang. (  X  , k )-anonymity: [39] X. Xiao and Y. Tao. Anatomy: Simple and effective privacy [40] X. Xiao, Y. Tao, and M. Chen. Optimal random perturbation [41] X. Xiao, K. Yi, and Y. Tao. The hardness and approximation [42] C. Yao, X. S. Wang, and S. Jajodia. Checking for [43] L. Zhang, S. Jajodia, and A. Brodsky. Information disclosure [44] Q. Zhang, N. Koudas, D. Srivastava, and T. Yu. Aggregate
