 Regression is a core task of machine learning, aiming at identifying the relation-ship between a series of predictor variable s and a special target variable (labeled instances) of interest [1]. Practitioners often face budget constraints in record-ing/measuring instances of the target variable, in particular due to the need for domain expertise [2]. On the other hand, the instances composed of predictor variables alone (unlabeled instances) appear in abundant amounts because they typically originate from less expensive automatic processes. Eventually the re-search community realized the potential of unlabeled instances as an important guidance in the learning process, establishing the rich domain of semi-supervised learning [2]. Semi-supervised learning is expressed on two flavors: regression and classification, depending on the metric used to evaluate the prediction of the target variable.

The principle of incorporating unlabeled instances relies heavily on exploring the geometric structure of unlabeled data, addressing the synchronization of the detected structural regularities against the positioning of the labeled instances. A stream of research focuses on the notion o f clusters, where the predicted target values were influenced by connections to labeled instances through dense data re-gions [3,4]. The other prominent stream elaborates on the idea that data is closely encapsulated in a reduced dimensionality space, known as the manifold princi-ple. Subsequently, the method of Manifold Regularization restricted the learning algorithm by imposing the manifold geometry via the addition of structural reg-ularization penalty terms [5]. Discretized versions of the manifold regularization highlighted the structural understanding of data through elaborating the graph Laplacian regularization [6]. The extrapolating power of manifold regularization have been extended to involve second-orde r Hessian energy regularization [7], and parallel vector field regularization [8].

Throughout this study we introduce a semi-supervised regression model. The underlying foundation of our approach considers the observed data variables to be dependent on a smaller set of hidden/latent variables. The proposed method builds a low-rank representation of the data which can reconstruct both the predictor variables and the target variable via nonlinear functions. The target variable is utilized in guiding the reduction process, which in comparison to unsupervised methods, help filtering only those features which boosts the tar-get prediction accuracy [9,10]. The prop osed method operates by constructing latent nonlinear projections, opposing techniques guiding the reconstruction lin-early [9]. Therefore we extend supervised matrix factorization into non-linear capabilities. The nonlinear matrix factorization belongs to the family of mod-els known as Gaussian Process Latent Variable Modeling [11]. Our stance on nonlinear projections is further elaborated in Section 3.

The modus operandi of our paper is defined as a joint nonlinear reconstruction of the predictors and target variable by optimizing the regression quality over the training data. The nonlinear functions are defined as regression weights in a mapped data space, which are expressed and learned in the dual-form using the kernel theory. In addition, a stochastic gradient descent algorithm is introduced for updating the latent data based on the learned dual regression weights. In the context of semi-supervision our model can operate with very few labeled instances. Detailed explanation of the method and all necessary derivations are described in Section 4.

No previous paper has attempted to compare factorization approaches against the state of the art in manifold regularization, regarding semi-supervised regres-sion problems. In order to demonstrate the superiority of the presented method we implemented and compared against five strong state-of-art methods. A bat-tery of experiments over eleven real life datasets at varying number of labeled instances is conducted. Our method clearly outperforms five state-of-art base-lines in the vast majority of the experiments as discussed in Section 5. The main contributions of this study are:  X  Formulated a supervised non linear factorizations model  X  Developed a learning algorithm in the dual formulation  X  Conducted a throughout empi rical analysis against the state of the art (man-Even though a plethora of regression models have been proposed, yet Sup-port Vector Machines (SVMs) are among the strongest general purpose learning models. A particular implementation of SVMs tailored for approximating square error loss is called Least Square SVMs (LS-SVM) [12], and is shown to perform equivalently to the epsilon loss regression SVMs [13]. This study empirically compares against LS-SVM, in order to demonstrate the additive gain of incor-porating unlabeled information.

The semi-supervised regression research was boosted by the elaboration of the unlabeled instances X  structure into the regression models. A major stream explored the cluster notion in utilizing high density unlabeled instances X  regions for predicting the target values [3,4]. The other stream, called Manifold Reg-ularization, assumes the data lie on a low-dimensional manifold and that the structure of the manifold should be resp ected in regressing target values of the unlabeled instances [5]. A discretized vari ant of the regularization was proposed to include the graph Laplacian representation of the unlabeled data as a penalty term [6]. The regularization of the manifold surfaces have been extended to in-volve second-order Hessian energy regularization [7], while a formalization of the vector field theory was employed in the so-called Parallel Field Regularization (PFR) [8]. In addition, a recent elabora tion of surface smoothing included en-ergy minimizations called total variation and Euler X  X  elastica [14]. Another study attempts to discover eigenfunctions of the integral operator derived from both labeled and unlabeled instances [15], while efforts have extended to incorporate kernel theory to manifold regularization [16]. In this study we compare against three of the strongest baselines, the Laplacian regularization, the Hessian Regu-larization and the PFR regularization. In contrast to these existing approaches, our novel method explores hidden data structures via latent nonlinear recon-structions of both predictors and target variable.

Supervised Dimensionality Reduction involves label information as a guidance for dimensionality reduction. The Linear Discriminant Analysis is the pioneer of supervised decomposition [17]. SVMs were adjusted to high dimen-sional data through reducing the dimensionality via kernel matrix decompo-sition [18]. Generalized linear models [ 19] and Bayesian mixture modeling [10] have also been combined with supervised dimensionality reduction. Furthermore convolutional and sampling layers of convolutional networks are functioning as supervised decomposition [20]. The fiel d of Gaussian Process Latent Variable Models (GPLVM) aims at detecting laten t variables through a set of functions having a joint Gaussian distribution [21]. A similar model to ours has utilized GPLVM for pose estimation in images [22].

Due to its empirical success, matrix factor ization has been employed in detecting latent features, while supervised matrix factorization is engineered to emphasize the target variable [9]. For the sakeof clarity, methods that reduce the dimensional-ity in a nonlinear fashion such as the kernel PCA [23], or kernel non-negative matrix factorization [24], should not be confused with the proposed method, because such methods are unsupervised in terms of targ et variable. Our method offers novelty compared to state-of-art techniques in proposing joint nonlinear reconstruction of both predictors and target variables, in a semi-supervised fashion, from a minimal-istic latent decomposition through dual-form nonlinearity. The majority of machine learning methods expect a target variable to be a con-sequence of, or directly related to, the p redictor variables. This study operates over the hypothesis that both the predictors and the target variables are observed effects of other hidden original factors/variables which are not recorded/known. Our method extracts original variables which can jointly approximate both pre-dictors and target variables in a nonlinear fashion. The current study claims that original variables contain less noise and therefore better predict the target variable, while empirical results of Section 5 demonstrate its validity.
Let us assume the unknown original data to be composed of D -many hidden variables in N training and N testing instances and denoted as Z  X  R ( N + N )  X  D . Assume we could observe M -many predictor variables X  X  R ( N + N )  X  M and one target variable Y  X  R N , with the aim of accurately pr edicting the test targets Y  X  R N . Semi-supervised scenarios where N &gt;N are taken into consideration. Our method learns the original variables Z and nonlinear functions g j ,h  X  R D  X  R which can jointly approximate X,Y . Equation 1 describes the idea, while we included natural Gaussian noise with variance  X  X , X  Y in the process. We introduce a syntactic notation N b a = { a, a +1 ,...,b  X  1 ,b } .
 As aforementioned, our novelty relies on l earning a latent low-rank representa-tion Z from observed data X,Y , such that the predictor variables and the target variable are jointly reconstructible from the low-rank data via nonlinear func-tions. Nonlinearity is achieved by expanding the low-rank data Z to a (probably much) higher-dimensional space R F space via a mapping  X  : R D  X  R F . Lin-ear hyperplanes V  X  R F  X  M ,W  X  R F with bias terms V 0  X  R M ,W 0  X  R can therefore approximate X,Y in the mapped space as described in Equation 2. 4.1 Maximum Aposteriori Optimization Consecutively the objective is to maximize the joint likelihood of the predic-tors X , target Y and the maximum aposteriori estimators V,V 0 ,W,W 0 as shown in Equation 3. The hyperplanes parameters incorporate normal priors V  X  X  (0 , X   X  1 V ) ,W  X  X  (0 , X   X  1 W ). The distribution of the observed variables is also assumed normal X  X  X  (  X  ( Z ) ,V , X  X )and Y  X  X  (  X  ( Z ) ,W , X  Y )and independently distributed. The logarithmic likelihood, depicted in Equation 4 converts the objective to a summation of terms.
Inserting the normal probability into E quation 4 converts logarithmic likeli-hoods into L2 norms with Tikhonov regularization terms as shown in Equation 5 and the variance terms  X  X , X  Y drop out as constants. An additional biased reg-ularization term  X  Z Z, Z is included in order to help the latent data avoid over-fitting.
Computing the  X  ( Z ) directly is intractable, therefore we will derive the dual-form representation in the next Section 4 .2, where the kernel trick will be utilized to compute Z in the original space R D . 4.2 Dual-Form Solution -Learning the Nonlinear Regression The optimization of Equation 5 is carried on in an alternated fashion. Hyper-plane weights V,V 0 ,W,W 0 are converted to dual variables and then solved by keeping Z fixed, while in a second step Z is solved keeping the dual weights fixed. This section is dedicated to lear ning the nonlinear weights in the dual-form. Each of the M -many predictors loss terms from Equation 5, (one per each predictor variable X : ,j ) can be learned isolated as described in the sub-objective function J j of Equation 6. To facilitate forthcoming derivations we multiplied the objective function by 1 2  X 
In order to optimize Equation 6, the equality conditions are added to the objective function through Lagrange multipliers  X  i,j . The inner minimization objective is solved by computing stationary solution points V : ,j ,V 0 j , X  : ,j and eliminating out the first derivatives (  X  X  j  X  X  in Equation 7.
Replacing the stationary point solution of V : , , X  : ,j,V 0 j back into the objective function 7, we get rid of the variables V : ,j , X  : ,j , yielding Equation 8.
The solution of the dual maximization is given through eliminating the deriva-
Combining Equation 9 and the constraint of Equation 8, the final nonlinear reconstruction solution is given through the closed-form formulation of  X  : ,j ,V 0 j as depicted in Equation 10.
Symmetrical to the predictors case, a dual-form maximization objective func-tion is created and a mot-a-mot procedure like Section 4.2 can be trivially adopted in solving the nonlinear regression for the target variable. The derived solution is shown in Equation 11. Instead of using the symbol  X  we denote the dual-weights of the target regression dual problem using the symbol  X  .
A prediction of the target value of a test instance t  X  N N + N N +1 is conducted using the learned dual weights as shown in Equation 12. Stochastic Gradient Descent -Learning the Low Dimensionality Rep-resentation. A novel algorithm is applied to learn Z for optimizing Equa-tion 8. Sub-losses composing only of  X  i,j , X  l,j are defined for all combinations  X  i  X  N N + N t 1 ,  X  l  X  N N + N t 1 and Z is updated in order to optimize each sub-loss in a stochastic gradient descent fashion as presented in Equation 13. The ad-dition of penalty terms controlled by the hyper-parameter  X  Z which controls the regularization of Z as described in Equation 5. Our model called Supervised Nonlinear Factorizations (SNF) utilizes polynomial kernels with the derivatives needed for gradient descent represented in Equation 13.

Algorithm 1 combines all the steps of the proposed method. During each epoch all predictors X  non-linear weights are solved and the latent data Z is updated. The target model is updated multiple times after each predictor model to boost convergence. The learning algorithm ma kes use of two different learning rates in updating Z , one for the predictors X  loss (  X  X ) and one for the target loss (  X  Y ). Algorithm 1. Learn SNF Five strong state of the art baselines and e mpirical evidence over eleven datasets are mainly the outline of our experiments, which will be detailed in this section, together with the results and their interpretation. 5.1 Baselines The proposed method Supervised Nonlin ear Factorizatoin s (SNF) is compared against the following five baselines:  X  Least Square Support Vector Machines (LS-SVM) [12] is a strong  X  Laplacian Manifold Regularization (Laplacian) [5], Hessian En- X  Linear Latent Reconstructions (LLR) [9] offers the possibility to un-5.2 Reproducibility All our experiments were run in a three fo ld cross-validation mode and the hyper-parameters of our model were tuned using only train and validation data. The evaluation metric used in all experim ents is the Mean Square Error (MSE).
SNF requires the tuning of seven hyper-parameters: the regularization weights  X 
Z , X  V , X  W , the learning rates  X  X , X  Y , the number of latent dimensions D and the degree of the polynomial kernel d . The search ranges of hyper-parameters are:  X  while the latent dimensionality was set to one of 50% , 75% of the original dimen-sions. The maximum number of epochs was set to 1000. A grid search method-ology was followed in finding the best combination of hyper-parameters. Please note that we followed exactly the same fair principle in computing the hyper-parameters of all baselines.

We selected eleven popular regression datasets in a random fashion from dataset repository websites. The select ed datasets are AutoPrice, ForestFires, BostonHousing, MachineCPU, Mpg, Pyrimidines, Triazines, WisconsinBreast-Cancer from UCI 1 ; Baseball, BodyFat from StatLib 2 ;Bears 3 . All the datasets were normalized between [-1,1] before usage. 5.3 Results The experiments comparing the accuracy of our method SNF against the five strong baselines were conducted in scenarios with few labeled instances, as typ-ically encountered in semi-supervised learning situations.

In the first experiment 5% labeled inst ances were selected randomly, while all other instances left unlabeled. Therefore, the competing methods had 5% target visibility and all methods, except LS-SVM, utilized the predictor variables of all the unlabeled instances. The results o f the experiments are shown in Table 1. The metric of evaluation is the Mean Square Error (MSE), while both the mean MSE and the standard deviation are shown in each dataset-method cell. The winning method of each baseline is highlighted in bold. As it is distinguishable from the sum of wins, SNF outperforms the baselines in the majority of datasets (six in total), while the closest competing baseline wins in only two datasets. Furthermore in datasets such as AutoPrice, BodyFat and Mpg the improvement is significant. Even when SNF is not the winning method, the margin to the first method is not significant, as it occurs in the Baseball, ForestFires and WisconsinBreastCancer datasets.
For the sake of completeness we repeated the experiments with another de-gree of randomly re-drawn labeled inst ances (10 % labeled instances). Table 2 presents the details of experiments over the selected eleven real -life datasets. The accuracy of SNF is prolonged even in th is experiment. The sum of the winning methods (depicted in bold) shows that SNF wins in five of the datasets against the only two wins of the closest baseline. In particular the cases of BodyFat and MachineCpu demonstrate significant improvements in terms of MSE. As shown by the results, even in cases where our method is not the first, still it is close to the winner.

In addition to the aforementioned results we extend our empirical analysis by conducting more fine-grained scale-up experiments with varying degree of labeled training instances. Figure 1 demonstrates the performance of all com-peting methods on a subset of datasets with a range of present labels varying from 5% up to 20%. SNF is seen to win in the earliest labeled percentages of the BostonHousing dataset (up to 10 %) while following in the later stages. On the contrary, we observe that our method dom inates in all levels of label presence in the scaled-up experiments involving the BodyFat and AutoPrice datasets. The accuracy of our method is grounded on a couple of reasons/observations. First of all, we would like to emphasize that each mentioned method is based on a different principle and modus operandi . Consequently, the dominance of a method compared to baselines depends on whether (or not) the datasets follow the principle of that particular method. Arguably the domination of SNF over manifold regularization baselines is due to the fact that our principle of mining hidden latent variables is likely (as result s show) more present in general real-life datasets, therefore SNF is suited to the detection of those relations. Throughout the present paper, a novel method that addresses the task of semi-supervised regression was proposed. The proposed method constructs a low-rank representation which jointly approximates the observed data via nonlinear func-tions which are learned in their dual formulation. A novel stochastic gradient descent technique is applied to learn the low-rank data using the obtained dual weights. Detailed experiments are conducted in order to compare the perfor-mance of the proposed method against five strong baselines over eleven real-life datasets. Empirical evidence over experiments in varying degrees of labeled instances demonstrate the efficiency of our method. The supervised nonlinear factorizations outperformed the manifold regularization state-of-art methods in the majority of experiments.

