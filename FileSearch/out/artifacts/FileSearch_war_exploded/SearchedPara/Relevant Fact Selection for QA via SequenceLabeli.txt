 Question answering (QA) is one of the key tasks in artificial intelligence and is not yet completely resolved. Given a query and a story containing several chaining facts, answering the query based on the story can be divided into two parts: identifying the relevant facts from the chaining facts and answering the question based on the relevant facts. Taking a sample from the bAbI dataset [13] as an instance (see below), to answer the question  X  X hat is Emily afraid of? X , we first identify Facts 7 and 5 from the chaining facts, then based on these two relevant facts, the answer  X  X olf X  is the output.
 its variants, try to jointly solve these two tasks by adding a memory component, which contains fully distributed semantics, to a recurrent neural network[6]. However, the contents in the memory are hard to analyze, especially when using neural networks to decide how to write the memory. In other words, it is hard to trace the problem according to the contents in the memory when an error occurs. Although there are some works studying the neural network reasoning (e.g.[7]), the capability of reasoning and inferencing of neural network is still questionable.
 jointly, we deal with the two tasks separately. This kind of separation makes the relevant fact selection and question answering evaluated independently. In this paper, we only focus on selecting relevant facts in chaining facts. To the best of our knowledge, this is the first work analyzing this problem. The goal of our research is to design a framework which can efficiently select relevant facts in chaining facts with limited amount of labelled data. The facts in a story can be interdependent such that reasoning and inferencing over the whole story is necessary during the process.
 dependent relationship among the chaining facts. CRF chooses the labels over the entire sequence at once which makes it utilize not only the local features but also the global information in defining the labels. In other words, CRF can define whether a fact is related to the given question based on inferencing over the whole story. In addition, as a discriminative model, CRF can model overlap-ping and non-independent features. Moreover, it is more traceable and requires less training data than neural networks based methods. The contributions of our work are as follow:  X  To select relevant facts from chaining facts, we propose a framework called  X  We evaluate the performance of CRF fact selector on the bAbI dataset. With  X  We apply the trained CRF fact selector to all the training and testing data Some studies solve question answering problems by manually defining string matching rules or learning text patterns [9][5]. These methods are ineffective when the chaining facts in a story are interdependent. In [2], recursive neural network (RNN) is used to model textual compositionality in question answer-ing, but the vanish problem of RNN would hinder the answering accuracy when the story is long[3]. Very recently, memory network (MemNN)[14] is introduced to solve the memorization problem of applying RNN to QA task by adding a long-term memory component to the RNN. MemNN introduced a generaliza-tion process which updates old memories given the new input and an output feature map which produces a new output given the new input and the current memory state. In the generalization process, a function H (  X  ) is responsible for selecting the slot. Theoretically, H (  X  ) should organize the memory and discard the least useful fact when the memory is full. The output component reading from memory and performing inference over the facts. The MemNN is trained in a fully supervised setting: besides the question answer, the supporting fact-s are also provided in the training data (but not in the test data). End-to-end memory networks (MemN2N)[10] realizes end-to-end training (only the question answer is provided in the training data) by using a continuous representation in the memory component. Dynamic memory networks (DMN)[4] add an attention mechanism in memory networks. In particular, the attention mechanism calcu-lates an attention score between the memory and the new input which indicates the importance of the new input, then the memory is updated according to the attention score. The attention score is computed using the Gated Recurrent Unit (GRU)[1]. MemNN and its related works solve the relevant fact selection joint-ly with question answering, the contents in the memory are hard to analyze, especially when using neural networks to decide how to write the memory. In addition, large amount of labelled data is required during the training process. tion in QA. However, the main CQA task can be defined as: given a new question and a large collection of question-comment threads created by a user community, rank the comments according to the usefulness in answering the new question. It is commonly solved as a ranking or clustering problem based on the similarities between the questions and existing answers (e.g.[16], [8] and [15]). Reasoning and understanding the interdependency among the chaining facts are often not involved in the process. This is different from our problem setting. The relevant fact selection in chaining facts is a binary classification of deciding if a fact is relevant (thus, will be selected) or not. In this paper, we propose a framework to select the relevant facts in chaining facts by sequence labelling. The process can be divided into 3 steps: fact reordering, fact to vector and fact selection. 3.1 Fact Reordering The facts of a story will be reordered to provide a more significant sequence pattern for the sequence labeling process. Three reordering methods are used in our experiment, namely ordering by subject, ordering by object and ordering by time. Ordering by subject is sorting the facts according to the subjects of the facts. Facts with the same subject will be put in a continuous sequence in a story. The other two types of ordering are similar. For a given task, we will choose a fact ordering which has the most significant sequence pattern. Specifically, when we treat the relevant fact selection as a binary classification, there are four patterns for the selection of a fact and its preceding fact:  X  Both of the current fact and its preceding fact are not selected  X  The current fact is selected while its preceding fact is not selected  X  The current fact is not selected while its preceding fact is selected  X  Both of the current fact and its preceding fact are selected The label of the four patterns can be represented as 00,01,10 and 11 respectively. Let T be the number of facts in a story and N be the amount of stories in the training set, the count of a pattern p is defined as Eq. 1 Let C = { c 00 ,c 01 ,c 10 ,c 11 } be the counts of the four patterns, the standard devi-ation of C is used to indicates the dispersion of the sequence labels. (Eq. 2) where  X  is the mean of C .
 calculate the  X  of different ordering methods and choose the one has the highest  X  value as the ordering method of the task. 3.2 Fact to Vector Given a story s containing T facts, each fact will be represented by a vector with a set of designed features. Let a t denotes the t th fact, the set of features are set, a word ID is assigned for each word. The IDs of the specific words which appear in a fact serve as the most basic features of the fact. Then we create two kinds of features to evaluate the distance between current fact a t and the question q , one is named as  X  X emi-global X  distance and the other is  X  X lobal X  distance. The semi-global distance is extracted from a t and its stream history (i.e. a 1 to a t  X  1 ). We first find the query object of the question, denoted as o q , then find a noun o t from f t , the semi-global distance of o t and o q , denoted as d , is evaluated by the least steps they can be connected according to a 1 to a t  X  1 . We use O to represent the set of objects (nouns) appearing in story s and question q . o t and o q are an object in a t and an object in q respectively. The process of semi-global distance calculation is listed in Algorithm 1. The semi-global distance is sequence sensitive, for the same o t , the d ( o t ,o q ) might be different when it appears in different facts. On the other hand, the global distance is extracted from a t and the whole story s . For a noun o t from a t and a query object o q , the global distance is evaluated by the closest path from o t to o q based on the whole story. The calculation process is similar to that of semi-global distance, but the input is the whole story s instead of a 1 to a t . The global distance is sequence insensitive, the global distance between a given o t and o q is consistent over the whole story. There might be more than one noun in a fact a t , let O t = { o t } denotes the set of nouns appear in a t , we use D t = { d ( o t ,o q ) ,o t  X  O t } and the minimum value in D t as the semi-global features between a t and o q . Similarly, D t = { d features between a t and o q .
 Algorithm 1: Semi-global distance calculation 3.3 Fact Selection We select relevant facts in chaining facts by sequence labelling, Conditional Random Field (CRF) is used as the tagging algorithm. CRF is an undirected graphical model that encode a conditional probability distribution using a set of features. Selection of relevant facts can be interpreted as follows. Recall T is the total number of facts in a story, each fact in the story can be represented according to the reordering process. The overall story x = { x 1 ,x 2 ,...,x T } can be viewed as a chain of the facts and linear-chain CRF will be used to predict the labels. A linear-chain CRF can be described as a factor graph over x and y which defines the conditional probability of y given x . Figure 1 indicates the structure of the linear-chain CRF. Eq. 3 The model parameters are a set of real weights  X  = {  X  k } , one weight for each fea-factor over all state sequences for the sequence x .
 Given the model defined in Eq. 3, the most probable labeling sequence for an input x is Eq.5 To estimate the parameters, recall we used N to represent the number of labeled examples, { ( x ( i ) ,y ( i ) ) } N i =1 , the log-likelihood of Eq. 3 is as Eq. 6. The Eq.6 is concave, and can be efficiently maximized by second-order techniques such as conjugate gradient and L-BFGS. We use the bAbI dataset which contains 20 different types of questions to eval-uate text understanding and reasoning. For each task, there are 1,000 questions for training and 1,000 questions for testing. A statistics overview of the dataset is listed in Table 1. We can notice that the lengths of the stories vary from 2 to 228 and the selected sentence proportion is less than 35% for most of the tasks, which means the classes of relevant facts and irrelevant facts are imbalanced. Task 17 is not involved in our experiment since all the facts in Task 17 are useful in answering the questions.
 1/10 of the supporting fact labels provided by the bAbI training set. That is, we train the CRF fact selector based on only 100 stories. The selection result is evaluated by F-measure. From Table 2, CRF fact selector achieves a perfect result in 13 tasks out of 19 tasks. For three tasks, 3, 14, 18, the F-scores (0.80 to 0.89) are relatively lower. This might be because we simply used the linear-chain CRF in our experiment, some of the long-term dependencies in these tasks might need to be modeled by higher order CRF.
 divide the tasks into two categories, namely information complete tasks and information incomplete tasks. Information complete tasks are tasks that CRF fact selector can identifies all the relevant facts from the chaining facts, contrary to the information incomplete tasks. The information complete tasks includ-ing task 1,4-7,8-13,15,16,19,20 while the information incomplete tasks are task 2,3,5,6,14,18 in the bAbI dataset. We compare the QA result of MemN2N 3 and LSTM[11] 4 with and without CRF fact selector. We combine CRF fact selector and MemN2N/LSTM by pre-selecting relevant facts from the chaining facts us-ing the trained CRF fact selector. Instead of using the whole stories, only the selected facts are used as the input. The result is shown in Table 3, the result of MemNN which is using a strongly supervised mode is also listed in the table for comparison. The CRFfs, C.I and I.I in the table are short of CRF fact selector, complete information and incomplete information.
 proved from 88% to 94% and 66% to 91% by using CRF fact selector in the 13 complete information tasks. The QA accuracy of MemN2N with CRF fact selector is even slightly higher than that of MemNN which uses strong supervi-sion. It shows that if irrelevant facts can be filtered correctly before inputting the facts to the QA systems, the accuracies of the answering part of the systems will be improved since there is less data to be analyzed by the system. On the other hand, CRF fact selector hinder the accuracy of MemN2N in 6 incomplete information tasks since some relevant facts are missing. An interesting phenom-ena is that the QA accuracy of LSTM improved by using CRF fact selector to pre-select the input even in incomplete information tasks. For the result of all the 20 tasks, the CRF fact selector can improve the average answering accuracy of MemN2N and LSTM from 87% to 90% and 62% to 86%, respectively. The accuracy of LSTM with CRF fact selector is comparable to that of MemN2N which means we have more flexibility in choosing the learning algorithm for QA by using CRF fact selector. In this paper, we showed that we could separate the two major steps in solving the QA problem. In particular, we consider the first step of selecting relevant facts. We proposed a CRF (conditional random field) fact selector and showed that it can effectively identify relevant facts and can be used to improve the accuracy of existing QA systems by pre-selecting relevant facts as inputs to the systems. We plan to further investigate how to improve the effectiveness and accuracy of relevant fact selection and also work on how to combine the relevant fact selection with the question answering part to achieve a better overall performance result.

