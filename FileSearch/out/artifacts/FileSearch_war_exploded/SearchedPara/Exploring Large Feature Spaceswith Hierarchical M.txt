 In the last two decades, kernel methods have been a prolific th eoretical and algorithmic machine learning framework. By using appropriate regularization b y Hilbertian norms, representer theorems implicit feature space no larger than the number of observat ions. This has led to numerous works on kernel design adapted to specific data types and generic kern el-based algorithms for many learning tasks (see, e.g., [1, 2]).
 recent years. While early work has focused on efficient algor ithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of inside the feature space. Indeed, feature spaces are large and we ex pect the estimated predictor function to require only a small number of features, which is exactly the situation where  X  1 -norms the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, applying multiple kernel learning directly in this decompo sition would be intractable. In order to peform selection efficiently, we make the extra as sumption that these small kernels can be embedded in a directed acyclic graph (DAG). Following [5], we consider in Section 2 a spe-patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of s elected kernels (Section 3). In simu-lations (Section 5), we focus on directed grids , where our framework allows to perform non-linear variable selection. We provide extensive experimental val idation of our novel regularization frame-petitive and often leads to better performance, both on synt hetic examples, and standard regression and classification datasets from the UCI repository.
 Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple ker-framework by giving necessary and sufficient conditions for model consistency. In particular, we Hence, by restricting the statistical power of our method, w e gain computational efficiency. We consider the problem of predicting a random variable Y  X  X   X  R from a random variable X  X  X , where X and Y may be quite general spaces. We assume that we are given n i.i.d. observations ( x i , y i )  X  X  X  Y functions are the square loss for regression, i.e.,  X  ( y,  X  y ) = 1 y  X  X  X  1 , 1 } , leading respectively to logistic regression and support v ector machines [1, 2]. 2.1 Graph-structured positive definite kernels We assume that we are given a positive definite kernel k : X  X X  X  R , and that this kernel can be expressed as the sum, over an index set V , of basis kernels k map of k omitted and can always be inferred from the context.
 Our sum assumption corresponds to a situation where the feat ure map  X ( x ) and feature space F for k is the concatenation of the feature maps  X  is equivalent to looking jointly for  X  As mentioned earlier, we make the assumption that the set V can be embedded into a directed acyclic descendants and ancestors . Given a node w  X  V , we denote by A( w )  X  V the set of its ancestors, and by D( w )  X  V , the set of its descendants. We use the convention that any w is a descendant and an ancestor of itself, i.e., w  X  A( w ) and w  X  D( w ) . Moreover, for W  X  V , we let denote sources( W ) the set of sources of the graph G restricted to W (i.e., nodes in W with no parents belonging to W ). Given a subset of nodes W  X  V , we can define the hull of W as the union of all ancestors of w  X  W , i.e., hull( W ) = S defined, as T The goal of this paper is to perform kernel selection among th e kernels k relevant vertices; in Section 2.2, we design a specific spars ity-inducing norms adapted to hulls. kernels. Namely, we assume that the input space X factorizes into p components X = X and that we are given p sequences of length q + 1 of kernels k Figure 1: Example of graph and associated notions. (Left) Ex ample of a 2D-grid. (Middle) Example sources of the set of all red points ( + ). { 0 , . . . , q } , such that k ( x, x  X  ) = P q j DAG on V = Q p of selecting a given product of kernels only after all the sub products are selected. Those DAGs Polynomial kernels We consider X to than q ), since our kernel considers polynomials of maximal degree q .
 Gaussian kernels We also consider X following decomposition is the eigendecomposition of the n on centered covariance operator for a normal distribution with variance 1 / 4 a (see, e.g., [6]): where c 2 = a 2 + 2 ab , A = a + b + c , and H single Hermite polynomials, and the ( q + 1) -th kernel is summing over all other kernels, we ob-tain a decomposition of a uni-dimensional Gaussian kernel i nto q + 1 components ( q of them are one-dimensional, the last one is infinite-dimensional, but can be computed by differencing). The decomposition ends up being close to a polynomial kernel of i nfinite degree, modulated by an ex-ponential [2]. One may also use an adaptive decomposition using kernel PCA (see, e.g., [2, 1]), which is equivalent to using the eigenvectors of the empiric al covariance operator associated with the data (and not the population one associated with the Gaus sian distribution with same variance). In simulations, we tried both with no significant difference s.
 of subsets) with the inclusion DAG. In this setting, we can de compose the ANOVA kernel [2] as Q framework will select the relevant subsets for the Gaussian kernels.
 Kernels or features? In this paper, we emphasize the kernel view , i.e., we are given a kernel (and i.e., we have a large structured set of features that we try to select from; however, the techniques developed in this paper assume that (a) each feature might be infinite-dimensional and (b) that we thus seems slightly more natural. 2.2 Graph-based structured regularization Given  X   X  Q Penalizing with this norm is efficient because summing all ke rnels k mial time and we can bring to bear the usual kernel machinery; however, it does not lead to sparse solutions, where many  X  As said earlier, we are only interested in the hull of the sele cted elements  X  need to determine which v  X  V are such that D( v )  X  I c . In our context, we are hence looking at selecting vertices v  X  V for which  X  We thus consider the following structured block  X  1 -norm defined as P P will indeed impose that some of the vectors  X  Our Hilbertian norm is a Hilbert space instantiation of the h ierarchical norms recently introduced particular choice of norms corresponds to an  X   X  1 -norm of  X  2 -norms X . While with uni-dimensional when the DAG is a not a tree. In Section 3, we propose a novel alg orithm to solve the associated optimization problem in time polynomial in the number of sel ected groups/kernels, for all group sizes, DAGs and losses. Moreover, in Section 4, we show under which conditions a solution to the problem in Eq. (1) consistently estimates the hull of the spa rsity pattern.
 its ancestors [5]. This is another explanation why hulls end up being selec ted, since to include a given vertex in the models, the entire set of ancestors must a lso be selected. In this section, we give optimality conditions for the probl ems in Eq. (1), as well as optimization algorithms with polynomial time complexity in the number of selected kernels. In simulations we to the success of hierarchical multiple kernel learning (HK L). 3.1 Reformulation in terms of multiple kernel learning Following [8, 9], we can simply derive an equivalent formula tion of Eq. (1). Using Cauchy-Schwarz inequality, we have that for all  X   X  R V such that  X  &gt; 0 and P with equality if and only if  X  that if  X  set of allowed  X  and Z the set of all associated  X  . The set H and Z are in bijection, and we can interchangeably use  X   X  H or the corresponding  X  (  X  )  X  Z . Note that Z is in general not convex 2 (unless the DAG is a tree, see [10]), and if  X   X  Z , then  X  descendant kernels are smaller, which is consistent with th e known fact that kernels should always be selected after all their ancestors.
 The problem in Eq. (1) is thus equivalent to Using the change of variable  X   X  the optimal  X  (and associated  X  ),  X  corresponds to the solution of the regular supervised learn ing problem with kernel matrix K = P with kernel k dual parameters associated with the single kernel learning problem.
 Thus, the solution is entirely determined by  X   X  R n and  X   X  R V (and its corresponding  X   X  R V ). More precisely, we have (see proof in [10]): Proposition 1 The pair (  X ,  X  ) is optimal for Eq. (1), with  X  w,  X  only if ( a ) given  X  ,  X  is optimal for the single kernel learning problem with kerne l matrix K = P Moreover, the total duality gap can be upperbounded as the su m of the two separate duality gaps for that in the case of  X  X lat X  regular multiple kernel learning, w here the DAG has no edges, we obtain back usual optimality conditions [8, 9].
 Following a common practice for convex sparsity problems [1 1], we will try to solve a small problem where we assume we know the set of v such that k  X  leaving out is exponential (Section 3.2). 3.2 Conditions for global optimality of reduced problem We let denote J the complement of the set of norms which are set to zero. We thu s consider the optimal solution  X  of the reduced problem (on J ), namely, with optimal primal variables  X  by  X  = P Proposition 2 ( N in the extreme points of J are active, then we have max Proposition 3 ( S then the total duality gap is less than  X  .
 exponentially many dimensions in polynomial time.
 The necessary condition ( N condition ( S practice (as shown in Section 5, we consider V of cardinal often greater than 10 30 ). Here, we need in this paper, if d and we can compute the sum over all v  X  D( t ) in linear time in p . Moreover we can cache the sums P 3.3 Dual optimization for reduced or small problems When kernels k of the single kernel learning problem with kernel matrix P to minimizing B (  X  (  X  )) with respect to  X   X  H .
 Moreover, the function  X  7 X   X  (  X  ) is differentiable on ( R  X  use the same projected gradient descent strategy as [8] to mi nimize it. The overall complexity of algorithm is only used for small reduced subproblems for whi ch V has small cardinality. 3.4 Kernel search algorithm the sufficient condition ( S The previous algorithm will stop either when the duality gap is less than  X  or when the maximal number of kernels Q has been reached. In practice, when the weights d v in the DAG (which we use in simulations), the small duality ga p generally occurs before we reach fact that we may actually know that we have an  X  -optimal solution.
 In order to obtain a polynomial complexity, the maximal out-degree of the DAG (i.e., the maximal number of children of any given node) should be polynomial as well. Indeed, for the directed p -grid (with maximum out-degree equal to p ), the total running time complexity is a function of the number of observations n , and the number R of selected kernels; with proper caching, we obtain the caching O ( Rp ) kernels, and computing O ( R 2 p ) quadratic forms for the sufficient conditions. simplicity, we consider the case of finite dimensional Hilbe rt spaces (i.e., F and we let n tend to infinity and  X  =  X  Following [4], we make the following assumptions on the unde rlying joint distribution of ( X, Y ) : (a) the joint covariance matrix  X  of ( X ( x is invertible, (b) E ( Y | X ) = P surely. With these simple assumptions, we obtain (see proof in [10]): Proposition 4 (Sufficient condition) If max &lt; 1 , then  X  and the hull of W are consistently estimated when  X  n n 1 / 2  X  X  X  and  X  n  X  0 . Proposition 5 (Necessary condition) If the  X  and the hull of W are consistently estimated for some sequence  X  Note that the last two propositions are not consequences of t he similar results for flat MKL [4], because the groups that we consider are overlapping. Moreov er, the last propositions show that we Figure 2: Comparison on synthetic examples: mean squared er ror over 40 replications (with halved standard deviations). Left: non rotated data, right: rotat ed data. See text for details. pumadyn-32fh 8192 32 pol4  X  10 22 57.3  X  0.7 56.4  X  0.8 57.5  X  0.4 56.4  X  0.7 56.4  X  0.8 pumadyn-32fh 8192 32 rbf  X  10 31 57.7  X  0.6 72.2  X  22.5 89.3  X  2.0 56.5  X  0.8 55.7  X  0.7 pumadyn-32fm 8192 32 pol4  X  10 22 6.9  X  0.1 6.4  X  1.6 7.5  X  0.2 7.0  X  0.1 3.1  X  0.0 pumadyn-32fm 8192 32 rbf  X  10 31 5.0  X  0.1 46.2  X  51.6 44.7  X  5.7 7.1  X  0.1 3.4  X  0.0 pumadyn-32nh 8192 32 pol4  X  10 22 84.2  X  1.3 73.3  X  25.4 84.8  X  0.5 83.6  X  1.3 36.7  X  0.4 pumadyn-32nh 8192 32 rbf  X  10 31 56.5  X  1.1 81.3  X  25.0 98.1  X  0.7 83.7  X  1.3 35.5  X  0.5 pumadyn-32nm 8192 32 pol4  X  10 22 60.1  X  1.9 69.9  X  32.8 78.5  X  1.1 77.5  X  0.9 5.5  X  0.1 pumadyn-32nm 8192 32 rbf  X  10 31 15.7  X  0.4 67.3  X  42.4 95.9  X  1.9 77.6  X  0.9 7.2  X  0.1 Table 1: Mean squared errors (multiplied by 100) on UCI regre ssion datasets, normalized so that the total variance to explain is 100. See text for details. we can ensure correct hull selection. Finally, it is worth no ting that if the ratios d case [4], in terms of pattern selection and universal consis tency. Synthetic examples We generated regression data as follows: n = 1024 samples of p  X  [2 2 , 2 7 ] variables were generated from a random covariance matrix, a nd the label y  X  R was sampled as a random sparse fourth order polynomial of the input variable s (with constant number of monomials). We then compare the performance of our hierarchical multipl e kernel learning method (HKL) with the polynomial kernel decomposition presented in Section 2 to other methods that use the same were transformed by a random rotation (in this situation, th e generating polynomial is not sparse anymore). We can see that in situations where the underlying predictor function is sparse (left), HKL outperforms the two other methods when the total number o f variables p increases, while in in non sparse problems,  X  1 -norms do not really help, but do help a lot when sparsity is ex pected. UCI datasets For regression datasets, we compare HKL with polynomial (de gree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to th e following approaches with the same (MKL). For all methods, the kernels were held fixed, while in T able 1, we report the performance for the best regularization parameters obtained by 10 rando m half splits.
 We can see from Table 1, that HKL outperforms other methods, i n particular for the datasets bank-32nm, bank-32nh, pumadyn-32nm, pumadyn-32nh, which are da tasets dedicated to non linear re-gression. Note also, that we efficiently explore DAGs with ve ry large numbers of vertices #( V ) . greedy) in Table 2. For some datasets (e.g., spambase), HKL w orks better, but for some others, in particular when the generating problem is known to be non spa rse (ringnorm, twonorm), it performs slightly worse than other approaches. We have shown how to perform hierarchical multiple kernel le arning (HKL) in polynomial time in the number of selected kernels. This framework may be applie d to many positive definite kernels and we have focused on polynomial and Gaussian kernels used f or nonlinear variable selection. feature space. We are currently investigating application s to string and graph kernels [2].
