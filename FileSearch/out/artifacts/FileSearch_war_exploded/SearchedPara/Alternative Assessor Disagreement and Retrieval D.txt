 Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagree-ment are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that a second assessor will disagree with the relevance assessment of the initial assessor, and find that there is a strong and consistent correlation between the two. We adopt a metarank method of summarizing a docu-ment X  X  rank across multiple runs, and propose a logistic regression predictive model of second assessor disagreement given metarank and initially-assessed relevance. The consistency of the model pa-rameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute system scores, but less consistent predictions of rela-tive scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Retrieval experiment, evaluation, sampling Measurement, performance, experimentation
When two humans are asked to assess the one document for rel-evance to a topic, they are surprisingly likely to disagree. One study finds that even expert assessors with similar backgrounds have only a 60% probability of agreeing that a document is rele-vant [Voorhees, 2000]. Not just the set of actually relevant doc-uments, but the measured reliability of a document retrieval, can vary substantially depending upon which assessor is used; and in human-intensive document productions, the results of comparative evaluation can depend on whether you agree with the humans who developed the production, or the humans who assessed it [Oard et al., 2008].

While several studies observe assessor disagreement, and a few have investigated its impact upon evaluation, little work has been done on characterizing, modeling, and predicting disagreement. It is not known what share of disagreement is attributable to assessor inattention, what to differing relevance conceptions, and what to variable thresholds for detecting relevance. Identifying correlates of disagreement will help predict, adjust for, and correct assessor disagreement and its impact upon evaluation; and better determin-ing the nature and causes of disagreement will enable preventative steps to be taken, and improve our understanding of the human per-ception of relevance.

In this paper, we examine a potential correlate of assessor dis-agreement, namely the ranks at which a document is retrieved by a set of retrieval systems. We summarize retrieval rank information across the run set using metarank scores, similar to the score aggre-gation methods used to metasearch. Working with the same TREC 4 and TREC 6 AdHoc datasets as Voorhees [2000], we then esti-mate the probability of second-assessor disagreement given metarank score, developing separate logistic models for documents assessed relevant and assessed irrelevant by an initial assessor. Making the model dependent upon the first assessor X  X  assessment reflects the question facing the practicing evaluator after an initial assessment: which of these assessments should I have checked by a second as-sessor?
Our experiments find the relationship between metarank and as-sessor disagreement to be a strong one; a high metarank document assessed relevant by one assessor is almost twice as likely to be as-sessed irrelevant by a second assessor as a low metarank one, and the relationship is even stronger for documents assessed as irrele-vant by a first assessor. The strength of the relationship, however, varies markedly between different topics; there are clearly other, topic-dependent factors at play in determining assessor disagree-ment, and a universal model has limited predictivity.

Models of disagreement by rank can be used to create artificial document assessment sets (or qrels), to simulate and explore the effects of assessor disagreement. Qrels simulated from per-topic rank-sensitive models give much more accurate estimates of ab-solute scores under alternate assessment than do rank-insensitive flip-rate models. Rank-sensitive simulated qrels, however, provide less stable relative evaluations and system rankings than flip-rate qrels, at least for the metarank scoring method we use. Metarank s core is evidently not independent of system, and (we hypothesize) the same reinforcement of like systems can be observed here as in other simulated relevance methods [Soboroff et al., 2001].
We demonstrate that rank-sensitive models based on sampled dual assessment produce absolute and relative score simulations almost as reliable as those of exhaustive dual assessment. More-over, sampled rank-sensitive models are more reliable than sam-pled rank-insensitive ones even for relative evaluation. Thus, one can predict the effect of assessor disagreement with only a fraction of repeat assessment effort.

Traditional tests of the significance of a comparative retrieval evaluation consider only the topics as variable, holding documents, assessors, and other aspects of the experiment fixed. With reliable, sample-based models of assessor disagreement available, variabil-ity in the assessor dimension can be simulated at a fraction of the cost of exhaustive multiple assessment. We demonstrate the use of our rank-sensitive model to determine evaluation significance with tests in which assessors can vary.

The remainder of the paper is structured as follows. Related work is surveyed in Section 2, and our materials and methods in Section 3. Section 4 describes our experiments, and Section 5 sum-marizes our findings and sketches future work.
The high level of inter-assessor disagreement on relevance has been noted by a number of studies. Voorhees [2000] examines multiple assessments of the TREC 4 and TREC 6 AdHoc collec-tions (the same datasets used here), finding overlap of between 0 . 42 and 0 . 49 on TREC 4. Roitblat et al. [2010] find even lower levels of inter-assessor agreement on an e-discovery production. Bailey et al. [2008] survey earlier studies with similar results.
Several studies have concluded that impact of assessor disagree-ment upon the comparative (rather than absolute) evaluation of au-tomated retrieval systems is minor. Voorhees [2000] finds a mean Kendall X  X   X  of 0 . 938 between system rankings produced by differ-ent assessors, suggesting a high degree of stability between assess-ment sets. Trotman and Jenkinson [2007] compare using multiple (non-overlapping) assessors per topic with a single assessor, and find a mean Spearman X  X  rank correlation coefficient of 0 . 986 . The effect upon relative assessment may be greater for runs created with a large amount of manual involvement, however, including through training classifiers; the correction of assessor errors led to large relative score changes in the interactive task of the TREC Legal Track [Oard et al., 2008].

The interactive task of the TREC Legal Track corrects assessor errors through participant appeals and adjudication by a topic au-thority [Oard et al., 2008]. Webber et al. [2010] propose instead that assessments be sampled for authoritative verification, and error rates estimated from these samples. Sheng et al. [2008] investigate using multiple overlapping assessors in annotation tasks.
Cuadra and Katter [1967] identify five factors influencing per-ceptions of relevance: document variables; topic statement vari-ables; judgment conditions; judgment scales; and personal factors. Saracevic [2007] surveys of experiments on these factors. Webber et al. [2010] present a taxonomy of topical grounds for appeal from the appeal documents submitted by a TREC Legal Interactive par-ticipant. Re-analyzing appealed assessments from the Interactive task, Grossman and Cormack [2011] conclude that the great major-ity of assessor error is due to inarguable failure to follow the task X  X  detailed relevance assessment guidelines; however, their sample is biased by that fact that participants only appealed assessments they Orig A lt 1 Alt 2 Total
Rel 4.2% 4.0% 5.1% 3.1% 8.2% ! Rel 4.8% 87.0% 7.5% 84.3% 91.8% Total 9.0% 91.0% 12.6% 87.4% Table 1: Macro-averaged estimated proportional contingency t ables between original and two alternatives assessors across the TREC 4 AdHoc topics. Agreement observed on sampled documents extrapolated to the rest of the pool. felt to be inarguable errors. Webber et al. [2012] find that more detailed instructions do not lead to fewer assessor errors.
Aslam et al. [2005] present a metasearch approach known as meta-AP in which documents are weighted by their implicit aver-age precision scores in each ranking; we adopt meta-AP as a predic-tor of assessor disagreement in our models. Aslam and Montague [2001] and Lee [1997] describe simpler metasearch scoring meth-ods, similar to our baseline predictors. As part of an evaluation score estimation method known as minimal test collection (MTC), Carterette [2007] explicitly builds a multi-level logistic model of probability of relevance based on retrieval rank and system reli-ability. The relative values of the estimated scores are a reliable estimate of full assessment, but the absolute estimated values are not, suggesting that absolute probabilities of document relevance are misestimated.

Soboroff et al. [2001] explore randomly assigning relevance as-sessments to documents, and find that the system ranking that re-sults is moderately correlated with the human-assessed ranking. Carterette and Soboroff [2010] use variable flip rate probabilities to simulate  X  X onservative X  and  X  X iberal X  assessors, finding that  X  X on-servative X  assessors maintain stable system rankings, while  X  X ib-eral X  assessors disrupt them.

Voorhees [1998] introduces the use of Kendall X  X   X  as a mea-sure of the stability of system ranking in the face of changes in the evaluation setup. Savoy [1997] proposes the use of Bootstrap significance tests in information retrieval evaluation. Bodoff and Li [2007] argue that choice of assessor should be included alongside choice of topics in assessing the generalizability of information re-trieval evaluation results.
This section describes our data sets and methods. For data (Sec-tion 3.1), we use the TREC 4 AdHoc collection, runsets, and qrels, including multiple assessments performed by TREC assessors; we also use the TREC 6 collection, with additional assessments per-formed by one of the track participants. We model assessor dis-agreement using logistic regression, with document rank as a pre-dictor (Section 3.2). Our experiments randomly generate qrels fol-lowing models built from the dataset, and investigate the stability of system evaluation using them (Section 3.3).
The TREC 4 AdHoc test collection consists of 49 topics. Doc-uments for assessment were selected by depth-100 pooling. In ad-dition to the 33 systems that ran on the full collection in the ad-hoc task, all of which were pooled, additional pooled documents were drawn from systems that ran on a subset of the collection, or ran in a different modality [Harman, 1995]. Only the 33 adhoc full-collection runs are included in this study. Initial assessment Table 2: Macro-averaged estimated proportional contingency t ables between original and alternative (Waterloo) assessors across the TREC 6 AdHoc topics. was performed by the author of the topic; we refer to this as the  X  X riginal X  assessor. Each topic was then re-assessed by two other TREC assessors, whom we refer to as the first and second alter-native assessors. We follow Voorhees [2000] in dropping Topic 214, as the first alternative assessor found no documents relevant for it. If there were more than 200 relevant or irrelevant docu-ments in the pool, then 200 were uniform randomly sampled for re-assessment [Voorhees, 2000]. The average number of reassessed relevant documents is 105 , with 12 topics having 200 or more rel-evant documents; all topics have more than 200 irrelevant docu-ments, with the average being 1 , 627 . Though Voorhees [2000] does not describe a systematic difference in the allocation of assessors as first and second alternate assessors, the first assessor finds an average of 17 . 3 more document relevant than the second assessor (sd = 51 . 2 ). This difference is statistically significant ( p = 0 . 02 in two-tailed, pair t test). The reason for this difference is not known; it may simply be non-randomness in assigning additional assessors (for instance, assessors who finished their initial assessments faster may have been more likely to be assigned as first than as second alternative assessors).

Table 1 gives macro-averaged proportional contingency tables between the original and the two alternative assessors for the TREC 4 AdHoc topics. We estimate agreement on the pool from agree-ment on the sample. In contrast, Voorhees [2000] calculates statis-tics on the sample only. Since a higher proportion of originally-relevant documents is sampled than of originally-irrelevant (a macro-average of 91.3% of the former, versus 13.6% of the latter), agree-ment on the sample tends strongly to overstate agreement on the pool. So, for instance, Voorhees [2000] quotes a (sample) positive overlap with the original assessor of 0 . 421 for the first and 0 . 494 for the second alternative assessor, whereas the estimated figures on the pool are 0 . 301 and 0 . 340 , respectively. (The pool itself is a se-lective, non-random sample. Agreement on the full collection, had it been exhaustively assessed or randomly sampled from, would be different yet again, and probably lower still, since the number of originally-irrelevant documents would greatly increase.)
The TREC 6 AdHoc multiple-assessment dataset was created as part of a run production by the TREC participant team from the University of Waterloo [Cormack et al., 1997]. The run was pro-duced by interactively developing queries, retrieved ranked results, and judging of top-ranked documents. The documents assessed as part of this process constitute the  X  X lternative X  assessments to the (subsequently formed) official assessments of the TREC assessors. The Waterloo assessors used a three-level relevance scale, with a middle grade of marginally relevant; following Voorhees [2000], we collapse the marginally relevant documents to not relevant.
Besides being performed by assessors with a different background and mode of operation from the official assessors, the TREC 6 al-ternative assessments differ from those from TREC 4 in that they are not a random sample. Rather, they are the documents returned Figure 1: Metarank weights for Meta-AP and Inverse Rank w eighting schemes, for a single document ranking. at top ranks by a series of interactive queries. The non-randomness of the selection of the alternative assessments makes it impossi-ble to extrapolate agreement measures to the full TREC pool, and may bias models built upon dual-assessed documents; the random sampling of the pool used in TREC 4, however, should ensure that the models have only sampling error, not systematic bias, though only for pooled documents. The proportional macro-averaged con-tingency table for the TREC 6 dataset is shown in Table 2. The macro-averaged overlap is 0 . 328 .
We propose to model assessor disagreement as a function of doc-ument rank. This requires three components: a statistic for summa-rizing rank information (Section 3.2.1); a modeling method (Sec-tion 3.2.2); and a way of choosing which statistic provides the best fit for the data under the modeling method adopted (Section 3.2.3).
A metarank measure summarizes the ranks at which a document is returned across a set of runs for a single topic. (Where metarank is used to form an aggregate ranking, the technique is referred to as metasearch.) Let N be the rank to which metarank measures are scored. We say that document returned in a run s at rank k has an inverse rank I in that run of N  X  k , or 0 if k &gt; N . Two simple measures are then maximum and mean inverse rank, which can be compared to the metasearch methods CombMax [Lee, 1997] and Borda count [Aslam and Montague, 2001].

Aslam et al. [2005] propose a metarank measure based upon the weighting of the average precision (AP) metric. For AP evaluating to depth N , the implicit weight of a document at rank k is 1 + H N  X  H k , where H n is the n  X  X h harmonic number, or 0 if k &gt; N . The mean meta-AP score for a document is its average AP weight across the set of runs.

We take N = 1 , 000 , the run depth in the ad-hoc tracks of TREC, as our metarank evaluation depth . The meta-AP score for rank 1 in evaluation to this depth is 7 . 5 . The relationship between meta-AP and inverse rank as a function of rank is shown in Figure 1.
We predict the probability that a document will be judged rele-vant by assessor B , based upon its metarank and the fact that asses-sor A h as judged it either relevant or irrelevant: p ( B = 1 | s, A = r ) , building separate models for A = 1 and A = 0 . We apply a logistic regression to this problem: where the metarank score s is the predictor variable, and the prob-ability p is the predicted value. The probability of disagreement is p ( B = 1) if A = 1 , and 1  X  p ( B = 1) is A = 0 . Lo-gistic regression is preferable to linear regression since the latter produces  X  X robabilities X  outside of the range [0 , 1] ; logistic regres-sion is more robust than smoothed average methods to sparse data, which can occur for topics that have very few relevant documents.
The fitted value  X  0 in Equation 1 is the intercept, which gives the log-odds of relevance when the score is 0 , while  X  1 coefficient, which gives the change or  X  X lope X  in log odds of rel-evance for every one point increase in metarank. The slope gives the strength of the relationship between metarank and probability of relevance, while the intercept shifts the regression curve up or down the score axis. An intercept of  X  1 means there is a 1 : e  X  27% chance of relevance when the score is 0 . Conversely, a slope of 1 means than an increase of 1 in the score will take probabilities from 1 : 1 = 50% to e : 1  X  73% . If we regard the observed documents as a sample from a larger population, then the slope on the sample is predictive of the population, and this prediction can be checked for statistical significance. Where all documents re-ceive the same alternative relevance assessment, no proper model can be constructed, though 0 or 1 probabilities can be assigned to all scores.

A model can be built for each topic individually, or else alterna-tive assessments and metarank scores from the full collection can be pooled into a single model. The degree to which a per-collection or  X  X niversal X  model is a good approximation for per-topic mod-els depends upon the strength of per-topic factors in influencing disagreement. The closer the per-collection model is to the per-topic models, the more likely it is that a generalized model can be built that is able to predict assessor disagreement on new collec-tions based only on metarank scores.

A simpler, rank-insensitive model of assessor disagreement is the flip rate model, which notes the proportion of originally-relevant documents that the alternative assessor assesses as irrelevant, and vice versa. Taking the original assessor as the objective standard, the flip rate is a pair of proportions, giving the false-positive and false-negative rates for the alternative assessor.
There are various ways to measure the goodness-of-fit of a logis-tic regression. A simple measure, found by [Hosmer et al., 1997] nevertheless to be more powerful than many more complex ones, is the unweighted residual sum-of-squares (RSS): The sum across all n observations i ; y i is the actual value of ob-servation i (here, whether document i is relevant or not), and b  X  the estimated probability of relevance for an item with the predictor variables of item i (here, the metarank of document i ). Instead of RSS itself, we report RMSE: The RMSE value is rank-equivalent to RSS for a single topic, but is more comparable between topics; moreover, being in the same units as the probability values themselves, it is indicative of the degree of deviation of estimated probability from actual (0, 1) rel-evance.
Our experiments in Sections 4.3, 4.4, and 4.5, involve generating qrels that simulate the assessments of an alternative assessor, and then examining the stability of system scores and rankings. To do this, we build a model of assessor disagreement from the original assessments, the metarank predictor, and a set of observed alter-native assessments. The simulated alternative qrels are generated by applying the predictive model to the original qrels. Each orig-inal assessment is plugged into the model with its metarank score to generate a probability of assessor disagreement. An indepen-dent uniform random number U in the range [0 , 1] is then gener-ated. If U is less than the flip probability, then document relevance is flipped in the simulated qrels; otherwise, the original document relevance is maintained. The same process is used for the metarank and the flip-rate models, except the latter takes no account of doc-ument metarank, and also for per-topic and universal models.
In Section 4.4, we compare the stability of our models when based on exhaustive and on sampled dual assessment. For the sampled dual assessment, n originally-relevant and n originally-irrelevant documents (where n = 20 is used in our experiments) are sampled, and the metarank and flip-rate models are built using just the samples. Uniform random sampling is used for both the flip-rate and the metarank models.

For the system score stability experiments in Sections 4.3 and 4.4, we generate s = 1 , 000 simulated alternative qrel sets for each model, and calculate the MAP scores achieved by systems for each alternative qrel set. The means and the 2.5% and 97.5% percentiles of these MAP scores are reported. Absolute change in system score is calculated by root mean squared error (RMSE) between the ob-served and all of the simulated MAP scores. Relative stability is calculated using mean Kendall X  X   X  between the system ranking un-der the observed alternative assessments and under the simulated qrels.
In this section we present a series of experiments on various as-pects of the model and its use. We start with the choice of predictor (Section 4.1), then evaluate the fidelity of the model of disagree-ment (Section 4.2). We next use the model to simulate qrels for use in calculating MAP in TREC evaluation experiments (Section 4.3). We investigate the use of sampling dual assessments to fit better models (Section 4.4), and finally investigate the use of simulated qrels to estimate variance due to differences in assessor when test-ing significance (Section 4.5).
Section 3.2.1 described three alternative metarank measures: mean meta-AP; mean inverse rank; and maximum inverse rank. Figure 2 compares the goodness-of-fit of these these metarank measures, using RMSE of predicted and actual value (Section 3.2.3), and compares them with the simple, rank-insensitive flip-rate model. There is not a great apparent difference in RMSE scores, but meta-AP gives significantly ( p &lt; 0 . 01 ) better fit than both alternative metarank schemes on both conditions, except when compared to mean inverse rank on the given-irrelevant condition, where the dif-ference is not significant. All three metarank models are signifi-cantly better fits than the flip rate model ( p &lt; 0 . 001 ). We select meta-AP as the metarank measure for the remaining experiments, and compare it with flip-rate in later sections. Figure 2: Goodness-of-fit in RMSE of models built from dif-f erent metarank measures, for the given-relevant and given-irrelevant cases, on the TREC 4 AdHoc dataset, averaged across all 48 topics and both first and second alternative as-sessors. Lower values indicate better fit.
 Dataset Condition P ositive Improper Negative
T4, alt1 p (  X  | 1) 23 19 2 5 0
T4, alt2 p (  X | 1) 21 24 0 4 0 Table 3: Number of per-topic models for TREC 4, both alter-n ative assessors, and TREC 6 datasets, giving ( p &lt; 0 . 05 ) sig-nificantly and non-significantly positive, improper, and signifi-cantly and non-significantly negative slopes.
The number of per-topic models giving slopes of different direc-tions and significances across both datasets is tabulated in Table 3 (see Section 3.2.2 for the mean of these values). Over 80% of mod-els show positive slopes (90% if improper models are ignored), and 60% of these are significant. None of the models for the randomly-sampled TREC 4 alternative assessments have a significantly neg-ative slope, and only two of the determinately-sampled TREC 6 models have a significantly negative slope. In summary, there is a strong and consistent relationship between document rank and the probability that an alternative assessor will disagree with an origi-nal assessor X  X  assessment.

The dispersion of slope and intercept coefficients for per-topic and universal models is shown in Figure 3, for the first alternative assessor of TREC 4. As a group, models for initial assessments of relevant have higher probability of alternative relevance (higher intercepts) than models for initial assessments of irrelevant, as one would expect. Parameters are highly variable in both slope and in-tercept between topics, however; the universal model for each con-Figure 3: Per-topic and universal logistic regression coefficients f or the first alternate assessor on the TREC 4 AdHoc dataset. The diagonal line shows the combination of intercept and slope for which the probability of alternative-assessor relevance at a meta-AP score of 0 is 50% . Figure 4: Universal logistic regression models for TREC 4 and T REC 6, with 95% error bounds. The TREC 4 model combines both alternative assessors. dition, though centered amongst the per-topic models, does not well characterize the spread of the per-topic models. Metarank alone (at least as measured by meta-AP) fails to capture all of the variance in assessor disagreement, even for the one collection; other, topic or assessor-pair, features also have an impact.

The universal regression models for TREC 4 and TREC 6 are shown in Figure 4. We have included both the first and the second alternative assessors in the TREC 4 model. A marked difference between TREC 4 and TREC 6 can be observed, particularly for the given-irrelevant model; this is likely due to the different selection and assessment methods for the TREC 6 alternative assessments. For the TREC 4 universal model, we can see that a relevant as-sessment on a high-ranking document is almost certain to be con-firmed by a second assessor, whereas a relevant judgment on a low-ranking document is as likely as not to be overturned. The relation-ship is even stronger for documents initially assessed as irrelevant: the alternative assessor is very likely to agree with the initial as-s essment for low-ranked documents, but for the (rare) high-ranked documents initially assessed as relevant, the alternative assessor is very likely to disagree with the initial assessment.

Figure 5 gives regressions for three example topics. For Topic 215, the given-relevant model dominates the given-irrelevant one, and predicts over four times the probability of alternative-relevance for high metarank than for low metarank documents. For Topic 211, the given-irrelevant model has low intercept but high slope, mean-ing a sudden transition from strongly-irrelevant to strongly-relevant, based on a small number of alternative-relevant documents; smooth-ing would give a less sudden transition. For Topic 250, the given-positive model has a negative slope, assigning a lower probability of alternative relevance to higher ranked documents; the negative slope is based on a small number of alternatively-irrelevant obser-vations, however, and is not significant.
A probabilistic model of a process can be tested by seeing how well a simulation based on the model reproduces the outcomes of the process; if the simulation performs well, it can then be used to predict process outcomes when the process itself is absent. The as-sessments in our datasets were made to form test collection qrels, used in calculating effectiveness metrics on system retrievals. If the qrels of an alternative assessor are used, then different effective-ness metric values will result. How well do probabilistic models based upon the observed assessor disagreement simulate the actual change in absolute and relative system scores?
We examine model accuracy in predicting system scores in Fig-ure 6. Mean AP scores for each TREC 4 system are shown, for the official qrels and for the qrels produced by the first alternative as-sessor. Simulated qrels are generated using the method described in Section 3.3, and the range of scores induced on each system by the randomly generated qrels are shown. The models we consider are a rank-insensitive universal flip rate based on disagreements micro-averaged across all topics (Figure 6(a)); a separate flip-rate model built for each topic (Figure 6(b)); a universal logistic regression model based upon meta-AP scores, pooling assessments across all topics (Figure 6(c)); and a different logistic model for each topic (Figure 6(d)). We also show the per-topic logistic regression model on the second alternate assessors for TREC 4 (Figure 6(e)), and the single alternative assessor for TREC 6 (Figure 6(f)).

The metarank models (Figures 6(c) and (d)) achieve much more accurate absolute scores than the flip-rate models (Figures 6(a) and (b)), with the flip-rate models generating MAP scores that are half or less of the true MAP scores. There is little difference between the universal and per-topic flip-rate models. There is, however, a noticeable difference between the universal and per-topic rank-sensitive models, with the per-topic models generating more accu-rate simulations of alternative-assessor scores; the universal model, in contrast, tends systematically to underestimate MAP scores. This result is not surprising, given the variance in per-topic model coef-ficients (Figure 3).

For comparative scores, however, the accuracy of the metarank and flip-rate models is reversed. Although the flip-rate models grossly understate true MAP, they do so by similar amounts for each system, leading to a ranking that is relatively consistent with the original. In contrast, while the average change in absolute scores is much smaller for the metarank models, the scores of dif-ferent systems change by different amounts and directions, leading to instability in relative ranking. These observations are summa-rized by the RMSE and  X  scores reported in Table 4.
 Table 4: Root mean squared error and Kendall X  X   X  , aver-aged across simulations, between MAP scores from alternative-assessor and from model-generated qrels; summarizing the in-formation in Figure 6.

The explanation for the better absolute but worse relative score stability of the metarank model, compared to the flip-rate model, is that while the metarank model is a much more accurate model of probability of disagreement, it is less independent of systems. Since the overwhelming majority of per-topic models have posi-tive slopes (Table 3), documents with higher metaranks have more chance of remaining (if originally assessed relevant), or flipping to (if originally irrelevant), relevant. Systems that return more docu-ments with higher metarank will have more positive score changes under the model than systems that return fewer such documents. Metarank, however, is an average of ranking position across differ-ent runs, so the more systems that return a document, the higher its metarank. This behavior is by design; but it does mean that docu-ments returned by similar systems get higher metaranks, and hence the systems bigger boosts, than documents returned by dissimilar systems. We have returned to a common problem with these pre-dictive models, that they favor conformist systems (say, automated methods using a standard document similarity measure) over non-conformist ones (say, hand-crafted manual runs). Down-weighting the vote of conformist systems, if possible, would likely lead to a more accurate model.

The metarank model applied to the second alternative assessor for TREC 4 (Figure 6(e)) gives similar results as for the first. In-terestingly, although the absolute scores under the second assessor differ from those for the first, the relationships between the ob-served and the simulated scores for each system are similar. For instance, the simulated scores for CLARTF and CLARTN (seventh and eighth systems from the left) both fall below the observed alter-native scores by a similar amount, even though the observed scores are higher for the second alternative assessor than for the first. This consistency in error reinforces the hypothesis that the errors are due to system-dependent mutual reinforcement, rather than assessor-dependent or purely random factors.

Finally, Figure 6(f) shows the per-topic metarank model applied to the alternative assessor for TREC 6. As described in Section 3.1, the dual assessments were not randomly sampled, but were made by one of the participants in the course of run development. Per-haps as a result of this, the MAP scores on the observed alternative assessments differ from the official ones more than for TREC 4. The simulated alternative assessments induce MAP scores that also depart further from the observed than for TREC 4. This may be a sign that the non-random selection of alternative assessments have biased the model.
Exhaustive dual assessment is expensive. An alternative is to perform second assessments on a sample of documents, and esti-mate a model of assessor disagreement on the sample. A flip-rate and slope.
 Table 5: Sampled dual assessment: root mean squared error a nd Kendall X  X   X  , averaged across simulations, between MAP scores from alternative-assessor and from model-generated qrels; summarizing the information in Figure 7. model is estimated by observing the flip rate on the sample; a logis-tic metarank model by fitting the regression curve to the sampled documents. Which model is more robust to sampling?
Figure 7 compares the reliability of the flip-rate and metarank models for sampled dual assessment. Only per-topic models are examined. Twenty relevant and twenty irrelevant documents are sampled, making an average of 19% of the former and precisely 10% of the latter. The comparisons are summarized statistically in Table 5. (These results should be compared to the full dual assess-ment reported in Figure 6 and Table 4). As before, the metarank models produce much more accurate absolute score estimates than the rank-insensitive flip-rate models. Indeed, while the RMSE of the flip-rate model increases by 0 . 060 or 60% with sampling, while the metarank model RMSE is only 0 . 003 or 20% higher. Unlike for full dual-assessment, however, the ranking and relative scores of the metarank models for sampled assessment are more stable than for the flip-rate models. Flip rate  X  falls from 0 . 919 to 0 . 779 with sampling, whereas metarank  X  remains the same on 0 . 867 .
The results in Figure 7 and Table 5 demonstrate that the metarank logistic models are highly robust to sampling: with only 10% to 20% of the pool dual-assessed, absolute and relative scores are al-most as reliable as with full dual-assessment. Again, this demon-strates the accuracy of the model as a predictor of disagreement (the dependence between metarank score and system conformity aside). In contrast, the flip-rate model degrades badly with sampling, giv-ing absolute scores as much as 80% below the correct values, and introducing sufficient noise that relative scores, and hence system ranking, become unreliable.
The purpose of significance testing is to determine whether a measured difference in the effectiveness of two retrieval algorithms is due to  X  X hance X . In practice, this almost always means determin-ing the extent to which variance due to the topic sample overrides the difference in effectiveness. There are other sources of variance apart from the topic sample, however; variance due to disagree-ment between assessors is one potentially important source. If two systems are significantly different for a sample of topics but not significantly different once assessor disagreement is modeled, the strength of the conclusion is reduced.

To incorporate disagreement into a significance test, we use our simulated qrels in a bootstrap-style significance procedure. For each pair of systems in the TREC-4 set, we sample a set of qrels produced by one of the models described above. We compute the significance between the two systems for that qrels. We can then compare this to the significance between the same two systems for the original qrels. Over many trials, we obtain a sense of the vari-ance in significance due to disagreement as modeled by the simu-lated assessors.

The above procedure only models variance due to assessors X  X t no longer models variance due to the topic sample because the topic sample is held fixed in every experiment. To model both sources of variance, when we select two systems to compare, we also obtain a bootstrap sample of 48 topics over which to compare them; since the 48 topics and the qrels will vary with each experiment, this will produce a bootstrap distribution that incorporates variance due to both topic and assessor.

We compare the results of significance tests using the original as-sessments using three measures: precision (the proportion of sys-tem pairs found significant using simulated assessments that are also significant with the original assessments), recall (the propor-tion of system pairs significant with the original assessments that are also significant with the simulated assessments), and accuracy (the agreement between original and simulated assessments on both F igure 7: System MAP scores for random qrels generated from sampled dual-assessment. For each topic, twenty originally-relevant and twenty originally-irrelevant documents were sam-pled for re-assessment by the first alternative assessor. significance and non-significance). Note that higher values of these measures indicate greater agreement with the original assessors, but they are not necessarily better . If the values were 1.0 across all three, it would suggest that variance due to assessors has no effect whatsoever on significance.

Table 6 summarizes results for four models of disagreement us-ing both procedures. The first set of results, with variance due only to assessor disagreement, has very high recall but fairly low pre-cision and accuracy. Using simulated assessments without varying the topic sample results in significance being found at a higher rate than it is when varying the topic sample without varying assess-ments. This suggests that variance due to assessments is generally lower than variance due to the topic sample. Within these results there is not a great deal of difference due to the choice of model for simulation.

In the second set of results, varying both assessments and topics, we see decreases in recall but increases in both precision and accu-racy; all three are very close in value. Disagreements now tend to be  X  X ymmetric X : a roughly equal number of pairs go from signifi-Table 6: Summary of comparisons between statistical signifi-c ance with the original TREC-4 assessors and significance with simulated assessors. cant to non-significant as do the other direction. The flip-rate model has slightly higher numbers than the metarank model, suggesting it is more conservative in its modeling of assessor disagreement.
The results are somewhat unexpected in that when incorporating more variance into an experiment, one would generally expect that fewer pairs would be found significant, i.e. that recall would de-crease while precision would remain high. In our experiment, some pairs that had not been significantly different became significantly different with simulated assessments. One possible reason is bias introduced by the simulation. The models are imperfect, and may assign relevance in such a way that works in favor of certain sys-tems that are less-favored by human assessors. Nevertheless, these results suggest that significance in IR is fairly robust to assessor disagreement.
In this paper, we have examined the relationship between the rank at which documents are returned and the probability that as-sessors will disagree about their relevance. Logistic regression has been deployed to test and model this relationship. The meta-AP metarank scoring method has been shown to be the best predictor of three metarank methods considered. We have found that there is a strong and consistent relationship between rank and probabil-ity of disagreement. An alternative assessor is much more likely to disagree with an original assessor X  X  relevant judgment if a doc-ument has low rank and is returned by few systems. Conversely, a high-ranked document that the original assessor found irrelevant is more likely to produce disagreement from an alternative assessor than is a low-ranked one.

Models of assessor disagreement allow for the simulation of the phenomenon. We have found that rank-sensitive models produce more accurate predictions of absolute scores than rank-insensitive ones, but less accurate relative scores, suggesting a dependence be-tween systems and metarank scores. However, metarank models based upon sample dual assessment are more reliable than rank-insensitive ones for both absolute and relative measures. Indeed, sampled metarank models are almost as reliable as exhaustively-assessed ones, at a fraction of the assessment cost. Finally, we have demonstrated the use of such sampled models in adding asses-sor variability to test the significance of retrieval evaluation results, finding that while the rate of significance overall does not change, around a tenth of system pairs switch from being significantly to non-significantly different, or vice versa.

We have observed that the dependence between systems and the meta-AP metarank measure produces simulated qrel sets that favor conformist systems. Finding a metarank score that avoids this bias, or developing a method that corrects for it if it occurs, is future work. For instance, metarank contributions from similar systems could be down-weighted. Also, the distribution of meta-AP scores is dependent on both assessment depth and the number of systems in the pool; it is desirable to find a metarank measure less dependent on these factors.

Finally, we observed in Figure 5 that models based even on ex-haustive assessment can be sensitive to the assessment and metas-core of a small number of documents, making them prone to anoma-lous behavior (inverse regression, for instance, or sudden transi-tions in probability, or again models that give zero probability to alternative relevance no matter what the metarank score). This sen-sitivity will be heightened for sample-based models, and higher still as sample size decreases. A multi-level Bayesian model would help alleviate this problem, by smoothing the logistic models for one topic based upon results on other topics.
 Ellen Voorhees provided, and helped with the interpretation of, the TREC 4 and TREC 6 multiple-assessment data sets.

