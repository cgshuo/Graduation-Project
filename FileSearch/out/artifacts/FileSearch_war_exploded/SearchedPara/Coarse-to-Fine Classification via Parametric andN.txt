 Classification is one of the core problems in Computer-Aided Diagnosis (CAD), targeting for early cancer detection using 3D medical imaging interpretation. High detection sensitiv-ity with desirably low false positive (FP) rate is critical for a CAD system to be accepted as a valuable or even indis-pensable tool in radiologists X  workflow. Given various spu-rious imagery noises which cause observation uncertainties, this remains a very challenging task. In this paper, we pro-pose a novel, two-tiered coarse-to-fine (CTF) classification cascade framework to tackle this problem. We first obtain classification-critical data samples (e.g., samples on the deci-sion boundary) extracted from the holistic data distributions using a robust parametric model (e.g., [8]); then we build a graph-embedding based nonparametric classifier on sam-pled data, which can more accurately preserve or formulate the complex classification boundary. These two steps can also be considered as effective  X  X ample pruning X  and  X  X eature pursuing + k NN/template matching X , respectively. Our ap-proach is validated comprehensively in colorectal polyp de-tection and lung nodule detection CAD systems, as the top two deadly cancers, using hospital scale, multi-site clinical datasets. The results show that our method achieves over-all better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers, such as the support vector machine variants, boosting, logistic re-gression, relevance vector machine [8], k -nearest neighbor or spectral projections on graph [2].
 Categories and Subject Descriptors: [Industrial and Application Paper]: Knowledge Management (KM) General Terms: Algorithms Keywords: Computer-Aided Diagnosis, Coarse-to-Fine Clas-sification, Class Regularized Graph Embedding, Total Breg-man Divergence (tBD) Clustering, t -centers Colon cancer and lung cancer are the top two leading causes of cancer deaths in western population. Meanwhile, these two cancers are also highly preventable or  X  X urable X  if detected early. Image interpretation based cancer detec-tion via 3D computer tomography has emerged as a common clinical practice, and many computer-aided detection tools for enhancing radiologists X  diagnostic performance and effec-tiveness are developed in the last decade. The key for radi-ologists to accept the clinical usage of a computer-aided di-agnosis (CAD) system is the high detection sensitivity with reasonably low false positive (FP) rate per case.
 This paper mainly focuses on the classification aspect of CAD. We propose and comprehensively evaluate a novel coarse-to-fine classification framework. The method con-sists of the following two steps, in both training and testing. (1) Sample Pruning: Parametric classification models (e.g., logistic regression, boosting, support/relevance vector ma-chines [8]) are trained on the complexly distributed datasets as coarse, distribution-level classification. The goal is not to assign class labels, but to prune data samples to select more  X  X lassification-critical X  candidates, which are expected to preserve the decision boundary in the high dimensional feature space (thus vast numbers of samples lying far from classification boundary are discarded.(2) Feature Pursuing + k NN/Template Matching: We first apply feature selection and graph embedding methods jointly to find intrinsic lower dimensional feature subspace that preserves group-wise data topology, and then employ nonparametric classifiers for final classification, using k NN or template matching. We argue that more precisely modeling the intrinsic geometric of de-cision boundary, by graph embedding and nonparametric classifiers in a finer level, can potentially improve the final classification performance. The overall process is illustrated as follows
Samples  X  Sample pruning  X  Feature selection  X  Class regularized graph embedding  X  k NN/Template matching
We applied our proposed framework on colon polyp and lung nodule detection, using two large scale clinical datasets collected from multiple clinical sites across continents.
We start by developing a  X  X oarse X  classifier for sample pruning using a parametric model. Considering the spe-cific characteristics of CAD classification problems, in this paper we use the RVMMIL approach [8] which is a powerful extension to integrate feature selection and handle multiple instance learning (MIL).

In RVMMIL, the probability for an instance x i to be pos-itive is p ( y =1 | x i )=  X  ( a x i ), where  X  is the logistic func-tion defined as  X  ( t )=1 / (1 + e  X  t )and a x i is the linear dot-product between data feature vector x i and model co-efficient vector a . For the contents of sparse feature selec-tion and multiple instance learning in RVM, we refer the readers to [8]. From our coarse-to-fine classification model, RVMMIL is adopted as the coarse-level cascade classifier for sample pruning, i.e., we will remove samples x i satisfying p ( y =1 | x i ) &lt;  X   X  . This step can eliminate massive amount of negatives without effecting much on sensitivity, by choosing a balanced  X   X  . The remained data samples p ( y =1 | x i are either true positives (at high recall) or  X  X ard X  false pos-itives, lying close to the classification boundary , which will largely impact the final classification accuracy. Note that other classifiers with confidence estimates, as boosting and regularized SVM, are also applicable.
Our goal of feature pursuit is to estimate intrinsic, lower dimensional feature subspace of data for later sensible non-parametric classification, while preserves generative data-graph topology. This is the key to achieve superior classi-fication performance with simple nonparametric classifiers. In the proposed framework it consists of two steps: feature selection and class regularized graph embedding.

Feature Selection: By applying feature selection, only a compact subset of highly statistical relevant features is retained, to simplify the later graph embedding or feature projection process. There are many feature selection tech-niques, we use Maximum Relevance Minimum Redundancy (MRMR) feature selection algorithm [7] due to its empirical good performance and computational efficiency. Given a set of features F = { f i } , its MRMR feature subset H maximizes the following objective  X  : where and m is the total number of elements in H .Duetospace limit, refer the algorithm details to [7].

Class Regularized Graph Embedding: We propose and exploit a new Class Regularized Graph Embedding (CRGE) scheme to project data (after feature selection) into an even lower dimensional subspace, where data samples from the same class getting closer and samples from different classes moving apart, to make NN or TM more robust and seman-tically interpretable, as shown later. In Graph embedding, feature projections can be learned in many different ways. We follow the principle that keeps the locality of nearby data and maps apart data further in the graph-induced subspace, which is similar to Laplacian Eigenmap [1, 3] and Locality Preserving Projection [4].

Given a set of N points X = { x 1 , x 2 ,  X  X  X  , x N } X  R n and a symmetric N  X  N matrix W which measures the sim-ilarity between all pairs of points in X .Theset X and matrix W compose a graph G ,with X as vertices and W as weights of the edges. The conventional graph embed-ding method will map X to a much lower dimensional space Y = { y 1 , y 2 ,  X  X  X  , y N } X  R  X  n ,  X  n n .Theoptimal Y minimize the loss function L ( Y ) which is defined as under some appropriate constraints. Though performed well in many applications [3, 4], the limitation of Eq. (4) is that it does not penalize the similarity between points belong-ing to different classes. For this means, we propose class regularized graph embedding (CRGE) to find a mapping  X  : X X  X  , such that  X  minimizes the function E ( Y )de-fined as
E ( Y )= subject to: Y F =1 . where i, j  X  X  means x i and x j belong to the same class, and i, j  X  X  means x i and x j are in different classes.  X  is the Frobenius norm. To avoid notation clutter, we rewrite (5) and get where H ij is the Heaviside function and The mapping function  X  ( x ) can be linear or nonlinear, and We use linear mapping because of its simplicity and gener-ality, such as Plugging (7) into (6), we get where the constraint M F = 1 eliminates the scaling effect. Eq. (8) can be solved very quickly using gradient descent technique along with iterative projections [9]. The compu-tation of W is chosen in the following manner, to fit our problem specific need.
 Finally, we argue that our stratified approach which prunes non-informative or redundant features, using feature selec-tion from an information-theoretic aspect, before feature graph embedding or projection, can simplify the optimization pro-cess of graph embedding on a reduced feature set. This strat-egy may achieve better overall results, compared from the joint sparsity-constrained graph embedding (as SPG) [2].
The naive KNN classification on data instances, in feature space Y =  X  ( x ), performs poorly due to unbalanced number of rare positives and dominating negatives. Thus we propose to do KNN voting using learned templates from clustering.
Clustering &amp; Templates: Our previous total Bregman divergence (tBD) clustering algorithm [5] is utilized here. tBD is based on the orthogonal distance between the con-vex generating function of the divergence and its tangent approximation at the second argument of the divergence, which is naturally robust and leads to efficient algorithms for soft and hard clustering. Denote that c 1 clusters, with c clusters with centers { z j + } c 2 j =1 for positives. The numbers of clusters c 1 , c 2 is chosen to minimize the intra-inter-validity index ,givenby where C i is the i th cluster with center z i . Each cluster is represented as the tBD center, termed t -center [5], which is the 1 norm median of all samples in the corresponding cluster. For example, if { y i } N i =1 is the set of samples, then its t -center z is where  X  f is the total Bregamn divergence generated by some convex and differentiable generator function f : Here, we use f ( y )= y 2 , and hence  X  f becomes the total square loss [5] and the t -center in Eq. (11) becomes
Template Matching via k NN Voting: Given a test sample y i , we need to find its k nearest neighbors from the t -centers. Suppose the neighbors are { z 1 ,z 2 , and the corresponding distance from y i to the neighbors are { d 1 ,d 2 ,  X  X  X  ,d k } . We define the empirical probability of y being positive as p ,and Based on the p value, we can draw the FROC curve of sensi-tivity and FP rate per case for training and testing datasets. Eq. (14) is a soft k NN voting scheme using the reciprocal of distance 1 /d i . We found that t -centers are more robust as they lead to preserve better sparsity and diversity of CAD lesion data distribution than proximity data samples (as in k NN). The number of nearest neighbors k is chosen dur-ing the training/validation stage, by maximizing the partial Area Under FROC Curve (AUC):
Our CTF classification frame is evaluated on representa-tive large scale colon polyp and lung nodule datasets, col-lected from dozens of hospitals across US, Europe and Asia.
Colon Polyp Detection &amp; Retrieval: In colon polyp dataset, each data sample is represented by a 96-dimensional computer extracted imagery feature vector, describing its shape, intensity pattern, segmented class-conditional likeli-hood statistics and other higher level features [6]. Upon ob-taining the parametric RVMMIL model [8], we get the clas-sification score of each instance being positive. Then we per-form thresholding and only consider candidates with p ( y = 1 | x i )  X   X   X  =0 . 0157 as a classification cascade with high-recall. A total of 3,466 data samples are retained, pruned from 134,116 polyp candidates on the training dataset. All the 554 true positive lesion instances are contained. For fine-level classification, we learn the mapping function  X  : X X  X  after feature selection using the pruned dataset, and the t -centers are fitted in the reduced Y feature space for the soft k NN classifier. We plot the FROC curves comparing us-ing RVMMIL as a baseline classifier, SPG as a integrated sparsity and dimension reduction approach, and our coarse-to-fine classifier, on training and testing datasets in Fig. 1 Left . For validation, the testing results demonstrate that our CTF method can increase the sensitivity of RVMMIL by 2 . 58% (from 0.8903 to 0.9161) at the FP rate = 4, or reduce the FP rate by 1.754 (from 5.338 to 3.584) when sensitivity is 0.9097, which are statistically significant improvements for colorectal cancer detection.

To fully leverage the feature space topology-preserving property of the lower-dimensional embedding Y ,wealsotest it for polyp retrieval, which is defined as giving a query polyp in one prone/supine scan, to retrieve its counterparts in the other view. To achieve this, we find the k nearest neighbors ( k NN) of a query y i  X  X  using the classified polyps, and check whether the true match is inside the neighborhood of k NN.Ifthetruematchedpolypisinthe k NN, a  X  X it X  will occur. We record the retrieval rate, as the ratio of the num-ber of  X  X it X  polyp divided by the query polyp number, at different k levels. Especially, high retrieval rate with small k can greatly alleviate radiologists X  manual efforts on find-ing the counterpart same polyp, with better accuracy. To demonstrate its advantage, we employ a conventional geo-metric distance feature based polyp retrieval scheme, namely geodesic distance that measures the geodesic length of a polyp to a fixed anatomical point (e.g., rectum), along the colon centerline. The retrieval rate comparison is illustrated in Fig. 1 Middle , for both training and testing datasets. The results indicate that the retrieval accuracy can achieve 80% when only 2 to 4 neighbors are necessary. This shows that nonparametric k NN in Y subspace based retrieval sig-nificantly improves the conventional polyp matching scheme, contingent on geometric computation of geodesic distance and the SPG based retrieval.

Lung Nodule Classification: The lung nodule dataset is collected from 1 , 000+ patients from multiple hospitals in different countries, using multi-vendor scanners. All types of solid, partial-solid and Ground Glass Nodules with a diam-eter range of 4-30mm are considered. Each sample has 112 informative imagery features, including texture appearance features, shape, location context, gray value and morpholog-ical features. First, FROC analysis by using our proposed coarse-to-fine classification framework, compared to single-layer RVMMIL classifier, for the lung nodule classification in training and testing is shown in Fig. 1 Right .Wecan see that the testing FROC of CTF dominates the RVMMIL FROC, when the FP rate  X  [3 , 4], with 1 . 0  X  1 . 5% con-sistent sensitivity improvements. We also compared with the SPG framework, and the FROC analysis is shown in Fig. 2 Left . The comparison also shows the higher clas-sification accuracy of our proposed method. Furthermore, our CTF classification performance compares favorably with other recent developments in lung CAD.Next, we compare our method to a related locality-classification framework, SVM-k NN [10] which shows highly competitive results on image based multiclass object recognition problems. SVM-k NN uses k NN to find data clusters as nearest neighbors and train a support vector machine (SVM) on each locality group for  X  X ivide-and-conquer X  classification [10]. The comparison results are illustrated in 2 Middle , which shows that our method outperforms the SVM-k NN method on both lung training and testing datasets. Finally, we evaluate the ef-fects of using t -center (default), mean or median as esti-mated templates in CTF. The comparison in testing using the lung dataset is shown in Fig. 2 Right . The comparison validates that t -center outperforms the templates formed by typical mean or median method.
