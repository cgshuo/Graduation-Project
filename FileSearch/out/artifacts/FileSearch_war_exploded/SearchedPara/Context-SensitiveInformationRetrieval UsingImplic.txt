 A major limitation of most existing retrie val models and systems is that the retrie val decision is made based solely on the query and document collection; information about the actual user and search conte xt is lar gely ignored. In this paper , we study how to ex-ploit implicit feedback information, including pre vious queries and clickthrough information, to impro ve retrie val accurac y in an in-teracti ve information retrie val setting. We propose several conte xt-sensiti ve retrie val algorithms based on statistical language models to combine the preceding queries and click ed document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search conte xt information, and quantitati vely evaluate our models using this test set. Experiment results sho w that using implicit feedback, espe-cially the click ed document summaries, can impro ve retrie val per -formance substantially .
 H.3.3 [ Inf ormation Sear ch and Retrie val ]: Retrie val models Algorithms Query history , query expansion, interacti ve retrie val, conte xt
In most existing information retrie val models, the retrie val prob-lem is treated as involving one single query and a set of documents. From a single query , howe ver, the retrie val system can only have very limited clue about the user X  s information need. An optimal re-trie val system thus should try to exploit as much additional conte xt information as possible to impro ve retrie val accurac y, whene ver it is available. Indeed, conte xt-sensiti ve retrie val has been identified as a major challenge in information retrie val research[2]. Cop yright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00.

There are man y kinds of conte xt that we can exploit. Rele vance feedback [14] can be considered as a way for a user to pro vide more conte xt of search and is kno wn to be effecti ve for impro v-ing retrie val accurac y. Ho we ver, rele vance feedback requires that a user explicitly pro vides feedback information, such as specifying the cate gory of the information need or marking a subset of re-trie ved documents as rele vant documents. Since it forces the user to engage additional acti vities while the benefits are not always ob-vious to the user , a user is often reluctant to pro vide such feedback information. Thus the effecti veness of rele vance feedback may be limited in real applications.

For this reason, implicit feedbac k has attracted much attention re-cently [11, 13, 18, 17, 12]. In general, the retrie val results using the user X  s initial query may not be satisf actory; often, the user would need to revise the query to impro ve the retrie val/ranking accurac y [8]. For a comple x or dif ficult information need, the user may need to modify his/her query and vie w rank ed documents with man y iter -ations before the information need is completely satisfied. In such an interacti ve retrie val scenario, the information naturally available to the retrie val system is more than just the current user query and the document collection  X  in general, all the interaction history can be available to the retrie val system, including past queries, informa-tion about which documents the user has chosen to vie w, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading). We define implicit feedback broadly as exploiting all such naturally available interaction history to impro ve retrie val results.

A major adv antage of implicit feedback is that we can impro ve the retrie val accurac y without requiring any user effort. For ex-ample, if the current query is  X  X a va X , without kno wing any extra information, it would be impossible to kno w whether it is intended to mean the Java programming language or the Java island in In-donesia. As a result, the retrie ved documents will lik ely have both kinds of documents  X  some may be about the programming lan-guage and some may be about the island. Ho we ver, any particular user is unlik ely searching for both types of documents. Such an ambiguity can be resolv ed by exploiting history information. For example, if we kno w that the pre vious query from the user is  X  X gi programming X , it would strongly suggest that it is the programming language that the user is searching for .
 Implicit feedback was studied in several pre vious works. In [11], Joachims explored how to capture and exploit the clickthrough in-formation and demonstrated that such implicit feedback informa-tion can indeed impro ve the search accurac y for a group of peo-ple. In [18], a simulation study of the effecti veness of dif ferent implicit feedback algorithms was conducted, and several retrie val models designed for exploiting clickthrough information were pro-posed and evaluated. In [17], some existing retrie val algorithms are adapted to impro ve search results based on the bro wsing history of a user . Other related work on using conte xt includes personalized search [1, 3, 4, 7, 10], query log analysis [5], conte xt factors [12], and implicit queries [6].

While the pre vious work has mostly focused on using click-through information, in this paper , we use both clickthrough in-formation and preceding queries, and focus on developing new conte xt-sensiti ve language models for retrie val. Specifically , we develop models for using implicit feedback information such as query and clickthrough history of the current search session to im-pro ve retrie val accurac y. We use the KL-di vergence retrie val model [19] as the basis and propose to treat conte xt-sensiti ve retrie val as estimating a query language model based on the current query and any search conte xt information. We propose several statistical lan-guage models to incorporate query and clickthrough history into the KL-di vergence model.

One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation. We thus use the TREC AP data to create a test collection with implicit feed-back information, which can be used to quantitatively evaluate im-plicit feedback models. To the best of our kno wledge, this is the first test set for implicit feedback. We evaluate the proposed mod-els using this data set. The experimental results sho w that using implicit feedback information, especially the clickthrough data, can substantially impro ve retrie val performance without requiring addi-tional effort from the user .

The remaining sections are organized as follo ws. In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later . In Section 3, we propose several implicit feedback models based on statistical language models. In Section 4, we describe how we create the data set for implicit feed-back experiments. In Section 5, we evaluate dif ferent implicit feed-back models on the created data set. Section 6 is our conclusions and future work.
There are two kinds of conte xt information we can use for im-plicit feedback. One is short-term conte xt , which is the immediate surrounding information which thro ws light on a user X  s current in-formation need in a single session . A session can be considered as a period consisting of all interactions for the same information need. The cate gory of a user X  s information need (e.g., kids or sports), pre-vious queries, and recently vie wed documents are all examples of short-term conte xt. Such information is most directly related to the current information need of the user and thus can be expected to be most useful for impro ving the current search. In general, short-term conte xt is most useful for impro ving search in the current session, but may not be so helpful for search acti vities in a dif ferent ses-sion. The other kind of conte xt is long-term conte xt , which refers to information such as a user X  s education level and general interest, accumulated user query history and past user clickthrough infor -mation; such information is generally stable for a long time and is often accumulated over time. Long-term conte xt can be applicable to all sessions, but may not be as effecti ve as the short-term conte xt in impro ving search accurac y for a particular session. In this paper , we focus on the short-term conte xt, though some of our methods can also be used to naturally incorporate some long-term conte xt.
In a single search session, a user may interact with the search system several times. During interactions, the user would contin-uously modify the query . Therefore for the current query cept for the first query of a search session) , there is a query history , H
Q = ( Q 1 ;:::;Q k 1 ) associated with it, which consists of the pre-ceding queries given by the same user in the current session. Note that we assume that the session boundaries are kno wn in this paper . In practice, we need techniques to automatically disco ver session boundaries, which have been studied in [9, 16]. Traditionally , the retrie val system only uses the current query Q k to do retrie val. But the short-term query history clearly may pro vide useful clues about the user X  s current information need as seen in the  X  X a va X  example given in the pre vious section. Indeed, our pre vious work [15] has sho wn that the short-term query history is useful for impro ving re-trie val accurac y.

In addition to the query history , there may be other short-term conte xt information available. For example, a user would presum-ably frequently click some documents to vie w. We refer to data associated with these actions as clic kthr ough history . The click-through data may include the title, summary , and perhaps also the content and location (e.g., the URL) of the click ed document. Al-though it is not clear whether a vie wed document is actually rele-vant to the user X  s information need, we may  X  X afely X  assume that the displayed summary/title information about the document is at-tracti ve to the user , thus con veys information about the user X  s infor -mation need. Suppose we concatenate all the displayed text infor -mation about a document (usually title and summary) together , we will also have a clic ked summary C i in each round of retrie val. In general, we may have a history of click ed summaries C 1 , ..., We will also exploit such clickthrough history H C = ( C 1 to impro ve our search accurac y for the current query Q k work has also sho wn positi ve results using similar clickthrough in-formation [11, 17].

Both query history and clickthrough history are implicit feed-back information, which naturally exists in interacti ve information retrie val, thus no additional user effort is needed to collect them. In this paper , we study how to exploit such information ( H develop models to incorporate the query history and clickthrough history into a retrie val ranking function, and quantitati vely evaluate these models. Intuiti vely , the query history H Q and clickthrough history are both useful for impro ving search accurac y for the current query Q . An important research question is how we can exploit such in-formation effecti vely . We propose to use statistical language mod-els to model a user X  s information need and develop four specific conte xt-sensiti ve language models to incorporate conte xt informa-tion into a basic retrie val model.
We use the Kullback-Leibler (KL) divergence method [19] as our basic retrie val method. According to this model, the retrie val task involv es computing a query language model Q for a given query and a document language model D for a document and then computing their KL divergence D ( Q jj D ) , which serv es as the score of the document.

One adv antage of this approach is that we can naturally incorpo-rate the search conte xt as additional evidence to impro ve our esti-mate of the query language model.

Formally , let H Q = ( Q 1 ;:::; Q k 1 ) be the query history and the current query be Q k . Let H C = ( C 1 ;:::;C k 1 ) be the click-through history . Note that C i is the concatenation of all click ed documents X  summaries in the i -th round of retrie val since we may reasonably treat all these summaries equally . Our task is to estimate a conte xt query model, which we denote by p ( w j k ) , based on the current query Q k , as well as the query history H Q and clickthrough history H C . We now describe several dif ferent language models for exploiting H Q and H C to estimate p ( w j k ) . We will use to denote the count of word w in text X , which could be either a query or a click ed document X  s summary or any other text. We will use j X j to denote the length of text X or the total number of words in
X .
Our first idea is to summarize the query history H Q with a un-igram language model p ( w j H Q ) and the clickthrough history with another unigram language model p ( w j H C ) . Then we linearly interpolate these two history models to obtain the history model p ( w j H ) . Finally , we interpolate the history model the current query model p ( w j Q k ) . These models are defined as follo ws. where 2 [0 ; 1] is a parameter to control the weight on each his-tory model, and where 2 [0 ; 1] is a parameter to control the weight on the current query and the history information.
If we combine these equations, we see that p ( w j k ) = p ( w j Q k ) + (1 )[ p ( w j H C ) + (1 ) p ( w j H That is, the estimated conte xt query model is just a fix ed coef ficient interpolation of three models p ( w j Q k ) , p ( w j H Q
One possible problem with the FixInt approach is that the coef fi-cients, especially , are fix ed across all the queries. But intuiti vely , if our current query Q k is very long, we should trust the current query more, whereas if Q k has just one word, it may be beneficial to put more weight on the history . To capture this intuition, we treat p ( w j H Q ) and p ( w j H C ) as Dirichlet priors and Q data to estimate a conte xt query model using Bayesian estimator . The estimated model is given by where is the prior sample size for p ( w j H Q ) and is the prior sample size for p ( w j H C ) . We see that the only dif ference between BayesInt and FixInt is the interpolation coef ficients are now adap-tive to the query length. Indeed, when vie wing BayesInt as FixInt, we see that = j Q k j we will have a query-dependent . Later we will sho w that such an adapti ve empirically performs better than a fix ed .
Both FixInt and BayesInt summarize the history information by aver aging the unigram language models estimated based on pre-vious queries or click ed summaries. This means that all pre vious queries are treated equally and so are all click ed summaries. Ho w-ever, as the user interacts with the system and acquires more kno wl-edge about the information in the collection, presumably , the refor -mulated queries will become better and better . Thus assigning de-caying weights to the pre vious queries so as to trust a recent query more than an earlier query appears to be reasonable. Interestingly , if we incr ementally update our belief about the user X  s information need after seeing each query , we could naturally obtain decaying weights on the pre vious queries. Since such an incremental online updating strate gy can be used to exploit any evidence in an interac-tive retrie val system, we present it in a more general way.
In a typical retrie val system, the retrie val system responds to every new query entered by the user by presenting a rank ed list of documents. In order to rank documents, the system must have some model for the user X  s information need. In the KL divergence retrie val model, this means that the system must compute a query model whene ver a user enters a (ne w) query . A principled way of updating the query model is to use Bayesian estimation, which we discuss belo w.
We first discuss how we apply Bayesian estimation to update a query model in general. Let p ( w j ) be our current query model and T be a new piece of text evidence observ ed (e.g., T can be a query or a click ed summary). To update the query model based on T , we use to define a Dirichlet prior parameterized as where T is the equi valent sample size of the prior . We use Dirich-let prior because it is a conjugate prior for multinomial distrib u-tions. With such a conjugate prior , the predicti ve distrib ution of (or equi valently , the mean of the posterior distrib ution of is given by where c ( w;T ) is the count of w in T and j T j is the length of Parameter T indicates our confidence in the prior expressed in terms of an equi valent text sample comparable with T . For exam-ple, T = 1 indicates that the influence of the prior is equi valent to adding one extra word to T .
We now discuss how we can update our query model over time during an interacti ve retrie val process using Bayesian estimation. In general, we assume that the retrie val system maintains a current query model i at any moment. As soon as we obtain some implicit feedback evidence in the form of a piece of text T i , we will update the query model.

Initially , before we see any user query , we may already have some information about the user . For example, we may have some information about what documents the user has vie wed in the past. We use such information to define a prior on the query model, which is denoted by 0 can update the query model based on the new observ ed data The updated query model 1 can then be used for ranking docu-ments in response to Q 1 . As the user vie ws some documents, the displayed summary text for such documents C 1 (i.e., click ed sum-maries) can serv e as some new data for us to further update the query model to obtain 0 the user , we can update 0 we may repeat such an updating process to iterati vely update the query model.

Clearly , we see two types of updating: (1) updating based on a new query Q i ; (2) updating based on a new click ed summary both cases, we can treat the current model as a prior of the conte xt query model and treat the new observ ed query or click ed summary as observ ed data. Thus we have the follo wing updating equations: where i is the equi valent sample size for the prior when updating the model based on a query , while i is the equi valent sample size for the prior when updating the model based on a click ed summary . If we set i = 0 (or i = 0 ) we essentially ignore the prior model, thus would start a completely new query model based on the query Q i (or the click ed summary C i ). On the other hand, if we set + 1 (or i = + 1 ) we essentially ignore the observ ed query (or the click ed summary) and do not update our model. Thus the model remains the same as if we do not observ e any new text evidence. In general, the parameters i and i may have dif ferent values for dif ferent i . For example, at the very beginning, we may have very sparse query history , thus we could use a smaller i , but later as the query history is richer , we can consider using a lar ger our experiments, unless otherwise stated, we set them to the same constants, i.e., 8 i;j; i = j ; i = j .

Note that we can tak e either p ( w j i ) or p ( w j 0 query model for ranking documents. This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrie val; instead, as soon as we collect click ed summary can update the query model and use p ( w j 0 any documents that a user has not yet seen.

To score documents after seeing query Q k , we use p ( w j
If we set the equi valent sample size parameters to fix ed con-stant, the OnlineUp algorithm would introduce a decaying factor  X  repeated interpolation would cause the early data to have a low weight. This may be appropriate for the query history as it is rea-sonable to belie ve that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the dis-played summary , rather than the actual content of a click ed doc-ument. One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = ( Q 1 ;:::;Q i 1 ) , but not for the clickthrough data buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through run-ning OnlineUp on pre vious queries. The updating equations are as follo ws. where i has the same interpretation as in OnlineUp, but i indicates to what extent we want to trust the click ed summaries. As in OnlineUp, we set all i  X  X  and i  X  X  to the same value. And to rank documents after seeing the current query Q k , we use
In order to quantitati vely evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic. Since there is no such data set available to us, we have to create one. There are two choices. One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrie val system (e.g., search engine). But the problem is that we have no rele vance judgments on such data. The other choice is to use a TREC data set, which has a text database, topic description and rele vance judgment file. Unfortunately , there are no query his-tory and clickthrough history data. We decide to augment a TREC data set by collecting query history and clickthrough history data.
We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has rel-atively complete judgments. There are altogether 242918 news ar-ticles and the average document length is 416 words. Most articles have titles. If not, we select the first sentence of the text as the ti-tle. For the preprocessing, we only do case folding and do not do stopw ord remo val or stemming.
 We select 30 relati vely dif ficult topics from TREC topics 1-150. These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Di vergence model with Bayesian prior smoothing [20]. The reason why we select dif ficult topics is that the user then would have to have several interactions with the retrie val system in order to get satisf actory results so that we can expect to collect a rela-tively richer query history and clickthrough history data from the user . In real applications, we may also expect our models to be most useful for such dif ficult topics, so our data collection strate gy reflects the real world applications well.

We inde x the TREC AP data set and set up a search engine and web interf ace for TREC AP news articles. We use 3 subjects to do experiments to collect query history and clickthrough history data. Each subject is assigned 10 topics and given the topic descriptions pro vided by TREC. For each topic, the first query is the title of the topic given in the original TREC topic description. After the subject submits the query , the search engine will do retrie val and return a rank ed list of search results to the subject. The subject will bro wse the results and maybe click one or more results to bro wse the full text of article(s). The subject may also modify the query to do another search. For each topic, the subject composes at least 4 queries. In our experiment, only the first 4 queries for each topic are used. The user needs to select the topic number from a selec-tion menu before submitting the query to the search engine so that we can easily detect the session boundary , which is not the focus of our study . We use a relational database to store user interactions, including the submitted queries and click ed documents. For each query , we store the query terms and the associated result pages. And for each click ed document, we store the summary as sho wn on the search result page. The summary of the article is query de-pendent and is computed online using fix ed-length passage retrie val (KL divergence model with Bayesian prior smoothing).

Among 120 ( 4 for each of 30 topics) queries which we study in the experiment, the average query length is 3 : 71 words. Altogether there are 91 documents click ed to vie w. So on average, there are around 3 clicks per topic. The average length of click ed summary is 34 : 4 words. Among 91 click ed documents, 29 documents are judged rele vant according to TREC judgment file. This data set is publicly available 1 .
Our major hypothesis is that using search conte xt (i.e., query his-tory and clickthrough information) can help impro ve search accu-rac y. In particular , the search conte xt can pro vide extra information to help us estimate a better query model than using just the current query . So most of our experiments involv e comparing the retrie val performance using the current query only (thus ignoring any con-text) with that using the current query as well as the search conte xt.
Since we collected four versions of queries for each topic, we mak e such comparisons for each version of queries. We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serv es as a good measure of the overall ranking accurac y. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents. In all cases, the reported figure is the average over all of the 30 topics.
 We evaluate the four models for exploiting search conte xt (i.e., FixInt, BayesInt, OnlineUp, and BatchUp). Each model has pre-cisely two parameters ( and for FixInt; and for others). Note that and may need to be interpreted dif ferently for dif-ferent methods. We vary these parameters and identify the optimal performance for each method. We also vary the parameters to study the sensiti vity of our algorithms to the setting of the parameters.
We compare the optimal performances of four models with those using the current query only in Table 1. A row labeled with the baseline performance and a row labeled with q i + H Q is the performance of using search conte xt. We can mak e several observ ations from this table: 1. Comparing the baseline performances indicates that on average reformulated queries are better than the pre vious queries with the performance of q 4 being the best. Users generally formulate better and better queries. 2. Using search conte xt generally has positi ve effect, especially when the conte xt is rich. This can be seen from the fact that the 1 http://sif aka.cs.uiuc.edu/ir/ucair/QCHistory .zip impro vement for q 4 and q 3 is generally more substantial compared with q 2 . Actually , in man y cases with q 2 , using the conte xt may hurt the performance, probably because the history at that point is sparse. When the search conte xt is rich, the performance impro ve-ment can be quite substantial. For example, BatchUp achie ves 92.4% impro vement in the mean average precision over q 3 77.2% impro vement over q 4 . (The generally low precisions also mak e the relati ve impro vement decepti vely high, though.) 3. Among the four models using search conte xt, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp. Since BayesInt performs better than FixInt and the main dif ference between BayesInt and FixInt is that the former uses an adapti ve coef ficient for interpolation, the results suggest that us-ing adapti ve coef ficient is quite beneficial and a Bayesian style in-terpolation mak es sense. The main dif ference between OnlineUp and BatchUp is that OnlineUp uses decaying coef ficients to com-bine the multiple click ed summaries, while BatchUp simply con-catenates all click ed summaries. Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the click ed summaries indeed should not be decaying. While OnlineUp is theoretically appealing, its performance is infe-rior to BayesInt and BatchUp, lik ely because of the decaying coef-ficient. Ov erall, BatchUp appears to be the best method when we vary the parameter settings.

We have two dif ferent kinds of search conte xt  X  query history and clickthrough data. We now look into the contrib ution of each kind of conte xt.
In each of four models, we can  X  X urn off X  the clickthrough his-tory data by setting parameters appropriately . This allo ws us to evaluate the effect of using query history alone. We use the same parameter setting for query history as in Table 1. The results are sho wn in Table 2. Here we see that in general, the benefit of using query history is very limited with mix ed results. This is dif ferent from what is reported in a pre vious study [15], where using query history is consistently helpful. Another observ ation is that the con-text runs perform poorly at q 2 , but generally perform (slightly) bet-ter than the baselines for q 3 and q 4 . This is again lik ely because at the beginning the initial query , which is the title in the original TREC topic description, may not be a good query; indeed, on av-erage, performances of these  X  X irst-generation X  queries are clearly poorer than those of all other user -formulated queries in the later generations. Yet another observ ation is that when using query his-tory only , the BayesInt model appears to be better than other mod-els. Since the clickthrough data is ignored, OnlineUp and BatchUp are essentially the same algorithm. The displayed results thus re-flect the variation caused by parameter . A smaller setting of 2.0 is seen better than a lar ger value of 5.0. A more complete picture of the influence of the setting of can be seen from Table 3, where we sho w the performance figures for a wider range of values of . The value of can be interpreted as how man y words we regard the query history is worth. A lar ger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich. Thus while for q 4 the best perfor -mance tends to be achie ved for 2 [2 ; 5] , only when = 0 : 5 see some small benefit for q 2 . As we would expect, an excessi vely lar ge would hurt the performance in general, but q 2 is hurt most and q 4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information. This also suggests that a better strate gy should probably dynamically adjust parameters according to how much history information we have.

The mix ed query history results suggest that the positi ve effect of using implicit feedback information may have lar gely come from the use of clickthrough history , which is indeed true as we discuss in the next subsection.
We now turn off the query history and only use the click ed sum-maries plus the current query . The results are sho wn in Table 4. We see that the benefit of using clickthrough information is much more significant than that of using query history . We see an overall posi-tive effect, often with significant impro vement over the baseline. It is also clear that the richer the conte xt data is, the more impro ve-ment using click ed summaries can achie ve. Other than some occa-sional degradation of precision at 20 documents, the impro vement is fairly consistent and often quite substantial.

These results sho w that the click ed summary text is in general quite useful for inferring a user X  s information need. Intuiti vely , us-ing the summary text, rather than the actual content of the docu-ment, mak es more sense, as it is quite possible that the document behind a seemingly rele vant summary is actually non-rele vant. 29 out of the 91 click ed documents are rele vant. Updating the query model based on such summaries would bring up the ranks of these rele vant documents, causing performance impro vement. Ho we ver, such impro vement is really not beneficial for the user as the user has already seen these rele vant documents. To see how much impro vement we have achie ved on impro ving the ranks of the unseen rele vant documents, we exclude these 29 rele vant doc-uments from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file. The results are sho wn in Table 5. Note that the performance of the baseline method is lower due to the remo val of the 29 rele vant documents, which would have been generally rank ed high in the results. From Table 5, we see clearly that using click ed summaries also helps im-pro ve the ranks of unseen rele vant documents significantly . Table 5: Bay esInt evaluated on unseen rele vant documents
One remaining question is whether the clickthrough data is still helpful if none of the click ed documents is rele vant. To answer this question, we took out the 29 rele vant summaries from our clickthrough history data H C to obtain a smaller set of click ed summaries H 0 method using H 0 ble 4. The results are sho wn in Table 6. We see that although the impro vement is not as substantial as in Table 4, the average pre-cision is impro ved across all generations of queries. These results should be interpreted as very encouraging as the y are based on only 62 non-r ele vant clickthroughs. In reality , a user would more lik ely click some rele vant summaries, which would help bring up more rele vant documents as we have seen in Table 4 and Table 5.
Table 6: Effect of using only non-r ele vant clickthr ough data
By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly  X  X dditi ve X , i.e., com-bining them can achie ve better performance than using each alone, but most impro vement has clearly come from the clickthrough in-formation. In Table 7, we sho w this effect for the BatchUp method.
All four models have two parameters to control the relati ve weights of
H Q , H C , and Q k , though the parameterization is dif ferent from model to model. In this subsection, we study the parameter sensi-tivity for BatchUp, which appears to perform relati vely better than others. BatchUp has two parameters and .

We first look at . When is set to 0, the query history is not used at all, and we essentially just use the clickthrough data com-bined with the current query . If we increase , we will gradually incorporate more information from the pre vious queries. In Table 8, we sho w how the average precision of BatchUp changes as we vary with fix ed to 15.0, where the best performance of BatchUp is achie ved. We see that the performance is mostly insensiti ve to the change of for q 3 and q 4 , but is decreasing as increases for The pattern is also similar when we set to other values.
In addition to the fact that q 1 is generally worse than q q , another possible reason why the sensiti vity is lower for q may be that we generally have more clickthrough data avail-able for q 3 and q 4 than for q 2 , and the dominating influence of the clickthrough data has made the small dif ferences caused by less visible for q 3 and q 4 .

The best performance is generally achie ved when is around 2.0, which means that the past query information is as useful as about 2 words in the current query . Except for q 2 , there is clearly some tradeof f between the current query and the pre vious queries and using a balanced combination of them achie ves better perfor -mance than using each of them alone.

We now turn to the other parameter . When is set to 0 , we only use the clickthrough data; When is set to + 1 , we only use the query history and the current query . With set to 2.0, where the best performance of BatchUp is achie ved, we vary and sho w the results in Table 9. We see that the performance is also not very sensiti ve when 30 , with the best performance often achie ved at = 15 . This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.

Ov erall, these sensiti vity results sho w that BatchUp not only per -forms better than other methods, but also is quite rob ust.
In this paper , we have explored how to exploit implicit feed-back information, including query history and clickthrough history within the same search session, to impro ve information retrie val performance. Using the KL-di vergence retrie val model as the ba-sis, we proposed and studied four statistical language models for conte xt-sensiti ve information retrie val, i.e., FixInt, BayesInt, On-lineUp and BatchUp. We use TREC AP Data to create a test set for evaluating implicit feedback models. Experiment results sho w that using implicit feedback, especially clickthrough history , can substantially impro ve retrie val performance without requiring any additional user effort.

The current work can be extended in several ways: First, we have only explored some very simple language models for incor -porating implicit feedback information. It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history . For example, we may treat a click ed sum-mary dif ferently depending on whether the current query is a gen-eralization or refinement of the pre vious query . Second, the pro-posed models can be implemented in any practical systems. We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms. We will also do a user study to evaluate effecti veness of these models in the real web search. Finally , we should further study a general retrie val frame work for sequential decision making in interacti ve informa-tion retrie val and study how to optimize some of the parameters in the conte xt-sensiti ve retrie val models.
This material is based in part upon work supported by the Na-tional Science Foundation under award numbers CAREER-IIS-0347933 and ITR-IIS-0428472. We thank the anon ymous revie wers for their useful comments. [1] E. Adar and D. Kar ger . Haystack: Per -user information [2] J. Allan and et al. Challenges in information retrie val and [3] K. Bharat. Searchpad: Explicit capture of search conte xt to [4] W. B. Croft, S. Cronen-T ownsend, and V. Larvrenk o. [5] H. Cui, J.-R. Wen, J.-Y . Nie, and W.-Y . Ma. Probabilistic [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit [7] L. Fink elstein, E. Gabrilo vich, Y. Matias, E. Rivlin, Z. Solan, [8] C. Huang, L. Chien, and Y. Oyang. Query session based term [9] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic [10] G. Jeh and J. Widom. Scaling personalized web search. In [11] T. Joachims. Optimizing search engines using clickthrough [12] D. Kelly and N. J. Belkin. Display time as implicit feedback: [13] D. Kelly and J. Teevan. Implicit feedback for inferring user [14] J. Rocchio. Rele vance feedback information retrie val. In The [15] X. Shen and C. Zhai. Exploiting query history for document [16] S. Sriram, X. Shen, and C. Zhai. A session-based search [17] K. Sugiyama, K. Hatano, and M. Yoshika wa. Adapti ve web [18] R. W. White, J. M. Jose, C. J. van Rijsber gen, and [19] C. Zhai and J. Laf ferty . Model-based feedback in the [20] C. Zhai and J. Laf ferty . A study of smoothing methods for
