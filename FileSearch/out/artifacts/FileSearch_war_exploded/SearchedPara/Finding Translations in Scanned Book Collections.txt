 This paper describes an approach for identifying translations of books in large scanned book collections with OCR errors. The method is based on the idea that although individual sentences do not necessarily preserve the word order when translated, a book must preserve the linear progression of ideas for it to be a valid translation. Consider two books in two different languages, say English and German. The En-glish book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book. Similarly, the book in German is rep-resented by its sequence of words which appear only once. An English-German dictionary is used to transform the word sequence of the English book into German by translating in-dividual words in place. It is not necessary to translate all the words and this method works even with small dictionar-ies. Both sequences are now in German and can, therefore, be aligned using a Longest Common Subsequence (LCS) al-gorithm. We describe two scoring functions TRANS-cs and TRANS-its which account for both the LCS length and the lengths of the original word sequences. Experiments demon-strate that TRANS-its is particularly successful in finding translations of books and outperforms several baselines in-cluding metadata search based on matching titles and au-thors. Experiments performed on a Europarl parallel cor-pus for four language pairs, English-Finnish, English-French, English-German, English-Spanish, and a scanned book col-lection of 50K English-German books show that the pro-posed method retrieves translations of books with an average MAP score of 1.0 and a speed of 10K book pair comparisons per second on a single core.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Collection, Systems Issues Algorithms, Experimentation Translation detection, sequence alignment, unique words, book collections
This paper describes an approach to finding translations of documents which are long and noisy -specifically scanned books with OCR errors in large collections such as the In-ternet Archive (IA) or Google Books. However, it is also applicable to documents produced by governments and com-panies.

Finding translations is useful for many reasons. It will enable search engines to display translated versions of a book as part of the results so that for example a Spanish reader may choose a Spanish version of Goethe X  X  Faust. By finding translations one can create parallel corpora for cre-ating better machine translation algorithms and for cross-lingual search systems. The humanities and library commu-nities have a great interest in aggregating works and finding translated versions of books such as Goethe X  X  Faust. IFLA X  X  Functional Requirements for Bibliographic Records (FRBR) requires that the next generation of cataloging systems in-clude works aggregation [22]. This will include information on which books are translated versions of each other. How-ever, no specific technique is proposed to do the FRBR-ization and it is implicitly assumed that metadata will be sufficient. Experiments show that metadata is not accurate enough to always determine which books are translations.
There are two distinct problems in the context. The first problem, which is the focus of this paper, is to decide whether given two books are translations of each other and to do it for all book pairs in the collection. Given that most book pairs are not translations, comparing all book pairs can be expensive since there are O ( nm ) distinct book pairs in collection of n books in one language and m books in the other. Hence, there is a need for an efficient approach. The second problem is to map the portions of translated text be-tween any two books in different languages. This is not the focus of this paper although we provide Figure 1 to illustrate translated portions of two example books.

Books and translations of books have many interesting characteristics. Books are usually much longer than web documents. Texts obtained from scanned books have also character recognition errors -in some cases substantial -and any algorithm must cope with them. Most translations do not have one-to-one overlap. Figure 1 shows the automat-ically generated overlap between Wiclif X  X  biography in the original German and a translated version in English which shows that only a portion of the two texts overlap.
One approach is to use the book metadata to find trans-lations of books. Our experience, however, is that metadata entries can be erroneous and therefore they are not com-pletely reliable. This approach, therefore, does not solve the problem as discussed further in the experiments section. There are several types of errors in the metadata of scanned books. First of all, the language of books are often specified incorrect. In a test collection of 378 books, the language of several books was incorrectly specified -they are marked as English even though they are clearly in German or vice versa. Books written in multiple languages are typically not clarified too. There are books marked as English although they are in German with an English preface and/or notes. Even if the metadata is correct, it is sometimes not easy to tell whether two books are translations or not. Quite of-ten titles do not translate exactly to other languages. Even though two books have the same title after translation, the translated version may have only the translator X  X  or editor X  X  name as the author. Metadata entries are manually entered to the system by the people who scan books, therefore the process is error prone. A similar problem does also exist for different Wikipedia articles. While some articles are direct translations of each other, many articles with the same title are actually written by different authors and therefore they are not translations. Therefore, Wikipedia articles can not be used for building translation detection corpora since the ground truth is not clear.

Techniques have been previously suggested for finding near more than 20 times in the entire text are filtered out in both books. The remaining words in the English book are translated in place to German using a word dictionary and aligned with the remaining words in the German book using LCS. For visualization purposes we use a binning approach where each bin in the figure is colored blue (black) if there are more than a specified amount of matching words in the range. The bin size is 100 words and the horizontal axis shows the number of bins for each book.
 duplicates in the same language using shingling (n-gram overlap) [4, 5] or even partial duplicates using the align-ment of  X  X nique words X  [30]. The applicability of such tech-niques to translation detection is not trivial. Word order is not usually preserved across languages and hence trans-lations of individual words in a book using a dictionary do not preserve n-grams of words. Thus, traditional shingling techniques are not directly applicable for translation detec-tion. In addition most free dictionaries available online are small. For example, the largest English-German dictionary available to us has 62K entries while a desktop edition of Merrian Webster X  X  Collegiate dictionary has 225K entries. Due to the fact that morphological variants of words are of-ten not found in small size dictionaries, less words get trans-lated. Another option is to use a machine translation system to translate all the books to a common language and apply mono-lingual duplicate detection techniques as Uszkoreit et al. [29] used at Google. However, this approach requires building robust translation systems for each language and the actual translation stage is computationally expensive. Given that most researchers and organizations do not have Google X  X  computational resources, a more practical solution is needed. Krstovski and Smith [19] use hapax words, i.e., words which are common between two different languages, to identify translation pairs in scanned book collections. They adopt a vector space representation for books and use Cosine distance as the translational similarity metric. The weakness of this approach is that there is no guarantee there exists hapax words between all pairs books. Their results also in-d icate that their approach fail for languages with different language families, such as English and Arabic.

To detect translations we exploit the fact that a trans-lation must preserve the long range order of events and/or ideas. That is, chapter 5 must precede chapter 6 in both En-glish and German versions of  X  X he Lord of the Rings X  even though individual sentences (and even paragraphs) do not preserve the word order across languages. Inspired by the work on mono-lingual partial duplicate detection of [30], we show that the sequence of words which occur only once in a book is sufficient to identify translations of books. Consider two books in two different languages, say English and Ger-man. The first step is to extract the sequence of words which occur only once in both books. Those words are referred as unique words . An English-German dictionary is used to transform the word sequence of the English book into Ger-man by translating individual words in place. Many words may end up being not translated since they do not exist in the dictionary. Some words may have multiple transla-tions which are all included in the translated sequence. It turns out that a small fraction of the words being trans-lated is sufficient for our purposes. Hapax words which are common in both sequences (examples of such words may include names which are not translated) are also included in the translated sequence. The resulting sequence is now in German and therefore can be compared with other Ger-man books. Comparison is performed using global align-ment, specifically Longest Common Subsequences (LCS) al-gorithm. The length of LCS is a clear indication of transla-tions. Two scoring functions are proposed: TRANS-cs and TRANS-its which normalize the LCS length by the length of the sequences in different ways. See Figure 2 for an illus-trative example of our methodology.

Experiments performed on non-noisy EUROPARL docu-ments for several languages and collections of real scanned book collections demonstrate that TRANS-its is very ef-fective and fast in identifying translations. Three different evaluation measures are defined and very high performance scores are obtained for four language pairs of the EURO-PARL dataset. English-Finnish experiments show that the technique works across language families. The technique also works on the noisy OCR output of scanned books as well. On a scanned book corpus of 2K English-German books, precision and recall score of 1.0 are achieved (out-performs Krstovski and Smith X  X  method [19]). Retrieval ex-periments including a scanned book collection of size 50K in-dicate that TRANS-its achieves a MAP of 1.0. We compare our results to several baselines including metadata search and show that TRAN-its outperforms the baselines over all evaluation metrics. The proposed method is also quite scal-able. With simple optimizations, it is seen that TRANS-its compares 10K books per second on a single core.

In the next section, we discuss the related work on trans-lation identification and also provide a brief discussion on mono-lingual duplicate detection methods. Section 3 ex-plains the proposed translation identification framework along with the unique word representation and the scoring func-tions. Evaluation measures, datasets and experiments are described next. Finally, conclusions are given along with future research directions.
The related problem of near duplicate detection in the same language has been well discussed especially for web documents. Most of the work uses either fingerprinting al-gorithms or relative frequency techniques (words with sim-ilar frequencies) [4]. Fingerprint techniques [4, 5] divide a document into distinctive chunks or shingles. The standard approach is to use n-grams of words or characters and sub-sample them using a variety of sampling techniques [14]. Relative frequency techniques assume that two documents with similar words and frequencies must be similar or du-plicated [14, 27]. We note that n-grams are not well pre-served across languages since word order in a sentence can change across translations. [30] find partial duplicates in collections of books by finding sequences of unique words and then aligning these sequences of unique words. How-ever, their work is restricted to books in the same language. Our work is inspired by their approach.

There has been work on finding comparable corpora for machine translation. Much of this work has been done on either finding parallel sentences from small corpora [28] or web pages [23, 26, 28, 32]. Most of the work on finding web page has utilized structural information -HTML markup such anchors, links, filenames -to find [23, 26] parallel re-sources. Alignment was specifically rejected as being too expensive. [32] limited the alignment to titles and a transla-tion dictionary to find parallel texts. Much of the machine translation work seems to be on the extraction of bilingual dictionaries [11] rather than finding document translations in large corpora. [28] is one of the few papers on identifying translations. The paper used several translation dictionar-ies and then computed the word overlap. Filtering was done based on document length for efficiency. The method was tested on a small dataset of about 1000 sentence pairs and another dataset of 325 web document pairs. [25] combined structural and content features to mine web pages for paral-lel corpora. [21] also used structural features paired with a content filtering scheme to find parallel corpora on the web. [18] used the idea that similar texts would have similar graph structures after compression to find translations of portions of texts.

Uszkoreit et al. [29] is one of two papers to find trans-lations of books. They use Google X  X  large computing re-sources to translate all the books in the collection to En-glish. This transforms the problem of finding translations to monolingual duplicate detection. Next, they match chunks (n-grams) of words in translated texts to determine transla-tion pairs. One drawback of this approach is that it requires building machine translation systems for all languages and translation of books is computationally expensive. Ideally, one should be able to find translations of books without having to translate them explicitly. The success of their approach is evaluated partially on a small dataset. Uszko-reit et al. X  X  method is further discussed in the experiments section. Krstovski and Smith [19] use words which are com-mon between translations of books to find translations of books. Each book is represented in the vector space and the translational similarities between books are defined by several distance measures such as Cosine distance. They use Locality Sensitive Hashing (LSH) to efficiently compute the translational similarity scores. Our technique is compared to their approach on the publicly available datasets and we demonstrate that our approach is more accurate.
There has been extensive work on mono-lingual and cross-l ingual plagiarism detection. Global alignment methods have been used to find plagiarized passages in the same language [7] but it is impractical for long documents and large col-lections. Most plagiarism detection techniques instead use a prefiltering stage which involves chunk overlap to detect possible duplicates before the global alignment [9]. Sequence alignment, word sampling and variants of chunking methods have also been tried for cross-lingual plagiarism detection. Please refer to [24] for a recent survey of those methods. It should be noted that cross-lingual plagiarism and translation detection for scanned book collections are different problem domains. Scanned book collections include very long docu-ments with severe amount of OCR errors which prohibit the use of conventional approaches.
The first stage of our framework is to identify the language of each book in the collection. This stage can be removed in case the languages of books is known reliably. The second stage involves extracting unique word sequences from all the books. This process is performed once for each book in the collection. In the final stage, all the book pairs between the source and target languages are aligned using Longest Common Subsequences. A translation score is calculated for each book pair based on the length of the LCS. This score is later used for classification and ranking of translation pairs. The details of each stage are elaborated in the following subsections.
Translation identification require that the language of the book be known. One approach to detect the language is to use the metadata, which is not always reliable. Language identification has been done in the past using stopwords and letter bigrams/trigrams. While letter bigrams/trigrams tend to be more accurate for short passages, on longer texts stopword counts work equally well [12]. Here we use the stopword approach to determine the language of the book. Stopwords for each language (English, French, German, Greek, Italian, Latin and Spanish) are learned from 20 noise free e-books downloaded from the Gutenberg archive. The top five most frequent stopwords are used. A stopword is appropri-ate for language identification as long as it is not a stopword in another language. This approach makes the language identification process simple, fast and easily generalizable for other languages. A more accurate check on OCR errors can be done using a dictionary but this would be slower and more expensive to create. Note that this technique may fail if the book has high rates of OCR errors which corrupts a large proportion of stopwords. A quick check on a mix of 378 English-German books reveals an accuracy of 100%.
Each book in the collection is represented by the sequence of words which appear only once in the entire text of the book. In this context these words are referred as  X  X nique words X . This sequence of unique words is highly descriptive of the content and flow of ideas in the book. This represen-tation is quite compact. There are are typically a few thou-sands of unique words for a book of size 100K words. The number of unique words increase as the amount of document noise and the length of the text increases. In a non-noisy book, every second sentence of the document is expected to contain a unique word. The unique word representation is highly tolerant to OCR errors for duplicate and translation detection purposes.

Punctuation and numeric characters are ignored at all stages. This also eliminates false matches caused by match-ing page numbers which by themselves form a consistent sequence between any two books. Hyphenated words are quite common at the end of each line and they are also corrected automatically before proceeding. For efficiency, unique words are precomputed and stored in binary files. Each unique word is represented by a 32-bit hashcode which is generated using a product sum algorithm over the entire text of the string. For batch processing, the sequences of hashcodes are appended one after another in to binary files which are referred to as  X  X arrels X . A barrel containing 2K books occupies 25-35 megabytes of disk space. Alternatively, one could also index unique words and assign a term ID for each unique word. However, it would be a two-pass approach with large memory and computation requirements since the vocabulary of scanned book collections becomes arbitrarily large as the size of the collection grows.

It should be noted that a unique word in one book may not be necessarily unique in another print version of the same book. This happens due to OCR errors and/or additional or missing text in the other book. Despite these factors, it is still highly probable to find a large number of common words between the two sequences preserving the same order for mono-lingual books. Here we show that this representation is also sufficient to find translation pairs at the book level.
Consider a pair of books -for example one in English and the other in German. At this point we have two unique word sequences extracted from these two books. The aim is to map the unique word sequence from the English book to German or vice versa. The first stage of mapping is to include the common words across translations (names are sometimes preserved across languages) in the translated se-quence. For the remaining words, we use a dictionary to translate them in place to German word by word. If there are multiple translations for a word, then they are also in-cluded in the translated sequence. It is clear that the trans-lated word sequence may include words repeated more than once after translation, but this is not an issue for the tech-nique.
Names of people and places are sometimes the same in both texts (i.e. not translated). They have high discrimina-tory power and it is desirable to incorporate them in to the analysis. For this purpose we first intersect and find all com-mon unique words prior to any translation. Then, the list of common words is interleaved with the translated unique word sequence and sorted based on their original location in text. Notice that names and places may be changed in the translated version of the book. In that case, we still have the translations of the unique words in the sequence which are sufficient to identify translation pairs.
The translation lexicon is an important component of the translation identification framework. Larger dictionaries help translate more unique words since they are more likely to b e found. It is desirable that the translation lexicon has as many inflections and forms of the word as possible for best performance -since we do not do any morphological pro-cessing. Our alignment algorithm (described later) will only match two words if they have the same characters in them. Preliminary experiments on stemming and lemmatizing the words produced no significant improvements in accuracy.
Translational probabilities do not play any role in our framework. The translation lexicon is therefore regarded as a table which maps one word in the source language to one or more words in the target language. There are two ways to obtain such a translation lexicon with one-to-many en-tries. One option is to train it automatically from a parallel corpus [17] and ignore (or threshold) translational proba-bilities. However, it was found that automatically learned translation lexicons contain a considerable amount of noise. There may be dozens of words most of which are actually not associated with the source word. Further, the training process is highly sensitive to the training corpus. A transla-tion lexicon learned from one corpus can not be generalized to another corpus.

A better option is to create a one-to-many translation lex-icon using a dictionary. One can make use of all information in the dictionary. All function words are removed on both sides of each entry using a language specific stopword list. If the source entry still consists of multiple words we delete it and do not use it. If the source side of an entry has a single word remaining, then one should include it in the translation lexicon along with all its possible translations one after the other. If a source word maps translates to multiple words then each of these possible translations is listed one after the other in the sequence. If the source word maps to a phrase, the phrase is split into separate words and every word in the phrase is listed as a possible translation in the hope that one of them will map correctly. If more than one dictionary is available, one can also create a larger dictionary by merging translation entries.
After the translating the unique word sequences of books in the source language to the target language, the next step is to compare each of them against all the books in the target language. Comparison is performed using the Longest Com-mon Subsequence (LCS) algorithm. LCS is basically a global alignment method which gives the longest sequence preserv-ing the long range order between two sequences. Having a large number of words in common preserving the order is a clear indication of translation.

There are a number of algorithms to compute LCS in the literature [8]. The standard dynamic programming algo-rithm has O ( mn ) time and space requirements, where m and n are the lengths of the input sequences. For long input se-quences, this algorithm has very large memory requirements. Therefore we adopt an O ( mn ) time and linear space LCS al-gorithm [13] to calculate the LCS length without computing the actual LCS sequence itself. There is also a O ( nloglogn ) time LCS algorithm for sequences where no element appears more than once within either input string [15]. This algo-rithm is not suitable for our purposes because the translated word sequence may include repeated words.

There are further improvements for fast LCS computa-tion. It is not necessary to compute LCS over the entire input sequences. One can disregard the words which do not appear in both sequences since a word must appear in both sequences at least once in order to be in the LCS. Another improvement is to avoid LCS computation entirely when conditions apply. Given the score threshold (used for classi-fying books pairs to be translations) and the lengths of the sequences, it is possible to solve for a lower bound for the LCS length L . If the number of common words between two sequences is less than L , then there is no need for the align-ment procedure since the resulting score is guaranteed to be lower than the threshold. These improvements provide sig-nificant speed-up. It should be noted that the intersection of elements between two sequences can be computed in linear time using a hashtable.
The length of LCS between the list of translated words and the list of unique words is used to classify or rank transla-tion pairs. The LCS length alone can not be used for trans-lation detection. The reason is that the number of unique words ( hence the length of LCS ) is a function of the book length according to Zipf X  X  Law. Longer texts are expected to have longer lists of unique words. It is therefore desir-able to normalize the LCS length based on the size of the books compared. Here we adopt the normalization tech-niques proposed in [30]. These approaches are elaborated in the subsections.
Using the analogy with correlation, the TRANS-cs score for two sequences of words X and Y is defined similar to the DUPNIQ-cs score in [30] as: where | LCS ( X, Y ) | is the LCS length for the aligned se-quences. | X | and | Y | represents the length of X and Y respectively. The resulting score has a range of [0,1]. The score is maximized when the two sequences are identical.
In this context, input word sequences are defined as ob-jects X and Y and those objects are assumed to be generated by a probabilistic model. Then, according to Lin [20], the similarity between any two objects can be defined as: Similarity is maximized when the two objects are identical. The joint description of two objects is defined to be over-all information content of both objects. In our case, the overlapping information content is defined by the longest common subsequence between X and Y and the total in-formation content (description) is defined by the alignment produced by LCS. Once the probability of any word sequence is assumed to be inversely proportional to its length, then Lin X  X  equation simplifies as:
T RAN S  X  its ( X, Y ) = TRANS-its has a range of [0,1]. The score is assumed to be zero if input sequences have no common words.
We investigate the effect of OCR errors on translation de-tection by generating synthetic errors in texts. A pair of texts is created as follows: Two error-free (no OCR errors) books are downloaded from the Project Gutenberg website [2] -one in the source language (the reference text) and a sec-ond in the target language. The latter is used for generating synthetic texts by adding a specified amount of random char-acter level document noise to simulate OCR errors. Unique words in the reference text are translated in to the target language. TRANS-its and TRANS-cs scores are computed for the reference and synthetic texts for different levels of document noise from 0% to 20% with 1% increments. Ex-periments are repeated one hundred times -each time with different random seeds -and the scores are averaged.
The noise model introduced in [10] is adopted for gen-erating the synthetic texts. The model basically performs string edit operations (insertion, deletion and replacement) over the entire text for the given amount for each type of noise. The total amount of noise is defined to be the total percentage of characters deleted, replaced and inserted over the entire string. The distribution of edit operations is de-fined to be uniform, i.e., [1/3, 1/3, 1/3] respectively. Case is folded and all punctuations and numerals are removed. The English-German dictionary used in the synthetic ex-periments contains 62K words including inflections.
Three different scenarios are investigated. In the first sce-nario, we evaluate the effect of OCR errors for true transla-tion pairs. In this case, the reference book is chosen to be  X  X gmont X  which is written in German by Johann Wolfgang von Goethe and synthetic texts are generated using the En-glish translation of the same book. In the second scenario, the same process is applied to two different books which are known not to be translations of each other but written by the same author -the German original of Goethe X  X  X  X gmont X  and an English translation of  X  X oethe X  X   X  X aust X . The pur-pose of this scenario is to test the robustness of the proposed method for texts having similar style and vocabulary. The third scenario investigates the case in which two different books are written by different authors -the German version of Goethe X  X  Egmont and an English version of  X  X he Critique of Pure Reason X  X y Immanuel Kant. In a collection the most common scenario is one where the books are not translations of each other and the authors are also different.
In Figure 3, it is clear that TRANS-its and TRANS-cs scores are substantially larger for the true translation pair compared to the other two non-translation pairs. For all scenarios, the translation scores are the highest when there is no document noise and they gradually fall as the amount of noise is increased. TRANS-cs score tend to fall more drastically compared to TRANS-its. For the true translation pair, TRANS-its and TRANS-cs scores fall below the given thresholds at approximate word error rate levels 49% and 44% respectively. Notice that these word error rates are very high and unlikely to happen in practice for printed books. [31] estimate that the OCR word error rate of scanned books in the IA database is less than 15% . The proposed method is robust to the OCR errors found in scanned book collections.
Table 1 provides further detail. In all scenarios, it is seen that the number of unique words increases as the amount of noise increases. The reason is that document noise (or OCR errors) tend to produce arbitrary words which are not in the vocabulary of the book (or even the language).

It is seen that the non-translation book pair having the same author has more common words and higher transla-tion scores compared to the third scenario where the non-translation book pair has different authors. The reason is that different books written by the same author are likely to have more common words in the vocabulary, even though one of them is translated by someone else. Despite this ef-fect, the proposed method successfully discriminates both non-translation book pairs from the true translation pair.
The length of the sequence of words following the same order in both contexts is a clear indication of translation. This can be seen more clearly for the book pairs having the same writer (scenarios 1 and 2). See Table 1. Both book pairs have comparable numbers of common words in their representations. This information alone does not help dis-criminate these two cases. However, the length of the LCS is considerably higher for the true translation pair. This means that there are a large number of words following the same order for the true translation pair whereas it is not the case for the other. The sequence information of words is there-fore a strong feature to detect translations. It is sufficient to have a small number of words in common preserving the same order compared to the total number of unique words in the book.
Three different evaluation methods are defined to eluci-date different aspects of the problem and also depending on what kind of ground truth is available. For large datasets, it is not possible to obtain manually labeled ground truth. In such cases, a retrieval approach must be adopted as de-scribed below.

Retrieval of Translations: In this approach, each book in the source language (English in our example) is regarded as a query and all the books written in the target language (German) are ranked according to their translational simi-larity score. MAP (Mean Average Precision) is calculated over the rank lists. The retrieval approach is feasible es-pecially for large datasets since the evaluation is practical. One can adopt a pooling approach in analogy with the tradi-tional IR ranking paradigm to obtain relevance judgments. The details are described in the experimental section.
Ranking All Book Pairs: Krstovski &amp; Smith [19] rank all the book pairs in a single list according to some simi-larity score and compute Average Precision (AP) over the e ntire ranked list. This is different than the retrieval of translations approach. Consider the following list of En-glish books E1, E2, E3 and German books G1, G2. Assume that the following ranked list is produced after comparing all the source-target book pairs (E3G1, E1G2, E2G2, E1G1, E3G2, E2G1). The retrieval of translations approach in-stead use E1, E2 and E3 as queries and compute the AP for each ranked list (E1G2, E1G1), (E2G2, E2G1) and (E3G1, E3G2) and average all the AP values to compute a MAP score. The ranking all book pairs approach is reasonable as long as the ground truth for the entire dataset is available. One may still go over the entire ranked list and annotate each pair manually. However, this is not feasible for large datasets since the number of book pairs to be checked is significantly larger than for the retrieval approach.
Binary Classification: This measure requires the sys-tem to classify each book pair as a translation or not. In the approaches we use this is done using a threshold over the translation scores. If the ground truth is available for the entire dataset, then precision and recall values can be gen-erated. It should be noted that precision/recall values are the most restrictive metrics, since they require translational scores to be comparable between different book pairs and a careful selection of the score threshold. Even if MAP and AP scores are both 1.0, it is possible to get either precision or recall values below 1.0. It happens when the score threshold is either too high or too low. The least restrictive evalua-tion metric is the MAP score for the retrieval task since it does not require the translational scores to be comparable between different queries.
This section begins with a listing of the datasets collected and used. This is followed by a description of the trans-lation lexicons used. Following this is a discussion of the baselines and other algorithms used for comparison. Finally, we describe a set of experiments carried out and the results obtained from them.
Books downloaded from the Internet Archive (IA) [1] were used to construct datasets. English-German training and
An English-German training set contains 30 scanned books (16 English, 14 German) from the IA database. It is manually verified that a book has at least one translation in the set. There are 31 true translation pairs in total. This set is used to estimate the translational similarity threshold for the scanned book experiments.

The EUROPARL parallel corpus is a standard collec-tion of text documents from the proceedings of the Euro-pean Parliament [16] used for machine translation. These documents are clean -since they have no OCR errors. Ver-sion 3 is used for our experiments in order to compare the results with the baseline approach described in [19]. It con-tains speeches from the period 04/1996-10/2006. There are over 600 documents each of which is translated in to 11 lan-guages. Unlike the scanned book collections, these texts do not include any document noise since they are translated and typed by humans. Among these parallel corpora, we use four language-pairs: English-Finnish, English-French, English-German and English-Spanish. Notice that Finnish is from a different language family compared to the other languages. The average number of words per document in the English collection is 50360 after removing the tags. Many of these documents are much shorter than most books.

The 2K dataset is an English-German collection of 2K scanned books and is one of the datasets used by Krstovski &amp; Smith in [19] and they refer it as the  X 17 book pairs X  dataset. The dataset is originally created by downloading a random selection of 1K German and 1K English books from the IA website and embedding 17 book translation pairs in it. However, our approach discovered that there are actu-ally 18 translation book pairs in the dataset. TRANS found three additional translation pairs and falsified two transla-tion pairs which were initially in the ground truth. After 2 h ttp://ciir.cs.umass.edu/downloads/trans-detect/ and http://books.cs.umass.edu/downloads/trans-detect/ manual investigation, the ground truth for this dataset has b een corrected and it is used for the experiments along with the updated results obtained from Krstovski &amp; Smith.
The 50K dataset is a collection of 50K books in Ger-man randomly selected from the the IA database. Using the language identifier, it is verified that the OCR outputs are not garbage and that the dominant language of these texts is German. This set is used only for ranking exper-iments. A set of 20 famous books in English are used for querying. Query books are chosen in a way that there exists at least one translation for each of them in the entire col-lection. The ground truth for the query set is obtained as follows: for each query book, books in the 50K collection are ranked according to the TRANS-cs, TRANS-its and meta-data scores. Each of these techniques produces a ranked list for each query. The top 200 ranking entries from all three lists were pooled for each query and then manually judged. This pooling approach provide a basis to determine relative effectiveness of the systems being compared. In total, 52 translation pairs were labeled for all 20 queries.
There are two ways to obtain a translation lexicon. The first one is to learn translations from a parallel corpus. The second one is to use a dictionary. We first tried to learn a translation lexicon for the English-German language pair using a statistical machine translation system [17]. Training was performed on the Europarl parallel corpus. However, final precision and recall figures were quite low compared to the dictionary approach. Therefore we decided to use the dictionary approach for the rest of our experiments.
Table 2 below shows statistics on the size of the dictionar-ies used in our experiments [3]. All the dictionaries provide translations for different forms of the word (such as plural, gerund, past participle etc.), whereas the English-German 5K and English-Finnish dictionaries lack this feature. We also provide the average percentage of unique words trans-lated using each dictionary. The percentages are generated for the EUROPARL corpus. We also tried a number of lemmatization techniques in order to improve translation success. Even if we observed improvements in the total num-ber of translated words, no improvement is observed in the precision and recall figures. Dictionary size and OCR er-ror rate are the determinants of the overall success of the framework.
Most work on creating parallel corpora has been focused on small datasets and using either structural information or the alignment of individual sentences [28] with two excep-tions: Uszkoreit et al. [29] and Krstovski &amp; Smith [19]. Uszkoreit X  X  approach is not used as a baseline since the datasets and the translation system they used are not avail-able to us. Here we use three baseline systems: metadata search, IBM MODEL 1 and where available numbers from Krstovski &amp; Smith [19].

META refers to using metadata search for finding trans-lation pairs in a collection of books. Here we use title and author information from the IA database as follows: first all the punctuation in the author and title fields are removed and all the characters are lowercased. Numeric characters are also ignored only for the author field since the date infor-mation leads to false matches. The title of the query book is also translated from English to German using the Google Translate API. The set of tokens in the author field of the query book is compared against the books in the collection of 50K German books using the Jaccard similarity. If the sim-ilarity is greater than zero, then the translated title is also compared against the title of each candidate book in the same way. The  X  X etadata score X  for a single pair of books is defined to be the average of the title and author Jaccard sim-ilarities. The metadata score is used to detect/rank books pairs for being translations. Notice that the metadata is not fully reliable since it is typed by people who scan and/or upload the book in to the IA database.

IBM M1 refers to the widely-used IBM Model 1 used for aligning words given two sentences in different languages [6]. It is used for different tasks over parallel corpora and essentially gives an estimate for the probability of a target sentence T in some language given a source sentence S in another language. There are several simplifying assump-tions in this model. It does not incorporate any information about the long range order of words in the source and tar-get sentences unlike the sequence of unique words. This approach is therefore ideal to demonstrate the effectiveness of bag-of-words models over long texts. Since this model is effective for ranking, we use it only for retrieval and rank-ing experiments. For fairness, the same dictionary is used for all techniques. Transition probabilities are estimated by assuming that all translations are equiprobable.

Krstovski &amp; Smith use an approach for generating a ranked list of book translation pairs without the use of bilin-gual dictionary or machine translation system [19]. Each book in the collection is represented in the vector space and cosine similarity is used to rank all the book pairs in the collection. The vector representation only accounts for the words which appear in both languages without any transla-tion. For each book, the weights of the vector representation are calculated by multiplying the frequency of the term in the book with the inverse document frequency of the term in the collection of books in the same language, i.e. (TFx-IDF). The Locality sensitive hashing (LSH) approximation algorithm is used to calculate cosine similarity to reduce the time complexity. We use their datasets and results which are publicly available.
The EUROPARL dataset is used to test the effectiveness of our approach for documents with no OCR errors. There are roughly 650 documents per language each of which has a translation in the other language. For each language pair we selected 50 translation pairs at random as a training set and used the remaining as a test set. The training set is used to train the score threshold (a different threshold for each lan-guage since dictionary sizes vary significantly). For English-German, the 62K dictionary is used. The evaluations are done on the test set. The retrieval and ranking all pairs ex-p eriments are shown in Tables 3 and 4 respectively. Binary classification results are given in Table 5. We notice that TRANS-its has a MAP score of 1.0 and an AP of 1.0 for both the retrieval and ranking of all pairs evaluations. TRANS-cs performs slightly worse on the English-German ranking of all pairs evaluation. We also list Krstovski&amp;Smith X  X  result for the English-German pair from their paper (their splits are different but the results are indicative). Krstovski&amp;Smith do not provide numbers for the other language pairs but they have graphs which clearly show that the AP score must be less than 1.0.

The binary classification experiments indicate that thresh-old selection is a hard problem compared to the ranking and retrieval paradigms. TRANS-its has a precision of 1.0 for all language pairs. TRANS-its also ranks all the document pairs perfectly since the AP score is 1.0. However, the recall values are slightly lower than 1.0 for English-Finnish and English-German datasets. The reason is that one pair in the English-Finnish and two pairs in the English-German dataset are below the score threshold although they are rel-evant. Further analysis of the results show that missing doc-ument pairs are actually very short (a few hundred words). Our technique is quite robust for longer documents. Preci-sion and recall values for TRANS-cs are both lower than 1.0 for the English-German dataset, which indicates there are relevant documents below the score threshold while there are false positives with a score higher than the threshold. Clearly TRANS-its performs very well on all metrics.
Table 6 shows results for the retrieval experiments on real scanned books for the English-German datasets (Train, 2K and 50K). The best scores are shown in bold face. TRANS-its (using the large dictionary) is the most successful system among all others providing MAP scores of 1.0. Note that the results are worse when the smaller dictionary is used. Metadata search (META) performs well in ranking books for both the train and test sets but not as well for the 50K dataset ( MAP = 0.821 ). TRANS-cs is much worse indicat-ing the importance of LCS length normalization. IBM-M1 performs poorly. In all the tables below  X  X ict X  refers to the size of the dictionary.

The evaluation for the ranking all book pairs experiment is g iven in Table 7. Experiments are not performed on the 50K dataset because it would require judging several thousand entries. TRANS-its again has an AP of 1.0 for the train and 2K datasets. Metadata search has a lower AP scores on both train and 2K datasets. IBM-M1 again performs poorly. Note that TRANS-its has an AP score of 1.0 even with a dictionary of size 5K. TRANS-cs performs slightly worse with a smaller dictionary. Krs.&amp;Smith obtained an AP = 0.945 for the 2K dataset (their precision, recall and MAP results are not available for the 2K dataset).
Binary classification is performed by learning the score threshold from the train set and it is used for the 2K dataset. As seen in Table 8, TRANS-its with 62K dictionary gives perfect precision and recall values for both datasets. TRANS-cs and TRANS-its both provide perfect scores on the train set even with a small dictionary. Precision values for the 2K dataset fall if the small dictionary is used. The drastic fall in the precision figures for the 2K dataset is due to the low score threshold. This indicates that there is a need for a bet-ter threshold selection paradigm since both score functions actually perform very well in ranking all book pairs exper-iment, as shown in Table 7. Surprisingly metadata search does not provide perfect scores (precision = 0.739, recall = 0.944) for either the 2K set or the train set.

Uszkoreit et al. [29] best published result (using an oracle to choose the threshold) for a dataset of 103 books (English-French) with 30 matching pairs has a precision of 1.0 and a recall of 0.71. Although it is not directly comparable, we note that TRANS-its has both precision and recall 1.0 on a 2K book dataset. TRANS also does not require complete translation of books. Unfortunately, their machine transla-tion system and datasets are not publicly available for us to be able to make a direct comparison.
A translation identification framework is presented for large scanned book collections with OCR errors. Unique words (which appear only once in the whole book) along with their actual order in the text are used to represent each book in the collection. This sampling strategy provides a compact representation and it enables efficient identifica-tion of translation pairs. A dictionary approach is adopted to translate word sequence representations. Fairly small dic-tionaries work well. The proposed approach is shown to be quite robust to high rates of OCR errors and it outperforms several baselines including metadata search. Retrieval ex-periments on several datasets including the Europarl parallel corpus with four different language pairs show that the pro-posed method retrieves translation pairs with a MAP score of 1.0. Future work includes further speed-ups, extensions to multiple languages and mapping translated portions.
W e would like to thank David Smith and Kriste Krstovski for providing their datasets and results, Ethem F. Can for helping with the figures and Internet Archive for their book collection. This work was supported in part by the Cen-ter for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the au-thors X  and do not necessarily reflect those of the sponsor. Approach Dict Thr Train 2K TRANS-its 62K 0.49 1.0 1.0 1.0 1.0 TRANS-cs 62K 0.023 1.0 1.0 0.782 1.0 TRANS-its 5K 0.395 1.0 1.0 0.122 1.0 TRANS-cs 5K 0.0085 1.0 1.0 0.01 1.0
META -0.275 0.882 0.968 0.739 0.944
