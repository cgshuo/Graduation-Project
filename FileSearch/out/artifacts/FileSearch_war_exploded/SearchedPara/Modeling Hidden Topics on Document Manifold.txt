 Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each docu-ment on the hidden topics independently and the number of param-eters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allo-cation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evi-dence that the document space is Euclidean, or flat . Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specif-ically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the am-bient space and directly performs the topic modeling on this doc-ument manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimen-tal results show that LapPLSI provides better representation in the sense of semantic structure.
 G.3 [ Mathematics of Computing ]: Probability and Statistics X  Statistical computing ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Algorithms, Performance, Theory Probabilistic Latent Semantic Indexing, Manifold Regularization, Document Representation, Generative Model
Document representation has been a key problem for document analysis and processing[8][10][11]. The Vector Space Model (VSM) might be one of the most popular models for document represen-tation. In VSM, each document is represented as a bag of words . Correspondingly, the inner product (or, cosine similarity) is used as the standard similarity measure for documents or documents and queries. Unfortunately, it is well known that VSM has severe draw-backs, mainly due to the ambiguity of words ( polysemy ) and the personal style and individual differences in word usage ( synonymy ).
To deal with these problems, IR researchers have proposed sev-eral dimensionality reduction techniques, most notably Latent Se-mantic Indexing (LSI) [8]. LSI uses a Singular Value Decompo-sition (SVD) of the term-document matrix X to identify a linear subspace (so-called latent semantic space ) that captures most of the variance in the data set. The general claim is that similarities between documents or between documents and queries can be more reliably estimated in the reduced latent space representation than in the original representation. LSI received a lot of attentions during these years and many variants of LSI have been proposed [1][20].
Despite its remarkable success in different domains, LSI has a number of deficits, mainly due to its unsatisfactory statistical for-mulation [12]. To address this issue, Hofmann [11] proposed a generative probabilistic model named Probabilistic Latent Seman-tic Indexing (PLSI). PLSI models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representa-tions of  X  X opics. X  Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribu-tion is the  X  X educed representation X  associated with the document. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus. This leads to serious problems with overfitting [16][5][19]. Latent Dirichlet Allocation (LDA) is then proposed to overcome this problem by treating the probability distribution of each document over topics as a K -parameter hidden random vari-able rather than a large set of individual parameters, where the K is the number of hidden topics.

Both of the above two topic modeling approaches discover the hidden topics in the Euclidean space. However, there is no convinc-ing evidence that the documents are actually sampled from a Eu-clidean space. Recent studies suggest that the documents are usu-ally sampled from a nonlinear low-dimensional manifold which is embedded in the high-dimensional ambient space [10][23]. Thus, the local geometric structure is essential to reveal the hidden se-mantics in the corpora. In this paper, we propose a new algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI). LapPLSI con-siders the topic modeling on the document manifold. It models the document space as a submanifold embedded in the ambient space and directly perform the topic modeling on this document mani-fold in question. By discovering the local neighborhood structure, our algorithm can have more discriminating power than PLSI and LDA. Specifically, LapPLSI first builds an nearest neighbor graph to model the local document manifold structure. It is natural to as-sume that two sufficiently close documents have similar probability distribution over different topics. The nearest neighbor graph struc-ture is then incorporated into the log-likelihood maximization as a regularization term for LapPLSI. In this way, the topic model esti-mated by LapPLSI maximizes the joint probability over the corpus and simultaneously respects the local manifold structure.
It is worthwhile to highlight several aspects of our proposed al-gorithm here: 1. The conventional generative probabilistic modeling approaches, 2. The graph Laplacian used in our algorithm is a discrete ap-3. Our algorithm constructs a nearest neighbor graph to model
The rest of the paper is organized as follows: in Section 2, we give a brief review of topic modeling with PLSI and LDA. Section 3 introduces our algorithm and give a theoretical analysis of the algorithm. Extensive experimental results on document modeling and document clustering are presented in Section 4. Finally, we provide some concluding remarks and suggestions for future work in Section 5.
The core of Probabilistic Latent Semantic Indexing (PLSI) is a latent variable model for co-occurrence data which associates an unobserved topic variable z k  X  X  z 1 ,  X  X  X  ,z K } with the occur-rence of a word w j  X  X  w 1 ,  X  X  X  ,w M } in a particular document d i  X  X  d 1 ,  X  X  X  ,d N } . As a generative model for word/document co-occurrences, PLSI is defined by the following scheme: 1. select a document d i with probability P ( d i ) , 2. pick a latent topic z k with probability P ( z k | d i 3. generate a word w j with probability P ( w j | z k ) . As a result one obtains an observation pair ( d i ,w j ) , while the la-tent topic variable z k is discarded. Translating the data generation process into a joint probability model results in the expression The parameters can be estimated by maximizing the log-likelihood where n ( d i ,w j ) the number of occurrences of term w j ment d i . The above optimization problem can be solved by using standard EM algorithm [9].

Notice that there are NK + MK parameters { P ( w j | z k ) ,P ( z which are independently estimated in PLSI model. It is easy to see that the number of parameters in PLSI grows linearly with the num-ber of training documents ( N ). The linear growth in parameters suggests that the model is prone to overfitting [16][5].
To address this issue, Latent Dirichlet Allocation (LDA) [5] is then proposed. LDA assumes that the probability distributions of documents over topics are generated from the same Dirichlet dis-tribution with K parameters. Essentially, LDA modifies the second step of PLSI generating scheme: 1. select a document d i with probability P ( d i ) , 2. pick a latent topic z k , 3. generate a word w j with probability P ( w j | z k ) . Dir (  X  ) is the Dirichlet distribution with a K -dimensional param-eter  X  .

The K + MK parameters in a K -topic LDA model do not grow with the size of the corpus. Thus, LDA does not suffer from the same overfitting issue as PLSI.
By assuming that the probability distributions of documents over topics are generated from the same Dirichlet distribution, LDA avoids the overfitting problem of PLSI. However, both of these two algorithms fail to discover the intrinsic geometrical and discrim-inating structure of the document spare, which is essential to the real applications. In this Section, we introduce our LapPLSI algo-rithm which avoids this limitation by incorporating a geometrically based regularizer.
Recall that the documents d  X  D are drawn according to the dis-tribution P D . One might hope that knowledge of the distribution P
D can be exploited for better estimation of the conditional distri-bution P ( z | d ) . Nevertheless, if there is no identifiable relation be-tween P D and the conditional distribution P ( z | d ) , the knowledge of P D is unlikely to be very useful.

Therefore, we will make a specific assumption about the con-nection between P D and the conditional distribution P ( z assume that if two documents d 1 ,d 2  X  D are close in the intrin-sic geometry of P D , then the conditional distributions P ( z P ( z | d 2 ) are similar to each other. In other words, the conditional probability distribution P ( z | d ) varies smoothly along the geodesics in the intrinsic geometry of P D . This assumption is also referred to as manifold assumption [3], which plays an essential rule in devel-oping various kinds of algorithms including dimensionality reduc-tion algorithms [3][10] and semi-supervised learning algorithms [4][24].
 Let f k ( d )= P ( z k | d ) be the conditional Probability Distribution Function (PDF), we use f k 2 M to measure the smoothness of f along the geodesics in the intrinsic geometry of P D .Whenwe consider the case that the support 1 of P D is a compact submanifold M X  R M , a natural choice for f k 2 where  X  M is the gradient of f k along the manifold M and the integral is taken over the distribution P D .

In reality, the document manifold is usually unknown. Thus, f tral graph theory [7] and manifold learning theory [2] have demon-strated that f k 2 M can be discretely approximated through a near-est neighbor graph on a scatter of data points.

Consider a graph with N vertices where each vertex corresponds to a document in the corpus. Define the edge weight matrix W as follows: where N p ( d i ) denotes the set of p nearest neighbors of d L = D  X  W ,where D is a diagonal matrix whose entries are col-umn (or row, since W is symmetric) sums of W , D ii = j W L is called graph Laplacian [7], which is a discrete approximation to the Laplace-Beltrami operator M on the manifold [2]. Thus, the discrete approximation of f k 2 M can be computed as follows: where f k =[ f k ( d 1 ) ,  X  X  X  ,f k ( d M )] T =[ P ( z k R k can be used to measure the smoothness of conditional proba-bility distribution function P ( z k | d ) along the geodesics in the in-trinsic geometry of the document set. By minimizing R k ,weget a conditional PDF function f k which is sufficiently smooth on the document manifold. A intuitive explanation of minimizing R that if two documents d i and d j are close ( i.e . W ij is big), f and f k ( d j ) are similar to each other.

Now we can define our new latent variable model. The new model adopts the generative scheme of PLSI. It aims to maximize
In mathematics, a support of a function f from a set X to the real numbers R is a subset Y of X such that f ( x ) is zero for all x that are not in Y . the regularized log-likelihood as follows: where  X  is the regularization parameter.
To see how we can estimate the parameters in our LapPLSI model, we first consider the case that  X  =0 . In this case, LapPLSI boils down to the traditional PLSI model.

The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Maximization (EM) al-gorithm [9]. EM alternates two steps: (i) an expectation (E) step where posterior probabilities are computed for the latent variables, based on the current estimates of the parameters, (ii) a maximiza-tion (M) step, where parameters are updated based on maximizing the so-called expected complete data log-likelihood which depends on the posterior probabilities computed in the E-step.
Recall in PLSI, we have NK + MK parameters { P ( w j | z k and the latent variables are the hidden topics z k . For simplicity, we use  X  to denote all the NK + MK parameters.
 E-step:
The posterior probabilities for the latent variables are P ( z which can be computed by simply applying Bayes X  formula on Eqn. (1): M-step:
With simple derivations [12], one can obtain the relevant part of the expected complete data log-likelihood for PLSI: Q ( X ) = Maximizing Q ( X ) with respect to the parameters  X  and with the constraints that K k =1 P ( z k | d i )=1 and M j =1 P ( w one can obtain the M-step re-estimation equations [12]: With a initial random guess of { P ( w j | z k ) ,P ( z k nately applies the E-step equation (7) and M-step equations (8, 9) until a termination condition is met.

Our LapPLSI model adopts the same generative scheme as that of PLSI. Thus, LapPLSI has exactly the same E-step as that of PLSI. For the M-step, it can be derived that the relevant part of the expected complete data log-likelihood for LapPLSI is
Q ( X ) = Q ( X )  X   X  R Since the regularization part R only involves the parameters P ( z we can get the same M-step re-estimation equation for P ( w as in Eqn. (8). However, we do not have a close form re-estimation equation for P ( z k | d i ) . In this case, the traditional EM algorithm can not be applied.

In the following, we discuss how to use the generalized EM al-gorithm (GEM) [14] to maximize the regularized log-likelihood of LapPLSI in Eqn. (6). The major difference between GEM and traditional EM is in the M-step. Instead of finding the globally op-timal solutions for  X  which maximize the expected complete data log-likelihood Q ( X ) in the M-step of EM algorithm, GEM only needs to find a  X  X etter X   X  .Let  X  n denote the parameter values of the previous iteration and  X  n +1 denote the parameter values of the current iteration. The convergence of GEM algorithm only requires that Q ( X  n +1 )  X Q ( X  n ) [14].
 In each M-step, we have parameter values  X  n and try to find  X  +1 which satisfy Q ( X  n +1 )  X Q ( X  n ) . Apparently, Q ( X  Q ( X  n ) holds if  X  n +1 = X  n .

We have Q ( X ) = Q ( X )  X   X  R . Let us first find  X  (1) n +1 maximizes Q ( X ) instead of the whole Q ( X ) . This can be done by simply applying Eqn. (8) and (9). Clearly, Q ( X  (1) n +1 ) does not necessarily hold. We then try to start from  X  (1) decrease R , which can be done through Newton-Raphson method [17]. Notice that R only involves parameters P ( z k | d i need to update P ( z k | d i ) n +1 part in  X  n +1 .

Given a function f ( x ) and the initial value x t , the Newton-Raphson updating formula to decrease (or increase) f ( x ) is as follows: where 0  X   X   X  1 is the step parameter. Since we have the Newton-Raphson method will decrease R k in each updating step. With  X  (1) n +1 and put R k into the Newton-Raphson updating formula in Eqn. (10), we can get the close form solution for  X  Eqn. (11) as long as K k =1 P ( z k | d i ) ( t ) n +1 =1 and P ( z 0 . Notice that the P ( w j | z k ) n +1 part in  X  n +1 will keep the same.
Every iteration of Eqn. (11) makes the topic distribution smoother on the nearest neighbor graph, essentially, smoother on the docu-ment manifold. The step parameter  X  can be interpreted as a con-trolling factor of smoothing the topic distribution among the neigh-bors. When it is set to 1, the new topic distribution of a document is the average of the old distributions from its neighbors. This pa-rameter will affect the convergence speed but not the convergence result.

We continue the iteration of Eqn. (11) until Q ( X  ( t +1) Then we test whether Q ( X  ( t ) n +1 )  X Q ( X  n ) . If not, we reject the proposal of  X  ( t ) n +1 , and return the  X  n as the result of the M-step, and continue with the next E-step. We summarize the model fitting approach of our LapPLSI by using generalized EM algorithm in Algorithm (1).
 Algorithm 1 Generalized EM for LapPLSI Input: N documents with a vocabulary size M Output: P ( z k | d i ) ,P ( w j | z k ) ,i =1 ,  X  X  X  ,N ; j =1 , 1: Compute the the graph matrix W as in Eqn. (4); 2: Initialize the probability distributions (parameters)  X  3: n  X  0 ; 4: while (true) 5: E-step : Compute the posterior probability as in Eqn. (7) ; 6: Compute P ( w j | z k ) n +1 as in Eqn. (8); 7: Compute P ( z k | d i ) n +1 as in Eqn. (9); 9: Compute P ( z k | d i ) (2) n +1 as in Eqn. (11); 12: Compute P ( z k | d i ) (2) n +1 as in Eqn. (11) 13: if Q ( X  (1) n +1 )  X Q ( X  n ) 15: else 17: if ( Q ( X  n +1 )  X  X  ( X  n )  X   X  ) 18: break; 19: n  X  n +1 ; 17: return  X  n +1
In this section, we evaluate our LapPLSI algorithm in two appli-cation domains: topic representation and document clustering.
In all the mixture models, the expected complete log-likelihood of the data has local maxima at the points where all or some of the mixture components are equal to each other. We run the EM algorithm multiple times with random starting points to improve the local maximum of the EM estimates. To make the comparison fair, we use the same starting points for PLSI and LapPLSI.
We empirically set the number of nearest neighbors p to 7, the value of the Newton step parameter  X  to 0.1, the value of the regu-larization parameter  X  to 1000.
In order to visualize the hidden topics discovered by LapPLSI approach, we conduct the following experiment on TREC AP cor-pus. We use a subset of the TREC AP corpus containing 2,246 newswire articles with 10,473 unique terms 2 .

To compare different approaches, we randomly pick four terms ( i.e .,  X  X ilm X ,  X  X chool X , X  X pace X  and  X  X omputer X ), and find four top-ics that have these four terms as the most representative terms, re-spectively. That is, for term w j , we find the topic z k different approaches on the same topic and evaluate the terms gen-
This TREC AP subset can be downloaded at http://www.cs.princeton.edu/  X  blei/lda-c/ Table 1: The 15 most representative terms generated by our LapPLSI algorithm for four topics. The terms are selected ac-cording to the probability P ( w | z ) .
 Table 2: The 15 most representative terms generated by the PLSI algorithm for four topics. The terms are selected accord-ing to the probability P ( w | z ) .
 erated by them that are used to represent this particular topic. Table 1, 2 and 3 show the terms generated by the LapPLSI, PLSI, and LDA algorithms, respectively, to represent the four topics. For all these three algorithms, we need to pre-define the number of hidden topics in the data set. We empirically set it to 100 as suggested in [5].

All the three topic modeling approaches have quite good per-formance on these four topics. For the first three topics, although different algorithms select slightly different terms, all these terms can describe the corresponding topic to some extent. For the forth topic ( X  X omputer X ), LapPLSI is slightly better than PLSI and LDA. As can be seen, LapPLSI selects more terms related to  X  X omputers X  ( e.g ., technology, equipment) than PLSI and LDA. In the next sub-section, we give a quantitative evaluation of these three algorithms on document clustering.
Clustering is one of the most crucial techniques to organize the documents in an unsupervised manner. The hidden topics extracted by the topic modeling approaches can be regarded as clusters. The estimated conditional probability density function P ( z k Table 3: The 15 most representative terms generated by the LDA algorithm for four topics. The terms are selected accord-ing to the probability P ( w | z ) .
 used to infer the cluster label of each document. In this experi-ment, we investigate the use of topic modeling approach for text clustering.
We conducted the performance evaluations using the TDT2 3 the Reuters 4 document corpora. These two document corpora have been among the ideal test sets for document clustering purposes because documents in the corpora have been manually clustered based on their topics and each document has been assigned one or more labels indicating which topic/topics it belongs to.
The TDT2 corpus consists of data collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11201 on-topic documents which are classified into 96 semantic categories. In this experiment, those documents appearing in two or more categories were removed, and only the largest 30 categories were kept, thus leaving us with 9,394 documents in total.

The Reuters corpus contains 21578 documents which are grouped into 135 clusters. Compared with TDT2 corpus, the Reuters corpus is more difficult for clustering. In TDT2, the content of each cluster is narrowly defined, whereas in Reuters, documents in each cluster have a broader variety of content. Moreover, the Reuters corpus is much more unbalanced, with some large clusters more than 200 times larger than some small ones. In our test, we discarded doc-uments with multiple category labels, and only selected the largest
Nist Topic Detection and Tracking corpus at http://www.nist.gov/speech/tests/tdt/tdt98/index.htm
Reuters-21578 corpus is at http://www.daviddlewis.com/resources/testcollections/reuters21578/ 30 categories. This left us with 8067 documents in total. Table 4 provides the statistics of the two document corpora.
The clustering result is evaluated by comparing the obtained la-bel of each document with that provided by the document corpus. Two metrics, the accuracy ( AC ) and the normalized mutual in-formation metric ( MI ) are used to measure the clustering perfor-mance [21][6]. Given a document x i ,let r i and s i be the obtained cluster label and the label provided by the corpus, respectively. The AC is defined as follows: where n is the total number of documents and  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map( r i ) is the permutation mapping function that maps each clus-ter label r i to the equivalent label from the data corpus. The best mapping can be found by using the Kuhn-Munkres algorithm [13].
Let C denote the set of clusters obtained from the ground truth and C obtained from our algorithm. Their mutual information metric MI ( C, C ) is defined as follows: where p ( c i ) and p ( c j ) are the probabilities that a document arbi-trarily selected from the corpus belongs to the clusters c ily selected document belongs to the clusters c i as well as c the same time. In our experiments, we use the normalized mutual information MI as follows: where H ( C ) and H ( C ) are the entropies of C and C , respec-tively. It is easy to check that MI ( C, C ) ranges from 0 to 1. MI =1 if the two sets of clusters are identical, and MI =0 if the two sets are independent.
To demonstrate how the document clustering performance can be improved by topic modeling approaches, we implemented four state-of-the-art clustering algorithms as follows. Table 5 and 6 show the evaluation results using the TDT2 and the Reuters corpus, respectively. The evaluations were conducted with the cluster numbers ranging from two to ten. For each given clus-ter number k , 50 test runs were conducted on different randomly chosen clusters, and the final performance scores were obtained by averaging the scores from the 50 tests.

These experiments reveal a number of interesting points:
Our LapPLSI model has two essential parameters: the number consistent good performance with the  X  varying from 500 to 50000. of nearest neighbors p and the regularization parameter  X  . Figure 3 and Figure 4 show how the performance of LapPLSI varies with the parameters  X  and p , respectively. As we can see, the LapPLSI is very stable with respect to both the parameter  X  and p . It achieves consistent good performance with the  X  varying from 500 to 50000 and p varying from 5 to 11. We have presented a novel method for topic modeling, called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI). Lap-PLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. As a result, LapPLSI can have more discriminating power than traditional topic modeling ap-proaches which discover the hidden topics in the Euclidean space, e.g . PLSI and LDA. Experimental results on document modeling and document clustering show that LapPLSI provides better repre-sentation in the sense of semantic structure.

Several questions remain to be investigated in our future work: 1. There is a parameter  X  which controls the smoothness of our 2. We consider the topic modeling on document manifold and 3. It would be very interesting to explore different ways of con-The work was supported in part by the U.S. National Science Foun-dation NSF IIS-05-13678, NSF BDI-05-15813 and MIAS (a DHS Institute of Discrete Science Center for Multimodal Information Access and Synthesis). Any opinions, findings, and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies. to 11. [1] R. Ando. Latent semantic space: Iterative scaling improves [2] M. Belkin. Problems of Learning on Manifolds . PhD thesis, [3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral [4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. [6] D. Cai, X. He, and J. Han. Document clustering using [7] F. R. K. Chung. Spectral Graph Theory , volume 92 of [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum [10] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality preserving [11] T. Hofmann. Probabilistic latent semantic indexing. In Proc. [12] T. Hofmann. Unsupervised learning by probabilistic latent [13] L. Lovasz and M. Plummer. Matching Theory .Akad  X  e miai [14] R. Neal and G. Hinton. A view of the EM algorithm that [15] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral clustering: [16] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. [17] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. [18] J. Shi and J. Malik. Normalized cuts and image [19] L. Si and R. Jin. Adjusting mixture weights of gaussian [20] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Latent semantic [21] W. Xu, X. Liu, and Y. Gong. Document clustering based on [22] H. Zha, C. Ding, M. Gu, X. He, , and H. Simon. Spectral [23] D. Zhang, X. Chen, and W. S. Lee. Text classification with [24] X. Zhu and J. Lafferty. Harmonic mixtures: combining
