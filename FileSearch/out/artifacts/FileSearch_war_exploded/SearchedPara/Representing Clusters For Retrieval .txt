 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Retrieval models Keywords: Cluster-based Retrieval, Cluster Representation In cluster-based retrieval ( CBR), documents are grouped into clusters which are then retrieved directly in their entirety to the query or used to smooth docum ent language models [1]. The clusters are typically represented as simple concatenation of their member documents [1, 2, 3]. Th is representation, while being simple and intuitive, may have a number of problems. For example, if one of the member documents is very long and has many occurrences of the query terms wh ile other member documents are short with only few query terms appearing, then simply concatenating these documents w ould result in a representation that is largely biased by one par ticular document. This is what we would want to avoid because the quality of clusters is usually judged by the total number of relevant documents they contain rather than how good one of the doc uments is [4]. Clusters with more relevant documents are cons idered better. Based on this, a representation that would allow for a more principled way of taking contributions from member documents is desired. Other representations have also been used in the past, e.g., centroid vector [5], but again they do not explicitly consider individual documents. This work describes an ongoing effort toward developing new cluster representa tions that would yield improved effectivess in CBR. We investigat e two methods  X  one is based on a mixture of term frequencies from member documents and the other is based on a mixture of member document language models. These representations are compared with the standard approach of concatenating documents in the context of CBR using query-specific clustering. Early results show that these methods are promising. The language modeling (LM) frame work has been shown to be theoretically attractive and very effective for studying information retrieval problems, including CBR [6, 1]. In this work, we focus on the traditional approach to CBR that is to retrieve clusters in their entirety to a query. To use LM approach for retrieving clusters, we first need to derive language models from cluster representations and then apply retrieval models. Let X  X  take the query likelihood retrieval model fo r example. Clusters are ranked based on the likelihood of generating the query, i.e. P(Q|Cluster) . It can be estimated by: where Q is the query, q i is the i th term in the query, and P(q i |Cluster) is specified by the cluster language model where P ML (w|Cluster) is the maximum likelihood estimate of word w in the document, P ML (w|Coll) is the maximum likelihood estimate of word w in the collection, tf(w, Cluster) is the term frequency of w in the cluster, tf(w, Coll) is the term frequency of w in the entire collection, w X  is any word, V is the vocabulary, and  X  is a general symbol for smoothi ng which takes different forms when different smoothing methods are used [1]. Commonly used smoothing methods include Dirichlet and Jelinek-Mercer smoothing, among others. The standard approach to representing clusters is to treat them as if they were big documents formed by concatenating their member documents. Thus, tf(w, Cluster) is computed by: where } ,..., { cluster. Clusters are ranked by equation (1) with components estimated from equations (2) and (3). Our first method is to represent clusters by a weighted mixture of term frequencies from member documents, that is, where  X  is a weighting parameter between 0 and 1, and 1 Clusters are ranked by equation (1) with components estimated from equations (2) and (4). This approach is referred to as TF mixture. Our second way of representing cl usters is to build language models for individual member doc uments and the cluster language model is a weighted mixture of these member document models. Again,  X  is a general symbol for smoothing. where  X  is a weighting paramete r between 0 and 1, and Clusters are ranked by equation (1) with components estimated from equation (5). We refer to th is approach as DM mixture. Three data sets are used in the experiments. They are: TREC topics 51-150 (title only) with the whole disks of TREC disk 1 and 2 (TREC12), TREC topics 301-400 (title only) with the whole disks of TREC disk 4 and 5 (TREC45), and TREC topics 51-150 (title only) with the Associated Press newswire (AP) collection. Both the queries and collecti ons have been stemmed and stopwords have been removed us ing the standard INQUERY list of 418 words. The data sets are summarized in table 1. TREC12 and TREC45 are large, heterogene ous collections in which both document sizes and topics vary widely. AP is an example of homogeneous collections. We first perform document-base d retrieval using the query likelihood (QL) retrieval model [6 ] with Dirichlet smoothing at 1000. Next, we take the top 1000 re trieved documents and cluster them using the K Nearest Neighbor cl ustering method. K is set to 5. The cosine similarity measure is used to determine the similarity between documents. As we discusse d in section 2, different ways of estimating the cluster language models are employed when different cluster representations are considered. Clusters are ranked by their query likelihood. Again, Dirichlet smoothing at 1000 is used for TF mixture and the standard approach of concatenating documents, which is the best parameter setting for the latter. We also use this smoothing parameter for setting the  X  in DM mixture (equation (5)). Currently, both  X  and  X  in equations (4) and (5) are estimated by the first-stage retrieval log QL score of each document divided by the sum of log QL scores of all member documents in a cluster. Note that the log QL scores are documents that match the query poorly. Retrieving clusters that have most relevant documents in the top ranks is the goal of any CBR system. We are most interested in studying whether the proposed cluster representations can help improve the ranking of relevant cl usters. Here, we take relevant clusters to be those that give a precision that is better than document-based retrieval with th e same number of documents (as top K documents from document-base d retrieval are all relevant, then we consider any cluster with all relevant documents to be relevant. Relevant clusters are identified based on the relevance judgments of documents provided by NIST (http://trec.nist.gov/). Evaluation is done using the Mean Reciprocal Rank (MRR). We go through the list of ranked clusters and mark the highest rank at which a relevant cluster is retrieve d. The reciprocal of the rank is computed. The MRR score is the average of reciprocal ranks across all queries on a data set. From table 2, we observe that, in general, both TF mixture and DM mixture perform better than the standard approach. DM mixture consistently gives the best performance on all three data sets. TM mixture and DM mixture are more effective for large heterogeneous collections such as TREC12 and TREC45, than for a homogeneous collection like AP . Significant improvements are obtained on the TREC12 collection. A further examination on AP reveals that the member documents in a cluster are similar in length and tend to contribute evenly to the query term distribution in the cluster model, so the proposed methods do not have much advantage over the standard approach in this case. We developed and evaluated nove l cluster representations for cluster-based retrieval in this wo rk. Early results show that the proposed methods generally perfo rm better than the standard approach. The DM mixture method performs best for all three data sets. These methods seem to be more effective for heterogeneous collections. For future work, we plan to continue exploring different ways of representing clusters and carry out more thorough evaluations on these methods. We will also look into features that can be used to improve cluster representations for homogeneous collections. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #CNS-0454018. 
