 I.5.1 [ Pattern Recognition ]: Mo dels-statistical; I.5.4 [ Pattern Recognition ]: Application-T ext pro cessing Algorithms Performance text categorization, feature selection, mac hine learning
Recursiv e feature elimination using SVM (SVM-RFE) has become an e ectiv e approac h in selecting predictiv e features. For example, [1] found 8 genes from 7,000 for cancer predic-tion using SVM-RFE. It is not well understo od, however, whether the success of SVM-RFE comes from the recursiv e pro cess or the choice of the classi er? And, more generally , what prop erty of a classi er would mak e it successful in RFE? We answ er this question by examining multiple clas-si ers (SVM, ridge regression (RR) and Rocchio) with fea-ture selection in recursiv e and non-recursiv e settings, on two DNA microarra y datasets and a text categorization benc h-mark dataset (Reuters-21578). The two DNA microarra y datasets we studied are AM-LALL dataset and GCM dataset. AMLALL dataset consists of a matrix whose rows are genes and columns are cancer-ous patien ts with either AML or ALL type of cancer. The elemen ts of the matrix are the gene expression levels. Di er-ent feature selection metho ds have been evaluated on this dataset and the best result so far (zero error rate with 8 genes) was obtained by [1] using SVM-RFE. GCM dataset has similar form but 14 categories. Since we only focus on binary classi cation here, we pick its largest category (Leuk emia) as POSITIVE category and merge all the other categories as NEGA TIVE category .
 error rate error rate
Exp erimen tal results on AMLALL data are presen ted in gure 1. Recursiv e RR sho ws the best performance and nds 3 genes (M27891, X51521 and Y00787) from 7,000 with zero error rate, outp erforming the best result ever rep orted. Rocchio has the worst performance and SVM is in between. Results on GCM data sho w similar patterns (and they are not plotted due to space limitation).

The third dataset, Reuters-21578 corpus (ApteMo d ver-sion), is a benc hmark in text classi cation evaluations. There are 90 categories in this corpus and eac h documen t belongs to 1.23 categories in average. The num ber of features and the classi cation thresholds are tuned on a per-category ba-sis using 2-fold cross validation on the training data. The results of the two classi ers without any feature selection are also pro vided as the baselines. Figure 2 sho ws the re-sults of SVM and RR (Ro cchio's result is not sho wn since it is much worse); We did a t-test to compare the macro-averaged F1 scores of the baseline RR and the recursiv e RR, and found that the latter was signi can tly better than the former with a p-v alue of 0.0372. On the other hand, we did not nd the recursion with SVM impro ving the average performance over the case of using the baseline SVM. An in-teresting observ ation is that recursiv e RR mainly impro ves the performance on rare categories.

To visualize the di erences among the three classi ers in selecting features, we sho w the correlation matrix for the top 32 features selected by eac h classi er on the AMLALL dataset (in gure 3). In eac h matrix, features are sorted according to the order of being eliminated in the recursiv e pro cess. The color intensit y of the ( i; j ) elemen t in those Figure 2: Results of SVM and RR on Reuters21578. graphs re ects the magnitude of correlation coecien t of gene i and gene j: brigh ter color means stronger correlations. Rocchio has more brigh t pixels in the upp er-left corner of its matrix, while RR has more evenly distributed brigh t pixels. This means that Rocchio tends to reserv e correlated fea-tures and RR tends to reserv e non-correlated features. This observ ation gives an intuitiv e explanation for the sup erior performance of RR. That is, the few er redundan t features a small subset includes, the more information it would o er for accurate prediction. From these graphs we can also see that SVM is worse than RR but better than Rocchio, with re-spect to their preference in choosing non-redundan t features over redundan t ones. We can also observ e that in recursiv e version, the brigh t pixels tend to evenly distribute , mean-ing that the recursiv e pro cess will increase the tendency of the classi ers to choose non-redundan t features over redun-dan t ones. In fact, this is the reason why recursiv e feature selection is often preferred than the non-recursiv e version.
Figure 3: The correlation matrix on AMLALL
Rocchio-style classi ers are commonly used for their sim-plicit y. The weigh t of the feature q is q = 1 n n P y i = 1 y i x iq . Obviously , the redundan t features are not penalized and the feature ranks will not change during the recursiv e pro cess.

Ridge regression has a loss function L RR = P n i =1 (1 y h ~ ; ~x i i ) 2 + k ~ k 2 . To minimize L RR , its partial deriv a-tive should be zero, whic h yields: q = 1 ( P n i =1 x iq P ects relev ancy and the second term re ects redundancy of feature q . In fact, the term P n i =1 x iq x ip is the dot-pro duct of two "feature vectors", re ecting the similarit y between feature p and q in the n training examples. Clearly , without the second term, RR is very similar to Rocchio; with the sec-ond term, however, the redundan t features will be penalized and feature ranks will change in the recursiv e pro cess.
SVM has a loss function L svm = P n i =1 (1 y i h ~ ; ~x k ~ k 2 that is not di eren tiable, whic h mak es it hard to di-rectly apply above analysis. Thus in our analysis, we used a mo di ed logistic regression (LR) to sim ulate SVM. The loss function of the mo di ed LR is L log = P n i =1 1 t ln (1 + large enough, the mo di ed LR ranks features in the same way SVM does. In order to get an analytical form, we fur-ther use tangen t lines to sim ulate the sigmoid function. That is, we use sig ( z ) = a z z + b z to replace sig ( z ) = solution of mo di ed LR: q = 1 2 ( P n i =1 ( a i t + 1 b P t ( y i ~ ~x i 1). This form ula is similar to RR's, except that it assigns weigh t a i t + 1 b i to the relev ancy term and weigh t a t to the redundancy penalization term for eac h sample i. Supp ose t is large enough, then for samples with y i ~ ~x viously less than one, a i t + 1 b i tends to be one while a tends to be zero. For these samples, redundan t features are almost not penalized, just like Rocchio. For samples whose y ~x i become close to one, a i t will become larger, thus the redundancy penalization term will play a role, in a way sim-ilar to ridge regression. For samples with y i ~x i obviously larger than one, a i t + 1 b i and a i t will both become close to zero and these samples tend to be ignored. Thus mo di ed LR penalizes redundan t features in a way between Rocchio and RR. This argumen t is also suitable for SVM.
We studied the prop erty of classi ers leading to the suc-cess of RFE. We found the abilit y of a classi er for penalizing redundan t features and promoting indep enden t features in the recursiv e pro cess has a strong in uence on its success. RR sho ws best performance in our exp erimen ts, follo wed by SVM and then by Rocchio. These results are consisten t to our loss function analysis of these classi ers.
This researc h work is supp orted by National Science Foun-dation Information Technology Researc h gran t num ber 0225656.
