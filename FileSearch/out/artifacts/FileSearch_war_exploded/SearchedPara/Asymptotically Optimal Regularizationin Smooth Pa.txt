 Many problems in machine learning and statistics involve the estimation of parameters from finite data. Although empirical risk minimization has favorable limiting properties, it is well known that this procedure can overfit on finite data. Hence, various forms of regularization have been employed to control this overfitting. Regularizers are usually chosen based on assumptions about the problem domain at hand. For example, in classification, we might use L 2 regularization if we expect the data to be separable with a large margin. We might regularize with a generative model if we think it is roughly well-specified [7, 20, 15, 17]. In multi-task learning, we might penalize deviation between parameters across tasks if we believe the tasks to be similar [3, 12, 2, 13].
 In each case, we would like (1) a procedure for choosing the parameters of the regularizer (for exam-ple, its strength) and (2) an analysis that shows the amount by which regularization reduces expected risk, expressed as a function of the compatibility between the regularizer and the problem domain. In this paper, we address these two points by developing an asymptotic analysis of smooth regular-izers for parametric problems. The key idea is to derive a second-order Taylor approximation of the expected risk, yielding a simple and interpretable quadratic form which can be directly minimized with respect to the regularization parameters. We first develop the general theory (Section 2) and then apply it to some examples of common regularizers used in practice (Section 3). to denote constant limits of random variables. For a  X  -parametrized differentiable function  X  7 X  f (  X  ;  X  ) , let  X  f ,  X  f , and random variables for which n  X  X n is bounded in probability. Let X n P  X  X  X  X denote convergence in and V [  X  ] , respectively. 2.1 Setup linear regression). Our goal is to minimize the expected risk , which averages the loss over some true data generating distribution p  X  ( Z ) . We do not have access unregularized estimator minimizes the empirical risk : tions, it is well known that regularization can improve performance substantially for finite n . Let R n (  X , X  ) be a (possibly data-dependent) regularization function, where  X   X  R b are the regulariza-where  X   X  R determines the strength. Define the regularized estimator as follows: The goal of this paper is to choose good values of  X  and analyze the subsequent impact on perfor-mance. Specifically, we wish to minimize the relative risk : which is the difference in risk (averaged over the training data) between the regularized and unreg-on deriving an asymptotic expansion for L n (  X  ) . In this paper, we make the following assumptions: 1 Assumption 1 (Compact support) . The true distribution p  X  ( Z ) has compact support. Assumption 2 (Smooth loss) . The loss function ` ( z, X  ) is thrice-differentiable with respect to  X  . Assumption 3 (Smooth regularizer) . The regularizer R n (  X , X  ) is thrice-differentiable with respect to  X  and differentiable with respect to  X  . Assume R n (0 , X  )  X  0 and R n (  X , X  ) P  X  X  X  0 as n  X  X  X  . 2.2 Rate of regularization strength possible risk in our hypothesis class. To achieve this, it suffices (and in general also necessitates) that (1) the loss class satisfies standard uniform convergence properties [22] and (2) the regularizer verified given our assumptions.
 The next question is at what rate R n (  X , X  ) should converge to 0? As we show in [16], R n (  X , X  ) = O case  X   X   X  n is the maximum a posteriori (MAP) estimate. 2.3 Asymptotic expansion Our main result is the following theorem, which provides a simple interpretable asymptotic expres-sion for the relative risk, characterizing the impact of regularization (see [16] for proof): Theorem 1. Assume R n (  X , X   X  ) = O p ( n  X  1 ) . The relative risk admits the following asymptotic expansion: in terms of the asymptotic relative risk: The most important equation of this paper is (6), which captures the lowest-order terms of the relative risk defined in (4).
 Interpretation The significance of Theorem 1 is in identifying the three problem-dependent con-tributions to the asymptotic relative risk: Mahalanobis metric given by  X  L . Note that the squared regularizer bias is always positive: it always increases the risk by an amount which depends on how  X  X rong X  the regularizer is.
 I `` . For convex regularizers, regularizing. Furthermore, if the loss is the negative log-likelihood and our model is well-specified reduction term simplifies to tr {  X  R (  X  )  X  L  X  1 } .
 The alignment has two parts, the first of which is nonzero only for non-linear models and the second of which is nonzero only when the regularizer depends on the training data. The unregularized the random regularizer compensates for the loss (tr {I `r (  X  )  X  L  X  1 } &lt; 0 ). 2.4 Oracle regularizer The principal advantage of having a simple expression for L (  X  ) is that we can minimize it with  X   X  in the important special case that the regularization parameter  X  is the strength of the regularizer: Corollary 1 (Oracle regularization strength) . If R n (  X , X  ) =  X  n r (  X  ) for some r (  X  ) , then Proof. (6) is a quadratic in  X  ; solve by differentiation. Compute L (  X   X  ) by substitution. In general,  X   X  will depend on  X   X  and hence is not computable from data; Section 2.5 will remedy this. Nevertheless, the oracle regularizer provides an upper bound on performance and some insight into the relevant quantities that make a regularizer useful.
 Note L (  X   X  )  X  0 , since optimizing  X   X  must be no worse than not regularizing since L (0) = 0 . But what might be surprising at first is that the oracle regularization parameter  X   X  can be negative helps (  X   X  &gt; 0 and L (  X  ) &lt; 0 for 0 &lt;  X  &lt; 2  X   X  ). 2.5 Plugin regularizer While the oracle regularizer R n (  X   X  , X  ) given by (7) is asymptotically optimal,  X   X  depends on the We cannot simply write L n (  X   X  n ) and apply Theorem 1 because L (  X  ) can only be applied to non-random arguments. However, we can still leverage existing machinery by defining a new plugin superscript  X  will denote quantities concerning the plugin regularizer. The corresponding estimator  X   X  We could try to squeeze more out of the plugin regularizer by further optimizing  X   X  according to  X  Table 1 summarizes all the estimators we have discussed.
 The following theorem relates the risks of all estimators we have considered (see [16] for the proof): of the oracle plugin estimator is L  X  (  X   X  X  X  ) = L  X  (1) + E 2 4 Note that the sign of E depends on the nature of the error  X  n , so P LUGIN could be either better or worse than O RACLE . On the other hand, O RACLE P LUGIN is always better than P LUGIN . We can get a simpler expression for E if we know more about  X  n (see [16] for the proof): results of Theorem 2 hold with E =  X  tr {I ``  X  L  X  1  X   X  R (  X   X  )  X  f  X  L  X  1 } . calculations to obtain the asymptotic relative risks and regularization parameters for a given problem. We first explore two classical examples from statistics (Sections 3.1 and 3.2) to get some intuition for the theory. Then we consider two important examples in machine learning (Sections 3.3 and 3.4). 3.1 Estimation of normal means Assume that data are generated from a multivariate normal distribution with d independent compo-so the model is well-specified. In his seminal 1961 paper [14], Stein showed that, surprisingly, the standard empirical risk minimizer  X   X  is essentially equivalent to O RACLE P LUGIN with quadratic regularization ( r (  X  ) = 1 2 k  X  k 2 ). weight is  X   X  = d k  X  cally) worse than O RACLE but better than U NREGULARIZED if d &gt; 4 .
 To get O RACLE P LUGIN , compute  X   X  X  X  = 1  X  2 d (note that this doesn X  X  depend on  X   X  ), which results small improvement over P LUGIN (and is superior to U NREGULARIZED when d &gt; 2 ).
 Note that the O RACLE P LUGIN and P LUGIN are adaptive: We regularize more or less depend-J benefit that it always shrinks towards zero by an amount between 0 and 1, whereas J AMES S TEIN can overshoot. Empirically, we found that O RACLE P LUGIN generally had a lower expected risk than J
AMES S TEIN when k  X   X  k is large, but J AMES S TEIN was better when k  X   X  k X  1 . 3.2 Binomial estimation Consider the estimation of  X  , the log-odds of a coin coming up heads. We use the negative log-example serves to provide intuition for the bias B appearing in (6), which is typically ignored in first-order asymptotics or is zero (for linear models).
 Choosing  X  has been studied extensively in statistics. Some common choices are the Haldane prior (  X  = 0 ), the reference (Jeffreys) prior (  X  = 1 ), the uniform prior (  X  = 2 ), and Laplace smoothing (  X  = 4 ). We will choose  X  to minimize expected risk adaptively based on data.
 regularization always helps.
 We can compute the difference between O RACLE and P LUGIN : E = 2  X  2 v b 2 . If | b | &gt; which means that P LUGIN is worse; otherwise P LUGIN is actually better. Even when P LUGIN is worse than O RACLE , P LUGIN is still better than U NREGULARIZED , which can be verified by checking that L  X  (1) =  X  5 2 vb  X  2  X  2 v  X  1 b 2 &lt; 0 for all  X   X  . 3.3 Hybrid generative-discriminative learning In prediction tasks, we wish to learn a mapping from some input x  X  X to an output y  X  Y . A common approach is to use probabilistic models defined by exponential families, which is defined  X   X  R d . These features can be used to define a generative model (8) or a discriminative model (9): Table 2: The oracle regularizer for the hybrid generative-discriminative estimator. As misspeci-fication increases, we regularize less, but the relative risk is reduced more (due to more variance reduction).
 G n (  X  ) =  X  1 n P D n (  X  ) =  X  1 n P There has been a flurry of work on combining generative and discriminative learning [7, 20, 15, As n  X  X  X  , the discriminative objective dominates as desired. Our approach generalizes the analysis of [6], which applies only to unbiased estimators for conditionally well-specified models. ties (write  X  for  X  ( X,Y ) ):  X  L = v x def = E p  X  ( X ) [ V p  X  ( parameter is then The sign and magnitude of  X   X  provides insight into how generative regularization improves pre-larization is helpful. To simplify, assume that the discriminative model is well-specified, that is, p I Since v v x (the key fact used in [17]), the variance reduction (plus the random alignment term from I `r ) is always non-negative with magnitude equal to the fraction of missing information pro-sign depends on the problem. Finally, the denominator (always positive) affects the optimal magni-and the regularizer should be trusted more (large  X   X  ). Since our analysis is local, misspecification (how much p  X   X  ( x,y ) deviates from p  X  ( x,y ) ) is measured by a Mahalanobis distance between  X  and  X  xy , rather than something more stringent and global like KL-divergence.
 An empirical example To provide some concrete intuition, we investigated the oracle regularizer specification is controlled by 0  X   X   X  1 , the fraction of examples whose features are perfectly correlated.
 Table 2 shows how the oracle regularizer changes with  X  . As  X  increases,  X   X  decreases (we regularize less) as expected. But perhaps surprisingly, the relative risk is reduced with more misspecification; Figure 1(a) shows the relative risk L n (  X  ) for various values of  X  . The vertical line corresponds to  X   X  , which was computed numerically by sampling. Note that the minimum of the curves empirically justifies our asymptotic approximations.
 Unlabeled data One of the main advantages of having a generative model is that we can lever-age unlabeled examples by marginalizing out their hidden outputs. Specifically, suppose we have m i.i.d. unlabeled examples X n +1 ,...,X n + m  X  p  X  ( x ) , with m  X   X  as n  X   X  . Define the unlabeled regularizer as R n (  X , X  ) =  X   X  nm P m i =1 log p  X  ( X n + i ) .
 We can compute  X  R =  X   X   X  xy using the stationary conditions of the loss function at  X   X  . Also,  X  R = v  X  v x , and I `r = 0 (the regularizer doesn X  X  depend on the labeled data). If the model is conditionally well-specified, we can verify that the oracle regularization parameter  X   X  is the same as if we had regularized with G n . This equivalence suggests that the dominant concern asymptotically is developing an adequate generative model with small bias and not exactly how it is used in learning. 3.4 Multi-task regression The intuition behind multi-task learning is to share statistical strength between tasks [3, 12, 2, 13]. Suppose we have K regression tasks. For each task k = 1 ,...,K , we generate each data point R  X  &gt; ( X   X  I d )  X  . For simplicity, assume E X k  X  Most of the computations that follow parallel those of Section 3.1, only extended to matrices. Sub-its associated relative risk L ( X   X  ) =  X  1 2 d 2 tr { ( X  &gt;  X   X   X  )  X  1 } .
 We can do slightly better using O RACLE P LUGIN (  X   X  X  X  = 1  X  2 d ), which results in a relative risk of L completely independently with K independent regularization parameters, our relative risk would have been  X  1 2 ( d  X  2) 2 ( P K k =1 k  X  k  X  k  X  2 ) (following similar but simpler computations). We now compare joint versus independent regularization. Let A =  X  &gt;  X   X   X  with eigendecompo-sition A = UDU &gt; . The difference in relative risks between joint and independent regularization MHC-I binding prediction We evaluated our multitask regularization method on the IEDB MHC-I peptide binding dataset created by [19] and used by [13]. The goal here is to predict the binding affinity (represented by log IC 50 ) of a MHC-I molecule given its amino-acid sequence (rep-resented by a vector of binary features, reduced to a 20-dimensional real vector using SVD). We created five regression tasks corresponding to the five most common MHC-I molecules. We compared four estimators: U NREGULARIZED , D IAG CV (  X  = cI ), U NIFORM CV (using I binding prediction task, test risk for the four multi-task estimators; P LUGIN CV (estimating all pairwise task affinities using P LUGIN and cross-validating the strength) works best. 30 independent train/test splits. Multi-task regularization actually performs worse than independent learning (D IAG CV) if we assume all tasks are equally related (U NIFORM CV). By learning the full of  X  via cross-validation is not computationally feasible, though other approaches are possible [13]. The subject of choosing regularization parameters has received much attention. Much of the learning bounds. Our analysis provides a different type of approximation X  X ne that is exact in the first few terms of the expansion. Though we cannot make a precise statement about the risk for any given n , whereas our analysis is based on the variance of the estimator. Vanilla uniform convergence bounds yield worst-case analyses, whereas our asymptotic analysis is tailored to a particular problem ( p  X  and  X   X  ) and algorithm (estimator). Localization techniques [5], regret analyses [9], and stability-based bounds [8] all allow for some degree of problem-and algorithm-dependence. As bounds, however, they necessarily have some looseness, whereas our analysis provides exact constants, at least the ones associated with the lowest-order terms.
 expansion of the risk is reminiscent of AIC [1]. However, our aim is different: AIC is intended for model selection, whereas we are interested in optimizing regularization parameters. The Stein unbiased risk estimate (SURE) is another method of estimating the expected risk for linear models [21], with generalizations to non-linear models [11].
 In practice, cross-validation procedures [10] are quite effective. However, they are only feasible when the number of hyperparameters is very small, whereas our approach can optimize many hy-perparameters. Section 3.4 showed that combining the two approaches can be effective. To conclude, we have developed a general asymptotic framework for analyzing regularization, along with an efficient procedure for choosing regularization parameters. Although we are so far restricted to parametric problems with smooth losses and regularizers, we think that these tools provide a complementary perspective on analyzing learning algorithms to that of risk bounds, deepening our understanding of regularization. [1] H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic [2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural [3] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal [4] M. S. Bartlett. Approximate confidence intervals. II. More than one unknown parameter. [5] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of [6] G. Bouchard. Bias-variance tradeoff in hybrid generative-discriminative models. In Sixth [7] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classifiers. In [8] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning [9] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games . Cambridge University Press, [10] P. Craven and G. Wahba. Smoothing noisy data with spline functions. estimating the correct [11] Y. C. Eldar. Generalized SURE for exponential families: Applications to regularization. IEEE [12] T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Jour-[13] L. Jacob, F. Bach, and J. Vert. Clustered multi-task learning: A convex formulation. In Ad-[14] W. James and C. Stein. Estimation with quadratic loss. In Fourth Berkeley Symposium in [15] J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and discrimi-[16] P. Liang, F. Bach, G. Bouchard, and M. I. Jordan. Asymptotically optimal regularization in [17] P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudo-[18] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Genera-[19] B. Peters, H. Bui, S. Frankild, M. Nielson, C. Lundegaard, E. Kostem, D. Basch, K. Lam-[21] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics , [22] A. W. van der Vaart. Asymptotic Statistics . Cambridge University Press, 1998.
