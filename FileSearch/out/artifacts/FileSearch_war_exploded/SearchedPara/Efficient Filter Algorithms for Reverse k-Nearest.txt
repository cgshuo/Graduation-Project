 Reverse k Nearest Neighbor (R k NN) processing has got considerable attentions over the recent years. R k NN plays an important role in decision support  X  resource allocation, data mining, profile-based marketing and other important areas. One object know how many objects it is supposed to serve. The objects which request services could be soldiers in a battle field, tourists in dangerous environments, etc. The services providing objects could be aid workers, etc. Example 1: In a multi-player computer game, players often shoot their nearest neighbors. In order to dodge the fires from others, players need R k NN queries. Example 2: Many road accidents occur in the city. Aid workers or related law-enforcement staff should use R k NN queries to handle the accidents in the most efficient way. Given a multidimensional object set P and a query point qP  X  , a R k NN query assume Euclidean distance). () pkNNq  X  does not necessarily imply () pRkNNq  X  The red circle is the vicinity circle of q (centered at q with radius equal to the distance lies in the vicinity circles of p 4 and p 5 , 45 () { , } R NN q p p = . 
Early R k NN works fail to solve at least one of the following problems: they (i) do (iii) are applicable only to 2D dataset but not to higher dimensionality and (iv) retrieve only approximate results. Tao et al. [1] proposed the TPL method which successfully solves these problems. However, when the value of k is large, the amount of calculation becomes very heavy. The TPL method is not suitable for mobile deceives such as mobile phone. It is necessary to provide a new method to reduce the calculation. Motivated by the above reasons, we propose a new solution to R k NN queries. Our experiments will show that the proposed method is superior to TPL.Then we extend our methodology to continuous reverse k nearest neighbor (C-R k NN) [2] queries. 
The rest of the paper is organized as follows. Section 2 surveys related work on k NN and R k NN search. Section 3 presents filter step for R k NN with static query point. Section 4 discusses filter step for continuous R k NN processing. Section 5 contains an experimental evaluation that demonstrates the superiority of the proposed methods over the previous algorithms. Section 6 concludes the paper. 2000, Korn et al. [3] introduced RNN queries and gave a processing method which relied on pre-processing. Yang et al. [4 ] and Maheshwari et al. [5] respectively improved the method in [3]. But the pre-computing is necessary in the above methods [3-5]. And their updates can not be performed efficiently, because each insertion or the six-regions pruning method. This method does not need any pre-computation. This method is only applicable to 2D-space. It was extended to arbitrary values of k by Tao et al [1]. Singh et al.[7] used a intuitive idea that the NNs of objects are likely to be RNNs. The major shortcoming of the method is that it may incur false misses. called TPL. Wu et al. [8] proposed an algorithm FINCH for R k NN queries in 2D-space. Instead of using the bisectors to prune the objects, they use a convex polygon obtained from the intersection of the bisectors. 
Benetis et al. [2] presented the first continuous RNN algorithm. Their method is continuously maintained, and each candidate X  X  k NNs are also continuously maintained to do continuous refinement. TPL-pruning and Six-regions pruning can also be used in C-R k NN queries [9,10]. The queries based on R-tree indexing generally adopt the two-step (filter-refinement) processing. The filter step eliminates the false objects to obtain a set of candidate results. And our work focuses on the filter step. Like other R k NN works, the pruning heuristics are used to eliminate the false objects in the filter step. We use two pruning heuristics here. We present a method called BRC in the filter step. This method needs pre-computing the cover-value [11] for each node when the R-tree is built. The cover-node. The R-tree is built by inserting the objects one by one. When an object is should be increased by one. For the R-tree based R k NN queries, traverse the nodes of the R-tree is necessary. To increase the speed of a query, reducing the number of the visited nodes and lessening the I/O cost are very important. If it can be vindicated by dealing with the current node that the sub-tree rooted by the current node does not contain the real results, the current node can be pruned and its child nodes should not be visited. The following content mainly discusses how the nodes of a R-tree are pruned. In other not be included in the final results. 
The essence of R k NN queries: a multidimensional object set P and points qPpP  X  X  X   X   X  if it exists a set '' PPPk  X  X  X   X  X  X  , for each object 1 pP  X   X  q isn X  X  one of kNN ( p ). That is, p isn X  X  one of RkNN ( q ) .
 Pruning Heuristic 1 The motivation of the pruning heuristic 1: If a region R on data space contains n ( n&gt;k ) objects and objects in this region R mutually close (that is, for each object p contained RkNN ( q ) contained in R.
 Lemma 1: For a node N ,if () 1 cover N k  X + and (,) () mindist N q diag N &gt; , the node N can be pruned. Here, (,) mindist N q is the minimal distance between the MBR of the node and q , () diag N is the diagonal of the MBR of the node (see Fig. 2). RkNN ( q ).  X 
In the following cases, a node N will not be pruned by the pruning heuristic 1. (a). () cover N k  X  (b). () cover N k &gt; and q lies on the light region (see Fig.3) where the length of the dotted line equals to () diag N . The experiment will show most of objects can be pruned by pruning heuristic 1 and the amount of calculation is rather minimal. Pruning Heuristic 2 The motivation of the pruning heuristic 2: If a region R on data space contains n ( n&gt;k ) pruned. Proof. The farthest distance between object p existed in shaded area and the node N So the nodes or objects existed in the shaded area can X  X  be contained in RkNN(q) .  X  Taking 2D-space for instance, to increase the effectivity of the pruning heuristic 2, the whole object space is divided as Fig. 5, where q is the center. 
In 2D-space, whole object space is divided in 4 sections. There is a rectangle RN for each section and () cover RN k = .Then the pruning heuristic 2 is used in every sections, each section forms a shaded area. And objects cont ained in the shaded area are not RkNN(q ). For increasing the pruning affectivity and forming greater shaded area, k nearest objects are selected in each section, and using these k objects form the rectangle RN. Because the BRC algorithm uses a priority queue based on the min-distances, the front k objects belonging to each section are the ones. 
In Fig.5, when the nodes of the R-tree (the green rectangle) entirely belong to one section, the pruning heuristic 2 can be used. Through the observation, it is clear that when the relative position of a node N and the central point of RN identifies with the relative position of RN and q , the nodes or objects can be pruned by the pruning space (the relative positions of 4 rectangles and the red point in Fig.5). The partition about multidimensional object space is similar to 2D-space. Assuming the dimension is n then the multidimensional object space is divided into 2 n sections. Similarly, there are 2 n relative positions. BRC Algorithm The BRC algorithm uses a priority queue ba sed on the min-distances to preserve the nodes of the R-tree and objects which haven X  X  been visited for now but will be visited. At first, BRC inserts the root of the R-tree into a heap H sorted in ascending order of its entries X  minimal distance from q . Then, BRC de-heaps an entry e and visits e . If it can not be pruned, its child nodes are inserted into H. The filter step terminates when H=  X  . Next, it is specifically described how an entry can be pruned. 
The case of () cover e k &gt; : First BRC verdicts whether e can be pruned by pruning heuristic1. If e can X  X  be pruned by the pruning heuristic1, and e entirely belongs to one section, then the pruning heuristic 2 is used to prune. If e can X  X  be pruned by the above pruning heuristics, then the child nodes are inserted to H. 
The case of () cover e k  X  : (1) An entry e entirely belongs to a section and e is an object. If this section have not formed the pruning rectangle, then the object is inserted to the set of rectangle which is used to form pruning rectangle. If this section have formed the pruning rectangle and the pruning heuristic 2 is successful, then the object is inserted to the set S trim , else consider whether the maximal distance between this object and the pruning rectangle is lesser than the distance between this object and q . If the answer is yes, then the object is inserted to the set S trim . Otherwise then the object is inserted to the set S scnd . (2) An entry e entirely belongs to a section and e is a node of the R-tree. If this section have formed the pruning rectangle and the inserted to the set S scnd . (4) Otherwise, the child nodes are inserted to H. 
At the last, we will briefly discuss the refinement step of R k NN processing. The output of the filter step is used as the in put of the refinement step. There are usually two processing methods for the refinement step of R k NN: one is using k NN queries. k NN queries are performed for each object. If the distance between a candidate object and the k -nearest neighbor is not lesser than the distance between the candidate object traverse R-tree again. So we employ the refinement method of the TPL. After the objects are vindicated by using the nodes of the R-tree and objects pruned. So it prevents revisiting the same nodes from happening. The continuous query point is represented by a segment which begins at q A ends at q B . Given a segment q A q B , a C-R k NN query aims at reporting the R k NNs for every point on the segment. The goal is to find a set of split points that partition q A q B into disjoint sub-segments, such that all points in the same sub-segment have identical R k NNs. 
As with R k NN queries, C-R k NN queries also have a filter and a refinement step for retrieving and verifying candidates, respectively. However, unlike conventional R k NN search, C-R k NN includes a third step(the splitting step) for obtaining the split points. 
Since C-BRC is similar to BRC, our discussion focuses on clarifying the differences between the two algorithms. C-R k NN Pruning Heuristic 1 Lemma 3: For a node N , if () 1 cover N k  X + and (, ) () node N can be pruned. Here, (, ) Fig. 6). Proof. Given objects p and p  X  contained in N and a point C-R k NN Pruning Heuristic 2 Lemma 4: For a node N ,if () cover N k  X  and it is showed the relative position of q q B and N in Fig.7 ( q A q B lies on the bottom left corner of green lines ), the node can be pruned. Proof. The farthest distance between object p existed in shaded area and the node N is (, ) dist A p ( the red line in Fig.7) and the point intersect in a point E . Obviously, (,) (,) (,) For increasing the effectivity of the C-R k NN pruning heuristic 2, the whole object space is divided like R k NN with the static query point. The point q is replaced by the the 4 green rectangles are formed in the same way and objects contained in the shaded area are not RkNN ( q A q B ). So when the nodes of the R-tree entirely belong to a shaded q q B , the nodes or objects can be pruned by the pruning heuristic 2. The relative space. The partition about multidimensional object space is similar to 2D-space. C-BRC Algorithm The C-BRC algorithm for filter step is similar to BRC. In this paper, we employ the refinement and the splitting step of TPL. The criterion of the refinement step is that a object p is a final result if and only if no other objects exists in the circle centered at p between the circle of every candidate and q A q B . R k NN with Static Query Point In this section we study the performance of our proposed algorithm, namely the BRC algorithm for R k NN queries with static query point. We compare its performance with the TPL algorithm, which is the state-of-the-art R k NN algorithm. Experiments are run on a Windows XP desktop machine with a Core 2 Duo 2.8GHZ CPU and 1G memory. We deploy four real datasets (LB,hypsogr,wave and color) whose statistics are summarized in Table 1. We also employ SpatialDataGenerator to create synthetic data following the uniform and Zipf distri butions. The random coordinates of each point in a uniform dataset are specified in [0,10000], the coordinates of each point in a Zipf dataset follow Zipf distribution (with a skew coefficient 0.8). Each point X  X  coordinates in the above datasets on each dimension are mutually independent. Each dataset is indexed by an R*-tree, and the size of a node is fixed to 1k bytes. 
The experiments focus on investigating the influence of these factors: data distribution, dataset cardinality, dimensionality, value of k . For one and the same query, we will execute 400 queries (the query points for the 400 queries belong to the object space), all of the reported value in the following diagrams each reported value is the average of all the 400 queries. 
Fig.9 shows the query time (in seconds) of the BRC and TPL when k takes different step and the refinement step). The numbers in Fig.9 indicate the percentages of objects pruned by the pruning heuristic 1 in the total dataset cardinality. It is showed that BRC costs lesser than TPL for all datasets when k takes larger value. And the pruning heuristic 1 of BRC is independent of k , so the time taken by the filter step of BRC varies within a small range. The percentages in Fig. 9 confirm that most of objects can be pruned by the pruning heuristic 1. Besides the amount of calculation of the pruning heuristic 1 is very small. These two causes lead to the lesser query time taken by BRC. 
Next, the experiments inspect the impact of the dimensionality. Here, the value of k shows that he performance of BRC and TPL degrades with the dimensionality growing, because the R*-tree become less efficient with the dimensionality growing. Moreover, the percentages in Fig.10 show that the pruning heuristic 1 of BRC is more efficient on Zipf datasets than Uniform datasets. 
To study the effect of the dataset cardinality, we use the same data distribution, same dimensionality and different cardinalities datasets. In these experiments, k takes 8, the cardinalities of 2D datasets ( Uniform and Zipf ) range from 10000 to 40000. Fig.11 shows the query time increases with the cardinalities growing, because the height of the R*-tree increases with the cardinalities growing. R k NN with Moving Query Point Above having demonstrated the efficiency of BRC for R k NN queries with static query point, we proceed to evaluate C-BRC for C-R k NN. We compare C-BRC with C-TPL. In addition to the above factors, the query performance is also affected by the length l of the query segment q A q B . 
Fig.12 shows the query time of the C-BRC and C-TPL when k takes different values with the real datasets (here l =100). Its comparison with C-TPL is similar to that between BRC and TPL in Fig. 9. 
Next, similar studies about the effects of dimensionality and cardinality on the performance of C-BRC are illustrated in Fig.13 and Fig.14 (here, k =8 and l =100) . 
Finally, Fig. 15 examines the performance of C-BRC and C-TPL by varying l from 10 to 200. And we can see that C-BRC still outperforms C-TPL significantly. This paper proposes a new filter processing for R k NN with static query point and continuous R k NN. There are two pruning heuristics proposed for the filter step. The amount of calculation of the two pruning heuristics is lesser. The processing speed of BRC is also acceptable when the value of k is great. And we extend the BRC method to answer the continuous R k NN queries. Future work will be focused on the continuous R k NN for moving objects trajectories.
 Acknowledgment. This work is supported by Jilin University Research Project under Grant No. 200903178, Seed Foundation of Jilin University and Open Project of Key laboratory of symbolic computing and knowledge engineering of ministry of education in China. 
