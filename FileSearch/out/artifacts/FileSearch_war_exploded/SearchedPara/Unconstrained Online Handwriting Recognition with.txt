 tion is performed on images of handwritten text. In online handwriting the location of the pen-tip on a surface is recorded at regular intervals, and the task is to map from the sequence of pen positions to the sequence of words.
 each letter or word is distributed over many pen positions poses a problem for conventional sequence labelling algorithms, which have difficulty processing data with long-range interdependencies. The problem is especially acute for unconstrained handwriting, where the writing style may be cursive, printed or a mix of the two, and the degree of interdependency is therefore difficult to determine in advance. The standard solution is to preprocess the data into a set of localised features. These pseudo-offline information from a generated image, and character level shape characteristics [6, 7]. they split up the characters and therefore interfere with localisation. HMMs [6] and hybrid systems incorporating time-delay neural networks and HMMs [7] are commonly trained with such features. The issue of classifying preprocessed versus raw data has broad relevance to machine learning, and merits further discussion. Using hand crafted features often yields superior results, and in some favour of raw data. Firstly, designing an effective preprocessor requires considerable time and ex-pertise. Secondly, hand coded features tend to be more task specific. For example, features designed for English handwriting could not be applied to languages with substantially different alphabets, such as Arabic or Chinese. In contrast, a system trained directly on pen movements could be ap-and the whole system to be trained together. For example, convolutional neural networks [10], in which a globally trained hierarchy of network layers is used to extract progressively higher level handwriting, because they require the text images to be presegmented into characters [10]). In this paper, we apply a recurrent neural network (RNN) to online handwriting recognition. The RNN architecture is bidirectional Long Short-Term Memory [3], chosen for its ability to process data with long time dependencies. The RNN uses the recently introduced connectionist temporal classi-fication output layer [2], which was specifically designed for labelling unsegmented sequence data. An algorithm is introduced for applying grammatical constraints to the network outputs, thereby providing word level transcriptions. Experiments are carried out on the IAM online database [12] which contains forms of unconstrained English text acquired from a whiteboard. The performance of the RNN system using both raw and preprocessed input data is compared to that of an HMM based system using preprocessed data only [13]. To the best of our knowledge, this is the first time whole sentences of unconstrained handwriting have been directly transcribed from raw online data. Section 2 describes the network architecture, the output layer and the algorithm for applying gram-matical constraints. Section 3 provides experimental results, and conclusions are given in Section 4. 2.1 Bidirectional Long Short-Term Memory One of the key benefits of RNNs is their ability to make use of previous context. However, for standard RNN architectures, the range of context that can in practice be accessed is limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the recurrent connections. This is often referred to as the vanishing gradient problem [4].
 Long Short-Term Memory (LSTM; [5]) is an RNN architecture designed to address the vanishing gradient problem. An LSTM layer consists of multiple recurrently connected subnets, known as memory blocks. Each block contains a set of internal units, known as cells, whose activation is and access information over long periods of time.
 For many tasks it is useful to have access to future as well past context. Bidirectional RNNs [14] achieve this by presenting the input data forwards and backwards to two separate hidden layers, both of which are connected to the same output layer. Bidirectional LSTM (BLSTM) [3] combines the above architectures to provide access to long-range, bidirectional context. 2.2 Connectionist Temporal Classification Connectionist temporal classification (CTC) [2] is an objective function designed for sequence la-belling with RNNs. Unlike previous objective functions it does not require pre-segmented training to map directly from input sequences to the conditional probabilities of the possible labellings. A CTC output layer contains one more unit than there are elements in the alphabet L of labels for the task. The output activations are normalised with the softmax activation function [1]. At each labels. The extra output estimates the probability of observing a  X  X lank X , or no label. The combined output sequence estimates the joint probability of all possible alignments of the input sequence with all possible labellings. The probability of a particular labelling can then be estimated by summing over the probabilities of all the alignments that correspond to it.
 step according to the probabilities implied by the network outputs defines a probability distribution over the set of length T sequences of labels and blanks. We denote this set L 0 T , where L 0 = L  X  probability of a path  X   X  L 0 T is given by or equal to T on the alphabet L as L  X  T . Then Paths are mapped onto labellings l  X  L  X  T by an conditional probability of a given labelling l  X  L  X  T is the sum of the probabilities of all paths corresponding to it: Although a naive calculation of the above sum would be unfeasible, it can be efficiently evaluated with a graph-based algorithm [2], similar to the forward-backward algorithm for HMMs. To allow for blanks in the output paths, for each label sequence l  X  L  X  T consider a modified label sequence l 0  X  L 0  X  T , with blanks added to the beginning and the end and inserted between every pair of labels. The length of l 0 is therefore | l 0 | = 2 | l | + 1 .
 length t prefixes are mapped by B onto the length s/ 2 prefix of l , i.e. down to an integer value.
 The backward variables  X  t ( s ) are defined as the summed probability of all paths whose suffixes starting at t map onto the suffix of l starting at label s/ 2 Both the forward and backward variables are calculated recursively [2]. The label sequence proba-bility is given by the sum of the products of the forward and backward variables at any time step: The objective function for CTC is the negative log probability of the network correctly labelling the where target sequence z is at most as long as input sequence x . Then the objective function is: The network can be trained with gradient descent by differentiating O CTC with respect to the out-puts, then using backpropagation through time to differentiate with respect to the network weights. Once the network is trained, we would ideally label some unknown input sequence x by choosing the most probable labelling l  X  : Using the terminology of HMMs, we refer to the task of finding this labelling as decoding . Unfortu-nately, we do not know of a tractable decoding algorithm that is guaranteed to give optimal results. However a simple and effective approximation is given by assuming that the most probable path corresponds to the most probable labelling, i.e. 2.3 Integration with an External Grammar For some tasks we want to constrain the output labellings according to a predefined grammar. For example, in speech and handwriting recognition, the final transcriptions are usually required to form sequences of dictionary words. In addition it is common practice to use a language model to weight the probabilities of particular sequences of words.
 probabilistic grammar G , as well as the input sequence x : Absolute requirements, for example that l contains only dictionary words, can be incorporated by setting the probability of all sequences that fail to meet them to 0.
 At first sight, conditioning on G seems to contradict a basic assumption of CTC: that the labels are conditionally independent given the input sequences (see Eqn. (1)). Since the network attempts to model the probability of the whole labelling at once, there is nothing to stop it from learning inter-label transitions direct from the data, which would then be skewed by the external grammar. However, CTC networks are typically only able to learn local relationships such as commonly occur-as the probability of one word following another when the outputs are letters) it doesn X  X  interfere with the dependencies modelled by CTC.
 that x is conditionally independent of G given l . If we assume x is independent of G, this reduces the grammar depend on the underlying generator of the data, for example the language being spo-ken. However it is a reasonable first approximation, and is particularly justifiable in cases where the grammar is created using data other than that from which x was drawn (as is common prac-tice in speech and handwriting recognition, where independent textual corpora are used to generate language models).
 Finally, if we assume that all label sequences are equally probable prior to any knowledge about the input or the grammar, we can drop the p ( l ) term in the denominator to get assigning equal prior probabilities does not lead to an improper prior.
 We now describe an algorithm, based on the token passing algorithm for HMMs [16], that allows us to find an approximate solution to (11) for a simple grammar.
 does not form a sequence of dictionary words is 0.
 For each word w , define the modified word w 0 as w with blanks added at the beginning and end and to be a pair consisting of a real valued score and a history of previously visited words. In fact, each token corresponds to a particular path through the network outputs, and its score is the log probability of that path. The basic idea of the token passing algorithm is to pass along the highest scoring tokens at every word state, then maximise over these to find the highest scoring tokens at word to the first state in another. The output word sequence is given by the history of the highest scoring end-of-word token at the final time step.
 At every time step t of the length T output sequence, each segment s of each modified word w 0 holds 1: Initialisation: 2: for all words w  X  D do 3: tok ( w, 1 , 1) = ( ln ( y 1 b ) , ( w )) 4: tok ( w, 2 , 1) = ( ln ( y 1 w 5: if | w | = 1 then 6: tok ( w,  X  1 , 1) = tok ( w, 2 , 1) 7: else 8: tok ( w,  X  1 , 1) = (  X  X  X  , ()) 9: tok ( w,s, 1) = (  X  X  X  , ()) for all s 6 =  X  1 10: Algorithm: 11: for t = 2 to T do 12: sort output tokens tok ( w,  X  1 ,t  X  1) by ascending score 13: for all words w  X  D do 14: w  X  = arg max  X  w  X  D tok (  X  w,  X  1 ,t  X  1) .score + ln ( p ( w |  X  w )) 15: tok ( w, 0 ,t ) .score = tok ( w  X  ,  X  1 ,t  X  1) .score + ln ( p ( w | w  X  )) 16: tok ( w, 0 ,t ) .history = tok ( w  X  ,  X  1 ,t  X  1) .history + w 17: for segment s = 1 to | w 0 | do 18: P = { tok ( w,s,t  X  1) ,tok ( w,s  X  1 ,t  X  1) } 19: if w 0 s 6 = blank and s &gt; 2 and w 0 s  X  2 6 = w 0 s then 20: add tok ( w,s  X  2 ,t  X  1) to P 21: tok ( w,s,t ) = token in P with highest score 22: tok ( w,s,t ) .score += ln ( y t w 0 23: tok ( w,  X  1 ,t ) = highest scoring of { tok ( w, | w 0 | ,t ) ,tok ( w, | w 0 | X  1 ,t ) } 24: Termination: 25: find output token tok  X  ( w,  X  1 ,T ) with highest score at time T 26: output tok  X  ( w,  X  1 ,T ) .history all W words. However, because the output tokens tok ( w,  X  1 ,T ) are sorted in order of score, the search can be terminated when a token is reached whose score is less than the current best score with the transition included. The typical complexity is therefore considerably lower, with a lower bound of O ( TWlogW ) to account for the sort. If no bigrams are used, lines 14-16 can be replaced by a simple search for the highest scoring output token, and the complexity reduces to O ( TW ) . Note that this is the same as the complexity of HMM decoding, if the search through bigrams is exhaustive. Much work has gone into developing more efficient decoding techniques (see e.g. [9]), typically by pruning improbable branches from the tree of labellings. Such methods are essential for applications where a rapid response is required, such as real time transcription. In addition, many decoders use more sophisticated language models than simple bigrams. Any HMM decoding algorithm could be applied to CTC outputs in the same way as token passing. However, we have stuck with a relatively basic algorithm since our focus here is on recognition rather than decoding. The experimental task was online handwriting recognition, using the IAM-OnDB handwriting database [12], which is available for public download from http://www.iam.unibe.ch/ fki/iamondb/ For CTC, we record both the character error rate, and the word error rate using Algorithm 1 with a language model and a dictionary. For the HMM system, the word error rate is quoted from the of the target transcriptions in the test set.
 representation designed for HMMs. 3.1 Data and Preprocessing IAM-OnDB consists of pen trajectories collected from 221 different writers using a  X  X mart white-board X  [12]. The writers were asked to write forms from the LOB text corpus [8], and the position of their pen was tracked using an infra-red device in the corner of the board. The input data consisted of the x and y pen coordinates, the points in the sequence when individual strokes (i.e. periods when the pen is pressed against the board) end, and the times when successive position measurements were made. Recording errors in the x,y data were corrected by interpolating to fill in for missing readings, and removing steps whose length exceeded a certain threshold.
 IAM-OnDB is divided into a training set, two validation sets, and a test set, containing respectively 5364, 1438, 1518 and 3859 written lines taken from 775, 192, 216 and 544 forms. The data sets contained a total of 3,298,424, 885,964, 1,036,803 and 2,425,5242 pen coordinates respectively. For our experiments, each line was used as a separate sequence (meaning that possible dependencies between successive lines were ignored).
 numbers, and punctuation). A dictionary consisting of the 20 , 000 most frequently occurring words in the LOB corpus was used for decoding, along with a bigram language model optimised on the training and validation sets [13]. 5.6% of the words in the test set were not in the dictionary. Two input representations were used. The first contained only the offset of the x,y coordinates art preprocessing and feature extraction techniques [13]. We refer to this as the preprocessed input were normalised with respect to such properties as the slant, skew and width of the letters, and the  X  X nline X  features, such as pen position, pen speed, line curvature etc., and the second consisting of  X  X ffline X  features created from a two dimensional window of the image created by the pen. 3.2 Experimental Setup The CTC network used the BLSTM architecture, as described in Section 2.1. The forward and backward hidden layers each contained 100 single cell memory blocks. The input layer was fully connected to the hidden layers, which were fully connected to themselves and the output layer. The there were 4 input units and a total of 100,881 weights. For the preprocessed representation, there were 25 inputs and 117,681 weights. tanh was used for the cell activation functions and logistic sigmoid in the range [0 , 1] was used for the gates. For both input representations, the data was normalised so that each input had mean 0 and standard deviation 1 on the training set. The network was trained with online gradient descent, using a learning rate of 10  X  4 and a momentum of 0.9. Training was stopped after no improvement was recorded on the validation set for 50 training epochs. The HMM setup [13] contained a separate, left-to-right HMM with 8 states for each character ( 8  X  81 = 648 states in total). Diagonal mixtures of 32 Gaussians were used to estimate the observation Table 1: Word Error Rate (WER) on IAM-OnDB . LM = language model. CTC results are a mean over 4 runs,  X  standard error. All differences were significant ( p &lt; 0 . 01 ) probabilities. All parameters, including the word insertion penalty and the grammar scale factor, were optimised on the validation set. 3.3 Results From Table 1 we can see that with a dictionary and a language model this translates into a mean Without the language model, the error reduction was 26.8%. With the raw input data CTC achieved a character error rate of 13.9  X  0.1%, and word error rates that were close to those recorded with the preprocessed data, particularly when the language model was present.
 fore requires more use of context. A useful indication of the network X  X  sensitivity to context is Looking at the relative magnitude of the sequential Jacobian over time gives an idea of the range of context used, as illustrated in Figure 1. We have combined a BLSTM CTC network with a probabilistic language model. We have applied this system to an online handwriting database and obtained results that substantially improve on a state-of-the-art HMM based system. We have also shown that the network X  X  performance with raw sensor inputs is comparable to that with sophisticated preprocessing. As far as we are aware, our system is the first to successfully recognise unconstrained online handwriting using raw inputs only. Acknowledgments This research was funded by EC Sixth Framework project  X  X anoBioTact X , SNF grant 200021-111968/1, and the SNF program  X  X nteractive Multimodal Information Management (IM)2 X . Figure 1: Sequential Jacobian for an excerpt from the IAM-OnDB, with raw inputs (left) and pre-processed inputs (right). For ease of visualisation, only the derivative with highest absolute value is plotted at each time step. The reconstructed image was created by plotting the pen coordinates recorded by the sensor. The individual strokes are alternately coloured red and black. For both rep-resentations, the Jacobian is plotted for the output corresponding to the label  X  X  X  at the point when range of sensitivity extends in both directions from the dashed line. For the preprocessed data, the Jacobian is sharply peaked around the time when the output is emitted. For the raw data it is more spread out, suggesting that the network makes more use of long-range context. Note the spike in
