 It has been pointed out that conceptual search technologies using vector space models, unlike string-matching-type text retrieval, are quite effective for information retrieval longer and includes many keywords in a document. In vector space models, input computation typically with cosine similarity measure. 
Unlike Web document retrieval,  X  X ecall X  is more important than  X  X recision X  in in-nyms and the number of different ways of expressing the same entity (polysemy) are much larger than those of a typical Web page. 
A standard tool which has been often used in conceptual search using vector space dimension of words, and have sound feature that synonyms get closer in lower dimen-sional space if we wisely choose k , the number of dimension to reduce. However, the selection of the most appropriate dimension k has been a controversial problem. 
Assistance of information retrieval with clustering has been reported by many re-searchers since late 1990 X  X . Examples on information retrieval using clustering in-clude Cutting [3], Eguchi [7], and Chang [1]. 
In this paper, we will describe a method for enhancing retrieval performance using query expansion with a novel data structure, which we call  X  X luster granularity hierar-chy X . This is a hierarchical data structure, where a hierarchy consists of a granularity links are generated when the similarity between the two clusters in different granular-ity levels are higher than a predefined threshold. 
In our experiment, we have used Japanese patent data, and the granularity of clus-ters has been set to the power of two (i.e.16, 32, 64, 128, 256, 512, ). We adopted occurrence of words and documents. We also conducted comparative experiments of our method with na X ve vector space model and dimensionality reduction techniques such as LSI. Previous work using clustering to improve information retrieval has primarily limited to a single clustering (i.e. document clustering). We have observed that the co-unlike ordinary Web documents. Co-clustering is a technology proposed by Dhillon et al [5,6], which generates both document and word clusters simultaneously. In co-clustering, we seek document and word clusters that minimize a Kullback Leibler distance (See [2] for more details). 
Generally, clustering is expected to provide a useful piece of information for given methods, they tend to suffer from the following problems: clusters may contain useless information or noise. (2) The resulting cluster quality might be heavily affected by the choice of the initial seed of random variables. (3) The na X ve application of cluster information without removing noise may aggra-vate the performance of relevance feedback of information retrieval. occurrence between documents and words, are expected to be effective for patent and paper archive dataset. For instance, every patent data has one or more IPCs (Interna-tional Patent Classification codes) and technical terms specific to IPC subclasses. An example is an IPC subclass  X  X 05B X , where technical terms such as  X  X ewing X ,  X  X erfo-ration X ,  X  X eedle work X , and  X  X ewing machine X  are frequently used. 3.1 Our Solutions to Clustering Problems To mitigate problem (1), we have run the clustering algorithm multiple times by fit into a particular granularity level of clusters appropriately. In addition, we have put links between clusters of different granularity if arbitrary two clusters found in differ-constructed a hierarchy of clusters which we call  X  X luster granularity hierarchy X . 
To cope with problem (2), we have run the clustering algorithm multiple times by changing the initial conditions, followed by applying a  X  X luster averaging algorithm X  described later. 
To alleviate problem (3), apparent noise (outlier) documents in each cluster are (as depicted in Figure 1) is traversed from coarser level to finer level in sequel. Once there is a cluster whose similarity with the given query is higher than a threshold, we apply query expansion by considering the cluster average vector. 3.2 Cluster Averaging and Hierarchy Generation Algorithm We have developed a method for query expansion based on the observation discussed tering algorithm by changing granularity levels as granularity cluster levels. Part 2 is a cluster hierarchy construction algorithm. z Cluster Averaging Algorithm with Multiple Granularity Levels ( Part 1 ) [step1] Perform clustering with granularity level 1 ( , ..., ) k MM M R times by chang-ing the initial seed of random variables. Denote the obtained document cluster by { , , ..., } ( 1, ..., ) rR == rr r
D . [step2] Initialize a vector { , , ..., } = vector X ) by 1 D . [step3] Apply cluster smoothing algorithm.

In [step3], we perform  X  X veraging X  two similar vectors by taking their sum, fol-lowed by normalization. Applying this algorithm makes the resulting cluster less els, we then construct a cluster hierarchy by applying Cluster Hierarchy Generation Algorithm as shown below. z Cluster Hierarchy Generation Algorithm ( Part 2 ) [step1] Compute the similarity between i M and 1 i M + with average cluster vectors. [step2] If the similarity of document cluster i D in H is larger than a threshold, add a link from set
Figure 1 illustrates a part of hierarchy obtained by applying cluster hierarchy gen-eration algorithm. Clusters found in coarser granularity level are basically major clus-ters, and the number of elements is generally large. For example, at granularity level 16, we can find clusters labeled  X  X ell, Enzyme X ,  X  X uel, Engine X , and  X  X ransmission, Pachinko X  comes in, and some of clusters at granularity level 16 are divided into sub-clusters. 
On getting  X  X luster granularity hierarchy X , given a user X  X  input query, we traverse the data structure to see if there is any cluster (i.e., an average cluster vector) that has query expansion with the formula as shown below. Otherwise we replace query ex-pansion by ordinary similarity computation with the plain query vector. 
Note that is a non-negative coefficient. The data we used for experimentation is a collection of patents included in NTCIR-3 patent task [9]. The total number of patents is about 340,000, and we randomly sam-pled 40,000 documents. For the sake of comparison, we took na X ve vector space model (VSM) and dimensionality reduction methods (LSI) with three different di-ance, we take a couple of examples as follows: (1)  X  X ice-planting and Transplanting X  (2)  X  X ibration-proof architecture X  
For input query (1), it fits into IPC subclass  X  X 63F X . The result in Figure 2 shows that our proposed method outperforms other methods. Three different dimensionality reductions with LSI have almost the same results. 
For input query (2), it fits into multiple IPC subclasses,  X  X 04H X ,  X  X 04G X ,  X  X 16F X ,  X  X 32B,  X  X 01L X ,  X  X 02D X ,  X  X 04F X  ,  X  X 01D,  X  X 63B X  , and  X  X 04C X , where all of them have common descriptions on vibration-proof architecture of various devices and methods. LSI with k=256 performs the second best result. 
During preprocessing, we extracted 38,400 keywords, and made document vectors for each patent document. Summing up document vectors, we obtained a matrix which can be fed into co-clustering. We selected 16, 32, 64, 128, 256, and 512 (i.e., 6 levels) granularity levels for clusters to be generated. Five different initial values for to construct a cluster hierarchical data structure. 
Given user X  X  input query, it was converted by morphological analyzer to a pseudo document vector. Taking advantage of the hierarchical data structure, we expanded the query, and sorted the search results in the descending order of similarity values. We described a method for improving the retrieval performance using query expan-plying multiple co-clustering alg orithms with different initial conditions. Comparative experiments using patent data show that our method is a promising approach for im-proving retrieval performance. Because it is difficult to foresee what value k (dimen-sion to reduce) is the best with a well-known dimensionality reduction method (LSI), we claim that our hierarchy data structure makes it possible to automatically choose a set of appropriate words for query expansion. 
Although our hierarchical data structure was constructed by applying co-clustering algorithms, it might be possible to employ other clustering algorithms, since the data future research work might include scalability enhancement, examination of the cases in which an input query fits into minor or outlier clusters, and application of our algo-rithm and data structure to a variety of data sets other than patent data. 
