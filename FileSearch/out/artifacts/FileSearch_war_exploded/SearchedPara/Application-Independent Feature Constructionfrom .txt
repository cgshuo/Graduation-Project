 Real-world data may contain noise. When dealing with classification tasks, it has been shown that presence of noise in the data may have a negative impact on the performance of classifiers learned from the data [1]. One may differentiate two main types of noise: class noise when noise affects the class label, and attribute noise when noise affects all attributes but the class label. Many solutions have been proposed to tackle class noise, e.g., by noise elimination or noise correction (see [1] for a survey) and more recently b y instance weighting [2]. On the other hand, others approaches aim at solving the problem of attribute noise by noise identification and modeling [3,4], and noise cleansing [1,5]. In this paper, we focus on Boolean attribute noise problem and we address the following important question: how to learn accurate predictive models from attribute-noisy data sets?
Instead of removing noisy instances or corr ecting noisy values, we propose a method to cope with attribute noise without changing or removing any attributes values in the training data. Our approac h combines two recent advances in fault-tolerant itemset mining and feature construction. The goal of fault-tolerant item-set mining [6] is to support the discovery of relevant frequent itemsets in noisy binary data (see, e.g., [7] for a recent s urvey). Among others, an extension to (frequent) closed set mining towards faul t-tolerance has been studied in [8] that enables a bounded number (  X  ) of errors per item/attribute. It is based on the so-called  X  -free sets, i.e., the approximate condensed representation of frequent itemsets introduced in [9]. Using such patterns is the core of our approach to ro-bust feature construction . Following the proposals from, for instance, [10,11,12], we consider that attribute sets may be more relevant than single attributes for class discrimination. Then, pattern types based on the so-called closedness prop-erties enable to avoid redundant features in an application-independent setting. In this paper, we investigate further the use of  X  -freeness (and thus  X  -closedness) when considering feature construction from noisy training data. From that per-spective, it extends our previous work [11] which focussed on noise-free samples only. Our proposal can be summarized as follows. First, we mine non-redundant fault-tolerant patterns based on (  X  )-free itemsets. Then, we process these pat-terns to compute new features that will enable to encode a new training set. Finally, classical classification algorithms can be applied. This proposal is yet another contribution to pattern-based classification. In pioneering studies, asso-ciation rules were considered [13]. Since then, emerging patterns [14] (see [15] for a survey and [16] for a noise tolerant pa ttern-based approach), and more recently condensed representations of frequent i temsets have been studied [10,11,12]. The paper is organized as follows. Section 2 provides the needed definitions before the description of our proposal in Sect ion 3. Section 4 provides an experimen-tal validation on selected UCI (http://archive.ics.uci.edu/ml/) data. Section 5 concludes. Let us consider a binary database r = {T , I , R} ,where T is a set of transactions (or objects) described by a set I of Boolean items (or attributes) and R : T X  I  X  X  0 , 1 } .When R ( t, i ) = 1, we say that transaction t contains item i .An itemset I  X  X  is a set of items. The f requency of itemset I  X  X  is freq ( I,r )= |
Objects ( I,r ) | where Objects ( I,r )= { t  X  X  | X  i  X  X  R ( t, i )=1 } . I is said to be  X  -frequent if freq ( I,r )  X   X  .
 Definition 1 (association rule,  X  -strong rule,  X  -free itemset). An asso-ciation rule  X  on r is an expression I  X  J ,where I  X  X  and J  X  X \ I .
 The frequency of the rule  X  is freq ( I  X  J, r ) and its confidence is conf (  X , r )= freq ( I  X  J, r ) /f req ( I,r ) .Let  X  be an integer. A  X  -strong rule is an association rule of the form I  X   X  J which is violated in at most  X  objects, and where I  X  X  and J  X  X \ I .Anitemset I  X  X  is a  X  -free itemset iff there is no  X  -strong rule which holds between its proper subsets. When  X  =0 ,  X  is omitted, and we talk about strong rules ,and free itemsets .
 First introduced in [9],  X  -free itemsets and  X  -strong rules have been designed as an approximate condensed repres entation for frequency queries.  X  -freeness is a generalization of the key pattern concept [17] (case  X  = 0) and it can also be discussed in terms of equivalence classes.
 Definition 2 (  X  -closure, equivalence class). Let  X  be an integer. The  X  -closure of an itemset I on r is cl  X  : P ( I )  X  X  ( I ) s.t. cl  X  ( I,r )= { i  X  X | freq ( I,r )  X  freq ( I  X  X  i } ,r )  X   X  } .When  X  =0 , cl 0 ( I,r )= { i  X  X | freq ( I,r )= freq ( I  X  X  i } ,r ) } and it corresponds to the well-known closure operator. We can also group itemsets by  X  -closure equivalence classes (  X  -CECs): two  X  -free itemsets I and J are said  X  -equivalent ( I  X  cl  X  J )if cl  X  ( I,r )= cl  X  ( J, r ) . Once again, when  X  = 0, we get the formalization of closure equivalence classes from [17]. We can also derive  X  -strong rules from  X  -CECs (i.e., from  X  -free item-sets and their  X  -closures). Indeed, we have a  X  -strong association rule between a  X  -free itemset and each element of its  X  -closure. In [7,8],  X  -free itemsets and their associated  X  -closures are combined to define the so-called  X  -bi-sets. Definition 3 (frequent  X  -bi-set). Abi-set ( T,I ) such that T  X  X  and I  X  X  and Objects ( I 1 ,r )= T .  X  -bi-sets appear as an extension of the so-called formal concepts or associated closed itemsets. Indeed, they appear as e xamples of maximal combinatorial rect-angles of 1 values having at most  X  0 per column [8]. We now consider how we use  X  -CECs to derive robust features. In order to manage classification tasks, we are interested in relevant  X  -strong association rules contained in  X  -CECs. Figure 1(a) shows a typical case of an interesting  X  -CEC:  X  -free itemsets X and Y do not contain a class attribute and their (equal)  X  -closure ( X, Y, Z, c i ) contains a class attribute c i . Indeed, we may derive two potentially interesting  X  -strong rules : X  X  c i and Y  X  c i . According to the formalization from [18],  X  : X  X  c i is a  X  -strong charact erization rule (  X  -SCR) if c i is a class attribute and body X is minimal. X is minimal if there is no other frequent rule  X  : Y  X  c i s.t. Y  X  X and conf (  X  ,r )  X  1  X   X   X  .Moreover, when  X   X  [0;  X / 2 [, the set of  X  -SCRs does not contain included or equal body conflicts.

Defining  X  -SCR only based on the confidence measure is not sufficient for prediction. Therefore, we propose to exploit a Growth rate measure Gr which has been already proved useful in such a context [14]. The Growth rate of  X  : X  X  c i is defined as a ratio of relative frequencies as follows: where r c i is the database restricted to objects of Class c i . In [19], Gr is set in the general framework of the so-called  X  -dependent measures. Such measures depend on the rule antecedent frequency (  X  ) and the rule number of exceptions (  X  ) following two principles: (i) ,when  X  is fixed, Gr (  X , r )increaseswith freq (  X , r ) and (ii) ,when  X  is fixed, Gr (  X , r )increaseswith  X  .
 This leads us to lower bounds for several interestingness measures (including Growth rate) w.r.t.  X  and  X  values (see [19] for details). In Figure 1(b), contin-gency table for  X  : X  X  c i a  X  -strong rule concluding on a class attribute c i shows that, by construction, we have a lower bound (  X   X   X  )for freq ( X, r c i ), an upper bound  X  for freq ( X, r \ r c i ) and other deductible bounds for dotted cells. Moreover, we can deduce a lower bound for Gr and conf measure. Indeed, Then, with few deduction steps, we get: where c j is the majority class. Thus, for a given frequency threshold  X  ,  X  -frequent  X  -free itemsets whose  X  -closures contain a class attribute s.t.  X  satisfies Equa-tions (1) and (2) are emerging patterns. In the rest of the paper,  X  and  X  values are constrained w.r.t. Equations (1) and (2).
 Our feature construction process may now be summarized with Algorithm 1. Procedure FeaturesExtraction (Line 1) mines all  X  -frequent  X  -free itemsets I that are bodies of  X  -strong character ization rules in r .Thisstepisperformed efficiently using straig htforward extension of AC-like 1 . AC-like is an implemen-tation of a levelwise algorithm presented in [9] that benefits from anti-monotonic properties of  X  -freeness and  X  -frequency to compute  X  -frequent  X  -free itemsets. Moreover, since we are interes ted in minimal itemsets whose  X  -closures contain a class attribute, there i s no need to check supersets of itemsets selected in a precedent level.Then, each I becomes a new descriptor for r , and (Line 1) the value of I for a transaction t is the proportion of items in I that are veri-fied by t in r . Items is the dual operator for Objects .Now, R  X  [0; 1] and
Algorithm 1 : Building new data set with noise-tolerant features R ( t, I )  X  X  0 , 1 p ,..., p  X  1 p , 1 } with p = | I | ). Finally, r is the new database made of noise-tolerant feat ures, ready for a classi fication learning step. To assess the noise-tolerance of our process, we ran our feature construction process ( NTFC ) on several UCI data sets and several no isy versions of them. We want to learn accurate predictive models despite of noisy samples. Therefore, in our experiments we deal with attribut e-noisy training s ets and clean test sets. We add random noise at different levels only on attributes and only in training sets. For a data set d and a x %noiselevel( x  X  X  10 , 20 , 30 , 40 , 50 } ), each attribute value has x % chances to be changed (within its range values) in each transaction of the training set 2 . When dealing with continuous attributes, we first discretize the training data: each attribute is split into several intervals using entropy-based splitting method [20]. Then, we add x % noise and perform a simple binarization. Finally, we run C4.5 decision tree [21] and Naive Bayes classifier ( NB ) on each noisy version of the data set and on each data set enhanced by new descriptors generated with Algorithm 1. All pre-processing steps (adding noise, discretization, binarization) and accuracy results for NB and C4.5 classifiers are obtained with 10-folds stratified cross-validation  X  using Weka platform [22]. A strategy for setting  X  : Automatically setting frequency threshold is still an open question (see [23,24] for prelimin ary results in that direction). Given a frequency threshold  X  , how can we determine relevant  X  values? Evolution of  X  -dependent interestingness measure (such as growth rate) w.r.t.  X  is well known. Decreasing  X  implies higher values for Gr but such interesting patterns could be rare, especially in noisy d ata sets. When increasing  X  , extracted patterns tend to match with noisy patterns in training data, but higher  X  values tend to be less relevant (with low Gr values). In Figure 2, we plotted training accuracy results w.r.t.  X  values for tic-tac-toe data set. As expected, accuracy on training data increases with  X  until stabilization (or slowing down).  X  values around stabilization are interesting since lower values lead to less accurate models and higher values bring nothing more. Let  X  opt denote these values. These interesting values depends on the amount of noise in data. Since in real case, noise level is not known a priori, a reasonable way to reach these  X  opt values is (1) to increase  X  starting from 0, and (2) to check evolution of training data accuracy for stabilization.
 Accuracy results: Accuracy results comparison are shown in Tab. 1. For each data set, we used different frequency thr esholds indicated i n parenthesis: (f: x -y / z ) means that absolute frequency thresholds vary from x to y with step z (when regular). NTFC accuracy results are reported in two columns: Avg for average accuracy overall tested  X  values for  X  opt values and Max for maximal accuracy overall  X  and  X  values.
 First, we remark that there often exists a combination of  X  and  X  values (col. Max) for which NTFC &amp; C4.5 (44/48) and NTFC &amp; NB (36/48) achieve better accuracy results than on original data. S econd, even if less impressive, our simple strategy for setting  X  (col. Avg) also allows us to catch better accuracy results on noisy data sets enhanced by NTFC (36/48 for C4.5 and 27/48 for NB ). In this paper, we have proposed an original process ( NTFC ) combining recent advances in fault-tolerant pattern mining and feature construction to handle classification tasks in attribute-noisy environments. We enhance data description with new features based on relevant fault-tolerant patterns, say  X  -frequent  X  -free itemsets and their  X  -closures. Experimental results show that classification tools such that NB and C4.5 achieve higher accuracy results with an enhanced description of data (i.e., using noise-tolerant features) than with original data.  X  values have shown to be related to the amount of noise in the data. Finally, we have given a strategy to automatically set  X  values that are relevant w.r.t. the noise level in data samples  X  thus supporting a parameter tuning dedicated to our process.

