 Sinead Williamson SINEAD @ CS . CMU . EDU Jordan Boyd-Graber JBG @ UMIACS . UMD . EDU iSchool and UMIACS, University of Maryland, College Park, MD USA Latent feature models assume data are generated by combin-ing latent features shared across the dataset and aim to learn this latent structure in an unsupervised manner. Such mod-els typically assume all properties of a feature are common to all data points X  X .e., each feature appears in exactly the same way across all observations. This is often a reasonable assumption. For example, microarray data are designed so each cell consistently corresponds to a specific condition. This does not hold for images. Consider a collection of im-ages of a rolling ball. If a model must create new features to explain the ball X  X  every position, it will devote less attention to other aspects of the image and will be unable to generalize across the ball X  X  path. Instead, we would like some proper-ties of a feature, e.g., shape, to be shared across data points but properties, e.g., location, to be observation-specific. Models that generalize across images to discover transformation-invariant features have many applications. Image tracking, for instance, discovers mislaid bags or ille-gally stopped cars. Image reconstruction restores partially corrupted images. Movie compression recognizes recurring image patches and caches them across frames.
 We argue that latent feature models of images should:  X  Discover features needed to model data and add addi- X  Generalize across transformations so features can  X  Handle properties of real images such as occlusion. A nonparametric model that comes close to our goals is the noisy-OR transformed Indian buffet process (NO-tIBP, Austerweil &amp; Griffiths, 2010); however, its likelihood model is inappropriate for real images. Existing unsupervised mod-els that handle realistic likelihoods (Jojic &amp; Frey, 2001; Titsias &amp; Williams, 2006) are parametric and cannot dis-cover new features. In Section 2, we further describe these and other models that meet some, but not all, of our criteria. In Section 3, we propose models that fulfill these proper-ties by combining realistic likelihoods with nonparametric frameworks. In Section 4, we introduce novel inference algorithms that dramatically improve inference for trans-formed IBPs in larger datasets (Section 5). In Section 6, we show that our models can discover features and model data better than existing models. We discuss relationships with other nonparametric models and extensions in Section 7. a z r  X  s x In this section, we review the Indian buffet process and how its extension, the transformed IBP, models simple images. We then describe likelihood models for images. These mod-els are a prelude to the models we introduce in Section 3. 2.1. The Indian Buffet Process The Indian buffet process (IBP, Griffiths &amp; Ghahramani, 2005) is a distribution over binary matrices with exchange-able rows and infinitely many columns. This can define a nonparametric latent feature model with an unbounded number of features. This often matches our intuitions. We do not know how many latent features we expect to find in our data; neither do we expect to see all possible latent features in a given dataset.
 To use the IBP to model data, we must select a likelihood model that determines the form of features corresponding to columns of Z and how the subset of features selected by a row of Z combine to generate a data point. 1 Many likelihoods have been proposed for the IBP, several of which are appropriate for modeling images. 2.2. The Transformed IBP Most IBP-based latent feature models assume a feature is identical in every data point in which it appears. This pre-cludes image modeling, where (for example) a car moves from location to location or where a person may be in either the foreground or background. Na  X   X ve models would learn different features for each location a car appears in; a more appropriate model would learn that each observation is in fact a transformation of a common feature.
 The transformed IBP (tIBP, Austerweil &amp; Griffiths, 2010) extends the IBP to accommodate data with varying locations. In the tIBP, each column of an IBP-distributed matrix Z is (as before) associated with a feature. In addition, each non-zero element of Z is associated with a transformation r nk Transforming the features and combining them according to a likelihood model produces observations. In the original tIBP paper, features were generated and combined using noisy-OR (Wood et al., 2006); we refer to this model as the noisy-OR tIBP (NO-tIBP), which allows the same feature to appear in different locations, scales, and orientations. 2.3. Likelihoods for Latent Feature Image Models In addition to the noisy-OR, another likelihood that has been used with the IBP is a linear Gaussian model, which assumes images are generated via a linear superposition of features (Griffiths &amp; Ghahramani, 2005). Each IBP row se-lects a subset of features and generates an observation by ad-ditively superimposing these features and adding Gaussian noise. This is demonstrated in Figure 1(a). This model can be extended by adding weights to the non-zero elements of the IBP-distributed matrix (Knowles &amp; Ghahramani, 2007) and incorporating a spiky noise model (Zhou et al., 2011) appropriate for corrupted images.
 If we want to model images where features can occlude each other, linear Gaussian models are inappropriate. In the vision community, images are often represented via overlapping layers (Wang &amp; Adelson, 1994), including in generative probabilistic models (Jojic &amp; Frey, 2001; Titsias &amp; Williams, 2006). In these  X  X prite X  models, features are Gaussian-distributed, and an ordering is defined over a set of features. In each image, every active feature has a trans-formation (as in the tIBP) and a binary mask for each pixel. Given the feature order, the image is generated by taking the value, at each pixel, of the uppermost unmasked feature. This model is appealing. It is an intuitive occlusion model; features have a consistent ordering; and only the topmost feature is visible. However, this likelihood model has only been used for parametric feature sets and on data where the number of features is known a priori . While the NO-tIBP likelihood model is incompatible with real images, it provides a foundation for nonparametric models with transformed features. In this section, we use the tIBP to build models that combine nonparametric feature models with more useful and realistic likelihood functions for real images.
 We begin by providing a general representation for the trans-formed IBP with an arbitrary likelihood. 1. Sample a binary matrix Z  X  IBP (  X  ) , determining the 2. For k  X  N , sample a feature  X  k  X  p (  X  ) . 3. For each image n  X  X  1 ,...,N } The distribution over transformations p ( r ) , the feature likeli-hood p (  X  ) , and the image likelihood p ( x |  X  , z n , r defined in various ways. In the remainder of this section, we will use this generic framework to define concrete models with a parameterization of transformations and two different likelihood models.
 Transformations Following Austerweil &amp; Griffiths (2010), we consider three categories of transformation: translation, rotation and scaling. We parameterize a transfor-mation r : R D  X  R D using a vector ( r x ,r y ,r r ,r s ) . The parameters ( r x ,r y ) parameterize translations, and the trans-formed feature r ( a k ) is obtained by shifting each pixel in a k by ( r x ,r y ) . Rotations are parameterized by r r  X  [0 , 2  X  ) , and scaling is parameterized by r s  X  R + . In practice, we restrict the possible rotations and scaling factors to a finite set, and assume a uniform prior on transformations. Linear Gaussian transformed IBP Our first attempt to define a likelihood applicable to real data is based on the lin-ear Gaussian likelihood for the IBP described in Section 2.3. Each feature  X  k is represented using a real-valued vector a k  X  N ( 0 , X  2 a I ) . In each image, the transformed features are combined using superposition,
We refer to the resulting model as the linear Gaussian transformed IBP (LG-tIBP).
 Masked transformed IBP While the LG-tIBP model is appropriate for real-valued data, it cannot handle feature occlusion. To address this problem, we propose a masked transformed IBP (M-tIBP), based on the sprite model (Sec-tion 2.3). In this model, each feature  X  k is represented by a Gaussian feature a k and a shape vector  X  k . Let  X  be a per-mutation of N that imposes an ordering on the features. We can interpret feature i being  X  X ehind X  feature k if  X  i &lt;  X  Each time a feature appears in an image, we sample a mask s n,k from the Bernoulli probabilities in the corresponding shape vector  X  k . These masks  X  X cclude X  lower layers so that at each pixel; only the uppermost unmasked feature contributes to the final image.
 The generative process can be described as follows. For each image n and feature k , define an auxiliary variable M n,k , the visibility indicator, The visibility indicator M d n,k , is 1 when feature k is the uppermost unmasked feature at pixel d in image n . The image and feature likelihoods for the M-tIBP are where the operator  X  is the Hadamard product on matrices. Figure 1 shows how the IBP-distributed matrix Z and other transformations variables combine features to form images for the IBP, LG-tIBP, and M-tIBP. We perform inference of both LG-tIBP and M-tIBP us-ing MCMC. At each iteration, we sample the Gaussian-distributed features A , the IBP-distributed binary matrix Z , the transformations R , the hyperparameters  X  ,  X  x and  X  and, for M-tIBP, the binary masks S and ordering  X  . 4.1. Sampling Indicators, Transformations, and Masks In all models, the binary indicator matrix Z , the matrix of transformations R , and (where appropriate) the feature masks S are all closely coupled. Austerweil &amp; Griffiths (2010) sampled each z nk of Z by explicitly marginalizing over r nk , and then sampling r nk . However, explicitly com-puting the conditional distribution for all transformations for each feature cannot scale to even moderate-sized images (as discussed in Section 5). Instead, we sample z nk , r and s n,k jointly via a Metropolis-Hastings step. The efficacy of a Metropolis-Hastings sampler depends on the quality of the proposal distribution. We design a data-driven proposal distribution (Tu &amp; Zhu, 2002) tablished pattern matching technique that assigns high prob-ability to plausible states.
 Feature Indicator Proposal Distribution Let K + be the highest feature index represented in the data, excluding the current data point. Our proposal distribution for z nk ,k  X  K
Our proposal distribution for previously unseen features follows Griffiths &amp; Ghahramani (2005): sample K  X  new features according to Poisson (  X /N ), where N is the number of observations.
 Transformation Proposal Distribution To obtain a pro-posal distribution for translations r nk that matches our intuitions about the true posterior, we look at the cross-correlation between the feature a k and the residual  X  x obtained by removing all but that feature from the image x Cross-correlation (Duda &amp; Hart, 1973) is a standard tool in classical image analysis and pattern-matching. The cross-correlation u ? v between two real-valued images u and v is a measure of the similarity between u and a translated version of v , i.e., ( u ? v )( t ) := P T  X  =1 u (  X  ) v ( t +  X  ) . Since our proposal distribution for r  X  nk must be strictly posi-tive, we use the exponentiated function for our proposal distribution, 2 and define the residual  X x for M-tIBP, and for LG-tIBP.
 In Figure 2, we show the proposal distribution for r  X  nk a feature and three data points. The proposal distribution peaks in the locations that best match the pattern of pix-els in the feature. If no locations match the feature, the proposal distribution is relatively entropic. Thus, the cross-correlation proposal distribution will cause us to consider good candidates for r nk .
 To incorporate scaling and rotation in addition to transla-tion, we must increase the space over which we define our Metropolis-Hastings proposal. For a small transformation space (e.g., multiples of  X  2 rotation and half / double scaling) it remains practical to extend the proposal distribution to include all possible scaling and rotation combinations. We separately obtain cross-correlations of these transformed features with the residual image, and concatenate the result-ing vectors to obtain a distribution over all possible trans-formations. For new features, r nk is set to be the identity transformation. Mask Proposal Distribution In the M-tIBP, we must also propose a binary mask s n,k . We use, as a proposal distribu-tion, the conditional distribution Unseen Features For previously unseen features, we sam-ple a new feature a k  X  X  (0 , X  2 a ) . Our proposal distribution for the corresponding mask is obtained by normalizing a k and sampling each pixel of the proposed mask s  X  n,k accord-ing to a series of Bernoulli distributions parameterized by the normalized entries of a k . 4.2. Resampling Transformation and Masks In addition to sampling z nk , r nk and s n,k jointly, we also resample r nk (and, for M-tIBP, s n,k ) for values of n and k for which z nk = 1 . We jointly resample r nk using a Metropolis-Hastings step with proposal distribution q r ( r (or q r ( r nk ) q s ( s nk ) ). For the M-tIBP, we also Gibbs sample the binary masks using the conditional distribution where p ( s d n,k | s d  X  ( n,k ) ) is given in Eqn. (9). 4.3. Sampling the Feature Order We assume the feature order  X  is sampled from a uniform distribution over permutations. We sample the feature or-der using a Metropolis-Hastings step where we uniformly choose two consecutive features and propose an order swap. 4.4. Sampling Features and Hyperparameters Conjugacy eases the sampling of a k . For the M-tIBP, we sample the d th pixel of the k th feature as The hyperparameters  X  ,  X  x and  X  a can be Gibbs sampled via closed form equations (Doshi-Velez, 2009). 4.5. Modeling Color Images The derivation above assumes that each pixel is a single real number. However, natural images are typically have color information, represented as a three-dimensional vector for each pixel. In our model, all colors contribute to the image likelihoods. Similarly, the proposal distribution is an element-wise sum over all possible channels, where  X x c n,k and a c k are c -channel contribution of  X x a , respectively.
 In the M-tIBP case, for feature k in image n , we assume all channels share a common mask s n,k . The main motivation behind the algorithm proposed in Sec-tion 4 is to allow the transformed IBP to be applied to large data. Austerweil &amp; Griffiths (2010) calculate the likelihood of the data for every possible transformation. Replacing this naive approach with the sampler presented above can achieve a speed-up of at least O ( D min( SR,K/ log D )) , where R is the number of rotations considered, S is the number of scales considered, D is the number of pixels, and K is the number of non-zero elements in z n .
 Evaluating the LG-tIBP and M-tIBP likelihoods for a single image requires O ( DK ) computations. Since the number of possible translations 3 is O ( D ) , calculating the likelihood for all possible translations in O ( SRD 2 K ) , yielding a total per-iteration complexity of O ( ND 2 K 2 ) for the inference method used by Austerweil &amp; Griffiths (2010). If we were to also sum over values of s n,k , this would scale as O (2 By contrast, calculating the cross-correlation between a feature and an image residual can be done using the fast Fourier transform in O ( D log D ) , so the proposal dis-tribution described in Section 4.1 can be calculated in O ( SRD log D ) . The likelihood need only be evaluated twice in the Metropolis-Hastings step, so our sampler scales as O ( NSRDK max( K, log D )) . We evaluate the LG-tIBP and M-tIBP models 4 on both sim-ulated and real-world data against the linear Gaussian IBP (IBP), the noisy-OR transformed IBP (NO-tIBP) and the sprite model ( S PRITE , Jojic &amp; Frey, 2001). Experiments on simulated data show that both LG-tIBP and M-tIBP recover the underlying features and locations more effectively than IBP. All data sets were scaled to have zero mean and unit variance for linear Gaussian models.
 Simulated Data To qualitatively assess the ability of LG-tIBP and M-tIBP to find translated features, we generated data using four colorful features:  X  O  X ,  X  &gt;  X ,  X  t  X , and  X   X   X . Each synthetic dataset contains 100 images generated by selecting features independently with probability 0 . 5 and sampling a transformation uniformly. Since the noisy-OR likelihood cannot process color images, data are binarized for NO-tIBP. Although the other models can cope with Gaussian noise, NO-tIBP cannot, so no noise was added. Each experiment ran 100 iterations; we present features and reconstructions from the final iteration.
 Figure 3 compares the performance of the four models on a dataset constructed by translating four features. NO-tIBP achieves good results. While the IBP struggles to find common structure, both LG-tIBP and M-tIBP generalize across locations and discover features qualitatively similar to NO-tIBP X  X . Where features overlap, M-tIBP obtains the correct reconstruction; LG-tIBP does not.
 Figure 4 shows the training set likelihood at each iteration, plotted against accumulated CPU time, obtained using both the proposed Metropolis Hastings inference and Gibbs sam-pling in the LG-tIBP model on two datasets: 9  X  9 and 15  X  15 pixel images respectively. Each marker indicates a single iteration; each plot shows 100 iterations. Time was measured on a machine with 6-Core 2.8-GHz CPU and 16GB memory. The speed-up predicted in Section 5 is real-ized in practice; while convergense requires slightly more iterations, it requires far less total CPU time.
 In addition, we trained LG-tIBP and M-tIBP on a dataset where features have been scaled, rotated, and translated . This was not implemented by Austerweil &amp; Griffiths (2010), presumably due to the computational cost. Figure 5 shows that our two models successfully detected the underlying features. The ordering learned by M-tIBP matches the true order, except in the case of the green  X  O  X  and the blue  X   X   X , which did not often overlap.
 Real-world data To show that the performance on sim-ulated data in Section 6 carries over to real images, we evaluated LG-tIBP and M-tIBP on four image datasets, cho-sen to reflect various levels of complexity from simple video games with static/dynamic background to real-world scenes. All images were resized to 101  X  101 pixels. We trained and tested the models using the full three-channel RGB data. For each dataset, we trained LG-tIBP, M-tIBP, IBP and S
PRITE 9 on a randomly selected 80% of the images with the remaining 20% held out for testing. Since the NO-tIBP is only appropriate for binary data, we could not compare with this method. We used the features extracted from the training set to estimate Z and R on test data, and evaluated the reconstructions using test set RMSE. Table 1 shows that LG-tIBP and M-tIBP achieve better performance than IBP across all datasets; M-tIBP performs equally well as S PRITE on three datasets, and much better on the SMB dataset. M-tIBP performs better than LG-tIBP on SMB and WLK datasets, but worse than LG-tIBP on DNK. This is because DNK has limited occlusions and a black background, and so can be adequately represented using the simpler LG-tIBP. Figure 6 shows reconstructions and features obtained us-ing the IBP, S PRITE , LG-tIBP, and M-tIBP. The IBP only matches the image background. In contrast, both LG-tIBP and M-tIBP identify shapes that appear in different loca-tions. For example, in the first column of Figure 6, LG-tIBP identifies Donkey Kong (cyan) and a fireball (yellow), in addition to the background (green). Interestingly, LG-tIBP mis-identifies a pie 10 as a fireball but missed the actual fireball. Our M-tIBP model detected the pie (red) and the fireball (blue), while Donkey Kong (cyan) and background (yellow) are also clearly identified. Though M-tIBP has slightly larger RMSE than LG-tIBP on this dataset, the features seems more intuitive.
 In the Super Mario dataset, while LG-tIBP extracted the bush and brick clearly, M-tIBP managed to extract the text  X 100 X , denoting points earned by the player (green). S PRITE performs poorly, possibly due to the large, sparsely observed feature set. M-tIBP identified the blue sky as two parts: one is the red feature and the other is the green feature. Because bricks often appear in the center of the screen, the model learns to  X  X cclude X  that location with a patch of sky. While LG-tIBP and M-tIBP can learn features and transfor-mations, M-tIBP is, on the whole, more accurate and the reconstructions are clearer. S PRITE can generally recon-struct data as well as M-tIBP, but the extracted features are less clear. One possible reason is that S PRITE assumes all features are present in each image. Moreover, in practice it is difficult to know a priori the number of features in a dataset. These two factors mean S PRITE is unlikely to scale to heterogeneous datasets such as SMB. We have presented two nonparametric latent feature models for real-valued images, and presented a novel and efficient inference scheme. In this section, we discuss further ap-plications of this inference paradigm, and discuss possible extensions to our models.
 Exploitation of Pattern Matching Algorithms This in-ference scheme uses scoring functions from classical im-age analysis as the proposal distribution in a Metropolis-Hastings algorithm and combines the robustness and compu-tational appeal of a well-established pattern recognition tool with the flexibility of probabilistic models. This approach, or similar methods based on other classical pattern recog-nition techniques (Tu &amp; Zhu, 2002; Tu et al., 2005), can be applied across a range of Bayesian models to improve inference in large state spaces.
 An alternative to modeling images as real-valued vectors is to use image codewords (Li Fei-Fei &amp; Perona, 2005). Other techniques have used transformed Bayesian nonparametric models to build high-performing vision systems using fixed codewords (Sudderth et al., 2005); a combination of these models would allow for a joint model to infer transforma-tions, codewords, and feature cooccurrence patterns. Rotation and scaling are implemented by extending the space for our cross-correlation-based proposal distribution. One avenue for future work is to investigate how existing non-statistical models for pattern recognition can sample a broder class of transformations using Metropolis-Hastings. Additional Modeling Directions Features can appear more than once in an image, contrary to the assumptions of the tIBP. One avenue for future work is to extend the model to allow multiple instances of a feature in a given image. The infinite gamma-Poisson process (Titsias, 2007) is a distribution over infinite non-negative integer valued matrices. It has been used for image modeling, but that application required presegmentation of images. This work would allow extension to non-segmented images.
 As in the original tIBP paper, we assumed that transfor-mations associated with each (data point, feature) pair are sampled i.i.d. from some distribution f ( r ) over possible transformations. One possible avenue for future research is to allow correlations (e.g., over time) between transfor-mations in different images, leading to an image tracking model, which also leads to more efficient inference, by re-stricting the range in which an feature can appear in the t to a neighborhood of the feature X  X  location in the ( t  X  1) image. This idea was used to speed up inference in the S
PRITE implementation of Titsias &amp; Williams (2006). In-corporating spatial information into the mask distribution would also lead to more coherent feature appearances and counteract some of the  X  X potty X  features observed for M-tIBP.
 In addition, a more informative prior on the features a k could be used to encode domain-specific knowledge X  X or example, for data comparable to the Walker video, one might make use of vertically oriented ellipses to find human-shaped features. The authors would like to thank Joseph Austerweil, Frank Wood, Finale Doshi-Velez and Michalis K. Titsias for pub-lishing implementations. This research was supported by NSF grant #1018625. Jordan Boyd-Graber is also supported by the Army Research Laboratory through ARL Coopera-tive Agreement W911NF-09-2-0072. Sinead Williamson is supported by NIH grant #R01GM087694 and AFOSR grant #FA9550010247. Any opinions, findings, conclusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors.

