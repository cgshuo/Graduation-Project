 As graphical models are applied to more complex problems, it is increasingly necessary to learn parameters from data. Though the likelihood and condi tional likelihood are the most widespread training objectives, these are sometimes undes irable and/or infeasible in real applications.
 With low treewidth, if the data is truly distributed accordi ng to the chosen graphical model with some parameters, any consistent loss function will rec over those true parameters in the high-data limit, and so one might select a loss function acco rding to statistical convergence rates [1]. In practice, the model is usually misspecified to s ome degree, meaning no "true" parameters exist. In this case, different loss functions lea d to different asymptotic parameter estimates. Hence, it is useful to consider the priorities of the user when learning. For low-treewidth graphs, several loss functions have been propose d that prioritize different types of accuracy (section 2.2). For parameters  X  , these loss functions are given as a function be efficiently computed by loss-specific message-passing sch emes[2, 3].
 The likelihood may also be infeasible to optimize, due to the computational intractability of computing the log-partition function or its derivatives in high treewidth graphs. On the other hand, if an approximate inference algorithm will be us ed at test time, it is logical to design the loss function to compensate for defects in infere nce. The surrogate likelihood (the likelihood with an approximate partition function) ca n give superior results to the true likelihood, when approximate inference is used at test time [4].
 The goal of this paper is to efficiently fit parameters to optimi ze an arbitrary function of predicted marginals, in a high-treewidth setting. If  X  (  X  ) is the function mapping parameters to (approximate) marginals, and there is some loss function L (  X  ) defined on those marginals, we desire to recover dL d  X  . This enables the use of the marginal-based loss functions m entioned previously, but defined on approximate marginals.
 There are two major existing approaches for calculating dL d  X  . First, after performing infer-ence, this gradient can be obtained by solving a large, spars e linear system[5]. The major disadvantage of this approach is that standard linear solve rs can perform poorly on large Figure 1: Example images from the Berkeley dataset, along wi th marginals for a conditional random field fit with various loss functions. graphs, meaning that calculating this gradient can be more e xpensive than performing infer-ence (Section 4). A second option is the Back Belief Propagat ion (BBP) algorithm[6]. This is based on application of reverse-mode automatic different iation (RAD) to message passing. Crucially, this can be done without storing all intermediat e messages, avoiding the enormous memory requirements of a naive application of RAD. This is effi cient, with running-time in practice similar to inference. However, it is tied to a speci fic entropy approximation (Bethe) and algorithm (Loopy Belief Propagation). Extension to sim ilar message-passing algorithms appears possible, but extension to more complex inference a lgorithms [7, 8, 9] is unclear. Here, we observe that the loss gradient can be calculated by f ar more straightforward means. r  X  0 . This result follows from, first, the well-known trick of app roximating Jacobian-vector products by finite differences and, second, the specia l property that for marginal inference, the Jacobian matrix d  X  takes place over the local polytope with an entropy that is co ncave and obeys a minor technical condition. It can also be used with non-concave en tropies, with some assumptions on how inference recovers different local optima. It is easy t o use this to compute the gradient of essentially any differentiable loss function de fined on marginals. Effectively, all one needs to do is re-run the inference procedure on a set of pa rameters slightly "perturbed" in the direction  X  X   X   X  . Conditional training and tied or nonlinear parameters can also be accommodated.
 One clear advantage of this approach is simplicity and ease o f implementation. Aside from this, like the matrix inversion approach, it is independent of the algorithm used to perform independence, and applicable to a variety of different infer ence approximations. Like BBP, the method is efficient in that it makes only two calls to infere nce. 2.1 Marginal Inference This section briefly reviews the aspects of graphical models and marginal inference that are required for the rest of the paper. Let x denote a vector of discrete random variables. We use the exponential family representation where f ( x ) is the features of the observation x , and A = log P x exp  X  f ( x ) assures normal-ization. For graphical models, f is typically a vector of indicator functions for each possib le configuration of each factor and variable. With a slight abus e of set notation to represent components of vectors like those in Eq. 1 using function nota tion. Write  X  ( x  X  ) to refer to This gives an alternative representation for p , namely Marginal inference means recovering the expected value of f or, equivalently, the marginal probability that each factor or variable have a particular v alue. Though marginals could, in principle, be computed by the bru te-force sum in Eq. 3, it is useful to consider the paired variational representation [ 10, Chapter 3] in which A and  X  can both be recovered from solving the same optimization pro blem. Here, M = {  X  (  X  ) |  X   X   X  n } is the marginal polytope X  those marginals  X  resulting from some parameters that produces the marginals  X  .
 As M is a convex set, and H a concave function, Eq. 5 is equivalent to a convex optimizat ion problem. Nevertheless it is difficult to characterize M or compute H (  X  ) in high-treewidth graphs. A variety of approximate inference methods can be se en as solving a modification of Eqs. 4 and 5, with the marginal polytope and entropy replac ed with tractable approxi-mations. Notice that these are also paired; the approximate  X  is the exact gradient of the approximate A .
 The commonest relaxation of M is the local polytope This underlies loopy belief propagation, as well as tree-re weighted belief propagation. Since a valid set of marginals must obey these constraints, L X  M . Note that since the equality constraints are linear, there exists a matrix B and vector d such that A variety of entropy approximations exist. The Bethe approx imation implicit in loopy belief propagation [11] is non-concave in general, which results i n sometimes failing to achieve the global optimum. Concave entropy functions include the t ree-reweighted entropy [12] , convexified Bethe entropies [13], and the class of entropies obeying Heskes X  conditions [14]. 2.2 Loss Functions Given some data, {  X  x } , we will pick the parameters  X  to minimize the empirical risk Likelihood. The (negative) likelihood is the classic loss function for t raining graphical models. Exploiting the fact that dA/d  X  =  X  (  X  ) , the gradient is available in closed-form. Surrogate Likelihood. Neither A nor  X  is tractable with high treewidth. However, if written in variational form (Eqs. 4 and 5), they can be approx imated using approximate in-ference. The surrogate likelihood [4] is simply the likelih ood as in Eq. 9 with an approximate A . It has the gradient as in Eq. 10, but with approximate margin als  X  .
 Unlike the losses below, the surrogate likelihood is convex when based on a concave inference method. See Ganapathi et al.[15] for a variant of this for inf erence with local optima. Univariate Likelihood. If the application will only make use of univariate marginal s at test time, one might fit parameters specifically to make these univariate marginals accurate. Kakade et al.[3] proposed the loss This can be computed in treelike graphs, after running belie f propagation to compute marginals. A message-passing scheme can efficiently compute the gradient.
 Univariate Classification Error. Some applications only use the maximum probability marginals. Gross at al.[2] considered the loss where S is the step function. This loss measures the number of incorr ect components of  X  x if each is predicted to be the  X  X ax marginal X . However, since this is non-differentiable, it is suggested to approximate this by replacing S with a sigmoid function S ( t ) = (1 + exp(  X   X t ))  X  1 , where  X  controls the approximation quality. Our experiments use  X  = 50 . As with the univariate likelihood, this loss can be computed if exact marginals are available. Computing the gradient requires another message passing sc heme.
 Clique loss functions. One can easily define clique versions of the previous two loss functions, where the summations are over  X  , rather than i . These measure the accuracy of clique-wise marginals, rather than univariate marginals. 2.3 Implicit Differentiation As noted in Eq. 7, the equality constraints in the local polyt ope are linear, and hence when the positivity constraint can be disregarded, approximate marginal inference algorithms can be seen as solving the optimization  X  (  X  ) = arg max  X  ,B  X  = d  X   X  + H (  X  ) . Domke showed[5], in our notation, that where D =  X  2 H  X   X   X   X  T is the (diagonal) Hessian of the entropy approximation. Unfortunately, this requires solving a sparse linear syste m for each training example and iteration. As we will see below, with large or poorly conditi oned problems, the computational restricting what solvers can be used. Another limitation is that D can be singular if any counting numbers (Eq. 16) are zero. 2.4 Conditional training and nonlinear parameters.
 For simplicity, all the above discussion was confined to full y parametrized models. Nonlinear and tied parameters are easily dealt with by considering  X  (  X  ) to be a function of the  X  X rue X  Algorithm 1 Calculating loss derivatives (two-sided). parameters  X  . Once dL/d  X  is known dL/d  X  can be recovered by a simple application of the chain rule, namely Conditional training is similar: define a distribution over a random variable y , parametrized both of these are in the experiments. This section shows that when  X  (  X  ) = arg max  X   X  X   X   X  + H (  X  ) , the loss gradient can be computed by Alg. 1 for a concave entropy approximation of the form when the counting numbers c obey (as is true of most proposed entropies) For intuition, the following Lemma uses notation (  X  ,  X  , H ) suggesting the application to marginal inference. However, note that the result is true fo r any functions satisfying the stated conditions.
 Lemma. If  X  (  X  ) is implicitly defined by where H (  X  ) is strictly convex and twice differentiable, then Proof. First, form a Lagrangian enforcing the constraints on the ob jective function. The solution is  X  and  X  such that d L /d  X  = 0 and d L /d  X  = 0 . Recall the general implicit function theorem. If f (  X  ) is implicitly defined by the constraint that h (  X  , f ) = 0 , then Using Eq. 20 as our definition of h , and differentiating with respect to both  X  and  X  , we have We see that  X  d  X  /d  X  T is the upper left block of the matrix being inverted. The resu lt follows, since the inverse of a symmetric matrix is symmetri c.
 The following is the main result driving this paper. Again, t his uses notation suggesting the application to implicit differentiation and marginal in ference, but holds true for any functions satisfying the stated conditions.
 Theorem. Let  X  (  X  ) be defined as in the previous Lemma, and let L (  X  ) be defined by L (  X  ) = M  X  (  X  ) for some differentiable function M (  X  ) . Then the derivative of L with respect to  X  is given by Proof. First note that, by the vector chain rule, Next, take some vector v . By basic calculus, the derivative of  X  (  X  ) in the direction of v is The result follows from substituting  X  X / X   X  for v , and using the previous lemma to establish that d  X  /d  X  T = d  X  T /d  X  .
 Alg. 1 follows from applying this theorem to marginal infere nce. However, notice that this does not enforce the constraint that  X   X  0 . The following gives mild technical conditions under which  X  will be strictly positive, and so the above theorem applies.
 Theorem. If H (  X  ) = P  X  c  X  H (  X  c ) + P i c i H (  X  i ) , and  X   X  is a (possibly local) maximum of  X   X  + H (  X  ) , under the local polytope L , then This is an extension of a previous result [11, Theorem 9] for t he Bethe entropy. However, extremely minor changes to the existing proof give this stro nger result.
 Most proposed entropies satisfy these conditions, includi ng the Bethe entropy ( c  X  = 1 , c i + P probability that  X  appears in a randomly chosen tree) and any entropy satisfyin g the slightly strengthened versions on Heskes X  conditions [14, 16, Secti on 2].
 What about non-concave entropies? The only place concavity was used above was in es-tablishing that Eq. 20 has a unique solution. With a non-conc ave entropy this condition is still valid , not not unique, since there can be local optima. BBP essenti ally calculates this Figure 2: Times to compute dL/d  X  by perturbation, Back Belief Propagation (BBP), sparse matrix factorization (direct) and the iterative symmetric -LQ method (symmlq). Inference with BP and TRWS are shown for reference. As these results use two-sided differences, perturbation always takes twice the running time of the base inference algorithm. BBP takes time similar BP. Results use a pairwise grid with x i  X  { 1 , 2 , ..., 5 } , with univariate terms varying a . Top Left : Bethe entropy for varying grid sizes, with a = 1 . Matrix factorization is efficient on small problems, but scales poorly. Top Right : Bethe entropy with a grid size of 32 and varying interaction strengths a . High interactions strengths lead to poor conditioning, slowing iterative methods. Bottom Left : Varying grid sizes with the TRW entropy. Bottom Right : TRW entropy with a grid size of 32 and varying interactions. derivative by  X  X racking X  the local optima. If perturbed bel iefs are calculated from constant initial messages with a small step, one obtains the same resu lt. Thus, BBP and perturbation give the same gradient for the Bethe approximation. (This wa s also verified experimentally.) It remains to select the perturbation size r . Though the gradient is exact in the limit r  X  0 , numerical error eventually dominates. Following Andrei[ 17], the experiments here use r = For inference, we used either loopy belief propagation, or t ree-reweighted belief propagation. As these experiments take place on grids, we are able to make u se of the convergent TRWS algorithm [18, Alg. 5], which we found to converge significan tly faster than standard TRW. Base code was implemented in Python, with C++ extensions for inference algorithms for efficiency. Sparse systems were solved directly using an inte rface to Matlab, which calls LAPACK. We selected the Symmetric LQ method as an iterative s olver. Both solvers were the fastest among several tested on these problems. (Recall , the system is indefinite.) BBP results were computed by interfacing to the authors X  implem entation included in the libDAI toolkit[19]. We found the PAR mode, based on parallel updates [6, Eqs. 14-25] to be much slower than the more sophisticated SEQ_FIX mode, based on sequential updates [6, extended Table 1: Binary denoising results, comparing the surrogate likelihood against three loss functions fit by implicit differentiation. All loss function s are per-pixel, based on tree-reweighted belief propagation with edge inclusion probabi lities of . 5 . The  X  X est Published X  results are the lowest previously reported pixelwise test e rrors using essentially loopy-belief propagation based surrogate likelihood. (For all losses, l ower is better.) version, Fig. 5]. Hence, all results here use the latter. Oth er modes exceeded the available 12 GB memory. All experiments use a single core of a 2.26 GHz ma chine.
 Our first experiment makes use of synthetically generated gr id models. This allows system-atic variance of graph size and parameter strength. With the TRW entropy, we use uniform edge appearance probabilities of  X  = . 49 , to avoid singularity in D . Our results (Fig. 2) can be summarized as follows. Matrix inversion (Eq. 13) with a di rect solver is very efficient on small problems, but scales poorly. The iterative solver is e xpensive, and extremely sensitive to conditioning. With the Bethe approximation, perturbati on performs similarly to BBP. TRWS converges faster than BP on poorly conditioned problem s.
 The second experiment considers a popular dataset for learn ing in high-treewidth graphical models[21]. This consists of four base images, each corrupt ed with 50 random noise patterns (either Gaussian or bimodal). Following the original work, 10 corrupted versions of the first base image are used for training, and the remaining 190 f or testing. This dataset has been used repeatedly [22, 23], though direct comparison is sometimes complicated by varying model types and training/test set divisions. This e xperiment uses a grid model over neighboring pairs ( i, j ) Because the previous dataset is quite limited (only four bas e 64x64 images), all methods perform relatively well. Hence, we created a larger and more challenging dataset, consisting of 200 200x300 images from the Berkeley segmentation datase t, split half for training and testing. These are binarized by setting y i = 1 if a pixel is above the image mean, and y i = 0 uniform on [0 , 1] .
 Table 1 shows results for all three datasets. All the results below use batch L-BFGS for learning, and uniform edge appearance probabilities of  X  = . 5 . The surrogate likelihood performs well, in fact beating the best reported results on t he bimodal and Gaussian data. However, the univariate and clique loss functions provide b etter univariate accuracy. Fig. 1 shows example results. The surrogate likelihood (which is convex), was used to initialize the univariate and clique likelihood, while the univariate likelihood was used to initialize the smooth classification error.
