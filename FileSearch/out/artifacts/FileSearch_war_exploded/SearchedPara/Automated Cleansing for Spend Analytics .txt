 The development of an aggregate view of the procurement spend across an enterprise using transactional data is increasingly becoming a very important and strategic activity. Not only does it provide a complete and accurate picture of what the enterprise is buying and from whom, it also allows it to consolidate suppliers, as well as negotiate better prices. The importance, as well as the complexity, of this cleansing exercise is further magnified by the increasing popularity of Business Transformation Outsourcing (BTO) wherein enterprises are turning over non-core activities, such as indirect procurement, to third parties, who now need to develop an integrated view of spend across multiple enterprises in order to optimize procurement and generate maximum savings. However, the creation of such an integrated view of procurement spend requires the creation of a homogeneous data repository from disparate (heterogeneous) data sources across various geographic and functional organizations throughout the enterprise(s). Such repositories get transactional data from various sources such as invoices, purchase orders, account ledgers. As such, the transactions are not cross-indexed, refer to the same suppliers by different names, and use different ways of representing information about the same commodities. Before an aggregated spend view can be developed, this data needs to be cleansed, primarily to normalize the supplier names and correctly map each transaction to the appropriate commodity code. Commodity mapping, in particular, is made more difficult by the fact that it has to be done on the basis of unstructured text descriptions found in the various data sources. We describe an on-demand system to automatically perform this cleansing activity using techniques from information retrieval and machine learning. Built on standard integration and application infrastructure software, this system provides enterprises with a fast, reliable, accurate and on-demand way of cleansing transactional data and generating an integrated view of spend. This system is currently in the process of being deployed by IBM for use in its BTO practice. H.3 [ Information Storage and Retrieval ] Spend analysis, information retrieval, knowledge management, unstructured data, commodity mapping. A significant amount of financial resources are expended by enterprises towards the procurement of goods and services. As such, streamlining procurement activities can lead to significant cost savings that have a direct impact on the bottom line of such organizations. A significant barrier, however, in doing this is the fact that enterprises, especially the large ones, seldom have an integrated view of their procurement spend across the entire enterprise. This is more so in the case of  X  X ndirect procurement X  which relates to commodities that are used by an enterprise for its day to day operations but which are not part of its main business, such as office furniture, automobile rentals and hotel expenses, etc. In this case, procurement is normally done by a large number of people using a variety of procurement functions deployed across the enterprise (such as requisitions, supplier catalogs and payments) leading to even more inconsistency, disparity, noise and errors in the data. On the other hand, there are significant savings and competitive advantages that are to be had by developing an integrated view of spend across the enterprise. By aggregating spend across various dimensions such as suppliers, commodities, plants, businesses, etc., an enterprise can get an accurate picture of how the procurement money is being spent. This enables it to generate a variety of savings by consolidating suppliers (e.g. different suppliers may be used to purchase the same commodity albeit under different descriptions), reducing number of suppliers to a preferred list of suppliers, and negotiating better prices by consolidating order volume to these fewer suppliers. Such saving opportunities get even more magnified in Business Transformation Outsourcing (BTO) engagements wherein enterprises turn over non-core activities, such as indirect procurement, to third parties such as IBM, who now need to develop an integrated view of spend across multiple enterprises in order to optimize procurement and generate maximum savings. By aggregating its own spend with that of a BTO customer, a company like IBM can negotiate better rates with its own preferred suppliers, based on the combined volume, and share the savings with the BTO clients. The BTO clients get the best of both worlds  X  they do not have to deal with a non-core-business function and at the same time save money. There are four basic activities that need to be performed in order to develop an integrated view of spend across an enterprise. First, data has to be extracted from multiple, heterogeneous back end systems. Second, basic error checking and transformation operations have to be done to ensure data from these multiple sources can be properly merged (e.g. making date formats or currencies identical). Third, the supplier names have to be normalized (e.g. IBM and International Business Machines have to be recognized as a single entity) and transactions have to be mapped to a common commodity taxonomy (e.g. hazar dous waste handling expense, pollution control expense need to be mapped to the hazar dous waste control commodity). Finally, the cleansed data has to be aggregated to develop the required enterprise-wide view of procurement spend. Of these, only step three presents a significant technical challenge, since the others can be readily performed, either by using off-the-shelf or easily developed software tools. Supplier name normalization involves the mapping of multiple names for the same entity to a single, common name for that entity. Multiple names arise due to different locations, different business undertaken by the same enterprise, parent child relationships arising out of acquisitions, as well as errors and noise in the transactional data. Since enterprises have demographic information about their suppliers, such as addresses, contact information and tax identifiers, this data can be used in conjunction with the names to normalize the supplier names. Commodity mapping requires each transaction to be mapped to an appropriate spend category. Two different types of commodity mapping needs to be done, based on whether transactions have to be mapped to a company-specific commodity code or to a standard classification code (such as the UNSPSC code [11]). In cases where an enterprise has its own commodity taxonomy, the first type of mapping is clearly preferred. However, in cases such as BTO engagements where the procurement-spend of multiple enterprises has to be merged, the spend transactions of the corresponding enterprises need to be mapped to a uniform taxonomy, generally a standard taxonomy like the UNSPSC rather than any individual enterprise X  X  taxonomy. This can be done in one of two ways. In any case, commodity mapping is, in general, a much more difficult task than supplier name normalization since it has to be done based on textual descriptions that have been gleaned from various sources, such as part descriptions, general ledger entries, invoice items, purchase order items and accounts payable entries. Any or all of these descriptions may be misspelled, incomplete or absent. Of more significance is the fact that this is highly unstructured data consisting of words, broken phrases, abbreviations and numeric parametric data. As such, methods from the information retrieval and machine learning literature cannot usually be directly used to good effect since they have not been developed for such noisy, unstructured data. One reason is that descriptions, both in transactions as well as commodity definitions, are normally very short. As such, each word is significant, but distinguishing between different potential matches becomes correspondingly harder, since the number of items in a taxonomy often numbers in the tens of thousands of which the best one has to be selected based on a couple of words. Second, they normally contain significant amounts of domain-specific terminology as well as abbreviations and terms. Third, the order of words in descriptions becomes an important issue that is often not considered in string similarity methods. Fourth, computational complexity becomes a significant factor, and many techniques that perform well on simple problems become unusable when applied to spend cleansing tasks. This is true for supplier normalization as well. Another factor that we found to be extremely important with regards to human users taking the tool seriously and using it for the mapping was their perception of how good it was. For supplier name normalization, users expressed a strongly disapproval of false negatives (such as suppliers that clearly looked to be the same were considered by the tool to be different). However, in the case of commodity mapping they considered false positives in bad light, especially if the mapped or suggested commodity was too far from the item being mapped. For example, an item such as  X  X ax software X  being mapped to  X  X oftware tax X  was regarded to be extremely bad, even though the two terms were very similar by some approaches (e.g. tf-idf). These issues, and steps we took to address them, are discussed in more detail later in the following section. However, note that due to the proprietary nature of the work, we have been unable to describe in complete detail the steps we took to address most of these issues. Much of this mapping is nowadays done manually where transactions/supplier names are mapped to corresponding normalized-commodities/ normalized-supplier-names by the use of filtering rules as well as manual matches of various transactional descriptions and supplier names/addresses. Not only is this method cumbersome and error prone, it is also devoid of any fixed methodology that can be consistently repeated for future mapping exercises since it is highly subjective and non-algorithmic. In the following sections, we describe the Spend Analysis Tool that we have developed as an end-to-end system for automatically cleansing transactional data and generating an integrated view of spend across an enterprise. We first describe the technical approach we took in building a spend-data cleansing engine for the automation of supplier name normalization (Section 3.1) and commodity mapping (Section 3.2). Later on, in Section 4, we describe the system architecture and design of the overall spend analysis tool in which this engine is embedded. As described earlier, the supplier name normalization exercise involves the identification of the sets of names in the transactional data that actually refer to the same entity, based on the names as well as associated demographic information. The system has two distinct, default processes depending upon the availability of a unique (normalized) set of suppliers for the enterprise. If such a list is already available, then the task boils down to comparing the supplier associated with each transaction to the set of s uppliers in the normalized list and identifying the closest match. On the other hand, if no such master list exists, then one has to be created during the mapping process itself. Each process can be run as-is or configured by a user based on the quality and type of data at hand. Additionally, the system also provides several other methods and algorithms that can be used by a user if necessary. In the first scenario, a list of normalized suppliers is already available. For example, such a list may have a supplier name, IBM. The task now is to see which of the suppliers mentioned in the spend transactions are actually the same as a supplier in the normalized list. In our example, the task boils down to mapping I.B.M, IBM Poughkeepsie, IBM Belgium S.A., International Business Machines Corp., etc., to the same normalized name, IBM. In order to do this, each s upplier name has to be potentially compared to all the names in the normalized name list, as well as all the names already mapped to normalized names. On the other hand, while the second scenario can be handled similarly (build the normalized list on the fly by adding names that do not match any name added to the list thus far), it can be far more efficient to cluster all the supplier names into sets of names used for the same enterprise, and then choose one of the names in the cluster as the normalized name for that cluster. While string similarity measures, such as Levenshtein distance [1,5,8], Jaccard distance [3] and token-frequency/inverse-document-frequency (tf-idf) [10] could be used directly on the supplier names alone to do the normalization exercise, each approach has some weakness that made it unsuitable for use by itself. For example, Levnshtein distance calculation is computationally expensive, and its usage on real data with tens-to hundreds-of-thousands of supplier names made the mapping process computationally prohibitive. Moreover, while such edit-distances do not distinguish between positional differences, just the differences; in supplier names, however, positional differences are very important. For example, differences in the beginning of names are more likely to mean that the names belong to different enterprises than differences towards the end of the names. Even cheaper methods like tf-idf had similar problems, since edit-distance computations still had to be done at the token level. Also, tf-idf too does not take into consideration the order of the tokens in a name. However, in the case of names, differences in the beginning are generally more significant and highly suggestive of the names being different, as opposed to differences towards the end of the names. Moreover, there are generally a fairly large number of similar looking names with the same tokens; thus, the number of matches returned by approaches such as tf-idf is also big, and further filtering needs to be little use since the number of names in which a particular token occurs although large in the absolute sense, is fairly small relative to the total number of names and hence the differentiating ability of the  X  X df X  component is greatly reduced since most tokens have fairly close values. As an example, of the datasets we cleansed with the tool had 487K unique suppliers in its transactional data. Finally, the quality of the results was greatly enhanced by using the supplier names in conjunction with the demographic information, and combining these algorithms to create hybrid algorithms and using them with simple rules. As such, the default process for first scenario described above consists of an application of various rules based on string similarity methods. These rules include exact and fuzzy matches on whole or parts of supplier names along with demographic data. These rules are further enhanced by the use of various techniques such stop word elimination, removal of special characters, transformation of numbers to uniform format, abbreviation generation and comparison, etc. Furthermore, we use tf-idf based indexes, dictionaries and standard company name databases (such as the Fortune 500 list) to assign different weights to different words and tokens in a name. Thus, differences towards the beginning of names is considered to be more important than difference towards the end of the name, proper nouns are weighted more than other types of words, etc. Simpler rules are evaluated first; more complex rules and methods are applied later. Matching of addresses, however, can introduce some complications in the process due to different formats used across various systems, especially if the address is available as a string rather than in attribute-value form. In such cases, extracting various attributes such as zip codes, street addresses and city may require the use of manually created regular expressions or rules, or in some cases, automatically created rules using machine learning techniques like RAPIER [2] or Winnow [12]. The engine provides these for use by the user if so needed. For the second scenario described above, where a normalized list of suppliers does not exist, a clustering exercise is used in conjunction with the above mentioned approach to break down the list of all the suppliers that exist in the transactions into sets of names where the names in each set belong to the same s upplier. To do the clustering, we use the canopy based clustering method described in [7] on a random subset of the data. The idea is to first use computationally cheap methods to make some loose clusters, called canopies X  followed by the more computationally intensive methods to refine the canopies further into appropriate clusters. Due to the extremely large datasets normally encountered in the real-world (as the 487K dataset mentioned earlier), this clustering approach was particularly attractive. Nevertheless, even this approach was found to be have lots of problems when used with the string similarity methods since very small canopies led to large errors in data while big canopies resulted in unacceptable computational expense. As such, we had to rely heavily on various kinds or rules on names, name tokens, and address fields and limit strong string similarity approaches to a minimum. While this led to more errors than we would have liked, it provided us a good middle ground between mapping performance and computational performance. At the same time, the system has the flexibility of changing the rules to use more stringent similarity methods if the dataset is small for a given client, etc. To create canopies, cheap methods including zip code matches, phone number matches and name and/or address token are used in various rules. Once the canopies have been formed, the more expensive techniques consisting of the elaborate rules used for the supplier name mapping described previously are used for performing similarity matches across supplier names and street addresses. Once this exercise is over, the normalized list is manually approved by a domain expert, and is used to normalize supplier across the entire data. Note, however, that simple that it sounds, even a review is not an easy task since manually reviewing several hundred thousand suppliers is a slow and expensive task, even for a team of reviewers. Hence, the system has to do as good a task at mapping as possible. Finally, we use external data providers, such as Dun and Bradstreet (D&amp;B) to provide parent-child relationships and mappings for suppliers which cannot be mapped automatically or manually. As in the case of supplier name normalization, the system has two distinct processes based on the availability of historical transactional data that has already been mapped to the corresponding commodities. As described in the previous section, the commodity mapping task has two distinct flavors to it. One is mapping one commodity taxonomy to another, such as would be required in a BTO engagement where the taxonomy of the BTO client has to be mapped to the BTO host taxonomy (possibly a standard like the UNSPSC code) to enable spend aggregation. Once this is done, then commodity mapping is straightforward for those transactions which have explicit commodity codes in them. The more difficult, and common, task involves mapping transactions where no such commodities have been specified and hence the mapping has to be done on the basis of various descriptions. Second type of commodity mapping involves mapping transactions directly to a target taxonomy, based on descriptions in the transactions. If the target taxonomy is one different from the company taxonomy, then even this mapping exercise can be boiled down to one where the company taxonomy is first mapped to the target taxonomy and then transactions mapped to target taxonomy by mapping them first to the company taxonomy. In our exercises with actual spend data from various enterprises, we discovered that unlike supplier normalization, simple rules were simply not enough, and far more richer classification approaches were needed. Moreover, the process needed and the results achieved varied widely in the real-world based on what kind of data was available and what exactly needed to be done. In the case of taxonomy mapping, mapped data was almost never available. Hence, the system uses only string similarity based methods for commodity taxonomy mapping along with simple rules. Some of the problems in mapping taxonomies have already been discussed in Section 2. These include the computational expense of edit-distance methods, the absence of positional differentiation in all the string similarity methods as well as the use of domain specific terminology. Compounding all this is the fact that the source and target taxonomy may have wide structural differences. As a case in point, consider the UNSPSC code. It has roughly 20K commodities in a four-level taxonomy. However, while the taxonomy is very broad, and includes commodities and services in almost all industrial sectors, it is not very deep in any given sector. Company taxonomies, on the other hand, are not very broad but are generally far more specific in terms of commodities, especially in the case of items used in production. For example, while the UNSPSC has a commodity codes for desktop and notebook computers, companies are much more specific in terms of the specific types of desktop and notebook computers. This is more so in the case of production parts, but also in the case of services. As such, there is often a many-to-many mapping that needs to be done between the two taxonomies. Another important factor, also pointed out in Section 2, is to determine exactly what the commodity description is referring to. For example, software tax is a sort of tax while tax software is a type of software. To enable the system to do this mapping properly, we have used various techniques from classical information retrieval literature including stop word removal, stemming, tokenization using words and grams, coupled with dictionaries and domain specific vocabulary. Moreover, we have integrated the system with WordNet [12] to enable use of synonyms, sense determination, morphological analysis and part of speech determination in the creation of rules and methods for better identifying the main keyword(s) in a description and ranking the results of mapping better. While this has enabled the system to perform very well for general, non-production type of commodities, the performance is still way off for production level commodities, simply because the amount of domain knowledge needed is very high and direct mappings generally do not exist between the different taxonomies. In most of the cases of transactional commodity mapping, mapped historical data is also not available. In the few cases where mapped historical data was available, we explored the use of a variety of machine learning techniques that are commonly used for classification tasks involving textual data, including maximum entropy [9], support vector machines [4] and Bayesian methods [6] with very good results. In the default process provided with the system, maximum entropy (maxent) methods are used for commodity mapping as they were found to be the most effective at this task. The other approaches are available if a user desires to use them. As a first step, the system uses string-similarity based fuzzy tf-idf indexes into the commodity taxonomy to see if a map can be found with high level of confidence. The more expensive maxent-models are used only if that does not result in a successful map. The second default process, for cases where historical mapped data is not available, attempts to create such mapped data for training the classifier models. Once such data has been prepared, the first process described above (for mapped data) is then used to do the commodity mapping exercise. Consequently, the available transactional data is randomly split into two parts. One part is then used to create the mapped data, and then used to train the classification models. The remaining data is then mapped to the appropriate commodities using these classification models. In order to create the mapped data needed for training, the process again uses the clustering and string similarity methods described in Section 3.1. A human then uses the sign-off tools described in the following section to ensure that the mapped data produced by the clustering exercise is correct before the remaining transactional data is mapped. The Spend Analysis tool is an end-to-end solution that provides a fast, reliable, accurate and on-demand way of extracting transactional spend data from multiple back-end systems going all the way to the generation of detailed, aggregate spend reports. Figure 1 shows the main steps of the process flow of this tool. There are four main steps: The data extraction and warehousing phase involves the extraction of data from various kinds of datasources, such as relational databases, XML documents, legacy systems, flat files, as well as proprietary platforms such as SAP and Ariba. Once the data is extracted, various kinds of error checking is performed and transformations (such as reformatting of data) is performed, and the data is loaded onto a staging database from where the cleansing process gets its data. This step can be performed manually by running scripts, or can be setup to work automatically in the background on a schedule. After the data has been loaded onto the staging database, it is cleansed in parallel along two dimensions: supplier names are normalized and transactions are mapped to appropriate commodity codes. Commodity mapping may be done in either one stage or in two stages. One stage involves directly mapping transactions to the target taxonomy. Two stage process first involves commodity taxonomy mapping followed by transactional mapping using the commodity taxonomy mapping. While the as-is cleansing process can be started either from a web-based tool, or scheduled to run in the background, any changes to the process or configuration needs off-line activity such as modification of property files, etc. Based on whether a master list of normalized suppliers is available, one of the two processes described in Section 3.1 is selected. The selection of the process, as well as the actual configuration of the process (such as which rules and/or algorithms are used, the precedence of these rules, the sub-process flow) can all be configured via property files. However, if new rules or algorithms are needed, they have to be developed at the application program level. If a normalized list is not available, then it may be created manually using the Sign-off tools with cleansing being run subsequently, or it may be created automatically using clustering techniques with manual approval via the sign-off tools. The supplier normalization is primarily rule-based with rules being based on a variety of string similarity and indexing methods. Some of the default rules used include exact name matches, fuzzy (string similarity based) matches on name and associated demographic information such as address or telephone number (e.g. similar partial name and same address, or same tax identifier), parent-child relationships established from D&amp;B data, etc. Supplier names that can be normalized with a confidence level higher than a user set threshold are automatically used to update the master table, and hence are immediately available for subsequent mapping. Those that cannot be mapped with the requisite confidence are put in transitional tables and need manual sign-off after which they too become available for subsequent mapping. If commodity taxonomy mapping needs to be done, then that process can be started independent of the other steps. While it is obviously better for the commodity taxonomy mapping to be finished prior to the transactional mapping, the system does not require it to be done first. For the transactional mapping, as in the case of supplier name normalization, the process chosen is based on the availability of a master commodity taxonomy. If one is not available, one can be created manually using the sign-off tools or automatically using clustering, string similarity and index-frequency methods as outlined in Section 3.2 Once again, the default process may be used, configured via property files, or deeper changes may be made at the application program level. The system maps each transaction based on data from several possible sources, including descriptions from invoices, purchase orders, account ledgers, general ledgers, as well as commodity descriptions. The target commodity taxonomy can be something that is proprietary, or a standard classification such as the UNSPSC code [11]. Similar to supplier name normalization, mapped items are added to the master tables and become available immediately, while the unmapped items have to await the sign off process. The default mapping process provided with the system uses three distinct approaches, performed in increasing order of complexity (and based on availability of mapped data and the type of mapping to be done). First, string similarity based methods (default is edit distance [5]) are used to see if a similar description has been mapped previously. Second, frequency index based methods (default is tf-idf [10] but with similarity-match-based tokens rather than exact match tokens) are used to see if a strong enough match can be found directly with a taxonomy entry. Third, maximum-entropy models [9] are used to map tokenized descriptions to commodity names/descriptions. Each type of description (invoice, purchase order, etc.) in a transaction is individually mapped, and a majority or k-nearest neighbor approach is taken to decide the mapped commodity. After the transactional spend data has been scrubbed and cleansed, the fallouts from the mapping process are manually handled by the Sign-Off tools. These fallouts include items which could not be normalized at all, such as for commodity mapping where no descriptive information is available, as well as those for which the mapping could not be done with a high degree of confidence. The confidence level is a user-settable attribute; any mapping done with a confidence value higher than the user specified threshold is assumed to be correct and is automatically added to the master tables, while a mapping with lower confidence level is flagged for mapping via the sign-off tool. In these cases, the cleansing engine suggests a list of candidates, if possible, from which the user can choose one if appropriate. The user is also free to choose any other item which is not on amongst the candidates, as well as create an entirely new normalized item. The number of candidates suggested is again user defined. Figure 2 shows a snapshot of the Commodity Taxonomy sign off page of the application. In this figure, the entire normalized list of commodities (target taxonomy) is shown on the left side, while the list of unmapped commodities (source taxonomy) is displayed on the top-right side. Upon selection of an unmapped item to sign-off, the tool then displays a list of possible candidates on the right-middle side. The user can then choose one of the suggestions, or a name from the normalized list. In either case, the tool then displays a list of all the commodities that have been mapped to the chosen normalized name on the bottom right. The user can then sign-off the unmapped commodity to the chosen item, or can create a totally new commodity and map the unmapped item to that. Unmapped supplier names are similarly signed-off by the supplier sign-off tool. In the case of commodity taxonomy mapping, it is possible to map multiple commodities in the source taxonomy to multiple items in the target taxonomy. However, each supplier or transaction can be mapped only to a single normalized supplier or commodity, respectively. In all cases, it is possible to select multiple sign-off items and sign them off simultaneously, as long as they need to be mapped to the same item, or items, as the case may be. Independent of the sign-off process, the entire current normalized supplier and commodity lists can be reviewed using the respective mapping review tools. Figure 3 shows a snapshot of the supplier name mapping review tool. This allows any user to see all the items that have been mapped to any item in the normalized list. Thus, for example, Figure 3 shows all the different names and addresses that have been mapped to the normalized name, International Business Machines. By separating this from the sign-off process, the application allows many more users to manually verify any mapping and perform corrective action if necessary, than the small number of domain experts who would be authorized to do the sign-off activity. A third, independent activity consists or making changes to mappings that have already been done, either automatically by the tool, or manually by some user. Users are able to edit any mapping, and change to a more appropriate mapping if necessary. A user first deletes any incorrect mappings using the editing tools; the items for which the mapping is deleted then automatically become available within the sign-off application where they can then be assigned to the correct normalized items. After the sign-off/editing/review process has been completed, the system updates the master tables, generates the star-schema, and populates these tables so that the cleansed data can be analyzed by the spend analytics and report generation tool. Once the data has been cleansed, the procurement spend has to be aggregated and analyzed along various dimensions. We have developed a standalone spend analytics and reporting tool that offers a variety of slicing/dicing and drill-down capabilities. Thus, the spend can be viewed and analyzed by supplier, commodity, time-period, business and location, as well as drilled down to see spend for various commodities for each s upplier, or spend for each commodity with various suppliers, etc. Figure 4 shows a snapshot of a spend report displaying the amount of spend for various commodities. Besides the actual spend amount, additional statistics are also reported such as the number of invoices, number of invoice lines, etc. To drill down further, one can chose any particular commodity, and view a list of suppliers, and associated spend, for that commodity. Similarly, one can drill down to see spend for a commodity by time period, or business, etc. The dimensions along which the data can be sliced/diced as well the drilling capabilities are configurable via property files, and can be chosen based on the kind of data available. The tool also provides graphical views of the spend reports allowing a user to see, for example, a bar-diagram of the entire spend across the chosen dimensions. Moreover, an invoice-level view is also available which allows a user to drill down to the invoice level, and see the actual spend line-items for any given spend report generated using the tool. Besides the tree-view (shown in Figure 4) that enumerates the spend according to the chosen dimensions, the tool also has a query-view where a user can construct any query along the various dimensions to generate more specific and personalized spend reports. For example, one could use the query view to generate a report to show the list of suppliers who had each accounted for spend over a particular amount over a particular period (say, over US $1 million during 2004). The spend analytics and reporting tool is fully integrated with the data cleansing and warehousing engine. However, since it is a standalone application, it needs to be deployed on a client (user) machine as opposed to the cleansing and warehousing application that is deployed on a web server. As such, a web-based version of this tool is currently under development using DB2 Alphablox, thus providing very robust reporting and analytic capabilities that will be seamlessly integrated with the rest of the spend application to offer an end-to-end web-based spend analysis application. The spend analysis system has been built entirely around state-of-the-art, highly scalable IBM middleware software. IBM DB2 is used as the base platform for handling all the data. The data extraction and transformation activities are performed using IBM WebSphere Information Integrator. In conjunction with the IBM DB2 Warehousing Center, this allows data to be pulled in from a large number of datasource types, including relational databases, legacy systems, flat files, web sources, etc., and perform various transformations such as formatting, basic error checks and simple transformations. The cleansing, sign-off and reporting capabilities are all written in Java. The sign-off, review and cleansing tools are all deployed as portlets using the WebSphere Portal Server/WebSphere Application Server. Security is provided via LDAP using the IBM Directory. Additional access control is provided via database based authorization to allow users to access data only for those clients for which they have authorization. Additional role-based authorization is used to restrict users to certain kinds of tasks, such as cleansing, read-only or sign-off. The system allows multiple clients X  data to be cleansed and manipulated simultaneously, with multiple people doing multiple tasks on different or same datasets concurrently. Each step of the entire process can be executed manually on-demand, either by running scripts or from the web applications. Moreover, the system can be configured to run automatically on a schedule, whereby data would be automatically extracted from backend sources, transformed and formatted, supplier names and commodities mapped, and star schema generated. An alert would then be sent to designated users to launch the sign-off tools and complete the mapping process and update the master and star-schema tables, as well as analyze the cleansed data and generate reports. This paper describes an end-to-end spend analysis system that provides enterprises with a fast, reliable, accurate and on-demand way of cleansing transactional data and generating an integrated view of procurement spend. The development of an aggregated view of spend is increasingly becoming an extremely important and strategic activity as enterprises, especially large ones with multi-billion dollar annual procurement spend, are realizing that they are losing out on large amounts of savings by not having an aggregated view of their spend. On the other hand, enterprises are often finding that getting this view is extremely difficult, given that their transactional data resides in multiple, disparate systems with data generated by different sources, no means of cross-indexing the records and different commodity codes, if any, and/or descriptions to describe these transactions. This becomes even more difficult and critical as more and more businesses, such as IBM, take over the procurement practices of other organizations with the promise of generating savings for everyone involved by aggregating spend across all the parties. The spend analysis system described in this paper uses state-of-the-art software components for the basic infrastructure, and uses techniques from machine learning and information retrieval to build the core supplier normalization and commodity mapping engine. One important lesson learnt from this exercise was that the various techniques and algorithms in the literature were not directly effective on this problem. This was partly due to the fact that commodity mapping, in particular, often needs to be done on the basis of descriptions provides in invoices, purchase orders, etc. These descriptions are generally unstructured, consisting of words, phrases and numeric specifications. As such, approaches from the NLP and machine learning literature, for example, were not directly very effective. However, when we combined various approaches to come up with hybrid methods, we were much more su ccessful. While we have not yet carried out an exhaustive, systematic evaluation of the system, the results of deploying this system with several real customers are very encouraging. In cases where mapped data was available, both supplier normalization and commodity mapping was done with a very high (85-90% plus accuracy). However, most of the real world transactional datasets do not have mapped data available. For the cases where mapped data was not available, supplier normalization still performed very well. The accuracy (as evaluated by humans examining ra ndom subsets of the data) was still of the order of around 85-90% or more). However, commodity mapping results varied widely. While mapping of non-production level taxonomy items worked very well, production-level item mapping was fairly bad in some cases and moderate to acceptable in others. We are working on enhancing domain dictionaries, as well as enhancing the web-tools to allow human reviewers to enhance the domain dictionaries to improve the performance further. Regardless of the performance, two major benefits of the commodity mapping highlighted by human users were that their speed of mapping was greatly enhanced, and even if the tool was not entirely correct in picking up the correct taxonomy item, it was generally in the close vicinity of the correct mapping and that was very helpful for the human user (who did not have to search through large taxonomies to find relevant items). This system is currently in process of being incorporated into IBM X  X  BTO practice where it will be used to automatically cleanse client transactional data and aggregate the client X  X  spend with that of IBM. At the same time, we are enhancing the functionality of this tool on several levels, including the use of the Ascential (now IBM) enterprise suite to provide enhanced ETL functionality at the front end as well as adding more algorithms and a meta-learning framework at the cleansing level, thereby making it better at the mapping problem while allowing for automatic selection of algorithms as well as optimal parametric selection. [1] Baeza-Yates, R. and Ribeiro-Neto, B. Modern Information [2] Califf, M.E. Relational learning techniques for natural [3] Jaccard, P. The distribution of flora in the alpine zone, New [4] Joachims, T. Text categorization with support vector machines: [5] Levenshtein, V.I. Binary codes capable of correcting deletions, [6] McCallum, A. and Nigam, K. A comparison of event models [7] McCallum, A., Nigam, K., and Ungar, L.H. Efficient [8] Navarro, G. A guided tour to approximate string matching, [9] Nigam, K., Lafferty, J., McCallum, A. Using maximum [10] Salton, G., Buckley, C. Term weighting approaches in [11] UNSPSC, The United Nations Standard Products and Services [12] Zhang, T., Damerau, F., Johnson, D. Text chunking based on a [13] WordNet. A lexical database for the English language. 
