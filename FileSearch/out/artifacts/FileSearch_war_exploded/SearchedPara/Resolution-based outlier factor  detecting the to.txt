 Hongqin Fan  X  OsmarR.Za X ane  X  Andrew Foss  X  Junfeng Wu Abstract One of the common endeavours in engineering applications is outlier detection, which aims to identify inconsistent records from large amounts of data. Although outlier detection schemes in data mining discipline are acknowledged as a more viable solution to efficient identification of anomalies from these data repository, current outlier mining algorithms require the input of domain parameters. These parameters are often unknown, difficult to determine and vary across different datasets containing different cluster features. This paper presents a novel resolution-based outlier notion and a nonparametric outlier-mining algorithm, which can efficiently identify and rank top listed outliers from a wide variety of datasets. The algorithm generates reasonable outlier results by taking both local and global features of a dataset into account. Experiments are conducted using both synthetic datasets and a real life construction equipment dataset from a large road building contractor. Comparison with the current outlier mining algorithms indicates that the proposed algorithm is more effective and can be integrated into a decision support system to serve as a universal detector of potentially inconsistent records. 1 Introduction Automation and sensing techniques for data collection and real-time monitoring, coupled with various information management systems greatly simplified many of the routine works, which used to be tedious and time-consuming in engineering disciplines. Examples include bridge health monitoring systems [ 8 ], weight-in-motion monitoring systems in transporta-tion [ 21 ], construction equipment status monitoring systems in construction [ 24 ], as well as various environmental monitoring systems. Nevertheless, appearance of anomalies in the data collection either negatively undermines the reliability and usability of the systems if the outliers are noisy records, or positively measures how effectively the system can perform by alerting anomalous conditions if the outliers are indicators of status change. The term  X  X utlier X  can refer to any single data point of dubious origin or disproportionate influence. Commonly, given a set of observations X, an outlier is an observation that is an element of this set but which is inconsistent with the majority of the data or inconsistent with a sub-group of X to which the element is meant to be similar . The above definition has two implications: outlier vis- X -vis the majority; and outlier vis- X -vis a group of neighbours. Whether it is an interesting contaminant or dubious data entry, an outlier is often considered noise, which can have a harmful effect on statistical analysis or pattern identification.

Attempts have been made to remove the noisy data using various outlier detection tech-niques; in one example, Raz et al. [ 21 ] designed an expert system to automatically detect unlikely vehicles and erroneously classified vehicles from weigh-in-motion data. In contrast to being classified as noisy harmful data, some relevant outliers contain important information on system malfunction, mismanagement, or even abnormal phenomena (for instances, envi-ronmental change, natural disaster), which should be detected for further investigation rather than discarded. In either of these two cases, the inconsistent records should first be identified as much as possible from the dataset. Data validation (range validation, single variate pattern validation etc.) using structural query language (SQL) can only filter out a small portion of outliers. Traditional statistical approaches including uni-and multi-variate outlier detection are not applicable due to their pre-assumption of certain statistical distributions, which may not exist for datasets containing multiple clusters.

Outlier mining techniques in data mining seem to be a more viable solution for outlier detection in engineering applications, because of their relaxed conditions for application. Some of the popular algorithms include among others a distance-based outlier mining algo-rithm by Knorr and Ng [ 13 ]; a local outlier mining algorithm by Breunig et al. [ 1 ]anda connectivity-based mining algorithm by Tang et al. [ 22 ]. However, these algorithms are not yet widely accepted in civil engineering disciplines because they do not cater to the special features of engineering datasets, including:  X  Current outlier mining algorithms need domain-dependent parameters, such as the number  X  Current outlier mining algorithms cannot find outliers effectively from datasets containing  X  Current outlier mining algorithms are capable of mining either global or local outliers;  X  In engineering applications, there exists a need for ranking the top-listed outliers based In this paper, we present a resolution-based outlier (RB-outlier) notion and an associated outlier detection algorithm efficient for engineering applications. The RB-outlier notion is proposed based on a nonparametric clustering algorithm called TURN* by Foss and Za X ane with the same idea of resolution change [ 4 ]. The proposed algorithm can detect and rank top-n outliers from any kind of datasets without the need for input parameters.
We also compared RB-outlier with distance-based outlier (DB-outlier) and Local density based outlier (LOF-outlier) mining algorithms using both synthetic datasets and a construc-tion equipment dataset from a large road building contractor. Our experimental results show that our RB-outlier mining algorithm generates equivalent or better results than the other two competitive algorithms on all the datasets while benefiting from the absence of input para-meters; the RB-outlier results seem to combine the results from both DB-outlier which tends to look for global outliers and LOF-outlier which tends to look for local outliers. Analysis on the detected outliers from the construction equipment datasets shows that these combined results make more sense for engineering data.

The flexibility and robustness of the proposed algorithm allow it to be easily built into any real time monitoring system or decision support system for efficient outlier detection  X  X n the fly X .

Our contributions through this novel RB-outlier notion and outlier mining algorithm are as follows:  X  A novel resolution-based outlier definition which can be used for measuring the degree  X  A nonparametric outlier mining algorithm for efficient detection and ranking of outliers  X  We demonstrate through experimental results that the outliers identified by our  X  We propose a means to identify the number of the most significant outliers as opposed to  X  RB-outlier mining algorithm can improve the clustering results by effectively filtering The remaining of the paper is organized as follows: Sect. 2 covers related work in outlier mining research, particularly the issue of ranking outliers; Sects. 3 and 4 introduce the resolution-based outlier notion and RB-outlier mining algorithm respectively. A comparison of results from RB-outlier and two contenders, DB-outlier and LOF-outlier is given in Sect. 5 followed by the summary of our experimental results and analysis in Sect. 6 . Section 7 explains how the RB-outlier notion and algorithm benefit outlier mining in engineering applications; Sect. 8 summarizes and concludes the paper. 2 Related work Hawkins defines an outlier as  X  X n observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism X  [ 7 ]. Traditionally outlier detection in engineering disciplines depends on statistical approaches. In a uni-variate case, the outliers are detected based on the deviation of objects from the assumed data distribution: i.e. after fitting a data series into a bell-shaped statistical distribution (e.g. normal distribution), those data points located far away from the mean (e.g. 3 standard deviations) are deemed outliers; other commonly used techniques are quartile methods and visualization methods using scatter plot, etc. Yet in most engineering applications, rather than a single attribute, it is usually the combination of multiple attributes that makes the records stand out from their major patterns of the dataset.
Multivariate outlier detection is known as an important but challenging task in statis-tics because most statistical methods make assumption about the data distributions. Most approaches need to handle joint distributions, which are more difficult to handle statistically. For example, many methods involve calculating Mahalanobis X  X  distance for each observation using robust estimators of the covariance matrix and the mean vector [ 18 , 19 ]. Mahalanobis X  X  distance is defined as: where V  X  The covariance matrix between attributes X i  X  X he i th observation u  X  Mean vector This definition measures the distance of each point from their gravity centre taking the covariance among attributes into account. When the covariance equals 1, the distance becomes the Euclidian distance of the data point from the gravity centre of the dataset. The data points far away from the centre are classified as outliers.

Other approaches in statistics try to break the multivariate detection problem into a set of uni-variate detection problems by projecting data attributes into some directions [ 5 ]. However, these methods have limited applications because they provide correct solutions only when the outliers are located close to the direction of the principle component [ 19 ].
The common problem with statistical methods for multivariate outlier detection is their assumption of distributions, which is difficult, inaccurate, or even impractical, especially for contaminated cases (with mixed data from normal classes and contaminated classes). Their inability of handling high-dimensional data is another well-cited problem with statistical approaches. These problems prevented their use for outlier detection in modern engineering applications due to the data complexity and sheer data volume in an engineering system. More robust, efficient solutions have been provided by data mining-based approach in recent years.

A distance-based definition of outliers was first proposed by Knorr and Ng. They intro-duced DB-outlier to identify outliers from a large database (i.e. with high dimensions and high data volume) [ 13 ]. A DB-outlier is defined as follows:  X  X n object O in a dataset T is considered a DB( p,D )-outlier if at least a fraction p of the objects in T lies greater than distance D from O  X . The authors claim that this definition generalizes the notion of outlier defined in statistical tests for standard distributions. DB-outlier tends to find outliers in global context, which means some important outliers deviated from their local clusters are probably missed if a large number of isolated points or loosely packed clusters appear. While Knorr and Ng X  X  definition is distribution free, it lacks a mechanism to rank outliers. Without viola-ting the original notion of DB(p,D) outlier, Ramaswamy et al. further propose to rank each point based on its distance to its k th nearest neighbour, and use a partition-based algorithm to efficiently mine top-N outliers from a large database [ 20 ]. For them, the outliers are the top n data elements whose distance to the k th nearest neighbour is greatest . This definition also eliminates the need to estimate an appropriate distance D.
 Another popular algorithm is a local density based outlier-mining algorithm proposed by Breunig et al. [ 1 ]. A local outlier factor (LOF) is assigned for each object with respect to its surrounding neighbourhood. The LOF value depends on how the data points are closely packed in its local reachable neighbourhood. These points deep inside a dense cluster have a LOF value of approximately 1 while the isolated points have a much higher value. The authors claim that this definition also catches the spirit of the outlier definition given by Hawkins [ 7 ]. The local outlier notion seems more reasonable than DB-outlier because each data point can be measured with a numerical factor based on how the data is deviated from its genuine cluster. Therefore the outliers can be ranked as per their LOF values.
Tang et al. improved to some extent the LOF definition by using their connectivity-based outlier factor (COF) for a dataset containing low density patterns. In such a case, LOF would not be effective to measure the density of an outlier with respect to its sparse neighbourhood [ 22 ].

Jin et al. [ 10 ] proposed a pruning method for efficiently mining top-n local outliers from large databases.  X  X icro-clusters X  are defined to remove these objects which cannot possibly be top-n outliers.

A comprehensive review, experiments and comparisons of above outlier mining schemes are conducted by Tang et al. [ 23 ]. The authors evaluated existing outlier mining algorithms using precision and recall, rank power and an implementation-free metric on a number of datasets.

The biggest hurdle of effectively applying these afore-mentioned outlier-mining algo-rithms in the engineering domain is the determination of their input parameters. All the para-meters should be either known a priori or estimated and optimized by trial-and-error with the help of domain experts. In particular, LOF is very sensitive to its parameter  X  X inimum number of nearest points X  (MinPts) and DB-outlier results vary greatly when distance (D) and percentage (p) change. Subjective results make it difficult to implement these outlier-mining algorithms for outlier detection in engineering applications. 3 Resolution-based outlier TURN* is a nonparametric clustering algorithm introduced by Foss and Za X ane [ 4 ]. The optimum clustering of a dataset can be obtained automatically based on resolution change: when the resolution changes on a dataset, the clusters in the dataset redistribute. All the objects are in the same cluster when the resolution is very low, meanwhile every object is a single cluster when the resolution is very high, and therefore the optimum clustering can be achieved at a point between these two extreme scenarios. The  X  X URN-CUT X  technique was introduced to detect this critical point during the resolution change by looking for a plateau in the curve of the differential of some collected cluster statistics [ 4 ].

The same observation holds when viewing all the objects in the dataset from the perspective of outliers. All the objects are outliers when the resolution is high enough to warrant no neighbours (by distance measure) for any objects in the dataset; meanwhile all the objects are  X  X nliers X  when the resolution is low enough to have all the objects close-packed in a single cluster. If the resolution of a dataset changes, different outliers demonstrate different clustering-related behaviours during resolution change; those objects more isolated, with less neighbours, and far away from large data communities are more liable to be outliers. On the other hand, the top outliers will be merged into a cluster later when the resolution is decreased. As a result, the accumulated cluster-related properties collected on one object can be used to measure its degree of outlying-ness relative to its close neighbourhood and community (reachable neighbourhoods). We first define the neighbourhood of an object: Definition 1 Neighbourhood of object O :
If an object O has a nearest neighbouring points P along each dimension in k-dimensional dataset D and the distance between P and O is less or equal to 1, then P is defined as the close neighbour of O, all the close neighbours of P are also classified as the close neighbours of O, and so on. All these connected objects are classified as the same neighbourhood.
 The threshold value is taken as 1 to measure whether two points are close enough to become neighbours. The absolute value of this threshold is not important because the pair-wise distances between points are relative measurements during resolution change. The algorithm finds the maximum resolution S max at which all the points are far enough from each other to be non-neighbours, and the minimum resolution S min at which all the points are close enough to be neighbours.
 Secondly we define the resolution-based outlier factor for each object: Definition 2 Resolution-based outlier factor (ROF)
If the resolution of a dataset changes consecutively between maximum resolution where all the points are non-neighbours, and minimum resolution where all the points are neighbours, the resolution-based outlier factor of an object is defined as the accumu-lated ratios of sizes of clusters containing this object in two consecutive resolutions. where r 1 , r 2 ... r i ... r R . are the resolutions at each step, R is the total number of resolution change steps from S max to S min , ClusterSize ( O , r ) is the number of objects in the cluster containing object O at a resolution r . r 0 is the state before the resolution scaling begins. At that stage all cluster sizes are 1 (i.e. 1 point) and the ROF of all points is 0.
As a simple example to illustrate the idea, suppose a token dataset with 5 points as illustrated in Fig. 1 . We can observe that point 1 is the furthest outlier followed by point 2. The figure illustrates the clustering at 4 different resolution levels. Initially, in R1 all points are separate clusters. At the next resolution R2 points 3, 4 and 5 form a cluster while the others remain separated. At resolution R3 point 2 also merges with the existing 3-point-cluster. Finally, at resolution R4 all points merge in one cluster. Table 1 shows the cluster size for each point at all resolutions and the calculation of the ROF. Ranking those shows point 1 as the most outlier followed by point 2 then the three other points as expected (Table 2 ).
At each resolution, we cluster the points based on the distance between every two objects and the neighbourhood definition. The time complexity of clustering at each resolution is O (NLogN) as demonstrated in [ 4 ]. The cluster size of each object is set to 1 at the beginning (i.e. at Max. resolution), then the cluster size increases for the object whenever the object gets merged, or the cluster containing the object merges other objects at the next lower resolution.
The cluster size at the previous resolution is reduced by 1 for ease of comparison, which will set the ROF of an object to 0 before the object gets merged. Top n outliers are the n elements with the lowest ROF values.

Figure 2 illustrates the resolution-based outliers in a simple 2D dataset, which contains four clusters C1, C2, C3, C4 and a number of outliers. Let X  X  take a look at how ROF can measure the degree of outlying-ness for each outlier based on their clustering behaviour during resolution change. First of all, the most isolated objects tend to get smaller ROF values. For outliers in the top list, ROF stays at zero until the object is merged into a cluster. The later it is merged, the smaller its ROF value is, the last merged has the lowest ROF. This observation shows ROF reflects the general density distribution of objects and guarantees the sparse objects gets lower ROF values; secondly, objects with enough neighbours as well as those affiliated with large size clusters (C1, C2) increase their ROF values (approximately equal to 1) faster than smaller, isolated clusters (C3, C4). Finally, this definition can measure the degree of outlying for an object against its genuine cluster. This can be explained by comparing O1 with O2 in Fig. 2 . At the previous resolution r i  X  1 , the ROF values are zero for both points since they are still isolated. If O1 is merged with C1 (with 71 objects) while O2 is merged with C2 (with 61 objects), then the ROF increase at r i is added to the ROF for both of them. The increase for O1 is larger than for O2 because it is affiliated with a larger cluster (C1 has more objects than C2); therefore, O2 is a stronger outlier than O1. This is in coincidence with the fact that O2 deviates from a smaller cluster C2, more suspicious (with higher outlier score) than O1 deviating from a larger cluster C1.

Universally defining an outlier is somewhat controversial and is often subjective, relative to the application at hand. While considering locality and globality, our definition still embodies the essence of the accepted definition given by Hawkins [ 7 ] with regard to deviation from other observations. 4 Resolution-based outlier mining algorithm Using the definitions of close neighbourhood and ROF in the previous section, a resolution-based outlier detection and ranking algorithm (RB-MINE) is proposed for mining top-N outliers in a dataset with multiple numerical attributes. The implementation procedures are presented in a flowchart in Fig. 3 .

The resolution change ratio r is a percentile value used for computing the resolution change step size. The reason why this parameter is not considered an input parameter for the nonparametric clustering algorithm TURN* is explained in [ 4 ]. The same argument can be applied here to the RB-outlier mining algorithm.

Our experimental tests show the resolution change ratio, so long as it is kept in a moderate range, has a minor effect on the outlier results. Theoretically the smaller the resolution change ratio r, the better outlier mining results can be achieved, as a matter of fact, this improvement is not significant, while the hit to algorithm performance is because more steps are required to reach minimum resolution. This disproportional improvement to the outlier mining results with smaller resolution steps can be explained by the fact that most outliers are located far away from the major patterns (most inliers will be merged after a few steps of resolution change). As a rule of thumb, in case of stringent requirements for outlier ranking, the ratio should be such a value that the number of resolution change steps is larger than the number of outliers to be detected; generally the user does not need to spend much effort to fine-tune this parameter. We found in our extensive experiments that setting this resolution change between 4 and 10% gave good results for a wide range of datasets. Moreover, this ratio need not be static. It can decrease dynamically starting with a large step declining to smaller steps progressively.

The finalized RB-outlier algorithm is comprised of a subroutine algorithm RB-CLUSTER in Fig. 4 , and an overall algorithm RB-MINE in Fig. 5 . RB-CLUSTER clusters objects in the dataset at each resolution step while RB-MINE steers the mining process through the change of resolution and collection of ROF for each object. 5 Comparison of RB-outlier with DB-outlier and LOF-outlier One of the primary factors determining the outlier mining results is the outlier notion, which describes what an outlier is and assigns it a measuring factor, if it is possible.
For examples, DB( D , p ) outlier evaluates the inconsistency of an object by judging if there are sufficient number of objects within D (or the distance to its k th nearest neighbour is closer than D, where k is the (1  X  p ) percent of the total number of objects), therefore the algorithm searches for the specified number of nearest points in a global context; LOF-outlier measures the degree of outlyingness by taking only a restricted neighbourhood into account, LOF varies depending on how an object is deviated from its  X  X est-guess X  cluster in a local context.
 The RB-outlier notion measures how an object deviates from its close neighbourhood. The definition of neighbourhood implies that this neighbourhood actually includes a series of chained neighbourhoods (we call it a  X  X ommunity X ), as such, RB-outlier measures an object against its degree of outlyingness by taking both  X  X lobal X  and  X  X ocal X  features into account.

The following table compares the RB-outlier with DB-outlier and LOF-outlier from the different perspectives of outlier notion, algorithm, implementation and results. 6 Experimental results To validate the RB-outlier mining results and compare with those detected by DB-outlier and LOF-outlier mining algorithms, we implemented all the three algorithms in the same C++ development environment, and conducted experiments on a number of synthetic datasets as well as a real life construction equipment dataset obtained from a large road building contractor. This section summarizes our experimental results and comparative analysis on a token 200-tuple 2D synthetic dataset, a 10,000-tuple 2D synthetic dataset, and a 1,033-tuple 3D construction equipment dataset. 6.1 A synthetic 2D dataset of four clusters with a total number of 200 objects The token dataset shown in Fig. 6 contains four distinct clusters and 20 outliers. This dataset, used in many clustering research projects for validating clustering results (e.g. [ 2 ]) can be equally applied for visual validation of outlier detection.

The resolution change ratio is set to 10% to observe the merging process. In the first round of data scaling and resolution change, the ROF values of all the objects are zero because the cluster size at the beginning is 1. With the changing of resolutions, those objects close to each other are merged into clusters and start to increase their ROF values, those isolated objects will be merged in the last few steps of resolution change. Table 3 shows the last few steps in which the ROF values of relatively lonely objects change to rank the top outliers.

To evaluate the influence of the resolution change ratio on the mining results, we set the change ratio to 30, 20, 10, and 5% separately and compared the results. Table 4 shows the the objects are quickly merged but the results are not satisfactory. The outliers cannot be identified and their degree of outlying cannot be compared effectively. When r is set to a smaller value, the results are improved. However, further decrease of r, such as from 10 to 5% would not improve the results, but adversely would compromise the efficiency.
We also ran the three algorithms separately on this dataset, each identifying the top-10 and top-20 outliers and marking up these points in the density plot. Figure 6 summarizes the top outliers detected by the three algorithms.

Among the top-10 outliers, eight out of the top-10 outliers are the same with the three algorithms; visual judgment on the density plot confirms the  X  X utlyingness X  of the identified objects and the eight objects voted unanimously as outliers are truly isolated as compared with others. This finding indicates that all the three algorithms can effectively find outliers from this dataset. However, there are some differences between the three sets of results: 1. Though top-3 outliers are exactly the same in the three sets of results, the other five 2. The outliers No. 4 and 5 in LOF-outlier results are interpreted differently by DB-outlier The afore-mentioned conclusions are further enhanced by increasing the number of outliers in the top list. For example, if we look at the top-20 outliers in Fig. 6 , all the top-20 outliers are detected by all the algorithms, RB-outlier shifts the unanimously identified outliers to higher rankings in its top-20 list.

Precision and recall are two measures in information retrieval that could be used as criteria for evaluating the outlier results. Precision is defined as the percentage of correct outliers in the set of detected outliers (i.e. the total number of correctly detected outliers divided by the number of labelled outliers); recall is the percentage of all known outliers correctly detected (i.e. total number of correctly detected outliers divided by the total number of existing outliers). For top-20, Precision and Recall are the same and all three algorithms achieve 100%. However, scrutinizing at a lower n for top-outliers, RB-outlier always has better or equivalent precision and recall. 6.2 A synthetic 2D dataset containing clusters of different features with a total number This dataset is also widely used for visually validating clustering results [ 4 , 12 ]. The dataset contains nine clusters of difference shapes and significant noise. This experiment aims to verify the capability of RB-outlier to distinguish outliers from  X  X nliers X  in a relatively large set and in the presence of clusters of arbitrary shapes.
 In this experiment, we compared the top-200 outlier results identified by LOF-outlier and RB-outlier respectively, as marked up in Fig. 7 by  X  o  X  X r X  x  X . Though both identify similar objects as outliers in this dataset, a detailed observation reveals that LOF-outlier biases objects in locally sparse areas, and RB-outlier takes local scarcity and global scarcity into consideration when picking up outliers. The LOF algorithm assigns a higher outlier factor for the object whose local density is lower than its neighbours. Therefore, when the number of outliers to detect is increased, LOF-outlier marks the objects at the edge of dense clusters as outliers, while RB-outlier tends to pick up outliers from isolated objects. RB-outlier X  X  performance is indeed the sought for behaviour in construction engineering data. RB-outlier mining algorithm can detect outliers with relatively higher degree of precision based on visual inspection. Indeed LOF finds outliers that are in some cases too close to real clusters, while RB-Outlier X  X  outliers are more spread. Comparing rankings (such as relevance-based ranking) is still an open problem in databases. The point of the figure with this known dataset is to demonstrate that the accuracy of our approach is still correct with large data since no cluster point was labelled as outlier. 6.3 A 3D construction equipment management dataset To test-drive the RB-outlier mining algorithm and validate its usefulness in engineering datasets, we conducted experiments on a 3-D construction equipment dataset obtained from a large building contractor. The dataset includes characteristic attributes for 1,033 pieces of equipment in the contractor X  X  equipment fleet. Three numerical attributes are defined for each unit in addition to the identify attribute: the yearly repair/maintenance cost (yearly cost), the rate of charge, and the age, as described below:  X  Yearly repair/maintenance cost ( in C$ per year ) : this attribute sums up the total cost of  X  Rate of charge ( in C$ per hour ) : this is the rate (updated on a yearly basis) set by the  X  Age ( in number of years ) : The equipment yearly cost to the owner is also loosely related The 3D dataset is visualized in Fig. 8 . The objective of the experiments is to identify top listed inconsistent units from the dataset. The inconsistency of these units indicates the anomalous combination of the three attributes for a unit with respect to its similar equipment sub-group;
Using DB-outlier, LOF-outlier and RB-outlier algorithms, we identify the top-20 outliers from the selected equipment fleet and summarize their individual results in Table 5 .
A comparison of the three sets of outliers determines that 11 out of 20 are unanimously identified as outliers by the three algorithms in their top-20 lists. LOF-outlier generates very similar results to DB-outlier with 16 being the same out of the top-20. This is not surprising because a large number of isolated objects appear in this dataset (see Fig. 8 ). LOF definition becomes similar to the DB-outlier notion for a sparsely distributed dataset: both start by looking for minimum number of neighbouring objects. Two additional units are identified as outliers by both LOF-outlier and RB-outlier; but no addition outlying units are identified by both DB-outlier and RB-outlier besides the 11 common units.

RB-outlier moves all the 11 common units to higher rankings and adds 6 new units in the top-20 outlier list. Analysis of the 6 added units against their individual equipment sub-group and the entire fleet confirmed their interestingness. For example, unit #138 X 405 is a heavy duty concrete float, a slice of data on this equipment sub-group finds out that its yearly cost is significantly higher than other similar units at comparable rental rate and age.
The inability of identifying some outstanding outliers by DB-outlier and LOF-outlier mining algorithms can be illustrated from their outlier notions: these two algorithms evaluate the degree of outlying by drawing a hyper-sphere around each object; and the number of objects inside the hyper-sphere influences the outlier measurement of the object. The outliers harboured inside the concave of a cluster can not be identified efficiently based on this rational, subsequently some outliers become missing in the results if the two algorithms are applied on a real life dataset which may contain clusters of any arbitrary shape.

Among the top-20 outliers, Unit# 505-401, a soil cement plant (300 X 600 TPH), is detected as No.1 outlier by DB-outlier and No. 2 outlier by LOF-outlier, but this unit is not identified as an outlier by RB-outlier. Detailed analysis of this unit finds out it is the only unit in this equipment class, therefore it is not a  X  X rue X  outlier in the application. No. 16 (Unit#505-405: Portable conveyor batch) and No. 17 (Unit#505-575: soil cement plant) outliers in top-20 of DB-outliers are also the only units in their respective equipment class; No. 18 (Unit#117-700: 6-wheel Truck with drill) of LOF-outliers is identified as outlier because it is in a small 2-object cluster. These outlying units in DB-outlier and LOF-outlier results are of no particular interests; their appearance somehow degrades the general quality of the top-N outlier mining results. On the contrary, all the 11 common outlying units, the three found by RB-outlier and by another method, and the six additional units unique to the RB-outlier results are indeed inconsistent units relative to their individual equipment sub-group. If we consider the 11 common units as  X  X rue X  outliers, and look at the top-20 or less in the outlier results, the recall is generally better for RB-outlier when we compare the three algorithms.

Execution time is not reported or discussed because all the three algorithms are compa-rable. While LOF uses an index structure like R * trees [ 17 ] to find nearest neighbours, this index structure collapses with high dimensional spaces and the time complexity of all three methods becomes in the order O ( N 2 ) when the dimensionality is above 15 or 20 dimensions. In those cases, outlier analysis can benefit from linear approaches approximating k -nearest neighbours such as [ 6 , 14 , 15 ]. 7 Determining the number of outliers in a dataset using RB-outlier mining algorithm Looking for the top n outliers will always return the n data points with the highest score based on the ranking used, whether it is our ROF or even LOF or distance-based. The parameter n is simply provided as input. However, while in some applications the number of outliers sought is known, in some other applications n is unknown or difficult to guess, and a legitimate question would be  X  X hat are the most significant outliers? X  i.e. what is n ? To answer this question we suggest a method inspired by TURN* X  X  differential technique to find the optimal clustering resolution [ 4 ].

The idea is to process the ROF of each data point then sort the data points based on their respective ROF. We scan the sorted list of points and compare the ROF of two adjacent data points. If the differential of the ROFs is significant then we stop and designate the ranking of the current data point as n . The significance of the differential can either be determined by a threshold provided by the user or simply based on statistical significance tests such as Chi-square or other.

If we take the example with 200 data points illustrated in Fig. 6 ,weknowwehave20 obvious outliers. Sorting these 200 points by their ROF produces the graph in Fig. 9 in which we can clearly see that the ROF increases significantly after 20 and later plateaus for those points that are within clusters. This artificial dataset contains 20 outliers which deviate significantly from the clusters. This fact makes it easy to determine n as 20 based on the plateau. Theoretically the number of outliers can be determined for the other dataset where outliers deviate significantly. For noisy datasets, determining an optimal or appropriate n is an open problem. In general, domain knowledge would dictate a suitable n . 8 Benefits of RB-outlier mining in engineering applications Outlier mining has wide applications in the engineering discipline but special features of engineering data constitute specific challenges to effectively detect outliers using traditio-nal statistical approaches or current outlier mining algorithms. The resolution-based outlier mining algorithm we proposed in this research caters to the special needs of engineering applications from the following perspectives: 8.1 Mining results Equivalent or higher precision/recall: test results on both synthetic datasets and real-life engineering datasets prove that the RB-outlier mining algorithm generates equivalent or better results when measured in precision/recall. The general approach of measuring  X  X loseness X  by drawing a hyper-sphere around each object used by DB-outlier and LOF-outlier mining algorithms may lead to failure of detecting the outliers harboured inside the concave of a cluster. On the contrary, RB-outlier mining algorithm can effectively detect outliers in a dataset containing clusters of arbitrary shapes by taking both local and global features of a dataset into consideration.
 Better ranking of top-n: RB-outlier mining algorithm shifts the commonly detected out-liers by DB-outlier and LOF-outlier mining algorithms to higher rankings according to our test results. The top-n ranking generated by RB-outlier is more persuasive in engineering applications. 8.2 Implementation Robust: RB-outlier mining algorithm fits into a wide variety of datasets due to being non-parametric. This makes it suited for integration with decision support systems in engineering. Scalable: RB-outlier mining algorithm is scalable for higher dimensions with some perfor-mance hit; however, this can be improved by pre-processing, such as feature reduction using a decision tree [ 11 ].
 Real-time: A large number of civil engineering applications in need of outlier detection are real-time systems that are time-sensitive and condition-variant. RB-outlier mining algorithm can detect changes  X  X n-the-fly X  without human interference.

Another important application of the resolution-based algorithm is noise reduction. Two important features of engineering data are large amounts of noise and not well-segmented clusters. Removing noisy records from the data repository helps improve data quality for better identification of common patterns, example applications include clustering image data generated from Geographical Information Systems [ 16 ]; system fault diagnosis, bridge design [ 3 ]; Though the current outlier mining algorithms can be used for the same purpose, RB-outlier mining algorithm is more effective in consideration of its capability of exhausti-vely detecting the top listed outliers and effectively ranking them. 9 Summary and conclusions There is much debate regarding what to do with extreme or influential data points, whether one should bulk filter them to improve further data analysis, or pinpoint them to better understand and remedy errors in a system. It is clear that in engineering applications, effectively detecting the most relevant outliers is crucial for enhanced systems. Hence, outlier mining becomes an increasingly important task for analysing engineering data. Outlier mining provides unprecedented advantages for detecting inconsistent records from a large database, which could not be possibly accomplished with traditional statistical techniques. Nevertheless the current popular outlier mining algorithms, when applied for such a purpose, are neither efficient nor effective because of their domain-dependent parameters. The outlier notions in the current algorithms are targeted at either global or local outliers, but in engineering data, the data is typically noisy and clusters in the dataset are not well bounded as in the synthetic datasets used by some authors. At the same time, the current outlier mining algorithms cannot efficiently detect outliers from a dataset containing clusters of arbitrary shapes, such as in the engineering data.

The resolution-based outlier definition and the nonparametric RB-outlier mining algorithm introduced in this paper are more suited to engineering data when compared with the popular DB-outlier and LOF-outlier schemes. The algorithm overcomes the problems stated above and can be used for robust outlier detection in a wide variety of multi-dimensional datasets in engineering applications.

The ability of mining top-n outliers based on ROF provides a mechanism of ranking by the degree of  X  X nterestingness X  for engineering data. Problems in the records can be identified by looking at the top listed outliers for further investigation. The outlier mining technique we propose is expected to eliminate the burden of domain experts spent on estimating and tuning unknown parameters.

In the presence of very high dimensional spaces, where data structures for exact k -nearest neighbours collapse into linear scans of the data (or worse) [ 9 ], outlier detection and ranking becomes inefficient but remains effective. For efficiency sake (and often feasibility), approxi-mation of the k -nearest neighbours is recommended. Many approaches exist [ 15 ] and can be adapted to RB-MINE. However, it is worth investigating the extent of error in ranking and in detection due to these approximations. Different approximations could have singular impact on the outlier detection accuracy and the ranking suitability.
 References Author biographies
