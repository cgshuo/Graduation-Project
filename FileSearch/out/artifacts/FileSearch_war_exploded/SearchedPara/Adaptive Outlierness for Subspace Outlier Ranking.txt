 Outlier mining is an important data analysis task to distin-guish exceptional outliers from regular objects. However, in recent applications traditional outlier mining approaches miss outliers as they are hidden in subspace projections.
In this work, we propose a novel outlier ranking based on the degree of deviation in subspaces. Object deviation is measured only in a selection of relevant subspaces and is based on adaptive neighborhoods in these subspaces. We showthatourapproachoutperformscompetingoutlierrank-ingapproachesbydetectingoutliersinarbitrarysubspaces. Categories and Subject Descriptors: H.2.8 Database management: Database applications [Data mining] General Terms: Management Keywords: data mining, ranking, outliers, subspaces
Outlier mining has become an important data mining task to detect inconsistent or suspicious objects in large databases. For recent applications, outlier mining as an un-supervisedlearningtaskisimportantforconsistencychecks ofsensornetworkmeasurements,frauddetectioninfinancial transactions,emergencydetectioninhealthsurveillanceand many more. As measuring and storing of data has become verycheap,inalloftheseapplications,objectsaredescribed by many attributes. However, for each object only subsets of relevant attributes provide the meaningful information, theresidualattributesareirrelevantforthisobject. Forex-ample in health surveillance, for one patient attributes such as X  X ge X  X nd X  X kin humidity X  X ight be important to detect the abnormal  X  X ehydration X  status of this patient. Other attributes such as  X  X eart beat rate X  are irrelevant for the detection of this outlier, but are relevant for the detection of abnormal patients with a heart disease. All of these at-tributes are required for some outlier detection, but each outlier occurs only in subsets of these attributes. Thus, the distinction between outliers and regular objects is heavily hindered by considering all available attributes as typically done in traditional outlier mining methods.

Traditionaloutlierrankings,e.g.basedonthelocaldegree of deviation for each object as in the well established local outlier factor (LOF) approach [2] or its extension (ABOF) [4], provide for each object the extent of outlierness .How-ever, traditional outlier rankings using outlierness measures in full space are not appropriate for outliers hidden in sub-spaces. Inthefullspaceallobjectsappeartobealikesothat traditional outlier rankings cannot distinguish the outlier-nessofobjectsanymore. Incontrast,recentapproachescon-sider subspaces for outlier ranking. While some approaches consideronlyonesubspacespannedasahyperplanebyaset of reference points (SOD) [3], others are based on subspace cluster properties as indicators for outliers (OutRank) [6]. However, none of the proposed subspace methods consid-ers a meaningful selection of relevant subspaces, while they all ignore the incomparable deviation of objects in different subspaces for their outlierness measures.

In this work, we focus on key properties for outlier rank-ing in subspaces. In relevant subspaces the neighborhood of an object is clustered and the object is an outlier if it deviates from these clustered objects. In case these rele-vantsubspacesinhabitdifferentclustersintermsofobjects, some objects might be members of a cluster in one sub-space while being outliers in others at the same time. In contrast,inirrelevantsubspacestheneighborhoodofanob-ject is distributed uniformly random such that all objects seemtobeoutliers. Furthermore,objectdeviationincreases with the number of attributes in a relevant subspace. As distances between objects grow more and more alike due to the X  X urse of dimensionality X , objects are clustered in dense regionsinonedimensionalsubspaceswhileobjectsarescat-teredinhigherdimensionalspaces. Ingeneral,thedeviation of objects is highly influenced by the number of attributes in the considered subspaces [1, 5]. Thus, for outlier ranking in subspace projections, we have to cope with two major challenges:
To tackle these challenges, we propose OUTRES anew methodforoutlierrankinginrelevantsubspaceprojections. Weproposeanoveladaptiveoutlierrankingmeasuringcom-parable degrees of deviation for objects in arbitrary sub-spaces. We define outliers to be objects highly deviating fromtheestimateddensityintheirlocalsubspaceneighbor-hood.
Ourgeneralideaistomeasurethedeviationofeachobject inasetofrelevantsubspaceprojections. Incontrasttoother subspace outlier ranking approaches, we consider for each objectitsdeviationinmultiplesubspaces. Furthermore,our outlier ranking measures deviation in subspaces and adapts to the number of attributes in each considered subspace.
In general, the aim of outlier ranking is to provide a sort-ing of all objects  X  giveninadatabase  X  X  X  . Technically, one ranks according to the degree of deviation measured by a ranking function  X  :  X  X  X   X  . The ranking function provides a real valued measure of the objects X  outlierness. Ranking functions can be defined arbitrarily based on the object X  X  features  X  =(  X  1 ,..., X   X  ). In contrast to traditional approaches that measure the degree of deviation in the full  X  -dimensional space  X  = { 1 ,..., X  } , we measure deviation in subspace projections  X   X   X  . Thus, we ensure to find outliers hidden in arbitrary subspace projections.
Thegeneralchallengeforoutlierrankingapproaches,isto provideameaningfulrankingfunctionwhichachievestodis-tinguish between an outlier object  X  and a regular object  X  by providing a clear distinction:  X  (  X  )  X   X  (  X  ). However, tra-ditional outlier ranking functions fail for outliers hidden in subspaces as they provide for all objects very similar rank-ing values  X  (  X  )  X   X  (  X  )  X   X , X   X   X  X  X  . Although outliers do not show up in full space, they deviate in subspace projec-tions. Thus, we cope with the curse of dimensionality by considering the outlierness of each object in a selection of relevant subspaces. This set of relevant subspaces  X  X  X  (  X  )is selected individually for each object  X  such that these sub-spaces provide a high contrast between  X  and its surround-ing neighborhood. We measure the outlierness  X  X  X  X  X  X  X  (  X , X  ) by restricting distance functions  X  X  X  X  X  X   X  (  X , X  ) to the subspace dimensionsin  X  . Theoverallrankingvalue  X  (  X  )ofanobject  X  is then simply computed by aggregating its outlierness in all relevant subspaces: Definition 1. SubspaceRankingFunction The overall ranking value  X  (  X  ) of an object  X   X   X  X  X  w.r.t. a set of relevant subspaces  X  X  X  (  X  ) and an outlierness measure  X  X  X  X  X  X  X  (  X , X  ) is defined as:
While traditional ranking functions consider the outlier-ness of an object only in the full space  X  ,weaimatcon-sidering outliernessina set of subspaces  X  X  X  (  X  )  X  X  X  (  X  )out of the powerset of possible subspace projections. This is meaningfulasoutliersmightbehiddeninmultiplesubspace projections. However, two novel challenges arise:
Totacklethesechallenges,ourkeyhypothesisisthatout-liers can be distinguished in local neighborhoods of non-uniformly distributed subspaces.
Fortheselectionofasetof relevant subspaces  X  X  X  (  X  )we propose to exclude scattered subspaces with only low con-trast between outliers and regular objects. For more details on our statistical selection of relevant subspace, please refer to an extended version of this paper. Due to space limita-tions we have to skip the selection process in this paper.
In this paper we focus on the adaptive outlierness in the selected subspaces. As a density-based approach OUTRES measuresitsoutlierness  X  X  X  X  X  X  X  (  X , X  )accordingtothedensity  X  X  X  X  (  X , X  )ofanobjectinsubspace  X  . Low density values on an object indicate its outlierness and lead to low scores. However, objects might be outliers in multiple subspaces, thus,ameaningfuloutliernessmeasurehastobecomparable overdifferentsubspaces. Wewillproposeaninstantiationof our outlierness function  X  X  X  X  X  X  X  (  X , X  ) in the following section, wherewewillalsogivedetailsabouttheunderlyingadaptive density  X  X  X  X  (  X , X  )foracomparableoutliernessmeasurement.
For a meaningful outlier ranking based on outlierness in multiple subspace projections the definition of  X  X  X  X  X  X  X  (  X , X  ) has to provide an adaptive outlierness measure as the over-all ranking combines object properties out of very different subspaces  X   X   X  X  X  (  X  ). We propose such an adaptive out-lierness measurebydefiningan adaptive density anda local deviation foreachobject.
There is a strong dependence of densities on the number ofattributesintheconsideredsubspacesasalreadyobserved in our previous work for subspace clustering [1, 5]. For two subspaces  X , X   X   X  with  X   X   X  asimplecountingofob-jects in a fixed neighborhood yields  X  X  X  X  (  X , X  )  X   X  X  X  X  (  X , X  ). The main problem is the fixed neighborhood. As distances between objects grow with increasing number of attributes, a fixed neighborhood  X  (  X , X  )= {  X   X   X  X  X  X  X  X   X  (  X , X  )  X   X  } be-comes empty. All objects tend to have higher distance than the fixed  X  parameter. To tackle this general problem of density estimation in arbitrary subspaces, we propose an adaptivedensityusingavariableneighborhood. Byincreas-ing the neighborhood distance  X  with increasing number of attributes, our density measure can automatically adapt to the expected data distribution. Thus, different subspaces become comparable and outlierness based on density esti-mationcanautomaticallyadapttothenumberofattributes.
In general, we propose an adaptive neighborhood based on a variable  X  (  X   X   X  ) range.
The general idea is to derive the variable range out of a common observation in subspace projections. While in-creasing the number of attributes in a subspace projection the volume of a fixed neighborhoods decreases significantly compared to the overall volume of the subspace. For ex-ample the volume of a unite sphere (  X  = 1) in subspace  X  is withthegammafunction X (  X  +1)=  X   X   X (  X  ) ,  X (1)=1 ,  X (1 / 2)=  X   X  . In general, the volume decreases with increasing num-berofattributes.
Thus, the expectation of detecting objects in such neigh-borhoods is decreasing as well, resulting in very low density estimations. Ourvariableneighborhoodrangeadaptstothis phenomenon. Byincreasingtherangeweensurethattheex-pected number of objects remains constant. Thus, we pro-videacomparabledensityestimationinarbitrarysubspaces.
In the following we instantiate the basic idea of adaptive neighborhoodstoaspecificdensityestimationtechnique. As a flexible outlier model, OUTRES could be used with any density measure such as the simple counting of objects in the objects neighborhood. However, we base our density measure on more enhanced and well established density es-timation techniques [9]. As the overall density distribution ofthedataisnotknowninadvance, density  X  X  X  X  (  X , X  )ofan object  X  canbeestimatedbyusingkerneldensityestimators.
Eachobject  X  contributestotheoveralldensitybyalocal impactdefinedbyakernelfunction  X  (  X  )with  X  =  X  X  X  X  X  X   X  (  X , X  ) beingthescaleddistanceofanyotherobject  X  totheobject  X  . Thebandwidthparameter  X  isusedtoscaletheinfluence of each object to a maximal distance of  X  . The overall den-sityforanobject  X  isthensimplythesumofkernelfunctions over all objects in the database. As kernel function we use the Epanechnikov Kernel ,providingoptimaldensityestimationaccordingtothemean integrated squared error [9]. Concludingly,  X  X  X  X  (  X , X  )iscal-culated by the formula: Sinceobjectsbeingfartherawaythan  X  fromacertainobject donotcontributetoitsdensity,weobtainalocaldensityon which our outlier detection is based.

Incontrasttosimplecounting  X  X  X  X  (  X , X  )  X  ofobjectsinthe neighborhood, kernel density estimation has major benefits due to the weighted influence of each object. The sum of Epanechnikov Kernels provides a theoretically sound den-sity definition. However, for density estimation in arbitrary subspace projections the fixed bandwidth  X  shows similar drawbacks to the fixed  X  range. For comparable outlierness over arbitrary subspaces, we propose to adapt the density by a variable kernel bandwidth  X  (  X   X   X  ). As the true underly-ingdensitydistributionisunknown, weonlyusethedimen-sionality of the space to derive the bandwidth for adaptive density estimation. For a fixed space with dimensionality  X  optimal bandwidth  X   X  X  X  X  X  X  X  X  X  X  (  X  )isgivenbythefollowing formula: where  X  =  X   X  X  X   X  isthedatabasesizeandthegammafunction as in the previous computation of the sphere volume [9].
As motivated in the previous paragraph, optimal band-widthiscomputedbasedontheexpectednumberofobjects in a neighborhood. The formula for optimal bandwidth can simplybeseenastheoptimalradiusofan  X  -spheresuchthat oneyieldstatisticallyoptimaldensityestimationresults. For density estimation in subspaces this means that one has to chooseabandwidthforeachindividualsubspace. Assuming that  X  is fixed in a static database, we observe  X   X  X  X  X  X  X  X  X  X  X  to be a monotonically increasing function. So intuitively, for increasing dimensionality the influence (bandwidth) of eachobjectisincreasedaswellinordertomaintainoptimal density estimates, while the data space is becoming sparse. For comparable outlierness we use the optimal bandwidth to adapt density estimation in arbitrary subspaces. By the user parameter  X  we allow the user to quantify a notion of locality and adjust this value for arbitrary subspaces based on the optimal bandwidth. Formally, the bandwidth for a given number of attributes in a subspace  X  is defined by Definition 2: Definition 2. Adaptiveneighborhood For a subspaces dimensionality  X   X   X  ,  X   X   X  X  X  2 , the adaptive neighborhood  X  (  X   X   X  ) is defined by
Thus,wesimplyscalethegivenstartingbandwidth  X  from 2dspaceuptofulldataspaceandusethesevaluefordensity estimation. Incontrasttothefixedbandwidthinkernelden-sityestimationweuseouradaptiveneighborhoodasvariable bandwidth for each individual subspace. Consequently, our automatic bandwidth adaption ensures comparable density estimates for arbitrary dimensional subspaces.
Forouroutlierrankingbasedondeviationsofdensitywe first compute the density for each object and compare it with the local (average) density in a relevant subspace. By that,ourapproachisabletodetectobjectshighlydeviating from the residual data in a relevant subspace, i.e., objects havingexceptionallylowdensities. Whileouradaptiveden-sityensurescomparabilityovermultiplesubspaces,ourlocal deviation ensures meaningful outlierness values inside one subspace. Hence,inadditiontotheadaptivedensity,ween-sure to highlight an outlier with very low density compared to its local neighborhood in the considered subspace.
Having such a comparable density estimation, an outlier can be detected as an object showing significantly low den-sity. As we aim at a local outlierness we measure deviation basedonanadaptivethreshold. Asfirstfilterstepweselect only objects with significantly low density Fromstatisticalobservations,onlyveryrareobjectsdeviate morethantwostandarddeviationsfromthemeanvalue(cf. Chebyshev X  X  inequality). As statistical probability for such objects is low (e.g. for normal distributed data it is less than 2 . 1%), their outlierness has to be high. Using mean  X  andstandarddeviation  X  oftheestimated(local)densitywe ensure to be adaptive to varying density. Object deviation is then defined by: Definition 3. Objectdeviation The deviation of an object  X  with respect to mean and stan-dard deviation of the estimated density:
An object shows high deviation if its density compared to the average density  X  in its neighborhood  X  X  X  (  X , X  )is significantly low.
Overall the outlierness of an object  X  has to fulfill two major requirements. First, it has to be adaptive to arbi-trary dimensional subspaces. Thus, based on our adaptive object density we propose an adaptive outlierness which is comparable for different subspaces (cf. Sec. 2.3.2). Second, our adaptive outlierness has to cope with object deviation considering statistically deviation from the mean value (cf. Sec. 2.3.3). Incorporating both aspects in our adaptive out-lierness measure we define  X  X  X  X  X  X  X  (  X , X  )asfollows: Definition 4. AdaptiveOutlierness The outlierness of an object  X  in subspace  X  is derived by its density and its deviation in this subspace:
Ournoveloutliernessincorporatesbothaspectsderivedby density and the deviation of each object: low density and highdeviationarebothindicatesforhighoutlierness. Highly deviating objects show up by  X  X  X  X  (  X , X  )  X  1asdensityis significantlylowcomparedtomeanandstandarddeviation.
Overall we cope with the different behaviors of objects in different subspaces: Scattered irrelevant subspaces are ex-cluded. Objects in a dense subspace  X  result only in high densityandalmostnodeviationsuchthatweset  X  X  X  X  X  X  X  (  X , X  )= 1 they do not affect the ranking value (cf. Def. 1). Only if objects show up with low density or high deviation in a rel-evant subspace they contribute to the overall ranking value (cf. Def. 4).
We demonstrate the quality of our OUTRES approach on synthetic data. We compare OUTRES to the full space approaches LOF [2] and ABOF [4]. Furthermore, we com-pare against OutRank [6] and SOD [3] as the most recent outlier rankings based on subspaces. For repeatability, all approaches are included in our subspace outlier exploration
For scalability experiments, we generate synthetic data following a method proposed in [1, 7] to generate density-based clusters in arbitrary subspaces. In addition, our gen-erator adds outliers deviating from one of these subspace clusters. Astherearenoglobalpatternshiddenindata,the hiddenoutliersdonot appearinthescatteredfullspace. In our preliminary experiment, we evaluate the quality of the competingapproachesonasyntheticdatasetwith4765ob-jects represented by four subspace clusters each using 4 out of16givenattributesandadditionally61hiddenoutliersde-viating from these clusters. Figure 1 illustrates the quality with respect to ROC plot. We observe that all approaches show high increase in true positive rates of detected out-liers with only very few false positive. However, all hidden outliers (  X  X  X  X  =1) are found after thousands of considered objects,indicatedby  X  X  X  X   X  0. OurnovelOUTRESshows bestperformancecomparedtoLOF,ABOF,SODandOut-Rank, as it archives to detect more hidden outliers within thefirstrankedobjectsshowingbothhigher  X  X  X  X  andlower  X  X  X  X  than the competitors.
In this work, we proposed a novel outlier ranking for ob-jects deviating in subspace projections. The OUTRES ap-proach computes local density deviation by looking at a se-lection of relevant subspaces for each object. For compara-bleoutliernessmeasuresindifferentsubspaces,wederivean adaptivedensitymeasurewhichautomaticallyadaptstothe considered subspace. Overall, OUTRES achieves to detect outliers hidden in subspace projections.
 Acknowledgments: Thisresearchwasfundedbytheclus-ter of excellence on Ultra-high speed Mobile Information andCommunication(UMIC)oftheDFG(GermanResearch Foundation grant EXC 89). [1] I. Assent, R. Krieger, E. M  X  uller, and T. Seidl. DUSC: [2] M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. LOF: [3] H.-P. Kriegel, P. Kr  X  oger, E. Schubert, and A. Zimek. [4] H.-P. Kriegel, M. Schubert, and A. Zimek. Angle-based [5] E. M  X  uller, I. Assent, S. G  X  unnemann, R. Krieger, and [6] E. M  X  uller, I. Assent, U. Steinhausen, and T. Seidl. [7] E. M  X  uller, S. G  X  unnemann, I. Assent, and T. Seidl. [8] E. M  X  uller, M. Schiffer, P. Gerwert, M. Hannen, [9] B. Silverman. Density Estimation for Statistics and
