 In this paper we address the subject of large multimedia database indexing for content -based retrieval.

We introduce multicurves , a new scheme for indexing high -dimensional descriptors. This technique, based on the simultane-ous use of moderate -dimensional space-filling curves, has as main advantages the ability to handle high -dimensional data (100 dimensions and over) , to allow the easy maintenance of the indexes (inclusion and deletion of data) , and to adapt well to secondary storage, thus providing scalability to huge databases (millions, or even thousands of millions of descriptors) . 
We use m ulticurves to perform the approximate k nearest neighbors search with a very good compromise between precision and speed . The evaluation of multicurves , carried out on large databases, demonstrates that the strategy compares well to other up-to -date k near est neighbor search strategies.

We also test multicurves on the real -world application of image identification for cultural institutions. In this application, which requires the fast search of a large amount of local descriptors, multicurves allows a drama tic speed -up in comparison to the brute -force strategy of sequential search , without any noticeable precision loss.
 H.2.2 [ Database management ]: Physical design  X  access me-thods. H.2.4 [ Database management ]: Systems  X  mul timedia databases. H.3.1 [ Information storage and retrieval ]: Content analysis and indexing  X  indexing methods.
 Algorithms, Measurement, Performance, Design, Experiment a-tion.
 High -dimensional data indexing, k nearest neighbors search, local descriptors, i mage retrieval, d ocument identification, copy detection, near -duplicate detection . 
The retrieval of multimedia data often carries two intrinsic challenges: the sheer amount of information and the complexity of this information . In contrast with relational or textual data, the relationship between low -level representation of multimedia and its high -level semantic content i s much less direct. Therefore, retrieval must be mediated by the use of descriptors , which are usually very high -dimensional and often proliferate at the rate of hundreds per described item.

From both the theoretical and the technical standpoints, t he indexing of those descriptors is a very difficult problem . From the theoretical point of view, the well -known  X  X urse of dimensional i-ty X  makes the indexin g inefficient as dimensionality grows [1][2] . From the technical point of view, the memory hierarchy , the workings of disks and caches , and the integration with DBMS create additional constraints which must be addressed if the scheme is to have any practical usefulness . 
In this paper we describe multicurves , a very e fficient index for high -dimensional data, which address es the need of multimedia descriptor indexing, in the context of very large databases. Our scheme is based on space-filling curves  X  a technique which has attracted considerable attention on the field for its ability of creating a  X  X icinity -sensitive  X  total order on the da ta, and thus allowing the adaptation of the efficient and convenient one -dimensional indexing techniques, like the B -Tree, to multidime n-sional data. The originality of our method is t o integrate these curves into a very effective structure, where, through a smart combination of multiple, moderate -dimensional curves, boundary effects are adequately addressed and precision is considerably improved, without losing the benefits of scalability and ease of index maintenance.

We introduce the problem of multimedia descriptor indexing using kNN search on  X  2. On  X  2.2 we specifically discuss the methods which use space -filling curves. The detailed description of our method, multicurves, follows, on  X  3. On  X  3.3 we discuss some important considerations of parameteri zation and integration with DBMS. On  X  4, we compare multicurves with two other methods based on space -filling -curves from the state-of -the -art, showing that multicurves presents the best compromise between precision and speed. On  X  5 we show multicurves i n action. In the context of an application of image retrieval, it allows a dramatica l-ly accelerat ing of the results, without any noticeable performance losses.

As we have suggested in the introduction, in what concerns information retrieval , a crucial difference between multimedia data and relational / textual data is that the low -level represent a-tion has a very indirect relationship with the semantic content.
Because of that, indexing and retrieval of those data is mediated by descriptors , i.e., more compact and semantically -rich represen-tation of the data. Those descriptors appear in a large variety of forms: color and texture histograms, invariant moments, Fourier coefficients, local jets, gradient maps, etc. They often appear in the form of vectors, which are frequently high-dimensional.

To establish the similarity / dissimilarity between documents , their descriptors are compared . Therefore, to answer the user queries, a descriptor matching scheme has to be established, and this is usually done by kNN search (also known as k nearest neighbors search or similarity search ). kNN search can be understood as the operation of finding, on a given space, the k points which are the nearest to the query point . For a more formal definition, consider that we have:  X  a d -dimensional domain space D ;  X  a base set B with the elements ;  X  a query ;  X  and a dissimilarity function ;  X  the kNN search for the k elements most similar to q con-
An obvious solution is the sequential search, where each el e-ment of the base is compared to the query, and the k most similar are kept. Unfortunately, this brute -force solution is acceptable only for small bases, being unfeasible in our context. The altern a-tive is using multidimensional indexing scheme s, which are able to accelerate the search.

However, t he efficiency of those schemes depends greatly on the dimensionality of the elements. The search time can be made to grow only logarithmically to the size of the base, but at the expense of introducing a hidden constant which will grow exp o-nentially with the dimensionality. Typically, for over 10 dime n-sions , it will be impossible to perform exact kNN search faster than the sequential method. This phenomenon is known as  X  X urse of dimensionality X  and expresses the difficulty in partitioning the data or the spa ce in an efficient way when dimensionality is very high [1][2] . 
Multimedia descri ptors at dimensionalities around 30 are co m-monplace, and those at around 100 are not unusual. Dimensional i-ties of 4 00 and above are not unheard of. For spaces like those, kNN search is extremely challenging. kNN search is an important topic of research, which finds many applications in addition to descriptor matching, and not surpr i-sin gly there is a vast literature about the subject. For a compr e-hensive introduction and sta te of art on the subject the reader is referred to [3][4] . Here, however, we are only interested in those methods of practical interest in our specific context . The method should:  X  perform well for high-dimensional data, presenting a good  X  adapt well to secondary -memory storage, which in practice  X  ideally, the index ge nerated should by dynamic, i.e., allow 
Surprisingly few methods are able to accomplish those r e-quirements : many methods assume implementation in main memory (and thus, cheap random acc ess throughout the index), other methods have prohibitively high index building times (with a forced rebuilding if the index changes too much), and so on.
LSH, or locality -sensitive hashing, uses locality -preserving hashing functions to index the data. The method uses several of those  X  X ash tables X  at once, to improve reliability [5] . 
Another interesting method is MEDRANK, which projects the data into several random straight lines. The one -dimensional position in the line is used to index the data [6] . The NV -Tree (formerly known as PvS) is an improveme nt on MEDRANK, which uses segmentation and re -projection in random straight lines. Points are grouped together only if they are simultaneously near in multiple straight lines [7] . 
A common pattern of all those methods is the use of multiple subindexes. Conceptually what happens is that a sub-query is performed on each of those subindexes, and the method then aggregates th e results. Intuitively, the idea is to increase the chances of a succes sful search by using several parallel structures.
LSH is backed by a solid theoretical background, which allows predicting the approximation bounds for the index, for a given set of par ameters. Setting those parameters however is not easy, as there are several of them, some of which are quite critical (like the radius of analysis, which sets a bound beyond which we allow ourselves to ignore the points). Also, t he number of hash tables necessary to have good performance is high (30 or more), which represents a cost in terms of storage and of random accesses performed during the search.

MEDRANK is also backed by a theoretical background, but the guaranteed bounds are much looser. The greatest advantage of the method is the fact the data are projected into one-dimensional lines, and can then be managed by very efficient structures, like B-Trees. In empiric tests, however, the method has failed to index multimedia descriptors efficiently, mostly due to the lack of correlation between the distance in the straight lines and the distance in the high -dimensional space [8] . 
Trying to overcome the troubles of MEDRANK, NV -Tree projects the data onto one straight line, segments th e data and then re-projects them into another straight line. The process can be repeated several times, until segments of appropriate size are obtained. Points will fall in the same segment only if they are near in all lines, simultaneously. The major diff iculty with NV -Tree is the need of redundant coverage, to avoid boundary effects during segmentation. This makes the index doubly redundant (the points are present in each subindex, but also may be replicated unpr e-dictably within the segments of a subindex ), making the mainten-ance of the index rather complex.

We became acquainted with MEDRANK and seduced by the possibility of simply using a sorted list in the subindexes , since this allows us to create a fast, dynamic and easy to implement index. At the same time, we wanted to use multiple dimensions in the subspace represented in the subindexes, to minimize the distortion of the distance function . The solution to conciliate those two objectives was given by the use of space -filling curves.
Fractal geometry presents many unexpected results: the concept of  X  X ractional X  dimensionality, the existence of figures of finite area but infinite perimeter, and the possibility of relatively simple rules generating very com plex ( X  X haotic X ) sets. Not less surprising is the existence of continuous mappings from the unit interval [0; 1] and any unit hypercube [0; 1] d . Simply stated, this shows that there exist continuous curves which completely cover any d -dimensional hypercube . Those curves are called space -filling curves (Figure 1). a) Hilbert; b) Lebesgue or  X  X  -order X ; c ski. Three iterations of each curve are shown: the actual space -filling 
Most space-filling are constructed by a recursive procedure, in which the space is progressively divided in smaller cells, which are then traversed by the curve. In this case, we say that the order of the curve is the number of iterations which have been carried out in the infinite series of a space -filling curve, i.e., the degree of refinement obtained so far. The space is filled by the curve to which the series co nverges. A curve of a given finite order is said to be an approximation of the named space-filling curve.

For an in -depth introduction to the subject of the space -filling curves, their mathematical properties and the proofs of the most important theorems, the reader is referred to [9] . 
The use of space-filling curves to perform the kNN search in multidimensional spaces is not new. Apparently Faloutsos was the first author to explicitly refer to the concept of space -filling curves [10] , though implicit use of the Z -order curve (through the notion of  X  X it -interleaving X ) can be traced back to 1966 [11] . Faloutsos was also the first to suggest that other curves could perform better than the Z -order, first proposing the Gray -code curve and then the Hilbert curve [10][12]. Skubalska -Rafaj X owicz and Krzy X ak make a seemingly independent development of the use of the curves to perform kNN, using the generalized form of the Sierpi X ski curve [13] . 
All th ose methods are conceptually very simple. They map the high -dimensional points in the curve , obtaining a one -dimensional coordinate , called extended-key , which represents its relative position in the length of the curve . The extended -key is used to perform a one -dimensional similarity search, which is straight-forward. The hypothesis is that points which are near to each other in the curve always correspond to points that are near to each other in the space, so it is a good heuristic to take the nearest points in the curve as the nearest points in the space.

Unfortunately, the converse of the hypothesis is not true, in the sense that near points in the space are not always near in the curve. This is because of the boundary effects, which tend to put very far apart points in certain regions of the curve (Figure 2). The matter is seriously aggravated as the dimensionality increases.
Figure 2: The problem with boundary effects on the space -filling curves. The points in the centre of the space are further apart in the curve than the points in the lower -left quadrant
To conquer the boundary effects, different authors propose different solutions, usually through the use of multiple curves [14][15][16]. A notably different approach is suggested by Mainar -Ruiz and P X rez-Cor X s, suggesting using multiple (slightly disturbed) instances of the same point in only one curve [17] . 
As we have seen, the greatest problem of the use of space-filling curves comes from the boundary effects, and different methods propose different solutions, usually through the simult a-neous use of multiple curves.

Our method, multicurves , is also based on the use of multiple curves, but with the important distinction that each curve is only responsible for a subset of the dimensions. So not only we gain the intrinsic advantages of using multiple curves (i.e., points that are incorrectly separated in one curve will probably stay together in another), but also, we lower the boundary effects inside each one of the curves.

Multic urves has a simple structure: divide the dimensions of the data element s among a certain number of space-filling curves. On each curve map the corresponding projections of the data , co m-pute their extended -keys, and put each pair &lt;extended -k e y, element &gt; in a list sorted by extended -k e y. Each one of those sorted lists is a subindex (Figure 3). 
The search is done the same way: the query is decomposed into several projections (corresponding to the dimensions associated to each subindex) and each projection has its extended-key co m-puted. Then, for each subindex, we explore the element s whose extended -keys are the nearest to the corresponding query e x-tended -k e y.

Any recursive space-filling curve can theoretically be used, but in comparison with other space -filling curves, the Z-order curve or the Gray -code curve, the Hilbert curve has better clusteri ng properties, which is important for the kNN search and other applications (see [18] ). Other important properties of the curve, which resulted in its widespread applic ation, are its  X  X airness X  (symmetric behavior on all dimensions) and the absence of distant jumps [19] . 
Before we present the construction and search algorithms , let us make a few definitions :  X  d : the dimensionality of the data elements ;  X  m : the number of bits needed to represent each dimension  X  c : the number of curves to build;  X  d i : the dimensionality of the i th curve;  X  A : an association between the dimensions of the data and 
The construction algorithm for multicurves (show n in Alg o-rithm 1 ) is relatively simple, since the underlying structure of the index is just a set of sorted list s, one for each Hilbert curve generated. Each data element is decomposed into c projections, accordingly to the association of dimensions A . Those projections are used to compute the extended -key in each curve. The pairs &lt;extended -k e y, data element &gt; are then inserted in the curves. Algorithm 1: The construction algorithm for multicu rves
The complexity of the construction depends on the underlying structure used to implement the sorted lists. The computation of the projections of each element in steps 7 X 9 takes, at worst, operations. The computation of the extended-key using the B utz algorithm on line 10 makes bit operations. Assuming an efficient sorted list structure, insertions will take steps. Since we are building c lists, algorithm should take at worst, 
The search algorithm is als o simple (Algorithm 2 ). We choose, beforehand, the number of elements to examine in each subindex. Then we project the query o nto the same c subspaces used to build the index. We find, on each subindex, the elements nearest to the corresponding projection, and keep the k nearest to the query.
The complexity analysis of the search comparison is easy: the construction of the extended-key using the Butz algorithm makes nearest to the extended -key depend s on the underlying data structure, but generally it can be assumed to take at most forced to make at least one random access to the data. The complexity of steps 9 X  15 is known beforehand and grows linearly with the number of elements to be examined. The expensive operation here is the computation of the distances, which takes Euclidean distance).

Figure 3: The base structure of multicurves: using simult a-neously several moderat ely -dimensional space -filling curves
Figure 4: Two sample parameterizations of the multicurves showing different choices for the parameters c , d i , and the association A , leading either to a regular scheme (a), or not (b).
The dimensionality has also a  X  X idden X  influence, in that it increases linearly the amount of data which must be transferred by the algorithm. This is non -negligible when we use secondary storage.

The whole operation (steps 2  X 17) is repeated on ce for every subindex, which means a linear growth with this parameter. 
In summary, the time spent on the search grows linearly with the number of points to be examined in each index , the number of indexes and the number of dimensions in the data. It also grows logarithmically on the number of elements in the database.
In order to use the Hilbert curve in any intended application, it is necessary to compute a mapping between the coordinates in the hyperdimensional space and the one -dimensional ordinal position in the curve (i.e., the extended -key ). There is an algorithm which uses little memory, is iterative, and can map any curve using only 
Any adequate data structure can be used to store the sorted lists, the choice being based on practical considerations. Normally some flavor of B -t ree [21] should be the best solution for most database applications. In our test implementations, for the sake of simplicity, we have chosen a two -level indexing, with the first level fitting entirely in main memory . 
The most important parameters for the construction of the mul-ticurves are th e number of subindexes and the association between the dimensions. We propose to set as the highest value which efficiency will allow (which cannot be much higher than 10, anyway, since each additional index means a new random access to the data). Currentl y, the association of dimensions we use is a simple balanced partition among the curves.

The parameterization for the search is simple: only one param e-ter must be decided which is the number of elements to examine in each subindex.

Those parameters (number of subindexes and number of el e-ments to examine) are a subject of empirical evaluation in the next section.

Since we have established that it is critical to limit the number of random accesses, it is absolutely critical that the subindexes store copies of the descriptors and not pointers to the descriptor information. The reason for this is clear: if the subindex has just a pointer to the data (as indexes usually do), when we arrive at the loop 9 X  15 of the algorithm in Algorithm 2, we will have to make one additional random access for each candidate, which is unac-ceptable.

This of course, is not a specific problem of multicurves: most multimedia indexing methods face this implementation challenge: the use of pointers to the data, which in the majority of cases works perfectly for relational data, incurs into unacceptable high costs for multimedia descriptors. 
Replication of data, however, introduces the opportunity of inconsistencies and other anomalies. Besides the logical database design, must remain free of those implementation considerations. The best solution, therefore, is to delegate the management of the replication of the data to the physical d esign of the database, and let the DBMS to take care of the details. 
For the evaluation of our system, we have used two image databases.

The APM database is composed by SIFT descriptors [22] gen-erated from image transformations of a selection of 100 original images. The images are old photographs (XIX and first half of XX centuries) from the collection of the Arquivo P X blico Mineiro, the State Ar chives of the Brazilian State of Minas Gerais. Each image suffered three rotations, four scale changes, four non-linear photometric changes (gamma corrections), two smoothings and two shearings  X  a total of 15 transformations. Each transformed image had it s SIFT descriptors calculated and aggregate into a database of 2 871 300 descriptors . The quer ies are the SIFT descriptors calculated from the original images , amounting to 263 968 descriptors . The dimensionality of SIFT descriptors is 128.

The Yorck datab ase is composed of SIFT descriptors generated from the images of the Yorck Project, a collection of images of works of art. The images were downloaded from the Wiki media Commons website, reduced to a smaller resolution, and had their SIFT points computed a nd put on a database  X  which amounts to 21 591 483 descriptors . The queries are the SIFT descriptors calculated from images of the Yorck Project selected at random and then transformed. One hundred images were selected, of which, 20 were rotated, 20 were r esized, 20 suffered a gamma correction, 20 were sheared and 20 suffered a dithering (halfton-ing)  X  summing up to 166 315 query descriptors.

The ground truth is the set of the correct nearest neighbors for all query descriptor s, according to the Euclidean distance. It was computed using the sequential search, a slow method, but which guarantees exact results. However, not all nearest descriptor s are equally useful for image identification. In fact, the match can only be considered correct, when it correspo nds to a real match in the image. To take this into account we eliminate from the ground truth all answers which do not correspond to true matches. This is done by taking in turn every pair of correct query -target images (which are known beforehand), matching the descriptors and keeping only those which are geometrically consistent (using a RANSAC model fitting). After all pairs are processed, the only matches remaining are at the same time discriminating (from the point of view of the descriptor vector) and correct (from the point of view of the visual features).

Performance is measured in two ax es: efficacy ( the capability of the method to return the correct results ) and efficiency ( the capability of the method to use as little resources as possible).
To measure the efficacy we use the MAP (mean average preci-sion), a classic efficacy metric of information retrieval, which is defined as  X  X he area underneath a non -interpolated recall -precision curve X  [23] . It can be considered a condensed single-value summary of the precision at all recall values.

From the point of view of the user, the most critical efficiency metric is the wall time spent on the search, but using it to compare the methods can be misleading, since it depends heavily on the machine, the operating system, and even on the current load (concurrent tasks) at the time the experiment is performed. We choose, therefore, to co mpare the methods by counting , for each method, how many target descriptors were accessed per query descriptor . 
As we have mentioned in  X  3.3 , an important parameter for multicurves is the number of subindexes. This is also important for the methods of Mainar -Ruiz et al. and Liao et al. It is impo r-tant to keep in mind that each new subindex represents additional costs in terms of construction time and disk space occupied. More critical, however: each subindex represents more descriptors acessed and more random access to the data (for multicurves and for Liao et al.), which we want to avoid. Therefore, we want to keep this number reasonably low. 
We did not feel it was useful or informative to cover all the parameter scale for all methods  X  instead, we cov ered the range adequate to make the general tendencies unequivocal, in order to allow a fair comparison.
We have implemented and tested three methods : our mult i-curves, the methods by Liao et al. [16] and the one by Mainar -Ruiz and P X rez-Cort X s [17] . Those are methods from the recent state -of -the -art in kNN search, wh ich also employ space-filling -curves.

Like us, Liao et al. propose solving the problem of boundary effects on the Hilbert space-filling curves by using several curves at once. But in their work, all curves map the entire dimensional i-ty of the data  X  the di fference between the subindexes is a translation of the data.

Mainar -Ruiz and P X rez-Cort  X s suggest, instead, using only one curve but multiple representants of the data. Before inserting those representants in the curve, the algorithm disturbs their position, to give them the opportunity of falling into different regions of the curve. In that way , even if the query falls in to a problematic region, chances are it will be reasonably near to at least one of the representants . The advantage of this method is that the index is composed by a single sorted list and , therefore, the number of random accesses necessary to perform the search is reduced to the bare minimum. Our reimplementation of thi s method had a slight adaptation: while originally a generalized Sierpi X ski curve is used , we use the Hilbert curve (both curves are very similar in terms of their boundary domains).

All methods were implemented in Java, using the Java Plat-form, Standard Edition v. 1.6. 
Figure 5 shows the performance comparison between the three methods in the APM data base. In the horizontal axis, representing efficiency, we plot the number of points examined per query. In the vertical axi s, representing efficacy, we plot the MAP obtained. The best compromise is towards the left upper corner.

To check how the methods depend on the number of subinde x-es, we vary this parameter (indicated next to each data point). In Figure 5: Performance comparison (precision  X  speed) for the towards the upper left corner. The numbers next to the points Figure 6: Performance comparison (precision  X  speed) for the towards the upper left corner. The numbers next to the points Mainar -Ruiz et al. there i s, of course, only one sorted list, and this number corresponds to the number of representants given to each original point. 
By a large margin, m ulticurves presented a better compromise for all interval of parameters tested. Either for a given number of subindexes, or a given number of points examined, it is much more precise. For 8 subindexes, which is the parameter which covers all three methods, the MAP of multicurves is 70% greater than that of t he other methods . The methods by Liao et al. and Mainar -Ruiz et al show roughly comparable compromise s of efficacy  X  efficiency in this database . 
In what concerns the number of subindexes, though the interval studied is small, there seems to be a fast law of diminishing returns for multicurves. Anyway, practical considerations prevent us from having much more than 10 subindexes (for at this point we already have a tenfold increase in storage needs and 10 disk random access, at least, per query). For this particular database, the interval between 4  X 8 subi ndexes seems to be the  X  X weet spot X .
The experiments performed on the Yorck database ( Figure 6) keep the number of subindexes set at 8, but vary the number of element s examined in each subindex, which is indicated next to each data point. Keeping the same axes as before, we plot the performance of the three methods.
 Multicurves has again the best compromise by a large margin. For the same number of descriptors examined, it increases precision by roughly 35% in comparison to the method by Mainar -Ruiz et al . In this database, which is about 10 X  larger than the previous, the method by Liao et al. lagged behind.

Multicurves seems to be relatively insensitive to the number o f descriptors examined per subindex: doubling this value increases the precision by 3% in this database. This suggests that small values should be employed for most applications.
Document identification or copy detection consists in taking a query document and finding the original from where it derives, together with any relevant metadata, such as titles, authors, copyright information, etc. It is an important operation to instit u-tions poss essing large documental collections, both to answer the needs of the users, which often want to establish the provenance of unidentified documents , and to detect copyright violations . 
The task is challenging for visual documents, since w e are interested in recovering more than exact pixel -by -pixel copies: even if the document has been subjected to a series of deform a-tions, we still want to identify them (Figure 7). The set of tran s-formations varies from application to application but usually includes translations, rotations , scale changes , photometric and colorimetric transformations, cropping and occlusions, noise of several kinds, and any combination of those.

Image i dentification system s are a specialization of content -based image retrieval (CBIR) systems, proposed to solve the problem of copy detection. Contrarily to traditional textual search, those schemes work in the absence of annotations, and in contrast to wate rmarking, they can identify documents outside the co n-trolled chain of distribution (which will lack the watermarks).
Like all CBIR systems, image identification systems use d e-scriptor s to establish the similarity between the images. But instead of stimulating generalization, exploration and trial-and -error, typical goals of semantic-oriented CBIR systems, they are tuned to emphasize the exactness of image identification and to tolerate transformations which completely disrupt the appearance of the image (such as conversion to grayscale or dithering).

The images may be describ ed either by one descriptor or a set of descriptors. In the former case, when a single descriptor must capture the entire information of the image, we say it is a global descriptor . In the later case, the descriptors are associated to different features of the image (regions, edges or small patches around points of interest) and are called local descriptors . 
Systems based on local descriptors adopt a criterion of vote count: each query descriptor matches with its most similar descriptor s stored in the database (using a simple distance, like the Euclidean distance). Each matched descriptor gives one vote to the image to which it belongs. The number of votes is used as a criterion of similarity.

Local descriptor based systems are unsurprisingly muc h more robust. Because the descriptors are many, if some get lost due to occlusions or cropping, enough will remain to guarantee good results. Even if some descriptors are matched incorrectly, giving votes for the wrong images, only a correctly identified image will receive a significant amount of votes. Unfortunately, the multi p-lic ity of descriptors brings also a performance penalty, since hundreds, even thousands of matches must be found in order to identify a single image. 
Systems based on global descri ptors [24] have not shown enough precision on the task of image identification, except for slight transformations. In all comparisons, local -descriptor methods have performed better [25][26][27] . Figure 7: Visual document identification  X  we should be able to retrieve the original images (right) from the qu eries (left) 
Local -descriptor image and video identification are application scenario s where multicurves show s all its advantages. The elevated number of query descriptors makes a fast indexing scheme an absolute requirement for the system is to have practical interest. Furthermore, the approximation of the results induced by the index is not serious, because the loss of a few matches is unlikely to affect the final results. Finally, the enormous size of the databases demands a scalable, disk -friendly and easy to update indexing technique. We tested multicurves in an image identification context, for the Yorck Project database, containing over 10 thousand reproduc-tions of paintings. The system architecture follows a classic scheme : w e compute the descriptors for every image in the database, and then stock and index those descriptors . When a query image is p resented, its descriptors are computed and matched to the 10 nearest descriptors in the database. To get rid of false positives and improve the solution, we apply a geometric consistency step (using a robust model fitting technique [28] ), discard all inconsistent matches and then count the votes. The images are ranked by number of votes and presented to the user. The descriptor used is SIFT [22] , which has a dimensionality of 128 . 
One hundred images were selected and suffered intense tran s-formations, which included rotation, size reduction, gamma correction, shearing and dither ing. The task consisted in using those images as queries to locate their originals.

First, we have run the system using the exact sequential search to match the descriptors. Since our query images have a large number of descriptors, it is unsurprising that we o btain perfect results (the original is always found), since at least a few dozens of descriptors is correctly matched between query and target every time (and typically, much more).

Then, we have run the system using multicurves with 8 subi n-dexes and examining 512 descriptors per subindex to match the descriptors. Each correct ly identified image has lost, on average, about 20% of its votes, but those were so many to begin with, that this did not result in changes in the final ranking , which w as still perfect. Running time, however, was between 20 and 25 times shorter.

These results are a testimony of both the robustness of the local -descriptor architecture, and the potential efficiency gains provided by multicurves in those architectures.
In this paper we have proposed multicurves: a new method for the indexing of multimedia data, based on the use of multiple, moderate-dimensional space -filling curves. Multicurves addresses several requirements rarely considered by multidimensional access meth ods: small cost for index construction, easy index maintenance, scalability, compatibility with secondary memory storage and eas y implementation. When used to perform approx-imate kNN search, it presents very good performance, compar ing very favorably with other state -of -the -art methods based on space-filling curves , both in terms of efficiency and efficacy . 
We have also presented a local -descriptor based information -retrieval architecture were multicurves demonstrates its excellent results. In those architectures, a large number of local descriptors is used to retrieve the document, and the eventual mismatching of a few of them is unlikely to harm the final results. In the applic a-tion we have chosen to eval uate  X  image identification  X  multicurves allowed a speed -up of up to 25, without any notice a-ble precision losses.

Multicurves can be embedded in a DBMS to provide fast access to multimedia data. In a well-conceived architecture, the DBMS will manage all t he replications and redundancies at the physical layer , and the logical design may remain untarnished of those considerations.

Our current work is to establish the bounds of approximation of multicurves, to give it a theoretic framework as solid as that of LSH or MEDRANK. In the process, we are also investigating if there are convenient ways to determine the (near -)optimal values for the number of curves ( c ) and for the association between the dimensions of the data and the dimensions of the subindexes ( A ). 
Since most local -descriptor schemes incur in an excessive nu m-ber of query descriptors, we are also investigating what is the best compromise between having a very precise matching over a few carefully chosen query descriptors, and a very approximate match ing of all query descriptors. The images in the APM database were lent by the Arquivo P X blico Mineiro. The images in the Yorck database were gen e-rously put on public domain by the German company Directmedia Publishing, and made available to the public thanks to the cont i-nuous efforts of the Wikimedia Foundation.

Eduardo Valle wa s sponsored by the CAPES Foundation through the CAPES / COFECUB Program. [1] B X hm, C., Berchtold, S. and Keim, D. A. 2001. Searching in [2] Weber, R., Schek, H. -J. and Blott, S. 1998. A Quantitative [3] Samet, H. J. 2006. Foundations of Multidimensional and [4] Shakhnarovich, G., Darrell, T. and Indyk, P. 2006. Nearest -[5] Indyk, P. and Motwani, R. 1998. Approximate nearest [6] Fagin, R., Kumar, R. and Sivakumar, D. 2003. Efficient [7] Lejsek, H.,  X smundsson, F. H., J X nsson, B.  X . and Amsaleg, [8] Lejsek, H.,  X smundsson, H. F., Bj X rn X  X r, J. and Amsaleg, L. [9] Sagan, H. 1994. Space-filling curves. Springer -Verlag, Berlin [10] Faloutsos, C. 1986. Multiattribute hashing using Gray codes. [11] Morton, G. M. 1966. A computer oriented geodetic data base [12] Faloutsos, C. and Roseman, S. 1989. Fractals for secondary [13] Skubalska -Rafaj X owicz, E. and Krzy X ak, A. 1996. Fast k -NN [14] Megiddo, N. and Shaft, U. 1997. Efficient nearest neighbor [15] Shepherd, J. A., Zhu, X. and Megiddo, N. 1999. A fast [16] Liao, S., Lopez, M. A. and Leutenegger, S. T. 2001. High [17] Mainar -Ruiz, G. and P X rez-Cort X s, J. -C. 2006. Approximate [18] Moon, B., Jagadish, H. V., Faloutsos, C. and Saltz, J. H. [19] Mokbel, M. F., Aref, W. G. and Kamel, I. 2003. Analysis of [20] Butz, A. R. 1971. Alternative Algorithm for Hilbert's Space -[21] Bayer, R. and McCreight, E. M. 1972. Organization and [22] Lowe, D. G. 2004. Distinctive Image Features from Scale-[23] Voorhees, E. M. and Harman, D. 2001. Overview of TREC [24] Chang, E. Y., Wang, J. Z., Li, C. and Wiederhold, G. 1998. [25] Ke, Y., Sukthankar, R. and Huston, L. 2004. An efficient [26] Mo X llic, P. -A. and Fluhr, C. 2007. ImagEVAL Official [27] Valle, E., Cord, M. and Philipp -Foliguet, S. 2006. Content -[28] Fischler, M. A. and Bolles, R. C. 1981. Random sample 
