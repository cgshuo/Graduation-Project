 Most machine learning algorithms used in computa-tional linguistics are parametric , i.e., they learn a nu-merical weight (e.g., a probability) associated with each feature, where the set of features is fixed be-fore learning begins. Such procedures can be used to learn features or structural units by embedding them in a  X  X ropose-and-prune X  algorithm: a feature proposal component proposes potentially useful fea-tures (e.g., combinations of the currently most useful features), which are then fed to a parametric learner that estimates their weights. After estimating fea-ture weights and pruning  X  X seless X  low-weight fea-tures, the cycle repeats. While such algorithms can achieve impressive results (Stolcke and Omohundro, 1994), their effectiveness depends on how well the feature proposal step relates to the overall learning objective, and it can take considerable insight and experimentation to devise good feature proposals.
One of the main reasons for the recent interest in nonparametric Bayesian inference is that it offers a systematic framework for structural inference, i.e., inferring the features relevant to a particular prob-lem as well as their weights. (Here  X  X onparamet-ric X  means that the models do not have a fixed set of parameters; our nonparametric models do have pa-rameters, but the particular parameters in a model are learned along with their values). Dirichlet Pro-cesses and their associated predictive distributions, Chinese Restaurant Processes, are one kind of non-parametric Bayesian model that has received consid-erable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchi-cal Dirichlet Processes (HDP) (Teh et al., 2006).
Lexical acquisition is an ideal test-bed for explor-ing methods for inferring structure, where the fea-tures learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for eval-uating structural inference in this paper. Nonpara-metric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008).

In a computational linguistics setting it is natu-ral to try to align the HDP hierarchy with the hi-erarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to ex-plore a wide variety of HDP grammar-based mod-els. Given an appropriate adaptor grammar, the fea-tures learned by adaptor grammars can correspond to linguistic units such as words, syllables and col-locations. Different adaptor grammars encode dif-ferent assumptions about the structure of these units and how they relate to each other. A generic adaptor grammar inference program infers these units from training data, making it easy to investigate how these
However, there are a number of choices in the de-sign of adaptor grammars and the associated infer-ence procedure. While this paper studies the im-pact of these on the word segmentation task, these choices arise in other nonparametric Bayesian infer-ence problems as well, so our results should be use-ful more generally. The rest of this paper is orga-nized as follows. The next section reviews adaptor grammars and presents three different adaptor gram-mars for word segmentation that serve as running examples in this paper. Adaptor grammars contain a large number of adjustable parameters, and Sec-tion 3 discusses how these can be estimated using Bayesian techniques. Section 4 examines several implementation options within the adaptor grammar inference algorithm and shows that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an er-ror reduction of over 35% compared to the best pre-viously reported results on this corpus. This section informally introduces adaptor gram-mars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars.
Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learn-ers must solve. Elman (1990) introduced an ideal-ized version of this task, and Brent and Cartwright (1996) presented a version of it where the data consists of unsegmented phonemic representations of the sentences in the Bernstein-Ratner corpus of child-directed speech (Bernstein-Ratner, 1987). Be-cause these phonemic representations are obtained by looking up orthographic forms in a pronounc-ing dictionary and appending the results, identifying the word tokens is equivalent to finding the locations of the word boundaries. For example, the phoneme string corresponding to  X  X ou want to see the book X  (with its correct segmentation indicated) is as fol-lows: We can represent any possible segmentation of any possible sentence as a tree generated by the follow-ing unigram grammar .
 The nonterminal Phoneme expands to each pos-sible phoneme; the underlining, which identifies  X  X dapted nonterminals X , will be explained below. In this paper  X + X  abbreviates right-recursion through a dummy nonterminal, i.e., the unigram grammar ac-tually is: A PCFG with these productions can represent all possible segmentations of any Sentence into a se-quence of Word s. But because it assumes that the probability of a word is determined purely by mul-tiplying together the probability of its individual phonemes, it has no way to encode the fact that cer-tain strings of phonemes (the words of the language) have much higher probabilities than other strings containing the same phonemes. In order to do this, a PCFG would need productions like the following one, which encodes the fact that  X  X ant X  is a Word . Adaptor grammars can be viewed as a way of for-malizing this idea. Adaptor grammars learn the probabilities of entire subtrees, much as in tree sub-stitution grammar (Joshi, 2003) and DOP (Bod, 1998). (For computational efficiency reasons adap-tor grammars require these subtrees to expand to ter-minals). The set of possible adapted tree fragments is the set of all subtrees generated by the CFG whose root label is a member of the set of adapted non-terminals A (adapted nonterminals are indicated by underlining in this paper). For example, in the uni-gram adaptor grammar A = { Word } , which means that the adaptor grammar inference procedure learns the probability of each possible Word subtree. Thus adaptor grammars are simple models of structure learning in which adapted subtrees are the units of generalization.

One might try to reduce adaptor grammar infer-ence to PCFG parameter estimation by introducing a context-free rule for each possible adapted subtree, but such an attempt would fail because the number of such adapted subtrees, and hence the number of corresponding rules, is unbounded. However non-parametric Bayesian inference techniques permit us to sample from this infinite set of adapted subtrees, and only require us to instantiate the finite number of them needed to analyse the finite training data.
An adaptor grammar is a 7-tuple (
N, W, R, S,  X  , A, C ) where ( N, W, R, S,  X  ) is a PCFG with nonterminals N , terminals W , rules R , start symbol S  X  N and rule probabilities  X  , where  X  the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A , so C adaptor for adapted nonterminal X  X  A .
 Informally, an adaptor C maps a stream of trees from a base distribution H whose support is T node is X  X  N generated by the grammar X  X  rules) into another stream of trees whose support is also T are determined by the PCFG rules expanding X and the other adapted distributions, as explained in John-son et al. (2007b). When called upon to generate an-other sample tree, the adaptor either generates and returns a fresh tree from H it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Pro-cesses (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs non-deterministically generate infinite sequences of nat-ural numbers z z  X  X hinese Restaurant X  metaphor samples produced by the adaptor are viewed as  X  X ustomers X  and z is the index of the  X  X able X  that the n th customer is seated at. In adaptor grammars each table in the adaptor C base distribution H at that table; thus the n th sample tree from the adap-tor C
CRPs and PYPs differ in exactly how the sequence { z ( z 1 , . . . , z n max( z ) . Then a CRP generates the next table index z
P( Z where n in z and  X  &gt; 0 is an adjustable parameter that deter-mines how often a new table is chosen. This means that if C it generates is the same as a previously generated of times C tree t sampled from H tional to  X  get-richer X  effect in which popular trees are gener-ated with increasingly high probabilities.

Pitman-Yor Processes can control the strength of this effect somewhat by moving mass from existing tables to the base distribution. The PYP predictive distribution is: P( where a  X  [0 , 1] and b &gt; 0 are adjustable parame-ters. It X  X  easy to see that the CRP is a special case of the PRP where a = 0 and b =  X  .

Each adaptor in an adaptor grammar can be viewed as estimating the probability of each adapted subtree t ; this probability can differ substantially from t  X  X  probability H tion. Because Word s are adapted in the unigram adaptor grammar it effectively estimates the proba-bility of each Word tree separately; the sampling es-timators described in section 4 only instantiate those Word s actually used in the analysis of Sentence s in the corpus. While the Word adaptor will generally prefer to reuse Word s that have been used elsewhere in the corpus, it is always possible to generate a fresh Word using the CFG rules expanding Word into a string of Phoneme s.
 We assume for now that all CFG rules R panding the nonterminal X  X  N have the same probability (although we will explore estimating  X  below), so the base distribution H keys banging on typewriters X  model. That means the unigram adaptor grammar implements the Goldwa-ter et al. (2006a) unigram word segmentation model, and in fact it produces segmentations of similar ac-curacies, and exhibits the same characteristic under-segmentation errors. As Goldwater et al. point out, because Word s are the only units of generalization available to a unigram model it tends to misanal-yse collocations as words, resulting in a marked ten-dancy to undersegment.

Goldwater et al. demonstrate that modelling bi-gram dependencies mitigates this undersegmenta-tion. While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocations (Johnson, 2008). A collocation adaptor grammar generates a Sentence as a sequence of Colloc ations, each of which expands to a sequence of Word s. Because Colloc is adapted, the collocation adap-tor grammar learns Colloc ations as well as Word s. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar seg-ments as well as the Goldwater et al. bigram model, which we confirm here.

Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as se-quences of syllables can learn and exploit these con-straints, significantly improving segmentation accu-racy. Here we present an adaptor grammar that mod-els collocations together with these phonotactic con-straints. This grammar is quite complex, permitting us to study the effects of the various model and im-plementation choices described below on a complex hierarchical nonparametric Bayesian model.

The collocation-syllable adaptor grammar gen-erates a Sentence in terms of three levels of Colloc ations (enabling it to capture a wider range of interword dependencies), and generates Word s as sequences of 1 to 4 Syllable s. Syllable s are subcat-egorized as to whether they are initial (I), final (F) or both (IF).
 Sentence  X  Colloc3 + Colloc3  X  Colloc2 + Colloc2  X  Colloc1 + Colloc1  X  Word + Word  X  SyllableIF Word  X  SyllableI (Syllable) (Syllable) SyllableF Syllable  X  Onset Rhyme Onset  X  Consonant + Rhyme  X  Nucleus Coda Nucleus  X  Vowel + SyllableIF  X  OnsetI RhymeF OnsetI  X  Consonant + RhymeF  X  Nucleus CodaF CodaF  X  Consonant + SyllableI  X  OnsetI Rhyme SyllableF  X  Onset RhymeF Here Consonant and Vowel expand to all possible consonants and vowels respectively, and the paren-theses in the expansion of Word indicate optional-ity. Because Onset s and Coda s are adapted, the collocation-syllable adaptor grammar learns the pos-sible consonant sequences that begin and end syl-lables. Moreover, because Onset s and Coda s are subcategorized based on whether they are word-peripheral, the adaptor grammar learns which con-sonant clusters typically appear at word boundaries, even though the input contains no explicit word boundary information (apart from what it can glean from the sentence boundaries). Adaptor grammars as defined in section 2 have a large number of free parameters that have to be chosen by the grammar designer; a rule probabil-ity  X  two hyperparameters for each adapted nonterminal X  X  A , depending on whether Chinese Restaurant or Pitman-Yor Processes are used as adaptors. It X  X  difficult to have intuitions about the appropriate set-tings for the latter parameters, and finding the opti-mal values for these parameters by some kind of ex-haustive search is usually computationally impracti-cal. Previous work has adopted an expedient such as parameter tying. For example, Johnson (2008) set  X  by requiring all productions expanding the same nonterminal to have the same probability, and used Chinese Restaurant Process adaptors with tied pa-rameters  X 
We now describe two methods of dealing with the large number of parameters in these models that are both more principled and more practical than the ap-proaches described above. First, we can integrate out  X  , and second, we can infer values for the adap-tor hyperparameters using sampling. These meth-ods (the latter in particular) make it practical to use Pitman-Yor Process adaptors in complex grammars such as the collocation-syllable adaptor grammar, where it is impractical to try to find optimal parame-ter values by grid search. As we will show, they also improve segmentation accuracy, sometimes dramat-ically. 3.1 Integrating out  X  Johnson et al. (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities  X  , and these techniques can be used directly with adap-tor grammars as well. Just as in that paper, we place Dirichlet priors on  X  : here  X  of  X  corresponding to rules expanding nonterminal X  X  N , and  X  X is a corresponding vector of posi-tive real numbers specifying the hyperparameters of the corresponding Dirichlet distributions: Because the Dirichlet distribution is conjugate to the multinomial distribution, it is possible to integrate out the rule probabilities  X  , producing the  X  X ollapsed sampler X  described in Johnson et al. (2007a).
In our experiments we chose an uniform prior  X  integrating out  X  only has a major effect on re-sults when the adaptor hyperparameters themselves are not sampled, and even then it did not have a large effect on the collocation-syllable adaptor grammar. This is not too surprising: because the Onset , Nucleus and Coda adaptors in this gram-mar learn the probabilities of these building blocks of words, the phoneme probabilities (which is most of what  X  encodes) play less important a role. 3.2 Slice sampling adaptor hyperparameters As far as we know, there are no conjugate priors for the adaptor hyperparameters a responds to  X  so it is not possible to integrate them out as we did with the rule probabilities  X  . However, it is possible to perform Bayesian inference by putting a prior on them and sampling their values.

Because we have no strong intuitions about the values of these parameters we chose uninformative priors. We chose a uniform Beta(1 , 1) prior on a and a  X  X ague X  Gamma(10 , 0 . 1) prior on b (MacKay, 2003). (We experimented with other pa-rameters in the Gamma prior, but found no signifi-cant difference in performance).

After each Gibbs sweep through the parse trees t we resampled each of the adaptor parameters from the posterior distribution of the parameter using a slice sampler 10 times. For example, we resample each b P( b Here P( t | b quence of sample parse trees (we only need the fac-tors that depend on b is the prior. The same formula is used for sampling a tribution.

In general we cannot even compute the normaliz-ing constants for these posterior distributions, so we chose a sampler that does not require this. We use a slice sampler here because it does not require a pro-posal distribution (Neal, 2003). (We initially tried a Metropolis-Hastings sampler but were unable to find a proposal distribution that had reasonable ac-ceptance ratios for all of our adaptor grammars).
As Table 1 makes clear, sampling the adaptor pa-rameters makes a significant difference, especially on the collocation-syllable adaptor grammar. This is not surprising, as the adaptors in that grammar play many different roles and there is no reason to to expect the optimal values of their parameters to be similar. Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implemen-tation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian in-ference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed be-low will be relevant to other applications as well.
The inference algorithm maintains a vector t = ( t 1 , . . . , t n parse for the i th sentence w sentence w t for w and the parses t 4.1 Maximum marginal decoding Sampling algorithms like ours produce a stream of samples from the posterior distribution over parses of the training data. It is standard to take the out-put of the algorithm to be the last sample produced, and evaluate those parses. In some other applica-tions of nonparametric Bayesian inference involv-ing latent structure (e.g., clustering) it is difficult to usefully exploit multiple samples, but that is not the case here.

In maximum marginal decoding we map each sample parse tree t onto its corresponding word seg-mentation s , marginalizing out irrelevant detail in t . (For example, the collocation-syllable adaptor grammar contains a syllabification and collocational structure that is irrelevant for word segmentation). Given a set of sample parse trees for a sentence we compute the set of corresponding word segmenta-tions, and return the one that occurs most frequently (this is a sampling approximation to the maximum probability marginal structure).
 For each setting in the experiments described in Table 1 we ran 8 samplers for 2,000 iterations (i.e., passes through the training data), and kept the sam-ple parse trees from every 10th iteration after itera-tion 1000, resulting in 800 sample parses for every sentence. (An examination of the posterior proba-bilities suggests that all of the samplers using batch initialization and table label resampling had  X  X urnt in X  by iteration 1000). We evaluated the word to-ken f-score of the most frequent marginal word seg-mentation, and compared that to average of the word token f-score for the 800 samples, which is also re-ported in Table 1. For each grammar and setting we tried, the maximum marginal segmentation was bet-ter than the sample average, sometimes by a large margin. Given its simplicity, this suggests that max-imum marginal decoding is probably worth trying when applicable. 4.2 Batch initialization The Gibbs sampling algorithm is initialized with a set of sample parses t for each sentence in the train-ing data. While the fundamental theorem of Markov Chain Monte Carlo guarantees that eventually sam-ples will converge to the posterior distribution, it says nothing about how long the  X  X urn in X  phase might last (Robert and Casella, 2004). In practice initialization can make a huge difference to the per-formance of Gibbs samplers (just as it can with other unsupervised estimation procedures such as Expec-tation Maximization).

There are many different ways in which we could generate the initial trees t ; we only study two of the obvious methods here. Batch initialization assigns every sentence a random parse tree in parallel. In more detail, the initial parse tree t is sampled from P( t | w obtained from the adaptor grammar by ignoring its last two components A and C (i.e., the adapted non-terminals and their adaptors), and seated at a new table. This means that in batch initialization each initial parse tree is randomly generated without any adaptation at all.

Incremental initialization assigns the initial parse trees t grammar as it goes. That is, t w , t 1 , . . . , t i  X  1 ) . This is easy to do in the context of Gibbs sampling, since this distribution is a minor variant of the distribution P( t Gibbs sampling itself.

Incremental initialization is greedier than batch initialization, and produces initial sample trees with much higher probability. As Table 1 shows, across all grammars and conditions after 2,000 iterations incremental initialization produces samples with much better word segmentation token f-score than does batch initialization, with the largest improve-ment on the unigram adaptor grammar.

However, incremental initialization results in sample parses with lower posterior probability for the unigram and collocation adaptor grammars (but not for the collocation-syllable adaptor grammar). Figure 1 plots the posterior probabilities of the sam-ple trees t at each iteration for the collocation adap-tor grammar, showing that even after 2,000 itera-tions incremental initialization results in trees that are much less likely than those produced by batch initialization. It seems that with incremental initial-ization the Gibbs sampler gets stuck in a local op-timum which it is extremely unlikely to move away from.

It is interesting that incremental initialization re-sults in more accurate word segmentation, even though the trees it produces have lower posterior probability. This seems to be because the most prob-able analyses produced by the unigram and, to a lesser extent, the collocation adaptor grammars tend to undersegment. Incremental initialization greed-ily searches for common substrings, and because such substrings are more likely to be short rather than long, it tends to produce analyses with shorter words than batch initialization does. Goldwater et al. (2006a) show that Brent X  X  incremental segmenta-tion algorithm (Brent, 1999) has a similar property.
We favor batch initialization because we are in-terested in understanding the properties of our mod-els (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initializa-tion as (say) cognitively more plausible. 4.3 Table label resampling Unlike the previous two implementation choices which apply to a broad range of algorithms, table label resampling is a specialized kind of Gibbs step for adaptor grammars and similar hierarchical mod-els that is designed to improve mobility. The adap-tor grammar algorithm described in Johnson et al. (2007b) repeatedly resamples parses for the sen-tences of the training data. However, the adaptor grammar sampler itself maintains of a hierarchy of Chinese Restaurant Processes or Pitman-Yor Pro-cesses, one per adapted nonterminal X  X  A , that cache subtrees from T subtrees will occur many times in the parses for the training data sentences. Table label resampling re-samples the trees in these adaptors (i.e., the table labels, to use the restaurant metaphor), potentially changing the analysis of many sentences at once. For example, each Colloc ation in the collocation adaptor grammar can occur in many Sentence s, and each Word can occur in many Colloc ations. Resam-pling a single Colloc ation can change the way it is analysed into Words , thus changing the analysis of all of the Sentence s containing that Colloc ation.
Table label resampling is an additional resam-pling step performed after each Gibbs sweep through the training data in which we resample the parse trees labeling the tables in the adaptor for each X  X  A . Specifically, if the adaptor C X for X  X  A currently contains m tables labeled with the trees t = ( t places each t pled from P( t | t yield of t ple all of the trees t in a randomly chosen order).
Table label resampling is a kind of Gibbs sweep, but at a higher level in the Bayesian hierarchy than the standard Gibbs sweep. It X  X  easy to show that ta-ble label resampling preserves detailed balance for the adaptor grammars presented in this paper, so in-terposing table label resampling steps with the stan-dard Gibbs steps also preserves detailed balance.
We expect table label resampling to have the greatest impact on models with a rich hierarchi-cal structure, and the experimental results in Ta-ble 1 confirm this. The unigram adaptor grammar does not involve nested adapted nonterminals, so we would not expect table label resampling to have any effect on its analyses. On the other hand, the collocation-syllable adaptor grammar involves a rich hierarchical structure, and in fact without table la-bel resampling our sampler did not burn in or mix within 2,000 iterations. As Figure 1 shows, table label resampling produces parses with higher pos-terior probability, and Table 1 shows that table la-bel resampling makes a significant difference in the word segmentation f-score of the collocation and collocation-syllable adaptor grammars. This paper has examined adaptor grammar infer-ence procedures and their effect on the word seg-mentation problem. Some of the techniques inves-tigated here, such as batch versus incremental ini-tialization, are quite general and may be applica-ble to a wide range of other algorithms, but some of the other techniques, such as table label resam-pling, are specialized to nonparametric hierarchi-cal Bayesian inference. We X  X e shown that sampling adaptor hyperparameters is feasible, and demon-strated that this improves word segmentation accu-racy of the collocation-syllable adaptor grammar by almost 10%, corresponding to an error reduction of over 35% compared to the best results presented in Johnson (2008). We also described and investigated table label resampling, which dramatically improves the effectiveness of Gibbs sampling estimators for complex adaptor grammars, and makes it possible to work with adaptor grammars with complex hier-archical structure.
 We thank Erik Sudderth for suggesting sampling the Pitman-Yor hyperparameters and the ACL review-ers for their insightful comments. This research was funded by NSF awards 0544127 and 0631667 to Mark Johnson.
