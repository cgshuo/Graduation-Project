 D.2.8 [ Software ]: Software Engineering X  Metrics: Process met-rics ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing methodology; language engineering economics; NERC; cost met-rics; machine learning; named entity tagging Economics, Experimentation, Measurement
Computational Linguistics as a scientific discipline uses meth-ods to obtain knowledge of human language processing that must be subjected to repeated falsification attempts based on criteria of descriptive and explanatory adequacy, among others, but which is not affected by practical concerns such as computational efficiency. Language Engineering, on the other hand, is a technological disci-pline. Information Extraction, and its most essential part, Named Entity Recognition and Classification (NERC), are part of this tech-nological (as opposed to purely knowledge-seeking) realm, and like for any engineering discipline, they should thus abide by engineer-ing principles. One principal desideratum of engineering work is that the engineer develops a product or service to a specification , which acts as a set of external constraints imposed on him or her from the outside [2]. Such constraints reflect limited resources in the physical environment in which engineering work takes place: time is precious (most projects are of short duration), the number of staff allocated to a project is limited (and increasing staff count decreases productivity), and most projects have rigid budgets. The goal of this study is to assess empirically the development cost of named entity taggers for the same task, using three alternative methods.
In Information Extraction, data about system development ef-forts are not easily available. Riloff has shown, for instance, that dictionaries for a typical IE tasks can be extracted in as little as 5 person hours using a bootstrapping approach [6]; however, her comparison of a novel method with her previously constructed man-ual dictionary that reportedly required around 1,500 person hours is based on an estimate rather than a controlled measurement. We are not aware of previous work comparing development time for named entity taggers in a controlled fashion.
Inspired by the Surprise Language Task [5], a  X  X urprise domain X  (chosen and annotated without the knowledge of the system de-velopers) was selected: an astronomy data set comprising abstracts of radio astronomical papers was picked, in which the non-standard entity types instrument name (names of measurement instruments), source name (celestial objects), source type (types of objects), and spectral feature (spectral lines and their properties) needed to be marked up [1]. Then a group of developers set out to develop three named entity taggers for these named entity classes using three dif-ferent methods.
To measure the cost in time of developing the three named entity taggers described in this report, we used a Web-based time track-ing tool. Participating language engineers and domain experts were asked to bookmark the location of a Web based graphical user in-terface, and to use it to record briefly their identity and the length and content of each work session. Each time slot was categorised into one of five classes: (I.) System 1: Co-training; (II.) System 2: Active learning; (III.) System 3: Clustering; (IV) Infrastructure; and (V) Communication. The first three of these indicate that time was dedicated to a task specific to one of the three methods covered in this study. The Active Learning effort includes five hours of ad-ditional, (inter-)active annotation. The Infrastructure category was Table 1: Person hours per method and resulting performance. used to assign tasks to that relate to the overall setup, such as writ-ing batch scripts to evaluate or convert data-sets. Communication
The resulting time records by category are shown in Figure 1. If the Infrastructure and Communication categories are added to each method X  X  individual development time, we can obtain a conserva-tive estimate of cost per method (Table 1).
Individuals developed different recording behaviour: some tracke d their work immediately, whereas others preferred to take paper notes and track their time in one  X  X atch X  session. Researchers were al-lowed to record other researchers X  time on their behalf, and this was used, for instance using email to ask for time to be accounted when remote access to the intranet (to which the use of the time tracking tool was restricted) was felt to be more cumbersome than sending an email. In our study, Co-training was found to have the highest cost by far; Clustering was found to have the least cost and performance. The differences are not very dramatic in absolute terms, but the evidence overall seems to favour Active Learning. This raises the question whether differences are truly caused by a method or whether they are an artifact of developer experience. Our time monitoring setup can be criticised for its lack of strictness: it does not enforce technically that every minute is really accounted for, because monitoring is a voluntary activity, and while there is no incentive to track time, researchers have many motivations to ignore it: it might be forgotten or neglected due to time pressure. One alternative is automatic time tracking; however, this is diffi-cult to achieve in an environment where researchers have to bal-ance their time between several projects. In addition, there might be ethical implications: for instance, [3] use WinVNC to monitor corporate use of email automatically and without the knowledge of employees. We believe this represents an intrusion of the subjects private sphere, and is illegal without consent in many countries. The time tracking tool used in this study appears to be a practical compromise; it was easy to implement and to use, and a certain lack of accuracy is outweighed by ethical or legal advantages. The results allow a rough quantification of the development cost of the ble 1, we find that 35 days are required (development time, not cal-work should consider modelling development cost to ultimately al-low approximately correct project cost predictions.
We present the first study of (time) resource monitoring for the construction of a set of three named entity taggers for the same task, based on three different, previously published methods X  X o-training, Clustering, and Active Learning. The results show that development cost does not differ dramatically across alternative methods, and the fact that Co-training development cost was found higher than Active Learning leads us to the conjecture that differ-ences might be artifacts of developer experience rather than intrin-sic advantages or disadvantages of particular methods. We believe further (eventually stricter and more fine-grained) time monitoring experiments will have to be conducted to develop Language Engi-neering into a discipline that abides by the engineering principle of development to specification [2].
 Acknowledgements. This study was funded in part by Edinburgh-Stanford Link grant R36759 and DAAD scholarship D/02/01831. We thank the implementors/domain experts for this study: B. Alex, M. Becker, S. Dingare, R. Dowsett, B. Hachey, O. Johnson, Y. Krymolowski, R. Mann and M. Nissim. [1] M. Becker, B. Hachey, B. Alex, and C. Grover. Optimising [2] H. Czichos, M. Hennecke, and Akademischer Verein [3] T. W. Jackson, R. Dawson, and D. Wilson. Understanding [4] C. F. Kemerer. An empirical validation of software cost [5] D. W. Oard. The surprise language exercises. ACM [6] E. Riloff. Automatically constructing a dictionary for experts of $75,000 per year (taken from a real job ad), and assumin g 300 workdays per year, the staff cost per capita and day amounts to $250. The 64 person days consumed by the project then amount to $16,000. member waiting for another developer to complete a task.
