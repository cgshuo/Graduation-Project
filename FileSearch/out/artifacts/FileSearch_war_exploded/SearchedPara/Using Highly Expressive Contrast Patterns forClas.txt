 Classification is a well studied area in data mining. Contrast patterns [1,2] capture strong differences be tween classes, and useful for building accurate classi-fiers. Existing pattern-based classifiers consider simple cont rasts, such as emerg-ing patterns [3], which are conjunctions of attribute values. A highly expressive class of contrast, namely disjunctive emerging patterns [4], allows disjunctions as well as conjunctions of attribute values. Their use for classification is an open question though, which we aim to answer in this paper.

Expressive contrasts can potentially overcome some of the limitations of sim-ple contrasts. E.g. the following disjunctive pattern from the income [5] data set: [ age  X  [30 .. 39]  X  ( industry = X  X anufacturing X   X   X  X ransportation X )] differen-tiates males from females, being true for more than 10% of the males but not true for any female. If the two industries were considered individually, the non-disjunctive combination [ age  X  [30 .. 39]  X  industry =  X  X anufacturing X  X , would be true for far fewer males, thus, a weaker contrast. This issue often arises when the data is sparse, or lacking in data insta nces. Despite their low frequency, rare contrasts can be useful for classification, but they are often not identified.
Since emerging patterns assume discrete data, the rarity of contrasts can also result from the data discretisation used, when the input data set has a continuous-valued domain. We call this problem the resolution problem .In coarsely discretised data, patterns may be lacking class-distinguishing ability, but in a finely discretised data, patterns may be lacking frequencies (or sup-port). Expressive patterns provide a solution to this problem, by allowing several discrete attribute-values to be combined into a disjunction.

Expressive contrasts may help remedy the above-mentioned situations, but they may also have limitations: i) an increased number of patterns become avail-able, ii) more patterns may be noisy. E.g., [( age  X  [20 .. 24]  X  [40 .. 44])  X  industry =  X  X anufacturing X  X  is a valid disjunctive pattern, but the two age groups, [20 .. 24] and [40 .. 44], may be irrelevant. Such irrelevance within a pattern may, in turn, cause misclassification. To address this issue, we propose a method for statisti-cally testing the significance of disjunctive patterns.

This paper investigates the advantages and disadvantages of using highly ex-pressive contrasts, instead of simple con trasts, for classification. We aim to an-swer the following questions: i) When should disjunctions be allowed in contrast patterns for building a classifier? ii) Which types of contrast patterns are most suitable for various data characteristics? Our contributions are three-fold:  X  We propose a classifier model based on disjunctive emerging patterns [4]. To  X  We present experimental results using several real [5] data sets, to study  X  Based on our findings, we present a series of recommendations for practition-A dataset D is defined upon a set of k attributes (also referred as dimensions) {
A 1 ,A 2 ,...,A k } . For every attribute A i , the domain of its values (or items) is denoted by dom ( A i ). Let I be the aggregate of the domains across all the attributes, i.e. I = k i =1 dom ( A i ). An itemset is a subset of I .Let P and Q be two itemsets. We say P contains Q if Q is a subset of P , Q  X  P ,and P is a superset of Q .A dataset is a collection of transactions, each transaction T is a set of attribute-values, i.e. T  X  I . The number of transactions in D is denoted by | D | .The support of an itemset P in dataset D , denoted by support ( P, D ), is the transactions in D which contain P , divided by | D | (0  X  support ( P, D )  X  1).
Assume two classes in dataset D ,namely D p (the positive class) and D n (the negative class). The support ratio of an itemset between two classes, termed as growth rate (gr) : gr ( P, D p ,D n )= support ( P,D p ) support ated with a discriminating power (or contrast strength ): strength ( P, D p ,D n )= ing Pattern (EP) [3] is a simple contrast pattern, defined as an itemset P , s.t. support ( P, D n )  X   X  (i.e. infrequent in D n ), and support ( P, D p )  X   X  (i.e. frequent in D p ). Moreover, P is a minimal emerging pattern if it does not contain other emerging patterns. A Jumping Emerging Pattern (JEP) is an EP which has an infinite growth rate. In the remainder of this paper we use the term pattern to refer to an emerging pattern. The support of a pattern refers to its support in the positive class.

A Disjunctive Emerging Pattern (DEP) is an itemset P which con-tains one or more items from the domain of every attribute, and satifies two support constraints: i) support ( P, D p )  X   X  , and ii) support ( P, D n )  X   X  .E.g. { c trasts as conjunctions of disjunctions (CNF), where disjunctions are only al-lowed between items within attributes. The boolean function that x represents, multi-dimensional space considers x as a subspace (see Fig. 1a). Thus, we can calculate support by counting the transactions which are subsets of x .
For an attribute with an ordered domain, a set of adjacent items within the same dimension (or attribute) is called a contiguous itemset. We call a set of non-occuring items between two adjacent items (within the same dimension) as a gap . If the gap in each dimension of x is no larger than a given minimum threshold g ,thenwe say that x is a g -contiguous itemset, where 0  X  g  X  k  X  2, k is the number of domain items for that attribute. Moreover, a disjunctive pattern is a g -contiguous pattern if it does not contain non g -contiguous itemsets in any of its dimensions. Consider Fig. 1, x is g -contiguous for g  X  2, and y is g -contiguous for g  X  1. For the purpose of our study, we use the existing JEP-classifier framework [2] as a baseline, which is highly accurate for dense and large datasets. It is based on minimal JEPs, which are considered the most powerful JEPs for classifica-tion, since their supports are largest. To adapt the framework, our classifier uses maximal disjunctive patterns which have infinite growth rate. Given a test in-stance T , all patterns which contain T can be found from each class. Based on its distinguishing class fre quencies, a JEP favors D p over D n . Each pattern which occurs in T makes a contribution to classify T as an instance of D p , based on its support . The JEP classifier then chooses the class which has the highest total contribution to be the winner.

Since disjunctive patterns are relatively longer (i.e. contain more items) than the simple patterns, intuitively not every item makes an equally-high contri-bution to the contrast strength of a pattern. Thus, we propose two levels of significance testing: i) external significance: tests whether the pattern is highly associated with the class, ii) internal sign ificance: tests whether each element in a pattern makes a significant contribution in the pattern X  X  strength. 3.1 Statistical Fisher Exact Test and Externally Significant Patterns Work in [6] showed that the Fisher Exact Test (FET) is useful for finding statis-tically significant associatio n rules, which makes it potentially useful for contrast patterns as well. To test the significance of a pattern P , FET uses a 2x2 contin-gency table containing the support of P and its complemented support in each class (shown in Table 1). The test returns a p -value, which is a probability that the null-hypothesis should be accepted, i.e. there is no significant association between the pattern and the class. If the p -value is below the significanceLevel (typically 0.05), we reject the hypothesis and say P is externally significant . Given a contingency table [ a, b ; c, d ], and n = a + b + c + d .The p -value is computed by: 3.2 Internally Significant Di sjunctive Emerging Patterns The testing methodology for significant association rules [6] tests whether the inclusion of each condition significant ly contributes to the rule X  X  association. However, it was originally fashioned for purely conjunctive rules. To adapt the method for our needs, we use a negative representation of a disjunctive pattern, which is a pure conjunction of negated items. A pattern is significant if each of the negated items makes a significant contribution. This differs from previous work on significant association r ules, which consider conjunctions of positive items, instead of negative items. E.g. The NNF (Negative Normal Form) representation of a disjunctive pattern x in Fig. 1a, denoted f N ( x ), is the conjunction of the non-occurring items: f N ( x )=(  X  a 3 )  X  (  X  b 2  X  X  b 3 )  X  (  X  c 3  X  X  c 4 ).
Given ordered attribute domains, a disjunctive pattern can be projected to a subspace, possibly with some holes in it (correspond to gaps ). Small holes may not be worth retaining if they contain very few data instances from the positive class. On the other hand, big holes may b e necessary if they contain many data instances from the negative class. A gap is a significant gap if it passes the internal significance test. We call the generalisation of a pattern that is obtained by filling-in a gap as the gap-filled generalisation . A gap is maximal if it is not a subset of another gap. If all maximal gaps in a pattern are significant, then we say that the pattern is internally significant .

E.g. Reconsider pattern x = { a 1 ,a 2 ,a 4 ,b 1 ,b 4 ,c 1 ,c 2 } .Itcontainsthreemax-imal gaps:  X { a 3 } ,  X { b 2 ,b 3 } ,  X { c 3 ,c 4 } . These correspond to the negative rep-resentation of x . The significance of a gap  X  z is calculated between x and its generalisation (by inverting  X  z to z ). Let z = { b 2 ,b 3 } .Let y be the gap-filled calculate the p -value using Eq. 1 and the contingency table in Table 1, by letting to transactions in D p (resp. D n ) which support y .Alow p -value indicates the significance of gap  X { b 2 ,b 3 } in x . 3.3 Classification by Significant D isjunctive Emerging Patterns Our classifier is built based on the maximal disjunctive patterns which have an infinite growth rate. Using only those patterns, however, may overfit the training data. In real situations, there may be training instances which have significant association with the class, but are overlooked, due to the strict infinite growth rate constraint. To eliminate this problem, our classifier allows some limited constraint violation by filling-in the insignificant gaps, based on two criteria: i) the gap is not significant in the original pattern, and ii) the resulting gap-filled pattern is externally significant. Thus, all patterns which are used by the classifier are externally and internally significant. We refer to such patterns as significant disjunctive patterns . In this section, we study the performance of our classifier described in Section 3.3, based on significant disjunctive patterns, which we call CNF-Classifier . We will compare its classification performance against strictCNF-Classifier ,whichisalso based on significant disjunctive patterns, but does not employ the significance testing (strictly imposing the support constraints on the patterns). As a baseline, we also use the Jumping Emerging Pattern Classifier ( JEPC ) [2]. The accuracy is based on 10-fold stratified cross validation. We use four data sets [5], which contain continuous attributes, and categorise them by their sparsity/density. The first two data sets are dense, namely breast-cancer-w and horse-colic ,which contain two classes. The other data sets, wine and glass , contain multiple classes and are considered sparser. The glass data set is greatly imbalanced and ex-tremely sparse, having 7 classes wit h only a few instances in each class. Performance comparison with respect to discretisation granularity: In this experiment, we vary the number of bins (or discretised intervals) when discretising each data set using equal-density discretisation. Fig. 2 shows the classification accuracies from two data sets. In the breast-cancer-w data set, it is shown that the CNF-Classifier has the hig hest accuracy for all scenarios. Given finer granularties (i.e. more bins), strictCNF-Classifier and CNF-Classifier are able to outperform JEPC by 12% accuracy. In the horse-colic data set, the CNF-Classifier is more accurate than the strictCNF-Classifier when 6 or more bins are used, but it is less accurate otherwise. It shows that the significance test is useful when the data is finely discretised. The JEPC has the lowest accuracy in this data set.
 Sensitivity of classification with respect to the support constraint: We now compare the sensitivity of the classifier w.r.t. to the minimum support of the contrast patterns. Fig. 3 shows the lower bound of the accuracy for various support thresholds, which is computed as (mean -2 st.dev) ,foreachdiscreti-sation granularity. In the dense data sets, JEPC has the lowest lower bound and its accuracy greatly varies across the discretisation granularities. When a 12-bin discretisation was used for the breast-cancer-w data set, the JEPC has a mean accuracy of 79%, which indicates its large deviation or sensitivity w.r.t. the support constraint. (the relevant figure is not included in this paper, due to space limitation). The other classifiers, on the other hand, have mean accura-cies of 96% and 99%, showing their low sensitivity w.r.t. the support constraint. In the sparse glass data set, the strictCNF-Classifier has a similar performance to JEPC, whereas the CNF-Classifier has a high sensitivity w.r.t the minimum support threshold and the data discretisation. In the less sparse wine data set, CNF-Classifier has the highest lower bound accuracy, and JEPC has the lowest lower bound.
 Practical recommendations for users: Answering the questions posed at the beginning of this paper, we now present our recommendations:
When should disjunctions be allowed in contrast patterns for building a classi-fier? Disjunctions should be allowed in contrast patterns when the data is sparse, that is when the classes are imbalanced, or when the data is finely discretised, e.g. 8 bins or finer.

Which types of contrast patterns are most suitable for various data character-istics? When the data is sparse, expressive contrasts are more appropriate than simple contrasts. The significance test should be performed, except when the data is greatly imbalanced. Simple contrasts are useful for dense and coarsely discretised data sets. A contrast pattern is similar to a highly confident class association rule [7]. More expressive association rules ha ve been studied [8], but they allow DNF (disjunction of conjunctions) rules, instead of CNF, which is the kind of rules con-sidered in this paper. Our significance testing methodology could be extended for disjunctive association rules. Previous work on significant association rules [9,6] only considers conjunctive rules. In an ordered domain, contiguous disjunctive patterns correspond to quantitative association rules [10], which are conjunctions of intervals of ordered values, however gaps are disallowed in a quantitative as-sociation rule. The negative representation of a disjunctive pattern in this paper is similar to a negative association rule [ 11], but the rule X  X  antecedent contains only negative items, and the consequent contains a class label. In this paper, we investigated the advantages and disadvantages of using expres-sive (in the form of CNF combinations) contrast patterns in classification. We proposed a statistical testing for finding significant CNF patterns, which can also be adopted for disjunctive association rules or negative association rules. As our results suggest, expressive forms of patterns can be beneficial for classification, being less sensitive to the data sparsity. For future research, we would like to investigate their use in other types of classifiers and other data mining tasks.
