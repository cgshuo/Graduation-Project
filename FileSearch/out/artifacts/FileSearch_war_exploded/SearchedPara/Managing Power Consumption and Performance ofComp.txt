 customers and for government regulators concerned with ene rgy and environmental matters. To cite a prominent example, the US Congress recently mandated a stu dy of the power efficiency of servers, including a feasibility study of an Energy Star standard for servers and data centers [16]. Growing interest in power management is also apparent in the formati on of the Green Grid, a consortium of systems and other vendors dedicated to improving data cente r power efficiency [7]. Recent trade press articles also make it clear that computer purchasers a nd data center operators are eager to reduce power consumption and the heat densities being exper ienced with current systems. In response to these concerns, researchers are tackling int elligent power control of processors, mem-ory chips and whole systems, using technologies such as proc essor throttling, frequency and voltage manipulation, low-power DRAM states, feedback control usi ng measured power values, and packing and virtualization to reduce the number of machines that nee d to be powered on to run a workload. This paper presents a reinforcement learning (RL) approach to developing effective control poli-cies for real-time management of power consumption in appli cation servers. Such power manage-ment policies must make intelligent tradeoffs between powe r and performance, as running servers in low-power modes inevitably degrades the application per formance. Our approach to this entails designing a multi-criteria objective function U and using it to give reward signals in reinforcement learnin g. We let U application response time RT , and total power Pwr consumed by the servers in a decision int erval. Specifically, U tives. This approach admits other objective functions such as  X  X erformance value per watt X  U pp = U ( RT ) / straint on total power.
 The problem of jointly managing performance and power in IT-systems was only recently studied in the literature [5, 6, 17]. Existing approaches use knowledg e-intensive and labor-intensive modeling, such as developing queuing-theoretic or control-theoreti c performance models. RL methods can potentially avoid such knowledge bottlenecks, by automati cally learning high-quality management policies using little or no built-in system specific knowled ge. Moreover, as we discuss later, RL may have the merit of properly handling complex dynamic and dela yed consequences of decisions. In Section 2 we give details of our laboratory testbed, while Section 3 describes our RL approach. Results are presented in Section 4, and the final section disc usses next steps in our ongoing research and ties to related work. Figure 1 provides a high-level overview of our experimental testbed. In brief, a Workload Gener-ator produces an HTTP-based workload of dynamically varyin g intensity that is routed to a blade BladeCenter containing xSeries HS20 blade servers.) A comm ercial performance manager and our RL-based power manager strive to optimize a joint power-per formance objective cooperatively as with the other manager. RL techniques (described subsequen tly) are used to train a state-action value function which defines the power manager X  X  control pol icy. The  X  X tate X  is characterized by a set of observable performance, power and load intensity me trics collected in our data collection module as detailed below. The  X  X ction X  is a throttling of CPU frequency 1 that is achieved by setting a  X  X owercap X  on each blade that provides an upper limit on the power that the blade may consume. Given this limit, a feedback controller embedded in the serv er X  X  firmware [11] continuously monitors the power consumption, and continuously regulates the CPU c lock speed so as to keep the power consumption close to, but not over, the powercap limit. The C PU throttling affects both application performance as well as power consumption, and the goal of lea rning is to achieve the optimal level of throttling in any given state that maximizes cumulative d iscounted values of joint reward U We control workload intensity by varying the number of clien ts n varied n observations of a highly accessed Olympics web site [14]. Cl ients behave according to a closed-loop model [12] with exponentially distributed think times of me an 125 msec.
 The commercial performance manager is WebSphere Extended D eployment (WXD)[18], a multi-node webserver environment providing extensive data colle ction and performance management parameters on individual blades, such as the maximum worklo ad concurrency.
 Our data collector receives several streams of data and prov ides a synchronized report to the power policy evaluator on a time scale  X  scales than  X  Among the aggregated data are several dozen performance met rics collected by a daemon running on the WXD data server, such as mean response time, queue lengt h and number of CPU cycles per transaction; CPU utilization and effective frequency coll ected by local daemons on each blade; and current power and temperature measurements collected by th e firmware on each blade, which are polled using IPMI commands sent from the BladeCenter manage ment module. 2.1 Utility function definition Our specific performance-based utility U ( RT ) in Eq. 1 is a piecewise linear function of response time RT which returns a maximum value of 1.0 when RT is less than a specified threshold RT and which drops linearly when RT exceeds RT Such a utility function reflects the common assumptions in cu stomer service level agreements that there is no incentive to improve the performance once it reac hes the target threshold, and that there experiments, we set RT At this value of  X  the power-performance tradeoff is strongly biased in favor of performance, as is commonly desired in today X  X  data centers. However, larger v alues of  X  could be appropriate in future scenarios where power is much more costly, in which case the o ptimal policies would tolerate more frequent performance threshold violations in order to save more aggressively on power consumption. 2.2 Baseline Powercap Policies To assess the effectiveness of our RL-based power managemen t policies, we compare with two different benchmark policies:  X  X N X  (unmanaged) and  X  X C X  (h and-crafted). The unmanaged policy always sets the powercap to a maximal value of 120W; we verifie d that the CPU runs at the highest frequency under all load conditions with this setting.
 The hand-crafted policy was created as follows. We measured power consumption on a blade server at extremely low ( n ranged between 75 and 120 watts. Given this range, we establi shed a grid of sample points, with p running from 75 watts to 120 watts in increments of 5 watts, an d the number of clients running from 0 to 50 in increments of 5. For each of the 10 possible settings of p minutes to permit WXD to adapt to the workload, and then decrem ented n Finally, the models RT ( p and P wr between the sampled grid points.
 We substitute these models into our utility function U ity function U  X  depending on p can then choose the optimal powercap for any workload intens ity n arg max p One may naturally question whether RL could be capable of lea rning effective control policies for systems as complex as a population of human users interactin g with a commercial web application. Such systems are surely far from full observability in the MD P sense. Without even considering depend, for example, on the states of the underlying middlew are and Java Virtual Machines (JVMs), and these states are not only unobservable, they also have co mplex historical dependencies on prior load and performance levels over multiple time scales. Desp ite such complexities, we have found in our earlier work [15, 9] that RL can in fact learn decent polic ies when using severely limited state descriptions, such as a single state variable representing current load intensity. The focus of our work in this paper is to examine empirically whether RL may ob tain better policies by including more observable metrics in the state description.
 Another important question is whether current decisions ha ve long-range effects, or if it suffices to simply learn policies that optimize immediate reward. The a nswer appears to vary in an interest-ing way: under low load conditions, the system response to a d ecision is fairly immediate, whereas under conditions of high queue length (which may result from poor throttling decisions), the respon-siveness to decisions may become sluggish and considerably delayed.
 Our reinforcement learning approach leverages our recent  X  Hybrid RL X  approach [15], which orig-inally was applied to autonomic server allocation. Hybrid R L is a form of offline (batch) RL that of (state, action, reward) tuples, and then using a standard RL/function approximator combination to learn a value function V ( s, a ) estimating cumulative expected reward of taking action a in state s . (The term  X  X ybrid X  refers to the fact that expert domain kno wledge can be engineered into the initial policy without needing explicit engineering or int erfacing into the RL module.) The learned value function V then implies a policy of selecting the action a  X  in state s with highest expected value, i.e., a  X  = arg max For technical reasons detailed below, we use the Sarsa(0) up date rule rather than Q-Learning (note that unlike textbook Sarsa, decisions are made by an externa l fixed policy). Following [15], we set the discount parameter  X  = 0 . 5 ; we found some preliminary evidence that this is superior to perform standard direct gradient training of neural net wei ghts: we train a multilayer perceptron with 12 sigmoidal hidden units, using backprop to compute th e weight changes. Such an approach is appealing, as it is simple to implement and has a proven tra ck record of success in many practical applications. There is a theoretical risk that the approach could produce value function divergence. entail any live performance costs, since we train offline. Ad ditionally, we note that instead of direct gradient training, we can use Baird X  X  residual gradient met hod [4], which guarantees convergence to local Bellman error minima. In practice we find that direct gr adient training yields good convergence to Bellman error minima in  X  5-10K training epochs, requiring only a few CPU minutes on a 3 GHz workstation.
 In implementing an initial policy to be used with Hybrid RL, o ne would generally want to exploit the best available human-designed policy, combined with su fficient randomized exploration needed expected in designing such initial policies, it would be adv antageous to be able to learn effective policies starting from simplistic initial policies. We hav e therefore trained our RL policies using an extremely simple performance-biased random walk policy fo r setting the powercap, which operates as follows: At every decision point, p by 1 watt with probability p of current mean response time to response time threshold acc ording to: p this rule implies an unbiased random walk when r = 1 and that p when r  X  1 . This simple rule seems to strike a good balance between keep ing the performance near the desired threshold, while providing plenty of explo ration needed by RL, as can been seen in Figure 2.
 Having collected training data during the execution of an in itial policy, the next step of Hybrid RL is to design an (input, output) representation and functional form of the value function approximator. Figure 2: Traces of (a) workload intensity, (b) mean respons e time, and (c) powercap and consumed power of the random-walk (RW) powercap policy.
 We have initially used the basic input representation studi ed in [15], in which the state s is repre-sented using a single metric of workload intensity (number o f clients n scalar variable X  X he powercap p sensitivity to exact learning algorithm parameter setting s. In later experiments, we have expanded the state representation to a much larger set of 14 state vari ables, and find that substantial improve-ments in learned policies can be obtained, provided that cer tain data pre-processing techniques are used, as detailed below. 3.1 System-specific innovations In our research in this application domain, we have devised s everal innovative  X  X ricks X  enabling us to achieve substantially improved RL performance. Such tri cks are worth mentioning as they are likely to be of more general use in other problem domains with similar characteristics. ity (reward) using Q-Learning. However, we can take advanta ge of the fact that total utility U in equation 1 is a linear combination of performance utility U and power cost  X   X   X  P wr . Since the separate reward components are generally observable, a nd since these should have completely different functional forms relying on different state vari ables, we propose training two separate func-tion approximators estimating future discounted reward co mponents V This type of  X  X ecompositional reward X  problem has been stud ied for tabular RL in [13], where it is shown that learning the value function components using S arsa provably converges to the correct total value function. (Note that Q-Learning cannot be used t o train the value function components, as it incorrectly assumes that the optimal policy optimizes each individual component function.) Second, we devised a new type of neuronal output unit to learn V shape of U , which is a piecewise linear function of RT , with constant value for low RT and linearly decreasing for large RT . This functional form is is not naturally approximated by ei ther a linear or a sigmoidal transfer function. However, by noting that the d erivative of U is a step function (chang-ing from 0 to -1 at the threshold), and that sigmoids give a goo d approximation to step functions, this suggests using an output transfer function that behave s as the integral of a sigmoid function. chosen so that  X   X  0 as x  X  X  X  X  . We find that this type of output unit is easily trained by stan dard backprop and provides quite a good approximation to the true expected rewards.
 We have also trained separate neural networks to estimate V tecture and a standard linear output unit. However, we found only a slight improvement in Bellman error over a simple estimator of predicted power  X  = p Hence for simplicity we used V Thirdly, we devised a data pre-processing technique to addr ess a specific rate limitation in our sys-tem that the powercap decision p 30 seconds, whereas we collect state data from the system eve ry 5 seconds. This limitation was imposed because faster variations in effective CPU speed or in load disrupt WXD X  X  functionality, as its internal models estimate parameters on much slower time scales, and in particular, it assumes that CPU speed is a constant. As a result, we cannot do standard RL o n the 5 second interval data, since this would presume the policy X  X  ability to make a new decisio n every 5 seconds. A simple way to address this would be to discard data points where a decision was not made (5/6 of the data), but this would make the training set much smaller, and we would lose va luable state transition information contained in the discarded samples. As an alternative, we di vide the entire training set into six sub-sets according to line number mod-6, so that within each subs et, adjacent data points are separated by 30 second intervals. We then concatenate the subsets to fo rm one large training set, with no loss of data, where all adjacent intervals are 30 seconds long. In effect, a sweep through such a dataset replays the experiment six times, corresponding to the six d ifferent 5-second phases within the 30-second decision cycle. As we shall see in the following secti on, such rearranged datasets result in substantially more stable policies.
 Finally, we realized that in the artificially constructed da taset described above, there is an inaccu-racy in training on samples in the five non-decision phases: s tandard RL would presume that the powercap decision is held constant over the full 30 seconds u ntil the next recorded sample, whereas we know that decision actually changes somewhere in the midd le of the interval, depending on the phase. To obtain the best approximation to a constant decisi on over such intervals, we compute an equally weighted average  X  p and train on  X  p reduction (  X  40%) in Bellman error, and the combination of this with the mo d-6 data reordering enables us to obtain substanial improvements in policy perf ormance. Figure 3: Comparison of mean metrics (a) response time, (b) p ower consumed, (c) temperature and (d) utility for six different power management policies:  X  X  N X  (unmanaged),  X  X C X  (hand-crafted),  X  X W X  (random walk),  X 2NN X  (2-input neural net),  X 15NN X  (15-i nput neural net, no pre-processing),  X 15NNp X  (15-input neural net with pre-processing).
 While we have conducted experiments in other work involving m ultiple blade servers, in this section we focus on experiments involving a single blade. Fig. 3 plot s various mean performance metrics in identical six-hour test runs using identical workload trac es for six different power management poli-cies:  X  X N X  and  X  X C X  denote the unmanaged and hand-crafted po licies described in Sec. 2.2;  X  X W X  is the random-walk policy of Sec. 3;  X 2NN X  denotes a two-inpu t (single state variable) neural net;  X 15NN X  refers to a 15-input neural net without any data pre-p rocessing as described in Sec. 3.1, and  X 15NNp X  indicates a 15-input neural net using said pre-proc essing. In the figure, the performance metrics plotted are: (a) mean response time, (b) mean power c onsumed, (c) mean temperature, and most importantly, (d) mean utility. Standard error in estim ates of these mean values are quite small, as indicated by error bars which lie well within the diamond-shaped data points. Since the runs use identical workload traces, we can also assess significance o f the differences in means across policies via paired T-tests; exhaustive pairwise comparisons show t hat in all cases, the null hypothesis of no difference in mean metrics is rejected at 1% significance lev el with P-value  X  10  X  6 . We see in Fig. 3 that all RL-based policies, after what is effe ctively a single round of policy iter-ation, significantly outperform the original random walk po licy which generated the training data. Using only load intensity as a state variable, 2NN achieves u tility close to (but not matching) the hand-crafted policy. 15NN is disappointing in that its util ity is actually worse than 2NN, for rea-sons that we discuss below. Comparing 15NNp with 15NN shows t hat pre-processing yields great improvements; 15NNp is clearly the best of the six policies. Breaking down overall utility into sep-arate power and performance components, we note that all RL-based policies achieve greater power savings than HC at the price of somewhat higher mean response times. An additional side benefit of this is lower mean temperatures, as shown in the lower left plot; this implies both lower cooling costs as well as prolonged machine life. Figure 4: Traces of the five non-random policies: (a) workloa d intensity; (b) UN response time; (c) UN powercap; (d) HC response time; (e) HC powercap; (f) 2NN re sponse time; (g) 2NN powercap; (h) 15NN response time; (i) 15NN powercap; (j) 15NNp respons e time; (k) 15NNp powercap. Fig. 4 shows the actual traces of response time, powercap and power consumed in all experiments except the random walk, which was plotted earlier. The most s alient points to note are that 15NNp exhibits the steadiest response time, keeping closest to th e response time goal, and that the powercap decsions of 15NN show quite large short-term fluctuations. W e attribute the latter behavior to  X  X ver-reacting X  to response time fluctuations above or below the ta rget value. Such behavior may well be correct if the policy could reset every 5 seconds, as 15NN p resumes. In this case, the policy could react to a response time flucutation by setting an extre me powercap value in an attempt to quickly drive the response time back to the goal value, and th en backing off to a less extreme value 5 seconds later. However, such behavior would be quite poor i n the actual system, in which the extreme powercap setting is held fixed for 30 seconds. This paper presented a successful application of batch RL co mbined with nonlinear function ap-proximation in a new and challenging domain of autonomic man agement of power and performance in web application servers. We addressed challenges arisin g both from operating in real hardware, and from limitations imposed by interoperating with commer cial middleware. By training on data from a simple random-walk initial policy, we achieved high-quality management polices that out-performed the best available hand-crafted policy. Such pol icies save more than 10% on server power while keeping performance close to a desired target.
 In our ongoing and future work, we are aiming to scale the appr oach to an entire Blade cluster, and to achieve much greater levels of power savings. With the exi sting approach it appears that power savings closer to 20% could be obtained simply by using more r ealistic web workload profiles in which high-intensity spikes are brief, and the ratio of peak -to-mean workload is much higher than in our current traffic model. It also appears that savings of  X  30% are plausible when using multi-core processors [8]. Finally, we are also aiming to learn pol icies for powering machines off when feasible; this offers the potential to achieve power saving s of 50% or more. In order to scale our approach to larger systems, we can leverage the fact that Bla de clusters usually have sets of identical machines. All servers within such a homogeneous set can be ma naged in an identical fashion by the performance and power managers, thereby making the size of t he overall state space and the action space more tractable for RL.
 An important component of our future work is also to improve o ur current RL methodology. Be-yond Hybrid RL, there has been much recent research in offline RL methods, including LSPI [10], minimizing Bellman residuals [3]. These methods are of grea t interest to us, as they typically have stronger theoretical guarantees than Hybrid RL, and have de livered impressive performance in appli-cations such as helicopter aerobatics. For powering machin es on and off, we are especially interested in offline model-based RL approaches: as the number of traini ng samples that can be acquired is likely to be severely limited, it will be important to reduce sample complexity by learning explicit state-transition models.

