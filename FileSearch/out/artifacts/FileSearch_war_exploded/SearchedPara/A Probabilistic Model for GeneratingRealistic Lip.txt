 Generative systems that model the relationship between face and speech offer a wide r ange of exciting prospects. Models combining speech and face information have been shown to improve automatic speech recognition [4]. Conversely, generating video-real istic animated faces from speech has immediate applications to the games and movie industries. T here is a strong correlation between lip movements and speech [7,10], and there have b een multiple attempts at generating an animated face to match some given speech realisti cally [2,3,9,13]. Studies have indicated that speech might be informative not only of lip movement but also of movement in the upper regions of the face [3]. Incorporating speech therefore s eems crucial to the generation of true-to-life animated faces.
 Our goal is to build a generative probabilistic model, capable of generating realis tic facial animations in real time, given speech. We first use an Active Appearance Model (AA M [6]) to extract features from the video frames. The AAM itself is generative and all ows us to produce video-realistic frames from the features. We then use a Hidden Markov Model (HMM [12]) to align phoneme labels to the audio stream of video sequences, and use thi s information to label the corresponding video frames. We propose a model which, when trained on these labelled video frames, is capable of generating new, realistic vi deo from unseen phoneme sequences. Our model is a modification of Switching Linear Dynamical Systems (SLDS [1,15]) and we show that it performs better at generation than other existing models. We compare its performance to two previously proposed models by compar ing the sequences they generate to a golden standard, features from real video sequences, and by asking volunteers to select the  X  X eal X  video in a forced-choice test.
 The results of human evaluation of our generated sequences are extremely encouraging. Our system performs well with any speech, and since it can easily handle real-time g eneration of the facial animation, it brings a realistic-looking, talking avatar w ithin reach. We used sequences from the freely available on-line news broadcast Democracy Now! The show is broadcast every weekday in a high quality MP4 format, and as such consti tutes a constant source of new data. The text transcripts are available on-line, thus gr eatly facilitating the training of a speech recognition system. We manually extr acted short video sequences of the news presenter talking (removing any inserts, telephone interviews, etc. ), cutting at  X  X atural X  positions in the stream, viz. during pauses for breath and silences. The than a month. There was no reason to restrict the data to a single person, other t han the difficulty to obtain sequences of similar quality from other sources.
 was visible and the sound was not corrupted by external sound sources. The sequences do include hesitations, corrections, incomplete words, noticeable fatigue, breath, swal lowing, etc. The speaker visibly makes an effort to speak clearly, but obviously makes no effo rt to reduce head motion or facial expression, and the data is hence probably as representative of the problem as can be hoped for.
 The data was split into independent training and test sets for a 10-fold cross val idation, based on the number of sequences in each set (rather than the total amount of data). T his resulted in training sets of an average of 60 minutes of data, and test sets of approximately 7 min. All models evaluated here were trained and tested on the same data sets. Sound features and labelling. The sequences are split into an audio and a video stream, which are treated separately (see Figure 1). From the sound stream, we extract Mel Frequency Cepstrum Coeffi-cients (MFCC) at a rate of 100Hz, using tools from the HMM Tool Kit [16], resulting in 13-dimensional feature vectors. We train a HMM on these MFCC features, and use it to align phonetic labels to the sound. This is an easier task than unrestricted speech recognition, and is done satisfactorily by a simple HMM with monophones as hidden states, where mix-tures of Gaussian distributions model the emission densities. The sound samples are labelled with the Viterbi path through the HMM that was  X  X nrolled X  with the phonetic transcription of the text.
 The labels obtained from the sound stream are then used to label the corresponding vi deo frames. The difference in rate (the video is processed at 29.97 frames per second whil e MFCC coefficients are computed at 100 Hz) is handled by simple voting: each video fra me is labelled with the phoneme that labels most of the corresponding sound frames. Face features. The feature extraction for the video was done using an Active Appearance Model (AAM [6]). The AAM represents both the shape and the texture of an object i n an image. The shape of the lower part of the face is represented by the location of 2 3 points on key features on the eyes, mouth and jaw-line (see Figure 2). Given the position of the points in a set of training images, we align them to a common co-ordinate frame and a pply PCA to learn a low-dimensional linear model capturing the shape change [5]. The intensities across the region in each example are warped to the mean shape using a simple triangul ation of the region (Fig 2), and PCA applied to the vectors of intensities sampled from each image. This leads to a low-dimensional linear model of the intensities in the mean frame. E fficient algorithms exist for matching such models to new images [6]. By combining shap e and intensity model together, a wide range of convincing synthetic faces can be generated [6]. In gives far better results as shape and texture are decoupled [8]. Since the AAM paramet ers Figure 2: The face was modelled with an AAM. A set of training images is manually lab elled as are a low-dimensional linear projection of the original object, projecting those parameters back to the high-dimensional space allows us to reconstruct the modelled part of the ori ginal image. We model the face using only phoneme labels to capture the shared information between speech and face. We use 41 distinct phoneme labels, two of which are reserved for brea th and silence, the rest being the generally accepted phonemes in the English language. Most earlier techniques that use discrete labels to generate synthetic video sequences use some form of smooth interpolation between key frames [2, 9]. This requires finding the correct key frames, and lacks the flexibility of a probabilistic formulation. Brand uses a HMM where Gaussian distributions are fitted to a concatenation of the data features and  X  delta X  features, the resulting  X  X istribution X  cannot be sampled, as it would result in non-sens ical mismatch between features and delta features. It is therefore not genuinely generativ e and obtaining new sequences from the model requires solving an optimisation problem. Under Brand X  X  approach, new sequences are obtained by finding the most likely sequence of equations can be solved relatively efficiently thanks to its block-band-diagonal st ructure. the dimensionality of the face features and T is the number of frames in a sequence. This becomes non-trivial for sequences exceeding a few tens of seconds. More important, however, is that this cannot be done in real time, as the last label of the sequence must be know n before the first observation can be computed.
 In this work, we consider more standard probabilistic models of sequential dat a, which are genuinely generative. These models are shown to outperform Brand X  X  approach for t he generation of realistic sequences.
 Switching Linear Dynamical Systems. Before introducing the SLDS, we introduce some notational conventions. We have a set of S video sequences, which we index with where T s is the length of the sequence. Continuous hidden variables are indicated as x and discrete state labels are indicated with  X  , where  X   X  [1 . . .  X ].
 In an SLDS, the sequence of observations { y } T s 1 is modelled as being a noisy version of a associated with a transition matrix A  X  and with a distribution for the output noise v and the process noise w , such that y s t = B  X  s for 2 6 t 6 T s . Both the output noise v t and the process noise w t are normally distributed with zero mean; v t  X  N ( 0 , R  X  s the phonemes, which are obtained from the sound. Notice that in general, when the state labels are not known, computing the likelihood in an SLDS is intractable as it requires the enumeration of all possible state sequences, which is exponential in T [1]. In our case, however, the state label  X  s t of each frame is known from the sound and the likelihood can be computed with the same algorithm as for a standard Linear Dynamical System s (LDS), which is linear in T . Parameter optimisation can therefore be carried out efficiently with a standard EM algorithm. Also note that neither SLDS or LDS are commonly descri bed with the explicit state bias  X   X  s vector x s t with a 1 and incorporating  X   X  s using a diagonal matrix for A  X  s to good prediction while the lack of sufficient data or, as is the case with our da ta, the `a priori known approximate independence of the data dimensions may make the reduction of the complexity of A  X  s In this form, the model is over-parametrised; it can be simplified without any lo ss of gener-ality either by fixing Q  X  s dimensionality for x and y , by setting B  X  s We trained a SLDS by maximum likelihood and used the model to generate new sequences of face observations for given sequences of labels. This was done by computing the mo st likely sequence of observations for the given set of labels. An in-depth evaluation of the trained SLDS model, when used to generate new video sequences, is given in section 4. This evaluation shows that SLDS is overly flexible: it appears to explain the data well and results in a very high likelihood, but does a poor job at generating realistic new sequences. Deterministic Process Dynamical System. We reduced the complexity of the model by simplifying its covariance structure. If we set the output noise v t of the SLDS to zero, leaving only process noise, we obtain the autoregressive hidden Markov model [11] . This model has the advantage that it can be trained using an EM algorithm when the sta te labels are unknown, but we find that it performs very poorly at data generation. If we set t he process noise w t = 0 , however, then we obtain a more useful model. The complete hidden is given by log p ( { y }|{ x } ) =  X  1 Deterministic Process Dynamical System (DPDS, see Figure 3). In our implement ation we (a) Mean L 1 distance (b) RMS Error (c) Mean L  X  distance (d) Log-likelihood model all matrices R  X  s output noise covariance over all states. It is reasonable to assume this because the features are the result of PCA and are therefore uncorrelated.
 Since in this case the labels  X  s t are known, equation (1) does not contain any hidden vari-ables. Applying EM is therefore not necessary. Deriving a closed-form solution for the ML estimates of the parameters, however, results in solving polynomial equatio ns of the order T x t = f ( {  X  } t 1 ). The log-likelihood must thus be computed by a forward iteration over all can be computed numerically in a similar fashion, by applying the chain rule iter atively at each time step and storing the result for the next step. The same could be done for o ther parameters, however for given values of A  X  s the likelihood can be computed exactly by solving a set of linear equations. This m arkedly improves the rate of convergence. An algorithm for the computation of the gradi ents with respect to A  X  s Sequence generation. Since all models parametrise the distribution of the data, we can sample them to generate new observation sequences. In order to evaluate the performance of the models and compare it to Brand X  X  model, it is however useful to generate the mos t likely sequence of observation features for a sequence of labels with the features of the corresponding real video sequence.
 For both SLDS (when B  X  s {  X  } T 1 is found by a forward iteration starting with  X y 1 =  X   X  s  X y as the current observation only depends on the previous one. In setups where artificial speech is generated, the video sequence can therefore be generated at the same time as the where d is the dimensionality of the face features (without delta features). We evaluated the models in two ways: (1) by computing the error between generated fa ce features and a ground truth (the features of real video), and (2) by asking human s ubjects to rate how they perceived the sequences. Both tests were done on the same real-world data, but partitioned differently: the comparison to the ground truth was done using 10-fold cross-validation, while the test on humans was done using a single partit ioning, due to the limited availability of unbiased test subjects.
 Test error and likelihood. In order to test the models against the ground truth, we use the sound to align the labels to the video and generate the corresponding face feat ures. We use 10-fold cross validation and evaluate the performance of the models usi ng different metrics, see Figure 4. Plot (a) shows, for different models, the L 1 error between the face Table 1: Raw results of the Psychophysical test conducted by human voluntee rs. Every model is features generated for the test sound sequences and the face features extracted from the real video. We compared the sequences generated by DPDS, Brand X  X  model and SLDS to the most likely observations under a standard HMM. This last model just generates t he mean face for each phoneme, hence resulting in very unnatural sequences. It illustrates how an obviously incorrect model nevertheless performs very similarly to the other m odels in terms of generation error. Plots (b) and (c) respectively show the corresponding R oot Mean Square (RMS) and L  X  error. We can see that, except for the SLDS which performs worse than the other methods in terms of L 1 , RMS and L  X  error, the generation error for the the traditional HMM and DPDS clearly perform worst, while SLDS performs dramatically better. The model with the highest likelihood generates the sequences with the largest er ror. The likelihood under Brand X  X  model cannot be compared directly as it has double the amount generated video sequences, and the models giving the lowest error or the highest likelihoo d are far from generating the most realistic sequences. We have therefore perform ed a rigorous test where volunteers were asked to evaluate the quality of the sequences.
 Psychophysical test. For this experiment, we trained the models on a training set of 642 sequences of an average of 5 seconds each. We then labelled the sequences in our test set, which consists of 80 sequences and 436 seconds of video from sound with phonemes. These are substantial amounts of data, showing the face in a wide variety of pos itions. We set up a web-based test, where 33 volunteers compared 12 pairs of video sequences. All video sequences had original sound, but the video stream was generated by any one of four methods: (1) from the face features extracted from the corresponding real v ideo, (2) from SLDS, (3) from Brand X  X  model and (4) from DPDS. A pool of 80 sequences w as generated from previously unseen videos. The 12 pairs were chosen such that each generat ion method was pitted against each other generation method twice (once on each side, lef t or right, in order to eliminate bias towards a particular side) in random order. F or each pair, corresponding sequences were chosen from the respective pools at random. The volunteer s were only told that the sequences were either real or artificial, and were asked to either select the real video or to indicate that they could not decide. The test is kept available on-line for validation at http://www.cs.manchester.ac.uk/ai/public/dpdseval . The results are shown in Table 1. The first row, e.g. , shows that when comparing Brand X  X  model with the DPDS, people thought that the sequence generated with the former model was real in 5 cases, could not make up their mind in 7 cases, and thought the sequence generated with DPDS was real in 54 instances. These results indicate that DPDS per forms quite well at generation, clearly much better than the two other models. Note howev er that this test discriminates the models very harshly. Despite the strong down-voti ng of Brand X  X  model in this test, the sequences generated with that model do not look all that bad. They are over-smoothed, however, and humans appear to be very sensitive to that. Also remember that Brand X  X  model is the only model considered here with a closed form solution for the parameter estimation given the labels. Contrary to the other two models, it can easily be trained in the absence of labelling, using an EM algorithm. In order to correlate human judgement with the generation errors discussed at the s tart of this section, we have computed the same error measures on the data as partitioned fo r the psychophysical test. These confirmed the earlier conclusions: the SLDS, which humans lik e least, gives the highest likelihood and the worst generation errors while DPD S and Brand X  X  model do not give significantly different errors. In this work we have proposed a truly generative model, which allows real-ti me generation of talking faces given speech. We have evaluated it both using multiple error mea sures and with a thorough test of human perception. The latter test clearly shows that o ur method perceptually outperforms the others and is virtually indistinguishable fr om reality. Compared to Brand X  X  method it is slower during training, and cannot easily be tra ined in the absence of labelling. This is a trade-off for the very fast generation and visua lly much more appealing face animation.
 In addition, we have shown that traditional metrics do not agree with human per ception. The error measures do not necessarily favour our method, but the human preference for it is very significant. We believe this deserves deeper analysis. In future work, w e plan to investigate different error measures, especially on the more directly inter pretable video frames rather than on the extracted features. We also intend to experiment with a covariance matrix per state and an unrestricted matrix structure for the transition m atrix A  X  s The log-likelihood of a sequence is given by eq. 1, which is a multiplicative funct ion of A ( x diagonal matrices and using L t to denote the log-likelihood of a single observation at time t , that  X  L 1  X  A n = 0 and  X  L t  X  A n = R  X  1  X  s There we give the gradients for diagonal matrices for simplicity of notati on and because we used diagonal matrices for this work, but the same principle applies to full matri ces. The is done for the other parameters of the model, however when the covariance is shared by a ll states, the value of the other parameters can be maximised exactly as described b elow. In x equations (3) for which the coefficients D and b are computed by Algorithm 1, which takes {  X  } , { y } and the current values of A 1 ...  X  as input:  X   X  Algorithm 1 Maximisation of L with respect to  X  and  X  for n  X  X  1 . . .  X  } do end for
