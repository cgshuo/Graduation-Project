 This paper presents a tag-topic model with Dirichlet Forest prior (TTM-DF) for semantic knowledge acquisition from blog . The TTM-DF model extends the tag-topic model (TTM) by replacing the Dirichlet prior with the Dirichlet Forest prior over the topic-word multinomial. The correlation between words are calculated to generate a set of Must-Links and Cannot-Links , then the structures of Dirichlet tre es are obtained though encoding the constraints of Must-Links and Cannot-Links. Words under the same subtrees are expected to be more correlated than words under different subtrees. We conduct experiments on a synthetic and a blog dataset. Both of the experimental results show that the TTM-DF model performs much better than the TTM model. It can improve the coherence of the underlying topics and the tag-topic distributions , and capture semantic knowledge effectively. I.2.7 [Artificial Intelligence]: Natural Language Processin g  X  Text analysis . Algorithms, Experimentation, Design. Topic model, Dirichlet For est prior, Tag, Blog Th e prevalence of Web 2.0 services and applications has brought an large amount of resources, especially text resources, such as encyclopedia, blogs, and social networks. These resources contain a wealth of semantic information, which can be applied to a variety of fields of information processing to improve the service quality. How to automatically obtain semantic knowledge from online text resources and represent them in a way that can be calculated by computer programs has become a hot research area in natural language processing(NLP). Blog is the fourth communication means in web following E-mail, BBS and instant communication, and it is an important platform for self-expression, information release and social networking. Blog can be seen as a knowledge base with wide coverage and good scalability. Tagging has recently emerged as a popular way to organize user generated content for Web 2.0 applications, such as blogs and bookmarks. In blogs, users can assign one or more tags for each log. Usually, these tags can reflect the concerned subjects of the contents. Tags can be seen as labeled meta-information about the content, and they are beneficial for knowledge acquisition from blogs. This paper focuses on semantic knowledge acquisition from blog s with topic model. In order to use the tags, we use the framework of the TTM model [1], which extends the Latent Dirichlet Allocation (LDA) [2] by adding a tag layer between the document and topic layer. The model represents each document with a mixture of tags, each tag is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. However, the TTM model is an unsupervised graphical model. The topics don X  X  always make sense to users because the model only depends on the statistical strength without any additional knowledge. But, users may have additional knowledge about the word distribution. For example, we know that some words are synonyms, and they should have aligned topics. Incorporating the knowledge into topic models would be useful to improve their performance. In this paper, we extend the tag-topic model by incorporating domain knowledge via Dirichlet Forest prior [3]. We compute the relatedness between words, and use them to generate a set of Must-Links and Cannot-Links [4], then encode them using a Dirichlet Forest prior, to replace the Dirichlet prior over the topic-word multinomial distribution p(word|topic) . After estimating the parameters of the model, we get the document-tag, tag-topic and topic-word distributions, and represent the semantic knowledge underlying in the topics as c:(w 1 , r 1 ), (w 2 , r 2 ), ... (w where c is a tag, w 1 ... w n are the top words arranged to the top topics of it, r i is the semantic relatedness between w The remainder of this paper is organized as follows: Section 2 reviews the related works . Section 3 presents the TTM-DF model and introduces the parameter estimation process. Section 4 shows the experimental results on synthetic documents and blog corpus . At last, we conclude the paper in Section 5. In this section, we introduce the related works of semantic knowledge acquisition and variations of LDA from topic level. Recently, the methods of semantic knowledge acquisition based on large-scale text collection are widely used. There are mainly two kinds of text collections. One is the online encyclopedias and the other is the online or off-line unstructured plain texts. Online encyclopedias are semi-structured texts. Various relations, links, categories between the pages are formatted, and they have been widely used to mine semantic knowledge [5-8] . For unstructured text collections, predefined patterns are applied to web pages to produce peer or sibling relationship words [9, 10 ], and generative models[3, 11 ] are used to mine the underlying topics , and they have been used to semantic analysis in several tasks[12 , 13]. The status of LDA is a fully generative probabilistic model, which allows principled extensions and variations capable of incorporating rich knowledge. At the topic level, there has been an effort to inject various knowledge into topic models. For example, Word sense was seen as a hidden variable to be integrated into the topic model[14]. Topic-in -Set knowledge was defined to recover topics relevant to user interests[1 5]. The Dirichlet Forest prior (DF) was proposed to improve the topic-wo rd multinomial [3]. Seed words were used to improve both topic-word and document-topic distributions[ 16], et al . In LDA, the Dirichlet distribution has two key limitations: the multinomial topic-word distribution  X  is drawn from a shared symmetric Dirichlet prior, and the topic assignments of words are independent except the normalization constraint. The Dirichlet -Tree distribution [17] overcomes the limitations while preserving the conjugat ion with multinomial dis tribution . It is a tree with the words as leaf nodes. Each node in the tree is represented by a Dirichlet distribution over the branches to its child nodes. The probability of a leaf is the product of branch probabilities leading to that leaf . An independe nce relationship is given for words in the tree. The Dirichlet Forest prior is a mixture of Dirichlet Tree with different structures , and it assign s a tree for each topic. The trees are constructed by encod ing the set of Must -Links and Cannot -Links associa ted with the domain knowledge. Must -Lin k(A, B) means the two words A and B tend to be generated by the same topic. Cannot -Link(A, B) means A and B tend to be generated by different topics. The details can be seen in [3 , 17]. The TTM model extends LDA by adding an additional tag layer between the document and topic layer. Its basic idea is that before writing an article, a blogger is clear that the content will contains which main aspects, and for each aspect he will choose a tag to describe it. To generate a word, it first chooses a tag, describing the aspect that the word would convey, and then chooses a topic conditioned on that tag, and samples a word from th at topic. However, the TTM model is an unsupervised graphical model. It discovers the latent topics only depending on the statistical strength without any additional knowledge. Unrelated or loosely related words may be mixed into a topic if they co-occur frequently, or related words may be assigned to different topics if they co-occur rarely. The discovered topics don X  X  always make sense as expected. In many applications, users may have additional knowledge about the word distribution. For example, we know that some words are synonyms, which should be assigned to a same topic no matter whether they co-occur frequently or not, and some words are not related, which should appear in different topics. We would like these preferences to guide the recovery of latent topics. But the TTM model lacks a mechanism for incorporating such domain knowledge.
 We extend the TTM model by replacing the Dirichlet prior with the Dirichlet Forest prior, into which we incorporate the domain knowledge to improve the quality of topics. We express the knowledge as a set of Must-Links and Cannot-Links, and encod e the se constraints through the structures of Dirichlet trees. The Dirichlet Forest distribution assigns a tree for each topic. Because the structures of the trees are different, the Dirichlet Tree priors of each topic are also different. To generate  X  , a tree is sampled from the Dirichlet Forest prior for each topic, and then the multinomial distributions  X  are sampled conditioned on these trees. For each tree , a multinomial of branching probabilities is drawn from a Dirichlet distribution at each int ernal node independently , and t hen the probability  X  w of word w is computed as the product of multinomial parameters on the edges from the root node to the leaf node w . The other procedures of generating a document set are same as before. So, t he generativ e process i n the TTM-DF model can be described as follows , and the notations are explained in Table 1. The joint distribution of the model is: Where, The Dirichlet Forest distribution is a mixture of Dirichlet Tree distribution, which is conjugate to the multinomial distribution, so we can use the Gibbs s ampling [ 18] for parameter inference. In the TTM-DF model, the Markov-chain Monte Carlo (MCMC) state is defined by both the (t, z) pair and the tree indices q The sampling scheme consists of two parts: sampling ( t i word w i and sampling q k for topic z k . Sampling a ( t i , z i ) pair for word w i : ( , | t , z ,w, , , , , ) p t l z k q Sampling a tree q k for topic z k : Since the connected components are independent, sampling q be decomposed into sampling the cliques for each connected component )( r k q respectively. After a set of sampling processes, a sample (t, z, q 1:K from the Markov chain can be used to estimate the parameters with the following formulas: We use three words  X  X , B, C  X  and two tags  X  T1, T2 X  to synthesize a corpus to show the property of the TTM-DF model. The dataset contains three documents: &lt; T1, T2 # A B C C &gt; , &lt; T2# C C C C &gt; , &lt; T1 # A B A B &gt; . Let K=2, the topics of the TTM model are mainly three kinds: one kind is around [0.5A , 0.5B | C], which is shorthand for 0.5, 0 ), 2  X  = (0, 0, 1), and it appears 103 times. The other two kinds are around [B | 0.33A , 0.67C] and [A | 0.33B, 0.67C] , which appear 33 and 45 times respectively. They correspond to the clusters 1, 2 and 3 in Figure 1(a-1). For every kind of topics, the tag-topic distribution has mainly two kinds: around [0.7, 0.3 | 0.2, 0.8] and [0.8, 0.2 | 0.3, 0.7] on the topics [0.5A, 0.5B | C] , they correspond to the clusters 1, 4 in Figure 1 (a-2) . For topics [ B | 0.33A, 0.67C] and [ A | 0.33B, 0.67C] , the tag-topic distribution s are same. They are around [0.5, 0.5 | 0.2, 0.8] and [0.4, 0.6 | 0.2, 0.8] , and they correspond to the clusters 2 , 3 in Figure 1 (a-2) . We can see that A and B are almost interchangeab le. We assume that they have high correlation and should be present or absent together in the topics , so we add a Must-Link(A, B) . Let  X  = 1000, the topic s are showed in Figure 1(b-1) . The clusters 2 and 3 vanish because they violate the Must-Link. The cluster 1 become bigger and the topics appear about 1 90 times. And i n Figure 1(b-2) , the clusters 2 and 3 vanish. In the distributions, T1 is response to A , B much more than C and T2 is response to C much more than A, B. But the TTM model cannot capture the information. It means the TTM-DF model can not only improve the underlying topics but also the tag-topic distributions. In Figure 1(b-1) , the discrete points are mainly around [0.33A, 0.33B, 0.33C | C], which accord with the Must-Link (A, B). However, we assume that AB and C are not related, and they should not appear in the same topic, so we add a Cannot-Link(B, C). The results are shown in Figure 1(c-1) and 1(c-2). The discrete points almost disappear and the clusters in Figure 1(c-2) become more concentrated. The dataset is a collection of 1000 blogs from the Chinese blog corpus (http://pop.clr.org.cn/ ). It contains 81942 7 words and 4620 tags. After words segmentation and part of speech tagging, stop words , extremely common words are removed. Then, Only the nouns or nominal phrases are retained and other words are filtered. These preprocessing lead to a vocabulary size of 19789 unique words and 2 452 unique tags. And i n the experiments, the hyperparameters of the model are set as:  X  =0.1,  X  =0.01,  X  =0.05. The domain knowledge incorporated in the model is a set of Must-links and Cannot-Links. We compute the relatedness between words, then set Must-links between words that are high ly related and Cannot-links between words with low relatedness. In the experiments, two methods are used to compute the relatedness. One is based on HowNet [ 19 ]. If the relatedness of two words is larger than 0.9, a Must-link is set between them, and if the relatedness is smaller than 0.09, a Cannot-link is set. Finally, we select 196 Must-links and 124 Cannot-links. The other method is based on Wikipedia [ 20]. The thresholds for Must-links and Cannot-links are 0. 45 and 0. 08 respectively. And 128 Must-links and 120 Cannot-links are selected. The strength parameters  X  of the domain knowledge are both set to 1000. Perplexity is used as the criterion of model evaluation, and lower perplexity score indicates better generalization performance. In this paper, the perplexity reflects the ability of the model in predicting words with the tags. It is equivalent to the inverse of the geometric mean of per-word likelihood. So, the perplexity can be calculated with the following formula: The number of topics K has great impact on the performance of the topic model. We test a serial of perplexity of the model for different number of topics. Figure 2(a) and 2(b) show the perplexities of different settings of K (K=50,80,100,150) with domain knowledge from HowNet and Wikipedia respectively. In general, while the number of iterations increases, all the perplexities with different topic numbers decrease and larger topic number always bring better perplexity from the beginning. But when the topic number is set too large (both K=150 in Figure 2(a) and 2(b)), the perplexity goes up. So we set K= 100 which leads to the minimum perplexity in the following experiments. The performance of the TTM-DF model based on HowNet (TTM-DF Based on HowNet) and Wikipedia (TTM-DF Based on Wikipedia), and the performance of the TTM model are shown in Figure 2(c). We can s ee that the TTM-DF Based on HowNet and TTM-DF Based on Wikipedia both perform better than the TTM with all the different numbers of K. It illustrates that they have better ability of word prediction. By incorporating the Must-links and Cannot-links, they can generate better topics, and they can also improve the tag-topic distributions, so the models can give better performance. The perplexities of the TTM-DF Based on Wikipedia are low er than that of the TTM-DF Based on HowNet, even though it adds less domain knowledge into the model. This means that the domain knowledge obtained from Wikipedia is better than that from HowNet. It is mainly becaus e that some words are not included in the HowN et, the relatedness with other words cannot be calculated. A nd the relatedness between words in Hownet depend on their locations in the trees, so word pairs with different correlations will have same values if their path are similar in the trees. It is difficult to select the thresholds for Must-links and Cannot-links, and this will be influenced the quality of the knowledge. So we choose Wikipedia as the source of domain knowledge in the final experiments. There is no quantitative measure for distributions of topic model, so we evaluate them by observing the top topics assigned to each tag and the top words assigned to the topics. Table 2 displays two examples. Each tag is illustrated with the top 2 topics and the top 10 words of the corresponding topics. For the TTM-DF model, the topics 72 and 28 are the two top topics of the tag  X   X  X  X  (dian ying; English: movie)  X . By analyzing the top words, we can find that only the topic 72 is related to the tag. The theme of the topic 28 is  X  X V play  X . I t can be the second top topic because its top words  X   X  X  X  (dao yan; English: director)  X  and  X   X  X  X  X  (zhi pian ren; English: producer)  X  are related to the tag. For the TTM model , the top words of the topics 5 and 9 8 are both related to the tag  X   X  X  X  (dian ying; English: movie)  X  except some noise words such as  X   X  X  X  (quan guo; English: nationwide ) X  ,  X   X  X  X  (jiao shi; E nglish: teacher)  X  and so on. These related words should be assigned to a same topic. The TTM -DF model merge s the two topics together with a single Must -Link ( X   X  X  X  (dian ying; English: movie)  X ,  X   X  X  X  (piao fang; English: box offic e) X ). It illustrate s that the M ust-Link can pull other words (such as  X   X  X  X  X  (tou zi fang; English: investor ) X  and  X   X   X  X  X  (zhi pian ren; English: producer)  X ) along even though they are not contained in the domain knowledge. And for the tag  X   X  X  X   X  (huang yan dao; English: Huangyan Island)  X , a Mus t-Link (  X   X   X  (jun jian; English: warship ) X  ,  X   X  X  X  (hai jun; English: navy)  X ) assign s word s  X   X  X  X  (warship ) X  and  X   X  X  (guo jia; English: nation ) X  to topic 13 together with word  X   X  X  X  (hai jun; English: navy)  X . From Table 2, we can find that topics of the TTM-DF model are more concentrated than topics of the TTM model. It shows that the TTM-DF model can generate better topics with the constraints of Must-links and Cannot-links. Besides, the model can also improve the tag-topic distributions. The probabilities of the related topics are larger than before, so tags could be response to the related words much more than other words. Model TTM -DF TTM With the tag-topic and topic-word distributions, the tags could be used to describe the topics. So, the semantic knowledge underlying in the topics can be represented explicitly and formally as c:(w 1 , r 1 ), (w 2 , r 2 ), ... (w i , r i ) ... (w concepts, and the top 10 words from the related topics were selected as related words. The probabilities of the words under the topics could be interpreted as the semantic relatedness. And we normalized the probabilities by their sum. T able 3 gives some examples of the semantic knowledge. we can find that the semantic knowledge obtained from blog is real-time, and some is the information of hot events, such as Huangyan Island , since some blogs are focused on the daily events. The semantic knowledge is maneuverable. It can be easily utilized for relatedness computing , information retrieval, etc. Besides, the document-tag, tag-topic and topic-word distributions can be used for tag recommendation and other text mining applications. In this paper, we expand the tag-topic model by incorporating domain knowledge via Dirichlet Forest prior. With Dirichlet Forest prior, topics have different Dirichlet Tree priors, and tree s have different Dirichlet priors on words. So, word correlation s can be encoded into the model. It can improve the coherence of the underlying topics and the tag-topic distribution. But the Must-links may lead to conflict as they are transitive. For example, if there are two Must-Links: Must-Link(apple, banana) and (apple, Jobs), it will generate a false transitive closure Must-Link(apple, banana, Jobs). In the future, we will use techniques of word sense disambiguation to prevent the transitive. This work was supported by the NSF of China (No.90920005 , No.61003192), The Major Project of State Language Commission in the Twelfth Five-year Plan Period (No.ZDI125-1), Project in the National Science &amp; Technology Pillar Program in the Twelfth Five -year Plan Period (No. 2012BAK24B01), the Program of Introducing Talents of Discipline to Universities (No.B07042), the NSF of Hubei Province (No.2011CDA034) and the self-determined research funds of CCNU from the colleges X  basic research and operation of MOE (No.CCNU10A02009 and No.CCNU10C01005 ). [1] Tingting He, Fang L i. 2012. Semantic Knowledge [2] DM Blei, AY Ng, MI Jordan. 2003. Latent Dirichlet [3] David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. [4] Basu, S., Davidson, I., &amp; Wagstaff, K. (Eds.). 2008. [5] S.P. Ponzetto, M. Strube. 2007. Deriving a large-scale [6] Suchanek, F. M., G. Kasneci &amp; G.Weikum . 2007. YAGO: A [7] Michael Strube and Simon Paolo Ponzetto. 2006. WikiRelate! [8] Xinhui Tu, Tingting He, Jing L uo and Long Chen. 2010. [9] Marius Pasca. 2004. Acquisition of Categorized Named [10] Keiji Shinzato and Kentaro Torisawa. 2005. A Simple [11] Thomas Hofmann. 1999. Probabilistic latent semantic [12] X. Wei and W.B. Croft. 2006. LDA-based document models [13] R. Arora and B. Ravindran. 2008. Latent dirichlet allocation [1 4] Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. [15] David Andrzejewski , Xiaojin Zhu. 2009. Latent Dirichlet [16] Jagadeesh Jagarlamudi, Hal Daume III, Raghavendra Udupa . [17] Minka, T. P. 1999. The Dirichlet-tree distribution (Technical [18] T.L. Griffiths, and M. Steyvers. 2004, Finding scientific [19] Qun LIU, Sujian LI. 2002. Word Similarity Computing [20] Xinhui Tu, Tingting He, Hongchun Zhang and Kunfeng 
