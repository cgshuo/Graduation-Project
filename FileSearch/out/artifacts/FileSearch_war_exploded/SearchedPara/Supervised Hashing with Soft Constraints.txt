 Due to the ability to preserve semantic similarity in Ham-ming space, supervised hashing has been extensively studied recently. Most existing approaches encourage two dissimilar samples to have maximum Hamming distance. This may lead to an unexpected consequence that two unnecessarily similar samples would have the same code if they are both dissimilar with another sample. Besides, in existing method-s, all labeled pairs are treated with equal importance with-out considering the semantic gap, which is not conducive to thoroughly leverage the supervised information. We present a general framework for supervised hashing to address the above two limitations. We do not toughly require a dissim-ilar pair to have maximum Hamming distance. Instead, a soft constraint which can be viewed as a regularization to avoid over-fitting is utilized. Moreover, we impose differen-t weights to different training pairs, and these weights can be automatically adjusted in the learning process. Experi-ments on two benchmarks show that the proposed method can easily outperform other state-of-the-art methods. H.3.3 [ Information Systems ]: Information Search and Re-trieval Supervised Hashing; Soft Constraints; Weights; Boosting
Hashing based approximate nearest neighbor (ANN) search methods have attracted much attention recently. Hashing methods map the two nearby points in the original space to close binary codes in a compact Hamming space. This enables very fast searching since Hamming distance can be efficiently calculated with XOR operation in modern CPU. According to whether supervised information is utilized or not in the training process, hashing methods can be divided (a) The labeled data Fi gure 1: Paradox in traditional supervised Hashing methods. Two unnecessarily similar data x 2 and x 3 will have the same code. into unsupervised and supervised categories. In the unsu-pervised setting, hashing methods such as Locality Sensitive Hashing (LSH) [1] and Iterative Quantization (ITQ) [2] at-tempt to preserve the data similarity defined in Euclidean space, e.g., l 2 distance. However, this is not sufficient for various practical applications such as image retrieval, where semantically similar neighbors are preferred.

In order to construct efficient hash functions that pre-serve the semantic similarity, supervised hashing methods [3, 6, 5, 4, 7] have been extensively studied. The super-vised information here is typically based on some pairwise constraints, i.e.,  X  X  and B is similar X  or  X  X  and B is dis-similar X , which is analogous to the  X  X ust link X  and  X  X annot link X  constraints in metric learning [8]. Some representative supervised hashing methods include Binary Reconstruction Embedding (BRE) [3], Kernel Supervised Hashing (KSH) [5] and Two Step Hashing (TSH) [4]. These supervised meth-ods can be formally formulated with following objective [4]: where  X ( x ) 2f 1 ; 1 g r is the r bits code of x . L ( ) is a loss function that measures how well the codes match the ground truth y ij . Different algorithms corresponds to different loss functions, for example, l 2 loss for BRE and KSH. Although promising performance has been shown from these methods, some limitations exist in them.

Inspired by metric learning, all these supervised meth-ods attempt to learn codes whose Hamming distances are minimized on similar pairs and simultaneously maximized on dissimilar pairs. This principle is widely used in metric learning and proved to be effective. However, metric learn-ing executes in continuous real number space while hashing executes in discrete Hamming space. Importantly, although it makes sense in metric learning, we argue that maximiz-
X2 X3 Fi gure 2: Difference between real space and Ham-ming space. (a) The circle is the  X  X oundary X  of a continuous real space. Maximizing the distance to x , point x 2 and x 3 can be anywhere on this circle. (b) However, in 2 d discrete Hamming space, x 2 and x 3 will collide in one corner. ing Hamming distance on dissimilar pairs, namely hard con-straints, will lead to over-fitting in hashing. Fig.1 gives an illustration about our observation. In this example, point x is labeled to be dissimilar with x 2 and x 3 separately, while the relationship between x 2 and x 3 is unknown. Under the hard constraints, both x 2 and x 3 would have optimized code that is completely opposite with x 1 . As a result, x 2 and x will have the same code. This is apparently unreasonable because x 2 and x 3 are not necessarily similar. This para-dox derives from the difference between the continuous s-pace and discrete space (Fig.2). In fact, all the supervised hashing methods with hard constraints contain an implicit assumption, namely, if both  X  X  X  and  X  X  X  are dissimilar with  X  X  X , then  X  X  X  and  X  X  X  are similar. However, this assumption typically does not hold and leads to over-fitting.
In addition, existing methods treat all labeled pairs equal-ly. If each labeled pair is taken as one constraint in hashing, some of them are easy to satisfy while some others not. This is because the gap between the feature space and semantic space. Sometimes two semantically similar points are al-so close in the feature space. These two points are easy to be embedded to similar codes even without supervised information. Meanwhile, other labeled pairs may be with larger semantic gap, and need more attention in the learn-ing process. Therefore, treating different pairs with different importance is necessary.

In this paper, a general framework is presented to ad-dress the above two limitations. We propose to apply soft constraints to the dissimilar pairs. Specifically, instead of toughly requiring a dissimilar pair to have maximum Ham-ming distance in the objective, we just request them to be far enough in the Hamming space. This can be viewed as a regularization to avoid over-fitting in supervised hashing. Furthermore, we impose different weights to different train-ing pairs, and these weights can be automatically adjusted with boosting technique in a batch-wise learning process. Experiments on two benchmarks show that the proposed method can significantly outperform other state-of-the-art supervised hashing methods.
First of all, some notations are defined as follows: Let X = [ x ; x 2 ; ; x n ] denote a set of n data points, where x i 2 R d is the i -th data point. For each x i , its binary hashing code is denoted as  X ( x i ) = [ h 1 ( x i ) ; h 2 ( x i ) ; ; h r where r is the code length and  X  = [ h 1 ( ) ; h 2 ( ) ; ; h is a set of r hash functions. L denotes the set of labeled data pairs. Two categories of label information, S and D , are available. ( x i ; x j ) 2S represents a similar-pair in which x and x j are similar in semantic space, e.g., share the same labels. Similarly, ( x i ; x j ) 2D is called a dissimilar-pair if two samples are far away in semantic space.

Without loss of generality, we establish our model based on Hamming affinity [5] in this paper. Specifically, Hamming affinity is defined as: Obviously,  X ( x i ) T  X ( x j ) 2 [ r; r ] and S ij 2 [ 1 ; 1]. In this case, the ground truth affinity in Eq.(1) is defined as: Following [5], by adopting Euclidean loss function, the ob-jective function in Eq.(1) can be written as:
In Eq.(3) and the objectives of other supervised methods [3, 5], two dissimilar points are encouraged to have com-pletely different codes so that the corresponding Hamming affinity will be -1 (i.e. maximal Hamming distance). This idea comes from metric learning and works very well in con-tinuous real space [8]. However, as illustrated in Fig.1 and Fig.2, hashing executes in discrete Hamming space and the hard constraints will result in entirely opposite codes of a dissimilar pair, which brings over-fitting, e.g. two unneces-sarily similar samples would have the same code.

With the target of hashing based ANN search, we only need to require the codes of a dissimilar pair to be far away enough but not necessarily entirely opposite. Considering an example that, if the difference between two data points in a pair ( x i ; x j ) 2D is 90% of the total bits, their Hamming distance will be 0 : 9 r and the corresponding Hamming affinity will be -0.8, and this is typically enough to separate the two samples in Hamming space. That is to say, there is no need to restrict the Hamming affinity of a dissimilar pair to be -1. To this end, we modify the objective function in Eq.(3) to be: and ij is defined as: where p 2 (0 ; 1) is a parameter in our method.
 For any dissimilar pair ( x i ; x j ) 2D , ij y ij = p 2 ( 1 ; 0). In other words, we relax the original hard labels into a soft range for the dissimilar pairs. Hence, the optimized codes will have 1+ p 2 r bi ts to be different. In this paper, we call this kind of constraint as soft constraint. Obviously, under such soft constraints, the codes of x 2 and x 3 are not necessar-ily identical like in Fig.1. It is worth noting that for a similar p air ( x i ; x j ) 2S , ij = 1, we do not change the constraints on similar pairs, and keep to encourage similar samples in semantic space to have the same codes in Hamming space. Multiplying y ij with ij can be viewed as a kind of regu-larization to avoid over-fitting in supervised hashing. As we will find in the following experiments, this little change can greatly improve the quality of learned codes.

Eq.(4) treats each labeled pair as equally important, which fails to take the semantic gap into account. Regarding each labeled pair in L as a constraint, some of them are easy to satisfy while some others not. For the sake of max-imally leveraging the supervised information, we need to treat different pairs with different importance. This can be implemented by giving a different weight w ij to each pair ( x ; x j ) 2L , and we arrive at: While it can be intuitively understood, an important ques-tion arises: how to determine the weight of each labeled pair? In the next section, we present a batch-wise learning approach to automatically determine the adaptive weights for different labeled pairs in the learning process.
Most of methods solve the optimized code by a single-shot optimization to the objective, i.e., learn all bits in a single run of the algorithm. However, for any a piece of binary code, it can be regarded as a concatenation of many pieces of shorter codes. As an example, one piece of 32-bits code can be considered as the result of concatenating two pieces of 16-bits codes, or four pieces of 8-bits codes. The Hamming affinity evaluated with the long code is the mean of those evaluated with the short ones. From this point of view, hashing can be understood as an ensemble learning process if we repeatedly generate short codes and then concatenate them. The weight of each labeled pair can be adjusted automatically with boosting trick in this process.

Specifically, in the first run, all labeled pairs are treated with equal importance, and we only generate a piece of short code  X  1 of t bits (e.g. 4 bits) for each sample. In the second and following runs, the weight of each pair will be updated by considering the deviation of the Hamming affinity eval-uated with the previous code to the ground truth. Higher weights will be imposed to the pairs with bigger deviation. In detail, in the k -th run, the weight can be defined as: where  X ( x ) = [ X  1 ( x );  X  2 ( x ); ;  X  k 1 ( x )] is the ( k 1) t bits code of x by concatenating the previous k 1 pieces of short codes  X  i ( x ) j k 1 i =1 .
 with the previous code. For the similar pairs, 1 ( x i ) T is the deviation to the ground truth affinity. The pairs with bigger deviation are associated with higher weights in the next iteration. Note that for the dissimilar pairs, a hinge-like function is used to measure the bias. This is to be consistent with the soft constraints we have used. For any dissimilar pair whose Hamming affinity is smaller than p , the deviation is set to be zero. After K iterations, we can get a piece of K t bits code for each training data.
A remaining problem is optimizing Eq.(6) to learn one piece of t bits code for each training data. Here we follow the block coordinate descent (BCD) method which was also used in [5, 4]. In particular, BCD picks one bit to be optimized every time with other t 1 bits fixed. The optimization for the m -th bit can be written as: where z ( m ) is the binary codes of the m -th bit. l m is the loss function defined on the m -th bit, i.e. Here L is the loss function defined in Eq.(4). z i;m is the binary code of the i -th sample and the m -th bit.  X  z i binary codes of the i -th sample excluding the m -th bit.
When optimizing the m -th bit, as indicated in TSH [4], there are only two possible cases for the code of any pair, namely, same or different. We denote the loss of appointing different code. By taking advantage of the Proposition 1 in [4], the optimization of Eq.(8) can be rewritten as: can be written in a matrix form: where the element of matrix A is a ij = w ij ( l (+) m;i;j The optimization of Eq. (11) have been well studied. To be specific, by dropping the binary constraints, the optimiza-tion becomes: The optimum solution is the eigenvector corresponding to the minimum eigenvalue of A . Subsequently, the obtained solution will be quantized to f 1 ; 1 g t with the sign function.
Until now, we have only optimized the binary codes for training data, which is not enough because hashing has to handle the out-of-sample extension problem, i.e., generating codes for new samples that are unseen before. Inspired by TSH [4], for every bit we regard the binary value z i;m as the pseudo label of training data x i . Therefore the given training set has already been X  X abelled X  X y the above learning process, and we can learn a binary classifier f ( m ) based on it for every bit. The resulting binary classifiers f ( m ) j t taken as the hash function. In the experiments, we choose SVM with Gaussian kernel as classifier, which is widely used and of good performance.
We compare our method with several state-of-the-art ap-proaches, including four supervised methods: BRE [3], ML-H [6], KSH [5], TSH [4], and two representative unsuper-vised methods: LSH [1], ITQ [2]. Comparison experiments Fi gure 3: Precision curves of different methods with 24 bits on MNIST and CIFAR-10 are conducted on two widely used benchmarks: MNIST 1 and CIFAR-10 2 . MNIST consists of 784 dimensional 70,000 samples associated with digits from  X 0 X  to  X 9 X . CIFAR-10 is a labeled subset of the Tiny Images dataset. 512 dimensional GIST descriptor is extracted to represent each image. For both datasets, we randomly select 1,000 samples to be the query set and the remainders as database. 1,000 samples in the database are used to randomly generate training pairs. Specifically, we suppose that for each sample in the train-ing set, only the relationship to 500 other samples in this set are known. Thus, about 500,000 training pairs are available. We adopt the Hamming Ranking commonly used in the lit-erature. All points in the database are ranked according to their Hamming distance to the query. The ground truth is defined as semantic neighbors based on label agreement.
To give a comprehensive validation of the proposed ap-proach, we present two versions of our method. In the first version, denoted as SCH u w, we only apply soft constraints to the learning process and ignore the weights of different pairs. The second version, denoted as SCH, considers both the soft constraints and weighted loss. In this version, the learning process optimizes 4 bits at each run and then ad-justs the weight of each pair. We empirically set the param-eter p in Eq.(5) as p = 0 : 6.

MAP scores: The MAP scores of SCH, SCH u w and other baselines are shown in Table 1. By leveraging side-information, the supervised methods like KSH and TSH can achieve significant improvement on the unsupervised meth-ods like ITQ. The proposed SCH achieves the highest search accuracy on both two datasets. The optimization of our method is similar to that in TSH and KSH, but it is easy to find that SCH outperforms them with a large margin, especially on the CIFAR-10 dataset. More notably, even ht tp://yann.lecun.com/exdb/mnist/ http://www.cs.toronto.edu/~kriz/cifar.html ignoring the weights of pairs, SCH u w achieves the best re-sults except SCH in most of settings. This confirms that the proposed soft constraints can effectively avoid over-fitting in the supervised hashing. By considering the weights of dif-ferent pairs, SCH achieves further improvement on SCH u w, which demonstrates that treating different pairs with differ-ent importance is beneficial to take full advantage of the supervised information.

Precision Curves: Fig.3 shows the precision curves of d-ifferent methods with 24 bits on two datasets. Similar to the trends in Table 1, SCH works better than SCH u w, which is the second best in all competitors. In Fig.3(a), the precision decreases in all hashing methods as the number of retrieved points increases, but our methods decrease more slowly and achieve a very high precision on MNIST even when 5,000 samples are returned. These results clearly show the supe-riority of the proposed methods over other state-of-the-art methods.
In this paper, we proposed a general framework for su-pervised hashing with soft constraints and weighted loss. Experiments on two benchmarks demonstrated the effective-ness of the proposed method.
 This work was supported in part by National Natural Sci-ence Foundation of China (Grant No. 61332016), 863 pro-gram (Grant No. 2014AA015104 and 2014AA015105).
