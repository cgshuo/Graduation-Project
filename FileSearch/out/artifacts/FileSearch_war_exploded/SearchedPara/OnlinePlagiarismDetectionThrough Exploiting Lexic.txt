 Plagiarism Detection, Lexical, Syntactic, Semantic Online plagiarism , the action of tr ying to create a new piece of writing by copying, reorganizing or rewriting others X  work identified through search engines, is one of the most commonly seen misusage o f the highly matured web technologies. As implied by the experiment condu cted by ( Braumoeller and Gaines , 2001) , a powerful plagiarism detection system can effectively discourage people from plagiarizing others X  work . A common strategy people adopt for online -plagiarism detection is as follows . F irst they identify several suspicious sentences from the write -up and feed them one by one as a query to a search engine to obtain a set of documents . T he n human reviewers can manually examine whether these documents are truly the sources of the suspicious sentences . While it is qui te straightforward and effective, the limitation of this strategy is obvious. First, since the length of search query is limited , suspicious sentence s are usually queried and examined indepen dently . T herefore, it is harder to identify document level plagia rism than sentence level plagiarism . Second, manually check ing whether a query sentence plagia rizes certain website s requires specific domain and language knowledge a s well as considerable amount of energy and time. To overcome the above shortcomings , we introduce an online plagiarism detection system using natural language processing techniques to simulate the above reverse -engineering approach. We develop a n ensemble framework that integrates lexical, synta ctic and semantic features to achieve th is goal. Our system is language independent and we have implemented both Chinese and English versions for evaluation. Plagiarism detection has been widely discussed in the past decades ( Zou et al., 2010) . Table 1 . summarize s some of them :
Comparing to those systems , our system exploits more sophisticated syntactic and semantic information to simulate what plagiarists are try ing to do .

There are s everal online or charged/free downloadable plagiarism detection systems such as Turnitin, EVE2, Docol X  c, and CATPPDS which detect mainly verbatim copy . Others such as Microsoft Plagiarism Detector ( MPD ) , Safeassign, Copyscape and VeriGuide , claim to be cap able of detecting obfuscations . Unfortunately those commercial systems do not reveal the detail strateg ies used, therefore it is hard to judge and reproduce their results for comparison. The data flow is shown above in F igur e 1 . 3.1 Query a Search Engine We first break down each article into a series of queries to query a search engine . Several systems such as ( Liu a t al . , 2007) ha ve propos ed a similar idea . T he main difference between our method and theirs is that we send un quoted queries rather than quoted ones . W e do not require the search result s to complete ly match to the query sentence . This strategy allows us to not only identify the copy/paste type of plagiarism but also re -wr i te/edit type of plagiarism. 3.2 Sentence -b ased Plagiarism Detection Since not all outputs of a search engine contain an exact copy of the query , we need a model to quantify how likely each of them is the source of plagiarism . For better efficiency, our experiment exploits the sni ppet of a search output to represent the whole document. That is, we want to measure how likely a snipp et is the plagiarized source of the query. We designed several model s which utilized rich lexical, syntactic and semantic features to pur sue this goal, and the details are discussed below . 3.2.1 Ngram Matching (NM) One straightforward measure is to exploit the n -gram similarity between source and target texts. We first enumerate all n -grams in source, and then calculate the overlap percentage wi th the n -grams in the target. The larger n is, t he harder for this feature to detect plagiarism with insert ion , replacement, and deletion. In the experiment, we choose n=2. 3.2.2 Reordering of Words (RW) P lagiari sm can come from the reordering of words. W e argue that the permutation distance between S 1 and S 2 is an important indicator for reordered plagiarism. Th e permutation distance is defined as the minimum number of pair -wise exchanging of matched words needed to transform a sentence, S 2 , to contain the same order of matched words as another sentence , S 1 . As mentioned in ( S X rensena and Sevaux , 2005) , the permutation distance can be calculated by the following expression w here
S 1 (i) and S 2 (i) are ind ices of the i th matched word in sentence s S 1 and S 2 respectively and n is the number of matched words between the sentence s S 1 and S 2 . Let  X  = n 2  X  n normalized term, which is the maximum possible distance between S 1 and S 2 , t hen the reordering score of the two sentences, expressed as s(S 1 , S 3.2.3 Alignment of W ords ( AW ) B esides reordering, plagiari sts often insert or delete words in a sentence. W e try to model such behavior by finding the align ment of t wo word sequences. We perform the alignment using a dynamic programming method as mentioned i n ( Wagner and Fischer , 1975) .

However, such alignment scor e does not reflect the continuity of the matched words , which can be an important cue to identify plagiarism. To overcome such drawback, we modify the score as below.
 w here  X   X  = 1
M is the list of matched words, and M i is the i th matched word in M . This implies we prefer fewer unmatched w ords in between two matched one s. 3.2.4 POS and P hrase Tags of W ords ( PT , PP ) Exploiting only lexical features can sometimes result in some false positive cases because t wo sets of matched words can play different roles in the sentences. See S 1 and S 2 in Table 2 . as a possible false positive case .

Therefore , we further explore syntactic features for plagiarism detection. To achieve this goal, we utilize a parser to obtain POS and phrase tags of the words . T hen we design an equation to measure the tag/phrase similarity .

We paid s pecial attention to the case that transforms a sentence from an active form to a passive -form or vice versa . A subject originally in a Noun Phrase can become a Pr e position Phrase , i.e.  X  by ... X  , in the passive form while the object in a Verb Phrase can become a new subject in a Noun Phrase. Here we utilize the Stanford D ependency provide d by Stanford P arser to match the tag/phrase between active and passive sentences . 3.2.5 Semantic Similarity ( LDA ) Plagiarists, sometimes, chang e words or phrases to those with similar meanings. While previous works ( Y. Lin et al . , 2006) often explore semantic similarity using lexical databases s u c h a s WordNet to find synonyms, w e exploit a topic model, specifically latent Dirichlet allocation (LDA , D. M. Blei et al . , 2003 ) , to extract the semantic features of sentences. Given a set of documents represented by their word sequences , and a topic number n, LDA learns the word distribution for each topic and the topic distribution for each document which maximize the likelihood of the word co -occurrence in a document. The topic distribution is often taken as semantic s of a document. We use LDA to obtain the topic distribution of a query and a candidate snippet, and compare the cosine similarity of them as a measure of their semantic similarity. 3.3 Ensemble Similarity Score s Up to this point, for each snippet the system generates six similarity scores to measure the degree of plagiarism in different aspects. In this stage, we propose two strategies to linearly combine the scores to make better prediction. The firs t strategy utilizes each model X  X  predictability (e.g. accuracy) as the weight to linear ly combine the scores. In other words, the models that perform better individually will obtain higher weights. In the second strategy we exploit a learning model (in the experiment section we use Liblinear) to learn the weights directly. 3.4 Document Le vel Plagiarism Detection For each query from the input article, our system assigns a degree -of -plagiarism score to some plausible source URLs. Then, for each URL, the system sums up all the scores it obtains as the final score for document -level degree -of -plagiarism. We set up a cutoff threshold to obtain the most plausible URLs. At the end, our system highlights the suspicious areas of plagiarism for display. We evaluate our system from two different angles. We first evalaute the sentence level plagirism detection using the PAN corp us in English. We then eva l u a te the capability of the full system to detect on -line plagiarism cases using annotated results in Chinese . 4.1 Sentence -based Evaluations We want to compare our model with the state -of -the -art methods, in particular the winning entries in plagiarism detection competition in PAN 1 . However, the competition in PAN is designed for off -line plagiarism detection ; the entries did not exploit an IR system to search the Web like we d o . Nevertheless, we can still compare the core component of our system, the sentence -based measuring model with that of other systems. T o achieve such goal, w e first randomly sampled 370 documents from PAN -2011 external plagiarism corpus (M. Potthast et al . , 2010) containing 2882 labeled plagiarism cases. evaluation, we built a full -text index on the corpus using Lucene package. Then w e use the suspicious passages as queries to search the whole dataset using Lucene. Since there is length limitation in Lucene (as well as in the real search engines), we further break the 2882 plagiarism cases into 6477 queries. We then extract the top 3 0 snippets returned by the search engine as the potential negative candidates for eac h plagiarism case . Note that for each suspicious passage, there is only one target passage ( given by the ground truth) that is considered as a positive plagiarism case in this data , and it can be either among these 3 0 cases or no t. However , we union th ese 3 0 cases with the ground truth as a set, and use our (as well as the competitors X ) models to rank the degree -of -plagiarism for all the candidates. We then evaluate the rank by the area -under -PR -curve ( AUC ) score. We compared our system with the winning ent ry of PAN 2011 ( Grman and Ravas , 2011 ) and the stopword ngram model that claims to perform better than this winning entry by Stamatatos ( 2011 ) . The results of each individual model and ensemble using 5 -fold cross validation are listed in Table 3. It shows that NM is the best individual model, and an ensemble of three features outperforms the state -of -the -art by 26%.
 4.2 Evaluating the Full System
To evaluate the overall system, w e manually collect 60 real -world review articles from the Internet for books (20), movies (20), and music albums (20) . Unfortunately for an online system like ours, there is no ground truth available for recall measure. We conduct two differement evalautions. First we use the 60 articles as inputs to our system, ask 5 human annotators to check whether the articles returned by our system can be considered as plagiarism. Among all 60 review articles, o ur system identifi es a considerable ly high number of copy/paste articles, 231 in total . However, i dentifying this type of plagiarism is trivial, and has been done by many similar tools. Instead we focus on the so -called smart -plagiarism which cannot be found through quoting a query in a searc h engine . Table 4 . shows the precision of the smart -plag i arism articles returned by our system. The precision is very high and outperforms a commertial tool Microsoft Plagiarism Detector .
In the second evaluation, we first choose 30 reviews randomly. Then we use each of them as queries into Google and retrieve a total of 5 636 pieces of snippet candidate s . We then ask 63 human beings to annotate whether those snippet s represent plagiarism case s of the original review article. Eventually we have obtained an annotated dataset and found a total of 502 plagiarized candidates with 4966 innocent ones for evalautio n . Table 5 . shows the average AUC of 5 -fold cross validation. The results show that our method outperforms the Pan -11 winner slightly, and much better than the Stopword Ngram.
 4.3 Discussion There is some inconsistency of the performance of single features in these two experiments. The main reason we believe is that the plagiarism cases were created in very different manners . Plagiarism cases in PAN external source are created artificially through word insertions , deletions, reordering and synonym s ubstitutions. As a result, features such as word alignment and reordering do not perform well because they did not consider the existence of synonym word replacement. On the other hand, real -world plagiarism cases returned by Google are those with matching -words, and we can find better performance for AW .

The p erformance s of syntactic and semantic features, namely PT , PP and LDA , are consistent ly inferior than other features. It is because they often introduce false -positives as there are some non -plagiarism cases that might have high ly overlapped syntactic or semantic tags . Nevertheless, experiments also show that these features can improve the overall accuracy in ensemble.

We also found that the stopword N gram model is not applicable universally. For one thing, it is less suitable for on -line plagiarism detection, as the length limitation for queries diminishes the usability of stopword n -grams. For another, Chinese seems to be a language that does not rely as much on stopwords as the latin languages do to maintain its syntax structure.

S amples of our system  X  s finding can be found here, http://tinyurl.com/6pnhurz We developed an online demos system using JAVA (JDK 1.7). The system currently supports the detection of documents in both English and Chinese. Users can either upload the plain text file of a suspicious document, or copy/paste the content onto the text area , as shown below in Fig ure 2 .

Then the system will output some URLs and snippets as the pote ntial source of plagiarism . ( see Figure 3 . )
Comparing with other online plagiarism detection systems, ours exploit more sophisticate d features by modeling how human beings plagiari ze online sources . We have exploited sentence -level plagiarism detection on lexical, syntactic and semantic levels. Another noticeable fact is that our approach is almo st language independent. Given a parser and a POS tagger of a language, our framework can be extended to support plagiarism detection for that language. 
