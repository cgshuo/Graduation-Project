 Ranking documents with respect to users' information needs is a challenging task, due, in part, to the dynamic nature of users' interest with respect to a query, which can change over time. In this paper, we propose an innova tive method for characterizing the interests of a community of users at a specific point in time and for using this characteri zation to alter the ranking of documents retrieved for a query. By generating a community interest vector (CIV) for a given query, we measure the community interest by computing a score in a specific document or web page retrieved by the query. This score is based on a continuously updated set of recent (daily or past few hours) user-oriented text data. When applying our method in ranking Yahoo! Buzz results, the CIV score improves relevant results by 16% as determined by real-world user evaluation. H.3.3 [ Information Search and Retrieval ] General Terms : Algorithms, Human Factors, Experimentation Keywords : Information Retrieval, Ranking, Topic, User, Blog, Community Interest, LDA Ranking is a key step in Information Retrieval (IR) systems. All ranking algorithms work to find the most important documents and show them to users at the top of the search results. Generally, existing ranking algorithms measure the  X  importance  X  of the document in the search results in several different ways, such as the distance between query and document in a high dimension vector space, probabilit y of the document generating the query, social network popularity in the retrieved result and so on. Two basic hypotheses are common in the existing algorithms: first, the query contains the key information for ranking, which provides hints used to discriminate the important results from others. Second, some unique featur es on the document or user side can help rank the results, such as citations, page links, or user behavior data. There are also some limitations regarding these hypotheses. First, web queries tend to be short (J ansen, 1998; Silverstein, 1999) and the algorithms have relatively limited information on which to base their ranking. At the same time, some additional ranking information, such as links, citations or user behavior data can be biased. For instance, a blog po sting getting a high number of citations or clicks may due to two different reasons: the content is interesting (it should get the high rank), or the blogger is popular in a certain community (the cont ent may be pedestrian and does not deserve the high rank outsi de the local community). The content-free ranking algorithm will favor these postings no matter which scenario they belong to. In this paper, we use  X  community interest  X  as an indicator to represent this importance score; namely, we compute a measure of the interest level in the global community in a specific retrieved document for a given query at a gi ven time. Instead of employing a large number of users to make judgments as to what is interesting and what isn X  X , we use user oriented text data (such as daily or hourly blog postings or user selected news text) to represent users X  interests, and the text is represented by different features. By using a popular topic-modeling algorithm (LDA), we discover topics of community interest in the user text data as probability distributions over the sp ace of features. Each topic is then weighted by historical text data from the community. Finally we construct the CIV as a vector of weighted topics to represent the current interests of the comm unity. For each document in the search results, we also infer a score (using the precomputed probability topic models) that is proportional to the level with which the community may be interested in this specific document given the query. This score, the CIV score, is then used for ranking the entire set of search results. In our algorithm, each user oriented text is viewed as an  X  X gent X  of user, and the real time topics of the text will be used to  X  X ote X  for the important documents over others. Some terminology mentioned in this paper is listed below: A number of techniques have been developed for ranking retrieved documents and web pa ges for a given query. The classical method is content-base d or query-dependent ranking, which is based on the similarity or probability of matching between query and target document. In web search, additional ranking information can be used, su ch as the hyperlinks between web pages, anchor text, user be havior data and the popularity of the page. Content based ranking Content based methods rank the documents according to their relevance to a given search query. In vector-space content based ranking, the ranking score of a document with respect to a query is determined by its  X  X istance X  to the query vector (Kobayashi &amp; Takeda, 2000), using a vector space model (Salton &amp; Yang, 1973). In order to reduce the dimensionality of the vector and to represent  X  X atent X  word similarities, Latent Semantic Indexing, dimensional word-document matrix into a lower one. Similarly, language models (Lafferty &amp; Zhai, 2001) and probabilistic models (Joneset al., 1976) calculate th e probability of a document generating the query and the probability of relevance based on the query and the document respectively. Related to our work, a topic based language model using LDA has been studied for ad-hoc information retrieval by (Azzopardi et al, 2004, Wei&amp; Croft 2006). Recent studies combined existing ranking algorithms by machine learning to create ne w ranking functions trained by evaluation method, which is learning to rank (Trotman, 2005). In this approach, user interests and requirements are represented by query terms. Linkage based ranking In the WWW environment, the network structure of a hyperlinked network can be a rich source of information about the content of the pages, providing an effectiv e means to understand it. The related ranking algorithms are PageRank (Page et al., 1998) and HITS (Hypertext Induced Topic Selection) algorithm (Kleinberg, 1999), which are based on traditiona l citation analysis and social network analysis. Some more recent research combines these two approaches together and uses both content and hyperlinks to rank the search results. For instance (Haveliwala, 2003) worked with topic-sensitive PageRank, which created a list of PageRank vectors by using a set of representative topics in order to capture the context of each hyperlink. Similarly, a probabilistic model was used by (Richardson&amp;Domingos, 2002) to generate PageRank score for each possible query term. In this approach, user interest is partially represented by ne twork connected to the target document. User behavior based ranking User behavior data have been used and proved as an effective indicator for ranking. The rela tionship between implicit and explicit user data was studied by (Fox et al 2005), and two different Bayesian models were bu ilt to correlate different kinds of implicit measures and exp licit relevance judgments for individual page visits and entire search session. Joachims (2002) employed clickthrough da ta to learn ranking function by using SVM, and his work proved that clickthrough data is a significant predictor of user interest for ra nking. Similarly, Agichtein et al (2006) incorporated noisy user behavior data into the search process, and the user data were used to train the ranking functions. In this approach, user interest is partially represented by statistical user behavior data. In this paper, we are focusing on representing user interest from the topic distribution over user generated real-time text data, which is separated from the retrieved results. Instead of using statistics of user behavior data, we rank the documents in terms of the content of large amount of real time user generated text. In the Web 2.0 context, a user may generate different kinds of text data, such as blogs, selected ne ws, or comments to express their opinion. We hypothesize that a la rge amount of user-oriented data can represent the overall opinion of the community. A simple example is the 2008 presidential election. As the following diagram shows, the number of blog postings about  X  X bama X  and  X  X cCain X  changed over time (data from Yahoo! Buzz, http://buzz.yahoo.com, from 2008-10-11, before election, to 2008-11-10, after election). It is shown that before electio n day (2008-11-4), the number of postings about the two candidates is almost equal, but after the election, because of the results, the gap between the winner and the loser significantly increased, representing the situation in the real world. Similar blog research about the 2004 election can be found in (Adamic &amp; Glance, 2005). In this paper, there are three central questions to answer: The protagonist defined in our paper is the main actor (not necessarily a person) of a user generated posting. And the protagonist list is generated th rough query log. One protagonist can correspond to one query or multiple similar queries. Query similarity is well studied in different researches, such like (Wen et al, 2002, Baeza-Yates et al., 2004). In this section, we will describe our method and try to answer the aforementioned questions. If we index user oriented text by protagonist, and each protagonist represents one or several similar hot (i.e. high frequency) queries identified in query logs, then for each protagonist, we can collect a number of user oriented postings for a period of time (e.g. today, or past few hours). We call this collection of postings the  X  X urrent Protagonist Collection X  (CPC). When the number of postings increases, the representabil ity of this collection (and of community) also increases. We define the community X  X  interest toward each protagonist as a vector  X  the Community Interest Vector (CIV), and each component of the vector represents a (normalized)  X  topic  X  related to the target protagonist. This in terest vector may change in two different ways over time: If we use the protagonist  X  X bama X  as an example, when n = 3, the CIV of  X  X bama X  for the 1 st of Aug, Oct, and Dec of 2008 may look like following: In August, the community was interested in three different topics about  X  X bama X : 1. Obama X  X  campa ign 2. the relationship between Obama and Clinton 3. the relationship between Obama and McCain. The weight of the  X  X econd topic X  is larger than the other two since the community was more interested in this topic at that time. In October, these topics may still exist, but the weights of first and third topic have increased, while the weight of the second has decreased. In December, after the election, the third topic is replaced by the  X  X conomy X , and the weight of each topic also changes. Our algorithm uses real-time user oriented text data as the corpus to extract and weight CIV, and each item in CIV represents a current topic, which is a probability distribution over features. Fig 3 shows how CIV algorithm ra nks the documents generally. In the simplest way, each quer y corresponds to one protagonist. The query is sent to both indexe d documents and user oriented text collections. The algorithm will extract the CIV from current protagonist collection based on topic extraction and topic weighting modules, and finally the CIV will rank the candidate retrieved results against current topic distribution. We hypothesize that the postings in the CPC incorporate a fixed number of latent topics and we proceed to extract these topics. A topic in our model is a probability distribution over features. There are various techniques to perform this topic modeling step, and we chose an off-the-shel f public domain algorithm called Latent Dirichlet Allocation (LDA), (Blei, Ng, &amp; Jordan, 2003). In a nutshell, LDA is similar to probabilistic Latent Semantic Analysis (Hofmann, 1999) in that it decomposes the posting-by-features matrix into a document-by-topics matrix ,  X  , and a topics-by-features matrix ,  X  . LDA is a generative probabilis tic model in the hierarchical Bayesian framework, and the topic proportions are randomly drawn from a Dirichlet distribution. As the above diagram shows, traditional document indexing sy stems represent each document as a vector of features. By using LDA, the document-feature matrix can produce two different matrices:  X  contains the document (posting)  X  topic probabil ity distributions, i.e. each row represents the probability of topic given the posting P (topic | posting) .  X  contains the topic-feature pr obability distributions, i.e. each row represents the probability of each feature given the topic P (feature | topic) . In the LDA model, the document corpus is generated by the following process: 1) For z = 1:K, where K is the fi xed number of latent topics, draw parameters for a multinomial distribution  X  z for each topic z from a Dirichlet distribution with hyperparameters  X  .  X  z relative frequencies of features in topic z. 2) For each document d, draw parameters for a multinomial distribution  X  d from a Dirichlet distribution with hyperparameter  X  .  X  d models the relative frequenc ies of topics in document d. 3) For each feature (e.g. word) w in document d, An example of an LDA result from a user oriented text collection is shown as following: The above table shows three samp le topics extracted from the 2008-08-11 blog posting collection (1086 postings, number of topic = 30, protagonist =  X  X lympi c X ). Each topic is represented by features (bag-of-words + entities + Wikipedia ID  X  see a detailed description in the next s ection), and the probability of the feature given topic P(feature | topic) . We printed the top 20 features of each sample topic. Based on the topic-feature probability distribution, we can use the learned LDA model to infer the topic distribution in a new document. Given a new unseen doc ument, by inverting the LDA generative process, we can obtain the topic probability distribution in the new document. Each dimension represents the relative frequency (probability) of each topic in the document belongs: TV(doc x ) is the topic vector of the given document X , while the P(topic m |doc x ) score represents the probability that topic correct descriptor of the given document. In any information retrieval and text mining system, features are important as the units which represent the indexed document and corpus. However, compared with traditional retrieval systems and web search engines, the quality of user-generated text (such as blog postings) is low, due to spelling mistakes, grammatical mistakes, and spoken language expr essions. These issues pose a challenge to the performance of existing NLP, IR, and mining algorithms. Furthermore, users te nd to use different terms and phrases to express the same thing, which not only increases the dimensions of the feature space, but also misleadingly divides the same feature into different ones. La st but not least, we find users sometimes write people X  X  na me or locations without capitalization, and this behavi or removes one of the most important features for entity recognition algorithms. In order to solve the aforementioned problems, we need to design an algorithm to: In our experiment, we find using th e correct type of features to represent user generated text can improve the system performance. However, this sect ion is somehow independent from our algorithm. So if you are not interest in the feature representation, please jump to section 3.4. Entity recognition We employed Contextual Shortcuts Platform (von Brzeski et al., 2007) to extract entities from text. Contextual Shortcuts uses a combination of dictionary and machine learning approaches to determine the set of most releva nt named entities and keywords (concepts) in a piece of text. Dictionaries themselves consist of editorially constructed lists of named entities (e.g. persons, places, organizations, etc., organized in a shallow taxonomy) and an automatically generated list of c oncepts derived from query terms found in web search logs. In fact , the automatically generated list is much larger than the set of editorially derived terms. Because some existing popular terms in que ry logs may also contain the same errors as user oriented text data, such as  X  barrack obama  X  (spelling mistake), this entity extraction algorithm can extract some low quality entities. We can project such entities to high Contextual Shortcuts Platform performs entity disambiguation and ranks the entities discovered in a piece of text according to their interestingness to the broad user community (Irmak et al., 2009). Finding the candidate Wikipedia IDs Bloggers tend to use abbrevia tions and ambiguous entities; readers can understand the meaning by the context. An example could be the following sentences containing the entity  X  X etroit X : When considering the context (such as the query context or blog context) of the ambiguous entity, we can figure out the real concept that the entity corresponds to (1.  X  X etroit Pistons X ; 2.  X  X etroit Red Wings X ; 3.  X  X etroit, Michigan X ). We designed the semantic similarity algorithm based on the work of (Cilibrasi &amp; Vitanyi, 2007) to automatically identify the closest concept from the Wikipedia database. Cilibrasi &amp; Vitanyi (2007) computed the normalized semantic relatedness between two entities us ing the Google distance. In our experiment, we first search for th e extracted (ambiguous) entity in a Wikipedia resolver component, wh ich returns a list of Wikipedia IDs given a named entity or con cept (e.g. Detroit_Pistons or Detroit_Red_Wings from  X  X etroit X ). The Wiki resolver was built by analyzing the link structure of Wikipedia in order to associate anchor text with Wikipedia page names. It uses query terms in web search logs in order to asso ciate queries (e.g. named entities) with Wikipedia page names, and it also uses Wikipedia X  X  editorially created redirect page s to associate those page names with Wikipedia pages. Finally, the above associations are merged into one final score mapping a query or named entity into one or more Wikipedia IDs. We then compute the Google distance between the entity and each Wikipedia concept (ID) in the context of protagonist (because we indexed the blog by protagonist) by the following formula. P is the target protagonist; C is the count of results returned from Google general web search; M is the total number of web pages indexed by Google; G_dist is the normalized Google distance between the Wikipedia ID and entity, ranging from 0 to 1 (0 means semantic identity, 1 means no semantic relatedness). In this way, we can find the closest semantic concept feature to replace the entity feature in the protagonist context (for instance, when protagonist is  X  X BA X , the closest concept of  X  X etroit X  is  X  X etroit Pistons X ). Since we built the protagonist collection directly from query logs and used it later for ranking, the concept feature will logically help us improve the topi c model learning as well as the ranking performance. The final feature space is this combination of terms, extracted entities, and Wikipedia IDs. After we generated the topic model from the CPC, it is very important to weight each topic. The weight of each topic measures the degree of community interest in this topic at the current moment. Overall, there are four di fferent kinds of topics we found through our experiments: We used historical data (past fe w hours or days) to classify topic type and compute the weight of each topic for the target protagonist. The most straightforw ard method is to compute a list of topic models for each corpus for a specific period of time, and then compute the similarity of those topics, and also weight each current topic for ranking. However, there are two major limitations. First, the computational cost is very high, as we need to train several LDA models and compute feature-topic distribution distance for each topi c pair. Second, this is not an accurate way to compute weights when similarity across topics is low. In order to avoid those limitations , we used the learned CPC topic model to infer the topics in the historical prota gonist corpuses. The algorithm is as follows in Fig. 5. As mentioned above, from the LDA model, we obtained two probability distributions:  X  -the probability of topic given the posting P (topic | posting); and  X  -the probability of each feature given the topic P (feature | topic). Based on these distributions, we compute the  X  Current_topic_score[k]  X  by summing the posting vectors from  X  . We also run the mode l against historical data (past n days or hours, n corpuses, worth of user oriented documents for the protagonist) and infer the topic distributions  X  in the historical data. Because the LDA model was build using the CPC, the historical postings can be viewed as unseen data. For each document, the inference result is: which indicates the probability of each specific topic in the unseen document from the current perspective. For each past day or hour, by summing these topic probability vectors together, we can obtain a  X  History_topic_score[i][k ]  X , which reflects, from the current viewpoint (topic model), th e probability that on the past i day (or hour), the community (represented by the user oriented corpus) is interested in topic k . By comparing the mean and the standard deviation of specific topics X  scores for a window of the past n days (or hours), we can deci de if the topic is a  X  X ot topic X ,  X  X iminishing topic X , or  X  X egular t opic X  as shown in the algorithm. The hot-topic and diminishing-topi c CIV scores were adjusted by the change rate of current topic score and the mean of the historical topic scor es; a bonus parameter ( b, b&gt;1 ) and penalize parameter ( p, p&lt;1 ) were used in our algorithm to update the topic weight. In our experiments, b = 1.2 , while p = 0.8 . Because we identify the topic category by mean and standard deviation, the diminishing-topic is always &lt; 1. In our experiments, we also found that some topic X  X  mean pr obability score is significant larger than all other topics X  scores (at least 5 times larger)  X  we define these topics as  X  X ackgro und topics X , and penalize these topics X  weights by p X  = 0.2 (penalize parameter of background-topic). The background topic is mainly composed by a list of general and domain specific stop words (for instance protagonist =  X  X bama X , the stopwords can be  X  X bama X  and  X  X resident X ). Even though the background topics X  weights are large in all the corpuses, these topics are harmful for community interest based ranking. (As shown in formula (3)) The following diagrams (Fig 6 &amp; 7) are examples of CIV topic weighting. The protagonist is  X  X  bama X , and experiment time is Nov 5th 2008, one day after the 44th president election, the training corpus is Yahoo! Buzz pos tings (we will discuss the data in next section) and corpus size is 1,491 user generated postings. We show the highest weighted  X  X ot topic X , which can be summarized as  X  X bama wins the election X , and whose top features are  X  X arack_Obama X ,  X  X lection X ,  X  X frican_American X ,  X  X ictory X ,  X  X ictory_Records X  a nd  X  X irst_black_president X . We also show the lowest weighted  X  X iminishing topic X , which can be characterized as  X  X arah Palin and Hillary Clinton X , with top features like  X  X arah Palin X ,  X  X arah X ,  X  X alin X   X  X illary clinton X ,  X  X ewsweek X ,  X  X lub X ,  X  X loth X . In the example (Fig. 6 and 7), we compute the topic-feature distribution using the current  X  X ov 5 th  X  corpus about  X  X bama X , and then use it to infer topic distributions in the past 30 days (from Oct 5 th to Nov 4 th ). By computing the mean and the standard deviation of the topic probability scores, we can identify hot and diminishing topics by their final weights in the CIV. In weight, and the last bar on the right is the final adjusted weight of the topic in the CIV. When a query is equal to or is similar to the protagonist, we can bias the ranking result by using the current Community Interest Vector. For any given retrieved document collection R (doc distribution ), we can infer the topic di stribution of each document in the search results as mentioned earlier. Because the topic vector of each document in the search results is in the same vector space as CIV, we can compute the final document interest score by cosine vector similarity: Since the CIV represents the community X  X  interest with respect to each protagonist, the final ranking score can be viewed as a pseudo-voting based ranking, where the user oriented text data serves as a proxy for the votes. Thus, the ranking score can represent P(interest | doc) , the probability that a community is interested in a given document. In this paper, we focus on computational community interest, and we need to use real-time user oriented text data to represent the community X  X  interest s . We chose Yahoo! Buzz data (http://buzz.yahoo.com) for the following reasons. First, this is a user oriented text dataset (mai nly in the news domain), which primarily includes two parts: user selected news stories and user oriented news comments. Second, a user may copy and paste from other news services (like CNN.co m), but they tend to select only specific parts of the news instead of the whole story. The selective sentences or passages have higher probability of being interesting to the global community, since background context information is filtered out. This is also beneficial for our interest extraction algorithm and ranking. Third, compared with blog data, Yahoo! Buzz focuses more on news instead of social network communications, which facilitates news based ranking and user evaluation. Finally, each Buzz posting contains a time stamp that can be used for corpus selection. We selected 129 hot queries from news search engine query logs, and used those query terms and entities directly as the protagonist to search and index Buzz pos tings. Here, we do not use a protagonist detection algorithm (a n algorithm which attempts to confirm that a given posting is actually about the target protagonist) for two reasons. First, we want to get enough text data for building the community interest topic model (corpus size is important in obtaining a usef ul model); protagonist detection algorithms may filter a large pe rcentage of postings. Second, some existing protagonist algorit hms are very time consuming, and we want the ranking algorithm to be used in an online information retrieval system. However, using a query directly as a protagonist will bring in some noisy data (see below).
 From Oct to Dec 2008, we indexed 274,400 postings with an average length of 769 characters. The postings were indexed by protagonist (the entity from query log), stemmed words, entities and Wikipedia concepts as well as the published time stamp. As mentioned earlier, we used Cont extual Shortcuts as the named entity recognition algorithm to find all the entities within the postings, and then, for each entity, we find the candidate Wikipedia concepts for each ambiguous entity by using the Wikipedia resolver co mponent mentioned earlier. Finally, we computed the Normalized Google Distance (NGD) in the context of the protagonist (equation 4), and replace the entity feature with the Wikipedia concept feature if possible. One posting may correspond to severa l different protagonists. The index module was checking for new postings from the user community about these 129 protagonists in real-time during the experiment, and the attached publis h time stamp was used to filter the training ( X  X urrent X ) and historical corpuses for topic extraction and topic weighting. We select the training corpus as follows: For each protagonist, if there were more than 1000 postings published in the past 24 hours, we capped the training corpus (CPC) size at 1000, which represents community X  X  interest toward the protagonist for the past t hours ( t &lt; 24). Otherwise, we will use the past 24 hours worth of postings for LDA model construction. After comparing several different values for the  X  X umber of topics X  parameter K in LDA, we fixed K at 30, as the extracted topics should be neither too gene ral nor too specif ic. We set the experiment, we find the topic number and  X  value significantly influenced the validity of CIV and ranking performance. Generally the lower  X  , the sparser the topics will be, which means the model prefers to assign few te rms to each topic (Heinrich, 2005). Meanwhile, the number of topic K defines how many cognitive dimensions we need to define for each CIV. Unfortunately, we cannot guaran tee that the experimental parameter value is optimiz ed as we didn X  X  have enough evaluation resources (we will men tion that in next section) to compare the performance across different parameter settings. Instead, in this paper, we used intuitive best values for this experiment by comparing different pa rameter setting by ourselves. To build the final Community Intere st Vector (CIV), we need to infer the corpus-topic distributions for the past n days (or hours) and use trend analysis to weight each topic. For each protagonist, we used a corpus of size (m) , as in the previous LDA training step. In the historical posting collection about a target prota gonist, we composed h corpora (in experiment h = 30) ordered by publish time, each corpus containing m postings the same as the training corpus, because daily or hourly variation of corpus size may be large and we don X  X  want the inference performance to drop significantly due to the corpora imbalance. By analyzing the in ferred topic probability scores (component by component) across different days X  corpuses, we computed the final topic weight as the CIV: The CIV reflects the current community interest toward the protagonist. We will use this vect or directly for ranking. During the experimental period, we computed a CIV for each protagonist 4 times a day. We use the same Yahoo! Buzz source for the ranking test. We sent each protagonist as a query to Buzz search, which returned the ranked Buzz postings for the re cent three days. Each retrieved document was treated as an unsee n document and we inferred the document-topic distribution base d on the existing LDA topic model for the target protagonist . The rank score for a document was calculated using equation 4. However, in the ranked result, we find some duplicate results for two reasons: We attempt to detect and remove duplicate news stories from the result. For content duplication, bi-gram fuzzy edit distance was used to identify duplicate documents. If the fuzzy similarity of the title and the first paragraph was larger than a threshold (in the experiment it was 0.8), the duplicate document will be removed from the result. For topic duplication, the inferred document-topic vector cosine similarity was computed between documents; if the topic similarity is larger than a threshold (in the experiment it was 0.8), the duplicate documen t will be removed. The evaluation of a ranking algorithm is difficult, especially for our real-time ranking task, whic h cannot employ existing test collections such as TREC. Precision-at-document-n (Anh &amp; Moffat, 2002) is currently a good measure for the web, as most users will be focusing on only the very fi rst page of n results. And Normalized Discount Cumulative Gain (NDCG) (J X rvelin &amp; Kek X l X inen, 2002) works when user graded relevance data is available. For this paper, the most important contribution is to capture the dynamic community interest since community interest may change from time to time. As a re sult, we have to conduct a real-world evaluation based on selected protagonists over a period of time. We focus on the  X  X nteresting &amp; hot rate X -at-document-n as well as the  X  X ot interesting or no t relevant rate X -at-document-n. We set up a preliminary evaluation with five real readers for a period of 5 days (Nov 11, 12, 13, 14, 17 2008) intended to test the concept and may serve for future large scale evaluations. Nine queries were randomly chosen in the evaluation (from top frequent query log of the first two weeks of November 2008) as following table shows: On the five evaluation days, we constructed a topic model every day at 14:00PM and users accesse d our evaluation system on 15:00PM to make their judgments . The above table shows the average number of training documents for LDA topic extraction for each query (protagonist).  X  X bama X  and  X  X conomy X  are the popular protagonists during that time, which exceeded the threshold, and we used only 1 000 most recent Buzz postings for training (the 1000 postings covered 18.3 and 16.1 hours community interests respectively). In the evaluation system, afte r logging in, the judges were required to click nine queries one by one and read the top 5 ranked documents in two collections each: Yahoo! Buzz ranked results and CIV algorithm ranked results. Two diffe rent ranked set were randomly presented to user. After reading each ranked search result, users were asked to grade one of the following options about this document: For 5 five days for 5 users and 9 queries corresponding to top 5 documents for each algorithm, there were a total of 2,250 valid evaluation results collected. We first employed the idea of Precision-at-document-n , and averaged rates of above four categories for each ranking method. The results are shown in the following table: In the evaluation results, we focus on two questions: whether the CIV ranking can improve  X  X nteres ting &amp; Hot X  rate; and whether the CIV ranking can decrease  X  X ot interesting or Not relevant X  rate. In the following diagram, we present these answers in a clearer way: In Fig. 8, the lower line shows th at for five days, the  X  X nteresting &amp; Hot X  rate increased 16.74% on average as compared to the existing Yahoo! Buzz ranking al gorithm. The upper line shows that the  X  X ot interesting or Not relevant X  rate decreased 20.59% on average. On Monday, 11/11/2008, the  X  X nteresting &amp; Hot X  rate increased by only 6.57%, on all the other days it increased by more than 15%. Secondly, NDCG evaluation was used to process the graded relevance judgments. We set the  X  X nteresting &amp; Hot X  relevance rate = 3, as user values these re sults significantly better than other results. Meanwhile,  X  X ildly interesting X  rate = 1,  X  X ot interesting or Not relevant X  rate = 0 and  X  X uplicate X  rate = 0. Based on these definitions, we compute the average NDCG@3 and NDCG@5 across test queries based on (J X rvelin &amp; Kek X l X inen, 2002). The results are shown in the following table with significant test: From the evaluation results, we find that the CIV ranking algorithm significantly increased the ranking performance (for both Precision-at-document-n and NDCG) compared with an existing popular search engine ranking algorithm. The preliminary evaluation shows that the global community interest is a good indicator for IR ranking. However, in our experiments, we still found some limitations in this algorithm. First, some queries (or prot agonists) are am biguous, and LDA cannot directly help us separate the topics semantically for ranking. This can be a problem of the polysemy effect (Sparck Jones, 1972). For instance  X  X eorgia X  is an ambiguous query, which can represent  X  X  state in the United States X  or  X  X  country X . Building a CIV for  X  X eorgia X  (in Oct 2008) is risky, as it will mix the  X  X S presidential election X  and the  X  X eorgia war X  topics into the same vector space, and we may need a word sense disambiguation algorithm to solve his problem. Second, we did not use a prot agonist detection/verification algorithm to better clean the training corpus in the experiment, resulting in some noisy data leak ing into the training corpus. For instance, the word  X  X bama X  shown in one posting does not necessarily mean that  X  X bama X  is the protagonist of the posting. In our experiment, we did not implement the protagonist detection/verification because of the data (training) size problem. In future research, we need to collect more (user-oriented) text data and filter a higher quality trai ning corpus for topic extraction. Last, we find CIV ranking algor ithm generates more topic duplicate results (shows in Tab 4) , even though we used duplicate detection. A possible reason is the topic distributions among the top ranked results are similar in our algorithm as they are all extracted from the same corpus. Better duplicate detection algorithm should be used in the future work to reduce the duplicate rate. Another finding was that the traini ng and historical corpus size is important for the CIV ranking algorith m. An example of this is in the ranking performance on Monday 11/11/2008; it is lower than the other weekdays because users generated less text data (fewer postings) over the prior weekend, and we thus obtained fewer postings for training for communi ty interest extraction. In future evaluations, we will be targeting a larger scale user study with the goals of deciding an ideal training corpus, best feature types, and the optimized training and weighting parameter settings. Meanwhile, we hope to compare the CIV algorithm with the existing specific ranking algor ithm individually. We will also develop the ranking algorithm with more sophisticat ed techniques. We are currently working on Community Interest Language Model (CILM). In summary, community interest modeling with the real world user oriented text data is an effective method for mirroring real world community from a cognitive perspective. By weighting each topic extracted from query driven time sensitive corpus, we can measure the degree of interest , namely, interest based ranking, which is differ from relevance ranking. The community interest vector in our experiments demons trates as an effective way to rank retrieved results. [1] Adamic, L. A., &amp; Glance, N. (2005). The political [2] Agichtein E., Brill E., Dumais S., (2006). Improving web [3] Anh, V. N., &amp;Moffat, A. (2002). Improved retrieval [4] Azzopardi, L., Girolami, M and van Rijsbergen, C.J. [5] Bernard J. Jansen, A. S., Judy Bateman, Tefko Saracevic. [6] Blei, D. M., Ng, A. Y., &amp; Jo rdan, M. I. (2003). Latent [7] von Brzeski, V., Irmak, U., &amp; Kr aft, R. (2007). Leveraging [8] Cilibrasi, R. L., &amp; Vitanyi, P. M. B. (2007). The Google [9] Craig Silverstein, H. M., Monika Henzinger, Michael [10] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. [11] Fox S., KarnawatK., Mydla ndM., DumaisS. &amp; White T., [12] Haveliwala, T. H. (2003). Topic-Sensitive PageRank: A [13] Heinrich, G. (2005). Parameter estimation for text analysis o. [14] Hofmann, T. (1999). Probabilisti c latent semantic indexing. [15] Irmak, U., von Brzeski, V, &amp; Kraft, R. (2009). Contextual [16] J X rvelin K. &amp; Kek X l X inen J., Cumulated gain-based [17] Joachims, T. (2002). Optimizing search engines using [18] Jones, K. S., Walker, S., &amp; Robertson, S. E. (1976). A [19] Kleinberg, J. M. (1999). Authoritative sources in a [20] Kobayashi, M., &amp; Takeda, K. (2000). Information retrieval [21] Lafferty, J., &amp; Zhai, C. (2001) . Document language models, [22] Page, L., Brin, S., Motwani, R., &amp; Winograd, T. (1998). The [23] Richardson, M., &amp; Domingos, P. (2002). The Intelligent [24] Salton, G., &amp; Yang, C. S. ( 1973). On the Specification of [25] Sparck Jones, K. (1972). A sta tistical interpretation of term [26] Wei,X., Croft,W. B., (2006).L DA-based document models [27] Wen J., Nie J. &amp; Zhang H., (2002). Query Clustering Using 
