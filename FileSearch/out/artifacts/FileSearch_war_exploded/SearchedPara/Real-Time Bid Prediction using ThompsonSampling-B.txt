 We study online meta-learners for real-time bid prediction that predict by selecting a single best predictor among sev-eral subordinate prediction algorithms, here called  X  X xperts X . These predictors belong to the family of context-dependent past performance estimators that make a prediction only when the instance to be predicted falls within their areas of expertise. Within the advertising ecosystem, it is very common for the contextual information to be incomplete, hence, it is natural for some of the experts to abstain from making predictions on some of the instances. Experts X  areas of expertise can overlap, which makes their predictions less suitable for merging; as such, they lend themselves better to the problem of best expert selection. In addition, their per-formance varies over time, which gives the expert selection problem a non-stochastic, adversarial flavor. In this paper we propose to use probability sampling (via Thompson Sam-pling) as a meta-learning algorithm that samples from the pool of experts for the purpose of bid prediction. We show performance results from the comparison of our approach to multiple state-of-the-art algorithms using exploration scav-enging on a log file of over 300 million ad impressions, as well as comparison to a baseline rule-based model using produc-tion traffic from a leading DSP platform.
 I.I [ Symbolic and Algebraic Manipulation ]: Applica-tions; I.2 [ Artificial Intelligence ]: Learning X  Concept learn-ing ; I.6 [ Simulation and Modeling ]: Applications X  Mis-cellaneous Performance, Design, Experimentation Randomized probability matching, Bayesian online learning, Online algorithms, Online advertising, Multi-armed bandits c  X 
With the increasing adoption of real time bidding (RTB), internet advertising models are undergoing a revolution. RTB enables marketers to participate in real time auctions for ad placement that occur seconds before a website loads. This functionality has helped to make advertising much more pre-cise and personalized and enables advertisers to reach their target audiences. Programmatic trading, the technology be-hind the RTB, offers an immense potential for cost effective advertising. However, in order to realize this potential, ad-vertisers need to intelligently adjust their bids as the mar-ket conditions fluctuate and real time feedback is received regarding the efficacy of a campaign. Demand-Side Plat-forms (DSPs) offer real time bid prediction [25, 18] to help advertisers find the optimal bid value in milliseconds. The goal of real time bid prediction is to maximize campaign performance goals under a budget constraint. Typical per-formance goals are minimizing cost-per-click (CPC) or cost-per-action (CPA), as well as maximizing click-through-rate (CTR) or action-rate (AR).

Predicting the performance of an ad is a challenging prob-lem for many reasons. First of all, each decision to buy an impression and at what price needs to be made in just few milliseconds. Top DPSs typically receive a few million bid requests per second coming from users that are exploring the web all around the globe. Most of the time, the necessary information to make the optimal bidding decision is miss-ing or delayed. For example, user data is based on cookies rather than real users. In addition, a significant number of visitors are likely to be entirely new with no historical con-sumption record whatsoever. In many web-based scenarios, the content universe undergoes frequent changes, with con-tent popularity changing over time as well. The situation is similar with respect to advertisers X  content (ads and cam-paigns); this is known as the cold-start problem.

The above problem is known as a feature-based exploration-exploitation problem [21] and to solve this problem we pro-pose learning a pool of simple online prediction algorithms (predictors) with a partial view of the full contextual infor-mation. Specifically, for each triple of (user, advertiser, publisher) features we create a prediction strategy (an  X  X x-pert X ) that examines only those features and makes predic-tions based on their values. In our framework the experts may choose to abstain from giving a prediction on a given instance. As in [6, 11], we shall call these experts special-ists , because they are allowed to abstain when the instance doesn X  X  fall into their areas of expertise, or specialities.
In this paper we shall deal with a pool-based real-time bid prediction. The learner is presented with a fixed pool of specialists that learn according to a modified online learn-ing protocol with batch updates. Learning proceeds in a sequence of trials. On each trial the learner receives one in-stance from a fixed domain and feeds this instance to the specialists in the pool. The learner then selects one spe-cialist from the pool and uses its estimate towards the bid computation. If the bid wins in the external auction then the learner pays a cost. After some time, the adversary pro-vides the learner with the true labels and then the learner induces an updated set of specialists based on the newly received information.

We frame the problem of best specialist selection in the multi-armed bandit framework and propose to use random-ized probability matching. An important component of the online learning framework is a loss function that associates a non-negative loss to each pair of prediction and outcome. In the multi-armed bandit framework, however, there is no obvious loss function defined on an instance basis.
Contributions . Our contributions are threefold. First, we describe how randomized probability matching can be naturally applied to settings like real-time bid prediction for online advertising. In particular, we describe a new version of a Thompson Sampling algorithm customized to suit such conditions 1 . It can be viewed as a meta-learning algorithm that selects from a pool of experts . Our work is one of the first in trying to use bandit type ideas for sampling from an ensembles of experts in real-time bid prediction with bud-get constraints, and we strongly believe that one can build extremely practical, yet very simple and scalable algorithms by understanding the interplay between multi-armed ban-dits and learning from a pool of experts. Second, we show that this approach is useful in practice, scales well, and is easily deployed at large-scale in real world industrial systems for real time ad allocation. Finally, we provide empirical support and comparison with several state-of-the-art algo-rithms using real world campaign data provided by Turn, a leading Demand-Side Platform (DSP).
A simplified illustration of the advertising ecosystem and a typical ad call flow is given in Fig. 1. The figure illustrates a user with a web browser, an ad server, an ad exchange, a Data Management Platform (DMP) and a Demand Side Platform (DSP).

The ad call flow starts from the user X  X  browser. The call is initiated when a user with uid opens a webpage at URL url . The user X  X  browser then contacts the ad exchange the publisher is integrated with. The ad exchange appends uid and url to a request submitted to several DSP partners. Using the uid and url , DSPs return to their decision enginers the information about the given user and the web page that they can find in their stores. Each DSP runs an auction among the eligible ads (normally those that satisfy certain targeting constraints) from its list of existing campaigns. Then, one or more ads, depending on the space and layout characteristics, are chosen for display and ranked based on some function of expected performance and advertiser X  X  bid. e.g. minimizing effective CPC (eCPC) and maximizing ad-vertiser ROI.
 Figure 1: Ad call flow between a web browser and a demand side platform.

The bidding DSPs return back their proposed ad and bid pair back to the ad exchange. Once the ad exchange collects all the bids, it runs an auction (usually a second price auc-tion) and determines the winning bid and the corresponding ad. It then passes the winning ad and the location of its creative back to the browser. The browser then collects the creative and finally returns the page to the user [19].
This entire flow needs to happen within a small portion of a second so that the user can see the page with ads as soon as the page appears on her web browser. This latency and throughput constraints put extreme time constraints on each bidding DSP and their decision engines. Our proposed approach is at the heart of the decision engine and works under such time constraints with a satisfactory performance.
For the sake of clarity, we will only talk about click events, and focus on maximizing the CTR or minimizing the cost-per-click (CPC). Our methodology is equally applicable to other types of events such as conversion events, engagement events, etc. Moreover, without loss of generality, we will focus on the display channel. We provide a bottom-up for-mulation of the problem that provides the skeleton of our solution constructed on ideas such as: feature hierarchies, randomized probability matching, and sleeping experts.
In the online advertising context, the feature space is usu-ally constructed along three main dimensions: user , pub-lisher represented as webpages, advertisers represented as ads. Time and other dimensions are extra. Let U = { u 1 ,u P = { p 1 ,p 2 ,...,p m } , and A = { a 1 ,a 2 ,...,a n } represent all the users, webpages and ads (with creatives) in a DSP, re-spectively. The goal in bid prediction is then to find among all eligible ads the ad that has the highest probability of a click for a given user on a given webpage. One way to model this is to represent users, webpages, and ads using a set of explicit features and build a classification model. However, due to the sparsity of user-level data, an alter-native and better idea is to compute the number of times an ad was displayed to  X  X imilar X  users on a given website and observe how many of these impressions result in a click. Then, the CTR for this user can simply be estimated as the total number of clicks among all similar users divided by the total number of impressions. This kind of user grouping can either be achieved by explicit clustering based on some similarity metric or it can be implicitly done by using data hierarchies [19].

For example, every ad in the DSP can be considered as be-longing to an advertising campaign which in turn belongs to an advertiser (e.g., Advertiser:  X  X cme Cars X   X  Campaign:  X 2011 Year End Sales X   X  Ad:  X  X ncredible Year End Sales Event! X ). Similarly, a website on which an ad will be dis-played is under a top level domain (TLD) which is owned by a publisher and the publisher itself might belong to some category based on its main content (e.g., Publisher Type:  X  X ews X   X  Publisher:  X  X cme City Times X   X  Page:  X  X uto News X ). If we assume that user, publisher, and advertiser data have l u , l p , l a levels in their respective data hierar-chies, there are l u  X  l p  X  l a possible ways of combining count data for a given ( user  X  publisher  X  advertiser ) triple.
Using these various levels of count data aggregated along the time dimension we can create simple context-dependent predictors that are able to produce a maximum likelihood estimate of the CTR. Experts that use higher-level data will be susceptible to high bias in their estimates. On the other hand, experts that use lower-level data will be susceptible to high variance, due to a limited number of data points from which the estimates are drawn. Naturally, by observing more data points in the lower levels of the data hierarchies the corresponding experts will become more accurate over time. In the next section, we discuss how we can combine these estimators to obtain a final prediction. likelihood estimates of the CTR for user u i , webpage p j ad a k , at K different level combinations. For clarity, we will drop the indexes and use  X  x 1 ,  X  x 2 , ...,  X  x K instead. Recall that each of these levels corresponds to a distinct combination of the user, publisher and advertiser hierarchies and, as such, it is not always clear which one will yield the best estimate of the true CTR p ijk .

Most of the time user-publisher-advertiser data will be missing, and the estimators using these information sources simply will not be computed. For example, a user IDs might not be found in the user profile servers, or the pub-lisher X  X  webpage will not match any of the known categories in the publisher taxonomy, or the advertiser has no past-performance data in the system. Moreover, there may not be sufficient number of click events in one of the hierarchi-cal levels to calculate a reliable estimator output using the past performance observations. As a result, the number of available estimates will change over time, and in some cases might be zero.

One way to aggregate these estimates is to use a learning method that combines the available estimates into a single prediction. For example, Lee et al. [19] have used logistic regression to combine experts estimates for action rate pre-diction. Further, there exist a number of online learning algorithms designed for the scenario of missing experts from a pool of learners, such as the online SBayes [11]. However, due to the dynamic nature of the problem and the time-varying performance of the experts, these approaches are not most suitable. Rather than trying to combine multiple predictions, in this work we try to pick the best estimator among the K experts. The reasoning behind this decision is two-fold: First, through an online evaluation we have ob-served that under specific contextual decisions, the experts are able to provide very good estimates of the CTR without merging; second, their performance varies significantly over time. Hence, our problem can be formulated as an online expert selection problem, where the set of available experts changes over time.
At an abstract level, advertisers place their bids in the form of CPC or CPA goals. In pure second-price auctions the dominant bidding strategy for the advertisers is to sub-mit their private true value [10]. Therefore, the value of the impression is typically calculated with the probability of a click (predicted by DSPs) multiplied by the value of the click (given by advertisers). The resulting bid for a given impression at time t would be calculated as: where the goal is a fixed number w.r.t. the given campaign that amounts to the monetary value the advertiser assignes to a click, and  X  ( t ) is the click probability. In practice, how-ever, this is never the case. Besides the additional bid ad-justments which are done post-CTR computation, the ad-vertisers can further influence the bid price through multiple scaling factors.

Therefore, when minimizing the eCPC, one needs to take into account the expected cost for the expected number of clicks. In other words, minimizing the eCPC is not equiva-lent to the CTR maximization strategy. To illustrate this, let X  X  assume that we need to minimize the eCPC, and the total budget is $1000. Assume further that we have two dif-ferent prediction strategies whose average estimates for the CTR are: 12  X  10  X  3 and 20  X  10  X  3 correspondingly. Due to the difference in the expected values of these estimates and the inherent sampling biases, the bid price would be differ-ent though not linearly dependent, so let X  X  assume that the average cost per impression is $0.05 and $0.12 correspond-ingly. That means one could buy on average around 20,000 impressions in the first case and around 8,334 impressions in the second case. This leads us to about 200 clicks with the first strategy and 175 with the second, and correspondingly $5 and $5.7 effective cost per click (eCPC). In this case, us-ing the first strategy whose CTR estimate is lower can lead us to a better eCPC.
Choosing the best expert for bid prediction can be natu-rally viewed from the perspective of exploration-exploitation trade-off. The concept of exploration-exploitation is central to problems in decision making under uncertainty, and is best illustrated by the multi-armed bandit (MAB) problem.
In its simplest formulation (generally referred to as stochas-tic), a bandit problem consists of a set of K probability distributions &lt; D 1 ,...,D K &gt; with associated expected val-ues &lt;  X  1 ,..., X  K &gt; . Initially, the distributions are unknown to the player. In fact, these distributions are generally in-terpreted as corresponding to arms on a slot machine; the learning algorithm is viewed as a gambler whose goal is to collect as much money as possible by pulling these arms over many turns. At each turn, t = 1 , 2 ,..., the player selects an arm, with index a t , and receives a reward y t  X  D j ( t ). The goal of the algorithm is to find out which distribution has the highest expected value, and also to gain as much reward as possible during the course of the game.
The added complexity of using bandit algorithms in bid prediction problems is two-fold: First, we assume that the bandit algorithm has access to a set of experts S t  X  1, K whose performance can change over time; second, for each turn t the bandit has to pay an unknown variable cost c associated with winning the auction. Hence, each trial of the bid prediction problem is a trial between the algorithm and an adversary in a game that consists of the following steps: 1. The adversary choses a set S t  X  { 1 ,...,K } of experts 2. Each available expert i  X  S t provides an estimate  X  x 3. The algorithm chooses one expert s j from S t and uses 4. The adversary chooses a cost c t for buying that im-5. The algorithm pays a cost c t .

Various strategies have been proposed that maximize the expected reward, but very few consider the inherent cost for playing an arm. As we showed, an algorithm that strives to maximize the expected reward (CTR) can be sub-optimal in minimizing the effective cost-per-click (eCPC). We propose a solution based on randomized probability matching , better known as Thompson Sampling [27] (TS). In the next section we provide the details of our solution.
Randomized probability matching plays each action ran-domly in proportion to its probability of being optimal . This approach is known as Thompson Sampling and has been shown to be especially successful in systems whose limita-tions allow only periodic (batch) updates. It is broadly ap-plicable, easy to implement and can be extended to work with a broad class of reward distributions.

We consider a general probabilistic, or Bayesian, formu-lation in which uncertain quantities are modeled as random variables. Let y t = ( y 1 ,...,y t ) denote the sequence of re-wards observed up to time t , such that y i  X  { 0 , 1 } ,  X  i = 1 ,...,t . We focus on binary rewards because clicks are bi-nary events. Let a t denote the played arm at time t . The success of each arm is modeled as an IID random variable, Y , Y 2 , ..., Y K living in a space Y = { 0 , 1 } K . Since each random variable has a binary outcome, each arm is mod-eled with a Bernoulli distribution with success probability  X  . It follows that each y t was generated independently from the reward distribution f a t ( y |  X  ) of the played arm, where  X  is an unknown parameter vector that governs the success probabilities of the Bernoulli distributions of the arms.
Let c t = ( c 1 ,...,c t ) denote the sequence of costs that the algorithm has payed for each observed reward up to time t , such that c i  X  R ,  X  i = 1 ,...,t . The cost of each arm is mod-eled as an independent and identically distributed random variable, C 1 , C 2 , ..., C K defined in a space C = R K the cost distribution of each arm is modeled with a Gaus-sian distribution with parameters m a and  X  a . It follows that each c t was generated independently from the cost dis-tribution f a t ( c | m, X  ) of the played arm, where m and  X  are unknown parameter vectors.

Let  X  a (  X  ) = E ( y t |  X ,a t = a ) denote the expected reward coming from the reward distribution f a t ( y |  X  ). Let  X  E ( c t | m, X ,a t = a ) denote the expected cost coming from the cost distribution f a t ( c | m, X  ). If our goal was to maxi-mize the number of clicks without considering the price we are paying for each impression, then the optimal long run strategy would be to always choose the arm with the largest  X  (  X  ). However, our goal is to minimize the ratio of the total cost and the total number of clicks, i.e., the cost per click. Therefore, we will denote with  X  a (  X ,m, X  ) the expected cost per click coming from the reward distribution f a t ( y |  X  ) and the cost distribution f a t ( c | m, X  ).

Let p (  X  ) denote a prior probability on  X  , and p ( m, X  ) de-note a prior probability on m and  X  . Assuming that we have a method to compute  X  a =  X  a (  X ,m, X  ), according to the principles of randomized probability matching one can compute the initial allocation probabilities: Eq. 1 can be expressed as an integral of an indicator func-tion. Let I a (  X ,m, X  ) = 1 if  X  a (  X ,m, X  ) = min {  X  1 ...,  X  K (  X ,m, X  ) } , and I a (  X ,m, X  ) = 0 otherwise. Then w a, 0 = E ( I a (  X ,m, X  )) = Here we assume that the success variables and the cost vari-ables are independent from each other. The prior distribu-tions represent our beliefs or a-priori knowledge about the success probability of each arm as well as the cost parame-ters per arm. As rewards and costs from the bandit process are observed, the parameters of the reward and cost distribu-tions are updated through the process of Bayesian updating. After observing the sequence of rewards y t and costs c t time t the posterior distribution of  X  is while the posterior distribution of m and  X  is Hence, to compute the allocation probabilities in the next iteration all we need is a component that will be able to es-timate the expected reward  X  a and up-to-date beliefs repre-sented with the posterior distributions p (  X  | y t ) and p ( m, X  | c Eq. 1 and Eq. 5 can be computed using the law of large numbers by simulation. Let  X  (1) , ...,  X  ( G ) be a sample of independent draws from p (  X  | y t ), and ( m (1) , X  ), ..., ( m be a sample of independent draws from p ( m, X  | c t ). By the law of large numbers, Eq. 6 estimates the allocation probabilities w a,t by the em-pirical proportion of Monte Carlo samples in which  X  a (  X  is maximal. Choosing adequate conjugate prior distribu-tions p (  X  ) and p ( m, X  ) makes sampling independent draws of  X  and m possible. In our case we will be working with a Beta distribution as a prior for the Bernoulli distribution used to model the reward likelihood, and a Gaussian prior for the Gaussian distribution used to model the cost likeli-hood.

Estimating  X  a (  X ,m, X  ) is a separate problem which will not be discussed here in full detail. The method simply esti-mates the expected cost per click by the method of simula-tion, using the posterior distributions p (  X  | y t ) and p ( m, X  | c Since posterior draws are all that is needed to compute the allocation probabilities one can apply randomized probabil-ity matching with almost any family of reward distributions.
The pseudo-code of our proposed Thompson Sampling al-gorithm is given in Algorithm 1. The algorithm starts from its prior beliefs on the expected success rate and the ex-pected cost, implemented with a Beta and a Gaussian prior distribution, respectively. In each trial, the algorithm first observes the set of available experts S t and their estimates  X  x t (line 3), and performs a Monte Carlo simulation to cal-culate the allocation probabilities using Eq. 6 (line 4). The simulation consists of drawing independent samples from the posterior distributions of  X  and m . Then the algorithm chooses to play an expert according to the allocation prob-abilities w t (line 5), and uses its estimate of the CTR to calculate the bid. If the bid wins the external auction, the algorithm observes the outcome y t and the cost c t (line 8), and updates the posterior distributions by updating their parameters (lines 10-14).
 Algorithm 1 Thompson Sampling with double priors 1: Initialize S(1) = 0, F(1) = 0, , T(1)=0,  X  m (1) = 0 2: for t = 1 , 2 ,...,T do 3: Observe S t and  X  x j t 4: For i  X  S t calculate the allocation probabilities w i,t 5: Choose one expert a at random according to w t 6: Use estimate  X  x a t to submit a bid b t 7: if b t won the auction then 8: Observe the outcome y t and the cost c t 9: Update the posterior distributions of expert a : 10: S a ( t + 1) = S a ( t ) + y t 11: F a ( t + 1) = F a ( t ) + 1  X  y t 12: m ( t + 1) =  X  2 0  X  2 13:  X  ( t + 1) 2 = 1  X  2 14: T a ( t + 1) = T a ( t ) + 1 15: end if 16: end for
Using Beta(  X  ,  X  ) priors is useful for Bernoulli rewards be-cause the Beta distribution is a conjugate prior distribution for Bernoulli. This enables simple Bayesian updates through updating the parameters  X  and  X  . At time t having observed S ( t ) successes and F a ( t ) failures in T a ( t ) plays of arm a , the posterior distribution is Beta( S a ( t )+ y t , F a ( t ) + 1 -y
Similarly, the conjugate prior distribution for a Gaussian likelihood distribution is Gaussian, which again enables sim-ple updates through the parameters of the distribution. We choose to model the likelihood of the cost f a ( c | m, X  ) with a Gaussian distribution N ( m, X  2 )  X  1  X  t exp  X  1 2  X  2 P
By choosing a conjugate prior distribution N ( m 0 , X  2 0 where the prior mean m 0 is typically chosen to be 0, and the prior variance  X  0 is some large value, the resulting posterior distribution p ( m | c t ) is obtained by simply multiplying the likelihood f a ( c | m, X  ) and the prior p ( m 0 , X  0 ): Both of the distributions can be updated using batch up-dates of their parameters.
In a variety of practical applications as well as in our sys-tem the time evolution of the system, and in particular of the reward distributions, is gradual. Actually, if the rela-tionship between the input features and the target variable is stationary then we can confidently predict that the reward distributions for each cold-started arm will be evolving in a similar way. As the estimates of the experts improve over time we should expect that there will a gradual increase in the running CTRs and a shift towards convergence. All of the experts will be characterized with an initial period of high instability and uncertainty. An example of how one should expect the evolution of the CTR would look like is given in Fig. 2. Figure 2: Evolution of the click-through-rate (CTR) for one specialists w.r.t. a given campaign.

Hence, for dynamic multi-armed bandit problems one should prefer policies that take into account the fact that the reward distributions change in a gradual manner, starting from a point of high instability and uncertainty and moving slowly towards a more stable phase. While Thompson Sampling naturally handles this evolution, it can be easily adapted to track potential changes.
As discussed previously the problem we are solving falls in the family of multi-armed bandit problems with non-stationary reward distributions, where the optimal arm changes over time. We have discovered two sources of change in the reward distributions of the experts (arms): 1. Due to the slow increase in the accuracy and predic-2. Due to the dynamic nature of the bidding process and
To be able to track changes in the probability of success of each expert, we adopt an approach based on exponential smoothing proposed by Gupta et al. [14]. This is a filtering technique that can be easily and efficiently implemented in a large-scale system like ours. The gist of exponential smooth-ing is exponential weighting of the outcomes in each trial, such that older outcomes get smaller weights and hence con-tribute less to the current estimate of the success probability  X  . This is a typical forgetting mechanism used in many on-line learning algorithms designed for dynamic environments, and has been used for computing prequential (predictive-sequential) error estimates as well, where the sum of losses L A ( t ) at time t is computed as: where  X   X  [0 , 1] is a fading factor that determines the speed with which past errors will be diminished.

As discussed previously the posterior of the reward distri-bution at time t of expert a is modeled with a Beta( S a ( t )+ y F ( t ) + 1  X  y t ) distribution, where S i ( t ) and F i the successes and the failures up to time t . Exponential smoothing is simply implemented through a new set of up-date rules on the parameters of the Beta distributions S and F i ,  X  i = 1 ,...,K and a threshold parameter C which determines the upper bound on the variance of the Beta distribution: 1. If S a ( t  X  1) + F a ( t  X  1) &lt; C 2. If S a ( t  X  1) + F a ( t  X  1)  X  C
The effect of these update rules are that: 1) they ensure that the sum of S a ( t ) and F a ( t ) never grows above C ; 2): where  X  = C C +1 ; and 3): The direct effect of bounding the variance of the Beta dis-tribution through the parameter C is to enabling indefinite exploration and through that, detection of relative changes in the expected values of the reward distributions of the ex-perts. Due to the scale of our system and the large number of campaigns with different pacing parameters, the value of the parameter in practice needs to be set individually.
Exponential smoothing, however, is not sufficient to cap-ture the dynamics of the reward distributions that are spe-cific to cold-started experts. For that purpose, our online learning samplers operate on sliding windows. Using a slid-ing window enables us to discard the counts of successes and failures from the early stages of the specialized learners and maintain estimates using the most recent data.
In the traditional, non-contextual multi-armed bandit prob-lem, the learner has no access to arm features and simply competes with pulling the best of K arms in hindsight. As opposed to the traditional K -armed bandit problems, fea-tures of the arms may be useful to infer the conditional av-erage payoff of an arm and improve the total average payoff over time. Our idea is to use real-time performance features of each expert, and time decaying functions that model the uncertainty in their performance in the early stages of their learning process as contextual information.

We use a predefined set of performance features and the time span w.r.t. the number of times the specialist was given a chance to learn: 1) the empirical estimate of the success probability f 1 ( t ) = S a ( t ) /T a ( t ), the running average of the specialist X  X  predictions f 2 ( t ) = 1 T the running variance of the specialist X  X  predictions f 3 ( t ) = and the time-decaying function f 5 ( t ) = 1  X 
The most notable example of bandits with side infor-mation are contextual bandits with linear payoff functions, which is a well studied problem in statistics and machine learning. Agrawal and Goyal have designed the first ex-tension of Thompson Sampling for the case of the stochastic contextual multi-armed bandit problem, using Gaussian pri-ors and Gaussian likelihood model [3]. We have adopted the same design of a contextual Thompson Sampling algorithm without any changes and used it for CTR estimation.
The practical deployment of the Thompson Sampling al-gorithm involves the design and implementation of an of-fline and an online component. The offline component is de-signed to collect performance data which is being stored and accessed periodically, using Apache Hadoop and Pig. The online component, on the other hand, is executed by the ad servers in real time, and involves the process of sampling from the posterior distribution and calculating the alloca-tion probabilities.

To ingest the data from the outside world, our data pro-cessing pipeline defines a process of transformation, join, and compression stages. Raw web logs need to be first synced to a designated location on the Hadoop file system, where they would be picked up by the modules for click attribu-tion. This stage is followed by click de-duplication and fraud detection, and at last the aggregated data becomes accessi-ble for querying. Because of this complicated pipeline, up-dates to the learners are periodical and can only happen in batches. It becomes clear that algorithms that cannot operate with batch updates cannot be implemented in our platform.

Further, having different campaigns with different bud-gets, bidding pace, bidding tactics, goals and targeting cri-teria, the expert selection problem needs to be carried out Figure 3: Illustration of the learning flow on HDFS. at a more granular level. Therefore the data collection and model generation is done on a campaign level. For each cam-paign and for each expert that has been played, the data collection flow consists of a cyclic repetition of the following stages: collection and aggregation, exponential smoothing, generation of new hyper-parameters, as shown in Figure 3. The resulting model is in essence a key-value map, where the key is a combination of a campaign id and expert id, and value is a list of multiple summary statistics and distribution parameters.

After the new model is generated, it is synced to the 1000s of ad servers on the distributed platform, and its execution proceeds without real-time updates. It is worth mention-ing that sampling from a Beta and a Gaussian distribution can be time consuming especially when the whole bid pre-diction process has to be done within few milliseconds. In order to speed up the sampling, we have used a fast im-plementation of the Mersenne-Twister algorithm [23] as a replacement of the random sampling function available in Java, as well as an implementation of the Ziggurat algorithm for generating random numbers from a Gaussian distribu-tion [22]. Although using these improved samplers within the algorithms for sampling from a Beta and Gaussian dis-tribution improved the sampling time significantly, due to the extremely sensitive time restrictions for some cases we also used precomputed samples and estimates.
The exploration / exploitation dilemma is an old problem and has received a significant amount of attention from the statistics and machine learning communities. The Multi-armed bandit problem has close connections to the Boost-ing method in classification [29], and to sparse recovery and compressed sensing [28]. Various strategies have been pro-posed throughout the decades, probability matching, greedy, hybrid strategies, methods based on indices such as the upper confidence bounds (UCB) and Gittins indices [12]. Among the oldest heuristics are the randomized probability matching or posterior sampling strategies. The first version of this Bayesian heuristic is around 80 years old, dating back to Thompson [27].

Thompson Sampling (TS) has only recently been estab-lished as a top performer for MABs with Bernoulli dis-tributed rewards, and the reason for this delay has been the lack of theoretical understanding. Several studies [13, 26, 8, 24, 15] have empirically demonstrated the efficacy of TS. Weak guarantees have been provided by [13, 24] with a bound of o ( T ) on the expected regret in time T . Some significant progress has recently been made by [1, 15, 2], who provided optimal regret bounds on the expected regret; Agrawal and Goyal provided high probability, near-optimal regret bounds for stochastic contextual bandits with linear payoffs [3].

It is useful to mention that one could imagine trying to solve the best specialist selection problem using EXP3-or EXP4-type approaches [5]. In EXP4-type approaches the player X  X  goal is to combine the advice of the experts in such a way that its return is close to that of the best expert. EXP4 stands for  X  X xponential-weight algorithm for Explo-ration and Exploitation using Expert advice X , hence, it is an algorithm that learns how to mix the probability distri-butions coming from N experts in order to generate a final mixture distribution over the set of arms of size K . It can be used in a simplified scenario, where each expert corresponds to a deterministic policy that maps a context to only one arm with probability 1.0, and with probability 0.0 to the rest of the arms. EXP3 is in essence a special case of EXP4 for this simplified scenario.

While in the expert-learning framework, each expert cor-responds to a contextual policy for arm selection, in our setup experts are not a mapping from a context to an arm, but they are specialized arms whose outputs are the con-ditional probabilities f i ( y t |  X  ). EXP3-and EXP4-type ap-proaches cannot be easily modified towards more complex reward distributions or optimization goals. Therefore, none of these approaches provide any advantage over Thompson Sampling. Posterior sampling can be applied to a much broader class of problems, and one of its greatest strengths is its ability to incorporate prior knowledge in a flexible and coherent way.
 Recently Li [20], motivated by the connection between Thompson Sampling and exponentiated updates, has de-signed a new family of algorithms called Generalized Thomp-son Sampling in the expert-learning framework [7]. General-ized Thompson Sampling (GTS) is very similar in structure to EXP4, with the difference of a more general update rule that uses a loss function to adjust the expert X  X  weights. Each expert represents a greedy policy with respect to the prob-ability of success (reward prediction) that maps a context to an arm. As in all existing analysis for Thompson Sam-pling, the assumption is that one of the experts correctly predicts the expected reward (probability of success). The generalization involves two loss functions: Logarithmic loss and square loss, for which regret bounds are derived. The author shows that Thompson Sampling is a special case of GTS when the logarithmic loss is used 2 .

Due to the complexity of the performance-based bid pre-diction problem, very few algorithms are designed to maxi-mize the expected reward (CTR) having into account budget limitations or costs for playing the arms. To the best of our
The loss function is used to measure how well the expert predicts the average reward, given the context and the se-lected arm. In general, the loss function and the reward may be completely unrelated. knowledge, there is only one work that treats the bandit problem with budget constraint and variable costs [9]. The authors propose UCBBV2 an UCB-type algorithm whose expected regret depends on the budget which constrains the total number of pulls. While this algorithm does take into account the variable costs per arms, it is not designed to work with batch updates and as such it is not appropriate for our platform.
Our proposed framework of bid prediction has been im-plemented, tested, and deployed at Turn, a leading DSP. In this section, we present experimental results from our testing environment where we compared different strategies for expert selection. In addition, we also show results from real campaigns that serve large amounts of daily impres-sions in order to demonstrate the overall performance im-provement in terms of CTR, eCPC, and ROI performance metrics. Advertiser X  X  return of investment (ROI) is the to-tal value (# click  X  value ) divided by the total cost incurred ( P i c i ).
The first part of our experimental evaluation consists of an offline policy evaluation of the proposed Thompson Sam-pling algorithm and its extension, the Contextual Thompson Sampling algorithm with a number of state-of-the-art multi-armed bandit strategies. The offline evaluation is performed on a large collection of over 300 million ad impressions, col-lected by Turn X  X  processing pipeline in a period of 30 days. The impressions belong to 7 top spending advertisers that are running several campaigns on the platform. The second part of our experimental evaluation is an online comparison of Thompson Sampling on production data to our existing baseline algorithm through fair A/B testing. The baseline algorithm is a static, rule-based model that chooses the best expert in a deterministic manner.

First, we would like to emphasize the volatility of the CTR in few top performing estimators through the daily evolution of the CTR shown on Fig. 4. This CTR is computed by ordering our historic impressions based on the time stamp the impression was shown to a user and aggregated by day. We can see that the daily CTR varies significantly, which has strongly discouraged us from running experiments on simulated data.
 Figure 4: Illustration of the evolution of the CTR per day for the top performing experts.

In the comparison we have used the following state-of-the-art MAB algorithms: Chernoff UCB is an index based UCB-style algorithm with a tight upper confidence bound derived from the Chernoff bound ([16], page 278), EXP3 and EXP3.S [5] are bandit algorithms with experts advice, UCB1 and Tuned UCB1 [4], UCBBV2 [9]. For a fair comparison we ran all of the algorithms without using any explicit change detection or blind concept drift management. All of the algorithms were further tuned for best performance.
As a DSP, we do not have the luxury of executing arbi-trary policies on real traffic, and the only viable alternative is to do offline policy evaluation. Offline policy evaluation is the process of evaluating a new strategy for behavior, or policy, using only observations collected during the execu-tion of another policy. The difficulty of this problem stems from the lack of control over available data. In our case we are bounded to use the existing logged impressions and the corresponding choices of the baseline algorithm, which is our exploration policy.

Exploration Scavenging (also known as offline replay [17]) is a popular method for offline evaluation of new MAB poli-cies  X  i , by using the logged decisions of an existing baseline policy s . This method suggests to calculate an expected reward of a new policy s using the following equation:  X  R (  X  ) = 1 where T denotes the total number of historic logs used in the replay, A is the set of all possible actions (arms), x t is the context for impression at time t , 1 is the indicator function, and w x,a is a normalization weight calculated from the whole set of impressions with contextual information x as w
Exploration scavenging first estimates the average reward of each action from the set of impression on which both the evaluation  X  and the exploration s policies agree on choos-ing the same action a , then applies the estimate to all the impressions for which the evaluation policy  X  suggests the action a . Fig. 5 shows the total scavenged CTR for each algorithm computed at the end of the run.
 Figure 5: Total scavenged CTR per algorithm.

Beside exploration scavenging we also show results com-puted using only the impressions on which the evaluation policy agreed with the exploration policy, without apply-ing precomputed estimates of the average reward. We will refer to this approach as the matched CTR. Unlike the ex-ploration scavenging estimates this approach will provide rather optimistic estimates of the CTR. These estimates are highly dependent on the number of matched impressions be-tween the evaluation and the exploration policy. From all algorithms, TS and Contextual TS had the largest overlap with the baseline policy, with Contextual TS having twice as much more matched decisions than TS. Fig. 6 shows a comparison of the total matched CTR for all algorithms at the end of the run.

At last, Fig. 7 shows relative results in terms of the to-tal ROI at the end of the run, which is our main metric of interest. The top leading algorithms are evidently TS and Contextual TS, with UCBBV2 and the baseline being third. As discussed previously, to work properly, UCB-type of algorithms require updates of the arm X  X  upper confidence bounds instantaneously due to the deterministic indexes. In practice, this is rarely doable. On the other hand, Thomp-son sampling relies purely on random distribution sampling, and can maintain a reasonable balance between exploration and exploitation in each round while using a batch-updates.
Due to the dynamic behavior of the expert X  X  performance it is important to understand how the performance of each of the top algorithms evolves over time. Due to space limi-tations we will only show the evolutions of the CTR and the ROI for the Thompson Sampling algorithm and our baseline model evaluated on production traffic over the timeline of one day. Figure 8 and Figure 9 show summaries of the cumu-
Figure 8: Evolution of CTR on production traffic.
Figure 9: Evolution of ROI on production traffic. lative CTR and ROI on a global level as time proceeds. We can see that Thompson Sampling performs better than the baseline model both in terms of CTR and in terms of global ROI. Similarly, Figure 10 compares the cumulative eCPC of the two models as a function of time. As the plot indicates, TS is able to provide a lower overall eCPC compared to the baseline model. Figure 10: Evolution of eCPC on production traffic.
Multi-armed bandits have an important role to play in modern production systems that emphasize  X  X ontinuous im-provement X , where products remain in a perpetual state of feature testing even after they have been launched. In this paper we advocate using randomized probability matching as a superior algorithm, and the industry standard, due to its performance both in terms of accuracy, as well as speed, broad applicability and ease-of-use. Thompson Sampling of-fers superior real-time bid prediction compared to the base-line algorithm in just a few milliseconds. [1] S. Agrawal and N. Goyal. Analysis of thompson [2] S. Agrawal and N. Goyal. Further optimal regret [3] S. Agrawal and N. Goyal. Thompson sampling for [4] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time [5] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. [6] A. Blum. Empirical support for winnow and [7] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, [8] O. Chapelle and L. Li. An empirical evaluation of [9] W. Ding, T. Qin, X.-D. Zhang, and T.-Y. Liu.
 [10] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [11] Y. Freund, R. E. Schapire, Y. Singer, and M. K. [12] J. C. Gittins. Bandit processes and dynamic allocation [13] O. Granmo. Solving two-armed bernoulli bandit [14] N. Gupta, O.-C. Granmo, and A. Agrawala.
 [15] E. Kaufmann, N. Korda, and R. Munos. Thompson [16] J. Langford. Tutorial on practical prediction theory for [17] J. Langford, A. Strehl, and J. Wortman. Exploration [18] K.-C. Lee, A. Jalali, and A. Dasdan. Real time bid [19] K.-c. Lee, B. Orten, A. Dasdan, and W. Li.
 [20] L. Li. Generalized thompson sampling for contextual [21] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [22] G. Marsaglia and W. W. Tsang. A simple method for [23] M. Matsumoto and T. Nishimura. Mersenne twister: [24] B. C. May, N. Korda, A. Lee, and D. S. Leslie. [25] C. Perlich, B. Dalessandro, R. Hook, O. Stitelman, [26] S. L. Scott. A modern bayesian look at the [27] W. R. Thompson. On the likelihood that one unknown [28] S. Jafarpour, V. Cevher, and R. E. Schapire. A game [29] R. E. Schapire, and Y. Freund. Boosting: Foundations
