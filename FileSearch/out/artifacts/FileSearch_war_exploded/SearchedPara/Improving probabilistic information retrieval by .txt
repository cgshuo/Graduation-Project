 1. Introduction els for each query: relevant class and non-relevant class. The probability ranking principle (PRP) (Robertson, 1977 ) proves that ranking documents by their decreasing probabilities of relevance optimizes the retrieval performance, if the probabil-ities are estimated as accurately as possible.
 approximation techniques for modeling the relevant class.
 1999; Lafferty &amp; Zhai, 2001 ). These language modeling approaches focus on effective estimation techniques for document modeling, and have been demonstrated to achieve excellent retrieval accuracy and efficient implementation in practice.
The major difference between the language model (LM) and probabilistic retrieval model (PRM) is that the LM evaluates the query likelihood and the PRM evaluates the document likelihood.

A major limitation of the LM approach has been the lack of a clear connection with the explicit modeling of relevance. This have demonstrated the probability equivalence of the LM to the PRM under some very strong assumptions, which may or may not hold in practice.

Lewis (1998) has pointed out the connection between the PRM in information retrieval and the naive Bayes classification model in machine learning. He also discussed the fact that the multinomial distribution performs well in the naive Bayes
The word independence assumption for the multinomial distribution results in a poorly performing score function, which linearly depends on term frequency and ignores document length. Accordingly, an open research problem that remains to be
The multinomial distribution cannot capture word burstiness: the phenomenon that if a word appears once, it is more likely kan, 2006 ) has been shown to be effective in accommodating word burstiness, and achieves better performance in text clas-sification and text clustering. Relying upon hierarchical Bayesian modeling (HBM), the DCM distribution integrates out the parameters of the multinomial distribution. The DCM distribution is also motivated by the Polya urn scheme ( Johnson, Kotz, better distribution for probabilistic models. In our paper, we use the DCM distribution as the generative source for the new probabilistic model.

The optimal setting of the retrieval parameters is usually achieved by brute force tuning. Since both the query and doc-ing already tuned parameters to another retrieval task may not result in consistently good performance. Therefore, automatically setting retrieval parameters is critical to accommodate various retrieval tasks. Zhai and Lafferty (2002) have derived a general two-stage parameter estimation method for the LM. We propose several parameter estimation approaches that explicitly capture the different impacts of document collection and query on the optimal settings of retrieval parame-ters. We first estimate the non-relevant model by fitting a DCM distribution to the whole document collection based on three approaches, and then estimate the query interpolation parameter which controls the relevance model generation.
Pseudo-relevance feedback has been demonstrated to be one of the most efficient approaches to improve retrieve accu-racy. Pseudo-relevance feedback assumes that the top ranked retrieved documents are relevant, and reformulates the query representation by using these documents. Various pseudo-relevance feedback algorithms have been applied to several re-trieval models. For the vector space model approaches, the original query is expanded with the centroid of feedback docu-ments ( Rocchio, 1971 ). For the LM approaches, Zhai and Lafferty (2001a) have proposed several model based feedback algorithms, which expand the original query representation by the relevant topic terms from the feedback documents. Lan-guage modeling approaches apply query expansion to incorporate information from pseudo-relevance feedback documents
Thus, probabilistic approaches based on document generation have the advantage of being able to inherently improve the estimation of the probabilistic models by exploiting both the positive and negative feedback information. Considering the advantages of the probabilistic model, Lavrenko and Croft (2001) proposed a novel technique to estimate the relevance mod-el from the top ranked retrieved documents by combining language modeling estimation techniques with probabilistic mod-el framework. Nevertheless, they essentially model the pseudo-relevance feedback process, and their model relies on a baseline language modeling approach. In this paper, we design a pseudo-relevance feedback algorithm based on a mixture of two DCM distributions: one for the feedback relevant model, and the second for the collection model. We use the EM algo-rithm to estimate the feedback relevant model, and propose several improvements on the EM algorithm to find a better local optimum.

In summary, this paper focuses on an effective probabilistic model which applies text document modeling and estimation techniques. This paper also develops a theoretical understanding of PRM and LM under different distribution assumptions.
The main contributions of this paper are four fold: (1) A formal PRM based on the DCM distribution, with associated analyses. (2) Several approaches to effectively estimate parameters of the DCM probabilistic model. (3) A pseudo-relevance feedback algorithm based on the mixture modeling of feedback documents. (4) A discussion on the connections among PRM, naive Bayes classifier and LM under different distributional assumptions. 2. Related work 2.1. Probabilistic retrieval models be found in ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ). The PRM assumes that the probability of relevance depends on the query and the document representation only, and then ranks documents by the log-odds of their probability of being observed of randomly selecting the document d from the class R of relevant documents, and we rank documents based on one if it exists, otherwise 0), the basic ranking function for document d can be obtained by where I  X  w 2 d  X  is the indicator function, defined as and thus, I  X  w 2 q  X  assigns a binary value for word w in query q . R and R represent relevant class and non-relevant class of the above equation is derived by assuming independence of index words. The third line of the above equation is derived by
P  X  w j R  X  X  P  X  w j R  X  X  1. The above model is called the binary independence model, since it assumes word independence and uses binary indexing. We could also consider the above model as the probabilistic ranking model under the assumption that relevant class and P  X  w j R  X  for non-relevant class, respectively.

Robertson (1977) has proved that the probability ranking principle minimizes the expected fallout given the expected the Bayesian decision theory.

Various approaches to estimate of relevance models have been considered in previous literature. The binary indepen-pendence between words. The 2-Poisson model (Harter, 1975 ) treats the term frequency as a mixture of 2-Poisson distributions, but ignores document length. Robertson and Walker (1994) approximate the 2-Poisson model to account for several influential variables, including document length. However, these approaches fail to capture some essential ele-ments of retrieval score functions, and thus cannot provide satisfying retrieval performance. 2.2. Language models
Thelanguagemodelingapproaches(Ponte&amp;Croft,1998;Song&amp;Croft,1999;Lafferty&amp;Zhai,2001 )haveattractedsignificant attention in recent years. These language modeling approaches have demonstrated excellent retrieval accuracy and efficient implementation in practice by applying effective estimation techniques for document modeling. Language modeling ap-proachestreateachdocumentinthecollectionas auniquemodelandthequery asstrings oftextrandomlysampledfrom these models.Thebasicideaoflanguagemodelingapproachescanbedescribedasfollows.Weassumethataquery q isgeneratedbya given the document LM, which can be solved by the Unigram LM. Prior probability P  X  d  X  can be modeled to capture other information besides the content of the document. Link structures as well as other features of a document have been incor-porated in the LM (Miller &amp; Leek, 1999; Kraaij &amp; Westerveld, 2002 ).
 Lafferty and Zhai (2001) have formalized the language modeling in the risk minimization framework, and proved that the query likelihood P  X  q j d  X  retrieval model. The KL divergence model ranks documents based on the KL divergence between the estimate of the query model and the estimate of the document model. The Unigram LM based on the multinomial dis-tribution is an appropriate and efficient model for documents. The Maximum likelihood estimate of the LM is calculated by counting the relative word frequency in the document the maximum likelihood estimate gives zero probabilities to the words not occurring in the documents (unseen words), and applied to overcome the underestimation problem of unseen words. These approaches use the collection language model to assign extra probability mass to these unseen words. The collection language model P  X  w j C  X  can be estimated as
The Jelinek X  X ercer smoothing method (Zhai &amp; Lafferty, 2001b ) uses a linear interpolation of the maximum likelihood estimate with the collection model tuned by a coefficient k
The Dirichlet prior smoothing model (Zhai &amp; Lafferty, 2001b ) relies on hierarchical Bayesian modeling techniques, and achieves better performance than other smoothing approaches. The Dirichlet prior smoothing approach treats the collection LM as a prior distribution for the multinomial parameters
The parameter l indicates the equivalent sample size of the prior data implied in the Dirichlet prior distribution. The maximum a posteriori estimate (MAP) of the multinomial parameters distribution With the Dirichlet prior smoothing, the retrieval score formula is n  X  d  X  and n  X  q  X  are the length of document and query respectively; ring both in document d and query q .

Besides the above LM approaches, there are some other LM variants which account for other modeling factors. The Dirich-let prior smoothing approach uses the MAP estimate as the parameters of the smoothed document multinomial distribution.
However, the posterior distribution contains richer information than the single point estimate. With this motivation, Zara-goza, Hiemstra, and Tipping (2003) proposed a Bayesian predictive model to extend the Dirichlet prior LM. The basic LM has been extended to incorporate linguistic structure. For instance, Gao, Nie, Wu, and Cao (2004) proposed a dependence lan-guage modeling technique which extends the basic language modeling approach based on the Unigram LM by introducing a linkage of a query as a hidden variable which expresses the term dependencies within sentence. The basic LM has also been extended to model corpus structure. For instance, Kurland and Lee (2004) proposed a novel algorithmic framework in which information provided by document based language models is enhanced by incorporating information drawn from clusters of similar documents. 3. Probabilistic retrieval model based on the DCM distribution 3.1. Motivation
In this section, we analyze the PRM based on the multinomial distributions to shed some light on the intuition of using the DCM distribution. Recall that the PRM model in Section 3.1 is based on the multivariate Bernoulli distribution, which ignores word frequencies. Here we discuss PRM model based on the multinomial distribution to account for word frequen-cies. Documents are modeled as multinomial distributions capturing the word occurrences within the documents, in the lan-guage modeling retrieval algorithms and naive Bayes classifiers. We model the relevant model and non-relevant model in the PRM as two multinomial distributions. We denote the parameters of relevant and non-relevant document LM by h of document d l generated by relevant class is computed by using the multinomial distribution described below:
The score function of the PRM based on the multinomial distribution can be derived by taking the log-odds ratio of the two multinomial distributions Since the relevant class contains the query information, h increasing term frequency from 1 to 2 should be larger than that caused by increasing term frequency from 2 to 3. In other words, the score function should be a concave function of term frequency such that score increase rate decreases with term frequency. Therefore, the probabilistic model based on the multinomial distribution violates the concavity constraint of the
Moreover, because the multinomial distribution has the constraint that the summation of the parameters must equal to one, h does not equal to h R w for query terms as well as non query terms (that is terms only occurring in the documents but not in sequently, the multinomial distribution is neither appropriate nor effective for the probabilistic model. Because the multi-nomial distribution assumes the independence of the word repetitive occurrences, it results in a score function which incorporates undesired linearity in term frequency.
 again. The Dirichlet compound multinomial (DCM) distribution (Minka, 2003; Madsen et al., 2005 ), which is motivated by the Polya urn scheme, is able to capture word burstiness, and thus better addresses the need to capture score function con-cavity and document length. 3.2. A detailed description of the model
In the Bayesian hierarchical modeling framework, the Dirichlet distribution is a commonly used conjugate prior distribu-tion for the multinomial distribution. The Dirichlet distribution for the parameters of the multinomial distribution is where h denotes the parameters of the multinomial distribution; b S  X 
Hierarchical Bayesian modeling (HBM) treats the generation of a document in the following way: A sample is drawn from the Dirichlet distribution to generate a multinomial distribution, and then a document is generated by the multinomial dis-tribution. This hierarchical Bayesian model is called the Dirichlet compound multinomial (DCM) distribution ( Minka, 2003;
Madsen et al., 2005; Elkan, 2006 ). In the DCM distribution, the actual parameters are the Dirichlet parameters b the multinomial parameters h w have been integrated out. The DCM distribution is defined below The second line of the above equation is derived by treating log-odds ratio.

Unlike the multinomial distribution, where the parameters are constrained to sum to one, The DCM distribution has one more degree of freedom to account for burstiness: the smaller the b tion. The Bayesian hierarchical modeling perspective does not provide very intuitive insights on how the DCM distribution with one color for each word in the vocabulary, and thus, the generation of a document can be simulated as drawing color balls from the urn. The multinomial distribution models the standard draw with replacement scheme, and the Polya (DCM) distribution models the draw with one additional replacement scheme. Consequently, following the second scheme, words that have already been drawn are more likely to be drawn again, which is word burstiness.

The classical probability ranking principle (Robertson, 1977 ) suggests ranking the documents by the log-odds ratio of their probabilities of being generated by the relevant class against the non-relevant class. Using the DCM distribution (7), we can rank documents by the following score function: where S R  X  P w b R w ; b R w are parameters for the relevant class, and S The first line of the above equation is derived by canceling the common term n  X  d  X  ! = numerator. The second line of the above equation is derived by noticing that C  X  s  X  n  X  = C  X  s  X  X  (8) consists of two components: the first term depends on all the words occurring in the documents and the second term depends on the document length n  X  d  X  . Therefore, the complexity of scoring all the documents in the collection depends cantly reduce the computational complexity, and we will further analyze the computational issue below. evant model and non-relevant model without any relevance feedback information is a challenging problem in implementing a limited amount of text contained in query, smoothing becomes extremely critical. The smoothing method involves a linear combination of the non-relevant model and the query, and is similar to the document smoothing approaches in the LM ( Zhai &amp; Lafferty, 2001b ) this initialization, a word not occurring in the query has the same parameter in the relevant model as the non-relevant mod-el. A mathematically better justified smoothing formulation is weighted average as the Jelinek X  X ercer smoothing method in the final score function of the DCM retrieval algorithm where n  X  q  X  is the length of query q . The above score function consists of two components: The first term depends on the frequency of word co-occurring in the query and the document (that is term frequency); the second term depends on the query length and the document length. The first term of the score function increases with the term frequency, and the score intuition that repeated occurrences of query terms in the document have less impact on the relevance than their first occur-if two documents contain the same number of query terms, the shorter document is more likely to be relevant because it
Now we analyze the computational efficiency of the score function (10). The first term of the score function depends only on ument length. Thus, this model is very efficient to implement for large scale datasets. 3.3. Estimation of the non-relevant model
In this section, we propose three approaches to estimate the non-relevant model from the document collection. The first approach fits a DCM distribution to the collection via the maximum likelihood estimate (MLE). To reduce the computation, the second approach fits an approximated DCM distribution to the collection via the MLE. The third approach fits the DCM distribution to the collection via the leave-one-out (LOO) estimate. 3.3.1. MLE based on the DCM distribution
Applying the maximum likelihood estimator based on the DCM distribution is the most straightforward approach to esti-mate the non-relevant model. We cannot derive a closed-form solution for the maximum likelihood parameter values for the
DCM model. An iterative gradient descent optimization method can be used to estimate the vector by computing the gradi-ent of the DCM log likelihood. The maximum likelihood estimate (Madsen et al., 2005; Minka, 2003 ) can be computed using the fixed point iteration where the digamma function W is defined as W  X  b  X  X  d d b 3.3.2. MLE based on the EDCM distribution
Although the estimation procedure can be done off-line, the above formulation is still very inefficient when the collection size is large. The DCM distribution can be approximated by the EDCM distribution (Elkan, 2006 ) to reduce the computation, when the dimension is very large. The EDCM distribution is defined as Based on the EDCM distribution, we can derive the maximum likelihood estimate by the following steps: where I  X  w 2 d  X  is the indicator function defined in Eq. (1). We can use the fixed point iteration to calculate S the maximum likelihood estimate for the EDCM distribution is more efficient than that of the DCM distribution, because only S is calculated in the fixed point iteration in the MLE of the EDCM distribution, while all the b
DCM distribution. 3.3.3. LOO based on the DCM distribution
An alternative to estimate the non-relevant model is to maximize the leave-one-out (LOO) likelihood ( Zhai &amp; Lafferty, ted. It is defined as the sum of log-likelihood of each word in the observed data computed in terms of a model constructed is probability is derived from above optimization problem 3.4. Estimation of the relevant model
The manual tuning of free parameters to accommodate different retrieval tasks dominates much of the research in infor-mation retrieval. The robustness of the retrieval system significantly improved by automatically estimating the retrieval parameters. The parameter c in the score function Eq. (9) indicates the amount of information from the query in the relevant model. Because c is query dependent, we process the estimation of interpolation parameter c online after the user sends a query. Thus, the computational requirement in estimating the parameter c is more demanding than in estimating the non-relevant model. Here we apply the EDCM distribution given by Eq. (11) to expedite the estimation of the parameter c .In order to estimate the parameter c , we approximate the relevant model space by the set of documents which contain all the terms in the query. Thus, we maximize the log likelihood of the set of documents over parameter c with the EDCM dis-tribution. By plugging Eq. (9) into Eq. (11), we get where A indicates the set of documents containing all the terms in the query. To speed up the computation, we can for the above optimization problem. In such a case, gradient descent approaches provide an alternative avenue for esti-mating parameter c .

This approach to estimate parameter c is similar to the pseudo-relevance feedback algorithm which we will introduce in the next section. We only estimate coefficient c from the set of documents which contain all the query terms here, while the pseudo-relevance feedback algorithm estimates the complete set of parameters for the relevant model from the top ranked documents. The first factor of Eq. (14) decreases with c , and the second factor of Eq. (14) increases with c . The optimal c is a balance between the first factor and the second factor. Documents low down in the retrieved ranked more (or less) contribution to the likelihood function, and thus the balanced value of parameter c is smaller (or larger) accordingly. Therefore, if we decrease the size of relevance set, the estimated parameter c increases. With this heuristic, we approximate the relevance set as all the documents that contain query words to achieve a good balance, which is val-idated in the experiments. 4. Pseudo-relevance feedback
A natural way to estimate the relevant model from pseudo-relevance feedback documents is to assume that the feedback documents are directly generated by the relevant DCM distribution. In addition, the feedback documents also include gen-eral English words besides the words relevant to the search topic. Therefore, a more reasonable model would be a mixture model that generates a feedback document by mixing the relevant topic model with a collection model. We model the feed-back documents as a mixture of two generative model components based on the DCM distributions: F and R . F is the feedback relevant model variable, which represents terms occurring in the feedback documents and pertinent to the user X  X  search in-tent. R is the non-relevant class representing the general English words occurring frequently in the whole collection, which could be estimated by one of the three approaches discussed in Section 2.3 Thus, a document is generated by picking a word either from the feedback relevant model F or the collection model R . The goal of this algorithm is to estimate the feedback relevant model F and use the most frequently occurring terms in F to enrich the original relevant model.
In order to speed up the computation, we employ the approximated DCM distribution in Eq. (11) as the underlying gen-erative source. Thus, the log likelihood function of the feedback document is where F is the feedback documents set. We fix the background collection model R , and apply the EM algorithm to estimate the parameters for the relevant feedback model S F and b F proposed here assumes that each document is generated by multiple topics, and the mixture model proposed by ( Elkan, 2006) assumes that each document is only generated by one topic. Therefore, the derivation of the EM algorithm in Appendix B is different with that in (Elkan, 2006 ).
 documents, even though some feedback documents presumably have more noise than others. Without fixing the mixing coefficients, the EM algorithm will converge to the local optimum corresponding to P  X  z  X  F j d  X  X  1 as explained below. To address this problem, we propose three improvements to the EM algorithm. These are the critical elements which lead to improvement of the algorithmic performance.
 whose parameters are estimated from the whole collection. Since the feedback documents set F contains substantially fewer terms than the whole collection, directly applying the collection model results in occurring in the feedback documents S R Reduced  X  P d 2 F algorithm is very efficient and requires fewer EM iterations than the simple mixture model.

Second, a tempered expectation and maximization (TEM) (Hofmann, 2001 ) procedure allows the EM algorithm to find better 1998), which is also proposed for the EM clustering based on the approximated DCM distribution in ( Elkan, 2006 ). We re-place the original Expectation step with where T is a temperature parameter. In each iteration, we decrease T ! g T until the EM steps converge. The TEM step is cess is somewhat contrary to the spirit of annealing in terms of that we decrease the temperature. Hofmann (2001) demon-strated that this annealing approach not only accelerates the model fitting procedure significantly, but also improves the estimation performance. In (Hofmann, 2001 ), TEM decreases the temperature parameter until the performance on hold-out data deteriorates. Since we estimate model parameters only based on a very small set (usually less than 10) of docu-ments, we do not use the hold-out data approach. In summary, the effect of T is to dampen the posterior probabilities such that they will get closer to the uniform distribution with decreasing T .

Third, the simple mixture model (Zhai &amp; Lafferty, 2001a ) does not involve the original query in any way during the EM query q as a relevant document occurring k times in the M -step of the algorithm. Therefore, the parameter k controls the relative weight of the prior data when we add the original query to the feedback relevant model. The detailed formula for query regularization is listed in the estimation of S how the parameter k influences the retrieval performance. After the algorithm converges, we interpolate the parameters of feedback relevant model b F w with the parameters of non-relevant model b evant model b R w .

The estimated values for parameters b F w are much larger than the interpolation parameter c we used in the baseline DCM model, and overweights the feedback terms in the updated relevant model. Pseudo relevance feedback algorithm uses less leads to larger value of parameter b F w . Therefore, we adjust the estimated maximum value of b c in the baseline retrieval model in step 3 of Table 1 . 5. Experiments 5.1. Experimental datasets and procedures
To evaluate our DCM retrieval algorithm and its pseudo-relevance feedback algorithm described in the previous sections, we experimented with three TREC datasets. The first one is the TREC 2003 HARD track, which uses part of the AQUAINT data-in the TREC 2003 HARD track. Our results are still comparable to other published TREC 2003 HARD results, although the data is a little different. The second one is the TREC 7 dataset, which contains data from the TREC disks 4 and 5 (excludes con-topics, because they are closer to the actual queries used in real applications. Data pre-processing is standard: terms were stemmed using the Porter Stemming and stop words were removed by using standard stop word list. Table 2 shows some document set characteristics and the estimated value of non-relevant class parameter b tribution. Table 2 includes the number of queries (#qry) used on the document set, the average number of relevant docu-ments per query (#rel/qry), the number of documents (#docs), the vocabulary size (#voc), the mean document length (avg(dl)), the standard deviation of document length (dev(dl)), the mean length of query (avg(ql)), the mean value of b which ensures that the algorithm could generalize to a wide range of IR applications.

We employed the Lemur Toolkit as our retrieval system. To measure the performance of the retrieval algorithms, we used three standard ad hoc retrieval measures: (1) mean average precision (MAP), which is calculated as the average of the pre-cision after each relevant document is retrieved and reflects the overall retrieval accuracy. (2) Precision at 10 documents (Pr@10): this measure gives us the precision for the first 10 documents. 5.2. Effectiveness of the DCM retrieval algorithm To evaluate the effectiveness of our new DCM retrieval algorithms, we performed extensive experiments to compare the
DCM retrieval algorithm with other retrieval models which perform well, such as the vector space model, language model and probabilistic model. We compared the performance of our DCM retrieval algorithms with: the Okapi algorithm (OKAPI)
Bayesian predictive LM (BP) (Zaragoza et al., 2003 ), the PRM based on the multinomial distribution (MN) and the two stage and the notations are shown in Table 3 .

In order to obtain a fair comparison, we performed 5-fold cross-validation on the following models: the Okapi BM25 mod-mial distribution (MN), and DCM retrieval algorithm with tuned parameters (DCM-F-T, DCM-A-T, DCM-L-T), and then compared their cross-validation performance (CVP) with the DCM retrieval algorithms with automatic parameter estimation (DCM-F-E, DCM-A-E, DCM-L-E) and the two stage smoothing algorithm (these four algorithms are parameter free, that is, parameters are estimated or optimized, and not tuned by brute force search). We estimate the parameters of the DCM re-trieval model based on the approaches in Section 3.4. For the probabilistic model based on multinomial distribution, we use the collection multinomial model as the non-relevant model, and interpolate the query multinomial model with the col-lection multinomial model to generate the relevant model. We separated 50 queries into five parts, where each part contains age performance on the five test query sets.

In Table 4 , we show the experimental results of these retrieval algorithms and indicate the best performance in bold. The results of language modeling approaches presented here are slightly worse than the published results ( Zhai &amp; Lafferty, the best performing runs with tuned parameters (that is, parameter is tuned by brute force search), but without cross val-
From the results, all our DCM retrieval algorithm variants consistently outperform the existing language modeling algo-rithms: the Dirichlet prior smoothing model (DP), the Bayesian predictive model (BP) and the two stage smoothing model (TS). The probabilistic model based on multinomial distribution performs the worst among all the algorithms, because it ignores the concavity of score function and document length. The performance improvement over the language modeling algorithms is moderate. We will show in the next section that the power of the DCM model and its gains with respect to the language modeling approach would be amplified in pseudo-relevance feedback, where we have some training data. Be-cause of its explicit modeling of the non-relevant class, negative feedback algorithms based on the DCM model outperform those based on language models even more significantly (Xu &amp; Akella, 2008a ).

The DCM retrieval algorithms with LOO estimate (DCM-L-T and DCM-L-E) have the best performance among all the vari-ants of the DCM retrieval algorithm. We observe that S R L eters estimated by the LOO estimator based on the full DCM, the MLE based on the full DCM, and the MLE based on the
EDCM, respectively. We can rank these three estimation approaches in terms of retrieval accuracy from the most accurate to the least accurate: LOO estimate, MLE based on Full DCM distribution, and MLE based on the approximated DCM distri-bution. Because the MLE overfits the data, LOO estimate performs consistently better than the other two approaches. 5.3. Comparison of pseudo-relevance feedback algorithms
In this section, we further compare the DCM pseudo-relevance feedback algorithm (DCM X  X R) with the DCM baseline re-trieval model without any feedback, and the LM pseudo-relevance feedback algorithms, including the simple mixture model mixture model (RMM) (Tao &amp; Zhai, 2006 ).

The simple mixture model and the divergence minimization model are the standard LM based feedback algorithms ( Zhai &amp; Lafferty, 2001a ) with strong performance. The simple mixture model algorithm models the feedback documents as a mix-ture of feedback topic model and background collection model. It uses the EM algorithm to estimate the feedback topic mod-el and interpolates with the original query model. The divergence minimization algorithm models the relevance feedback in an optimization framework, and tends to minimize the divergence between the feedback topic model and the feedback doc-uments, and at the same time maximize the divergence between the feedback topic model and the background collection
Zhai, 2006 ) is the most up to date pseudo-relevance algorithm, which uses query regularization technique and performs an early stopping of the EM iteration.

We use the Dirichlet prior LM with a tuned parameter (DP) as the baseline retrieval model for the simple mixture model (SMM), divergence minimization model (DM) and regularized mixture model (RMM) algorithms. We use the DCM PRM with the leave-one-out estimate and tuned parameter c (DCM-L-T) as the baseline retrieval model for the new pseudo relevance feedback algorithm (DCM X  X R). We trained these algorithms on TREC 2003 HARD dataset, and set parameter values by opti-mizing the performance on TREC 2003 HARD dataset. For the SMM and DM algorithms, the tuning parameters are the weighting parameter k and the interpolation parameter a . We varied both k and a from 0 to 1.0 with step 0.2. For the TREC 2003 HARD dataset, the SMM algorithm performed best when k  X  0 : 8 and a  X  0 : 8; the DM algorithm performed best when k  X  0 : 8 and a  X  0 : 4. The RMM algorithm performed best when setting in ( Tao &amp; Zhai, 2006 ); the DCM-PR algorithm performs best when the query regularization parameter k  X  125 and annealing damping parameter g  X  0 : 96. We fixed these parameter settings for all the remaining datasets, and chose the top 20 terms with the largest probabilities in the feedback relevant model for all these algorithms. We compare these algo-rithms by setting feedback documents size equal to 10 and 30, respectively. The results are shown in Table 5 . The DCM-PR algorithm consistently outperforms the SMM, DM and RMM algorithms on all the three datasets in terms of MAP and Pr@10 with significant improvement. The performance gain comes from two sources: 1. The baseline DCM retrieval model with im-proved performance provides the pseudo relevance feedback algorithm with a better source of training data; 2. The new pseudo relevance feedback algorithm also improves overall performance by directly estimating the DCM distribution from feedback documents, as well as using several model estimation improvement techniques. The benefits of pseudo relevance feedback decreases as we increase the feedback document size from 10 to 30, because more noise is introduced to the model as more documents are used for pseudo-relevance feedback beyond a limit. When the number of feedback document in-creases, the performance improvement becomes less evident. 5.4. Robustness of the parameter k
We study the robustness of query regularization parameter k for the DCM pseudo-relevance feedback algorithm in this section. In the DCM pseudo-relevance feedback algorithm, k indicates the confidence of the original query model. The larger el has a larger impact on the terms in the query if parameter k is larger.

In the previous experiments, we set the k  X  125. We conducted another set of experiments by fixing feedback documents number equal 10 and varying parameter k .In Fig. 1 , we plotted the MAP for several values of k for the DCM pseudo-relevance 50. The performance insensitivity to the parameter k ensures the robustness of the model. 6. Discussion 6.1. Word independence assumption
However, the strong independence assumption does not hold in practice, since the probability of document being relevant given single terms should be elaborated to cover probabilities of document being relevant given various combinations of terms. Accounting for possible combination is computationally intractable. Several attempts (van Rijsbergen, 1977; Robert-is misleading to confuse the term independence assumption and the probability ranking principle. The probability ranking assumption in the statement. Therefore, the DCM model captures the word dependence among repetitive terms, and still satisfies the probability ranking principle when applied in a probabilistic retrieval framework. 6.2. Multinomial distribution under different frameworks
The experimental result in Section 5.2 shows that the performance of multinomial distribution based PRM is very poor, since the resulting score function fails to capture the concave dependency on term frequency and document length. However, the multinomial distribution is a suitable model in the context of naive Bayes classifier, while not very effective in PRM.
The term independence assumption in the naive Bayes classifier results in high degree of bias, which makes it generally unsuitable for accurately approximating the target probability function. Large bias, however, can produce low boundary er-ror, which explains why the naive Bayes method has seen so much success in classification despite its  X  X  X aive X  approach. Na-ive Bayes classifier is able to withstand considerable bias for the savings in variance such a word independence assumption in multinomial distribution entails.

Classification tasks only compare the posterior of log odds of classes for each document individually. Therefore, multino-mial based classifier never compares on documents of different lengths. Ignorance of document length does not hurt the classifier performance much.

By way of contrast with classification, the goal of ad hoc information retrieval is to rank documents, which requires com-parison across documents. Multinomial distribution does not penalize word repetitive occurrence, and thus modeling text as multinomial distribution assigns extreme posterior log odds to long documents. In the probabilistic ranking model based on sure very significantly, as MAP penalizes the top misorderings much more. This observation also holds for other information tolerate the large bias caused by the independence assumption, and would presumably be very poor.

The multinomial distribution also performs well in the LM framework. Since the LM calculates the query likelihood given the document multinomial distribution, the multinomial distribution fails to capture query word burstiness. However, words usually only occur once in the query, and thus the multinomial distribution can still perform effectively in the LM despite not capturing word burstiness in the query. In contrast, the PRM ranks documents by calculating document likeli-hood; therefore, the PRM based on multinomial distribution is not effective in capturing the word burstiness in the documents. 6.3. Relationship to other retrieval models
In this section, we compare the DCM PRM Eq. (10) with the Dirichlet prior LM Eq. (3) introduced in Section 2.2. The Dirich-let prior LM estimates a LM for every document in the collection and assumes a query is generated by the document LM. The
Dirichlet prior LM is based on a query likelihood model, and the DCM PRM is based on a document likelihood model. Besides the difference in the relevance likelihood framework, these approaches differ in their parameter estimation methods. The Dirichlet prior model uses the collection LM to compute the prior distribution parameters, which is the MAP estimate of col-lection based on the multinomial distribution. However, our DCM probabilistic retrieval model estimates the parameters by fitting a DCM distribution to the whole collection, and thus is able to capture the word burstiness in the collection. 7. Conclusions
The main contribution of this paper is a new retrieval algorithm based on the probabilistic model framework and the DCM document modeling and estimation techniques. The probabilistic model framework guarantees the theoretical rigorousness, while the DCM document modeling and estimation techniques lead to efficient retrieval and accurate ranking. We have pro-ture the dependency among repetitive word occurrences. To reduce the effort in the parameter tuning step, we have proposed several estimation approaches that automatically set the parameters of retrieval ranking function. To further im-prove the retrieval accuracy, we have proposed a pseudo-relevance feedback algorithm based on the DCM distribution. We have evaluated the algorithm on various TREC datasets. The experimental results show that our algorithm outperforms the existing LM based algorithms significantly.
 There are three interesting research directions that will help to better understand the role of the DCM retrieval model.
First, it would be promising to apply the DCM retrieval model in the relevance feedback context, where the negative feed-back documents will help the DCM retrieval algorithm to attain additional benefits. Second, the DCM distribution only ac-counts for the burstiness among repetitive terms, and ignores burstiness between different but related terms. A new distributional model which is able to capture the burstiness between different terms would be a promising direction to fur-ther improve the text retrieval accuracy. Third, the DCM retrieval model could be applied to web search engine, since plen-relevant class using DCM probabilistic model enables to effectively utilize the non-query information in the context of web search engine. In web search problems, query expansion achieved by using relevant external web information has proved retrieval, to the broader web search context.
 Acknowledgments
We acknowledge support from Cisco, and the University of California X  X  MICRO program. We also appreciate suggestions from Charles Elkan, Tom Minka and anonymous reviewers.
 Appendix A. Proof of P  X  w j z  X  X  b z w = S z There are several places we use P  X  w j z  X  X  b z w = S z , where z is a DCM distribution, here we prove this equation. where i is a dummy variable. The second line of the above equation is derived by treating version of the Dirichlet distribution with parameters b z Appendix B. Expectation and maximization algorithm for the mixture of approximated DCM distribution
The log likelihood function of the feedback documents is
We resort to the expectation and maximization ( EM ) algorithm (Dempster, Laird, &amp; Rubin, 1977 ) to maximize the above mixing variables, based on the current estimates of the parameters and (ii) a maximization ( M ) step, where parameters are updated based on the so-called expected complete data log-likelihood which depends on the posterior probabilities com-puted in the E-step.
 For the E -step one simply applies Bayes X  formula to obtain
In the M-step one has to maximize the expected complete data log-likelihood E
One of the key problems with maximizing the above likelihood function is that it involves the logarithm of a (big) sum, sen X  X  inequality. We defined the likelihood function as:
In order to take care of the normalization constraints. Eq. (B) is augmented by appropriate Lagrange multipliers s
To maximize H , we take derivative of H with respect to b where w  X  x  X  is the digamma function, and defined as w  X  x  X  X  d
We substitute s z into Eq. (B.2) to obtain We substitute s z into Eq. (B.3) to obtain parameter
We sum over z in Eq. (B.4) to obtain q d , and substitute q In order to speed up the computation, we approximate the above equation as References
