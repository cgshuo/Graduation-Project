 Content-based recommendation systems can provide recom-mendations for  X  X old-start X  items for which little or no train-ing data is available, but typically have lower accuracy than collaborative filtering systems. Conversely, collaborative fil-tering techniques often provide accurate recommendations, but fail on cold start items. Hybrid schemes attempt to combine these different kinds of information to yield better recommendations across the board.

We describe unified Boltzmann machines, which are prob-abilistic models that combine collaborative and content in-formation in a coherent manne r. They encode collaborative and content information as features, and then learn weights that reflect how well each feature predicts user actions. In doing so, information of different types is automatically wei-ghted, without the need for careful engineering of features or for post-hoc hybridization of distinct recommender systems.
We present empirical results in the movie and shopping domains showing that unified Boltzmann machines can be used to combine content and collaborative information to yield results that are competitive with collaborative tech-nique in recommending items that have been seen before, and also effective at recommending cold-start items. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information filtering ;G.3[ Proba-bility and Statistics ]: correlation and regression analysis ; I.2.6 [ Artificial Intelligence ]: Learning X  parameter learn-ing Algorithms, Performance recommender systems, collaborative filtering, content-based filtering, cold start, Boltzmann machines
Recommender systems suggest items of interest to users based on available information such as previous usage pat-terns, the usage patterns of other users, and features of the items themselves [20]. Collaborative filtering techniques pro-vide recommendations to a user by using the preferences of other users that have similar preferences to him [5, 17, 28]. For example, the familiar Amazon item-to-item system [17] recommends items for a user viewing a current item on the basis of other items purchased by other users that have viewed the current item. Such systems have the drawback that they suffer from the item cold start problem X  X n item cannot be recommended until it has been rated number of existing users.

This problem can be alleviated by recommender systems that use information about the content of items. This in-formation can be meta-data about the item such as actors appearing in movies for movie recommendation or informa-tion derived from the item such as counts of words in docu-ments in the case of document recommendation [21]. Unfor-tunately, purely content-based approaches often do not per-form as well as collaborative filtering approaches when cold-start is not an issue [8]. Hybrid systems [6] that combine collaborative and content information are therefore used.
We extend our previous work on tied Boltzmann machines [10], yielding a model that can naturally combine content and collaborative features, which we term unified Boltzmann machines . Both are improvements upon Boltzmann ma-chines, which model the joint distribution of a set of binary variables through their pairwise interactions. In our con-text, the binary variables indicate whether or not a user has acted on each item of interest. Thus, we use Boltzmann machines to learn weights that reflect the importance of dif-ferent pairwise interactions such as  X  X eople who buy item A also buy item B X  in explaining usage data. Tied Boltzmann machines are variants of Boltzmann machines that explain such usage data using content features of the form  X  X eople who buy one dairy item also buy other dairy items. X  Because they do not use features that explicitly model interactions between individual items, they provide the same predictions for items that share the same content information, poten-tially reducing accuracy. Unified Boltzmann machines make use of both kinds of information. They learn weights that reflect how much each feature contributes in explaining us-age data, and in doing so they automatically learn to bal-ance and combine the different information sources to make better predictions. We present empirical results comparing the performance of untied, tied, and unified Boltzmann ma-chines in both cold start and non-cold-start scenarios in the MovieLens and Ta-Feng shopping [13] data sets. We show that the unified Boltzmann machine provides a single unified approach that naturally combines content and collaborative features to yield competitive results in both cold-start and non-cold-start scenarios.
In this paper, we examine the problem of predicting the user X  X  future actions on items given the user X  X  past actions on items. We will restrict attention to binary actions such as listening to a song, watching a movie, or buying a book. Extending our models to more general actions such as rating a movie with an integer between 1 and 5 or spending $3.99 on rice is left for future work.

Our goal is to predict the probability that a user will act on an item i given whether or not he has acted on all other items. In other words, we wish to obtain a good estimate of p ( a i | a  X  i )where a is a column vector of binary variables a that specify whether or not a user has acted on an item i ,and a  X  i is shorthand for { a i | i = i } .Welearnmodels of the interactions between actions on items from the action vectors a ( u ) of a population of users u =1 ,  X  X  X  ,M .For example, the models will learn that people who buy hot dogs may also buy hot dog buns, or that people who watch the movie Sleepless In Seattle may also watch the movie You X  X e Got Mail . Given such models, as yet unused items i can be recommended if they have high p ( a ( u ) i | a ( u )  X  i
Instead of estimating separate conditional distributions p ( of a user X  X  action vector a by a Boltzmann machine: The parameter vector  X  has components  X  i and  X  ij corre-sponding to all items i and item pairs ( i, j ). We use denote the sum over unordered pairs ( i, j ). Since  X  ij is only defined for i&lt;j , we abuse notation somewhat and use  X  and  X  ji interchangeably to denote the weight associated with the unordered pair ( i, j ) without introducing any ambiguity. The partition function z (  X  ) is a normalizer that ensures that p ( a ;  X  ) is properly normalized over all configurations of the action vector a . The pairwise weights  X  ij capture pairwise collaborative effects X  X  high value of  X  ij indicates that users with a i = 1 also tend to have a j = 1. The per-item weights  X  capture popularity effects X  X  high value of  X  i indicates a higher likelihood of a i = 1 irrespective of the user X  X  other actions
Example 1. Let us restrict attention to an inventory of three movies: Sleepless in Seattle , You X  X e Got Mail ,and When Harry Met Sally .Thevariables a SS , a YGM and a WHMS will represent whether the user watched each of these movies respectively. The Boltzmann machine relating these vari-ables will have a parameter vector  X  consisting of 3 unary weights  X  SS ,  X  X  X  ,  X  WHMS and 3 pairwise weights  X  SS,Y GM  X  X  X  ,  X  YGM,WHMS .Ahighvalueof  X  SS would indicate that a
SS tends to be 1 X  X .e. that many users watch Sleepless in Seattle .Ahighvalueof  X  SS,Y GM would indicate that a SS and a YGM are correlated X  X .e. that users who watch Sleepless in Seattle also tend to watch You X  X e Got Mail .
Note that the partition function z (  X  )isexpensivetocom-pute [15]. It requires summing over all 2 N configurations of the action vector. However, our desired conditional proba-bility of a user acting on a single item given his actions on all other items is easy to compute, and is given by This conditional distribution can be seen to be a logistic regression of a i on a  X  i . Because the logistic regressions for predicting each a i come from the single joint model given by the Boltzmann machine (1), the weight for regressing a on a j and the weight for regressing a j on a i are constrained to be equal for every item pair ( i, j ).
One weakness of the model discussed in section 2.1 is that reliable estimation of the model parameters  X  ,inparticu-lar the pairwise weights  X  ij , requires sufficiently many ob-servations. Since real data tends to be sparse, with many items having low probability of occurrence, pairwise weights involving these items are difficult to estimate in practice. Thus, the Boltzmann machine model described above may not make good predictions for such items. In the extreme case where an item is not seen during training, we have the item cold start problem, where none of the weights associ-ated with that item can be reliably estimated.

We alleviate this difficulty through the use of tied Boltz-mann machines. Tied Boltzmann machines are Boltzmann machines where the parameters have been tied so that they are no longer estimated independently. When the parame-ters are tied, the data used to estimated them can be pooled, allowing more reliable estimation.

In order to retain the ability to model the interactions between items, we use content information to guide the pa-rameter tying. We assume that the content associated with item i is represented by a feature vector f ( i )  X  R D com-posed of D components. For example, features could be counts or TF-IDF weights of words in documents, or binary flags indicating whether specific actors appeared in a movie. Features with different semantics could be combined in a single vector. For example, some feature components could correspond to actors in a movie, while others could corre-spond to genres, while still others could take on numerical values such as movie length in minutes. We constrain  X  to satisfy where  X   X  R D and  X   X  R D  X  D is symmetric. In this pa-per, we will only consider diagonal  X  , in order to reduce the number of parameters that need be estimated, and use  X  k to denote the k th diagonal component of  X  .

Example 2. Let us tie the parameters of Example 1 using a 2-dimensional feature vector with the components f MR and f TH corresponding to Meg Ryan and Tom Hanks ap-pearing in a movie. The new parameters that correspond to this case are  X  MR , X  TH which reflect how much Meg Ryan and Tom Hanks contribute to the popularity of a movie, and  X  MR , X  TH which reflect how much having Meg Ryan and Tom Hanks respectively as a common actor contribute to a pair of movies both being watched by the same user. Note that the number of parameters has been reduced from 6 to 4. In addition, note that because Sleepless in Seattle and You X  X e Got Mail both have Tom Hanks and Meg Ryan in the cast, we have  X  SS =  X  YGM =  X  MR +  X  TH .These weights have been tied together by the content information. pair Sleepless in Seattle and When Harry Met Sally and the pair You X  X e Got Mail and When Harry Met Sally both share the single common actor Meg Ryan. If we had modeled off-diagonal terms in  X  , we would also have had parameters such as  X  MR,TH which models how much watching movies starring Meg Ryan makes a user more likely to also watch movies starring Tom Hanks.

In effect, we have reparametrized the Boltzmann machine, so that the joint distribution of equation (1) is now para-metrized through through  X  and  X  : p ( a ;  X ,  X  )= 1 where z (  X ,  X  )= z (  X  (  X ,  X  )).

The conditional probability of a user acting on item i given her actions a  X  i on all other items can be efficiently computed as p ( a i =1 | a  X  i ;  X ,  X  )= In contrast to equation (2), this conditional distribution is a logistic regression of a i on the features of the item i and the common features of item i and other items j with a j =1, instead of on i and other such items j directly.
One drawback of the parameter tying scheme described above is that the approximate estimates of equation (4) are used even if good untied estimates of  X  ij and  X  i can be made from the training data. Thus, the use of tied Boltzmann ma-chines can lead to loss of expressive power. In addition, it is possible for the estimates of  X  k to be heavily influenced by a few common item pairs ( i, j )withnon-zero f ( i ) k f ( j ) ing the resulting tied estimate of  X  ij to be not as applicable to other item pairs. Similarly the estimates of  X  k can be dominated by a few common items so the the resulting tied estimates of  X  i for other items may not be as applicable to other items.

One solution to these problems is to use a unified Boltz-mann machine with the reparametrization where the new parameters  X  i and  X  ij explicitly model the residuals in the tied estimates of  X  i and  X  ij . The residual parameters will be estimated when they can be estimated reliably, and be kept close to zero when they cannot, via the use of regularization as described below in section 2.4. Thus, the tied estimates are corrected when reliable estimates of  X  and  X  ij can be obtained, and conversely, the influence of common items or item-pairs in estimating  X  or  X  is corrected by the residual terms.

Example 3. We now unify the parametrizations of exam-ples 1 and 2. We will now use the 4 content parameters  X 
MR ,  X  TH ,  X  MR and  X  TH , 3 unary item parameters  X  SS ,  X  X  X  ,  X  WHMS and 3 pairwise item parameters  X  SS,Y GM ,  X  X  X   X  and In contrast to example 2 the existence of the parameters  X  means that the weights  X  are no longer constrained. How-ever, they still make use of content information.
The joint distribution of a user X  X  actions is now given by p ( a ;  X ,  X ,  X  )= 1
As in the tied and untied cases, the partition function z (  X ,  X ,  X  ) requires exponential computation, but the condi-tional probability of a user acting on item i given her actions a  X  i on all other items can be efficiently computed as p ( a i =1 | a  X  i ;  X ,  X  )= exp 1+exp This conditional probability is also a logistic regression, this time on both the items and their features.

We note in passing that predictions can easily be condi-tioned on other kinds of information by adding features that depend on that information to this model. For example, if user age groups are available in the movie domain, features such as  X  X ser is under 10 and the genre is cartoons X  can be used.

For convenience, we will denote the joint distribution given by the untied, tied, and unified Boltzmann machines given in equations (1), (5) and (9) as p ( a ;  X  ) and the conditional distributions of equations (2), (6) and (10) as p ( a i | a where the parameter vector  X  contains components corre-sponding to  X  i or  X  i ,  X  ij or  X  ij ,and  X  k , X  k as appropriate. The models are trained from a training set consisting of M  X  N binary variables a ( u ) i for all N items i  X  1 ,  X  X  X  ,N and all M users u  X  1 ,  X  X  X  ,M , which specifies whether or not each user acted on each item. As discussed above, all items a user has not yet read, viewed, bought, etc., depend-ing on the application, will have a ( u ) i = 0. In addition, tied and unified Boltzmann machines will use a content database which specifies the feature vectors f ( i ) for all items. Model-ing user action vectors as being i.i.d. 1 , the log likelihood of the training set is given by This log likelihood is hard to optimize because it requires the evaluation of the partition function z (  X  ) for each candidate value of  X  .

Because of this, we approximate the log likelihood of the training set by its log pseudo-likelihood [3, 14]: As discussed above, the conditional likelihood p ( a i | a easily computed for all three models. If we treat each user u  X  X  action on each item i as an independent training event given u  X  X  other actions, the pseudo-likelihood can be inter-preted as the conditional likelihood of the training set under a conditional model given by equation (2), (6), or (10). We note that the pseudo-likelihood is convex in the parameters, and therefore easily optimized via gradient methods.
In order to get robust estimates, we train the parameters  X  by maximizing the regularized pseudo-likelihood The regularization parameter  X  prevents parameters from growing (and therefore having a large influence on the model) without significantly improving the likelihood of the data. This can be viewed as MAP estimation of the set of tied logistic regressions mentioned above under a Gaussian prior [7]. Typically, when  X  is high, parameters are not allowed to grow unless they better explain a large portion of the training data.
With large data sets, optimizing the training criterion of equation (13) may be too expensive since it requires com-puting a conditional likelihood for each of M  X  N user-item pairs, where M and N may both be large. Since the set of items for which a ( u ) i = 1 is sparse, a further speedup is possible. We will denote this sparse set of positive training items by I ( u ) + and its complement by I ( u )  X  .Asampledset of negative items I ( u )  X  ( p s ) will be chosen by sampling items from I ( u )  X  with probability p s [18]. We choose p s so that the total number of sampled negative items the order of the total number of positive items
If user information such as demographics is available, we would model user action vectors as being i.i.d. given the user information The regularized pseudo-likelihood of equation (13) is then approximated through stratified sampling by L (  X ,  X  )  X  Now, evaluating the training cri terion only requires comput-ing the conditional likelihood for O ( A )items,where A is the total number of actions observed. Of course, the speedup de-pends on how sparse actions are, and the loss in accuracy is data dependent. This approximate training criterion, like the exact criterion of equation (13), can be optimized using gradient methods.

In our experiments, we optimized the training criterion using the RPROP [25] algorithm. This is a non-linear gradi-ent technique that maintains an adaptive step size for each dimension. The step size during optimization was initial-ized to the inverse dimensionality of the parameter vector, the step growth and shrinkage parameters of the RPROP algorithm were set to 1 . 2and0 . 5, while backtracking was enabled. The algorithm was run for at most 500 iterations. The regularization parameter  X  was chosen by optimizing likelihood on a held-out set, but we observed that the re-sults were not very sensitive to this value. The sampling parameter p s was set to | A | MN  X  X  A | . We note that past work suggested that sampled training results in little accuracy loss [10]. Finally, in order to control memory utilization, we use count cutoffs as follows. First, we count the number of users for whom each item, item-pair, and content vector component f ( i ) j occur in the training set. We then select the ones that occur only a small number of times and drop the corresponding parameters from the model.
We evaluated our models on two data sets X  X he  X 1,000,000 rating X  MovieLens movie rating corpus (www.movielens.org) and the Ta-Feng supermarket corpus (aiia.iis.sinica.edu.tw) [13]. We evaluated the models using a  X  X andom X  and  X  X old-start X  protocol on each of these data sets, as described below.
This data set contains 1,000,209 ratings of 3,706 movies by 6,040 users. Not all movies are rated by all users. Movies are tagged with 18 genres. A movie can belong to more than one genre. The MovieLens movie description pages were crawled to obtain the headlining actors of each of these movies. This yielded 8406 actors. The task in experiments on this data was to predict which movies users rated given other movies they rated. More formally, we set a ( u ) i =1ifuser u had rated movie i . For tied and unified Boltzmann machines, we used binary feature vectors f ( i ) with components corresponding to actors and genres. We present results comparing the use of genre only, actors only, and both genre and actors.
For the  X  X andom X  protocol, we randomly selected 800,167 (80%) of the ratings to form training partition, with the re-maining 200,042 forming a test partition. We then selected the test ratings of 500 randomly selected users, yielding a test set of 3,706  X  500 = 1,853,000 possible recommendations, 16,310 of which corresponded to actual ratings. For the  X  X old-start X  protocol, we randomly drew 2,962 (80%) of the movies to be training movies and remaining 744 to be test movies. We randomly selected 500 users to evaluate rec-ommendations on, yielding a test set of 744  X  500 = 372,000 possible recommendations, 16,717 of which corresponded to actual ratings. The training set consisted only of 816,527 ratings for the 2,962 training movies. Since users tend to rate movies they have watched, we believe that the results of these experiments are suggestive of how well our models would predict movie watching.
This is a data set containing the purchase records of 32,267 supermarket customers. A total inventory of 23,812 distinct items were purchased. We ignored repeat purchases, yield-ing 743,228 total items bought. Each item is categorized ac-cording to a three-level hierarchy with 26 coarse categories, 201 medium categories and 2,012 fine categories. Each item belongs to exactly one coarse, medium, and fine category each. We set a ( u ) i =1ifuser u had purchased item i .For tied and unified Boltzmann machines, we used binary fea-ture vectors f ( i ) with components corresponding to coarse, medium, and fine categories. We present results comparing the use of coarse categories only, coarse and medium cate-gories, and all categories.

For the  X  X andom X  protocol, we randomly selected 594,582 (80%) of the purchases to train on. From the remaining 148,646 purchases, we then selected those of 500 randomly selected users, yielding a test set of 23,812  X  500 = 11,906,000 possible recommendations, only 2,621 of which corresponded to actual purchases. For the  X  X old-start X  protocol, we ran-domly selected 19,050 (80%) of the items to be training items, leaving 4,762 test items. We randomly selected 500 users to evaluate recommendations on, yielding a test set of 4,762  X  500 = 2,381,000 possible recommendations, 2,915 of which corresponded to actual ratings. The training set con-sisted of 590,474 purchase records for the 19,050 training items.
We compare our models to three simple baselines. When using the  X  X andom X  protocol, we compared to Pearson cor-relation [24, 5] and to item-item recommendation [17]. For Pearson correlation based-recommendations, we used user neighborhoods of size 25, which was found to work well across a number of domains.

In the case of item-item recommendation, we used count-cutoffs to keep the item-item co-occurrence matrix sparse for memory efficiency. The count cut-offs were chosen to ensure that the item-item systems and untied Boltzmann machine systems had the same memory utilization. In order to gen-erate item-item recommendations given a set of items, we computed the item-item similarity of each potential recom-mendation to each given item. We then used the maximum similarity over the given items for ranking potential recom-mendations.
 In the case of the  X  X old-start X  protocol we compare to the Naive Bayes approach of [21]. However, since we have binary content information rather than free text, we use a multi-variate Bernoulli event model instead of a multinomial event model [19]. We investigated the use of aspect models [30] as a baseline, but they failed to improve upon the Naive Bayes approach in preliminary experiments, despite a much higher training cost.
We compare the performance of our algorithms using pre-cision of a fixed length list of recommendations. To compute this metric for an algorithm, we first rank items given the training items used by a test user using the algorithm. We then exclude the training items used by the user from the ranked items, and define the per-user precision at K recom-mendations to be the proportion of the top K recommenda-tions that were actually used by the user in the test set. We average the resulting per-user precisions over all test users to get an average precision at K recommendations. The results we obtained were consistent for K = 5, 10, 25, and 50.
Figure 1 shows the average precision at 5 recommenda-tions when evaluating with the  X  X andom X  protocol on the MovieLens and Ta-Feng data sets. Among the purely collab-orative algorithms, untied Boltzmann machines outperform both item-item and Pearson correlation on both data sets. Note that the performance of these algorithms does not de-pend on the content information used since they ignore it. Unified Boltzmann machines match or beat untied Boltz-mann machines in both domains, and gives the best per-formance across the board. The purely content based Naive Bayes approach has the worst performance across the board. The performance of tied Boltzmann machines improve with the amount of content information provided. The results obtained at 10, 25, and 50 recommendations was consistent with the results at 5 recommendations.

Figure 2 shows the precision at 5 recommendations when evaluating on the  X  X old-start X  protocol. Item-item, Pear-son, and untied Boltzmann machines perform no better than chance in this scenario. Note that in cold-start tests, it is not possible to use simple baselines such as recommending the most popular items, since the popularity of a new item is not yet known. Once again, unified Boltzmann machines are competitive across the board. The results obtained at 10, 25, and 50 recommendations was consistent with the results at 5 recommendations.

We note that most cases, unified Boltzmann machines benefit from additional content information, although the addition of the finest-grained category information does cause a small degradation in the Ta-Feng data when using the  X  X andom X  protocol. With the MovieLens data, using both genre and actor information always outperforms using only actor information or only genre information.

Surprisingly, unified Boltzmann machines usually outper-form tied Boltzmann machines even under the  X  X old-start X  protocol, especially when the content information becomes richer, even though the item-item weights are never used during test time. We speculate that the item-item weights learn biases in the training data, allowing the content weights to generalize better to tests sets where item frequencies are different from the training set. However, verifying this con-jecture is left for future work.
While our untied Boltzmann machines are pure collabo-rative recommenders, tied and unified Boltzmann machines are hybrid recommenders. Tied Boltzmann machines com-bine content and collaborative information is a somewhat subtle way X  X ven though the model uses only content-based features, they are trained using collaborative data. In the terminology of Burke [6], unified Boltzmann machines are examples of  X  X eature combination. X  Other approaches in-clude combining the results of different recommenders by weighting the resulting scores [8] or voting [23], switching between different recommenders [4], and filtering or rerank-ing the results of one recommender with another [6]. These approaches all require building separate recommender sys-tems using techniques that are specialized to each kind of information used, and then combining the outputs of these systems. Feature combination schemes such as ours are at-tractive because they allow a single unified technique to be used regardless of the types of information used. However, Burke [6] points out that previous attempts at hybridization through feature combination, such as rule induction from collaborative and content data [2], require careful engineer-ing of the features used. Our approach learns weights which reflect features are most predictive, and uses regularization to ensure that the model generalizes to unseen data, so that such engineering of the features is unnecessary.

The use of aspect models [30] has been proposed as an approach to recommendation that is robust in the face of the item cold-start problem. Extending this approach to use many types of meta-data (e.g. actors, genres, and directors for movies) was not reported to lead to improvements [29]. The Naive Bayes approach of Mooney [21] explicitly uses meta-data of different kinds in making recommendations, but does not take collaborative data into account. In our experiments, the Naive Bayes approach outperformed user-actor aspect models.

The Hybrid Poisson-Aspect Model [13] approach combines a user-item aspect model with a content-based user cluster model. Restricted Boltzmann machines [27] and dyadic data models [1] are also latent variable models, in the same spirit as soft clustering methods such as user-item aspect models [12] and matrix factorizations such as SVD, and do not at-tempt to directly model pairwise interactions between items. Our approach could be extended to use latent variables that model user clusters. Approaches such as restricted Boltz-mann machines [26], dyadic models [1], and online SVD [16] are typically used with ratings data where most ratings are missing, and take advantage of this fact for computational tractability. In our setting, where we have an observed bi-nary value for each user and ite m, and where there are large number of users and ratings, it may be intractable to use these approaches directly.

There has been some recent work on using filter-bots [9] for hybrid recommendations [22]. This work attempts to use bots implementing various heuristics, some based on content information, to generate synthetic data for training recom-mender systems on. Our approach learns joint models of user actions using collaborative and other information. Our models could be used as more principled  X  X ilter-bots X  that generate synthetic data by sampling from the joint distribu-tions.

Finally, we note that tied and unified Boltzmann machines can also be viewed as dependency networks [11] where the conditional probabilities are logistic regressions instead of decision trees, and where the parameters are tied across nodes. It is this parameter tying that distinguishes tied and unified Boltzmann machines from dependency networks, and that improves generalization to rarely seen items.
We have presented unified Boltzmann machines, an im-provement over tied Boltzmann machines that allow the use of both content-based and item-specific features that allows better recommendations of both cold-start and non-cold-start items. We showed that unified Boltzmann ma-chines outperform simple collaborative filtering techniques such as item-item recommendation and Pearson correlation in recommending non-cold-start items while being compet-itive with content-based recommendation in recommending cold-start items. Thus, unified Boltzmann machines provide a convenient and natural approach for combining collabora-tive information with collaborative information, without the need for special-purpose hybridizing techniques.

Boltzmann machines explicitly model the interactions be-tween pairs of items. The use of Boltzmann machines to model the set of items acted on by a user assumes that any higher order interactions between these items can be explained in terms of pairwise effects. For example, while the model can learn that hot dogs and hot dog buns are bought together, it cannot explicitly learn that users typi-cally buy hot dogs with buns of brand A or brand B, but usually not both. However, it is hoped that this higher order interaction can be learned implicitly by learning the three binary interactions  X  X ot dogs are often bought with brand A buns, X   X  X ot dogs are often bought with brand B buns, X  and  X  X rand A buns and brand B buns are seldom bought to-gether. X  We note that while item-to-item recommendations also make this implicit assumption, user-based approaches such as Pearson correlation do not. The good performance of untied Boltzmann machines and item-to-item recommenda-tion over Pearson correlation suggests that this assumption is not too harmful to make.

A number of directions for further inquiry remain. The models we have presented only predict binary user actions. The models need to be extended to predict non-binary user actions, such as numeric ratings of movies. Our models do not explicitly take temporal effects into account, even though these effects can be important in some domains [31]. Extending our models to take such effects into account is left for future work.

We have so far only investigated the combination of con-tent and collaborative information. After training on this kind of information, the models provide an estimate that a previously unseen user will use a particular item, since the marginal probability that an item will be used is well-defined. However, this estimate is not likely to be very in-formative. Our models can easily be conditioned on other information such as demographics or user context in order to provide better recommendations for users whom we have little or no history. How well the models perform when com-bining this kind of information with content and collabora-tive information remains to be seen.

The ability of the models to learn to use different kinds of features while generalizing to new data depends on care-ful regularization of the parameters. We have so far used a simple squared length regularization of the parameters. The fact that addition of the finest-level category information caused a small degradation in the  X  X andom X  protocol exper-iments on the Ta-Feng data indicates that there is room for improvement. Investigation of more sophisticated regular-ization schemes remains to be done.

In the work described here, we have used maximum pseudo-likelihood estimation combined with stratified sampling to efficiently train our models. Investigating the use of re-cent advances in variational and Monte Carlo techniques for training Boltzmann machines [26] is another direction for future research.
The authors wish to thank Guy Shani for useful discus-sions, and Galen Andrews for model training software. [1] D. Agarwal and S. Merugu. Predictive discrete latent [2] C. Basu, H. Hirsh, and W. Cohen. Recommendation [3] J. Besag. Statistical analysis of non-lattice data. The [4] D. Billsus and M. J. Pazzani. User modeling for [5] J.S.Breese,D.Heckerman,andC.Kadie.Empirical [6] R. D. Burke. Hybrid recommender systems: Survey [7] S. F. Chen and R. Rosenfeld. A Gaussian prior for [8] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, [9] N.Good,J.B.Schafer,J.A.Konstan,A.Borchers, [10] A. Gunawardana and C. Meek. Tied Boltzmann [11] D. Heckerman, D. M. Chickering, C. Meek, [12] T. Hofmann and J. Puzicha. Latent class models for [13] C.-N. Hsu, H.-H. Chung, and H.-S. Huang. Mining [14] A. Hyv  X  arinen. Consistency of pseudolikelihood [15] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [16] M. Kurucz, A. A. Bencz  X  ur, and K. Csalog  X  any. [17] G. Linden, B. Smith, and J. York. Amazon.com [18] R. F. Lyon and L. S. Yaeger. On-line hand-printing [19] A. McCallum and K. Nigam. A comparison of event [20] M. Montaner, B. L  X  opez, and J. L. de la Rosa. A [21] R. J. Mooney and L. Roy. Content-based book [22] S.-T. Park, D. Pennock, O. Madani, N. Good, and [23] M. J. Pazzani. A framework for collaborative, [24] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [25] M. Riedmiller and H. Braun. A direct adaptive [26] R. Salakhutdinov. Learning and evaluating boltzmann [27] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [28] B. Sarwar, G. K. nad Joseph Konstan, and J. Reidl. [29] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [30] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [31] A. Zimdars, D. M. Chickering, and C. Meek. Using
