 Steven C.H. Hoi chhoi@ntu.edu.sg Rong Jin rongjin@cse.msu.edu Kernel methods have attracted more and more atten-tion of researchers in computer science and engineering due to their superior performance in data clustering, classification, and dimensionality reduction (Scholkopf &amp; Smola, 2002; Vapnik, 1998). Kernel methods have been applied to many fields, such as data mining, pat-tern recognition, information retrieval, computer vi-sion, and bioinformatics, etc. Since the choice of ker-nel functions or matrices is often critical to the per-formance of many kernel-based learning techniques, it becomes a more and more important research prob-lem for how to automatically learn a kernel func-tion/matrix for a given dataset. Recently, a number of kernel learning algorithms (Chapelle et al., 2003; Cris-tianini et al., 2002; Hoi et al., 2007; Kondor &amp; Laf-ferty, 2002; Kulis et al., 2006; Lanckriet et al., 2004; Zhu et al., 2005) have been proposed to learn kernel functions or matrices from side information. The side information can be provided in two different forms: ei-ther labeled examples or pairwise constraints. In the latter case, two types of pairwise constraints are exam-ined in the previous studies: a must-link pair where two examples should belong to the same class, and a cannot-link pair where two examples should belong to different classes. In this study, we focus on kernel learning with pairwise constraints.
 Most kernel learning methods, termed as  X  X assive ker-nel learning X , assume that labeled data is provided beforehand. However, given the labeled data may be expensive to acquire, it is more cost effective if we are able to identify the most informative example pairs such that the kernel can be learned efficiently with only a small number of pairwise constraints. To this end, we focus on active kernel learning (AKL) whose goal is to identify the example pairs that are informative to the target kernels. We extends our previous work on non-parametric kernel learning (Hoi et al., 2007) to active kernel learning. As shown in (Hoi et al., 2007), the parametric approaches for kernel learning are of-ten limited by their capacity in fitting diverse patterns of real-world data, and therefore are not as effective as the non-parametric approach for kernel learning. The simplest approach toward active kernel learning is to measure the informativeness of an example pair by its kernel similarity. Given a pair of examples ( x i , x we assume that K i,j , the kernel similarity between x i and x j , is a large positive number when x i and x j are in the same class, and a large negative number when they are in different classes. Thus, by follow-ing the uncertainty principle of active learning (Tong &amp; Koller, 2000; Hoi et al., 2006), the most informa-tive example pairs should be the ones whose kernel similarities are closest to zero. In other words, the criterion is to select the e xample pair with the least absolute kernel similarity (i.e., | K i,j | ). Unfortunately, this simple approach may not always be effective in obtaining informative pairwise constraints for kernel learning. Figure 1 illustrates an example of active kernel learning for data clustering. In this example, Figure 1(a) shows an artificial dataset of two classes together with a few pairwise constraints. Figure 1(b) shows the pairwise constraints with the least | K i,j | . We observe that most of them are must-link pairs with two data points separated by a modest distance. Since must-link constraints are not informative to the clus-tering boundary, a relatively small improvement is ob-served in clustering accuracy (from 51% to 58%) when using the kernel learned by this simple approach. In contrast, as shown in Figure 1(c), the proposed ap-proach for active kernel learning is able to identify a pool of diverse pairwise constraints, including both must-links and cannot-links. The clustering accuracy is increased significantly, from 51% to 86%, by using the proposed active kernel learning.
 The rest of this paper is organized as follows. Sec-tion 2 presents the min-max framework for our active kernel learning method, in which the problem is formu-lated into a convex optimization problem. Section 3 describes the results of the experimental evaluation. Section 4 concludes this work. Our work extends the previous work on non-parametric kernel learning (Hoi et al., 2007) by in-troducing the component of actively identifying the example pairs that are most informative to the target kernel. In this section, we will first briefly review the non-parametric approach for kernel learning in (Hoi et al., 2007), followed by the description of the min-max framework for active kernel learning. 2.1. Non-parametric Kernel Learning Let the entire data collection be denoted by U = ( x 1 , x 2 ,..., x N ) where each data point x i  X  R d is a vector of d elements. Let S  X  R N  X  N be a symmetric matrix where each S i,j  X  0 represents the similarity between x i and x j . Unlike the kernel similarity ma-trix, S does not have to be positive semi-definite. For the convenience of presentation, we set S i,i =0forall the examples. Then, according to (Hoi et al., 2007), a normalized graph Laplacian L is constructed using the similarity matrix S as follows: where D =diag( d 1 ,d 2 ,...,d N ) is a diagonal matrix with d i = N j =1 f ( x i , x j ). A small  X &gt; 0 is introduced to prevent L from being singular. Let X  X  denote by T the set of pairwise constraints. We construct a matrix T  X  R N  X  N to represent the pairwise constraints in T , i.e., T Given the similarity matrix S and the pairwise con-straints in T , the goal of kernel learning is to identify akernelmatrix Z  X  R N  X  N that is consistent with both T and S . Following (Hoi et al., 2007), we formulate it into the following convex optimization problem: The first term in the above objective function plays a similar role as the manifold regularization (Belkin &amp; andd P. Niyogi, 2004), where the graph Laplacian is used to regularize the classification results. The second term in the above measures the inconsistency between the learned kernel matrix Z and the given pairwise constraints. Note that unlike the formulation in (Hoi et al., 2007), we change  X  i,j in the loss function to  X  2 This modification is specifically designed for active ker-nel learning, and the reason will be clear later. It is not difficult to see that the problem in (1) is a semi-definite programming problem, and therefore can be solved by the standard software package, such as Se-DuMi (Sturm, 1999). 2.2. Min-max Framework for Active Kernel The simplest approach toward active kernel learning is to follow the uncertainty principle of active learn-ing, and to select the example pair ( x i , x j )withthe least | Z i,j | 1 . However, as already discussed in the in-troduction section, the key problem with this simple approach is that the example pairs with the least | Z i,j may not necessarily be the the most informative ones, and therefore may not result in an efficient learning of the kernel matrix. To address this problem, we pro-pose a min-max framework for active kernel learning that measures the informativeness of an example pair by how significantly the selected example pair will af-fect the target kernel matrix.
 Consider an unlabeled example pair ( x k , x l ) /  X  X  .To measure how this example will affect the kernel matrix, we consider the kernel learn ing problem with the addi-tional example pair ( x k , x l ) labeled by y  X  X  X  1 , +1 i.e., Let us denote by  X  (( k, l ) ,y ) the optimal value of the above optimization problem. Intuitively,  X  (( k, l ) ,y ) measures the overall classi fication accuracy with the additional example pair ( x k , x l ) labeled by y .To further measure the informativeness of example pair ( x k , x l ), we introduce the quantity  X  ( k, l ) as follows Clearly,  X  ( k, l ) measures the worst classification error with the addition of example pair ( x k , x l ). Overall,  X  ( k, l ) measures how the example pair ( x k , x l affect the overall objective function, which indirectly measures the impact of the example pair on the tar-get kernel matrix. To see t his, consider an example pair ( x k , x l ) that is highly consistent with the current kernel Z with label y (i.e., Z k,l y  X  1). According to the definition  X  ( k, l ), we would expect a large  X  ( k, l ) for pair ( x k , x l ). This is because by assigning a label  X  y to example pair ( x k , x l ), we expect a large clas-sification error and therefore large  X  ( k, l ). Hence, we use  X  ( k, l )tomeasurethe uninformativeness of exam-ple pairs, i.e., the smaller  X  ( k, l ), the less informative the example pair is. Therefore, the most informative example pair is found by minimizing  X  ( k, l ), i.e., Directly solving the min-max optimization problem in (4) is challenging because function  X  (( k, l ) ,t ) is defined implicitly by the optimization problem in (2). The following theorem allows us to significantly simplify the optimization problem in (4) Theorem 1. The optimization problem in (4) is equivalent to the following optimization problem Proof. The above theorem follows the fact that the so-lution y  X   X  X  X  1 , +1 } maximizing  X  (( k, l ) ,y )is y  X   X  sign( Z k,l ). This fact allows us to remove the maxi-mization within (4) and obtain the result in the theo-rem.
 The following corollary shows that the approach of se-lecting the example pair with the least | Z k,l | indeed corresponds to a special solution for the problem in (5). Corollary 2. The optimal solution to (5) with fixed kernel matrix Z is the example pair with the least |
Z Proof. By fixing Z , the problem in (5) is simplified as It is easy to see that the optimal solution to the above problem is the example pair with the least | Z k,l | . Note that a similar observation is described in the study (Chen &amp; Jin, 2007) for standard active learn-ing. 2.3. Algorithm The straightforward approach toward the optimiza-tion problem in (5) is to try out every example pair ( x k , x l ) / well when the number of example pairs is large. Our first attempt toward solving the problem (5) is to turn it into a continuous optimization problem. To this purpose, we introduce variable p k,l  X  0torepresent the probability of selecting the example pair ( k, l )  X  X  Using this notation, we have the optimization problem in (5) rewritten as The following theorem shows the relationship between (6) and (5).
 Theorem 3. Any global optimal solution to (5) is also a global optimal solution to (6).
 The proof of the above theorem can be found in Ap-pendix A.
 Unfortunately, the optimization problem in (6) is non-convex because of the term p k,l  X  2 k,l . It is therefore difficult to find the global optimal solution for (6). In order to turn (6) into a convex optimization problem, we view the constraint ( k,l ) /  X  X  p k,l  X  1 as a bound for the arithmetic mean of p k,l , i.e., where m = |{ ( k, l ) | ( k, l ) /  X  X }| .Wethenrelaxthis constraint by the harmonic mean of p k,l , i.e., The above relaxation is based on the property that a harmonic mean is no larger than an arithmetic mean. By replacing the constraint ( k,l ) /  X  X  p k,l  X  1with(7), we have (6) relaxed into the following optimization problem By defining variable h k,l = p  X  1 k,l ,wehave Notice that constraint 0  X  p k,l  X  1 is transferred into h the formulation in (8) Theorem 4. We have the following properties for (8)  X  (8) is a semi-definite programming (SDP) prob- X  Any feasible solution to (8) is also a feasible solu-The proof is provided in Appendix B. Note that us-ing  X  2 i,j instead of  X  i,j for the loss function is key to turning (6) into a convex optimization problem. The second property stated in Theorem 4 indicates that by minimizing (8), we guarantee a small value for the objective function in (6).
 The following theorem shows the dual problem of (8), which is the key to the efficient computation. Theorem 5. The dual problem of (8) is max s. t L Q  X  T + W  X   X  T where matrix  X  T is defined as and  X  stands for the element wise product of matrices. The proof can be found in Appendix C. In the dual problem, variables Q i,j and W i,j are the dual variables that indicate the importance of labeled example pairs and unlabeled examples, respectively. We thus will se-lect the unlabeled example pair with the largest | W i,j | To speed up the computation, in our experiment, we first select a subset of example pairs (fixed 200) with smallest | Z i,j | using the current kernel matrix Z .We then set all W k,l to be zero if the corresponding pair is not selected. In this way, we significantly reduce the number of variables in the dual problem in (9), thus simplifying the computation. In our experiments, we follow the work (Hoi et al., 2007), and evaluate the proposed algorithm for active kernel learning by the experiments of data clustering. More specifically, we first apply the active kernel learn-ing algorithm to identify the most informative example pairs, and then solicit the class labels for the selected example pairs. A kernel matrix will be learned from the labeled example pairs, a nd the learned kernel ma-trix will be used by the clustering algorithm to find the right cluster structure. 3.1. Experimental Setup We use the same datasets as the ones described in (Hoi et al., 2007). Table 1 summarizes the information about the nine datasets used in our study. We adopt the clustering accuracy defined in (Xing et al., 2002) as the evaluation metric. It is defined as follows
Accuracy = where 1 { X } is the indicator function that outputs 1 when the input argument is true and 0 otherwise. c i and  X  c i denote the true cluster membership and the predicted cluster membership of the i th data point, re-spectively. n is the number of examples in the dataset. For the graph Laplacian L used by the nonparamet-ric kernel learning, we apply the standard method for all experiments, i.e., by calculating the distance ma-trix by Euclidean distance, then constructing the ad-jacency matrix with five nea rest neighbors, and finally normalizing the graph to achieve the final Laplacian matrix. 3.2. Performance Evaluation To evaluate the quality of the learned kernels, we ex-tend the proposed kernel learning algorithm to solve clustering problems with pairwise constraints. In the experiments, we employ the kernel k-means as the clustering method, in which the kernel is learned by the proposed non-parametri c kernel learning method. In addition to the proposed active kernel learning method, two baseline approaches are implemented to select informative example pairs for kernel learning. Totally we have:  X  Random : This baseline method randomly sam- X  AKL-min-| Z | : This baseline method chooses the  X  AKL-min-H : This is the proposed AKL algo-To examine the performance of the proposed AKL al-gorithm in a full spectrum, we evaluate the clustering results with respect to different sampling sizes. Specif-ically, for each experiment, we first randomly sample N c pairwise constraints as the initially labeled pair examples. We then employ the nonparametric kernel learning method to learn a kernel from the given pair-wise constraints. This learned kernel is engaged by the kernel k-means method for data clustering. Next, we apply the AKL method to sample 20 pair examples (i.e. 20 pairwise constraints) for labeling in an itera-tion, and then examine the clustering results based on the kernel that is learned from the augmented set of example pairs in each iteration.
 Each experiment is repeated 50 times with multiple restarts for clustering. Fig. 2 shows the experimen-tal results on the nine datasets with five active ker-nel learning iterations. First of all, we observe that AKL-min-| Z | , i.e., the naive AKL approach that sam-ples the example pairs with the least | Z | ,doesnot always outperform the random sampling approach. In fact, it only outperforms the random sampling ap-proach on five out of the nine datasets. It performs noticeably worse than the random approach on dataset  X  X onar X  and  X  X eart X . Compared with the two baseline approaches, the proposed AKL algorithm (i.e., AKL-min-H ) achieves considerably better performance for most datasets. For example, for the  X  X ouble-Spiral X  dataset, after 3 active kernel learning iterations, the proposed algorithm is able to achieve the clustering accuracy of 99 . 6%, but the clustering accuracies of the other two methods are less than 98 . 8%. These exper-imental results show the effectiveness of the proposed algorithm as a promising approach for active kernel learning. In this paper we proposed a min-max framework for active kernel learning that specifically addresses the problem of how to identify the informative pair ex-amples for efficient kernel learning. A promising al-gorithm is presented that approximates the original min-max optimization problem into a convex program-ming problem. Empirical evaluation based on the per-formance of data clustering showed that our proposed algorithm for active kernel learning is effective in iden-tifying informative example pairs for the learning of kernel matrix.
 The work was supported in part by the National Science Foundation (IIS-0643494), National Institute of Health (1R01-GM079688-01), and Singapore NTU AcRF Tier-1 Research Grant (RG67/07). Any opin-ions, findings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of NSF and NIH. Proof. First, for any global optimal solution to (6), we have ( k,l ) /  X  X  p k,l = 1 though the constraint in (6) is ( k,l ) /  X  X  p k,l  X  1. This is because we can always scale down p k,l if ( k,l ) /  X  X  p k,l &gt; 1, which guarantees to reduce the objective function. Second, any extreme point solution (i.e., p k,l = 1 for one example pair and zero for other pairs) to (6) is a global optimal solution to (5). This is because (6) is a relaxed version of (5). Third, one of the global optimal solutions to (6) is an extreme point. This is because the first order condi-tion of optimality requires p  X  k,l tobeasolutiontothe following problem: where  X   X  k,l is the optimal solution for  X  k,l . Since (11) is a linear optimization problem, it is well known that one of its global optimal solutions is an extreme point. Combining the above arguments together, we prove there exists a global solution to (5), denoted by with p ( k,l )  X  = 1. We extend this conclusion to any other global solution (( k, l ) ,Z , X  i,j ) to (5) because (( k, l ) ,Z , X  i,j ) results in the same value for the prob-our proof.
 Proof. To show (8) is a SDP problem, we introduce slack variables for both labeled and unlabeled example these two nonlinear constraints into LMI constraints, i.e., Using the slack variables, we rewrite (8) as which is clearly a SDP problem.
 To show the second part of theorem, we follow the inequality that a harmonic mean is upper bounded by an arithmetic mean, i.e., 1
Chessboard (N=100, C=2, D=2, Nc=20) Hence, any feasible solution to (8) is also a feasible solution to (6), and (8) is a restricted version of (8), which leads to the conclusion that the optimal output value for (8) provides the upper bound for that of (6). Proof. We first constructe the Lagrangian function for the above problem
L =tr( L Z )+ c In the above, we introduce Lagrangian multiplier for constraints By setting the derivative to be zero, we have max s. t L Q  X  T + W  X   X  T The two LMI constraints can be simplified as Substituting the above constraints into (13), we have (9).
 Belkin, M., &amp; andd P. Niyogi, I. M. (2004). Regulariza-tion and semi-supervised learning on large graphs. Intl. Conf. on Learning Theory (COLT) .
 Chapelle, O., Weston, J., &amp; Sch  X  olkopf, B. (2003). Clus-ter kernels for semi-supervised learning. .
 Chen, F., &amp; Jin, R. (2007). Active algorithm selec-tion. Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI) .
 Cristianini, N., Shawe-Taylor, J., &amp; Elisseeff, A. (2002). On kernel-target alignment. JMLR .
 Hoi, S. C. H., Jin, R., &amp; Lyu, M. R. (2007). Learning nonparametric kernel matrices from pairwise con-straints. ICML2007 . Corvallis, OR, US.
 Hoi, S. C. H., Jin, R., Zhu, J., &amp; Lyu, M. R. (2006).
Batch mode active learning and its application to medical image classification. ICML2006 (pp. 417 X  424). Pittsburgh, Pennsylvania.
 Kondor, R., &amp; Lafferty, J. (2002). Diffusion kernels on graphs and other discrete structures. ICML X 2002 . Kulis, B., Sustik, M., &amp; Dhillo n, I. S. (2006). Learning low-rank kernel matrices. ICML2006 (pp. 505 X 512). Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui,
L. E., &amp; Jordan, M. (2004). Learning the kernel matrix with semi-definite programming. JMLR , 5 , 27 X 72.
 Scholkopf, B., &amp; Smola, A. (2002). Learning with ker-nels . MIT Press.
 Sturm, J. (1999). Using sedumi: a matlab toolbox for optimization over symmetric cones. Optimization Methods and Software , 11 X 12 , 625 X 653.
 Tong, S., &amp; Koller, D. (2000). Support vector machine active learning with applications to text classifica-tion. ICML2000 (pp. 999 X 1006). Stanford, US.
 Vapnik, V. N. (1998). Statistical learning theory .John Wiley &amp; Sons.
 Xing, E. P., Ng, A. Y., Jordan, M. I., &amp; Russell, S. (2002). Distance metric learning with application to clustering with side-information. NIPS2002 . Zhu, X., Kandola, J., Ghahramani, Z., &amp; Lafferty, J. (2005). Nonparametric transforms of graph kernels
