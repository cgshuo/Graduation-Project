 Ric h Caruana caruana@cs.cornell.edu Nik os Karampatziakis nk@cs.cornell.edu Ain ur Yessenalina ainur@cs.cornell.edu In the last decade, the dimensionalit y of man y mac hine learning problems has increased substan tially . Muc h of this results from increased interest in learning from text and images. Some of the increase in dimension-alit y, however, results from the dev elopmen t of tech-niques suc h as SVMs and L 1 regularization that are practical and e ectiv e in high dimensions. These ad-vances may mak e it unnecessary to restrict the feature set and thus promote building and learning from data sets that include as man y features as possible. At the same time, memory and computational power have in-creased to supp ort computing with large data sets. Perhaps the best kno wn empirical studies to exam-ine the performance of di eren t learning metho ds are STATLOG (King et al., 1995) and (Caruana &amp; Niculescu-Mizil, 2006). STATLOG was a very thor-ough study , but did not include test problems with high dimensions and could not evaluate new er learn-ing metho ds suc h as bagging, boosting, and kernel metho ds. More recen tly, (Caruana &amp; Niculescu-Mizil, 2006) includes a num ber of new learning algorithms that emerged after the STATLOG pro ject, but only ex-amined performance on problems with low-to-medium dimension. One must question if the results of either of these studies apply to text data, biomedical data, link analysis data etc. where man y attributes are highly correlated and there may be insucien t data to learn complex interactions among attributes. This pap er at-tempts to address that question.
 There are sev eral limitations to the empirical study in (Caruana &amp; Niculescu-Mizil, 2006). First, they per-formed all exp erimen ts using only 5000 training cases, despite the fact that much more lab eled data was avail-able for man y problems. For one of the problems (CO VTYPE) more than 500,000 lab eled cases were available. Inten tionally training using far less data than is naturally available on eac h problem mak es the results somewhat con triv ed. Second, although they evaluated learning performance on eigh t performance metrics, examination of their results sho ws that there are strong correlations among the performance mea-sures and examining this man y metrics probably added little to the empirical comparison and may have led to a false impression of statistical con dence. Third, and perhaps most imp ortan t, all of the data sets ex-amined had low to medium dimensionalit y. The aver-age dimensionalit y of the 11 data sets in their study was about 50 and the maxim um dimensionalit y was only 200. Man y mo dern learning problems have orders of magnitude higher dimensionalit y. Clearly learn-ing metho ds can beha ve very di eren tly when learning from high-dimensional data than when learning from low-dimensional data.
 In the empirical study performed for this pap er we complemen t the prior work by: 1) using the natural size training data that is available for eac h problem; 2) using just three imp ortan t performance metrics: ac-curacy , area under the ROC curv e (AUC), and squared loss; and 3) performing exp erimen ts on data with high to very high dimensionalit y (750-700K dimensions). 2.1. Learning Algorithms This section summarizes the algorithms and parame-ter settings we used. The reader should bear in mind that some learning algorithms are more ecien t at handling large training sets and high-dimensional data than others. For an ecien t algorithm we can a ord to explore the parameter space more exhaustiv ely than for an algorithm that does not scale well. But that's not unrealistic; a practitioner may prefer an ecien t algorithm that is regarded as weak but whic h can be tuned well over an algorithm that migh t be better but where careful tuning would be intractable. Belo w we describ e the implemen tations and parameter settings we used. An algorithm mark ed with an asterisk (e.g. ALG ) denotes our own custom implemen tation de-signed to handle high-dimensional sparse data. SVMs: We train linear SVMs using SVM per f (Joac hims, 2006) with error rate as the loss function. We vary the value of C by factors of ten from 10 9 to 10 5 . For kernel SVMs we used LaSVM, an appro xi-mate SVM solv er that uses stochastic gradien t descen t (Bordes et al., 2005), since traditional kernel SVM im-plemen tations simply cannot handle the amoun ts of data in some of our exp erimen ts. To guaran tee a rea-sonable running time we train the SVM for 30 min utes for eac h parameter setting and use the gradien t based strategy for the selection of examples. We use polyno-mial kernels of degree 2 and 3 and RBF kernels with the value of C by factors of ten from 10 7 to 10 5 . ANN : We train neural nets with gradien t descen t bac kpropagation, early stopping and no momen tum (cf. section 5). We vary the num ber of hidden units f 8 ; 16 ; 32 g and learning rate f 10 4 ; 10 3 ; 10 2 g . Logistic Regression (LR): We use the BBR pac kage (Genkin et al., 2006) to train mo dels with either L 1 or L 2 regularization. The regularization parameter is varied by factors of ten from 10 7 to 10 5 . Naiv e Bayes (NB ): Con tinuous attributes are mo deled as coming from a normal distribution. We use smo othing and vary the num ber of unobserv ed values f 0,0.1,0.2,0.5,1,2,5,10,20,50,100 g .
 KNN : We use distance weigh ted KNN. We use the 1000 nearest neigh bors weigh ted by a Gaussian kernel with 40 di eren t kernel widths in the range [0 : 1 ; 820]. The distance between two points is a weigh ted eu-clidean distance where the weigh t of eac h feature is determined by its information gain.
 Random Forests (RF ): We gro w 500 trees and the size of the feature set considered at eac h split is s= 2 ; s; 2 s; 4 s or 8 s where s is the square root of the num ber of attributes, as suggested in (Breiman, 2001). Bagged Decision Trees(BA GDT ): We bag 100 ID3 trees. The same implemen tation is used for boosted stumps (BSTST ) and boosted trees (BSTDT ) but because full-size ID3 trees can't eas-ily be boosted, we stop splitting when a node con tains less than 50 examples. We do 2 10 and 2 15 iterations of boosting for trees and stumps resp ectiv ely and use the validation set to pick the num ber of iterations from the set f 2 i j i = 0 ; 1 ; ::: 15 g .
 Perceptrons (PR C ): We train voted perceptrons (Freund &amp; Schapire, 1999) with 1,5,10,20 and 30 passes through the data. We also average 100 perceptrons eac h of whic h is obtained by a single pass through a perm utation of the data.
 With SVM, ANN, LR, KNN and PR C and datasets with con tinuous attributes we train both on raw and standardized data. This prepro cessing can be though t of as one more parameter for these algorithms. To preserv e sparsit y, whic h is crucial for the implemen ta-tions we use, we treat the mean of eac h feature as zero, compute the standard deviation, and divide by it. 2.2. Performance Metrics To evaluate performance we use three metrics: accu-racy (ACC), a threshold metric, squared error (RMS), a probabilit y metric, and area under the ROC curv e (AUC), an ordering metric. To standardize scores across di eren t problems and metrics, we divide per-formances by the median performance observ ed on eac h problem for that metric. For squared error we also invert the scale so that larger num bers indicate better performance as with accuracy and AUC. 2.3. Calibration The output of some learning algorithms suc h as ANN, logistic regression, bagged trees and random forests can be interpreted as the conditional probabilit y of the class given the input. The common implemen tation of other metho ds suc h as SVM and boosting, however, are not designed to predict probabilities (Niculescu-Mizil &amp; Caruana, 2005).
 To overcome this, we use two di eren t metho ds to map the predictions from eac h learning algorithm to calibrated probabilities. The rst is Isotonic Regres-sion (Zadrozn y &amp; Elk an, 2002), a metho d whic h ts a non-parametric non-decreasing function to the pre-dictions. The second calibration metho d is Platt's metho d (Platt, 1999) whic h ts a sigmoid to the pre-dictions. (Niculescu-Mizil &amp; Caruana, 2005) suggests that Platt's metho d outp erforms isotonic regression when there is less than about 1000 points available to learn the calibration function, and that calibration can hurt predictions from metho ds suc h as ANN and logistic regression (Caruana &amp; Niculescu-Mizil, 2006). We will revisit those ndings later in the discussion. Finally , note that calibrating can a ect metrics other than probabilit y metrics suc h as squared loss. It can a ect accuracy by changing the optimal threshold (for calibrated predictions the optim um threshold will be near 0.5) and Isotonic Regression can a ect AUC by creating ties on calibration plateaus where prior to cal-ibration there was a de nite ordering.
 To summarize our metho dology , we optimize for eac h dataset and metric individually . For eac h algorithm and parameter setting we calibrate the predictions us-ing isotonic regression, Platt's metho d, and the iden-tity function (no calibration) and choose the parame-ter settings and calibration metho d that optimizes the performance metric on a validation set. 2.4. Data Sets We compare the metho ds on 11 binary classi cation problems whose dimensionalit y ranges from 761 to 685569. The datasets are summarized in Table 1. TIS 1 is from the Ken t Ridge Bio-medical Data Rep os-itory . The problem is to nd Translation Initia-tion Sites (TIS) at whic h translation from mRNA to proteins initiates. CR YST 2 is a protein crys-tallograph y di raction pattern analysis dataset from the X6A beamline at Bro okha ven National Lab ora-tory . STURN and CALAM are ornithology datasets. 3 The task is to predict the app earance of two bird species: sturnella neglecta and calamospiza melano co-rys. KDD98 is from the 1998 KDD-Cup. The task is to predict if a person donates money . This is the only dataset with missing values. We impute the mean for con tinuous features and treat missing nom-inal and boolean features as new values. DIGITS 4 is the MNIST database of handwritten digits by Cortes and LeCun. It was con verted from a 10 class problem to a hard binary problem by treating digits less than 5 as one class and the rest as the other class. IMDB and CITE are link prediction datasets. 5 For IMDB eac h attribute represen ts an actor, director, etc. For CITE attributes are the authors of a pap er in the CiteSeer digital library . For IMDB the task is to predict if Mel Blanc was involved in the lm or television program and for CITE the task is to predict if J. Lee was a coau-thor of the pap er. We created SPAM from the TREC 2005 Spam Public Corp ora. Features tak e binary val-ues sho wing if a word app ears in the documen t or not. Words that app ear less than three times in the whole corpus were remo ved. Real-Sim (R-S) is a compilation of Usenet articles from four discussion groups: sim u-lated auto racing, sim ulated aviation, real autos and real aviation. 6 The task is to distinguish real from sim-ulated. DSE 7 is newswire text with annotated opinion expressions. The task is to nd Sub jectiv e Expressions i.e. if a particular word expresses an opinion. To split data into training, validation and test sets, if the data came with original splits for train and test sets (i.e. DIGITS, KDD98) we preserv ed those splits and created validation sets as 10% of the train set. If the data originally was split into folds, we merged some folds to create a training set, a validation set and a test set. (W e did this because running these exp erimen ts is so costly that we could not a ord to perform N-fold cross validation as this would mak e the exp erimen ts about N times more costly .) DSE came in 10 folds plus a dev elopmen t fold twice as big as other folds. We used the dev elopmen t fold as the validation set and merged the rst 5 folds for the train set and the rest for the test set. CR YST came in 5 folds. One fold became the validation set, 2 folds were merged for training and the rest became the test set.
 For the rest of the datasets we tried to balance be-tween the follo wing factors: (a) The test sets should be large enough so that di erences between learning al-gorithms are apparen t. (b) The test sets can be larger when the learning task is easy , but more data should be kept in the training set when the learning task is hard. (c) Some datasets would inevitably have more attributes than examples in the training set (IMDB, CITE, SPAM); for the rest we tried to put enough ex-amples in the training set so that metho ds with small bias migh t learn something interesting. (d) The vali-dation sets should be big enough so that parameter se-lection and calibration works well. In general we split the data so that we have around 50% in the training set and 50% in the test set. Validation data is dra wn from the training set. Table 2 sho ws the performance of eac h learning metho d on eac h of the elev en problems. In the ta-ble, the problems are arranged left to righ t in order of increasing dimensionalit y. The table is brok en into four sections. The top three sections rep ort results for Accuracy (ACC), Squared Error (RMS), and Area un-der the ROC Curv e (AUC). The bottom section is the average of the performance across these three metrics. For eac h metric and problem, the performances have been standardized by dividing by the median perfor-mance observ ed for that problem and metric. With-out standardization it is dicult to perform an unbi-ased comparison across di eren t datasets and metrics. A score of one indicates that the metho d had typical performance on that problem and metric compared to the other learning metho ds. Scores above one indi-cate better than typical performance, while scores less than one indicate worse than typical performance. The scale for RMS has been rev ersed so that scores above one represen t better than typical, i.e., lower, RMS. 8 The median performance for eac h problem and metric is included in the table to allo w calculating raw perfor-mances from the standardized scores. The last column in the table is the average score across the elev en test problems. In eac h section of the table, learning algo-rithms are sorted by these average scores. The last column of the last section represen ts the average score across all problems and metrics.
 Examining the results in the bottom section sho ws that on average across all problems and metrics, ran-dom forests have the highest overall performance. On average, they perform about 1% (1.0102) better than the typical mo del and about 0.6% (1.0102 vs. 1.0039) better than the next best metho d, ANN. The best metho ds overall are RF, ANN, boosted decision trees, and SVMs. The worst performing metho ds are Naiv e Bayes and perceptrons. On average, the top eigh t of ten metho ds fall within about 2% of eac h other. While it is not easy to achiev e an additional 1% of perfor-mance at the top end of the scale, it is interesting that so man y metho ds perform this similarly to eac h other on these high-dimensional problems.
 If we examine the results for eac h of the metrics in-dividually , we notice that the largest di erences in performance among the di eren t learning algorithms occur for AUC and the smallest di erences occur for ACC. For accuracy , boosted decision trees are the best performing mo dels follo wed by random forests. How-ever, a closer examination of the table sho ws that boosted trees do better in accuracy mostly because of their excellen t performance on the datasets with rel-ativ ely low dimensionalit y. Comparing boosted trees with random forests in the left part of the table we see that random forests outp erform boosted trees only on the TIS dataset. The situation is rev ersed on the righ t part of the table where boosted trees outp erform ran-dom forests only on the CITE dataset. As dimension-alit y increases, we exp ect boosted trees to fall behind random forests.
 In RMS, random forests are marginally better than boosted trees. This is con rmed by a bootstrap anal-ysis (cf. Section 4): random forests have 33% and 35% chance of ranking 1st and 2nd resp ectiv ely, while for boosted trees the corresp onding probabilities are 31% and 21%. However, in AUC random forests are clear winners follo wed by, somewhat surprisingly , KNN. Interestingly , although ANN is the 2nd best metho d overall in the bottom of the table, is does not per-form 1st or 2nd for any of the individual metrics in the top of the table. It is 2nd overall only because ANNs consisten tly yield very good, though perhaps not exceptional, performance on all metrics. A fact that is not apparen t from the table is that cali-bration with isotonic regression works better than cal-ibrating with Platt's metho d, or no calibration, on most problems and thus was used for almost all of the results rep orted in the table. Since our valida-tion sets alw ays are larger than 1000 examples, this con rms the ndings in (Niculescu-Mizil &amp; Caruana, 2005) that isotonic regression is preferred with large validation sets.
 3.1. E ect of Dimensionalit y In this section we attempt to sho w the trends in per-formance as a function of dimensionalit y. In Figure 1 the x-axis sho ws dimensionalit y on a log scale. The y-axis is the cumulative score of eac h learning metho d on problems of increasing dimensionalit y. The score is the average across the three standardized performance metrics where standardization is done by subtracting the median performance on eac h problem. 9 Subtract-ing median performance means that scores above (be-low) zero indicate better (worse) than typical perfor-mance. The score accum ulation is done left-to-righ t on problems of increasing dimensionalit y. A line that tends to slop e upwards (do wnwards) signi es a metho d that performs better (worse) on average compared to other metho ds as dimensionalit y increases. A horizon-tal line suggests typical performance across problems of di eren t dimensionalit y. Naiv e Bayes is excluded from the graph because it falls far below the other metho ds. Caution must be used when interpreting cum ulativ e score plots. Due to the order in whic h scores are aggregated, vertical displacemen t through much of the graph is signi can tly a ected by the per-formance on problems of lower dimensionalit y. The end of the graph on the righ t, however, accum ulates across all problems and thus does not favor problems of any dimensionalit y, The slop e roughly corresp onds to the average relativ e performance across dimensions. From the plot it is clear that boosted trees do very well in mo dest dimensions, but lose ground to ran-dom forests, neural nets, and SVMs as dimensionalit y increases. Also, linear metho ds suc h as logistic regres-sion begin to catc h up as dimensionalit y increases. Figure 2 sho ws the same results as Figure 1, but pre-sen ted di eren tly to avoid the complexit y of accum u-lation. Here eac h point in the graph is the average per-formance of the 5 problems of lowest dimension (from 761 to 1344), the 5 problems of highest dimension (21K to 685K) and 5 problems of intermediate dimension (927 to 105K). Care must be used when interpreting this graph because eac h point averages over only 5 data sets. The results suggest that random forests overtak e boosted trees. They are among the top performing metho ds for high-dimensional problems together with logistic regression and SVMs. Again we see that neu-ral nets are consisten tly yielding above average per-formance even in very high dimension. Boosted trees, bagged trees, and KNN do not app ear to cop e well in very high dimensions. Boosted stumps, percep-trons, and Naiv e Bayes perform worse than the typical metho d regardless of dimension.
 Figure 3 sho ws results similar to Figure 2 but only for di eren t classes of SVMs: linear-only (L), kernel-only (K) and linear that can optimize accuracy or AUC (L+P) (Joac hims, 2006). We also plot com binations of these (L+K and L+K+P) where the speci c mo del that is best on the validation set is selected. The re-sults suggest that the best overall performance with SVMs results from trying all possible SVMs (using the validation set to pick the best). Linear SVMs that can optimize accuracy or AUC outp erform simple lin-ear SVMs at mo dest dimensions, but have little e ect when dimensionalit y is very high. Similarly , simple lin-ear SVMs though not comp etitiv e with kernel SVMs at low dimensions, catc h up as dimensionalit y increases. We could not a ord cross validation in these exp eri-men ts because it would be too exp ensiv e. For some datasets and some metho ds, a single parameter set-ting can tak e days or weeks to run. Instead we used large test sets to mak e our estimates more reliable and adequately large validation sets to mak e sure that the parameters we select are good. However, without a statistical analysis, we cannot be sure that the di er-ences we observ e are not merely statistical uctuation. To help insure that our results would not change if we had selected datasets di eren tly we did a bootstrap analysis similar to the one in (Caruana &amp; Niculescu-Mizil, 2006). For a given metric we randomly select a bootstrap sample (sampling with replacemen t) from our 11 problems and then average the performance of eac h metho d across the problems in the bootstrap sample. Then we rank the metho ds. We rep eat the bootstrap sampling 20,000 times and get 20,000 po-ten tially di eren t rankings of the learning metho ds. Table 3 sho ws the results of the bootstrap analysis. Eac h entry in the table sho ws the percen tage of time that eac h learning metho d ranks 1st, 2nd, 3rd, etc. on bootstrap samples of the datasets. Because of space limits, we only sho w results for average performance across the three metrics.
 The bootstrap analysis suggests that random forests probably are the top performing metho d overall, with a 73% chance of ranking rst, a 20% chance of ranking second, and less than a 8% chance of ranking below 2nd place. The ranks of other good metho ds, however, are less clear and there app ears to be a three-w ay tie for 2nd place for boosted trees, ANNs, and SVMs. Running this kind of exp erimen t in high dimensions presen ts man y computational challenges. In this sec-tion we outline a few of them.
 In most high dimensional data features are sparse and the learning metho ds should tak e adv antage of sparse vectors. For ANN, for example, when inputs are sparse, a lot of computation in the forw ard direc-tion can be saved by using a matrix times sparse vector pro cedure. More savings happ en when the weigh ts are updated since the gradien t of the error with resp ect to a weigh t going out of a unit with zero value van-ishes. This is why our ANN implemen tation does not use momen tum. If it did, all weigh ts would have to be updated eac h iteration.
 Another caveat is that for tree learning algorithms, indexing the data by feature instead of by example can speed up queries about whic h examples exhibit a particular feature. These queries are common during learning and one should consider this indexing scheme. Our random forest implemen tation indexes by feature. Boosted decision trees on con tinuous data was the slowest of all metho ds. For bagged trees running times were better because we only grew 100 trees that can be gro wn in parallel. The same holds for random forests whic h have the added bene t that computation scales with the square root of dimensionalit y. Train-ing ANNs was sometimes slow, mainly because ap-plying some of the techniques in (Le Cun et al., 1998) would not preserv e the sparsit y of the data. For SVMs and logistic regression, we didn't have computational problems thanks to recen t adv ances in scaling them (Genkin et al., 2006; Bordes et al., 2005; Joac hims, 2006; Shalev-Sh wartz et al., 2007). As a sanit y chec k we compared the performance of the appro ximate ker-nel SVM solv er with the exact SVM light on some of our smallest problems and found no signi can t dif-ference. Naiv e Bayes and perceptrons are among the fastest metho ds. KNN was sucien tly fast that we didn't have to use specialized data structures for near-est neigh bor queries. Our work is most similar to (Caruana &amp; Niculescu-Mizil, 2006). We already pointed out shortcomings in that study , but we also borro wed much from their metho dology and tried to impro ve on it. STATLOG (King et al., 1995) was another comprehensiv e empir-ical study that was discussed in Section 1. A study by LeCun (LeCun et al., 1995) compares learning algo-rithms not only based on traditional performance met-rics but also with resp ect to computational cost. Our study addresses this issue only qualitativ ely. Clearly , computational issues have to be tak en into considera-tion in suc h large scale. A wide empirical comparison of voting algorithms suc h as bagging and boosting is conducted in (Bauer &amp; Koha vi, 1999). The imp or-tance of evaluating performance on metrics suc h as AUC is discussed thoroughly in (Pro vost &amp; Fawcett, 1997). The e ect of di eren t calibration metho ds is discussed in (Niculescu-Mizil &amp; Caruana, 2005). Although there is substan tial variabilit y in perfor-mance across problems and metrics in our exp eri-men ts, we can discern sev eral interesting results. First, the results con rm the exp erimen ts in (Caruana &amp; Niculescu-Mizil, 2006) where boosted decision trees perform exceptionally well when dimensionalit y is low. In this study boosted trees are the metho d of choice for up to about 4000 dimensions. Ab ove that, random forests have the best overall performance. (Random forests were the 2nd best performing metho d in the previous study .) We susp ect that the reason for this is that boosting trees is prone to over tting and this be-comes a serious problem in high dimensions. Random forests is better beha ved in very high dimensions, it is easy to parallelize, scales ecien tly to high dimensions and performs consisten tly well on all three metrics. Non-linear metho ds do surprisingly well in high dimen-sions if mo del complexit y can be con trolled, e.g. by exploring the space of hypotheses from simple to com-plex (ANN), by margins (SVMs), or by basing some decisions on random pro jections (RF). Logistic regres-sion and linear SVMs also gain in performance as di-mensionalit y increases. Con trary to low dimensions, in high dimensions we have no evidence that linear SVMs can bene t from training pro cedures that directly op-timize speci c metrics suc h as AUC.
 The results suggest that calibration nev er hurts and almost alw ays helps on these problems. Even meth-ods suc h as ANN and logistic regression bene t from calibration in most cases. We susp ect that the reasons for this are the availabilit y of more validation data for calibration than in previous studies and that high di-mensional problems are harder in some sense. We thank all the studen ts who took CS678 at Cor-nell in the spring of 2007 and help ed with this study . We esp ecially thank Sergei Fotin, Mic hael Friedman, Myle Ott and Ragh u Raman ujan who implemen ted KNN, ANNs, Random Forests and Boosted Trees re-spectiv ely. We also thank Alec Bern tson Eric Brec k and Art Munson for pro viding the crystallograph y, DSE and ornithology datasets resp ectiv ely. Art also put together the calibration pro cedures. Finally , we thank the 3 anon ymous review ers for their helpful commen ts. This work was supp orted in part by NSF Aw ards 0412930 and 0412894.
 Bauer, E., &amp; Koha vi, R. (1999). An empirical com-parison of voting classi cation algorithms: Bagging, boosting, and varian ts. MLJ , 36 , 105{139. Bordes, A., Ertekin, S., Weston, J., &amp; Bottou, L. (2005). Fast kernel classi ers with online and ac-tive learning. JMLR , 6 , 1579{1619.
 Breiman, L. (2001). Random Forests. MLJ , 45 , 5{32. Caruana, R., &amp; Niculescu-Mizil, A. (2006). An empir-ical comparison of sup ervised learning algorithms. ICML '06 , 161{168.
 Freund, Y., &amp; Schapire, R. (1999). Large Margin Clas-si cation Using the Perceptron Algorithm. MLJ , 37 , 277{296.
 Genkin, A., Lewis, D., &amp; Madigan, D. (2006). Large-scale bayesian logistic regression for text categoriza-tion. Technometrics .
 Joac hims, T. (2006). Training linear SVMs in linear time. SIGKDD , 217{226.
 King, R., Feng, C., &amp; Shutherland, A. (1995). Statlog: comparison of classi cation algorithms on large real-world problems. Applie d Arti cial Intel ligenc e , 9 , 259{287.
 Le Cun, Y., Bottou, L., Orr, G. B., &amp; M X  uller, K.-
R. (1998). Ecien t bac kprop. In Neur al networks, tricks of the trade , LNCS 1524. Springer Verlag. LeCun, Y., Jac kel, L., Bottou, L., Brunot, A., Cortes, C., Denk er, J., Druc ker, H., Guy on, I., Muller, U.,
Sac kinger, E., et al. (1995). Comparison of learning algorithms for handwritten digit recognition. Inter-national Confer ence on Arti cial Neur al Networks , 60 .
 Niculescu-Mizil, A., &amp; Caruana, R. (2005). Predicting good probabilities with sup ervised learning. ICML '05 , 625{632.
 Platt, J. (1999). Probabilistic outputs for supp ort vec-tor mac hines and comparisons to regularized likeli-hood metho ds. Advanc es in Large Mar gin Classi-ers , 10 .
 Pro vost, F. J., &amp; Fawcett, T. (1997). Analysis and visualization of classi er performance: Comparison under imprecise class and cost distributions. KDD '97 (pp. 43{48).
 Shalev-Sh wartz, S., Singer, Y., &amp; Srebro, N. (2007).
Pegasos: Primal estimated sub-gradien t solv er for svm. ICML '07 (pp. 807{814).
 Zadrozn y, B., &amp; Elk an, C. (2002). Transforming clas-si er scores into accurate multiclass probabilit y es-
