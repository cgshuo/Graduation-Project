 Vocal activity detection (VAD) is an important tech-nology for any application with an automatic speech recognition (ASR) front end. In meetings, partic-ipants typically vocalize for only a fraction of the recorded time. Their temporally contiguous contri-butions should be identified prior to ASR in order to leverage speaker adaptation schemes and language model constraints, and to associate recognized out-put with specific speakers (who said what). Segmen-tation into such contributions is informed primarily by VAD on a frame-by-frame basis.

Individual head-mounted microphone (IHM) recordings of meetings present a particular challenge for VAD, due to crosstalk from other participants. Most state-of-the-art VAD systems for meetings rely on decoding in a binary speech/non-speech space, assuming independence among participants, but are increasingly relying on features specifically designed to address the crosstalk issue (Wrigley et al., 2005).
A feature which has attracted attention since its use in VAD post-processing in (Pfau et al., 2001) is the maximum cross-channel correlation (XC), max  X   X  jk (  X  ), between channels j and k , where  X  is the lag. When designing features descriptive of the k th channel, XC is frequently normalized by the en-ergy in the target 1 channel k (Wrigley et al., 2003). Alternately, XC can be normalized by the energy in the non-target channel j (Laskowski et al., 2004), a normalization which we refer to here as NT-Norm, extending the Norm and S-Norm naming conventions in (Wrigley et al., 2005). Table 1 shows several types of normalizations which have been explored.
Normalization of XC Mean Min Max Table 1: Normalizations and statistics of cross-channel correlation features to describe channel k . In [1], a median-smoothed version was used in post-processing. In [3], the sum (JMXC) was used in-stead of the mean. In [5], cross-correlation was com-puted over samples and features. In [6], the mini-mum and the maximum were jointly referred to as NMXC. References in bold depict features selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006))
The present work revisits NT-Norm normalization, which has been successfully used in a threshold de-tector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and Stolcke, 2006). Our main contribution is a geo-metric interpretation of NT-Norm XC, in Section 2. We also describe, in Section 3, several contrastive experiments, and discuss the results in Section 4. We propose an interpretable geometric approxima-tion to NT-Norm XC for channel k ,
We assume the simplified response in the k th IHM microphone at a distance d k from a single point source s ( t ) to be where c , A k and  X  k ( t ) are the speed of sound, the gain of microphone k , and source-uncorrelated noise at microphone k , respectively. Cross-channel corre-lation is approximated over a frame of size  X  by where  X   X  ( d j  X  d k ) /c . Letting P s  X  R  X  s 2 ( t ) dt and P respectively, as the maximum of  X  jk (  X  ) occurs at  X   X  = ( d k  X  d j ) /c . In consequence, provided that i.e., under assumptions of similar microphone gains, a non-negligible farfield signal-to-noise ratio at each microphone, and the simplifications embodied in Equation 2, NT-Norm XC approximates the relative distances of 2 microphones to the single point source s ( t ). We stress that this approximation requires no side knowledge about the true positions of the par-ticipants or of their microphones.
 lies within the integration window  X  in Equation 3. In (Boakye and Stolcke, 2006), the authors showed that when the analysis window is 25 ms, the NMXC feature is not as robust as frame-level energy flooring followed by cross-channel normalization (NLED). 3.1 VAD and ASR Systems Our multispeaker VAD system, shown in Figure 1, was introduced in (Laskowski and Schultz, 2006). Rather than detecting the 2-state speech ( V ) vs. non-speech ( N ) activity of each partipant indepen-dently, the system implements a Viterbi search for tion space, where K is the number of participants. Segmentation consists of three passes: initial la-bel assignment (ILA), described in the next subsec-tion, for acoustic model training; simultaneous multi-participant Viterbi decoding; and smoothing to pro-duce segments for ASR. In the current work, during decoding, we limit the maximum number of simulta-neously vocalizing participants to 3.

This system is an improved version of that fielded in the NIST Rich Transcription 2006 Meeting Recog-nition evaluation (RT06s) 2 , to produce automatic segmentation in the IHM condition on conference meetings. The ASR system which we use in this paper is as described in (F  X ugen et al., 2007). 3.2 Unsupervised ILA For unsupervised labeling of the test audio, prior to acoustic model training, we employ the criterion  X q [ k ] = Assuming equality in Equation 6, this corresponds to declaring a participant as vocalizing when the dis-tance between the location of the dominant sound source and that participant X  X  microphone is smaller than the geometric mean of the distances from the source to the remaining microphones , ie. when We refer to this algorithm as ILAave. For contrast we also consider ILAmin, with the sum in Equation 8 re-placed by the minimum over j 6 = k . This corresponds to declaring a participant as vocalizing when the dis-tance between the location of the dominant sound source and that participant X  X  microphone is smaller than the distance from the source to any other mi-crophone . We do not consider ILAmax, whose inter-pretation in light of Equation 6 is not useful. 3.3 Data The data used in the described experiments con-sist of two datasets from the NIST RT-05s and RT-06s evaluations. The data which had been used for VAD system improvement, rt05s eval* , is the complete rt05s eval set less one meeting, NIST 20050412-1303 . This meeting was excluded as it contains a participant without a microphone, a condition known a priori to be absent in rt06s eval ; we use the latter in its entirety. 3.4 Description of Experiments The experiments we present aim to compare ILAave and ILAmin, and to show how the size of the inte-gration window,  X , affects system performance. As our VAD decoder operates at a frame size of 100ms, we introduce a reframing step between the ILA com-ponent and both AM training and decoding; see Fig-ure 1. V is assigned to each 100ms frame if 50% or more of the frame duration is assigned V by ILA; otherwise, the 100ms frame is assigned an N label.
We measure performance in four locations within the combined VAD+ASR system architecture, also shown in Figure 1. We compute a VAD frame er-ror just after reframing (  X  q F ), just after decoding ( q  X  ), and just after smoothing (  X  ( q  X  )). This er-ror is the sum of the miss rate (MS), and the false alarm rate excluding intervals of all-participant si-lence (FAX), computed against unsmoothed word-level forced alignment references. We use this met-ric for comparative purposes only, across the vari-ous measurement points. We also use first-pass ASR word error rates (WERs), after lattice rescoring, as a final measure of performance impact.

We evaluate, over a range of ILA frame sizes, the performance of ILAave(3), with a maximum number of simultaneously vocalizing participants of 3, and for the contrastive ILAmin. We note that ILAmin is capable of declaring at most one microphone at a time as being worn by a current speaker. As a re-sult, construction of acoustic models for overlapped vocal activity states, described in (Laskowski and Schultz, 2006), results in states of at most 2 simul-taneously vocalizing participants. We therefore refer to ILAmin as ILAmin(2), and additionally consider ILAave(2), in which states with 3 simultaneously vo-calizing participants are removed. We show the results of our experiments in Ta-ble 2. First-pass WERs, using reference segmenta-tion ( .stm ), vary by 1.3% absolute (abs) between rt05s eval and rt06s eval . We also note that re-moving the one meeting with a participant without a microphone reduces the rt05s eval manual seg-mentation WER by 1.7% abs. WERs obtained with automatic segmentation should be compared to the manual segmentation WERs for each set.

As the  X  q F columns shows, ILAmin(2) entails sig-nificantly more VAD errors than ILAave. Notably, although we do not show the breakdown, ILAmin(2) is characterized by fewer false alarms, but misses much more speech than ILAave(2). This is due in part to its inability to identify simultaneous talk-ers. However, following acoustic model training and use ( q  X  ), the VAD error rates between the two algo-rithms are approximately equal.

In studying the WERs for each ILA algorithm in-dependently, the variation across ILA frame sizes in the range 25 X 100 ms can be significant: for example, it is 1.2% abs for ILAmin(2) on rt06s eval , com-pared to the difference with manual segmentation of 3.1% abs. Error curves, as a function of ILA frame size, are predominantly shallow parabolas, except at 75 ms (notably for ILAmin(2) at  X  q F ); we believe that ILA  X   X  q a 100 31.3 16.7 16.0 39.0 34.1 39.6 v 75 33.6 16.6 15.9 38.9 34.1 39.9 e 50 35.2 16.7 16.0 38.8 34.0 39.3 3 25 36.8 17.3 16.3 39.6 34.2 39.7 a 100 31.3 15.8 15.2 37.8 34.4 39.7 v 75 33.6 15.6 15.0 37.9 34.4 39.6 e 50 35.2 15.8 15.2 37.6 34.3 39.3 2 25 36.8 16.4 15.6 38.1 34.3 39.5 m 100 43.4 15.8 14.7 38.2 35.2 39.3 i 75 51.9 15.6 14.6 38.1 35.2 39.3 n 50 47.1 15.7 14.6 37.9 35.1 40.1 2 25 47.7 16.2 14.9 38.1 35.4 40.5 refs 9.5 9.5 9.5 36.1 34.4 37.4 Table 2: VAD errors, measured at three points in our system, and first-pass WERs for rt05s eval ( 05 ), as well as first-pass WERs for rt05s eval* ( 05* ) and rt06s eval ( 06 ). Results are shown for 3 con-trastive VAD systems (ILAave(3), ILAave(2) and ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms, 50ms, and 25ms). this is because 75 ms does not divide evenly into the decoder frame size of 100 ms, causing more deletions across the reframing step than for other ILA frame sizes. Error minima appear for an ILA frame size somewhere between 50 ms and 75 ms, for both ASR and post-decoding VAD errors.

Although (Pfau et al., 2001) considered a maxi-mum lag of 250 samples (15.6ms, or 5m at the speed of sound), their computation of S-Norm XC used a rectangular window. Here, as in (Laskowski and Schultz, 2006) and (Boakye and Stolcke, 2006), we use a Hamming window. Our results suggest that a large, broadly tapered window is important for Equa-tion 6 to hold.

The table also shows that for datasets with-out uninstrumented participants, rt05s eval* and rt06s eval , ILAmin(2) is outperformed by ILAave(2) by as much as 1.1% abs in WER, espe-cially at small frame sizes. The difference for the full rt05s eval dataset is smaller. The results also sug-gest that reducing the maximum degree of simulta-neous vocalization from 3 to 2 during decoding is an effective means of reducing errors (ASR insertions, not shown) for uninstrumented participants. We have derived a geometric approximation for a particular type of normalization of maximum cross-channel correlation, NT-Norm XC, recently intro-duced for multispeaker vocal activity detection. Our derivation suggests that it is effectively comparing the distance between each speaker X  X  mouth and each microphone. This is novel, as geometry is most often inferred using the lag of the crosscorrelation maxi-mum, rather than its amplitude.

Our experiments suggest that frame sizes of 50 X 75 ms lead to WERs which are lower than those for ei-ther 100 ms or 25 ms by as much as 1.2% abs; that ILAave outperforms ILAmin as an initial label as-signment criterion; and that reducing the degree of simultaneous vocalization during decoding may ad-dress problems due to uninstrumented participants. This work was partly supported by the European Union under the integrated project CHIL (IST-506909), Computers in the Human Interaction Loop.
