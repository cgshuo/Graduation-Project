 Structured documents such as web pages or multimodal data usually consist of main and additional information. Namely, they have multiple components. Addi-tional information includes citations in books and papers, hyperlinks and images on web pages, and text descriptions added to images and music. Although the main information plays an important role when designing a classifier, additional data may contain substantial information for classification. Recently, classifiers have been developed for dealing with multiple components such as text and hy-perlinks on web pages [4,6,16,10], text and citations in papers [6,10], and text and music [2].
 dealing with arbitrary multiple components, rather than for special relations between components such as a pair consi sting of Web text and its hyperlinks as studied in [4]. Here we categorize probabilistic approaches into generative, discriminative, and a hybrid of the two.
 and class label y , compute P ( y | x ) by using the Bayes rule, and then take the most probable label y . However, such direct modeling is hard for arbitrary components consisting of completely different types of media. In [2], under the assumption of the class conditional independence of all components, the class conditional probability P ( x j | y ) for each component is modeled separately, where x j stands for the feature vector corresponding to the j th component. Hence, as described later, the joint probability is expressed by the simple product of P ( x j | y ). learn mapping from x to y . Multinomial logistic regression [8] can be used for this purpose. However, we believe that any approach that ignores structural in-formation has an intrinsic limitation in terms of achieving good classification performance. In [10], the class posterior probability P ( y | x j ) for each compo-nent is modeled separately, and then the simple product of P ( y | x j )isusedfor predicting the class that x belongs to.
 scribed above might not always be sufficiently effective for designing high perfor-mance classifiers because the component models for noisy additional information may degrade the classification performance. The main and additional informa-tion differ in importance as regards classification. We consider the hybrid ap-proach described below to be promising with a view to effectively combining such component classifiers.
 component, P ( x j | y ), and directly model class posterior probability P ( y | x )by using trained component models. Namely, each component model is estimated on the basis of the generative approach, while the classifier is constructed on the basis of the discriminative approach. For binary classification problems, such a hybrid classifier has already been prop osed and applied to the document clas-sification of two components ( X  X ubject X  and  X  X ody X ) [13]. It has been shown experimentally that the hybrid classifier achieves higher accuracy than either a pure generative or a pure discriminative classifier.
 More specifically, we separately design component generative models P ( x j | y )for main and additional information, and estimate the models individually. Then, basedonthe maximum entropy (ME) principle [3], we design the class posterior probability P ( y | x ) by using the estimated generative models. Unlike standard ME approaches to classification that deal directly with input vectors x [11], with our ME approach, the formulation deals with generative component models. applied to multiclass classification probl ems by using the well-known one-against-all (OAA) or all-pairs (AP) schemes (cf. [1,14]). However, we consider there are problems in the application of the OAA and AP schemes to the binary hybrid classifiers. The OAA scheme designs a bina ry classifier for a single class against the other classes. Here, we need to assume a single generative model for samples belonging to the other classes by ignoring the individual characteristics of each class. However, this assumption is unlikely to make full use of the advantage of the generative model. The AP scheme designs an individual binary hybrid classi-fier for each pair of classes. Here, each classifier can only classify samples into one of a pair of classes. For multiclass classification, in some way, we need to com-bine the results obtained from these classifiers. However, the combination of the binary hybrid classifiers might not provide the accurate discriminative boundary needed to classify samples into the most probabilistically appropriate class. model for each class and construct a single hybrid classifier, which provides discriminative boundaries between all classes. Therefore, our hybrid approach to multiclass classification avoids the problems of the OAA and AP schemes. Documents consist of text components. W eb pages consist of text and link com-ponents. We use naive Bayes models as generative models for both text and link components. Using three test collections, we show experimentally that our hy-brid classifier design method is more effective than the conventional approaches for multiclass classification problems. In multiclass ( K classes) classification problems, a classifier categorizes a fea-ture vector x into one of K classes y  X  X  1 ,...,k,...,K } . Each feature vector consists of J separate components as x = { x 1 ,..., x j ,..., x J } . The classifier is trained on training sample set D = { x n ,y n } N n =1 . In the following, we derive basic formulas for the conventional approaches. 2.1 Generative Approach Generative classifiers model a joint probability P ( x ,y ). However, as mentioned above, such direct modeling is hard for arbitrary components that consist of completely different types of media. Under the assumption of the class condi-tional independence of all components, the joint probability can be expressed as where  X  j is a model parameter for the j th component, and  X  = {  X  j } J j =1 .Note that the component generative model P ( x j | k,  X  j ) should be selected according to the features of the component: for example, a multinomial model for text information or a Gaussian model for continuous feature vectors.
 (MAP estimation). According to the Bayes rule, P (  X  | D )  X  P ( D |  X  ) P (  X  ), the objective function of MAP estimation is given by Here, P (  X  j ) is a prior over parameter  X  j . Clearly, component model parameter  X  j can be optimized without considering the other parameters.
 derived as The class of x is determined as the k that maximizes P ( k | x , X  ). Note that since the denominator of Eq. (3) is same for all k , it can almost be predicted from the simple product of P ( x j | k,  X  j ). 2.2 Discriminative Approach Discriminative classifiers directly model posterior class probabilities P ( y | x )for all classes. With multinomial logistic regr ession [8], the posterior class probabil-ities are modeled as where W = { w 1 ,..., w K } is a set of unknown model parameters. w k  X  x repre-sents the inner product of w k and x . W is estimated to maximize the following penalized conditional log-likelihood: Here P ( W )isaprioroverparameter W .
 separately for each component, and the estimate  X  W j of the model parameter is independently computed. The class of x is determined as the y that maximizes the product of the posterior probabilities estimated in the components as 2.3 Hybrid Approach for Binary Classification Hybrid classifiers learn the class conditional probability model separately for each component, P ( x j | y ), and directly model the class posterior probability P ( y | x ) by using the trained component models. In [13], binary classifiers are de-rived as follows. The class posterior probability in Eq. (3) is equivalently trans-formed to Then by introducing the weight parameters b j for the components and b 0 = P ( k =1) , the class posterior probability is extended as follows: The weight parameter set B = { b j } J j =0 is estimated as the parameter of logistic regression, according to the maximum class posterior likelihood as mentioned above. As mentioned earlier, we present a new hybrid classifier formulation extended for multiclass classification. In this section, we provide details of our formulation and our parameter estimation method. 3.1 Component Generative Models In our formulation, we simply design ea ch component generative model sepa-rately without strictly assuming class conditional independence as described in Section 2.1. Let P ( x j | k,  X  j )bethe j th component generative model, where  X  j denotes the model parameter.  X  j is computed using MAP estimation. The  X  j estimate is computed to maximize the objective function using training sample set D . 3.2 Discriminative Posterior Design After computing the estimates {  X   X  j } J j =1 of the component generative model para-meters, we provide the posterior probability based on the weighted combination of the component generative models to improve the classification performance. More specifically, based on the maximum entropy (ME) principle [3], we design the class posterior probability using component generative models.
 which prefers the most uniform models that satisfy any given constraints. Let R ( k | x ) be a target distribution that we wish to specify using the ME principle. A constraint is that the expected value of log-likelihood w.r.t. the target distrib-ution R ( k | x ) is equal to the expected value of log-likelihood w.r.t. the empirical where  X  P ( x )= 1 N N n =1  X  ( x  X  x n ) is the empirical distribution of x .Wealso restrict R ( k | x )sothatithasthesameexpectedvaluefortheclassindicator variable z k as seen in the training data, where z k =1if x belongs to the k -th class, otherwise z k = 0, such that  X  x target distribution: the combination weight of the j th component generative model, and  X  k is the bias parameter for the k th class. The distribution R ( k | x ,  X   X ,  X  )givesusthe formulation of a discriminative classifier that consists of component generative models.
 ing sample set D . However, since D is used for estimating  X  and  X  ,abiased estimator may be obtained. Thus, when  X  is estimated, a leave-one-out cross-validation of the training samples is used [13]. Let  X   X  (  X  n ) be a generative model parameter estimated by using all the training samples except ( x n ,y n ). The ob-jective function of  X  then becomes where R (  X  )isaprioroverparameter  X  . We used the Gaussian prior [5] as R (  X  )  X  j exp(  X  the L-BFGS algorithm [9], which is a quasi-Newton method. We summarize the algorithm for estimating these model parameters in Fig. 1. 3.3 Discussion We can regard the class posterior R ( k | x ,  X   X ,  X  ) derived from the ME principle as a natural extension of the class posterior P ( k | x , X  ) shown in Eq. (3). Actually, if  X  j =1 ,  X  j and P ( k )= e  X  k , R ( k | x ,  X   X ,  X  ) is reduced to P ( k | x , X  ). R ( k | x , X ,B ) for the binary classifications shown in Eq. (8). If K =2, b j =  X  j , and b 0 =  X  2  X   X  1 , R ( k | x ,  X   X ,  X  ) is reduced to R ( k | x , X ,B ). based on a hybrid of the generative mode ls and multinomial logistic regression as by using the constraint: instead of Eq. (9), following the feature setting in [11]. In the next section, we will omit the experimental results for the classifier obtained by using Eq. (13), because the classifier was overfitted into a small number of training samples in our experiments. 4.1 Test Collections Empirical evaluation was performed on three test collections: 20 newsgroups (20news), NIPS ,and WebKB data sets. 20news and WebKB have often been used as benchmark tests of classifiers in text classification tasks [11], and NIPS 1 is the ASCII text collection of papers from NIPS conferences created by Yann using optical character recognition. articles. Each article belongs to one of the 20 groups. We extracted two compo-nents, main (M) and title (T) , from each article, where T is the text description following  X  X ubject: X  and M is the main information in each article except for the title. Each component contains words as features. We removed vocabulary words included either in the stoplist [15] or in only one article. There were 52313 and 5320 vocabulary words, respectively, in components M and T in the data set.
 pers from NIPS conferences 5-12 in our experiments. Each paper is related to one of nine research topics, for example, neuroscience, theory, and applications. We extracted four components, main (M), title (T), abstract (A) ,and ref-erences (R) , from each paper, where M is the main information in each paper excluding the title, abstract, and references. We removed vocabulary words in the same way as for 20news. There were 20485, 904, 5021, and 8303 vocabulary words, respectively, in components M, T, A, and R in the data set.
 categories, and each page belongs to one of these categories. Following the setup in [11], only four categories course, faculty, project ,and student were used. The categories contained a total of 4199 pages. We extracted six components, main (M), title (T), in-links (IL), out-links (OL), file-links (FL) ,and anchor-text (AT) , from each page. Here, T is the text description between &lt; TITLE &gt; and &lt; /TITLE &gt; tags, and M is the main information except for the title, tags, and links. The IL for a page is the set of links from the other pages to the page. AT is the set of anchor text for each page, which consists of text descriptions that express the link to the page found on other web pages. We collected IL and AT from the links within the data set. The OL for a page is the set of links to the other pages, and the FL is the set of links to files such as images. M, T, and AT contain words as features, and IL, OL, and FL contain URLs for web pages. We removed vocabulary words in the same way as for 20news and removed URLs included in only one page for each component. There were 18471, 995, and 496 vocabulary words, respectively, in components M, T, and AT in the data set. Components IL, OL, and FL contained 500, 4131, and 484 different URLs, respectively.
 or web pages) whose components are not empty. | F t | for each component shows the total number of features (words or URLs) contained over all the documents in the data sets. | D t | is the total number of documents in a data set. As shown This shows that M is not empty in most of the documents. | D f | / | D t | for the other components, especially AT, IL, OL, and FL with respect to hyperlinks on WebKB, were smaller than in component M. This shows that AT (IL/OL/FL) were empty in many documents. The average feature frequency | F t | / | D t | in M was much larger than in the other components in all the data sets. 4.2 Experimental Settings Generative Models for Proposed Method. For text information, we employed naive Bayes (NB) models [12] as component generative models P ( x of-Words (BOW) representation. Let x j =( x j 1 ,...,x ji ,...,x jV j )representthe word-frequency vector of the j th component (text information) of a data sample, where x ji denotes the frequency of the i th word in the j th component and V j denotes the total number of words in the vocabulary included in the j th com-ponent in a text data set. In an NB model, text information x j in the k th class is assumed to be generated from a multinomial distribution Here,  X  jki &gt; 0and V j i =1  X  jki =1.  X  jki is the probability that the i th word appears in the j th component of a document belonging to the k th class. OL, and components, we used the Bag-of-URLs representation as well as BOW for text information. x j for the link information represents the URL-frequency vector. x ji for the IL component, for example, represents the frequency of the i thURLlinkedfromthewebpage x .
  X  j as P (  X  j ) parameters  X  jk by using the leave-one-out cross-validation of labeled samples to maximize the log likelihood of generative probabilities estimated for unseen samples with the help of the EM algorithm [7], because we confirmed the hy-perparameter tuning was practically useful for classification. Since it is not an essential part of the method and because of space constraints, we will omit the details of the hyperparameter estimation procedure.
 generative models in Eq. (5), we obtain the class posterior distribution for the hybrid classifier: Comparison Method. The proposed method was compared with classifiers based on either the generative or discriminative approach. First, the proposed classifier was compared with naive Bayes (NB) and multinomial logistic regres-sion (MLR) using only component M, in order to examine the effect of combining additional information on classificatio n performance. Second, we compared the proposed classifier with the four classifiers that used additional information em-ployed in the other methods. One method involves designing classifiers based on the simple product of the component models as presented in Section 2. We examined the performance of two product -based classifiers that used naive Bayes (PNB) or multinomial logistic regression (PMLR) models as component mod-els. Another method involves designing a single model without considering the separate components. Although the model design might be inappropriate when the features are different from those of the other components, we used a sin-gle model for evaluating the proposed method. We examined the classification performance of single naive Bayes (SNB) and multinomial logistic regression (SMLR) classifiers.
 let prior for NB/PNB/SNB and in the Gaussian prior for MLR/PMLR/SMLR that provided high average classification accuracy for the test samples to observe the potential ability of the methods.
 Evaluation Measure. We examined the classification accuracy with test sam-ples to evaluate the proposed and conventional methods. In our experiments, we selected the training and test samples randomly from each data set. We made ten different evaluation sets for each data set by random selection. For 20news, 8000 articles were selected as test samples for each evaluation set. For NIPS and WebKB, 500 papers and 2000 web pages were selected as test samples, respec-tively. After extracting the test samples, training samples were selected from the remaining samples in each data set. The av erage classification accuracy over the ten evaluation sets was used to evaluate the methods with each of the three data sets. 4.3 Results and Discussion Effect of Combining Additional Information. The proposed method was compared with NB and MLR classifiers using only component M. With the pro-posed method, we designed the classifier using all the components for 20news and NIPS. For WebKB, we designed two classifiers, one using two text components, M and T, the other using all the components of the text and link information. We examined the classification accuracy by changing the number of training sam-ples. Figure 2 shows the average classification accuracies over the ten different evaluation sets for (a) 20news, (b) NIPS, and (c) WebKB.
 and MLR using only component M. That is, we confirmed that combining the additional information improved the classification performance.
 MLR except when there were 32 training samples. The classifier using compo-nents M and T performed better than the classifier using all the components when the number of training samples was small. In Table 1, | D f | / | D t | for AT, IL, OL, and FL is small. This indicates that there are few training samples whose components (AT/IL/OL/FL) are not empty. A classifier that uses all the components might be more overfitted into the training samples than a classifier that uses M and T, when the number of training samples is small.
 Effect of Combination Weight of Component Models. We compared our proposed classifier with PNB/PMLR and SNB/SMLR classifiers. For this comparison, we designed these classifiers using all the components in the data sets. Table 2 shows the average accuracies over the ten different evaluation sets for (a) 20news, (b) NIPS, and (c) WebKB. Each number in parentheses in the table denotes the standard deviation of the ten evaluation sets. | D | represents the number of training samples.
 of the five. We confirmed that the proposed method provided the combination weight of the component generative models thus improving the classification performance.
 ative classifiers, PNB and SNB, except when | D | = 32. The performance of the proposed method was similar to or better than that of the pure discriminative classifiers, PMLR and SMLR.
 generative (discriminative) classifiers when the SMLR (SNB) performance was better than that of SNB (SMLR). When the performance of the pure generative and classifier classifiers was similar, the proposed method performed better than the classifiers. Analysis of Estimated Combination Weight. We examined the combina-tion weight of the component generative models based on the ME principle. In Fig. 3, the circles represent the average estimated combination weight  X   X  j in the proposed method. In these experiments, we used 10240, 576, and 2048 training samples for 20news, NIPS, and WebKB, respectively. The bars in Fig. 3 repre-sent the test average classification accuracies of naive Bayes classifiers designed using only one component. The test classification accuracy of the naive Bayes model for each component (component NB model) was examined using only test samples whose component was not empty. Figure 3 confirms that the proposed method provided a larger estimate of the combination weight for components that provided better classification performance.
 of the data samples, where | x j | denotes the total number of features (words or URLs) included in the j th component of data sample x .If | x j | is constant over Then, the combination of component NB models in Eq. (16) is consistent with the simple product of those models as shown in Eq. (3). Although | x j | is not constant over real data samples, we can roughly examine the difference between the combination of component NB models in the proposed method and the simple product by comparing the estimate of  X  j with  X  j .InFig.3,theestimateof  X  j for components other than M is larger than the  X  j values. We confirmed that the proposed method adjusted the combination weight so that minor components ignored in the classifier based on the simple product were used effectively. with weights induced from the classification performance of individual compo-nents. However, we can consider there to be various possible functional forms for combining components and various possible methods for inducing the combina-tion weights. The proposed method provides one formulation for the weighted combination of components according to the ME principle and can compute the combination weights on the basis of a discriminative approach. We experimen-tally confirmed that the proposed method adequately combined components and thus improved classification performance.
 Comparison with Straightforward Extensions of Binary Hybrid Clas-sifiers to Multiclass Classification Problems. We can straightforwardly extend binary hybrid classifiers as shown in Eq. (8) to multiclass classification problems according to the well-known one-against-all (OAA) or all-pairs (AP) schemes (cf. [1,14]). In the OAA scheme, K different binary classifiers are trained. The k th binary classifier was designed to classify samples in the k th class from samples in the other classes. We estimated the k th class posterior probability using the k th trained binary classifier and determined the class of x as the k that maximized the probabilities. In the AP scheme, K ( K  X  1) / 2 different binary classifiers for all pairs of classes are trained. The class of x was determined as the k that won the most choices from these classifiers.
 WebKB obtained with the proposed method, OAA, and AP. The performance of the proposed method was better than that of the OAA and AP schemes for all data sets except when there were 32 training samples in WebKB. This indicates that the problems in the application of the OAA and AP schemes to binary hybrid classifiers as mentioned in the introduction might prevent them from achieving a high level of performance. We proposed a new classifier formulation that effectively uses both main and ad-ditional information for multiclass classification based on a hybrid of generative and discriminative approaches. The main idea is to design separate component generative models for the main and additional information and model the class posterior based on the combination of these models by employing the maximum entropy principle.
 sets, we confirmed that the classification performance of the proposed method was generally better than that of classifiers based on the simple product of component models. The proposed method was especially useful when the classi-fication performance levels of the pure generative and discriminative classifiers were comparable. We believe that the proposed method improved the classifi-cation performance by providing combination weights of component generative models. The combination weights estimated with the proposed method tended to be larger for the component generative model, which provided better classifi-cation performance. We also confirmed that the proposed method was effective in terms of classification performance wh en compared with straightforward ex-tensions of binary hybrid classifiers to multiclass classification problems using the one-against-all or all-pairs schemes.
 in which different generative models are employed, to show the usefulness of the method for more general multi-component data.

