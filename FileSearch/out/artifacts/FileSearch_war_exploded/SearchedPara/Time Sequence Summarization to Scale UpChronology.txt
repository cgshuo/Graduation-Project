 In this paper, we present the concept of Time Sequence Summa-rization to support chronology-dependent applications on massive data sources. Time sequence summarization takes as input a time sequence of events that are chronologically ordered. Each event is described by a set of descriptors. Time sequence summarization produces a concise time sequence that can be substituted for the original time sequence in chronology-dependent applications. We propose an algorithm that achieves time sequence summarization based on a generalization, grouping and concept formation process. Generalization expresses event descriptors at higher levels of ab-straction using taxonomies while grouping gathers similar events. Concept formation is responsible for reducing the size of the input time sequence of events by representing each group created by one concept. The process is performed in a way such that the over-all chronology of events is preserved. The algorithm computes the summary incrementally and has reduced algorithmic complexity. The resulting output is a concise representation, yet, informative enough to directly support chronology-dependent applications. We validate our approach by summarizing one year of financial news provided by Reuters.
 E.4 [ Coding and Information Theory ]: Data Compaction and compression; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Algorithms, Experimentation, Performance, Theory Time sequences, Summarization, Taxonomies, Clustering
Domains such as medicine, the WWW, business or finance gen-erate and store on a daily basis massive amounts of data. This data is represented as a collection of time sequences of events where each event is described as a set of descriptors taken from various de-scriptive domains and associated with a timestamp. These archives represent valuable sources of insight for analysts to browse, an-alyze and discover golden nuggets of knowledge. For instance, biologists could discover disease risk factors by analyzing patient history [27], web content producers and marketing people are in-terested in profiling client behaviors [23], traders investigate fi-nancial data for understanding global trends or anticipating market moves [29]. However, analysts are overloaded with the size of this data and increasingly need methods and tools allowing exploratory visualization, query or analysis.

As an example, Google has developed Google Finance [1]. In a user-defined timeline, Google Finance provides analysts with a tool to browse through companies X  stock values while visualizing background information about the companies. This background in-formation is provided in the form of a sequence of chronologically ordered news events that appeared at some interesting moments, e.g., during price jumps. We call applications, such as Google Fi-nance, that rely on the chronological order of the data to be mean-ingful: Chronology-dependent applications.

In this context, we observed that sequences of events relating to an entity A occurring in a short period of time are likely to relate to a same topic, e.g., events about Lehman Brothers mid-September 2008 relate to its bankruptcy. This observation shows that it could be more practical and meaningful for the analyst to navigate in the chronology of events through summarized events that gather sev-eral events about a same topic, e.g., Lehman Brothers X  X  Backruptcy , rather than the entire set of individual events. At the same time, the analyst should be given the possibility to browse the details of these summarized events for a more in-depth analysis. This existing ex-ample puts forward the need for a data representation where mul-tiple events describing a same topic are grouped while preserving the overall chronology of events X  topic.

During the past decade, semantic data summarization has been addressed in various areas such as databases, data warehouses, datas-treams, etc., to represent data in a more concise form by using its semantics [10, 11, 14, 5, 13, 22]. However, including the time di-mension into the summarization process is an additional constraint that requires the chronology of events X  topic to somehow be pre-served. For instance, the sequence Lehman Brothers X  X  Bankruptcy , Lehman Brothers X  X  Rescue only makes sense because the events related to the Bankruptcy need to occur before the Rescue can hap-pen. We name this type of data transformation, based on the data X  X  semantic content and temporal characteristics, Time Sequence Sum-marization . A time sequence summarizer should take as input a time sequence of events, where each event is described by a set of descriptors, and output a time sequence of summarized events. The produced summarized time sequence should have the follow-ing properties: 1. Brevity: The number of summarized events in the output time sequence should be reduced in comparison to the number of events in the input time sequence. 2. Substitution principle: A chronology-dependent application that performs on a time sequence of events should be capable of performing seamlessly if the input time sequence is replaced by its summary. 3. Informativeness: Summarization should reduce time sequen-ces of events in a way that keeps the semantic content available to and understandable by the analyst without the need for desumma-rization . 4. Accuracy and usefulness: The input time sequence of events should not be overgeneralized to preserve descriptive precision and keep the summarized time sequence useful. However, ensuring high descriptive precision of events in the summarized time se-quence requires trading off the brevity property of the summary. 5. Chronology preservation: The chronology of summarized events in the output time sequence should reflect the overall chrono-logy of events in the input time sequence. 6. Computational scalability: Time sequence summaries are built to support chronology-dependent applications and, thus their construction should not become a bottleneck. Applications such as data mining might need to handle very large and long collections of time sequences of events, e.g., news feeds, web logs or market data, to rapidly discover knowledge. Therefore, the summarization process should have low processing and memory requirements.
Designing a time sequence summary that displays all these prop-erties is a challenging task. There exists a bulk of work for design-ing summaries in different areas such as datastreams, transaction databases, event sequences or relational databases. However, to the best of our knowledge, this research corpus does not address simul-taneously all six mentioned properties.
 Contributions. In this paper, we present the concept of Time Sequence Summarization and propose a summarization technique that satisfies all six mentioned properties to support chronology-dependent applications. Our contributions are as follows:  X  We give a formal definition of Time Sequence Summarization . The time sequence summary of a time sequence of events is a time sequence of summarized events where: (i) events occurring close in time and relating to a same topic are gathered into a same summa-rized event and (ii) each summarized event is represented by a con-cept formed from the underlying events. For example, assume the summary could be: ( t 1 , Subprime crisis ) .  X  We propose a T ime S equence Summ aR ization (TSaR) algo-rithm. TSaR relies on the ideas of Generalization and Merging, introduced by Han et al. for discovering knowledge in relational databases [10, 11]. TSaR is a 3-step process that uses background knowledge in the form of taxonomies, supposedly given by the an-alyst to generalize event descriptors at higher levels of abstraction.
Assuming events occurring close in time might relate to a same topic, grouping is performed on generalized events whose general-ized descriptors are similar. TSaR X  X  grouping process gathers gen-eralized events in a way that respects the chronology of topics in the input time sequence. For this purpose, a Temporal Locality is de-fined so that temporally close events can be gathered. Temporal lo-cality is a term borrowed from Operating systems research [8] and defined in Section 4.3. It can intuitively be understood as the fact that a series of events close in time have high probability of relating to a same topic. However, events relating to different topics might locally overlap in that period, e.g., due to network delays. Thus, defining a temporal locality allows TSaR to gather these overlap-ping events into their corresponding topics. Finally, each group is represented by a concept. In total, higher numerosity reduction can be achieved while the chronology of topics in the input time se-quence is preserved.

TSaR summaries are built in an incremental way by processing an input time sequence of events in a one-pass manner. The algo-rithm has small memory and processing footprints. TSaR main-tains in-memory a small structure that holds a limited number of grouped events, i.e., grouped events that fit into the temporal lo-cality. TSaR X  X  algorithmic complexity is linear with the number of events in the input time sequence.

We validate these characteristics with a set of experiments on real world data. We performed experiments using one year of English financial news events obtained from Reuters X  X . These archives con-tain after cleaning and preprocessing approximately 1.28M events split over 34458 time sequences. Each event in the time sequences is a set of words that precisely describes the content of the corre-sponding news article. Our extensive set of experiments on sum-marizing this data shows that TSaR has (i) interesting numerosity reduction capabilities, e.g., compression ratio ranges from 10% to 82%, and (ii) low and linear processing cost.

Roadmap. The rest of the paper is organized as follows. Sec-tion 2 presents related work. Section 3 formalizes the concept of Time Sequence Summarization . Section 4 presents the novel tech-nique we contribute in the paper. Section 5 discusses the experi-mentation we performed on financial news data. We conclude and discuss future work in Section 6.
Our work relates to lines of research, where a concise repre-sentation of massive data sources is desirable for storage or for knowledge discovery in constrained processing and memory en-vironments. Related research domains are those where summaries are built from sequences of objects ordered by their time of occur-rence. In this context, and in the light of the requirements men-tioned in the introduction, we examine summarization techniques produced for datastreams, transaction databases and event sequen-ces. This study, however, do not encompass time series summa-rization as time series summarization rely on methods that only consider numerical data.

Datastreams. Datastreams is a domain characterized by data of infinite size and eventually generated at very high rates. The com-mon assumptions are that (i) any processing should be performed in a single pass and (ii) input data can not be integrally stored. Such constraints have motivated researchers to represent input streams in a more concise form to support analysis applications, e.g., (approx-imate) continuous queries answering, frequent items counting, ag-gregation, clustering, etc.. Techniques proposed maintain in mem-ory small structures, e.g., samples, histograms, quantiles or syn-opses, for streams of numerical data (we refer the reader to [9] for a more complete review). In contrast with numerical data that is defined on continuous and totally ordered domains, categorical descriptors are defined on discrete and partially ordered domains. Therefore, descriptors can not be handled and reduced in the same way as numerical data using conventional datastreaming summa-rization techniques, e.g, min/max/average functions.

To the best of our knowledge, small interest has been given to designing summaries[4, 22] for categorical datastreams. Aggarwal et al. [4] proposed a clustering method for categorical datastreams. The approach relies on the idea of co-occurrence of attribute values to build statistical summaries and gather input data based on these statistics. However, the clusters built do not reflect the chronology of the input data and require pre-processing before analysis. We proposed in previous work an approach to summarize datastreams using a conceptual summarization algorithm [20]. The summary produced does not reflect the chronology of the input data and can not be directly exploited by chronology-dependent applications.
Transaction database summarization. Transaction databases (TDB) are collections of time sequences where each time sequence is a list of chronologically ordered itemsets. A bulk of work [6, 24, 25, 28] has focused on creating summaries for transaction databases. Chandola et al. X  X  approach [6] and SUMMARY [25] are techniques that rely on closed frequent itemset mining to build an informa-tive representation that covers the entire TDB. Building these sum-maries require multiple passes over the input TDB and the output is a set of frequent itemsets. This output does not endorse the substi-tution principal and does not reflect the chronology of transactions of the input TDB. HYPER [28] summarizes a TDB as a set of hy-perrectangles that covers the database. HYPER X  X  output set of hy-perrectangles requires preprocessing before chronology-dependent applications can exploit the summary and hyperrectangles are com-puted in polynomial time. But, designing a time sequence summa-rizer requires a single pass over the data and the output to preserve the chronology of transactions. Wan et al. [24] summarize a TDB into the compact form of a CT-tree specifically to support Sequen-tial Pattern Mining (SPM). The output tree structure allows SPM to perform but looses the chronology of transactions of the input TDB.

Event sequence summarization. Kiernan and Terzi [16] rely on the Minimum Description Length (MDL) principle to produce in a parameter-free way a comprehensive summary of an event se-quence, where events are taken from set E of m different event types. The authors segment the input event sequence timeline into k segments. Summarization is achieved by describing each seg-ment S i with a local model M i that is a partition of E where groups X ij  X  M i gather event types of similar rate of appearance in S Each event group X ij  X  M i is then associated with a probability of appearance p ( X ij ) of X ij in S i . However, the output summary can not be directly piped to a chronology-dependent application and needs some form of desummarization . Hence, the authors X  defini-tion of summarization does not endorse the substitution principle and one can not seamlessly substitute the original event sequence for the summary.

The TSaR approach builds on top of the ideas in Attribute Ori-ented Induction (AOI) [10, 11]. The TSaR process is split into three sub-routines that allow input time sequences to be processed in an incremental way: (i) generalization, (ii) grouping and (iii) concept formation. Generalization transforms event descriptors into a more abstract but informative form. Then, grouping gathers generalized events that are semantically close, i.e., having similar generalized descriptors, and chronologically close. Grouping is performed in a way that preserves the overall chronology of topics in the time sequence. Finally, each formed group is represented by a concept, i.e., a set of descriptors, formed from the underlying events X  de-scriptors. The output can then be directly interpreted by a human analyst or piped to any chronology-dependent application.
In this section, we give a running toy example to illustrate all the concepts presented in this paper. We also introduce the basic ter-minology used throughout the rest of the discussion and formalize the concept of Time Sequence Summarization .
To illustrate the ideas exposed in this paper, we generate in Ta-ble 1 a simple toy example with a time sequence extracted from conference proceedings. The author N. Koudas is associated with a time sequence where each event is one publication timestamped by its date of presentation. For simplicity, the set of descriptors describing an event is taken from one single descriptive domain, namely, the paper X  X  topic . Without loss of generality, this discus-sion is valid for any number of descriptive domains. This exam-ple is purposely unrelated to the application domain we provide in Section 5. It illustrates all the concepts introduced and shows the genericity of our approach.

Let  X  be the universe of discourse, i.e., the set of all descrip-tors that could describe an event in a time sequence of events.  X = A D A is organized into several descriptive domains D A corresponding to each domain A that interests the analyst, e.g., the topic of research papers.

We refer to a part of  X  , i.e., a subset of descriptors taken from various descriptive domains, as itemset x  X  X  ( X ) . Given an object of interest (e.g.,  X  X . Koudas X ), an event e is defined by an itemset x that describes e (e.g., { Datastreams , Aggregation }) and is asso-ciated with a timestamp t (e.g., t =  X  X UN05 X ). We assume the data input for time sequence summarization, also called raw data, is a collection of time sequences of events as defined in Definition 1. Definition 1 (Time Sequence of events) A time sequence of events s = ( x 1 ,t 1 ) ,..., ( x m ,t time sequence for short, is a series of events ( x j ,t j m and x j  X  X  ( X ) , ordered by increasing timestamp t j . We denote by S = { x 1 ,...,x m } the support multi-set of s . A time sequence s verifies:  X  ( x j ,x k )  X  S 2 ,j&lt;k  X  t j &lt;t k . We denote by s the set of timestamps of elements in S .

This definition of a time sequence and the total order on times-tamps allow us to equivalently write: s = ( x 1 ,t 1 ) ,..., ( x m ,t m )  X  s = { ( x j ,t j ) } ,
By convention, we further simplify the notation of a time se-quence and note s = x 1 ,...,x m where each x j , 1  X  j  X  an itemset and all itemsets x j are sorted by ascending index j .This simplification of the notation allows us to interchangeably use the term event to refer to the itemset x j in event ( x j ,t j by S ( X ) the set of time sequences in P ( X ) .

This notion of time sequence can be generalized and used to de-fine a sequence of time sequences that we hereafter call second-order time sequence . Second-order time sequences are more for-mally defined in Definition 2.
 Definition 2 (Second-order time sequence) A second-order time sequence defined on  X  is a time sequence s { ( of events defined on  X  . Events ( y i ,t i ) in s ,where y are ordered thanks to the minimum timestamp value t i =min { The set of second-order time sequences defined on  X  is denoted S
An example of second-order time sequence from Table 1 for au-thor N. Koudas can be defined as follows: s = ( y 1 ,t 1 ) ( y 3 ,t 3 )) where:
A second-order time sequence can be obtained from a time se-quence s as defined in Definition 1 by the means of a form of clus-tering based on the semantics and temporal information of events x i,j in s . We refer the reader to the following surveys for more indepth on clustering [15, 26]. Reversely, a time sequence can be obtained from a second-order time sequence s by means of concept formation [18, 19] computed from time sequences y i in s .
We formally define in Definition 3 the concept of a time se-quence summary using the concepts introduced previously. Definition 3 (Time sequence summary) Given a time sequence s = { ( x i ,t i ) } X  X  ( X ) , using clustering terminology, we define the time sequence summary of s , denoted  X  ( s )=( s 2 C ,s M )  X  X  2 ( X )  X S ( X ) , as follows: Hence, s 2 C and s M can be understood as the extension and the intention , respectively, of the summary  X  ( s ) .

We defined time sequence summarization using clustering ter-minology as the underlying ideas are similar, i.e., grouping objects basedontheir proximity . The novelty of time sequence summaries relies on the fact that events are clustered thanks to their semantic and temporal information. Conventional clustering methods mostly rely on the joint features of the objects considered and their proxim-ity is evaluated thanks to a distance measure, e.g., based on entropy or semantic distances. Similarly, in time sequence summarization, a form of temporal approximation should also be applicable so that objects that are close from temporal view point are grouped. Con-sequently, local rearrangement of the objects on the timeline should also be allowed.

In a nutshell, the objective of time sequence summarization is to find the best method for grouping events based on their semantic content and their proximity on the timeline. This general defini-tion of time sequence summarization can partially encompass some previous works such as Kiernan and Terzi X  X  research on large event sequences summarization [16]. Indeed, the authors perform sum-marization by partitioning an event sequence S into k segments S 1  X  i  X  k ; this segmentation can be understood as organizing S into a second-order time sequence s 2 C where C is their segmenta-tion method, e.g., Segment-DP. Note that the authors X  segmenta-tion method does not allow any form of event rearrangement on the timeline. Then, each segment S i is described by a set of event type groups { X i,j } where each X i,j groups event types of similar ap-pearance rate. Thus, the model used to describe each segment is a probabilistic model. To this point, our definition of a time sequence summary fully generalizes Kiernan and Terzi X  X  work. However, the authors add for each X i,j its probability of appearance p S . By doing so, the authors do not support the substitution prin-ciple and thus do not completely respect our definition of a time sequence summary.
In this section we present a T ime S equence Summ aR ization tech-nique called TSaR. The basic principle of TSaR is illustrated in Fig-ure 1. The idea is to gather events whose descriptors are similar at some high level of abstraction and that appear close in time. This is done in three steps: (i) reduce the data X  X  domain of representation by generalizing descriptors to a user defined level of abstraction, (ii) group identical sets of descriptors within a certain sliding time window then (iii) represent each group with a single set of descrip-tors, a.k.a. concept.

In practice, the process is parametrized by three inputs: (i) do-main specific taxonomies, (ii) a semantic accuracy parameter and (iii) a temporal precision parameter. The generalization process in phase 1 takes as input a time sequence, domain specific taxonomies and the user defined semantic accuracy parameter. It outputs a time sequence of generalized events where event descriptors are ex-pressed at higher levels of taxonomy. This output is then fed to the grouping process in phase 2 where identical generalized events are grouped together. The overall chronology of events is preserved by grouping only generalized events present in a same temporal local-ity (as defined in Section 4.3). Phase 3 forms a concept to represent each group. Here, since all sets of descriptors in a group are iden-tical, one instance of the group is selected to represent the group. We will detail these steps in the following sections.

In this work, we assume that each descriptive domain D A ,on which event descriptors are defined, is structured into a taxonomy H
A  X  X  = A H A , defining a generalization-specialization rela-tionship between descriptors of D A . The taxonomy H A provides a partial ordering  X  A over D A and is rooted by the special descrip-tor any_A , i.e.,  X  a  X  D A ,a  X  A any_A . For convenience, we assume in the following that the descriptor any_A belongs to D The partial ordering  X  A on D A defines a cover relation &lt; corresponds to direct links between items in the taxonomy. Hence, we have  X  ( x, y )  X  D 2 A ,x  X  A y  X  X  X  ( y 1 ,y 2 ,...,y k from x to y is ( x, y )= k +1 . In other words, we need k generalizations to reach y from x in H A . For example, given the topic taxonomy in Figure 2, it takes 2 generalizations to reach the concept Queries from the concept Top-k query .

In practice, we assume these taxonomies are available to the an-alyst. We believe this assumption is realistic as there exists nu-merous domain specific ontologies available, e.g., WordNet [3] or Onto-Med for medicine, as well as techniques that allow the auto-matic generation of taxonomies [17, 21].
Here, we detail the generalization phase that uses taxonomies to represent the input data at higher levels of abstraction. We assumed values in  X  are partially ordered by the partial order relation so, thanks to this relation, we can define a containment relation over subsets of  X  , i.e., P ( X ) .
For generalization, we need to replace event descriptors with up-per terms of the taxonomies. We call generalization vector on denoted  X   X  N i , a list of integer values.  X  defines the number of generalizations to perform for each descriptive domain in denote by  X  [ A ] the generalization level for the domain A . Equipped with this generalization vector  X  , we are now able to define a re-striction  X   X  of the containment relation above: Definition 4 (Generalization of a Time Sequence) Given a generalization vector  X  and a set of taxonomies define a parametric generalization function  X   X  that operates on a time sequence s = x 1 ,...,x n as follows:
For example, given the topic taxonomy in Figure 2 the gener-alized version of Table 1 with a generalization vector  X  = 1 shown in Table 2. We can notice that the  X   X  relation also allows to reduce itemsets X  cardinality. Indeed, Datastreams and Top-k query both generalize into QO . As a result, in N. Koudas X  X  time sequence, the event {Datastreams, Top-k query} is generalized into {QO}.
From the analyst X  X  view point,  X  represents the semantic accu-racy he desires for each descriptive domain. If he is interested in the minute details of a specific domain, e.g., a paper X  X  topic , he can set  X  to a low value, e.g.,  X  [ topic ]=0 or  X  [ topic ]=1 he can set  X  to higher values for a more abstract description of the domain. Once the input time sequence has undergone generaliza-tion, the output undergoes a grouping process as described in the following section.
Here, we detail the grouping phase responsible for gathering generalized events. This phase relies on two concepts: (i) second-order time sequence as defined in Definition 2 and (ii) Temporal locality . We define this notion of temporal locality hereafter.
In many research areas, it is assumed that a sequence of events generated close in time have high probability of relating to a same topic. This notion of temporal locality is borrowed from Operat-ing systems research [8]. For example, Paper deadline , Authors notification , Camera-ready paper is a chronology of events de-scribing the Conference submission topic. However, in a time se-quence, events describing different topics can eventually be inter-twined, e.g., due to network delays. Thus, defining a temporal lo-cality for grouping events of a time sequence allows to capture the following notions: (i) sequentiality of events for a given topic and (ii) chronology between topics.

Temporal locality is measured as the time difference d T ,ona temporal scale T , between an incoming event and previously group-ed events, hereafter called groups for short. The temporal scale may be defined directly through the timestamps or by a number of inter-mediate groups since we assume they are chronologically ordered. While grouping, an incoming event can only be compared to pre-vious groups within a distance w ( d T  X  w ). Indeed, w actsasa sliding window on the time sequence of groups and corresponds to the analyst X  X  estimation of the temporal locality of events. In other words, w can be understood as the temporal precision loss the an-alyst is ready to tolerate for rearranging and regrouping incoming events.

This temporal window can be defined as a duration, e.g., w one month, or as a number of groups, e.g., w =2 . Defining w as a number of groups is useful in particular when considering bursty sequences, i.e., sequences where the arrival rate of events is uneven. The downside of this approach is the potential grouping of events distant in time. However, this limitation could be solved by defin-ing w as both a duration and a constraint on the number of groups. As some domains, e.g. Finance, give more importance to the most recent information, we choose in our work to express our scale as a number of groups in order to handle bursts of events.
The grouping process is responsible for gathering generalized events relating to a same topic w.r.t. the temporal parameter w .It produces a second-order time sequence of groups defined as fol-lows: Definition 5 (Grouping of a time sequence) Given a sliding temporal window w , we define a parametric group-ing function  X  w that operates on a time sequence s as follows: such that: (Cont) ( x i,q x i,r ) , 1  X  r&lt;q  X  m i , 1  X  i  X  n (TLoc) d T ( t i,q ,t i )  X  w , 1  X  i  X  n, 1  X  q  X  m i (Max)  X  x i,q  X  y i ,  X  x j,r  X  y j , 1  X  i&lt;j  X  n,
Property (Part) ensures that the support multi-set of w -contiguous time sequences in  X  w ( s ) , e.g., y 1 ,...,y w , y 2 ,...,y ... , is a non-overlapping part of S . This is a direct consequence of grouping events that relate to a same topic within a same temporal locality. Property (Cont) gives a containment condition on events of every time sequence in  X  w ( s ) . Given a time sequence y  X  w ( s ) , all events x i,j  X  y i are comparable w.r.t. the containment relation and x i, 1 is the greatest event, i.e., x i, 1 contains all other events in y i . Property (TLoc) defines a temporal locality constraint on events that are grouped into a same time sequence y i in  X  (TLoc) ensures that y i only groups events ( x i,j ,t i,j a distance d T inferior to w from timestamp t i = min ( y d ( t i,j ,t i )  X  w . Property (Max) guaranties that the joint condi-tions (Cont) and (TLoc) are maximally satisfied.

In the grouping function  X  w , the temporal locality parameter w controls how well the chronology of groups should be observed. When a small temporal window w is chosen, a very strict order-ing of topics in the output time sequence is required. Incoming events can only be grouped with the latest groups. If w =1 contiguous events are eligible for grouping. Table 3 gives the ex-pected output when performing grouping on Table 2 with w =1 For example, note that in N. Koudas X  X  generalized time sequence, when event x 2 = { QO } is considered, x 2 can only be compared to previous group y 1 = {{ QO, DM }} for grouping.

When a large temporal window is chosen, the ordering require-ment is relaxed. This means that a large number of groups can be considered for grouping for each incoming event. As a conse-quence, the minute details of the chronology of topics may be lost but higher numerosity reduction could be achieved.

The second-order time sequence output by the grouping function  X  w can be understood as the extension of the summary, i.e., s defined in Section 3.3. The intention of the summary, and thus the reduced version of the input time sequence, is obtained thanks to the concept formation phase as presented in the following section. The concept formation phase is responsible for generating s the intention of the summary, from the time sequence of groups s obtained in the grouping phase. In the TSaR approach, we gather generalized events that have identical sets of descriptors. There-fore, this phase is straightforward.

Here, concept formation is achieved thanks to the projection op-erator  X  defined in Definition 6. Intuitively,  X  represents each time sequence y i in second-order time sequence s 2 C by a single concept, called representative event , x k contained in y i . Consequently,  X  produces from s 2 C a regular time sequence s M =  X  ( s 2 intention of the summary, also called representative sequence .In addition, this operator is responsible for reducing the numerosity of events in the output time sequence, w.r.t. the original number of events in the input time sequence.
 Definition 6 (Projection of a Second-Order Time Sequence) We define the projection of a second-order time sequence s ( y  X  : S 2 ( X )  X  X  X S ( X )
From our toy example in Table 3, N. Koudas X  X  representative sequence is therefore:  X  ( s 2 C )= (  X  ( y 1 ) ,t 1 ) , (  X  ( y 2 ) ,t 2 ) , (  X  ( y
In TSaR, summarization is achieved by the association of the three functions presented in the previous section, namely, the gen-eralization, grouping and projection functions  X  ,  X  and  X  respec-tively. The summarization function is formally defined in Defini-tion 7.
 Definition 7 ( T ime S equence Summ aR ization (TSaR) function) Given a time sequence s defined on  X  , a set of taxonomies fined over  X  , a user defined generalization vector  X  for taxonomies in
H and a user defined sliding temporal window w , the summary of s is the combination of a generalization  X   X  , followed by a group-ing  X  w and a projection  X  : where s 2 C =  X  w  X   X   X  ( s ) and s M =  X  ( s 2 C ) .
The association of the generalization and grouping function,  X  and  X  respectively, outputs the extension of the summary, i.e., s The reduced form of the summary, i.e., its intention s M , is a time sequence obtained by forming concepts from groups in s 2 C to the projection operator  X  . The extension of the summary s then satisfies the conditions of the generalization-grouping process. The (Cont) property of  X  w is then enforced by the generalization phase  X   X  of events in s . Note that every element in the reduced form of the summarized time sequence, i.e., s M , is an element of  X  ( s ) .Inotherwords, s M is a representative subsequence of the generalized sequence of s .

From a practical view point, the analyst and applications are only given the intention form s M of s  X  X  time sequence summary  X   X ,w ( s ) . Indeed, s M is the most compact form of the summary. In addition, s M is a time sequence that can seamlessly replace s and be directly processed by any chronology-dependent application that performs on s . Thus, s M is the most useful form from application view point. In the following, we will interchangeably use the term summary to designate the intention s M of a summary.
 Let us give an illustration of a summary with our toy example. The representative sequences extracted from Table 3 are give Ta-ble 4. Here, we achieve the dual goal of numerosity reduction (from 6 events to 3) and domain reduction (from 6 descriptors to 2). These compression effects are obtained thanks to the user de-fined parameters, i.e., the generalization vector  X  and the temporal sliding window w , that control the trade-off between resp. semantic accuracy vs. standardization and time accuracy vs. compression.
From an operational view point, our implementation of TSaR is shown in Algorithm 1. The summary is computed through an incre-mental algorithm that generalizes and appends incoming events one at a time into the current output summary. In other words, assume the current summary is s M =  X   X ,w ( x 1 ,...,x n )=  X  ( y  X  ( y j ) and the incoming event is ( x n +1 ,t n +1 ) . The algorithm computes  X   X ,w ( x 1 , ..., x n ,x n +1 ) with a local update to s i.e., changes are only made within the last w groups y j  X  w
More precisely, ( x n +1 ,t n +1 ) is generalized into ( x (line 6). Then, assuming we denote W = { y k } , 1  X  j  X  w k  X  j the set of groups that are included in temporal window w , TSaR checks if x n +1 is included in a group y k  X  W . x n +1 either incorporated into a group y k if its  X  -generalized version sat-isfies the (Cont) condition (line 8 to 9), or it initializes a new group { ( y j +1 ,t j +1 = t n +1 ) } in W (line 11 to 14). Once all input events are processed, the last w groups contained in W are projected and added to the output summary s M (line 18 to 20). The final output summary is then returned and/or stored in a database.

Memory footprint. TSaR requires w groups to be maintained in-memory for summarizing the input time sequence. The algo-rithm X  X  memory footprint is finite ( O (1) ) and bound by the width of w and the average size m of an event X  X  set of descriptors. The overall process memory footprint is obtained by adding the cost necessary to maintain in-memory the output time sequence s the taxonomies and/or hashtable index to compute descriptors X  gen-eralization. However, s M can be projected and written to disk at regular intervals. Therefore, TSaR X  X  overall memory footprint re-mains constant and limited compared to the amount of RAM now available on any machine.

Processing cost. TSaR performs generalization, grouping and concept formation on the fly for each incoming event. The process has an algorithmic complexity linear with the number of events O ( n ) . The processing cost is weighted by a constant cost c a  X  b . a is the cost for generalizing an event X  X  set of descriptors Algorithm 1 TSaR X  X  pseudo-code 1. INPUT:  X  , taxonomies H , w , time sequence of events s 2. LOCAL: W FIFO list containing the w last groups 3. OUTPUT: Summary s M 4. for all incoming event ( x n +1 ,t n +1 )  X  s do 5. {// Generalization using H } 7. {// Grouping} 8. if  X  ( y k ,t k )  X  W, where x n +1 y k then 9. y k  X  y k  X  x n +1 {// x n +1 is grouped into y k } 10. else 11. if | W | X  w {// Case where W is full} then 12. Pop W  X  X  1 st group y j  X  w ,add  X  ( y j  X  w ) into s 13. end if 14. W  X  W  X  X  ( x n +1 ,t n +1 ) } {// Updating W } 15. end if 16. end for 17. {// Add all groups in W into s M } 18. while W =  X  do 19. Pop W  X  X  1 st group y i ,add  X  ( y i ) into s M 20. end while 21. return s M and mainly depends on the number of taxonomies and their size. b is the cost to scan the finite list of groups in W .However, a is a cost that can be reduced by precomputing the generalization of each descriptive domain and storing the results in a hashtable in-dex. b is a cost that is negligible since the temporal windows w used are small, e.g., mostly w  X  25 in our experiments. Hence, we satisfy the memory and processing requirements presented earlier.
In this section we validate our summarization approach through an extensive set of experiments on real-world data from Reuter X  X  financial news archives. First, we describe the data and how tax-onomies are acquired for the descriptive domains of the news. We summarize the raw data with different temporal windows w and show the following properties of the TSaR algorithm: (i) low pro-cessing cost, (ii) linearity and (iii) compression ratio.
Our experiments were performed on a Core2Duo 2.0GHz laptop, 2GB of memory, 4200rpm hard drive and running Windows Vista Pro. The DBMS used for storage is PostgreSQL 8.0 and all code was written in C#.
In financial applications, traders are eager to discover knowledge and eventual relationships between live news feeds and market data in order to create new business opportunities [29]. Reuters has been generating and archiving such data for more than 10 years. To ex-periment and validate our approach in a real-world environment, we used one year of Reuters X  X  news articles (2003) written in En-glish. The unprocessed data set comes as a log of 21,957,500 en-tries where each entry includes free text and a set of  X  30 value pairs of numerical or categorical descriptors. An example of raw news event is given in Table 5. As provided by Reuters, the data can not be processed by the TSaR algorithm. Hence, the news data was cleaned and preprocessed into a sequence of events processable by TSaR.

Among all the information embedded in Reuters X  X  news articles we focused on 3 main components for representing the archive as a time sequence:  X  Timestamp: This value serves for ordering a news article within a time sequence.  X  Topic_codes: When news articles are written, topic_codes are added to describe their content. There are in total 715 different codes relating to 20 different topics. We used 7 of the most popular topics to describe the data, i.e., {Location, Commodities, Economy Central Banking and Institution, Energy, Equities, Industrial sector, General news}.  X  Free text: This textual content is a rich source of information from which precise semantic descriptors can be extracted. Give 5 additional topics, namely, {Business, Operation, Economics, Gov-ernment, Finance}, we used the WordNet [3] ontology to extract additional descriptors from this content.

Extracting pertinent descriptors from free text is a non trivial task w.r.t. the need for organizing the descriptors extracted into tax-onomies. Research in Natural Language Processing (NLP) could be leveraged to tag texts based on their corpus, e.g., using Term Frequency-Inverse Document Frequency (TF-IDF) weights as done in [12] or using online resources such as Open Calais [2]. How-ever, creating taxonomies from the tags extracted is not trivial and requires prior knowledge on the descriptors. The paradox lies in the fact that these descriptors are not known in advance. We chose to use the WordNet [3] ontology as a guide for extracting descrip-tors and structuring them into taxonomies thanks to the hierarchical organization already existing in WordNet. This choice leaves room for improvement by leveraging more complex techniques for both extracting descriptors from the free text and structuring these de-scriptors. For example, automatic approaches [17] or hierarchical sources such as Wikipedia [21, 7] could also be used. However, such research is out of the scope of this paper.

In total, we preprocessed the input archive into a sequence of 1,283,277 news events.Each news event is described on the 12 de-scriptive domains selected earlier and several descriptors from each domain can be used. We generated a taxonomy for each of these descriptive domains. As the domains of topic_code themes are limited in size and already categorized, corresponding taxonomies were manually generated. Descriptive domains extracted from the free text were generated using the WordNet ontology as shown in Figure 3. In a nutshell, for a given subject, e.g., business , its senses are used as intermediary nodes in the taxonomy. If there are several synonyms for one sense, e.g., { Commercial enterprise , Business enterprise }, one is arbitrarily chosen, e.g., Commercial enterprise . Specialized descriptors are then used as lower level descriptors.
The TSaR algorithm takes as input taxonomies H A , a generaliza-tion vector  X  , temporal window parameter w and a time sequence. The expected output is a more concise representation of the input sequence where the descriptive domains and the number of input events are reduced.

The quality of the algorithm can be evaluated by several differ-ent methods. First, we could evaluate the summarization algorithm w.r.t. the application that it is meant to support, e.g., Sequential Pattern Mining (SPM). In this case, the summary can be evaluated based on its ability to increase the quality of the output knowledge or increase the speed of the mining process (a preliminary study is proposed in our technical report).

Second, we can measure the semantic accuracy of summarized event descriptors in the summary produced. This accuracy can be evaluated thanks to the ratio  X  = |  X  | |  X  | ,where  X  is the set of de-scriptors in the raw data and  X  is the set of generalized descriptors in the output summary. The higher  X  , the better the semantic accu-racy. In addition to the semantic accuracy of the summary, we can also measure its temporal accuracy. For this purpose, given a time sequence s = { x i } , 1  X  i  X  n , temporal locality w&gt; temporal window W , we define a temporal rearrangement penalty cost for grouping an incoming event x i with a group y j  X  We denote this penalty cost C  X  ( x i ) . C  X  ( x i ) expresses the num-ber of rearrangements necessary on the timeline so that event x can be grouped with y i in window W . C  X  ( x i ) penalizes incoming events x i that are grouped with the older groups y i in W ;onthe other hand, if y i is the most recent group in W , no penalty occurs. C ( x i ) is formally defined as follows:  X   X   X   X   X   X   X 
The total temporal rearrangement penalty cost for summarizing s into s M , denoted C  X  ( s M ) ,isthen C  X  ( s M )= n i =1 penalty cost should then be normalized so that our results are com-parable. Hence, we choose to compute the relative temporal ac-curacy of the summaries. We normalize all temporal rearrange-ment cost by the maximum cost obtained in our experiments, i.e., C
Finally, we also evaluate the summarization algorithm on its nu-merosity reduction capability by reporting its compression ratio number of events in a time sequence seq . The higher CR , the bet-ter. We decide to use CR as it was also used by Kiernan and Terzi X  X  in [16] and we weight the CR with the summaries X  semantic and temporal accuracy,  X  and  X  , respectively.

We start by setting the generalization vector  X  = 1 , i.e., all descriptors are generalized once, and w  X  X  1 , 2 , 3 , 4 , 25 , 50 , 100 } where the maximum value w = 100 was chosen to represent a very strong temporal relaxation. Figure 4 gives the pro-cessing time of the TSaR algorithm with different temporal win-dows. For the sake of readability, we only display the plots for tem-poral windows w  X  X  1 , 5 , 100 } . These plots show that the TSaR algorithm is linear in the number of input events for temporal win-dows w of any size. In addition, we can observe that processing times are almost constant whatever the temporal window consid-ered. The slight variation observed in between different values of w have two complementary explanations. First, it is more costly to scan large windows during grouping. Second, a larger window w allows to maintain more groups in-memory and, so, requires less I/O operations for writing into storage.

We compute the CR of the summaries built with different gen-eralization vectors  X   X  X  0 , 1 , 2 , 3 } . The results are given in Figure 5. Note that the best compression ratio achieved with  X  = 0 is only 0.39. For a given temporal window w , relaxing the precision of the data by generalizing each descriptor once, twice or three times allows an average gain in compression capabilities of 46.15%, 94.87% and 133.33% respectively. In other other words, the compression ratio is approximatively doubled when increasing the generalization level. Another interesting observation is that for all  X  , the plots show that highest numerosity reduction is achieved with larger temporal windows while processing times are almost constant, as shown in Figure 4. This observation is very helpful from user view point for setting the summarization parameters  X  and w . In effect, as processing times are almost constant whatever the temporal window considered, the user needs only to express the desired precision in terms of (i) semantic accuracy for each de-scriptive domain and (ii) temporal locality without worrying about processing times.

Table 6 gives the semantic accuracy of the summaries produced and Figure 6 gives their temporal accuracy. Note in Table 6 that the number of descriptors in the raw data, i.e.,  X  = 0 , is 1208. When summarizing each descriptive domain once, i.e.,  X  = 1 the number of descriptors in the summaries drops to 50. This loss of semantic information can be explained by the fact the data was preprocessed using the WordNet ontology and the taxonomies were also generated from the WordNet ontology. Numerous descriptors extracted from the free text are in fact synonyms and are easily generalized into one common concept. Consequently, the concepts obtained with  X  = 1 should be considered as better descriptors than the raw descriptors. Hence, we choose to compute  X  using as baseline |  X  | =50 , as shown in Table 6. In this case, each time  X  is increased, the semantic accuracy is approximatively halved. This observation is consistent with our previous observation on the average compression gain.
 Figure 6 gives the relative temporal accuracy of each summary. Higher levels of generalization reduce the temporal accuracy of the summaries. This phenomenon is due to the fact that more generic descriptors allow more rearrangements for grouping events. How-ever, the temporal accuracy remains high, i.e.,  X  0 . 80 ,forsmall and medium sized temporal windows, i.e., w  X  25 . The temporal accuracy only deteriorates with large windows, i.e., w  X  25 result means that the analyst can achieve high compression ratios without sacrificing the temporal accuracy of the summaries.
However, guaranteeing the CR with TSaR is a difficult task, if not impossible, as its depend on the input parameters and on the data X  X  distribution. In addition, the analyst needs to weight the se-mantic and temporal accuracy he is ready to trade off for higher CR . Guaranteeing the CR becomes an optimization problem that requires the algorithm to self-tune the input parameters and take into account the analyst X  X  preferences.
Massive data sources appear as collections of time sequences of events in a number of domains such as medicine, the WWW, busi-ness or finance. A concise representation of these time sequences of events is desirable to support chronology-dependent applications. In this paper, we have introduced the concept of Time Sequence Summarization to transform time sequences of events into a more concise but informative form, using the data X  X  semantic and tempo-ral characteristics.
We propose a T ime S equence Summ aR ization (TSaR) algorithm that transforms a time sequence of events into a more reduced and concise time sequence of events using a generalization, grouping and concept formation principle. TSaR expresses input event de-scriptors at higher levels of abstraction using taxonomies and re-duces the size of time sequences by grouping similar events while preserving the overall chronology of events. The summary is com-puted in an incremental way and has an algorithmic complexity lin-ear with the number of input events. The output is directly under-standable by a human operator and can be used, without the need for desummarization , by chronology-dependent applications. One such application could be conventional mining algorithms to dis-cover high order knowledge . We have validated our algorithm by performing an extensive set of experiments on one year of Reuters X  X  financial news archives using our prototype implementation.
TSaR summaries are built using background knowledge in the form of taxonomies and the semantic and temporal precision of the output summary are controlled by user defined parameters. One direction in our future work is to render the generalization, group-ing and concept formation process more flexible. We would like to allow automatic tuning of the input parameters with regard to an objective to achieve, e.g., a compression ratio. The problem then turns into an interesting optimization issue between semantic accu-racy vs. standardization and time accuracy vs. compression. Also, much research in the temporal databases and datastreaming have worked under the assumption that analysts are more interested in recent data and desire high precision representations for new data items while older data can become obsolete. Works in temporal databases have introduced the concept of decay functions to model ageing data. We would also like to extend TSaR in future work by introducing decay functions to further reduce descriptive domains and data compression of older or obsolete information. We would like to thank Sherif Sakr, Juan Miguel Gomez and Themis Palpanas for their many helpful comments on earlier drafts of this paper. This work was supported by the ADAGE project, the Atlas-GRIM group, the University of Nantes, the CNRS and the region Pays de la Loire. [1] Google finance. http://finance.google.com. [2] Open calais. http://www.opencalais.com. [3] Wordnet. http://wordnet.princeton.edu/. [4] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. On Clustering [5] S. Babu, M. Garofalakis, and R. Rastogi. Spartan: A [6] V. Chandola and V. Kumar. Summarization -compressing [7] K. Chandramouli, E. Izquierdo, T. Kliegr, J. Nemrava, and [8] P. J. Denning. The locality principle. Commun. ACM , [9] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. Mining [10] J. Han, Y. Cai, and N. Cercone. Knowledge discovery in [11] J. Han and Y. Fu. Exploration of the power of [12] A. Hotho and G. Stumme. Conceptual clustering of text [13] H. Jagadish, R. Ng, B. Ooi, and A. Tung. Itcompress: an [14] H. V. Jagadish, J. Madar, and R. T. Ng. Semantic [15] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a [16] J. Kiernan and E. Terzi. Constructing comprehensive [17] K. Krishnapuram and K. Kummamuru. Automatic taxonomy [18] R. S. Michalski. Knowledge acquisition through conceptual [19] R. S. Michalski and R. Stepp. Learning from observation: [20] Q.-K. Pham, N. Mouaddib, and G. Raschia. Data stream [21] S. P. Ponzetto and M. Strube. Deriving a large scale [22] R. Saint-Paul, G. Raschia, and N. Mouaddib. General [23] J. Srivastava, R. Cooley, M. Deshpande, and P.-N. Tan. Web [24] Q. Wan and A. An. Compact transaction database for [25] J. Wang and Karypis. On efficiently summarizing categorical [26] T. Warren Liao. Clustering of time series data X  X  survey. [27] A. Wright, T. N. Ricciardi, and M. Zwickc. Application of [28] Y. Xiang, R. Jin, D. Fuhry, and F. F. Dragan. Succint [29] D. Zhang and K. Zhou. Discovering golden nuggets: data
