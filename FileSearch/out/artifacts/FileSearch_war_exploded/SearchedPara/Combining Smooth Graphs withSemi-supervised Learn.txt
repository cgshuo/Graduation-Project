 In many machine learning settings, there are only a few labeled examples but quite a number of unlabeled examples. Sin ce labeling often requires expensive human labors, unlabeled examples are much easier to get compared with labeled ones. The goal of the learner is to predict the labels of the unlabeled data points. This learning method is often called a semi-supervised or transductive learning. A typical application of this learning method is web categorization. To train such a system to automatically classify web pages, one would typically rely on manually labeled web pages and the manual work highly increases the label-ing cost. In contrast, the number of unlabeled web pages is large. To achieve good performance, the learning algorithm must take as much advantage of the unlabeled data as possible.

To make use of unlabeled data, two basic assumptions are employed directly or indirectly by semi-supervised learning. The first one is labels smoothness assumption, which supposes that neighboring data points tend to obtain the same label. The other one is called  X  X l uster assumption X  which assumes that data points on the same structure (typically referred as a cluster) are likely to have the same label, or that the decision boundary should lie in regions of low density. These two assumptions are true in many real world settings and also true in human intuition.

Based on these two assumptions, some methods are successfully applied in semi-supervised learning. Seeger[1] and Zhu[2] reviewed the existing techniques of semi-supervised learning. Gaussian function[3] uses the average of the indicat-ing function value f at neighboring points as the value of each unlabeled data point. In the consistency method[4], the indicating function values of unlabeled data points are smoothed by regularization method. A common aim of these methods is to make sure that neighboring points have approximately the same indicating function value. Corresponding to these methods, cluster kernels[6] are constructed such that induced distance is smaller for points in the same cluster and larger for points in different clusters. In [5], low density regions are identi-fied to separate clusters. The main differ ence between various semi-supervised learning algorithms lies in their way they realize these assumptions.
Although these two assumptions seem different, in many semi-supervised learning settings they share the same meaning that is a good indicating func-tion or decision function estimated by the semi-supervised learning algorithms should change slowly on the coherent structure aggregated by a quantity of data and greatly on the incoherent region. The assumptions also reflect the same data distribution requirement that the data of one cluster is usually distributed continuously in a relatively high density region, and low density regions usually separate different clusters[12].

However, real world data might not be distributed ideally and the assumptions might be damaged to some extent, especially in high dimension cases such as text data. In this case, the density of data varies largely, and low density regions may lie inside some clusters. Therefore, it is important to build a much smooth graph representation which highly reflects the nature of the data for graph-based semi-supervised learning just like Gaussian function method and the consistency method.

Most of these methods focus on the proces s of constructing the decision func-tion itself to smooth the graph. We attempt to smooth the graph explicitly first, and then label the unlabeled data using any other graph-based semi-supervised learning methods. To overcome the difficulties of bad data distribution, we build a smooth graph using Markov random walk model. Our method can combine local and global distribution of labeled and unlabeled data to make the graphical representation fit labels smoothness and cluster assumption better than original representation.

The paper is organized as follows: Sect ion 2 introduces Markov random walk model. Section 3 introduces some typical graph-based semi-supervised methods. In section 4, the paper explores the idea of how to build a novel graph represen-tation which takes into account the labels smoothness and cluster assumption. Section 5 presents the exp erimental results and finally we conclude this paper in Section 6. Many graph based methods which have done well in semi-supervised learning are a type of Markov random walk[11]. More formally, consider a point set points have desired labels { y 1 ,y 2 ,...,y l } X  X  1 ,...,c } and the remaining points are unlabeled. We use L and U to denote labeled and unlabeled data points respectively. Consider a graph G ( V, E )withnodes { v 1 ,v 2 ,...,v l } corresponding corresponding to the unlabeled points. An n  X  n symmetric weight matrix W on the edges of the graph is constructed and the weight of each undirected edge in the graph is defined as:
Where x i is a vector representing the i -th data point.  X  is a parameter for exponential weight. Thus, nearby point s are assigned large edge weights. The one-step transition probabilities p ij from data point i to j can be obtained directly from these weights:
We can get a transition matrix P =[ p ij ]. Such a random walk graph can be viewed as a Markov chain. For classification, data points are mapped to nodes in the graph. The transition probability can be seen as the similarity between data point pairs. The labeled data points are assumed as the ending states for the Markov random walk. We can evaluate the probabilities that the Markov process starts from an unlabeled data point and ends up in a labeled point after t steps. If the probabilities are quite high, the unlabeled point belongs to the same category as the labeled point. The probabilities should stay low if both points belong to two different categories. Such a Markov random walk graph representation is used in several semi-supervised learning methods [2][3][4][6][7]. These methods formulate the semi-supervised learning problem as a form of label propagation on the graph, where a node X  X  label propagates to neighboring nodes according to their proximity. In the process, the labels on the labeled data are fixed, and thus labeled data act as sources that push out labels through unlabeled data. 3.1 The Gaussian Field and Harmonic Function This method is present in [3]. The transition matrix P is calculated as above . Also define a l  X  c label matrix Y L with Y ij =1if x i is labeled as y i = j and Y ij =0otherwise. F is a n be interpreted as the probability distribution over labels. The Gaussian Field and Harmonic Function is as follows: 2. Clamp the labeled data F L = Y L . 3. Repeat from step 1 until F converges.

We are solely interested in F U and split P into labeled and unlabeled sub-matrices.
 It can be shown that: F
U is the unique fixed point and the solution to iterative algorithm. 3.2 The Local and Global Consistency Method The second label propagation algorithm is the local and global consistency method[4]. Define a n  X  c matrix Y with Y ij =1if x i is labeled as y i = j and Y ij = 0 otherwise. Clearly, Y is consistent with the initial labels. Iterate the following equation until convergence.

Let F  X  denote the limit of the sequence F ( n ) . Suppose F (0) = Y and by the iteration equation, we have:
The difference between the two typical label propagation methods is that in the local and global consistency method, e ach data point receive s the information from its neighbors and also retains its initial information in each repetition. The parameter  X  specifies the relative amount of the information from its neighbors and its initial label information. In label propagation we need a graph, represented by the transition matrix which reflects the structure of the graph. Data points usually propagate their labels to the ones nearby according to the possibilities on the edges, and the class boundaries will be pushed through high density regions and settled in low density gapes.

The smoothness here refers to the fact that the distribution of data changes slowly in one category and greatly between two separated categories. Based on Markov random walk graph, we propose the following framework which is consisted of two steps to construct a smooth graph under labels smoothness and cluster assumption in order to get a m ore accurate classi fication result. 4.1 Step 1: Graph Smoothing Step This step aims to change the local structure of the graph and avoid low density regions inside some clusters in order to make data points get higher probabilities to the ones nearby and walk denser regions more likely than sparser ones. As before, we can get a full connected graph G and its representation matrix W from labeled and unlabeled data points. Define D to be the diagonal matrix whose ( i, i )-element is the sum of W  X  X  i -th row, and compute the transition matrix P = D  X  1 / 2 WD  X  1 / 2 . We propose the following graph as the substitute for the basic graph representation.

If we map the graph G ( V, E ) into a Markov chain, the node set V is the state set I . It is easy to satisfy the following conditions: it is a finite-state Markov chain with no two disjoint closed sets and it is aperiodic. And we can have the for all i, j  X  I
Where  X &gt; 0and0 &lt; X &lt; 1. In particular, for all i, j  X  I
In equation (8)(9), p ( m ) ij is an element in matrix P m . Therefore, we can prove that as m becomes larger, p ( m ) ij gets closer to the fixed value  X  j .
From previous section, we know that larger probabilities indicate larger weights in the graph. As the number of m gets larger, p ( m ) ij between far points in the same cluster becomes larger, which means its corresponding weight gets larger. And in turn, it means the distance between the two points far away now becomes smaller. Furthermore, this kind of distance shortening is based on the density of the data. Distances between po ints in one cluster are shortened more but in different clusters are shortened less.

In addition, it should be noted that m here should not get too large. When p  X  X  X  , P m will become an uniform distribution for each point, which provides little information about the learning processing. 4.2 Step 2: Graph Clustering Step Graph clustering step aims to change the global structure of graph such that the distance between the points in differ ent clusters is much larger than that between the points in the same cluster. In other word, data points in the same cluster are grouped together in the new representation. The detail of the step is as follows: 1. Find r 1 ,r 2 ,...,r n , the eigenvectors of P  X  (chosen to be orthogonal to each 3. Compute  X  P = U X  U T and let  X  P ii = 0 which form a non-lazy random walk.
The smooth Markov walk random graph is represented by  X  P . We then conduct a new transition matrix for sem i-supervised classification.

The reason why this step may help implement the purpose of cluster assump-tion is described as follows. If there are k clusters in the dataset infinitely far apart from each other, it can be shown that the first k eigenvalues of the tran-sition matrix will be 1 and the ( k + 1)-th eigenvalue will be strictly less than other such that each data point is mapped to one of those k points depending on which cluster it belongs to. To illustrate this case, we consider an  X  X deal X  case in which all data points are in k clusters, and the size of each cluster is n are separated far apart from each other and ordered by the cluster that they are x i and x j in the same cluster. Note that W and P are block-diagonal: W = vectors are the union of the eigenvalues and eigenvectors of its blocks. Thus, we can construct  X  X by stacking P  X  X  first k largest eigenvector in columns:
Since 1 is a repeated eigenvalue of P , when we renormalize each row of  X  X to have unit length we could have picked k orthogonal vectors spanning the same subspace as  X  X  X  X  columns and defined them to be the first k eigenvectors. In other words, there are k mutually orthogonal points on the surface of the unit k -sphere around which P  X  X  rows will cluster.

In a general case, P  X  X  off-diagonal blocks are non-zero, but we can still make sure that similar to  X  X deal X  case under certain assumptions. According to Matrix perturbation[8] and spectral graph[9][10] theory, the gap of the k -th and ( k +1)-th eiganvlaue determines the stability of the eigenvectors of a matrix. The eigengap depends on how well each cluster is conn ected: the better they are connected, the larger the gap is. The subspace spanned by P  X  X  first k eigenvectors will only subject to a small change to P if and only if the difference between the k -th and ( k + 1)-th eigenvalues of P is large.
 D ( ii )  X  X  diagonal elements. There are four assumptions.
 Assumption A1. There exists  X &gt; 0sothat,forall i =1 , 2 ,...,k ,  X  ( i ) 2  X  1  X   X  . i = i 2 we have that: Assumption A3. For a fixed  X  2 &gt; 0, for every i  X  X  1 , 2 ,...,k } , j  X  S i ,we have: Assumption A4. There is a constant C&gt; 0sothatforevery i  X  X  1 , 2 ,...,n i } , j  X  X  1 , 2 ,...,k } ,wehave:
Informally, the assumptions A 1, A 2and A 3 require that all points must be connected to the points in the same cluster more than to the points in other clusters. And the last assumption A 4 indicates that no points in a cluster are  X  X oo much less X  connected than other poi nts in the same cluster. In a word, each we can have the following theorem: Theorem 1. Under assumptions A 1 , A 2 , A 3 and A 4 hold. Set  X  = k ( k  X  1)  X  1 + k X  2 2 .If  X &gt; (2 +
Thus, the rows of P will generate tight clusters around k well-separated points on the surface of the k -sphere. Moreover, these clust ers correspond exactly to the true clustering of the original data. Details for Matrix perturbation and spectral graph are given in [8][9][10]. To take advantage of our smooth graph representation, we employ a graph based method, i.e. consistency method presented by [4] to propagate labels to unlabeled data points. We compare our method to standard consistency method [4] and Gaussian function [3] using artificial and real world dataset. The performance of the algorithm is measured by the accu racy rate on the unlabeled points. 5.1 Toy Problem In this experiment we considered the t oy problem which is the switch or two-moon data mentioned in many semi-supervised learning papers. The transition matrix is formed using equation (1). From Fig.1 we can see that the classification result of consistency method with smooth graph is completely the same as the ideal case and Gaussian function. Furthermore, when  X   X  [0 . 2 , 0 . 7], the accuracy of classification using Gaussian function is great than 98%. But, the consistency method with smooth graph can enlarge the range of parameter  X  to [0 . 2 , 1 . 2] with k=2 and remain the accuracy at 100%.

The transition matrix of original graph and the smooth Markov walk random graph can be visualized in Fig.2.

There are 150 data points and two categories in the switch data set. The first 75 data points belongs to the same category. From Fig.2, we can see that the smooth Markov walk random graph present label smoothness and cluster assumptions better. The transition possibilities of two data points are extremely high if they are in the same cluster, on the contrary, the transition possibilities will be very low if the points are in the different clusters. 5.2 Image Recognition In this experiment, we design a classific ation task using colored teapots images from different views. There are 135 examples for each category with a total of 270 examples. Each example is represented by a 23028-dimension vector. We use the following weight function on the edges: From 1 to 5 labeled data points of each category are randomly selected to gener-ate the labeled data set and the rest of the data points in the categories form the unlabeled data points. The values of k and m are all set to 2. The experimental results of the classification are shown in Figure 2. The results show that con-sistency method with smooth graph outperforms other methods on the teapots image dataset. 5.3 Text Classification In this experiment, we investigate the task of text classification using 20 News-groups dataset. The articles are processed by the Rainbow software package with below options: (1) passing all words through the porter stemmer before counting them; (2) excluding words in the stoplist of SMART system; (3) skipping any headers; (4) ignoring words that occur in 5 or fewer documents. No further pre-processing step is taken. After removing the empty documents, the documents are normalized into a TF.IDF representation.
 We select the binary problems to compare the results: MS-Windows vs. Mac. For each category, we randomly label data points from 1 to 32 to generate the labeled data and the rest of the articles in the category remain to be the unlabeled data points. We performance ten runs for each dataset and count the average accurate rate of different methods. The parameter  X  of equation (1) is 0.7. The value of k and m is 8 and 2. The experiment results are shown in Figure 3. 5.4 Result Evaluation Difficulty to provide enough information about categories and rough data dis-tribution are the challenges to any semi-supervised learning algorithms. The process of constructing the smooth graph is to reshape the data distribution and make clusters separated clearly. There are two ste ps for the construction. The first step tends to compress the distance between any two points in the same cluster, and the second step enla rges the gap between two clusters. It is quite similar with the notion of clustering. With the help of smooth graph, label propagation on the graph is simplified to label each cluster. Since the clustering accuracy has no relation t o labeled data points, gra ph-based semi-supervised learning with smooth graph can still get high classification accuracy with quite few labeled data points. Our experiments show that smooth Markov walk random graph can improve the performance of graph-based semi-supervised methods. This paper presents an approach to create graphs by graph smoothing and clus-tering step. The smoothed graph can capture the nature of data distribution and reflect smoothness labels and cluster a ssumptions more accurately. We combine the graph-based semi-supervised classification methods with our smooth graph to reduce the errors and biases caused by rough transition probabilities. Exper-imental results show that the graph-base d semi-supervised classification meth-ods with our smooth graph outperform the same methods with original graph representation.
 The authors would like to thank Prof. Chunping Li for his invaluable advice on the paper.
