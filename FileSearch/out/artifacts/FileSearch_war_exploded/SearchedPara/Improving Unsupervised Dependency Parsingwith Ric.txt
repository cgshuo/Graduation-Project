 The last decade has seen great strides in statisti-cal natural language parsing. Supervised and semi-supervised methods now provide highly accurate parsers for a number of languages, but require train-ing from corpora hand-annotated with parse trees. Unfortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is valuable to study methods for parsing without re-quiring annotated sentences.

In this work, we focus on unsupervised depen-dency parsing. Our goal is to produce a directed graph of dependency relations (e.g. Figure 1) where each edge indicates a head-argument relation. Since the task is unsupervised, we are not given any ex-amples of correct dependency graphs and only take words and their parts of speech as input. Most of the recent work in this area (Smith, 2006; Co-hen et al., 2008) has focused on variants of the Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV was the first unsu-pervised dependency grammar induction system to achieve accuracy above a right-branching baseline. However, DMV is not able to capture some of the more complex aspects of language. Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Gram-mar (EVG) and its lexicalized extension (L-EVG). The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments a head takes but not their cat-egories. In contrast, EVG allows different distri-butions over arguments for different valence slots. L-EVG extends EVG by conditioning on lexical in-formation as well. This allows L-EVG to potentially capture subcategorizations. The downside of adding additional conditioning events is that we introduce data sparsity problems. Incorporating more valence and lexical information increases the number of pa-rameters to estimate. A common solution to data sparsity in supervised parsing is to add smoothing. We show that smoothing can be employed in an un-supervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-of-the-art results on this task. To our knowledge, this is the first time that grammars with differing levels of detail have been successfully combined for unsuper-vised dependency parsing.

A brief overview of the paper follows. In Section 2, we discuss the relevant background. Section 3 presents how we will extend DMV with additional features. We describe smoothing in an unsupervised context in Section 4. In Section 5, we discuss search issues. We present our experiments in Section 6 and conclude in Section 7. In this paper, the observed variables will be a corpus of n sentences of text s = s word s the set of all words as V speech as V t = t tribution over t . A dependency tree t acyclic graph whose nodes are the words in s graph has a single incoming edge for each word in each sentence, except one called the root of t edge from word i to word j means that word j is an argument of word i or alternatively, word i is the head of word j . Note that each word token may be the argument of at most one head, but a head may have several arguments.

If parse tree t sentence with no crossing edges, it is called projec-tive . Otherwise it is nonprojective . As in previous work, we restrict ourselves to projective dependency trees. The dependency models in this paper will be formulated as a particular kind of Probabilistic Con-text Free Grammar (PCFG), described below. 2.1 Tied Probabilistic Context Free Grammars In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same. This will allow us to make independence assumptions for smooth-ing purposes without losing information, by giving analogous rules the same probability.

Let G = ( N , T , S, R ,  X  ) be a Probabilistic Con-text Free Grammar with nonterminal symbols N , terminal symbols T , start symbol S  X  X  , set of productions R of the form N  X   X  , N  X  X  ,  X   X  (
N X  X  )  X  . Let R N indicate the subset of R whose left-hand sides are N .  X  is a vector of length |R| , in-dexed by productions N  X   X   X  X  .  X  the probability that N rewrites to  X  . We will let  X  indicate the subvector of  X  corresponding to R
A tied PCFG constrains a PCFG G with a tying relation, which is an equivalence relation over rules that satisfies the following properties: 1. Tied rules have the same probability. 2. Rules expanding the same nonterminal are 3. If N As we see below, we can estimate tied PCFGs using standard techniques. Clearly, the tying relation also defines an equivalence class over nonterminals. The tying relation allows us to formulate the distribu-tions over trees in terms of rule equivalence classes and nonterminal equivalence classes. Suppose  X  R is the set of rule equivalence classes and  X  N is the set of nonterminal equivalence classes. Since all rules in an equivalence class  X  r have the same probability (condition 1), and since all the nonterminals in an equivalence class  X  N  X   X  N have the same distribu-tion over rule equivalence classes (condition 1 and 3), we can define the set of rule equivalence classes  X  R  X  N , and a vector  X   X  of probabilities, indexed by rule equivalence classes  X  r  X   X  R .  X   X   X  vector of  X   X  associated with nonterminal equivalence class  X  N , indexed by  X  r  X   X  R  X  same equivalence class have the same probability, we have that for each r  X   X  r ,  X 
Let f ( t , r ) denote the number of times rule r ap-pears in tree t , and let f ( t ,  X  r ) = P see that the complete data likelihood is That is, the likelihood is a product of multinomi-als, one for each nonterminal equivalence class, and there are no constraints placed on the parameters of these multinomials besides being positive and sum-ming to one. This means that all the standard es-timation methods (e.g. Expectation Maximization, Variational Bayes) extend directly to tied PCFGs.
Maximum likelihood estimation provides a point estimate of  X   X  . However, often we want to incorpo-rate information about  X   X  by modeling its prior distri-bution. As a prior, for each  X  N  X   X  N we will specify a Dirichlet distribution over  X   X   X   X  Thus the prior over  X   X  is a product of Dirichlets,which is conjugate to the PCFG likelihood function (John-son et al., 2007). That is, the posterior P (  X   X  | s , is also a product of Dirichlets, also factoring into a Dirichlet for each nonterminal  X  N , where the param-eters  X   X  r is observed in tree t : We can see that  X  ber of times  X  r is observed prior to t .
 To make use of this prior, we use the Variational Bayes (VB) technique for PCFGs with Dirichlet Pri-ors presented by Kurihara and Sato (2004). VB es-timates a distribution over  X   X  . In contrast, Expec-tation Maximization estimates merely a point esti-the variational distribution, which approximates the KL divergence of P from Q . Minimizing the KL divergence, it turns out, is equivalent to maximiz-ing a lower bound F of the log marginal likelihood log P ( s |  X  ) . log P ( s |  X  )  X  The negative of the lower bound,  X  X  , is sometimes called the free energy .

As is typical in variational approaches, Kuri-hara and Sato (2004) make certain independence as-sumptions about the hidden variables in the vari-ational posterior, which will make estimating it simpler. It factors Q ( t ,  X   X  ) = Q ( t ) Q (  X   X  ) = Q Q (  X   X  ) , the estimate of the posterior distribution over parameters and Q ( t ) , the estimate of the posterior distribution over trees. Finding a local maximum of F is done via an alternating maximization of Q (  X   X  ) and Q ( t ) . Kurihara and Sato (2004) show that each Q (  X   X   X   X   X  2.2 Split-head Bilexical CFGs In the sections that follow, we frame various de-pendency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O ( m 3 ) time (Eis-ner and Blatz, 2007; Johnson, 2007) where m is the
In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a ter-minal symbol. For dependency grammars, these annotations correspond to words and/or parts-of-speech. Additionally, split-head bilexical CFGs re-quire that each word s in a split form by two terminals called its left part s stitutes the terminal symbols of the grammar. This split-head property relates to a particular type of de-pendency grammar in which the left and right depen-dents of a head are generated independently. Note that like CFGs, split-head bilexical CFGs can be made probabilistic. 2.3 Dependency Model with Valence The most successful recent work on dependency induction has focused on the Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV is a generative model in which the head of the sentence is generated and then each head recur-sively generates its left and right dependents. The arguments of head H in direction d are generated by repeatedly deciding whether to generate another new argument or to stop and then generating the argument if required. The probability of deciding whether to generate another argument is conditioned on H , d and whether this would be the first argument (this is the sense in which it models valence). When DMV generates an argument, the part-of-speech of that argument A is generated given H and d . The grammar schema for this model is shown in Figure 2. The first rule generates the root of the sen-tence. Note that these rules are for  X  H, A  X  V there is an instance of the first schema rule for each part-of-speech. Y right components. L sion given that we have not generated any arguments so far. L  X  ing one or more arguments. L 1 bution over left attachments. To extract dependency relations from these parse trees, we scan for attach-ment rules (e.g., L 1 A depends on H . The schema omits the rules for right arguments since they are symmetric. We show
Much of the extensions to this work have fo-cused on estimation procedures. Klein and Manning (2004) use Expectation Maximization to estimate the model parameters. Smith and Eisner (2005) and Smith (2006) investigate using Contrastive Estima-tion to estimate DMV. Contrastive Estimation max-imizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences. The results of this approach vary widely based on regularization and neighborhood, but often outperforms EM.
Smith (2006) also investigates two techniques for maximizing likelihood while incorporating the lo-cality bias encoded in the harmonic initializer for DMV. One technique, skewed deterministic anneal-ing, ameliorates the local maximum problem by flat-tening the likelihood and adding a bias towards the Klein and Manning initializer, which is decreased during learning. The second technique is structural annealing (Smith and Eisner, 2006; Smith, 2006) which penalizes long dependencies initially, grad-ually weakening the penalty during estimation. If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter se-lection on a held-out set without the use of gold de-pendencies does not perform as well.
 Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Nor-mal prior, which allows the model to capture correla-tions between different distributions. They initialize using the harmonic initializer of Klein and Manning (2004). They find that the Logistic Normal distri-bution performs much better than the Dirichlet with this initialization scheme.

Cohen and Smith (2009), investigate (concur-rently with our work) an extension of this, the Shared Logistic Normal prior, which allows differ-ent PCFG rule distributions to share components. They use this machinery to investigate smoothing the attachment distributions for (nouns/verbs), and for learning using multiple languages. DMV models the distribution over arguments iden-tically without regard to their order. Instead, we propose to distinguish the distribution over the argu-ment nearest the head from the distribution of sub-
Consider the following changes to the DMV grammar (results shown in Figure 4). First, we will introduce the rule L 2 sion of what argument to generate for positions not nearest to the head. Next, instead of having L  X  pand to H to nearest argument and stop) or L 2 nearest argument and continue). We call this the Ex-tended Valence Grammar (EVG).

As a concrete example, consider the phrase  X  X he big hungry dog X  (Figure 5). We would expect that distribution over the nearest left argument for  X  X og X  to be different than farther left arguments. The fig-ure shows that EVG allows these two distributions to be different (nonterminals L 2 DMV forces them to be equivalent (both use L 1 the nonterminal). 3.1 Lexicalization All of the probabilistic models discussed thus far have incorporated only part-of-speech information (see Footnote 2). In supervised parsing of both de-pendencies and constituency, lexical information is critical (Collins, 1999). We incorporate lexical in-formation into EVG (henceforth L-EVG) by extend-ing the distributions over argument parts-of-speech A to condition on the head word h in addition to the head part-of-speech H , direction d and argument po-sition v . The argument word a distribution is merely conditioned on part-of-speech A ; we leave refining this model to future work.

In order to incorporate lexicalization, we extend the EVG CFG to allow the nonterminals to be anno-tated with both the word and part-of-speech of the head. We first remove the old rules Y for each H  X  V minal which is annotated with a part-of-speech as also annotated with its head, with a single excep-tion: Y H  X  V  X  , h  X  V w , and the rules Y H  X  Y H,h and Y responds to selecting the word, given its part-of-speech. In supervised estimation one common smoothing technique is linear interpolation , (Jelinek, 1997). This section explains how linear interpolation can be represented using a PCFG with tied rule proba-bilities, and how one might estimate smoothing pa-rameters in an unsupervised framework.

In many probabilistic models it is common to esti-mate the distribution of some event x conditioned on some set of context information P ( x | N by smoothing it with less complicated condi-tional distributions. Using linear interpolation we model P ( x | N age of two distributions  X   X  P the conditioning event N
In a PCFG a nonterminal N can encode a collec-tion of conditioning events N termines a distribution conditioned on N over events represented by the rules r  X  X  example, in EVG the nonterminal L 1 three separate pieces of conditioning information: the direction d = left , the head part-of-speech H = NN , and the argument position v = 0 ;  X  ating JJ as the first left argument of NN . Sup-pose in EVG we are interested in smoothing P ( A | d, H, v ) with a component that excludes the head conditioning event. Using linear interpolation, this would be: P ( A | d, H, v ) =  X  We will estimate PCFG rules with linearly interpo-lated probabilities by creating a tied PCFG which is extended by adding rules that select between the main distribution P and also rules that correspond to draws from those distributions. We will make use of tied rule proba-bilities to make the independence assumption in the backoff distribution.

We still use the original grammar to parse the sen-tence. However, we estimate the parameters in the extended grammar and then translate them back into the original grammar for parsing.

More formally, suppose B X  X  is a set of non-terminals (called the backoff set) with conditioning events N conditioning event N same cardinality. If G is our model X  X  PCFG, we can define a new tied PCFG G  X  = ( N  X  , T , S, R  X  ,  X  ) , where N  X  = N X  N b  X  | N  X  X  ,  X   X  X  1 , 2 } , meaning for each nonterminal N in the backoff set we add two nonterminals N b 1 , N b 2 represent-ing each distribution P set R  X  = (  X  rule set R  X  to use; and for N  X  X  and  X   X  X  1 , 2 } , R draw from distribution P R specify a tying relation between the rules in R  X  and R  X  has the effect of making an independence assump-tion about P ing event N each time a nonterminal N b 2 is rewritten.

For example, in EVG to smooth P ( A = DT | d = left , H = NN , v = 0) with P d = left , v = 0) we define the backoff set to be L 1 define the tying relation to form rule equivalence classes by the argument they generate, i.e. for each argument A  X  V n
We can see that in grammar G  X  each N  X  X  even-tually ends up rewriting to one of N  X  X  expansions  X  in
G . There are two indirect paths, one through N b 1 and one through N b 2 . Thus this defines the proba-bility of N  X   X  in G ,  X  rewriting N as  X  in G  X  via N b 1 and N b 2 . That is: The example in Figure 6 shows the probability that L
Typically when smoothing we need to incorporate the prior knowledge that conditioning events that have been seen fewer times should be more strongly smoothed. We accomplish this by setting the Dirich-let hyperparameters for each N  X  N b 1 , N  X  N b 2 decision to ( K, 2 K ) , where K = |R ber of rewrite rules for A . This ensures that the model will only start to ignore the backoff distribu-tion after having seen a sufficiently large number of 4.1 Smoothed Dependency Models Our first experiments examine smoothing the dis-tributions over an argument in the DMV and EVG models. In DMV we smooth the probability of argu-ment A given head part-of-speech H and direction d with a distribution that ignores H . In EVG, which conditions on H , d and argument position v we back off two ways. The first is to ignore v and use back-off conditioning event H, d . This yields a backoff distribution with the same conditioning information as the argument distribution from DMV. We call this EVG smoothed-skip-val.

The second possibility is to have the backoff distribution ignore the head part-of-speech H and use backoff conditioning event v, d . This assumes that arguments share a common distribution across heads. We call this EVG smoothed-skip-head. As we see below, backing off by ignoring the part-of-speech of the head H worked better than ignoring the argument position v .

For L-EVG we smooth the argument part-of-speech distribution (conditioned on the head word) with the unlexicalized EVG smoothed-skip-head model. Klein and Manning (2004) strongly emphasize the importance of smart initialization in getting good performance from DMV. The likelihood function is full of local maxima and different initial parameter values yield vastly different quality solutions. They offer what they call a  X  X armonic initializer X  which initializes the attachment probabilities to favor ar-guments that appear more closely in the data. This starts EM in a state preferring shorter attachments.
Since our goal is to expand the model to incor-porate lexical information, we want an initializa-tion scheme which does not depend on the details of DMV. The method we use is to create M sets of B random initial settings and to run VB some small number of iterations (40 in all our experiments) for each initial setting. For each of the M sets, the model with the best free energy of the B runs is then run out until convergence (as measured by like-lihood of a held-out data set); the other models are pruned away. In this paper we use B = 20 and M = 50 .

For the b th setting, we draw a random sample from the prior  X   X  ( b ) . We set the initial Q ( t ) = P Expectation-Maximization E-Step. Q (  X   X  ) is then ini-tialized using the standard VB M-step.

For the Lexicalized-EVG, we modify this proce-dure slightly, by first running M B smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG dis-tribution is initialized from its corresponding EVG distribution. The new P ( A | h, H, d, v ) distributions are set initially to their corresponding P ( A | H, d, v ) values. We trained on the standard Penn Treebank WSJ cor-pus (Marcus et al., 1993). Following Klein and Man-ning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out de-velopment corpus, and present results evaluated on section 23. The models were all trained using Varia-tional Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in us-ing the mean of the variational posterior Dirichlets as a point estimate  X   X   X  . For the unsmoothed models we decode by selecting the Viterbi parse given  X   X   X  , or argmax t P ( t | s,  X   X   X  ) .

For the smoothed models we find the Viterbi parse of the unsmoothed CFG, but use the smoothed prob-abilities. We evaluate against the gold standard dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003). We mea-sure the percent accuracy of the directed dependency edges. For the lexicalized model, we replaced all words that were seen fewer than 100 times with  X  X NK. X  We ran each of our systems 10 times, and report the average directed accuracy achieved. The results are shown in Table 1. We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).
Looking at Table 1, we can first of all see the benefit of randomized initialization over the har-monic initializer for DMV. We can also see a large gain by adding smoothing to DMV, topping even the logistic normal prior. The unsmoothed EVG ac-tually performs worse than unsmoothed DMV, but both smoothed versions improve even on smoothed DMV. Adding lexical information (L-EVG) yields a moderate further improvement.

As the greatest improvement comes from moving to model EVG smoothed-skip-head, we show in Ta-ble 2 the most probable arguments for each val, dir , using the mean of the appropriate variational Dirich-let. For d = right, v = 1 , P ( A | v, d ) largely seems to acts as a way of grouping together various verb types, while for d = lef t, v = 0 the model finds that nouns tend to act as the closest left argument. We present a smoothing technique for unsupervised PCFG estimation which allows us to explore more sophisticated dependency grammars. Our method combines linear interpolation with a Bayesian prior that ensures the backoff distribution receives proba-bility mass. Estimating the smoothed model requires running the standard Variational Bayes on an ex-tended PCFG. We used this technique to estimate a series of dependency grammars which extend DMV with additional valence and lexical information. We found that both were helpful in learning English de-pendency grammars. Our L-EVG model gives the best reported accuracy to date on the WSJ10 corpus.
Future work includes using lexical information more deeply in the model by conditioning argument words and valence on the lexical head. We suspect that successfully doing so will require using much larger datasets. We would also like to explore us-ing our smoothing technique in other models such as HMMs. For instance, we could do unsupervised HMM part-of-speech induction by smooth a tritag model with a bitag model. Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags. This research is based upon work supported by National Science Foundation grants 0544127 and 0631667 and DARPA GALE contract HR0011-06-2-0001. We thank members of BLLIP for their feed-back.
