 Xiaoyan Shen, Junliang Chen, Xiangwu Meng, Yujie Zhang, and Chuanchang Liu Finding related pages is not an easy job. Many studies have been conducted to evalu-for finding related pages are divided into three categories. 
The first category is content-based algorithms which compute page similarities completely based on page contents [1-2]. However, these algorithms ignore the avail-content-based methods is that they generally require large storage and long computing time, so these methods are not easily applied to large corpus. 
The second category is link-based algor ithms, which use the connectivity informa-tion to evaluate the similarity between web pages [3-7]. Though link-based algorithms are in general more resistant against spamming, they ignore the differences of hyper-links within a page and treat a web page as a single node. However, web pages can be subdivided into several smaller sections and each of which may contain some useful tween links in different blocks. 
The third category is anchor-based algor ithms which use the similarity between texts near anchors, denoted as the anchor window, to find related pages [8-9]. Using anchor window, which often has a topic summary of the pointed target document, can avoid the expensive time and space cost in the content analysis process. revise the co-citation algorithm by only considering link pairs in the same block. We dle large number of pages efficiently. 
This paper is organized as follows: In Section 2, we introduce the details of how to find related pages and the parallel implementation of the algorithm by using MapRe-MapReduce model and use manual evaluation strategies to compare the experiment results of proposed algorithm with traditiona l co-citation algorithm. Finally, we con-clude with discussion and future work in Section 4. The most simple yet powerful non-vision based methods was proposed in [10]. A list of partitioning tags was used to identify the boundary between HTML sections. Though their method is clear and performing well, it can not be applied directly by us. The granularity of blocks is too small to get enough co-citation link pairs. 
For the reasons above mentioned, we revise the segmenting algorithm in [10] to bet-page. The indexes of links in the block represent their relative order in original page. initial blocks obtained from [10] with text information is called as a section here. 
In order to avoid generating many trivial blocks, we should merge several neighbor-tion are extracted to construct a block. 
For arbitrary two neighbor sections A and B, the following rules are used to decide whether two sections can be merged. Each of the rules is used, if and only if previous rules are not satisfied. section is greater than a threshold, and the number of links in the block is larger than a threshold (we set it to 2), we call the type of the section is a link section, otherwise, is color attribute of the CSS style in them is the same, they are merged. Normally, B. A and B can not be merged, if one of the sections has boldfaced texts. them are different. We combine these factors into a formula to quantify this difference. longed to different topics. 
D. A and B can not be merged, if the number of words in both two blocks is larger than 10. 
E. When the type of A and B is not the same, but they can be merged into a section with type is identical to the type of original one which has more words than the other, A and B are merged. F. If A and B can not comply with each of the above rules, they can not be merged. 
In order to validate the proposed segmenting strategies, 56 pages sampled ran-domly from the experimental corpus. The links in each page are segmented into generated using the algorithm, then the total number of blocks is expressed as: For ease of analysis, define i BM as the set of blocks generated manually. 
E-precision is the ratio of the number of blocks the same with the blocks generated manually, to NB. where l is a link in a block. 
The E-precison of original algorithm and our algorithm are 9.6% and 29.7%, re-spectively. It appears that original algorithm generates more blocks. Before describing how to generate related pages, the following terms are defined. Definition 1. Two pages p 1 and p 2 are block co-cited once if they are both pointed by contains both p 1 and p 2 , only the first is valid avoiding p devoting too high similarity directly under the leftmost top-level domain, denoted as TSD(p). For example, TSD(www.yahoo.cn) is  X  X ahoo.cn X , since  X  X n X  is the country code top-level domain governed by China Internet Network Information Center. The flow of finding related pages from takes all blocks as input and includes three key steps: 
The similarity is computed as follow: words set of the anchor texts s , expecting all stop words with highest IDF. the threshold to differentiate near neighbors from far neighbors. (3) Merging related pairs. For illustration clearly, we first define the following concepts. Definition 3. For a given top sub-domain name d, arbitrary two pages 1 p and 2 p , the domain similarity from 1 p to 2 p is denoted as: 12 () d DS p p  X  and vice versa. 
DS p p  X  is computed by summing up all 12 (, ) pp Spp &lt;&gt; generated from page p who belong to d. In order to avoid template links making many noises in our will be added to 12 () d DS p p  X  , and so as 21 () d DS p p  X  . In order to limit some malicious top sub-domain names provide too many similarities between two pages, called domain similarity threshold. Finally, Definition 4. For arbitrary two pages 1 p and 2 p , the total similarity from p1 to p2 is calculated as: where 2 p in is the number of blocks which contain page 2 p . 
Now we discuss the parallel implementation of the steps mentioned in section 3, which is inspired by MapReduce [12]. 
Figure 1 shows the MapReduce flow of find related pages from the blocks of pages. routine calculates the similarity of each rela ted pairs according to (4). For a given key p outputted by Mapper1, the corresponding value is a quaternion, namely, &lt;&lt;&gt;&gt; .Besides these items, we also output an In Sort1 operation, the comparison method of the URL keys simply compares method to compare the output quad values of mapper1 is the sequential comparison, parison result of them, otherwise, we will compare the second components, and so on. following auxiliary value are also grouped by their four components in order. 
In Reducer1 operation, the number of auxiliary quaternion corresponding to the the () ji TS p p  X  is computed simply by summing all similarities with the limitation showed in (10). At last p with its total similarity to pm1 is outputted as a key-value in Reducer1. 
The Mapper2 and Reducer2 operation finish generating the related pages set for results. We also limit the minimum value of the total similarity to T. The values of the algorithm parameters mentioned in Section 2 are determined based Number of related pages M and Total similarity threshold T are 80, 8, 9, 10, 15, 4. 
In order to implement the algorithm, Hadoop (http://lucene.apache.org/hadoop/) is selected as the MapReduce platform, CWT200G (http://www.cwirf.org) is used as the pages corpus. 
The experiment is conducted on 8 HP servers each with 4GB memory and two dual 1.8 GHz AMD processors. All 37482913 pages in CWT200G are processed and ap-proximately 270MB related pages database are generated. The database is divided disk. We set the mapper and reducer numbers to 64 based on the suggestions in [12]. The time required to find related pages on 8 servers are measured by using the input set with different size from 5M to 35M pages. It is observed in Figure 2 that the total running time is linear with the number of pages. The reason is that the I/O time cost is significant in our parallel implementation. 
We also implement traditio n co-citation algorithm by using Hadoop. In order to tion of the results of two algorithms on 28 different pages. 
For each test page, we combine and shuffle the top 15 related page results of both two algorithms before presenting them to our human evaluators. Each of the 7 evalua-following criteria: Score 0 stands for page could not be accessed or is totally unrelated to the test page; Score 1 stands for page belongs to a similar but not identical topic as page. 
The performance of retrieving related pages is investigated through average R-then the sum of the average score is divided by the number of test pages, namely: where N is the number of test pages, the i-th relate page result of the j-th test page. traditional co-citation algorithm over all the R values. In this paper, the block co-citation algorithm based on HTML segmentation technolo-gies is introduced to find related pages for a given page, and demonstrate one imple-mentation of this method using MapReduce framework to achieve scalability and same top sub-domain name and the value of domain similarity, the influence of tem-distorted by malicious hyperlinks in malicious domains. Experiments conducted on a large corpus suggest that block co-citation algorithm outperforms traditional co-pages service. 
