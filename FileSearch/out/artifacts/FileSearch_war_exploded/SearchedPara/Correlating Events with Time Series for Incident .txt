 As online services have more and more popular, incident diagnosis has emerged as a critical task in minimizing the service downtime and ensuring high quality of the services provided. For most online services, incident diagnosis is mainly conducted by analyzing a large amount of teleme-try data collected from the services at runtime. Time series data and event sequence data are two major types of teleme-try data. Techniques of correlation analysis are important tools that are widely used by engineers for data-driven in-cident diagnosis. Despite their importance, there has been little previous work addressing the correlation between two types of heterogeneous data for incident diagnosis: continu-ous time series data and temporal event data. In this paper, we propose an approach to evaluate the correlation between time series data and event data. Our approach is capable of discovering three important aspects of event-timeseries correlation in the context of incident diagnosis: existence of correlation, temporal order, and monotonic effect. Our ex-perimental results on simulation data sets and two real data sets demonstrate the effectiveness of the algorithm. Correlation, Incident Diagnosis, Two-sample Problem
The research in this paper is motivated by a real-world application in incident investigation of online services[20]. Unlike traditional shrink-wrapped software, online service systems are designed to run continuously and be available  X  This work is conducted during his internship at Microsoft Research.
 all time(24x7). However, during the operation of an on-line service, live-site service incidents (unplanned interrup-tion/outage of the service) are often unavoidable, and can lead to significant economic loss or other serious consequences. For example, many reputable online services such as those provided by Amazon, Google, and Citrix have experienced live-site incidents during the past couple of years [1, 14]. In order to minimize service downtime caused by service incidents, much effort has been invested in improving the efficiency of service-incident diagnosis.

Diagnosis of service incidents mainly depends on analyz-ing the telemetry data collected at the runtime of the service [20]. Unlike a desktop application, the back-end system of an online service is often a large-scale distributed system. It is usually impractical to attach a debugger to the service to investigate service incidents. In most online systems, data analysis is the only way for engineers to diagnose service incidents. Telemetry data such as service-level logs, perfor-mance counters, and machine/process/service-level events can often provide enough information for incident diagnosis. Most telemetry data can be grouped into two categories: continuous time series data and temporal event data. A time series is a sequence of real-valued data points, mea-sured typically at successive time points equally spaced with a uniform time interval. A typical example of time series in an online service system is the performance counter of CPU usage. An event sequence in an online service is used to record the occurrences of a specific software message indi-cating that something has happened in the system, e.g., an event sequence of  X  X ut of memory X  contains events of  X  X ut of memory X , which occur when there is not enough memory in the system.

Among all data-driven techniques for incident diagnosis, correlation analysis between telemetry data and system states plays a very important role [7] because correlation relation-ships often provide hints for causality analysis. Although the correlated metrics may not exactly be the root causes of incidents, they could be intermediate useful information that pinpoints the root causes. In practice, engineers usu-ally start their incident investigation by searching for a small set of system metrics that are correlated to the Key Perfor-mance Indicators (KPI) of the service (Here, a KPI is an indicator that can be used to manifest system X  X  health state, e.g., service availability or request latency) [9]. task )
Because of the importance of correlation analysis, a lot of tools have been built to analyze the correlation between time series [32] and the correlation between system events [18, 17, 2]. However, how to evaluate the correlation between a time series and an event sequence is still not well studied. Be-cause of the heterogeneous property of these two types of data, traditional correlation analysis, such as Pearson cor-relation and Spearman correlation [8], often cannot provide satisfying results. Furthermore, in a large-scale system, an occurrence of an event may be related to a change of a time series during a time period rather than a point-to-point cor-responding relationship in the traditional correlation analy-sis techniques.

In this paper, we propose an approach to evaluate the correlation between a time series and an event sequence. Motivated by the real requirement of incident management, our correlation technique tries to answer the following three questions: a. Are they correlated? b. What is the delay relationship between them? c. What is the monotonic effect between them (refer to Section 2 for the definition of mono-tonic effect)? In the context of service incident management, the answers to these questions can provide rich information to engineers for incident diagnosis. We formulate the cor-relation problem as a two-sample problem [21], and then use the nearest neighbors method to evaluate the existence of the correlation. At the same time, we identify temporal relationships and monotonic effects by analyzing the rela-tionships between sub-series before and after the occurrence of each event(refer to Section 2).

The contributions of this paper are as follows: 1. Motivated by real applications, we articulate the corre-2. We formulate the correlation problem as a two-sample 3. The experiments on simulated data and real data from
The rest of the paper is organized as follows: The problem statement and formulation are introduced in Section 2. We propose our approach in Section 3. The Empirical evalua-tion is shown in Section 4. In Section 5, we introduce some related works. Finally, we conclude our work in Section 6.
In statistics, correlation is any statistical relationship be-tween two random variables in which random variables do not satisfy a mathematical condition of probabilistic inde-pendence. Let us give an example first to demonstrate the correlation between event data and the time series data. Fig. 2 shows two types of events (e.g., starting a disk inten-sive program and a CPU intensive program ) and a time se-ries (e.g., a performance counter of CPU Usage ). Every time a CPU intensive program starts, the performance counter of CPU Usage will increase significantly. On the other hand, the value of CPU Usage does not have any specific pattern related to the starting events of a disk intensive program . As a result, we can say that the starting events of the CPU in-tensive program and the performance counter of CPU Usage have a correlating relationship, while there is no correlation between the starting events of a disk intensive program and the CPU Usage . Such kind of correlation analysis is an im-portant step for incident diagnosis [9, 6].

In incident diagnosis scenarios, engineers typically study the following three aspects of the correlation among telemet-ric data when they are investigating service incidents.
Given an event sequence (or event type) denoted as E , the timestamps of the events are denoted as T E = ( t 1 ,t 2 ,  X  X  X  ,t where n is the number of events that happened. A time se-ries, denoted as S = ( s 1 ,s 2 ,  X  X  X  ,s m ), where m is the number of points in the time series. Unlike the timestamps of an event sequence, the timestamps of a time series, denoted t ( s i ) = t ( s i  X  1 ) +  X  , where  X  is the sampling interval.
Here, we assume that each time series has an even sam-pling interval, which is true in many applications. In our analysis, we also assume that the effect of an event on a time series only lasts a certain time interval that is very small compared to the total lasting time of the time series. This assumption is valid in a real-world online service sys-tem which is in a normal and healthy state most of the time. Any incident will be resolved soon after its occurrence by the system operation team.

As mentioned before, if an event type E and a time se-ries S have a correlation relationship, every time an event E happens, there is a corresponding change of the time series S . The potential temporal relationships between an occur-rence of an event e i  X  E and its corresponding change of the time series are illustrated in Fig. 2. Here, each change is represented as a sub-series of S .

Supposing ` rear k ( S,e i ) denotes a sub-series of S after e happens with length of k , and ` front k ( S,e i ) denotes a sub-series of S before the e i happens. Intuitively, if event E does not correlate to time series S , then both ` rear k ( S,e ` k ( S,e i ) ( i = 1  X  X  X  n ) are not related to the occurrences of e . In other words, the sub-series  X  front = { ` front k ( S,e tically different from other sub-series with length of k (de-noted as  X  ) that are randomly sampled from S . On the other hand, if there is a correlation relationship between E and S , the sub-series  X  front = { ` front k ( S,e i ) ,i = 1  X  X  X  n } different from the randomly sampled sub-series  X  .

Based on this information, we have the following defini-tions.

Definition 1. An event sequence E and a time series S are correlated and E often occurs after the changes of S (de-noted as S  X  E ), if and only if the probabilistic distribution randomly sampled sub-series  X  .
 Definition 2. An event sequence E and a time series S are correlated and E often occurs before the changes of Figure 2: Example of front sub-series and rear-series S (denoted as E  X  S ), if and only if the probabilistic dis-from the randomly sampled sub-series  X  and the probabilistic different from  X  .

Definition 3. An event sequence E and a time series S are correlated (denoted as E  X  S ), if there is a relationship of E  X  S or S  X  E .

Definition 4. If E  X  S (or S  X  E ) and the event oc-currences of E are related to significant value-increases of S , we denote the correlation as E +  X  S (or S +  X  E ). If E  X  S (or S  X  E ) and the event occurrences of E are related to significant value-decrease of S , we denote the cor-relation as E  X   X  S (or S  X   X  E ).
Based on the above definitions, our correlation analysis problem can be transformed to a multivariate two-sample hypothesis-testing problem. Two-sample tests are commonly used when checking whether two samples come from the same underlying distribution, which is assumed to be un-known. In our context, one sample is the set  X  front  X  rear , the other sample is  X  that is the set of sub-series randomly sampled from S . Each data point is actually a sub-series, which can be represented as a vector of k dimen-sions. Therefore, the original problem becomes a multivari-ate two-sample test.
 Here, we use one set  X  front and  X  as our example. Let  X  front and  X  be independent random samples generated from unknown distributions F and G , respectively. The dis-tributions are assumed to be absolutely continuous with re-spect to Lebesgue measure. Their densities are denoted as f and g , respectively. The hypotheses of the two-sample test can be stated as follows: If H 1 is true, that means the probabilistic distribution of  X  front is statistically different from the randomly sampled sub-series  X  . In addition, as Definition 1, time series S and event E are correlated. Otherwise, if H 0 is true, then time series S and event E may not be correlated.
In this section, we propose a nearest-neighbor based method algorithm to analyze the three aspects of correlation, and then analyze the complexity of the proposed approach.
Multivariate two-sample tests have been of continuous in-terest to the statistics community. Several different algo-rithms of this kind have been proposed [10, 21, 28]. To uti-lize some structure based distance measures such as DTW [4] or DTW-D [5], we apply the nearest neighbor statistic based method [28] in our scenario. It is worth pointing out that, other two-sample tests algorithms [10, 21] can also be used in our method.

In this subsection, we take  X  front as an example. Given the two samples  X  front = { ` front k ( S,e i ) ,i = 1  X  X  X  n } and  X  = {  X  1 , X  2 ,  X  X  X  , X   X  n } , and the pooled sample by Z =  X   X   X  , we label the pooled sample as Z 1 ,  X  X  X  ,Z p with p = n +  X  n where For a finite set of sub-series A and a sub-series x  X  A , let NN r ( x,A ) denote the r -th nearest neighbor of x within the set { A \ x } . For two mutually exclusive subsets A 1 , A a point x  X  A , we define an indicator function:
I r ( x,A 1 ,A 2 ) = The function I r ( x,A 1 ,A 2 ) indicates whether x and its r -th nearest neighbor in A \ x belong to the same subset.
In our problem, the nearest-neighbor based tests rely on the following quantity and its generalizations:
Here, p = | n +  X  n | is the size of the sample. The test statistic T r,p is the proportion of pairs containing two sub-series from the same sample, among all pairs formed by a sample sub-series and one of its nearest neighbors in the pooled sample Z . Intuitively T r,p is small under the null hypothesis when the two samples are mixed well, while T r,p is large when the two underlying distributions are different.
Then, as in [28], when p is large enough, ( pr ) 1 / 2  X  ) / X  r obeys a standard Gaussian distribution with the fol-lowing parameters: and where  X  1 = n/p and  X  2 =  X  n/p . The traditional Gaus-sian distribution test,  X  front and  X  are significantly differ-P = 0 . 025 (or  X  = 2 . 58 for P = 0 . 001). For more about the hypothesis test in a Gaussian distribution, please refer to [15].
According to the definitions in Section 2, we can summa-rize that if event E and time series S are correlated, the fol-lowing statement must be true: the front sub-series  X  front Figure 3: Example of temporal order, CPU intensive program  X  CPU usage and CPU usage  X  query alert Figure 4: Effect examples: loading data task +  X  mem-ory and program exiting  X   X  memory and randomly sampled sub-series  X  are statistically differ-ent; or the rear sub-series  X  rear and randomly sampled sub-series  X  is statistically different. In our approach, we use the above nearest-neighbor method to determine whether two sets of sub-series have an identical distribution.
If the front sub-series  X  front and randomly sampled sub-series  X  are statistically different, then S  X  E . Otherwise, if the rear sub-series  X  rear and  X  is statistically different, then E  X  S . Fig. 3 shows an example of Correlation Temporal Order. From Fig. 3, we can see CPU Intensive Program  X  CPU Usage, and CPU Usage  X  SQL Query Alert.
As explained in Definition 4, in order to determine the monotonic effects such as E +  X  S (or E  X   X  S ) of correlation, we need to check whether there exists a significant value increase (or decrease) on the time series S after an event E happens. Similarly, it can also be formulated as a problem of statistical hypotheses testing. In this research, we use t  X  test [15] to check whether there is a significant value increase (or decrease) from  X  front to  X  rear .

Here, the t score between  X  front and  X  rear can be calcu-lated by the following equation: Algorithm 1: The Overall Algorithm Input : Event E = ( e 1 ,e 2 ,...,e n ), and Time Series
Output : The correlation flag C , the direction D , and 2 Initialize  X  ; 3 Initialize R = false , D = NULL , T = NULL ; 4 Normalize each ` front k ( S,e i ) and ` rear k ( S,e i 5 Test  X  front and  X  using Nearest Neighbors Method.
The result is denoted as D f .; 6 Test  X  rear and  X  using Nearest Neighbors Method.
The result is denoted as D r .; 7 if ( D r == true &amp;&amp; D f == false ) then 8 R = true .; 9 Calculate t score using Equation (8).; 10 if ( t score &gt;  X  ) then 11 T = E  X   X  S .; 12 else if ( t score &lt;  X   X  ) then 13 T = E +  X  S .; 14 else if ( D r == false &amp;&amp; D f == true ) || ( D r == true &amp;&amp; D f == true ) then 15 R = true .; 16 Calculate t score using Equation (8).; 17 if ( t score &gt;  X  ) then 18 T = S  X   X  E .; 19 else if ( t score &lt;  X   X  ) then 20 T = S +  X  E .; 21 Out put R , D and T ; 22 Algorithm End.  X  research, n 1 = n 2 = n , and n is the event number in E , thus Equation 7 can be reduced to:
Then, if t score &gt;  X  , we have a negative monotonic effect (e.g, E  X   X  S or S  X   X  E ); and if t score &lt;  X   X  , we have a positive monotonic effect (e.g., E +  X  S or S +  X  E ). Here,  X  = 1 . 96 for P = 0 . 025 (or  X  = 2 . 58 for P = 0 . 001) [15].
At the same time, for a given  X  ,if | t score | &lt;  X  , we cannot determine the effect type with a high level of confidence. However, such situations are seldom found in the real sce-narios of incident diagnosis. Most services have be designed with a large certain margin to fault-tolerate. When a ser-vice incident is detected, the corresponding time-series must have a significant difference from its normal behavior (i.e., t score &gt;  X  ) [9]. The overall algorithm is then summarized as Algorithm 1. This algorithm implements the methods we have introduced in the above subsections. It performs two two-sample tests based on the sub-series  X  front ,  X  rear , and  X  . The output of this algorithm contains all three aspects of the correlation
Confident Coefficient between a time series and an event sequence. In this section, we discuss details of the algorithm implementation.
The length of sub-series k is an important parameter which can directly influence the performance of our algorithm. We use one example to illustrate the influence of k . Fig. 5 shows two curves, where the vertical axis represents the is directly related to the confidence of hypotheses H H . We denote it as  X  X onfidence Coefficient X . In Fig. 5, the black curve is the confidence coefficient obtained by eval-uating the correlation between a pair of  X  X PU Extensive Program X  and  X  X PU Usage X . We know there does exist any correlation between the pair of data. This means the higher confidence of H 1 is better. Because the front sub-series (or the rear sub-series) with a very small length (i.e., k &lt; 4) only contains limited information, the confidence in testing is not high enough. As the length increases, we can see a significant increase in confidence in Fig. 5. However, when the length is larger than a certain value (e.g., 8), the con-fidence decreases after the maximum value. The reason is that as the sub-series length increases thereby the effect of event to time series starts to diminish which makes the sub-sequence closer to the population  X  . In Fig. 5, the blue curve is the confidence coefficient obtained from the pair of  X  X isk Transfer Extensive Program X  and  X  X PU Usage X . Do-main knowledge clearly tells us this pair is not correlated. The result also confirms this fact. The curve of confidence coefficient is flat and very close to 0 for any value of k . It means the rejection of H 1 is not influenced by k if two data sets are not correlated.

In some cases, the value of k can be selected based on domain knowledge and experiments. However, in most real world situations, there are millions of time series and events, and we do not have enough domain knowledge to pre-select the values of all sub-series lengths. In this research, we de-sign a method to auto-select the sub-series length for a time series based on the autocorrelation function [12] of the time series. Given a time series S = ( s 1 ,s 2 ,...,s n ), the autocor-relation is showed as follows: Figure 6: Example of a time series and the corre-sponding autocorrelation.
 where l denotes the lag of the correlation. The autocorrela-tion function of a time series can be used to represent the energy of signals in the time series with a period of l [12]. Therefore, our length k can be assigned the value of the first peak to include the significant signal of the time series. The experiment (Section 4.5) shows the effectiveness of this method and Fig. 6 shows an example.

Besides k , the number of neighbors r is also a very impor-tant parameter. It has been demonstrated that r = ln ( p ) is a good choice in literatures [28]. In this paper, we directly follow their suggestion.
In this section, we make an empirical evaluation of our algorithm by performing a set of experiments on a data set from a controlled environment and two real data sets.
In order to evaluate the effectiveness of our algorithm, we choose two baseline algorithms in our experiment. The first one is the Pearson correlation [8], which is the widely used method for correlation mining in time series. The second baseline is J-measure [24], which is a widely used method for correlating event data [16]. In the rest of this section, we brief introduce these two baseline algorithms.
The Pearson correlation method is one of the most widely used methods for measuring the correlation between two time series. The Pearson correlation coefficient, denoted as  X  . is calculated as follows: where cov is the covariance,  X  X is the standard deviation of X ,  X  X is the mean of X and E [  X  ] denotes the expectation.
The Pearson correlation can not be directly used in the correlation evaluation between time series and event data. In order to make it comparable, we transform the event E to a time series, denoted as S E = ( s E 1 ,s E 2 ,...,s E is the size of time series S , and s E i is calculated as: where t ( s i ) and T E is the same meaning as before (see section 2.1).

After transforming the event data to the time series form, we directly calculate the Pearson correlation coefficient  X  between the S E and S . As introduced in [8], due to our application background, we assign the threshold as 0 . 1 (weak correlation as in [8]). If  X  &gt; 0 . 1, then E +  X  S . And if correlated.
We choose the widely used event correlation method, J-measure [24], as the second baseline. J-measure has been used in many research efforts [16].

However, J-measure is proposed for the event correlation and can not be used directly. Therefore, in our experiment, we first transform each time series to an event sequence as follows: Given a time series S = ( s 1 ,s 2 ,...,s m responding event sequence is E S = ( e E 1 ,e E 2 ,...,e each e E i is the change point in the time series, and n is the number of changes in the time series. In this research, we use a widely used GLR algorithm [3] to detect changes in time series.

After transforming each time series to its corresponding event sequence, we can obtain a complete event sequence by combining all the event sequences from two sets: (1) event sequences transformed by time series, (2) event sequences from the original data set. Then we can directly use J-measure to evaluate the correlation between these complete event sequences. Similar to the first baseline (Pearson cor-relation), we assign a correlation threshold as 0.1 (weak cor-relation in [8]). If the J-Measure is larger than 0.1, then E  X  S . Otherwise, E and S are not correlated. Considering this, the Temporal Order and the Monotonic Effect Type of the correlation can not be determined using this method.
In this section, we introduce the experiment on the data from a controlled environment.
In this research, we setup a SQL environment for a con-trolled experiment to demonstrate the mutual influencing behavior in a system. For example, when a CPU Inten-sive program starts, the performance counter of CPU usage may increase. In a SQL server, database operations can be influenced by the increase of CPU usage or memory us-age . In the experiment, three types of programs including Disk Intensive program , CPU Intensive program and Mem-ory Intensive program are launched several times randomly. Each running lasts for an interval of [5 , 15] minutes. At the same time, we query a SQL server database every second. If the query latency exceeds the maximum latency limitation (200 ms ), the query will trigger a time-out alert.
We conduct the experiment for 6 hours. The CPU Inten-sive program run 43 times; the Memory Intensive program run 41 times; the Disk Intensive program run 38 times ; and 285 Query Alerts are triggered. At the same time, we collect four system events (e.g., the starting events of each program and query time-out alerts) and three system perfor-mance counters (CPU usage, memory usage, and disk I/O usage) as showed in Table 1.
After obtaining the data from the controlled environment, we run the three algorithms on this data. The results pro-duced by the proposed algorithm and two baseline algo-rithms are showed in Table 2.

The sub-series length k of the three time series (CPU us-age, Memory Usage, Disk Transfer Rate) is assigned using the first peak of the autocorrelation (as introduced in sec-tion 3.4.1). We choose DTW distance [4] as the distance measure in Nearest neighbors method of our algorithm.
In Table 2, we see that the correlation relationship, pro-duced using our method, can reflect real system behavior: the CPU Intensive program has a positive effect on the CPU usage; the Memory Intensive program has a positive effect on both Memory usage and CPU usage; the Disk Intensive program has a positive effect on the Disk Transfer rate. The Query Alert depends on the high usage of CPU and Memory.
On the other hand, in the results of the Pearson Corre-lation Algorithm, some of the correlation relations are lost (Memory Using Task +  X  CPU usage, and Memory Usage +  X  Query Alert). The result produced by J-Measure is very bad, because J-Measure can only encode the occurrence be-tween event and changes. In addition, the Temporal Order and the Monotonic Effect of Dependencies can not be cal-culated using J-measure . From this view, our algorithm has great advantages compared to the baseline algorithms.
In this section, we will compare the proposed algorithm with the baseline algorithms on two real data sets. The first real world dataset is a System Monitoring Dataset. This dataset is collected from a large scale online service of Microsoft. We collect 24 different kinds of time series as our input time-series from a number of machines including per-formance counters of Memory usage, CPU usage, Physical Disk usage etc. The sampling interval of each time series is 5 minutes. At the same time, we also collect a set of event sequences. Each event sequence contains the starting events of a specific job that is automatically submitted by a timer in the system. There are 52 different event in this dataset. The ground truth of this data is labeled by a software engi-neer in the product team.
 The second real world dataset is the Custom Support Dataset. In Microsoft, the customer support team is in charge of the support requests from customers about the above online service. Customers may send us support re-quests when they do not know how to use the service or they encounter some service quality issues. In this paper, we only collect the requests caused by service issues. The requests are grouped to different categories based on their problem phenomena and root causes (also called support topic ) by the support team. The occurrences of requests in a single category form an event sequence in this study, because we try to understand what kind of service telemetry data is related to a specific request topic. There are 57 different events in this dataset. The time series data in this study is collected from the runtime system. The front end of our sys-tem is a Web Server. For every HTTP request from an end user, we have a HTTP status code. We group the requests from every hour based on their HTTP status code into 7 groups:  X 200  X  300 X  (it means the status code is in the range from 200 to 300.),  X 300  X  400 X ,  X 400  X  500 X ,  X 500  X  600 X ,  X  &gt; 600 X ,  X  Exception  X  (it means no HTTP response, the pro-gram throws an exception.) and  X  Sum  X  ( X  Sum  X  means all the status code larger than  X 300 X ). Each group will form a time series to record the number of requests hourly.
The basic model of our algorithm is a hypothesis-test model , we use the F 1 score to evaluate our method, which has been used in many research works [26] for analyzing hypothesis-test accuracy. As introduced in [26], F 1 can be calculated as follow:
F In the above, TruePositive = 1  X  FalseNegative .

Given a time series S and an event E , for the test of dependency existence, a FalseNegative is a case where the algorithm result shows S  X  E , but actually S and E are not correlated. A FalsePositive is a case where S and E are not correlated, but the result shows that S  X  E . For the test of temporal ordering, a FalsePositive is a case where the result shows S  X  E , while the actual temporal order is E  X  S . A FalseNegative is a case where the result does not show S  X  E , while the actual temporal order is S  X  E . For the Monotonic Effect Problem, a FalsePositive is a case that the result shows a positive monotonic effect while the actual effect type is not positive. A FalseNegative is a case that the result does not show a positive monotonic effect while the actual effect type is positive.
We choose three distance measures for the nearest neigh-bor method of our algorithm, including: DTW [4] , L 1 dis-tance and L 2 distance. The result is shown in Table 3.
For the different distance measures, we can see that there is no significant difference among them. It seems that the DTW distance has a slightly better performances than oth-ers in the experiment on Custom Support Data. When a  X  NC 1 NC NC  X   X   X  NC NC NC  X   X  customer encounters a problem, s/he may not call Microsoft immediately. Many of them often try several times to make sure they cannot fix by themselves before they call in. Dif-ferent customers have different delay time before their calls. DTW distance can decrease the influence of latency values [5].

By comparing our algorithm with the baseline algorithms (Pearson correlation and J-Measure ), we can see that our algorithm has significant advantages.
Now, we shall study the efficiency of our algorithm. We use synthetic datasets to evaluate the efficiency of our al-gorithm, because we can manipulate the size of the dataset flexibly.

In Fig. 7(a), we fix the value of sub-series length k , and vary the event data size n . We can see that the CPU execu-tion time increased by enlarging the data size. Because we need to calculate the pair-wise distance among sub-series, the computational cost is at O ( n 2 ). In Fig. 7(b), we fix the size of event data, and vary the value of length k . Based on the results, we can see that the running time of the program is almost linear to the value of length k (note: we use L our distance measure in this experiment.).
In this subsection, we study the influence caused by differ-ent length values of sub-series k in the algorithm. We select two event-time series pairs which are really correlated (CPU Extensive Program and CPU usage; Memory Extensive Pro-gram and Memory Usage). In this experiment, we use the confidence coefficient as an indicator to evaluate the per- X  X C X  denotes not correlated  X  X /A X  denotes no result. (a) Varying event data size Figure 7: Efficiency by varying data size and sub-series length formance of the analysis (as we have introduced in section 3.4.1). The results are shown in Fig. 8.

The first peak of the autocorrelation of CPU usage ap-pears at the index of 5, as shown in Fig. 8(b). In Fig. 8(a), the largest value of the Confidence coefficient appears just at the index of k = 5. Similarly, in Fig. 8(d), the first peak of the autocorrelation of Memory Usage appears is at the index of 11. In Fig. 8(c), the value of the Confidence Coeffi-cient is also at a peak when k = 11. From these results, we can see that by using the first peak of the autocorrelation, one can obtain a good configuration of the length k .
In this section, we briefly introduce some related research efforts.
The correlation between two time series has been widely studied, and some of them have been included in text books [15]. The Pearson correlation [8] is a basic correlation measure usage , (c) (d): memory intensive program and memory usage between time series, which has been widely used in prac-tice [32]. Some extensions of the Pearson correlation are also widely used. For example, lagged correlation is an ex-tension to correlate a lagged dataset with another unlagged dataset using the Pearson product-moment method. In [31], the author uses the lagged-correlation to estimate the lead relationship between a set of time series. Because the Pear-son correlation is sensitive outliers in data set, Spearman Rank correlation and Kendall Rank correlation have been used in some scenarios [27] to overcome the drawbacks of Pearson correlation. In the Spearman correlation, data is first sorted and ranked, e.g., rank 1 is assigned to the lowest value. The Spearman Rank correlation is calculated by tak-ing the Pearson product-moment correlation of the ranks in the dataset. The Kendall correlation tries to measure the similarity of the orderings in the dataset. Because there is no ordering relationship among the different events, the above rank based algorithms cannot be directly used in our scenario.
Correlation between events have also been studied in many works [13, 19, 25, 22, 11]. Here, correlation mainly means the co-occurrence of different types of events. Because event data generated in online services can naturally be regarded as sequences, association rule mining algorithms [13] for se-quence data can be directly used to analyze the correla-tion among event data. For example, [19], the authors try to construct the dependency relationships between differ-ent system components by mining the co-occurrence among the unconstructed log events generated from the system. In [22], the authors add user guided information during the event correlation mining. Therefore, this algorithm allows users to effectively navigate the result. Correlation Mining algorithms have also been used in some natural science. In [25], the authors apply a correlation mining algorithm to mine the correlation between climate events by analyzing the historical climate data of the North Atlantic and China.
Two-sample Test is an important part of our work. For univariate data, the classical two-sample test includes the nonparametric Kolmogorov-Smirnov test and Mann Whit-ney U test [21]. For multi-variant data, many methods have been proposed in previous works [28, 10, 29]. In [28], the au-thor uses the proportion of all r nearest neighbors in which observations and their neighbors belong to the same sample as a statistic. The algorithm proposed in [10] is a kernel method for the two sample problem, this method maps the distance between two observations into a reproducing ker-nel Hilbert space (RKHS), and then uses Maximum Mean Discrepancy to solve the problem. They further propose a unifying framework by linking the two classes of statistics (i.e., energy distances in statistics and distances between distributions in RKHS) in [29]. In this paper, we use the nearest neighbor based algorithm in [28] for simplicity. In fact, the kernel based algorithms can also be applied in our scenarios.
In this paper, we have investigated the problem of cor-relation mining between time series data and event data. We have defined the correlation problem and proposed a novel approach which is able to discover three important aspects of event-time-series correlation: existence of cor-relation, temporal ordering, and monotonic effect. In our approach, we first transform the correlation problem to a two-sample problem, use the nearest neighbors method to solve such a two-sample problem and then evaluate the cor-relation existence. To the best of our knowledge, this is the first work to answer the three correlation questions between event sequences and time series data for incident diagnosis. The experiments on the data from a controlled environment and two real datasets demonstrate the effectiveness and effi-ciency of our algorithm. The proposed method has already been implemented to an internal tool-set designed for service problem diagnosis.

Our current method does not consider the event combi-nation problem, that only a certain combination of multiple events rather than a single type of event is correlated to a time series. This problem is much more complex, and will be a part of our future work. [1] Amazon X  X  s3 cloud service turns into a puff of smoke. [2] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. [3] M. Basseville, I. V. Nikiforov, et al. Detection of [4] D. J. Berndt and J. Clifford. Using dynamic time [5] Y. Chen, B. Hu, E. Keogh, and G. E. Batista. Dtw-d: [6] I. Cohen, J. S. Chase, M. Goldszmidt, T. Kelly, and [7] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, [8] J. Cohen. Statistical power analysis for the behavioral [9] Q. Fu, J.-G. Lou, Q.-W. Lin, R. Ding, Z. Ye, [10] A. Gretton, K. M. Borgwardt, M. Rasch, [11] B. Gruschke et al. Integrated event management: [12] J. D. Hamilton. Time series analysis , volume 2. [13] J. Han, M. Kamber, and J. Pei. Data mining: concepts [14] J. N. Hoover. Outages force cloud computing users to [15] R. A. Johnson and D. W. Wichern. Applied [16] S. Kandula, R. Chandra, and D. Katabi. What X  X  going [17] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal, [18] J.-G. Lou, Q. Fu, Y. Wang, and J. Li. Mining [19] J.-G. Lou, Q. Fu, S. Yang, J. Li, and B. Wu. Mining [20] J.-G. Lou, Q. Lin, R. Ding, Q. Fu, D. Zhang, and [21] H. B. Mann and D. R. Whitney. On a test of whether [22] H. R. Motahari-Nezhad, R. Saint-Paul, F. Casati, and [23] J. Pearl. Causality: Models, Reasoning, and Inference . [24] G. Piateski and W. Frawley. Knowledge discovery in [25] S. C. Porter and A. Zhisheng. Correlation between [26] D. M. Powers. Evaluation: from precision, recall and [27] B. Rosner. Fundamentals of biostatistics . Cengage [28] M. F. Schilling. Multivariate two-sample tests based [29] D. Sejdinovic, A. Gretton, K. Fukumizu, and B. K. [30] T. J. VanderWeele and J. M. Robins. Signed directed [31] D. Wu, Y. Ke, J. X. Yu, S. Y. Philip, and L. Chen. [32] Y. Zhu and D. Shasha. Statstream: Statistical
