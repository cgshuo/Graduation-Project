
Log-linear models are in widespread use for feature vector classification [1], [2] and in graphical models [3], [4]. Given training observations x and labels y , the goal is to learn a weight vector  X   X  d to fit a conditional distribution
Pr( Y = y | x )= where f ( x, y )  X  d is a feature vector and Z ( x ) is a normaliz-ing constant. Throughout we will assume f j ( x, y ) &gt; 0 .Given instances { ( x i ,y i ) } we seek  X  to maximize i log Pr( Y = y | x i ) , i.e., to minimize In practice, one often asserts a Gaussian prior on each  X  zero mean and variance  X  2 (i.e., Pr(  X  )  X  exp(  X   X / (2  X  and minimizes wrt  X  the objective The term  X   X / (2  X  2 ) is also called the ridge penalty .
Part of the popularity of conditional log-linear models arises from the flexibility of adding enormous numbers of diverse, noisy, possibly correlated and redundant features without the fear of corrupting a naively-estimated joint density between x and y , as in naive Bayesian classifiers. Logistic regression (LR, an instance of log-linear models) gives much more accurate text classifiers than naive Bayes models, and conditional random fields (CRFs, another instance) gives more accurate information extractors than hidden Markov models (HMMs).
In traditional machine learning, training is considered a one-time computational effort, after which the trained model  X  is applied to test cases x to compute arg max y  X  f ( x, y ) , which is computationally much cheaper. However, in deployed classification or extraction systems, training is a continual or  X  X ife-long X  activity, with class labels changing, instances being added or removed, labeling actively modified, and error cases interactively analyzed [5]. Consequently, fast training is important, and much effort has been invested on accelerating the estimation of  X  [6], [1], [7], [2].

Leading the field, and used in the vast majority of applica-tions, is the limited-memory quasi-Newton method called  X  X -BFGS X  [6], [7]. To use a Newton method, one must implement routines to calculate, given a specific vector  X   X  , the objective (  X   X  ) and the gradient  X   X  (  X  ) of wrt  X  evaluated at  X  The optimizer starts with an initial guess  X  (0) and iteratively user X  X  objective and gradient method in each iteration. If the best-of-breed optimizers are used, most of the training time is spent in these user-defined methods and relatively little time is spent inside the optimizer itself.
 optimizer L-BFGS, widely used in log-linear models, and propose a simple and practical approach to further speed it up substantially without any deep understanding of or modifica-tion to the optimizer. Our approach achieves convergence 2 X 9 times faster than the baseline. In fact, we achieve saturation of test-set accuracy (F1 scores typically within 2 X 5% of best possible, and sometimes better than the baseline) even faster than that. While other approaches are generic to the log-linear family [2], [8], the crux of our approach is to expose explicitly to the optimizer the redundancy and correlations among the  X  s evolving in time. We achieve this by periodically clustering and projecting  X  down to an optimization over a lower dimensional parameter vector  X  , optimize  X  for a while, then  X  X ift X   X  back to  X  and  X  X atch up X  the estimates, repeating this process if necessary. We have applied this idea to text classification using logistic regression (LR) and to named-entity tagging using CRFs, demonstrating substantial speedups beyond state of the art.

Thus, our idea helps retain a key advantage of conditional models X  X he ability to exploit large numbers of potentially useful features X  X hile substantially reducing the computa-tional burden of doing so. Our proposal may also be regarded as a means to model parsimony that is quite distinct from feature selection: even as application writers introduce millions of features f j , there may be no evidence that millions of difference weights  X  j are justified by the data.

The reduction in training time is almost entirely due to sim-plifying the original high-dimensional optimization problem into a lower dimensional one. By removing redundancies in the optimization surface, we make it easier for the Newton optimizer to find the best hill-climbing directions. In log-linear models, we can take advantage of feature clustering in another way: by pre-aggregating high-dimensional feature vectors into lower dimensional ones, thus saving repeated access to the feature generators in the original space (which can be quite expensive in text-processing applications). We believe we can improve training performance even further by clever engineering of feature aggregation.
 mization for log-linear models in Section II, together with their adaptation to LR and CRFs. We present the feature-clustering approach for LR in Section III, and its performance on text classification tasks. In Section IV we extend the approach to CRFs and present experimental results on information extraction or named-entity tagging tasks. We conclude in Section V.
 A. Optimization review 1) Iterative scaling variants: Suppose weight vector  X  is to be updated to  X  +  X  . We would like to maximize the reduction in the objective (2). The trick is to lower-bound the reduction with a  X  X ice X  function Q (  X  |  X  )  X  (  X  )  X  maximize Q wrt  X  , then apply an additive update  X  (  X  t  X  1) +  X  . The tighter the bound Q (  X  |  X  ) the better, pro-vided finding arg max  X  Q (  X  |  X  ) does not become too difficult. Several bounding functions and  X  optimization strategies have been devised, leading to Improved Iterative Scaling (IIS) [9] and Faster Iterative Scaling (FIS) [2]. We will not discuss these methods in detail because, in text-processing and graphical model applications, they have been supplanted by direct New-ton optimization, discussed next. In experiments we describe shortly, FIS (which was already shown to be faster than IIS) was decisively beaten by a direct Newton method. 2) Direct Newton optimization: Let (  X  ) be the (scalar) objective at  X  and g (  X  )=  X   X   X  d be the gradient vector. A Newton method seeks to minimize by walking against the gradient, i.e., by setting where  X  t  X  is a step size, g ( t ) = g (  X  ( t ) ) is the gradient evaluated at  X  ( t ) , and H t is the Hessian matrix [10]. (In one dimension, H is the familiar second derivative (  X  ) and H is just 1 / (  X  ) , and g (  X  )= (  X  ) , leading to the usual update  X 
For large d , maintaining the Hessian matrix from iteration to iteration is a computational challenge, and that is where the BFGS family of update methods [11], [6] comes into play. 3) Limited Memory BFGS (L-BFGS): L-BFGS is a limited memory version of BFGS which saves the time and space required to compute the Hessian matrix. It maintains the last m (typically 3 X 7 in applications) corrections s ( t )  X  the gradient respectively, and stores an initial approximation B 0 of the inverse of Hessian (identity matrix by default). It then calculates the product B t g ( t ) using efficient sparse matrix multiplications during each iteration. For the first m iterations, L-BFGS is exactly same as BFGS. The specifics of L-BFGS are listed in Figure 1. A complete analysis of L-BFGS has been given by Liu et al. [6]. 4) Memory Requirements of L-BFGS: L-BFGS methods are particularly helpful because of their low memory requirements as compared to actual BFGS. Since only m pairs of updates { s to store the updates is 2 md + O ( m ) . Additional memory is required to store the initial approximation H 0 . L-BFGS, by default starts with H 0 being the identity matrix (or some user-defined matrix), which requires and additional d memory cells. So the total memory requirement for L-BFGS is d (2 m +1)+ O ( m ) . Each update operation H k g k , where H k is obtained by updating H 0 m times, is done in 4 md + O ( m ) floating point operations. 5) FIS vs. BFGS experimental comparison: We are not aware that FIS [2] has been compared directly with any BFGS algorithm, so we coded up FIS and BFGS in Matlab and used has about 7000 training documents and over d = 26000 raw 2 word features. To keep memory requirements of H tractable, we chose 500 word features that had the largest mutual information wrt to the classes. (See Section II-B for details of LR-based text classification.)
Not only does BFGS take significantly less time than FIS for minimization, but it also attains better objective values (not shown). There can be quite a few reasons for this. One, the BFGS approximates the Hessian of the function, hence goes up to second order of approximations whereas, FIS is just a first order approximation. Moreover, in FIS, calculating the difference vector  X  at each iteration is not feasible since it takes this vector just once at the start and use it for all subsequent iterations to update the weight vector. This approximation may have caused the quality of FIS solution to suffer.

While BFGS is faster than iterative scaling, Liu et al. [6] showed that L-BFGS is even faster, so we did not do an in-house comparison between BFGS and L-BFGS. Text mining tools overwhelmingly often use L-BFGS.
 B. Logistic regression for text classification
LR is a canonical member of the log-linear family. In the simplest case labels can take two values Y  X  X  X  1 , +1 text classification, it is common to create a feature f j each word j in the document x and each y . One common def-inition is f j ( x,  X  1) = 0 for all j and x , while f j ( x, +1) = 1 if word j appears in document x and 0 otherwise. (Some form of term weighting, like counting the number of occurrences of j in x , can also be used.) This leads to because exp (  X  f ( x,  X  1)) = exp(0) = 1 . The two class LR is commonly used for  X  X ne-vs-rest X  text classification, e.g., to determine if x is or is not about cricket. Each model parameter  X  j corresponds to a word. If  X  j is strongly positive (respectively, negative), the existence of word j in x hints that Y is likely to be +1 (respectively,  X  1 ). E.g., for the cricket vs. not-cricket document classifier, we would expect  X  wicket positive, and  X  parachute to be negative.  X  for function words like the , an will tend to have much smaller magnitude, because they appear at similar rates in positive documents (with Y =+1 ) and negative documents (with Y =  X  1 ). LR does implicit feature selection by driving these  X  j s toward zero.

Suppose we take a word j and, in every document where j occurs, add a new unique word j . Features f j and f j will be perfectly correlated, and j would add absolutely no predictive power to any classifier. Clearly, the LR optimization would be at liberty to keep  X  j unchanged and set  X  j =0 , and achieve the same accuracy as before on any test data.

However, thanks to the ridge penalty, this will not happen, because, upon including the ridge penalty into the objective, a better strategy for the optimizer would be to  X  X plit the evi-dence X  and set  X  new j =  X  new j =  X  old j / 2 , because 2(  X  remains unchanged whenever  X  new j +  X  new j =  X  old j .
Summarizing, the ridge penalty shrinks weights correspond-ing to redundant features toward similar values  X  X  vital property that we exploit. To be sure, this is not always a desirable property, in particular, the ridge penalty destroys sparseness of  X  even if training x s are sparse. Alternatives like the Lasso assert an L1 penalty  X  1 = j |  X  j | is better at keeping  X  sparse [13, Figure 3.9]. However, the Lasso penalty leads to a quadratic programming problem that is computationally more complex, and therefore the L2 ridge penalty is still overwhelmingly popular.

We conclude this section by noting that the feature set, with one feature for each word, is very large and highly redundant in this application. Reuters-21578 [12], a standard text classi-fication benchmark, has about 10000 labeled documents and more than this number of distinct words.
 C. CRFs for information extraction
Conditional Random Fields (CRFs) are a graphical repre-sentation of a conditional distribution Pr( Y | x ) where both x and Y can have non-trivial structure (e.g., Y may be a random sequence of labels), x is observed, and properties of the distribution over Y are sought. In a linear-chain CRF com-monly used for sequential tagging, training instances ( x are provided. Each instance i is a sequence x i of W tokens and a sequence y i of W labels. Positions in the sequence are indexed by p  X  X  1 ,...,W } , thus we will write y i p as the p th or one of person name, place name, time or date, or  X  X one of the above X . Let there be S possible labels, also called states . The feature vector corresponding to an instance ( x, y ) will be written as F ( x, y )  X  d , and is the result of computing a vector sum over feature vectors defined at each position p : The full set of d scalar features in f is a concatenation of S scalar state transition features f ( y p  X  1 ,y p ,x,p )= t ( y that only depend on and expose dependencies between y and y p , and something like VS scalar so-called symbol emis-x p and y p and expose dependencies between them. Here V is the number of predicates defined on each token x p , e.g., does the word have digits, does it start with an uppercase letter, etc. (We are simplifying a bit for exposition; in applications, V may include features defined over multiple positions of x .)
For many NLP tasks, there are also lexicalized predicates, one for each word in a large vocabulary. Lexicalization assists 3 rote learning, i.e., recognizing in test documents tokens that appeared in training documents and labeling them accordingly, and more important, in conjunction with other features, induct to the immediate neighborhood, e.g. from York and New York labeled as places, induct that New Hampshire is a place. To further assist this, lexicalization is sometimes extended to neighboring tokens at p  X  1 and p +1 , leading to a threefold (or worse, if features combine x p  X  1 and x p in any way) increase in the number of features from a set that already ranges into tens of thousands. A million features is not uncommon in NLP tasks.

Given a trained weight vector  X   X  d and an instance ( x, y ) , we can write Pr( y | x )=Pr( y 1 ,...,y W | x )= Finding the most likely label sequence means finding arg max y  X  F ( x, y ) . We do not wish to enumerate all S possible label sequences, so dynamic programming is used instead. Let A ( y, p ) be the unnormalized probability of a labeling of positions 1 through p ending in state y . A ( y, p ) can be defined inductively as Similarly we can define the unnormalized probability of a labeling of positions p through W , starting with state y : B ( y, W +1)= y = endState , and (10) Note that y A ( y, W )= Z ( x )= y B ( y, 1) .

For training the CRF using L-BFGS, as in LR, we must estimate the gradient wrt each  X  j , corresponding to the j th el-ement f j ( y, y ,x,p ) of feature vector f ( y, y ,x,p ) . To within a constant, the gradient of 0 in (2) is and we generally add on  X   X  j / X  2 corresponding to the ridge penalty  X  2 j / (2  X  2 ) . The crux is to compute E Y which, by linearity of expectation, can be written as Note that Y , not y i is involved in the above expression. I.e., we must sum over all possible Y p  X  1 and Y p . Again, through dynamic programming this can be computed via A and B : =
In the previous Section we saw the usefulness of BFGS update and its limited memory version L-BFGS in solving large-scale optimization problems. When L-BFGS is applied to text classification and information extraction, it is used as a black-box, and no specific advantage is taken of the peculiarities of those problem domains. In this Section we seek to remedy this limitation.

As mentioned in Section II-B, log-linear text classification models treat each word of the document as a potential attribute, leading easily to instances with d in the tens of thousands. In multi-label (as against one-vs-rest) classification, this would generally be multiplied by the number of class labels, poten-tially leading to millions of features of the form f j ( x, y ) where j ranges over words and y over class labels. If the labels are arranged in a hierarchy, the feature space may be even larger.
An important property of text classification problems is that, although there are potentially many features that can determine the label of a document, very few features actually end up getting significant weights  X  j that decide the score of the document. The remaining words are regarded as noise. It is also common to find two informative words strongly correlated with each other, e.g. Opteron and HyperTransport . As explained in Section II-B, their corresponding weights may end up being very similar in value.
 A. Motivating examples
To see if this is indeed the case, we trace the evolution application. For these preliminary experiments, we run BFGS provided by MATLAB on the Reuters data. To keep memory requirements of H tractable, we chose 500 word features that had the largest mutual information wrt to the nine most frequent classes. Figure 2 shows how the weights of these 500  X  X mportant X  terms evolve.
A close-up is shown in Figure 3. We point out two important features. First, many weights co-evolve in largely parallel 4 trajectories, and, after the  X  X ig bang X  moment has passed,  X  clustered close together usually tend to remain clustered close together. Tracing back from the features to class labels and words gives some interesting insight. E.g., one tight cluster of contiguous  X  j s corresponded to class grain in the Reuters data, and these words: marketing, price, imports, annual, average, traders, credit, exporters. Meanwhile, another tight cluster of  X  s corresponded to class grain and these words: was, these, been, among, like, can, what, say, given, and the meta-word  X  X igits X  X  X or topic grain , these were all low-signal words.
However, it is important to recognize that there are excep-tions to the  X  X ersistent cluster X  hypothesis, and crossovers do happen, and therefore, any algorithm that seeks to exploit re-dundancy between clusters must occasionally regroup features. B. The feature clustering technique
From the above discussion, we come up with the pseu-docode shown in Figure 4. The basic idea is to run fineBFGS for a while, cluster the d features into d &lt;d groups, project down the weight vector  X   X  d to a coarse approximation  X   X  d ,runafaster coarseBFGS procedure over  X  , project back up from  X  to  X  , and run a  X  X atch-up X  optimization over  X  . If needed we can repeat several alternating rounds between fineBFGS and coarseBFGS before the final patch-up. cmap is an index map from feature IDs in [1 ,d ] to cluster IDs in [1 ,d ] .Given cmap ,  X  can be reduced to  X  by averaging values in each cluster. Conversely, we can copy back  X   X  cmap ( j ) . The only change required for specific applications is to generalize the code for calculating (  X  ) and  X   X  calculate (  X  ) and  X   X  (see this and the next Section).
The intuition is that d being much smaller than d , coarseBFGS will run much faster than fineBFGS , but will have the benefit of reducing the objective and providing the next copy of fineBFGS with a better starting point, which will also ease convergence.

The important policy choices are embedded in these vari-ables: Clustering: How we represent features and how we cluster 1: Let initial approximation to the solution be  X  2: for numRounds do 3: for fineIters iterations do 4: [ f, g ]  X  computeFunctionGradient (  X  ) 5:  X   X  fineBFGS (  X , f, g ) 6: end for 7: Set numClusters = d/ clusterFactor 8: Prepare the feature cluster map 9:  X   X  projectDown (  X , cmap ) 10: for coarseIters iterations do 11: [  X ,  X  ]  X  computeFunctionGradient (  X  ) 12:  X   X  coarseBFGS (  X ,  X ,  X  ) 13: end for 14:  X   X  projectUp (  X , cmap ) 15: end for 16: while not converged do 17: [ f, g ]  X  computeFunctionGradient (  X  ) 18:  X   X  fineBFGS (  X , f, g ) 19: end while numRounds : The number of fine-coarse alternations. 1 X 2 fineIters : The number of iterations allowed to a fineBFGS coarseIters : Likewise for coarseBFGS . In fact, we may clusterFactor : A target for the reduction factor d/d . A crite-The above policies should try to ensure that very little time is spent in fineBFGS , and that when coarseBFGS terminates, its output can be used to land fineBFGS close to the optimum.

We do not have perfect recommendations for all these policies for all learning problems. However, for a specific domain and application, only a few trial runs suffice to tune the above policies quite effectively. Unlike in a one-shot learning exercise, this small effort will be paid back in a continual or lifelong learning scenario, e.g. in operational text classification.
 tering approaches. L-BFGS is very fast and we need a very lightweight clustering algorithm that consumes negligible CPU compared to the optimization itself. For L-BFGS, we simply sorted the current  X  values and performed a one-dimensional clustering with a fixed cap of clusterFactor on the cluster size. This contiguous chunking approach already performed reasonably well.

BFGS, being somewhat slower than L-BFGS, gives us a little more time for clustering. Here we tried to bring in a second attribute of each feature: the temporal behavior of its weight in recent iterations. Each feature f j is characterized 5 clustering together feature pairs where one is momentarily crossing over the other, e.g. the sample in Figure 3. However, this did not make a significant difference, probably because crossovers are rare overall.

In our experiments so far, we have found that clusterFactor matters more than the clustering algorithm itself.
 fine L-BFGS over  X  . This ensures that we end up as close to the original optimum as possible. This process actually brings our algorithm back near the actual optimum, correcting if coarseBFGS has taken our solution far from it. In our experiments, we sometimes found that coarseBFGS actually helped the patch-up stage to find a  X  better than the baseline.
A bad clustering can take our solution far from original optimum, implying that we would have to spend more time in the final patch-up to get back near the original optimum. This implies that we need a good clustering algorithm to ensure that we spend as little time in patching up as possible.
We will discuss the remaining issues while narrating our experiments in Section III-D.
 C. Computing (  X  ) and  X   X 
Our recipe requires that, starting with code to compute (  X  ) and  X   X  , we write code to compute (  X  ) and  X   X  , for a given clustering expressed through cmap . Computing (  X  ) is trivial, and so is  X   X  : Most log-linear training routines initialize an array of gradients wrt  X  ,ofsize d , and update it additively: 1: gradWrtBeta [1 ...d ]  X  0 2: for each instance i do 3: for each feature index j do 4: gradWrtBeta [ j ] += ... 5: end for 6: end for The above code is simply replaced with the many-to-one accumulation: 1: gradWrtGamma [1 ...d ]  X  0 2: for each instance i do 3: for each feature index j do 4: gradWrtGamma [ cmap ( j )] += ... 5: end for 6: end for It is thus trivial to transform fineBFGS into coarseBFGS completely mechanically.
 D. Experiments with BFGS
To further motivate the point, we show the results of the experiments done in MATLAB using its in-built minimization subroutine which uses BFGS updates. We use the same 500-attribute version of the Reuters Corpus. We have each document represented in a 500 dimensional feature space and we choose just one of the 9 classes for the one-vs-rest problem. We call the basic optimization process as  X  X asic optimizer X  and our algorithm as the  X  X luster optimizer. X  The policy decisions were:  X  Fix the value of numRounds to one  X  Run coarseBFGS to convergence  X  Fix the initial fineIters to 5  X  Fix dimensionality reduction factor clusterFactor =15  X  Use the KMEANS subroutine in MATLAB for clustering.

Figure 5 compares the value of 0 (  X  ) vs. CPU time in seconds (no ridge penalty was used here, but it will be in later experiments). The bold line denotes standard BFGS in original feature space. The dotted line denotes our algorithm working (part of the time) in the projected feature space with parameters  X  . When coarseBFGS completes, final patch-up iterations are never more than one or two. We see that our clustering collapses the feature space nicely, and the new features have captured all the key variations of the original feature space to drive the function faster toward the optimum. E. Experiments with L-BFGS
Encouraged with our MATLAB experiments, we imple-mented the scheme in Java on top of the publicly-available and widely-used L-BFGS package from http://riso. sourceforge.net/LBFGS-20020202.java.jar . 1) Setting the ridge parameter: The  X  X egularizer X  or the ridge penalty 1 / X  2 in (3) is an important parameter, not only to the accuracy of the learnt model, but also to the training time. Since this parameter provides a check on  X  2 , it determines the number of cross-overs in the  X  vector evolution. There is no algorithmic solution known to find a good ridge penalty for a given problem and a given dataset, so we find the best ridge parameter using cross validation. For the Reuters data, a plot of F 1 -score vs. ridge parameter (by which we generally mean 1 / X  2 ) is shown in Figure 6. 6
F 1 score is a standard accuracy measure defined in terms of recall and precision. Let TP be the number of true positives, that is, the number of documents on which were marked and predicted to be positives. FN be the false negatives, that is, the number of documents that were marked positive but predicted negative; FP be the false positives marked negative but pre-dicted positives. Then recall is defined as R = TP / ( TP + FN ) and precision is defined as P = TP / ( TP + FP ) . F 1 harmonic mean of recall and precision: F 1 =2 RP/ ( R + P ) .
From Figure 6, it is evident that lower values of ridge penalty are preferable for Reuters dataset. 10  X  7 and 10 win on test F1-score for most of the topics. We choose both these values for comparisons in the sections that follow.
Figure 7 shows the performance implications of the ridge parameter on the training time of the basic LR algorithm. It is curious to note that training becomes most time-consuming for a middle range of ridge parameters, where the test accuracy is not the best possible! Although LR is so widely used, we have seen no detailed report of the accuracy and performance implications of the ridge parameter. It turns out that our idea accelerates L-BFGS at the best ridge value , and is therefore even faster compared to the worst-case training time. 2) coarseBFGS termination policy: For every call to coarseBFGS , since it is faster than fineBFGS , our goal we do not put any upper bound on the number of coarse iterations for any call to coarseBFGS . However, the last few coarse iterations, without any significant change in the objective, are of little use to us. We do not want coarseBFGS to drag slowly towards it optimum in the end. We leave full termination for fine patch-up and decide to terminate coarseBFGS when the relative change it brings in the objective in last w iterations, is less than some  X  . That is, when (max (  X  )  X  min (  X  )) &lt; X  min (  X  ) , where max (  X  ) and min (  X  ) are the maximum and minimum objective values in the last w iterations.

Through trial-and-error we arrived at choices of history size w = 5 and  X  = 10  X  4 , which worked well for all our experiments. However these were not very sensitive values. 3) projectUp and projectDown : While going from the original space to the reduced space ( projectDown ), we use the cluster centroids (obtained by averaging of weights of features mapped to same cluster) as the  X  vector in the reduce feature space.

We compute the function value and gradient by aggre-gating the features using cmap . Note that there is a call to computeFunctionGradient before each iteration of L-BFGS. We pass cmap to it, to calculate the function value and gradient in reduced feature space on the fly.

While going back from the reduced space to the original space via projectUp , we just replicate the values as  X   X  cmap ( j ) . computeFunctionGradient then calculates the objective value and gradient vector in the original space because calls to coarseBFGS are implemented using the identity map from [1 ,d ] to [1 ,d ] passed as cmap . 4) numRounds , the number of alternations: The parameter numRounds determines how many calls to coarseBFGS we would make. numRounds should not be too small, or the final fineBFGS patch-up will take a lot of time. It should not be too large, or we would be doing unnecessary work in coarseBFGS , while fineBFGS was already getting close to the optimum in original space.

Luckily, the choice of numRounds is easy. In practice, we have seen that a value between 1 and 3 suffice for all class labels and both our application domains. In Table II, we compare the optimization time for three classes and ridge parameter 10  X  7 with several values of numRounds .

As is clear from the table, there is no clear winner among numRounds = 1 or 2. In practice, we choose the numRounds = 2, to ensure that we are not over doing any work and at the same time we have less fine patch-up iterations as well. 5) Comparing basic and cluster optimizers: Fixing the optimizer and cluster optimizer. Figure 8 shows the time trace of objective (  X  ) in the basic and cluster optimizers for two classes and ridge parameter 10  X  7 .

It is immediately visible that, as in BFGS, here, too, the 7 clustered approach drives down the objective much quicker than the basic L-BFGS optimizer. What is not visible in Figure 8 is that the clustered approach also terminates faster, which is shown in Table III.

For some of the major classes in the Reuters corpus, clustered L-BFGS is almost 10 times faster than basic L-BFGS; typically, it is a factor of 2 X 3 faster. We emphasize that this gain is CPU-invariant, in the sense that it will persist as CPUs become faster.

Table IV gives a break-up account of how the time is spent in fineBFGS , coarseBFGS and clustering itself by clustered L-BFGS. Clustering time itself is negligible, and fineBFGS and coarseBFGS divide up the time into fair-sized chunks, while their sum remains far below the time taken by basic L-BFGS.

Another important measurement is the F 1 score on a sep-arate test set as the optimizer spends time on the training data. Figure 9 shows that initial growth in test F clustered L-BFGS is indistinguishable from baseline L-BFGS, but over-iterating can lead to some instability. Therefore test-set validation is recommended. 6) clusterFactor and feature pre-aggregation: While calcu-lating (  X  ) and  X   X  , rather than first calculate  X   X  and then transform them to  X   X  as in (14), we can exploit the linear interaction between  X  and  X  with f by pre-aggregating feature values down to the  X  space. For k =1 ,...,d , we define new  X  X seudofeatures X  and then compute (  X  ) using  X  f alone.

Pre-aggregation is needed after every round of clustering, but the results are exploited repeatedly inside coarseBFGS . A large clusterFactor can save a large number of floating point operations per iteration of coarseBFGS . But at the same time, larger the clusterFactor , the worse is the approximation 8 involved in coarseBFGS and hence more work is required during the final fine patch-up. There is a trade-off in the quality of solution (and time required for training) and clusterFactor . Therefore, we study their combined effect in Table V.

Increasing clusterFactor beyond 50 adversely affects our solution. The higher training time at 50, as compared to other factors, can be attributed to increased fine patch-up iterations, due to poorer quality of approximation in the reduced space. There are some other classes that are not so sensitive the quality of approximation, and for those cases we do save training time. Empirically, 50 is too large a value, and lower values are preferred. At lower values, the benefits of pre-aggregation are modest. Only some classes like money-fx with ridge parameter 10  X  7 show significant reduction in time by pre-aggregation. What this implies is that the reduction in training time we observed is, to a large extent, because of the simplification of feature space in coarseBFGS .

We chose Named Entity Recognition (NER) as our task of sequential labeling. In NER, each word is labeled with an entity tag indicating whether it is a mention of a person, an organization, a place or none of those. We use the CoNLL dataset, which provides the part-of-speech (POS) tags and entity tags for Reuters-21578 data. We have used the current word, the previous word and POS tag of the current word as the attributes which form our symbol emission features. A.  X  evolution in NER
To see that our approach is promising for NER as well, we repeat the  X  evolution study from Section III-A. We ran a CRF ( http://crf.sf.net ) trainer with standard L-BFGS for NER on CoNLL data. Figure 10 shows the results of some features evolving with time. A close-up is shown in Figure 11.
Crossovers appear more frequent than in LR-based text classification, but Figure 11 shows that there are a lot of similarly evolving features, too. These observations hint that we need to keep clusterFactor small, and that we can expect more iterations during the final patch-up. B. Computing (  X  ) and  X   X 
We limit feature clustering to the symbol emission features s ( y p ,x ) because for NER tasks too much of the model infor-mation would get corrupted if we allowed transition features to be clustered.

A brief inspection of the formulas in Section II-C leading up to expressions for (  X  ) and  X   X  readily reveals that the transformation (14) can still be computed using the same code style outlined in Section III-C.

Pre-aggregation of features takes slightly more involved software engineering and is deferred to future work.
 C. Experiments and results
Gaining experience from the experiments in Section III-E, we set the policies of our algorithm as follows. 9 Fixing the policies as above, we show the objective attained by the basic and clustered optimizers against CPU time in Figure 12.

As with LR, the clustered optimizer reduces the objective much more quickly, and attains convergence in half the time taken by the baseline NER implementation.

To ensure that this has not led to any significant damage to the quality of the solution, we plot the F 1 score of some sample entity types for both the basic and clustered optimizers. The result is shown in Figure Figure 13. The F 1 score grows more quickly in the clustered optimizer, and quickly becomes essentially comparable to the baseline accuracy. In fact, about half the time, the clustered tagger achieves test accuracy slightly higher than the baseline tagger.

Summarizing, the clustered optimization approach speeds up NER tasks substantially, achieving a 50% reduction in time to convergence. It reaches baseline F 1 scores even faster, and there is no systematic loss of accuracy.

We have proposed a very simple and easy-to-implement  X  X rapper X  around state-of-the-art quasi-Newton optimizers that can speed up training text learning applications by large factors between 2 and 9. We have achieved this by exploiting natural co-evolving clusters of model parameters in high-dimensional text learning applications, as well as the specific form of log-linear models for learning in vector spaces and graphical models. No understanding of the math behind the Newton optimizer is required to implement our proposal. A mechanical and local rewriting of the routines that compute the objective and gradient suffice. However, a slightly more involved feature pre-aggregation step may buy even larger performance benefits; this is ongoing work. We are also interested in extending the approach to other general graphical inference and training algorithms, and exploring more complex  X  to  X  transformations.
 10
