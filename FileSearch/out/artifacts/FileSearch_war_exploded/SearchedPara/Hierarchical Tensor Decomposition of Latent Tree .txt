 Le Song, Haesun Park { lsong,hpark } @cc.gatech.edu Mariya Ishteva mariya.ishteva@vub.ac.be ELEC, Vrije Universiteit Brussel, 1050 Brussels, Belgium Ankur Parikh, Eric Xing { apparikh,epxing } @cs.cmu.edu Latent tree graphical models capture rich probabilistic dependencies among random variables and find appli-cations in various domains, such as modeling dynam-ics, clustering, and topic modeling (Rabiner &amp; Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003). Recently, there is an increasing interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012). Com-pared to Expectation-Maximization (EM) algorithms (Dempster et al., 1977) traditionally used for the same task, the advantages of spectral algorithms are their computational efficiency and good theoretical guaran-tees. Unlike EM, these spectral algorithms recover a set of alternative parameters (rather than the origi-nal parameters) which consistently estimate only the marginal distributions of observed variables.
 Previous spectral algorithms are carefully constructed based on the linear algebraic properties of latent tree graphical models. It is however unclear what objective function these algorithms are optimizing and whether they are robust when latent tree graphical models are misspecified. Recently, Balle et al. (2012) provided some partial answers to these questions by formulat-ing the problem in terms of a regularized local loss minimization and presenting their results in the set-ting of weighted automata. However, it is still unclear whether spectral algorithms can be interpreted from a global loss minimization point of view as typically used in iterative algorithms such as EM, and how to further understand these algorithms from basic linear algebraic point of view, using concepts such as low rank approximations. Therefore, the goal of this pa-per is to provide new insight to these questions. Our first contribution is deriving a global objective function and the optimization space for spectral algo-rithms. More specifically, we approach the problem of estimating the parameters of latent tree graphical models from a hierarchical tensor decomposition point of view. In this new view, the marginal probability ta-bles of the observed variables in latent tree graphical models are treated as tensors, and we show that the space of tensors associated with latent tree graphical models has the following two properties: ( i ) various matricizations of the tensor according to the edges of the tree are low rank matrices; ( ii ) this collection of low rank matricizations induces a hierarchical low rank decomposition for the tensors. Overall, the optimiza-tion problem aims to minimize the Frobenius norm of the difference between the original tensor and a new tensor from the space of hierarchical low rank tensors. Our second contribution is showing that previous spec-tral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) are spe-cial cases of the proposed framework, which elucidates the global objective these algorithms are optimizing for. Essentially, these algorithms recursively apply a low rank matrix decomposition result to solve the op-timization problem. When the latent tree models are correctly specified, these algorithms find a global op-timum of the optimization problem.
 When the latent tree models are misspecified, previous spectral algorithms are no longer optimal. Our third contribution is deriving a better decomposition algo-rithm for these cases, based on our hierarchical low rank tensor decomposition framework, and providing some theoretical analysis. In both synthetic and real world data, the new algorithm significantly improves over previous state-of-the-art spectral algorithms. Latent tree graphical models (LTGM). We focus on discrete latent variable models whose conditional independence structures are undirected trees. We use uppercase letters to denote random variables ( X i ) and lowercase letters for their instantiations ( x i ). A latent tree graphical model defines a joint probability distri-bution over a set of O observed variables { X 1 ,...,X O } and a set of H hidden variables { X O +1 ,...,X O + H } . For simplicity, we assume that ( I ) all observed vari-ables are leaves having n states, { 1 ,...,n } , and all hidden variables have k states, { 1 ,...,k } , with k  X  n . The joint distribution of all variables in a latent tree graphical model is fully characterized by a set of condi-tional probability tables (CPTs). More specifically, we can arbitrary select a node in the tree as root, and sort the nodes in the tree in topological order. Then the set of CPTs between nodes and their parents P ( X i | X  X  i ) (the root node X r has no parent, so P ( X r | X  X  r P ( X r )) are sufficient to characterize the joint dis-marginal distribution of the observed variables can be obtained by summing out the latent ones, P ( x 1 ,...,x O ) = X Latent tree graphical models allow complex distribu-tions over observed variables ( e.g. , clique models) to be expressed in terms of more tractable joint models over the augmented variable space. This is a signifi-cant saving in model parametrization.
 Latent tree graphical models as tensors. We view the marginal distribution P ( X 1 ,...,X O ) of a la-tent tree model as a tensor P , each variable corre-sponding to one mode of the tensor. The ordering of the modes is not essential so we simply label them using the corresponding random variables.
 We can reshape (unfold) a tensor into a matrix by grouping some of its modes into rows and the remain-ing ones into columns. The resulting matrix has ex-actly the same entries as the original tensor but they are reordered. Let O = { X 1 ,...,X O } be the set of modes and I 1 and I 2 be two disjoint subsets with O = I 1  X  I 2 . Similarly to the Matlab function, denotes a matricization of P ( X 1 ,...,X O ) for which variables corresponding to I 1 are mapped to rows and those corresponding to I 2 are mapped to columns. Each row of the resulting matrix corresponds to an assignment of the variables in I 1 . For instance, P for simplicity we also use P { 2 } ; { 1 , 3 } to denote P the values of variables with lower index change faster than those with higher index. We similarly arrange the column indexes. We will overload the reshape op-eration to deal with matrices. For instance, we may We will show that the latent tree structure T induces a hierarchical low rank structure in P ( X 1 ,...,X O ). We will reshape this tensor into a collection of matrices, each of which corresponding to an edge in the latent tree. We will show that ( i ) although the sizes of these matrices can be exponential in the number of variables, the ranks of these matrices cannot exceed the number of states k of the hidden variables; ( ii ) the low rank structures of this collection of matricizations further induce a hierarchical decomposition of the tensor. 3.1. Low Rank Matricizations of Tensors Each edge in the latent tree corresponds to a pair of variables ( X s ,X t ) which induces a partition of the ob-served variables into two groups, I 1 and I 2 (such that O = I splitting the latent tree into two subtrees by cutting the edge. One group of variables reside in the first sub-tree, and the other group in the second subtree. If we unfold the tensor according to this partitioning, then Theorem 1 Under condition I , rank( P I 1 ; I 2 )  X  k . Proof Due to the conditional independence struc-ture induced by the latent tree, P ( x 1 ,...,x O ) = P written in a matrix form as where P I reshape ( P ( I 2 | X t ) , I 2 ) and P { s } ; { t } = P ( X P { s } ; { t } is a k  X  k matrix, so its rank is at most k . Since the rank of a product of matrices cannot exceed the rank of any of its factors, rank( P I 1 ; I 2 )  X  k . Theorem 1 implies that although the dimensions of the matricizations are large, their rank is bounded by k . For instance, P { I nential in the number of observed variables, but its rank is at most k . Essentially, given a latent tree graphical model, we obtain a collection of low rank matricizations { P I 1 ; I 2 } of the tensor P ( X 1 ,...,X each corresponding to an edge ( X s ,X t ) of the tree. More interestingly, the low rank structures of the ma-Algorithm 1 decompose ( P , T , E ,k ) Input: tensor P , tree T , set of primary edges E , rank k . Output: factors of a hierarchical rank k decomposition 1: Pick a primary edge e = ( X s ,X t )  X  X  and: 3: According to (2): 4: Call decompose ( F s , T s , E s ,k ) if E s 6 =  X  ; tricizations also imply that the tensor can be decom-posed hierarchically as we see later. 3.2. Hierarchical Low Rank Decomposition We say that a tensor P has hierarchical rank k ac-cording to a tree T with leaves corresponding to the modes of P , if it can be exactly decomposed accord-ing to Algorithm 1. The decomposition is carried out recursively according to T and its primary edges E (see Fig. 1 for notation). The end result of the hi-erarchical (recursive) decomposition is a collection of matrices and 3rd order tensors (or factors with 3 in-dexes). By reshaping and combining these factors in reverse order of the recursion, we can obtain the original tensor P . Alternatively, one can think that each entry in P is obtained by a sequence of sum and product of factors. That is, P ( x 1 ,...,x O ) = P dexes in F and F correspond to  X  X ummy X  variables, and the summation ranges over all  X  X ummy X  variables X e . We denote H ( T ,k ) the class of tensors P admit-ting hierarchical rank k decomposition according to tree T . 1 Similar decompositions have also been pro-posed in tensor community, but not for latent variable models (Grasedyck, 2010; Oseledets, 2011). 3.3. Low Rank Matricizations Induce Next, we show that if all matricizations of a tensor P according to a tree T have rank at most k , then P admits a hierarchical rank k decomposition, i.e. , P  X  H ( T ,k ). We note that this property applies to a gen-eral tensor P where the tensor modes { X 1 ,...,X O } are organized hierarchically into a tree structure T . A latent tree graphical model is a special case. In gen-eral, the low rank factors F and F in the hierarchical decomposition do not have an interpretation as CPTs. More specifically, we have the following theorem Theorem 2 Let P be a tensor and T be a tree. If rank( P I 1 ; I 2 )  X  k for every matricization P I 1 ; I according to an edge of T , then P admits a hierarchical rank k decomposition, i.e., P  X  X  ( T ,k ) .
 Proof Let the modes of the tensor be labeled as X 1 ,...,X O and { according to an edge e of T . Since rank( P I 1 ; I 2 )  X  k , it admits a rank k decomposition P I 1 ; I 2 = UV &gt; (with U and V having k columns), or in index form The matrix V can be expressed as V = P &gt; I P Now the matrix V can be reshaped into a ( | I 2 | + 1)-th order tensor V with a new tree T t and a dummy variable X e . We will consider its unfolding according to an edge e 0 = ( X s 0 ,X t 0 ) in this new tree (and the associated partition of tensor modes { I 0 1 , I 0 2 } ) and show that rank( V I 0 Then matricizing the original tensor P according to form (see Fig. 1 for notation) which also has rank k . Using this, we obtain V ( x I 2 ,x e ) = X and R ( x e ,x I Now row and column indexes of V I 0 and rank( V I 0 out recursively until obtaining a hierarchical rank k decomposition.
 Theorem 2 suggests that the low rank constraints on the matricizations of the original tensor P induce a hierarchical low rank decomposition of P . This result allows us to define the space of tensors H ( T ,k ) using the matricizations rather than the recursive decompo-sition in Algorithm 1 or the factorization form. Now we can write out an optimization problem which recovers previous spectral algorithms as special cases and suggests new algorithms. Given a tensor P which might not admit a hierarchical low rank decomposition as H ( T ,k ), we seek the closest (in Frobenius norm) hierarchical low rank tensor in H ( T ,k ). That is, where the constraint is equivalent to rank( Q I 1 ; I 2 )  X  k for all matricizations of Q according to edges of tree T . In general, the optimization problem is not convex. Learning the parameters of a latent tree graphical model is a special case of optimization problem (3). In this case, P is the joint probability tensor of the observed variables, and we seek a hierarchical rank k decomposition Q  X  H ( T ,k ) with tree structure T . The question is whether we can design an efficient al-gorithm for carrying out this hierarchical decomposi-tion. Naively applying existing low rank decomposi-tion techniques to the unfoldings of the tensor will re-sult in algorithms with exponential computational cost ( O ( n O ) or even higher) since such algorithms typically operate on all entries of the input matrix. Therefore, the goal is to develop efficient low rank decomposition algorithm that exploits the structure of the problem. When P itself admits a hierarchical low rank decom-position, P  X  H ( T ,k ), or P is indeed generated from a (correctly specified) latent tree graphical model, we can achieve kP  X  Qk 2 F = 0. The solution is how-ever usually not unique in terms of the factors in P erate different factors by applying invertible transfor-mations. Many previous spectral algorithms are spe-cial cases of this framework (  X  4.1).
 An interesting question arises when P itself does not admit a hierarchical low rank k 0 decomposition H ( T ,k 0 ), but we want to obtain a Q  X  H ( T ,k 0 which best approximates it (misspecified model with k 0 smaller than the true rank). In this case, the optimiza-tion is more difficult. Previous spectral algorithms in general cannot achieve the best objective and lack ap-proximation guarantees. In  X  4.2 we propose a new al-gorithm to cope with this situation which has approxi-mation guarantees and low computational complexity. 4.1. Correctly Specified Models Having the correct k , we can derive hierarchical low rank decompositions with zero approximation error. We first prove a matrix equality key to the decompo-sition. It will be applied recursively according to the latent tree structure, each time reducing the size of the latent tree (and the joint probability tables). This re-cursive algorithm also provides a new view of previous spectral algorithms for latent tree graphical models. Theorem 3 Let matrix P  X  R l  X  m have rank k . Let A  X  R l  X  k and B  X  R m  X  k be such that rank ( A &gt; P ) = k and rank ( PB ) = k . Then P = PB ( A &gt; PB )  X  1 A &gt; Proof Let the singular value decomposition of P be P = U U  X  U  X  R l  X  k and V  X  R m  X  k have orthonormal columns, U  X  and V  X  are their orthogonal complements and  X   X  R k  X  k is a diagonal nonsingular matrix. Then A and B can be represented A = UC + U  X  D and B = V E + V  X  F respectively, where C,E  X  R k  X  k . Then we obtain Note that since rank ( A &gt; P ) = rank ( V &gt; B ) = k , then C and E are nonsingular. Finally, we prove the claim noting that the r.h.s equals U  X  V &gt; = P .
 We can recursively apply Theorem 3 according to Algorithm 1. We only need to let P = P I 1 } ; { I 2 } , and replace the r.h.s. of equation (2) by F Furthermore, we will choose A and B such that PB and A &gt; P are easy to compute. In particular, when P is the reshaped probability matrix, A and B are chosen to marginalize (or sum) out certain variables. The latent tree model in Fig. 1. First, let us examine the case n = k , and all pairwise marginal distributions are invertible. Then we can reshape the joint probability table P ( X 1 ,...,X 6 ) according to edge e = ( X 7 ,X 10 ) and decompose the unfolding to
P | {z } P where A = I n  X  1 n sums out variable X 1 , and B = 1  X  1 n  X  1 n  X  I n sums out variables X 4 , X 5 and X 6 ( I is the n  X  n identity matrix, 1 n is a vector of all ones of length n ). We call the variables appearing in the the middle matrix ( X 2 and X 3 ) the linker variables. The choice of linker variables is arbitrary as long as they reside in different sides of edge e . Carrying out such decomposition recursively, where A = I n  X  1 n  X  1 n , and B = 1 n  X  I n . And then where A = I n  X  1 n and B = 1 n  X  I n . In the end only second and third order factors are left, and no further reduction of the order of the tensor can be made. When k &lt; n , the middle matrices for the linker variables are no longer invertible.
 For instance, P { 2 } ; { 3 } has size n  X  n , but P ( x 2 ,x 3 ) = P x means rank ( P { 2 } ; { 3 } )  X  k &lt; n . In this case, we can use a slightly modified A and B matrices and Theorem 3 still applies. More specifically, we will introduce matrices U 2 and V 3 with orthonormal where A = ( I n  X  1 n ) U 2 and B = ( 1 n  X  1 n  X  1 n  X  I ) V 3 perform marginalization and projection simul-taneously. A natural choice for U 2 and V 3 is provided by the singular value decomposition of P { 2 } ; { 3 } P decomposition recursively with U and V matrices in-troduced in each decomposition.
 A nice feature of the above recursive decomposition algorithm is that it never needs to access all entries in the tensor and it only works on small linker matrices ( e.g. , inverting P { 2 } ; { 3 } ). This hierarchical decompo-sition also provides latent tree graphical models with a representation using only marginal distributions of triplets of observed variables. Furthermore, many pre-vious spectral algorithms become special cases. Parikh et al. (2011); Song et al. (2011) pro-posed spectral algorithms for general latent tree graph-ical models. The main difference between their approach and our framework is in the linker ma-P where  X  is the pseudo inverse. We note that which is the k  X  n case under our framework.
 Hsu et al. (2009); Foster et al. (2012) derived spectral algorithms for hidden Markov models which are special latent tree graphical models. The build-ing block for the reduced dimension model of Foster et al. (2012) coincides with the decomposition from Theorem 3 although they derived their model from a very different perspective. Furthermore, they show that their model is equivalent to that of Hsu et al. (2009) by making use of the relation in (4). 4.2. Misspecified Models The algorithms we discussed in  X  4.1 assume that P it-self admits a hierarchical rank k decomposition, i.e. , P  X  H ( T ,k ). In practice, we do not know the exact hierarchical rank k for P , and we want to obtain an approximate latent tree graphical model by using a k 0 different from the true k (usually, k 0 &lt; k ). In this case, it becomes difficult to obtain a global optimum of the optimization problem in (3). And these spectral algorithms no longer have performance guarantees. In this section, we will consider a particular type of mis-specification in models: P  X  H ( T ,k ), but we supply the algorithms with a k 0 such that k 0 &lt; k , where we design a new algorithm with provable guarantees. When k 0 &lt; k , the matrix decomposition result used in previous spectral algorithms in general produces only an approximation, i.e. , P  X  PB ( A &gt; PB )  X  1 A &gt; Furthermore, given A and B (or given the edge where we split the tree), the middle linker matrix M = ( A &gt; PB )  X  1 is no longer the best choice. Instead, we will use a new linker matrix (Yu &amp; Schuurmans, 2011) which has a closed form expression as denotes the truncation of its matrix argument at k 0 -th singular value. When k 0 = k , the new decomposition reduces to the decomposition in  X  4.1.
 We can apply P  X  ( PB ) M  X  ( A &gt; P ) recursively ac-cording to Algorithm 1. Again, we only need to let P = P { I equation (2) by ( PB ) M  X  ( A &gt; P ) with Here we no longer have an exact decomposition in each step of the recursion, and the final hierarchical decom-position is an approximation to the original tensor. For the model in Fig. 1, its joint probability tensor P ( X 1 ,...,X 6 ) can be decomposed as | {z } P where A and B are set as before, and the matrix M P until only second and third order tensors are left. In the end, we obtain a set of low order tensors, and by combining them backwards we obtain an approxima-tion to the original tensor. We provide further analysis of the properties of the hi-erarchical decomposition in Algorithm 1 for misspec-ified models, including approximation guarantees, a sketch analysis of the sample complexity and compu-tational complexity. 5.1. Approximation Guarantee Applying Algorithm 1 with equation (6) in the mis-specified case does not provide a global optimal so-lution. It only constructs a particular Q  X  H ( T ,k 0 based on P . Nonetheless, we can provide an approx-imation guarantee which was considered in previous spectral algorithms.
 Theorem 4 Let the matricizations of P  X  H ( T ,k ) according to edge e i of the latent tree T be P and its best rank k 0 approximation error be i = archical rank k 0 decomposition using Algorithm 1 and equation (6). Assume rank( PB ) = rank( A &gt; P ) = rank( P ) in all recursive applications of (6), then where = max { i ,  X  e i } and d is the number of edges. Proof Suppose in each iteration we choose the edge to split in such a way that only one subtree needs to be further decomposed. We denote P L = PB = F equation (2). This means that we further decompose P
R as e P R , but not P L . Then the approximation error kP  X  X k 2 F can be bounded as k P  X  P L M  X  e P R k 2 F = k P  X  P L M  X  ( e P R  X  P R + P = k P  X  P L M  X  P R k 2 F + k P L M  X  ( e P R  X  P R ) k 2  X  + k P L M  X  ( e P R  X  P R ) k 2 F  X  + ... = d. (9) where in (8), we used ( P  X  P L M  X  P R ) &gt; P L M in (9), we used k P L M  X  ( e P R  X  P R ) k 2 F  X  and applied induction on the edges in the latent tree.
 Furthermore, under similar conditions, the new hier-archical decomposition is close to optimal.
 and Q X  X  ( T ,k 0 ) be obtained from Algorithm 1 based on equation (6). Then kQ X  X k 2 F  X  d kQ  X   X  X k 2 F where d is the number of edges in the latent tree T . the error for the best rank k 0 approximation to unfold-ing P i . However, Q  X  minimizes the same objective but with more constraints. Hence i  X   X  = kQ  X   X  X k 2 F for all e i . Using Theorem 4, this proves the claim. 5.2. Computational Complexity When we are provided finite samples only, let b P be the finite sample estimate of P needed in the decom-position in Algorithm 1. The major computations of the algorithm are repeatedly computing M  X  in equa-tion (5). First, we observe where b PB = Q B R B and A &gt; b P = R &gt; A Q &gt; A are QR-factorizations, and R B ,R A  X  R n  X  n are small square matrices. We note that QR-factorization is generally faster than SVD, and after QR-factorization, we can then compute SVD for much smaller matrices in equa-tion (6). Second, matrix B b P (similarly A &gt; b P and is the unfolding of certain joint probability table, and is extremely sparse: the number of nonzero entries is not larger than the number of data points and much smaller than the size of the matrix b P (which can be O ( n O )). We can exploit this fact and use sparse matrix operations. For instance, the QR-factorization needs to work only on the nonzero entries. This gives us a Q
B which has just a small number of nonzero rows (no larger than the number of data points). Although this new algorithm is more expensive than the previ-ous spectral algorithms, it is still much faster than the EM algorithm as we observed in the experiments. 5.3. Sample Complexity Given m samples, we only have a finite sample esti-mate b P of the the joint probability tensor P . Then we hierarchically decompose b P into b Q using Algorithm 1. The Frobenius norm difference between b Q and the true P can be decomposed into two terms k b Q X  X k F  X  k b Q X  X k F where Q is the hierarchical decomposition of P us-ing Algorithm 1. The result in Theorem 4 shows that the approximation error is bounded by thermore is determined by the tail of the singu-lar values of the matricizations P i of P . That is j -th singular value of its argument. We can see that when k 0 = k , = 0 and the approximation error is 0. In general, when k 0 &lt; k , the number of nonzero singu-lar value of P i can be exponential in its size, which is about O ( n O ). If the singular values decays very fast, then the approximation error can still be small. Estimation error can be bounded in two main steps: first, the empirical estimators for all joint probabil-ity matrices needed in the hierarchical decomposition have nice finite sample guarantees, typically of order O ( m  X  1 / 2 ); second, the estimation errors can accumu-late as we combine lower order tensor components to form the final joint probability tensor. For the mo-ment, we do not yet have a complete sample complex-ity analysis, which deserves a full treatment in a sep-arate paper. In sketch, we expand the error for the first iteration of the recursive decomposition ( P L , P R and e P R are defined similarly as in Theorem 4, and the versions with hat are finite sample counterparts): kQ X  b Qk F  X k b P L c M  X  b e P R  X  P L M  X  e P R k F  X  k P L M  X  ( b e P R  X  e P R ) k F + k ( b P L c M  X   X  P -where in the last inequality, we have ignored higher order error terms and use -to denote  X  X pproximately larger X . The occurrence of the k -th singular value P R in the denominator is due to the pseudo-inverse term in M  X  (see equation (6)). Then the error analysis for the subsequent steps of recursion can be carried out on k b e P R  X  e P R k F . Based on this sketch, the er-ror will accumulate exponentially with respect to the number of recursion, resulting in a sample complexity of O ( c O m  X  1 / 2 ) where c is some constant dependent on the singular values. However, in our experiments, we did not observe such exponential degradation. We compare the new algorithm with EM (Dempster et al., 1977) and spectral algorithms of Hsu et al. (2009); Parikh et al. (2011).
 Synthetic data. We generate synthetic data from latent tree models with different topologies: non-homogeneous hidden Markov models (NH HMM), ran-dom and balanced binary latent tree graphical models. We experimented with a variety of observed states n , hidden states k and approximation state k 0 .
 For an experiment on a given tree type with N train-ing points, we randomly generate 10 sets of model pa-rameters and sample N training points and 1000 test points for each parameter set. For EM, we learn the CPTs (with 5 restarts) based on the training points and the true latent tree topology, and then perform in-ference on test points using message passing. Conver-gence for EM is determined by measuring the change in the log likelihood at iteration t (denoted by f ( t )) the performance of estimating the joint probability of and we vary the training sample size N from 200 to 100,000, and plot the test error for inference in Fig. 2. Fig. 2 shows that our new algorithm performs the best when the sample sizes grows. Although EM performs the best for small training sizes, its performance levels off when the sample sizes go beyond 5,000 and our new algorithm overtakes EM. Furthermore, in our experi-ments when both n and k are reasonably large, previ-ous spectral algorithms become less stable. In terms of training time, the new algorithm is more expensive than previous spectral algorithms, but still much faster than the EM algorithm.
 Genome sequence data. We next consider the task of predicting poly ( A ) motifs in DNA se-quences (Kalkatawi et al., 2012) and consider the AATAAA variant of the motif. This is a binary se-quence classification problem (either the sequence has the motif or it doesn X  X ). For each sequence, we take a contiguous subsequence of 100 nucleotides, which we model as a non-homogeneous hidden Markov model with n = 4 observed states( A , T , C , G ). We then vary the training set size from 500 to 9500, while the test set size is fixed to 800. We compare our approach with EM and spectral for both k 0 = 2 and k 0 = 3. The classification results are shown in Fig. 3. For k 0 = 2, all algorithms are relatively comparable. For k 0 = 3, our new approach and EM also perform comparably. How-ever, spectral algorithm performs considerably worse as the number of hidden states increases. We approach the problem of estimating the param-eters of a latent tree graphical model from a hierar-chical low rank tensor decomposition point of view. Based on this new view, we derive the global opti-mization problem underlying many existing spectral algorithms for latent tree graphical models. We show that these existing algorithms obtain a global opti-mum when the models are correctly specified. How-ever, when the models are misspecified, these spectral algorithms are no longer optimal. Based on our frame-work, we derived a new decomposition algorithm with provable approximation guarantee and show the em-pirical advantages of our approach. In future, we will perform statistical analysis of this new algorithm and investigate potentially better algorithm.
 Balle, Borja, Quattoni, Ariadna, and Carreras, Xavier.
Local loss optimization in operator models: A new insight into spectral learning. In Proceedings of the International Conference on Machine Learning , 2012.
 Blei, D., Ng, A., and Jordan, M. Latent Dirichlet allocation. Journal of Machine Learning Research , 3:993 X 1022, January 2003.
 Clark, A. Inference of haplotypes from PCR-amplified samples of diploid populations. Molecular Biology and Evolution , 7(2):111 X 122, 1990.
 Dempster, A. P., Laird, N. M., and Rubin, D. B. Max-imum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B , 39(1):1 X 22, 1977.
 Foster, D.P., Rodu, J., and Ungar, L.H. Spectral di-mensionality reduction for hmms. Arxiv preprint arXiv:1203.6130 , 2012.
 Grasedyck, Lars. Hierarchical singular value decompo-sition of tensors. SIAM Journal on Matrix Analysis and Applications , 31(4):2029 X 2054, 2010.
 Hoff, Peter D., Raftery, Adrian E., and Handcock,
Mark S. Latent space approaches to social network analysis. Journal of the American Statistical Asso-ciation , 97(460):1090 X 1098, 2002.
 Hsu, D., Kakade, S., and Zhang, T. A spectral al-gorithm for learning hidden markov models. In
Proc. Annual Conf. Computational Learning The-ory , 2009.
 Kalkatawi, M., Rangkuti, F., Schramm, M., Jankovic,
B.R., Kamau, A., Chowdhary, R., Archer, J.A.C., and Bajic, V.B. Dragon polya spotter: predictor of poly (a) motifs within human genomic dna se-quences. Bioinformatics , 28(1):127 X 129, 2012. Oseledets, IV. Tensor-train decomposition. SIAM
Journal on Scientific Computing , 33(5):2295 X 2317, 2011.
 Parikh, A., Song, L., and Xing, E. P. A spectral al-gorithm for latent tree graphical models. In Pro-ceedings of the International Conference on Machine Learning , 2011.
 Rabiner, L. R. and Juang, B. H. An introduction to hidden Markov models. IEEE ASSP Magazine , 3 (1):4 X 16, January 1986.
 Song, L., Parikh, A., and Xing, E.P. Kernel embed-dings of latent tree graphical models. In Advances in
Neural Information Processing Systems , volume 25, 2011.
 Yu, Y. and Schuurmans, D. Rank/norm regularization with closed-form solutions: Application to subspace clustering. In Conference on Uncertainty in Artifi-
