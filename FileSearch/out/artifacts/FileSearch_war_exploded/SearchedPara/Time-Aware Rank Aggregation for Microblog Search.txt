 We tackle the problem of searching microblog posts and frame it as a rank aggregation problem where we merge result lists generated by separate rankers so as to produce a final ranking to be returned to the user. We propose a rank aggregation method, TimeRA, that is able to infer the rank scores of documents via latent factor model-ing. It is time-aware and rewards posts that are published in or near a burst of posts that are ranked highly in many of the lists being ag-gregated. Our experimental results show that it significantly outper-forms state-of-the-art rank aggregation and time-sensitive micro-blog search algorithms.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Rank aggregation; data fusion; ad hoc retrieval; microblog search The task of searching microblog posts has been studied widely. Many effective algorithms have been proposed, including, for ex-ample, those based on language modeling [9], query expansion [21], and time-sensitive features [3]. Different microblog search algo-rithms have different merits; hence, combining these alternative al-gorithms into a single model may boost retrieval performance. In this paper we follow this idea and tackle the challenge of microblog search by following a rank aggregation (also called data fusion ) ap-proach. Our input consists of a number of ranked lists generated by different microblog search algorithms and the output is an aggre-gated and, hopefully, better list.

Rank aggregation approaches can inherit the merits of the indi-vidual search algorithms whose outputs are being aggregated. Also, they may boost recall [31] and because they aggregate the results of multiple strategies, they mitigate risks associated with opting for a single strategy [31]. In rank aggregation, documents in the fused list are ranked in deceasing order of their fusion scores. The fu-sion score of a document is usually a function of the rank scores from the individual input lists, such as the weighted sum. Previ-ous work on rank aggregation often assumes X  X ither implicitly or explicitly X  X hat the rank score of a document is set to zero for an input list if that document does not appear in the list. We challenge this assumption; our intuition is that documents that are similar to a document d that does occur in an input list, should get similar rank scores. Let us call a document that does not appear in list L , but does appear in other lists we aim to fuse, a missing document for L (see Figure 1). We propose a rank aggregation algorithm where we apply a latent factor model to predict the rank scores of missing documents. In our rank aggregation algorithm, we define a list-document rank score matrix, factorize this matrix, and utilize the factorized list-specific and document-specific matrices to assign scores to missing documents.

State-of-the-art rank aggregation methods often work on the as-sumption that only documents that are ranked highly in many of the result lists being aggregated are likely to be relevant [1, 4, 5, 11, 13, 22]. As a result, a relevant document might be ranked low in an ag-gregated list if it appears in only few of the input result lists and is ranked low within these. The characteristics of microblog en-vironments suggest a different perspective. In such environments people tend to talk about a topic within specific short time inter-vals [10, 20, 23]. E.g., people talked about the  X 2014 Eastern Syn-chronized Skating Sectional Championship X  mainly between Jan-uary 30 and February 1 2014, which is when the championship took place. Posts generated before or after the event are less likely to talk about the championship and, hence, less likely to be relevant to a query about the championship.

Inspired by these characteristics of microblog search scenarios, we propose a rank aggregation algorithm, TimeRA, that is time-aware. TimeRA detects windows (intervals) of timestamps of posts ranked high in many of the lists to be fused. These windows give rise to bursts of posts, and subsequently TimeRA rewards posts that are published in the vicinity of a burst that contains highly scored posts. Specifically, if a post d i and posts d 1 , . . . , d within the same narrow time window, and the posts d 1 , . . . , d ranked highly in many of the lists being merged, then post d  X  X ewarded, X  even if, in the extreme case, it appears in only one list where it is ranked low. Fig. 1 illustrates this: post d 2 in list L 1 but will be rewarded as it was published in a time window in which a large number of posts are published that are ranked high Figure 1: Rewarding posts that are published in the same nar-row time frame as a large number of (supposedly) relevant posts. The vertical axis indicates the summed scores of posts with the same timestamps. There are m lists in response to a query. Post d 2 only occurs in list L 1 and is ranked low; d occurs in a single list, L m , but is ranked very high. Surround-ing d 2 there are many documents that are ranked high in many lists; therefore, d 2 should be  X  X ewarded, X  while d 8 should not. Documents marked in blue are  X  X issing X  documents; e.g., d is a missing document for L 1 , as it is not observed in L the aggregation process. in many lists. But d 8 , while ranked high in L m , receives no such bonus as it was published outside the time window.

Our results show that TimeRA is able to effectively aggregate result lists, subject to real-time search requirements, running al-most as fast as standard fusion methods such as CombSUM [5], and outperform both time-sensitive microblog search methods and competing fusion algorithms.

Our contributions can be summarized as follows:
Three major types of research are closely related to our work: microblog search, rank aggregation and latent factor modeling.
Searching posts that are short and creatively spelled in microblog-ging platforms, like Twitter, is challenging. Previous work on micro-blog search has focused on employing traditional IR techniques, i.e., learning to rank [6], document expansion [19], pseudo-relevance feedback retrieval [21] to improve the performance. Some previ-ous work explores time information to boost performance. For in-stance, Li and Croft [12] propose a time-sensitive language model to explore time and relevance of queries; Massoudi et al. [19] inte-grate temporal information into a dynamic expansion microblog re-trieval model. And Dakka et al. [3] consider the publication time of documents in conjunction with the topic similarity to derive a time-sensitive ranking. More recently, Miyanishi et al. [21] integrate temporal and lexical evidence into their query expansion method for microblog search.

We propose to develop a rank aggregation-based approach to microblog search. To the best of our knowledge, this is the first attempt to tackle the problem of microblog search as a rank aggre-gation problem.
Rank aggregation approaches attempt to inherit the merits of dif-ferent search algorithms, and to combine their result lists in order to produce a new and hopefully better ranking [14, 16, 26]. In recent years, rank aggregation has featured in many applications, includ-ing federated search [28], vertical search [2], and more recently, search result diversification [14, 15]. Many rank aggregation meth-ods have been proposed, including CombSUM and CombMNZ [26], an outranking approach [14, 31], semantic fusion [16], cluster-based fusion [7] and  X  -Merge [27]. CombSUM and CombMNZ are the oldest but fast and effective rank aggregation methods. The out-ranking approach only utilizes the rank of documents in the result lists during aggregation. Both semantic fusion and cluster-based fusion use the assumption that content-similar documents should be clustered to boost the performance of aggregation. Finally,  X  -Merge tries to learn a model from labeled data for aggregation.
Previous rank aggregation algorithms ignore the temporal infor-mation of the documents to be merged. We propose a new rank aggregation where we fully utilize the temporal information and re-ward posts that are published in the vicinity of a burst to be ranked higher in the fused list. To the best of our knowledge, this is the first attempt to integrate temporal information into rank aggregation.
Latent factor models are often used in collaborative filtering (CF) and recommender systems [8, 25]. Matrix factorization methods form a group of well-known latent factor models, that include, for instance, singular value decomposition [8], probabilistic ma-trix factorization [25] and social regularization [18].These methods first model users with latent interests and the products with latent features by matrix factorization, and then try to predict the rating of products for the given users with the observations of the existing users X  rating data [8, 18, 25]. Motivated by latent factor models, rather than simply setting no rank scores for missing documents as in previous rank aggregation methods [5, 7, 27, 31], our proposed rank aggregation algorithm applies matrix factorization to model both result lists (called  X  X sers X  in CF) and documents ( X  X roducts X  in CF) as a mixture of latent factors ( X  X nterests X  in CF), such that the rank scores of missing documents ( X  X he rating of unobserved products X  in CF) in a result list for aggregation can be inferred. Reasons of why a latent topic model can infer scores can be found in [8, 18, 25].

To the best of our knowledge, this is the first attempt to predict rank scores for missing documents in rank aggregation.
We detail the task that we address and recall two standard rank aggregation algorithms, CombSUM and CombMNZ.
The task we address is this: given an index of microblog posts, a query, and a set of ranked lists of posts with timestamps produced in response to the query, aggregate the lists into a final ranked list of posts to be returned in response to the query. Hence, the rank aggregation problem consists of finding a ranking function f that: where L is a set of lists to be fused, L i is the i -th result list, m = |L| is the number of input result lists, C is the corpus of microblog posts, q is a query, L f is the final fused list.
Before we move on to the details of our rank aggregation method, we set the stage regarding rank aggregation by briefly recalling two standard fusion methods: CombSUM and CombMNZ. Let R ij de-note the rank score of document d j in list L i based on the rank of d in the list L i ; in the literature on rank aggregation [5, 7, 11, 31], one often finds R ij = 0 if d j /  X  L i ( d j still in the combined set of posts C L := S m i =1 L i ). In both CombSUM and CombMNZ, R often defined as: where | L i | is the length of L i and rank( d j | L i )  X  { 1 , . . . , | L the rank of d j in L i . The well-known CombSUM fusion method [5, 31], for instance, scores d j by the sum of its rank scores in the lists: while CombMNZ [5, 31] rewards d j that ranks high in many lists: where |{ L i : d j  X  L i }| is the number of lists in which d
We detail our time-aware rank aggregation algorithm in this sec-tion;  X 4.1 describes the way we detect bursts,  X 4.2 details our ag-gregation method, TimeRA, and  X 4.3 contains a brief complexity analysis of TimeRA.

To be able to present rank aggregation methods in a uniform way, we first introduce and summarize our notation in Table 1. Although introduced some time ago, CombSUM and CombMNZ are still ef-fective and very efficient rank aggregation methods. More recent fusion methods have been proposed but most of them may be in-appropriate for searching posts in microblogging scenarios that are dynamic and need rapid updates. For instance, cluster-based rank aggregation [7] and  X  -Merge [27] have been shown to be more ef-fective than CombSUM and CombMNZ in several cases. However, a drawback of cluster-based fusion is that it has to access the con-tent of documents to compute inter-document similarities so as to generate a set of clusters and determine probabilities based on the clusters. A drawback of  X  -Merge is that it also has to access the content to extract features and it often overfits [27].
Our proposed TimeRA method utilizes burst information. To detect bursts, we proceed as follows. Let t be a timestamp. Let d  X  C L denote a post with timestamp t . Here, C L is the set of posts appearing in the result lists. We regard posts published during the same hour as having the same timestamp. Although it is possible to define  X  X he same timestamp X  in many different ways, we found that this is a suitable level of granularity for the effectiveness of rank aggregation. Before we detect bursts in posts C L , we need to define H t , the burst-time score at time t . Let R kt Eq. 1) be the rank score of d t given L k . Then: where T is the total number of different timestamps belonging to posts in C L , C t is a set of posts having the same timestamp t , P the same timestamp t , and P T t 0 =1 P d of the rank scores of all posts in C L . According to Eq. 2, the burst-time score H t &gt; 0 if it is above the average score (i.e., 1 /T ), otherwise H t  X  0 .

Using Eq. 2, we compute a burst-time score H t for each time point t  X  X  t 1 , t 2 , . . . , t T } . In this manner we generate a burst-time score sequence segment T , is a maximal segment in ii. No proper super-segments of As an example, consider the input sequence  X  3 ,  X  4 ,  X  1 ,  X  3 , 5 ,  X  1 , 3 ,  X  2 } . The maximal segments in this 3 } is not maximal, since it has a nonempty zero-scoring prefix { 2 ,  X  2 } appending to the left of { 4 , 3 } ; { 5 } is not a maximal segment, since { 5 ,  X  1 , 3 } has a total higher score of 7.

Each maximal segment b [ t i : t j ] with start timestamp t i and end timestamp t j : it contains any post d  X  C L whose timestamp is between t i and t j . We write B to denote the set of all bursts , i.e., B = S b [ t i : t short for b [ t i : t j ] in the following. Next we detail our time-aware rank aggregation method, Time-RA. TimeRA utilizes matrix factorization techniques. The input of the matrix factorization in TimeRA is an m  X  n matrix R  X  R m  X  n which we call a list-document matrix; here, again, m is the number of result lists and n is the number of posts to be fused, i.e., n = |C L | . R is initialized by Eq. 1, viz., its elements R defined by Eq. 1. The output of the matrix factorization consists of two new matrices S  X  R A  X  m and V  X  R A  X  n , obtained by scores of the elements in the sequence. Algorithm 1: TimeRA: Time-Aware rank aggregation.
 factorizing R , which we call the factor-list matrix and the factor-post matrix, respectively. Here, A is the number of latent factors. After obtaining S and V , we can infer the normalized scores of missing posts, based on which we arrive at the aggregation scores of all the posts.

We present TimeRA in Algorithm 1. We first provide a high-level walkthrough. To begin, it sets matrices S  X  R A  X  m R
A  X  n with random values and f TimeRA ( d | q ) with the value 0 (lines 1 X 3 in Algorithm 1). Let S i be the i -th column of S , meant to get the latent factors of the list L i after matrix factorization, and let V j be the j -th column of V , meant to get latent factors of post d after matrix factorization. After detecting bursts (line 4), TimeRA utilizes burst information and tries to get the final S and V by per-forming a gradient descent process on a cost function C 2 12). To this end, aggregation scores f TimeRA ( d | q ) of d  X  C be obtained by S and V (lines 13 X 14). The defined cost function plays an important role: (1) it tries to keep the original rank scores of posts that rank high in many of the lists to be fused, (2) it rewards posts that rank low in a few lists but in the vicinity of bursts, and (3) it gets the latent factors of both the lists and the posts such that missing posts X  rank scores can be inferred.

Our matrix factorization approach in TimeRA seeks to approx-imate the list-document matrix R by a multiplication of A latent factors, To obtain S and V , traditionally, the Singular Value Decomposition (SVD) method [8] is utilized to approximate the list-document rank matrix R by minimizing where || X || 2 F denotes the Frobenius norm. Due to the definition that R ij = 0 if d j is a missing document, i.e., d j  X  C L \ L i need to factorize the observed scores in R so that (4) changes to where I ij is an indicator function that is equal to 1 if d L i and equal to 0 otherwise, S i  X  R A  X  1 is a column vector in S that serves to get latent factors of L i , and, similarly, V is a column vector in V that serves to get latent factors of d R ij  X  [0 , 1] according to Eq. 1, (5) can be rewritten as where g ( x ) is the logistic function defined by g ( x ) = 1 / (1 + exp(  X  x )) , which makes it possible to bound the range of S within the same range [0 , 1] by R ij .

In order to avoid overfitting, two regularization terms are added to Eq. 6: where  X  1 , X  2 &gt; 0 . For a probabilistic interpretation with Gaussian observation noise for Eq. 7 we refer to Salakhutdinov and Mnih [25]. To reduce the model X  X  complexity we set  X  1 experiments below [8, 18, 25].

The cost function defined by Eq. 7 punishes posts equally when they shift from the original rank scores R ij . However, a post ranked higher should be punished more than posts ranked lower if they shift from the original rank scores: higher ranked posts are more likely to be relevant so that keeping their original rank scores will be of more value. Thus, we modify Eq. 7 to obtain the following cost function C 1 : S , V = arg min where w ( d j | L i ) is a rank punishment weighting function defined for d j given L i . Specifically, we define w ( d j | L i Our next step is to bring in burst information. After detecting a set of bursts, we introduce the following item into the cost function C to boost the relevance of posts by burst information, such that where B is a set of detected bursts, b is a burst in B , d d is ranked higher than d j in list L i , |{ d k  X  b : d k L total number of posts in the burst b that are ranked higher than the post d j in L i , and r ( d j | d k ) is the  X  X eward X  score for d if d j is within or near the burst b to which d k belongs. In Eq. 10, d
L i d j indicates that only the posts ranked higher than the post d to be rewarded are capable of boosting the relevance of d
If d j , the post to be rewarded, is generated at (almost) the same time as the highly relevant post d k , it will be rewarded more than posts that are further away in time from d k , which is measured by r ( d j | d k ) in Eq. 10. Accordingly, we define r ( d j normal distribution as where t d j and t d k are the timestamps of post d j and d tively, and  X  b is the standard deviation of timestamps in burst b : where n b is the number of different timestamps of posts in b . Ac-cording to Eq. 11, the more likely d j is generated at the same time with d k , the larger the score r ( d j | d k ) is, resulting in a bigger re-ward for d j from d k .
 We are almost ready now to define the cost function C 2 that we use in TimeRA. We integrate the original rank score of the posts in the result lists (Eq. 8) with rewards for posts generated in the vicinity of bursts (Eq. 10). To this end, we substitute Eq. 10 into Eq. 8. Our cost function C 2 for TimeRA is defined as arg min where  X   X  [0 , 1] is a free parameter that governs the linear mixture.
In words, in response to a query q , TimeRA uses a two-com-ponent mixture model to score d  X  C L . The first component (the first term of Eq. 12) tries to maintain the original rank scores R The second component (the second term of Eq. 12), which uses bursts for posts,  X  X ewards X  d if it is strongly associated with the posts in the bursts. Clearly, if  X  = 0 in Eq. 12, TimeRA will only try to maintain the original rank scores, i.e., Eq. 8.

A local minimum of the objective function given by Eq. 12 can be found by performing gradient descent in both S i and V same can apply to Eqs. 4, 5, 6, 7, and 8. The algorithm for obtain-ing S and V is straightforward: we first randomly initialize S and V , then iteratively update these two matrices based on their gradi-ents until the value of the cost (objective) function converges. The derivation of these equations are included in Appendix A.
After optimizing Eq. 12, posts d j  X  L i they will end up with a score g ( S &gt; i V j ) = R ij +  X  X ome reward. X  Unlike previous work that assigns 0 to missing documents d j  X  X  L \ L i [5, 7, 27, 31], we infer a rank score R L i d j for missing posts d j as: where min( R id ) is the minimum rank score of the lowest ranked post that appears in L i as computed by Eq. 1. As shown in Eq. 13, if the inferred rank score g ( S &gt; i V j ) is smaller than the minimum rank score, we maintain the inferred score for that post. However, if the inferred rank score is greater than the minimum rank score, we give up the inferred score and let R ij = min( R id ) , as d means that it should at least be ranked lower than any post d  X  L
The final rank aggregation score for d j  X  X  L is obtained by The final rank aggregation score for a post consists of two parts, i.e., scores for lists L i it appears in (the first term in Eq. 14) and scores for lists it does not appear in (the second term in Eq. 14), as inferred by Eq. 13. Finally, the final fused list L f response to q is obtained by ranking posts in decreasing order of f
TimeRA ( d j | q ) . We call the model defined by Eq. 14, TimeRA , and the variant that ignores inferred rank information R ij in Eq. 14 is called TimeRA-Infer ( X  X imeRA minus Infer X ).
The main computation of TimeRA is detecting bursts and its gra-dients against matrices S and V . Our burst detection has compu-tational complexity O ( n ) because the detection algorithm runs in linear time [24], and the gradients of C 2 have computational com-plexity of O ( e  X  m  X  n  X  A ) . In practice, the number of result lists to be fused, m , the number of posts to be aggregated, n , the number of latent factors, A , and the number of epochs, e , are all quite small. Therefore, the rank aggregation procedure can run in real-time, as we will see in the experiments presented in  X 6.6.
In the limiting case, TimeRA reverts back to CombSUM. That is, if we set  X  = 0 (ignoring burst information), set the weight function w ( d | L i ) = 1 for all documents d and lists L over, set  X  1 =  X  2 = 0 , and do not try to infer the rank score of posts, then g ( S &gt; i V j ) = R ij after performing gradient descent in Eq. 12. Our experimental results below show that the performance of TimeRA is almost the same as CombSUM when  X  = 0 . Note that  X  1 , X  2 6 = 0 in the experiments.
In this section, we list our research questions, describe the data set, specify our baselines, detail the metrics as well as our training and optimization setup, and describe the experiments. The research questions guiding the remainder of the paper are: RQ1 Does TimeRA outperform traditional and state-of-the-art un-RQ2 What are the relative contributions of the main ingredients of RQ3 What is the effect of using burst information in TimeRA (i.e., RQ4 What is the effect of the number of lists to be aggregated in RQ5 Can we observe the hypothesized effect sketched in Fig. 1, RQ6 Does TimeRA meet real-time search requirements? ( X 6.6) RQ7 Does TimeRA beat existing time-sensitive microblog search
In order to answer our research questions we use the Tweets 2011 corpus [17, 29] provided by the TREC 2011 and 2012 Microblog tracks. The collection is comprised of around 16 million tweets collected over a period of 2 weeks (January 23, 2011 until Febru-ary 8, 2011). Different types of tweets are present in this data set, including replies and retweets, each with their own timestamp.
The task at the TREC 2011 and 2012 Microblog tracks was: given a query with a timestamp, return relevant and interesting tweets in reverse chronological order. In response to a query, a run was required to retrieve 30 posts. In total, for the 2011 track 49 test topics were created and 2965 tweets were deemed relevant; some topics have just two relevant tweets, while others have more than 100 relevant tweets.

A total of 59 groups participated in the TREC 2011 Microblog track, with each team submitting at most four runs, which resulted in 184 runs 2 [17]. The official evaluation metric was precision at 30 (p@30) [17, 29]. The p@30 scores of these 184 runs varied dramatically, with the best run achieving a p@30 score of 0.4551 and the worst runs achieving scores below 0.10. Details about the implementation of each run can be found in [17, 29]. We also run experiments on the TREC 2012 Microblog track runs, and find that they yield the same overall results and trends. Due to space limita-tions, we only report results for the TREC 2011 edition runs below.
We compare TimeRA to 5 aggregation baselines: 2 traditional unsupervised methods, i.e., CombSUM, CombMNZ, 2 start-of-the-art cluster-based fusion methods, ClustFuseCombSUM and Clust-FuseCombMNZ [7], and a start-of-the-art supervised method,  X  -Merge [27], in which we integrate temporal features. As TimeRA utilizes temporal information, we also compare TimeRA to 4 state-of-the-art time-sensitive microblog search algorithms: time-based language model (TBLM) [12], textual quality factor model with temporal query expansion (LM-T(qe)) [19], direct time-sensitive BM25 retrieval model (DIRECT-BM25 (mean)) [3] and temporal tweet selection feedback method (TSF+QDRM) [21]. To build the index of the dataset that some of our baselines require, we apply Porter stemming, tokenization, and stopword removal (using IN-QUERY lists) to posts using the Lemur toolkit. 3 Features used in  X  -Merge, including time-sensitive features, and the settings of  X  -Merge are briefly described in Appendix B.

For performance evaluation we use the official TREC Microblog 2011 metric, p@30. We also report on p@5, p@10, p@15 and MAP scores. MAP scores are of special interest to us: we hypoth-esize that TimeRA has both a precision and recall-enhancing effect and we use MAP to measure this. Statistical significance of ob-served differences between two runs X  performances is tested using a two-tailed paired t-test and is denoted using N (or H ) for signifi-cant differences for  X  = . 01 , or M (and O ) for  X  = . 05 .
A single free parameter in Eq. 12,  X  (  X  { 0 , 0 . 1 ,..., 1 } ), is in-corporated in TimeRA, which is set using leave-one-out cross vali-dation performed over the entire set of 49 queries. The performance of MAP is optimized in the learning phase. In other words, the per-formance for a query is attained using a value of  X  that maximizes MAP performance over all other queries. The same optimization strategy is used for one of our baselines, cluster-based fusion. Other baselines do not incorporate free parameters. Following [18], we set the parameters  X  1 =  X  2 = 0 . 001 .
We report on 7 main experiments in this paper aimed at under-standing (1) the performance of TimeRA in general via sampling lists and fusing them; (2) the contribution of the main ingredients in TimeRA; (3) the performance of TimeRA with increasing numbers of runs to be fused; (4) query level performance; (5) TimeRA X  X  effi-ciency; (6) the effect of inferring rank scores of posts by TimeRA; (7) the performance of TimeRA against temporal retrieval models.
To understand the overall performance of TimeRA, we sample  X  10% from the ranked lists produced by participants in the TREC 2011 Microblog track based on the lists X  p@30 distribution: 18 out of the runs submitted to the TREC 2011 Microblog track, 6 with p@30 scores between 0.20 and 0.30 (Class 3), 6 between 0.30 and 0.40 (Class 2), and 6 over 0.40 (Class 1). We also randomly choose two runs from each class to construct Class 4; see Table 2. The runs in Class 1 are the 6 best runs in the TREC 2011 Microblog track. In every class, we use run1, run2, run3, run4, run5 and run6 to refer to the runs in descending order of p@30.

Next, to understand the contributions of the two main ingredi-ents of TimeRA, viz., burst detection and inferring scores, we make comparisons among TimeRA, TimeRA-Infer and CombSUM. We also gradually increase the parameter  X  in Eq. 12 from 0.0 to 1.0 to see if burst information is helpful to boost fusion performance.
To understand the effect of the number of lists being merged, we randomly choose k = 2 , 4 , 6 , . . . , 36 lists from the 184 lists and aggregate them. We repeat the experiments 20 times and report the average results and standard deviation. In order to understand the query-level performance of TimeRA, we provide a detailed com-parison of its performance against the baseline methods. To deter-mine whether TimeRA can respond to a given query in (near) real time, we again randomly fuse k = 2 , 6 , 12 , 18 , 30 lists for all 49 test queries and report the average time required. Finally, we com-pare TimeRA against state-of-the-art time-sensitive retrieval mod-els that utilize time/burst information.  X 6.1 and  X 6.2 show the results of fusing the sample lists, the contributions of burst detection and score inference in TimeRA, re-spectively;  X 6.3 analyzes the effect of using burst information;  X 6.4 shows the effect of the number of lists on the overall performance;  X 6.5 provides a topic-level analysis;  X 6.6 examines the run times. Finally,  X 6.7 compares TimeRA against time-sensitive models.
The performance of TimeRA and the 5 baselines is presented in Table 3, with numbers based on the  X  10% sample mentioned in  X 5.4. The performance of all the fusion methods is better than that of the best performing result list that is used in the merging process (run1) for all classes and on almost all metrics. Many of these improvements are statistically significant. More importantly, when fusing the top 6 result lists (Class 1), all of the p@30 scores generated by any rank aggregation method are higher than that of the best run in TREC 2011 Microblog track (0.4551), especially for TimeRA, which achieves 0.5531. These findings attest to the merits of using rank aggregation methods for microblog search.
It is also worth noting in Table 3 that in almost all cases, the cluster-based method does not beat the standard fusion method that it integrates, and the performance differences between the two are usually not significant. The reason behind this may be that it is challenging to do clustering in a microblog environment, with lim-ited amounts of text and very creative language usage.
The performance of TimeRA is better than that of the baseline methods, and all of the differences are substantial and statistically significant. The performance of  X  -Merge is almost the same as that of CombSUM, CombMNZ and the cluster-based methods when fusing the lists in Class 2, but in the other classes the performance tends to be a bit below that of the other methods, on all metrics. This may be due to overfitting.

Interestingly, the higher the quality of the result lists that are be-ing aggregated, the bigger the improvements that can be observed in Table 3. For instance, the p@30 scores after aggregation are highest in Class 1 followed by those in Class 2 and Class3, and the quality of Class 1 is best followed by Class 2 and Class 3, respec-tively. The p@30 aggregation scores in Class 4 are almost the same as those in Class 2, as some of the lists X  scores in Class 4 are better than those in Class 2.
Next, we compare the relative contributions of the main ingredi-ents of TimeRA against the second best baseline, viz., CombSUM: burst detection and score inference. The effect of burst detection in TimeRA can be seen through comparisons between TimeRA-Infer and CombSUM; the effect of score inference can be seen through comparisons between TimeRA and TimeRA-Infer in Fig. 2.

Interestingly, in Fig. 2 there are large gaps in performance be-tween TimeRA-Infer and CombSUM in terms of all of metrics; all improvements are statistically significant. This illustrates that burst detection makes an important contribution to the performance of rank aggregation. When we compare TimeRA and TimeRA-Infer in Fig. 2, we see that the performance of TimeRA-Infer in terms of p@5 is almost the same as that of TimeRA, while in terms of p@10 and p@30, TimeRA has some small advantages over TimeRA-Infer X  X ome of these improvements are statistically significant. This observation confirms that inferring scores for posts during aggregation can boost performance as well. It also shows, however, that enhancing the performance of p@ k becomes easier for larger values of k . This is because the cost of boosting the performance (i.e., changing the original rank score to be higher) is smaller when the posts are ranked lower. TimeRA is unable to beat TimeRA-Infer in terms of p@5 (.6939 for both), but TimeRA does boost the p@30 performance (.5531 M for TimeRA vs .5405 for TimeRA-Infer).

As burst information is such an important contributor to the per-formance of TimeRA, we analyze it further in  X 6.3.
Next we examine the effect of using different amounts of burst information or cluster information in our time-aware aggregation or cluster-based methods, respectively. What is the impact of the free parameter  X  in Eq. 12 and in the cluster-based methods? Fig. 3 depicts the MAP performance curves for all rank aggregation meth-ods when fusing the lists in Class 1, Class 2, Class 3 and Class 4, respectively. For  X  = 0 , TimeRA almost amounts to CombSUM, while the cluster-based methods are the same as the standard fu-sion methods they incorporate, e.g., ClustFuseCombSUM has no difference with CombSUM in this case; more weight is put on burst information and cluster information with higher values of  X  in TimeRA and the cluster-based methods, respectively. For 0 &lt;  X  &lt; 1 , both the CombSUM scores of posts and burst in-formation are utilized for aggregating lists in TimeRA.
 In each of our four classes of runs, when aggregating lists, the MAP scores of TimeRA where burst information is used (  X  &gt; 0 ) are always higher than that of any other fusion method. In Class 1 and Class 4 the gain increases gradually as the weight of burst in-formation increases. These findings attest to the merits of using burst information to boost the performance in fusing ranked lists for microblog search. Putting more weight on cluster information in the cluster-based methods hurts performance in many cases.
We explore the effect of varying the number of lists to be merged on the performance of TimeRA. Fig. 4 shows the fusion results of randomly sampling k  X  { 2 , 4 , 6 ,..., 36 } lists from the 184 lists. For each k , we repeat the experiment 20 times and report on the average scores. We use CombSUM as a representative example for comparisons with TimeRA; the results of other baseline methods are worse or qualitatively similar to those of CombSUM.
 From Fig. 4 we can see that TimeRA performs better than Comb-SUM over all performance evaluation metrics no matter how many lists are fused. For both precision metrics (p@5 and p@30) we find that as long as the number of lists  X  10 , the performance of both TimeRA and CombSUM gradually increases as the number of lists to be merged increases. The increases level off when the number of lists exceeds 12 . For MAP we find that performance keeps increas-ing until we fuse 26 lists; then, the performance increase levels off. Interestingly, in Fig. 4 the improvement of TimeRA over Comb-SUM on p@5 becomes smaller when more lists are merged. For example, the increase in p@5 of TimeRA over CombSUM is .1063 (.5861 for TimeRA vs .4798 for CombSUM) when two lists are fused. The performance increase, however, drops to only .0281 (.6712 for TimeRA vs .6431 for CombSUM) for 36 lists. Looking at the other metrics, which take a larger part of the merged result into account (p@30 and especially MAP), the gap remains.
We take a closer look at per test query improvements of TimeRA over other runs. For brevity, we only consider CombSUM as a rep-resentative and we only consider runs in Class 1. The results of TimeRA against CombSUM for other classes of runs and for other baseline methods are qualitatively similar. Fig. 5 shows per query performance differences in terms of AP, p@5, p@10 and p@30, respectively, between TimeRA and CombSUM. TimeRA displays both a precision and recall enhancing effect (with increases in pre-cision oriented metrics as well as in MAP). As the metric at hand considers a larger chunk of the result list, there are more instances where TimeRA outperforms CombSUM. This is due mainly to top-ics that are discussed only in very specific time intervals. Exam-ples include queries MB010 (Egyptian protesters attack museum), MB011 (Kubica crash) and MB015 (William and Kate fax save-the-date) etc. For such queries we found evidence of the intuition depicted in Fig. 1: posts that are ranked low in a small number lists but that TimeRA pushes up because they are central to a burst. E.g., in response to query MB010, post #30354903104749568 is ranked near the bottom in just two lists (at ranks 26 and 27 in the runs clarity1 and DFReekLIM30, respectively). Many posts for the query were generated around the same time interval (Jan. 26 X 29) and are ranked high in many lists; post #30354903104749568 was also published around this time and ranked 6th in the merged list because of this.

Queries for which TimeRA cannot beat CombSUM tend to be quite general and unrelated to any specific time windows. Exam-ples include queries MB023 (Amtrak train service), MB027 (re-duce energy consumption) and MB029 (global warming and weath-er) etc. For a very small number of queries, TimeRA X  X  performance is slightly worse than that of CombSUM. One reason that we ob-served for this phenomenon is that not all posts that are ranked low in a small number of lists but central to a burst need to be rewarded. An example here is query MB024 (Super Bowl, seats).
We now explore how fast TimeRA can merge result lists in re-sponse to a query. TimeRA is developed in C++ and the experi-ments are run on a 10.6.8 MacBook Pro computer with 4GB mem-ory and a 2.3 GHz Intel core i5 processor. In Table 4, we randomly choose k  X  { 2 , 6 , 12 , 18 , 30 } lists from the 184 lists. For each k , we repeat the experiment 20 times and report the average run time per query (in seconds) that the fusion methods require. ClustSUM and ClustMNZ are short for ClustFuseCombSUM and ClustFuse-CombMNZ, respectively in Table 4.

TimeRA does not run as fast as CombSUM or CombMNZ, but it manages to merge lists near real-time. TimeRA merges the lists within 0.1s when given 30 result lists and within 0.01s when fus-ing two lists. As the number of lists to be fused increases, the time spent on fusing is linear for CombSUM, CombMNZ, TimeRA, and  X  -Merge; for ClustSUM and ClustMNZ, the time increases dramat-ically with larger numbers of lists. When fusing 30 lists, ClustMNZ needs to spend 235.91s, although it only spends 1.03s on fusing two lists. The times clocked for CombSUM and CombMNZ are similar, and likewise for those of ClustSUM and ClustMNZ.
TimeRA uses temporal information in an essential way. How does it compare against retrieval models that explore temporal in-formation? To answer this question, we conduct 4 additional ex-periments for generating 4 time-sensitive result lists, and explore the performance of the baseline fusion methods and TimeRA with each of which respectively fuses these 4 lists. These lists are gen-erated by TBLM [12], LM-T(qe) [19], DIRECT-BM25 (mean) [3] and TSF+QDRM [21].
 Table 5 shows the fusion result. Obviously, except ClustFuse-CombMNZ and  X  -Merge, fusion baselines and TimeRA outper-form the best time-sensitive component run (TSF+QDRM) for all metrics and most of the improvements are statistically significant. This illustrates that exploring time information in data fusion has a different effect than utilizing time information in an individual ranking function, an effect that can lead to performance increases. standard deviations. Note: the figures are not to the same scale. Table 4: Time spent on fusing lists by different aggregation methods. Recorded in seconds with standard deviations (std). One reason behind this is that posts within intervals in which many relevant posts appear can only be confirmed to be relevant by gath-ering data from multiple lists, time-sensitive or not.
The special nature of microblog posts, e.g., their limited length and their creative language usage, raises challenges for searching them. However, this special nature also provides unique algorith-mic opportunities. In this paper, we focus on utilizing time in-formation to boost the performance of searching microblog posts. Specifically, we proposed a novel rank aggregation approach, Time-RA, that utilizes bursts and only rank information to aggregate re-sult lists. TimeRA first detects bursts of posts across the lists utiliz-ing original rank information of the posts, and then rewards posts that are ranked low in few lists but in the vicinity of a burst that con-tains higher ranked posts. It also infers the rank scores of missing posts by modeling lists and posts as a mixture of latent factors.
Our experimental results show that both utilizing burst informa-tion and score inference for rank aggregation can significantly en-Table 5: Performance on 4 time-sensitive result lists. Boldface marks the better result per metric; a statistically significant dif-ference between TimeRA and the best baseline is marked in the upper right hand corner of TimeRA score; a statistically signif-icant difference with TSF+QDRM for each fusion method is marked in the upper left hand corner of the score.
 hance retrieval performance when compared against traditional and state-of-the-art, supervised and unsupervised rank aggregation ap-proaches for microblog post search. Additional analyses show that TimeRA is a robust and efficient rank aggregation method that out-performs state-of-the-art temporal retrieval algorithms.
As to future work, we plan to look into combining social infor-mation, such as user relationships into rank aggregation and fur-ther analyze our model in scenarios where the documents being searched are published in bursts.

