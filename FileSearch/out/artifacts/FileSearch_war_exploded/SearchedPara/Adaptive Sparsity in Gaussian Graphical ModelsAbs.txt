 Eleanor Wong eleswong@sci.utah.edu Suyash P. Awate suyash@sci.utah.edu P. Thomas Fletcher fletcher@sci.utah.edu The structure learning problem for Markov random fields is to infer the graph structure of a model, i.e., the conditional dependencies between the random vari-ables, from observed data. In the case of Gaussian graphical models (GGMs) (Rue &amp; Held, 2005), this is equivalent to estimating which entries of the precision matrix are nonzero. These nonzero entries correspond to the edges in the graphical model. For many ap-plications, especially those involving high-dimensional data, it is desirable to prevent overfitting by utilizing priors that favor parsimonious models. Such models should exhibit as few interactions between variables as possible, which in the GGM case corresponds to a sparse precision matrix. A recent approach to si-multaneously learn the structure and estimate the pa-rameters in GGMs has been to augment the Gaussian likelihood with a penalty on the L 1 norm of the preci-sion matrix (Banerjee et al., 2008; Duchi et al., 2008; Friedman et al., 2008; Meinshausen &amp; B  X uhlman, 2006; Rothman et al., 2008; Yuan &amp; Lin, 2007). This can also be thought of as a maximum a posteriori (MAP) estimation problem with a Laplace prior on the entries of the precision matrix.
 One challenge with these methods is that they re-quire selection of a parameter that controls the amount of sparsity by weighting the penalty term. Fried-man et al. (2008) select this parameter through cross-validation by either minimizing prediction error or maximizing likelihood of the left-out data. Foygel et al. (2010) propose an extended Bayesian informa-tion criterion (EBIC) for choosing the regularization parameter. Fan and Li (2001) discuss the general setting of coefficient selection and estimation in sta-tistical models using penalized likelihood functions. They point out that one issue with all L p penalties is that the resulting estimates of large coefficients will be shrunk, i.e., biased towards zero. They introduce a penalty function called the smoothly-clipped abso-lute deviation (SCAD), which mitigates this bias for large estimates but still requires parameter tuning of the weight on the penalty. Scaled lasso from Sun and Zhang (2012) estimates noise level and regression co-efficients to find an appropriate penalty level, but this again faces issues with L 1 penalty X  X  biasedness. We propose a hierarchical Bayesian model for sparse precision matrix estimation in GGMs that has several attractive features. First, it does not require any pa-rameter tuning X  X he sparsity of the model is adapted automatically to the data. Sparsity is achieved by modeling the precision matrix with a hierarchical scale mixture of Gaussians prior with a parameter-free Jef-freys X  hyperprior. Second, we show that this hierar-chical prior does not suffer from the bias problem in-herent to L 1 penalties. Finally, we naturally enforce the symmetry and positive-definiteness of the precision matrix by utilizing a Cholesky decomposition. Most importantly, we show empirically that our model es-timates have improved performance over the widely-used GLASSO (Friedman et al., 2008) method with optimal sparsity parameter chosen by cross-validation. We test performance of the different estimators on sim-ulated data, by measuring error to the correct solu-tion, and on real data from a cell-signaling experiment, where we measure performance based on the ability of the trained model to explain a left-out test set. Let x denote the n  X  d matrix of observed data, which we think of as realizations of a multivariate Gaussian random variable X = ( X 1 ,...,X d )  X  N (0 ,  X   X  1 ). Our goal is to estimate the precision matrix  X , while con-trolling the complexity of the model. First, we re-view the L 1 -penalized likelihood approach for favoring sparse  X  estimates. This is given by the estimation problem where S is the sample covariance matrix of x , and  X  is a parameter that controls the amount of sparsity. Due to its computational efficiency, the most popular algorithm for solving this estimation problem is the graphical least absolute shrinkage and selection oper-ator (GLASSO) method (Friedman et al., 2008). This is equivalent to a MAP estimate with a Laplacian prior on the entries of  X . That is,  X  ij  X  Laplace(0 , X  ), with  X  = 2  X /n , which gives the prior density In this section we formulate a Bayesian hierarchical model that is designed to induce sparsity on  X  while removing the need for parameter tuning. We begin with a discussion of a natural parameterization of the Gaussian likelihood using the Cholesky decomposition. Next, we formulate a hierarchical prior on the entries of  X , which extends the adaptive sparsity prior devel-oped in (Figueiredo, 2003) for sparse regression prob-lems. Finally, we develop a MAP estimation procedure using the Expectation Maximization (EM) algorithm, which we show has closed-form coordinate ascents for the maximization step. 2.1. Gaussian Likelihood Parameterized by Denote the Cholesky decomposition of  X  as  X  = LL T , where L is lower triangular. Using this decomposition, the multivariate Gaussian model can be naturally for-mulated in the form of a regression problem (Rue &amp; Held, 2005). Define the coefficients  X  ij =  X  L ij /L and the precision of X j as  X  j = 1 / X  2 j = L 2 jj . Then the lower triangular entries of  X  are given by Now the multivariate Gaussian model N (0 ,  X   X  1 ) is equivalent to the set of regression problems, We use these  X  ij and  X  j coefficients to parameterize the Gaussian precision matrix in our proposed model. This formulation naturally enforces the symmetry and positive-definiteness of the precision matrix. The ma-trix  X  = LL T is clearly symmetric for any choice of coefficients. Also, the  X  j coefficients are the eigenval-ues of  X , so  X  being positive-definite is equivalent to the  X  j being strictly positive. This is satisfied for any reasonable prior that does not shrink the  X  j to zero, which we demonstrate holds for our model below. A more practical form of the multivariate Gaussian likelihood utilizes the sufficient statistic, the sample covariance matrix S . Computations involving the sam-ple covariance matrix are more efficient than those us-ing the full data matrix (which is typically larger). The multivariate Gaussian log-likelihood is given by Note that the Gaussian likelihood written this way would technically involve the biased sample covariance matrix, i.e., with a factor of (1 /n ). We rewrite this form of the Gaussian log-likelihood in terms of the  X  and  X  coefficients as ` (  X , X  | x )  X  n The two terms inside the parentheses of (5) correspond to the off-diagonal terms in  X , which arise in pairs, and the diagonal terms, which appear just once. 2.2. Adaptive Sparsity Prior As first described by Andrews and Mallows (1974), the Laplace distribution is equivalent to the marginal distribution of a scale mixture of Gaussians, with ex-ponential scale distribution. More specifically, let  X  be a random variable with the hierarchical distribution Then the marginal distribution of  X  with respect to  X  integrates to giving us the Laplace distribution with  X  = The level of sparsity in LASSO regression depends upon the parameter on the L 1 penalty. In this hierarchical-Bayes context, sparsity is controlled by  X  . Figueiredo (2003) has proposed to remove  X  by re-placing the exponential hyperprior on  X  by a Jeffreys X  noninformative hyperprior, i.e., This is equivalent to a log penalty on  X  , which yields sparse solutions and is nearly unbiased for large coeffi-cients. Although this hyperprior is improper, it has the advantages that it is scale-invariant and parameter-free. This modified hierarchical model is no longer equivalent to the Laplace prior, but has been shown in (Figueiredo, 2003) to be effective in regression and classification problems. The same hierarchical prior has also been used by Park and Casella (2008) in a Bayesian formulation of the LASSO model for regres-sion. Adopting this prior for the Gaussian precision matrix estimation problem, we use the following hier-archical model for an adaptive sparsity estimation of  X : Notice that we only put a prior on the off-diagonal entries of  X . In other words, we do not seek to shrink the diagonal elements, although the model could easily be changed to include this. In what follows, we will treat the  X  ij as latent variables that are integrated out. This leaves us with a posterior p ( X  | x ) that does not have any parameters that need to be tuned, which is a main advantage over the Laplace prior.
 Also, the following simple example demonstrates that the hierarchical model in (6) does not suffer from the bias problem for large coefficients that plagues the Laplace prior. We note that this approximate unbi-asedness of the MAP estimate comes at the cost of a non-convex posterior function. As Fan and Li (2001) show, convex penalty functions cannot simultaneously satisfy the properties of sparsity, continuity, and ap-proximate unbiasedness. Consider a 2  X  2 sample co-variance matrix, where s is a scalar parameter satisfying | s | &lt; 1 to ensure positive-definiteness. Then the maximum-likelihood estimate (MLE) of  X  is just given by the inverse of S . Now consider the L 1 -penalized Gaussian likelihood model in (1), but with the L 1 penalty only on the off-diagonal element,  X  12 =  X  21 . Because the diagonal entries of  X  will remain equal, we can param-eterize  X  with two unknown coefficients a =  X  11 =  X  22 and b =  X  12 =  X  21 . Then the estimate of  X  under the L -penalized likelihood is The solution to this problem for the two unique en-tries a,b of  X  is shown in Figure 1 for varying values of the parameter s in the sample covariance and for a sample size of n = 10. This is compared to the MLE solution, which has solution  X  a = 1 and  X  b = s , and to the MAP estimate of the posterior under the pro-posed model (6). The proposed model estimate was computed using the EM algorithm described in the next subsection. There are two important features to notice in these plots. First, the L 1 solution for the off-diagonal b entry gives the familiar  X  X oft-threshold X  rule, which forces the estimate to be zero below some threshold, but is biased towards zero by an additive constant for larger coefficient values. Second, the di-agonal a entry is nearly perfect (error within 10  X  4 for the proposed MAP estimate, while the a estimate for the L 1 penalty estimate is biased downwards away from s = 0. This is the case even though there is no L 1 penalty directly on the a entry, and it arises from the interaction between a and b in the log-determinant term in (7). 2.3. Expectation Maximization Algorithm We develop an EM algorithm to estimate a sparse  X  as the maximizer of the posterior defined in (6). Al-though the prior on the precision matrix in (6) is writ-ten in terms of the entries  X  ij , we will find it conve-nient to rewrite this in terms of the  X  ij and  X  j coeffi-cients discussed in Section 2.1. We can pass back and forth between the two parameterization of  X  using the relationship (2).
 E step: The EM algorithm iteratively maximizes the Q function, which is a lower bound on the log-posterior, ln p (  X , X  | x ), that we wish to maximize. The Q function is defined as the expectation over the la-tent variables  X  ij of the complete log-posterior, given the current estimate of the parameters at iteration t , which we denote  X   X  ( t ) ,  X   X  ( t ) . Thus, we have We can split the complete log-posterior inside the in-tegral above into the sum of the log-likelihood, which does not depend on  X  , and the log-prior, which does. Then the Q function breaks into two terms, where ` (  X , X  | x ) is the log-likelihood given by (4), and E (  X , X  ) corresponds to the integral of the log-prior term. It is given by E (  X , X  ) = Here we have used (2) in the last line to write the final expression in terms of the  X  and  X  coefficients. The evaluation of the integral above follows the same derivation as in (Figueiredo, 2003).
 M step: The maximization step cannot be done in closed form for the entire set of  X  and  X  coefficients at once. However, we derive a closed-form solution for the maximization of Q along a single coordinate at a time, i.e., a single  X  ij or  X  j , keeping the other coordinates fixed. We begin by taking the derivate of the Q function (8) with respect to the  X  ab . First, the derivative of the log-likelihood term is given by Next, the derivative of the E term in the Q function is given by Both equations (9) and (10) are linear in  X  ab . So, maximization of the Q function with respect to  X  ab is equivalent to solving a linear equation k 1  X  ab + k 0 = 0, that is, setting  X  ab =  X  k 0 /k 1 , with Next, the derivative of Q with respect to  X  m is Setting to zero and multiplying through by  X  m , this becomes a quadratic in  X  m . So, the maximum of Q with respect to  X  m is a solution to the equation c 2  X  2 c  X  m + c 0 = 0, with Notice that c 2 is always negative, and thus the dis-criminant of the quadratic formula, c 2 1  X  4 c 0 c 2 , is al-ways positive, and both solutions are real. Also, the fact that c 0 is always nonzero ensures that  X  m = 0 is never a solution, and therefore the estimate  X   X  is guar-anteed to remain strictly positive-definite. An impor-tant implementation detail is that many of the entries of  X 
 X  ( t ) will be driven to zero, but we need to divide by them in the above calculations. A similar issue arises in quadratic approximations of penalized likelihoods. Following the approach in (Hunter &amp; Li, 2005), we add a small epsilon to the denominator to ensure numer-ical stability. This gives the following fast gradient ascent algorithm, which iterates over each coordinate and updates them one at a time using the above equa-tions (linear in  X  ab and quadratic in  X  m ).
 Algorithm 1 Expectation Maximization for Sparse Gaussian Estimation
Input: Sample covariance matrix S 1. Initialize  X   X ,  X   X  to MLE, given by solutions to the 2. Use (2) to initialize  X   X  (0) 3. Until convergence, i.e., until gradient is zero: 3.1. Simulated Data We generate data from a range of precision matrices of varying size and sparsity levels and compare our results with that of ordinary least squares (OLS), i.e., the Gaussian MLE, and GLASSO. The sizes of the pre-cision matrices are d 2 = 10 2 , 20 2 , and 40 2 . We test on precision matrices composed from lower triangle ma-trices with levels of 5, 10, and 20% non-zero entries in the off-diagonals. The diagonal entries are Gaussian distributed with  X  diag = 1 and  X  diag = 0 . 1, and the non-zero off-diagonal entries are Gaussian distributed with  X  \ diag = 0 and  X  \ diag = 1. This ensures that the precision matrices we work with are positive definite. For each of the nine scenarios, we test our method on twenty different sets of data, each having 1 . 5 d 2 sample points. We run a first pass of our algorithm initializ-ing from the OLS solution with an epsilon value of 1e-3 and a stopping criterion of when the maximum change in the estimated entries is less than 1e-7. We then re-peat the algorithm initializing from the output of the previous step with epsilon values of 1e-5 and 1e-7. This three-step refinement to lower epsilons while maintain-ing numerical stability serves to avoid local minima and confirms that the choice of epsilon does not af-fect the solution. The MAP estimate should correctly set sparse entries exactly to zero, such as is shown in Figure 1 (right plot). However, because we solve the MAP with a gradient descent, some zero entries may not be numerically exactly zero. To differentiate between truly nonzero entries and zero entries that nu-merically off, we maximize the posterior with respect to a numerical zero threshold value (i.e., a value for which all entries numerically smaller get set to zero). We stress that this is simply part of the optimization process, and that the MAP estimates do truly pro-duce sparse solutions. In all experiments initializing from the OLS solution, the EM algorithm converges to a local maximum that is better than the GLASSO and OLS solutions. This improvement in the solutions says that we converge to a reasonable answer, even if the EM algorithm does not guarantee we find the global maximum. All computations are done in R. The GLASSO models used for comparison are selected through tenfold cross-validation for the regularization parameter  X  that maximizes likelihood. Friedman et al. (2008) use the list of ten  X  values suggested by glas-sopath() function in the GLASSO package for their cross-validation. Because we have found that the op-timal  X  sometimes lies below this range, we expand the range of glassopath() values with ten more equally spaced  X  values starting from 1e-4 to the minimum glassopath() value to make sure that the optimal  X  is within range.
 The first metric we use to rate the performance of our proposed method is the Frobenius norm of the differ-ence of the estimations from the true  X , i.e., the root mean squared error, k  X   X   X   X  k F . We also split this error to see how much of it results from what should be zero versus nonzero terms in the true  X . With the thresh-old that maximizes the MAP estimate, we then calcu-late the percentage of zero entries in the true solution our method identifies correctly. The results have been averaged over the twenty different trials from each sce-nario and are summarized in Table 1. From the results, we can see that our method has the best performance in terms of errors and zero predic-tion in all scenarios, whereas GLASSO tends to fail especially in denser and larger matrices. The bias from the Laplace prior in GLASSO shrinks entries that should be nonzero as described in Section 2.2, whereas our method mitigates this bias for large entries. This can be seen in the comparison of the nonzero errors, where in some cases of the experiment, GLASSO has a higher nonzero error than the OLS estimate. For the error and prediction accuracy of zero entries, our method performs the best by far. It is known that cross-validation parameter selection for  X  in GLASSO is overly-conservative, i.e., it does not strongly induce sparsity, which can be seen in the poor performance for GLASSO on zero prediction and zero entry er-ror. Another reason for GLASSO X  X  poor performance is that for even only moderately dense matrices, cross-validation yields  X  values very close to 0, and thus gives results near that of the OLS, as can be observed in the similar zero error rates between OLS and GLASSO for the denser matrices. Because of our hierarchical prior that adaptively induces sparsity, our method is able to consistently predict with mid-90% accuracy the zero entries in  X  across the various sparsity scenarios. In the nonzero accuracy numbers, GLASSO is consis-tently close to 100%, but this is because its overly-conservative sparsity finds far too many entries to be nonzero. We note that a large majority of the en-tries in the matrices are zero, so the zero accuracy contributes more to the overall accuracy. Figure 2 shows ROC curves for GLASSO generated by vary-ing the sparsity parameter  X  . Our method achieves higher specificity/sensitivity than GLASSO, over all parameter values. 3.2. Cell Signaling Data To show how our method works on real-world data, we run our algorithm on the protein signaling dataset from (Sachs et al., 2005). This is the same dataset an-alyzed in (Friedman et al., 2008). The d = 11 protein dataset has n = 7466 samples. In both GLASSO and our proposed method, the likelihood term dominates the prior because of the large sample size. We find that the optimal  X  for running GLASSO on all of the data is 0, which is what Friedman et al. (2008) have re-ported. Therefore, OLS, GLASSO, and our proposed method produce almost identical results.
 The estimated graph presented in (Sachs et al., 2005) is conventionally accepted by biology experts. A more interesting experiment is to train on smaller data sam-ples and test whether GLASSO and our proposed method would yield estimates similar to what biol-ogists expect, keeping in mind that Sachs X  directed graph with only binary values may not be an exact ground truth for direct comparison. Sachs X  graph con-tains 43 edges and 78 pairs of nodes without edges. We train both GLASSO and our method on samples of 200 points. Averaging over 100 runs, we calculate the percentage of zero and nonzero entries in agree-ment between the Sachs X  model and both GLASSO and our proposed model. For our method, we re-peat the same three-step epsilon refinement procedure that we do for synthetic data experiments with epsilon down to 1e-9 because the entries in the precision ma-trix of the cell signaling data are much smaller by a factor of 1/1000. For GLASSO, we do leave-one-out cross-validation (LOOCV) on the subsample over the range of  X  from glassopath augmented with ten evenly spaced values from 1e-4 to the minimum glassopath value to find the optimal  X  . Also, as we do with the simulated data experiment, to account for numerical error we apply a threshold of delta according to the maximum posterior. We also try this for GLASSO, but GLASSO X  X  optimization attains its maximum pos-terior with no thresholding necessary. We note that the results from GLASSO are not symmetric positive definite. The average relative difference between cor-responding off-diagonal entries is 19%, calculated by (  X 
 X  For calculating the prediction accuracies relative to Sachs X  model, we convert the MAP estimates for GLASSO and our method into binary matrices where all nonzero entries are coded into ones. The predic-tion accuracies of the zero and nonzero entries for both methods are displayed in Table 2, along with the combined prediction accuracies for all entries of the precision matrix. The results are consistent with the accuracy rates from synthetic experiments. Our pro-posed method performs much better than GLASSO with the zero prediction but slightly worse with the nonzeros. We observe that for GLASSO the cross-validation can at times choose a model with a  X  very close to zero, resulting in a precision matrix that is not sparse. Overall, our proposed method gives esti-mations with higher similarity to the conventionally-accepted graph from (Sachs et al., 2005).
 Because of the large sample size in the full set of data, the OLS precision matrix based on the entire dataset is a good approximation to the true precision matrix. The fit of the models to the left-out test data is mea-sured by the Gaussian log-likelihood, ` (  X   X  | x test ). An-other experiment we do is to calculate as a measure of the error rate the Frobenius norms of the differences between the OLS precision matrix based on the entire dataset and the estimated models trained from sub-sampling via GLASSO and our method. Again, we use the three-step epsilons up to 1e-9 for our method and do LOOCV for GLASSO over the same range of  X  as the previous cell data experiment. Results are av-eraged over twenty trials. Table 3 shows these results. Compared to OLS and GLASSO, our method has a higher likelihood and a lower norm difference from the OLS precision matrix from all of the dataset. In this paper, we have introduced a hierarchical Bayesian model for the estimation of a sparse precision matrix in a Gaussian graphical model. The primary advantage of our approach, in comparison to the com-monly used L 1 penalty methods, is the ability of our model to adapt to different levels of sparsity without the need for parameter tuning. In fact, this adaptive sparsity proved to give much improved structure learn-ing (zero identification) over GLASSO with optimal sparsity parameter chosen by cross-validation in exper-iments on simulated data. In addition, we showed that the estimated coefficients from the proposed model do not suffer from the same bias problems that L 1 penal-ties display.
 There are several avenues for future work. First, it would be nice to understand what theoretical guar-antees can be derived from the proposed hierarchical model, e.g., asymptotic consistency. To the best of our knowledge, this has not been worked out for such priors even in the regression case. Such guarantees do exist for L 1 -penalized likelihoods. Finally, we expect that the true benefits of our proposed model will be evident in applications where the dimension is much greater than the sample size, for example, in genetics analysis and in identifying functional brain networks. This work was supported by NIH/NIMH Grant Num-ber R01MH084795.
 Andrews, D. F. and Mallows, C. L. Scale mixtures of normal distributions. Journal of the Royal Statisti-cal Society , 36(1):99 X 102, 1974.
 Banerjee, Onureena, Ghaoui, Laurent El, and d X  X spremont, Alexandre. Model selection through sparse maximum likelihood estimation for multivari-ate Gaussian or binary data. Journal of Machine Learning Research , 9:485 X 516, 2008.
 Duchi, John, Gould, Stephen, and Koller, Daphne. Projected subgradient methods for learning sparse
Gaussians. In Proceedings of the Conference on Un-certainty in Artificial Intelligence , 2008.
 Fan, Jianqing and Li, Runze. Variable selection via nonconcave penalized likelihood and its oracle prop-erties. Journal of the American Statistical Associa-tion , 96(456):1348 X 1360, 2001.
 Figueiredo, M  X ario A. T. Adaptive sparseness for super-vised learning. IEEE Transactions on Pattern Anal-ysis and Machine Learning , 25(9):1150 X 1159, 2003. Foygel, Rina and Drton, Mathias. Extended Bayesian information criteria for Gaussian graphical mod-els. In Proceedings of Neural Information Processing Systems , 2010.
 Friedman, Jerome, Hastie, Trevor, and Tibshirani,
Robert. Sparse inverse covariance estimation with the graphical lasso. Biostatistics , 9(3):432 X 441, 2008.
 Hunter, David R. and Li, Runze. Variable selection using MM algorithms. The Annals of Statistics , 33 (4), 2005.
 Meinshausen, Nicolai and B  X uhlman, Peter. High-dimensional graphs and variable selection with the lasso. The Annals of Statistics , 34(3):1436 X 1462, 2006.
 Park, Trevor and Casella, George. The Bayesian lasso. Journal of the American Statistical Society , 103(482):681 X 686, 2008.
 Rothman, Adam J., Bickel, Peter J., Levina, Eliza-veta, and Zhu, Ji. Sparse permutation invariant co-variance estimation. Electronic Journal of Statistics , 2:494 X 515, 2008.
 Rue, H  X avard and Held, Leonhard. Gaussian Markov
Random Fields: Theory and Applications . Chapman and Hall/CRC, 2005.
 Sachs, Karen, Perez, Omar, Pe X  X r, Dana, Lauffen-burger, Douglas A., and Nolan, Gary P. Causal protein-signaling networks derived from multipa-rameter single-cell data. Science , 308:523 X 529, 2005. Sun, Tingni and Zhang, Cun-Hui. Sparse matrix inver-sion with scaled lasso. arXiv:1202.2723 [math.ST]. Yuan, Ming and Lin, Yi. Model selection and estima-tion in the Gaussian graphical model. Biometrika ,
