 Detecting and eliminating duplicates in databases is a task of critical importance in many applications. Although so-lutions for traditional models, such as relational data, have been widely studied, recently there has been some focus on solutions for more complex hierarchical structures as, for in-stance, XML data. Such data presents many different chal-lenges, among which is the issue of how to exploit the schema structure to determine if two objects are duplicates. In this paper, we argue that structure can indeed have a significant impact on the process of duplicate detection. We propose a novel method that automatically restructures database ob-jects in order to take full advantage of the relations between its attributes. This new structure reflects the relative impor-tance of the attributes in the database and avoids the need to perform a manual selection. To test our approach we applied it to an existing duplicate detection system. Exper-iments performed on several datasets show that, using the new learned structure, we consistently outperform both the results obtained with the original database structure and those obtained by letting a knowledgeable user manually choose the attributes to compare.
 H.2.5 [ Database Management ]: Heterogeneous Databases Standardization, Algorithms, Experimentation data quality, duplicate detection, hierarchical data, object structure, support vector machine, simulated annealing
Fuzzy duplicates, or duplicates for short, are multiple rep-resentations of a same real-world object in a given data source, such as a relational database or an XML document. Their presence can cause analysis applications to generate wrong results, based on the assumption that each repre-sentation describes a different object. Detecting and elim-inating duplicates is, therefore, a task of critical practical relevance in many applications, including data cleaning [27], data integration [10], and personal information management [13].
The challenge in duplicate detection lies in the fact that duplicate object representations usually cannot be identified using a universal identifier (e.g., the ISBN of a book) and are not exactly equal, due to errors or inconsistencies. Typical errors are typos, misspellings, the lack of a standard repre-sentation, and missing, outdated, or contradictory data. As a consequence, in order to detect duplicates, more sophis-ticated methods than the comparison of object representa-tions based on equality have to be devised.

Duplicate detection has been studied extensively for rela-tional data stored in a single table [21, 28, 5, 12, 3, 7, 8, 13, 31]. Algorithms performing duplicate detection in a sin-gle table generally compare tuples, each of which represents an object, based on attribute values. However, data usu-ally comes in more complex structures. For instance, data stored in a relational table relates to data in other tables through foreign keys. Recently, duplicate detection algo-rithms for data stored in such complex structures have been proposed [1, 18].

Among these, several works focus on the special case of de-tecting duplicates in hierarchical and semi-structured data, most notably, on XML data [34, 20, 17, 22]. Methods de-vised for duplicate detection in a single relation do not di-rectly apply to XML data, due to the differences between the two data models [34]. In particular, the hierarchical relationships in XML provide useful additional information that can help improve the quality of duplicate detection. Nevertheless, such solutions do not typically consider the relative importance of attributes, individually. They rely on human knowledge to determine which attributes are signifi-cant to use. This carries two different problems: (i) the user may not always have knowledge about data and (ii) some attributes, though considered less significant, may provide a useful contribution to the process.

In this paper, we confirm that data structure is indeed a factor to be considered when searching for duplicates in hierarchical data. More specifically, we show that by care-fully choosing the relationships between object attributes, we can effectively improve the duplicate detection process. We present a novel method to automatically discover such relationships and apply it in a duplicate detection framework called XMLDup [20].

Using Machine Learning techniques, we are able to deter-mine a new structure for the objects being compared that can optimize duplicate detection. Determining a new struc-ture means automatically changing the nesting characteris-tics of its hierarchical data tree, i.e., its depth, breadth and node positions. In fact, although the XMLDup framework was initially designed for duplicate detection in XML data, here we show that it can be equally applied to flat data records. Our approach is therefore independent of the ini-tial object structure and can also be applied to relational data.

To achieve this, we combine an optimization method, based on Simulated Annealing [19], with a Support Vector Ma-chine [32] classifier. Although this is a supervised technique, and therefore requires a known set of duplicate objects, we show that the classification model can be learned on com-pletely different databases. Thus, no previous knowledge of the database being processed is required and user interven-tion is reduced to a minimum.

Through experiments, performed on a set of 8 distinct databases, we were able to achieve R-precision gains of up to 35%, when compared to the original objects structure. Furthermore, our optimization managed to outperform the results obtained when using only the attributes selected by a user with deep knowledge of the datasets.
 The contributions of this work are, therefore, twofold. First, we show that object structure can effectively be used to improve duplicate detection. Second, we present a novel method for object restructuring that not only enhances the duplicate detection results, but also performs the task au-tomatically, avoiding the need for a knowledgeable user to perform attribute selection.
 The remainder of this paper is organized as follows. In Section 2, we present previous related work. In Section 3, we describe the XMLDup framework and discuss how ob-ject structure can influence the outcome of the duplicate detection process. In Section 4, we present our structure optimization approach. In Section 5, we present our exper-iments and the achieved results. Finally, in Section 6, we conclude the paper and suggest future research directions.
Duplicate detection, originally defined by Newcombe et al. [24] and formalized by Fellegi and Sunter [14], has been extensively studied, particularly for relational databases [15, 4, 35, 11]. Only more recently, solutions directed to more structured data models have emerged.

The work developed in [1] presents an algorithm for elim-inating duplicates in a data warehouse. The authors ex-plore the hierarchies between dimensional tables to relate co-occurrences in the database. With this strategy, Anan-thakrishna et al. show that the quality of duplicate detection can be effectively improved for entities stored across multiple tables, connected by a foreign key.

In [6], the authors propose a solution to the problem of in-tegrating tree-structured data extracted from the Web. Two object representations, e.g. two hierarchical representations of person elements, are compared by transforming each into a vector of terms and using a variation of the cosine measure to evaluate their similarity [2]. The hierarchical structure of object representations is mostly ignored, and a linear com-bination of weighted similarities is used to account for the relative importance of the different fields within the vec-tors. The authors show that this simple strategy manages to achieve high precision values in a collection of scientific publications. Nevertheless, and because of its more general nature, their approach does not take advantage of the useful features existing in Web data, such as the element structure or tag semantics.

Solutions that directly address the problem of detecting duplicates in XML data have more recently been proposed. Works like [17, 33, 34, 20, 22, 26], explore the native charac-teristics of the XML data model. Therefore, aspects like the object structure along with element contents and semantics are considered in the process.

In [17, 33] the authors provide strategies to match XML documents. To determine the degree of similarity, the con-tent and structure of the documents is taken into considera-tion, by combining node paths with text contents similarity. In order to improve the similarity results, element names are relabeled according to their semantic similarity.
The work proposed in [34] aims at both efficiency and ef-fectiveness in duplicate detection. Their framework consists of three main steps: candidate definition, duplicate defini-tion, and duplicate detection. Whereas the first two provide the data necessary for duplicate detection (i.e., the set of ob-ject representations to compare and the duplicate classifier to use), the third component includes the actual algorithm, an extension to XML data of the work of Ananthakrishna et al. [1].

The XMLDup system [20] uses a Bayesian Network model for XML duplicate detection. The model is used to capture not only the textual information contained by data objects but also the information provided by their structure. Its approach is further described in Sec. 3.1 and serves as basis to our structure optimization solution.
 Milano et al. propose a distance measure between two XML object representations that is defined based on the concept of overlays [22]. An overlay between two XML trees U and V is a mapping between their nodes, such that a node u  X  U , is mapped to a single node v  X  V if, and only if, they have the same path from the root. This measure is then used to perform a pairwise comparison between all candidates. If the distance measure determines that two XML candidates are closer than a given threshold, the pair is classified as a duplicate.

Finally, SXNM (Sorted XML Neighborhood Method) [26] is a duplicate detection method that adapts the relational sorted neighborhood approach (SNM) [15] to XML data. Like the original SNM, the idea is to avoid performing use-less comparisons between objects by grouping together those that are more likely to be similar
It is worth noting that all the proposals described above use the object structure in its original form. This means that they assume the original database schema already provides sufficient information on how to compare two object repre-sentations. Here, we follow a different approach, trying to discover an object structure that best reflects the properties of data.
The process of detecting if two objects are duplicates re-lies in confronting the information they both contain. This information is typically found as values of the objects at-tributes. Nevertheless, we know that some attributes are more useful than others to differentiate the objects. Thus, we can assume that they should be given more relevance in the duplicate detection process. In this section we present the XMLDup system, which uses the hierarchical structure of the database to determine the impact that each attribute should have when comparing objects. We then argue that the original object structure may not produce the best re-sults and that there may be an optimal structure, which maximizes duplicate detection effectiveness.
XMLDup is a duplicate detection system, originally pro-posed for XML data. Its approach for duplicate detection is centered around one basic assumption: The fact that two XML nodes are duplicates depends only on the fact that their values are duplicates and that their children nodes are du-plicates . Thus, we say that two XML trees are duplicates if their root nodes are duplicates. Note that we assume a schema mapping step has preceded duplicate detection, so that all XML documents comply to the same schema.
To illustrate this idea, consider the goal of detecting that both people represented in Figure 1 are duplicates. This means that the two person objects, represented by nodes tagged prs , are duplicates depending on whether or not their children nodes, tagged pob (place of birth) and cnt (contact), and their values for attributes name and dob (date of birth) are duplicates. Furthermore, the nodes tagged pob are du-plicates depending on whether or not their values are du-plicates, and the nodes tagged cnt are duplicates depending on whether or not their children nodes, tagged eml (email) and add (address) are duplicates. This process goes on re-cursively until the leaf nodes are reached.
 Figure 1: Two XML objects that represent the same person. Nodes are labeled by their XML tag name and an index for future reference.
 In XMLDup, this process is represented by the Bayesian Network illustrated in Figure 2. In this network, the node labeled prs 11 represents the possibility of node prs 1 in tree U being a duplicate of node prs 1 in tree U .Thisnodehastwo parent nodes: V prs 11 , which represents the possibility of the values in the prs nodes being duplicates, and node C prs 11 which represents the possibility of the children of the mv nodes being duplicates. Following the same reasoning, node V prs 11 has two parent nodes, shown as rectangles in Fig. 2, which represent the possibility of the name values in the prs nodes being duplicates and of the dob values in the prs nodes being duplicates. This can be repeated recursively until all node comparisons are represented.
 Figure 2: BN to compute the similarity of the trees in Figure 1.

All nodes are assigned binary random variables, taking the value 1 to represent the fact that the corresponding data in trees U and U are duplicates, and taking the value 0 to represent the opposite. Thus, to decide if two objects are duplicates the algorithm has to compute the probability of the root nodes being duplicates, P ( prs 11 =1),whichcanbe interpreted as a similarity value between the two XML ele-ments. To obtain this probability, the algorithm propagates the prior probabilities associated to the network leaf nodes, which will set the intermediate nodes probabilities, until the root probability is found. This yields the formula:
P ( prs 11 )= w name P ( prs 11 [ name ])+ w dob P ( prs 11 1  X 
To compute the probability P ( prs 11 ), the prior probabil-ities in Eq. (1) need to be given a value. This can be ac-complished using a any chosen similarity measure between the values in the corresponding XML nodes, normalized to fit between 0 and 1 (0 meaning that the values are totally dissimilar and 1 meaning that they are exactly equal). A normalized version of Levenshtein X  X  string edit distance is a common choice.

For further details of this process, we refer the reader to [20]. Detailed information on Bayesian Networks and their applications can be found in [25].
If we examine Eq. (1), we can see that, implicitly, XMLDup assigns more importance to the values that are closer to the root of the XML trees. In our example, the similarity be-tween person names ( P ( prs 11 [ name ])) and between dates of birth ( P ( prs 11 [ dob ])) carries as much weight as all the re-maining attributes together. XMLDup relies, therefore, on the assumption that, when creating database schemas, we tend to group together and emphasize attributes that we believe to be more distinctive.
However, this may not always be the case. Either by de-sign or, for instance, if we are dealing with a flat structure, such as a relational table, structure may not reflect the ac-tual importance of the attributes. In fact, many other char-acteristics of the attribute values, unrelated to structure, can have a critical impact on the outcome of the duplicate detection process. For instance, although the name of a person is expected to be a reliable source of information, it may happen that, in our particular instance of the database, the values for name contain many typographical errors, thus rendering it useless.

To take advantage of the capabilities of XMLDup using structure to give the appropriate weight to the attributes being compared, we could redesign the objects structure, placing the most important elements closer to the root, and those that carry less information in lower levels. For ex-ample, the objects in Fig. 1 could be restructured so that elements dob and pob are grouped together under a new ele-ment called brt (birth), as shown in Fig. 3. In this case, the equation for determining the probability of tree U and tree U being duplicates would be:
P ( prs 11 )= P ( prs 11 [ name ])  X  1  X  As an implication, the similarity of nodes name has now gained in importance, actually acting as a scale on the re-maining similarity values.
 Figure 3: XML object that represents a person, re-structured so that the most important elements are placed closer to the root.

Of course, there is no guarantee that this structure would yield better results than the one shown in Fig. 1. It might happen that combining the name with date of birth is better to distinguish people than just using the name alone. In fact, to find an optimal new structure, the user interested in the duplicate detection process would need a thorough knowledge of the database contents and of the relationships between the attributes. Unfortunately, such knowledge is often unavailable.

In the following section, we show that a quasi-optimal structure can be found automatically by analyzing some sta-tistical properties of the objects attributes. This is achieved without user intervention and does not require any previous knowledge of the data.
As argued in the previous section, object structure can influence the outcome of the duplicate detection process. In this section we propose a strategy to solve this problem, by automatically restructuring the database objects in order to maximize the impact of highly informative attributes, while minimizing the impact of attributes that carry less informa-tion.
Our object restructuring proposal consists in the follow-ing. Let D be an instance of a database, i.e., a set of objects where we intend to discover duplicates. Let A be the set of attributes in the schema of D ,whereby attribute we mean any element that can carry a value. Each attribute a i  X  X  is represented as vector of features f i =( f i 1 ,f i 2 ,...,f where each feature corresponds to a statistical property of the attribute.

The idea is to find a mapping function M : A X  L , where L = { 1 , 2 ,  X  X  X } is the set of possible levels in the new hierarchic object schema (level 1 is immediately below the root, level 2 is below level 1, and so on). This function will allow us to build a new object structure, by placing each attribute a i  X  X  at level M ( a i ) in the new schema. To discover the mapping function M , we use a Support Vector Machine classifier [32]. The classifier is trained by taking as input a set of example attribute feature vectors and their corresponding level on an ideal object structure .Thus, let T = { T 1 ,T 2 ,...,T N } be a set of database instances. Each T i does not need to have any intersection with D ,nor does it need to be on the same domain. This means that, for instance, D might contain objects representing movies, while T 1 might contain objects representing people or bank accounts, or any other thing.

Assume, for now, that, for each T i , we know its ideal struc-ture, i.e., the schema that maximizes the effectiveness of the duplicate detection process. We can use T to provide the example (attribute,level) pairs to train our classifier. For ex-ample, if we knew that the best duplicate detection results could be achieved using the structure in Fig. 1, our classi-fier would receive as examples the pairs ( f prs , 1), ( f ( f can use the classifier to map each attribute to its correspond-ing level. This is achieved by Algorithm 1.
 Algorithm 1 Algorithm for building a new object structure. Require: A mapping function M : A X  L Ensure: A new schema for the objects 1: n  X  new root node R 2: l  X  1 { current level } 3: l  X  max a  X  X  M ( a ) { maximum level returned by M} 4: for i  X  1 ,...,l do 5: A  X  X  a  X  X |M ( a )= i } 6: Place all nodes in A (if any) under node n 7: if i = l then 8: n  X  a new node, placed under node n 9: l  X  l +1 10: n  X  n 11: end if 12: end for 13: return the new schema, represented by the tree rooted
It is important to note that, by using a function that sim-ply maps attributes to hierarchic levels, we are not explor-ing every possible alternative object structure. Nevertheless, our experiments show that this simplification is still capable of achieving high quality results, while reducing the com-plexity of the problem to a manageable level.

Fig. 4 summarizes our restructuring procedure. Two is-sues, thus, remain. First, how do we discover such an ideal structure? Second, what statistical features are appropriate to represent the object attributes? We propose an answer to both questions in the following sections.
 Figure 4: Training and structure classification pro-cess.
To discover the ideal structure in a database T i  X  X  ,we require that, in each T i , some of the duplicate objects are known. A brute force solution to this problem would be to simply take all possible structure configurations, apply them in a duplicate detection process using XMLDup, and choose that which yielded the best results, i.e., that which would maximize some criteria for result quality.

This approach is, of course, unfeasible. Thus, we take an approximate approach, using the well-known search al-gorithm of Simulated Annealing (SA) [19] to find an optimal or sub-optimal solution. We chose SA because of its simplic-ity, allowing a straightforward adaptation to our problem domain.

SA is an algorithm intended to determine the maximum (or minimum) value of a function with several independent variables, under a fraction of the time needed to find an ab-solute maximum (or minimum) in a large search space. This function is called the objective function and its results de-pend on the possible variable configurations, or states .SA consists in shifting among candidate states in order to ap-proximate the objective function to a global optimum, using an acceptance function to decide if the new state should be accepted.

Our duplicate detection problem is modeled in SA as fol-lows. Each state corresponds to a specific object struc-ture. In Fig. 5, we illustrate with four possible states, on a database were the objects contain four attributes ( A 1 A ,and A 4 ). For an objective function, we choose the mea-sure of R-precision [2]. R-precision is the percentage of cor-rectly determined duplicates among the first R items in the list, where R is the total number of known duplicate objects. In this case, our goal is to maximize R-precision. Figure 5: Structure topologies generated from the attributes A 1 , A 2 , A 3 and A 4 .

Thus, given a state s , we execute our duplicate detec-tion system on the database, building the Bayesian Net-work according to the corresponding structure. As a result, we obtain a list of object pairs, ordered according to the probability of being duplicates. Using this list, we compute our objective function. It should be noted that, although, in the worst case, building the network for each state can be quadratic in the number of attributes, in practice this process is linear [20]. Furthermore, this is only performed during the structure learning phase, and thus can be done offline.

To determine if the algorithm should change state, we use the acceptance function defined in Eq. (3): where p s is the objective function value for the current state, p n is the objective function value for the new state, and t is the fraction of iterations the algorithm still needs to execute. Eq. (3) is a commonly used function in SA implementations and returns the probability of accepting the new state.
To select the candidate new states, the algorithm chooses those that represent close neighbors of the current state, i.e., states that differ from the current by one edit opera-tion in the structure. An edit operation can be swapping two nodes, moving an attribute to a different level, or the insertion/deletion of a node. Since the search space is still too large to provide viable results, we impose an additional restriction: any non-leaf node can only have one child node that is not itself a leaf node. This means that we will not have several attributes grouped under different nodes at the same level of the tree.

We believe that, despite this restriction, it is still possible to find a near-optimal structure, which is confirmed by our experiments. In fact, convergence is achieved relatively fast, with an average of about 53 iterations for the datasets with the highest number of attributes.
As stated above, each attribute a i in a database is repre-sented as a vector of features f i =( f i 1 ,f i 2 ,...,f features represent particular statistical characteristics of the attribute, computed by examining the different values it con-tains in the database D . We now explain each feature used and the rationale for choosing it. Since some features are similar in the type of information they provide about at-tributes, we have grouped them accordingly.

In the remaining of this section, we use the following no-tation: O total number of objects in the database
N number of objects that contain attribute a t total number of values of attributes a d set of distinct values in attribute a ds set of distinct string lengths of the values in at-Uniqueness features try to capture the diversity with which attribute values occur in the dataset. Intuitively, attributes with more distinct values should be better at distinguishing objects. Format features provide information about the type of con-tents present in the attribute values. The intuition is that values composed of alphabetical characters may be better to distinguish objects than those based on numbers, or vice-versa. Since in XMLDup, as in most other duplicate detection frameworks, values are compared using a string edit dis-tance, the string length of the attribute values can influence the results. For instance, attributes with a smaller length are more affected by typographical errors than attributes with a greater length. Missing data is the characteristic that affects most nega-tively the duplicate detection results [20]. With this feature we intend to measure the absence of data in the attributes. Several elements with the same label can occur within an object (e.g., element add in Fig. 1). This can increase the probability of finding contradictory information when com-paring objects. This feature tries to infer if the number of occurrences can influence the quality of the outcome.
Using this set of features and the process described above, we have performed several experiments, to validate our ap-proach. We describe the experimental setup and the results achieved in the following section.
In this section we present an evaluation of our structure optimization strategy. We start by evaluating the impact that the choice of features has in the optimization results and then test our approach in four real world datasets, using the best set of features obtained.
Our tests were performed using eight different datasets, representing six different data domains. The first four datasets, Country , CD , IMDB and Person , consist of a set of XML ob-jects taken from a real database and artificially polluted by inserting duplicate data and different types of errors, such as typographical errors, missing data, and duplicate erroneous data. Contrarily to the other datasets, where data was orig-inally in XML format, the Person dataset was stored in a plain text file, presenting one record per line. Thus, we con-verted it to the XML format respecting its original flat form, that is, every field from the person record was converted to a node placed immediately below the root node.

The remaining four datasets, Cora , IMDB+FilmDienst , another dataset containing CD records (which we are going to refer to as CD 2 ), and Restaurant , are composed exclu-sively of real world data, containing naturally occurring du-plicates. The characteristics of the Restaurant dataset are similar to those of the Person dataset.
 The Cora , Country , IMDB , IMDB+FilmDienst and both CD datasets are available at the Hasso Plattner Institute website 1 .The Restaurant and Person datasets were ob-tained from the University of Texas Machine Learning Re-search Group website 2 and from the FEBRL record linkage tool package [9], respectively.
To assess the effectiveness of our method, we applied the commonly used precision and recall and r-precision mea-sures [2]. Precision measures the percentage of correctly identified duplicates, over the total set of objects determined as duplicates by the system. Recall measures the percent-age of duplicates correctly identified by the system, over the total set of duplicate objects. R-precision measures the pre-cision at cut-off R, where R is the number of duplicates in the dataset. We used the SVMLight implementation for our classifier [16], with a linear function kernel. http://www.hpi.uni-potsdam.de/naumann/projekte/ repeatability/datasets.html http://www.cs.utexas.edu/users/ml/riddle/data. html
We performed two sets of experiments. The first set aimed at discovering the best features to build the attribute clas-sification model. The second set aimed at evaluating the performance of our method on real-world datasets. We now present the achieved results.

In all experiments, the classifier was trained using all the datasets, except the one being tested. Again, we note that the datasets used for training do not need to have any ob-jects nor attributes in common with the tested dataset and, therefore, no knowledge of the testing dataset is needed.
With this set of experiments we intended to discover which of the features described in Sec. 4.3 are more reliable to per-form the structure classification. We started by verifying the impact of each group of features in the duplicate detection results. To this effect, we tested each group in isolation and also the removal of each group from the full set of features. Tab. 1 shows the R-precision values obtained.
 Table 1: Average R-precision for all datasets, using only one feature group (Single group) or all features excluding one group (Group excluded).

As we can see, G1 presents the more distinctive features, showing a significant higher R-precision than the remaining groups. This is reinforced by the score obtained when G1 is excluded, which is clearly the lowest from all tests. Aside from G1, none of the other groups show a similar behavior. However, we can observe that G3 exhibits the second highest individual score and that, when it is excluded, the score is at an intermediate value. This suggests that G3 may contain features that, although not as important as those in G1, provide useful complementary information.

To further explore the interaction between groups of fea-tures, we experimented several combinations. Due to space constraints, we show only the results obtained by combining the best individual groups with the remaining. Results are shown in Tab. 2. As a baseline we used the results obtained with the original object structure (All attributes) and with the original object structure, manually choosing only the attributes more appropriate to compare the objects (Best attributes).

Results show that most combinations are able to perform effectively. In particular, sets (G1, G3, G5) and (G1, G3, G4) present better results than the original structure for all datasets. In fact, (G1, G3, G4) managed to present better or equal results than the those obtained with manually se-lected attributes. This reveals the importance accounting for missing data and confirms our intuition about the im-portance of the length of strings to the duplicate detection. In fact, the size of the strings being compared seems to be more decisive to the process than the actual contents, since the combination (G1, G2, G3) shows worse results.
The high scores from (G1, G3, G5) also stress the impor-DE,DI,APO,D,SD,HM,EMP,MCR 68 88 82 94 83 90 72 84 equal or higher than Best attributes . tance of considering the replication of attributes of the same type in a single object. Interestingly, when combining (G1, G3, G4, G5) results show a decrease in three databases. This indicates that some features in G4 and G5 might be caus-ing cause mutual interference and, although absence and occurrence features are both useful, they should not be used together.

Results are also inferior when using one or two groups of features, showing that the additional complementary in-formation is needed by the classifier in order to obtain a ground-base to evenly provide effective structures for differ-ent domains.

To evaluate the impact of single features, we sorted the full set of features according to the measure of information gain for each feature [23]. We then tested all combinations of the features with the highest information gain. In Tab. 2, we show the two combination that yielded the best results. We note that, only when combining 5 or more features, the results became comparable to those achieved by the base-lines. Also, most features are from group 1 and group 5, which confirms their importance.
We now present precision-recall results for four real world datasets from distinct domains. By showing these plots we intend to provide a more detailed analysis on how restructur-ing the objects influences the duplicate detection outcome.
Fig. 6 shows results of performing duplicate detection on three different structures: the original structure, the original structure with only the most significant attributes manually selected, and the structure learned by our structure opti-mization strategy, using the sets of features G1, G3 and G4.
Similarly to what was observed in previous experiments, the duplicate detection results obtained using the optimized structure are, in general, similar or better than those using both versions of the original structure. In the Restaurant dataset, our learned structure presents a generalized gain over both versions of the original structure, showing a sig-nificant increase in precision for recall values above 80%.
For the Cora dataset, the optimized structure presents a similar curve to that showed by the best attributes. How-ever, after 90% of recall, the learned structure shows a less abrupt decrease in the quality of the duplicates detected. This is symptomatic of the importance of building an accu-rate object structure. For duplicate pairs where the objects contain attributes with very distinct or very similar values, the final score is not much affected. However, when the at-tribute values get more similar, it becomes important to have all attributes contributing to the score, although weighted by their relative importance, which explains the higher pre-cision at later recall values.

In the IMDB+FD and CD 2 datasets the results gathered from the two best structures are very similar. For the CD 2 dataset the learned structure presented an initial decrease in the early ranking places, caused by the occurrence of two false positives, though it recovered quickly to higher preci-sion scores. The IMDB+FD dataset presents very similar results. However, given that the best version of the original structure already produces a near optimal result, it becomes difficult to improve the outcome. Nevertheless, our opti-mization manages to present a slightly smoother decrease at later recall values.
As the experimental evaluation revealed, the placement of attributes inside the object structure has a significant impact in duplicate detection. Results showed that our method was able to learn a structure that, in the best case, caused R-precision improvements of 13% over the original structure, with the best attributes manually selected, and 35% over the original structure considering all attributes. The superior results achieved by the learned structure in real world data, confirm our intuition that attributes with less distinctive power should, although in a less emphasized way, contribute to duplicate detection.

Moreover, the dataset object structures were learned from a model which was trained with datasets from different con-texts. This shows that our strategy does not require the classifier to be trained with specific domain data. This fact also stresses that data should be looked at according to its statistical characteristics and not necessary according to its semantic domain. Our method also has the advantage that, once the classifying model is created from the training data, it can be used to restructure any object from any dataset where one needs to find duplicates.

Finally, since this approach is independent of the data structure because it only considers the individual attributes to build the hierarchy, it can virtually be applied to any dataset where information can be segmented in the form of attributes as, for example, in the case of the traditional (c), and the Restaurant dataset (d). relational databases. Besides the improvements it delivers, the process is performed in a fully automatic way, therefore relieving the user from the need to have prior knowledge of data.
In this paper we presented a novel method to automati-cally restructure data objects in order to improve duplicate detection effectiveness. Our approach places the objects at-tributes in a hierarchical structure, according to a set of statistical characteristics extracted from the data, and does not require user intervention.

This method relies on the premise that data attributes should be placed in the structure according to their overall importance in distinguishing between two objects and that, to this end, objects should be looked at by their intrinsic characteristics rather than by the domain they belong. Our approach maps attributes from an original object structure into a new structure level according to their relevance to the duplicate detection process. To achieve this we used a Simulated Annealing algorithm combined with a Support Vector Machine classifier.

Experiments performed on eight different datasets showed that the learned structure consistently produced better re-sults than the original object structure and than a selection of attributes performed by a knowledgeable user. Addition-ally, the method obtained effective results, even when using trained data from completely different domains.

The success demonstrated in our experimental results leaves motivation for future work. Despite the good results ob-tained by looking at attributes individually and mapping their relative importance into a single level, the correlation between attributes at the same or different levels may pro-vide additional information that can be useful to further optimize the process. Therefore, the classification of more complex features, such as the co-occurrences of attributes with certain characteristics is an issue we should address. A more complex classification strategy may benefit from with-drawing the topology restriction that avoids the appearance of two grouping nodes with the same parent. This requires the search space reduction strategy to be adapted to a larger domain, a problem we also intend to consider in future work. This work was partially supported by FCT, through project SMARTIS (ref. PTDC/EIA-EIA/115346/2009) and PhD grant SFRH/BD/41450/2007 (Lu  X   X s Leit  X  ao). [1] R. Ananthakrishna, S. Chaudhuri, and V. Ganti. [2] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern [3] I. Bhattacharya and L. Getoor. Relational clustering [4] I. Bhattacharya and L. Getoor. Collective entity [5] M. Bilenko and R. J. Mooney. Adaptive duplicate [6] J. C. P. Carvalho and A. S. da Silva. Finding similar [7] S. Chaudhuri, V. Ganti, and R. Motwani. Robust [8] Z. Chen, D. V. Kalashnikov, and S. Mehrotra.
 [9] P. Christen and A. Pudjijono. Accurate synthetic [10] W. W. Cohen and J. Richman. Learning to match and [11] D. Dey, V. Mookerjee, and D. Liu. Efficient techniques [12] A. Doan, Y. Lu, Y. Lee, and J. Han. Profile-based [13] X. Dong, A. Halevy, and J. Madhavan. Reference [14] I. P. Fellegi and A. B. Sunter. A theory for record [15] M. A. Hern  X andez and S. J. Stolfo. The merge/purge [16] T. Joachims. Making large-scale support vector [17] A. M. Kade and C. A. Heuser. Matching XML [18] D. V. Kalashnikov and S. Mehrotra.
 [19] S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. [20] L. Leit  X  ao, P. Calado, and M. Weis. Structure-based [21] E.-P. Lim, J. Srivastava, S. Prabhakar, and [22] D. Milano, M. Scannapieco, and T. Catarci. Structure [23] T. Mitchell. Machine Learning . Mc-Graw-Hill, 1997. [24] H. Newcombe, J. Kennedy, S. Axford, and A. James. [25] J. Pearl. Probabilistic Reasoning in Intelligent [26] S. Puhlmann, M. Weis, and F. Naumann. XML [27] E. Rahm and H. H. Do. Data cleaning: Problems and [28] S. Sarawagi and A. Bhamidipaty. Interactive [29] C. E. Shannon. A mathematical theory of [30] E. H. Simpson. Measurement of diversity. Nature , [31] P. Singla and P. Domingos. Object identification with [32] V. N. Vapnik. The nature of statistical learning theory . [33] W. Viyanon and S. K. Madria. A system for detecting [34] M. Weis and F. Naumann. Dogmatix tracks down [35] S. E. Whang, D. Menestrina, G. Koutrika, M. tin
