 To support privacy-preserving video sharing, we have pro-posed a novel framework that is able to protect the video content privacy at the individual video clip level and pre-vent statistical inferences from video collections. To protect the video content privacy at the individual video clip level, we have developed an effective algorithm to automatically detect privacy-sensitive video objects and video events. To prevent the statistical inferences from video collections, we have developed a distributed framework for privacy-preserving classifier training, which is able to significantly reduce the costs of data transmission and reliably limit the privacy breaches by determining the optimal size of blurred test samples for classifier validation. Our experiments on a spe-cific domain of patient training and counseling videos show convincing results.
 Categories and Subject Descriptors I.4.8 [ Image Processing and Computer Vision ]: Scene Analysis-object recognition , H.2.8 [ Database Management ]: Database Applications -video databases .
 General Terms Algorithms, Measurement, Experimentation Keywords: Video content privacy, statistical inferences, privacy-preserving video sharing, unlabeled samples.
Digital video plays an important role in supporting online patient training and counseling by illustrating real medical treatment procedures by means of videos [1]. In order to im-prove the quality of online patient training and counseling, it is very important to share patient training and counsel-ing videos among multiple competitive groups and organiza-tions (i.e., video owners). Increasing the amount of available videos results in a better offering for patients. However, pri-vacy regulations, such as HIPAA, consumer backlash, and other privacy concerns often prevent multiple competitive video owners from sharing their patient training and coun-seling videos [2-4], because no good comprehensive frame-work is today available addressing all the following inter-related challenging problems: (a) Owner-Adaptive Video Privacy Modeling: Many approaches to privacy protection have been recently devel-oped [5-7]; these approaches however have a limited applica-bility because they do not cater for individual privacy pref-erences. We believe that a suitable approach for privacy-preserving video sharing must take into account a funda-mental aspect of privacy well expressed by Alan Westin ac-cording to whom  X  X rivacy is the claim of individuals, groups, or institutes to determine for themselves when, how and to what extend information is communicated to others X . We thus need techniques supporting owner-adaptive video pri-vacy modeling . (b) Video Content Privacy Protection: At the indi-vidual video clip level, the content privacy for the patient training and counseling videos encompasses two major as-pects: (1) privacy for the human objects shown in video as the professional patient trainers or doctors; and (2) pri-vacy for the human objects shown in video as the patients to illustrate the relevant clinic examples. In addition, video content privacy is also highly context-dependent and thus there is an urgent need to detect the privacy-sensitive video events. To protect the video content privacy, some tech-niques have been proposed that simply block the human faces [9-11]. However, in order to achieve more effective online patient training and counseling, it is also very impor-tant to let the patients see the real clinic examples at a high quality. Therefore, protecting the human object X  X  privacy by simply blocking human faces may seriously reduce the video quality. (c) Statistical Inference Control: Protecting the con-tent privacy for individual video clips may not be enough to ensure privacy-preserving video sharing; we may also need to prevent statistical inferences from video collections [8, 17]. Statistical inferences represent an important challenge for video privacy protection because of non-sensitive data can be exploited to infer privacy-sensitive information. Such a challenge is beyond the reach of most existing privacy pro-tection methods.

Based on these observations, we propose a novel frame-work able to assure the privacy of the video contents and control the statistical inferences in the specific domain of patient training and counseling videos . This paper is orga-nized as follows. Section 2 presents the proposed framework for owner-adaptive video privacy modeling. To protect the Figure 1: The proposed framework for owner-adaptive video content privacy at the individual video clip level, Sec-tion 3 and Section 4 introduce a new algorithm for auto-matic detection of the privacy-sensitive video objects and video events. Section 5 introduces our approach to privacy-preserving video sharing for distributed classifier training and for preventing statistical inferences from video collec-tions. Section 6 reports the results of an extensive evalua-tion we have performed on the proposed techniques. Finally, we conclude this paper in Section 7.
The definition of video privacy largely depends on three inter-related factors [10]: video content sensitivity , video re-ceiver , and receiver X  X  usage of video contents . Obviously, video privacy also depends on the video owner X  X  percep-tions/judgement of privacy of the videos being shared be-cause privacy means different things to different people. In order to achieve owner-adaptive video privacy modeling, six inter-related factors need to be taken into account as shown in Fig. 1: video content sensitivity , video receiver , receiver X  X  usage of video contents , video owner X  X  perceptions/judgement of video privacy , trust between the video owner and the video receiver, and risks/benefits for video sharing. In addition, a good balance is crucial between the risks of privacy breaches and the benefits of video sharing. Based on this motivation, we propose a novel framework by taking all these inter-related factors into account in a comprehensive approach to achieve owner-adaptive video privacy modeling .
In order to implement the proposed framework for owner-adaptive video privacy modeling, we have defined a basic vocabulary of privacy-sensitive video objects in the specific domain of patient training and counseling videos; each video owner is thus able to select a subset of these privacy-sensitive video objects according to his/her individual privacy con-cerns.
To support our framework, the basic vocabulary of privacy-sensitive video objects is pre-defined by the video owners. To detect these privacy-sensitive video objects, video shots are first detected automatically [1]. To detect the privacy-sensitive video objects associated with each video shot, we have designed a set of automatic video object detection func-tions, where each video object detection function is able to detect only one certain type of these privacy-sensitive video objects in the basic vocabulary.

After the video shots are detected from a given video clip, our automatic video object detection functions are executed. To detect a given type of privacy-sensitive video object, our Figure 2: The flowchart of the proposed algorithm for detection function takes the following steps as shown in Fig. 2: (a) Automatic image segmentation is first performed on each video frame to obtain the homogeneous image regions [1, 16, 18]; (b) The given type of video object may have very different visual properties because of presence/absence of distinctive parts, variability in overall shape, changing ap-pearance due to lighting conditions, viewpoints etc. Thus, automatic image segmentation itself is unable to directly de-tect the privacy-sensitive video objects and machine learning should be involved for region classification and object gener-ation. Based on this understanding, the homogeneous image regions are classified into two classes by using SVM binary classifier, that is, into object regions versus non-object re-gions; (c) The connected object regions are then merged and aggregated according to a knowledge-based object model for generating the given type of privacy-sensitive video object; (d) Object tracking is finally performed to obtain the tempo-ral relationships of object regions among video frames within the same video shot.

After the privacy-sensitive video objects are extracted, the original video streams are decomposed into a set of privacy-sensitive video objects such as human beings with race and gender, background, and areas of interest.
Another difficulty in video privacy protection is that the video privacy is also highly context-dependent. To detect the privacy-sensitive video events, the video shots for a given video clip are classified into a set of pre-defined semantic video concepts that are sensitive to the context-dependent privacy breaches. The contextual relationships, between a given semantic video concept C j and the relevant video ob-jects, are interpreted by using a finite mixture model: where  X  c j = {  X  j , X  c j , X  c j } is the parameter set for model structure, weights, and model parameters; in particular,  X  is the model structure,  X  c j = {  X  1 ,  X  X  X  ,  X   X  j } is the set of weights for  X  j mixture components;  X  c j = {  X  1 ,  X  X  X  ,  X  the set of model parameters for  X  j mixture components, P ( X | C j ,  X  i ) is the mixture component (i.e., one specific video context class) that is used to approximate the class distribution for one specific type of the relevant video ob-jects, X is a set of m -dimensional object-based features.
To learn the semantic video concept accurately, we have developed an adaptive EM algorithm to achieve more ef-fective model selection and parameter estimation by using a maximum likelihood approach. Based on a limited number of labeled samples  X  c j , the optimal model parameters  X  for the specific semantic video concept C j are determined by: where L ( X  c j ) =  X  is the objective function,  X  the likelihood function, and log p ( X  c j ) =  X  m +  X  j +3 length (MDL) term to penalize the complex models [14-15], N is the total number of samples that are used for classifier training, m is the feature dimensions.
 To achieve more effective classifier training, our adaptive EM algorithm can re-organize the distribution of mixture components and select the optimal number of mixture com-ponents by performing automatic merging , splitting and elimination of mixture components.

Our adaptive EM algorithm uses symmetric Jensen-Shannon (JS) divergence JS ( C j ,  X  l ,  X  k ) to measure the divergence between two mixture components P ( X | C j ,  X  l ) and P ( X | C  X  ) for the same concept model C j .

JS ( C j , X  l , X  k ) = H (  X  1 P ( X | C j , X  l ) +  X  2 P ( X | C where H ( P (  X  )) =  X  non entropy,  X  1 and  X  2 are the weights. In our experiments, we set  X  1 =  X  2 = 1 2 .

If the intra-concept JS divergence JS ( C j ,  X  l ,  X  k small, these two mixture components are strongly overlapped and may overpopulate the relevant sample areas; thus they are merged into a single mixture component P ( X | C j ,  X  In addition, the local JS divergence JS ( C j ,  X  lk ) is used to measure the divergence between the merged mixture compo-nent P ( X | C j ,  X  lk ) and the local sample density P ( X ,  X  Our adaptive EM algorithm tests  X  j (  X  j  X  1) 2 pairs of mixture components that could be merged and the pair with the minimum value of the local JS divergence is selected as the best candidate for merging .

Two types of mixture components may be split : (a) The elongated mixture components which underpopulate the rel-evant samples (i.e., characterized by the local JS divergence); (b) The tailed mixture components which overlap with the mixture components from other concept models (i.e., char-acterized by the inter-concept JS divergence ). To select the mixture component for splitting, two criteria are combined: (1) The local JS divergence JS ( C j ,  X  i ) to characterize the divergence between the i th mixture component P ( X | C j and the local sample density P ( X |  X  i ); (2) The inter-concept JS divergence JS ( C j , C h ,  X  i ,  X  m ) to characterize the over-lapping between the mixture components P ( X | C j ,  X  i ) and P ( X | C h ,  X  m ) from two relevant semantic video concepts C and C h .

By splitting the elongated and tailed mixture components, some mixture components locating at the sample distribu-tion boundary may be unrepresentative and be supported by few samples. If a specific mixture component is only supported by few samples, it may be removed from the un-derlying concept model. To determine the unrepresentative mixture component for elimination , our adaptive EM al-gorithm uses the local JS divergence JS ( C j ,  X  i ) to charac-terize the representation of the mixture component P ( X | C  X  ) for the relevant samples. The mixture component with the maximum value of the local JS divergence is selected as the candidate for elimination.

To jointly optimize these three operations of merging, splitting and elimination, their probabilities are defined as: where  X  is a normalized factor and it is determined by: The acceptance probability to prevent poor operation of merging, splitting or elimination is defined by: where L ( C j ,  X  1 ) and L ( C j ,  X  2 ) are the objective functions for the models  X  1 and  X  2 (i.e., before and after performing the merging, splitting or elimination operation) as described in Eq. (2),  X  is a constant that is determined experimentally. In our current experiments,  X  is set as  X  = 9 . 8.
To learn the underlying concept model accurately, a large number of labeled samples is needed. When only a limited number of labeled samples is available for classifier train-ing, it is difficult to select the optimal model structure and estimate the accurate model parameters. However, obtain-ing a large number of labeled samples is very expensive, and incorporating the outlying unlabeled samples for clas-sifier training may lead to worse performance rather than improvement [12-13]. Thus, it is very important to develop new techniques able to eliminate the misleading effects of the outlying unlabeled samples.

After the weak classifier for the given semantic video con-cept C j is learned from a limited number of available labeled samples, the Bayesian framework is used to achieve  X  X oft X  classification of unlabeled video clips. The confidence score for an unlabeled sample with the given semantic video con-cept C j is defined as: ability for the unlabeled sample { X l ,S l } with the given se-mantic video concept C j ,  X   X  ( X l ,C j ,t ) =  X  log P ( X is the log-likelihood value of the unlabeled sample { X l with the given semantic video concept C h . For one specific unlabeled sample { X l ,S l } , its confidence score  X  ( X can be used as the criterion to indicate the possibility to be taken as an outlier for the given semantic video concept C .

In order to eliminate the misleading effects of the outly-ing unlabeled samples for semi-supervised classifier training, the unlabeled samples are first categorized into two classes according to their confidence scores: (a) certain unlabeled samples with high confidence scores may originate from the known video context classes that have already been learned from the available labeled samples; (b) uncertain unlabeled samples with low confidence scores may originate from new concept, outliers or unknown video context classes that can-not be directly learned from a limited number of available labeled samples.

The certain unlabeled samples can be incorporated to im-prove the mixture density estimation incrementally (i.e., reg-ularly updating the model parameters without changing the model structure) by reducing the density variance. With the updated concept model for the given semantic video con-cept C j (i.e., incremental classifier), the confidence scores for some uncertain unlabeled samples may be changed over time when they originate from the unknown video context classes that cannot be interpreted intuitively by a limited number of labeled samples. For the uncertain unlabeled sample, the changing scale of its confidence scores with the given semantic video concept C j is defined as: where y l  X  0,  X  ( X l ,C j ,t ) and  X  ( X l ,C j ,t + 1) indicate its confidence scores with the same concept model C j before and after the model update. The uncertain unlabeled sam-ples with a large value of y l may originate from the unknown video context classes induced by concept drift, and should therefore be used to achieve more accurate video concept interpretation and semi-supervised classifier training. Thus, we name the uncertain unlabeled samples with a large val-ues of y l as informative unlabeled samples . To address the concept drift problem, one or more new mixture components can be added to the residing areas for the informative unla-beled samples (i.e. birth ).
 where  X   X  j +1 is the weight for the (  X  j + 1)th mixture com-ponent P ( X | C j ,  X   X  j +1 ) to characterize the appearance of unknown video context class for the given semantic video concept C j .

On the other hand, the outlying unlabeled samples with the y l value close to zero may originate from new concept or outliers. To eliminate the misleading effects of the outlying unlabeled samples, a penalty term  X  l is defined as: where 0  X   X  l  X  1,  X  l = 0 if y l = 0. Thus, the penalty term  X  can provide an effective solution to select the informative unlabeled samples for semi-supervised classifier training.
To avoid the problem of overfitting the unlabeled samples , the MDL term for model selection is updated by including Figure 3: The experimental results for video event de-the size of unlabeled samples which have nonzero-value of  X  . In addition, the likelihood function as described in Eq. (2) is replaced by a joint likelihood function for both the labeled samples and the unlabeled samples. Thus, the joint objective function is defined as: where the discount factor  X  = N u N the relative contribution of the unlabeled samples for semi-supervised classifier training, N u is the total number of un-labeled training samples, N L is the total number of labeled training samples. Using the joint objective function in Eq. (11) to replace the objective function in Eq. (2), our adap-tive EM algorithm is applied to the mixture training sample set, both originally and probabilistically labeled, to learn the classifier accurately.

Once the classifiers for the semantic video concepts of par-ticular interest are available, they are used to classify the video shots for the given video clip into a set of privacy-sensitive semantic video concepts. Semantic understand-ing of the given video clip is thus achieved. The context-dependent video shots that are mapped onto the same privacy-sensitive semantic video concept are then merged as the privacy-sensitive video event . Our experimental results for privacy-preserving video event detection are given in Fig. 3.
Once the detection functions for the privacy-sensitive video objects and video events are available, they are used to pro-tect the content privacy at the individual video clip level. To filter out the privacy-sensitive human objects (i.e., doc-tors, professional patient trainers, patients in video), we use digital human models (i.e., virtual human objects) to re-place the appearances of privacy-sensitive human objects in video. Thus, the blurred video streams are able to pro-tect the privacy-sensitive information about who are in the Figure 4: Experimental results for video content pri-Figure 5: Experimental results for video content pri-video scene. The blurred video streams are still able to pro-vide enough non-sensitive information about the real med-ical treatment procedure for one certain infectious disease and enable high-quality online patient training and coun-seling. In addition, the blurred video streams are able to provide the non-sensitive information about the number of people in the scene and a rough idea about their postures, but it makes impossible for the receivers to guess who these persons are because no image details are conveyed in the blurred video streams. Experimental results on video con-tent privacy protection are given in Fig. 4, Fig. 5, and Fig. 6.

To protect the context-dependent video content privacy, a set of video shots that are relevant to the detected privacy-sensitive video events are removed from the original video clip, and the residual non-sensitive video shots are re-packaged as a new MPEG video stream.
To support more effective online patient training and coun-seling, it is very important to enable privacy-preserving video sharing among multiple competitive groups and organiza-tions. However, sharing large-scale video clips may induce the privacy breaches because the dishonest users may use statistical inference techniques to infer the individual vide o owner X  X  privacy. To prevent the statistical inferences from video collections, we propose a distributed framework that enables a privacy-preserving classifier training by treat-ing  X  individual video owners as  X  horizontally partitioned data sources. For a given semantic video concept C j , each Figure 6: The experimental results for video content video owner can independently learn an individual weak con-cept model (i.e., local classifier) by using his/her own train-ing samples (as shown in Fig. 7). Our model-based clas-sifier training technique described in Section 4 can be used to select the optimal model structures and to estimate the accurate model parameters for these  X  weak concept models.
In order to achieve more accurate classification of distrib-uted video contents, it is very important to learn the clas-sifier accurately by collecting the training samples from all these  X  video owners. However, sending the training sam-ples to the central site is undesirable from the privacy per-spective because the dishonest users may be able to infer the individual video owners X  privacy-sensitive information by using statistical inference techniques. To prevent statis-tical inference from video collections, we have proposed a distributed approach to enable privacy-preserving classifier training. Instead of sending the original training samples to the central site, each individual video owner has to send his/her weak concept model to the central site for combined classifier training (i.e., learning global concept model for ac-curately interpreting the given semantic video concept).
To enable privacy-preserving distributed classifier train-ing, virtual samples are directly generated from the avail-able weak concept models at the central site by using Markov Chain Monte Carlo sampling technique [7]. We call these training samples generated from the  X  weak concept mod-els at the central site as the virtual samples because they are not obtained directly from the original video streams. The virtual samples asymptotically have the same statistical properties as the original video data because both of them originate from the same mixture density function (i.e., same weak concept model). Such virtual samples are thus able to effectively train the combined classifier [7]. In addition, the virtual samples are also sufficiently different from the origi-nal video data and thus they are able to protect the privacy of the original video streams. Without having available the blurred video streams, it is impossible for the dishonest users at the central site to reliably relate the virtual samples to the original video streams and to violate the individual video owner X  X  privacy. Thus, generating the virtual samples from these  X  weak concept models at the central site can signifi-cantly reduce the privacy breaches and can also drastically reduce the costs for data transmission.

Based on these observations, our framework for combined classifier training takes the following steps: (a) The mix-ture components from the  X  weak concept models are com-bined to obtain a  X  X seudo-complete X  global concept model Figure 7: The distributed framework for privacy-for interpreting the given semantic video concept C j more accurately. The virtual samples from these  X  weak concept models are integrated as the combined virtual samples to learn the underlying global concept model for accurately in-terpreting the given semantic video concept C j . (b) Based on the available mixture components shared from these  X  weak concept models, our adaptive EM algorithm is used to select the optimal model structure and estimate the ac-curate model parameters for the global concept model by performing automatic merging , splitting , and elimination of mixture components. (c) The mixture components with less prediction power on the combined virtual samples are elim-inated. The overlapped mixture components from different weak concept models are merged into a single mixture com-ponent. The elongated mixture components that underpop-ulate the combined virtual samples are split into multiple representative mixture components.

By integrating all these  X  weak concept models shared from  X  data sites, the global concept model for interpreting the given pattern or concept C j is defined as: where  X  c j = components shared from the  X  individual data sites, M h  X   X  , and M h is the number of mixture components shared from the h th data site (i.e., the h th weak concept model has  X  h mixture components totally).

If one mixture component, P ( X | C j ,  X  m ), is eliminated , the global concept model for accurately interpreting the given pattern or concept C j is then refined as:
P ( X,C j ,  X  c j ) =
If two mixture components P ( X | C j ,  X  m ) and P ( X | C  X  ) are merged as a single mixture component P ( X | C j ,  X  ml ), the global concept model for accurately interpreting the given semantic video concept C j is refined as:
P ( X,C j ,  X  c j ) =
If one mixture component, P ( X | C j ,  X  h ), is split into two new mixture components, P ( X | C j ,  X  r ) and P ( X | C global concept model for accurately interpreting the given semantic video concept C j is refined as:
P ( X,C j ,  X  c j ) = By using our adaptive EM algorithm to directly combine these  X  weak concept models that are independently learned Figure 8: The classifier performance (i.e., precision  X  ) from  X  individual video sources, our framework for combined classifier training is expected to derive the global concept model able to interpret the given semantic video concept C more accurately.

In order to validate the combined classifier (i.e., global concept model) at the central site, each individual video owner has to share a limited number of blurred test samples . However, it is impossible to prevent misuse of these blurred test samples once they are released. In order to prevent statistical inferences from the blurred test samples at the central site, we have proposed a novel approach whose goal is to estimate the optimal size of such samples. Such optimally sized samples are able to prevent statistical inferences while reliably validating the combined classifier.

Because each individual video owner O l sends not only the blurred test sample set S but also his/her weak con-cept model to the central site, it is possible for the dishon-est users to incorporate the O l  X  X  weak concept model with his/her blurred test sample set S to infer the O l  X  X  private information. In order to present our approach for prevent-ing statistical inferences, we first need to define a metrics to estimate the individual video owner O l  X  X  privacy disclo-sure induced when sharing the weak concept model and the blurred test sample set S with size n . The metrics we adopt is defined as follows: where H (  X  ) is the well-known Shannon entropy, P ( C,O l  X  , S ) is the posterior probability of the users X  prediction of the O l  X  X  privacy C after exploiting the O l  X  X  blurred test sample set S and his/her weak concept model, P ( C ) is the prior probability of the users X  prediction of the O l  X  X  privacy C .

To incorporate the O l  X  X  blurred test sample set S for clas-sifier validation, it is very important to determine what size of the blurred test sample set gives statistically significant validation results while preventing the dishonest users from inferring the individual video owner X  X  private information. We use the well-known distribution-independent bound (i.e., Chebychev inequality [8]) to determine the minimum size n min of the blurred test sample set S : where Prob (  X  ) is the underlying probability distribution such as Gaussian distribution,  X  is the pre-defined bound for the expected error rate of the combined classifier, 0  X   X   X  1, p is the error rate. where  X  p is the average error rate of the blurred test sample set S ,  X  2 is the variance of the blurred test sample set S .
On the other hand, it is also critical to determine the maximum size n max of the blurred sample set S that may result in privacy breaches [8, 17]; this size is etimated as follows: % ( C,n max ,O l ) = inf X  X  S where  X  is the pre-defined confidence bound, P ( C,O l | X ,  X  S ) is the highest posterior probability for the user X  X  predic-tion of the O l  X  X  privacy C, and &lt; is credible set for the potential predictors which have the highest posterior prob-ability close to  X  . In our experiments, we set  X  = 50% so that data mining tools cannot obtain reliable results [5-12].
Thus, the optimal size n optimal of the blurred test sample set S is determined by the following low and up bounds: where the n min and n max are the low and up bounds deter-mined by Eqs. (16) and (18).

To achieve a good balance between limiting the privacy breaches and enabling reliable classifier validation, the opti-mal size n optimal of the O l  X  X  blurred test sample set S to be shared is determined by an optimization procedure: By optimizing the criterion given by Eq. (20), the optimal size n optimal of the blurred test samples from each individ-ual video owner, that are necessary to reliably validate the combined classifier while limiting the privacy breaches, can be obtained accurately.

By determining the optimal size of the blurred test sam-ples to be shared, our framework is able to enable privacy preserving distributed classifier training and to effectively prevent statistical inferences from video collections.
Our experimental algorithm evaluation focuses on: (a) evaluating the performance of our adaptive EM algorithm with different combinations of merging, splitting and elimi-nation; (b) evaluating the performance of our classifier train-ing technique when using different sizes of unlabeled sam-ples; (c) evaluating our distributed framework for privacy preserving classifier training to prevent statistical inferenc es.
The benchmark metric for the classifier evaluation includes precision  X  and recall % . They are defined as: where  X  is the set of true positive samples that are related to the corresponding concept and are classified correctly,  X  is the set of true negative samples that are irrelevant to the corresponding concept and are classified incorrectly, and  X  Figure 9: The empirical relationship between the classi-is the set of false positive samples that are related to the corresponding concept but are misclassified.

In our adaptive EM algorithm, multiple operations, such as merging, splitting, and elimination, have been integrated to re-organize the distributions of mixture components, se-lect the optimal model structure and construct more flexible decision boundaries among different concepts according to the real class distributions of the training samples. Thus, our adaptive EM algorithm is expected to have better per-formance than the traditional EM algorithm and its recent variants [14-15].

In order to evaluate the real benefits of the integration of these three operations (i.e. merging, splitting, and elim-ination), we have tested the performance differences of our adaptive EM algorithm with different combinations of these three operations. As shown in Fig. 8, we have tested the performance of the classifiers under different combinations of three operations: only splitting, only merging, combining splitting and merging (i.e. SM), combining splitting, merg-ing and elimination (i.e., SM + Neg). From these experi-mental results, one can find that our adaptive EM algorithm can improve the classifiers X  performance significantly.
Given a limited number of labeled samples, we have tested the performance of our classifiers by using different sizes of unlabeled samples for classifier training (i.e. with different size ratios  X  0 = N u N the labeled samples N L ). The average performance differ-ences are given in Fig. 9 and Fig. 10.

When a limited number of labeled samples is available and more unlabeled samples are involved for semi-supervised classifier training (i.e.,  X  0 = N u N also obtained a decrease in the classifier X  X  performance be-cause large-scale outlying unlabeled samples have dominated the statistical properties of the joint class distribution and misleaded the classifier. Ideally, it is possible for the dishon-est users to integrate large-scale non-sensitive data (i.e., un-labeled samples) with a limited number of privacy-sensitive blurred test samples (i.e., labeled samples) to infer the indi-vidual video owner X  X  privacy. However, this empirical obser-vation (i.e., decrease of prediction accuracy) has provided very convincing evidence for the efficiency of our proposed solution on preventing statistical inferences: It is impossible for the dishonest users to obtain reliable results when only a limited number of blurred test samples are shared .
To evaluate our distributed framework for privacy pre-serving classifier training, we partitioned each data set into three individual groups and performed classifier training on Figure 10: The empirical relationship between the clas-these three individual data groups independently. We have obtained the empirical relationships between the quality of the global concept model (i.e., precision of the combined classifier) and the privacy disclosures as shown in Fig. 11.
For validating the combined classifier at the central site, each individual video owner has to share not only his/her weak concept model but also a limited number of blurred test samples. To prevent statistical inferences, we have also obtained the empirical relationships between the privacy dis-closures and the number of blurred test samples to be shared as shown in Fig. 12. One can find that sharing more blurred test samples decreases the individual video owner X  X  ability on controlling the statistical inferences and results in the privacy breaches.
To enable privacy-preserving video sharing among multi-ple competitive groups and organizations, we have proposed a novel framework able to both protect the video content pri-vacy and control the statistical inferences. By detecting the privacy-sensitive video objects and video events automati-cally, our proposed algorithm is able to effectively protect the video content privacy at the individual video clip level. By determining the optimal size of blurred test samples for classifier validation, our proposed framework for privacy-preserving distributed classifier training is able to not only limit the privacy breaches but also improve the classifier X  X  accuracy significantly. Our experiments in the specific do-main of online patient training and counseling videos show that our techniques are effective. Figure 11: The empirical relationship between the clas-Figure 12: The empirical relationship between the pri-
