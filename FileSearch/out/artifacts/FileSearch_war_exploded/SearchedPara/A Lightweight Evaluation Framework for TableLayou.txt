
Feng Zhu 1 , 2 ,JieLiu 2 , 3 , Lijie Xu 1 , 2 ,DanYe 2 ,JunWei 2 , 3 , and Tao Huang 2 The MapReduce [6] programming model and its open-source implementation Hadoop [1] has now become the de facto framework to store and process mas-sive data. However, it is usually too complicated to express complex analytical tasks (e.g., the business intelligence) as primitive MapReduce jobs. To enhance productivity, high level systems, that brings the relational concepts like SQL, table, row and column, have been built, such as Pig [8] and Hive [7].
In such environment, data is managed as the structured relational table that consists of rows and columns. Table layout determines the way how the two di-mensional data values will be organized in the underlying distributed file system. For example, the one based on Hadoop X  X  built-in textfile encapsulates table rows into records; properties, like the field delimiter, need to be explicitly specified to store and resolve tables. Besides, tabl e layouts with sophisticated mechanisms have also been proposed, such as Zebra [5], CFile in Llama [10], CIF [11], RCFile [9], ORCFile [2], Parquet [3], Trevni [4], Trojan [13] and SLC-Store [12].
However, the different table layouts are proposed and implemented indepen-dently; a comprehensive study to compare them has not been done [14]. In prac-tice, many situations involve evaluation on table layouts for appropriate choice, for example the following two scenarios. (1) TableConfigurationinDesignPhase. It is common for users to configure tables X  layouts under specific workloads in the design phase. Migrating tables and queries from relational database to Hadoop is a representative case. In such case, the schema of tables and the queries are known, appropriate table layouts can achieve comprehensive advantages on data loading, query performance and storage space. (2) Adaptively Set Intermediate Table Layout. High level language systems translate queries into MapReduce job workflows. The temporary intermediate tables produced by the previous job will be consumed as input by subsequent job in the workflow. For the performance improvement, can we adaptively set the intermediate tables X  layouts rather than the default setting (for example, the sequencefile in Hive)? Above all, the problem can be abstracted as: Given workloads and the schema of tables, make suggestion on appropriate table layout.

The straightforward solution is to run queries on generated or sample data, and then compare the results. However, it has many defects. To begin with, the query X  X  MapReduce runtime in distributed environment is nondeterminis-tic. Even the time costs on performing the same query with the same configu-ration twice are always different. This nondeterministic behavior is caused by many table-layout-independent factors (like scheduling), and incurs the inac-curacy of evaluating the real performan ce of table layouts. Moreover, it is too time-consuming to run all queries; actually, table layouts are aware of only few operations even in a complex query. Another way for the problem is to estab-lish fine-grained performance models that cover all factors for each table layout. The challenges, which will be illustrated later, result that common performance models are unrealistic to establish.

To address the problem, we propose and implement an evaluation framework. (1) The framework, from the perspective of a task (stack:  X  query  X  workflow  X  job  X  task  X  ) that directly interacts with table layouts, is lightweight. (2) The framework evaluates high-lev el user-oriented metrics, r ather than table layouts X  X  internal implementation factors, and provides users with comprehensive eval-uation report. (3) The framework adopts the testing approach which is based on the black box method and the query aware strategy . Different table layouts are treated as black boxes with configuration knobs. The query aware strategy extracts the table-layout-related opera tions from upper com puting layer X  X  query. Contributions. We identify three high-level user-oriented metrics of table lay-outs. Based on the black box method and query aware strategy, we design and implement a practical lightweight framework to evaluate different table lay-outs. From the perspective of a task, the framework avoids the nondeterministic behavior and the time cost of running the query. It automatically extracts table-layout-related operations from queries a nd rapidly evaluates the metrics. More-over, we conduct extensive experimental studies to make empirical analysis on representative table layouts and discuss the optimizations chances for interme-diate tables.

The reminder of this paper is organized as follows. In section 2, we introduce the related works. Section 3 presents th e black box method and the query aware strategy. In section 4, we describe the evaluation framework. Section 5 conducts experimental study and makes discussion. We conclude our work in section 6. As early as the development era of relational database systems, table layouts had been widely studied. For example the row-oriented store [15], the column-oriented store [16,17,18] and the hybrid PAX store [19]. The weaknesses and advantages for each table layout were intensively investigated. In [18], D.J. Abadi et al makes an in-depth discussion among them.

In Hadoop X  X  distributed environment, table layout has also attracted a wide range of interests in both academia and industry. However, these table layouts are proposed and implemented independently or even for specific workloads. Most recently, Yin Huai et al. [14] makes a comprehensive and systematic experimental study; they define three core operations to abstract table layouts X  behaviors and evaluate key factors: (1) table X  X  horizontal logical subset size, (2) the function of mapping columns to column groups, and (3) the function of packing columns or column groups in a row group into physical blocks. Based on the evaluation, practical actions to optimize read performance are suggested. Our work differs from it as follows. To begin with, we argue that the abstraction of three core operations to describe the table layouts has its limitations. It is difficult to unify all table layouts and the three factors are not common. For example, CIF has no concept of row group and column group. Our work covers both the read and write performance. Moreover, we consider com puting layer X  X  queries and implement a lightweight evaluation framework for practical use.

Related works for query optimization are usually conducted from two lay-ers: the translator and the MapReduce computing primitive. In the MapRe-duce primitive layer, the study focuses on MapReduce X  X  performance models and some common optimization techniques. For example, H. Herodotou [20] proposes cost models to analyze and optimize MapReduce programs. MRShare [21] investigates opportunities to reduce the number of MapReduce jobs, i.e., the so-called  X  X ertical packing X ; while Stubby [22] further covers  X  X orizontal packing X  opportunities between two MapReduce job workflows. Particularly, optimizing the SQL-to-MapReduce translator also gains broad attentions and several rule-based translators has eme rged in recent years, such a s YSmart [23]. It applies a set of rules to use the minimal number of MapReduce jobs to execute multiple correlated operations in a complex query. 3.1 Table Layout Insight We take four representative table layou ts as examples: TextFile, SequenceFile, RCFile and ORCFile, which include both unstructured and structured layouts and cover various performance-related factors.  X  TextFile. The ASCII encoded text based file format in HDFS. The MapRe- X  SequenceFile. SequenceFile provides a persist ent append-only data struc- X  RCFile. RCFile stores columns of a table i n a record columnar way. It first  X  X RCFile. The optimized version of RCFile. Compared with RCFile, it pro-
Fig. 1 demonstrates four optional consecutive phases (i.e., decomposition, buffer phase, compression and serialization) for writing a tuple into the file sys-tem (the case of ORCFile is similar to that of RCFile). Correspondingly, the common phases for reading records are reversed, i.e., deserialization, decom-pression, buffer phase and reconstruction. For simplified presentation, we only illustrate the writing behaviors here.
 Tuple decomposition is the first phase for those none row-oriented table layout. In this phase, the tuple will be decomposed as different concepts like columns, column groups and so on. Before flushed to disk, data will be first managed in the buffer. The buffer not only stores data, but also maintains the metadata for data. The metadata stores the information such as column data size, col-umn type and even statistics (like max, min and etc.) in sophisticated table layouts. When the data reaches the co nfigured capacity, it will be written out to the disk. The compression phase consists of two levels, the column encoding schemes and the generic compression algorithms. The column encoding scheme adopts fine-grained algorithms for different column types, for example, the run length encoded algorithm fo r integers and the dictionar y for strings. The generic compression algorithms take table as common binary files using algorithms like gzip, zlib and so on. Compression results in lower IO cost at the expense of higher CPU computing. Serialization converts in-memory data structures into bytes that can be transmitted over the network or flushed to the disk. The serialization phase is common with all table layouts.

The above insights reveal difficulties to establish fine-grained performance models for table layouts. (1) It is too complex for the models that covers all fac-tors and table layout X  X  configuration parameters, resulting that models are hard to be accurate. (2) Fine-grained models are difficult to validate and more im-portantly, the internal factors are meaningless to users and the computing layer. (3) The performance of table layout is cl osely related with implementations. In such case, comparisons among them are apple to an orange. 3.2 Black Box Method Throughout this paper, we adopt the testing approach based on the black box method to evaluate the table layouts. As depicted in the left part of Fig. 2, the computing layer of MapReduce based sy stems write and read data row by row, respectively through the unified write() and read() interfaces.

The black box method treats different table layouts as different boxes with knobs that can be adjusted by users to define configuration parameters. The knobs for writing are key/value pairs (i.e., buffer size, row group size and etc) stored in the conf . Based on these specified settings, the layout determines how the record data will be organized. The knob s for records reading are projection and predicate, with which the IO cost can be reduced by pushing down the operations into the storage layer. However, the two knobs are optionally im-plemented and not exposed by all table layouts; for example, the RCFile only supports projection. 3.3 Query Aware Strategy It is unnecessary to evaluate a table layout through executing the query. Actually, table layouts are aware of only few operations even in a complex query; the most query execution time is spent on in-memory computations. Rather than executing the query, the query aware strategy only extracts the table-layout-related operations.
 Table-Layout-Related Operations Extraction. Given a specific query, the strategy translates it into abstract syntax tree ( AST ) and then analyzes AST to extract the operations that can be pushed down to table layouts. In MapReduce based query systems, only the operations of column projection and predicate push down can be exploited by the computing l ayer. The process of extracting these two operations from AST is straightforward, the algorithm searches the tree in a depth-first behaviour and get s the operations in the nodes with the corresponding keywords (e.g, the TOK_SELECT, TOK_WHERE and so on).

As demonstrated in the right part of Fig. 2, the column projection ( X  X ,D X ) and the predicate ( X  D&lt; 133  X ) are extracted from th e query. Assume query Q has m times scans on table T , we denote the operations as the set of projection the example query, the result is OP ( Query, T )= { &lt; ( A, D ) , ( D&lt; 133) &gt; } . 4.1 Design Methodology and Evaluation Metrics The task (i.e., mapper and reducer ), which directly interacts with table layout, is the bottom unit of the stack  X  query  X  workflow  X  job  X  task .Weadvocate  X  X ee big things through small ones X , and propose TLEF to evaluate the critical metrics of table layout from the perspective of a task.

Rather than table layouts X  detailed implementation factors, TLEF evaluates three high-level user-oriented metrics, cr, ws, rs . Under the specific configu-ration conf , we get the compression rate (short as cr ), average writing speed (short as ws ), average reading speed (short as rs ). As the data may be stored in different node as the node of computing task, rs consists of the local reading speed rs local and remote reading speed rs remote . 4.2 Framework Architecture The architecture of TLEF is demonstrated in Fig. 3. Users can input the work-loads (expressed as HiveQL language), table layout configuration to be override and table specifications (including the schema of tables and other configurations like the range of a string column X  X  length) through TLEF X  X  user interface. The core of TLEF is the evaluation manager, which generates the evaluation plan and calls the evaluator to generate the read/write workloads.

The Basic Environment Inspector inspects the environment-specific configu-rations, such as the cluster X  X  node number, the block size in HDFS and so on. The Operator Analyzer parses the query and extracts the table-layout-related operations (including projection and predicates) that can be pushed down to the storage layer. For the general cases, the Table Generator randomly gener-ates the tables according to the table specifications. The Table Layout Lib not only contains built-in table layouts, but also exposes the unified interfaces. De-velopers can implement the interfaces for the upcoming new table layouts. The Result Reporter produces the evaluation metrics, based on which TLEF will make suggestions on table layout. 4.3 Configuration Space Exploration There are a large number of variables that can be specified by users in table layout configuration and the default configuration may not be expected to be the most efficient. A straightforward exploration approach is to apply enumeration and search techniques to the full space of parameter settings. Some parameters have small and finite domains, e.g., compression type, while the domain of some parameters is unbounded. In TLEF, each implemented table layout enumerates the domains in Econf for individual configuration parameters. The exploration of our evaluation framework is as follows: (1) Evaluate individual parameters. For each parameter, set other parameters as default value, test the table layout with the parameter setting in Econf to get the metrics. (2) Heuristic search with user defined algorithm. TLEF exposes the algorithm interface for user. 4.4 Evaluation Report: Extending to Distributed Environment Instead of solely making the choice, TLEF generates the evaluation report to users, including the contents of three resp ects: the high level user-oriented met-rics, table layout suggestion for each query and the comprehensive analysis.
Assume the table has n rows, the row size can be estimated as rsz according to the schema; then the table size is ts = cr  X  n  X  rsz . We note the MapReduce split size as splitsize and the total mapper slots number as msn . Then the length of mapper round can be calculated as ts/ ( splitszie  X  msn ) . For example, there are 1204 / (64  X  16) =2 rounds of mappers for a 1GB table with the configuration of 64MB split size and 12 mapper slots.

Table layout suggestion for each query. TLEF lists the table-layout-related oper-ations and calculates the new average read speed under column projection, rs new = rs  X  projectedSize/rowSize . During the query execution, the results produced by each reducer are certain and irrelevant to the table layouts; hence the write perfor-mance is determined by the metric ws . For example, when the query is data load-ing, table layout with better ws is preferred. As for the read performance, TLEF suggests the candidate with efficient rs new  X  ts/ ( splitszie  X  msn ) . In the dis-tributed environment, the I O read performance may be disturbed by many factors. For the local and remote cases, we respect ively define the perturbation functions  X  and  X  r . Then the upper bound and lower bound can be estimated as the follow-ing expressions. ts/ ( splitszie  X  msn )  X   X  l ( rs local ) and ts/ ( splitszie  X  msn )  X   X  ( rs
Comprehensive analysis. It is straightforward for users to make choice accord-ing to the table layout suggestions for each query. Generally, there is a tradeoff of storage space utilization and query performance, which depend on user X  X  pref-erence. If the previous case is the main co nsideration, table layout with efficient cr metric is preferred. For the latter one, it depends on different queries X  weights (e.g., the one with the largest proportion or the one for the most frequent query, and etc.); we discuss some cases here. When the workload only includes data loading, table layout with better ws is preferred. However, write-once-read-more is frequent and query performance is usually the focus in MapReduce based query systems. For the extreme case of no extracted operation, a row-oriented table layout is appropriate for all column scans. For the general case with many table-layout-related operations, a table layout supporting them is important. In this section, we conduct experimental study from two dimensions. First, we empirically evaluate the popular table l ayouts from the perspective of a single task; we then extend the results to the distributed environment. Next we make discussion on the intermediate table layout strategy for query optimization.
Out experimental study is conducted on a local cluster of 10 DELL OptiPlex-990 nodes connected by a 1GB ethernet sw itch. Each node is equipped with four Intel i7-2600 3.4GHz cores, 16GB RAM and 2TB hard disk drives. Operating system is Ubuntu-11.04 x86_64. Hadoop 1.2.0 and Hive 0.12.0 are used. One node is reserved for Hadoop JobTracker and the NameNode. The other 9 nodes are used for HDFS DataNode and MapReduce TaskTracker. The HDFS block size is 256MB and each file has 2 replications. In fact, these settings can be automatically detected by ba sic environment inspector.
 5.1 Empirical Performance Analysis We use a synthetic relational table generated by TLEF table generator as fol-lows: each record consists of an increm ental integer ID, 6 string columns and 6 integer columns. The integers are randomly assigned values between 0 and 100000; random strings of length between 10 and 40 are generated over readable ASCII characters, similar to the schema defined in [11].
 Metrics with Default Configuration. In practice, table layouts are often adopted with the default configuration, especially for the none-expert users. As the default configuration indicates the most common usage case, we first conduct experiments to study the general performance for table layouts. Table sizes are respectively 242.5MB, 263.9MB, 184.2MB and 169.7MB for TextFile, SequenceFile, RCFile and ORCFile. We write and read whole column set without the upper layer X  X  query semantics and the results are shown in Fig. 4. It can be seen that, fine-grained column encoding schemes gain significant compression rate and row-oriented table layouts, without extra metadata maintenance and row reconstruction, outperform others for data loading and row scanning. Configuration Space Exploration. We take two configuration parameters for ORCFile, compression-kind with the finite domain {None, Zlib, Snappy} and the strip-size with the infinite domain, to show the exploration process. The strip size varies with the values in geometrical sequences {256MB, 64MB, 16MB, 4MB, 1MB}, which is automatically generated by TLEF according to the properties defined in table layout lib. As depicted in Fig. 5, the evaluated metrics varies according to different co nfigurations. But these differences are not very significant, diversed in the same order of magnitude.
 Query Aware. As only ORCFile supports both column projection and predicate pushdown, we take it as the example to demonstrate how the query semantics can be leveraged. The client respectively reads 1400000 rows in the local mode with 2 integer column, 1 integer column and 1 string column, and all columns. The left and middle graphics in Fig. 6 respectively show the read time costs and the data sizes in different cases. The resul ts indicate the effectiveness of column projection for filtering unnecessary columns.

For the predicate pushdown, ORCFile will check the statistics every configured number and the rows will be skipped if it doesn X  X  satisfied the predicates. As the default index-strip is 10000 (maintaining column statistics, like max and min, every 10000 rows to skip rows), we evaluate the predicates  X  1 = id &lt; 140000 ,  X  2 = 140000 &lt;id&lt; 830000 and  X  3 = id &gt; 139999 . It can be seen from the right graphic of Fig. 6 that the effectiv eness of predicate push down is heavily relied on the predicate X  X  selectivity on data.
 Distributed Environment. We generate the table with 56 millions rows (about 9.2GB) stored as textfile. The benchmark query is  X  X elect count(*) from T X  , which will be translated into one MapReduce job with 39 mappers and 1 re-ducer in our environment. Fig. 7 demonstr ates the runtime of query X  X  execution. It can be seen that the performance of task with local data is stable, while those with the remote data differ greatly.
 5.2 Intermediate Table Layout for High Level Query Systems Assume the producer query Q 1 has n columns WC = { WC i | 1  X  i  X  n } and the consumer query Q 2 will read m columns RC = { RC i | 1  X  i  X  m } . To investigate whether the intermediate table layout can be adaptively set based on the Q 1 and Q 2 , we discuss the characters of column projection and predicate push down.
Column projection. Generally, the column pruning (called  X  X p X ) is adopted in high level language systems X  query optimizers. With this optimization, Q 2 reads all columns produced by Q 1 , i.e., m = n and WC = RC . Hence, Table layouts that support column projection can not exploit the advantage.

Predicate push down. Similarly, predicate push down (called  X  X pd X ) is also supported in query optimizers. Assume the predicate is for one column WC k in Q 2 , if and only if the WC k is a dynamically new column generated by Q 1 (i.e., the aggregated column, calculated by user-defined functions and etc.), the predicate push down can be exploited. For instance, Q 1 is  X  X elect col1, sum(c2) as col2 group by col1 X  and Q 2 is  X  X elect co1, col2 where col2&gt;2 X  .
In addition, the intermediate table size can only be determined after the execution of Q 1 . Optimizing the intermediate table layout consequently brings not too much improvement on p erformance. Considering Q 2 scans all columns, setting the intermediate table layout as a row-oriented one (for example, the default sequencefile setting in Hive) is reasonable for most cases. In this paper, we propose the testing approach based on black box method and the query aware strategy to evaluate the table layouts in MapReduce based environment. To assist users make the appropriate choice under specific work-loads, we develop a lightweight evaluation framework. It automatically extracts table-layout-related oper ations from queries and rapidly evaluates the metrics. Acknowledgement. This work was partially supported by National Science Technology Support Plan (2013BAH05F03) and National Natural Sciences Foun-dation of China (61202065, U1435220, and 61170074).

