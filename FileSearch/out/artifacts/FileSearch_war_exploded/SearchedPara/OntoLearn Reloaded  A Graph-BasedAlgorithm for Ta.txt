 Sapienza University of Rome Sapienza University of Rome Sapienza University of Rome to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has ogy, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from this graph via optimal branching and a novel weighting policy. Our experiments show that we obtain high-quality results, both when building brand-new taxonomies and when reconstructing sub-hierarchies of existing taxonomies. 1. Introduction
Ontologies have proven useful for different applications, such as heterogeneous data integration, information search and retrieval, question answering, and, in general, for fostering interoperability between systems. Ontologies can be classified into three main types (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose cat-egories are distinguished by axioms and formal definitions, stated in logic to support complex inferences and computations; ii) prototype-based ontologies, which are based on typical instances or prototypes rather than axioms and definitions in logic; iii) lexical-ized (or terminological) ontologies, which are specified by subtype-supertype relations and describe concepts by labels or synonyms rather than by prototypical instances. language applications such as semantically enhanced information retrieval and ques-tion answering, we need a clear connection between our formal representation of the domain and the language used to express domain meanings within text. And, in turn, this connection can be established by producing full-fledged lexicalized ontologies for the domain of interest. Manually constructing ontologies is a very demanding task, however, requiring a large amount of time and effort, even when principled solutions are used (De Nicola, Missikoff, and Navigli 2009). A quite recent challenge, referred to as ontology learning , consists of automatically or semi-automatically creating a lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and
Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al. 2011). As a result of ontology learning, the heavy requirements of manual ontology construction can be drastically reduced.
 of an ontology) entirely from scratch. Very few systems in the literature address this task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this area. In OntoLearn taxonomy learning was accomplished in four steps: terminology extraction, derivation of term sub-trees via string inclusion, disambiguation of domain terms using a novel Word Sense Disambiguation algorithm, and combining the sub-knowledge, namely, WordNet (Miller et al. 1990; Fellbaum 1998), prevented the system from learning taxonomies in technical domains, however.
 a taxonomy from the ground up. OntoLearn Reloaded preserves the initial step of our 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminology extraction from a domain corpus, but it drops the requirement for WordNet (thereby avoiding dependence on the English language). It also drops the term compositionality assumption that previously led to us having to use a Word Sense Disambiguation algorithm X  X amely, SSI (Navigli and Velardi 2005) X  X o structure the taxonomy. Instead, we now exploit textual definitions, extracted from a corpus and the Web in an iterative fashion, to automatically create a highly dense, cyclic, potentially disconnected hyper-nym graph. An optimal branching algorithm is then used to induce a full-fledged tree-like taxonomy. Further graph-based processing augments the taxonomy with additional hypernyms, thus producing a Directed Acyclic Graph (DAG).
 taxonomy learning: 666 2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) we enhance our methodology with a final step aimed at creating a DAG, rather than a strict tree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluation of the taxonomy learning algorithm on six domains; and iv) we contribute a novel methodology for evaluating an automatically learned taxonomy against a reference gold standard.
 induction algorithm in Section 3. In Section 4 we present our experiments, and discuss the results. Evaluation is both qualitative (on new A F
INANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Sec-tion 5 is dedicated to concluding remarks. 2. Related Work
Two main approaches are used to learn an ontology from text: rule-based and distri-butional approaches. Rule-based approaches use predefined rules or heuristic patterns to extract terms and relations. These approaches are typically based on lexico-syntactic patterns, first introduced by Hearst (1992). Instances of relations are harvested from text Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999;
Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques (Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computa-tional lexicons such as WordNet (Ponzetto and Navigli 2009).
 classification task, and draw primarily on the notions of distributional similarity (Pado and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such approaches are based on the assumption that paradigmatically-related concepts in similar contexts and their main advantage is that they are able to discover relations that do not explicitly appear in the text. They are typically less accurate, however, and the selection of feature types, notion of context, and similarity metrics vary considerably depending on the specific approach used. tion framework that integrates contextual, co-occurrence, and syntactic dependencies, incrementally clustered on the basis of their ontology metric scores. In their work, the authors assume that the set of ontological concepts C is known, therefore taxonomy learning is limited to finding relations between given pairs in C . In the experiments, they only use the word senses within a particular WordNet sub-hierarchy so as to avoid any lexical ambiguity. Their best experiment obtains a 0.85 precision rate and 0.32 recall rate in replicating is-a links on 12 focused WordNet sub-hierarchies, such as P B using a probabilistic model. In their work they combine evidence from multiple supervised classifiers trained on very large training data sets of hyponymy and cousin the problem of finding the taxonomy that maximizes the probability of having that evidence (a supervised logistic regression model is used for this). Rather than learning a new taxonomy from scratch, however, this approach aims at attaching new concepts under the appropriate nodes of an existing taxonomy (i.e., WordNet). The approach is evaluated by manually assessing the quality of the single hypernymy edges connecting leaf concepts to existing ones in WordNet, with no evaluation of a full-fledged struc-tured taxonomy and no restriction to a specific domain. A related, weakly supervised approach aimed at categorizing named entities, and attaching them to WordNet leaves, was proposed by Pasca (2004). Other approaches use formal concept analysis (Cimiano,
Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn tax-onomies from a folksonomy (Tang et al. 2009), and Markov logic networks and syntactic parsing applied to domain text (Poon and Domingos 2010).
 initial given set of root concepts and basic level terms, the authors first use Hearst-like lexico-syntactic patterns iteratively to harvest new terms from the Web. As a result a set of hyponym X  X ypernym relations is obtained. Next, in order to induce taxonomic relations between intermediate concepts, the Web is searched again with surface pat-terns. Finally, nodes from the resulting graph are removed if the out-degree is below a threshold, and edges are pruned by removing cycles and selecting the longest path in the case of multiple paths between concept pairs. Kozareva and Hovy X  X  method has ating their methodology, the authors discard any retrieved nodes not belonging to a
WordNet sub-hierarchy (they experiment on P LANTS ,V EHICLES it all comes down to Yang and Callan X  X  (2009) experiment of finding relations between a pre-assigned set of nodes.
 to the task of creating a new taxonomy for an arbitrary domain of interest truly from scratch. Instead, what is typically measured is the ability of a system to reproduce as far as possible the relations of an already existing taxonomy (a common test is WordNet against a gold standard is, indeed, a reasonable validation methodology. The claim to be 668  X  X utomatically building X  a taxonomy needs also to be demonstrated on new domains for which no a priori knowledge is available, however. In an unknown domain, tax-onomy induction requires the solution of several further problems, such as identifying domain-appropriate concepts, extracting appropriate hypernym relations, and detect-ing lexical ambiguity, whereas some of these problems can be ignored when evaluating the predecessor of OntoLearn Reloaded, that is, OntoLearn (Navigli and Velardi 2004), suffers from a similar problem, in that it relies on the WordNet taxonomy to establish paradigmatic connections between concepts. 3. The Taxonomy Learning Workflow
OntoLearn Reloaded starts from an initially empty directed graph and a corpus for the points of our algorithm, has been manually defined (e.g., from a general purpose taxon-omy like WordNet) or is available for the domain. 4 Our taxonomy-learning workflow, summarized in Figure 1, consists of five steps: 1. Initial Terminology Extraction (Section 3.1): The first step applies a term 2. Definition &amp; Hypernym Extraction (Section 3.2): Candidate definition 3. Domain Filtering (Section 3.3): A domain filtering technique is applied 4. Graph Pruning (Section 3.4): As a result of the iterative phase we obtain 5. Edge Recovery (Section 3.5): Finally, we optionally apply a recovery 3.1 Initial Terminology Extraction
Domain terms are the building blocks of a taxonomy. Even though in many cases an initial domain terminology is available, new terms emerge continuously, especially the taxonomy induction process. Thus, we start from a text corpus for the domain of interest and extract domain terms from the corpus by means of a terminology extraction algorithm. For this we use our term extraction tool, TermExtractor, implements measures of domain consensus and relevance to harvest the most relevant terms for the domain from the input corpus. 7 As a result, an initial domain terminol-ogy T (0) is produced that includes both single-and multi-word expressions (such as,
G noisy = ( V noisy , E noisy ) for each term in T (0)  X  X hat is, we set V terminologies (cf. Section 4 for more details). Note that our initial set of domain terms (and, consequently, nodes) will be enriched with the new hypernyms acquired during the subsequent iterative phase, described in the next section. 3.2 Definition and Hypernym Extraction
The aim of our taxonomy induction algorithm is to learn a hypernym graph by means of several iterations, starting from T (0) and stopping at very general terms U , that we take as the end point of our algorithm. The upper terms are chosen from WordNet topmost 670
Table 2 we show representative synonyms of the upper-level synsets that we used for the A RTIFICIAL I NTELLIGENCE and F INANCE domains. Seeing that we use high-level concepts, the set U can be considered domain-independent. Other choices are of course possible, especially if an upper ontology for a given domain is already available. t  X  U ). If it is, we just skip it (because we do not aim at extending the taxonomy beyond an upper term). Otherwise, definition sentences are sought for t in the domain corpus and in a portion of the Web. To do so we use Word-Class Lattices (WCLs) (Navigli and
Velardi 2010, introduced hereafter), which is a domain-independent machine-learned corresponding hypernym (i.e., lexical generalization) in each sentence.
 from the domain corpus, Web documents, and Web glossaries, by harvesting all the sentences that contain t . To obtain on-line glossaries we use a Web glossary extraction system (Velardi, Navigli, and D X  X madio 2008). Definitions can also be obtained via a lightweight bootstrapping process (De Benedictis, Faralli, Navigli 2013). tional. We show some terms with their definitions in Table 3 (first and second column, respectively). The extracted hypernym is shown in italics.
 3.2.1 Word-Class Lattices. We now describe our WCL algorithm for the classification of definitional sentences and hypernym extraction. Our model is based on a formal notion of textual definition. Specifically, we assume a definition contains the following fields (Storrer and Wellinghoff 2006): manually annotated with these fields, as shown in Table 4. or multi-word expression denoting the hypernym was also tagged. In Table 4, for each sentence the definiendum and its hypernym are marked in bold and italics, respectively. Unlike other work in the literature dealing with definition extraction (Hovy et al. 2003;
Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not only a variety of definition styles in our training set, in addition to the classic XisaY pattern, but also a variety of domains. Therefore, our WCL algorithm requires no re-training when changing the application domain, as experimentally demonstrated by Navigli and
Velardi (2010). Table 5 shows some non-trivial patterns for the VF field. 672 models as detailed hereafter.
 Generalized sentences. First, training and test sentences are part-of-speech tagged with the TreeTagger system, a part-of-speech tagger available for many languages (Schmid 1995).
The first step in obtaining a definitional pattern is word generalization. Depending on its frequency we define a word class as either a word itself or its part of speech. Formally, let T be the set of training sentences. We first determine the set F of words in the defined term with the token TARGET (note that TARGET  X  its words t i to word classes t i as follows: that is, a word t i is left unchanged if it occurs frequently in the training corpus (i.e., t  X  F ); otherwise it is replaced with its part of speech ( POS ( t generalized sentence s . For instance, given the first sentence in Table 4, we obtain the corresponding generalized sentence:  X  X n NNS, a TARGET is a JJ NN, X  where NN and
JJ indicate the noun and adjective classes, respectively. Generalized sentences are dou-bly beneficial: First, they help reduce the annotation burden, in that many differently to their reduction of the definition variability, they allow for a higher-recall definition model.

Star patterns. Let T again be the set of training sentences. In this step we associate a star pattern  X  ( s ) with each sentence s  X  T .Todoso,let s the star pattern  X  ( s ) associated with s is obtained by replacing with * all the tokens t that is, all the tokens that are non-frequent words. For instance, given the sentence  X  X n arts, a chiaroscuro is a monochrome picture, X  the corresponding star pattern is  X  X n *, a TARGET is a *, X  where TARGET is the defined term.
 Sentence clustering. We then cluster the sentences in our training set with the sentences in T . We create a clustering C = ( C 1  X  ( s ) =  X  i } ,thatis, C i contains all the sentences whose star pattern is  X  reported in Table 4 are all grouped into cluster C 3 . We note that each cluster C sentences in T belonging to two different clusters.

Word-class lattice construction. The final step consists of the construction of a WCL for each sentence cluster, using the corresponding generalized sentences. Given such a cluster C i  X  C , we apply a greedy algorithm that iteratively constructs the WCL. create a directed graph G = ( V , E ) such that V = { t 1 ( t and each sentence s k  X  C i such that k &lt; j according to the following dynamic program-ming formulation (Cormen, Leiserson, and Rivest 1990, pages 314 X 319): where a  X  X  0, ... , | s k |} and b  X  X  0, ... , | s j |} , S a -th token of s k and the b -th token of s j ,and M 0,0 , M all values of a and b .
 follows: where t k , a and t j , b are the a -th and b -th tokens of s matching score equals 1 if the a -th and the b -th tokens of the two generalized sentences have the same word class.
 the minimal number of misalignments between the two token sequences. We repeat this calculation for each sentence s k ( k = 1, ... , j  X  1) and choose the one that maximizes its alignment score with s j . We then use the best alignment to add s to the set of nodes V the tokens of s j for which there is no alignment to s
E the edges ( t 1 , t 2 ), ... ,( t | s a TARGET is a *. X  The corresponding WCL is built as follows: The first part-of-speech tagged sentence,  X  X n/IN arts/NN , a/DT TARGET /NN is/VBZ a/DT monochrome/JJ picture /NN, X  is considered. The corresponding generalized sentence is  X  X n NN 1 ,a TARGET is a JJ NN 2 . X  The initially empty graph is thus populated with one node for each word class and one edge for each pair of consecutive tokens, as shown in Figure 2a. Note that we use a rectangle to denote the hypernym token to the graph a start node and an end node , and connect them to the corresponding initial and final sentence tokens. Next, the second sentence,  X  X n mathematics, a graph the graph together with the edges  X  X  X   X  NN 3 and NN edges in bold). Finally, the third sentence in Table 4,  X  X n computer science, a pixel is a dot that is part of a computer image, X  is generalized as  X  X n NN is a NN 2 . X  Thus, a new node NN 4 is added, corresponding to  X  X omputer X  and new 674 edges are added that connect node  X  X n X  to NN 4 and NN 4 to NN resulting lattice.

Variants of the WCL model. So far we have assumed that our WCL model learns lattices from the training sentences in their entirety (we call this model WCL-1). We also consid-ered a second model that, given a star pattern, learns three separate WCLs, one for each of the three main fields of the definition, namely: definiendum (DF), definitor (VF), and definiens (GF). We refer to this latter model as WCL-3. Note that our model does not take into account the R EST field, so this fragment of the training sentences is discarded.
The reason for introducing the WCL-3 model is that, whereas definitional patterns are highly variable, DF, VF, and GF individually exhibit a lower variability, thus WCL-3 improves the generalization power.
 s , the classification phase for the WCL-1 model consists of determining whether there exists a lattice that matches s . In the case of WCL-3, we consider any combination of definiendum, definitor, and definiens lattices. Given that different combinations might match, for each combination of three WCLs we calculate a confidence score as follows: where s is the candidate sentence, l DF , l VF ,and l each definition field), coverage is the fraction of sentence tokens covered by the pattern.
 appropriate combination of lattices impacts the performance of hypernym extraction.
Given its higher performance (Navigli and Velardi 2010), in OntoLearn Reloaded we use WCL-3 for definition classification and hypernym extraction. 3.3 Domain Filtering and Creation of the Hypernym Graph
The WCLs described in the previous section are used to identify definitional sentences and harvest hypernyms for the terms obtained as a result of the terminology extraction phase. In this section we describe how to filter out non-domain definitions and create a dense hypernym graph for the domain of interest.
 the flow network example provided at the beginning of this section). Many of these from the Web or if they define ambiguous terms. For instance, in the C S
CIENCE domain, the cash flow definition of flow network shown in Table 3 was not pertinent. To discard these non-domain sentences, we weight each definition candidate formula: where B d ( t ) is the bag of content words in the definition candidate d ( t )and D is given by the union of the initial terminology T (0) and the set of single words of the terms in
T (0) that can be found as nouns in WordNet. For example, given T rithm , information retrieval , minimum spanning tree } , our domain terminology D = T { algorithm , information , retrieval , tree } . According to Equation (3), the domain weight of a definition is normalized by the total number of content words in the definition, so as to penalize longer definitions. Domain filtering is performed by keeping only those old. 9 In Table 3 (third column), we show some values calculated for the corresponding definitions (the fourth column reports a check mark if the domain weight is above the threshold, an  X  otherwise). Domain filtering performs some implicit form of Word
Sense Disambiguation (Navigli 2009), as it aims at discarding senses of hypernyms which do not pertain to the domain.
 which survived this filtering phase. For each t  X  T ( i ) ( V edge ( h , t ) 10 for each hypernym h  X  H t , that is, we set E 676 of this step, the graph contains our domain terms and their hypernyms obtained from domain-filtered definitions. We now set:
T the algorithm. Next, we move to iteration i + 1 and repeat the last two steps, namely, we perform definition/hypernym extraction and domain filtering on T of subsequent iterations, the initially empty graph is increasingly populated with new nodes (i.e., domain terms) and edges (i.e., hypernymy relations).
 that potentially contains more than one connected component. Finally, we connect all the upper term nodes in G noisy to a single top node . As a result of this connecting step, only one connected component of the noisy hypernym graph X  X hich we call the backbone component  X  X ill contain an upper taxonomy consisting of upper terms in U .
 the vast majority of nodes. In order to eliminate noise and obtain a full-fledged taxon-omy, we perform a step of graph pruning, as described in the next section. 3.4 Graph Pruning
At the end of the iterative hypernym harvesting phase, described in Sections 3.2 and 3.3, the result is a highly dense, potentially disconnected, hypernymy graph (see Section 4 for statistics concerning the experiments that we performed). Wrong nodes and edges might stem from errors in any of the definition/hypernym extraction and domain filter-ing steps. Furthermore, for each node, multiple  X  X ood X  hypernyms can be harvested.
Rather than using heuristic rules, we devised a novel graph pruning algorithm, based on the Chu-Liu/Edmonds optimal branching algorithm (Chu and Liu 1965; Edmonds 1967), that exploits the topological graph properties to produce a full-fledged taxonomy.
The algorithm consists of four phases (i.e., graph trimming, edge weighting, optimal branching, and pruning recovery) that we describe hereafter with the help of the noisy graph in Figure 3a, whose grey nodes belong to the initial terminology T bold node is the only upper term. 3.4.1 Graph Trimming. We first perform two trimming steps. First, we disconnect  X  X alse X  roots, i.e., nodes which are not in the set of upper terms and with no incoming edges (e.g., image in Figure 3a). Second, we disconnect  X  X alse X  leaves, namely, leaf nodes which are not in the initial terminology and with no outgoing edges (e.g., output in Figure 3a).
We show the disconnected components in Figure 3b. 3.4.2 Edge Weighting. Next, we weight the edges in our noisy graph G only on graph connectivity (e.g., in-degree or betweenness, see Newman [2010] for a complete survey) is not sufficient for taxonomy learning.
Figure 3: In choosing the best hypernym for the term token sequence , a connectivity-based measure might select collection rather than list , because the former reaches more nodes.
In taxonomy learning, however, longer hypernymy paths should be preferred (e.g., data structure  X  collection  X  list  X  token sequence is better than data structure token sequence ).
 between path length and the connectivity of traversed nodes. It consists of three steps: i) Weight each node v by the number of nodes belonging to the initial ii) For each node v , consider all the paths from an upper root r to v .
 iii) Assign the following weight to each incoming edge ( h , v )of v (i.e., h is one 678 taxonomy on the basis of our edge weighting strategy. A maximum spanning tree algorithm cannot be applied, however, because our graph is directed. Instead, we need to find an optimal branching, that is, a rooted tree with an orientation such that every node but the root has in-degree 1, and whose overall weight is maximum. To this end, noisy graph, we consider a number of cases, aimed at identifying a single  X  X easonable X  root node to enable the optimal branching to be calculated. Let R be the set of candidate roots, that is, nodes with no incoming edges. We perform the following steps: ii) Else if | R | &gt; 1, if an upper term is in R , we select it as root, else we choose iii) Else (i.e., if | R | = 0), we proceed as for step (ii), but we search candidates root node for each component, from which the optimal branching algorithm can start.
We then apply the Chu-Liu/Edmonds algorithm (Chu and Liu 1965; Edmonds 1967) to each component G i = ( V i , E i ) of our directed weighted graph G optimal branching. The algorithm consists of two phases: a contraction phase and an expansion phase. The contraction phase is as follows: 1. For each node which is not a root, we select the entering edge with the 2. If no cycles are formed in S , go to the expansion phase. Otherwise, 3. Given a cycle in S , contract the nodes in the cycle into a pseudo-node k , 4. Select the edge entering the cycle which has the highest modified weight 5. Go to step 2 with the contracted graph.

Otherwise, this phase is skipped and T i = ( V i , S ) is the optimal branching of component
G (i.e., the i -th component of G noisy ). During the expansion phase, pseudo-nodes are replaced with the original cycles. To break the cycle, we select the real node v into which cycle. Finally, the weights on the edges are restored. For example, consider the cycle in Figure 4a. Nodes pagerank , map ,and rank are contracted into a pseudo-node, and the edges entering the cycle from outside are re-weighted according to Equation (7).
According to the modified weights (Figure 4b), the selected edge, that is, ( table , map ), is the one with weight w = 13. During the expansion phase, the edge ( pagerank , map )is eliminated, thus breaking the cycle (Figure 4c).
 algorithm to our example in Figure 3b is shown in Figure 3c. 3.4.4 Pruning Recovery. The weighted directed graph G
Edmonds algorithm might contain many (weakly) connected components. In this case, an optimal branching is found for each component, resulting in a forest of taxonomy trees. Although some of these components are actually noisy, others provide an impor-tant contribution to the final tree-like taxonomy. The objective of this phase is to recover from excessive pruning, and re-attach some of the components that were disconnected have only one backbone component, that is, a component which includes an upper tax-onomy. Our aim is thus to re-attach meaningful components to the backbone taxonomy.
To this end, we apply Algorithm 1. The algorithm iteratively merges non-backbone trees to the backbone taxonomy tree T 0 in three main steps: 680 Algorithm 1 PruningRecovery ( G , G noisy )
Require: G is a forest 1: repeat 2: Let F : = { T 0 , T 1 , ... , T | F | } be the forest of trees in G = ( V , E ) 3: Let T 0  X  F be the backbone taxonomy 5: for all T in F \{ T 0 } do 6: r T  X  rootOf ( T ) 7: if  X  v  X  T 0 s.t. ( v , r T )  X  G noisy then 8: E  X  E  X  X  ( v , r T ) } 9: break 10: else 11: if out -degree ( r T ) = 0 then 12: if  X  v  X  T 0 s.t. v is the longest right substring of r 13: E : = E  X  X  ( v , r T ) } 14: break 15: else 16: E  X  E \{ ( r T , v ): v  X  V } 17: break 18: until E = E is made to the graph (line 18). As a result of our pruning recovery phase we return the enriched backbone taxonomy. We show in Figure 5 an example of pruning recovery that starts from a forest of three components (including the backbone taxonomy tree on top,
Figure 5a). The application of the algorithm leads to the disconnection of a tree root, that is, ordered structure (Figure 5a, lines 15 X 17 of Algorithm 1), the linking of the trees rooted at token list and binary search tree to nodes in the backbone taxonomy (Figures 5b and 5d, lines 7 X 9), and the linking of balanced binary tree to binary tree thanks to lexical inclusion (Figure 5c, lines 11 X 14 of the algorithm). 3.5 Edge Recovery
The goal of the last phase was to recover from the excessive pruning of the optimal branching phase. Another issue of optimal branching is that we obtain a tree-like tax-onomic structure, namely, one in which each node has only one hypernym. This is not fully appropriate in taxonomy learning, because systematic ambiguity and polysemy often require a concept to be paradigmatically related to more than one hypernym. In fact, a more appropriate structure for a conceptual hierarchy is a DAG, as in WordNet.
For example, two equally valid hypernyms for backpropagation are gradient descent search 682 procedure and training algorithm , so two hypernym edges should correctly be incident to the backpropagation node.
 the following step: for each  X  X oisy X  edge ( v , v )  X  E noisy but the edge ( v , v ) does not belong to the tree, we add ( v , v )to T ii) the absolute difference between the length of the shortest path from v to rithm, namely: one version that does not perform edge recovery (i.e., which learns a tree-like taxonomy [TREE], and two versions that apply edge recovery (i.e., which learn a DAG) with different intervals for constraint (ii) above ( DAG [1, 3] and DAG [0, 99]; note that the latter version virtually removes constraint (ii)). Examples of recovered edges will be presented and discussed in the evaluation section. 3.6 Complexity
We now perform a complexity analysis of the main steps of OntoLearn Reloaded. Given the large number of steps and variables involved we provide a separate discussion of the main costs for each individual step, and we omit details about commonly used data structures for access and storage, unless otherwise specified. Let G be our noisy graph, and let n = | V noisy | and m = | E noisy 1. Terminology extraction: Assuming a part-of-speech tagged corpus as 2. Definition and hypernym extraction: In the second step, we first retrieve 3. Domain filtering and creation of the graph: The cost of domain filtering 4. Graph pruning , consisting of the following steps: 5. Pruning recovery: In the worst case, m iterations of Algorithm 1 will be 6. Edge recovery: For each pair of nodes in T 0 we perform i) the polynomial in the main variables of the problem, namely, the number of words in the corpus and nodes in the noisy graph. 4. Evaluation
Ontology evaluation is a hard task that is difficult even for humans, mainly because taxonomies might model a particular domain of interest equally well. Despite this difficulty, various evaluation methods have been proposed in the literature for assessing 684 the quality of a taxonomy. These include Brank, Mladenic, and Grobelnik (2006) and
Maedche, Pekar, and Staab (2002): a) automatic evaluation against a gold standard; b) manual evaluation performed by domain experts; c) structural evaluation of the taxonomy; d) application-driven evaluation, in which a taxonomy is assessed on the completeness, consistency (V  X  olker et al. 2008), and more theoretical features (Guarino and Welty 2002) like essentiality, rigidity, and unity. Methods (a) and (b) are by far the most popular ones. In this section, we will discuss in some detail the pros and cons of these two approaches.

Gold standard evaluation. The most popular approach for the evaluation of lexicalized taxonomies (adopted, e.g., in Snow, Jurafsky, and Ng 2006; Yang and Callan 2009; and Kozareva and Hovy 2010) is to attempt to reconstruct an existing gold standard (Maedche, Pekar, and Staab 2002), such as WordNet or the Open Directory Project.
This method is applicable when the set of taxonomy concepts are given, and the between concept pairs. The evaluation is far more complex when learning a specialized taxonomy entirely from scratch, that is, when both terms and relations are unknown.
In reference taxonomies, even in the same domain, the granularity and cotopy abstract concept might vary according to the scope of the taxonomy and the expertise terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in and shading techniques whereas collage is classified under image-making processes and techniques . As long as common-sense, non-specialist knowledge is considered, it is still Web will provide abundant evidence for it. For example, Kozareva and Hovy (2010, K&amp;H hereafter) are very successful at reproducing the WordNet sub-taxonomy for
ANIMALS , because dozens of definitional patterns are found on the Web that classify, for example, lion as a carnivorous feline mammal ,or carnivorous ,or feline .Asweshow later in this section, however, and as also suggested by the previous AA&amp;T example, finding hypernymy patterns in more specialized domains is far more complex. Even in simpler domains, however, it is not clear how to evaluate the concepts and relations not found in the reference taxonomy. Concerning this issue, Zornitsa Kozareva comments that:  X  X hen we gave sets of terms to annotators and asked them to produce a taxonomy, people struggled with the domain terminology and produced quite messy organization.
Therefore, we decided to go with WordNet and use it as a gold truth X  (personal communication). Accordingly, K&amp;H do not provide an evaluation of the nodes and relations other than those for which the ground truth is known. This is further clarified in a personal communication:  X  X urrently we do not have a full list of all is-a outside
WordNet. [...] In the experiments, we work only with the terms present in WordNet [...] The evaluation is based only on the WordNet relations. However, the harvesting algorithm extracts much more. Currently, we do not know how to evaluate the Web taxonomization. X  ation metric. The most common measure used in the literature to compare a learned with a gold-standard taxonomy is the overlapping factor (Maedche, Pekar, and Staab simply computes the ratio between the intersection and union of these sets. Therefore two taxonomies. It provides no structural comparison, however: Errors or differences in grouping concepts in progressively more general classes are not evidenced by this measure.
 by Zavitsanos, Paliouras, and Vouros (2011) and Brank, Mladenic, and Grobelnik (2006).
They propose two different strategies for escaping the  X  X aming X  problem that we have outlined. Zavitsanos, Paliouras, and Vouros (2011) propose transforming the ontology concepts and their properties into distributions over the term space of the source data from which the ontology has been learned. These distributions are used to compute pairwise concept similarity between gold standard and learned ontologies. ing and unsupervised clustering, and propose OntoRand, a modified version of the Rand Index (Rand 1971) for computing the similarity between ontologies. Morey and
Agresti (1984) and Carpineto and Romano (2012), however, demonstrated a high de-pendency of the Rand Index (and consequently of OntoRand itself) upon the number of clusters, and Fowlkes and Mallows (1983) show that the Rand Index has the undesirable property of converging to 1 as the number of clusters increases, even in the unrealistic case of independent clusterings. These undesired outcomes have also been experienced by Brank, Mladenic, and Grobelnik (2006, page 5), who note that  X  X he similarity of an ontology to the original one is still as high as 0.74 even if only the top three levels of the ontology have been kept. X  Another problem with the OntoRand formula, as also remarked in Zavitsanos, Paliouras, and Vouros (2011), is the requirement of comparing ontologies with the same set of instances.

Manual evaluation. Comparison against a gold standard is important because it repre-sents a sort of objective evaluation of an automated taxonomy learning method. As we have already remarked, however, learning an existing taxonomy is not particularly interesting in itself. Taxonomies are mostly needed in novel, often highly technical do-mains for which there are no gold standards. For a system to claim to be able to acquire a taxonomy from the ground up, manual evaluation seems indispensable. Nevertheless, none of the taxonomy learning systems surveyed in Section 2 performs such evaluation.
Furthermore, manual evaluation should not be limited to an assessment of the acquired 686 hypernymy relations  X  X n isolation, X  but must also provide a structural assessment aimed at identifying common phenomena and the overall quality of the taxonomic
Deciding whether or not a concept belongs to a given domain is more or less feasible for a domain expert, but assessing the quality of a hypernymy link is far more complex.
On the other hand, asking a team of experts to blindly reconstruct a hierarchy, given a set of terms, may result in the  X  X essy organization X  reported by Zornitsa Kozareva. In contrast to previous approaches to taxonomy induction, OntoLearn Reloaded provides a natural solution to this problem, because is-a links in the taxonomy are supported by one or more definition sentences from which the hypernymy relation was extracted. As shown later in this section, definitions proved to be a very helpful feature in supporting manual analysis, both for hypernym evaluation and structural assessment.
 taxonomies, four of which attempt to replicate already existing gold standard sub-hierarchies in WordNet 17 and in the MeSH medical ontology, new taxonomies acquired from scratch. Next, we present a large-scale multi-faceted evaluation of OntoLearn Reloaded focused on three of the previously described eval-uation methods, namely: comparison against a gold standard, manual evaluation, and structural evaluation. In Section 4.2 we introduce a novel measure for comparing an induced taxonomy against a gold standard one. Finally, Section 4.3 is dedicated to a manual evaluation of the six taxonomies. 4.1 Experimental Set-up
We now provide details on the set-up of our experiments. 4.1.1 Domains. We applied OntoLearn Reloaded to the task of acquiring six taxonomies: A
The first four taxonomies were used for comparison against three WordNet sub-hierarchies and the viruses sub-hierarchy of MeSH. The A P
LANTS domains were selected to allow for comparison with K&amp;H, who experimented on the same domains. The A RTIFICIAL I NTELLIGENCE and F INANCE amples of taxonomies truly built from the ground up, for which we provide a thorough manual evaluation. These domains were selected because they are large, interdisci-plinary, and continuously evolving fields, thus representing complex and specialized use cases. 4.1.2 Definition Harvesting. For each domain, definitions were sought in Wikipedia and in Web glossaries automatically obtained by means of a Web glossary extraction system (Velardi, Navigli, and D X  X madio 2008). For the A RTIFICIAL also used a collection consisting of the entire IJCAI proceedings from 1969 to 2011 and the ACL archive from 1979 to 2010. In what follows we refer to this collection as the  X  X I corpus. X  For F INANCE we used a combined corpus from the freely available collection of Journal of Financial Economics from 1995 to 2012 and from Review Of Finance from 1997 to 2012 for a total of 1,575 papers. 4.1.3 Terminology. For the A NIMALS ,V EHICLES ,P LANTS initial terminology was a fragment of the nodes of the reference taxonomies, the initial terminology was selected using our TermExtractor tool
TermExtractor extracted over 5,000 terms from the AI corpus, ranked according to a combination of relevance indicators related to the (direct) document frequency, domain pertinence, lexical cohesion, and other indicators (Sclano and Velardi 2007). We manu-ally selected 2,218 terms from the initial set, with the aim of eliminating compounds domain relevant. For similar reasons a manual selection of terms was also applied to the terminology automatically extracted for the F INANCE domain, obtaining 2,348 terms from those extracted by TermExtractor. An excerpt of extracted terms was provided in
Table 1. upon: A NIMALS ,V EHICLES ,P LANTS ,andV IRUSES . For the AI and F which are more general and complex, we selected from WordNet a core taxonomy of 32 upper concepts U (resulting in 52 terms) that we used as a stopping criterion for our iterative definition/hypernym extraction and filtering procedure (cf. Section 3.2).
The complete list of upper concepts was given in Table 2. WordNet upper concepts are general enough to fit most domains, and in fact we used the same set U for AI and F
INANCE . Nothing, however, would have prevented us from using a domain-specific core ontology, such as the CRM-CIDOC core ontology for the domain of A A 4.1.5 Algorithm Versions and Structural Statistics. For each of the six domains we ran the three versions of our algorithm: without pruning recovery (TREE), with [1, 3] recovery remind the reader that the purpose of the recovery process was to reattach some of the edges deleted during the optimal branching step (cf. Section 3.5).

Notice that, even though the taxonomy looks good overall, there are still a few errors, such as  X  neuron is a neural network  X  and overspecializations like  X  network is a digraph . X 
Figure 7 shows a sub-hierarchy of the F INANCE tree-like taxonomy under the concept value .
 domains. In the table, edge and node compression refers to the number of surviving nodes and edges after the application of optimal branching and recovery steps to the noisy hypernymy graph. To clarify the table, consider the case of V we started with 281 initial terms, obtaining a noisy graph with 1,174 nodes and 1,859 edges. These were reduced to 297 nodes (i.e., 1,174 X 877) and 339 edges (i.e., 1,859 X 1,520) after pruning and recovery. Out of the 297 surviving nodes, 222 belonged to the initial 688 terminology; therefore the coverage over the initial terms is 0.79 (222/281). This means that, for some of the initial terms, either no definitions were found, or the definition was rejected in some of the processing steps. The table also shows, as expected, that the term coverage is much higher for  X  X ommon-sense X  domains like and PLANTS , is still over 0.75 for V IRUSES and AI, and is a bit lower for F (0.65). The maximum and average depth of the taxonomies appears to be quite variable, run. We would like to point out that providing textual glosses for the retrieved domain hypernyms is a novel feature that has been lacking in all previous approaches to ontology learning, and which can also provide key support to much-needed manual validation and enrichment of existing semantic networks (Navigli and Ponzetto 2012). 4.2 Evaluation Against a Gold Standard taxonomy against a gold standard. We borrow the Brank, Mladenic, and Grobelnik representing the two taxonomies as flat clusterings, we propose a measure that takes into account the hierarchical structure of the two analyzed taxonomies. Under this perspective, a taxonomy can be transformed into a hierarchical clustering by replacing each label of a non-leaf node (e.g., perspective and shading techniques ) with the transitive closure of its hyponyms (e.g., cangiatismo, chiaroscuro, foreshortening, hatching ). 4.2.1 Evaluation Model. Techniques for comparing clustering results have been surveyed in Wagner and Wagner (2007), although the only method for comparing hierarchical clusters, to the best of our knowledge, is that proposed by Fowlkes and Mallows (1983).
Suppose that we have two hierarchical clusterings H 1 and H objects. Let k be the maximum depth of both H 1 and H 2 ,and H for each cut i , the two hierarchies can be seen as two flat clusterings C concepts. When i = 0 the cut is a single cluster incorporating all the objects, and when i = k we obtain n singleton clusters. Now let: 690 cut i ( i  X  X  0, ... , k } ), as reformulated by Wagner and Wagner (2007), is defined as: recall of an automated method in clustering the same concept pairs as in a gold-standard clustering. This formula has a few undesirable properties: first, the value of B close to its maximum 1 . 0 as we approach the root of the hierarchy ( i = 0); second, the two hierarchies need to have the same maximum depth k ; third, the hierarchies need to have the same number of initial objects and a crisp classification.
 standard taxonomy, we need to mitigate these problems. Equation (8) copes with the third problem without modifications. In fact, if the sets of objects in H different, the integers n 10 and n 01 can be considered as also including objects that belong to one hierarchy and not to the other. In this case, the value of B of the overlapping objects in the learned taxonomy and the gold standard one. In order to take into account multiple (rather than crisp) classifications, again, there is no need more than one cluster. As before, mismatches between H 1 and H values of n 10 and n 01 and lower B i 1,2 .
 the value of the formula, whereas, ideally, we would like to reward similar clusterings when the clustering task is more difficult and fine-grained, that is, for cuts that are close to the leaf nodes. To assign a reward to  X  X arly X  similarity values, we weight the values with the following formula: policy that penalizes a learned taxonomy that is less structured than the gold standard one, and rewards X  X r at least does not penalize X  X he opposite case.
 non-identical sets of objects { a , b , c , d , e , f } and labeled by its distance from the root node (the value i in the F&amp;M formula). Notice that
H and H 2 have multiple classifications (i.e., multiple hypernyms in our case) for the object e , thus modeling the common problem of lexical ambiguity and polysemy. Let us suppose that H 1 is the learned taxonomy, and H 2 the gold standard one. We start comparing the clusterings at cut 0 and stop at cut k r  X  1, where k 692 we replicate the cut k l  X  1for k r  X  k l times (where k learned taxonomy), whereas if it is more structured we stop at cut k previous evaluation models, our aim is to reward (instead of penalize) more structured taxonomies provided they still match the gold standard one.

B value is 0, if H 2 is the learned taxonomy, and is not defined, if H
Therefore, when computing the cumulative Equation (9), we obtain the desired effect of penalizing less the structured learned taxonomies. Note that, when the two hierarchies have different depths, the value k  X  1 in Equation (9) is replaced by k introduced by Brank, Mladenic, and Grobelnik (2006). The Rand Index measures the similarity between two clusterings C l and C r by the formula: where n 11 , n 00 ,and n have the same meaning described earlier. In Brank, Mladenic, ontology instance to its concept. The set of clusters is hence represented by the set of leaf concepts in the hierarchy, namely, according to our notation, the clustering C order to take into account the hierarchical structure, they define the OntoRand formula. given instances i and j belong to the same cluster in the compared ontologies, returns a real number in [0, 1] depending upon the distance between i and j in terms of common ancestors. In other terms, if i and j do not belong to the same concept but have a very close common ancestor, the OntoRand measure returns a value still close to 1. ii) It does not require that the two hierarchies have the same depth, nor that iii) The measure can be extended to lattices (e.g., it is not required that each iv) It is not dependent, as the Rand Index is, on the number n 4.2.2 Results. This section presents the results of the F&amp;M evaluation model for gold standard evaluation, therefore we focus on four domains and do not consider AI and F
INANCE . The three WordNet sub-hierarchies are also compared with the taxonomies automatically created by Kozareva and Hovy (2010) in the same domains, kindly made available by the authors. It is once more to be noted that Kozareva and Hovy, during hy-pernym extraction, reject all the nodes not belonging to WordNet, whereas we assume no a-priori knowledge of the domain, apart from adopting the same set of seed terms used by K&amp;H.

As far as the comparison with K&amp;H is concerned, we notice that, though K&amp;H obtain better performance in general, OntoLearn has higher coverage over the domain, as is shown by the highest values for i = 0, and has a higher depth of the derived hierarchy, especially with DAG [0, 99]. Another recurrent phenomenon is that K&amp;H curves grace-fully degrade from the root to the leaf nodes, possibly with a peak in the intermediate levels, whereas OntoLearn has a hollow in the mid-high region (see the region 4 X 6 for A NIMALS and 1 X 2 for the other three hierarchies) and often a relative peak in the lowest 694 levels. In the manual evaluation section we explain this phenomenon, which also occurs icking the clustering criteria of a taxonomy created by a team of experts proves very lowest taxonomy levels there are two opposing phenomena: overgeneralization and overspecialization. For example, macaque has monkey as a direct hypernym in WordNet, and we find short-tailed monkey as a direct hypernym of macaque . An opposite case is case does not reward the learned taxonomy (though, unlike for the overlapping factor [Maedche, Pekar, and Staab 2002], it does not cause a penalty), whereas the second is quite penalizing. More of these examples will be provided in Section 4.3. cording to Equation (9). Here, except for the V EHICLES
DAG [0, 99] performs best. 4.3 Manual Evaluation
This section is dedicated to the manual assessment of the learned ontologies. The section is divided in three parts: Section 4.3.1 is concerned with the human validation of hypernymy relations, Section 4.3.2 examines the global learned taxonomic structure in the search for common phenomena across the six domains, and finally Section 4.3.3 in-vestigates the possibility of enhancing our hypernymy harvesting method with K&amp;H X  X 
Hearst-like patterns, applying their method to the AI domain and manually evaluating the extracted hypernyms. 4.3.1 Hypernym Evaluation. To reduce subjectivity in taxonomy evaluation, we asked three annotators, only one of whom was a co-author, to validate, for each of the three experiments of each of the six domains, a random sample of hypernymy relations. For especially helpful for domains like V IRUSES , but also P the annotators were not expert. The size of each random sample was 300 for the (larger) AI and F INANCE domains and 100 for the others.
 was due to the selection of non-domain definitions (e.g., for V golf club with a near vertical face that is used for hitting long shots from the tee X ), to a poor definition (e.g., for AI:  X  X  principle is a fundamental essence, particularly one producing a given quality X ) or to a wrong selection of the hypernym. As an example of the latter, in the P LANTS domain, we extracted the hypernym species from the sentence:  X  geranium is a genus of 422 species of flowering annual, biennial, and perennial plants  X  Interpretation that are commonly known as the cranesbills X  since, in the WCL verb set, we have  X  X s Annotators could mark with ? a hyponym X  X ypernym pair for which they felt uncertain.
Though it would have been useful to distinguish between the different types of error, we found that regarding many error types there was, anyway, very low inter-annotator agreement. Indeed the annotation task would appear to be intrinsically complex and tasks in isolation was already provided by Navigli and Velardi (2010).
 majority basis, and we used Fleiss X  kappa statistics (Fleiss 1971) to measure the inter-annotator agreement. In general, the precision is rather good, though it is lower for the
AI domain, probably due to its high  X  X itality X  (many new terms continuously arise, and for some of them it is difficult to find good quality definitions). In general, precision is higher in focused domains (V IRUSES ,A NIMALS ,P LANTS ,andV range domains (AI and F INANCE ). The former domains, however, have just one quite 696  X  X arrow X  upper concept ( virus for VIRUSES , etc.), whereas AI and F upper concepts (e.g., person or abstraction ), and furthermore they are less focused. This means that there is an inherently higher ambiguity and this may be seen as justifying the lower performance. In Table 9 we also note that TREE structures achieve in general a higher precision, except for PLANTS , whereas the DAG has the advantage of improving recall (see also Section 4.2.2).

In this case, each single relation is evaluated in isolation, therefore overgenerality or overspecificity do not imply a penalty, provided the relation is judged to be correct. learning  X  parametric technique , and eventually ends up in technique , whereas belief network learning  X  machine learning algorithm ends up in algorithm and then in procedure .
In isolation, these hypernymy patterns are acceptable, but within a taxonomic structure one would like to see a category node grouping all terms denoting machine learning algorithms. This behavior should be favored by the node weighting strategy described in Section 3.4, aimed at attracting nodes with multiple hypernyms towards the most populated category nodes. As in the previous example, however, there are category nodes that are almost equally  X  X ttractive X  (e.g., algorithm and technique ), and, further-more, the taxonomy induction algorithm can only select among the set of hypernyms extracted during the harvesting phase. Consequently, when no definition suggests that concept connected to machine learning algorithm , there is no way of grouping distance metric learning and belief network learning in the desired way. This task must be postponed to manual post-editing.
 substantial in most cases. These numbers are apparently low, but the task of evaluating hypernymy relations is quite a complex one. Similar kappa values were obtained in
Yang and Callan (2008) in a human-guided ontology learning task. 4.3.2 Structural Assessment. In addition to the manual evaluation summarized in Table 9, a structural assessment was performed to identify the main sources of error. To this end, one of the authors analyzed the full AI and F sample of the other four domains in search of recurring errors. In general, our optimal branching algorithm and weighting schema avoids many of the problems highlighted in well-known studies on taxonomy acquisition from dictionaries (Ide and V  X  eronis 1993), like circularity, over-generalization, and so forth. There are new problems to be faced, however.

Ambiguity . Concerning ambiguity of terms, consider Figures 10 and 11, which show the distribution of errors at the different levels of the learned AI and F for the TREE experiment. The figures provide strong evidence that most errors are the upper ontology, the extracted terms become progressively more general and con-sequently more ambiguous. For these terms the domain heuristics may turn out to be inadequate, especially if the definition is a short sentence.
 levels? To understand this, consider the following example from the AI domain: For person who creates classifications. X  In many cases, wrong hypernyms do not accumulate sufficient weight and create  X  X ead-end X  hypernymy chains, which are pruned during the optimal branching step. But, unfortunately, a domain appropriate definition is 698 presence of the domain word category . On the other hand, this new sentence produces an attachment that, in a sense, recovers the error, because category is a  X  X ood X  domain concept that eventually ends up in subsequent iterations to the upper node abstraction .
Therefore, what happens is that the upper taxonomy nodes, with the help of the domain heuristic, mitigate the  X  X emantic drift X  caused by out-of-domain ambiguity, recovering the ambiguity errors of the intermediate levels. This phenomenon is consistently found in all domains, as shown by the hollow that we noticed in the graphs of Section 4.2.2.
An example in the A NIMALS domain is represented by the hypernymy sequence fawn  X  color  X  race  X  breed  X  domestic animal , where the wrong hypernym color was originated by the sentence  X  fawn is a light yellowish brown color that is usually used in reference to a dog X  X  coat color. X  Only in V IRUSES is the phenomenon mitigated by the highly specific and very focused nature of the domain.
 2010; Faralli and Navigli 2012). Consider the example of Figure 12a, from the V domain: tractor has two definitions corresponding to two meanings, which are both correct. The airplane meaning is  X  tractor is an airplane where the propeller is located in front of the fuselage, X  whereas the truck meaning is  X  tractor is a truck for pulling a semi-trailer or trailer. X  Here the three hyponyms of tractor (see the figure) all belong to the truck sense. We leave to future developments the task of splitting in-domain ambiguous nodes in the appropriate way.
 its hyponyms, are both a methodology and a representation . Another example is shown in Figure 12b for the P LANTS domain, where systematic polysemy can be observed because hyponyms of polysemous concepts inherit the polysemy: In the two graphs of Figures 13 and 12b, both partitioned semantic network and tangerine preserve the polysemy of their ancestors. Note that in-domain ambiguity and polysemy are only captured by the DAG structure; therefore this can be seen as a further advantage (in addition to higher recall) of the DAG model over and against the more precise TREE structure.
 Low quality of definitions . Often textual definitions, especially if extracted from the
Web, do not have a high quality. Examples are:  X  artificial intelligence is the next big development in computing X  or  X  aspectual classification is also a necessary prerequi-this problem is much less pervasive than for Hearst-like lexico-syntactic patterns, although, neither domain heuristics nor the graph pruning could completely eliminate the problem. We can also include overgeneralization in this category of problems: Our algorithm prefers specific hypernyms to general hypernyms, but for certain terms no specific definitions are found. The elective way to solve this problem would be to assign a quality confidence score to the definition source (document or Web page), for example, by performing an accurate and stable classification of its genre (Petrenz and Webber 2011).

Hypernym is a clause . There are cases in which, although very descriptive and good term or multi-word expression. For example  X  anaphora resolution is the process of determining whether two expressions in natural language refer to the same real world
This is not completely wrong, however, and in some case is even fully acceptable, as for  X  summarizing is a process of condensing or expressing in short something you have read, watched or heard X : here, process of condensing is an acceptable hypernym.
An example for F INANCE is:  X  market-to-book ratio is book value of assets minus book value of equity plus market value of equity, X  where we extracted book value , rather than the complete formula. Another example is:  X  roa is defined as a ratio of operating income to book value of assets, X  from which we extracted ratio , which is, instead, acceptable.
Lack of an appropriate definitional pattern . Though we acquired hundreds of different definitional patterns, there are still definitions that are not correctly parsed. We already 700 mentioned the geranium example in the P LANTS domain. An example in the AI domain is  X  execution monitoring is the robot X  X  process of observing the world for discrepancies hypernym is robot because we have no WCL with a Saxon genitive.
 substring of the correct one, like:  X  latent semantic analysis is a machine learning proce-dure. X  Here, the correct hypernym is machine learning procedure , but OntoLearn extracts machine because learning is POS tagged as a verb. In general, it is not possible to evaluate the extent of the hypernym phrase except case-by-case. The lattice learner acquired a variety of hypernymy patterns, but the precision of certain patterns might be quite low.
For example, the hypernymy pattern  X * of * X  is acceptable for  X  X n grammar, a lexical category is a linguistic category of words  X  X r X  page rank is a measure of site popularity  X  but not for  X  page rank is only a factor of the amount of incoming and outgoing links importance . X  The same applies to the hypernymy pattern ADJ NN : important algorithm is wrong, although greedy algorithm is correct. 4.3.3 Evaluation of Lexico-Syntactic Patterns. As previously remarked, Kozareva and Hovy (2010) do not actually apply their algorithm to the task of creating a new taxonomy, but rather they try to reproduce three WordNet taxonomies, under the assumption that the taxonomy nodes are known (cf. Section 4). Therefore, there is no evidence of the preci-sion of their method on new domains, where the category nodes are unknown. On the other hand, if Hearst X  X  patterns, which are at the basis of K&amp;H X  X  hypernymy harvesting algorithm, could show adequate precision, we would use them in combination with our definitional patterns. This section investigates the matter.
 lion ), they: 1) harvest new basic and intermediate concepts from the Web in an iterative 2) rank the nodes extracted with DAP by out-degree and those extracted 3) induce the final taxonomic structure by positioning the intermediate nodes analyze the quality of the extracted relations. To replicate the first two steps of K&amp;H algorithm we fed the algorithm with a growing set of seed terms randomly selected in the upper taxonomy (e.g., unsupervised learning is a method or maximum entropy is a measure ). We then performed the DAP and DAP  X  1 steps iteratively until no more terms could be retrieved, and we manually evaluated the quality of the harvested concepts and taxonomic relations using the same thresholding formula described in K&amp;H. give the results in Table 10.
 patterns in more technical domains, and the efficacy of DAP and DAP retrieving domain concepts and relations. Therefore, replicating step (3) above is not optimal branching and pruning recovery steps, at reorganizing and trimming the final graph.
 in the absence of a priori knowledge on the domain concepts the quantity and quality of the is-a links extracted by the K&amp;H algorithm is much lower than those extracted by
OntoLearn Reloaded. First, the number of new nodes found by the K&amp;H algorithm is quite low: For the same domain of A RTIFICIAL I NTELLIGENCE
Table 9, is able to extract from scratch 2,387  X  52 = 2,335 nodes, 247 new nodes of Table 10, obtained with 1,000 seeds. Second, many nodes extracted by domain and many hypernym relations are incorrect irrespective of their direction, like computer program is a slow and data mining is a contemporary computing problem . Third, the vast majority of the retrieved hypernyms are overgeneral, like discipline, method, area, problem, technique, topic , and so forth, resulting in an almost flat hypernymy structure. A high in-degree threshold and a very high number of seeds do not mitigate the problem, demonstrating that Hearst-like patterns are not very good at harvesting many valid hypernym relations in specialized domains. 26
K&amp;H X  X  work (and, as a consequence, over Hearst X  X  patterns): i) We obtain higher precision and recall when no a priori knowledge is 702 ii) We cope better with sense ambiguity via the domain filtering step. iii) We use a principled algorithmic approach to graph pruning and cycle iv) Thanks to the support provided by textual definitions, we are able to 4.3.4 Summary of Findings. We here summarize the main findings of our manifold evaluation experiments: ii) Errors are mostly concentrated in the mid-level of the hierarchy, where iii) The quality and number of definitions is critical for high performance. iv) Definitions, on the other hand, are a much more precise and high-coverage 5. Conclusions
In this paper we presented OntoLearn Reloaded, a graph-based algorithm for learning a taxonomy from scratch using highly dense, potentially disconnected, hypernymy remarkably well on arbitrary, possibly specialized, domains, using a weighting scheme that draws both on the topological properties of the graph and on some general prin-ciples of taxonomic structures. OntoLearn Reloaded provides a considerable advance-ment over the state of the art in taxonomy learning. First, it is the first algorithm that experimentally demonstrates its ability to build a new taxonomy from the ground up, without any a priori assumption on the domain except for a corpus and a set of (possibly and induce hypernymy links between these concepts. Instead, we automatically learn both concepts and relations via term extraction and iterative definition and hypernym extraction. Second, we cope with issues such as term ambiguity, complexity, and multiplicity of hypernymy patterns. Third, we contribute a multi-faceted evaluation, which includes a comparison against gold standards, plus a structural and a manual evaluation. Taxonomy induction was applied to the task of creating new A I
NTELLIGENCE and F INANCE taxonomies and four taxonomies for gold-standard comparison against WordNet and MeSH. 29 task of acquiring a taxonomy from scratch: Using a taxonomy validation tool, matter of hours, rather than man-months, also thanks to the automatic acquisition of textual definitions for our concepts. As with any automated and unsupervised learning tool, however, OntoLearn does make errors, as we discussed in Section 4. The accuracy of the resulting taxonomy is clearly related to the number and quality of discovered definitional patterns, which is in turn related to the maturity and generality of a domain.
Even with good definitions, problems might arise due to in-and out-domain ambiguity, the latter being probably the major source of errors, together with complex definitional structures. Although we believe that there is still room for improvement to OntoLearn
Reloaded, certain errors would appear unavoidable, especially for less focused and relatively dynamic domains like A RTIFICIAL I NTELLIGENCE new terms arise continuously and have very few, or no definitions on the Web.
ReVerb (Etzioni et al. 2011) and WiSeNet (Moro and Navigli 2012), and a more sophis-the issue of automatically discriminating between in-domain ambiguity and systematic polysemy (as discussed in Section 4.3.2).
 Acknowledgments References 704 706
