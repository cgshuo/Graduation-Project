 Statistical parsing models have recently been pro-posed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing deci-sions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in En-glish, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which im-plicitly assumes an unbounded stack, wasting prob-ability mass on trees whose complexity is beyond human recognition or generation capacity?
This paper presents parsing accuracy results us-ing transformed and untransformed versions of a corpus-trained probabilistic context-free grammar suggesting that this is indeed the case. Experimental results show a bounded-memory time-series parser using a transformed version of a grammar signifi-cantly outperforms an unbounded-stack CKY parser using the original grammar.

Unlike the tree-based transforms described previ-ously, the model-based transform described in this paper does not introduce additional context from corpus data beyond that contained in the origi-nal probabilistic grammar, making it possible to present a fair comparison between bounded-and unbounded-stack versions of the same model. Since this transform takes a probabilistic grammar as in-put, it can also easily accommodate horizontal and vertical Markovisation (annotating grammar sym-bols with parent and sibling categories) as described by Collins (1997) and subsequently.

The remainder of this paper is organized as fol-lows: Section 2 describes related approaches to pars-ing with stack bounds; Section 3 describes an exist-ing bounded-stack parsing framework using a right-corner transform defined over individual trees; Sec-tion 4 describes a redefinition of this transform to ap-ply to entire probabilistic grammars, cast as infinite sets of generable trees; and Section 5 describes an evaluation of this transform on the Wall Street Jour-nal corpus of the Penn Treebank showing improved results for a transformed bounded-stack version of a probabilistic grammar over the original unbounded grammar. The model examined here is formally similar to Combinatorial Categorial Grammar (CCG) (Steed-man, 2000). But the CCG account is a competence model as well as a performance model, in that it seeks to unify category representations used in pro-cessing with learned generalizations about argument structure; whereas the model described in this paper is exclusively a performance model, allowing gen-eralizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strate-gies to yield a set of derived incomplete constituents. As a result, the model described in this paper has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative com-position operations proposed in the CCG account, thought to be associated with available prosody and
Other models (Abney and Johnson, 1991; Gibson, 1991) seek to explain human processing difficulties as a result of memory capacity limits in parsing or-dinary phrase structure trees. The Abney-Johnson and Gibson models adopt a left-corner parsing strat-egy, of which the right-corner transform described in this paper is a variant, in order to minimize memory usage. But the transform-based model described in this paper exploits a conception of chunking (Miller, 1956)  X  in this case, grouping recognized words into stacked-up incomplete constituents  X  to oper-ate within much stricter estimates of human short-term memory bounds (Cowan, 2001) than assumed by Abney and Johnson.
Several existing incremental systems are orga-nized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems gen-erally keep large numbers of constituents open for modifier attachment in each hypothesis. This al-lows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and se-mantic) structure will be violated, or the assump-tion that syntax operates within a bounded mem-ory store will be violated, both of which are psy-cholinguistically attractive as simplifying assump-tions. The HHMM model examined in this pa-per upholds both the fixed-element and bounded-memory assumptions by hypothesizing fixed reduc-tions of right child constituents into incomplete par-ents in the same memory element, to make room for new constituents that may be introduced at a later time. These in-element reductions are defined natu-rally on phrase structure trees as the result of align-ing right-corner transformed constituent structures to sequences of random variables in a factored time-series model. The recognition model examined in this paper is a factored time-series model, based on a Hierarchic Hidden Markov Model (Murphy and Paskin, 2001), which probabilistically estimates the contents of a memory store of three to four partially-completed constituents over time. Probabilities for expansions, transitions and reductions in this model can be de-fined over trees in a training corpus, transformed and mapped to the random variables in an HHMM (Schuler et al., 2008). In Section 4 these probabil-ities will be computed directly from a probabilistic context-free grammar, in order to evaluate the con-tribution of stack bounds without introducing addi-tional corpus context into the model. 3.1 A Bounded-Stack Model HHMMs are factored HMMs which mimic a bounded-memory pushdown automaton (PDA), sup-porting simple push and pop operations on a bounded stack-like memory store.

HMMs characterize speech or text as a sequence of hidden states q tic categories) and observed states o words) at corresponding time steps t . A most likely sequence of hidden states  X  q sized given any sequence of observed states o  X  using Bayes X  Law (Equation 2) and Markov in-dependence assumptions (Equation 3) to define a full P ( q of a Transition Model (  X  P ( Model (  X  Q Transition probabilities P plex hidden states q nized levels of stacked-up component HMMs in an HHMM. HHMM transition probabilities are calcu-lated in two phases: a reduce phase (resulting in an intermediate, marginalized state f ponent HMMs may terminate; and a shift phase (re-sulting in a modeled state q HMMs transition, and terminated HMMs are re-initialized from their parent HMMs. Variables over intermediate f into sequences of depth-specific variables  X  one for each of D levels in the HHMM hierarchy: Transition probabilities are then calculated as a product of transition probabilities at each level, us-ing level-specific reduce  X  R decoding, the sums are replaced with argmax opera-tors. This decoding process preserves ambiguity by maintaining competing analyses of the entire mem-ory store. A graphical representation of an HHMM with three levels is shown in Figure 1.

Shift and reduce probabilities can then be defined in terms of finitely recursive Finite State Automata (FSAs) with probability distributions over transition, recursive expansion, and final-state status of states at each hierarchy level. In the version of HHMMs used in this paper, each intermediate variable is a reduc-tion or non-reduction state f d cating, respectively, a complete reduced constituent of some grammatical category from domain G , or a failure to reduce due to an  X  X ctive X  transition be-ing performed, or a failure to reduce due to an  X  X waited X  transition being performed, as defined in Section 4.3); and each modeled variable is a syn-tactic state q d constituent consisting of an active grammatical cat-egory from domain G and an awaited grammatical category from domain G ). An intermediate vari-able f d reduction according to  X  F-Rd at the depth level immediately below d , but must in-dicate non-reduction ( 0 ) with probability 1 if there where f D + 1
Shift probabilities over the modeled variable q d at each level are defined using level-specific transi- X   X   X  where f D + 1 conditioned on reduce variables at and immediately below the current FSA level. If there is no reduc-tion immediately below the current level (the first case above), it deterministically copies the current FSA state forward to the next time step. If there is a reduction immediately below the current level but no reduction at the current level (the second case above), it transitions the FSA state at the current level, according to the distribution  X  Q-Tr there is a reduction at the current level (the third case above), it re-initializes this state given the state at the level above, according to the distribution  X  Q-Ex The overall effect is that higher-level FSAs are al-lowed to transition only when lower-level FSAs ter-minate. An HHMM therefore behaves like a prob-abilistic implementation of a pushdown automaton (or shift X  X educe parser) with a finite stack, where the maximum stack depth is equal to the number of lev-els in the HHMM hierarchy. 3.2 Tree-Based Transforms The right-corner transform used in this paper is sim-ply the left-right dual of a left-corner transform (Johnson, 1998). It transforms all right branching sequences in a phrase structure tree into left branch-ing sequences of symbols of the form A noting an incomplete instance of an  X  X ctive X  category A gories have the same form and much of the same meaning as non-constituent categories in a Combi-natorial Categorial Grammar (Steedman, 2000).
Rewrite rules for the right-corner transform are  X  Beginning case: the top of a right-expanding  X  Middle case: each subsequent branch in a  X  Ending case: the bottom of a right-expanding The completeness of the above transform rules can be demonstrated by the fact that they cover all pos-sible subtree configurations (with the exception of bare terminals, which are simply copied). The soundness of the above transform rules can be demonstrated by the fact that each rule transforms a right-branching subtree into a left-branching sub-tree labeled with an incomplete constituent.
An example of a right-corner transformed tree is shown in Figure 2(b). An important property of this transform is that it is reversible. Rewrite rules for re-versing a right-corner transform are simply the con-verse of those shown above.

Sequences of left children in the resulting mostly-left-branching trees are recognized from the bot-tom up, through transitions at the same stack ele-ment. Right children, which are much less frequent in the resulting trees, are recognized through cross-element expansions in a bounded-stack recognizer. In order to compare bounded-and unbounded-stack versions of the same model, the formulation of the right-corner and bounded-stack transforms in-troduced in this paper does not map trees to trees, but rather maps probability models to probability models. This eliminates complications in comparing models with different numbers of dependent vari-ables  X  and thus different numbers of free parame-ters  X  because the model which ordinarily has more free parameters (the HHMM, in this case) is derived from the model that has fewer (the PCFG). Since they are derived from a simpler underlying model, the additional parameters of the HHMM are not free.
Mapping probability models from one format to another can be thought of as mapping the infinite sets of trees that are defined by these models from one format to another. Probabilities in the trans-formed model are therefore defined by calculating probabilities for the relevant substructures in the source model, then marginalizing out the values of nodes in these structures that do not appear in the desired expression in the target model.

A bounded-stack HHMM  X  Q,F can therefore be derived from an unbounded PCFG  X  G by: 1. organizing the rules in the source PCFG 2. enforcing depth limits on these direction-3. mapping these probabilities to HHMM random 4.1 Direction-specific rules An inspection of the tree-based right-corner trans-form rewrites defined in Section 3.2 will show two things: first, that constituents occurring as left chil-dren in an original tree (with addresses ending in  X 0 X ) always become active constituents (occurring before the slash, or without a slash) in incomplete constituent categories, and constituents occurring as right children in an original tree (with addresses end-ing in  X 1 X ) always become awaited constituents (oc-curring after the slash); and second, that left chil-dren expand locally downward in the transformed tree (so each A whereas right children expand locally upward (so each .../A This means that rules from the original grammar  X  if distinguished into rules applying only to left and right children (active and awaited constituents)  X  can still be locally modeled following a right-corner transform. A transformed tree can be generated in this way by expanding downward along the ac-tive constituents in a transformed tree, then turning around and expanding upward to fill in the awaited constituents, then turning around again to generate the active constituents at the next depth level, and so on. 4.2 Depth bounds The locality of the original grammar rules in a right-corner transformed tree allows memory limits on in-complete constituents to be applied directly as depth bounds in the zig-zag generation traversal defined above. These depth limits correspond directly to the depth levels in an HHMM.

In the experiments described in Section 5, direction-specific and depth-specific versions of the original grammar rules are implemented in an ordi-nary CKY-style dynamic-programming parser, and can therefore simply be cut off at a particular depth level with no renormalization.

But in an HHMM, this will result in label-bias ef-fects, in which expanded constituents may have no valid reduction, forcing the system to define distri-butions for composing constituents that are not com-patible. For example, if a constituent is expanded at depth D , and that constituent has no expansions that can be completely processed within depth D , it will not be able to reduce, and will remain incompatible with the incomplete constituent above it. Probabili-ties for depth-bounded rules must therefore be renor-malized to the domain of allowable trees that can be generated within D depth levels, in order to guaran-tee consistent probabilities for HHMM recognition.
This is done by determining the (depth-and direction-specific) probability P or P depth d and rooted by a left or right child will fit within depth D . These probabilities are then esti-mated using an approximate inference algorithm, similar to that used in value iteration (Bellman, 1957), which estimates probabilities of infinite trees by exploiting the fact that increasingly longer trees contribute exponentially decreasing probability mass (since each non-terminal expansion must avoid generating a terminal with some probability at each step from the top down), so a sum over probabilities of trees with increasing length k is guaranteed to converge. The algorithm calculates probabilities of trees with increasing length k until convergence, or to some arbitrary limit K :
Normalized probability distributions for depth-bounded expansions  X  G-L, calculated using converged  X  B-L, mates: 4.3 HHMM probabilities Converting PCFGs to HHMMs requires the calcu-lation of expected frequencies F of generating symbols A nonterminal symbol A ing a left child of A of A trees of increasing length k using the same approx-imate inference technique described in Section 4.2, which guarantees convergence since each subtree of increasing length contributes exponentially decreas-ing probability mass to the sum: where: and P
A complete HHMM can now be defined us-ing depth-bounded right-corner PCFG probabilities. HHMM probabilities will be defined over syntac-tic states consisting of incomplete constituent cat-egories A
Expansions depend on only the incomplete con-stituent category ../A at Transitions depend on whether an  X  X ctive X  or  X  X waited X  transition was performed at the current level. If an active transition was performed (where f complete constituent category A awaited category  X  ..  X ) at q d constituent category ../A  X  .. If an awaited transition was performed (where f d 0 ), the transition depends on only the complete con-stituent category A constituent category A Reduce probabilities depend on the complete con-stituent category at f d +1 stituent category A gory  X  ..  X ) at q d egory ../A the complete constituent category at f d +1 match the awaited category of q d is [ f d at f d +1 t does match the awaited category of q d t  X  1 : and:
The correctness of the above distributions can be demonstrated by the fact that all terms other than  X 
G-L, d and  X  G-R, d probabilities will cancel out in any sequence of transitions between an expansion and a reduction, leaving only those terms that would A PCFG model was extracted from sections 2 X 21 of the Wall Street Journal Treebank. In order to keep the transform process manageable, punctua-tion was removed from the corpus, and rules oc-curring less frequently than 10 times in the corpus were deleted from the PCFG. The right-corner and bounded-stack transforms described in the previous section were then applied to the PCFG. The origi-nal and bounded PCFG models were evaluated in a CKY recognizer on sections 22 X 24 of the Treebank, icant only for sentences longer than 40 words. On these sentences, the bounded PCFG model achieves about a .15% reduction of error over the original PCFG ( p&lt;. 1 using one-tailed pairwise t-test). This suggests that on long sentences the probability mass wasted due to parsing with an unbounded stack is substantial enough to impact parsing accuracy. Previous work has explored bounded-stack parsing using a right-corner transform defined on trees to minimize stack usage. HHMM parsers trained on applications of this tree-based transform of train-ing corpora have shown improvements over ordinary PCFG models, but this may have been attributable to the richer dependencies of the HHMM.

This paper has presented an approximate in-ference algorithm for transforming entire PCFGs, rather than individual trees, into equivalent right-corner bounded-stack HHMMs. Moreover, a com-parison with an untransformed PCFG model sug-gests that the probability mass wasted due to pars-ing with an unbounded stack is substantial enough to impact parsing accuracy.
 This research was supported by NSF CAREER award 0447685 and by NASA under award NNX08AC36A. The views expressed are not nec-essarily endorsed by the sponsors.
