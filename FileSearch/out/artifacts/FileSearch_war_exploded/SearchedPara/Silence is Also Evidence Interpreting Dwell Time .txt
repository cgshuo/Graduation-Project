 Social media is a platform for people to share and vote con-tent. From the analysis of the social media data we found that users are quite inactive in rating/voting. For example, a user on average only votes 2 out of 100 accessed items. Traditional recommendation methods are mostly based on users X  votes and thus can not cope with this situation. Based on the observation that the dwell time on an item may re-flect the opinion of a user, we aim to enrich the user-vote matrix by converting the dwell time on items into users X   X  X seudo votes X  and then help improve recommendation per-formance. However, it is challenging to correctly interpret the dwell time since many subjective human factors, e.g. user expectation, sensitivity to various item qualities, read-ing speed, are involved into the casual behavior of online reading. In psychology, it is assumed that people have choice threshold in decision making. The time spent on making decision reflects the decision maker X  X  threshold. This idea inspires us to develop a View-Voting model, which can es-timate how much the user likes the viewed item according to her dwell time, and thus make recommendations even if there is no voting data available. Finally, our experimental evaluation shows that the traditional rate-based recommen-dation X  X  performance is greatly improved with the support of VV model.
 H.1.2 [ User/Machine Systems ]: Human factors dwell time, recommendation, psychological
Social media provides a good platform for their users to easily publish, view and rate/vote multi-form contents via their computers and smart phones. In such a system a con-tent recommendation service is an important application for both common users and website providers. For the users, popular social media has an overwhelming volume of con-tent with varied quality, making users difficult to distill the information of their interests. Thus, a recommenda-tion service that helps users to find interesting content is highly desirable. On the other hand, for some social web sites (e.g., Amazon 1 ), recommendation is often exploited by the providers to encourage more transactions from users, e.g., recommending laptop accessories after a person orders anotebook.

Collaborative filtering (CF) is a widely adopted recom-mendation technique. CF is mainly based on users X  behav-iors of voting, rating or buying, which express their opinions on certain items. While there exist various actions, without loss of generality, here we consider only voting for simplic-ity. CF assumes that users with similar interests exhibit similar voting behaviors and thus measures the similarity of two users based on their historical item voting. If a user has few ratings, the derived similarity would not provide a good basis to make effective recommendation.

Although lots of methods are proposed to alleviate the impact of sparse data in certain degree [28, 26, 3, 7, 24, 5], they need users X  explicit opinions to make recommenda-tions. Unfortunately, a common situation is that users often stay quiet instead of taking actions to vote. Based on our analysis of real data collected from a joke sharing mobile ap-plication JokeBox 2 , a person on average only votes 2 out of 100 accessed items. Therefore, the data sparsity problem is partially a voting sparsity problem . We call the phenomenon that a user silently viewing an item without expressing their opinions (i.e., giving a vote)  X  X ilent viewing behavior X  .In this work, we aim to exploit it for recommendation.
To view an item, no matter a vote is eventually given or not, a person has to spend some time on it. The time spent provides valuable information about her interest in the item to some degree. Consider a shopping scenario where a lady was comparing two handbags. Suppose she spent 1 minute on one but 5 minutes on the other. Even if she eventually bought neither one, the time difference indicates her prefer-ence to the second handbag. Thus, from the standpoint of a http://www.amazon.com/ http://itunes.apple.com/us/app/all-in-1-joke-box-no-ads!/id363494433?mt=8 merchant, it may be a good idea to recommend a handbag similar to the second one with a lower price. As these silent viewing behaviors actually indicate their potential interests on certain items, we aim to exploit such information for recommendation. Specifically, we carefully model the dwell time , the time a user spends on an item, and convert them into users X   X  X seudo votes X  on items. These pseudo votes are then used to enrich the sparse user-vote matrix and hope-fully improve the recommendation performance.

There exists some recent research works in psychology which focus on the dwell time in the recognition process known as information accumulation [27]. In their test of two alternative forced-choice (2-AFC) task, the participants are required to choose an answer out of two choices (e.g., yes or no). The psychologists proposed a diffusion model to simulate this recognition process, in which people collect evidence for decision making. It assumes that a person has an action bound in making choices and would not make deci-sion until the evidence of one choice exceeds the bound. The dwell time reflects the process of information accumulation. Generally, the easier the task is, the shorter the dwell time will be. However, this model can not be applied directly to our case due to the following reasons.  X  Experiments of the diffusion model are designed to re-quire participants to follow certain strict rules and then make an explicit response for each testing task. In contrast, viewing items in social web sites is such a casual behavior that people may terminate the viewing process at any time. Neither do they need to give a response.  X  Tasks tested with the diffusion model are very specific and usually quite simple (e.g., read a word, scan a short symbol string). The dwell time is easy to model in those tasks. In our case, however, the dwell time on items may vary a lot due to the factors from both items and persons. On one side items may differ not only in their form and vol-ume (e.g., different length of articles, video, different size of pictures, etc.), but also in their quality to attract further consumption. On the other side there are many subjective human factors to affect the dwell time. For example, dif-ferent people receive information at different speed and the time of consuming the same item (e.g., reading an article) may differ from person to person. Furthermore, some peo-ple are very  X  X icky X  and are willing to spend time only on items which match their tastes and expectations. However, some people are rather tolerant and would like to read all items despite their diversified quality. All these factors on the sides of items and persons jointly determine the dwell time of a person on a certain item.  X  In 2-AFC, positive correlation is observed between dwell time and answer accuracy. Thus the action bound is sub-jectively set by the researchers in accordance with the par-ticipant X  X  answer accuracy, where higher accuracy indicates a more careful personality and thus a higher action bound. In real life where people view and vote items, there are no such  X  X ight X  or  X  X rong X  answers as in 2-AFC, and thus the bound, if existing, can not be determined heuristically.
In this work, we propose a Viewing-Voting (VV) model to capture both the silent viewing and explicit voting be-haviors, and explain the implication behind a person X  X  dwell time on an item. We assume that each item has a qual-ity value and each person has multiple latent action bounds (LABs) . These LABs determine the expectation levels of items that may motivate the person X  X  viewing and voting behavior. When a person begins to view an item, one of her LABs is selected. Then, we have the following three situa-tions. First, if the item quality is lower than the bound, the dwell time tends to be short, suggesting that the user does not enjoy the article and thus stops reading before reach-ing the end. Second, if the quality matches the bound, the dwell time is close to the time needed to  X  X omprehend the item X  (i.e., finish reading the article). Finally, if the quality is much higher than the bound, the dwell time is long be-cause the person tends to read a story more before letting it go, indicating that she finds it really good. This models the viewing behavior of users.

As for the modeling of the voting behavior, we consider that if the selected bound is smaller than the item quality, the user is more likely to give a positive vote. Otherwise, the user may simply keep silent. With the VV model, even if a user leaves no vote after viewing an item, we can still esti-mate the user X  X  possible opinion based on the dwell time and exploit it for recommendation. To sum up, our contribution is three-fold.  X  By analyzing the data, we discover users X  infrequent voting behavior and coin the voting sparsity problem. We argue that this problem can be addressed in content viewing applications by mining the dell time.  X  We propose a Viewing-Voting (VV) model to i) explain people X  X  silent viewing behavior based on their dwell time on the item; and ii) model the users X  voting behavior. Based on the VV model, we develop a strategy that interprets the dwell time to user X  X  possible vote, referred to as pseudo vote and exploit it for recommendation.  X  We conduct extensive experiments on a real dataset and demonstrate the improvement of conventional recommenda-tion techniques when combined with VV model.

The rest of the paper is organized as follows. Section 2 presents our data analysis results, which serve as a guidance for the model development. Section 3 and Section 4 provide details of the VV model and show how it interprets dwell time for recommendation. Section 5 and Section 6 respec-tively shows the evaluation result and reviews the related work. Finally Section 7 concludes the paper.
In this section we perform a data analysis and show the results, some of which guide the design of the VV model. The analysis mainly consists of three parts, (i) revealing the fact of vote sparsity, (ii) testing the existence of the action bound, and (iii) exploring the characteristics of dwell time. The data we use are the log of JokeBox from June to November, 2011. JokeBox is a popular iPhone applica-tion, where people publish and vote jokes. Figure 1 shows the snapshots of its main interfaces. When the application starts, a list of jokes with abstract is displayed as shown in Figure 1a. Users need to tap on some joke to see the full content, whose interface is shown in Figure 1b. After reading a joke, user can vote for (i.e., the green hand in Fig-ure 1b) or against (which is the red hand in Figure 1b) the joke or simply retreat to the list by tapping the X  X eturn X  X ut-ton at the upper-left corner. The log records such actions as  X  X ap X ,  X  X ote X  and  X  X etreat X  with time stamp. The dwell time is obtained by computing the time stamp difference be-tween  X  X ap X  and  X  X etreat X  of the same user on the particular item. There are in total 638,899 records containing 108,743 users and 143,258 items (jokes) 3 . The log also records the final vote situation for each item, i.e., the number of positive votes minus that of negative votes, which, as an example, is 12 for the joke in Figure 1b. This value to some degree reflects the public evaluation for this item. Thus, in this work we treat it as the quality of the item. Additionally, the statistics from our preliminary analysis show that the negative vote occupies a rather small proportion (1,395 neg-ative votes in 143,258 items) and thus in this work we only focus on analyzing and modeling positive voting and silent viewing behavior.
We first find statistics on the frequency of user giving vote and define a metric user-voting ratio r u in Equation (1). The statistics of r u is shown in Table 1. We can see that most (97.91%) of users X  voting ratio is quite small (less than 30%). Moreover, 95.93% of people never give a vote. A further analysis shows that the average user-voting ratio is only 2.02%. Thus the data sparsity problem, or exactly the voting sparsity problem , is caused by the user X  X  infrequent voting behavior although they may be quite active in viewing items.

We then test i) whether there exists action bounds for users when viewing and voting an item and ii) whether the bound is different from person to person.

We firstly group people with regarding to the number of positive votes they give. Intuitively a person X  X  bound, if Not all users X  actions were recorded due to the version issue. However, since the recorded users are quite random, the analysis result is still reliable. existing, would be high if she had high expectation on the item quality and hardly gave a positive vote. Next, for each person in the group, we identify all her accessed items and classify them into two categories,  X  X ike X  and  X  X eutral X , de-pending on whether she gave (positive) vote to the items or not. Then, for each person we compute the average quality values of her accessed items from the two categories, re-spectively. Finally, we average the quality values (i.e., as mentioned above, the final vote status) of these two cat-egories over all persons with the same number of positive votes. The result is shown in Figure 2. Note that as the number of positive vote increases, the sample size of users who give exactly that number of votes decreases. Therefore we only display the result of groups that have more than 20 samples in Figure 2.
It can be easily seen that for the first 10 groups the  X  X ike X  category (blue solid line) is above the  X  X eutral X  one (green dashed line). It indicates that on average the quality of the items in the  X  X ike X  category for a person is bigger than the quality of her accessed items that failed to receive her positive vote. Also, the average item qualities for both cat-egories have a decreasing trend as the number of positive vote in x axis increases. This supports the action bound hypothesis. The persons who give less positive votes may have higher level of expectation on the item quality, namely the higher action bound. Thus, only the items with higher quality, which exceeds her action bound, can motivate her to vote positively. Finally, for large numbers of positive vote ( x&gt; 10), the trend of these two categories becomes unclear. One possible reason is that people may have multiple bounds whose values are quite different. As the number of positive votes increases, different bounds are likely to be selected to guide the voting behavior and thus affect the statistic curve.
Next we explore the trend of the dwell time. As mentioned in Section 1, the dwell time for item viewing is different from item to item due to their variant formats (e.g., text, picture, video). For jokes which consist of text, the intuition is that it should be proportional to the text length. We group all the viewing events according to the item length. For the view-ing events with the similar item length we further classify them into two categories, those that end with a positive vote from users (denoted as  X  X ike X ) and those with silent viewing (denoted as  X  X eutral X ). For each category we calculate their average dwell time and plot it with regarding to the item length in Figure 3.

From the figure, we can see that for both categories the dwell time is positively proportional to the item length. This is reasonable because longer texts take more time for peo-ple to read. Another interesting observation is that the blue solid line is always above the green dashed line, suggest-ing that the average dwell time for positively voted items is longer than that of the neutral ones when the item length is similar. Recall that in Figure 2 we found positively voted item has a higher quality than neutral. This suggests that people tend to spend more time on the items of higher qual-ity. This observation makes sense because in reality when a person meets a good item, e.g., a really funny joke, she is more likely to read it multiple times, resulting in longer dwell time.

Finally we explore the distribution of dwell time. As shown in Figure 3, the average dwell time correlates with the story X  X  length. Therefore we first group all stories according to their length. Then we select the group of items with the same length which has most number of views and count the frequency of the dwell time. Figure 4a shows the frequency for stories whose length ranges from 36 to 46. Note that the x -axis is in log-scale. The statistical result indicates that the dwell time may satisfy a log-Gaussian distribution. To fur-ther prove this hypothesis, we compare the log value of dwell time sample to the standard Gaussian distribution and plot the quantile-quantile curve in Figure 4b. As can be seen, the blue markers closely lie on the red straight line, indicating that the two distributions are linearly related. Therefore it is proper to use a log-Gaussian distribution to model the dwell time.
In the end of this section, we summarize our observa-tions into principles that will guide the design of our model. Firstly, people tend to have multiple action bounds which differ from person to person (see Figure 2). Secondly, the length of a textual item determines the expected dwell time, which at the mean time is affected by the difference between the item X  X  quality and the user X  X  action bound (see Figure 3). Finally, it is proper to choose log-Gaussian distribution to model the dwell time (see Figure 4).
In this section we discuss the details of the model. Table 2 lists the symbols we used in the work. We assume that each user has several latent action bounds (LABs). When view-ing an item, the user will randomly select a LAB, which, to-gether with the quality of the item, jointly affects the user X  X  voting behavior, i.e., to leave a vote or not. Generally, if the selected LAB is bigger than quality, the user is less likely to leave a vote and vice versa. The LABs can be treated as the expectation of the person to the quality of the item. The higher the user X  X  expectation is, the harder the item can en-tertain her, and vice versa. However, different from item X  X  quality that can either be measured or reflected (by people X  X  votes), the  X  X xpectation X  is hard to measure. Therefore we model it as a hidden parameter.

Besides the voting, another value, the dwell time, which is corresponding to the viewing behavior, is also generated. This is a metric that measures how long the user would stay on this item before moving forward. In general, it is affected by LAB and quality in a way that is similar to how the vote is produced. For big LAB and small quality, the dwell time tends to be short since the item can hardly entertain the user. For small LAB and big quality, the dwell time would be long as the user is extremely attracted to the item. Figure 5: The graphical model. The shaded circles stand for observable values while the unshaded ones for latent vari-ables.

The graphical model is shown in Figure 5. Each user u has a set of LABs. Each item consists of two components, i.e., property l (in our case it is the length of the joke) and quality q . The quality q and bound b jointly generate the vote v , where 1 means positive vote and 0 no vote.
The dwell time t is determined by two groups of factors, (i) user-related and (ii) item-related. As the name suggests, the former one stands for the personalized factors that affect the dwell time. The latter one, on the other hand, may be different as the item differs. Specifically in the graph model, the t is generated by five parameters, among which LAB b , reading speed r and quality sensitivity  X  are user-related while the other two, namely q and l are item-related. The speed r measures how quickly a person can read while the sensitivity  X  reflects how sensitive the person is to items of diversified quality. The details will be discussed later in Section 3.2.

The item-related factors have different semantics with re-garding to the different item types. In this work, we focus our attention on the textual item and discuss these two fac-tors in the context of an article. In the graphical model, the q represents the quality of the item and l represents the length of the article.
 Algorithm 1 Generation process 1. Choose a LAB b  X  Multinomial (  X  i ), 1  X  i  X  k 2. Choose a parameter p  X  Be ( q,b ) 3. Choose a vote v  X  Bernoulli ( p ) 4. Choose a time log t  X  X  (  X ,  X  2 ) The generation process is summarized in Algorithm 1. The user first selects one of k LABs, where the probability of selecting i th bound b i is  X  i .AftertheLAB b is generated, it goes along with the item X  X  quality q to determine the vote, which is modeled as a Bernoulli process and the parameter satisfies a Beta distribution with parameters ( q,b ). We will discuss its details in Section 3.1.

Finally, a dwell time t is generated by a log-Gaussian dis-tribution whose mean is  X  and variance is  X  2 , as suggested in Figure 4. Details of the dwell time model as well as these parameters are discussed in Section 3.2.
Here we discuss the model of voting. As described earlier, it is a Bernoulli process, where the probability of positive vote is p while the neutral is 1  X  p . The parameter p is produced by a Beta distribution Be ( q,b ) defined in Equation (2).
 where q is the item X  X  quality and b is the user X  X  LAB.
In probability theory, Beta distribution is often used to describe a prior distribution of a parameter for some distri-bution, e.g., Bernoulli distribution. Specifically, the q b jointly determine the probability distribution of p .When q&gt;b  X  1, the value of p is more likely to be large. In case of 1  X  q&lt;b , the generated p is closer to 0. That means, the user is more likely to give a positive vote if the quality of the item is bigger than the LAB b . If not, a silent viewing may possibly happen.

To sum up, given a selected LAB b andanitemwhose quality is q , the probability of a positive vote is given in Equation (3).
 Similarly, the probability of no vote ( v = 0) is shown below:
P ( v =0 | b, q )=1  X  P ( v =1 | b, q )=1  X  q Finally, Equation (3) and (4) can be unified as below.
In this section we discuss the model of dwell time. Specif-ically, we consider the following three cases with regarding to selected LAB b and item quality q :  X  b&gt;q . The item is not good enough to motivate the user to vote. In this case, the user would not spend too much time on this item and she would even quit before reaching the end of the passage.  X  b  X  q . The item X  X  quality is very close to the user X  X  LAB, which may lead to the user X  X  hesitance of voting. Therefore, the user may continue to read the item to its end. And the time is related to the item X  X  property, i.e., the length of the article.  X  b&lt;q . The item X  X  quality goes beyond the user X  X  expec-tation and naturally the user may spend much more time reading this item and may remain on it after finishing read-ing, making the time longer than expected.

Based on the result of statistical analysis obtained in Sec-tion 2, we use log-Gaussian distribution to model the dwell time. The mean value  X  is determined by the item X  X  length quality q ,theselectedLAB b as well as two personalized pa-rameters reading speed r and quality sensitivity  X  .Recallin Figure 3 we found that the average dwell time is linearly cor-related to the item X  X  length and thus l/r measures how long the person would spend reading a whole article of length l The sensitivity  X &gt; 0 determines the influence of the differ-ence between quality and LAB, i.e., q  X  b on the dwell time. Given the item X  X  length l , quality q and the user X  X  LAB b well as r and  X  , the probability that the user would spend time t on this item is given in Equation (6).
 Note that if q&gt;b , the mean dwell time would increase and vice versa. The sensitivity  X  determines the time growth-decay proportion and is different from person to person. Big  X  may indicate a  X  X icky X  person whose dwell time is heav-ily quality-dependent, i.e., spending little time on trash but much time on high-quality items. The small  X  , on the other hand, suggests a  X  X olerant X  person whose dwell time does not vary too much on items of diversified quality. Based on earlier discussion, the model parameters are LAB distribution  X  i , 1  X  i  X  k , the series of LABs b i ing speed r , quality sensitivity  X  and dwell time variance Let  X  denote all of these parameters. The learning is a pro-cess to determine proper values for  X  in order to maximize the probability of observed data, i.e., the item X  X  quality and length l ,theuser X  X vote v  X  X  0 , 1 } as well as her dwell time t .

Let ( Q = { q 1 ,  X  X  X  ,q n } , L = { l 1 ,  X  X  X  ,l n } )denotethe items accessed by the user while V = { v 1 ,  X  X  X  ,v n } and T = { t 1 ,  X  X  X  ,t n } respectively stand for the vote and dwell time of the user on the corresponding item. Finally B are the unobserved LABs that affect the user X  X  behavior when view-ing the item. A loss function with constraint k i =1  X  i =1is defined in Equation (7).
 defined in Equation (8).
We use EM (expectation-maximum) algorithm [8] to solve the problem. In the following,  X  s denotes the parameter values for the s th iteration.
 E-step .
 The definition of P ( v i | b j ,q i )and P ( t i | l i ,q Equation (5) and (6).
 M-step . All parameters except b j can be directly computed while b can be obtained by finding the root for Equation (11) with Newton X  X  method.
Here we introduce how to use the proposed model to interpret the silent viewing behavior for recommendation. Specifically, we split the task into three phases. In phase I, given a user X  X  dwell time on an item, the model is applied to find her LAB that is most likely to be selected to guide the viewing behavior, referred to as LAB choice estimation . In phase II, namely vote prediction , the probability is com-puted that how likely the user would vote for the item given this estimated LAB. These  X  X seudo votes X  are used to en-rich the original sparse user-vote matrix. Finally in phase III, conventional rating-based recommendation techniques, e.g., CF, is applied to make recommendation based on both the actual and pseudo votes.

Formally, for a particular item o ,let q o and l o respectively denote its quality and length. Suppose the dwell time of i th user on the item is t o i .Let u i = {  X  i j ,b i j , X  i ,r j  X  k } denote the i th user X  X  profile, which is obtained after model training. LAB choice estimation is to find a LAB b  X  that is most likely to be selected to generate the observed dwell time, as shown in Equation (15).
After a proper estimated LAB b  X  i o is found, we can inter-pret the user u i  X  X  dwell time to her possible preference on that item, which is modeled as the expected vote as given in Equation (16).
 After the first two phases, the  X  X seudo votes X  from the VV model enrich the original sparse voting matrix, on which conventional recommendation techniques can be applied.
For evaluation we use the real log from JokeBox as in-troduced in Section 2. Although using multiple data would strengthen the work, it is really hard to obtain such view-vote data from another domain. In our experiments, we only keep the users who have viewed no less than 20 items. As a result, we obtain 960 users and 19,196 items, among which there are 2,053 votes and 33,158 silent views. As the default in our experimental setting, the number of LABs is set to 5 unless noted explicitly.

For evaluation, we split the vote data into four parts and conduct 4-fold cross-validation. For every round, three parts of the data are used as the training data to obtain user pro-files with Equation (10) to Equation (14). Then the remain-ing part is used as test data.

Two metrics are exploited in the evaluation. The first one is referred to as hit ratio . For each target user, a list of can-didate items are ranked and recommended. Among them some do receive positive vote from the target user, treated as a  X  X it X . Note that a random recommendation could also rank these candidate items by randomly generating a rec-ommendation list. The hit ratio measures the improvement of the proposed recommendation solution over the random recommendation. The performance of a random recommen-dation, as shown in [33], can be represented as | M | | C | X | U | C is the collection of candidate items, U is the collection of users while M is the set of test data. Formally, the cut-off hit ratio at rank N is defined in Equation (17). hratio @ N = Number of hits before N Note that higher hit ratio indicates better performance.
The second adopted metric is hit rank . Suppose the ranks of hits are r 1 ,  X  X  X  ,r m ,where1  X  r 1 &lt;  X  X  X  &lt;r m and | R | is the size of recommendation list. Ideally, a perfect ranking scheme should rank all items in the head of the list and results in such equation r i = i . The hit rank measures how close the ranking list of the proposed solution is to the ideal one X  X . Formally, it is computed via dividing the sum of r i by that of a perfect ranking scheme, which is N  X  ( N The formal definition is given in Equation (18). Since r i  X  i , smaller value of hit rank indicates a better recommendation strategy.
As for the parameter initialization, the starting value for impact  X  u 0 is set to 1. Suppose a user u  X  X  viewing history is quality and length, t u i is the user X  X  dwell time spent on her i th accessed item. The user X  X  reading speed r u 0 is initially set as in Equation (19). The variance (  X  u 0 ) 2 is initially set as the variance of log as in Equation (20).
The probability of LAB selection  X  j is initialized as a uni-form distribution. For the initial values of LABs, we select random numbers from 1 to 1000 4 .
In this section we show the results of evaluation. We use two variants of collaborative filtering, i.e., user-based collab-orative filtering (UCF) [12] and SVD++ [16], both imple-mented by MyMediaLite [11]. We are aware of that there are many Matrix Factorization variants, e.g., [1, 10] and usu-ally they embed such extra information as user/item meta data, item X  X  textual content. As we believe that our proposal is complementary to existing techniques, in this evaluation we only demonstrate how much improvement our solution brings to the rating-based recommendation techniques that adopts a naive interpretation of silence view. Particularly
We tested different values in the experiment and found no significant difference in performance. we adopt two baselines, respectively denoted as Aggressive and Neutral. The former interprets the silent viewing be-havior as a rating of 0.1 while the latter treats it as a rat-ing of 0.5 (neutral). Recommendations that adopt these three techniques (Aggressive, Neutral, VV) are denoted as A-UCF/SVD++ , N-UCF/SVD++ and VV-UCF/SVD++ .
The general performance of the six solutions are shown in Figure 6. We can see that methods that combine with our proposed VV model (VV-UCF/SVD++) benefit an im-provement over those with baseline (A/N-UCF/SVD++). Both Aggressive and Neutral scheme adopt a naive way of interpreting dwell time, either to negative vote or neu-tral. Therefore it is not as accurate as the VV model which provides a complex simulation of the dwell time that in-volves multiple subjective human factors such as expecta-tion (LAB), reading speed ( r ) and the sensitivity of quality difference (  X  ). Note that in Figure 6a and Figure 6b, A-UCF and N-UCF display equal performance. For UCF, a cosine similarity metric is adopted to measure the prefer-ence similarity between people. Therefore different assigned values, e.g., 1 or 2.5, for the unvoted items does not change the relative similarity between users, leading to the equal performance of A-UCF and N-UCF.

We then test the impact of data sparsity. Note that in previous experiment four-fold cross-validation is used. That means 25% of voting data is used as test data. Here, we eval-uate the performance with regarding to different numbers of cross-validation, where bigger value indicates more training data and less test data and vice versa. The performance where N = 20 is shown in Figure 7. From Figure 7a, we can see that the increase of cross-validation number brings better recommendation performance for UCF since more training data is available. In Figure 7c however, the data sparsity does not seem to have an obvious impact on SVD++. It may be due to the strength of SVD++ to UCF. Instead of basing on the raw user-item voting data, the SVD++ cre-ates virtual feature space for each user and item, thus may be more resistant to data sparsity than UCF. Finally in Fig-ure 7b and Figure 7d, the performance degrades as the num-ber of cross-validation increases for both UCF and SVD++. Recall the definition of these two metrics in Equation (17) and Equation (18), we can see that compared with hit ratio, hit rank focus more on the ranks of the recommended item. When the training data increases, the test data decreases, leading to more noise in candidate item. However, we can see that VV-SVD++ suffers less degrade compared to base-line, demonstrating its accurate interpretation of dwell time.
In the final experiment, we test the impact of LAB size lab . Figure 8 shows the performance of UCF and SVD++ that are combined with VV model w.r.t different LAB size. As can be seen, generally smaller k achieves better perfor-mance. Particularly when N&gt; 5 for hit ratio and N&gt; 10 for hit rank, VV model with single LAB display best per-formance. This scenario suggests that for this joke-reading dataset, people are quite consistent in terms of expectation and small number of LAB fits the dataset better than large one. A further check of learned user profiles reveals that when k&gt; 1, the learned dwell time variance  X  2 for all users tend to be small. But when k =1,forasmallnumber of people, their learned dwell time variance  X  2 is extremely huge, which means their time spent on items vary a lot and the prediction of their dwell time would be quite inaccurate. This observation provides such a possibility that besides the values of LAB, the number k might be another personalized parameter, which will be explored in future work.
Our work is related to three areas, i) data sparsity in recommendation, ii) implicit user feedback modeling and iii) diffusion model in psychology.

Data Sparsity in Recommendation . Data sparsity is a really big issue that impacts the performance of collabora-tive filtering in recommendation. Different approaches have been developed to incorporate various features on users and items into recommendation to tackle this problem. To build the connections among users, new similarity measurement were proposed to find the similar users in [3, 5]. To cre-ate new connections among items, different item-based sim-ilarity measures are proposed for different types of items, such as textual items [23, 28], POIs (point-of-interest) [32], movies [24] etc. Furthermore, Matrix factorization (MF) [17] is proposed to flexibly embed various features. For instance, in [22, 21], social network is imported to constrain the factor-ization process in such a way that socially connected peo-ple tend to have similar user feature vectors. Also in [1], Agarwal et. al. proposed fLDA , a hybrid recommendation method that combines matrix factorization with LDA, aim-ing to exploit items with rich textual information to improve the rate prediction. To sum up, these works are all based on the condition that users provide explicit opinions. Our work tries to throw away this burden, and tackle the sparsity problem from a new angle, i.e., predicting user X  X  interest in items according to her dwell time.

Study on Implicit Feedbacks . There are some previ-ous works focusing on different types of implicit feedbacks, e.g.,  X  X laycount X  X f music tracks or albums [14], frequency of visits to a content or category [25] and so on. Some meth-ods are also proposed to integrate implicit feedbacks to the rating-based solution [19]. We believe that our model is complement to these works.

There are a few existing works to study dwell time as im-plicit feedback of users. In [2, 4, 6], dwell time is treated as an extra feature to rank the relevance of retrieved web page to user X  X  query. In [15, 30], the correlation between display time and document usefulness/relevance is studied in information-seeking task. In [20], a system BrowseRank was developed which makes use of user X  X  browsing behavior to rank the importance of web page. Other works [13, 31] try to make use of people X  X  watching time for recommending TV shows or programs. Recently in [25], a positive correla-tion is observed between display time of picture and user X  X  high ratings. Although Liu et. al. [18] used Weibull distri-bution to model people X  X  dwell time on web pages, we found that a log-Gaussian distribution is more proper for the dwell time on jokes, as shown in Figure 4. Furthermore, unlike previous works that simply interpret longer dwell time to positive feedback, we explore the latent human factors that determine the dwell time by borrowing the concepts from Psychology, which to our best knowledge is the first work.
Diffusion Model in 2-AFC Task . Psychologist pro-posed a diffusion model [27] to simulate a person X  X  response time when facing a two-alternative forced choice (2-AFC) task. The participant is required to make a judgement on a simple task, e.g., whether a given letter string is a legal English word [29]. Researchers measure the response time as well as the answer precision. The diffusion model, also known as Wiener diffusion process [29], was first studied in [9] and later implemented by Ratcliff et. al. [27]. It aims to help scientists understand the human recognition process with regarding to factors such as gender, age and so on. The model assumes each person has a response threshold and the choice is made when the accumulated evidence hits the threshold. The response time is also determined by how fast the information is accumulated, namely the drift rate as in [27], which is a normally distributed variable.
As mentioned in Section 1, our scenario (item-viewing in social web) is much more complex than 2-AFC task and the diffusion model can not be directly applied.
In this work we propose a Viewing-Voting (VV) model to exploit dwell time for recommendation. Traditional rec-ommendation strategies are based on the opinion-expressing behaviors and do not consider silent viewing behavior. The VV model is developed to bridge the gap by correctly in-terpreting the dwell time to  X  X seudo vote X . As the experi-ment shows, the performance of traditional recommendation is greatly improved with the support of our VV model.
As for future work, we will study the trend of dwell time with regarding to different item formats (e.g., audio, video, picture etc), and consider different application scenarios (e.g., online shopping, mobile APP recommendation etc).
