 Retrieval system effectiveness can be measured in two quite different ways: by monitoring the behavior of users and gathering data about the ease and accuracy with which they accomplish certain specified information-seeking tasks; or by using numeric effectiveness metrics to score system runs in reference to a set of relevance judgments. In the second approach, the effectiveness metric is chosen in the belief that user task performance, if it were to be measured by the first approach, should be linked to the score provided by the metric.
This work explores that link, by analyzing the assumptions and implications of a number of effectiveness metrics, and exploring how these relate to observable user behaviors. Data recorded as part of a user study included user self-assessment of search task difficulty; gaze position; and click activity. Our results show that user behavior is influenced by a blend of many factors, including the extent to which relevant documents are encountered, the stage of the search process, and task difficulty. These insights can be used to guide development of batch effectiveness metrics.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Retrieval experiment; evaluation; system measurement.
There has been a tremendous amount of work undertaken in evaluating retrieval system effectiveness. Although there are many alternatives  X  direct observation of users, log files, or diary studies for example  X  by far the most common approach is to use one or more batch-evaluation metrics.

The traditional batch-evaluation metrics of precision and recall have been extended by a raft of alternatives, including average precision (AP); discounted cumulative gain (DCG) and normal-ized discounted cumulative gain (NDCG) [13]; rank-biased preci-sion (RBP) [18]; reciprocal rank (RR); expected reciprocal rank (ERR) [7]; BPref [3]; time-biased gain [21]; plus many more. Carterette [5] gives a framework in which many metrics can be seen as being related; and Moffat [16] categorizes metrics according to their numeric properties.

Underlying all metrics is the assumption that the retrieval system returns a ranked list of documents, and that each of the documents retrieved can be scored for relevance , a real value 0  X  r r = 1 indicating that the i th document in the ranking is highly (or even perfectly) relevant, r i = 0 indicating that the i th document is completely irrelevant, and gradations in between these extremes.
Each effectiveness metric can then be regarded as having a corre-sponding user model, describing how users interact with the ranked list. For example, Prec@ k models each user as inspecting exactly k documents, and by computing ( 1 / k )  X  k i = 1 r i , generates a score that represents the average rate at which a user accrues relevance.
In this paper we explore the connection between models, metrics, and user behaviors. We begin by establishing a framework in which each metric can be identified with three explicit parts of a user model: weights on each document rank; conditional probabilities of a user continuing to read past each rank; and probabilities of a user stopping at a given rank. Given this formalism, it is natural to then ask: what is a realistic model? Which metrics instantiate this?
Observations from a user study with 34 participants and three types of search task provide concrete data, and we demonstrate that there are a number of factors which contribute to a user X  X  reading behavior which are not considered in present models. Moreover, it is both plausible and possible to include them, and doing so should result in metrics that are more accurate than those in current use.
Effectiveness evaluations in IR commonly use batch evaluation metrics. For any one query, issued over a set of documents, a rele-vance score r i is assigned to each document. Considering the ranked list of documents returned by a search system, and the relevance of each, any number of alternative metrics can then be calculated. For example, precision amongst the first k documents (Prec@ k ) can be computed with
Prec@ k (  X  r ) = as the inner product of a weight function W () and the relevance vector  X  r . We do not consider the question of how r i is determined, but note that it is a human process and hence may be the subject of imprecision, and also that it might be context dependent, varying according to what documents have been observed by the user earlier in the ranking. That is, relevance  X  r is due to document and situation, and is not in our power to change. Note that there is no requirement that relevance be binary, and r i can be thought of as a fractional value to support graded relevance.
 Equation 1 can be generalized to an arbitrary metric M: where W M () is a probability distribution, with  X   X  i = 1 Metrics have also been suggested in which the sum is not 1 , but provided that  X   X  i = 1 W M ( i ) is bounded, these can be normalized into an equivalent set of probabilities. When  X   X  i = 1 W converge, normalization is not possible, and truncation at some limiting depth k is required. The issues that arise from truncation are discussed below.

There are two interpretations that can be placed on W M () when it is a probability distribution. In the first, W M ( i ) is the likelihood that document i is the one being inspected at any given moment by the person examining the ranking; that is, their document inspec-tions constitute a sequence of random selections from W M alternative interpretation of W M () is that users examine documents sequentially from the top of the ranked answer list, starting with the first-ranked document. Once they have reached depth i in the ranking, they proceed to depth i + 1 with conditional probability Hence, for the metric Prec@ k , which is to say the user always reads from rank 1 down to k , and then stops.

The relationship between W M () and C M () means that they can be computed from each other. Equation 3 shows how C M () can be derived from W M () ; the reverse is accomplished by noting that
There is a third equivalent way of specifying an effectiveness metric. Define L M ( i ) to be the probability that the i th document in the ranking is the last one observed by the user, that is, This function is also a probability distribution. For example, L is simply 1 when i = k , and 0 otherwise.
 To complete the circular relationship between W M ( i ) , C L M ( i ) , note that The expected number of documents inspected is then given by: Weighted precision metrics can be characterized by any of W C
M ( i ) , or L M ( i ) : that is, the definition of any one of those three functions completely specifies the metric. Since the three functions describe user behavior, specifying a metric this way also specifies a user model. We illustrate these ideas next.
We first consider a range of static user models, that is, user models in which the conditional continuation probabilities are a function of rank position alone.
 Precision Precision at depth k , or Prec@ k , was outlined above. The corresponding user model is that the user examines the first k elements in the ranking and then stops, with each of the k items equally-weighted. This metric is top-weighted in that it assigns zero weight to all documents beyond rank k , but it does not discriminate between ranks within the top k .
 Discounted Cumulative Gain J X rvelin and Kek X l X inen [13] ob-serve that top-weightedness is desirable, and propose a metric they call discounted cumulative gain , or DCG@ k . They specify DCG in terms of a non-convergent infinite weighting vector. To obtain a probability distribution it is necessary to truncate at some depth, and use a scaled discounted cumulative gain metric, defined as: There may be situations in which truncation at depth k is not accept-able. But if the unrestricted function is regarded as being the conditional continuation probability, as is implicit in the original proposal of J X rvelin and Kek X l X inen, then W
DCG ( i )  X  0 for all values i , and the user is assumed to inspect an unbounded number of items.
 Rank-Biased Precision To avoid the discontinuity in behavior at depth k , and to allow for infinite distributions while still giving a gradated response within the top k , Moffat and Zobel [18] suggest an effectiveness metric they call rank-biased precision (RBP): where p is a  X  X ersistence X  parameter that describes the propensity of the user to step from one document to the next. For example, when p = 0 . 7 , if the user has examined the i th document in the rank-ing, there is a 30 % probability that they will abandon their search and not proceed to document i + 1 . The weights W RBP ( i ) form a geometric sequence, W RBP ( i ) = ( 1  X  p ) p i  X  1 , and the expected number of objects examined is thus 1 / W RBP ( 1 ) = 1 / ( 1  X  p ) . It is also straightforward to show that for RBP, L RBP ( i ) = W meaning that for this metric there is a further interpretation possible  X  the score assigned to a ranking is numerically equal to the expected relevance of the last document inspected [5].

The proposal by Moffat and Zobel explicitly connects an effec-tiveness metric based on a convergent infinite distribution with a user model that does not limit the depth to which documents may be accessed. But, while it gives better top-weightedness behavior than does precision, RBP is  X  X tateless X , in that the user is envisaged as having exactly the same behavior at depth 100 in the ranking as at depth 1 , and the same behavior after observing a relevant document as after observing an irrelevant one.
 Inverse Squares Moffat et al. [17] propose the use of a different convergent sequence. Their inverse squares metric INSQ is parame-terized by a value T , the target number of relevant documents the user wishes to identify, and is defined by pected number of documents processed for INSQ is approximately 2 T + 0 . 5 , and that T serves the same role as the parameter p asso-ciated with RBP. The RRG metric identified by Carterette [5] also uses an inverse squares weight distribution, but does not have an equivalent of the parameter T .
 Figure 1 compares W M () , C M () , and L M () for SDCG, RBP, and INSQ. The parameters (respectively: k , the SDCG truncation depth; p , the RBP persistence; and T , the INSQ target) are chosen so that all have the same weight W M ( 1 ) , and hence all have the same expected value for the number of items inspected by the user.
Metrics have also been defined in which the user X  X  path through the ranking is adaptive , and affected by the relevance of the docu-ments that they see at each inspection. (Note that some authors refer to static models as being  X  X ositional X , and to the adaptive models described in this section as being  X  X ascade X .) Reciprocal Rank This metric is defined as: That is, RR computes the average precision across the documents down to, and including, the first fully relevant one. In the case of binary relevance judgments, if that first relevant document appears at depth d , then RR = 1 / d . The corresponding user model is also straightforward: users sequentially examine documents until a fully relevant one is identified, and then end their search.
 Average Precision In the case of binary relevance judgments the metric average precision is the average of the R =  X  N precision scores attained at the locations in the ranking at which relevant documents appear. Average precision can also be ex-pressed as a weighted precision metric by attributing to each rele-vant item the total contribution it makes. For example, if a rank-Table 1: Average precision as a weighted precision metric. The weights W AP ( i ) depend on the relevance vector  X  r as described in the text; then C AP ( i ) and L AP ( i ) are derived from W ing has relevant documents at depths 2 , 5 , and 6 (only), then there are R = 3 relevant documents in total, and the AP score is ( 1 / 2 + 2 / 5 + 3 / 6 ) / 3 = 0 . 467 . But the components of that score can be striped across the relevant items that contributed, with ranks 1 and so on. Table 1 completes this example, and adds conditional continuation probabilities C AP () and last probabilities L Hence, in our terminology AP can be specified as: The user model that corresponds to AP, described by Robertson [19], suggests that the user selects one of the relevant documents at ran-dom, and then examines every document down to and including that one in the result listing. In the form shown in Equation 6, AP models the user as always knowing how many relevant documents remain in the as-yet-unseen part of the ranking beyond depth i , and also what locations they are in. That is, the user model corresponding to AP is plausible only if the user can be assumed to base their decisions on documents that they have not yet seen, rather than on documents that they have.

Equations 5 and 6 indicate that the user continues down the ranking until at least one relevant document is found, while allowing no possibility that a user might exit from their search prior to finding even a single relevant document. As an extreme, neither metric is defined for rankings that do not contain any relevant documents. Despite being adaptive in terms of responding to the relevance of the result listing, neither RR nor AP have the flexibility to cope both with situations in which a single answer document is required (navigational queries) and situations that require many documents to be identified (informational tasks).
There are several metrics/models described above, either explic-itly, or via definition of one or more of W ( i ) , C ( i ) , and L ( i ) ; and many more in the literature. It is then reasonable to ask how to choose one over the others. There are several options: we can rely on rhetoric; we can compare outcomes (do they agree with each other? which is more stable? which is more sensitive?), or we can ask about the fidelity of the model.

In the rest of the paper we do the last of these, and ask: how well do the models corresponding to various IR metrics match real behavior? In particular, is there a  X  X ight X  formulation for W ( i ) , C ( i ) , and L ( i ) ? We tackle this question in two ways: first, by listing a set of possible user behaviors and asking if there is a model that encapsulates them all; and then, in the next section, by studying users carrying out search tasks.
 Possible User Behaviors There is a range of reasonable hypotheses about user behavior, which should be captured in any user model (and hence metric). The pattern of behavior suggested by these hy-potheses is summarized in Table 2, adapted from Moffat et al. [17]. 1. Users undertake searches for different reasons. Some searches 2. Users may wish to examine documents to arbitrary depth in 3. All other factors being equal, users may be more likely to con-4. Users may alter their behavior based on the part of the rank-5. Users may exit from their query without having (fully, or In summary, all of the common weighted-precision metrics can be criticized in some way or another when they are weighed up against hypothesized user behavior.
The five suggested behaviors listed above appear plausible, but are just hypotheses. In this section we describe a user study that examined user behavior (including gaze data) on a number of search tasks, seeking to gather evidence for or against them.
Subjects were presented with a set of six information need state-ments (see Table 3), plus one warm-up task (not shown), and asked to use a search engine to find and mark documents that would help them answer the questions. All interactions were undertaken using an instrumented interface that both limited user actions in certain ways, and allowed detailed logging that included click actions and query reformulations. Ethics committee approval for this project was granted at RMIT University.

Queries were executed via the Yahoo! API, but answer pages were presented to the subjects without any branded identification. Documents could be viewed via a pop-up window that obscured and de-activated the main search listing until it was closed again. To close the document pop-up, subjects were required to indicate whether viewing that document was  X  X seful X  or  X  X ot useful X . Doc-uments that for some reason were redisplayed collected a second subject-generated relevance judgment. We do not have relevance labels for documents which users did not view; we aimed to disrupt natural behaviors as little as possible, and a participant who is asked to label every document may well process a result list differently.
On closing a document, the result listing was redisplayed, with a brief color-coded highlighting of the link that had just been viewed (green for  X  X seful X , red for  X  X ot useful X ). Users were free to access further result pages for each query, and to issue fresh queries for the task, but were not able to open pages in browser tabs, or to open new windows. In addition to the instrumented browser logging, gaze-tracking hardware was used throughout each trial, so that implicit user interactions could also be captured.

The experimental sessions proceeded as follows. First, the par-ticipant was asked to complete a brief demographic survey. Next they were shown descriptions of some information seeking tasks, similar to those used later in the operational part of the session, and asked to estimate the number of useful documents they thought they would need to find in order to address the information need. The answers to these questions provided a basis for estimating T .
Once a participant had completed the survey, they embarked on the operational study, and after working through the warmup query, were shown the first of six information needs. Participants were expected to complete each task before moving on to the next one, and were required to make their own decision as to when a task was  X  X one X , with no time limit applied. Their instructions were: You will spend approximately one hour doing a sequence of seven web search tasks. For each task, you X  X l be given a question and you should use our search engine to help answer it. . . . There is no time limit on each of the tasks, and no minimum time limit overall either. So spend what feels to be an appropriate amount of time on each task, until you have collected a set of answer pages that in your opinion allow that information need to be appropriately met, and then move on to the next task.

Search tasks are of differing levels of complexity, and it is rea-sonable to expect that user behavior differs too. The six topics used in the study were modeled on the classes and topics proposed by Wu et al. [25]; although users were not able to choose topics freely, this allowed us to control task complexity. In particular, participants were presented with two topics in each of the remember , understand , and analyze categories, representing tasks of increasing levels of cognitive complexity. The first page of results shown to each user for each topic was generated by a uniform  X  X tarter query X , also shown in Table 3.

Task order was a controlled variable for each participant, pre-sented in a Graeco-Latin permuted order. A second controlled experimental variable was the quality of the search results; for half of the searches, the list of answers returned from the commercial service was interleaved with related-but-incorrect snippets [15]. In this paper we only consider data obtained for searches in the first, unadulterated, half.

Gaze records from the tracker were first reduced to fixations  X  series of records lasting at least 75ms and within a 5 -pixel radius  X  to remove saccades and glances too short to indicate reading. Se-quences of fixations on the same snippet, with no intervening clicks, were then further amalgamated. These records were combined with logs from our search software and logs from the browser to produce a complete record of each user X  X  interactions.
A total of 37 participants were recruited, consisting of research students and staff from the Australian National University. Due to eye-tracking calibration and recording quality issues, the data of three individuals could not be included for analysis. Of the remaining n = 34 participants, 8 were female and 26 were male, with an average age of 26 years. All were fluent in English, although 50 % indicated that it was not their first language. The participants all held or were working towards degrees in the areas of computing, engineering, information science or mathematics. There was a high level of familiarity with searching across the participants: all indicated that they carry out a search using a web search engine several times a day, with a median 11 years of experience with online searching. No participants indicated that they were color-blind.
We now present some of the collected data. Section 5 analyzes what this data implies in terms of the five conjectures listed earlier. Estimating T Before carrying out any searches, participants were shown three sample information need statements  X  one of each of the three task types remember , understand , and analyze , but tasks which were not used in the remainder of the experiment, to minimize anchoring effects  X  and asked to respond to the statement:  X  I X  X  expect to need to find nn useful web pages to answer this  X . The distributions of responses for T are shown in Figure 2a. Given the increasing complexity of the task types, we expected that the estimated number of needed documents would increase from top to bottom. However, the responses did not follow this trend, and the only pairwise difference that was significant was between the understand and analyze categories. The deviation from the expected outcome may be a consequence of the particular three example queries that were shown, or of the fixed ordering in which they were presented ( understand , analyze , then remember ). Followup experimentation is required in which a broader palette of scenarios is provided, and presented in a varied ordering.

Details of the documents that participants saved as being  X  X seful X  were also collected. Figure 2b shows the distribution of documents saved per user, by task type. While the anticipated trend that more documents would be needed for tasks of increasing complexity is present, the actual numbers are small. Even for the two analyze tasks, the number of documents saved to  X  X llow that information need to be appropriately met X  is relatively low.
 User Gaze Behavior A key assumption in our discussion has been the broadly accepted claim that users scan search result pages from top to bottom, viewing snippets 1 , 2 , 3 , 4 , and so on. Figure 3a plots the mean first arrival time, measured by the number of previous views of other ranks, at each of the top 10 rank positions, following the methodology of Joachims et al. [14], and shows that on average the first viewing of the snippet at rank i is indeed correlated with i . This is consistent with previous work [14], and has often been interpreted as evidence that users scan links from top to bottom.
However, that outcome needs to be treated carefully. The gaze position of any individual user is much more volatile than Figure 3a would suggest, and the fixation point both sometimes moves back-ward and sometimes advances by more than one: a viewing sequence might well be 1 , 2 , 4 , 3 , 1 , 2 , for example. To quantify this tendency, the sequence of fixation points for each user was processed into a set of  X  X umps X : + 1 , + 2 ,  X  1 ,  X  2 , + 1 for the same example. Figure 3b shows the resulting distribution. Jumps of + 1 (one step down the ranked list) dominate, but a significant fraction of the fixation shifts are also by  X  1 and + 2 and larger jumps also occur.

We also see effects due to screen layout. Figure 3c shows the distribution of last-viewed ranks (that is, the ranks viewed just before a search ended). There are distinct peaks at ranks 7 (the bottom of the screen) and 10 (the bottom of the first page). Taken together, there is evidence for a wide variety of reading behaviors. Reading top-to-bottom is common, but not universal, and there are definite discontinuities. We have investigated this further in other work [23]. Continue Probability Our primary purpose in the experimentation was to explore factors that affected C ( i ) , the user X  X  probability of continuing their inspection of documents after viewing the document at position i in the ranking.

To determine an experimental value for C ( i ) , a  X  X id continue X  indicator variable DC was associated with each snippet fixation in the gaze log, taking the value zero if this was the last snippet viewed for this result page, and the value one if it wasn X  X . For example, for the sequence of snippet views 1 , 2 , 5 , 1 , 2 , 4 , 2 the inferred set of DC observations, categorized into groups according to rank position, h 1 i , and DC ( 5 ) = h 1 i . Laplace smoothing was then applied to allow empirical values to be estimated; for the same example data, to compute C est ( 1 ) = 3 / 4 = 0 . 75 , C est ( 2 ) = 3 / 5 = 0 . 60 , C 2 / 3 = 0 . 67, C est ( 5 ) = 2 / 3 = 0 . 67. Figure 4: Observed C ( i ) , averaged across queries and users.
Figure 4 plots estimated C ( i ) , averaged over all users and all queries. When presented this way, C ( i ) appears almost constant, with a value of approximately 0 . 75 . However, this seemingly-consistent gross behavior is an amalgam of many contributing fac-tors. The next section examines the composition of C ( i ) in detail.
Section 2 noted that weighted precision metrics, both static and adaptive, can be specified by any of the interchangeable functions the more confident we can be in the corresponding metric.
For the users and tasks measured, Figure 4 shows C ( i ) to be approximately constant when aggregated across ranks. If that is really the case, then the simple model behind RBP is applicable. If not, what other factors explain the variation? For example, if C ( i ) varies only with rank, the static model behind a metric like SDCG@ k or INSQ may be useful for evaluation. On the other hand, Table 4: Factors in a fitted model of DC. Users become more persis-tent (have higher C ( i ) ) as they issue more queries and look further down each result page; and become less persistent as they accumu-late relevant documents. if C ( i ) varies with the relevance of each document, we might be better off with an adaptive model such as that of RR or AP.
We used logistic regression to model DC, the fixation-by-fixation continuation indicator variable, as a response to a number of poten-tial explanatory variables including indicators of user, task, task type, and search progress. Model selection was performed by considering specifications between the full model (including all explanatory variables) and the minimal model including only the intercept. The final model was selected to minimize the Akaike information crite-rion (AIC) [1], which combines the likelihood of the model with a penalty for each term which is included. Evaluation was carried out using R  X  X  stats::step.glm method. The full list of variables eval-uated was: user, task, task type, gaze sequence, gaze rank i , judged relevance of the current document r i , number of relevant documents found, relevant documents viewed as a proportion of documents viewed, proportion of T collected, amount of T remaining, and query number within the task. We used participants X  own estimates of T , according to the task type (see Section 4.3 and Figure 2a).
Table 4 summarizes the factors in the built model. The column labeled  X  X ffect X  is the coefficient assigned to each of the listed factors in terms of the odds of continuing: for example, a user currently at rank i + 1 is 1 . 06 times more likely to continue to the next document than a user currently at rank i . Effects greater than one represent an increased chance of continuing, that is they increase C ( i ) ; effects less than one decrease C ( i ) as the corresponding factor increases. The outcomes in the table lead to several observations.
First, there is a large effect due to user  X  some users are simply more likely to keep reading than are others. The variability due to user is large, with a base value for C ( 1 ) varying from 0 . 57 to 0 . 99.
Second, when per-user variance is allowed for, there are two strong effects arising from the relevance of the documents already seen. The odds of a user continuing decrease sharply as they ac-cumulate relevant documents towards their target T . By the time a user has seen as many relevant documents as they thought they would need, the odds of their continuing have dropped by two thirds. Carterette et al. [6] note that user patience  X  the p parameter in RBP  X  varies with task type; the results here are similar, but show dependence not on task type in isolation, but rather, on the user X  X  notion of how much information they need.
 The left-hand graph of Figure 5 illustrates this effect. It plots C ( i ) as the proportion of T gathered is varied, and all other factors are held constant. The lines are estimates from the model for three hypothetical users chosen to match the median, first-quartile, and third-quartile (among the experimental subjects) of the base C ( 1 ) values. The shaded areas mark a 95 % confidence interval for C ( i ) . Table 5: Quality estimates for models of DC from three represen-tative families, plus the model developed in Table 4. The values listed for  X  AIC are the difference in AIC values relative to the set of factors listed in Table 4; lower values represent better models. Note that the logistic regression model estimates the change in odds; the reason that the factors have such different effects on the C ( i ) of particular users is therefore due to the different starting probabilities. A similar effect, but less pronounced, arises in connection with the proportion of documents viewed which have been judged useful (recall that in our protocol, every document which was viewed was also judged). Users reading  X  X nformation-heavy X  rankings have a reduced C ( i ) , as was anticipated in Table 2.

Third, there are two effects due to query behavior. As users look at more results in each query, they are slightly less inclined to continue (factor  X  X aze sequence X , with effect 0 . 97 ). Counteracting this, the users were more inclined to continue the more queries they issued against a particular task (effect size 1 . 10).

Finally, there is indeed a component of C ( i ) attributable to i . The effect is slightly above 1 . 0 , meaning that all other things being equal, users are a little more likely to keep reading from a deeper result than from a shallower one  X  perhaps deeper results are not as good, and users are likely to look back to a better result before finishing. The effect due to rank is illustrated in the plot on the right side of Figure 5, for the same three hypothetical users.

We did not observe an effect due to task: that is, the task itself does not seem to affect C ( i ) , except indirectly via the participants X  estimates of T . Models which included task type and task instance, as well as the combination, did not improve on the model in Table 4. In fact using those factors alone did not improve on the simplest model which holds C ( i ) constant.

The effects of query count, and those due to relevance, accumulate over a search session. Given a better understanding of users, and more extensive data, we might expect that other effects of this kind could exist  X  spanning result pages, queries, and longer durations. Session-level or longer-term effects are not included in the user models behind any of the commonly-used metrics.
The model summarized in Table 4 is interesting, but also more complex than the models in Section 2. It is important to ask whether it is needed, or if the simpler models suffice. To resolve this ques-tion, models were also constructed for each of three simple classes, representing three families of effectiveness metrics.

Table 5 compares the fit of these models, based on the Akaike information criterion (AIC). The values listed as  X  AIC are the differ-ences between the AIC value for each model and the AIC value for the best learned model (Table 4): higher values indicate a less parsi-monious model, amongst the set of models presented. Differences above about 10 can be interpreted as indicating a model having  X  X ssentially no empirical support X  [4], so we can have a great deal of confidence that the fitted model improves on the alternatives.
The  X  X BP-like X  model computed in this way just assigns C ( i ) = 0 . 93 , and it is the simplest model. The static  X  X DCG-like X  model allows C ( i ) to vary as a function of i , but with a different effect allowed for each i so that C ( i ) can take any shape. This differs from SDCG itself, but is more flexible: in particular, it allows for discontinuities at the fold and the end of the page. This, however, produces a relatively poor fit, worse that the RBP-like approach. Finally, the adaptive  X  X R-like X  model allows C ( i ) to vary with r and this produces the lowest AIC (that is, the best fit) of the three families. However, none of the three are as powerful as the learned model. (It was not possible to produce an  X  X P-like X  model, as Equa-tion 6 requires relevance judgments of all documents in the result set including those which are never viewed. Post-hoc judgments were not performed, so the required data was not available.)
Since per-user effects are so large, we also fitted variants of the RBP-like, SDCG-like, and RR-like models which included a feature for user. This was not sufficient to improve the three models (  X  AIC = 51 X 52).

The AIC-based analysis confirms that better fidelity could be attained by using not just adaptive models, but models which ex-pressly allow for differing relevance targets T , and which adapt their behavior as T is approached. Indeed, adding  X  X roportion of T  X  as a factor to the RBP-like model explains DC better than the rank-varying SDCG-like or the relevance-based RR-like models (  X 
AIC = 28).
We can also ask whether the model of Table 4, which was built from observations of real users, aligns with the five hypotheses of Section 3, paraphrased as: 1. Users search for different reasons, with different targets. In 2. Users may wish to examine documents to arbitrary depth. In 3. Users may be more likely to continue the more they have 4. Users may alter their behavior based on what they read. This Figure 6: Proposed user model for search. For a given metric M, the quantity C M ( i ) is a function of T i and i . 5. Users may exit from their query at any time. Again, C ( i )
To complete the development, we describe a metric which is in-spired by the hypotheses and observations above. We do not suggest that this is the last word  X  it merely represents one more position in spectrum between simplicity and fidelity. It does, however, embody all five of the user behaviors that were hypothesized in Section 3, and in that sense can be regarded as providing proof-of-concept.
As noted in Section 2, the user model associated with INSQ (Equation 4) meets the first three listed conjectures. In particular, the parameter T in Equation 4 captures conjecture 1, and makes INSQ intent-sensitive, since T can be thought of as being an estimate of the number of relevant documents the user seeks to acquire. For a navigational query, T = 1 or T = 2 perhaps; and for an informational query, T = 5 or T = 10 might be more appropriate.

Figures 6 and 7 (the latter derived from Smucker and Clarke [21]) propose an extended model for INSQ that makes the metric adap-tive (conjecture 4) and session-based (conjecture 5). When a user examines an answer at rank i in the results list, they first read the snippet. Based on the snippet, a decision is made: to click through and read the underlying document, or to not click. In the latter case, the user is finished with the document. Importantly, from the user X  X  point of view, this means that the document is non-relevant ( r If the user clicked, they then read the full document, leading to two possible outcomes: it is useful, in which case r i = 1 ; or it is not, in which case r i = 0 . There are two key differences between this proposal and the similar state diagram presented by Moffat and Zobel [18]. The first is, as already discussed, the alteration of the conditional continuation probability at depth i from being a fixed value p (used in RBP) to a variable value C M ( i ) ; the second is that we introduce the possibility of the user reformulating a revised or substitute query as part of the same search session.

Figure 6 proposes that T be modified as the search proceeds. The user enters the ranked list with T 0 = T , their initial intention; but as documents are viewed, their information need is partially or fully satisfied, and their intention evolves. We propose that this evolution be modeled by computing where Rel ( i ) =  X  i j = 1 r j ; that is, T i estimates the  X  X urrent unmet demand for relevance X  after inspecting i documents in the ranking.
In a general metric it is probably not feasible to have a parameter per user, but we do want to incorporate the most important of the user-independent factors from Table 4: the proportion of T collected. Call this T prop = Rel ( i ) / T : now as T prop increases, C ( i ) should decrease. Equivalently, as 1  X  T prop decreases so should C ( i ) .
To add this notion to INSQ, and thereby create an adaptive version, we use T + T ( 1  X  T prop ) rather than 2 T to compute the conditional continuation probability. Since 1  X  T prop = T i / T , we have: The effect is that initially, T i = T and the model is as in Equa-tion 4. While T i remains high, so does the conditional continuation probability; then, as T i decreases and the user X  X  information need is increasingly satisfied, so the likelihood of continuation also de-creases, and the expected length of the remaining search decreases. If no relevant documents are accumulated, Equation 7 remains the same as Equation 4.

The probabilistic nature of searching using this model means that the user might exit their search before T i reaches 0 . If this happens, they can be expected to reformulate their query and start inspecting the new ranked list, or switch to a different search service, or just quit. In the case of the search being continued, we suggest that their initial T 0 for the follow-on query is inherited from the T was attained at the time the previous query was abandoned. The diagram in Figure 6 captures this aspect of the proposed model.
Clearly this is only one of any number of metrics which might be developed based on the model in Table 4 and pending further investigation we offer it as a proof-of-concept. Nevertheless a repre-sentative of this metric, modelling DC with i and T i / T , achieves a  X 
AIC of 28: this is substantially more accurate than others in Table 5.
There is a considerable literature in regard to effectiveness metrics, and in the limited space available, we can at best present a brief overview. The current definition of AP emerged in the 1990s as an evaluation measure associated with the deep rankings (often to depth k = 1 , 000 ) in the TREC project. J X rvelin and Kek X l X inen [13] then introduced the DCG and NDCG metrics (the latter not considered here), arguing that explicit top-weightedness was preferable to the way it was achieved in AP, and formalizing the notions of graded (that is, non-binary) relevance judgments, and of  X  X ain X  as a benefit received by the user. Moffat and Zobel [18] followed up with their RBP proposal and corresponding user model; an important recognition in this work is that it is preferable for the metric to assess the rate at which gain is accrued, rather than the total magnitude of gain that is accrued. Zhang et al. [27] considered a range of static weighted-precision metrics including SDCG and RBP, and show that RBP with p = 0 . 73 is a good match to the normalized document viewing characteristics inferred from the click densities arising from commercial search operations.
 A flurry of activity has taken place over the last four to five years. Craswell et al. [9] proposed a cascade model of click behavior (which we refer to as  X  X daptive X  here), which posits that users read search results from top to bottom, and at each rank make a decision of whether to click on that document, or to skip it. Once a single document is clicked, the user exits from the search. Unlike previous models of click behavior, the cascade model takes the relevance of answer items that are higher in the results list into account. Craswell et al. demonstrated a closer fit to click data from a commercial search engine than other models of user behavior, such as a position model (where click behavior depends only on the rank of a document, called  X  X tatic X  here), or an examination model (where clicks are a function of rank as well as document-specific factors). Chapelle and Zhang [8] extended the cascade model using a dynamic Bayesian network, allowing for the possibility that a user is not satisfied after a single click and returns to the search results list. In related work, Chapelle et al. [7] further argued that the history of what the user experiences as they process the answer list affects the way they address the remainder of the list. They synthesized a new metric ERR by combining the geometric distribution used in RBP with the approach embedded in RR, adapting to r i and also, via the choice of p , allowing persistence to be tailored to the search requirement. Yilmaz et al. [26] also explored metrics in which the probability of continuing the inspection of documents is conditional on the relevance level of the last document inspected.

Carterette [5] categorized a wide range of effectiveness metrics, grouping them into four classes; in doing so, the relationships be-tween weights, halting probabilities, and last viewed probabilities was raised, an understanding that we have also employed in this work. Carterette then explored the implications of the classification using a range of click and TREC data, concluding that despite the fact that it is a non-convergent sum, DCG has a range of merits.
Recently Smucker and Clarke [20, 21] have measured the time taken by users to inspect documents, and argued that a more pre-cise unit of  X  X nvestment X  against which utility is assessed should be search time, rather than documents examined. In a user study of search behavior, Smucker and Clarke demonstrate that short doc-uments require less inspection time than do long ones, and that repeated documents can be evaluated very quickly. Based on these, and other factors, they proposed time-biased gain as an effectiveness metric, and argued that it better reflects user search behavior.
In other interaction studies, Joachims et al. [14] examined the way in which user gaze fixations can be associated with result list-ings; Al-Maskari et al. [2] (see also Huffman and Hochster [12]) questioned the usefulness of deep evaluation metrics, and found that shallow metrics such as Prec@ 10 provide better correlation with the experience reported by users; Turpin and Scholer [24] raised doubts about the usefulness of AP to predict user task-completion ability; and Thomas et al. [22] examined the numeric stability of static met-rics when applied to perturbed or degraded rankings; they also note that page boundaries can be handled by altering the continuation probabilities at appropriate intervals.

Other authors have considered user models for metrics in more abstract settings: as already noted, Robertson [19] described a user model associated with AP, with Dupret and Piwowarski [11] pro-viding elaboration; Dupret [10] has also examined alternative user models for the DCG metric.
Metrics such as Prec, AP, and RR instantiate user models, implic-itly or explicitly, and these models can be described by any one of the interchangable functions W () , C () , or L () . This leads us to ask: what do these functions look like for real users? Is one model (or family of models) more or less accurate than others, and can that tell us anything about the associated metrics?
We conducted a user study with the goal of identifying the factors that contribute to C ( i ) , a user X  X  propensity to continue searching after viewing a document at rank i . Factors we thought would be of possible influence included the rank position i ; the relevance r of the document in that position; the amount of relevance collected until that point in the viewing sequence; and the fraction of the target relevance that had been collected. Our study of 34 users, each completing six search tasks, provided evidence that all of these factors are indeed contributors to the decision made by the user after viewing each snippet, namely whether to continue or stop.
Unsurprisingly, our investigation has also generated fresh ques-tions to be considered. First, Figure 6 contains provision for query reformulation and/or query reissue. But our sample of user activity contained too few such events for us to try and derive meaningful insights as to what factors drive these decisions, and we can only conjecture that the unanswered information need, T i = T  X  Rel ( i ) is positively correlated with the conditional probability of a reformu-lated query being issued. For example, the conditional probability of reformulation, given that the user is not continuing down the ranking, might be modeled as being some kind of T i / ( T i + k ) -like function, where k is a constant. A larger user study would be required for that conjecture to be explored.

Second, we have defined r i in terms of the user X  X  response to the document  X  if it was marked as being  X  X seful X  to answer the information need, then we regard the corresponding r i value as being 1 . But two users viewing the same ranking make different decisions about what to view, and even if they view the same documents, make different decision about whether to save it. Hence, r i itself must be thought of as being non-deterministic, introducing a further degree of freedom into any user model. The  X  X tarter query X  results may yield insights into this issue, and allow estimates to be made of the degree to which users differ.
 Acknowledgments This work was supported by the Australian Research Council. We thank Dingyun Zhu for his help running the user experiments.
