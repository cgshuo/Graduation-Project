 Among time series data mining tasks, the classification has attracted amount of interest during last decade. Actually, many studies on time series classification methods have been proposed and it is suggested that Nearest Neighbor classi-fier (especially, 1-NN) is difficult to beat [ 1 , 3 ]. Since the performance of 1-NN algorithm depends critically on the distance metric given for specific tasks, the subsequent question then becomes how to determine the distance metric for so many applications.
 A number of different distance metrics have been proposed. Among them, two of the most widely used are Euclidean distance and Dynamic Time Warping a good performance for certain applications. In contrast, DTW introduces the alignment of two sequences and allows the points of different time stamps to match, which leads to even better performance than Euclidean distance for some scenarios. However, one of the deficiencies of DTW is that it needs more time cost when calculating the distance. Also, even though 1-NN with DTW can achieve best performance in many domains, for some other applications, it performs no better than other distance metrics. In summary, current evidence shows that there is no distance metric that is best for all time series data [ 3 ]. Typically, the choice of distance metric has to be determined empirically, which is time expensive and not always effective. Hence, we believe that it X  X  a challenge to choose a suitable distance metric for the specific data set automatically. learning to obtain better distance metric and further to improve the classifica-tion performance for time series data. Indeed, many distance metric learning methods have been proposed. For instance, [ 4 ] provided a linear transformation model named Neighbourhood Components Analysis (NCA) to optimize the per-formance of k -NN in the learnt low-dimensional space. As [ 19 ] noted, the linear transformation has a limitation that  X  X t cannot model higher-order correlations between the original data dimensions X . Hence, [ 19 ] proposed a nonlinear dis-tance metric learning model named Nonlinear NCA (NNCA). The discovered low-dimensional representations could work better than previous linear NCA. Unfortunately, both Linear NCA (LNCA) and NNCA models cannot capture the intrinsic property of the time series data, i.e., time shift.
 Convolutional Neural Network (CNN), e.g., invariance of spatial-temporal, and propose a novel distance metric learning method for time series. Specifically, we follow NNCA model [ 19 ] and propose a novel Convolutional Nonlinear Neigh-bourhood Components Analysis (CNNCA) model, which could not only learn a nonlinear transformation from the data but also naturally capture the time shift of sequences. Based on the learnt distance metric, 1-NN classifier would be used to perform the classification. Moreover, we conduct comprehensive exper-iments on the data sets from UCR Time Series repository [ 7 ]. By comparing to conventional Euclidean distance, DTW and window constraint DTW, the experimental results reveal the classification performance is improved for many data sets, especially for the data sets that have sufficient training samples for each class. On the other hand, we also evaluate the efficiency of each method. It reveals that CNNCA is more efficient for larger data set and long time series. We summarize the contributions of this paper in these parts:  X  Though there are several studies that have explored the distance metric  X  Along this line, we propose a novel distance metric learning method CNNCA concatenating CNN and Multiple Perceptron (MLP), and then learn a dis-tance metric based on the scheme of stochastic neighbour assignments.  X  We conduct comprehensive experiments on amount of public data sets, then compare the performance of CNNCA with other distance metrics, including not only three conventional distance metrics, but also two learnt by LNCA and NNCA. The results prove that CNNCA can improve classification accu-racy to some extent, especially for the relatively large scale data sets. The rest of this paper is organized as follows. Section 2 shows the related studies. Definitions of time series and relevant distance metric learning methods are given in section 3 . In section 4 , the CNNCA is introduced and comprehensive experiments are presented in section 5 . Finally, we conclude the paper and give the future work in section 6 . We group the related studies into two categories. In the first category, researchers focus on improving the performance of time series classification by choosing dis-tance metrics combined with 1-NN classifier. As [ 1 , 16 , 22 ] claimed, the Nearest Neighbour (NN) classification algorithm (especially 1-NN) has been empirically proven as the current state-of-the-art [ 1 , 16 , 22 ]. Then the challenge of 1-NN is how to determine the distance metric for specific data sets. Extensive experi-ments have been conducted by [ 3 ] on amount of time series data sets and many distance metrics have been evaluated, i.e., Manhattan distance, Euclidean dis-tance, L  X  -norm, DISSIM, DTW, LCSS, EDR, Swale, ERP, TQuEST, SpADe [ 3 ]. According to the experimental results, they concluded that there is no clear evi-dence that there exists one similarity measure that is superior to others for most of data sets. Hence, for specific data set, it is challenging to determine a suitable distance metric for better performance.
 In the second category, researchers concentrate on the distance metric learn-ing (or manifold learning). Essentially, the aim of distance metric learning is to learn either a linear or nonlinear transformation based on the original data for instance, [ 4 ] proposed a method by optimizing the expected leave-one-out error of a stochastic nearest neighbor classifier in the projection space, which can learn a linear distance metric to be used for data visualization and fast classifi-cation. [ 19 ] said that the linear transformation cannot capture the higher-order correlations between original data dimensions and proposed a nonlinear NCA model, which stacks multiple neural networks to learn the nonlinear transforma-tion for handwritten digit recognition task. To the best of our knowledge, there are only several existing studies using distance metric learning on time series classification. For instance, [ 15 ] considered to learn a variation of Mahalanobis distance and performed the time series classification with 1-NN algorithm. They concluded that such a kind of distance is inferior to DTW in accuracy but it is more efficient. Recently, [ 12 ] proposed two novel models to learn a task-specific similarity measure for time series data, however, the transformation is still linear. ear cannot capture the time shift property well, thus the performance of time series classification are suffered. Motivated from the nonlinear distance metric learning and utilizing the merit of CNN, we will propose a convolutional non-linear NCA model to learn a better distance metric for time series, and further improve the performance of classification. In this section, we provide preliminaries for our work. Specifically, we first give the definitions of time series and subsequence. Then, two related distance metric learning models are explained. 3.1 Definitions of Time Series and Subsequence Definition 1 A time series (denoted as T ) is a sequence of data points, mea-sured typically at successive points in time spaced at uniform time intervals. A time series can be denoted as T = t 1 ,t 2 , ..., t n ,and the long time series instead of classifying time series with the whole sequence. Then, we proceed the classification with these subsequences, since the pattern or shape in the subsequences of time series could be a key feature to distinguish different classes of time series. The subsequence is defined as follows. Definition 2 Subsequence is a series of consecutive points which are extracted from a long time series T and could be denoted as s = t i is the length of subsequence, and we have 1  X  i  X  n , 1  X  k  X  n tance, DTW and window constraint DTW. Due to space limitations, we skip the details of these distance metrics (which could be found in [ 3 , 22 ]). 3.2 Distance Metric Learning Many distance metric learning methods have been proposed during last decade [ 4 , 19 ]. In this paper, we concentrate on two preliminary methods on Neighbor-hood Components Analysis (NCA), i.e., linear and nonlinear NCAs.
 Linear Neighbourhood Components Analysis (LNCA). Based on stoch-astic neighbour assignments in the transformed space, [ 4 ] introduced a differen-tiable cost function for learning neighbour components analysis. Specifically, for each point x i , it selects another point x j as its neighbour with the probability p ij , and furthermore, same probability. In the softmax scheme, the definition of distance is shown in Equation 1 . where A is the matrix that needs to be learnt for transforming the input data lin-early. Based on such a stochastic neighbour assignments scheme, the probability that point x i would be classified correctly is computed as follows. where C i represents the set of points that have same class label as point denotes the class label of point x i then we define this set as objective function of LNCA is shown in Equation 3 , which is also the expected number of points that is correctly classified.
 To maximize the objective function, the common method is to use a gradient based optimizer according to the derivative of L . When we denote that x  X  x Nonlinear Neighborhood Components Analysis (NNCA). The limita-tion of linear transformation is that it cannot capture the higher-order correla-tions between original data dimensions [ 19 ]. Based on LNCA and by introducing a multilayer neural network, [ 19 ] proposed a Nonlinear Neighborhood Compo-nents Analysis (NNCA) model.
 In contrast to Equation 1 , for NNCA model, the probability that point selects one of its neighbours x j and inherits the class label of Equation 5 . where f (  X  ) is the nonlinear transformation learnt by a multilayer neural network, which is different from the linear transformation of LNCA in Equation 1 (i.e., Ax ). Besides that, the subsequent process of NNCA model is similar to LCNA as shown in Equation 2 and 3 , which includes the probability that point to a certain class z and the objective function. The optimization of the objective function is performed with gradient ascent method. Denote derivative of L with respect to f ( x i ) is derived as: Through computing gradient and iterating to update the parameters, then we could obtain the nonlinear transformation when the model convergent. it does not consider the time shift and still cannot capture the intrinsic property of time series. Therefore, for time series classification, both of LNCA and NNCA cannot achieve good performance. We will verify this in the experiments. In this section, we show the novel distance metric learning model CNNCA, including the architecture and the learning procedure. Meanwhile, we explain how to perform classification with CNNCA at the end of this section. 4.1 Architecture We follow the scheme of NCA model and extend the nonlinear NCA model for subsequent classification. Specifically, we propose a novel Convolutional Nonlin-ear Neighborhood Components Analysis (CNNCA) model to learn a better dis-tance metric for time series. By consideration of time shift property of time series, the motivation of introducing CNN into distance metric learning is that convolu-tional and pooling operations can preserve the spatial and temporal locality, i.e., CNN has the advantage of time shift invariance to some extent [ 9 ], which may improve the performance of subsequent classification. Furthermore, MLP can combine the feature representations learnt by CNN and perform nonlinear trans-formation for better classification. Hence, CNNCA extends LNCA by combining CNN and MLP, in other words, the distance d ij between two projected points with respect to x i and x j , is calculated in this form: f (  X  ) defines a nonlinear transformation through convolutional neural networks and multilayer perceptron. We illustrate the architecture of CNNCA model in Fig. 1 . The probability that point i belongs to class z depends on the relative proximity for all other points that belongs to class z , which is the same as NNCA that was shown in Equation 2 . Moreover, similarly, the distribution of distance p is formalized and was shown in Equation 5 . The objective function of CNNCA is identical to that of linear and nonlinear NCA in Equation 3 . Our aim is to maximize this function, from another perspective, L is the expected number of correctly classified points for the training data. 4.2 Optimization Based on conventional backpropagation algorithm, to update the parameters iteratively, feedforward computation and backpropagation need to be performed alternatively until the model converges.
 Feedforward Pass. The feedforward pass aims to perform the nonlinear trans-formation from the input time series to the final low-dimensional space. Con-cretely, we use CNN to learn the features and then feed the output feature maps into a MLP, the purpose of which is to combine of the learnt features and obtain a good distance metric at the final layer. For the traditional CNN, it could consist We briefly recall the process of these three layers, i.e., filter (convolutional), activation and pooling layers. where  X  denotes the convolutional operation, pool (  X  ) represents the function used in pooling layer, and  X  (  X  ) represents the activation function. Besides, x denote the input and output of filter layer and the superscript and x j denote the input and output of pooling layer. For pooling layer, average and max pooling strategies are most widely used [ 13 , 20 ]. While the activation max pooling and ReLU function in this paper due to their good generality and fast convergence [ 13 , 14 , 20 , 23 ].
 After CNN, we also use a 2-layers fully-connected MLP to combine the learnt features, since the feedforward pass of MLP is standard and the space consump-tion is limited. More details of MLP can be referred to [ 10 ].
 Backpropagation Pass. In this paper, we utilize the backpropagation algo-rithm to train the CNNCA model. Specifically, once the loss function acquired, then based on the chain-rule of derivatives, the error can be prop-agated back from layer to layer reversely. Here, the derivative of to f ( x i ) is the same as that of NNCA model, which is already shown in Equa-tion 6 . Then the error could be propagated back to the conventional MLP based performed layer by layer reversely [ 2 , 24 ]. 4.3 Classification with Distance Metric Learning We adopt an objective and widely used evaluation method in this work [ 6 ], which uses 1-NN classifier on labeled training data to evaluate the classification accu-racy of the distance metric used. Each time series has been labeled with correct class in both of training and test sets. 1-NN classifier tries to find the nearest neighbour of input and predict its class label as that of nearest neighbour. For distance metric learning framework, once we have learnt the transformations, according to specific models (LNCA, NNCA, CNNCA), we first transform the test data and training data. Then, 1-NN classifier would be applied on the trans-formed training and test data for further classification. In this way, the better the distance metric the lower the classification error should be observed. In this section, we conduct experiments on a bunch of public time series data sets, and we demonstrate: 1) the classification accuracies/errors with respect to different distance metrics i.e., CNNCA and other existing distance metrics; 2) the comparison of classification performance on the largest 9 data sets with more training samples; 3) the efficiency analysis and discussion. 5.1 Experimental Setup We conduct comprehensive experiments on 39 diverse time series data sets, pro-vided by UCR Time Series repository [ 7 ], which is shown in the first column of Table 1 . As claimed by [ 3 ], these 39 diverse data sets could make up approx-imately more than 90% of all publicly available, labeled time series data sets. Besides, the preprocessing was also applied, e.g., standard normalization was formed for each data set and the maximum scale of each time series is 1.0. Warping (DTW) and window constraint DTW (denoted as DTW( r the percentage of time series length) are competitive distance metrics for time rics as baseline methods of our CNNCA, and they include ED, DTW, DTW( and two of the related distance metric learning models LNCA and NNCA. All the six distance metrics combine 1-NN to perform classification. 5.2 Experimental Results Overall effectiveness. The experimental results are shown in Table 1 .Six rightmost columns of this table exhibit the classification error with respect to these different distance metrics. Bold number accompanied with star symbol of each row indicates the best result for the corresponding data set. For all 39 data sets, our CNNCA model achieves the best results on 13 out of them, which is more than that of ED (3), DTW (9), LNCA (2), NNCA (6) and equals to that of DTW( r ). It reveals that our CNNCA model is competitive not only to conventional ED, DTW and DTW( r ) but also to LNCA and NNCA models. Especially, it is superior to ED, LNCA and NNCA for most of the data sets. To illustrate the performance of these different distance metrics more intu-itively compared to Table 1 , we also provide some scatter plots in Fig. 2 to depict the pair-wise comparisons between CNNCA and the baseline distance metrics. For each of the scatter plots, the vertical ( y ) and horizontal ( CNNCA and the compared distance metrics, which are denoted as  X  X  X  and  X  X  X  (e.g., ED), respectively. The classification error ratio of two distance metrics under comparison for certain data set is a point that locates at certain coor-dinates ( x, y ). Considering that we use classification error but not accuracy to compare the performance, if the classification error ratio (i.e., the point ( locates above the diagonal line (red line in 2 ), then it indicates that  X  X  X  is more accurate than  X  X  X , i.e., x&lt;y . Moreover, the further point ( diagonal line, the greater the margin of classification accuracy being improved. Otherwise, when  X  X  X  is more accurate than  X  X  X , and point ( below the diagonal line, i.e., x&gt;y . All the points that locate at diagonal line indicate that they achieve identical classification error on these data sets, i.e., x = y . Besides, more points on one side of the diagonal line indicates that one distance metric is more superior to the other.
 tional ED, DTW, DTW( r ), the results in Fig. 2 reveal that CNNCA is superior to ED on most of the data sets, which demonstrates that such a learnt distance metric can improve the classification accuracy to some extent. However, on total 39 data sets, there is no evidence that either DTW (or DTW( worse than CNNCA, even though window constraints DTW is a little better than DTW. Moreover, by comparing the classification accuracy between CNNCA and LNCA, NNCA, the results show that CNNCA outperforms both of LNCA and NNCA on most of the data sets, which provides the evidence that CNNCA is more effective than previous NCAs just as our expectation.
 Effectiveness on Large Data Sets. We provide both the number of classes and the size of training set in Table 1 (in the second and the third columns) and furthermore the average number of training samples per class is calculated as shown in the forth column. As is well known, if the training samples of each class are too few then neural networks cannot capture the features well and may obtain a poor performance. Motivated by this, we filter out the data sets that has few training samples per class, i.e., eliminating the ratio that is no larger than 40 as shown in Table 1 . Finally, we obtain 9 data sets (the top 9 rows of Table 1 ). Likewise, we provide Fig. 3 to depict the comparisons between CNNCA and the baseline distance metrics on these 9 data sets. From both the top rows of Table 1 and the results in Fig. 3 , we could observe that our CNNCA model is superior to all other methods on 8 out of these 9 data sets, which demonstrates that if the training samples are sufficient then our CNNCA model could achieve good performance and outperform the baseline methods.
 Efficiency Analysis. Supposed that the size of training set is time series of length D , then the time complexity of ED, DTW and DTW( O (
ND ), O ( ND 2 )and O ( ND 2 r ), respectively, when we apply dynamic program-ming to compute DTW. Usually, r is no larger than 10% for most applications. Before analyzing the time complexity for LNCA, NNCA and CNNCA, we should note that we only focus on analyzing the online classification of them and skip the offline training process due to the limited space, and it is necessary to define some notations. One hidden layer NNCA is considered for convenience and the number of its hidden neurons sets to  X  n h . For CNNCA, the number of kernels in filer layer and hidden neurons in MLP are denoted as n k and Moreover, the size of kernel and the pooling factor are usually set to 5 and 2. We use d to represent the dimensions of transformed space. To classify each test case, the time complexity of LNCA, NNCA and CNNCA are O (
D  X  n h +  X  where O ( N d ) is the time cost of 1-NN on the transformed data and the remain-der is the transformational cost. After reduction, the time cost of CNNCA is O ( n
D + D n cient for LNCA, NNCA and CNNCA compared to conventional ED, DTW and DTW( r )ifwefix  X  n h , n h , n k to constants. We also provide the real time cost of classification on the top 9 data sets in Fig. 4 , the data sets in which are ordered by the product of D and N increasingly. It reveals that CNNCA is more efficient for larger data set and long time series, i.e., either N or Discussion. In summary, the overall experimental results demonstrate that CNNCA is competitive to not only conventional ED, DTW and DTW( also LNCA and NNCA. Especially, after filtering out the relatively small data sets, our CNNCA is superior to all the other distance metrics, which verifies the motivation that CNNCA can capture the intrinsic features and improve the classification performance if the training samples per class are sufficient. By comparison with both of LNCA and NNCA, we also demonstrate that CNNCA is more effective than them to some extent, which is benefited from the capability of capturing time shift property. On the other hand, CNNCA is more efficient than DTW and DTW( r ) when the data set grows large enough and time series is long, e.g., for three large data sets, Thorax1, Thorax2 and StarLight in Fig. 4 . In this paper, we proposed a novel CNNCA model for time series classification. Specifically, we extended the NCA model with CNN and MLP to learn distance metric and then combined 1-NN to classify time series. The benefit of introducing CNN into NCA is to get good feature representations for further classification, and MLP is used to combine these learnt features and obtain nonlinear trans-formation for better distance metric. For evaluation, we conducted experiments on a bunch of public time series data sets, and observed encouraging results. In particular, CNNCA is superior to current state-of-the-art methods when the training samples are sufficient. We hope this work could lead to many future studies. Actually, we plan to investigate better methods based on CNNCA and further improve the performance (e.g., efficiency) of time series classification.
