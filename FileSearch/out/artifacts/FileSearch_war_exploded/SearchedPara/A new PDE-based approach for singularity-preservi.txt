 ORIGINAL PAPER Fadoua Drira  X  Frank LeBourgeois  X  Hubert Emptoz Abstract The massive digitization of heritage documents has raised new prospects for research like degraded doc-ument image restoration. Degradations harm the legibility of the digitized documents and limit their processing. As a solution, we propose to tackle the problem of degraded text characters with PDE (partial differential equation)-based approaches. Existing PDE approaches do not preserve sin-gularities and edge continuities while smoothing. Hence, we propose a new anisotropic diffusion by adding new con-straints to the Weickert coherence-enhancing diffusion filter in order to control the diffusion process and to eliminate the inherent corner rounding. A qualitative improvement in the singularity preservation is thus achieved. Experiments con-ducted on degraded document images illustrate the effective-ness of the proposed method compared with other anisotropic diffusion approaches. We illustrate the performance with the study of the optical recognition accuracy rates.
 Keywords Degraded text characters  X  Document images  X  PDE-based approaches  X  Optical character recognition  X  Restoration  X  Reconstruction  X  Enhancement more flexibility [ 9 , 10 ]. Buades et al [ 5 ] pointed out that the non-local means filter is a special case of the bilateral filter which gives better edge-preserved denoising results. The concept of the exemplary-based approaches was also successfully introduced in image denoising [ 11 ] and image deblurring [ 12 ]. The field of experts (FoE) model is another interesting approach recently used in image denoising [ 13 ]. By introducing this model, classical Markov random field (MRF) models become more powerful. All of the previously mentioned approaches use the natural redundancy of infor-mation in images to remove the noise. We refer the reader to [ 14 ] for the state-of-the-art denoising results.
The application area of restoration includes for example image surveillance and medical images where the imaging system introduces a light distortion. Most of these works model a degraded image as an original image that has been subjecttolinearfrequencydistortionandadditivenoiseinjec-tion. These two sources of degradations are the most studied. Faced with old documents, other types of degradations, such as the mixture of information (ink-bleed-through, spots), loss of information (gaps, discontinuities) and the pattern deformation (geometric deformation, noise around contours, unwanted jagged edges), appear to lead to other res-toration techniques adapted to them. In fact, document images with common noise models can be enhanced using traditional image restoration techniques such as median, Weiner [ 15 ] and morphological filtering [ 16 , 17 ]aswellas multi-resolution pyramid and fuzzy edge detectors [ 18 ], etc. Nevertheless, in practice, these popular models are not well suited to degradations arising from phenomena such as doc-ument aging or ink-bleeding. Specific degradation models for both document [ 19 ] and text [ 20 , 21 ] are thus introduced. For reconstructing low-resolution text images, H. Luong et al. [ 22 ] exploited the repetitive behavior of the characters on the whole text image and not only on a local surrounding neighborhood. J. Banerjee et al. [ 23 ] introduced a probabi-listic context model using an MRF to deal with severe doc-ument degradations including cuts, blobs, merges, and van-dalized documents. The loss of information is often repaired by diffusion approaches like inpainting and anisotropic dif-fusion. The mixture of information is generally restored by the separation of sources (ICA [ 24 ], Markov model [ 25 ], diffusion [ 26 , 27 ], classification or segmentation techniques [ 28 , 29 ]). As for pattern deformations, we can use geomet-ric restoration, mathematical morphology [ 30 ], and even local/non-local smoothing filters [ 31 ].

The main topic of this study is restoring degraded text characters, which consists of repairing the shapes of the features as well as extrapolating lost information. Text res-toration remains a challenge due to the variety of fonts, col-ors, and even to the presence of complex backgrounds [ 28 ]. Moreover, text character degradations are characterized by a random loss of information, which makes their processing 2 A study of document image degradations 2.1 Problem statement The term degradation means, as suggested by Henry S. Baird [ 34 ],  X  X very sort of less-than ideal property of real docu-ment images X . In fact, document image degradations could be divided into different sets according to their origin. We can distinguish, for example, defects stemming from time effect as well as digitization. By time effect defects, we refer to all defects related to bad environmental conditions mainly the humidity caused by unfit storage for many years. We notice that once the old document is scanned, the previous defects become part of the digitized image document. During the scanningprocess, other types of degradations couldbeadded. For instance, digital degradations are generated through the use of some image processing techniques (high compression rate, low-resolution, binarization,...). Mechanical degrada-tions refer to bad digitizing conditions, defected material, low scanner performance, etc. The digitization process could be  X  X irect X  if it is done from the original document or  X  X ndirect X  in case a copy is used (microfilm, negatives, photography, printed documents...). It is obvious that  X  X irect X  digitization is more complicated than  X  X ndirect X  digitization; yet it gen-erates images of better quality. Examples of degraded docu-ment images are given in Fig. 1 .

In order to deal with virtual restoration, [ 1 ]presentsa typology of document image degradations defined according to the further treatment that will be undertaken. It is decom-posed into two classes: uniform and non-uniform degra-dations. Uniform degradations alter the entire document. This could be illustrated by geometrical degradation and non-uniform illumination, common phenomena in document analysis resulting from the scanning of thick documents. As for the non-uniform degradations, they only affect local parts in the document. Degradations as such could affect either 2.2 Discussion Noisy foreground degradations, compared with the other types of degradations, face real problems that have not yet been resolved. Indeed, the existing state-of-the-art presents some contributions in this field defining a model reflecting character degradations. However, no generic model describ-ing all possible distortions has been published [ 34 ]. More-over, virtual restoration using a particular model requires the definition of an hypothesis applicable only to very spe-cific conditions of acquisition. A simple change of conditions taken into account degrades the performance of the model. Another solution, known by its ability to model the images in a continuous space, is based on partial differential equa-tions. This solution has been successfully evaluated for the restoration of natural images and especially in image denois-ing or noise removal. This treatment is necessary to facilitate further processing and even to improve the visual quality. The choice of PDE is due to their local formulations that are Either scalar or matrix-valued diffusivity tends to control the smoothing effect according to the local geometry of the image. 3.1 Nonlinear isotropic diffusion Perona and Malik X  X  work [ 43 , 44 ] on nonlinear isotropic dif-fusion was the first issue in the area. In particular, a diffusion coefficient function is chosen to be inversely proportional to the image X  X  gradient magnitude. Therefore, the diffusion flow increases within homogeneous regions where the gradi-ent tends to be small and decreases across nonlinear discon-tinuities where the gradient magnitude tends to be small, but it decreases across nonlinear discontinuities where the gra-dient magnitude tends to be either small or very high. This is clearly illustrated in the Perona-Malik equation given as: I t = di v ( C (  X  I )  X  X  I ) where  X  I is the gradient magnitude of the image I and C (  X  I ) is an edge stopping function chosen to satisfy C ( 0 ) = 1 and lim v  X  X  X  C (v) = 0. According to the Equa-tion 2 , we notice that the c X fficient C (  X  I ) serves the pur-pose of selecting the locations in the image for smoothing. If C is chosen constant at all image locations, the diffusion equation yields isotropic Gaussian smoothing. The function C could be defined by either C (  X  I ) = 1 C (  X  I ) = exp  X   X  I
Catt X  et al. [ 45 ] have proved the ill-posedness of the diffu-sion equation and proposed a regularized version where the c X fficient is a function of a smoothed gradient I t = di v ( C ( G  X   X  X  X  I )  X  X  I )
In general, the nonlinear isotropic diffusion filter acts sim-ilar to the linear diffusion filter inside uniform zones but acts 2. Smoothing within homogenous areas: if  X  + /  X   X  0, the
Among the promising anisotropic PDE-based approaches, we have noticed the Weickert approach [ 48 , 47 ] and the R. Kimmel et al. approach known as the beltrami-flow [ 49 , are different (Table 1 ). Both of these approaches are based on gradient divergence. D. Tschumperl X  [ 51 ] introduced a new formulation based on the computation of the trace oper-ators and the Hessian matrix instead of the divergence. Such formulation is given by I t = trace ( DH )
This section presents some typical PDE-based approaches commonly known for their performance in noise removal while preserving important features. The behavior of each approach in treating local image properties changes accord-ing to f + /  X  (Table 1 ). To illustrate this idea, we give in Fig. 7 , the diffusivity matrix D for the different studied functions  X  X  X , is characterized by the shape of an ellipse directed by
With the illustration given in Fig. 7 , we are able to high-light the advantages and the disadvantages of the studied approaches. In fact, the diffusion filter of Perona-Malik-Catt X  gives very good results for the preservation of charac-ter topologies. For instance, the value of the local smoothed gradient, which weights the diffusion, avoids corner round-ing and preserves edges. Nevertheless, the Perona-Malik-Catt X  filter does not smooth contours disturbed by noise and does not ensure their continuity. The filters of Beltrami and Tschumperl X  give better results as they have the property of reducing noise, smoothing contours and preserving the struc-
In the case of textual document images, singularities must be treated carefully since any modification could change one letter to another and consequently change the whole mean-ing. Thus, we will be faced with a meaningless textual doc-ument. To state one example, the loss of singularities could transform the letter  X  X  X  to the letter  X  X  X  which are completely different (Fig. 9 ).

The Weickert coherence-enhancing diffusion process, compared with the other processes, is characterized by its ability to repair stroke discontinuities on document images. This feature is a very desirable one mainly in treating bro-ken characters of degraded textual documents. However, Weickert X  X  process destroys singularities and leads generally to curved structures. This problem could be avoided with gradient is important. Looking for an approach that avoids corner rounding and takes into account the coherence-enhancing diffusion, we propose a combination of the Perona-Malik and Weickert processes (Fig. 10 ). 4.2 Description of our proposition The proposed filter benefits from the scalar diffusion model of Perona-Malik and the tensor-driven diffusion process of Weickert. As we have previously noticed, the filter of Perona-Malik is a special case of the Weickert filter. Thus, the main difference between these two approaches relies on  X  Near a singularity, we have  X  + &gt; K + and  X   X  &gt; K  X  . different restored images. The parameters K + and K  X  affect mainly slight parts; in the case of equal values (Fig. 12 b), they tend to erase these parts, otherwise, their extremities are prolonged in their flow direction (Fig. 12 c).

K + is the sensitivity threshold useful for the preservation of the edges. When the value of K + increases, edges of low contrast objects in the image could be erased. This is due to the fact that in this case, the value of  X  + is high near a con-tour whatever the value of  X   X  is. Besides the threshold K + , the proposed PDE-based filter introduces another threshold value, K  X  , useful for the preservation of singularities. When the value of K  X  increases, singularities in the image could be erased. In fact, the value of  X   X  increases near a singularity as it represents a complex structure compared with a simple line or a uniform area.

By comparing both the values of K  X  and K + , we can notice two situations:  X  X f K  X  K + , then the proposed filter processes oriented  X  X f K + K  X  , then we have a better preservation of image given image depends greatly upon the eigenvalues  X  + /  X  . These values vary according to the smoothing parameters  X  and  X  of the tensor field T  X   X  . If the regularization parame-ter  X  is generally equal to 0.5, the parameter  X  depends upon the image contents and controls the correct estimation of the anisotropic diffusion orientation. For document images,  X  varies according to the noise, the continuity of strokes and the text heights. In fact, poorly degraded documents char-acterized by a reduced noise along characters X  contours and continuous strokes and even document images with small text heights require small  X  values varying from 0.5 to 1.5. As for strongly degraded documents having large disconti-nuities or for documents having high text heights,  X  must be fixed between 1.5 and 3. In general, the values  X  = 0 . 5 and  X  = 1 . 5 work pretty well for a great majority of document images.

The eigenvalues are naturally square values since they are calculated from the tensor matrix also having square values. This explains the work by Perona-Malik [ 43 ]. For instance, the exponential argument is not put in square but contains the eigenvalue divided directly by the parameter K .This parameter stops the diffusion for real contours and boosts the diffusion in noisy regions. In our case, the eigenvalues of the tensor matrix replace the gradient magnitude argument in the control of the diffusion process. We must then auto-2D-joint histograms of these images show the occurrence of ous contours and singularities (Fig. 13 c, d).
 used for an estimation of the K parameters. Nevertheless, the  X  could be used to estimate the parameters K in the case of document image processing. In order to preserve these sin-gularities, we fix the parameter K  X  by taking 10% of the  X  assume that for document images written in Latin script, on average, 10% of the contours are located around singulari-ties. Hence, this estimation preserves the main singularities like corner and stroke endings. For the parameter K + ,we propose to fix it at half of the value of K  X  so as to reinforce stroke continuities efficiently: K  X  = 0 . 1  X  max (  X   X  )
Figure 14 details the automatic estimation of the K param-eters. For instance, Fig. 14 a shows the original image to restore followed by Fig. 14 b representing the diffusion c X ffi-cients exp (  X  + / K + ) defined with the estimated K + parame-ter. These c X fficients help to localize the contours to smooth with anisotropic diffusion. Another diffusion c X fficients defined with the automatic threshold K  X  is calculated as exp (  X   X  / K  X  ) . This equation extracts from the processed image all the singularities necessary to preserve. Fig-ures 14 c,e give the extracted singularities respectively before and after the restoration process. The result of the superposi-tion between different singularities and the restored image is shown in the Fig. 14 f. It highlights the difficulties to make a reconstruction due to the wrong alignment of the flow. In some cases (for example the characters  X  ` a  X ,  X  X  X  and  X  X  X  in the last line), the restoration can modify the topology of some characters because of the disorientation of the flow. But in most cases, the restoration repairs the shapes of characters. 5 Experimental results: application for image denoising 5.1 Document image quality measures Quality measures, necessary to compare the visual difference between two images, are a good issue in ranking, evaluat-ing and optimizing image restoration algorithms. Two solu-tions are possible to measure such a difference using either subjective or objective measures. The subjective measure it is a good solution since a person is the ultimate viewer, yet it is very costly. The objective measure is easier to implement and to test, but it does not always agree with the subjective one.
In this section, we choose to use the objective measure while testing the most popular distortion measures, such as the peak signal-to-noise ratio (PSNR), the improvement in signal-to-noise ratio (ISNR) and the mean square error (MSE). This choice is made in order to make the results gen-erated on document images comparable to the literature. For an image processing system having the input image I ( x , y ) , the processed output image I ( x , y ) and the dimen-sions of the images (w, h), the MSE is defined as MSE = The MSE measures the average of the square of the errors between two images. The growth of the assumed amount of measurement error, caused by the presence of several differ-ent features between the two images, leads to larger MSE values.

PSNR is defined as the ratio of peak signal power to aver-age noise power: PSNR ( dB ) = 20 log developed a document image model for the image defects which estimates the degradation of characters around the cor-ners. The model and metrics are mainly for the estimation of classification errors rather than the measurement of visual distortion. For instance, characters degradations are caused by a combination of several effects (digitizing, resizing, pre-processing, filtering, converting from a format to another, reducing color depth, lossy compression, etc). An adequate measure of document image degradations must also take into account the neighborhood of pixels and mainly pixels around informative parts which are difficult to locate. Most of the recent works on degradation models for document images show the difficulties to define a consistent measure of char-acter degradations which is coherent with human perception values (around 1). For images having large and noisy line structures to reinforce (Fig. 24 ),  X  must be set to high values (between 3 to 5).

K + is the threshold proposed to make a distinction between noise to remove and contours to preserve. Similarly to the Perona-Malik-Catt X  scheme while processing natural images, K + must be set properly according to the level of noise in the image.

K  X  is the additional parameter introduced by the pro-posed diffusion scheme. This parameter defines the threshold that stops the Weickert coherence effect. For natural images, when K  X  is equal to K + , we have a little coherence rein-forcement. By increasing K  X  to a value twice as big as that of K + , we amplify the Weickert effect reinforcing the lines X  coherence and repairing discontinuities.

For natural images without additive noise, K + is set between 20 and 80 and K  X  is set to 1 . 5  X  K + . In the case of natural images with additive noise, the proposed scheme outperforms the Perona-Malik-Catt X , Weickert, NL-means and NL-median restoration models. Only the FOE approach performs a little better than our scheme for some particular images.

For instance, a comparative study with different resto-ration/denoising approaches is conducted on several classical images (Castle(Fig. 19 ), Barbara(Fig. 21 ),Lena (Fig. 23 ),...).
Other kinds of images were processed. We give for exam-ple a fingerprint image (Fig. 24 ), a medical image (Fig. 25 ) and even a graphical printed image (Fig. 26 ). The latter image was processed by the proposed diffusion filter (Fig. 26 f) and the diffusion filters of Perona-Malik-Catt X  (Fig. 26 b), Alva-rez (Fig. 26 c), Weickert (Fig. 26 d) and Terebes (Fig. 26 e).
The different experiments given above illustrate that the scope of the proposed filter is not confined to the process-ing of textual documents; other kinds of images could also be processed. These claims are corroborated, for the most of them, by numerical tests. 5.3 Document image denoising For document images, the PSNR values directly extracted from gray level images are not significant, since each resto-ration approach generates different grayscale images char-acterized by a background with different gray levels. This is clearly noticeable in Figs. 27 and 29 where the different measure values are reported in Tables 6 and 8 , respectively.
Moreover, the distortions of the character patterns are more important to measure than the regularization of the background. As a solution, we propose to binarize, in a first step, each resulting image with an optimal fixed threshold. In a second step, we extract the different measures from the binary images. We also give the correlation c X fficient cor topology. According to the given results, the proposed diffu-sion filter is well suited for document images since it takes benefit from the Weickert coherence reinforcement while stopping this process around singularities like corners. With low noise level, NL-means, FOE and the diffusion filters restore correctly with almost the same ISNR. The difference of the behavior statement between all the restoration mod-els is noticeable in the case of severely degraded images and especially when adding a large amount of noise (Fig. 27 ). Faced with such a situation, the performance of the NL-means filter decreases quickly. The FOE and the proposed scheme outperform all the other restoration models.
Figure 27 illustrates that the different approaches per-form globally in a background without noise. For document images, the presence of noise in the background has a great impact on the overall performance of the restoration pro-cess. The FOE and the proposed scheme achieve again better results and remain almost stable.

We have also tested the limits of all the restoration mod-els in the case of excessive noise (Fig. 29 ). Restoration approaches based on PDE remain more efficient compared with the others. For images with excessive noise, the NL-means and the FOE filters fail in restoring the visual degra-dation.
 For the diffusion-based restoration approaches and the FOE filter, the computational cost depends on the number of iterations. Thus, we stop the iteration process when the PSNR improvement does not change. The Perona-Malik-level images. In fact, we transform a binary image into a color one, then after restoration, we binarize it by using a fixed threshold equal to 128 to allow the comparison with the original.

More tests with a noticeable visual quality improvement are shown in Figs. 31 , 32 and 33 . The restored images reveal the efficiency of the proposed anisotropic diffusion filter. For instance, we notice an isotropic smoothing in homogeneous regions, removing efficiently the noise and avoiding unde-sired image structure appearance. The diffusion preserves vector edges and it is well studied on sharp corners as it prevents corner erosion.
 that the image is not degraded, yet this is not actually true as this little modification has changed the meaning. Thus, the evaluation of the quality of restored textual document images is not an easy task. There exists no general meth-odology to evaluate the correctness of a given result. Since the ultimate aim of our PDE-algorithm is to restore dam-aged characters in noisy document images, we propose to evaluate the result through the study of the optical character recognition accuracy rate. An improved recognition rate is certainly related, in this case, to an improved quality image. Hence, this section focuses on the study of the OCR system response before and after restoring damaged characters with PDE-based approaches. This study will be with the same parameters and the same OCR package. ABBYY Finereader 8.0 professional Edition is the OCR software widely known for its high precision character recognition. Because modern OCR cannot recognize obsolete old letters, we do not count classical errors with the confusion between long  X  X  X  and  X  X  X  for example.

To prove the generality of our proposition, we choose par-ticularly degraded historical printed books and old news-papers of the sixteenth and eighteenth centuries. These resources are available on popular online digital libraries like Gallica or Google books. Since we participate in the digitiza-tion of the  X  X azettes of Leydes X , we include these documents showingdegradationsexplainedbythedocuments X  X gingand the bad quality of the printing.
 result ( a ) (b) (c) of these images is very beneficial for the OCR especially when the noise around the contours and in the background makes the segmentation and the recognition of characters difficult.
 The book entitled  X  X e bourgeois gentilhomme X  from Moli X re [ 57 ], printed in 1671, presents severe degradations due to the digitization process affecting the image quality with random dithering and replacing lines by isolated dots. Figure 36 details the effect of the restoration on an extract of an image ; the corresponding extracts of the OCR results a 94.47% success rate when detecting damaged letters. This rate is noticeably higher than the recognition rates calculated on the original non-processed image. The accu-racy of the OCR system was well improved by processing the OCR fails to segment the characters correctly when they have too many discontinuities. The restoration also degrades some characters like the letter  X  X  X  in the first line or  X  X  X  in the last line (Fig. 36 ). These reconstruction errors are explained by the disorientation of the flow explained ear-lier.

The second book [ 58 ], printed in 1787, is a priori the less degraded document of those we have collected (Figs. 38 , 39 ). Figure 38 shows minor differences between the original (Fig. 38 a) and the restored image (Fig. 38 b). A little improve-ment is thus achieved. Nevertheless, the restoration improves the OCR accuracy from 94.57% to 98.23% (Table 9 ). It shows that the performance of OCR is unpredictable due to its complexity and to the implication of numerous heuristics controlled by different parameters estimated automatically. Consequently,documentsthatappearsufficientlycleancould also be preprocessed by the OCR for a better result; even a little enhancement may have a great impact on the OCR.
Concerning the book of Nostradamus [ 55 ], printed in 1589, Fig. 40 shows the result of the restoration and the pres-ervation of the character shapes during the noise removal in the background. The recognition rate rises from 94.2%
Other experiments [ 61 ] have been done on a database of 106 color pages of  X  X azette of Leyde X , scanned at 300 dpi. With 144847 characters, we obtained 11279 errors before restoration and 8013 errors after restoring with the proposed filter. The OCR system had approximately a 94.46% suc-cess rate when detecting restored damaged letters compared with a 92.21% rate calculated on non-processed charac-ters (Table 10 ). It is shown that for real document images corrupted by damaged characters, the final restored images are of improved qualities. We notice that optical character recognition takes great advantage of this improvement for the majority of the tested pages. 6.2.3 Case of google books Google selects (in priority) non-degraded printed documents and digitizes them directly from the original document instead of the microfilms. Most of the Google book images are scanned in good quality for a correct recognition rate by an OCR. Few observed degradations are mainly due to the image processing techniques that try to enhance automati-cally the contrast around textual zones in order to remove ink-bleed through and improve legibility.  X  X n Epitomy of English History X  [ 62 ] is a represen-tative historical book (Fig. 42 a) from Google, printed in 1690. The layout of this book, which presents large spaces between characters and text lines, makes the character seg-mentation easier. The total improvement is less than 1% with 6.18% of improved characters and 5.22% of degraded characters (Table 9 ). The high rate of characters degraded by the restoration is essentially explained by the numer-ous characters  X  X  X  which become  X  X  X  after the restora-tion. For this book, the horizontal line makes the dif-ference between  X  X  X  and  X  X  X  too small and not of high enough contrast to be preserved during the filtering process (Fig. 42 ). 7 Conclusion We have proven in this work the efficiency of a novel PDE-based approach in enhancing the quality of degraded textual documentimagesandeveninreducingtheirOCRerrors.This approach requires neither training nor segmentation steps. It is based on the combination of the Weickert tensor-driven dif-fusion filter and the Perona-Malik scalar diffusion filter while inheriting their respective interesting properties: coherence-enhancing and singularities preservation. Properties as such are very essential in processing noisy and broken characters. For instance, the singularities are twofold as the contours. They improve the legibility of the text as well as the visual quality of an image. Experiments show that our proposition outperforms Perona-Malik and Weickert filters applied sep-
