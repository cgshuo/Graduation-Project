 1. Introduction
Modern organizations are increasingly operating upon distributed and heterogeneous information systems, as they con-tinuously build new autonomous systems, powered by the rapid advancement of information technology. They are facing challenges to integrate and effectively use the data scattered in different local systems. The need to integrate heterogeneous data sources, both within and across organizations, is indeed becoming pervasive. Data  X  X  X slands X  that independently emerge over time in different sections of an organization need to be integrated for strategic and managerial decision making pur-poses. Information systems previously owned by different companies need to be integrated following business mergers and acquisitions. Business partners involved in joint ventures need to share or exchange information across their system boundaries. The rapid growth of the Internet, especially the recent development of Web services, continuously amplifies the need for semantic interoperability across heterogeneous data sources; related data sources accessible via different Web services create new requirements and opportunities for data integration.

To integrate or link the data stored in a collection of heterogeneous data sources, either physically (e.g., by consolidating local data sources into a data warehouse [12] ) or logically (e.g., by building a wrapper/mediator system such as COIN [32] ), a critical prerequisite, among others, is to determine the semantic correspondences across the data sources [46] . Such corre-spondences exist on both the schema level and the instance level. Schema level correspondences consist of tables that rep-resent the same real-world entity type and attributes that represent the same property about some entity type. Instance level correspondences consist of records that represent the same entity in the real world.

In this paper, we deal with entity matching (i.e., detecting instance level correspondences) from heterogeneous data sources. A closely related problem is duplicate detection, where multiple approximate duplicates of the same entity stored in the same data source need to be identified [8] . The entity matching problem arises when there is no common identifier across the heterogeneous data sources. For example, when two companies merge, the customer databases owned by the companies need to be merged too, but the customer numbers used to uniquely identify customers are different across the databases. In such situations, the correspondence between two records stored in different databases needs to be deter-mined based on other common attributes.

The initial success of a recently-developed constrained cascade generalization method [63] in solving general classifica-tion problems has motivated us to investigate its applicability and utility in solving the important and difficult entity match-ing problem, which can be formulated as a classification problem. We have empirically evaluated the constrained cascade generalization method using several real-world heterogeneous data sources. In this paper, we describe how this method can be applied in learning entity matching rules from heterogeneous databases and report on some empirical results, which show that this method outperforms the base classification methods in terms of classification accuracy, especially in the dirt-iest case. We use the term  X  X  X irty data X  to refer to data that contain substantial incorrect, inaccurate, imprecise, out-of-date, duplicate, or missing items. As real-world data are often very dirty [25,49,52] , the constrained cascade generalization meth-od may be even more productive in performance improvement when matching real-world dirty data sources.

The rest of the paper is organized as follows. In the next section, we briefly review some past approaches to entity match-ing. We then describe the constrained cascade generalization method and its application in entity matching in Section 3 .We then report on some empirical evaluation for entity matching in Section 4 . Section 5 outlines some future research directions.
Finally, we summarize the contribution of this work in Section 6 . 2. Literature review
The various approaches to entity matching proposed in the literature can be classified into two broad categories: rule-based and learning-based. In rule-based approaches, domain experts are required to directly provide decision rules for matching semantically corresponding records. In learning-based approaches, domain experts are required to provide sample matching (and non-matching) records, based on which classification techniques are used to learn the entity matching rules.
In most of the rule-based approaches, the entity matching rules involve a comparison between an overall similarity mea-sure for two records and a threshold value; if the similarity measure is above the threshold, the two records are considered matching. The overall similarity measure is usually computed as a weighted sum of similarity degrees between common attributes of the two records. Conversion rules can be defined to derive compatible common attributes by converting the formats of originally incompatible attributes [7] . The weight of each common attribute needs to be specified by domain ex-perts based on its relative importance in determining whether two records match or not [9] . Segev and Chatterjee [45] also used statistical techniques, such as logistic regression, to estimate the weights and thresholds, besides consulting domain experts. Hern X ndez and Stolfo [25] developed a declarative language for specifying such entity matching rules.
Recently, classification techniques, especially decision tree techniques, such as CART [23] and C4.5 [21,54] , have been ap-plied in entity matching. Such techniques can automatically induce (learn) entity matching rules (classifiers) from pre-matched training samples. The learned classifiers can then be used to match other records not covered by the training samples.

Statistical methods for entity matching, under the name of record linkage, have an even longer history. Newcombe et al. [38] proposed the first probabilistic approach to record linkage, which computes odds ratios of frequencies of matches and non-matches based on intuition and past experience. The research along this line was summarized by Newcombe [37] .
Fellegi and Sunter [18] proposed a formal mathematical method for record linkage. This method can be seen as an extension of the Bayes classification method with a conditional independence assumption (the matches on the individual attributes of two records are assumed to be mutually independent given the overall match of the records). It allows users to specify the minimum acceptable error rates and classifies record pairs into three categories: match, non-match, and unclassified.
Unclassified record pairs then need to be manually evaluated. Several record linkage systems, e.g., GRLS of Statistics Canada [17] and the system of the US Bureau of the Census [57] , have been developed based on Newcombe X  X  [37] approach and Fel-legi and Sunter X  X  [18] theory of record linkage. Torvik et al. [51] recently proposed a probabilistic approach, which is indeed similar to that of Fellegi and Sunter.

Further refinements over the basic probabilistic approach have also been developed. Dey et al. [13] and Dey [12] proposed an integer programming formulation to minimize the total misclassification cost. Dey et al. [14] proposed another integer programming formulation, using distances, instead of probabilities, as similarity measures. Torvik et al. [51] proposed meth-ods for smoothing, interpolating, and extrapolating probability estimations and further adjusting predicted matching prob-abilities based on three-ways comparisons.

Other classification techniques, e.g., logistic regression [39] , have also been used in learning entity matching rules. A wide variety of classification techniques, including naive Bayes, logistic regression, decision tree/rule/table, neural network, and instance-based learning, have been empirically compared in the context of entity matching [64] . There are abundant com-parisons of different classification techniques in the literature (e.g., [55] ).

Learning-based approaches have an advantage over rule-based approaches in hard applications where the entity match-ing rules are non-trivial, involving approximate comparisons of many attributes. In such complex situations, it may be much more difficult for human experts to explicitly specify decision rules than to provide some classified examples.
Multiple learning techniques can be combined to further improve classification accuracy. Tejada et al. [50] combined multiple decision trees using bagging. Zhao and Ram [64] applied various ensemble methods , including cascading [20] , bag-ging [4] , boosting [19] , and stacking [59] , to combine multiple base classifiers into composite classifiers. Empirical results show that these methods can often improve classification accuracy. The cascading method used in that study was loose cou-pling [20] . Since then, a new method for cascade generalization [63] has been developed that is more general than the loose coupling and tight coupling strategies used in the past [20] . Its utility in general classification problems was empirically demonstrated [63] . However, its application in entity matching problems is yet to be evaluated.

Besides classification ( supervised learning ) techniques, clustering ( unsupervised learning ) techniques have also been ap-plied in entity matching [40] . Clustering techniques identify groups of similar entities based on some heuristic similarity measure without using pre-matched training examples. They do not support predictions on new data and need to be rerun when data have been updated. They seem to be more suitable for matching schema elements (tables and attributes), which are relatively static, than for matching records of entities, which are frequently updated in most operational systems [66] . 3. Constrained cascade generalization for entity matching
Entity matching can be formulated as a classification problem. As discussed earlier, classification techniques, especially decision tree learners, have been used in entity matching. Cascading other techniques with decision tree learners can alle-viate the representational bias of decision tree learners and potentially improve accuracy. A recently-developed constrained cascade generalization method [63] uses a parameter to constrain the degree of cascading and find a balance between model complexity and generalizability. Empirical evaluation in solving general classification problems has shown that this method outperformed the base classification methods and was marginally better than bagging and boosting on the average [63] .In this section, we describe its application in entity matching. 3.1. Entity matching
It has been demonstrated that entity matching in large heterogeneous data sources is often very complex and time-con-suming due to low data quality and various types of discrepancies across the data sources. For example, it was reported [56] that a project to integrate several mailing lists for the US Census of Agriculture consumed thousands of person-hours even though an automated matching tool was used. Another example is the Master Patient Index (MPI) project funded by the
National Institute of Standards, which aims to develop a unified patient data source, allowing care providers to access the medical records of all patients in the US stored in massively distributed systems [1] ; a key challenge in this large project is reliable matching of medical records across heterogeneous local systems. Automated (or semi-automated) tools are needed to facilitate human analysts in the entity matching process.

The entity matching problem can be defined as the following classification problem. Given a pair of records drawn from semantically corresponding tables (i.e., the tables represent the same real-world entity type) in two heterogeneous dat-abases, the objective is to determine whether the records represent the same real-world entity (e.g., whether the two given records are about the same person). This is a binary (match or non-match) classification problem. Two records can be com-pared on semantically corresponding attributes (i.e., the attributes describe the same property of some entity type). Similar-ity measures between the records on semantically corresponding attributes form a vector of input features. A classifier needs to be trained, based on a sample of pre-classified examples, to predict whether two records match or not based on these input features.

The input features can be various types of attribute-value matching functions. For two semantically corresponding attri-butes whose domains are D 1 and D 2 ,an attribute-value matching function is a mapping f : D degree of match between two values in D 1 and D 2 , where 1 indicates a perfect match and 0 indicates a total mismatch.
The simplest attribute-value matching function is equality comparison. However, equality comparison is often insuffi-cient due to various schema level and data level discrepancies across databases. Interested readers are referred to Kim and Seo [27] , Luj X n-Mora and Palomar [31] for more detailed discussions of semantic discrepancies across databases.
Special treatments are needed in comparing semantically corresponding attributes across data sources, accounting for the various types of discrepancies. Transformation functions can be defined to convert corresponding attributes into compatible formats, measurement units, and precisions. Approximate attribute-value matching functions can be defined to measure the degree of similarity between two attribute values, even though they are not identical. There are many general-purpose approximate string-matching methods, e.g. Soundex [6] , Hamming distance, Levenshtein X  X  edit distance, longest common substring, and q -grams [48] , string-matching methods specifically developed for entity matching [3] , and special-purpose methods that are suitable for comparing special types of strings, e.g., addresses [56] , entity names [11,30] , and different abbreviations [35] . Interested readers are referred to Stephen [48] and Budzinsky [6] for more detailed discussions of string-matching methods. Long texts can be analyzed using natural language processing [34] and information extraction [43] techniques.

Numeric attributes can be compared using normalized distance functions (possibly after transformation). Different cod-ing schemes can be resolved using special dictionaries. Multiple transformation and attribute-value matching functions can be combined to form an arbitrarily complex attribute-value matching function.
There are typically many more classification decisions to be made in entity matching than in other general classification problems (e.g., [63] ). Usually, in a general classification problem with n records, n classification decisions need to be made.
However, in an entity matching application involving two databases with m and n records, respectively, m n record pairs need to be classified. Even a small improvement in the classification accuracy, which may be deemed negligible for a general classification problem, such as credit rating, can be considered practically useful or even significant for an entity matching application. 3.2. Cascade generalization
After the input features (i.e., attribute-value matching functions) have been defined and training examples prepared, clas-sification techniques can be applied to learn entity matching rules. Recently, decision tree techniques have been frequently used in entity matching applications [15,21,23,50,54] .

However, most decision tree algorithms have an inherent representational bias, that is, they do not learn intermediate concepts and use only one of the original features in the branching decision at each intermediate tree node [55] . The trees such algorithms learn are called univariate trees [36] , as each node involves a univariate test. The trees are also called orthog-onal trees because the decision boundaries in the feature space are restricted to be geometrically orthogonal to the splitting feature X  X  axis. In a two-dimensional feature space, the decision boundaries consist of a sequence of line segments that must be parallel to one of the two axes. Consequently, this representational bias limits the ability of univariate decision trees to fit the training data, likely hampering classification accuracy.

It is possible to combine other classification techniques with decision tree techniques to alleviate the representational bias of decision trees and potentially improve the classification accuracy. Such generalization of decision trees is named cas-cade generalization [20] . It has been demonstrated that such generalized decision trees often outperform univariate decision trees [20] . The generalized decision trees are named multivariate decision trees [5] because the branching decision at an intermediate tree node is based on a multivariate test, which is a function of the original features learned by the cascaded classification techniques.

Past methods for cascade generalization combines multiple classification techniques via either loose coupling or tight coupling [20] . When the classification techniques are loosely coupled, they are applied sequentially. Some classification methods are used first to learn some initial classifiers. The discriminant functions generated by the classifiers are then used as additional features for learning the final generalized decision tree. Each method is applied only once. When the classifi-cation techniques are tightly coupled, the cascaded methods are applied locally at each tree node in learning the generalized decision tree and construct additional features based on the training examples covered by each node.

The structured induction approach used by Shapiro [47] in solving chess ending problems also generates multivariate decision trees with respect to the initially available low-level features. However, this approach is fundamentally different from cascade generalization, in that the high-level features are not automatically learned by an algorithm but manually pre-scribed by a human expert. Given a chess ending game, a domain expert first decomposes the problem into a hierarchical tree of subproblems (i.e., top-down decomposition), until the subproblem at each leaf node can be correctly solved by the
ID3 decision tree induction algorithm [41] . ID3 is then used to solve each of the subproblems from the bottom of the hier-archy to the top (i.e., bottom-up induction). The ID3 tree for solving a subproblem on the bottom of the hierarchy is a uni-variate tree with respect to the initially available low-level features. The solutions to the subproblems on lower levels of the hierarchy are then given to ID3 as primitive attributes in solving subproblems on a higher level. Thus, the ID3 trees on higher levels of the hierarchy are multivariate trees with respect to the initially available low-level features, although ID3 itself al-ways induces univariate trees with respect to the features given to it. Structured induction driven by human expertise is nec-essary in solving sufficiently complex problems, where a direct application of a learning algorithm does not seem to be promising [47] . 3.3. Constrained cascade generalization
Cascade generalization increases the flexibility (due to higher model complexity) of the learned decision trees and tends to improve apparent accuracy (i.e., accuracy on the training sample). In addition, tight coupling provides more flexibility than loose coupling and therefore has more potential in performance improvement. In general, as the model complexity increases, the model has more flexibility in fitting the training data and tends to do so better. However, as a more complex model fits the training data better, it is more sensitive to the training sample and therefore more prone to the overfitting problem (i.e., a model has high accuracy on training data but low predictive power on other data). Apparent accuracy is not a reliable mea-sure of the true performance of a learned classifier. A classifier with high apparent accuracy may not necessarily generalize to unseen data well. A classical example is that the simple normal linear classifier has been found to often beat a natural exten-sion (with higher complexity), namely, the quadratic classifier [55] .

There is always a trade-off between model complexity and generalizability. A critical issue in tuning learned classifiers, when multiple levels of complexity fit are possible, is to search for an appropriate level of complexity fit such that the learned classifiers fit the training data relatively well and, more importantly, generalize well to future data [55, p. 36] .As
Hand et al. [24, p. 138] state:  X  X  This issue of selecting a model of the appropriate complexity is always a key concern in any data analysis venture where we consider models of different complexities.  X 
This trade-off is also known as bias-variance trade-off [16,53] . The error of a classifier can be explained by two compo-nents, bias and variance, besides unavoidable noise. Bias reflects the difference between a model and the optimal model (assuming an infinite training sample is given), and is an indicator of the intrinsic capability of the model in fitting the train-ing sample. Variance reflects the variations of the model across samples. In general, lower bias demands higher model com-plexity, which tends to lead to higher variance. In other words, a more complex model may be able to more closely approximate the optimal model, but at the same time, will be more sensitive to the training sample. Finding a model that balances the bias and variance components and has low overall error requires identifying the appropriate model complexity.
Cascade generalization is not exempt from this trade-off. While cascading increases the model complexity (and therefore flexibility) by introducing more complex decision tree nodes and tends to increase classification accuracy on the training data, it does not mean that the deeper the classification methods are cascaded the better. If the other classification methods are fully cascaded with the decision tree learner (i.e., tight coupling), the local models learned by cascaded classification methods at each tree node are based on fewer and fewer training examples, which also become more and more unbalanced with more examples of the majority class covered by the current branch. Such local models become increasingly prone to overfitting the local training data. Tight coupling may not outperform loose coupling, which in turn may not even outper-form the underlying univariate decision tree learner. The most productive level of cascade generalization must be deter-mined empirically for a particular classification problem.

A recently-developed generic method for constrained cascade generalization [63] uses the maximum cascading depth as a parameter to simulate the trade-off between model complexity and generalizability, and to find the appropriate model com-plexity. This parameter effectively constrains cascading, and therefore model complexity, to some degree. Cascading meth-ods proposed in the past, including loose coupling and tight coupling, are strictly special cases of this method. As the cascading depth increases, the model complexity increases, lowering the bias, but the variance also increases, lowering the generalizability. Constrained cascade generalization allows the evaluation of multiple cascading depths to find one that gives appropriate complexity fit. Empirical evaluation in general classification problems, such as credit rating, disease diag-nosis, labor relations, image segmentation, and congressional voting, has shown that this method tends to outperform the base classification methods [63] .
 An algorithm of constrained cascade generalization is available in [63] and included here for the sake of convenience.
Note that because entity matching is a binary classification problem, the parameter corresponding to the class space in the original algorithm is not necessary any more and is removed in the following modified algorithm.
 Build _ Generalized _ Tree ( N , x , S , d , d cascade , W , C )
Begin x 0 :  X  the empty set.

Select a splitting test using the goodness measure of W , by which S is split into q subsets S If q =1, Else End.

The algorithm can be used to combine any discriminant function learners (i.e., the C ) with any decision tree algorithms algorithm when d cascade = 1 and d cascade = the total height of the tree, respectively. The larger the d bias, but at the same time, the higher the variance. The overall performance may fluctuate as d ate value for this parameter should be empirically determined for a given classification problem. A simple procedure is to build and evaluate all generalized decision trees under different d in general classification problems has shown that this new method can find a more appropriate degree of cascading and reach better classification accuracy than previous cascading methods [63] . In this paper, we apply this method in learning entity matching rules and evaluate its utility using real-world heterogeneous databases.

Zhao and Sinha [65] proposed a more efficient algorithm that can learn an entire forest consisting of the same set of gen-eralized decision trees under different d cascade values simultaneously without repeating any node that is common across multiple trees and provided a complexity analysis of the algorithm. Let f
W and C , respectively, where n and m are the number of training cases and the number of features, respectively. The training time of this efficient algorithm is not more than ( k 1) f is usually not larger than the depth of the univariate tree learned by W alone. 4. Application
We have implemented the efficient algorithm [65] for constrained cascade generalization on top of logistic regression [26] and C4.5 [42] (i.e., C = {logistic regression}, W = C4.5) available in the Weka machine learning toolkit [58] and applied the method to three real-world entity matching applications. We will report on some empirical results in this section. 4.1. Data description
To evaluate the usefulness of the constrained cascade generalization method under different situations, we have collected three sets of real-world heterogeneous data sources with different degrees of heterogeneity in their schemas and data. In the first application, we match passengers in flight ticket reservations maintained by an Application Service Provider (ASP) for the airline industry. In the second application, we compare the records about books extracted from the catalogs of two lead-ing online bookstores. In the third application, we link records about property items stored in two legacy databases managed by two independent departments, named Property Management and Surplus, of a large public university. Hereafter, we will refer to the three applications as Airline, Bookstore, and Property.

The Airline case consists of one database containing airline passenger reservation records. The ASP serves over 20 national and international airlines and maintains a separate database for each airline. All the databases have the same schema. Even though the database schemas are homogeneous, identifying duplicate passengers and potential duplicate passenger reser-vations (PNRs) is still not trivial due to various types of data errors and discrepancies in the databases. Currently the ASP only identifies duplicate PNRs within every individual airline. In the future the service may be extended to identify duplicate
PNRs across airlines. The information relevant to passenger matching includes: passenger name, frequent flyer number, PNR confirmation information, address, phone numbers, and itinerary segments. There are 351,438 passenger records in one database snapshot provided by the ASP.

In the Bookstore case, we have book data maintained by two online bookstores and need to determine which two records stored in different catalogs are about the same book. We have extracted 737 and 722 records from the two catalogs, respec-tively. There are several semantically corresponding attributes across the two catalogs, including ISBN, authors, title, list price, our price, cover type, edition, publication date, publisher, number of pages, average customer rating, and sales rank.
In the Property case, we have access to two heterogeneous databases, both about property items, and need to determine which two records stored in different databases are about the same property item. These two databases have been developed at different times by different people for different purposes. The database maintained by the Property Management Depart-ment is managed by IBM IDMS. The Surplus database is managed by Foxpro. The Property Management Department man-ages all property assets owned by the various departments of the university and keeps one record in its database for every property item. When some department wants to dispose an item, the item is delivered to the Surplus Office, where it is sold to another department or a public customer. The Surplus Office keeps one record in its database for every disposed item.
There are 115 and 32 attributes in the Property Management and Surplus databases, respectively. Although the formats of the two databases are very different, they share some semantically corresponding attributes, including tag number, description, class description, purchasing account, payment account, acquisition date, acquisition cost, disposal date, dis-posal amount, owner type, model, serial number, and manufacturer. We took a snapshot of each database, containing 77,966 (Property Management) and 13,365 records (Surplus), respectively.

We evaluate the usefulness of the constrained cascade generalization method using the above-mentioned data sets. These data sets have different degrees of heterogeneity in their schemas and data, allowing us to observe how the constrained cas-cade generalization method performs under different situations. In the Airline case, the schemas of the databases for differ-ent airlines are all identical. These databases can be easily merged using a simple union operation. The entity matching problem reduces to detecting approximately duplicate records in a single database. The data stored in the databases are clea-ner than the data in the other two cases. In the Bookstore case, the schemas of the two catalogs are different but contain very similar attributes. Over 90% of the attributes in each catalog have corresponding ones in the other catalog. The data of the two catalogs also significantly overlap with each other; there are many common books in the two catalogs. There are some data discrepancies across the two catalogs. In the Property case, the two schemas are very different and overlap for only a small portion (below 18%). The two databases contain some corresponding records. The data X  X specially in the Surplus database X  X re very dirty. Table 1 lists the percentage of mismatching values on each corresponding attribute pair across the two databases. Even the best matching attribute pair (purchasing account), except owner type, has about 25% of mismatch-ing values. The owner type has only about 1% of mismatching values, but is not very useful in entity matching, as 99% of the property items have the same owner type, the university. The mismatching values on other corresponding attribute pairs are due to various kinds of errors (incorrect, inaccurate, imprecise, out-of-date, or missing values) and discrepancies. In addition, there are some highly discriminative attributes in the Airline and Bookstore cases (i.e., passenger name in the Airline case and book title and authors in the Bookstore case), but not in the Property case. Based on these characteristics, we expect that the Property case is much harder than the other two cases for the entity matching task and the Airline case is the easiest. 4.2. Preparation of training samples
In all three cases, training examples can be easily generated as there is a (partial) common key attribute in each case. In the Bookstore case, ISBN is a common key for book records. In the Airline case, frequent flyer program and frequent flyer number can be used as a partial key, for passengers who use their frequent flyer numbers in reservations. Two records having identical frequently flyer number for the same frequent flyer program are very likely to be about the same passenger, while two records having different frequently flyer numbers for the same frequent flyer program are very likely to be about dif-ferent passengers. In the property case, the two databases use the same tag number for each property item. It is therefore easy to determine corresponding records in these cases. There is no need to train classifiers to match records in these cases (except the Airline case where not every passenger uses a frequent flyer number). We intentionally chose these cases to eval-uate various classification techniques in this study. We trained classifiers based on other attributes while withholding the common keys. In other entity matching problems, when there is no common key, domain experts need to manually match some records before training classifiers. In cases where some records contain a common key (such as in the Airline case), these records can be used for training and the resulting classifier can then be used to match other records that do not have the key.

Typically the two types of record pairs are extremely unbalanced in entity matching applications. There are usually many more non-matching record pairs than matching ones, unless there are many duplicate records in each of the databases under comparison. A representative sample of all record pairs would be a very sparse one. A trivial classifier that always predicts a record pair as non-matching would achieve extremely high accuracy on such a sample but is useless. While there are many different ways to deal with such extremely unbalanced data, Kubat and Matwin [29] proposed and argued for a simple one-sided sampling method: keep all examples of the rare class and take a sample of the majority class, to get a more balanced training set. Some learning algorithms, such as decision tree induction, are sensitive to an imbalanced distribution of exam-ples and may suffer from serious overfitting, which cannot be addressed by pruning. This problem can be mitigated by bal-ancing the training examples. The minority-class examples are too rare to be wasted, even under the danger that some of them are noisy. Thus, it is recommended that all minority-class examples be kept and only majority-class examples be pruned out. Kubat and Matwin [29] experimented on four largely unbalanced datasets and found that one-sided sampling helped to improve the performance of nearest neighbor and C4.5 decision tree induction.

We basically adopted the one-sided sampling method [29] and used a balanced sample for each of the three cases in our evaluation. In the airline case, we randomly selected 5000 matching record pairs and 5000 non-matching record pairs, resulting in a sample size of 10,000. In the Bookstore case, we used all of the 702 available matching record pairs and ran-domly selected an equal number of non-matching record pairs, resulting in a sample size of 1404. In the Property case, we used all of the 3124 available matching record pairs and randomly selected an equal number of non-matching record pairs, resulting in a sample size of 6248. The characteristics of the three training samples are summarized in Table 2 .
We designed multiple attribute-value matching functions for each semantically corresponding attribute pair and selected the one that is most highly correlated with the dependent variable (i.e., whether two records match or not). We also tried to combine multiple attribute-value matching functions. As a result, we selected 13, 16, and 12 (base or composite) attribute-value matching functions as input features for the three cases, i.e., Bookstore, Airline, and Property, respectively. The types of these matching functions included equality comparison (e.g., for edition, publish month, and publish year in
Bookstore), edit distance (e.g., for title in Bookstore), Soundex (e.g., for first name and last name in Airline), normalized dis-tance (e.g., for list price, our price, and number of pages in Bookstore), special dictionary lookup (e.g., for owner type in Prop-erty), and combinations of multiple matching functions. Table 3 summarizes these attribute-value matching functions. 4.3. Empirical results We ran logistic regression, C4.5, and constrained cascade generalization (specifically, cascading logistic regression with
C4.5) for the three entity matching applications. We used classification accuracy as the performance measure. In unbalanced, cost-asymmetric binary classification problems, a trade-off between two separate measures (e.g., false positive error rate and false negative error rate, sensitivity and specificity, precision and recall) needs to be made [58] . When the costs of the two types of misclassification errors are known, the two separate measures can be combined into a convenient overall perfor-mance measure, e.g., expected misclassification cost. Accuracy is a special case of expected misclassification cost, when the weights of the two classes are considered equal. Since we cannot assign a uniform weight ratio across the three appli-cations, we decided to use accuracy as the performance measure in our evaluation.

We used 10-fold stratified cross-validation, a frequently recommended estimation method [28,58] , to estimate the true predictive accuracy of each learned classifier. All performance measures reported subsequently will be (10-fold) cross-val-idated accuracy. Cross-validation randomly divides a training data set into n (stratified) subsets, called folds, and repeatedly selects each fold for testing while others are used for training. The average testing accuracy over the n runs is used as an overall estimate of the performance of the learned classifier.

To statistically compare the performance of the three classification methods, we needed to perform cross-validation mul-tiple times for each method on each case. The required sample size was determined as follows. Using a three-level (corre-sponding with the three classification methods) ANOVA, aiming at detecting a relatively large effect size in Cohen X  X  recommended convention ( f = 0.4), 1 setting the power at 0.8 and the statistical significance level at 0.05, we needed 21 exam-times for each method on each case.

Table 4 summarizes some descriptive statistics of the performance of the three methods in the three applications. Table 5 summarizes the ANOVA results comparing the performance of the three methods for the three applications. Table 6 summa-is detected, with a large effect size ( f = 1.239). Constrained cascade generalization statistically significantly outperforms lo-is detected, with a medium effect size ( f = 0.393). Constrained cascade generalization statistically significantly outperforms with a large effect size ( f = 1.137). Constrained cascade generalization statistically significantly outperforms both logistic regression ( p &lt; 0.001) and C4.5 ( p &lt; 0.001).

The base methods can already achieve classification accuracy over 99% in the Airline and Bookstore cases, leaving little space for further improvement. In the Airline case, constrained cascade generalization improves the performance of logistic regression by 0.056%. In the Bookstore case, constrained cascade generalization improves the performance of C4.5 by 0.105%.
However, the real significance of constrained cascade generalization comes into play when dealing with  X  X  X irty data X . The performance improvement is larger on the dirty, hard case (i.e., Property) than on the other two relatively clean, easy cases.
In the Property case, constrained cascade generalization improves the performance of logistic regression by 0.198% and that of C4.5 by 0.098%. As real-world data are often very dirty [25,49,52] , the cascading method may be more productive in per-formance improvement when matching even dirtier data sources. In addition, since there are typically a large number of re-cord pairs that need to be classified in an entity matching application X  X or two databases with m and n records, respectively, m n record pairs need to be classified X  X ven a small improvement in the classification accuracy may be considered prac-tically useful or even significant.

While we have demonstrated that the constrained cascade generalization method has a merit in potentially improving performance over the base methods and certainly deserves serious consideration, we do not suggest that it be preferred over the base methods in every entity matching application, as there may be multiple factors (performance, comprehensibility, efficiency, etc.) that should be considered in making the choice. Multiple promising methods should be evaluated before the choice is made.

First, although our empirical evaluation shows that constrained cascade generalization tends to outperform the base clas-sification methods in the three selected applications and never degrades the performance of the base methods, we cannot conclude that constrained cascade generalization will universally outperform or at least match the performance of the base classification methods in all entity matching applications. Indeed, the  X  X  no-free-lunch theorems  X  [60] have proven that there is never a panacea; no method can be universally superior to others in all applications. In practice, for a given new classification problem, various methods need to be empirically evaluated to find the best ones, with previous empirical results as refer-ences. Constrained cascade generalization is not an exception.

Second, comprehensibility of the final model may be another important criterion, besides performance, especially in applications where human analysts are assigned to manually review predicted matching record pairs. Comprehensible explanations of the predictions made by the model may be very useful for the analysts. Cascade generalization may some-what increase the complexity in interpreting the final model.

Efficiency may be another criterion. This may be a less critical factor, as long as a method can finish training in acceptable time period, as training typically only needs to be conducted once or once a while. The training time needed by the efficient algorithm for constrained cascade generalization [65] is at most k times that needed by the base algorithms, where k is the number of generalized trees and can be estimated with the depth of the univariate tree. The base algorithms can be used first to assess whether it is feasible to use constrained cascade generalization in a particular application. Table 7 lists the training times of different algorithms in the experiment, which was run on a Dell Optiplex/GX620 workstation configured with a 3 GHz Pentium D CPU and 1 GB of RAM, running the Windows XP operating system with Service Pack 2. 4.4. Visualizing the effects of constrained cascade generalization
The effects of the constrained cascade generalization method can be visualized in a two-dimensional feature space. For this purpose, we ran the base and composite classification methods for the Bookstore application using just two input fea-tures (i.e., attribute-value matching functions), Author and Title. Fig. 1 shows the learned classifiers. Fig. 2 shows the decision boundaries in the two-dimensional feature space of the learned classifiers.

Logistic regression ( Fig. 2 a) separates the two classes (i.e., match and non-match) using a linear boundary. The slope and intercept of the discriminant line are chosen such that apparent accuracy is maximized. The decision boundary generated by C4.5 ( Fig. 2 b) consists of a sequence of axis-parallel line segments. The directions of these line segments are not flexible.
When logistic regression and C4.5 are cascaded to some degree ( Fig. 2 c and d), some of the line segments are not restricted to be axis-parallel any more and can be oblique. These flexible oblique line segments are learned by logistic regression.
The parameter d cascade effectively controls how many such oblique line segments are allowed and therefore how flexible the decision boundary can be. As d cascade increases, the decision boundary becomes more flexible and the training data can be fitted better. However, the more flexible decision boundary may not fit unseen data better as well. The oblique line segments learned by logistic regression are based on fewer and fewer unbalanced local training examples and become increasingly unreliable. In this particular simple example, d cascade = 1 corresponds to loose coupling and d coupling. In a more complex application with more input features, larger trees may be constructed. An appropriate d value needs to be determined so that the learned generalized decision tree fits the training data well to an extent where it also generalizes well to unseen data. 5. Discussions on current study and future directions
The current study has several limitations that need to be addressed in future research. First, while we have compared the constrained cascade generalization method with the underlying base classification methods, we have not compared it with other existing entity matching methods yet. Second, as the extent of performance improvement by the constrained cascade generalization method seems to be small, we cannot conclude on the (statistical and practical) significance of this improve-ment. Third, only one of the three cases used in the current empirical evaluation contains substantially dirty data. More com-prehensive experiments, involving other state-of-the-art entity matching techniques and more real-world dirty heterogeneous databases, are needed to understand how well different techniques actually perform in real situations.
There are several other research issues that can be further investigated. First, in our empirical evaluation, we used clas-sification accuracy as the performance measure, as it is impossible to uniformly assign an arbitrary weight ratio across the three applications. Note that classification accuracy is a special case of a more general performance measure, expected mis-classification cost, when the weights of the two classes (i.e., match and non-match) are considered equal. In some real-world domains where the weights of the two classes are perceived as different, the two types of training examples need to be re-weighted according to the perceived weight ratio during training and expected misclassification cost be used as the performance measure [58] . Future research can evaluate classification techniques using expected misclassification cost as the performance measure under a series of possible weight ratios.
 Second, there are other possible parameters, such as statistical significance test of learned model, to constrain cascading.
In addition, these parameters are related to decision tree pruning parameters. The effects of different parameters and the interactions among them can be further analyzed and evaluated.

Third, multiple generalized decision trees with different d an ensemble strategy can be analyzed and evaluated. Fourth, cascading decision tree learners with nonlinear model inducers, such as Naive Bayes and artificial neural networks, can be investigated. Fifth, the accuracy of entity matching may be further improved by a follow-up transitive closure inference [25,33] or an iterative learning procedure [2,44] . The technique eval-uated in this paper is orthogonal to, and therefore can be easily combined with, these techniques.

Finally, we have focused on improving the performance of entity matching and left out the efficiency issue. Obviously, a simple pair-wise comparison of the records across data sources is too time-consuming, with time complexity O( MN ), where
M and N are the numbers of records in the two data sources, and the number of instances to be classified is M N .An efficient algorithm, such as the sorted-neighborhood method [25] , needs to be used when the learned entity matching rules are applied on the entire data sources. When such a method is used, the data set on which to learn classifiers will be much smaller and different, with many non-match instances filtered. This difference could improve the efficiency of the algorithm and impact the selection of model parameters like d cascade
A problem related to entity matching is schema matching, i.e., matching tables that represent the same real-world entity type and attributes that represent the same property of some entity type, across heterogeneous data sources [61] . Indeed, schema matching is a prerequisite for entity matching, as matching entities relies on comparing individual corresponding attributes. There is a growing research interest in matching Web data sources, both their schemas and instances. For exam-ple, Wu et al. [61] proposed an approach to schema matching, identifying matching data elements across exposed query interfaces (views) of data sources on the Web. The technique evaluated in this paper can then be used to match the instances of such data sources. Techniques for solving the two problems can be incorporated into an iterative procedure, so that correspondences on both the schema level and the instance level can be evaluated incrementally [62,66] . Such an iterative procedure is worth further investigation. 6. Conclusion
Many organizations rely on a variety of distributed and heterogeneous information systems in their daily operations and need to integrate the systems to make better use of the information scattered in different systems. Entity matching is a critical, yet difficult and time-consuming, problem in integrating heterogeneous data sources and demands automated (or semi-auto-mated) support. In this paper, we have described the application of a generic method for constrained cascade generalization of decision tree inducers in entity matching. We have empirically evaluated the method with three entity matching problems.
Empiricalresultsshowthatconstrainedcascadegeneralizationtendstooutperformormatchbaseclassificationmethods,such as logistic regression and C4.5 decision tree learner. Larger performance improvement may be observed in dirtier situations.
This research has important practical implications to heterogeneous database integration applications. Constrained cas-cade generalization allows multiple base classification methods to be combined to an appropriate level to generate better entity matching rules in such applications. Our empirical results show that this method outperforms the base classification methods in terms of classification accuracy, especially in the dirtiest case.

Unfortunately, real-world data are often very dirty [25] . According to Strong et al. [49] , X  X  50% to 80% of computerized criminal records in the US were found to be inaccurate, incomplete, or ambiguous . X   X  X  Poor-quality customer data costs US businesses some $ 611 billion a year in postage, printing and staff overhead, according to the Seattle-based Data Warehousing Institute , X  reported by Trembly [52] . When dirty data are integrated from multiple sources, the quality of the resulting consolidated data may further decrease (hence the saying  X  X  X arbage in, garbage out X ), consequently hampering the quality of decision support appli-cations running on the integrated data. Data quality problems, such as duplicate records, due to numerous types of discrep-ancies across local data sources have been considered the biggest reason some data warehousing projects had failed [22] .
The cascading method may be even more productive in performance improvement when matching dirty data sources in real-world data integration applications. It can potentially produce better entity matching rules, which identify correspond-ing records across heterogeneous databases more accurately, and improve the data quality of the integrated databases, con-sequently enabling better decision support using the integrated databases and reducing the cost due to bad data.
References
