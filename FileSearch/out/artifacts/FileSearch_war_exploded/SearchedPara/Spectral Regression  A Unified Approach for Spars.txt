
Recently the problem of dimensionality reduction (or, subspace learning) has received a lot of interests in many fields of information processing, including data mining, in-formation retrieval, and pattern recognition. Some popu-lar methods include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) and Locality Preserv-ing Projection (LPP). However, a disadvantage of all these approaches is that the learned projective functions are lin-ear combinations of all the original features, thus it is of-ten difficult to interpret the results. In this paper, we pro-pose a novel dimensionality reduction framework, called Unified Sparse Subspace Learning (USSL), for learning sparse projections. USSL casts the problem of learning the projective functions into a regression framework, which fa-cilitates the use of different kinds of regularizers. By using a L 1 -norm regularizer ( lasso ), the sparse projections can be efficiently computed. Experimental results on real world classification and clustering problems demonstrate the ef-fectiveness of our method.
Dimensionality reduction has been a key problem in many fields of information processing, such as data mining, information retrieval, and pattern recognition. When data is represented as points in a high-dimensional space, one is often confronted with tasks like nearest neighbor search. Many methods have been proposed to index the data for fast query response, such as K -D tree, R tree, R * tree, etc [13]. However, these methods can only operate with small dimensionality, typically less than 100. The effectiveness and efficiency of these methods drop exponentially as the dimensionality increases, which is commonly referred to as the  X  X urse of dimensionality X .

To deal with this problem, the dimensionality reduction technique can be used. One of the most popular dimension-ality reduction algorithms might be Principal Component Analysis (PCA) [20]. PCA performs dimensionality reduc-tion by projecting the original n -dimensional data onto the d ( n )-dimensional linear subspace spanned by the lead-ing eigenvectors of the data X  X  covariance matrix. Its goal is to find a set of mutually orthogonal basis functions that capture the directions of maximum variance in the data so that the pairwise Euclidean distances can be best preserved. If the data is embedded in a linear subspace, PCA is guar-anteed to discover the dimensionality of the subspace and produces a compact representation.

In many real world problems, however, there is no ev-idence that the data is sampled from a linear subspace. For example, it is always believed that the face images are sampled from a nonlinear low-dimensional manifold which is embedded in the high-dimensional ambient space [19]. Various researchers (see [2, 24, 26]) have considered the case when the data lives on or close to a low dimensional sub-manifold of the high dimensional ambient space. One hopes then to estimate geometrical and topological prop-erties of the sub-manifold from random points ( X  X cattered data X ) lying on this unknown sub-manifold. Along this di-rection, many subspace learning algorithms have been pro-posed for face recognition. Some popular ones include Lo-cality Preserving Projection (LPP) [19], Neighborhood Pre-serving Embedding (NPE) [17] and Isometric Projection (IsoP) [5]. Despite the different motivations of these al-gorithms, they can be nicely interpreted in a general graph embedding framework [3, 19, 28].

One of the major disadvantages of all the above algo-rithms is that the learned projective functions are linear combinations of all the original features, thus it is often difficult to interpret the results. Recently, there are con-siderable interests on developing sparse subspace learning algorithms. Zou et al . [30] proposed an elegant sparse PCA algorithm (SPCA) using their  X  X lastic Net X  frame-work for L 1 -penalized regression on regular principle com-ponents, solved very efficiently using least angle regression (LARS) [11]. Subsequently, d X  X spremont et al .[9]relaxed the hard cardinality constraint and solved for a convex ap-proximation using semi-definite programming. In [21, 22], Moghaddam et al . proposed a spectral bounds framework for sparse subspace learning. Particularly, they proposed both exact and greedy algorithms for sparse PCA and sparse LDA.

In this paper, we propose a novel Unified Sparse Sub-space Learning framework (USSL), for sparse projections learning. The proposed approach is fundamentally based on regression and spectral graph analysis [8]. Specifically, USSL decomposes the subspace learning as a two-step ap-proach: graph embedding for responses learning and regres-sion for projective functions learning. This decomposition links subspace leaning and regression. By incorporating the regression as a building block, different kinds of regular-izers can be naturally incorporated. With a L 1 -norm regu-larizer ( lasso or elastic net ), the sparse projections can be efficiently computed in USSL.

The specific contributions of this paper include:  X  It reviews and provides a unified graph embedding  X  It gives the formulation of sparse subspace learning  X  It proposes a novel unified sparse subspace learning  X  We have performed extensive experimental compar-We summarize our findings and discuss extensions to the current work in Section 6, which concludes the paper.
In this Section, we provide a general framework of anal-ysis for the existing subspace learning algorithms from the graph embedding viewpoint.

Suppose we have m data samples { x i } m i =1  X  R n , X = [ x 1 ,  X  X  X  , x m ] . In the past decades, many dimensionality re-duction algorithms have been proposed to find a low dimen-sional representation of x i . Despite the different motiva-tions of these algorithms, they can be nicely interpreted in a general graph embedding framework [3, 19, 28].

Given a graph G with m vertices, each vertex represents a data point. Let W be a symmetric m  X  m matrix with W ij having the weight of the edge joining vertices i and j . The G and W can be defined to characterize certain statis-tical or geometric properties of the data set. The purpose of graph embedding is to represent each vertex of the graph as a low dimensional vector that preserves similarities be-tween the vertex pairs, where similarity is measured by the edge weight.

Let y =[ y 1 ,y 2 ,  X  X  X  ,y m ] T be the map from the graph to the real line. The optimal y is given by minimizing under appropriate constraint. This objective function incurs a heavy penalty if neighboring vertices i and j are mapped far apart. Therefore, minimizing it is an attempt to ensure that if vertices i and j are  X  X lose X  then y i and y j are close as well [15]. With some simple algebraic formulations, we have where L = D  X  W is the graph Laplacian [8] and D is a diagonal matrix whose entries are column (or row, since W is symmetric) sums of W , D ii = j W ji . Finally, the min-imization problem reduces to a quadratically-constrained quadratic program (QCQP): The constraint y T D y =1 removes an arbitrary scaling fac-tor in the embedding. Notice that L = D  X  W , it is easy to see that the above optimization problem has the following equivalent variation: The optimal y  X  X  can be obtained by solving the maximum eigenvalue eigen-problem [12]: Many recently proposed manifold learning algorithms, like ISOAMP [26], Laplacian Eigenmap [2], Locally Linear Embedding [24], can be interpreted in this framework with different choice of W .

The graph embedding approach described above only provides the mappings for the graph vertices in the train-ing set. For classification purpose ( e.g ., face recognition, text categorization), a mapping for all samples, including new test samples, is required. If we choose a linear func-tion, i.e ., y i = f ( x i )= a T x i ,wehave y = X T a .Eq.(1) can be rewritten as: The optimal a  X  X  are the eigenvectors corresponding to the maximum eigenvalue of eigen-problem: This approach is called Linear extension of Graph Embed-ding (LGE). With different choices of W ,theLGEframe-work leads to many popular linear dimensionality reduction algorithms, e.g ., LDA, LPP and NPE. We will briefly list the choices of W for these algorithms as follows. LDA:
Suppose we have c classes and the t -th class have m t samples, m 1 +  X  X  X  + m c = m . Define With such W , it is easy to check that D = I . Please see [19], [7] for the detailed derivation.
 LPP:
Let N k ( x i ) denote the set of k nearest neighbors of x For supervised case, one can also integrate the label infor-mation into W by searching the k nearest neighbors of x i among the points sharing the same label with x i . Please see [19] for the details.
 NPE:
Let N k ( x i ) denote the set of k nearest neighbors of x and M be a m  X  m local reconstruction coefficient matrix. M is defined as follows: For i -th row of M , M ij =0 if x j /  X  N k ( x i ) . The other M ij can be computed by minimizing the following objec-tive function, Define and it is easy to check that D = I . Please see [17], [28] for the detailed derivation.

All the above mentioned linear subspace learning algo-rithms need to solve the eigen-problem in Eqn. (4). To get a stable solution of this eigen-problem, the matrices XDX T is required to be non-singular [14] which is not true when the number of features is larger than the num-ber of samples. A popular way to deal with the singularity of XDX T is to apply the idea of regularization, by adding some constant values to the diagonal elements of XDX T , as XDX T +  X I , for any  X &gt; 0 . It is easy to see that XDX T +  X I is nonsingular. The computational complex-ity of this approach scales as O ( n 3 + mn 2 ) where m is the number of samples and n is the number of features.
For simplicity, we define A = XWX T , B = XDX T and rewrite the optimization problem of LGE in Eqn. (3) as: Following [22], we define the Sparse Subspace Learning (SSL) optimization in terms of the following cardinality-constrained QCQP: The feasible set is all sparse a  X  R n with k non-zero el-ements and card( a ) as their L 0 -norm. Unfortunately, this optimization problem is NP-hard and therefor generally in-tractable .

In [21, 22], Moghaddam et al . proposed a spectral bounds framework for sparse subspace learning. Particu-larly, they proposed both exact and greedy algorithms for sparse PCA and sparse LDA. Their spectral bounds frame-work is based on the following optimal condition of the sparse solution.

A sparse vector a  X  R n with cardinality k yielding the maximum objective value in Eqn. (8) would necessarily imply that where b  X  R k contains the k non-zero elements in a and the k  X  k principle sub-matrices of A and B obtained by delet-ing the rows and columns corresponding to the zero indices of a .The k -dimensional quadratic form in b is equivalent to a standard unconstrained generalized Rayleigh quotient, which can be solved by a generalized eigen-problem.
The above observation gives the exact algorithm for sparse subspace learning: a discrete search for the k in-dices which maximize  X  max of the subproblem ( A k ,B k ) . However, such observation does not suggest an efficient al-gorithm because an exhaustive search is still NP-hard. To solve this problem, Moghaddam et al . proposed an effi-cient greedy algorithm which combines backward elimina-tion and forward selection [21, 22]. As we discussed in Section 2, many of the popular graph-based subspace learn-ing algorithms can be formulated as the generalized eigen-problem, Moghaddam X  X  approach provides a general solu-tion for learning sparse projections in all these subspace learning algorithms. However, there are two major draw-backs of their approach: 1. Even their algorithm is a greedy one, the cost of back-2. In reality, more than one projective functions are usu-
In [30], Zou et al . proposed an elegant sparse PCA al-gorithm (SPCA) using their  X  X lastic Net X  framework for L -penalized regression on regular principle components, solved very efficiently using least angle regression (LARS) [11]. The key idea of SPCA is formulating PCA as a regression-type optimization problem.

Without loss of generality, we assume the data are cen-tered 1 . The PCA objective function is and the optimal a  X  X  are the eigenvectors with respect to the maximum eigenvalues of the following eigen-problem: Suppose the rank of X is r and the Singular Value Decom-position (SVD) of X is: it is easy to verify that the column vectors in U are the eigenvectors of XX T [14], i.e ., the projective functions of PCA. Let Y =[ y 1 ,  X  X  X  , y r ]= U T X = X  V T , each row vector of Y is the sample vector in the r -dimensional PCA subspace. Thus, the projective functions of PCA are essen-tially the solutions of the linear equation systems: in other words, a t is the solution of the regression system: where y t i is the i -th element of y t .Zou et al . [30] add L regularizer to get the sparse solutions: where a j is the j -th element of a . The above regression problem is called Lasso [16] and can be efficiently com-puted using LARS algorithm [11].

If we want to apply the similar technique to those lin-ear graph embedding algorithms (LGE), the key problem is how we can formulate LGE as a regression-type optimiza-tion problem.
In this section, we describe our regression formulation of graph based subspace learning which is the key of the proposed unified sparse subspace learning approach.
In order to formulate LGE as a regression-type optimiza-tion problem, we use the following theorem: Theorem 1 Let y be the eigenvector of eigen-problem in Eqn. (2) with eigenvalue  X  .If X T a = y , then a is the eigenvector of eigen-problem in Eqn. (4) with the same eigenvalue  X  .
 Proof We have W y =  X D y . At the left side of Eqn. (4), replace X T a by y ,wehave XWX T a = XW y = X X D y =  X XD y =  X XDX T a Thus, a is the eigenvector of eigen-problem Eqn. (4) with the same eigenvalue  X  .

Theorem (1) shows that instead of solving the eigen-problem in Eqn. (4), the linear projective functions can be obtained through two steps: 1. Solve the eigen-problem in Eqn. (2) to get y . 2. Find a which satisfies X T a = y . In reality, such a It is clear that Eqn. (12) is exactly what we want, the regression-type formulation of LGE problem.
In the situation that the number of samples is smaller than the number of features, the minimization problem (12) is ill posed . We may have infinitely many solutions to the linear equations system X T a = y (the system is underde-termined). The most popular way to solve this problem is to apply the regularization technique, i.e ., impose a penalty on the norm of a . L 2 -norm and L 1 -norm are two of the most popular ones.

With a L 2 -norm on a ,wehave where a j is the j -th element of a . This is usually referred as ridge regression in statistics [16]. The  X   X  0 is a parameter to control the amounts of shrinkage. The ridge penalty does not provide a sparse solution.

With a L 1 -norm on a ,wehave which is usually referred as lasso regression [16]. Due to the nature of the L 1 penalty, some coefficients will be shrunk to exact zero if  X  is large enough. Therefore the lasso produces a sparse model, which is exactly what we want. However, the lasso has several limitations as pointed out in [29]. The most relevant one to this work is that the number of selected features by the lasso is limited by the number of samples. For example, if applied to the face im-age data where there are thousands of features ( n&gt; 1000 ) with less than 100 samples ( m&lt; 100 ), the lasso can only select at most m features, which is clearly unsatisfactory. The Elastic Net [29] generalizes the lasso to overcome its drawbacks by combining both the ridge and lasso penalty:
When  X &gt; 0 or  X &gt; 0 , the solution of the above opti-mization problem will not satisfy the linear equations sys-tem X T a = y and a will not be the eigenvector of eigen-problem in Eqn. (4). It is interesting and important to see when the solution of the optimization problem in Eqn. (15) gives the exact solutions of eigen-problem (4). Specifically, we have the following theorem: Theorem 2 Suppose y is the eigenvector of eigen-problem in Eqn. (2), if y is in the space spanned by row vectors of X , the solution of the optimization problem in Eqn. (15) will be the eigenvector of eigen-problem in Eqn. (4) as  X  and  X  decease to zero.
 Proof See Appendix A.

When the the number of features is larger than the num-ber of samples, the sample vectors are usually linearly in-dependent, i.e ., rank ( X )= m . In this case, we will have a stronger conclusion which is shown in the following Corol-lary.
 Corollary 3 If the sample vectors are linearly independent, i.e ., rank ( X )= m , all the solution of the optimization problem in Eqn. (15) (with different eigenvector y  X  X ) are the eigenvectors of eigen-problem in Eqn. (4) as  X  and  X  deceases to zero.
 Proof See Appendix B.

Our above two-step approach essentially performs re-gression after the spectral analysis of the graph, we called it Spectral Regression (SR) [6]. With lasso penalty, it provides a Unified Sparse Subspace Learning framework (USSL).
With different choices of W , the optimization problem in Eqn. (3) gives the solutions of various subspace learn-ing algorithms, i.e ., LDA, LPP and NPE. Thus, the USSL approach introduced in the previous section provides the sparse solutions of LDA, LPP and NPE.

Generally, we need to solve the eigen-problem in Eqn. (2) to get the responses vectors y  X  X . In some cases, i.e .LDA, the W has a block diagonal structure and there is no need to solve the eigen-problem.

Without loss of generality, we assume that the data points in { x 1 ,  X  X  X  , x m } are ordered according to their labels. It is easy to check that the matrix W defined in Eqn. (5) has a block-diagonal structure where c is the number of classes, m t is the number of sam-ples in t -th class and { W ( t ) } c t =1 is a m t  X  m t the elements equal to 1 /m t . Since the W is block-diagonal, its eigenvalues and eigenvectors are the union of the eigen-values and eigenvectors of its blocks (the latter padded ap-propriately with zeros) [14]. It is straightforward to show value 1, where e ( t ) =[1 , 1 ,  X  X  X  , 1] T . Also there is only one non-zero eigenvalue of W ( t ) because the rank of W ( t ) Thus, there are exactly c eigenvectors of W with the same eigenvalue 1. These eigenvectors are Since 1 is a repeated eigenvalue of W , we could just pick any other c orthogonal vectors in the space spanned by { y k } , and define them to be our c eigenvectors. The vector of all ones e is naturally in the spanned space. This vector is useless since the responses of all the data points are the same. In reality, we can pick e as our first eigenvector and use Gram-Schmidt process to get the remaining c  X  1 or-thogonal eigenvectors. The vector of all ones can then be removed.

In binary classification case, the above procedure will produce one response vector This is consistent with the previous well-known result on the relationship between LDA and regression for a binary problem [16]. The framework proposed in this paper ex-tends this relation to multi-class case. Moreover, this frame-work also establishes the connection between regression and many other graph based subspace learning algorithms, e.g ., LPP, NPE.
The USSL computation involves two steps: responses generation (calculate the eigenvectors of eigen-problem in Eqn. (2)) and regularized regression.

For the W in LDA, the cost of the first step is mainly the cost of Gram-Schmidt method, which is O ( mc 2 ) [25]. For a k -NN graph W in LPP, the cost of the first step is O ( m 2 n + m 2 log m + qdmk ) . O ( m 2 n ) is used to calculate the pairwise distance between m samples with n features and O ( m 2 log m ) is used for k -nearest neighbors finding for all the m samples. The k -NN graph matrix W is sparse and the Lanczos algorithm [14] can be used to efficiently compute the first d eigenvectors of the eigen-problem in Eqn. (2) within O ( qdmk ) , where q is number of iterations in Lanczos.

All of the three types of regularized regression problems can be solved in O ( n 3 + mn 2 ) [16][11]. By using the Least Angel Regression (LARS) algorithm [11], the entire solu-tion path (the solutions with all the possible cardinality on a ) of lasso and elastic net with a specific  X  can be computed in O ( n 3 + mn 2 ) .

Considering m c and m d , USSL provides a sparse LDA solution with O ( n 3 + mn 2 ) complexity and a sparse LPP solution with O ( m 2 n + m 2 log m + n 3 + mn complexity. This complexity is exactly the same as the ordinary non-sparse solution solved by generalized eigen-problem. Comparing to the O ( n 4 + mn 2 ) greedy algorithm described in [22], USSL is much more efficient.
In this section, we investigate the performance of our proposed USSL approach for both supervised learning (face recognition) and unsupervised learning (face clustering). All of our experiments have been performed on an Intel Pentium D 3.20GHz Linux machine with 2GB memory.
Two face databases were used in the experiment. The first one is the PIE (Pose, Illumination, and Experience) database 2 from CMU, and the second one is the Extended Yale-B database 3 .

The CMU PIE face database contains 68 human subjects with 41,368 face images as a whole. The face images were captured by 13 synchronized cameras and 21 flashes, under varying pose, illumination and expression. We choose the frontal poses (C27) and use all the images under different illuminations and expressions, thus we get 3329 face images in total.

The Extended Yale-B face database contains 16128 im-ages of 38 human subjects under 9 poses and 64 illumina-tion conditions. In this experiment, we choose the frontal pose and use all the images under different illumination. Fi-nally we get 2414 images in total.
 All the face images are manually aligned and cropped. The size of each cropped image is 32  X  32 pixels, with 256 gray levels per pixel. Thus each image is represented as a 1024-dimensional vector. In this experiment, we use the W in Eqn. (5). Thus, USSL provides a sparse LDA solution. We compare our algorithm with PCA, LDA and SparsePCA [30]. In face recognition, PCA and LDA are also called Eigenface [27] and Fisherface [1]. They are two of the most popular lin-ear methods for face recognition. We do not compare with Sparse LDA [22] since it can only be applied to two-class case. Please refer to [22] for the details.

For each database, r ( =33 , 50 , 67 ) percent of samples are randomly selected for training and the rest are used for testing. The training samples are used to learn the ba-sis functions. By using these basis functions, the testing images can be mapped into lower dimensional subspace where recognition is carried out by using nearest neigh-bor classifier. 5-fold cross validation has been performed in SparsePCA and USSL for selecting the best cardinality of the basis functions. The choices of the cardinality are 10, 20,  X  X  X  100, 150, 200,  X  X  X  , 1000, 1024.

For each given r , we average the recognition results over 20 random splits. Figure 1 and 2 show the plots of error rate versus dimensionality reduction for the PCA, SparsePCA, LDA, USSL and baseline methods on PIE and Yale-B databases, respectively. For the baseline method, the recognition is simply performed in the original 1024-dimensional image space without any dimensionality reduc-tion. Note that, the upper bound of the dimensionality of LDA is c  X  1 where c is the number of individuals [10]. We use the LDA graph W as defined in Section 2 in our USSL algorithm. Thus, the upper bound of the dimensionality of USSL is also c  X  1 . As can be seen, the performance of the PCA, SparsePCA, LDA and USSL algorithms varies with the number of dimensions. We show the best results to-gether with the standard deviations obtained by them in Ta-ble 1 and 2 and the corresponding face subspaces are called optimal face subspace for each method. Particularly, we also shown the sparsity of the basis functions for these al-gorithms. The sparsity is computed as the ratio of the num-ber of zero entries and the total number of entries. As can be seen, the sparsity for PCA and LDA are both zero, while the sparsity for sparse PCA and USSL are very high.
In this subsection, we investigate the use of our pro-posed approach for face clustering. Face clustering is an unsupervised task and we compare our algorithm with PCA, SparsePCA and Locality Preserving Projection (LPP) [18][19]. We use the same p -nearest neighbor graph in LPP and USSL. Thus, USSL provides a sparse LPP solution. We empirically set the value of p to 5.

We choose K-means as our clustering algorithm. K-means can be performed in the original feature space (Base-line) or in the reduced feature space (by using the dimen-sionality reduction algorithms, e.g ., PCA, LPP and USSL). The clustering result is evaluated by comparing the ob-tained label of each image with that provided by the ground truth. We use the normalized mutual information ( MI )to measure the clustering performance [4]. Let C denote the set of clusters obtained from the ground truth and C ob-tained from an algorithm. Their mutual information metric MI ( C, C ) is defined as follows: where p ( c i ) and p ( c j ) are the probabilities that a sample arbitrarily selected from the data set belongs to the clusters c and c j , respectively, and p ( c i ,c j ) is the joint probability that the arbitrarily selected document belongs to the clusters c as well as c j at the same time. In our experiments, we use the normalized mutual information MI as follows: where H ( C ) and H ( C ) are the entropies of C and C ,re-spectively. It is easy to check that MI ( C, C ) ranges from 0to1. MI =1 if the two sets of clusters are identical, and MI =0 if the two sets are independent.

Figure (3(a)) shows the plot of normalized mutual in-formation versus dimensionality for the PCA, SparsePCA, LPP, USSL and baseline methods. As can be seen, all the methods obtain the best performance with dimensionality less than 100, and there is no performance improvement with more dimensions. Our USSL algorithm outperforms the other four methods. LPP performs the second best. PCA performs the worst, close to the baseline.

Figure (3(b)) shows the performances of all the algo-rithm in the 100 -dimensional subspace. We show the per-formance change with the cardinality of basis functions in SparsePCA and USSL. As can be seen, the best perfor-mance is obtained with relatively small cardinality.
In this paper, we proposed a novel unified framework for learning sparse projections. Our framework is developed from a graph embedding viewpoint of dimensionality re-duction algorithms. It combines the spectral graph analysis and regularized regression to provide an efficient and effec-tive approach for sparse subspace learning problem. Many recently proposed linear subspace learning algorithms, e.g ., LDA [1], LPP [18], NPE [17] and IsoP [5] can be inter-preted as the linear extensions of specific graph embedding. Thus, all these algorithms can be fit into our framework and get sparse solutions. Extensive experimental results show effectiveness of the proposed approach.
 [1] P. N. Belhumeur, J. P. Hepanha, and D. J. Kriegman. [2] M. Belkin and P. Niyogi. Laplacian eigenmaps and [3] M. Brand. Continuous nonlinear dimensionality re-[4] D. Cai, X. He, and J. Han. Document clustering us-[5] D. Cai, X. He, and J. Han. Isometric projection. [6] D. Cai, X. He, and J. Han. Spectral regres-[7] D. Cai, X. He, and J. Han. SRDA: An efficient al-[8] F. R. K. Chung. Spectral Graph Theory , volume 92 [9] A. d X  X spremont, L. E. Chaoui, M. I. Jordan, and [10] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Clas-[11] B. Efron, T. Hastie, I. Johnstone, and R. Tibshi-[12] R. Fletcher. Practical methods of optimization . John [13] V. Gaede and O. G  X  unther. Multidimensional access [14] G. H. Golub and C. F. V. Loan. Matrix computations . [15] S. Guattery and G. L. Miller. Graph embeddings and [16] T. Hastie, R. Tibshirani, and J. Friedman. The Ele-[17] X. He, D. Cai, S. Yan, and H.-J. Zhang. Neighborhood [18] X. He and P. Niyogi. Locality preserving projections. [19] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. [20] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate [21] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral [22] B. Moghaddam, Y. Weiss, and S. Avidan. General-[23] R. Penrose. A generalized inverse for matrices. In [24] S. Roweis and L. Saul. Nonlinear dimensional-[25] G. W. Stewart. Matrix Algorithms Volume I: Basic [26] J. Tenenbaum, V. de Silva, and J. Langford. A global [27] M. Turk and A. Pentland. Eigenfaces for recognition. [28] S. Yan, D. Xu, B. Zhang, and H.-J. Zhang. Graph [29] H. Zhou and T. Hastie. Regression shrinkage and se-[30] H. Zhou, T. Hastie, and R. Tibshirani. Sparse prin-Proof Let  X  =0 , the regularized least squares in Eqn. (15) can be rewritten in the matrix form as: a = arg min Requiring the derivative of right side with respect to a van-ish, we get Suppose rank ( X )= r , the SVD decomposition of X is where  X = diag (  X  1 ,  X  X  X  , X  r ), U  X  R n  X  r , V  X  R m  X  r we have U T U = V T V = I .The y is in the space spanned by row vectors of X , therefor, y is in the space spanned by column vectors of V . Thus, y can be represented as the lin-ear combination of the column vectors of V . Moreover, the combination is unique because the column vectors of V are linear independent. Suppose the combination coefficients are b 1 ,  X  X  X  ,b r .Let b =[ b 1 ,  X  X  X  ,b r ] T ,wehave: To continue our proof, we need introduce the concept of pseudo inverse of a matrix [23], which we denote as (  X  ) Specifically, pseudo inverse of the matrix X can be com-puted by the following two ways: and The above limit exists even if X T X is singular and ( X T X )  X  1 does not exist [23]. Thus, the regularized least squares solution in Eqn. (19) a = XX T +  X I Combine with the equation in Eqn. (20), we have X T a = V  X  U T a = V  X  U T U  X   X  1 V T y = VV T y = y By Theorem (1), a is the eigenvector of eigen-problem in Eqn. (4).
 Proof The matrices W and D are of size m  X  m and there are m eigenvectors { y j } m j =1 of eigen-problem (2). Since rank ( X )= m , all these m eigenvectors y j are in the space spanned by row vectors of X . By Theorem (2), all m corre-sponding a j in Eqn (19) are eigenvectors of eigen-problem in Eqn. (4) as  X  and  X  decreases to zero.
