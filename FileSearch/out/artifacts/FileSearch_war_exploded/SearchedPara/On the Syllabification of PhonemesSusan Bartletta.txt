 Syllabification is the process of dividing a word into its constituent syllables. Although some work has been done on syllabifying orthographic forms (M  X uller et al., 2000; Bouma, 2002; Marchand and Damper, 2007; Bartlett et al., 2008), syllables are, technically speaking, phonological entities that can only be composed of strings of phonemes. Most linguists view syllables as an important unit of prosody because many phonological rules and con-straints apply within syllables or at syllable bound-aries (Blevins, 1995).

Apart from their purely linguistic significance, syllables play an important role in speech synthesis and recognition (Kiraz and M  X obius, 1998; Pearson et al., 2000). The pronunciation of a given phoneme tends to vary depending on its location within a syl-lable. While actual implementations vary, text-to-speech (TTS) systems must have, at minimum, three components (Damper, 2001): a letter-to-phoneme (L2P) module, a prosody module, and a synthesis module. Syllabification can play a role in all three modules.

Because of the productive nature of language, a dictionary look-up process for syllabification is in-adequate. No dictionary can ever contain all possi-ble words in a language. For this reason, it is neces-sary to develop systems that can automatically syl-labify out-of-dictionary words.

In this paper, we advance the state-of-the-art in both categorical (non-statistical) and supervised syllabification. We outline three categorical ap-proaches based on common linguistic theories of syllabification. We demonstrate that when imple-mented carefully, such approaches can be very ef-fective, approaching supervised performance. We also present a data-driven, discriminative solution: a Support Vector Machine Hidden Markov Model (SVM-HMM), which tags each phoneme with its syllabic role. Given enough data, the SVM-HMM achieves impressive accuracy thanks to its ability to capture context-dependent generalizations, while also memorizing inevitable exceptions. Our ex-periments on English, Dutch and German demon-strate that our SVM-HMM approach substantially outperforms the existing state-of-the-art learning ap-proaches. Although direct comparisons are difficult, our system achieves over 99% word accuracy on German and Dutch, and the highest reported accu-racy on English.

The paper is organized as follows. We outline common linguistic theories of syllabification in Sec-tion 2, and we survey previous computational sys-tems in Section 3. Our linguistically-motivated ap-proaches are described in Section 4. In Section 5, we describe our system based on the SVM-HMM. The experimental results are presented in Section 6. There is some debate as to the exact structure of a syllable. However, phonologists are in gen-eral agreement that a syllable consists of a nucleus (vowel sound), preceded by an optional onset and followed by an optional coda. In many languages, both the onset and the coda can be complex, i.e. , composed of more than one consonant. For exam-ple, the word breakfast [br lables, of which the first has a complex onset [br], and the second a complex coda [st]. Languages dif-fer with respect to various typological parameters, such as optionality of onsets, admissibility of co-das, and the allowed complexity of the syllable con-stituents. For example, onsets are required in Ger-man, while Spanish prohibits complex codas.

There are a number of theories of syllabification; we present three of the most prevalent. The Legal-ity Principle constrains the segments that can be-gin and end syllables to those that appear at the be-ginning and end of words. In other words, a sylla-ble is not allowed to begin with a consonant clus-ter that is not found at the beginning of some word, or end with a cluster that is not found at the end of some word (Goslin and Frauenfelder, 2001). Thus, a word like admit [ dm m t] because [dm] never appears word-initially or word-finally in English. A shortcoming of the le-gality principle is that it does not always imply a unique syllabification. For example, in a word like askew [ skju], the principle does not rule out any of [ -skju], [ s-kju], or [ sk-ju], as all three employ le-gal onsets and codas.

The Sonority Sequencing Principle (SSP) pro-vides a stricter definition of legality. The sonor-ity of a sound is its inherent loudness, holding fac-tors like pitch and duration constant (Crystal, 2003). Low vowels like [a], the most sonorous sounds, are high on the sonority scale, while plosive consonants like [t] are at the bottom. When syllabifying a word, SSP states that sonority should increase from the first phoneme of the onset to the syllable X  X  nu-cleus, and then fall off to the coda (Selkirk, 1984). Consequently, in a word like vintage [v can rule out a syllabification like [v [n] is more sonorant than [t]. However, SSP does not tell us whether to prefer [v Moreover, when syllabifying a word like vintner [v n r], even though [tn] is an illegal onset in English.
Both the Legality Principle and SSP tell us which onsets and codas are permitted in legal syllables, and which are not. However, neither theory gives us any guidance when deciding between legal onsets. The Maximal Onset Principle addresses this by stating we should extend a syllable X  X  onset at the expense of the preceding syllable X  X  coda whenever it is legal to do so (Kahn, 1976). For example, the principle gives preference to [ -skju] and [v alternatives. Unlike tasks such as part of speech tagging or syn-tactic parsing, syllabification does not involve struc-tural ambiguity. It is generally believed that syllable structure is usually predictable in a language pro-vided that the rules have access to all conditioning factors: stress, morphological boundaries, part of speech, etymology, etc. (Blevins, 1995). However, in speech applications, the phonemic transcription of a word is often the only linguistic information avail-able to the system. This is the common assumption underlying a number of computational approaches that have been proposed for the syllabification of phonemes.

Daelemans and van den Bosch (1992) present one of the earliest systems on automatic syllabification: a neural network-based implementation for Dutch. Daelemans et al. (1997) also explore the application of exemplar-based generalization (EBG), sometimes called instance-based learning. EBG generally per-forms a simple database look-up to syllabify a test pattern, choosing the most common syllabification. In cases where the test pattern is not found in the database, the most similar pattern is used to syllab-ify the test pattern.

Hidden Markov Models (HMMs) are another popular approach to syllabification. Krenn (1997) introduces the idea of treating syllabification as a tagging task. Working from a list of syllabified phoneme strings, she automatically generates tags for each phone. She uses a second-order HMM to predict sequences of tags; syllable boundaries can be trivially recovered from the tags. Demberg (2006) applies a fourth-order HMM to the syllabification task, as a component of a larger German text-to-speech system. Schmid et al. (2007) improve on Demberg X  X  results by applying a fifth-order HMM that conditions on both the previous tags and their corresponding phonemes.

Kiraz and M  X obius (1998) present a weighted finite-state-based approach to syllabification. Their language-independent method builds an automaton for each of onsets, nuclei, and codas, by count-ing occurrences in training data. These automatons are then composed into a transducer accepting se-quences of one or more syllables. They do not report quantitative results for their method.

Pearson et al. (2000) compare two rule-based sys-tems (they do not elaborate on the rules employed) with a CART decision tree-based approach and a  X  X lobal statistics X  algorithm. The global statistics method is based on counts of consonant clusters in contexts such as word boundaries, short vow-els, or long vowels. Each test word has syllable boundaries placed according to the most likely lo-cation given a cluster and its context. In experi-ments performed with their in-house dataset, their statistics-based method outperforms the decision-tree approach and the two rule-based methods.
M  X uller (2001) presents a hybrid of a categori-cal and data-driven approach. First, she manually constructs a context-free grammar of possible sylla-bles. This grammar is then made probabilistic using counts obtained from training data. M  X uller (2006) attempts to make her method language-independent. Rather than hand-crafting her context-free grammar, she automatically generates all possible onsets, nu-clei, and codas, based on the phonemes existing in the language. The results are somewhat lower than in (M  X uller, 2001), but the approach can be more eas-ily ported across languages.

Goldwater and Johnson (2005) also explore us-ing EM to learn the structure of English and Ger-man phonemes in an unsupervised setting, following M  X uller in modeling syllable structure with PCFGs. They initialize their parameters using a deterministic parser implementing the sonority principle and esti-mate the parameters for their maximum likelihood approach using EM.

Marchand et al. (2007) apply their Syllabification by Analogy (SbA) technique, originally developed for orthographic forms, to the pronunciation do-main. For each input word, SbA finds the most sim-ilar substrings in a lexicon of syllabified phoneme strings and then applies the dictionary syllabifica-tions to the input word. Their survey paper also in-cludes comparisons with a method broadly based on the legality principle. The authors find their legality-based implementation fares significantly worse than SbA. Categorical approaches to syllabification are appeal-ing because they are efficient and linguistically intu-itive. In addition, they require little or no syllable-annotated data. We present three categorical al-gorithms that implement the linguistic insights out-lined in Section 2. All three can be viewed as vari-ations on the basic pseudo-code shown in Figure 1. Every vowel is labeled as a nucleus, and every con-sonant is labeled as either an onset or a coda. The algorithm labels all consonants as onsets unless it is illegal to do so. Given the labels, it is straightfor-ward to syllabify a word. The three methods differ in how they determine a  X  X egal X  onset.

As a rough baseline, the M AX O NSET implemen-tation considers all combinations of consonants to be legal onsets. Only word-final consonants are labeled as codas.

L EGALITY combines the Legality Principle with onset maximization. In our implementation, we col-lect all word-initial consonant clusters from the cor-pus and deem them to be legal onsets. With this method, no syllable can have an onset that does not appear word-initially in the training data. We do not test for the legality of codas. The performance of L
EGALITY depends on the number of phonetic tran-scriptions that are available, but the transcriptions need not be annotated with syllable breaks.
 S ONORITY combines the Sonority Sequencing Principle with onset maximization. In this approach, an onset is considered legal if every member of the onset ranks lower on the sonority scale than ensuing consonants. S ONORITY requires no training data be-cause it implements a sound linguistic theory. How-ever, an existing development set for a given lan-guage can help with defining and validating addi-tional language-specific constraints.

Several sonority scales of varying complexity have been proposed. For example, Selkirk (1984) specifies a hierarchy of eleven distinct levels. We adopt a minimalistic scale shown in Figure 2. which avoids most of the disputed sonority contrasts (Jany et al., 2007). We set the sonority distance parame-ter to 2, which ensures that adjacent consonants in the onset differ by at least two levels of the scale. For example, [pr] is an acceptable onset because it is composed of an obstruent and a liquid, but [pn] is not, because nasals directly follow obstruents on our sonority scale.

In addition, we incorporate several English-specific constraints listed by Kenstowicz (1994, pages 257 X 258). The constraints, or filters , prohibit complex onsets containing: (i) two labials (e.g., [pw], [bw], [fw], [vw]), (ii) a non-strident coronal followed by a lateral (iii) a voiced fricative (e.g., [vr], [zw], except [vj]), (iv) a palatal consonant (e.g., [ A special provision allows for prepending the phoneme [s] to onsets beginning with a voiceless plosive. This reflects the special status of [s] in En-glish, where onsets like [sk] and [sp] are legal even though the sonority is not strictly increasing. If annotated data is available, a classifier can be trained to predict the syllable breaks. A Support Vector Machine (SVM) is a discriminative super-vised learning technique that allows for a rich fea-ture representation of the input space. In principle, we could use a multi-class SVM to classify each phoneme according to its position in a syllable on the basis of a set of features. However, a traditional SVM would treat each phoneme in a word as an in-dependent instance, preventing us from considering interactions between labels. In order to overcome tun et al., 2003), an instance of the Structured SVM formalism (Tsochantaridis et al., 2004) that has been specialized for sequence tagging.

When training a structured SVM, each training instance x the set of possible labels, Y ing instances x of phonemes, and their labels y represented as sequences of onset/nucleus/coda tags. For each training example, a feature vector  X ( x,y ) represents the relationship between the example and a candidate tag sequence. The SVM finds a weight vector w , such that w  X   X ( x,y ) separates correct tag-gings from incorrect taggings by as large a margin as possible. Hamming distance D ture how close a wrong sequence y is to y in turn impacts the required margin. Tag sequences that share fewer tags in common with the correct se-quence are separated by a larger margin.
 Mathematically, a (simplified) statement of the SVM learning objective is: This objective is only satisfied when w tags all train-ing examples correctly. In practice, slack variables are introduced, which allow us to trade off training accuracy and the complexity of w via a cost parame-ter. We tune this parameter on our development set.
The SVM-HMM training procedure repeatedly uses the Viterbi algorithm to find, for the current w and each ( x i ,y i ) training pair, the sequence y that most drastically violates the inequality shown in Equation 1. These incorrect tag sequences are added to a growing set, which constrains the quadratic op-timization procedure used to find the next w . The process iterates until no new violating sequences are found, producing an approximation to the inequality over all y  X  Y Tsochantaridis et al. (2004).

Given a weight vector w , a structured SVM tags new instances x according to: The SVM-HMM gets the HMM portion of its name from its use of the HMM Viterbi algorithm to solve this argmax . 5.1 Features We investigated several tagging schemes, described in detail by Bartlett (2007). During development, we found that tagging each phoneme with its syl-labic role (Krenn, 1997) works better than the simple binary distinction between syllable-final and other phonemes (van den Bosch, 1997). We also dis-covered that accuracy can be improved by number-ing the tags. Therefore, in our tagging scheme, the single-syllable word strengths [str beled with the sequence { O1 O2 O3 N1 C1 C2 C3 } .
Through the use of the Viterbi algorithm, our fea-ture vector  X ( x,y ) is naturally divided into emis-sion and transition features. Emission features link an aspect of the input word x with a single tag in the sequence y . Unlike a generative HMM, these emis-sion features do not require any conditional indepen-dence assumptions. Transition features link tags to tags. Our only transition features are counts of adja-cent tag pairs occurring in y .

For the emission features, we use the current phoneme and a fixed-size context window of sur-rounding phonemes. Thus, the features for the phoneme [k] in hockey [h preceding it, and the [i] following it. In experiments on our development set, we found that the optimal window size is nine: four phonemes on either side of the focus phoneme. Because the SVM-HMM is a linear classifier, we need to explicitly state any im-portant conjunctions of features. This allows us to capture more complex patterns in the language that unigrams alone cannot describe. For example, the bigram [ps] is illegal as an onset in English, but per-fectly reasonable as a coda. Experiments on the de-velopment set showed that performance peaked us-ing all unigrams, bigrams, trigrams, and four-grams found within our context window. We developed our approach using the English por-tion of the CELEX lexical database (Baayen et al., 1995). CELEX provides the phonemes of a word and its correct syllabification. It does not designate the phonemes as onsets, nuclei, or codas, which is the labeling we want to predict. Fortunately, extract-ing the labels from a syllabified word is straightfor-ward. All vowel phones are assigned to be nuclei; consonants preceding the nucleus in a syllable are assigned to be onsets, while consonants following the nucleus in a syllable are assigned to be codas.
The results in Table 1 were obtained on a test set of 5K randomly selected words. For training the SVM-HMM, we randomly selected 30K words not appearing in the test set, while 6K training examples were held out for development testing. We report the performance in terms of word accuracy (entire words syllabified correctly). Among the categori-cal approaches, S ONORITY clearly outperforms not only L EGALITY , but also tsylb (Fisher, 1996), an implementation of the complex algorithm of Kahn (1976), which makes use of lists of legal English onsets. Overall, our SVM-based approach is a clear winner.

The results of our discriminative method com-pares favorably with the results of competing ap-proaches on English CELEX. Since there are no standard train-test splits for syllabification, the comparison is necessarily indirect, but note that our training set is substantially smaller. For her language-independent PCFG-based approach, M  X uller (2006) reports 92.64% word accuracy on the set of 64K examples from CELEX using 10-fold cross-validation. The Learned EBG approach of van den Bosch (1997) achieves 97.78% word accu-racy when training on approximately 60K examples. Therefore, our results represent a nearly 50% reduc-tion of the error rate.

Though the SVM-HMM X  X  training data require-ments are lower than previous supervised syllabi-fication approaches, they are still substantial. Fig-ure 3 shows a learning curve over varying amounts of training data. Performance does not reach accept-able levels until 5K training examples are provided. 6.1 Error Analysis There is a fair amount of overlap in the errors made by the SVM-HMM and the S ONORITY . Table 4 shows a few characteristic examples. The CELEX syllabifications of tooth-ache and pass-ports fol-low the morphological boundaries of the compound words. Morphological factors are a source of er-rors for both approaches, but significantly more so for S ONORITY . The performance difference comes mainly from the SVM X  X  ability to handle many of these morphological exceptions. The SVM gener-ates the correct syllabification of northeast [nor ist], even though an onset of [ On the other hand, the SVM sometimes overgener-alizes, as in the last example in Table 4.
 6.2 The NETtalk Dataset Marchand et al. (2007) report a disappointing word accuracy of 54.14% for their legality-based imple-mentation, which does not accord with the results of our categorical approaches on English CELEX. Consequently, we also apply our methods to the dataset they used for their experiments: the NETtalk dictionary. NETtalk contains 20K English words; in the experiments reported here, we use 13K training examples and 7K test words.

As is apparent from Table 2, our performance degrades significantly when switching to NETtalk. The steep decline found in the categorical meth-ods is particularly notable, and indicates significant divergence between the syllabifications employed in the two datasets. Phonologists do not always agree on the correct syllable breaks for a word, but the NETtalk syllabifications are often at odds with linguistic intuitions. We randomly selected 50 words and compared their syllabifications against those found in Merriam-Webster Online. We found that CELEX syllabifications agree with Merriam-Webster in 84% of cases, while NETtalk only agrees 52% of the time.

Figure 5 shows several words from the NETtalk and CELEX datasets. We see that CELEX fol-lows the maximal onset principle consistently, while NETtalk does in some instances but not others. We also note that there are a number of NETtalk syllab-ifications that are clearly wrong, such as the last two examples in Figure 5. The variability of NETtalk is much more difficult to capture with any kind of principled approach. Thus, we argue that low per-formance on NETtalk indicate inconsistent syllabi-fications within that dataset, rather than any actual deficiency of the methods.

NETtalk X  X  variable syllabification practices notwithstanding, the SVM-HMM approach still outperforms the previous benchmark on the dataset. Marchand et al. (2007) report 88.53% word accu-racy for their SbA technique using leave-one-out testing on the entire NETtalk set (20K words). With fewer training examples, we reduce the error rate by almost 40%. 6.3 Other Languages We performed experiments on German and Dutch, the two other languages available in the CELEX lex-ical database. The German and Dutch lexicons of CELEX are larger than the English lexicon. For both languages, we selected a 25K test set, and two dif-ferent training sets, one containing 50K words and the other containing 250K words. The results are Method German Dutch M AX O NSET 19.51 23.44 S ONORITY 76.32 77.51 L EGALITY 79.55 64.31 SVM-HMM (50K words) 99.26 97.79 SVM-HMM (250K words) 99.87 99.16 presented in Table 3.

While our SVM-HMM approach is entirely lan-guage independent, the same cannot be said about other methods. The maximal onset principle appears to hold much more strongly for English than for Ger-man and Dutch (e.g., patron : [pe-tr n] vs. [pat-ron]). L
EGALITY and S ONORITY also appear to be less effective, possibly because of greater tendency for syllabifications to match morphological boundaries (e.g., English exclusive : [ clusief [ by our decision to employ the constraints of Ken-stowicz (1994), although they clearly pertain to En-glish. We expect that adapting them to specific lan-guages would bring the results closer to the level of the English experiments.
 Although our SVM system is tuned using an English development set, the results on both Ger-man and Dutch are excellent. We could not find any quantitative data for comparisons on Dutch, but the comparison with the previously reported re-sults on German CELEX demonstrates the qual-ity of our approach. The numbers that follow re-fer to 10-fold cross-validation on the entire lex-icon (over 320K entries) unless noted otherwise. Krenn (1997) obtains tag accuracy of 98.34%, com-pared to our system X  X  tag accuracy of 99.97% when trained on 250K words. With a hand-crafted gram-mar, M  X uller (2002) achieves 96.88% word accuracy on CELEX-derived syllabifications, with a training corpus of two million tokens. Without a hand-crafted grammar, she reports 90.45% word accu-racy (M  X uller, 2006). Applying a standard smoothing algorithm and fourth-order HMM, Demberg (2006) scores 98.47% word accuracy. A fifth-order joint N -gram model of Schmid et al. (2007) achieves 99.85% word accuracy with about 278K training points. However, unlike generative approaches, our SVM-HMM can condition each emission on large portions of the input using only a first-order Markov model, which implies much faster syllabification performance. 6.4 Direct Comparison with an MLE approach The results of the competitive approaches that have been quoted so far (with the exception of tsylb ) are not directly comparable, because neither the re-spective implementations, nor the actual train-test splits are publicly available. However, we managed to obtain the English and German data sets used by Goldwater and Johnson (2005) in their study, which focused primarily on unsupervised syllabi-fication. Their experimental framework is similar to (M  X uller, 2001). They collect words from running text and create a training set of 20K tokens and a test set of 10K tokens. The running text was taken from the Penn WSJ and ECI corpora, and the syl-labified phonemic transcriptions were obtained from CELEX. Table 4 compares our experimental results with their reported results obtained with: (a) su-pervised Maximum Likelihood training procedures, and (b) a Categorical Syllable Parser implementing the principles of sonority sequencing and onset max-imization without Kenstowicz X  X  (1994) onset con-straints.

The accuracy figures in Table 4 are noticeably higher than in Table 1. This stems from fundamen-tal differences in the experimental set-up; Goldwater and Johnson (2005) test on tokens as found in text, therefore many frequent short words are duplicated. Furthermore, some words occur during both training and testing, to the benefit of the supervised systems (SVM-HMM and Maximum Likelihood). Neverthe-less, the results confirm the level of improvement obtained by both our categorical and supervised ap-proaches. We have presented several different approaches to the syllabification of phonemes. The results of our linguistically-motivated algorithms, show that it is possible to achieve adequate syllabification word accuracy in English with no little or no syllable-annotated training data. We have demonstrated that the poor performance of categorical methods on En-glish NETtalk actually points to problems with the NETtalk annotations, rather than with the methods themselves.

We have also shown that SVM-HMMs can be used to great effect when syllabifying phonemes. In addition to being both efficient and language-independent, they establish a new state-of-the-art for English and Dutch syllabification. However, they do require thousands of labeled training examples to achieve this level of accuracy. In the future, we plan to explore a hybrid approach, which would benefit from both the generality of linguistic principles and the smooth exception-handling of supervised tech-niques, in order to make best use of whatever data is available.
 We are grateful to Sharon Goldwater for providing the experimental data sets for comparison. This re-search was supported by the Natural Sciences and Engineering Research Council of Canada and the Alberta Informatics Circle of Research Excellence.
