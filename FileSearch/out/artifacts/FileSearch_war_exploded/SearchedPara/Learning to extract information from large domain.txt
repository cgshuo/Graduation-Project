 In this article we describe a novel information extraction task on the web and show how it can be solved effectively using the emerging conditional exponential models. The task involves learning to find specific goal pages on large domain-specific websites. An example of such a task is to find computer science publications starting from university root pages. We encode this as a sequential labeling prob-lem solved using Conditional Random Fields (CRFs). These models enable us to exploit a wide variety of features in-cluding keywords and patterns extracted from and around hyperlinks and HTML pages, dependency among labels of adjacent pages, and existing databases of named entities in a unified probabilistic framework. This is an important ad-vantage over previous rule-based or generative models for tackling the challenges of diversity on web data. The World Wide Web continues to drive industry and re-searchers alike to new ways of information gathering and search that go beyond bulk crawling and keyword search. A steady, but not so populist, line of work has been on impos-ing some form of structure on the largely unstructured web, and then providing a richer search on the extracted struc-tures. Given the scale and diversity of the web, the dream of a full-fledged semantic embedding of the web is far from reality. However, it is possible to design task-oriented ex-traction primitives that are solvable with current technology and deployable immediately. This has driven a lot of work on web wrapper extraction in the past. However, most pre-vious wrapper systems have been successful only on fairly stylized machine-generated documents. In this article we present a new extraction task that pushes the envelope in this line of work by handling more heterogeneous sources, albeit from a limited domain.
 Often websites within a domain are structurally similar to each other. Humans are good at navigating these websites to reach specific information within it. Our goal is to learn the navigation path by observing the user X  X  clicks on a few example websites and then use the learnt model to auto-matically reach the goal pages using as few redundant page fetches as possible. We start from a listing of related web-sites and after watching the user find the specific information from a few websites in the list, we automate the search to the goal pages in the remaining.
 We present a scenario where such a capability would be use-ful. Citation portals like Citeseer need to gather publica-tions on a particular discipline from homepages of faculty starting from lists of universities easily obtained from web directories like Dmoz. This requires following a path start-ing from the root page of the university, to the homepages of departments relevant to the discipline, from there visiting the homepages of faculty members, and then searching for links such as  X  X apers X ,  X  X ublications X , or  X  X esearch Inter-ests X  that lead to the publications page, if it exists. Several university websites follow this template, although there is a lot of variety in the details of exact words on pages and anchors and linkage patterns. We expect such a learning based approach to still capture the main structure in a few examples so as to automatically gather all faculty publica-tions from any given list of universities without fetching too many superfluous pages.
 Other scenarios where similar path learning problems arise are, extracting contact addresses of companies in a list, ex-tracting computer science talk announcements from univer-sity web pages, and finding prices of specific products in a category hierarchy from electronic stores. We first discuss possible approaches that are based on ex-isting technology and research work.
 Keyword search. One approach is to find a set of keywords that can be used in conjunction with a search engine with the domain restricted to each website to get to the right pages. The set of keywords can be selected using a classi-fier trained to discriminate the goal pages from the non-goal pages. The search engine results can be re-ranked using a more detailed classifier to measure relevance to the goal pages. We claim that this method cannot provide high ac-curacy for the simple reason that the goal page itself may not hold enough information to correctly identify it as the goal page. The path leading to the goal page is important too. We will need to consider text around the entire path leading to the goal page in order to decide if it is relevant or not. For example, consider the publications scenario ex-plained earlier. If Citeseer wants to get all computer science publications starting from a university root page, then it is necessary to follow a path through computer science and related departments X  homepages. A publication page on its own might be hard to classify as holding  X  X omputer science publications X . Similarly, a list of publications on a course webpage of a computer science department may not qualify as publications of faculty of that department.
 signed to crawl all webpages on a topic specified through examples of pages related to the topic. The basic property that such crawlers exploit is that pages on a topic are often linked to each other. The crawler consists of two classifiers  X  a baseline classifier that trains on the page-tokens, and an apprentice that trains adaptively on link-tokens of newly discovered relevant pages to choose the best hyperlink out of the crawl frontier. The focused crawler selects the link to crawl next by first choosing pages on the crawl frontier that are relevant to the goal topic, and then selecting the most relevant link from the hyperlinks out of those pages. A problem with this approach is that it cannot exploit pat-terns of pages along a route leading to a goal page. The only pattern it exploits is relevance to the specific topic whereas often pages on unrelated topics might consistently lead to topics of interest. A solution to this problem is proposed in [3] where a context graph keeps track of path patterns to a goal page. The goal documents are at level 0, all pages link-ing to these are at level 1, and so on up to a maximum level N . All documents marked by the same level are clubbed together as being of the same class. Each page is indepen-dently classified to one of the levels and has an associated value of the certainty of classification. Each level also has a score that is directly proportional to the proximity to the goal state. A threshold function combines classification cer-tainty with queue scores to determine a priority order for page fetches. The paper does not provide details of how these scores and thresholds are determined.
 Rennie and McCallum[10] propose a similar solution to learn-ing the path leading to pages of interest. The main difference is that the set of pages over which the classifiers are trained is fixed in advance whereas in the above context graph ap-proach, the classifier is trained on-the-fly with training labels obtained via a backward crawling of pages linking to discov-ered goal pages. This paper provides a principled method of choosing among the classifiers at various layers using Q values derived from Reinforcement Learning.
 While both these strategies provide a good way of recog-nizing early pages on a route leading to a goal page, the underlying classification framework does not exploit all use-ful correlation that exists among the pages in a path. Pages along a path are classified independently to the different levels totally ignoring the fact that if a page is assigned to level j then most likely all pages that it links to should be assigned level j + 1. We propose to use graphical models to exploit such correlations. Also, they rely on naive Bayes classifiers which are not capable of exploiting the variety of possibly correlated features available from the text and other properties of links and layouts of pages.
 The problem we propose in this paper is different from all previous work on focused crawlers in that all focused crawlers have been developed for operating on the general web whereas we are trying to solve more of an information extraction problem from a given list of domain-specific websites like universities, companies and ecommerce websites. We ex-ploit the regularity in the structures of websites in a given domain to build more powerful models than is possible in the case of general-purpose focused crawlers.
 Web knowledge bases. One of the earliest projects that perform extraction of structured information from multi-page sources like a website is WebKB[2]. They describe similar learning tasks of recognizing relations by traversing paths through hyperlinks. However, their approach is based on generative classifiers (like na  X  X ve Bayes) for recognizing correct hits coupled with first order rules (like FOIL[4]) for finding the right page. There are two phases to this task: first is the training phase, where the user teaches the system by clicking through pages showing some paths that end in goal pages and others that do not. The second phase is the foraging phase where the given list of websites are automatically navigated to find all goal pages while fetching as few redundant pages as possible. We consider two different modes of training. 1. The first is a fully supervised case where the user deter-2. The second is the partially supervised case where the We treat this as a sequence tagging problem where the path is a sequence of pages ending in a goal page. We first train a Conditional Random Field (CRF) to recognize such paths. There are several options for encoding the path to states of the CRF and depends among other things on the level of supervision provided by the user. We detail this in Section 3 after presenting a background of the basic CRF technology in Section 2. We then superimpose ideas from reinforcement learning to prioritize the order in which pages should be fetched to reach the goal page. This provides an elegant and unified mechanism of modeling the path learning and foraging problem. A CRF models Pr( y | x ) as a Markov random field, with nodes corresponding to elements of the structured object y , and potential functions that are conditional on (features of) x . One common use of CRFs is for sequential learn-ing problems like NP chunking[11], POS tagging[5], and named-entity recognition (NER)[8]. For these problems, the Markov field is a chain and y is a linear sequence of labels from a fixed set Y , and the label at position i depends only on its previous label. For instance, in the NER application, where the task is to identify entity types like people names and organization in plain text, x might be a sequence of words, and y might be a sequence in { I, O } | x | , where y indicates  X  X ord x i is inside a name X  and y i = O indicates the opposite.
 CRFs have been shown to perform better than other sequen-tial models like hidden Markov models that learn a joint probability Pr( x , y ) of pairs of observation sequences x and label sequences y . The parameters of the model are trained to maximize the joint likelihood of the training examples. A major shortcoming of generative models like HMMs is that they maximize the joint probability of sequence and labels. This does not necessarily maximize accuracy. Also, the con-ditional independence of features is a restrictive assumption. Conditional Random Fields learn a single global conditional model for Pr( y | x ) and have been found to achieve high ac-curacy in a number of applications.
 Notation: We will use bold-faced symbols to denote vectors and non-bold faced symbols to denote scalars.
 Assume a vector f of local feature functions f =  X  f 1 , . . . , f each of which maps a pair ( x , y ) and a position i in the vector x to a measurement f k ( i, x , y )  X  R . Let f ( i, x , y ) be the vec-tor of these measurements and let F ( x , y ) = P | x | i For the case of NER, the components of f might include the measurement f 13 ( i, x , y ) = [[ x i is capitalized]]  X  [[ y where the indicator function [[ c ]] = 1 if c if true and 0 oth-erwise; this implies that F 13 ( x , y ) would be the number of capitalized words paired with the label I .
 For the sake of efficiency, we restrict any feature f k ( i, x , y ) to be local in the sense that the feature at a position i will depend only on the previous labels. With a slight abuse of notation, we claim that a local feature f k ( i, x , y ) can be can be simplified further to depend only on the current state and are independent of the previous state. We will refer to these as state features and denote these by f k ( y i , x , i ) when we want to make the distinction explicit. The term transition features refers to the remaining features that are not independent of the previous state.
 A Conditional Random Field (CRF)[5; 11] is an estimator of the form where W is a weight vector over the components of F and The inference problem for a CRF is defined as follows: Given W and x , find the best label sequence, arg max y Pr( y | x , W ), where Pr( y | x , W ) is defined by equation 1. arg max y Pr( y | x , W ) = arg max y W  X  F ( x , y ) An efficient inference algorithm is possible because all fea-tures are assumed to be local. Let y i : y denote the set of all partial labels starting from 1 (the first index of the se-quence) to i , such that the i -th label is y . Let  X  ( i, y ) denote the largest value of W  X  F ( x , y 0 ) for any y 0  X  y i : y following recursive calculation implements the usual Viterbi algorithm[9]:  X  ( i, y ) = max y 0  X  ( i  X  1 , y The best label then corresponds to the path traced by max Learning is performed by setting parameters to maximize the likelihood of a set of a training set T = { ( x ` , y expressed in logarithmic terms as L ( W ) = X We wish to find a W that maximizes L ( W ). The above equation is convex and can thus be maximized by gradient ascent or one of many related methods. (In our implementa-tion, we use a limited-memory quasi-Newton method[6; 7].) The gradient of L ( W ) is the following:  X  L ( W ) = X The first set of terms are easy to compute. However, we must use the Markov property of F and a dynamic pro-gramming step to compute the normalizer Z W ( x ` ), and the expected value of the features under the current weight vec-tor, E Pr( y 0 | W ) F ( x ` , y 0 ). Details of computing these can be found be in [11]. We first describe the model training phase where the user provided example positive and negative paths from a few websites are used to train a CRF model. We then describe how this trained model is used to locate goal pages starting from root pages of other websites. During training, we are given examples of several paths of labeled pages where some of the paths end in goal pages and others end with a special  X  X ail X  label. We first consider the fully supervised case where the user provides milestone states and later consider the partially supervised case where no intermittent labels are provided. We can treat each path as a sequence of pages denoted by the vector x and their corresponding milestone labels denoted by y . Each x i is a webpage represented suitably in terms of features derived from the words in the page, its URL, and anchor text in the link pointing to x i .
 A number of design decisions about the label space and fea-ture space need to be made in constructing a CRF to recog-nize characteristics of valid paths. One option is to assign a state to each possible label in the set L which consists of the milestone labels and two special labels  X  X oal X  and  X  X ail X . An example of such a model for the publications scenario is given in Figure 1(a) where each circle represents a label. State features are defined on the words or other properties comprising a page. For example, state features derived from words are of the form . The URL of a page also yields valuable features. For example, a  X  X ilda X  in the URL is strongly associated with a personal home page and a link name with word  X  X ontact X  is strongly associated with an address page. We tokenize each URL on delimiters and add a feature corresponding to each token.
 Transition features capture the soft precedence order among labels. One set of transition features are of the form: f ( i, x , y i , y i  X  1 ) = [[ y i is  X  X aculty X  and y i  X  1 They are independent of x i and are called edge features since they capture dependency among adjacent labels. In this model transition features are also derived from the words in and around the anchor text surrounding the link leading to the next state. Thus, a transition feature could be of the A second option is to model each given label as a dual-state  X  one for the characteristics of the page itself ( page-states ) and the other for the information around links that lead to such a page ( link-states ). Hence, every path alternates between a page-state and a link-state.
 In Figure 1(b), we show the state space corresponding to this option for the publications domain. There are two ad-vantages of this labeling. First, it reduces the sparsity of parameters by making the anchor word features be inde-pendent of the label of the source page. In practice, it is often found that the anchor text pointing to the same page are highly similar and this is captured by allowing multiple source labels to point to the same link state of label. Second for the foraging phase, it allows one to easily reason about intermediate probability of a path prefix where only the link is known and the page leading to it has not been fetched. In this model, the state-features of the page states are the same as in the previous model, the state features of the link states are derived from the anchor text. Thus, the anchor-text transition features of the previous model, become state features of the link state. Thus the only transition features in this model are the edge features that capture the prece-dence order between labels. In this case, apart from the start and goal pages none of the other pages have a label. A simple solution is to as-sign to each intermittent page a label equal to the distance from the goal or start state and then map it to the above problem by associating a label for each distance value. A similar labeling scheme was followed in [3]. The main lim-itation of this approach is that pages with similar content may not be exactly the same distance away for different websites and mapping them to different states might make it hard to exploit commonality across websites. In [10] the labeling scheme is discretized to coarser levels, but then it becomes difficult to determine a level of discretization that will neither generalize too much nor make it as specific as in [3]. We propose the following solution. Choose N , the path length from the start to the goal state that covers most except any outlying long paths. State i is assigned to pages distance i from the goal. This is similar to the method of [3] but with two crucial differences. Given the trained sequential model M and a list of starting pages of websites, our goal is to find all paths from the list that lead to the  X  X oal X  state in M while fetching as few unrelated pages.
 The key issue in solving this is to be able to score from a prefix of a path already fetched, all the set of outgoing links with a value that is inversely proportional to the expected work involved in reaching the goal pages. Consider a path prefix of the form P 1 L 2 P 3 . . . L i where L i  X  1 is a link to page P in the path. We need to find for link L i a score value that would indicate the desirability of fetching the page pointed to by L i . This score is computed in two parts. First, we estimate for each state y , the proximity of the state to the goal state. We call this the reward associated with the state. Then we compute for the link L i , the probability of its being in state y . We apply techniques from Reinforcement Learning to com-pute the reward score that captures the probability of a partially-observed sequence to end-up in a goal state of the CRF model M . Reinforcement Learning is a machine learn-ing paradigm that helps in choosing the optimal action at each state to reach the goal states. The goal states are as-sociated with rewards that start to depreciate as the goal states get farther from the current state. The actions are chosen so as to maximize the cumulative discounted reward. We estimate this probability based on the training data by learning a reward function R for each state. For each posi-tion i of a given sequence x we estimate the expected prox-imity to the goal state from a state y R x i ( y ) recursively as follows: When i = n , the reward is 1 for the goal state and 0 for every other label. Otherwise the values are computed recursively from the proximity of the next state and the probability of transition to the next state from the current state. We then compute a weighted sum of these positioned reward values to get a position independent reward values. The weight are controlled via  X  , a discount factor that captures the desirability of preferring states that are closer to the goal state as follows: where n is the length of the sequence.
 The final reward value of a state is computed by averaging over all training sequences x 1 . . . x N as Consider a path prefix of the form P 1 L 2 P 3 . . . L i where L is a link to page P i in the path. We need to find for link L i the probability of its being in any one of the link states. We provide a method for computing this. Let  X  i ( y ) denote the total weight of ending in state y after i states. We thus define  X  i ( y ) as the value of P y 0  X  y denotes all label sequences from 1 to i with i -th position labeled y . For i &gt; 0, this can be expressed recursively as with the base cases defined as  X  0 ( y ) = 1.
 The probability of L i being in the link state y is then  X  where Y L denotes the link states. Finally, the score of a link L i after i steps is calculated as the sum of the product of reaching a state y and the static reward at state y .
 If a link appears in multiple paths, we sum over its score from each path.
 Thus, at any give snapshot of the crawl we have a set of unfetched links whose scores we compute and maintain in a priority queue. We pick the link with the highest score to fetch next. The links in the newly fetched page are added to the queue. We stop when no more unfetched links have score above a threshold value. We refer the reader to [12] for a detailed experimental study of the fully supervised case. We experimented on two dif-ferent tasks: fetching publications starting from computer science department home pages of various universities and extracting pages containing contact addresses of companies. We first evaluated the accuracy of path classification us-ing the CRF-based sequential learning-based approach and compared it with the previously proposed method of inde-pendent classifiers using naive Bayes and maximum entropy classifiers. The sequential models obtained F1-values 10 to 20 percentage points higher than the best independent per class classifiers. During foraging, we were able to achieve harvest rates close to 90%.
 We are continuing our experiments in the semi-supervised setting where the user does not need to specify milestone states. We plan to integrate such website level extraction tasks with finer grained information extraction engines that can find more specific information like the exact address from the goal page.
