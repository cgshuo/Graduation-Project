 The fact that most visual learning problems deal with high di mensional data has made dimension-efficient approach, working with a new space of lower dimensi on often can gain the advantage of gorithms when the data of interest could be more precisely ch aracterized in other forms, such as Aiming to relax the two above-mentioned restrictions, we in troduce an approach called MKL-DR ysis (KFDA), but their method only considers binary-class data. Without the restriction, MKL-DR which presents a unified view for a large family of DR methods. Therefore the proposed MKL-DR is ready to generalize any DR methods if they are expressible by graph embedding. Note that these DR methods include supervised, semisupervised and unsuper vised ones. including graph embedding and multiple kernel learning. 2.1 Graph embedding Many dimensionality reduction methods focus on modeling th e pairwise relationships among data, a unified formulation for a set of DR algorithms. Let  X  = { x scheme accounted for by graph embedding involves a complete graph G whose vertices are over  X  . An affinity matrix W = [ w can be obtained by solving where X = [ x of optimization. If the first constraint is chosen, a diagonal m atrix D = [ d for scale normalization. Otherwise another complete graph G  X  over  X  is required for the second constraint, where L  X  and W  X  = [ w  X  of projected data (in the form of v  X  x ) are modeled by one or two graphs in the framework. By specifying W and D (or W and W  X  ), Yan et al . [19] show that a set of dimensionality reduction methods, such as PCA, LPP [7], LDA, and MFA [19] can be express ed by (1). 2.2 Multiple kernel learning MKL refers to the process of learning a kernel machine with mu ltiple kernel functions or kernel matrices. Recent research efforts on MKL, e.g ., [9, 14, 16] have shown that learning SVMs with Suppose we have a set of base kernel functions { k ensemble kernel function k (or an ensemble kernel matrix K ) is then defined by Consequently, the learned model from binary-class data { ( x Optimizing both the coefficients {  X  lems. Our approach leverages such an MKL optimization to yie ld more flexible dimensionality reduction schemes for data in different feature representa tions. we design an optimization procedure to learn the projection for dimensionality reduction. 3.1 Kernel as a unified feature representation Consider a dataset  X  of N samples, and M kinds of descriptors to characterize each sample. Let data samples to a kernel matrix [18, 20]. By coupling each rep resentation and its corresponding distance function, we obtain a set of M dissimilarity-based kernel matrices { K where  X  solving visual learning tasks. Nonetheless, care must be ta ken in that the resulting K It follows from (5) and (6) that determining a set of optimal e nsemble coefficients {  X  can be interpreted as finding appropriate weights for best fu sing the M feature representations. 3.2 The MKL-DR algorithm Instead of designing a specific dimensionality reduction al gorithm, we choose to describe MKL-DR upon graph embedding. This way we can derive a general framew ork: If a dimensionality reduction scheme is explained by graph embedding, then it will also be e xtendible by MKL-DR to handle constraints. For the ease of presentation, we discuss how to develop MKL-DR subject to constraint (4). However, the derivation can be analogously applied whe n using constraint (3). similar way, but with the key difference in using multiple ke rnels { K kernel K in MKL-DR is generated by linearly combining the base kernel s { K Let  X  : X  X  F denote the feature mapping induced by K . Through  X  , the training data can be implicitly mapped to a high dimensional Hilbert space, i.e. , To show that the underlying algorithm can be reformulated in the form of inner product and accom-plished in the new feature space F , we observe that plugging into (2) each mapped sample  X  ( x and projection v would appear exclusively in the form of v T  X  ( x in MKL-DR, v T  X  ( x  X  = With (2) and (11), we define the constrained optimization pro blem for 1 -D MKL-DR as follows: section, where using MKL-DR for finding a multi-dimensional projection V is considered. RKHS induced by each base kernel, the RKHS by the ensemble ker nel, and the projected space. 3.3 Optimization Observe from (11) that the one-dimensional projection v of MKL-DR is specified by a sample coef-importance among the samples and the base kernels. To genera lize the formulation to uncover a multi-dimensional projection, we consider a set of P sample coefficient vectors, denoted by With A and  X  , each 1 -D projection v the (shared) kernel weight vector  X  . The resulting projection V = [ v to a P -dimensional space. Analogous to the 1 -D case, a projected sample x The optimization problem (12) can now be extended to accommo date multi-dimensional projection: kernel, and the projected Euclidean space.
 alternately optimize A and  X  . At each iteration, one of A and  X  is optimized while the other is maximum number of iterations is reached.
 On optimizing A : By fixing  X  , the optimization problem (17) is reduced to The problem (18) is a trace ratio problem, i.e., min lem, i.e., min [  X  Algorithm 1 : MKL-DR Input : A DR method specified by two affinity matrices W and W  X  (cf. (2)); Output : Sample coefficient vectors A = [  X  Make an initial guess for A or  X  ; for t  X  1 , 2 , . . . , T do return A and  X  ; On optimizing  X  : By fixing A , the optimization problem (17) becomes a generalized eigenvalue problem. Indeed it now becomes a no nconvex quadratically constrained quadratic programming (QCQP) problem, and is known to be very difficult to solve. We i nstead consider solving its convex relaxation by adding an auxilia ry variable B of size M  X  M : where e by semidefinite programming (SDP). One can verify the equiva lence between the two optimization M , the number of the adopted descriptors. In practice the valu e of M is often small. ( M = 7 in our experiments.) Thus like most of the other DR methods, the computational bottleneck of our approach is still in solving the generalized eigenvalue pro blems.
 Listed in Algorithm 1, the procedure of MKL-DR requires an in itial guess to either A or  X  in the elements as 1 to equally weight each base kernel; 2) A is initialized by assuming AA  X  = I . In few iterations in all our experiments.
 Novel sample embedding. Given a testing sample z , it is projected to the learned space of lower dimension by To evaluate the effectiveness of MKL-DR, we test the techniq ue with the supervised visual learn-ing task of object category recognition. In the application , two (base) DR methods and a set of descriptors are properly chosen to serve as the input to MKL-DR. 4.1 Dataset background images. The total number of categories is 102 , and each category contains roughly 40 to them to around 60,000 pixels, without changing their aspect ratio.
 To implement MKL-DR for recognition, we need to select some p roper graph-based DR method to and a set of base kernels. The details are described as follow s. 4.2 Image descriptors base kernels (denoted below in bold and in abbreviation): GB-1 / GB-2 : From a given image, we randomly sample 300 edge pixels, and apply geometric blur which is constructed with a specific descriptor radius.
 [11] is used to extract features.
 SIFT-Grid : We apply SIFT with three different scales to an evenly sampl ed grid of each image, to derive this base kernel via (8).
 two kinds of C2 features, an RBF kernel is respectively const ructed.
 PHOG : We adopt the PHOG descriptor [2] to capture image features, and limit the pyra mid level 4.3 Dimensionality reduction methods We consider two supervised DR schemes, namely, linear discriminant analysis (LDA) and local discriminant embedding (LDE) [3], and show how MKL-DR can generalize them. Both LDA a nd LDE perform discriminant learning on a fully labeled datase t  X  = { ( x while LDE assumes they spread as a submanifold. Each of the tw o methods can be specified by dimensionality reduction schemes are respectively termed as MKL-LDA and MKL-LDE . Affinity matrices for LDA: The two affinity matrices W = [ w where n are simultaneously considered to construct the affinity mat rices W = [ w where i  X  N and average the resulting affinity matrices from all the desc riptors. 4.4 Quantitative results we randomly pick 30 images where 15 of them are included for training and the other 15 images task is accomplished there by enforcing the nearest-neighbor rule.
 rive MKL-LDA and MKL-LDE using Algorithm 1. Their effective ness is investigated by comparing with KFD (kernel Fisher discriminant) [12] and KLDE (kernel LDE) [3]. Since KFD considers only a decision on the class label can be made. The second is termed as KFD-SAMME . By viewing each KFD classifier as a multi-class weak learner, we boost them by SAMME [21], which is a multi-class generalization of AdaBoost. Analogously, we also have KLDE -Voting and KLDE-SAMME . classifiers. On the other hand, while KFD-Voting and KFD-SAMME try to combine the separately quantitative results show that MKL-LDA can make the most of f using various feature descriptors, also be observed for MKL-LDE.
 The recognition rates 74 . 6% in MKL-LDA and 75 . 3% in MKL-LDE are favorably comparable to those by most of the existing approaches. In [6], Grauman and Darrell report a 50% recognition Our related work [10] that performs adaptive feature fusing via locally combining kernel matrices can yield a top recognition rate of 87 . 82% by integrating visual cues like shape and color. most of the graph-based DR methods, and improves their perfo rmance. Such flexibilities allow one with a suitable DR scheme, MKL-DR can extend the multiple ker nel learning framework to address not just the supervised learning problems but also the unsup ervised and the semisupervised ones. Acknowledgements. This work is supported in part by grants 95-2221-E-001-031-MY3 and 97-2221-E-001-019-MY3.

