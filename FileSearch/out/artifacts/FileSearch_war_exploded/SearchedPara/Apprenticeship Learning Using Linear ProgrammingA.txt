 In apprenticeship learning, as with policy learning for Markov decision processes (MDPs), the objective is to find a good policy for an autonomous agent, called the  X  X ppren-tice X , in a stochastic environment. While the setup of an ap-prenticeship learning problem is almost identical to that o f policy learning in an MDP, there are a few key differences. In apprenticeship learning the true reward function is un-known to the apprentice, but is assumed to be a weighted combination of several known functions. The apprentice is also assumed to have access to demonstrations from an-other agent, called the  X  X xpert X , executing a policy in the same environment. The goal of the apprentice is to find a policy that is at least as good as the expert X  X  policy with respect to the true reward function. This is distinct from policy learning, where the goal is to find an optimal policy with respect to the true reward function (which cannot be done in this case because it is unknown).
 The apprenticeship learning framework, introduced by Abbeel &amp; Ng (2004), is motivated by a couple of observa-tions about real applications. The first is that reward func-tions are often difficult to describe exactly, and yet at the same time it is usually easy to specify what the rewards must depend on. A typical example, investigated by Abbeel car, it is plausible that her behavior can be viewed as maxi-mizing some reward function, and that this reward function depends on just a few key properties of each environment state: the speed of the car, the position of other cars, the terrain, etc. The second observation is that demonstration s of good policies by experts are often plentiful. This is cer-tainly true in the car driving example, as it is in many other applications.
 Abbeel &amp; Ng (2004) assumed that the true reward func-tion could be written as a linear combination of k known functions, and described an iterative algorithm that, give n a small set of demonstrations of the expert policy, output an apprentice policy within O ( k log k ) iterations that was (2008) gave an algorithm that achieved the same guarantee in
O (log k ) iterations. They also showed that by assum-ing that the linear combination is also a convex one, their algorithm can sometimes find an apprentice policy that is substantially better than the expert X  X  policy. Essentiall y, the assumption of positive weights implies that the apprentice has some prior knowledge about which policies are better than others, and their algorithm leverages this knowledge. Existing algorithms for apprenticeship learning share a couple of properties. One is that they each use an algorithm for finding an MDP X  X  optimal policy (e.g. value iteration or policy iteration) as a subroutine. Another is that they out-put apprentice policies that are  X  X ixtures X , i.e. randomiz ed combinations of stationary policies. A stationary policy i s a function of just the current environment state. Our first contribution in this paper is to show that, if one uses the linear programming approach for finding an MDP X  X  optimal policy (Puterman, 1994) as a subroutine, then one can modify Syed &amp; Schapire X  X  (2008) algorithm so that it outputs a stationary policy instead of a mixed pol-icy. Stationary policies are desirable for a number of rea-sons, e.g. they are simpler to describe, and are more natural and intuitive in terms of the behavior that they prescribe. Moreover, this technique can be straightforwardly applied to any mixed policy, such as the ones output by Abbeel &amp; Ng X  X  (2004) algorithms, to convert it to a stationary policy that earns the same expected cumulative reward.
 Our technique leads naturally to the second contribution of this paper, which is the formulation of the apprenticeship learning problem as a linear program. We prove that the solution to this LP corresponds to an apprentice policy that has the same performance guarantees as those produced by existing algorithms, and that the efficiency of modern LP solvers results in a very substantial improvement in runnin g time compared to Syed &amp; Schapire X  X  (2008) algorithm  X  up to two orders of magnitude in our experiments. In work closely related to apprenticeship learning, Ratlif f, Bagnell &amp; Zinkevich (2006) described an algorithm for learning the true reward function by assuming that the ex-pert X  X  policy is not very different from the optimal policy. They took this approach because they wanted to learn poli-cies that were similar to the expert X  X  policy. In apprentice -ship learning, by contrast, the learned apprentice policy c an be very different from the expert X  X  policy. Formally, an apprenticeship learning problem
S , A ,  X ,  X ,  X , R 1 . . . R k , D closely resembles a Markov decision process. At each time step t , an autonomous agent occupies a state s action a taking action a leads to state s 0 with transition probability  X  probabilities are given by  X  decides which actions to take based on its policy  X  , where  X  sa , Pr( a t = a | s t = s ) given by where R state-action pair ( s, a ) , and  X   X  [0 , 1) is a discount fac-tor. An optimal policy  X   X  is one that satisfies  X   X  = arg max  X  V (  X  ) . We say a policy  X  is -optimal if V (  X   X  )  X  V (  X  )  X  .
 A policy  X  has occupancy measure x  X  if for all s, a . In other words, x  X  number of visits to state-action pair ( s, a ) when following policy  X  .
 Unlike an MDP, in apprenticeship learning the true reward function R is unknown. Instead, we are given basis re-state-action pair ( s, a ) with respect to the i th basis reward function. We assume that the true reward function R is an unknown convex combination w  X  of the basis reward func-tions, i.e., for all s, a where the unknown weights satisfy w  X  1 . Each basis reward function R i has a corresponding basis value function V i (  X  ) given by Given the assumption of positive weights, the value of k can be viewed as a measure of how much the apprentice knows about the true reward function. If k = 1 , the (only) basis value of a policy is equal to its true value, and the sit-uation reduces to a traditional MDP. At the other extreme, if the i th basis reward function is just an indicator function for the i th state-action pair, then k = |SA| , and the ba-sis values of a policy are equal to its occupancy measure. In this situation, the apprentice knows essentially nothin g about which policies are better than others.
 The positive weight assumption also implies that if for state-action pairs ( s, a ) and ( s 0 , a 0 ) we have R i for all i , then R selves can encode prior knowledge about the true rewards. If we wish not to assert any such prior knowledge, we can simply add the negative of each basis reward function to the original set, thereby at most doubling the number of basis reward functions.
 We also assume that we are given a data set D of i.i.d. sample trajectories from an expert policy  X  E cuting in the environment, where the m th trajectory is a that all sample trajectories are truncated to the same lengt h H .
 The goal of apprenticeship learning (Abbeel &amp; Ng, 2004) is to find an apprentice policy  X  A such that even though the true value function V (  X  ) is unknown (since the true reward function is unknown). 2.1. A More Refined Goal By our assumptions about the reward functions (and the linearity of expectation), we have Consequently, for any policy  X  , the smallest possible differ-ence between V (  X  ) and V (  X  E ) is min because in the worst-case, w  X  Based on this observation, Syed &amp; Schapire (2008) pro-posed finding an apprentice policy  X  A that solves the max-imin objective Note that if  X  A is a solution to (3), then V (  X  A )  X  V (  X  E )+ v  X  (because v  X  = min i V i (  X  A )  X  V i (  X  E )  X  V (  X  A )  X  V (  X  E ) ). We also have v  X   X  0 (because  X  =  X  E is avail-able in (3)). Therefore  X  A satisfies the goal of apprentice-ship learning given in (2).
 Syed &amp; Schapire (2008) showed that in some cases where V (  X  E ) is small, v  X  is large, and so adding v  X  to the lower bound in (2) serves as a kind of insurance against bad ex-perts. Our algorithms also produce apprentice policies tha t achieve this more refined goal. 2.2. Estimating the Expert Policy X  X  Values Our algorithms require knowledge of the basis values of the expert X  X  policy. From the expert X  X  sample trajectories D , we can form an estimate b V i,E of V i (  X  E ) as follows: Clearly, as the number of sample trajectories M and the truncation length H increase, the error of this estimate will decrease. Thus the issue of accurately estimating V i (  X  E ) is related to sample complexity, while in this work we are primarily concerned with computational complexity. To make our presentation cleaner, we will assume that D is large enough to yield an estimate b V i,E of V i (  X  E ) such that | b
V i,E  X  V i (  X  E ) |  X  , for all i . We call such an estimate -good . The sample complexity of apprenticeship learning is treated in (Syed &amp; Schapire, 2008). 2.3. Policy Types Unless otherwise noted, a policy  X  is presumed to be sta-tionary, i.e.,  X  s . One exception is a mixed policy . A mixed policy  X   X  is de-fined by a set of ordered pairs { (  X  j ,  X  j ) } N is followed by choosing at time 0 one of the stationary poli-cies  X  j , each with probability  X  j , and then following that policy exclusively thereafter. The value of a mixed policy is the expected value of the stationary policies it comprise s, i.e., Syed &amp; Schapire (2008) observed that solving the objec-a certain two-player zero-sum game. Because the size of this game X  X  matrix is exponential in the number of states |S| , they adapted a multiplicative weights method for solv-ing extremely large games. The resulting MWAL (Mul-tiplicative Weights Apprenticeship Learning) algorithm i s described in Algorithm 1 below.
 Algorithm 1 MWAL algorithm 1: Given: S , A ,  X ,  X ,  X , R 1 . . . R k , D . 2: Using the expert X  X  sample trajectories D , compute an 3: Let  X  = 1 + 4: Initialize w 1 5: for t = 1 . . . T do 6: Compute -optimal policy  X  t for reward function 7: Compute -good estimate b V i,t of V i (  X  t ) , for i = 9: Renormalize w . 10: end for 11: Return: Let apprentice policy  X  A be the mixed policy In each iteration of the MWAL algorithm, an optimal pol-icy  X  t is computed with respect to the current weight vec-tor w t . Then the weights are updated so that w creased/decreased if  X  t is a bad/good policy (relative to  X  E ) with respect to the i th basis reward function. The next theorem bounds the number of iterations T quired for the MWAL algorithm to produce a good appren-tice policy. The computational complexity of each iteratio n is discussed in Section 3.1.
 Theorem 1 (Syed &amp; Schapire (2008)). Let  X  A be the mixed policy returned by the MWAL algorithm. If then where v  X  = max 3.1. MWAL-VI and MWAL-PI The specification of the MWAL algorithm is somewhat open-ended. Step 6 requires finding an -optimal policy in an MDP, and Step 7 requires computing -good estimates of the basis values of that policy. There are several procedure s available for accomplishing each of these steps, with each option leading to a different variant of the basic algorithm . We briefly describe some natural options, and remark on their implications for the overall computational complexi ty of the MWAL algorithm.
 In Step 6, we can find the optimal policy using value it-eration (Puterman, 1994), which has a worst-case running time of O log value iteration to compute the k basis values in Step 7 (this is sometimes called  X  X olicy evaluation X ), which implies a worst-case running time of O k log We call this variant the MWAL-VI algorithm.
 Another choice for Step 6 is to find the optimal policy us-ing policy iteration (Puterman, 1994). No polynomial time bound for policy iteration is known; however, in practice it has often been observed to be faster than value iteration. We call this variant the MWAL-PI algorithm. In Section 8, we present experiments comparing these algorithms to the ones described later in the paper. As we previously observed, the MWAL algorithm must re-peatedly find the optimal policy in an MDP, and this task is usually accomplished via classic iterative techniques suc h as value iteration and policy iteration. However, there are other techniques available for solving MDPs, and in this work we show that they can lead to better algorithms for apprenticeship learning. Consider the following linear pr o-gram: It is well-known (Puterman, 1994) that if x  X  is a solution to (4) -(6), then  X   X  is the occupancy measure of  X   X  . Often (5) -(6) are called the Bellman flow constraints .
 The linear program in (4) -(6) is actually the dual of the linear program that is typically used to find an optimal pol-icy in an MDP. Accordingly, solving (4) -(6) is often called the Dual LP method of solving MDPs .
 Having found an optimal policy by the Dual LP method, computing its values is straightforward. The next lemma follows immediately from the definitions of the occupancy measure and value of a policy.
 Lemma 1. If policy  X  has occupancy measure x  X  , then V (  X  ) = Recall that the MWAL algorithm produces mixed policies. In Sections 6 and 7, we will present algorithms that achieve the same theoretical guarantees as the MWAL algorithm, but produce stationary policies (and are also faster). To prove the correctness of these algorithms, we need to show that every mixed policy has an equivalent stationary policy . In Section 4, we said that the Dual LP method of solving an MDP outputs the occupancy measure of an optimal policy. In fact, all x that satisfy the Bellman flow constraints (5) -(6) are the occupancy measure of some stationary policy, as the next theorem shows.
 Theorem 2. Let x satisfy the Bellman flow constraints (5) -(6) , and let  X  x is the occupancy measure for  X  . Conversely, if  X  a stationary policy such that x is its occupancy measure, then  X  straints.
 An equivalent result as Theorem 2 is given in (Feinberg &amp; Schwartz, 2002), p. 178. For completeness, a simple and direct proof is contained in the Appendix.
 The Bellman flow constraints make it very easy to show that, for every mixed policy, there is a stationary policy th at has the same value.
 Theorem 3. Let  X   X  be a mixed policy defined by { (  X  j ,  X  j ) } N j =1 , and let x j be the occupancy measure of for all j . Let  X   X  be a stationary policy where Then V ( X   X  ) = V (  X   X  ) .
 Proof. By Theorem 2, x j satisfies the Bellman flow con-straints (5) -(6) for all j . Let  X  x earity,  X  x also satisfies the Bellman flow constraints. Hence, by Theorem 2, the stationary policy  X   X  defined by  X   X  P V ( X   X  ) = where these equalities use, in order: Lemma 1; the defini-tion of  X  x ; Lemma 1; the definition of a mixed policy. In this section, we will make a minor modification to the MWAL algorithm so that it outputs a stationary policy in-stead of a mixed policy.
 Recall that the MWAL algorithm requires, in Steps 6 and 7, a way to compute an optimal policy and its basis values, but that no particular methods are prescribed. Our proposal is to use the Dual LP method in Step 6 to find the occu-pancy measure x ward function R b implies b V i,t = V i (  X  t ) .
 Now we can apply Theorem 3 to combine all the policies computed during the MWAL algorithm into a single sta-tionary apprentice policy. This amounts to changing Step 11 to the following: We call this modified algorithm the MWAL-Dual algo-rithm, after the method it uses to compute optimal policies. It is straightforward to show that these changes to the MWAL algorithm do not affect its performance guarantee. Theorem 4. Let  X  A be the stationary policy returned by the MWAL-Dual algorithm. If then where v  X  = max Proof. By Theorem 3, the stationary policy returned by the MWAL-Dual algorithm has the same value as the mixed policy returned by the original MWAL algorithm. Hence the guarantee in Theorem 1 applies to the MWAL-Dual al-gorithm as well.
 Of course, the trick used here to convert a mixed policy to a stationary one is completely general, provided that the oc-cupancy measures of the component policies can be com-puted. For example, this technique could be applied to the mixed policy output by the algorithms due to Abbeel &amp; Ng (2004).
 Let T ( n ) be the worst-case running time of an LP solver on a problem with at most n constraints and variables. 2 For a pura, 1993), although they tend to be much faster in prac-tice. Using this notation, we can bound the running time of Steps 6 and 7 in the MWAL-Dual algorithm. Finding an optimal policy using the Dual LP method takes T ( |S||A| ) time. And by Lemma 1, given the occupancy measure of a policy, we can compute its basis values in O ( k |S||A| ) time. We now describe a way to use the Bellman flow constraints to find a good apprentice policy in a much more direct fash-ion than the MWAL algorithm. Recall the objective func-tion proposed in (Syed &amp; Schapire, 2008) for solving ap-prenticeship learning: We observed earlier that, if  X  A is a solution to (7), then V (  X  A )  X  V (  X  E ) + v  X  , and that v  X   X  0 . In this section, we describe a linear program that solves (7). In Section 8, we describe experiments that show that this approach is much faster than the MWAL algorithm, although it does have some disadvantages, which we also illustrate in Sec-tion 8.
 Our LPAL (Linear Programming Apprenticeship Learning) algorithm is given in Algorithm 2. The basic idea is to use the Bellman flow constraints (5) -(6) and Lemma 1 to define a feasible set containing all (occupancy measures of) stationary policies whose basis values are above a certain lower bound, and then maximize this bound.
 Algorithm 2 LPAL algorithm 1: Given: S , A ,  X ,  X ,  X , R 1 . . . R k , D . 2: Using the expert X  X  sample trajectories D , compute an 3: Find a solution ( B  X  , x  X  ) to this linear program: 4: Return: Let apprentice policy  X  A be the stationary Theorem 5. Let  X  A be the stationary policy returned by the LPAL algorithm. Then where v  X  = max Proof. By Theorem 2, the Bellman flow constraints (10) -(11) imply that all feasible x correspond to the occupancy measure of some stationary policy  X  . Using this fact and Lemma 1, we conclude that solving the linear program is equivalent to finding ( B  X  ,  X  A ) such that and B  X  is as large as possible. Since | b V i,E  X  V i (  X  E ) |  X  for all i , we know that B  X   X  v  X   X  . Together with (9) and Lemma 1 this implies V (  X  A ) = Note that the overall worst-case running time of the LPAL algorithm is T ( |S||A| + k ) , where T ( n ) is the complexity of an LP solver. 8.1. Gridworld We tested each algorithm in gridworld environments that closely resemble those in the experiments of Abbeel &amp; Ng (2004). Each gridworld is an N  X  N square of states. Movement is possible in the four compass directions, and each action has a 30% chance of causing a transition to a random state. Each gridworld is partitioned into several square regions, each of size M  X  M . We always choose M so that it evenly divides N , so that each gridworld has functions, where the i th basis reward function R i is a 0-1 indicator function for the i th region.
 For each gridworld, in each trial, we randomly chose a sparse weight vector w  X  . Recall that the true reward func-tion has the form R ( s ) = P periments the true reward function just encodes that some regions are more desirable than others. In each trial, we let the expert policy  X  E be the optimal policy with respect to R , and then supplied the basis values V i (  X  E ) , for all the MWAL-VI, MWAL-PI, MWAL-Dual and LPAL algo-Our experiments were run on an ordinary desktop com-puter. We used the Matlab-based cvx package (Grant gorithm took to find an apprentice policy  X  A such that V (  X  A )  X  0 . 95 V (  X  E ) . Each running time is the average of 10 trials.
 In the first set of experiments (Table 1), we tested the al-gorithms in gridworlds of varying sizes, while keeping the number of regions in each gridworld fixed (64 regions). Re-call that the number of regions is equal to the number of basis reward functions. In the next set of experiments (Ta-ble 2), we varied the number of regions while keeping the size of the gridworld fixed.
 Several remarks about these results are in order. For every gridworld size and every number of regions, the LPAL al-gorithm is substantially faster than the other algorithms  X  in some cases two orders of magnitude faster. As we previ-ously noted, LP solvers are often much more efficient than their theoretical guarantees. Interestingly, in Table 2, t he running time for LPAL eventually decreases as the number of regions increases. This may be because the number of constraints in the linear program increases with the num-ber of regions, and more constraints often make a linear program problem easier to solve.
 Also, the MWAL-Dual algorithm is much slower than the other algorithms. We suspect this is only because the MWAL-Dual algorithm calls the LP solver in each itera-tion (unlike the LPAL algorithm, which calls it just once), and there is substantial overhead to doing this. Modifying MWAL-Dual so that it uses the LP solver as less of a black-box may be a way to alleviate this problem. 8.2. Car driving In light of the results from the previous section, one might reasonably wonder whether there is any argument for using an algorithm other than LPAL. Recall that, in those exper-iments, the expert X  X  policy was an optimal policy for the unknown reward function. In this section we explore the behavior of each algorithm when this is not the case, and find that MWAL produces better apprentice policies than LPAL. Our experiments were run in a car driving simulator modeled after the environments in (Abbeel &amp; Ng, 2004) and (Syed &amp; Schapire, 2008).
 The task in our driving simulator is to navigate a car on a busy three-lane highway. The available actions are to move left, move right, drive faster, or drive slower. There are three basis reward functions that map each environment state to a numerical reward: collision (0 if contact with an-other car, and 1/2 otherwise), off-road (0 if on the grass, and 1/2 otherwise), and speed (1/2, 3/4 and 1 for each of the three possible speeds, with higher values correspond-ing to higher speeds). The true reward function is assumed to be some unknown weighted combination w  X  of the basis reward functions. Since the weights are assumed to be pos-itive, by examining the basis reward functions we see that the true reward function assigns higher reward to states tha t are intuitively  X  X etter X .
 We designed three experts for these experiments, described in Table 3. Each expert is optimal for one of the basis re-ward functions, and mediocre for the other two. Therefore each expert policy  X  E is an optimal policy if w  X  = w E , where w E is the weight vector that places all weight on the basis reward function for which  X  E is optimal. At the same time, each  X  E is very likely to be suboptimal for a randomly chosen w  X  .
 We used the MWAL and LPAL algorithms to learn appren-presented in Table 4. We let  X  = 0 . 9 , so the maximum value of the basis value function corresponding to speed was 10, and for the others it was 5. Each of the reported policy values for randomly chosen w  X  was averaged over 10,000 uniformly sampled w  X   X  X . Notice that for each ex-pert, when w  X  is chosen randomly, MWAL outputs better apprentice policies than LPAL.
 Each of the algorithms for apprenticeship learning pre-sented here have advantages and disadvantages that make them each better suited to different situations. As our ex-periments showed, the LPAL algorithm is much faster than any of the MWAL variants, and so is most appropriate for problems with large state spaces or many basis reward functions. And unlike the original MWAL algorithm, it produces a stationary policy, which make it a good choice whenever a simple and easily interpretable apprentice pol-icy is desired. On the other hand, we also presented evi-dence that LPAL performs poorly when the expert policy is far from an optimal policy for the true reward function. If one suspects in advance that this may be the case, then one of the MWAL variants would be a better choice for a learn-ing algorithm. Among these variants, only MWAL-Dual produces a stationary policy, although it has the drawback of being the slowest algorithm that we tested.
 Although the theoretical performance guarantees of both the MWAL and LPAL algorithm are identical, the results in Table 4 suggest that the two algorithms are not equally ef-fective. It seems possible that the current theoretical gua r-antees for the MWAL algorithm are not as strong as they could be. Investigation of this possibility is ongoing work . One way to describe the poor performance of the LPAL al-gorithm versus MWAL is to say that, when there are several policies that are better than the expert X  X  policy, the LPAL algorithm fails to optimally break these  X  X ies X . This char-acterization suggests that recent techniques for computin g robust strategies in games (Johanson et al., 2008) may be an avenue for improving the LPAL algorithm.
 It would also be interesting to examine practically and the-oretically how apprenticeship learning can be combined with MDP approximation techniques. In particular, the dual linear programming approach in this work might com-bine nicely with recent work on stable MDP approximation techniques based on the dual form (Wang et al., 2008). We would like to thank Michael Littman, Warren Powell, Michele Sebag and the anonymous reviewers for their help-ful comments. This work was supported by the NSF under grant IIS-0325500.
 Abbeel, P., &amp; Ng, A. (2004). Apprenticeship learning via inverse reinforcement learning. Proceedings of the In-ternational Conference on Machine Learning .
 Feinberg, E. A., &amp; Schwartz, A. (2002). Handbook of Markov Decision Processes: Methods and Applications . Springer.
 Grant, M., &amp; Boyd, S. (2008). CVX: Matlab software for disciplined convex programming (web page and soft-ware). http://stanford.edu/  X  boyd/cvx.
 Horn, R. A., &amp; Johnson, C. R. (1985). Matrix Analysis . Cambridge University Press.
 Johanson, M., Zinkevich, M., &amp; Bowling, M. (2008). Com-puting robust counter-strategies. Advances in Neural In-formation Processing Systems .
 Puterman, M. L. (1994). Markov decision processes: Dis-crete stochastic dynamic programming . John Wiley and Sons.
 Ratliff, N. D., Bagnell, J. A., &amp; Zinkevich, M. A. (2006).
Maximum margin planning. Proceedings of the Interna-tional Conference on Machine Learning .
 Shu-Cherng, &amp; Puthenpura, S. (1993). Linear Optimization and Extensions: Theory and Algorithms . Prentice Hall. Syed, U., &amp; Schapire, R. E. (2008). A game-theoretic ap-proach to apprenticeship learning. Advances in Neural Information Processing Systems .
 Wang, T., Lizotte, D., Bowling, M., &amp; Schuurmans, D. (2008). Stable dual dynamic programming. Advances in Neural Information Processing Systems . This is a proof of Theorem 2. Before proceeding, we in-troduce another linear system. For any stationary policy  X  , the  X  -specific Bellman flow constraints are given by the following linear system in which the x known: The next lemma shows that  X  -specific Bellman flow con-straints have a solution.
 Lemma 2. For any stationary policy  X  , the occupancy measure x  X  of  X  satisfies the  X  -specific Bellman flow con-straints (12) -(13) .
 Proof. Clearly, x  X  is satisfied. As for (12), we simply plug in the definition of tions and probabilities are conditioned on  X  ,  X  , and  X  . They have been omitted from the notation for brevity.) Now we show that the solution to the  X  -specific Bellman flow constraints given by Lemma 2 is unique.
 Lemma 3. For any stationary policy  X  , the  X  -specific Bell-man flow constraints (12) -(13) have at most one solution. Proof. Define the matrix and the vector b dexed by state-action pairs.) We can re-write (12) -(13) equivalently as The matrix A is column-wise strictly diagonally dominant. This is because P for all s 0 , a 0 where the last line is the definition of column-wise strict diagonal dominance. This implies that A is non-singular solution.
 We are now ready to prove Theorem 2.
 Proof of Theorem 2. For the first direction of the theorem, we assume that x satisfies the Bellman flow constraints (5) -(6), and that  X  Clearly x is a solution to the  X  -specific Bellman flow con-straints (12) -(13), and Lemmas 2 and 3 imply that x is the occupancy measure of  X  .
 For the other direction of the theorem, we assume that x is the occupancy measure of  X  . Lemmas 2 and 3 imply that x is the unique solution to the  X  -specific Bellman flow constraints (12) -(13). Therefore,  X  is given by (16). And since P which can be rearranged to show that x satisfies the Bell-man flow constraints, and also combined with (16) to show that  X 
