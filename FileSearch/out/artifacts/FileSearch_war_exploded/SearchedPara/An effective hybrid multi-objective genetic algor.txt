 1. Introduction
Batch processing machines (BPM) are encountered in many different environments. Etching process for creating circuitry in wafer fabrication, thermal stress testers for circuit boards inspection and numerically controlled routers for cutting metal sheets are samples of BPM stations ( Ahmadi et al., 1992). This research is motivated by burn-in operations in semiconductor manufacturing ( Uzsoy, 1994). The purpose of burn-in operations is to test the integrated circuits by subjecting them to thermal stress for an extended period. The burn-in oven has a limited capacity and sub-grouping the boards holding the circuits into batches would be inevitable. It is possible to maintain different circuits simultaneously in the oven longer than their predefined processing times. So the processing time of each batch is represented by the longest burn-in time among those of all the circuits grouped together in the batch. The processing times of burn-in operations are generally longer compared to those of other testing operations, so the burn-in operations constitute a bottleneck in the final testing operation and optimal scheduling of the burn-in operations is of great concern in productivity and on-time delivery management.

Optimizing a single objective generally may lead to deteriora-tion of other possible objective. Many industries such as semiconductor manufacturing have tradeoffs in their scheduling where multiple objectives need to be considered in order to optimize the overall performance of the system. For example, decision makers may need to consider both of their manufacture X  X  concern such as reducing work-in-process inventory, and their customer X  X  concern such as assuring on-time receipt.
In scheduling theory, makespan ( C max ) is equivalent to the completion time of the final job to leave the system. The smaller C max implies a higher utilization and for the bottleneck operation, the utilization is closely related to the throughput rate of the system. Therefore, reducing C max will lead to a higher throughput rate. Maximum tardiness ( T max ), which represents the worst violation of due dates, is related to the case of on-time delivery. If we can simultaneously control these objectives, then we can produce a superior schedule.

In this paper we consider a BPM scheduling problem with the following assumptions: (1) There are n jobs to be processed. With each job i we shall (2) Each machine has a capacity B and each job i has a size s sum of job sizes in a batch must be less than or equal to B .We assume that no job has a size exceeding the machine capacity. (3) Once processing of a batch is initiated, it cannot be interrupted and other jobs cannot be introduced into the machine until processing is completed. The processing time of batch j , T j , is given by the longest processing time of the jobs in the batch. (4) The objective is simultaneous minimization of the makespan and the maximum tardiness.
 The arrangement of the paper is as follows. In Section 2, we review the works related to scheduling batch processing machines. Section 3 gives the basic issues on the multi-objective optimization. Section 4 examines the combinatorial complexity of the problem. Section 5 reviews the batch first-fit (BFF) heuristic and also proposes heuristic approaches developed for the problem. Proposed multi-objective genetic algorithms (MOGA) are presented in Section 6. Section 7 deals with the test data generation, objective selection, performance measurement, para-meter setting and performance evaluation of MOGAs. The paper will be concluded in Section 8. 2. Literature review
Batching problems have attracted many researchers. Batching means generally a grouping of jobs with the aim to process them either sequentially (serial-batching scheduling) or simultaneously (parallel-batching scheduling) on the same machine ( Mathirajan and Sivakumar 2006 ). Here, we have to make batch formation decisions besides the common assignment and sequencing decisions in scheduling.

In this paper we investigate the case that all jobs in the batch are processed simultaneously, i.e. parallel-batching scheduling.
In the literature, the parallel-batching scheduling is known as scheduling of the batch processing machines. Mathirajan and
Sivakumar (2006) have done a quite complete survey on scheduling with batch processing machines.

Batch machine scheduling problems, where the batch proces-sing time is given by the processing time of the longest job in the batch, was addressed first by Lee et al., (1992). They gave efficient algorithms for minimizing maximum lateness and the number of tardy jobs, assuming agreeable processing times and due dates.
Brucker et al., (1998) gave several exact algorithms and complex-ity results for the case of infinite machine capacity. For the limited case, they proved that due date based scheduling criteria give rise to NP-hard problems. They provided an O ( n B ( B 1) ) exact algorithm for minimizing total completion time. Chandru et al. (1993) provided heuristics and a branch and bound method for minimizing total completion time that can be implemented for solving problems with up to 35 jobs. Adding some new elimination rules, Dupont and Jolai Ghazvini (1997a) make their method useful for the problems with up to 100 jobs. In another paper Dupont and Jolai Ghazvini (1997b) provided a heuristic solution for this problem. Uzsoy and Yang (1997) studied the problem of total weighted completion time and provided some heuristics and a branch and bound algorithm. Lee and Uzsoy (1999) studied the problem of minimizing makespan in the presence of dynamic job arrivals.

Scheduling BPM with incompatible job families, i.e. when the processing times of all jobs in the same family are equal and jobs of different families cannot be processed together in the same batch, has been addressed by researchers. Uzsoy (1995) devel-oped efficient algorithms for minimizing maximum lateness and total weighted completion time on a BPM. He also addressed the problems of minimizing makespan and maximum lateness in the presence of dynamic job arrivals. Dobson and Nambimadom (2001) studied the problem of minimizing total weighted completion time, where each job requires a different amount of machine capacity. Jolai (2005) showed that the problem of minimizing the number of tardy jobs is NP-hard and provided a dynamic programming algorithm with polynomial time complex-ity for the fixed number of job families and batch machine capacity.

Considering different size for jobs, Uzsoy (1994) gave complex-ity results for makespan and total completion time criteria and provided some heuristics and a branch and bound algorithm.
Dupont and Jolai Ghazvini (1998) proposed heuristics to minimize makespan. In another research Jolai Ghazvini and Dupont (1998) considered total completion time criterion for the same problem.
A branch and bound procedure for minimizing makespan was also developed by Dupont and Dhaenens-Flipo (2002) . Considering job release times, Li et al. (2005) presented an approximation algorithm with worst-case ratio 2+ e , for this problem. Zhang et al. (2001) examined the worst-case performance of algorithms proposed by Uzsoy (1994) to minimize the makespan. Besides, they also presented heuristics under the proportional assumption and for the general case. Husseinzadeh Kashan and Karimi (2009) generalized the proportional assumption made by Zhang et al. (2001) and obtained an O ( n log n ) algorithm with asymptotic worst-case ratio 4/3 for the special case in which the job sizes and job processing times are agreeable.

Several contributions have also used metaheuristics to search for near optimal solutions of a variety of BPM scheduling problems. Wang and Uzsoy (2002) coupled a random key representation based genetic algorithm with a dynamic program-ming algorithm to minimize maximum lateness on a single BPM in the presence of job release times. Melouk et al. (2004) developed a simulated annealing approach for minimizing makespan to schedule a single BPM with different job sizes.
Husseinzadeh Kashan et al. (2006a,b) proposed a genetic algorithm which could dominate the simulated annealing approach ( Melouk et al., 2004 ) and the constructive FFLPT heuristic ( Uzsoy, 1994 ). Koh et al. (2005) proposed some heuristics and a random key representation based genetic algorithm for the problems of minimizing makespan and total weighted completion time on a batch-processing machine with incompatible job families and non-identical job sizes. Husseinzadeh
Kashan and Karimi (2007) designed an ant colony optimization framework to solve BPM scheduling problems. They proposed different versions of an ant colony framework for the problem under the situation considered in Koh et al. (2005) . Chou et al. (2006) presented a hybrid genetic algorithm to minimize makespan for the dynamic case of the single batch-processing machine problem.

Scheduling non-identical size jobs on a flow shop of BPMs and on identical parallel BPMs are also addressed by researchers Koh et al. (2004) , Chang et al. (2004) , Husseinzadeh Kashan and Karimi (2008) , Husseinzadeh Kashan et al. (2008) .

To our best knowledge, this is the first effort on scheduling different size jobs on a BPM while optimizing more than one objective. Also, minimizing maximum tardiness on a BPM in the presence of non-identical job sizes has not been addressed in the literature. 3. Multi-objective optimization
A multi-objective optimization problem can be defined as optimization of a set of objective functions, which are usually in conflict with each other, via determining a vector of decision variables. Such a problem can be formulated as: Minimize f f 1  X  X  X  ; f 2  X  X  X  ... ; f M  X  X  X g s : t : X A D ;  X  1  X  where X is the vector of decision variables, f j ( X ) is the j -th objective function and D is the set of feasible solutions. A solution X 1 is said to be dominated by solution X 2 if 8 j A {1,2, y f Pareto-optimal if there is no X 0 A D that dominates X. The set of all Pareto-optimal solutions is called the Pareto-optimal set or Pareto-optimal front.

In multi-objective optimization, instead of obtaining a unique optimal solution, usually a set of equally good optimal solutions is obtained (Pareto-optimal set). More precisely, within a Pareto-set, by moving from one point to another, one objective improves while the other(s) deteriorates. In the absence of any other high level additional information, a decision maker normally cannot choose any one of these non-dominant solutions, since all of them are equally competitive and none of them can dominate the others.

One of the important issues in multi-objective optimization is to perform and discover the strength of relationships between particular objectives. This allows reducing the number of objectives into a group of few relevant objectives. Consequently, it gives the ability of focusing optimization process on a set of crucial objectives ( Hapke et al., 2002).

There are several methods available to solve multi-objective optimization problems, among them the weighted-average of the objectives ( Szidarovsky et al., 1986 ), the goal attainment method (Szidarovsky et al., 1986 ), the e -constraint method ( Szidarovsky et al., 1986), and many versions of multi-objective evolutionary algorithms ( Deb 2001 ) are better known. 4. Combinatorial complexity
Uzsoy (1994) proved that finding a batching scheme with the minimum makespan on a single BPM with non-identical job sizes is NP-hard in the strong sense. Brucker et al. (1998) showed that for a bounded model, n Z B and identical job sizes; due date based scheduling criteria give rise to NP-hard problems. Our problem investigates the case with non-identical job sizes, which is at least as difficult as the case with identical job sizes. Since the traditional counterpart is NP-hard, so minimizing the maximum tardiness with non-identical job sizes is also NP-hard.
Regarding the above conclusions, the NP-hardness of simulta-neous minimization of makespan and maximum tardiness with non-identical job sizes on a BPM can be inferred. To find out the intractability of our problem and getting an upper bound on the search space, we can enumerate all of the schedules (feasible and infeasible) by converting our problem to the problem of enumerating all the ways of distributing n distinguished jobs in at least l different batches where no empty batch is allowed. The total number of solutions to our problem is as follows: where n is the number of jobs to be scheduled, l  X  P n i  X  1 the minimum number of required batches ( x de denotes to the smallest integer greater than or equal to x ) and c ( k , j ) is the number of all possible ways to choose j batches from k different batches. As the problem size increases, the number of possible schedules increases exponentially. Since the problem is NP-hard, therefore obtaining the Pareto-optimal solutions may become impractical for large-scale problems. 5. Heuristics
The aim of this section is to review and propose constructive heuristics for batching jobs and scheduling batches on the machine. These heuristics construct the solution with respect to only one of C max or T max objectives. Among the existing heuristics, we introduce the batch first-fit (BFF) heuristic which is the first yet effective algorithm for batching jobs with respect to C criterion. Given a partial batching of jobs (feasible or infeasible), we propose heuristics trying to construct a feasible solution with respect to either C max or T max objectives. The main motivation for developing these heuristics is due to the type of chromosomal representation which we have introduced for one of the proposed MOGAs (see Section 6.1). In the following we give the details of these heuristics together with details of getting a lower bound on the makespan value.

Based on the first-fit procedure of Coffman et al. (1984), proposed for the bin-packing problem, Uzsoy (1994) constructed a set of heuristics. The batch first-fit heuristic (BFF) adapted to the scheduling problem is as follows: Heuristic BFF: Step 1. Arrange the jobs in some arbitrary order.

Step 2. Select the job at the head of the list and place it in the first batch with enough space to accommodate it. If it fits in no existing batch, create a new batch. Repeat Step 2 until all jobs have been assigned to a batch.

Step 3. Sequence the batches on the machine in any arbitrary order and compute the C max and T max values.

It has been shown that if in the first step of BFF the sorting of jobs is based on the longest processing time (LPT) rule, then the algorithm will be superior to minimize the makespan in both the average and the worst-case performance comparing to non-LPT based BFF algorithms. In this case it is called the FFLPT algorithm (Uzsoy, 1994 ).
 Definition 1. (Lee et al., 1992 ) We say a sequence of batches is in batch-EDD order if for any two batches P and Q in the schedule, where batch P is processed before batch Q , there is no pair of jobs T max value of a batch-EDD ordered schedule as T max EDD .
Since in terms of makespan criterion the ordering of batches is irrelevant, regarding the above definition we refine Step 3 of BFF as follows:
Step 3. If T max EDD o T max , sequence the batches on the machine in batch-EDD order.

In the first step of BFF algorithm, when the sorting of jobs is in increasing order of the due dates, it is entitled the FFEDD algorithm.

Based on our representation scheme used for one of the proposed MOGA, we developed two versions of a heuristic procedure called random batches procedure (RBP). The first version, RBP1, constructs a feasible schedule by considering particularly the makespan objective, whereas the second version, RBP2, constructs a feasible schedule by considering particularly the maximum tardiness objective. The following steps describe in details both RBP1 and RBP2.
 Heuristic RBP1:
Step 1. Assign all jobs to L batches randomly without considering the capacity restriction. If the resulted batching scheme is feasible, go to Step 4; otherwise go to Step 2.
Step 2. Choose a batch with capacity violation having the longest batch processing time. Select the job with the longest processing time in the batch. Put the selected job in the first feasible batch (with the smallest residual capacity) having the batch processing time longer than the processing time of the selected job, then go to Step 3. If there is no such a feasible batch with longer processing time, put the job in the feasible batch with longest processing time and go to Step 3. If the selected job fits in no existing batches, create a new batch and assign it the job. Step 3. Repeat Step 2 until getting a feasible batching scheme.
Step 4. Compute the C max and T max values. If T max EDD o T sequence the batches on the machine in batch-EDD order.
The above procedure tries to group jobs with longer processing time in the same batches and simultaneously minimizing the residual capacity of batches. Starting the heuristic with exactly one batch ( L= 1), the RBP1 almost works in a manner similar to FFLPT.
 Heuristic RBP2:
Step 1. Assign all jobs to L batches randomly without considering the capacity restriction. If the resulted batching scheme is feasible, go to Step 4 ; otherwise go to Step 2.
Step 2. Choose a batch with capacity violation having shortest batch due date, where for a given batch b the due date is defined as the shortest due date among those of all jobs in the batch.
Select the job with the shortest due date in the batch. Put the selected job in the first feasible batch (with the smallest residual capacity) having shortest batch due date and go to Step 3. If the selected job fits in no existing batches, create a new batch and assign it the job.
 Step 3. Repeat Step 2 until getting a feasible batching scheme.
Step 4. Compute the C max and T max values. If T max EDD o T sequence the batches on the machine in batch-EDD order.
RBP2 tries to group jobs with smaller due dates in the same batches and simultaneously minimizing the residual capacity of batches. If we start the heuristic with exactly one batch ( L= 1), the
RBP2 works in a manner similar to FFEDD. The effect of randomness included in the derived schedules by RBP procedures will be more highlighted by increasing the initial number of batches ( L ). So starting both RBP procedures with small number of batches would be more desirable.

Uzsoy (1994) proposed a lower bound on the optimal C max by relaxing the problem to allow jobs to be split and processed in different batches. A tighter bound can be obtained as follows when there are some jobs that cannot be grouped with any other job in the same batch. 1. Put each job y satisfying the following relation in the set J , and remove it from the set of whole jobs.

J  X f y 9 B s y o min 2. For the reduced problem construct an instance in which each job m with size s m and processing time t m is replaced by s of unit size with the processing time t m . This problem can be solved by sorting the jobs in decreasing order of their processing times, successively grouping the B jobs with longest processing times into the same batch, then processing the batches in any order. Let C LB max be the optimal makespan of the constructed instance. 3. The lower bound, C MLB max , can be obtained as follows: C 6. Proposed MOGAs and implementation
Evolutionary algorithms (EAs) are powerful and broadly applicable stochastic search and optimization techniques based on principles of evolution theory. In recent years, EAs have been known to be extremely robust techniques for solving complex multi-objective optimization problems. Since EAs work with a bunch of solutions, they are able to capture a population of
Pareto-optimal solutions in a single simulation run of the algorithm and abrogating the need of repetitive use of a single-objective optimization method to reach the Pareto-optimal set.
The non-dominated sorting genetic algorithm (NSGA) pro-posed by Srinivas and Deb (1995) is one of the first evolutionary algorithms for solving multi-objective optimization problems.
Although NSGA has been successfully applied to solving many problems, the main criticisms of this approach are its high computational complexity of non-dominated sorting, lack of elitism and need for specifying a tunable parameter called sharing parameter. In another paper, Deb et al. (2002) reported an improved version of NSGA, which they called NSGA-II, to remove all of the above deficiencies. In this paper we adapt NSGA-II to our problem. For detailed discussion on how to apply NSGA-II on multi-objective optimization problems readers may refer to Deb (2001) , Deb et al. (2002) .

Using the framework of NSGA-II, this paper proposes two different MOGAs for bi-criteria scheduling on a single BPM.
The first one is sequence based MOGA (SMOGA) that searches the solutions space via generating sequences of jobs by GA operators.
For each generated sequence, BFF heuristic is applied to group the jobs in batches. In the second MOGA entitled batch based hybrid MOGA (BHMOGA), searching the solution space is done directly by generating feasible batches of jobs by GA operators.
The feasibility is preserved via RBP heuristics. A local search heuristic approach is also combined with BHMOGA to improve its performance. The following steps describe in detail how the
MOGAs are implemented in this research. 6.1. Coding In our coding schemes, each gene corresponds to one of n jobs. In SMOGA, each chromosome is defined as a sequence of jobs.
The random key representation ( Bean, 1994 ) is a simple technique to generate random sequences of jobs. Since the random key representation eliminates the infeasibility of the offspring chromosomes (especially those resulted by crossover and muta-tion operators) by representing solutions in a soft manner, this representation is applicable to a wide variety of sequencing optimization problems. Using the random key representation, an order of jobs is represented with a sequence of uniformly distributed random numbers from [0,1]. The random numbers are then used as sort keys for decoding the solution. A simple example of using random key representation and the decoding mechanism to obtain a permutation of jobs can be found in Wang and Uzsoy (2002) .

In BHMOGA a solution for the problem of assigning jobs to batches is an array whose length is equal to the number of jobs to be scheduled. Each gene represents the batch to which the job is assigned. Figs. 1 and 2 show the sample chromosomes for a batch-scheduling problem with five jobs. 6.2. Initialization
SMOGA applies the random initialization in which each chromosome is initialized randomly through generating n numbers (keys) within [0,1]. In BHMOGA, chromosomes are initialized via RBP procedures to assign the jobs to batches.
As stated before, the performance of RBPs depends on the number of initial batches. So starting RBPs with a large number of initial batches would be unfavorable. To start with a relatively good population and to avoid trapping in locality, we use a truncated geometric distribution to generate randomly the number of initial batches used for RBPs to form the feasible initial population of chromosomes. Using a geometric distribution to simulate the number of initial batches ensures less probability for starting RBPs with large number of batches, against the high probability for starting with small number of batches. The following relation gives the random number of initial batches to start RBPs (details for obtaining Eq. (5) can be found in Husseinzadeh Kashan et al. (2006a) ): L  X  ln  X  1  X  1  X  1 p  X  where L is the random number of initial batches distributed with a truncated geometric distribution, r is a uniformly distributed random variable from [0,1], p is the probability of success, and l is the minimum number of required batches, l  X  P n i  X  1 s i
As stated before, the two RBP versions give the ability to make feasible the randomly generated batching schemes with respect to makespan or maximum tardiness criteria. So to reach a diverse and relatively good quality initial Pareto-set, we divide the initial population of BHMOGA into two subpopulations. The first sub-population includes the feasible schedules constructed by RBP1 and the second subpopulation includes the feasible schedules constructed by RBP2. For each subpopulation we consider a size equal to half of the population size.
 The above approach for generating the initial population of BHMOGA uses the knowledge of the problem when grouping the jobs and has the ability of localizing the search towards different objectives. However, there is no such knowledge for SMOGA. It is in the batching phase that we can use the problem knowledge (e.g. grouping jobs with longer processing times or shorter due dates in the same batch). A sequence of jobs (as SMOGA works with it) just reflects the order in which jobs are inserted to batchs and does not reflect any further information about the character-istics of the consecutive jobs. 6.3. Crossover
Following Wang and Uzsoy (2002) , the parameterized uniform crossover is used in SMOGA wherein a bias coin is used to determine which of the two selected parents should pass its gene. The parameterized uniform crossover allows us to bias the search more strongly towards the components of better solutions and to accelerate convergence. An example that shows the use of parameterized uniform crossover can be found in Wang and Uzsoy (2002) . Although in a single objective optimization problem a better solution in ranking is the solution with better objective function value, in case of a multi-objective optimization problem there is not a direct surpassing between Pareto-solutions. So, to determine the better parent for biasing the search towards it, we use the crowded tournament selection operator ( Deb et al., 2002 ).
 Definition 2. Crowded Tournament Selection Operator: a solu-tion i wins a tournament with another solution j if any of the following conditions are true: 1. If solution i has a better rank than solution j . 2. If they have the same rank but solution i has a better crowding distance than solution j .

The first condition makes sure that chosen parent lies on better non-dominated front. The second condition resolves the tie of both parents being on the same non-dominated front by deciding on their crowding distance. The parent residing in a less crowded area (with a large crowding distance) wins. For detailed discus-sion on crowding distance and its computation, readers may refer to Deb (2001) , Deb et al. (2002) .

It should be noted that in BHMOGA we do not use random keys, because in this case the chromosome is not represented as a sequence but as batch numbers that accommodate the jobs. For a fair comparison between BHMOGA and SMOGA, we extend the use of parameterized uniform crossover in BHMOGA. How-ever, our computational results reveal the good performance of applying parameterized uniform crossover in the body of BHMOGA.

In the crossover stage of BHMOGA, loss of some batch numbers in the generated offspring may happen. In this case we rename the batches sequentially. For making the generated offspring schedules feasible, and to address both diversity and quality objectives (see section 7.3), we adapt sequentially one offspring to Steps 2 and 3 of RBP1 and the other one to Steps 2 and 3 of RBP2. Fig. 3 shows the crossover procedure in BHMOGA. 6.4. Mutation
For each selected chromosome, swapping the content of two random genes gives the mutated chromosome. This type of mutation is known as swap mutation and we apply it in both MOGAs. In BHMOGA, the infeasible offspring are introduced to Steps 2 and 3 of RBP1 and RBP2, alternately. 6.5. Local search heuristic (LSH)
For BHMOGA, we developed an effective iterative local search heuristic, which reduces both C max and T max of a given schedule through reducing the completion time of the batches. It works by swapping jobs between batches consecutively, considering knowledge of the problem. The local search heuristic adapted to BHMOGA is as follows: Local search heuristic (LSH) For any selected schedule (name it the base schedule) do:
Step 1 . Find a batch having only one job with maximum processing time (call it batch a ). If there is no such batch a , consider the current schedule as LSH offspring and go to Step 3.
Step 2 . Find a batch (call it b ) with batch processing time equal to or longer than the processing time of batch a . Swap the job with longest processing time in a, ( j a ), with a job in b, ( j the processing time less than the processing time of job j respect to capacity constraint. Moreover, swapping should not increase the T max of the base schedule. Consider the current schedule as the base schedule and return to Step 1. If there is not any batch b, or any job j b , return to Step 1 and start with the next batch containing one job with maximum processing time (the new batch a ). Note that if for all such batches a in the base schedule, there is not any batch b or any job j b , then we should go to Step 3.

Step 3. Compute the C max and T max values for the obtained solution. If T max EDD o T max , sequence the batches on the machine in batch-EDD order.

The above procedure tries to make a selected solution closer to the final front. Inherently the proposed local search heuristic ensures no increasing in the C max value. Our preliminary computations show that even without applying the  X  X  X o increment in T max value X  X  restriction, LSH still has the ability of reducing T of the given schedules.

Suppose in the base schedule with m batches, there is a batch a which is processed before batch b . Swapping the jobs j a completion time of batch a is decreased. So, no job in batch a will increase the value of T max . Decrement in the completion time of batch a results decrement in the completion time of batches a+ 1, a+ 2, y , b , b+ 1, y , m . After swapping, if j a does not increase the
T value and the value of T max is related to a job in any batches a , Now assume there is a batch b which is processed before batch a .
Swapping the jobs j a and j b , the completion time of batch b is not increased. But the completion time of job j a is decreased. So, no job in batch b increases the T max value. Also after swapping, the completion time of batch a and consequently the completion time
T value and in the base schedule the T max value is related to a job in any batches a , a+ 1, a+ 2, y , m , then the T max swapped schedule will be decreased.

To decrease the computation time spent by LSH, only a small number of schedules generated by crossover and mutation operators should be selected to be improved. As a selection criterion we use the concept of closeness to the current best
Pareto-set ( P n ). To determine the closeness degree of any solution in the current population to the best front obtained so far, we use the following relation which finds the Euclidean distance between solution i and the nearest member in set P n .

D  X  min where f  X  k  X  m is the m th objective function value of the k member of P n . After sorting the solutions in increasing order of their distance (except for the solutions with distance equal to zero), the top solutions with smaller distance value are chosen based on a predefined rate equal to P LSH .
 To increase the effectiveness of the initial population of
BHMOGA and to start with a potentially good initial seeds, we apply LSH on all chromosomes generated by both RBP1 and RBP2 in the initial population. By mating the population resulted from
LSH with the initial population and performing sequentially the non-dominated sorting on the combined population, the top P ( P is the population size) solutions from better fronts are selected (with the probable use of the crowded tournament selection operator) to form the first population of BHMOGA. Although the performance of BHMOGA without applying LSH on the initial population of chromosomes is still good, its performance can be improved by applying LSH on the initial chromosomes. However in this case the time taken by BHMOGA is increased. Fig. 4 shows the performance of LSH applied on the initial population of
BHMOGA for a randomly generated problem. As can be seen, converging to the Pareto-optimal solutions may be best achieved through the hybridizing LSH with BHMOGA.

It seems in the refined initial population of BHMOGA, the non-dominated solutions are tended further to the tails of the initial
Pareto-front ( Fig. 4 ). Our computations show that the other parts of the final Pareto-front can be achieved using GA operators (selection, crossover, mutation and LSH). Thus, it is expected to reach a set of diverse Pareto-solutions at the end of BHMOGA execution. 30 80 130 180 230 Tmax
Rename Again, due to the representational restriction, we cannot use LSH in the body of SMOGA. In order to improve a solution (in form of a sequence) generated in an iteration of SMOGA, first the sequence should be transformed to its corresponding batching scheme using BFF and then LSH should be applied on the batching scheme. Now the improved batching scheme should be encoded into a sequence of jobs corresponding to it (because here we need to represent the solution as a sequence of jobs through random keys). The arising problem is that we cannot determine the corresponding sequence of a given batching scheme. 7. Experimentations and results 7.1. Test problem instances
For testing effectiveness of the proposed MOGAs, random test problem instances are generated. To cover various types of problems, some factors are identified: number of jobs, variation in job sizes, variation in job processing times and variation in job due dates. In general, 12 categories of problems are generated by combining three levels of job sizes (small, large and combination of small and large), two levels of job processing times and two levels of job due dates.

Job sizes, processing times and due dates are generated from discrete uniform distributions. The factors, their levels and ranges are shown in Table 1 (job sizes and processing times are generated similar to ( Melouk et al., 2004 )). The term C MLB the lower bound on the optimal C max which assembles the effect of problem size ( n ), job sizes and job processing times on job due dates. Using different values for parameter a (see Table 1 ), relatively tight or wide due dates can be assigned to jobs. The machine capacity is assumed to be 10 in all instances. Each category of problems is represented with a run code. For example, a problem category with 10 jobs, job processing times generated from the discrete uniform distribution within [1,10], job due dates generated with a =0.8 and job sizes generated from the discrete uniform distribution within [1,10] is shown by J1t1d1s1. 7.2. Choosing the objectives
As was mentioned before, reducing the number of objectives into a set of relevant conflict objectives make it possible to provide the decision maker with good solutions, much quicker and easier. Table 2 summarizes the correlations among the five most popular objectives used in the BPM scheduling problems. Based on the results, we can observe relationship between pairs of objectives to recognize their correlation. The correlations have been estimated based on the average correlations obtained by evaluating 720,000 randomly generated schedules (for each of 12 categories, 30 different problem instances with 50 jobs were generated and then 2000 random schedules were produced based on the data of each instance).

In Table 2 , a relatively strong correlation ( 9 R 9 Z 0.4) shows the non-conflict group of objectives. In this case, someone can consider only one of these objectives and so reduces the number of objectives. For example, the pairs of the total completion time and the total tardiness criteria, or the total completion time and the number of tardy jobs criteria are non-conflict cases. On the other hand, a relatively week correlation denotes the group of objectives with conflict that cannot be aggregated into one objective. As can be seen, the pair of makespan and maximum tardiness are such weakly correlated objectives. 7.3. Performance measures and comparison method
In the literature of multi-objective optimization, two general types of performance measures are considered as distinct goals: quality ( Q ) and diversity ( D ) based measures. Quality goal is considered as minimizing the distance between the obtained Pareto-set and the Pareto-optimal set. Diversity goal is considered as maximizing the extent of the obtained Pareto-set. Comparing two algorithms with respect to their quality and diversity of Pareto-solutions, Hyun et al. (1998) developed measures for comparison as follows:
Suppose P A and P B are the set of Pareto-solutions obtained by algorithms A and B , respectively and N A = 9 P A 9 and N the number of Pareto-solutions in P A and P B , respectively, where 9
X 9 denotes the cardinality of the set X . Let P C be the set of non-dominated solutions obtained after combining P A and P B . A quality measure of performance for A ( B ) can be defined as 9 P A ( B ) Also, a diversity measure for A ( B ) is simply defined as N
The above quality measure calculates the contribution of algorithm A ( B ) in construction of the final Pareto-set ( P be seen, it does not provide any further information to answer this question: how far are the solutions of A from solutions of B ?To answer such a question, we use the concept of hypervolume metric ( Veldhuizen, 1999 ), which calculates the volume (in the objective space) covered by the members of a Pareto-set Q .At first, for each member of Q a hypervolume is constructed with a reference point W and the current member as the diagonal corners of the hypercube. Thereafter, a union of all hypercubes is found and the hypervolume (HV) related to Q is calculated. Fig. 5 shows the hypervolume of the Pareto-set Q with N Q =4.
For making comparison between SMOGA and BHMOGA, we propose the following gap-index, which is the normalized difference between HV of SMOGA, HV (SMOGA), and HV of
BHMOGA, HV (BHMOGA): gap index  X  X  HV  X  SMOGA  X  HV  X  BHMOGA  X  = HV  X  IRS  X  X  7  X 
Since there is not any known reference set related to our (IRS), where HV(IRS) denotes the HV of IRS. To our best knowledge, there is no effective lower bound on the optimal value of T max . So we consider the most obvious lower bound point W as the pair of C UB max and T UB max , where C UB upper bound on the optimal C max computed by assigning each job the upper bound on the optimal T max that is obtained by max f C UB max min better performance of BHMOGA.

Regarding small problem instances, i.e. problem sets with 10 jobs, both MOGAs are compared with Pareto-optimal set obtained via total enumeration (TE). For making comparisons among other sets of problem instances we will use the quality and diversity measures and also the gap-index.

Proposed MOGAs and TE algorithm are coded in MATLAB 6.5.1 and are executed on a Pentium III, 800 MHz computer with 128 MB RAM. All test problem instances are solved 15 times by both SMOGA and BHMOGA and the comparisons are made based on the non-dominated solutions obtained after combining the
Pareto-solutions resulted from 15 times running of each algo-rithm. 7.4. Parameters setting
The performance of an EA is generally sensitive to the setting of the parameters influencing the search behavior and the quality of convergence. It is highly desirable to set these parameters to the levels that produce high quality solutions.

For both SMOGA and BHMOGA, we consider the population size equal to n and the number of generations equal to 200. Also both MOGAs are stopped if there is no improvement in the best solutions obtained in the last 100 generations.

For tuning crossover rate, mutation rate and the probability of getting a head in the biased coin toss used for the crossover operation, different values are considered: 0.5 and 0.7 for crossover rate: 0.05, 0.1 and 0.15 for mutation rate and 0.7, 0.9 for head probability. Both SMOGA and BHMOGA were tested on each triple combination of the above factors. Based on the preliminary experiments, we found 0.2 and 0.1 to be appropriate values for the truncated geometric distribution parameter (prob-ability of success) and P LSH , respectively. Table 3 shows the results of tuning. 7.5. Computational results and comparisons
For testing the effectiveness of the proposed MOGAs, random test problem instances are generated as described before. We have summarized the results for instances with 10 jobs in Table 4 .
In this table, columns 2 and 3 report the quality and diversity measures for SMOGA, respectively. For BHMOGA these values are presented in columns 5 and 6, respectively. Columns 4 and 7 report the average time among 15 runs of MOGAs. Tables 5 X 7 can be interpreted in the same manner. In Table 4 , columns 8 and 9 show the quality and diversity measures for TE algorithm, which are obtained after 7200 s. It is worth mentioning that the quality
T and diversity measures reported for the three algorithms in Table 4 are calculated based on comparing the result of each algorithm with the non-dominated solutions obtained after combining Pareto-solutions of the three algorithms. The last column in each of the resulting tables gives the gap-index between SMOGA and BHMOGA, which is calculated by (7). Fig. 6 illustrates the Pareto-fronts obtained by BHMOGA and SMOGA for all of the problem instances with 100 jobs.
Computational analysis shows that in all categories of test problem instances, BHMOGA performs better than SMOGA, Cmax 50 60 70 80 90 100 110 120
T max 145 155 165 175 185 195 205 215 225 Tmax 10 15 20 25 30 35 Tmax Cmax 20 40 60 80 100 120 Tmax especially for the problems with large number of jobs. With respect to increasing trend in the reported gap-index values, we expect BHMOGA to outperform SMOGA as the problem size increases.
 Looking at the values of the quality measure reported in Tables 4 X 7 , it can be inferred that there is not any solution obtained by SMOGA that dominates a solution by BHMOGA. This is because that the quality measure reported for BHMOGA is equal to 1 for all test problem instances. As can be seen, increasing the problem size significantly decreases the average value of quality measure reported for SMOGA.
 The results of Table 4 demonstrate high performance of MOGAs compared to TE algorithm. As can be seen, even for the problems with 10 jobs that we checked, TE algorithm has not been able to report the true Pareto-optimal solutions after 7200 s of CPU time. However, in a very small amount of time both MOGAs could reach to solutions equal or better than the solutions reported by TE algorithm.

The largest gap between SMOGA and BHMOGA is reported for the problems with small size jobs (J.t.d.s2). Since for these problems the feasible search space is larger than the others and LSH is more relaxed in swapping jobs, converging to the optimal front will be best achieved.

For the problems with mixed size jobs (J.t.d.s1), although the gap values are not as large as the case of problems with small jobs, they are still meaningfully large, especially for larger problems.
The smallest gaps are reported for the problems with large size jobs (J.t.d.s3). For these problems the feasible search space is smaller than that of other problems, due to having many batches with only one job (on average 40% of batches contains only one job with size equal to 7 or 8). This led to a reduction in the gap values between SMOGA and BHMOGA. However the increment in the gap values is more likely as the problem size increases.
Comparing the required running times, results show that the average time needed for BHMOGA is less than SMOGA. A major part of the running time in BHMOGA belongs to LSH. Eliminating LSH still gives BHMOGA the ability of outperforming SMOGA, at the expense of weakening the quality of outperforming.
Using RBP heuristics to get feasibility, generally takes less time than batching a sequence of jobs using BFF heuristic. In SMOGA for each generated offspring, BFF is applied to assign all of n jobs to batches, while in BHMOGA when an offspring with m batches is formed, to make it feasible, at most n X  X  jobs must be reassigned by RBPs because at least m jobs do not need to be reassigned.
With reference to results, it can be inferred that the good performance of BHMOGA is directly due to the mechanisms (such as RBPs and LSH) that exploit the knowledge of the problem. So its predominance on SMOGA would be expectable through applying the basic framework of any other type of MOGAs (instead of NSGA-II) that have been presented in the literature. 8. Conclusions and future directions
Bi-criteria scheduling on a batching machine with non-identical job sizes was addressed in this paper. The processing time of a batch is given by the longest processing time of the jobs in the batch. To examine the search space of the problem we developed an enumerative formula as a function of both problem size ( n ) and the least number of required batches ( l ). Checking the validity of considering both makespan and maximum tardiness objectives as a conflict group of objectives, we estimated the degree of correlation between these objectives. The result signified the incommensurability of these objectives. We also estimated the pair-wise correlations between some of the most popular objectives commonly used in BPM scheduling problems.
Based on different chromosome representations, we proposed two different multi-objective genetic algorithms for minimizing makespan and maximum tardiness. Computational results show that one of our MOGAs called BHMOGA significantly outperforms the other one called SMOGA, taking into account the quality and diversity measures and also a gap-based index.

Some characteristics that give BHMOGA the ability to discover solutions closer to the Pareto-optimal solutions and help it to reach a set of diverse solutions, can be summarized as follows: using a mechanism for generating the initial population, localiz-ing the search in crossover and mutation stages using RBP heuristics and using an effective local search heuristic which has the ability of steering the search quickly towards the Pareto-optimal solutions.

For future research, extension of our approach to the case of scheduling with incompatible job families, dynamic job arrivals and other shop systems e.g. flow shop and parallel batch processing machines are encouraged. Developing heuristics for minimizing maximum tardiness on a batching machine with non-identical job sizes and also providing efficient lower bounds for evaluation of algorithms would be interesting directions. Acknowledgements The authors would like to acknowledge the Iran National Science Foundation (INSF) for the financial support of this work. References
