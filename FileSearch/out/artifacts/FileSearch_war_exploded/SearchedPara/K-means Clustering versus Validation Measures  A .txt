 K-means is a widely used partitional clustering method. While there are considerable research efforts to characterize the key features of K-means clustering, further investigation is needed to reveal whether and how the data distributions can have the impact on the performance of K-means clus-tering. Indeed, in this paper, we revisit the K-means clus-tering problem by answering three questions. First, how the  X  X rue X  cluster sizes can make impact on the performance of K-means clustering? Second, is the entropy an algorithm-independent validation measure for K-means clustering? Fi-nally, what is the distribution of the clustering results by K-means? To that end, we first illustrate that K-means tends to generate the clusters with the relatively uniform distri-bution on the cluster sizes. In addition, we show that the entropy measure, an external clustering validation measure, has the favorite on the clustering algorithms which tend to reduce high variation on the cluster sizes. Finally, our exper-imental results indicate that K-means tends to produce the clusters in which the variation of the cluster sizes, as mea-sured by the Coefficient of Variation (CV), is in a specific range, approximately from 0.3 to 1.0.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering Algorithms, Experimentation K-means Clustering, Coefficient of Variation (CV), Entropy
Cluster analysis [9] provides insight into the data by di-viding the objects into groups (clusters) of objects, such that objects in a cluster are more similar to each other than to objects in other clusters. K-means [15] is a well-known and Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. widely used partitional clustering method. In the literature, there are considerable research efforts to characterize the key features of K-means clustering. Indeed, people have identi-fied some characteristics of data that may strongly affect the K-means clustering analysis including high dimension-ality, the size of the data, the sparseness of the data, noise and outliers in the data, types of attributes and data sets, and scales of attributes [20]. However, further investigation is needed to reveal whether and how the data distributions can have the impact on the performance of K-means clus-tering. Along this line, in this paper, we revisit K-means clustering by answering three questions. 1. How can the distribution of  X  X rue X  cluster sizes make 2. Is the entropy an algorithm-independent validation mea-3. What is the distribution of the clustering results by
The answers to these questions can guide us for the bet-ter understanding and the use of K-means. This is note-worthy since, for document data, K-means has been shown to perform as well or better than a variety of other clus-tering techniques and has the appealing computational ef-ficiency [19, 22]. To this end, we first illustrate that K-means clustering tends to generate the clusters with the relatively uniform distribution on the cluster sizes. Also, we show that the entropy measure, an external clustering validation measure, has the favorite on the clustering algo-rithms, such as K-means, which tend to reduce the varia-tion on the cluster sizes. In other words, entropy is not an algorithm-independent validation measure.

In addition, we have conducted extensive experiments on a number of real-world data sets from various different ap-plication domains. Our experimental results show that K-means tends to produce the clusters in which the variation of the cluster sizes is in a specific range. This data variation is measured by the Coefficient of Variation (CV) [2]. The CV, described in more detail later, is a measure of dispersion of a data distribution and is a dimensionless number that allows comparison of the variation of populations that have significantly different mean values. In general, the larger the CV value is, the greater the variability in the data.
Indeed, as shown in our experimental results, for the data sets with high variation on the  X  X rue X  cluster size (e.g. CV &gt; 1 . 0), K-means reduces this variation in the resulting cluster sizestolessthan1 . 0. Meanwhile, for the data sets with low variation on the  X  X rue X  cluster size (e.g. CV &lt; 0 . 3), K-means increases the variation slightly in the resulting cluster sizes to greater than 0 . 3 . In other words, for these two cases, K-means produces the clustering results which are away from the  X  X rue X  cluster distributions.
People have investigated K-means clustering from various perspectives. Many data factors, which may strongly affect the performance of K-means, have been identified and ad-dressed. In the following, we highlight some results which are mostly related to the main theme of this paper.
First, people have studied the impact of the high dimen-sionality on the performance of K-means and found that the traditional Euclidean notion of proximity is not very effec-tive for K-means on high-dimensional data sets, such as gene expression data sets and document data sets. To meet this challenge, one research direction is to employ dimensional-ity reduction techniques, such as Multidimensional Scaling (MDS) or Singular Value Decomposition(SVD). Another di-rection is to redefine the notions of proximity, e.g., by the Shared Nearest Neighbors (SNN) similarity [10].

Second, many clustering algorithms that work well for small or medium-size data sets are unable to handle larger data sets. Along this line, a discussion of scaling K-means clustering to large data sets is provided by Bradley et al. [1]. Also, Ghosh [5] discussed the scalability of clustering methods in depth and a more broad discussion of specific clustering techniques can be found in [16].

Third, outliers and noise in the data can also degrade the performance of clustering algorithms, especially for prototype-based algorithms such as K-means. There has been ser-val techniques designed for handling this problem. For ex-ample, DBSCAN automatically classifies low-density points as noise and removes them from the clustering process [4]. Chameleon [12], SNN density-based clustering [3], and CURE [6] explicitly deal with noise and outliers during the cluster-ing process.

Finally, the researchers have identified some other data factors, such as the types of attributes, the types of data sets, and scales of attributes, which may have the impact on the performance of K-means clustering. However, in this paper, we target on understanding the impact of the distribution of the  X  X rue X  cluster size on the performance of K-means clus-tering and the cluster distribution of the clustering results by K-means. Also, we investigate the relationship between K-means and the entropy measure.
In this section, we illustrate the effect of K-means clus-tering on the distribution of the cluster sizes, and show the relationship between the entropy measure and K-means.
K-means [15] is a prototype-based, simple partitional clus-tering technique which attempts to find a user-specified k number of clusters. These clusters are represented by their centroids (a cluster centroid is typically the mean of the points in the cluster). The clustering process of K-means is as follows. First, k initial centroids are selected, where k is specified by the user and indicates the desired number of clusters. Every point in the data is then assigned to the closest centroid, and each collection of points assigned to a centroid forms a cluster. The centroid of each cluster is then updated based on the points assigned to the cluster. This process is repeated until no point changes clusters.
In general, there are two kinds of clustering validation techniques, which are based on external criteria and internal criteria respectively. Entropy is a commonly used external validation measures for K-means clustering [19, 22]. As an external criteria, entropy uses external information  X  class labels in this case. Indeed, entropy measures the purity of the clusters with respect to the given class labels. Thus, if every cluster consists of objects with only a single class label, the entropy is 0. However, as the class labels of objects in a cluster become more varied, the entropy value increases.
To compute the entropy of a set of clusters, we first cal-culate the class distribution of the objects in each cluster, i.e., for each cluster j we compute p ij , the probability that a member of cluster j belongs to class i . Given this class distribution, the entropy of cluster j is calculated using the standard entropy, E j =  X  taken over all classes and the log is log base 2. The total entropy, E = as the weighted sum of the entropies of each cluster, where n j is the size of cluster j , m is the number of clusters, and n is the number of all data points.

In a similar fashion, we can compute the purity of a set of clusters. First, we calculate the purity of each cluster. For each cluster j , we have the purity P j =max i ( n i j where n i j is the number of objects in cluster j with class label i .Inotherwords, P j is the fraction of the overall cluster size that the largest class of objects assigned to that cluster represents. The overall purity of the clustering solution is obtained as a weighted sum of the individual cluster purities and is given as Purity = cluster j , m is the number of clusters, and n is the number of all data points. In general, we believe that the larger the value of purity, the better the clustering solution is.
Before we describe the joint effect of K-means clustering and the entropy measure, we first introduce Coefficient of Variation (CV) [2], which is a measure of dispersion for a data distribution. CV is defined as the ratio of the standard deviation to the mean. Given a set of data objects X = { x 1 ,x 2 ,...,x n } ,wehaveCV = s  X  x ,where  X  x = and s =
Please note that there are some other statistics, such as standard deviation and skewness, which can also be used to characterize the dispersion degree of data distributions. However, the standard deviation has no scalability; that is, the dispersion degree of the original data and the stratified sample data is not equal as indicated by standard devia-tion, which does not agree with our intuition. Meanwhile, skewness cannot catch the dispersion in the situation that the data are symmetric but do have high variance. Indeed, CV is a dimensionless number that allows comparison of the variation of populations that have significantly different mean values. In general, the larger the CV value, the greater the variability is in the data.
In this section, we illustrate the effect of K-means cluster-ing on the distribution of the cluster sizes.
Figure 1: Clusters before K-means Clustering.  X + X : the centroids
Figure 1 shows a sample data set with three  X  X rue X  clus-ters. The numbers of points in Cluster 1, 2 and 3 are 96, 25 and 25, respectively. In this data, Cluster 2 is much closer to Cluster 1 than Cluster 3. Figure 2 shows the cluster-ing results by K-means on this data set. As can be seen, three natural clusters could not be identified exactly. One observation is that Cluster 1 is broken: part of Cluster 1 is merged with Cluster 2 as new Cluster 2 and the rest of clus-ter 1 forms new Cluster 1. However, the size distribution of the resulting two clusters is more uniform now. This is called the  X  uniform effect  X  of K-means on  X  X rue X  clusters with different sizes. Another observation is that Cluster 3 is precisely identified by K-means. It is due to the fact that the objects in Cluster 3 are far away from Cluster 1. In other words, the uniform effect has been dominated by the large distance between two clusters. From the above, we can notice that the uniform effect of K-means clustering on  X  X rue X  clusters with different sizes does exist. We will fur-ther illustrate this in our experimental section.

In our practice, we have observed that the entropy mea-sure tends to favor clustering algorithms, such as K-means, which produce clusters with relatively uniform sizes. We call this the  X  biased effect  X  of the entropy measure. To illustrate this, we created the sample data set as shown in Table 1. This data set consists of 42 documents with 5 class labels. In other words, there are five  X  X rue X  clusters in this sample data. The CV value of the cluster sizes of these five  X  X rue X  clusters is 1 . 1187 as presented in the table.
For this sample document data set, we assume that we have two clustering results by different clustering algorithms as shown in Table 2. In the table, we can observe that the first clustering result has five clusters with relatively uni-form sizes. This is also indicated by the CV value, which is 0 . 4213. In contrast, for the second clustering result, the CV value of the cluster sizes is 1 . 2011. This indicates that the five clusters have widely different cluster sizes for the second clustering scheme. Certainly, according to entropy, cluster-ing result I is better than clustering result II (This result is due to the fact that the entropy measure more heavily pe-nalizes a large impure cluster.) However, if we look at five  X  X rue X  clusters carefully, we find that the second clustering results are much closer to the  X  X rue X  cluster distribution and the first clustering results are actually away from the  X  X rue X  cluster distribution. This is also reflected by the CV values. The CV value of five cluster sizes in the second clustering results is closer to the CV value of five  X  X rue X  cluster sizes.
Finally, in Table 2, we can also observe that the purity of the second clustering results is better than that of the first clustering results. Indeed, this is contradict to the results by the entropy measure. In summary, this example illustrates that the entropy measure has the favorite on the algorithms, such as K-means, which produce clusters with relatively uni-form sizes. In other words, if the entropy measure is used for validating the K-means clustering, the validation results can be misleading.
In this section, we present experimental results to show the impact of data distributions on the performance of K-means clustering. Specifically, we demonstrate: (1) the ef-fect of the  X  X rue X  cluster sizes on K-means clustering; and (2) the effect of the entropy measure on the K-means clus-tering results. Experimental Tool . In our experiments, we used the im-plementation of K-means in CLUTO [11]. For all the exper-iments, the cosine similarity is used in the objective function for K-means. Finally, please note that some notations used in our experiments are shown in Table 3.
 Experimental Data Sets . For our experiments, we used a number of real-world data sets that were obtained from different application domains. Some characteristics of these data sets are shown in Table 4. In the table,  X # of Classes X  indicates the number of  X  X rue X  clusters.
 Document Data Sets. The fbis data set was from the Foreign Broadcast Information Service data of the TREC-5 collection [21]. The hitech and sports data sets were derived from the San Jose Mercury newspaper articles that were distributed as part of the TREC collection (TIPSTER Vol. 3). Data sets tr23 and tr45 were derived from the TREC-5[21], TREC-6 [21], and TREC-7 [21] collections. The la2 data set was part of the TREC-5 collection [21] and contains news articles from the Los Angeles Times. The ohscal data set was obtained from the OHSUMED collec-tion [8], which contains documents from various biological sub-fields. The data sets re0 and re1 were from Reuters-21578 text categorization test collection Distribution 1 . 0 [13]. The data sets k1a and k1b contain exactly the same set of documents but they differ in how the documents were assigned to different classes. In particular, k1a contains a finer-grain categorization than that contained in k1b .The data set wap was from the WebACE project (WAP) [7]; each document corresponds to a web page listed in the subject hierarchy of Yahoo!. For all document clustering data sets, we used a stop-list to remove common words, and the words were stemmed using Porter X  X  suffix-stripping algorithm [18].
Biological Data Sets. LungCancer and Leukemia data sets were from the Kent Ridge Biomedical Data Set Repository (KRBDSR) which is an online repository of high dimensional features [14]. The LungCancer data set con-sists of samples of lung adenocarcinomas, squamous cell lung carcinomas, pulmonary carcinoid, small-cell lung car-cinomas and normal lung described by 12600 genes. The Leukemia data set contains 6 subtypes of pediatric acute lymphoblastic leukemia samples and 1 group samples that do not fit in any of the above 6 subtypes, and each is de-scribed by 12558 genes.

UCI Data Sets [17]. The ecoli data set is about the information of cellular localization sites of proteins. The page-blocks data set contains the information of 5-type blocks of the page layout of a document that has been de-tected by a segmentation process. The pendigits and letter data sets contain the information of handwritings. The former is the numeric information of 0-9, and the latter letter information of A-Z. Figure 3: The Distributions of CV Values before and after K-means Clustering. Figure 4: Illustration of the  X  X iased Effect X  of En-tropy on All the Experimental Data Sets. Figure 5: Illustration of the  X  X iased Effect X  of En-tropy Using Sample Data Sets from  X  X endigits X .
Here, we illustrate the effect of the  X  X rue X  cluster sizes on the results of K-means clustering. In our experiment, we first used K-means to cluster the input data sets, and then computed the CV values for the  X  X rue X  cluster distri-bution of the original data and the cluster distribution of the clustering results. The number of clusters k was set as the  X  X rue X  cluster number for the purpose of comparison.
Table 5 shows the experimental results on various real-world data sets. As can be seen, for the data sets with large CV 0 , K-means tends to reduce the variation on the cluster sizes of the clustering results as indicated by CV This result indicates that, for data sets with high variation on the cluster sizes of  X  X rue X  clusters, the uniform effect of K-means is dominant; that is, K-means tends to reduce the variation on the cluster sizes in this case.

Another observation is that, for data sets with low CV 0 values, K-means increases the variation on the cluster sizes of the clustering results slightly as indicated by the corre-sponding CV 1 values. This result indicates that, for data sets with very low variation on the cluster sizes of  X  X rue X  clusters, the uniform effect of K-means is not significant. Other factors, e.g., the variant shapes, densities, or the cen-troid distances between the  X  X rue X  clusters, tend to be the dominant factors instead.

Indeed, Figure 3 shows the link relationships between CV 0 and CV 1 for all the experimental data sets listed in Table 4, and there is a link between CV 0 and CV 1 for every data set. A very interesting observation is that, while the range of CV 0 is between 0 . 03 and 1 . 95, the range of CV 1 is re-stricted into a much smaller range from 0 . 33 to 0 . 94. Thus we empirically have the interval of CV 1 values: [0 . 3, 1 . 0].
In this subsection, we present the effect of the entropy measure on the K-means clustering results. Figure 4 shows the plot of entropy values for all the experimental data sets in Table 4. A general trend can be observed is that while the differences in CV values before and after clustering increase as the increase of CV 0 values, the entropy values tend to decrease. In other words, there is a disagreement between DCV and the entropy measure on evaluating the cluster-ing quality. The entropy measure indicates better quality, but DCV shows that the distributions of clustering results are away from the distributions of  X  X rue X  clusters. This indi-cates worse clustering quality. The above observation agrees with our analysis in Section 3 that the entropy measure has a biased effect on K-means.

To strengthen the above observations, we also generated two groups of synthetic data sets from two real-world data sets: pendigits and letter . To generate data sets from pendigits , we applied the following sampling strategy: 1) We first sampled the original data set to get a sample with 10  X  X rue X  clusters, each of which contains 1000 , 100 , 100 , objects, respectively. Then 2) we did random sampling on the biggest cluster and merged the samples with all the other objects in the rest 9 clusters to form a data set. We gradually reduced the sample size to 100, thus obtained various data sets with decreasing dispersion degrees. On the other hand, in order to have data sets with increasing dispersion degrees, 3) we did random, stratified sampling to the 9 smaller clus-ters, and merged the samples with the rest 1000 objects to form a data set. We gradually reduced the sample size for each of the 9 clusters to 30, thus got a series of data sets with increasing dispersion degrees. A similar sampling strategy was also applied to letter . Note that for each dispersion degree we did sampling 10 times and output the average values as the sampling results.

Figure 5 shows the corresponding plot of the entropy val-ues for the synthetic data sets derived from the pendigits data set. A similar trend has been observed; that is, the entropy values and the DCV values do not agree with each other for clustering validation as the increase of CV 0 val-ues. Due to the page limit, we have omitted a similar plot for the second group of synthetic data sets derived from the letter data set.
In this paper, we illustrate the relationship between K-means and the  X  X rue X  cluster sizes as well as the entropy measure. Our experimental results demonstrate that K-means tends to reduce the variation on the cluster sizes if the variation of the  X  X rue X  cluster sizes is high and increase the variation on the cluster sizes if the variation of the  X  X rue X  cluster sizes is very low. In addition, we found that, no matter what are the CV values of the  X  X rue X  cluster sizes, the CV values of the clustering results are typically located in a much smaller range from 0.3 to 1.0. Finally, we ob-served that many  X  X rue X  clusters were disappeared in the clustering results if K-means is applied for data sets with high variation on the  X  X rue X  cluster sizes; that is, K-means produces the clustering results which are far away from the  X  X rue X  cluster distribution. This is actually contradicted by the entropy measure, since the entropy values are usu-ally very low for the data sets with high variation on the  X  X rue X  cluster sizes. In other words, the entropy measure is not an algorithm-independent clustering validation measure and has the favorite on K-means. [1] P. Bradley, U. Fayyad, and C. Reina. Scaling [2] M. DeGroot and M. Schervish. Probability and [3] L.Ertoz,M.Steinbach,andV.Kumar.Anewshared [4] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [5] J. Ghosh. Scalable Clustering Methods for Data [6] S. Guha, R. Rastogi, and K. Shim. Cure: An efficient [7] E.-H. Han, D. Boley, M. Gini, R. Gross, K. Hastings, [8] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam. [9] A. Jain and R. Dubes. Algorithms for Clustering [10] R. Jarvis and E. Patrick. Clusering using a similarity [11] G. Karypis. In [12] G. Karypis, E.-H. Han, and V. Kumar. Chameleon: A [13] D. Lewis. Reuters-21578 text categorization text [14] J. Li and H. Liu. In http://sdmc.i2r.a-star.edu.sg/rp/ . [15] J. MacQueen. Some methods for classification and [16] F. Murtagh. Clustering Massive Data Sets, Handbook [17] D. Newman, S. Hettich, C. Blake, and C. Merz. Uci [18] M. F. Porter. An algorithm for suffix stripping. In [19] M. Steinbach, G. Karypis, and V. Kumar. A [20] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction [21] TREC. In http://trec.nist.gov . [22] Y. Zhao and G. Karypis. Criterion functions for
