 Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP com-munity. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Mar-tins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K  X  ubler et al., 2009; Goldberg and Nivre, Figure 1: Sketch of three recursive neural net-works (RNN). (a) is the standard RNN for con-stituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the depen-dency relations between the nodes; (c) is DAG-GRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014).

Recently, distributed representations have been widely used in a variety of natural language pro-cessing (NLP) tasks (Collobert et al., 2011; De-vlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neu-ral network based methods have also been increas-ingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015).

However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often se-lect the first and second leftmost/rightmost chil-dren of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neu-ral network. Since the concatenation operation is relatively simple, it is difficult to model the com-plicated feature combinations which can be man-ually designed in the traditional discrete feature based methods.

To tackle these problems, we use two het-erogeneous gated recursive neural networks, tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph gated struc-tured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compo-sitions of the nodes during parsing.
 Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is ap-plied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from chil-dren nodes into their parent according to their de-pendency structures, while DAG-GRNN further models the complicated combinations of extracted features and explicitly exploits features in differ-ent levels of granularity.

To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the ef-fectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we re-ceive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. 2.1 Transition Dependency Parsing In this paper, we employ the arc-standard tran-sition systems (Nivre, 2004) and examine only greedy parsing for its efficiency. Figure 2 gives an example of arc-standard transition dependency parsing.

In transition-based dependency parsing, the consecutive configurations of parsing process can sists of a stack s , a buffer b , and a set of dependency arcs A . Then, the greedy pars-ing process consecutively predicts the actions based on the features extracted from the corre-sponding configurations. For a given sentence w 1 ,...,w n , parsing process starts from a initial configuration c (0) = ([ ROOT ] , [ w 1 ,...,w n ] ,  X  ) , ([ ROOT ] ,  X  ,A (2 n ) ] , where n is the length of the given sentence w 1: n . As a result, we derive the parse tree of the sentence w 1: n according to the
In arc-standard system, there are three types of actions: Left-Arc , Right-Arc and Shift . Denot-ing s j ( j = 1 , 2 ,... ) as the j th top element of the stack, and b j ( j = 1 , 2 ,... ) as the j th front ele-ment of the buffer, we can formalize the three ac-tions of arc-standard system as:  X  Left-Arc ( l ) adds an arc s 2  X  s 1 with label  X  Right-Arc ( l ) adds an arc s 2  X  s 1 with label  X  Shift removes b 1 from the buffer, and adds it
The greedy parser aims to predict the correct transition action for a given configuration. There are two versions of parsing: unlabeled and labeled versions. The set of possible action candidates T = 2 n l + 1 in the labeled version of parsing, and T = 3 in the unlabeled version, where n l is number of different types of arc labels. 2.2 Neural Network Based Parser In neural network architecture, the words, POS tags and arc labels are mapped into distributed vectors (embeddings). Specifically, given the w R d e of E w according to its index in the dictionary, where d e is the dimensionality of embeddings and n w is the dictionary size. Likewise, The POS and arc labels are also mapped into embeddings by the where n t and n l are the numbers of distinct POS tags and arc labels respectively. Correspondingly, embeddings of each POS tag t i and each arc label l and E l respectively. Figure 3: Architecture of neural network based transition dependency parsing.

Figure 3 gives the architecture of neu-ral network based parser. Following Chen and Manning (2014), a set of el-ements S from stack and buffer (e.g.
 S = { s 2 .lc 2 .rc 1 ,s 2 .lc 1 ,s 1 ,b 2 ,s 2 .rc 2 .rc is chosen as input. Specifically, the information (word, POS or label) of each element in the set S (e.g. { s 2 .lc 2 .rc 1 .t,s 2 .lc 1 .l,s 1 .w,s 1 .t,b 2 are extracted and mapped into their corresponding embeddings. Then these embeddings are concate-nated as the input vector x  X  R  X  d . A special token NULL is used to represent a non-existent element.
We perform a standard neural network using one hidden layer with d h hidden units followed by a softmax layer as: b 2  X  R |T| .Here, g is a non-linear function which can be hyperbolic tangent, sigmoid, cube (Chen and Manning, 2014), etc. Recursive neural network (RNN) is one of classi-cal neural networks, which performs the same set of parameters recursively on a given structure (e.g. syntactic tree) in topological order (Pollack, 1990; Socher et al., 2013).

In the simplest case, children nodes are com-bined into their parent node using a weight matrix W which is shared across the whole network, fol-lowed by a non-linear function g (  X  ) . Specifically, given the left child node vector h L  X  R d and right child node vector h R  X  R d , their parent node vec-tor h P  X  R d will be formalized as: as mentioned above. In this paper, we apply the idea of recursive neu-ral network (RNN) to dependency parsing task. RNN needs a pre-defined topological structure. However, in each configuration during parsing, just partial dependency relations have been con-structed, while the remains are still unknown. Be-sides, the standard RNN can just deal with the bi-nary tree. Therefore we cannot apply the standard RNN directly.

Here, we propose two heterogeneous recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network Figure 4: Architecture of our proposed depen-dency parser using two heterogeneous gated recur-sive neural networks. (DAG-GRNN). Tree-GRNN is applied to the sub-trees with partial dependency relations in stack which have already been constructed according to the previous transition actions. DAG-GRNN is employed to model the feature composition of nodes in stack and buffer which have not been la-beled their dependency relations yet.

Figure 4 shows the whole architecture of our model, which integrates two different GRNNs to predict the action for each parsing configuration. The detailed descriptions of two GRNNs will be discussed in the following two subsections. 4.1 Tree Structured Gated Recursive Neural It is a natural way to merge the information from children nodes into their parent node recursively according to the given tree structures in stack. Al-though the dependency relations have been built, it is still hard to apply the recursive neural net-work (as Eq. 3) directly for the uncertain num-ber of children of each node in stack. By aver-aging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) in-corporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the Figure 5: Minimal structure of tree structured gated recursive neural network (Tree-GRNN). The solid arrow denotes that there is a weight matrix on the link, while the dashed one denotes none. crucial features according to the gate state. Fig-ure 5 shows the minimal structure of Tree-GRNN model.

In Tree-GRNN, each node p of trees in stack is composed of three components: state vector of left children nodes v p node v p n  X  R d n and state vector of right children nodes v p r  X  R d c , where d n and d c indicate the cor-responding vector dimensionalities. Particularly, we represent information of node p as a vector where v p  X  R q and q = 2 d c + d n . Specifically, v p n contains the information of current node including its word form p .w , pos tag p .t and label type p .l as shown in Eq. 5, and v p by zero vectors 0  X  R d c , then update as Eq. 6. and E l according to the indices of the correspond-ing word p .w , pos p .t and label p .l respectively. Specifically, in the case of unlabeled attachment Thus, the dimensionality d n of v p n varies. In la-beled attachment parsing case, we set a special to-ken NULL to represent label p .l if not available (e.g. p is the node in stack or buffer).

By given node p and its left children nodes p .lc i and right children nodes p .rc i , we update the left children information v p mation v p r as nodes p .lc i and p .rc i respectively as shown in Eq. 7. In addition, functions N L ( p ) and N R ( p ) re-sult the numbers of left and right children nodes of node p respectively. The operator indicates element multiplication here. W l  X  R d c  X  q and b r  X  R d c are bias terms.
 ized as where  X  indicates the sigmoid function, W o  X 
By the mechanism above, we can summarize the whole information into the stack recursively from children nodes to their parent using the partial-built tree structure. Intuitively, the gate mechanism can selectively choose the crucial fea-tures of a child node according to the gate state which is derived from the current child node and its parent. 4.2 Directed Acyclic Graph Structured Previous neural based parsing works feed the ex-tracted features into a standard neural network with one hidden layer. Then, the hidden units are fed into a softmax layer, outputting the probability vector of available actions. Actually, it cannot well model the complicated combinations of extracted features. As for the nodes, whose dependency relations are still unknown, we propose another recursive neural network namely directed acyclic graph structured gated recursive neural network (DAG-GRNN) to better model the interactions of features.

Intuitively, the DAG structure models the com-binations of features by recursively mixing the in-formation from the bottom layer to the top layer Figure 6: Minimal structure of directed acyclic graph structured gated recursive neural network (DAG-GRNN). The solid arrow denotes that there is a weight matrix on the link, while the dashed one denotes none. as shown in Figure 4. The concatenation opera-tion can be regraded as a mix of features in differ-ent levels of granularity. Each node in the directed acyclic graph can be seen as a complicated feature composition of its governed nodes.

Moreover, we also use the gate mechanism to better model the feature combinations by introduc-ing two kinds of gates, namely  X  X eset gate X  and  X  X pdate gate X . Intuitively, each node in the net-work seems to preserve all the information of its governed notes without gates, and the gate mech-anism similarly plays a role of filter which de-cides how to selectively exploit the information of its children nodes, discovering and preserving the crucial features.

DAG-GRNN structure consists of minimal structures as shown in Figure 6. Vectors h P , h L , h
R and h  X  P  X  R q denote the value of the parent node P , left child node L , right child node R and new activation node  X  P respectively. The value of parent node h P  X  R q is computed as: where z  X  new activation node  X  P , left child node L and right child node R respectively. Operator indicates element-wise multiplication.
The update gates z can be formalized as: which are constrained by: gates.
 The value of new activation node h  X  puted as: where W  X  and r R are the reset gates for left child node L and right child node R respectively, which can be formalized as: gates and  X  indicates the sigmoid function.

Intuitively, the reset gates r partially read the information from the left and right children, out-putting a new activation node h  X  date gates z selectively choosing the information among the the new activation node  X  P , the left child node L and the right child node R . This gate mechanism is effective to model the combinations of features.
 Finally, we concatenate all the nodes in the DAG-GRNN structure as input x of the architec-ture described in Section 2.2, resulting the proba-bility vector for all available actions. 4.3 Inference We use greedy decoding in parsing. At each step, we apply our two GRNNs on the current config-uration to extract the features. After softmax op-eration, we choose the feasible transition with the highest possibility, and perform the chosen tran-sition on the current configuration to get the next configuration state.
 In practice, we do not need calculate the Tree-GRNN over the all trees in the stack on the current configuration. Instead, we preserve the represen-tations of trees in the stack. When we need apply a new transition on the configuration, we update the relative representations using Tree-GRNN. We use the maximum likelihood (ML) criterion to train our model. By extracting training set ( x i ,y i ) from gold parse trees using a shortest stack oracle which always prefers Left-Arc ( l ) or Right-Arc ( l ) action over Shift, the goal of our model is to mini-mize the loss function with the parameter set  X  :
J (  X  ) =  X  where m is number of extracted training examples which is as same as the number of all configura-tions.

Following Socher et al. (2013), we use the diag-onal variant of AdaGrad (Duchi et al., 2011) with minibatch strategy to minimize the objective. We also employ dropout strategy to avoid overfitting.
In practice, we perform DAG-GRNN with two hidden layers, which gets the best perfor-mance. We use the approximated gradient for Tree-GRNN, which only performs gradient back propagation on the first two layers. 6.1 Datasets To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.  X  English For English Penn Treebank 3  X  Chinese For Chinese Penn Treebank 5 6.2 Experimental Settings For parameter initialization, we use random ini-tialization within (-0.01, 0.01) for all parameters except the word embedding matrix E w . Specifi-cally, we adopt pre-trained English word embed-dings from (Collobert et al., 2011). And we pre-train the Chinese word embeddings on a huge un-labeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a).

Table 1 gives the details of hyper-parameter set-tings of our approach. In addition, we set mini-batch size to 20. In all experiments, we only take s , s 2 , s 3 nodes in stack and b 1 , b 2 , b 3 nodes in buffer into account. We also apply dropout strat-egy here, and only dropout at the nodes in stack and buffer with probability p = 20% . 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Con-ventionally, punctuations are excluded in all eval-uation metrics.

To evaluate the effectiveness of our approach, we compare our parsers with feature-based parser and neural-based parser. For feature-based parser, we compare our models with two prevalent parsers: MaltParser (Nivre et al., 2006) and MSTParser (McDonald and Pereira, 2006). For neural-based parser, we compare our results with parser of Chen and Manning (2014). Compared with parser of Chen and Manning (2014), our parser with two heterogeneous gated recursive neural networks (Tree-GRNN+DAG-GRNN) re-ceives 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, and receives 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.

Since that speed of algorithm is not the focus of our paper, we do not optimize the speed a lot. On CTB (UAS), it takes about 2 days to train Tree-GRNN+DAG-GRNN model with CPU only. The testing speed is about 2.7 sentences per second. All implementation is based on Python. 6.4 Effects of Gate Mechanisms We adopt five different models: plain parser, Tree-RNN parser, Tree-GRNN parser, Tree-RNN+DAG-GRNN parser, and Tree-GRNN+DAG-GRNN parser. The experiment results show the effectiveness of our proposed two heterogeneous gated recursive neural networks.
Specifically, plain parser is as same as parser of Chen and Manning (2014). The difference between them is that plain parser only takes the nodes in stack and buffer into account, which uses a simpler feature template than parser of Chen and Manning (2014). As plain parser omits all children nodes of trees in stack, it performs poorly compared with parser of Chen and Manning (2014). In addition, we find plain parser outperforms MaltParser (standard) on PTB3 dataset making about 1% progress, while it performs poorer than MaltParser (standard) on CTB5 dataset. It shows that the children nodes of trees in stack is of great importance, especially for Chinese. Moreover, it also shows the effective-ness of neural network based model which could represent complicated features as compacted em-beddings. Tree-RNN parser additionally exploits all the children nodes of trees in stack, which is a simplified version of Tree-GRNN without incor-porating the gate mechanism described in Section 4.1. In anther word, Tree-RNN omits the gate the results are significantly boosted by utilizing the all information in stack, which again shows the importance of children nodes of trees in stack. Although the results of Tree-RNN are compara-ble to results of Chen and Manning (2014), it not outperforms parser of Chen and Manning (2014) in all cases (e.g. UAS on CTB5), which implies that exploiting all information without selection might lead to incorporate noise features. More-over, Tree-GRNN parser further boosts the perfor-mance by incorporating the gate mechanism. In-tuitively, Tree-RNN who exploits all the informa-tion of stack without selection cannot well capture the crucial features, while Tree-GRNN with gate mechanism could selectively choose and preserve the effective features by adapting the current gate state.

We also experiment on parsers using two heterogeneous gated recursive neural networks: Tree-RNN+DAG-GRNN parser and Tree-GRNN+DAG-GRNN parser. The similarity of Table 2: Performance of different models on PTB3 dataset. UAS: unlabeled attachment score. LAS: labeled attachment score. Table 3: Performance of different models on CTB5 dataset. UAS: unlabeled attachment score. LAS: labeled attachment score. two parsers is that they all employ the DAG structured recursive neural network with gate mechanism to model the combination of features extracted from stack and buffer. The difference between them is the former one employs the Tree-RNN without gate mechanism to model the features of stack, while the later one employs the gated version (Tree-GRNN). Again, the perfor-mance of these two parsers is further boosted, which shows DAG-GRNN can well model the combinations of features which is summarized by Tree-(G)RNN structure. In addition, we find the performance does not drop a lot in almost cases by turning off the gate mechanism of Tree-GRNN, which implies that the DAG-GRNN can help selecting the information from trees in stack, even it has not been selected by gate mechanism of Tree-GRNN yet. 6.5 Convergency Speed To further analyze the convergency speed of our approach, we compare the UAS results on devel-opment sets of two datasets for first ten epoches as shown in Figure 7 and 8. As plain parser only take the nodes in stack and buffer into ac-UAS(%) Figure 7: Performance of different models on PTB3 development set. UAS: unlabeled attach-ment score. UAS(%) Figure 8: Performance of different models on CTB5 development set. UAS: unlabeled attach-ment score. count, the performance is much poorer than the rest parsers. Moreover, Tree-GRNN converges slower than Tree-RNN, which shows that it might be more difficult to learn this gate mechanism. By introducing the DAG-GRNN, both Tree-RNN and Tree-GRNN parsers become faster to converge, which shows that the DAG-GRNN is of great help in boosting the convergency speed. Many neural network based methods have been used for transition based dependency parsing.
Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features.

Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the stan-dard RNN and used the binary combination to model the representation of two linked words. But his model does not achieve the performance of the traditional method.

Le and Zuidema (2014) proposed a genera-tive re-ranking model with Inside-Outside Recur-sive Neural Network (IORNN), which can pro-cess trees both bottom-up and top-down. How-ever, IORNN works in generative way and just es-timates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k -best candidate results. Besides, IORNN treats depen-dency tree as a sequence, which can be regarded as a generalization of simple recurrent neural net-work (SRNN) (Elman, 1990).

Although the two methods also used RNN, they just deal with the binary combination, which is un-natural for dependency tree.

Zhu et al. (2015) proposed a recursive convolu-tional neural network (RCNN) architecture to cap-ture syntactic and compositional-semantic repre-sentations of phrases and words in a dependency tree. Different with the original recursive neu-ral network, they introduced the convolution and pooling layers, which can model a variety of com-positions by the feature maps and choose the most informative compositions by the pooling layers.
Chen and Manning (2014) improved the transition-based dependency parsing by represent-ing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neu-ral network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action.
Compared with (Chen and Manning, 2014), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the com-plicated combination of all the features, while the traditional discrete feature based methods need manually design them.

Dyer et al. (2015) improved the transition-based dependency parsing using stack long short term memory neural network and received significant improvement on performance. They focused on exploiting the long distance dependencies and in-formation, while we aims to automatically model the complicated feature combination. In this paper, we pay attention to the syntac-tic and semantic composition of the dense fea-tures for transition-based dependency parsing. We propose two heterogeneous gated recursive neu-ral networks, Tree-GRNN and DAG-GRNN. Each hidden neuron in two proposed GRNNs can be re-garded as a different combination of the input fea-tures. Thus, the whole model has an ability to sim-ulate the design of the sophisticated feature com-binations in the traditional discrete feature based methods.

Although the two proposed GRNNs are only used for the greedy parsing based on arc-standard transition system in this paper, it is easy to gen-eralize them to other transition systems and graph based parsing. In future work, we would also like to extend our GRNNs for the other NLP tasks. We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), Na-tional High Technology Research and Develop-ment Program of China (2015AA015408), Shang-hai Science and Technology Development Funds (14ZR1403200).

