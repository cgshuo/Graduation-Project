 1. Introduction
M EDLINE is a well-known premier bibliographic collection that contains references to articles contained in journals on life sciences. The Genomics TREC 2004 evaluation campaign provides access to one third of this is to evaluate the retrieval performance of various IR models, including recent developments in probabilistic and language models, and also vector-space schemes.

Second, we accept the fact that manually assigned descriptors should increase the probability of retrieving
Manual indexing, usually based on controlled vocabularies, should prescribe a uniform and invariable choice ence X  X ,  X  X  X nformatics X  X ).

A third issue concerns information submitted in queries to express user needs. As is commonly recognized, users do not supply all details and thus there is a lack of certain synonyms or related terms. To partially resolve this problem, a query expansion technique should take different term-term relationships into account performance.

The rest of this paper is organized as follows. Section 2 describes related works in the two different sub-domains presented in this paper: manual and automatic indexing, and automatic query expansion approaches. applied during our experiments. Section 5 explains our new query expansion model, and Section 6 evaluates the performance of various IR models, in addition to two query expansion approaches. The main findings of this paper are presented in Section 7 . 2. Related work 2.1. Manual &amp; automatic indexing
Only a few studies have undertaken to directly compare the performance of manual vs. automatic indexing methods. The well-known Cranfield experiments for example studied and evaluated the retrieval impact of various manual-indexing strategies. For example, Cleverdon (1967) reported that single-word indexing was more effective than extracted terms from a controlled vocabulary, where both indexing schemes were compiled by human beings (1400 documents, 221 queries).

In order to evaluate the importance of manually assigned descriptors, Hersh, Buckley, Leone, and Hickam grounds (physicians or librarians, novices or expert users) when searching MeSH descriptors were not really advantageous. In an opposing viewpoint, Srinivasan (1996) reported that
MeSH may in some cases help retrieving information in M EDLINE
Based on the Amaryllis database, containing a French bibliographic collection (148,688 records and 25 que-ries), Savoy (2005) demonstrated that the inclusion of manually assigned descriptors could significantly enhance mean average precision by about 35% based on title-only queries, compared to an approach that ignored these additional descriptors. The question then arises:  X  X  X oes the inclusion of MeSH headings improve mean average precision within the M EDLINE corpus? X  X  Then, if the answer is positive:  X  X  X hat percentage improvement could we expect when such manually assigned descriptors are taken into account? X  X  2.2. Query expansion
To provide a better match between user information needs and documents, various query expansion tech-niques have been suggested. The general principle is to expand the query using words or phrases having mean-ings similar to or related to those appearing in the original request. To achieve this, query expansion approaches consider various relationships between these words, as well as term selection mechanisms and term sion approaches ( Efthimiadis, 1996 ).
 through displaying a ranked list of retrieved items returned by the first query. Using the WordNet thesaurus,
Voorhees (1994) demonstrated that terms having a lexical-semantic relation with original query words (extracted because of synonym relationship) provided very little improvement (around 1% compared to the original query).
As a second strategy for expanding the original query, Rocchio (1971) proposed taking the relevance or would then be built automatically in the form of a linear combination of the term included in the previous query and terms automatically extracted from both relevant (with a positive weight) and non-relevant docu-an approach is usually quite effective. Moreover, Buckley, Singhal, Mitra, and Salton (1996) suggested that even without looking at them or asking the user, it could be assumed that the top k ranked documents would be relevant. This method, denoted the pseudo-relevance feedback or blind-query expansion approach, is usually effective (at least when handling relatively large text collections).

As a third source, we might use large text corpora to derive various term-term relationships and apply sta-tistically or information-based measures. For example, Qiu and Frei (1993) suggested that terms extracted from a similarity thesaurus that had been automatically built through calculating co-occurrence frequencies in the search collection could be added to a new query. The underlying effect was to add idiosyncratic terms to those found in underlying document collections, and related to query terms in accordance to the language being used. Kwok, Grunfield, Sun, and Deng (2004) suggested building an improved request by using the web to find terms related to search keywords.

In these various query expansion approaches, different underlying parameters must be specified and gener-ally there is no single theory capable of finding the most appropriate values. Moreover, previous empirical studies conducted with newspaper corpora and more specific collections such as M other retrieval impacts that might result from the application of blind-query expansion methods. Answers to these questions can be found in the remaining sections. 3. Test collection The corpus used in our experiments was extracted from the M around 10 years of scientific publication (4,591,008 records, and around 10.6 GB of compressed data) and rep-resents one third of the entire M EDLINE collection (approximately 13 million references). As shown in Fig. 1 , each record is structured according to a specific set of fields, such as unique identifier), DP (publication date), AU (author), PT (publication type), SO (source), etc. From an
IR perspective the most important sources of information include the article X  X  title (TI), abstract (AB) and set of MeSH headings (MH) extracted from the MeSH 2 Thesaurus.

This test-collection derived from the TREC 2004 evaluation campaign contains fifty topics (see examples four different fields, namely a unique identifier (  X  ID  X  ), a brief title (  X  information need (  X  NEED  X  or N), and some background information to help assess the topic (  X 
We used the three logical sections TNC to build the queries, in which the number of search terms is 13.6 in average (median: 12, standard deviation: 6.05).

The relevance judgments made by two human assessors could be rated as  X  X  X efinitively relevant, X  X   X  X  X ossibly relevant X  X  or  X  X  X ot relevant. X  X  According to the Genomics relevance assessments, the average number of relevant records per query was 165.36 (median: 115.5; standard deviation: 166.8). Query #18 or #19 had only one pertinent document while Query #42 had the greatest num-ber of relevant articles (679). 4. IR models
In order to obtain a broader view of the relative merit of the various retrieval models, we used seven dif-ferent vector-space schemes and three probabilistic models. First, we adopted the classical tf idf model, wherein the weight attached to each indexing term was the product of its term occurrence frequency (or tf indexing term t j in document D i and its inverse document frequency or idf documents and requests, we computed the inner product after normalizing indexing weights (model denoted  X  X  X ocument = ntc, query = ntc X  X  or  X  X  X tc-ntc X  X ).

Other variants might also be created, especially in cases when the occurrence of a particular term in a doc-ument is deemed a rare event. Thus, it might be good practice to assign more importance to the first occur-rence of this word, compared to any successive, repeating occurrences. Therefore, the tf component might be could also be used for documents and requests, leading to different weighting combinations (see the Appen-dix). We might also consider that a term X  X  presence in a shorter document would provide stronger evidence than in a longer document, leading to more complex IR models; for example the IR model denoted by  X  X  X oc = Lnu X  X  ( Buckley et al., 1996 ),  X  X  X oc = dtu X  X  ( Singhal, Choi, Hindle, Lewis, &amp; Pereira, 1999 ).
In addition to these vector-space schemes, we also considered probabilistic models such as that of Okapi ( Robertson, Walker, &amp; Beaulieu, 2000 ). As a second probabilistic approach, we implemented the I( n )B2 approach taken from the divergence from randomness (DFR) framework ( Amati &amp; van Rijsbergen, 2002 ), based on combining the two information measures formulated below: in which Prob 1 ij is the pure chance probability of finding tf the other hand, Prob 2 ij is the probability of encountering a new occurrence of term t that we have already found tf ij occurrences of this term. Within this framework, the model I( n )B2 is based on the following formulae: where df j indicates the number of documents indexed with the term t in the collection, n the number of documents in the corpus, mean dl (= 146) the average document length, and c a constant (fixed at 1.5).

Finally, we also considered an approach based on a language model (LM) ( Hiemstra, 2000 ), known as a non-parametric probabilistic model (while the Okapi and I( n )B2 are parametric models). Thus the estimations needed would not be based on any known term distribution but rather directly from its occurrence frequencies in the document D or the corpus C . From the language model paradigm, we might consider various implemen-tations and smoothing methods. In this study, we adopted a model proposed by Hiemstra (2000, 2002) ,as described in Eq. (3) , which combines an estimation based on the document (Prob  X  t (Prob  X  t j j C  X  in which k j is a smoothing factor (fixed at 0.35 for all term t number of occurrences of each of the t terms included in the inverted file. 5. Blind-query expansion
Various general query expansion approaches have been suggested and in this paper we compared ours with that of Buckley et al. (1996), Rocchio (1971) . In this latter case, the system was allowed to add m terms extracted from the k best-ranked documents from the original query. Each new query was derived by applying the following formula: query (denoted by w j ), and w ij the indexing term weight attached to this j th term in the document D in the top k ranks. In our evaluation, we fixed a  X  2 : 0, b  X  0 : 75.

To define our new blind-query expansion denoted  X  X  X DF Query Expansion X  X  (or simply IDFQE), we adopted the following procedure. First we formed the root set of search terms composed of all terms included in the original query Q and all indexing terms appearing in the k best ranked documents. The weight attached to each term in this root set was computed as follows: where for term t j , idf j  X  log  X  n = df j  X  (or the classical idf value) and I returning the value 1 if the term t j belongs to the query Q (or the document D scheme, if a term appears only in the original query Q , its weight would be a tf only one document would have a weight of  X  b = k  X  idf j .

The root set elements were then sorted in descending order according to their weight. To form the new being the same as those used in the root set. We thus used the same weighting scheme to select and weight the new search terms. 6. Evaluation
To evaluate our various IR schemes, we adopted the mean average precision (MAP) to measure retrieval performance (based on 1000 records). To statistically determine whether or not a given search tests we applied, the null hypothesis H 0 states that both retrieval schemes produce similar MAP performance.
Such a null hypothesis would be accepted if two retrieval schemes returned statistically a similar MAP, otherwise it would be rejected. Thus, in the tables included in this paper, we have underlined any statisti-cally significant differences resulting from a two-sided non-parametric bootstrap test (significance level a  X  5 %  X  . 6.1. IR models evaluation
Table 1 depicts the MAP for the M EDLINE collection subset obtained using the Porter (1980) stemmer with different IR models. In this evaluation we used the documents X  title and abstract together with their MeSH descriptors. This table depicts the best performances under a given condition bold, and these values are then column the evaluation of the corresponding models.

The evaluations shown in Table 1 indicate that the I( n )B2 probabilistic model produces the best retrieval performance (baseline). The differences between the I( n )B2 on the one hand, and the language model (LM) or Okapi models on the other are not statistically significant. With the other seven vector-space IR models the MAP differences are always statistically significant, showing an improvement of around 170% over the classical tf idf IR model (denoted  X  X  X tc-ntc X  X  in Table 1 ). 6.2. Manually assigned headings
In Table 2 , we listed the mean average precision achieved when the best performing IR models used only the article X  X  title and abstract to build the document surrogate. Overall, retrieval performance under this indexing restriction was lower than the corresponding system using manually assigned descriptors (shown under the label  X  X  X ith MeSH X  X ). Average increases were around 8.5% when the search system included the
MeSH headings. The greatest enhancement however was obtained with the Okapi model (from 0.3217 to 0.3573, an absolute improvement of 0.035, or a relative increase of 11.1%). When comparing the MAP before forming IR systems (the first four IR models depicted in Table 2 ). 6.3. Evaluating query modification and expansion
To evaluate both our new blind query expansion method and the Rocchio scheme, we used the best per-forming model, namely I( n )B2 as shown in Table 3 . In the bottom part of this table, the rows starting with  X  X  k doc/ m terms X  X  indicate the number of top-ranked documents and the number of terms used to enlarge the original query. We then compared the MAP achieved by the two query expansion approaches using either the Rocchio or IDFQE strategy.

The data in Table 3 indicates that the Rocchio query expansion approach provided lower MAP than did the baseline performance (0.3810, MAP before query expansion), and that these differences are always statis-tically significant. Peat and Willett (1991) provide one explanation for the poor performance shown by the
Rocchio approach. In their study they show that query terms have a greater occurrence frequency than other terms. Second, query expansion approaches based on term co-occurrence data will include additional terms that also have a greater occurrence frequency in the documents. In such cases, these additional search terms the final effect on retrieval performance could be negative.

On the other hand, our suggested idf -based query expansion (IDFQE) tends to produce better MAP than the baseline and offers clearly better retrieval performance than the Rocchio scheme. When comparing our
IDFQE scheme with the baseline (0.3810), the performance differences are usually not significant. However, when comparing Rocchio with our IDFQE query expansion model, the performance differences were statis-tically significant, and always in favor of the IDFQE. 7. Conclusion
Experiments conducted on a large subset of the M EDLINE collection show that the best mean average pre-cision is obtained using the I( n )B2 probabilistic model (see Table 1 ). Moreover, when compared to various vector-space models, the performance differences are always statistically significant and in favor of the
I( n )B2 model. When compared to the Okapi or a language model however, the performance differences are not statistically significant.

Including the Medical Subject Headings (MeSH) when indexing scientific articles improves retrieval perfor-mance by between 2.4% and 13.5%, depending on the underlying IR model (see Table 2 ). These differences are usually statistically significant.

In this paper, we also proposed a new query expansion approach using the idf values to weight the new search terms. Depending on the parameter setting, the resulting improvement achieved by this new query expansion varied, but the retrieval performances for this new blind query expansion are statistically better than the MAP obtained by the Rocchio scheme.

Although the focus of this study was on retrieval effectiveness for meta-information sources, the effective-ness of manually assigned descriptors could of course be studied in a variety of other perspectives. One pos-sible research objective might be to build and evaluate the most effective means of automatically assigning
MeSH terms or generally controlled vocabulary system in automatic indexing is however questionable in some the same diagnosis in a given thesaurus). On the other hand, such automatic assignment could also be based content with the citing document (e.g., using a bibliographic coupling measure).

Moreover, given that MeSH is a hierarchical thesaurus, users looking for certain information might use or more general concepts. Moreover, such query expansion could be done automatically based on relation-concepts often fall into more than one class and finding related terms may be no more trivial when applying the  X  X  X ung Disease, X  X   X  X  X ancer, X  X   X  X  X umor, X  X  or  X  X  X ulmonary Neoplasms X  X  class. As is evidenced here there are several avenues for further research on the general theme of controlled vocabularies and on more general knowledge-based systems working in specific domains.
 Acknowledgements
This research was supported in part by the Swiss NSF under Grants #200020-103420 and #200021-113273.
 Appendix A. Term weighting formulae
In Table 4 , n indicates the number of documents in the collection, t the number of indexing terms, df j the number of documents in which the term t j appears, for D terms) is denoted by nt i , and avdl , b , k 1 , pivot and slope are constants (fixed at b  X  0 : 55, k 146  X  . References
