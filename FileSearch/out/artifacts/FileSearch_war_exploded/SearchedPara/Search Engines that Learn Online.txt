 The goal of my research is to develop self-learning search engines, that can learn online , i.e., directly from interactions with actual users. Such systems can continuously adapt to user preferences throughout their lifetime, leading to better search performance in settings where expensive manual tuning is infeasible. Challenges that are addressed in my work include the development of effec-tive online learning to rank algorithms for IR, user aspects , and evaluation .
 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Algorithms, Experimentation Online learning to rank, Implicit feedback, Evaluation
As search engines are becoming increasingly complex, and com-bine more and more sources of information, the question of how to tune these systems to optimize search results becomes essen-tial. Current methods for optimizing ranking functions typically work offline , meaning that systems are tuned to a given data set be-fore deployment. While offline methods are appropriate for many settings and can result in excellent search performance, there are limitations. Data needed for tuning is often created manually, and therefore expensive and error-prone.

We address the limitations of offline methods by working to-wards self-learning search engines, i.e., search engines that learn ranking functions online . Learning online means that there is no distinction between training and operating. Instead, the search en-gine observes users X  natural interactions with the search interface (so-called implicit feedback , e.g., clicks on result documents), in-fers information from them, and improves its ranking function au-tomatically. Because implicit feedback is used for learning, expen-sive data collection is not required, and the collected data matches the target users and the target setting.
Several steps have been taken to make online learning to rank for IR a reality, but many open challenges remain. My thesis work addresses the following questions.
 RQ 1 How can retrieval systems learn effectively from implicit Online learning to rank for IR differs from other learning to rank tasks, because of the characteristics of the implicit feedback avail-able for learning. While correlated with relevance, it varies widely across tasks, users, and queries. As a result, only noisy, relative preferences can be inferred. Furthermore, the feedback that can be obtained is biased towards the top-ranked documents displayed.
Our first results to address these issues focused on the problem of balancing exploration and exploitation , meaning that result lists to be displayed to the user need to both satisfy the user X  X  informa-tion need as much as possible (i.e., exploit what has already been learned), while also allowing to collect feedback that can provide as much new information as possible (i.e., explore new solutions). We found that such a balance can significantly improve the perfor-mance of online learning to rank systems [1].
 RQ 2 What user aspects need to be considered? Previous work in online learning to rank for IR has been grounded in user studies, but many open questions remain. Here, we in-vestigate the relation between explicit and implicit feedback, and develop methods for inferring information from implicit feedback more reliably. Our first results include a new, probabilistic method for comparing rankers using click data, that is more accurate and more robust to noise than previous methods [2].
 RQ 3 How can online learning to rank methods be evaluated ? Online learning to rank approaches for IR can be evaluated using live search engine traffic, lab studies, or log data. Each of these has advantages and limitations. Moving forward, we need to ana-lyze the benefits and limitations of existing evaluation methods for assessing online learning to rank approaches, and to develop new approaches to address any identified limitations.

Some online learning to rank approaches have been evaluated on life traffic of large-scale web search engines. However, such a setup is not always accessible or feasible, especially when poten-tially risky new methods need to be evaluated, or when many com-parisons under a wide range of settings are required. As an alterna-tive to real-life experiments, we developed an evaluation setup that simulates user behavior at a sufficient level of detail, using learning to rank data sets and a model of click behavior [1]. In the future, we need to continue to assess and refine this methodology. [1] K. Hofmann, S. Whiteson, and M. de Rijke. Balancing exploration [2] K. Hofmann, S. Whiteson, and M. de Rijke. A probabilistic
