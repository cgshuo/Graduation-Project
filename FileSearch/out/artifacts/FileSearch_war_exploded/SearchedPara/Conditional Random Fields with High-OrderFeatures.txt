 Department of Computer Science
National University of Singapore In a sequence labeling problem, we are given an input sequence x and need to label each component of x with its class to produce a label sequence y . Examples of sequence labeling problems include labeling words in sentences with its type in named-entity recognition problems [16], handwriting recognition problems [15], and deciding whether each DNA base in a DNA sequence is part of a gene in gene prediction problems [2].
 Conditional random fields (CRF) [8] has been successfully applied in many sequence labeling prob-on x as the learning cost mainly depends on the parts of y involved in the dependencies. However, the use of high-order features, where a feature of order k is a feature that encodes the dependency between x and ( k + 1) consecutive elements in y , can potentially lead to an exponential blowup in the computational complexity of inference. Hence, dependencies are usually assumed to exist only between adjacent components of y , giving rise to linear-chain CRFs which limits the order of the features to one.
 In this paper, we show that it is possible to learn and predict CRFs with high-order features effi-ciently under the following pattern sparsity assumption (which is often observed in real problems): the number of observed label sequences of length, say k , that the features depend on, is much smaller than n k , where n is the number of possible labels. We give an algorithm for computing the marginals and the CRF log likelihood gradient that runs in time polynomial in the number and length of the label sequences that the features depend on. The gradient can be used with quasi-newton methods to efficiently solve the convex log likelihood optimization problem [14]. We also provide an effi-cient decoding algorithm for finding the most probable label sequence in the presence of long label sequence features. This can be used with cutting plane methods to train max-margin solutions for sequence labeling problems in polynomial time [18].
 We show experimentally that using high-order features can improve performance in sequence la-beling problems. We show that in handwriting recognition, using even simple high-order indicator features improves performance over using linear-chain CRFs, and impressive performance improve-ment is observed when the maximum order of the indicator features is increased. We also use a synthetic data set to discuss the conditions under which higher order features can be helpful. We further show that higher order label features can sometimes be more stable under change of data distribution using a named entity data set. Conditional random fields [8] are discriminately trained, undirected Markov models, which has been shown to perform well in various sequence labeling problems. Although a CRF can be used to capture arbitrary dependencies among components of x and y , in practice, this flexibility of the CRF is not fully exploited as inference in Markov models is NP-hard in general (see e.g. [1]), and can only be performed efficiently for special cases such as linear chains. As such, most applications involving CRFs are limited to some tractable Markov models. This observation also applies to other structured prediction methods such as structured support vector machines [15, 18].
 A commonly used inference algorithm for CRF is the clique tree algorithm [5]. Defining a feature depending on k (not necessarily consecutive) labels will require forming a clique of size k , resulting in a clique-tree with tree-width greater or equal to k . Inference on such a clique tree will be exponen-tial in k . For sequence models, a feature of order k can be incorporated into a k -order Markov chain, but the complexity of inference is again exponential in k . Under the pattern sparsity assumption, our algorithm achieves efficiency by maintaining only information related to a few occurred patterns, while previous algorithms maintain information about all (exponentially many) possible patterns. In the special case of a semi-Markov random fields, where high-order features depend on segments of identical labels, the complexity of inference is linear in the maximum length of the segments [13]. The semi-Markov assumption can be seen as defining a sparse feature representation: though the number of length k label patterns is exponential in k , the semi-Markov assumption effectively of identical labels that can only depend on the label of the preceding segment. Compared to this approach, our algorithm has the advantage of being able to efficiently handle high-order features having arbitrary label patterns.
 Long distance dependencies can also be captured using hierarchical models such as Hierarchical Hidden Markov Model (HHMM) [4] or Probabilistic Context Free Grammar (PCFG) [6]. The time complexity of inference in an HHMM is O (min { nl 3 ,n 2 l } ) [4, 10], where n is the number of states been studied [17]. Inference in PCFG and its discriminative version can also be efficiently done in O ( ml 3 ) where m is the number of productions in the grammar [6]. These methods are able to capture dependencies of arbitrary lengths, unlike k -order Markov chains. However, to do efficient learning with these methods, the hierarchical structure of the examples need to be provided. For example, if we use PCFG to do named entity recognition, we need to provide the parse trees for efficient learning; providing the named entity labels for each word is not sufficient. Hence, a training set that has not been labeled with hierarchical labels will need to be relabeled before it can be trained efficiently. Alternatively, methods that employ hidden variables can be used (e.g. to infer the hidden parse tree) but the optimization problem is no longer convex and local optima can sometimes be a problem. Using high-order features captures less expressive form of dependencies than these models but allows efficient learning without relabeling the training set with hierarchical labels. Similar work on using higher order features for CRFs was independently done in [11]. Their work apply to a larger class of CRFs, including those requiring exponential time for inference, and they did not identify subclasses for which inference is guaranteed to be efficient. Throughout the remainder of this paper, x , y , z (with or without decorations) respectively denote an observation sequence of length T , a label sequence of length T , and an arbitrary label sequence. The function |  X  | denotes the length of any sequence. The set of labels is Y = { 1 ,...,n } . If by ). Let the features being considered be f 1 ,...,f m . Each feature f i is associated with a label sequence z i , called f i  X  X  label pattern , and f i has the form The observations x = ( x 1 ,...,x T ) may be a word sequence; g i ( x ,t ) may be an indicator function for whether x t is capitalized or may output a precomputed term weight if x t matches a particular word; and z i may be a sequence of two labels, such as ( person , organization ) for the named entity recognition task, giving a feature of order one.
 of f ( x ) over all elements of x satisfying the predicate Pred ( x ) . 3.1 Inference for High-order CRF In this section, we describe the algorithms for computing the partition function, the marginals and the most likely label sequence for high-order CRFs. We give rough polynomial time complexity bounds to give an idea of the effectiveness of the algorithms. These bounds are pessimistic compared to practical performance of the algorithms. It can also be verified that the algorithms for linear chain CRF [8] are special cases of our algorithms when only zero-th and first order features are considered. We show a work example illustrating the computations in the supplementary material. 3.1.1 Basic Notations As in the case of hidden Markov models (HMM) [12], our algorithm uses a forward and backward pass. First, we describe the equivalent of states used in the forward and backward computation. We shall work with three sets: the pattern set Z , the forward-state set P and the backward-state set S . The pattern set, Z , is the set of distinct label patterns used in the m features. For notational The transitions between states are based on the prefix and suffix relationships defined below. Let z the longest prefix and suffix relations with respect to the sets P and S as follows z 1  X  p S z 2 if and only if ( z 1  X  X  ) and ( z 1  X  p z 2 ) and (  X  z  X  X  , z  X  p z 2  X  z  X  p z 1 ) z 1  X  s P z 2 if and only if ( z 1  X  X  ) and ( z 1  X  s z 2 ) and (  X  z  X  X  , z  X  s z 2  X  z  X  s z 1 ) . Finally, the subsequence relationship defined below are used when combining forward and backward variables to compute marginals. Let z  X  z 0 denote that z is a subsequence of z 0 , z  X  z 0 denote that z  X  s z 0 1: j is satisfied as well (that is, z ends at position j in z 0 ).
 We shall give rough time bounds in terms of m (the total number of features), n (the number of labels), T (the length of the sequence), M (the number of distinct label patterns in Z ), and the maximum order K = max {| z 1 | X  1 ,..., | z M | X  1 } . 3.1.2 The Forward and Backward Variables We now define forward vector  X  x and backward vector  X  x . Suppose z  X  p y , then define y  X  X  prefix Z x ( z ) = exp( P The variable  X  x ( t, p i ) computes for x 1: t the sum of the scores of all its label sequences z having p  X  ( t,  X  ) has dimension |S| . We shall compute the  X  x and  X  x vectors with dynamic programming. of p . Let p i y be the concatenation of p i with a label y . The following proposition is immediate. Proposition 1 (a) For any z , there is a unique p i such that p i  X  s P z . Proposition 1(a) means that we can induce partitions of label sequences using the forward states. and Proposition 1(b) shows how to make well-defined transition from one forward state at a time for all p i 6 = . Using Proposition 1(b), the recurrence for  X  x is definition,  X  x ( T + 1 , ) = 1 , and  X  x ( T + 1 , s i ) = 0 for all s i 6 = . The recurrence for  X  x is Once  X  x or  X  x is computed, then using Proposition 1(a), Z x can be easily obtained: Time Complexity: We assume that each evaluation of the function g i (  X  ,  X  ) can be performed in unit O ( mnMKT ) ) time. In practice, this is pessimistic, and the computation can be done more quickly. For all following analyses, we assume that  X  p x has already been computed and stored in an array. Now all values of  X  x can be computed in  X ( n |P| T ) , thus O ( nMKT ) time. Similar bounds for  X  s x and  X  x hold. 3.1.3 Computing the Most Likely Label Sequence As in the case of HMM [12], Viterbi decoding (calculating the most likely label sequence) is ob-tained by replacing the sum operator in the forward backward algorithm with the max operator. for all p i 6 = , and using Proposition 1, we have We use  X  x ( t, p k ) to record the pair ( p i ,y ) chosen to obtain  X  x ( t, p k ) , in p  X  T , and the full sequence can be traced backwards using  X  x (  X  ,  X  ) as follows Time Complexity: Either  X  p x or  X  s x can be used for decoding; hence decoding can be done in  X ( n min {|P| , |S|} T ) time. 3.1.4 Computing the Marginals z | x ) for z  X  Z  X  X  . Unlike in the traditional HMM, additional care need to be taken regarding features having label patterns that are super or sub sequences of z . We define This function computes the sum of all features that may activate strictly within z .
 O within [ p i , z , s j ] .
 Proposition 2 Let z  X  Z  X  Y . For any y with y t  X  X  z | +1: t = z , there exists unique p i , s j such Multiplying by O x counts features that are not counted in Z p x Z s x while division by W x removes features that are double-counted. By Proposition 2, we have O ( K 2 ) time (with some precomputation). Thus a very pessimistic time bound for computing 3.2 Training Given a training set T , the model parameters  X  i  X  X  can be chosen by maximizing the regularized the degree of regularization. Note that L T is a concave function of  X  1 ,..., X  m , and its maximum is achieved when Given the gradient and value of L T , we use the L-BFGS optimization method [14] for maximiz-ing the regularized log-likelihood.
 The function L T can now be computed because we have shown how to compute Z x , and computing straightforward, and E ( f i ) can be computed using marginals computed in previous section: Time Complexity: Computing the gradient is clearly more time-consuming than L T , thus we shall just consider the time needed to compute the gradient. Let X = P ( x , y )  X  X  | x | . We need to compute at most MX marginals, thus total time needed to compute all the marginals has O ( M 3 K 4 X ) as an upper bound. Given the marginals, we can compute the gradient in O ( mX ) time. If the total number of gradient computations needed in maximization is I , then the total running time in training is bounded by O (( M 3 K 4 + m ) XI ) (very pessimistic). The practical feasibility of making use of high-order features based on our algorithm lies in the observation that the pattern sparsity assumption often holds. Our algorithm can be applied to take those high-order features into consideration; high-order features now form a component that one can play with in feature engineering.
 Now, the question is whether high-order features are practically significant . We first use a synthetic data set to explore conditions under which high-order features can be expected to help. We then use a handwritten character recognition problem to demonstrate that even incorporating simple high-order features can lead to impressive performance improvement on a naturally occurring dataset. Finally, we use a named entity data set to show that for some data sets, higher order label features may be more robust to changes in data distributions than observation features. 4.1 Synthetic Data Generated Using k -Order Markov Model We randomly generate an order k Markov model with n states s 1 ,...,s n as follows. To increase pattern sparsity, we allow at most r randomly chosen possible next state given the previous k states. This limits the number of possible label sequences in each length k + 1 segment from n k +1 to n r . The conditional probabilities of these r next states is generated by randomly selecting a vector from uniform distribution over [0 , 1] r and normalizing them. Each state s i generates an observation ( a 1 ,...,a m ) such that a j follows a Gaussian distribution with mean  X  ij and standard deviation  X  . Each  X  i,j is independently randomly generated from the uniform distribution over [0 , 1] . In the experiments, we use values of n = 5 , r = 2 and m = 3 .
 The standard deviation,  X  , has an important role in determining the characteristics of the data gener-ated by this Markov model. If  X  is very small as compared to most  X  ij  X  X , then using the observations alone as features is likely to be good enough to obtain a good classifier of the states; the label cor-distinguish the states based on the observations alone and the label correlations, particularly those captured by higher order features are likely to be helpful. In short, the standard deviation,  X  , is used to to control how much information the observations reveal about the states.
 We use the current, previous and next observations, rather than just the current observation as fea-tures, exploiting the conditional probability modeling strength of CRFs. For higher order features, we simply use all indicator features that appeared in the training data up to a maximum order. We considered the case k = 2 and k = 3 , and varied  X  and the maximum order. The training set and test set each contains 500 sequences of length 20; each sequence was initialized with a random se-quence of length k and generated using the randomly generated order k Markov model. Training was done by maximizing the regularized log likelihood with regularization parameter  X  reg = 1 in all experiments in this paper. The experimental results are shown in Figure 1.
 Figure 1 shows that the high-order indicator features are useful in this case. In particular, we can see that it is beneficial to increase the order of the high-order features when the underlying model has longer distance correlations. As expected, increasing the order of the features beyond the order of the underlying model is not helpful. The results also suggests that in general, if the observations are closely coupled with the states (in the sense that different states correspond to very different observations), then feature engineering on the observations is generally enough to perform well, and Figure 2: Accuracy (left) and running time (right) as a function of maximum order for the handwrit-ing recognition data set. it is less important to use high-order features to capture label correlations. On the other hand, when such coupling is not clear, it becomes important to capture the label correlations, and high-order features can be useful. 4.2 Handwriting Recognition words with an average length of around 8 characters. The data was originally collected by Kassel [7] from around 150 human subjects. The words were segmented into characters, and each character was converted into an image of 16 by 8 binary pixels. In this labeling problem, each x i is the image of a character, and each y i is a lower-case letter. The experimental setup is the same as that used in [15]: the data set was divided into 10 folds with each fold having approximately 600 training and 5500 test examples and the zero-th order features for a character are the pixel values. For higher order features, we again used all indicator features that appeared in the training data up to a maximum order. The average accuracy over the 10 folds are shown in Figure 2, where strong improvements are observed as the maximum order increases. Figure 2 also shows the total training time and the running time per iteration of the L-BFGS algorithm (which requires computation of the gradient and value of the function at each iteration). The running time appears to grow no more than linearly with the maximum order of the features for this data set. 4.3 Named Entity Recognition with Distribution Change The Named Entity Recognition (NER) problem asks for identification of named entities from texts. With carefully engineered observation features, there does not appear to be very much to be gained from using higher order features. However, in some situations, the training data does not come from the same distribution as the test data. In such cases, we hypothesize that higher order label features may be more stable than observation features and can sometimes offer performance gain. In our experiment, we used the Automatic Content Extraction (ACE) data [9], which is labeled with seven classes: Organization , Geo-political , Location , Facility , Vehicle , and Weapon . The ACE data comes from several genres and we use the following in our experiment: Broadcast conversation (BC), Newswire (NW), Weblog (WL) and Usenet (UN).
 We use all pairs of genres as training and test data. Scoring was done with the F1 score [16].
 The features used are previous word, next word, current word, case patterns for these words, and all indicator label features of order up to k . The results for the case k = 1 and k = 2 are shown in Figure 3. Introducing second order indicator features shows improvement in 10 out of the 12 combinations and degrades performance in two of the combinations. However, the overall effect is small, with an average improvement of 0.62 in F1 score. 4.4 Discussion In our experiments, we used indicator features of all label patterns that appear in the training data. For real applications, if the pattern sparsity assumption is not satisfied, but certain patterns do not appear frequently enough and are not really important, then it is useful to see how we can select a subset of features with few distinct label patterns automatically. One possible approach would be to use boosting type methods [3] to sequentially select useful features.
 An alternate approach to feature selection is to use all possible features and maximize the margin of the solution instead. Generalization error bounds [15] show that it is possible to obtain good generalization with a relatively small training set size despite having a very large number of features if the margin is large. This indicates that feature selection may not be critical in some cases. Theo-retically, it is also interesting to note that minimizing the regularized training cost when all possible high-order features of arbitrary length are used is computationally tractable. This is because the representer theorem [19] tells us that the optimum solution for minimizing quadratically regularized cost functions lies on the span of the training examples. Hence, even when we are learning with arbitrary sets of high-order features, we only need to use the features that appear in the training set to obtain the optimal solution. Given a training set of N sequences of length l , only O ( l 2 N ) long label sequences of all orders are observed. Using cutting plane techniques [18] the computational complexity of optimization is polynomial in inverse accuracy parameter, the training set size and maximum length of the sequences.
 It should also be possible to use kernels within the approach here. On the handwritten character problem, [15] reports substantial improvement in performance with the use of kernels. Use of ker-nels together with high-order features may lead to further improvements. However, we note that the advantage of the higher order features may become less substantial as the observations become more powerful in distinguishing the classes. Whether the use of higher order features together with ker-nels brings substantial improvement in performance is likely to be problem dependent. Similarly, observation features that are more distribution invariant such as comprehensive name lists can be used for the NER task we experimented with and may reduce the improvements offered by higher order features. The pattern sparsity assumption often holds in real applications, and we give efficient inference al-gorithms for CRF with high-order features when the pattern sparsity assumption is satisfied. This allows high-order features to be explored in feature engineering for real applications. We studied the conditions that are favourable for using high-order features using a synthetic data set, and demon-strated that using simple high-order features can lead to performance improvement on a handwriting recognition problem and a named entity recognition problem.
 Acknowledgements This work is supported by DSO grant R-252-000-390-592 and AcRF grant R-252-000-327-112. [1] B. A. Cipra,  X  X he Ising model is NP-complete, X  SIAM News , vol. 33, no. 6, 2000. [2] A. Culotta, D. Kulp, and A. McCallum,  X  X ene prediction with conditional random fields, X  [3] T. G. Dietterich, A. Ashenfelter, and Y. Bulatov,  X  X raining conditional random fields via gra-[4] S. Fine, Y. Singer, and N. Tishby,  X  X he hierarchical hidden markov model: Analysis and ap-[5] C. Huang and A. Darwiche,  X  X nference in belief networks: A procedural guide, X  International [6] F. Jelinek, J. D. Lafferty, and R. L. Mercer,  X  X asic methods of probabilistic context free gram-[7] R. H. Kassel,  X  X  comparison of approaches to on-line handwritten character recognition, X  [8] J. Lafferty, A. McCallum, and F. Pereira,  X  X onditional random fields: Probabilistic models [9] Linguistic Data Consortium,  X  X CE (Automatic Content Extraction) English Annotation [10] K. P. Murphy and M. A. Paskin,  X  X inear-time inference in hierarchical HMMs, X  in Advances [11] X. Qian, X. Jiang, Q. Zhang, X. Huang, and L. Wu,  X  X parse higher order conditional random [12] L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recog-[13] S. Sarawagi and W. W. Cohen,  X  X emi-Markov conditional random fields for information ex-[14] F. Sha and F. Pereira,  X  X hallow parsing with conditional random fields, X  in Proceedings of the [15] B. Taskar, C. Guestrin, and D. Koller,  X  X ax-margin Markov networks, X  in Advances in Neural [16] E. Tjong and F. D. Meulder,  X  X ntroduction to the CoNLL-2003 shared task: Language-[17] T. T. Tran, D. Phung, H. Bui, and S. Venkatesh,  X  X ierarchical semi-Markov conditional random [18] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun,  X  X upport vector machine learning for [19] G. Wahba, Spline models for observational data , ser. CBMS-NSF Regional Conference Series
