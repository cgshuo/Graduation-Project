
Traditional recommendation systems ( RS s) aim to recommend items that are relevant to the user X  X  interest. Unfortunately, the rec-ommended items will soon become too familiar to the user and hence fail to arouse her interest. Discovery-oriented recommen-dation systems ( DORS s) complement accuracy with discover utili-ties ( DU s) such as novelty and diversity and optimize the tradeoff between the DU s and accuracy of the recommendations. Unfor-tunately, DORS s ignore an important fact that different users have different appetites for DU s. That is, highly curious users can ac-cept highly novel and diversified recommendations whereas con-servative users would behave in the opposite manner. In this pa-per, we propose a curiosity-based recommendation system ( CBRS ) framework which generates recommendations with a personalized amount of DU s to fit the user X  X  curiosity level. The major contribu-tion of this paper is a computational model of user curiosity, called Probabilistic Curiosity Model ( PCM ), which is based on the cu-riosity arousal theory and Wundt curve in psychology research. In PCM , we model a user X  X  curiosity with a curiosity distribution func-tion learnt from the user X  X  access history and compute a curiousness score for each item representing how curious the user is about the item. CBRS then selects items which are both relevant and have high curiousness score, bounded by the constraint that the amount of DU s fits the user X  X  DU appetite. We use joint optimization and co-factorization approaches to incorporate the curiosity signal into the recommendations. Extensive experiments have been performed to evaluate the performance of CBRS against the baselines using a music dataset from last.fm. The results show that compared to the baselines CBRS not only provides more personalized recommenda-tions that adapt to the user X  X  curiosity level but also improves the recommendation accuracy.
 Recommendation, curiosity, psychology, personalization
Traditional recommendation systems ( RS s) based on content sim-ilarity and collaborative filtering aim to achieve high accuracy by recommending items that are relevant to the user X  X  interest. The problem with this approach is that the recommended items are very similar to the user X  X  interest as well as between themselves. Thus, the user will quickly find the recommended items too familiar and uninteresting for exploration. We call this the  X  X ccuracy overload-ing problem. X  To prevent accuracy from domainating the recom-mendations, Discovery-Oriented Recommendation Systems ( DORS s) introduce metrics called Discovery Utilities ( DU s) as additional di-mensions besides relevance for ranking the candidate items. The ranking problem is often modeled as a multi-objective optimiza-tion problem, which seeks an optimal tradeoff between the differ-ent dimensions [16]. Many DU s, including novelty and diversity, have been studied in the literature [3]. Although DORS s can help to alleviate the accuracy overloading problem, they neglect an impor-tant fact that different users have different curiosity levels, which lead to different levels of desire to discover new things. Specifi-cally, highly curious users would find recommendations with high DU s interesting but those with low DU s boring. The reverse is true for conservative users. We refer to this curiosity-driven, per-sonal demand of DU s as DU appetite. Without considering the DU appetite of each user, DORS s would favor items with high DU s (balanced by relevance) for every user. Consequently, while highly curious users are excited about high DU items, conservative users would find them too overwhelming. We call this  X  X uriosity mis-match problem. X 
In this paper, we present a framework for Curiosity-Based Rec-ommendation Systems ( CBRS s) to solve the curiosity mismatch problem. It consists of the Probabilistic Curiosity Model ( PCM ), which models a user X  X  curiosity with a curiosity distribution func-tion learnt from the user X  X  access history. Thus, each user has her own curiosity model estimated from her access behavior. It allows us to compute a curiousness score for each item representing how curious the user is about the item. CBRS then selects items which have both high relevance and curiousness scores, bounded by the constraint that the items X  DU s should fit the user X  X  DU appetite. We note that the CBRS framework is general enough for incorpo-rating different DU s. However, since novelty is by far the most studied DU in RS research, this paper focuses on novelty, leaving the details of modeling the other DU s as future research.
The Oxford dictionary defines curiosity as  X  X  strong desire to know or learn something. X  According to psychology research [4], a user X  X  appetite for novelty is influenced by her curiosity in that the higher/lower a user X  X  curiosity, the bigger/smaller is her appetite for novelty. For a user with a particular curiosity level, recommenda-tions with too much novelty will cause anxiety while too little will cause boredom. 1 To exploit this psychological phenomenon, a RS
This is an interesting finding in psychology. In most existing RS research, novelty is assumed to be the higher the better. However, we can learn from many real-life situations that this is not true. For example, most people going to a theme park for roller coaster rides should select recommendations with novelty commensurate with the user X  X  curiosity, instead of blindly maximize the novelty for ev-erybody . 2
In this paper, we address several challenges facing CBRS . First, we adopt the famous Wundt curve in psychology to model human curiosity [4]. The Wundt curve depicts the  X  X nverse U-shape X  re-lationship between an item X  X  stimulation degree ( sd ) (i.e., stimula-tion generated by, say, the novelty of an item upon a user) and the user X  X  response to the item (see Fig. 2). Briefly, given a user, as the novelty of an item increases, the user will become more and more interested in it until the novelty reaches a turning point where the interest is at a maximum; beyond this point the user will become less and less interested and will ultimately become disinterested in the item. This theory is adopted in CBRS . Unfortunately, Wundt curve is only a qualitative description of the relationship. In CBRS , we model the  X  X nverse U-shape X  with a Beta distribution and learn the distribution X  X  parameters from the user X  X  accessed history. Sec-ond, we use the curiosity model to compute for each candidate item a curiousness score representing how curious the user is about the item and select candidate items matching the user X  X  curiosity based on their curiousness scores. The input to the model is the stimula-tion degree of the item. In the context of this paper, it is the novelty of the item. To measure an item X  X  novelty, we propose three fea-tures, namely, the user X  X  access frequency to the item, the recency of the accesses, and the user-specified tags of the item. These fea-tures have been confirmed to be effective by psychology research [4]. Finally, we need to integrate the curiosity and relevance as-pects of the items to produce the final recommendations. We study two strategies, namely, the joint optimization of relevance and cu-riousness with the constraint that the novelty of the items should match the user X  X  novelty appetite, and co-factorization of the rele-vance and curiousness signals associated with the item.

Throughout this paper, we use music recommendation to illus-trate the ideas of CBRS . This is because music recommendation has been widely studied in recent years [8], and research has shown that music listening behavior is highly discovery oriented, highly per-sonal and heavily driven by curiosity [2]. The contributions of this paper are summarized as follows. would prefer rides that are not too exciting (too thrilling) or too easy (too boring).
Not explicitly mentioned here, the curiosity aspect must be bal-anced with the relevance of the items.
The rest of this paper is organized as follows. We review re-lated work in Section 2. Curiosity modeling will be introduced in Section 3. Section 4 describes the recommendation strategies, and Section 5 presents performance evaluation. Section 6 concludes our findings.
Discovery-Oriented Recommendation Systems ( DORS s) intro-duce various DU s to complement accuracy in solving the accuracy overloading problem. Common DU s include novelty [7, 15, 17], diversity [18, 21] and serendipity [13, 19]. Although DORS s were designed to produce recommendations that are more novel, diver-sified, or surprising, they ignore the fact that different users would accept different amount of DU s, which we call the DU appetite, be-cause users have different curiosity levels, causing what we call the curiosity mismatch problem described in Sec. 1. Recently, meth-ods that personalize the DU s of the recommendations for individual users, termed Personalized DORS s or PDORS , have been proposed to resolve the curiosity mismatch problem.
Personalization has been studied for a long time in information retrieval [6]. However, to the best of our knowledge, the only works on PDORS 3 were reported in [17] and [7]. [17] assumes that each user has a binary  X  X ovelty-seeking status X  indicating whether the user would seek completely novel (i.e., new) items or non-novel items (previously accessed items). However, this binary assump-tion is too strict in many situations, e.g., user may want to receive both completely novel and non-novel items in the same recommen-dation list, as in Youtube which recommends both new and old items. To extend this binary assumption, [7] learns a real-value novelty preference score for each user through logistic regression. The score is then used as a parameter for balancing similarity and novelty in the top-K ranking task. We call this method person-alized parameter balancing RS or PPBRS . The personalized nov-elty preference parameter of PPBRS linearly scales the novelty of all items in the final ranking process. The implication is that if a user is judged to accept novel items (large novelty preference score), then all novel items benefit equally in the ranking task. As discussed throughout this paper, linear scaling of novelty does not work for human curiosity: even for users having large novelty preference score, it does not mean that they can accept items with extremely high novelty (see Sec. 1 and Sec. 3). In our proposed CBRS , we model a user X  X  novelty preference as a probability distri-bution rather than a single real value and verify that the distribution resembles the non-linear  X  X nverse U-shape X  Wundt curve in psy-chology research. Since we only consider novelty in this paper, from now on, PDORS s refer to delivering personalized amount of  X  X ovelty X  ac-cording to the user X  X  novelty appetite.
There are several differences between CBRS and existing DORS s and PPBRS . Comparing to DORS s, (1) curiosity in CBRS is not a new dimension of DU s; instead, it governs each user X  X  acceptance level of DU s; (2) CBRS does not use DU s as ranking utility di-rectly since a given amount of DU s might be attractive to one user but may turn other users away. Instead, CBRS transforms DU s into a curiousness score representing a user X  X  curiousness about an item using the user X  X  curiosity model and the curiousness score is then used in ranking; (3) curiosity is a human trait born with a person, but DU s are based on an item X  X  property. Comparing to PPBRS , (1) user X  X  novelty preference in CBRS is not modeled as a real value but a curiosity distribution modeling the shape of Wundt curve as a probability density function pdf ; (2) rather than assuming that every item X  X  DU s is uniformly scaled by the user X  X  novelty pref-erence score, CBRS influences the discovery ranking utility with a curiousness score that is derived for each user and each item based on the user X  X  curiosity model.
Due to the interdisciplinary nature of our research, it is necessary to introduce terminologies widely used in the psychological curios-ity field. In psychology, curiosity is a human trait born with a per-son, driving her cognitive development through life. Since different people have different levels of curiosity, we introduce curiousness ( cur i u ), which is a real value, to quantify a user u  X  X  curiosity to explore an item i . CBRS adopts the Curiosity Arousing Model ( CAM ) developed in psychology research [4]. In CAM , a user re-ceives stimuli and would only respond to stimuli which can arouse her curiosity. Since CAM describes how a user selectively responds to the stimuli, it is also referred to as the Stimulus Selection Pro-cess ( SSP ). For recommendation systems, each recommended item presents a stimulus to the user. The strength of a stimulus is quan-tified by the Stimulus Degree, which is a real value denoted as sd . Note that the same item can produce different sd s to different users. The sd of a stimulus is defined by a number of factors called Collative Variables ( CV s). For this paper, CV s are the same as features, which are extracted from some measurable properties of a stimulus. The values of the CV s are called Collative Variable Values . As discussed, different users have different responses even if the stimulus is the same because of their difference in curiosity. We propose the Probabilistic Curiosity Model ( PCM ), which is a probabilistic view of CAM . It models a user X  X  selected sd  X  X  as a random variable, and curiosity as the probability distribution of the random variable, called Curiosity Distribution ( C u ). In this way, a user X  X  stimulus selection process ( SSP ) can be interpreted as drawing samples (stimuli) from the curiosity distribution under the guidance of the user X  X  curiosity.
Berlyne interprets curiosity as the driving factor for SSP , that is,  X  X hen several conspicuous stimulus are introduced at once, to which stimulus will human respond X  [4]. Following Berlyne X  X  in-terpretation, we can take an interior view of curiosity, which is to consider curiosity as an internal factor of a person influencing her selection of stimulus. We can also take an exterior view of curiosity, that is, a person X  X  SSP can reveal her curiosity inside. CAM points out the possibility that the interior curiosity trait can be estimated by the exterior SSP . To achieve this goal, we need to (1) quantify the stimulus and (2) model the curiosity. Sections 3.2 and 3.3 will discuss (1) and the remaining subsections will discuss (2).
Berlyne [4] discovered a principle set of features, named as Colla-tive Variables ( CV s), that can arouse curiosity. Four CV s were identified: novelty , uncertainty , conflict and complexity . Since CV s are dependent on user u , item i , time t , and user X  X  access history H u before time t , we use sd t u,i ( H t u ) to denote item i  X  X  sd with re-spect to u at time t given u  X  X  access history H t u . Similarly, we use cvv t u,i to denote the value of a CV associated with i at time t for u . Then, a stimulus can be quantified by Equation 1, where  X  is a scoring function. Since a stimulus is made up of CV s, it can be modeled as a (weighted) sum of cvv  X  X  of the CV s. In this paper, we focus on novelty only, so sd t u,i in Equation 1 is simplified to Nov t u,i , which denotes item i  X  X  novelty to user u at time t .
Our dataset is about the users X  music listening history, in which an item corresponds to a music track. When u clicks an item i to listen to it, we say  X  u accesses i , X   X  u accesses an artist X  if u ac-cesses at least one track performed by the artist, and u accesses a tag if at least one accessed track has the tag. From the recommen-dation point of view, the action that u accesses i is viewed as u  X  X  provision of a positive feedback on i . 4 From the curiosity point of view, u  X  X  access to i can be viewed as u responding to the stimulus generated by i . Thus, in this paper, we use  X  X eedback X ,  X  X ccess X , and  X  X esponse X  interchangeably depending on the context.
In the RS domain, the novelty of an item reflects how much the item differs from the user X  X  previously accessed items. In the psy-chology field, Berlyne suggested that novelty is inversely related to three factors [4]: (1) how often the stimulus has been experienced by the user, (2) how recent the stimulus has been experienced by the user before, and (3) how dissimilar the stimulus is to the user X  X  previous experience. Based on the three criteria, we formally define the novelty in Equation 2, where SF t u,i denotes the scaled frequency of the user X  X  accesses to item i before t ; high frequency indicates the user X  X  familiarity of the item, making it less novel to the user. SR t u,i denotes the scaled recency of the user X  X  access on item i w.r.t the current time t ; the more recent the user X  X  access to the item is, the less novel the item is to the user. Dissim t u,i denotes the dissimilarity between item i and u  X  X  historically accessed items; a large dissimilarity indicates i is more novel to the user. Note that SF u,i , SR u,i and Dissim correspond, respectively, to Berlyne X  X  three criteria above. In music recommendation, a user X  X  decision to listen to a track may depend on the performing artist. For example, after picking her favorite artist, the user may play all of the tracks sequentially in the album. In this case, the reason for the user to play the tracks in the album is not the novelty of the tracks but the performing artist. Thus, we include the artist in computing a track X  X  stimulus.
 We define SF t u,i formally in Equation 3: where A t u,i is the set of u  X  X  accesses to items in the user X  X  history H u (before time t ) having the same artist as i  X  X  artist, I of u  X  X  accesses to items i in the user X  X  history H t u , | . | denotes the
As with most web-based applications, we only consider implicit feedback from users. A user X  X  listening to a track indicates her preference on the track, which is a positive feedback; no negative feedback is available. cardinality of a set, and  X  a and  X  i are positive exponential scaling coefficients. The reason to model SF t u,i based on both the frequen-cies of the track itself (item i ) and tracks having the same artist as i  X  X  artist is that both tracks and artists are stimuli. It is clear that when | I t u,i | is high, the user has accessed item i many times so i  X  X  novelty is low, and when | A t u,i | is high, the user is familiar with i  X  X  artist, and so its contribution to novelty is low. Overall, a small SF t u,i value means i has low novelty.
 SR t u,i is defined in Equation 4: u  X  X  latest access in I t u,i and A t u,i . Here, we use  X  X ay X  as the time unit.  X  t is the forgetting coefficient. Equation 3 and 4 both use the decay function with exponential forgetting rate [10] to scale frequency and recency to the 0-1 range. Note that small SR value indicates less novelty.

To model Dissim t u,i , for each track, we extract six tags beling the genres of the music (e.g.,  X  X op X  and  X  X azz X ) using the LastFM API. The dissimilarity is calculated based on the number of common tags between the track and the historically accessed tags, and is formally defined in Equation 5, where Tags ( i ) denotes the set of tags associated with track i , and I u,tag denotes the set of accessed items in H t u labeled with tag .  X  tag is the coefficient for tag frequency. Note that small Dissim value means low novelty. Dissim t u,i = 1 | 2  X  Tags ( i ) |
Several observations can be made from Equations 2 to 5. First, items with small SF (i.e., frequently accessed), small SR (i.e., re-cently accessed), and small Dissim (genre has been frequently and recently accessed) have small Nov values and thus are less novel. Second, if the user listens to a new track performed by an artist whom she has never listened to before, SF t u,i and SR t u,i come 1. However, Dissim t u,i may not necessarily be 1. Third, we assume that the factors in each equation are equally weighted (e.g. in Equation 2, SF , SR , Dissim contribute equally to Nov ). Due to space limitation, we cannot show the results for all weight-ing combinations and the optimal balance is application dependent anyway. Fourth, Nov t u,i is defined on ( u,i,t ) triples and is a real value representing item i  X  X  novelty to u at time t . If we use Nov to denote the vector of Nov t u,i , within the time period T , when T  X  X  X  and we omit the superscript T , then Nov u can be viewed as a random variable representing u  X  X  inclination in accessing novel items, and each user X  X  novelty appetite nov u is defined as the ex-pectation of the random variable Nov u . Figure 1 illustrates the distribution of all users X  novelty expectation. We can see that the novelty appetites of different users vary a lot. Finally, although SF t u,i , SR t u,i and Dissim t u,i are defined based on the music rec-ommendation dataset, it is not difficult to extend them to other do-mains such as restaurant recommendation and movie recommenda-
For tracks which have less than six tags, we extract all of the tags available.
 tion, since user behaviors in these applications can be formulated as ( u,i,t ) triples.
Curiosity modeling has been studied in psychology for a long time. In 1870s, Wundt introduced the theory of  X  X ptimal level of stimulation X  and postulated an inverted  X  X -shape X  relationship be-tween stimulation level and hedonic response caused by the stimu-lus, which is referred to as the  X  X undt curve. X  Figure 2 is an illus-tration of Wundt curve, where the x -axis denotes stimulus degrees sd , and the y -axis denotes the user X  X  hedonic response. Berlyne formed the  X  X ntermediate arousal potential X  ( IAP ) theory, which states that too little stimulation results in boredom while too much stimulation results in anxiety. From Fig. 2, we can see that the user X  X  positive hedonic response increases first as sd increases. How-ever, after reaching a certain threshold, the positive hedonic re-sponse will drop with further increases of sd . We name the turning point as user u  X  X  anxiety turning point ( ATP u ). It means that be-yond the threshold the user will become anxious due to overwhelm-ing sd . An important note about applying Wundt curve to RS is that since we assume each of the user X  X  interaction with an item reflects her positive feedback on the item, there is no negative feedback in our our music recommendation application, leading to a Wundt curve that is entirely above the x -axis. Since a user X  X  curiosity can be revealed from her SSP and generally behaves like Wundt curve, the central task now is how to model Wundt curve.

From the probability point of view, we model each user X  X  se-lected sd  X  X  as a random variable and model the user X  X  curiosity as a probability distribution of the random variable, which determines the user X  X  SSP . The distribution is named Curiosity Distribution , denoted by C u . In this way, SSP is viewed as a sampling process from the user X  X  personal curiosity distribution, with which curiosity is able to guide SSP . Given a large amount of user-item interaction data and u  X  X  accessed sd  X  X , C u can be estimated. We name our model Probabilistic Curiosity Model ( PCM ), which can be consid-ered the probabilistic view of CAM . There are two main reasons we use PCM to model Wundt curve and curiosity. First, human X  X  SSP behaves in a probabilistic manner. For example, a curious user may also select small sd  X  X , although the chance is small compared with a conservative user. Second, although web data lacks explicit user preference data, abundant implicit feedback data are available. By modeling curiosity probabilistically, we can utilize web-scale user interaction data for the estimation of curiosity distribution.
Figure 3 illustrates a user X  X  SSP on five items with different sd values. According to the interior view of CAM , the user X  X  SSP (ei-ther acceptance or rejection of an item) is guided by her curiosity. PCM views a user X  X  SSP as continuously drawing samples from her curiosity distribution in such a way that stimuli whose sd  X  X  best meet the user X  X  curiosity have a high chance to be drawn. This en-sures that the stimuli with sd  X  X  that are too large or small, reflecting the fact that the items are too novel or boring for the user, have a lower chance to be selected by the users. In the example, the user selected four items and ignored one item and the average sd of the selected items is about 0.65.
Section 3.4 shows how curiosity guides a user X  X  SSP from the psychology and probability points of view. In this section, we will introduce how to estimate the curiosity distribution.

Since both CAM from psychology and our PCM try to model a user X  X  curiosity, it is natural to expect that the probability den-sity function ( pdf ) of C u exhibits the shape of Wundt curve. Fig-ure 4 shows the histograms of 5 users X  accessed sd  X  X , simulating the curiosity distribution. Several important findings can be ob-tained. First, the distribution generally fulfills the  X  X nverted-U X  shape of Wundt curve, which shows that PCM is suitable for mod-eling Wundt curve. Second, the curiosity distributions of differ-ent users are different. For example, the means for the five users are about 0.2, 0.4, 0.5 and 0.6, 0.7, respectively, showing that the users tend to show different levels of curiousness when they inter-act with the recommendation system. u 1 is relative more conserva-tive compared with u 4 and u 5 . Besides, the variance of the users X  distributions are different. u 1 and u 2 have relative small variance while u 3 to u 5 have relative large variance. Small variance means that curiosity is stable, which means that users X  curiosity tends not to change with topic or time, while large variance shows that the user X  X  curiosity may vary with topic and time. Third, generally, there is an optimal sd for each curiosity distribution, indicating the sd has large chance to be accessed.

According to the above findings, we propose to use the Beta dis-tribution, which has a flexible pdf and well studied parameter esti-mation techniques, to model the curiosity distribution C u curve X  X  inverse U-shape can be estimated with Beta parameters  X  and  X  larger than 1. We apply the method of moments for the esti-mation of  X  and  X  . The red curves in Figure 4(a) to 4(e) illustrate the estimated pdf  X  X  of five users X  Nov random variable.

Once the curiosity distribution is estimated, we can obtain the likelihood that the user is curious about an item with sd , i.e., the user X  X  curiousness on item i given its sd , denoted by cur where pdf is the probability density function of C u . cur viewed as a curiousness score mapped from an item X  X  stimulus on the curiosity distribution. According to the psychology of interest [14], the pleasant feeling obtained from the process of exploring novel and surprising items is an important component of human interest. Thus, in addition to relevancy, the fact that an item X  X  sd satisfies the user X  X  curiosity is also a reason for the user to access an item. In the following section, we will introduce how to incor-porate a user X  X  curiousness about the items into the ranking utility of an RS .
We model the recommendation problem as a top-K problem, which selects the top K items to recommend based on the relevancy of the items to the user and the user X  X  curiosity on the items. Let I be the item set of size | I | , and U be the user set of size | U | . The relevancy matrix R with dimension | U | X | I | is calculated using ex-isting accuracy-based recommendation techniques. Each element r u,i in R represents the relevancy of item i to user u . We use CR with dimension | U | X | I | to represent the curiousness matrix, with each element c u,i in CR recording u  X  X  curiousness on i (see Section 3.5). We use SD with dimension | U |  X  | I | to represent the stim-ulus degree matrix, where each element sd u,i denotes i  X  X  stimulus degree to u . The vectors R u , CR u and SD u represent, respectively, I  X  X  relevancy to u , u  X  X  curiousness over I and I  X  X  stimulus degree to u , and correspond to one row of R , CR and SD , respectively. It is useful to represent the recommendation list Rec  X  I using an | I | dimensional indicator vector y , such that y ( i ) = 1 if i  X  Rec and y ( i ) = 0 otherwise. y u represents u  X  X  indicator vector, denoting the items to be recommended to u . In the CBRS framework, we try to recommend items which are highly relevant to the user and stim-ulative to her curiosity. This is bounded by the constraint that the items in the recommend list should not exceed her anxiety turning point. With these expressions, it is possible to express the trade-offs between relevancy and curiosity as constrained optimization problems.

Given a fixed parameter  X   X  [0,1], find the vector y  X  such that
In this optimization problem, we seek to jointly optimize rele-vancy and curiosity, controlled by the parameter  X  . Optimal  X  can be tuned with a validation set. The final two constraints specify that y is a binary vector with K non-zero values. Recall that the anxiety turning point introduced in Section 3.4 is an item X  X  optimal stimulation. If the item X  X  stimulus exceeds this optimal stimula-tion, the user will feel anxious. In the constraints above, t the aggregate tolerance threshold of the K items. Here, we define t tol = c  X  k  X  ATP u , where c is a coefficient within [0,1], K is the number of items to be selected in the recommendation list. The constraint SD T u  X  y  X  t tol is referred to as the  X  ATP constraint X . The ATP constraint tends to be more loose when c is close to 1 and more strict when c is close to 0.
Matrix-factorization-based collaborative filtering ( MFCF ) has be-come popular in recent years due to its high accuracy [9]. From the psychology of interest [14], we know that curious emotion is an important component of interest. Thus, we believe a user X  X  cu-riosity affects her choice of items and try to incorporate this ad-ditional signal into MFCF . There are several approaches for the incorporation, e.g. co-factorization, ensemble and regularization. Due to the space limit, we only apply the co-factorization-based method, which jointly predict the missing preferences and curious-ness of the items. Other approaches will be left for future work. Specifically, MFCF maps both users and items to a latent space, denoted as R  X  U T V , where U  X  R l  X  m and V  X  R l  X  n with l &lt; min ( m,n ) , represent the users X  and items X  mapping to the latent space, respectively. In order to incorporate the curiosity in-formation, we create a user-item curiousness matrix C with the same size as R , and each entry c u,i denotes u  X  X  curiousness about item i . Then, learning of latent factors is done by minimizing the following sum-of-squared-errors objective functions with quadratic regularization terms: L = 1 +  X  U where L denotes the joint loss function, I R is the preference in-dicator matrix where I R i,j = 1 if U i has an access on V and 0 otherwise, I C  X  is defined in the same way as the curious-ness matrix C , Z denotes an item X  X  mapping to the curiosity la-tent space, and  X  C , X  U , X  V and X  Z denote the preset coefficients. Note that this minimization task is equivalent to maximizing the log-posterior distribution over U , V , Z if Gaussian priors are as-sumed [11]. Since we do not have users X  explicit ratings in our dataset, we use u  X  X  access frequency on i to approximate the rating r i,j in R . This setting is commonly used in music recommendation systems [5]. Since C i,j is within the range [0,1], in order to normal-ize R and C into the same [0,1] scale, we apply logistic function 1+ e  X  x to each entry of R . A local minimum of the objective func-tion given by Eq. 7 can be found by performing gradient descent in U , V j and Z k .  X  L  X  X  =  X  L  X  X  =  X  L  X  X  =  X  C
The CBRS framework is illustrated in Figure 5. In the training phase (Step 1), by recording a user X  X  access history on the recom-mendation list, we can collect the user X  X  responses to the stimuli (i.e., the ( u,i,t ) triples). The stimulus degree ( sd ) associated with a triple is computed as described in Section 3.3. The user X  X  cu-riosity distribution is then estimated based on the selected sd  X  X . In Steps 2 and 3, for each candidate item in the item repository, we compute the DU s based on Equation 1. 6 In Step 4, the user X  X  cu-riousness on the item is obtained by mapping the item X  X  sd to the pdf corresponding to the user X  X  curiosity distribution C u the recommender then integrates the user X  X  curiousness and the rel-evancy (calculated by traditional accuracy-based recommendation methods) of the item by solving the optimization problem depicted in Equation 6, or formulating the co-factorization problem denoted in Equation 7. The top K items which are both relevant and with high curiousness are selected into the recommendation list.
Figure 6 shows the curiosity distributions of two users A and B to illustrate how two users with different curiosity models respond dif-ferently to the same item. According to their curiosity distributions, A is more curious than B . Now, a candidate item is recommended to both A and B , and its sd is high (at 0.6, denoted by ATP figure). According to the mapping, A will have a higher curiousness score than B . If the item X  X  relevance is more or less the same to both users, the the item X  X  high curiousness score for A would push the item into A  X  X  recommendation but its low curiousness score for B may not be able to push itself into B  X  X  recommendation. However, if the item has small sd (at 0.3, denoted by ATP B in the figure), the situation will be reversed. B  X  X  curiousness score will be higher than that of A , and the item will be recommended to B instead of A .
In this section, we first describe the dataset used in the experi-ment. We then apply various metrics and vary the parameters to evaluate the performances of CBRS and the baselines.
We use the public dataset  X  X ast.fm Dataset -1K users X  [1] in the experiments. The whole dataset is 2.53GB in size. It con-tains 19,150,868 chronologically ordered listening records of 992
In this paper, only novelty is considered. Other DU s such as di-versity and serendipity can be plugged into the framework by de-veloping the corresponding formula to compute the sd of each DU . unique users over several years till May 5, 2009. Each record has the format  X  X ser id, time stamp, artist id, artist name, track id, track name X . The primary key is  X (user id, track id, time stamp) X , and we use ( u,i,t ) to denote the record of user u having accessed track i at time t . We remove all records without track id or artist id (ev-ery record in the dataset has a time stamp) and users with too few records for learning their curiosity distributions. The statistics of the cleaned dataset is given in Table 1.

To conduct an experiment, we need a training dataset, a histori-cal dataset and a test dataset derived from the original dataset. The training dataset is used to learn the curiosity function for each user. The historical dataset is needed for computing the stimulation de-gree sd for each record in the training dataset, because sd is deter-mined by novelty and novelty in turn depends on a user X  X  histori-cal accesses to the items (see below for further details). When a method is evaluated, we use the historical and training datasets to produce the recommendations and the test dataset as ground truth to evaluate the performance metrics of the recommendations.
Since the original dataset contains a large number of records, we divide it into 10 consecutive windows, w 0 ,...,w 9 , each of which contains one-tenth of the records in the dataset (denoted as | w | ). We use three consecutive windows w i ,w i +1 ,w i +2 to form, respec-tively, the training, historical and test datasets for one experimental run. That is, we use w 0 , w 1 and w 2 in the first run, w w 3 for the second, ..., and w 7 , w 8 and w 9 for the eighth run. Thus, we can perform eight experimental runs and average the results to obtain the value of a performance metric.

In the training phase, we need to compute the stimulation de-gree sd t u,i for each ( u,i,t ) record in the training dataset. Given a record r = ( u,i,t ) in the training dataset, we take the | w | records preceding r as r  X  X  historical window 7 H t u,i , and compute r  X  X  sd using Equations 1 to 5. The procedure is repeated for all records in the training dataset. At the end, each ( u,i,t ) record in the train-ing dataset is associated with a sd t u,i value. For each user u , we can obtain a series of sd values indicating u  X  X  accessed sd  X  X  in the training dataset and estimate u  X  X  curiosity distribution as described in Sectionrefsubsec:est.

The parameter setting in the experiments is as follows: the fre-quency scaling coefficients corresponding to  X  a and  X  i in Equation 3, and  X  tag in Equation 5 are set to 0.1, the time scaling coeffi-cient corresponding to  X  t in Equations 4 and 5 is set to 0.01. The goal of these settings is to ensure that different novelty compo-nents ( SF t u,i , SR t u,i , sim t u,i ) are within the (0,1) scale. We set  X  , X  U , X  V and X  Z in Equation 8 to 0.02. K denotes the number of items in the recommendation list, which is set to 10 by default and varied in Table 2 to study its impact on performance.
This subsection introduces the performance metrics used in the experiments.
The historical window for each record r contains the same number of accessed items to avoid bias because sd depends on the number of accessed items in the historical window H t u (see Equations 3 to 5).
As discussed in Section 1, a major difference between DORS s and CBRS is that CBRS delivers recommendations with the proper amount of novelty to suit a user X  X  curiosity. We introduce novelty fitness ( NF u ) to measure the fitness between the novelty of the recommended items and the user X  X  novelty appetite nov u (defined in Section 3.3). As defined in Equation 9, NF u is the root-mean-square error (RMSE), where K denotes the top K recommended items and nov u,i denotes the novelty of the recommended item i to u . A smaller NF value means that the novelty of the top K recommended items has a better fit to the user X  X  novelty appetite.
We use precision to measure how accurate the recommendations produced by a recommendation method predict the user X  X  future accessed items. Formally, we define precision as 1 | U | where U denotes the user set, R u denotes the recommendation list for user u , T u denotes the set of tracks that u has accessed in the test dataset (i.e., the ground truth). Note that we do not use recall as an evaluation metric, since the size of the recommendation list in Top-K recommendation is fixed.
Since CBRS makes recommendations adapted to a user X  X  per-sonal curiosity, we expect its recommendations to have larger dif-ferences across users comparing to traditional RS s. To test the va-lidity of this expectation, we use inter-user similarity ( IUS ) pro-posed in [20] to evaluate the system-wide personalization effect of a recommender. Equation 10 defines IUS i,j as the proportion of overlap between recommendation lists L i and L j received by users i and j , The IUS of a RS is the average of IUS i,j over all pairs of users. A large IUS i,j means a high similarity between recommendation lists received by users i and j , and a large IUS indicates a weak personalization effect of the RS .
According to Equations 6, the selection criteria of CBRS is to pick the items which are both relevant and stimulative to the user X  X  curiosity, bounded by the ATP constraint. The relative weight of an item X  X  relevancy and the user X  X  curiousness on the item is controlled by  X  . A large  X  promotes items with high curiousness, while a small  X  favors items with high relevance. In this subsection, we investigate the effect of  X  on the performance of a recommendation system in terms of the three metrics introduced in the preceding subsections. Since the optimization task for all items is very time consuming, we take the top 50 items as candidate items for each baseline recommender. Then, the top-K recommendation task is to select K items from the candidates.

To facilitate comparison, we pick two sets of recommenders, non-curiosity-aware ( non-CBRS ) and curiosity-aware ( CBRS ). For non-CBRS , we pick three baseline recommenders, namely, Popu-larity, Item-based collaborative filtering (Item-CF) and ranking ma-trix factorization (Ranking-MF). 8 For CBRS , we incorporate cu-riosity into the three baselines by solving the joint optimization problem in Equation 6. The CBRS recommenders are denoted as Cur-Popularity , Cur-Item-CF and Cur-Ranking-MF .

Figure 7 illustrates the influence of  X  on the performance met-rics. A general observation for all three metrics is that as  X  becomes larger (i.e., curiousness plays a more important role in recommen-dation), the performance of the recommenders generally increases. In Fig. 7(a), a small NF means a better fit between the novelty of the recommendations and the users X  novelty appetite. We can observe that CBRS has lower NF (i.e., better) than non-CBRS , because non-CBRS does not consider curiosity at all and hence cannot optimize the recommendations for novelty. This also explains why the NF of non-CBRS is flat w.r.t  X  . Further, the NF of CBRS improves as  X  increases, because as  X  increases CBRS will bias towards curiosity, thus producing recommendations with better novelty fitness. Fi-nally, for both CBRS and non-CBRS , Ranking MF is better than Item-CF, which in turn is better than Popularity in terms of NF . It is interesting to note that even for non-CBRS , which does not con-sider curiosity, Popularity has the worst NF , because popular items have less novelty.
 In Fig. 7(b), we can observe the same relative performance as in Fig. 7(a), that is, Ranking-MF has higher precision than item-CF, which in turn has higher precision than Popularity for both CBRS and non-CBRS . It is interesting to note that although CBRS is de-signed to balance relevance with curiousness, its precision not only has not suffered but is better than non-CBRS . This can be attributed to the fact that CBRS optimizes novelty according to each user  X  X  cu-riosity distribution. For example, if a user has low curiosity, CBRS would favor items that are less novel (i.e., more familiar) to the user. Since conservative users tend to listen to familiar tracks, precision is improved. In Fig. 7(c), a small IUS indicates a large difference and hence higher personalization effect between the recommenda-tion lists received by the users. Again, Popularity (both CBRS and non-CBRS ) performs the worst because it favors hot items and thus its recommendations to different users tend to be more similar. We can also see that Item-CF has the lowest IUS among all recom-menders because it does not recommend non-novel items. Further, its IUS is also lower than Cur-Item-CF because the latter would consider non-novel items (which could be hot items) if the user is judged to be conservative.

From the previous experiments, we can see that optimal perfor-mance is reached when  X  = 1 . 0 . This is equivalent to re-ranking the 50 candidate items purely based on the user X  X  curiousness on the
Popularity and Item-CF denote methods based on item popularity and similarity, respectively. Ranking-MF denotes the factorization machine approach [12]. An overview and implementation details of the three recommenders can be obtained from Dato Graphlab (https://dato.com/) API. items (see Equation 6) with the ATP constraint. This is because the candidate items produced by the recommenders already have very high relevance so further optimizing the items X  relevance has much smaller effect than re-ranking the items based on their curiousness scores. Thus, in the remainder of this paper, when we perform recommendation with the joint optimization method discussed in Section 4.1, we use  X  = 1 . 0 . In summary, take the best performer Ranking-MF as an example, CBRS produces 35.6%, 6.6%, 31.3% improvement in NF , precision, and IUS , respectively, compared to non-CBRS when  X  = 1 . 0 . It demonstrates the superior effective-ness of CBRS .
From Section 2, we know that existing DORS s have proposed various DU s to complement accuracy. However, they suffer from the curiosity mismatch problem. In contrast, CBRS believes a user X  X  need of novelty is non-linear and depends on her curiosity. In this section, we compare the performance of CBRS and DORS s. Figure 8 illustrates the result.
 To setup the experiment, we pick the best performers Ranking-MF and Cur-Ranking-MF in Section 5.3 as baselines. Our pro-posed method is labeled as Pure and Rel-Cur-ATP in Figure 8, rep-resenting, respectively, pure application of Ranking-MF and joint optimization according to Equation 6, which considers an item X  X  relevancy, curiousness and the ATP constraint. Rel-Nov performs joint optimization between relevancy and novelty as specified in Equation 6 without the ATP constraint, reflecting the implementa-tion of traditional DORS s. Rel-Nov-ATP is the same as Rel-Nov except the addition of the ATP constraint. From the comparison between Pure and Rel-Nov , we can find that with novelty comple-menting relevancy, IUS decreases, indicating the users X  recommen-dation lists become less similar to each other. This is an advan-tage of DORS s. However, recommendation precision is sacrificed and the recommendations deviate more from the user X  X  real nov-elty appetite (i.e., novelty fitness score increases). This trade-off is a common phenomenon in many DORS s. From the comparison between Rel-Nov and Rel-Nov-ATP , we can observe that with the ATP constraint, Rel-Nov-ATP can retain the benefit of small IUS in DORS s without suffering from decreased precision. The novelty fitness is even better compared to Pure . This could be attributed to the incorporation of the ATP constraint, which models individual user X  X  personalized novelty appetite. From the comparison between Rel-Nov-ATP and Rel-Cur-ATP , we can observe the importance of modeling a user X  X  personalized non-linear novelty appetite, since novelty fitness and precision are improved about 33% and 29%, respectively at  X  = 1 . 0 , while retraining the improvement of IUS .
Most existing RS s aim to recommend completely novel (new) items, because they assume that users will not be interested in items they have accessed before. This assumption is reasonable in some applications such as movie recommendation, but in some applica-tions such as music listening users may repeatedly access previ-ously accessed items. It is important to note that novelty in CBRS is not binary. Instead, novelty is a continuous value affected by many factors. For example, a music track that the user has listened to can still have high novelty to the user if she listened to the track or similar tracks only occasionally and a long time ago. In this sub-section, we will study how the inclusion/exclusion of previously accessed items in recommendations affects performance. We use the same experiment setting as in Section 5.3, except that in addi-tion to the 50 candidate items we randomly add N items that the user has previously accessed ( old items ). Now the recommendation problem is to pick the top K items for each user from these N +50 candidates. We set K = 10 , and vary N from 2 to 20.

The result is shown in Figure 9, from which several observations can be made. First, with the additional N old items, both non-CBRS and CBRS have improved performance in all of the three per-formance metrics. This can be attributed to the users X  habit of lis-tening to their favorite songs frequently, and adding old items in the recommendations will capture this user behavior. Second, although the performance of both non-CBRS and CBRS is improved, CBRS produces much larger improvement than non-CBRS . For example, compared with Ranking-MF , the best performer Cur-Ranking-MF achieves 85.5%, 166.7%, and 77.9% improvement on NF , preci-sion, and IUS , respectively, when N = 20 . This is attributed to CBRS  X  X  ability to capture a user X  X  unique novelty appetite. That is, if a user likes to listen to old songs, CBRS will model her as a conservative listener and recommend old songs to her and the con-trary is also true. Thus, CBRS can better utilize the N old items to satisfy the user X  X  preference on old songs. This is a significant contribution of this paper since very few works, if any, have dis-cussed the importance of non-novel items to the users and how to incorporate them into a recommendation list. Third, even when N = 0 , i.e., only new items are recommended, CBRS still out-performs non-CBRS even though the improvement is not as much as when old items are included. This can be attributed to the fact that new items still have different degrees of novelty to the user be-cause of their properties (e.g. a new track may have low novelty to a user when the user is familiar with the genre or performer of the track). Finally, with N becomes larger, the performance differ-ence among the CBRS recommenders narrows down. As discussed, users would listen to their favorite tracks repeatedly. Thus, as N in-creases, CBRS recommenders would recommend more from the N old items. This would greatly improve their performance and thus narrow down their performance gap. This is indeed an advantage of CBRS , since curiosity can be incorporated into any specific rec-ommender to make it curiosity-aware and improve its performance. From Sections 5.3 to 5.5, all recommendations are based on Equation 6, which performs joint optimization between relevancy and curiousness. In this subsection, we will compare CBRS to other state-of-the-art PDORS methods. As introduced in Section 2, the only two works on PDORS are [17] and [7]. We choose the PP-BRS method proposed in [7] as the baseline, because it is more re-cent and closer to our work. PPBRS learns a personalized novelty preference score for each user by logistic regression. For compar-ison, we use Ranking-MF as the reference recommender, instead of the Item-based collaborative filtering in [7]. We also choose the standalone Ranking-MF as another baseline, because it performs best among the traditional accuracy-based recommenders (shown in Figure 7) and does not consider curiosity. As introduced in Sec-tions 4.1 and 4.2, we propose two ways to incorporate curiosity information into an RS . Thus, our proposed CBRS methods are Cur-Ranking-MF and Cur-MFCF, corresponding to Equations 6 and 7, respectively. The results are shown in Table 2.

Comparing the two baselines PPBRS and Ranking-MF , we can see that although PPBRS can provide more personalized novelty to the users (small NF score), it is limited in enhancing precision. This is because although PPBRS learns a personalized  X  X elevancy-novelty X  balance parameter, it applies the parameter to all items in the same way. That is, if the parameter favors novelty, then all items X  novelty scores will be emphasized in the same way. This may result in some recommended items overly novel to the user because they may already have high novelty scores and amplifying them further will make them too novel for the user X  X  curiosity level. For the same reason, some recommended items could become too boring to the user. Comparing PPBRS and CBRS ( Cur-Ranking-MF and Cur-MFCF ), we can see that CBRS performs better in both novelty fitness and precision (e.g. Cur-Ranking-MF achieves 32.1%, 7.8%, 18.8% improvement over PPBRS on NF , precision, IUS , respectively, when K = 10 ) because it is able to capture a user X  X  non-linear novelty appetite. Between the two CBRS recom-menders, Cur-Ranking-MF performs better than Cur-MFCF . This might be attributed to the ability of explicit joint optimization to combine an item X  X  relevancy and the user X  X  novelty appetite more optimally. Detailed study on how to better incorporate the curios-ity signal into the traditional recommendation framework is beyond the scope of this paper and will be treated in future work.
Various Discovery-Oriented Recommendation Systems ( DORS s) have been proposed to address the accuracy overloading problem of traditional recommendation systems. They introduce Discov-ery Utilities ( DU s) such as novelty, diversity and serendipity as additional ranking dimensions to ensure that recommendations are not dominated by accuracy. However, DORS s do not consider the fact that different users have different appetite for DU s and suffer from the curiosity mismatch problem. In this paper, we present the Curiosity-based Recommendation System ( CBRS ) framework for integrating the curiosity and relevance aspects of recommenda-tions. The Probabilistic Curiosity Model ( PCM ) models a user X  X  unique appetite for novelty. PCM is formulated based on the cu-riosity arousal theory and Wundt curve in psychology [4]. It uses the Beta Distribution to model Wundt curve representing a user X  X  non-linear desire of novelty. With PCM , for each user we can ob-tain a curiousness score for each candidate item. Joint optimization and matrix factorization methods are used to incorporate the cu-riosity signal into RS s. Experimental results show that CBRS sig-nificantly outperforms the baselines in various performance met-rics. An important finding is that CBRS can provide personalized recommendations adapted to an individual user X  X  curiosity and at the same time improve the recommendation accuracy. We also study the impact of allowing previously access items (i.e., non-novel items) in the recommendations and show that CBRS can rec-ommend the right mix of non-novel and novel items to optimize the performance. This is an important finding because in many appli-cations (e.g., music recommendation) users may repeatedly access some items (e.g., favorite songs).

For future work, we plan to study other DU s such as diversity and surprisal and integrate curiosity and social relationship into the CBRS framework. Finally, we recognize the mutual benefit be-tween our work and psychology research and plan to apply web-scale data in the study of human X  X  psychological curiosity behavior. Research reported in this paper was supported by the Research Grants Council, HKSAR, GRF No. 615113. [1] Lastfm homepage, 2015. [2] A. Anglade, O. Celma, B. Fields, P. Lamere, and B. McFee. [3] I. Avazpour, T. Pitakrat, L. Grunske, and J. Grundy. [4] D. Berlyne. Conflict, Arousal and Curiosity . McGraw-Hill, [5] E. Bernhardsson. Collaborative filtering at spotify, 2013. [6] W. Fan, M. D. Gordon, and P. Pathak. Personalization of [7] K. Kapoor, J. A. Kumar, and P. Schrater. "i like to explore [8] P. Knees and M. Schedl. Music retrieval and [9] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [10] X. Li and W. B. Croft. Time-based language models. CIKM [11] H. Ma, Yang, and I. King. Sorec: Social recommendation [12] S. Rendle. Factorization machines. ICDM  X 10. [13] G. Shani and A. Gunawardana. Evaluating recommendation [14] P. J. Silvia. Interest the curious emotion. Current Directions [15] P. Vargas, Castells. Rank and relevance in novelty and [16] S. Vargas. Novelty and diversity enhancement and evaluation [17] F. Zhang, K. Zheng, Yuan, and X. Zhou. A novelty-seeking [18] M. Zhang and N. Hurley. Avoiding monotony: Improving the [19] Y. C. Zhang and T. Jambor. Auralist: Introducing serendipity [20] Y. Zhou, T.and Zhang. Solving the apparent [21] C.-N. Ziegler and G. Lausen. Improving recommendation
