 Short &amp; sparse text is becoming more prevalent on the web, such as search snippets, micro-blogs and product reviews. Accurately classifying short &amp; sparse text has emerged as an important while challenging task. Existing work has con-sidered utilizing external data (e.g. Wikipedia) to alleviate data sparseness, by appending topics detected from exter-nal data as new features. However, training a classifier on features concatenated from different spaces is not easy con-sidering the features have different physical meanings and different significance to the classification task. Moreover, it exacerbates the  X  X urse of dimensionality X  problem. In this study, we propose a transfer classification method, TCSST, to exploit the external data to tackle the data sparsity issue. The transfer classifier will be learned in the original feature space. Considering that the labels of the external data may not be readily available or sufficiently enough, TCSST fur-ther exploits the unlabeled external data to aid the transfer classification. We develop novel strategies to allow TCSST to iteratively select high quality unlabeled external data to help with the classification. We evaluate the performance of TCSST on both benchmark as well as real-world data sets. Our experimental results demonstrate that the pro-posed method is effective in classifying very short &amp; sparse text, consistently outperforming existing and baseline meth-ods.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.2.7 [ Arti cial Intelligence ]: Nat-ural Language Processing X  text analysis ; I.2.6 [ Arti cial Intelligence ]: Learning  X 
QC IS: centre for Quantum Computation &amp; Intelligent Sys-tems ALGORITHMS, PERFORMANCE, EXPERIMENTATION Short &amp; Sparse Text Mining, Classification, Transfer Learn-ing, Wikipedia, External Data
Text categorization, one of the most important tasks of text mining, has been studied intensively in the past decade. Various algorithms (e.g. Naive Bayes, Maximum Entropy) have been applied to text classification and proved to be ef-fective on different benchmark datasets (e.g. 20Newsgroups, Reuters-21578). Due to the rapid advancement of informa-tion technologies and web applications, there has been an increasingly large amount of short &amp; sparse text data avail-able online, such as micro-blogs, web search snippets, forum messages and product reviews etc. Consequently, mining short &amp; sparse text to derive high quality knowledge will benefit many information retrieval and web applications.
Briefly, short &amp; sparse text refers to a data set where each text instance is short, consisting of from a dozen words to a few sentences. Moreover, two instances usually do not share enough word co-occurrence. These new features ren-der traditional text mining tools ineffective, motivating the development of novel methodologies. For example, for the research tasks of measuring similarity between short texts and clustering short texts, methods have been proposed to overcome the data sparsity by expanding original data with information or context extracted from external data, such as the query results returned by search engines [14] and the online data repositories (e.g. Wikipedia) [2].

For the task of classifying short &amp; sparse text, although limited work has been done in this area, the proposed meth-ods [12][16] similarly resort to external or auxiliary data to deal with the data sparseness problem. For instance, Phan et al. [12] proposed to mine hidden topics from a very large external data collection, and represent each text instance as a vector of words and topics. Then, one classifier was trained on concatenated features from two spaces. The main limita-tions of the method are: 1) the original features of words and the detected topics are not comparable as they have differ-ent physical interpretations and, therefore, different contri-butions to the classification task. It may not be appropriate to directly concatenating features from different spaces to train a classifier; 2) appending detected topics as new fea-tu res will increase the dimension of the data, exacerbating the  X  X urse of dimensionality X  problem. Hence, in this paper, we aim to construct a classification model for short &amp; sparse text by using external data and dealing with original feature space.

In particular, we propose a method called, T ransfer C lass-ification of S hort &amp; S parse T ext (TCSST), which uses trans-fer learning to exploit the external data to aid the classifica-tion for short &amp; sparse text. It is usually assumed by tradi-tional transfer classification that, while there is only a very small set of labeled data in the new target domain, there are plenty of labeled data in the old source domain. There-fore, traditional transfer classification focuses on leveraging the labeled data in the source domain to learn an effective classifier for the target domain. However, for the problem of short &amp; sparse text classification, the external data is typi-cally acquired through querying search engines or crawling online data repositories. It can X  X  be expected that there are plenty of data in the external domain with labels read-ily available. Therefore, TCSST focuses on exploiting both labeled and unlabeled data in the external domain to con-struct the classification model for the short &amp; sparse text. We develop novel strategies to allow TCSST to efficiently and iteratively select from the large set of unlabeled exter-nal data a subset where the data are very likely to be useful in the transfer classification. The selected data will be used to train the transfer classifier together with other labeled training data.

The main contributions of the work is summarized as fol-lows.
The rest of this paper is organized as follows. In Section 2, we review existing research on short &amp; sparse text data min-ing. The problem definition of the transfer classification of short &amp; sparse text data and the proposed method, TCSST, are discussed in details in Section 3. In Section 4, we provide the results of the experiments evaluating the performance of the proposed method. Finally, we summarize the work and discuss the future study in Section 5.
In this section, we review related research in the areas of short &amp; sparse text mining, from similarity measure to clustering and classification.

Semantic similarity measure between short texts (e.g. words and named entities) has long been an integral part of in-formation retrieval and natural language processing. Ex-isting research can be generally divided into two categories: knowledge-based approaches and corpus-based approaches [10]. The former usually finds the relationship between words us-ing some external lexical database (e.g. WordNet); while the latter frequently identifies the similarity degree by re-sorting to the information derived from large corpora. For corpus-based approaches, the two representative approaches to obtain large text collection are querying search engines and crawling online data repositories. For example, Bol-legala et al. [3] used search engines to obtain enriched con-text data for measuring similarity between words. Similarly, Sahami and Heilman [14] measured the similarity between short text snippets by leveraging the search engines to ac-quire greater context. Yin and Meek [18] further improved the work [14] by introducing a web-relevance similarity mea-sure and using a machine learning approach. Gabrilovich and Markovitch [6] computed semantic relatedness by rep-resenting the meaning of any text as a vector of Wikipedia concepts.

Clustering short text has attracted growing interests in recent years. Most of the existing research similarly re-lies on auxiliary data to overcome the data sparseness. For example, Banerjee et al. [2] tried to improve the cluster-ing accuracy for short text by enriching their representation with additional features from Wikipedia. Hu et al. [7] pro-posed a hierarchical three-level structure to tackle the data sparsity problem and reconstruct the corresponding feature space with the integration of multiple semantic knowledge bases  X  Wikipedia and WordNet. Recently, Jin et al. [8] proposed to cluster short text messages via transfer learn-ing from auxiliary long text data. While we similarly use transfer learning to exploit the auxiliary data, we focus on a different text mining task  X  text categorization.
The short and sparse feature pose similar challenges to the task of text categorization. The insufficient word co-occurrence renders traditional techniques such as  X  X ag-Of-Words X  limited. Likewise, existing methods mainly exploit external data to expand the original data. In [16], Sriram et al. proposed to use a small set of features extracted from the author X  X  profile and text to classify tweets  X  microblogs provided by Twitter. Sun et al. [17] used semantic analy-sis to extract concept information from Wikipedia to deep classify short text. Phan et al. [12] proposed to mine hid-den topics from crawled Wikipedia corpus as new features and simply train a classifier from the augmented and com-bined feature space. However, it may not be appropriate to train a classifier on incomparable features with different physical meanings, and consequently different significance to the classification task. It also results in the increase of data dimensions. In our work, we study a different method  X  transfer learning  X  to exploit the external data, such that the classifier is trained in original feature space.
In this section, we first formally define the problem of transfer classification of short &amp; sparse text data. Next, we describe the framework of our proposed method, which is followed by the details of the algorithms.
Similar to the existing research in short &amp; sparse text min-ing, we consider exploiting external data (e.g. Wikipedia) for classifying short &amp; sparse text (details of crawling rele-vant Wikipedia documents will be discussed in Section 4). Therefore, we have text data from two domains: the short &amp; sparse domain  X  s and the external domain  X  e .
In particular, let  X  s = { x s 1 , x s 2 , . . . , x s m } instances in the short &amp; sparse domain,  X  e = { x e 1 be the set of instances in the external domain, and C = { ..., c k } be the set of category labels. The instances in  X  are divided into two disjoint subsets: the labeled subset T s = { ( x s where x s i  X   X  s and c ( x s i )  X  X  . Similarly,  X  e is composed of set S e = { ( x e j ) } , where x e j  X   X  e and c ( x e j problem of transfer classi cation of short &amp; sparse text data is : given a small number of labeled short &amp; sparse text data T s  X   X  s , a small number of labeled external data and a large number of unlabeled external data  X  e = T e  X  X  e the objective is to learn a classifier H  X  s  X  X  that minimizes the prediction error on the set of unlabeled short &amp; sparse text S s  X   X  s .

There are two major issues that differentiate our problem from traditional transfer classification. Firstly, we focus on dealing with the short &amp; sparse text data. Secondly, we aim to exploit both labeled and unlabeled data in the ex-ternal domain, while traditional transfer classification lever-ages only the labeled data in the source domain to help with the classification in the target domain. The reason we try to exploit the unlabeled external data is motivated by the situation in the real-world application. For instance, given a set of labeled short &amp; sparse text, we managed to crawl relevant documents from Wikipedia using carefully selected seeds. However, the labels of the crawled documents may not be readily available, or may be assigned very crudely. In this case, it is only affordable to manually label a very small number of documents in the external domain. There-fore, the proposed method will be practically useful if the unlabeled external data is made good use of to construct a high quality classification model for the short &amp; sparse text data.
The framework of our proposed method, Transfer Classi-fication of Short &amp; Sparse Text (TCSST), is shown in Fig-ure 1.

The input data is a small set of labeled short &amp; sparse text. From online data repositories, we should be able to gather a large collection of relevant documents as external data, among which only a small set is labeled. Certainly, without considering the unlabeled external data S e , existing transfer classification method (e.g. TrAdaBoost [4]) can be employed directly to consider only the labeled data, T s  X  and T e  X   X  e , to learn a classifier for the short &amp; sparse text. However, due to the insufficient number of labeled external data, the improvement brought by the traditional transfer classification might be limited. Therefore, TCSST aims to further exploit the unlabeled external data.

Intuitively, we can first predict labels for unlabeled ex-ternal data, using some semi-supervised classification algo-rithm, in consideration of the limited number of labeled data in the external domain. Then, traditional transfer classifica-tion can be applied straightforwardly by treating all external data (with known or predicted labels) as training data.
However, as will be shown in our experiments in Section 4, this straightforward method fails to train a classifier of high quality. Sometimes its performance is even worse than the one using only the labeled external data (referred to as the base classi er hereafter). This is because there are too much noise in the unlabeled external data with predicted labels. Therefore, rather than including all unlabeled external in-stances into the transfer classification, we devise an itera-tive sampling process to allow TCSST to select a small set of unlabeled external data in each round. The selected un-labeled external data, with predicted labels, will be used together with other labeled data to train a transfer clas-sifier. Each unlabeled external instance is associated with sampling probability, which will be updated in each round to reflect the importance of the instance to the transfer classifi-cation. Subsection 3.6 describes the details of the sampling process. Basically, an unlabeled external instance will be more likely to be sampled if it is regarded to be more useful in the transfer classification. If the learned transfer classi-fier works better than the base classifier, the classifier will be saved. As shown in the Figure 1, the output of TCSST is an ensemble classifier consisting of all intermediate classifiers better than the base classifier.

The main steps of TCSST are outlined as follows. 1. Predict labels for unlabeled external data (Subsection 3.4); 2. Assign initial sampling probabilities for unlabeled ex-3. Sample a set of unlabeled external data to train a 4. Save or discard the transfer classifier depending on its 5. Go back to step 3 or output the boosting classifier on
Today, Wikipedia has been known as the richest online encyclopedia which includes a huge number of documents in various topics. Since Wikipedia covers a lot of concepts and domains, it is reasonable to extract parts of Wikipedia documents to form an external dataset related to the target domain.

To collect data from Wikipedia, we prepare various seed crawling keywords from different class domains. The sim-plest method is utilizing the class name as the seed key-words. For example, in our five-class dataset, we utilize five class names (Human, Natural, Social, Physical, Finan-cial) as crawling keywords. Furthermore, if the class labels are inconsistent or irrelevant to instances context, we can generate some keywords from the context information by common sense. For example, Phan et al. [12] generated sev-enty keywords for classification task. We crawl the seed X  X  corresponding Wikipedia page and follow the outgoing hy-perlinks to acquire related pages. Each crawling transaction is limited by the maximum depth of hyperlink (usually 4).
After downloading the pages, we would like to assign la-bels for these pages. Intuitively, for each seed, we can as-sign class label by manual, and assign the same label to the seed X  related pages. However, this method has two limita-tions. Firstly, different seeds may share same related pages. Secondly, as the hyperlink depth increasing, the content of related pages are surprisingly different to the original seed page. Consider the limitations above, we manually assign labels for part of documents, and leave others as unlabeled data.
In order to use the unlabeled external data in the transfer classification framework, we first predict labels for unlabeled external data. Considering the limited number of labeled external data, we choose to use semi-supervised classifica-tion instead of supervised classification. In particular, we use the Naive Bayes based self-training approach [19], while any semi-supervised classification algorithm should be work-able here. The basic idea of self-training is to train firstly a classifier with the small amount of labeled data. The clas-sifier is then used to classify the unlabeled data. Typically the most confident unlabeled data, together with their pre-dicted labels, are added to the training set. The classifier is re-trained and the procedure repeated.

To avoid the over-fitting problem, we add a bagging frame-work on classifiers trained by the self-training at each iter-ation, and output an ensemble classifier H ( x ) as the final classifier of semi-supervised learning.
 where x  X  X  e  X   X  e , and h i is the classifier trained at the i iteration of the semi-supervised learning.
According to the categorization of transfer learning in [11], our task of classifying short &amp; sparse text using external data belongs to the category of inductive transfer learning . Differ-ent types of inductive transfer learning methods have been proposed, such as the feature representation based trans-ferring [13] and the parameter based transferring [5]. In our work we adopt the instance based transferring , which provides a more flexible framework to allow us to include unlabeled external data instances afterward.

In particular, we borrow the framework of TrAdaBoost [4], which is a representative instance based transferring classifi-cation method. It extends from AdaBoost [15] that aims to boost the accuracy of a weak learner by carefully adjusting the weights of training instances and learn a classifier ac-cordingly. Similar to AdaBoost, TrAdaBoost iteratively in-creases the weights for instances from the target domain (e.g. short &amp; sparse domain) if they are wrongly predicted, ex-pecting that the subsequent classifiers built will be tweaked in favor of the instances misclassified by previous classifiers. For instances in the source domain (e.g. external domain), the weights of wrongly predicted instances will be decreased, because it is regarded that these instances are most dissim-ilar to those in the target domain, exerting less impact on the classification.

In particular, the weight of instances in the target domain is updated as follows. where x i is the i th labeled instance in the target domain,  X  is the t th classifier X  X  average lost over the target domain, instance x i .

For wrongly predicted instances in the source domain, the weight is decreased based on the maximal iteration number, N , and the size of the data set n .
Classifiers trained from iterations 0 to N/ 2 are supposed to filter dissimilar instances in the source domain. Hence, TrAdaBoost finally outputs an ensemble classifier which is a weighted combination of the classifiers trained from iteration N/ 2 to N .

Recall that, due to the small number of labeled exter-nal data, TCSST aims to exploit the large set of unlabeled external data to boost the transfer classification. In partic-ular, TCSST first predicts labels for unlabeled external data and then iteratively selects a small set of unlabeled external data, with predicted labels, to learn the transfer classifier to-gether with the other training data (i.e. the labeled external data and labeled short &amp; sparse data). In this subsection, we describe the strategies TCSST uses to sample unlabeled external data.
The basic idea is that, the more likely an instance is use-ful in the transfer classification, the more possible it will be sampled. To implement the idea, we assign a sampling probability to each unlabeled instance in the external do-main. Given an instance x i  X  X  e  X   X  e , an initial sampling probability  X  ( x i ) 0 will be assigned. We first describe how to discuss how to assign the initial sampling probability  X  ( x later.

As introduced in Subsection 3.5, TrAdaBoost updates the weights for training instances in the external domain after each iteration. In particular, the weight of an instance will be decreased if it is regarded to be less useful in the trans-fer classification (i.e. Eq. 3). We will use the by-product of TrAdaBoost to update the sampling probability for un-labeled external data. First, a set of unlabeled external instances P 0 = { x 1 , x 2 , . . . , x p } X  X  e is selected, based on some initial sampling probabilities, to be included in TrAdaBoost. Each instance x i will be assigned a sequence of weights respectively in the N iterations of TrAdaBoost instance,  X  i , as the sum of the weights assigned in the itera-tions from  X  N/ 2  X  to N where the classifiers start to converge. Let the final weights of the instances in P 0 be &lt;  X  1 We normalize the weights by setting  X  i to  X  i / that  X  i  X  (0 , 1). Then, we update the sampling probability for each unlabeled external instance as follows.
For an unlabeled instance that has been selected in the t round, if it turns out to be useful in the transfer classification (e.g. the weight  X  i is high), Eq. 6 will decrease (because  X  1) its sampling probability less significantly. Otherwise, its weight  X  i will be low and its sampling probability will be decreased more significantly. If an unlabeled instance has not been selected in the t -th round, its sampling probability will not be decreased. That is, an unsampled instance will have more chances to be selected.

We now discuss how to assign the initial sampling prob-ability  X  ( x i ) (0) for each unlabeled external data. Since we have a large collection of unlabeled external data, if starting from a set P 0 consisting of instances that are not useful in the transfer classification, it may require more rounds of iter-ative sampling to find useful data to boost the classification. Therefore, instead of assigning equal sampling probabilities to all unlabeled external data, we assign higher sampling probabilities to instances which are more likely to be useful. In particular, we learn a base classifier by running TrAd-aBoost on labeled short &amp; sparse text (e.g. T s ) and labeled external text (e.g. T e ) only. Based on the results of the base classifier, we can similarly compute the final weight for each labeled external instance,  X  ( x j ), using Eq. 5 and normalize to make sure  X  ( x j ) ranges between 0 and 1. Next, given an unlabeled external instance x i  X  X  e , we measure its simi-larity with every labeled external instance x j  X  X  e . If it is more similar to a high quality labeled external instance, its Al gorithm 1 Transfer Classification of Short &amp; Sparse Text inp ut
T s , T e , S e , M -size of P , MAX I TERATION output
H ( x ) -the TCSST classifier 1: TrAdaBoost( T s , T e ) /*learn the base classifier h 0 2: For each x i  X  X  e , assign  X  ( x i ) (0) as Eq. 7 3: Let t = 0, H =  X  4: while t &lt; MAX I TERATION do 5: Sample S e to generate P ( t ) 6: h t ( x ) =TrAdaBoost( T s , T e , P ( t ) ) 7: if  X  t &gt;  X  0 then 8: H = H  X  X  h t ( x ) } /*  X  i is n -fold cross-validation 9: end if 10: t= t+1 11: For each x i  X  X  e , update  X  ( x i ) ( t ) as Eq. 6 12: end while 13: H ( x ) = arg min in itial sampling probability will be higher. That is, where sim ( x i , x j ) is the similarity between two text instances and the cosine similarity measure is adopted here.
As aforementioned, TCSST iteratively samples a set of unlabeled external instances with predicted labels, together other labeled training data to learn a transfer classifier. If the learned classifier performs better than the base classifier which uses labeled training data only, the learned classifier will be saved. The final output of TCSST is an ensemble classifier combining all saved classifiers. We use the n -fold cross-validation accuracy over the training data as the vot-ing weight of each saved classifier h i ( x ). Then, the output classifier H ( x ) of TCSST is, where  X  i is the n -fold cross validation accuracy of the i -th saved classifier.

The pseudo code of TCSST is given in Alg. 1. Firstly, we learn a base classifier by training TrAdaBoost on labeled training data (line 1), based on which we assign initial sam-pling probabilities to unlabeled external data (line 2). Then, an iterative process is started by sampling a subset of un-labeled external data P ( t ) (line 5), which will be used to train the t -th transfer classifier h t ( x ) (line 6). If the perfor-mance of the t -th classifier is better than the base classifier h ( t ), h t ( x ) will be saved (lines 7-9). Based on the results of t -th classifier, the sampling probabilities of unlabeled ex-ternal instances are updated (line 11). The final classifier is a weighted combination of saved classifiers (line 13). In this section, we evaluate the performance of TCSST. We describe first the data sets used in our experiments. Then, the methods used to be compared with TCSST will T able 1: Data set constructed from 20-Newsgroup
Dat a Set S hort &amp; Sparse D. Ext ernal D. rec vs talk S ize of  X  s 36 69 18 9 S ize of  X  e 35 61 11 37 S parse degree 99 .93%  X  X  X  99.98% 99 .92% V ocabulary size | V | 15 254 61 12
N umber of classes 2 5 b e discussed. Finally, we show the results of different types experiments.
We evaluate the performance of TCSST on a benchmark data set as well as a real-world data set.

For the benchmark data set, we consider the well-known 20newsgroup data [9], which has been frequently used for evaluating and comparing different transfer learning algo-rithms for text classification. The 20newsgroup is a text collection of approximately 20 , 000 news-group documents, which are partitioned across 20 different newsgroups nearly evenly. In our study, we generate a data set from two top categories, rec and talk , one as positive and the other as neg-ative. We then split the data in the two categories based on their sub-categories. Table 1 shows the specific sub-category based splitting strategy that divides the data into two do-mains. The splitting strategy ensures that the documents in the two domains are different but related, since they are un-der different sub-categories while same top categories. The task is to classify top categories. Since we focus on dealing with the classification of short &amp; sparse text, we make one of the domains (the one in the second column of Table 1) to be sparse, by randomly deleting features for instances. The other domain (i.e. the third column of Table 1) then serves as the external domain. In our experiments, we will vary the parameter  X  the percentage of features deleted from each instance  X  to generate sparse domains with different sparse degrees, and evaluate classification models on the generated sparse data sets. The second column of Table 2 summa-rizes some statistics of the benchmark data set. The top two numbers are the sizes of the short &amp; sparse domain and the external domain respectively. The third number is the sparse degree of the short &amp; sparse domain, which is defined as follows. Let M be a matrix representation of the short &amp; sparse domain  X  s such that m ij is non-zero if the i -th instance contains the j -th word. Then, the sparse degree of  X  s is defined as the ratio of zero elements to all elements in M .
 where | V | is the vocabulary size. Thus, the higher the sparse degree, the more sparse the data.

We also use a real-world data set to evaluate the classi-fication models. The data was collected from participatory workshops that were held to assess the capacity of natu-ral resource managers, such as landholders and graziers, to adapt to natural resource changes. Participants were in-vited to comment from five indicators: human , social , nat-ural , physical and nancial . Each comment thus is a short &amp; sparse text instance, belonging to one of the five cat-egories. To obtain relevant external data, we follow the strategy used in [12] to crawl Wikipedia documents. We use the five class labels as the seed keywords. For each seed keyword, we ran JWikiDocs 1 to download the correspond-ing Wikipedia pages. We also crawled the relevant pages by following outgoing hyperlinks, with the maximal depth of hyperlink as 3. We crawled overall 1137 relevant documents from Wikipedia. After removing HTML tags, noisy text and links, stop words, the statistics of the real-world data set are reported in the third column of Table 2. We refer to this data set as NRM Data and make it available online [1].
We compare the performance of TCSST against the fol-lowing methods.
Similar to Ensemble-TrAdaBoost and TCSST-No-Sampling, we consistently use RandomTree as the weak learner of TrAd-aBoost for TCSST.
JWi kiDocs: http://jwebpro.sourceforge.net
Accuracy Accuracy (%)
Firstly, we compare the performance of TCSST against the other methods on the benchmark data set. We vary the parameter, percentage of features deleted from instances, from 90% to 95% to generate two sparse domains, while the external domain is not changed. The results of the classi-fiers mentioned in Subsection 4.2 as well as those of TCSST on the two data sets are shown in Table 3. In this set of experiments, we respectively set the percentage of labeled data in the short &amp; sparse domain as 5%, 10%, 15% and 20%. For Ensemble-TrAdaBoost, TCSST-No-Sampling and TCSST, the percentage of the labeled external data is set to 10%. For TCSST, in each round, 20% of unlabeled external data is selected to be included in the training data. For Ensemble-TrAdaBoost, TCSST-No-Sampling and TC-SST, the number of iterations used by TrAdaBoost is set to 40. For Ensemble-TrAdaBoost and TCSST, the number of TrAdaBoost classifiers is set to 20. For LDA-Classification, 100 topics are detected from the sparse domain and the ex-ternal domain.

All results listed in Table 3 are generated from 10-fold cross validation. It can be observed that TCSST outper-forms all of the other four algorithms in nearly every set-ting. Comparing Random Tree and Ensemble-TrAdaBoost, we notice that transfer learning using external data improves the classification using labeled sparse data only. Compar-ing Ensemble-TrAdaBoost and LDA-Classification, it shows that transfer classification which deals features in the origi-nal space is better than the existing approach which trains one classifier by concatenating features from different spaces. For TCSST-No-Sampling, we observe that its performance is not stable. It works better than Ensemble-TrAdaBoost sometimes (and better than TCSST once), and worse than Ensemble-TrAdaBoost some other times. This might be caused by the noise in the large set of unlabeled external data. Therefore, the experimental results demonstrate the superiority of TCSST.

While Table 3 lists the accuracy results of the classifica-tion algorithms at a few settings of the two parameters: the percentage of labeled short &amp; sparse text and the percent-age of features deleted from each instance, Figure 2 plots the performance trend of each method along a wide range variations of the two parameters. Figure 2 ( a ) shows the results when the percentage of labeled short &amp; sparse text is varied from 5% to 90% while the percentage of features deleted is set to 90%. It can be observed that TCSST clearly and consistently outperforms other methods along the whole variation range. Figure 2 ( b ) shows the results when the percentage of features deleted from each instance is varied from 95% to 50%, while the percentage of labeled short &amp; sparse text data is 10%. Again, TCSST improves over all of the other four algorithms, while the improvement margin decreases when the data becomes less sparse.
 Table 4: Accuracy of TCSST and other methods on the NRM data Ens emble-TrAdaBoost 4 2.1053 % TCS ST-No-Sampling 4 4.5089 % The results of the algorithms on the NRM data is shown in Table 4. For Ensemble-TrAdaBoost, TCSST-No-Sampling and TCSST, the number of labeled data in the external do-main is set to be the same as the number of labeled data in the short &amp; sparse domain (i.e. 50%  X  189 = 95). For TCSST, in each round, twice of the number of labeled data in the external domain is selected to be included as training data (i.e. 95  X  2 = 190). The other settings of each classi-fier are same as in the previous experiment. Again, TCSST achieves the best performance on the real-world data.
We also conduct experiments to evaluate the influence of parameters on the performance of TCSST. In particular, we vary the two parameters: the percentage of labeled external data and the size of the unlabeled external data selected in each round. The results are shown in Figures 3 ( a ) and ( b ) respectively. The experiments were run on the benchmark data set where the sparse domain is generated by deleting 90% of features from instances. For both experiments, the percentage labeled short &amp; sparse text data is 20%.
For Figure 3 ( a ), we vary the percentage of labeled ex-ternal data from 5% to 50% and set the size of unlabeled external data selected in each round to be 20%. It can be observed the performance of TCSST will be better given more labeled external data. The results can be understood intuitively.

For Figure 3 ( b ), we vary the size of the unlabeled exter-nal data selected in each round from 5% to 50%, and set the percentage labeled external data to be 10%. We notice that, when the parameter increases, that is, more unlabeled external data is selected in each round, the performance of TCSST will decrease slightly. This is probably because when more unlabeled external data is selected in each round, some noise is introduced.
We further carry out experiments by varying the num-ber of MAX I TERATION in the Algorithm 1. That is, the number of iterative sampling process. We are interested in finding out how many iterations it takes before TCSST X  X  accuracy becomes stable.

We run the experiment on the benchmark data set where the sparse domain is generated by deleting 90% of features from each instance. The percentage of labeled short &amp; sparse text is 20%, the percentage of labeled external data is 10% and the size of unlabeled external data selected in each round is 20%. The experimental results are shown in Figure 4. It can be observed from the figure that TCSST becomes sta-ble approximately when the MAX ITE RATION reaches 20. Given the large set of unlabeled external data, we believe the result is quite promising. It indicates that our strategies of starting with a good initial set P (0) of unlabeled exter-nal data, and updating sampling probabilities based on the usefulness of unlabeled instances are effective. Fi gure 4: Iterations vs Accuracy of TCSST w.r.t.
 MAX ITE RATION.
TCCST X  X  computational performance is related to three factors: C -weak classifier X  X  complexity, N -number of it-eration for training classifier in TrAdaBoost, and M -num-ber of iteration for sampling in TCCST. The computational complexity of TCSST is O ( M N C ).
 Accurately the three factors above are not independent. For example, the weak classifier X  X  complexity C is usually related to the training data scale which influences the selec-tion of TrAdaBoost iteration number N . Furthermore, the N decides the external data sampling size ( S ) which empir-ically choose 10%  X  20% of N . Obviously, the S acts as the learning rate to induce the TCCST X  X  sampling iteration M .
In this paper, we proposed a novel method, TCSST, which classifies short &amp; sparse text by using transfer learning to exploit the external data. Motivated by the situation that there may not be enough labeled external data, TCSST leverages both labeled and unlabeled external data for trans-fer classification. In order to avoid the noise in the large set of unlabeled external data, TCSST uses an iterative sam-pling process to include only a subset of high quality unla-beled external data, with predicted labels, in each round into the training data, rather than including all unlabeled exter-nal instances into the training data in a batch. The experi-mental results demonstrate the effectiveness of the iterative sampling process and the strategies to allow the selection of external instances that are more likely to be useful. Com-pared with baseline as well as the existing methods, TCSST presents superior performance across benchmark and real-world data sets.

While TCSST borrows the framework of TrAdaBoost, which focuses on transferring the knowledge of instances, there are other types of inductive transfer learning, such as transfer-ring knowledge of feature representations, transferring knowl-edge of parameters, and transferring relational knowledge [11]. As ongoing research, we will study how to employ other types of inductive transfer learning frameworks to classify short &amp; sparse text using a small set of labeled external data and a large set of unlabeled external data, or using only a large set of unlabeled external data.
This work was supported, in part, by the Australian Re-search Council (ARC) Linkage Project under Grant No. LP120100566, Discovery Project under Grants No. DP1093762, and the NSW Department of Environment, Climate Change and Water (DECCW). [1] Natural resource manager capacity accessment data. [2] S. Banerjee, K. Ramanathan, and A. Gupta.
 [3] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring [4] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for [5] T. Evgeniou and M. Pontil. Regularized multi-task [6] E. Gabrilovich and S. Markovitch. Computing [7] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting [8] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. [9] K. Lang. Newsweeder: Learning to filter netnews. In [10] R. Mihalcea, C. Corley, and C. Strapparava.
 [11] S. J. Pan and Q. Yang. A survey on transfer learning. [12] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
 [13] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. [14] M. Sahami and T. D. Heilman. A web-based kernel [15] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. [16] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, [17] X. Sun, H. Wang, and Y. Yu. Towards effective short [18] W. Yih and C. Meek. Improving similarity measures [19] X. Zhu. Semi-supervised learning literature survey. In
