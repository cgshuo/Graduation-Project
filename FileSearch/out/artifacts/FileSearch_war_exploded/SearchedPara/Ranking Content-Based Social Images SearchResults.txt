 Social image hosting websi tes, e.g., Flickr, have recently become very popular. On these websites users can upload and tag their images for sharing them to others. This social tagging is similar to keyword annotation in traditional image retrieval systems. One difference is that keyword annotation requires several experts for annotating images. This requires too much time and labor if the image database is large. Social tagging does not have this problem because a large number of users can participate in tagging task. It is easier to construct a large database with tagged images. Another difference is that social tags are user-generated and folksonomy tags [1]. Compared with taxonomy keywords in keyword annotation which uses a number of specific fixed words, social tags have an open vocabulary in which the words are free and are neither exclusive nor hierarchical. This results in social tags having lots of noises.

Social tags have been proven to be effect ive for providing keyword-based im-age retrieval and widely used on social image hosting websites. It is regarded that textual information can naturally improve the results of keyword-based im-age retrieval, but whether social tags are b eneficial for improving content-based image retrieval (CBIR) has not been well investigated in previous work. CBIR has a long history and a large amount of research has gone into it, but its perfor-mance still needs to be improved for practical application. In CBIR, for a query image sample, systems search for content-based similar images from a specific multimedia database by image visual information. Since the query image does not include any textual information, the relationships between the query image and the textual information of other images in the database are hard to be eval-uated because of the well-known semantic gap problem. For example, for the query  X  X orse X  image in Fig. 1, it X  X  hard to know the relationship between it and the  X  X at X  tag of a  X  X at X  image in the database. The effectiveness of textual infor-mation, especially social tags, for improving content-based similar image results is unknown.

We observe that in content-based similar image results of a given query image and database, relevant images are relatively few while irrelevant images are many. There is a characteristic followed by image semantics that the image semantics of relevant images are alike while the image semantics of irrelevant images are diverse. For example, for a query  X  X orse X  image, its relevant images have alike  X  X orse X  concept while its irrelevant image have diverse concepts such as  X  X at X ,  X  X ird X , and so on. However in most cases content-based similar image results do not follow this characteristic. Fig. 1 shows a query image and its content-based similar images by SIFT feature [13]. These diverse similar images are regarded as  X  X elevant X  images in content-based similar image results by CBIR. This is one of the reasons that why the performance of CBIR is unsatisfactory. On the other hand, social tags sometimes follow this characteristic. In Fig. 1, the relevant images have alike tag sets including a  X  X orse X  tag, while the irrelevant images have diverse tag sets. It shows that social tags may be able to be used for improving content-based image search results.

We propose an unsupervised approach which automatically ranks the images in content-based similar image results. We construct an image-tag relationship graph model with both images and their tags as vertices, and using image sim-ilarity, tag co-occurrence and image-tag annotation relationships as edges. The approach propagates visual and textual information on the graph with a mutual reinforcement process. Fig. 1 gives a bri ef overview of the graph. It shows some of the content-based similar images and social tags on the graph. In the mu-tual reinforcement ranking process, the good tags (in red and bold) of relevant images contribute more scores on the graph; the bad tags of relevant images, and the good and bad tags (in blue and italic) of irrelevant images contribute less scores on the graph; the irrelevant images contribute less to their tags, while the relevant images contribute more. In other words, a high-ranked image is one to which many high-ranked tags point; a high-ranked tag is a tag that points to many high-ranked images. After several iterations, the relevant images can obtain higher rank scores.

The contributions of this paper are as follows.  X  We investigate whether social tags can be used for improving content-based  X  The mutual reinforcement process is not so novel and some approaches based
The remainder of this paper is organized as follows. In Section 2 we give a brief review of related work. In Sections 3, we propose our social image ranking approach. In Section 4, we report and discuss the experimental results, and present a summary and discuss future work in Section 5. There have been studies related to image ranking for keyword-based image re-trieval in unsupervised scenarios. Lin et al. [2] proposed an approach only based on text information. Several approaches [3,4,5] only based on visual information have also been proposed. The well-known visualrank approach proposed by Jing and Baluja [5] applies a random walk method for ranking images. We have dif-ferent goals than the above-mentioned studies based on keyword-based image retrieval. We concentrate on image ranking with social tags for content-based image retrieval. It has not been well investigated in previous work. The graph-based mutual reinforcement approach we propose efficiently uses both visual and textual information in the refining process, and performs better.

Relevance feedback (RF) has been widely used in image ranking in super-vised scenarios[6]. In early work some approaches [7,8] adjust the weights of different components of the queries or change the query representation to better suit the user X  X  information need. On the other hand many approaches use RF instances as training sets and include a offline learning process for learning a query-independent ranking model to cla ssify image search results into relevant and irrelevant images, e.g. the approaches [9,10] using support vector machines (SVM). We have different goals than the above-mentioned studies based on rele-vance feedback. We concentrate on automatically image ranking in unsupervised scenarios without user interactions. Our ranking task could be formulated as follows. For a given query image q ,the content-based image retrieval system computes the content-based similar image results A = { a 1 , ..., a n } from a specific multimedia database D .Let s iq as the similarity between q and a i . We regard A as the candidate image set and the social tags of images in A as the candidate tag set T = { t 1 , ..., t m } . We define T a i as the tag set of each image a i with the tag set T .

There is no user interaction information available for the ranking task in our unsupervised scenario. The approach should automatically gather visual and textual information to rank the content-based similar image results. We analyze the relationships among the images and social tags to construct our image-tag relationship model. We propose an approach with a mutual reinforcement process base on the characteristics of this graph model. 3.1 Image-Tag Relationship Model To leverage social image visual information as well as social tag textual infor-mation for ranking, we construct a graph model in Fig. 2 with candidate set A and T for analyzing the image-tag relationships. The vertices of the graph model denote social images which represent visual information and their tags which represent textual information. Note that query image q has no textual information.

The edges of the graph model denote the relationships among images and tags. There are three kinds of image-tag relationships: image-to-image relation-ship based on image similarity, tag-to-tag relationship based on tag co-occurrence to images, and image-to-tag annotation relationship. The first two kinds of re-lationships reflect the intra relationships among images or tags. The third one reflects the inter relations hip between images and tags. 3.2 Visual and Textual Descriptor To make use of visual and textual information in our approach, we convert them into visual and textual descriptors. The visual descriptors are based on image similarity. To compute image similarity, we use the following six types of low level features [13]: 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments and 500-D bag of words based on SIFT. The distance between image a i and a j on low level feature k is computed using a correlation distance d ( H ik , H jk ) defined as where H ik and H jk are feature vectors. N k isthesizeofthefeaturevector k .The image similarity s ij between two images with multi -feature is computed using a weighted sum. We use w k =1forany k in our work. It means that all low level features have same weights. This strategy has usually been used in existing work such as [11]. For each image a i in candidate image set A , we propose visual descriptor vd defined as vd i .

To leverage social tags information, different from some existing work in other topics such as [12] which use some pa ired tag co-occurrence measures tc xy for each pair of tags t x and t y ,weproposeasingletag co-occurrence measure tc x for each tag t x . This measure considers the local tag frequency in candidate tag set T as well as the global tag frequency in database D . It can evaluate how important tag t x is to current candidate tag set T in database D .Wepropose textual descriptor td defined as Here, | t x | T means the number of images in candidate tag set T that contain t x , | t | D means the number of images in database D that contain t frequency threshold for ignoring the noisy tags which have low frequency in T as well as in D and therefore have high value on tc x . 3.3 Social Image Ranking Following the image-tag annotation relationships in the graph model, we propa-gate the rank scores of images in A and tags in T along the links between images and tags. We observe a phenomenon that for an image a i , when propagating the rank scores from images to tags, if a i has a high rank score, its related tags will obtain higher rank scores. When propagating the rank scores is from tags to images, if the related tags of a i have high rank scores, a i will obtain a higher rank score. On the other hand, for a tag t x , it also has similar phenomenon. Therefore, we naturally co me to the following mutual reinforcement assump-tion: a high-ranked image for q is one to which many high-ranked tags point; a high-ranked tag for q is a tag that points to many high-ranked images. The iterative formulas for computing the rank scores are defined as follows: The iteration parameters  X  and  X  are damping factors. k is the number of iteration steps. We initiate Q 0 ( t ) of tags with textual descriptors and Q 0 ( a ) of images with visual descriptors. Q k (  X  ) is the normalized rank score of Q k (  X  ).
Content-based image similarity to the query image is an inherent property of a candidate image a i . The images which have high similarity can be regarded as more important on the graph. For a candidate tag t x , it is also similar. We therefore use visual descriptors and textual descriptors as the weights in the iterations. These weights represent the importance of these images and tags on the graph. 4.1 Experimental Settings The dataset we use for experiment is NUS-WIDE [13]. It is created by down-loading images and their social tags from social image hosting website Flickr. It has 269,648 images and about 425,000 unique original tags. For images, it pro-vides six types of low-level features extracted from the images, which we have introduced in section 3.2. For tags, the authors of this dataset set several rules to filter the original tag set. They delete the tags with too low frequency. The low frequency threshold is set to 100. They also remove the tags that does not exist in WordNet. At the end, they provide 5,018 unique tags. We keep this filtering in our experiment for the following reasons. It reduces the noises in the tag set. It also reduces the size of candidate tag set T and the number of links between images and tags, which can reduce the time cost in the ranking computation.
NUS-WIDE also provides image annotation ground-truth of 81 concepts for the entire dataset, but it doesn X  X  appoint query sample set and provide ground-truth for content-based image retrieval. We need to construct them by ourselves for our experiment. In our experiment, we randomly choose 100 images as a query image set from the entire dataset for our evaluation. Note that there is no textual information available for these queries. For each query, we rank the images in top-n content-based similar image results, n = 100. The images in content-based similar image results are labeled with five levels by human beings. The range of levels is from 0 (irrelevant) to 4 (relevant). The evaluation metric used in our experiment is Normalized Disc ounted Cumulative Gain (NDCG) [14]. NDCG is an effective metric often used in in formation retrieval for evaluating the rank results with relevance levels. It is defined as follows.
 r ( j ) is the relevance level of the image at rank j . Z k is a normalization con-stant and equal to the maximum DCG value that the top-k ranked images can reach, so that NDCG score is equal to 1 for the optimal results. We evaluate the performance with the average NDCG value of all query images. 4.2 Parameter Selections The proper local frequency threshold  X  of td is different for different approaches. According to Fig 3 which shows NDCG@100 value of the approaches with differ-ent  X  ,weset  X  = 3 for our approach. Note that the NDCG@100 of the content-based similar images results is 0.6168.

To select proper iter ation parameters  X  and  X  in our mutual reinforcement approach, we choose their candidate values by an interval of 0.1 in the range of [0 , 1] and obtain 121 pairs of candidate values. We run our approach with these pairs on the query image set and observe the performance on the NDCG@100 metric. According to Fig.4, we choose (  X ,  X  )as(0 . 0 , 0 . 4) in our experiments.
When  X  = 1, it means the iteration formula of an image rank score has degenerated into only depending on the visual descriptor. Because of using image similarity as the visual descriptor, the ranking result is equal to the content-based similar image results. The figure also shows that for any (  X ,  X  ) in the range, our approach performs not worse than content-based similar image results. 4.3 Experimental Results We compare our approach with three other approaches as well as with the content-based similar image results. Th e parameters selection and tuning is car-ried out for all these approaches. We compare the best results these approaches can generate. 1. Visual-based Approach (VisualRank): This approach does not use any social tag information. It is based on VisualRank but with modifications to make it appropriate for our scenario. VisualRank uses a random walk method on the image complete graph in which vertices are the candidate images, and uses content-based image similarity for computing the transition matrix. The iteration formula is as follows.
 We follow the settings in VisualRank and set damping factor  X  to 0.85. n is the size of the candidate image set A . We want to confirm that social tags are beneficial for ranking content-based similar image results and show that our approach performs better than VisualRank in the content-based social image ranking scenario. 2. Text-based Approach: To show that our mutual reinforcement process can use social tag information more effect ively, we design a text-based approach that uses social tag information but without a mutual reinforcement process. We compute the rank score of candidate image a i by using the following formula. Note that pure text-only-based approach in our scenario because the candidate image and tag set is generated by visual information. According to Fig 3, we set  X  =4. 3. Naive Mutual Reinf orcement Approach: The mutual reinforcement process is not so novel and some approaches based on it have been proposed in other areas. We design this naive mutual reinforcement approach to show that our proposed mutual reinforcement approach performs better than a naive one. There are two important differences between this naive one and our approach. One is that the iterative formulas of this naive one do not consider the importance of images and tags on the graph. The other is that we observe the performance among different parameters and sel ect them cautiously in our approach. | t | t descriptor and damping factors. The rule of parameters chosen here is to choose some intuitive parameters.

Fig. 5 illustrates the evaluation of NDCG@10, NDCG@20 metrics on rank-ing the top-100 images in content-based similar image results. Compared with content-based similar image results, all metrics are improved with our approach. Our approach performs best among all of the approaches here. Table 1 illustrates a social image ranking samples of our approach.

Our approach performs better than the visual-based approach in our content-based social image ranking scenario. Although VisualRank performs well in [5], the initial image results in that work are from keyword-based image retrieval, and the ranking is based on image similarity. In other words, the approach uses both visual and textual information to generate the final results. But in our scenario, since the initial image results are content-based, the visual-based approach can only use visual information. It illustrates that using social tag information for ranking the content-based image search results is beneficial .Furthermoreour approach also has better time complexity than VisualRank because our approach computes less links on the graph in real time iterations. In our experiments, on the average running time for all queries, VisualRank costs 0.076 seconds while ours costs 0.031 seconds.

Compared with the text-based approach which does not use our mutual rein-forcement process, our approach which is also based on textual information per-forms better. The text-based approach a lso performs better than VisualRank. It shows that social tags are beneficial for ranking content-based similar image results and our approach can use them more effectively.

Furthermore, our approach performs better than the naive mutual reinforce-ment approach which outperforms than VisualRank. It shows that a mutual reinforcement process is useful in our ra nking scenario, but a naive mutual re-inforcement approach without an optimized design still can not generate better rank results than the content-based similar image results from a statistical point. Our proposed mutual reinforcement approach can improve the content-based similar image results effectively. In this paper, we confirm that we can successfully use social tags to improve content-based social image search results. We propose an approach with a mutual reinforcement process using both visual and textual information on a image-tag relationship graph model. The experiments illustrate that our approach can reach the goals and performs better than other approaches. For future work, we will extend our work to a keyword-based image retrieval scenario.
 Acknowledgments. This work was partially supported by Grant-in-Aid for Scientific Research(B) (20300036) from Japan Society for the Promotion of Sci-ence (JSPS).

