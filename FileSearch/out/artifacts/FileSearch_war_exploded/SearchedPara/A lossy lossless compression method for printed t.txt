 ORIGINAL PAPER Hadi Grailu  X  Mojtaba Lotfizad  X  Hadi Sadoghi-Yazdi Abstract Pattern matching is the most widely used technique for the compression of printed bi-level text images. In some printed scripts, letters normally attach to each other, or some letters have a simple relation to each other, or there may be undesired touching characters. Detecting such situ-ations and exploiting them to reduce the library size, has a rather great effect on the compression ratio. In this paper, a lossy/lossless compression method for printed typeset bi-level text images is proposed for archiving purposes. For this, three techniques are proposed. First, the number of library prototypes is reduced by detecting and exploiting the mentioned situations. Second, a new effective encod-ing scheme is proposed for patterns and numbers. Third, three levels are proposed for lossy compression. Experimen-tal results show that the proposed method works better, as high as 1.4 X 3.3 times in lossy case and 1.2 X 2.7 times in lossless case at 300 dpi, than the best existing compression methods or standards.
 Keywords Pattern encoding  X  Library size reduction  X  Chain coding  X  Attaching characters  X  Pattern matching  X  Binary text image compression 1 Introduction There are many methods for compression of images of nat-ural scenes such as vector quantization [ 2 ], transform-based methods [ 3 ] and fractals [ 4 ]. These methods reduce the redun-dancy at the pixel level. However, text images have most of their redundancies at the symbol level. Hence such meth-ods have moderate or poor compression efficiency for text images [ 37 ]. This is particularly true when a text image contains a mixture of text, line art, pictures and graphic content.

Another reason for the inefficiency of some of the above mentioned methods is their lowpass behavior. Methods such as JPEG usually ignore the high frequency coefficients and thus smooth out the local sharp variations of images [ 40 ] and therefore act as a lowpass filter. In images of natural scenes, the information is distributed at almost all regions such as edges and inner regions of objects; thus this low-pass behavior has not much undesired effect on the recon-structed image quality. However, in text images, almost all the information exists at edges which have rather sharp vari-ations. Therefore, the lowpass behavior of the mentioned methods has a noticeable undesired effect on the quality of the reconstructed image due to smoothing out the sharp vari-ations at edges. Even the modified versions of JPEG such as JPEG2000 family have disadvantages such as ringing and blurring effects in high compression ratios [ 33 , 37 ]. Such undesired effects lower the text image quality.

Yet another difference between text and images of natural scenes relates to their need to different resolutions in space and value [ 37 ]. Text images, contrary to image of natural scenes, need higher resolution in space than the color val-ues of pixels [ 34  X  37 ]. Most existing color text image com-pression methods, such as Digipaper [ 39 ], DjVu [ 40 ] and LuraDocument [ 41 ], first decompose the input image into three or more layers each of which contains specific infor-mation such as text color, text location and graphics [ 34  X  38 ]. Then each layer is encoded using a specific encoding method. Usually the non-text layers are down-sampled because of the above-mentioned difference. The mask layer is a bi-level image which specifies the textual regions and is mostly com-pressed with the pattern-matching-based methods or stan-dards such as jb2 and JBIG2 [ 17 , 25  X  27 ].

The existing dedicated bi-level text image compression methods, such as jb2 , work on the basis of pattern matching (PM) technique and have noticeably better compression per-formance than that of the general image compression meth-ods such as JPEG or even the modified ones such as JPEG2000 . Figure 1 a shows a sample bi-level mixed Farsi  X  X nglish text image with spatial resolution of 300 dpi. A large scaled part of the resulting compressed image by JPEG2000 and jb2 compression methods is shown in Fig. 1 b, c, respectively. We have adjusted both methods to compress the original image approximately 30 times. As can be seen the PM-based method, i.e. jb2 , results noticeably better image quality because, in contrast to JPEG2000 , it has not any ring-ing effect and has well preserved the edges quality.
In PM-based methods, first all patterns (or marks) in the input image are extracted. Each pattern is a set of connected (or neighboring) black pixels. Then, all similar patterns are grouped together and assigned a prototype which is usually the most similar pattern to each of them. Each pattern is not necessarily fully similar to its corresponding prototype and is somewhat different. The difference image is called  X  X esid-ual pattern X  or  X  X rror map X , and the corresponding pixels are called  X  X esidual pixels X  or  X  X rror pixels X  [ 10 , 31 ]. The set of all prototypes is called the library. Each prototype in the library has an index number. The image patterns are extracted successively and compared with the prototypes to find a match. If a match occurs, the corresponding index of the matching prototype, the relative position of current pat-tern with respect to the previously processed pattern, and the corresponding residual pattern are computed and saved; oth-erwise, the current pattern is added to the library as a new prototype and a new index number is assigned. Finally, the library, all number sequences, and probably the whole or parts of the residual patterns are compressed and encoded. Since the residual patterns and even the residual pixels inside any residual pattern have not equal importance [ 10 , 13 ], it is reasonable to omit the less important ones and encode the rest for lossy compression. This technique preserves the quality of the reconstructed image as much as possible.

In the Farsi/Arabic script, contrary to the printed Latin script, letters normally attach to each other and produce many different patterns. A relatively high number of these patterns are fully/partially subset of, or similar to some others. The attaching letters are not limited to only Farsi/Arabic script. In some Latin font faces such as Brush Script MT, Edwardian Script ITC, French Script MT and Lucida Handwriting ,some letters normally attach to each other. In addition, degraded printed Latin text images usually contain undesired touching characters due to reasons such as re-sampling. The touch-ing/attaching characters are more important for the Farsi/ Arabic script because they occur very frequently. The com-pression performance of the PM technique is decreased in all above-mentioned situations. To quantify this undesired effect, we define two measures which evaluate the compres-sion performance of the PM technique. The first measure is the average number of occurrence of prototypes which is defined as the number of the image patterns divided by the number of the library prototypes. The second measure is the average area reduction which is defined as the ratio of the total area of the image patterns to the total area of the library proto-types. Figure 19 c, f shows two sample Farsi and English text images, respectively. The corresponding libraries are shown in Fig. 2 a, b. For these text images and the corresponding libraries, the average number of occurrence of prototypes is 2.82 and 22.17 for Farsi and English ones, respectively. Also the corresponding average area reduction measure is 3.3 and 20.2 for Farsi and English ones, respectively. Both of these measures show that compression performance of the PM technique for Farsi text image is noticeably lower than the English one due to the attaching letters.

The encoder and compressor are the most efficient units in existing modern bi-level text image compression meth-ods. The former is usually implemented by the entropy-based encoding methods especially the adaptive context-based arithmetic coding which is used in the JBIG1 standard [ 26 ]. The latter is usually implemented by the PM technique which is used in SPM [ 26 ] and mgtic [ 1 , 31 ] methods and the JBIG2 standard [ 17 ]. In Appendix A it is shown that the compres-sion performance of the entropy-based encoding methods, which assign variable X  X ength codes in inverse proportion to the symbols probabilities, is lowered for previously men-tioned situations especially for Farsi/Arabic text images.
To our best knowledge [ 28  X  32 ], existing text image com-pression methods or standards have not used the full/partial similarity of prototypes for increasing the compression ratio. Also the idea of combining the boundary description meth-ods, such as chain coding, with the run length and arithmetic coding techniques has not being used for pattern encoding.
In this paper, a method of printed typeset bi-level text image compression is presented on the basis of an improved pattern matching technique for archiving and storage pur-poses which demand for high compression ratios, relatively low reconstruction time and reasonable image quality. The improved PM technique uses the proposed template match-ing which is roughly insensitive to spot noise. The proposed method works well for all kinds of bi-level text images, but it has the best compression performance for those text images which contain a relatively high number of touching/attaching characters, especially for Farsi/Arabic text images and those printed Latin text images which contain special font faces mentioned before.

The proposed compression method uses three ideas. The first proposed idea is to improve the compression efficiency of the encoder unit for encoding the patterns and numbers. The proposed pattern encoder has two operation modes of chain coding and soft pattern matching , each of which uses a different encoding scheme. The former is used for com-pact patterns and the latter is used for sparse ones. In the sparse patterns, contrary to the compact ones, the pixels are distributed random-like and usually there are many small holes inside them. In addition the compact patterns usually have smoother boundaries. Example sparse and compact pat-terns are shown in Fig. 3 . Switching between these modes is done by computing a compactness measure which is equal to the ratio of the pattern area to its chain coding effec-tive length. We define the chain coding effective length of a bi-level pattern as the length of its run-length-coded chain code sequence. For encoding the number sequences we have modified and used the multi-symbol QM-coder [ 26 ].
In the soft pattern matching mode, we use the soft pat-tern matching technique [ 26 ] which uses the adaptive con-text-based arithmetic coding with a context which includes pixels from both the current and the reconstructed pattern. In the chain coding mode, we encode the run-length-coded chain code sequence by an adaptive context-based arithmetic coding technique. This mode usually has better compression efficiency than the other, because it in fact contains three successive compression stages. The first stage includes com-puting the chain code sequence. As we know, describing a compact bi-level pattern by its boundaries, instead of the inner regions, reduce the redundancy because it does not consider any extra bit for describing the inner regions [ 47 ]. The second and third compression stages include the run-length and 1D adaptive arithmetic coding techniques, respec-tively.

The second proposed idea is to improve the compression performance of the PM technique. For this, we, at first, use an improved template matching then we reduce the library size by detecting and omitting the fully/partially similar proto-types or those prototypes which have simple relation to each other. Figure 4 a shows an example of the full similarity for two sample Farsi sub-words (or patterns), which in that the whole smaller prototype is repeated in the other. Figure 4 b shows two types of the partial similarity. In the first (left) one, parts of a prototype are repeated in different parts of another prototype. In the second (right) one, parts of a prototype are repeated in different parts of two or more prototypes. Partial similarity occurs more frequently than the full similarity in Farsi/Arabic text images.
 The third proposed idea is related to lossy compression. The proposed compression method has three independent levels for lossy compression, each of which further increases the compression ratio. We may use only a number of these levels depending on the desired compression ratio. The first level includes employing proper chain code domain proce-dures such as omitting the small patterns and holes, omitting the inner holes of characters and smoothing the boundaries of the patterns. The second level includes selective pixel rever-sal technique which reverses the color of poorly predicted pixels in context-based adaptive arithmetic coding [ 26 ]. The third level includes using the proposed method of prioritizing the residual patterns for encoding.
 The proposed ideas are not limited to only the Farsi and Arabic text images. For example, they are applicable to printed Latin text images due to four reasons; first, almost in all Latin font faces, some letters are subset of, or have simple relation to another one(s). For example, Fig. 5 a, b shows some pairs of such letters for the font face of Times New Roman in uppercase and lowercase, respectively. In Fig. 5 a each of the left letters is subset of the right one. In Fig. 5 b two groups of letters are shown in pairs. Each of the left letters in the first (left) group is subset of the right one. The second (right) group shows the pairs of letters which have simple relation to each other. For example the two upper pairs of letters have a vertical symmetry axis and the lower ones have a symmetry point, i.e. can be obtained by 180  X  rotation of each other. Second, as it is shown in Fig. 5 c, in Latin font faces such as Brush Script MT , some letters normally attach to each other and thus cause similar situation as in the Farsi/Arabic script. Third, many Latin text images such as Fig. 19 f or the corre-sponding library in Fig. 2 b, have some touching characters which cause the same situation. Fourth, the proposed pattern encoder is useful for any compact sub-word (or pattern) inde-pendent of the related script.
 The remainder of this paper is organized as follows. Section 2 reviews the existing methods for printed bi-level text image compression. Section 3 and its subsections explain the proposed compression method and its stages including the proposed template matching, the proposed method for reducing the library size, the proposed method for encoding patterns and number sequences, and the proposed method for lossy compression. Section 4 shows the experimental results and Sect. 5 concludes the paper. 2 Review of the existing PM-based compression methods Pattern matching is the most widely used technique for compression of bi-level printed text images and almost all efficient existing text image compression methods or stan-dards work on the basis of this technique. These methods are grouped into lossy and lossless ones. The PM technique was originally introduced in [ 5 ] which did not encode the residual patterns and thus was a lossy compression method. Also it did not introduce any compression method for prototypes. In [ 6 ] a method named combined symbol matching (CSM) was introduced for improving the performance of the mentioned method and left the infrequently used patterns and encoded the residual patterns using the two dimensional run-length coding similar to the G 3 and G 4 standards. The proposed method in [ 14 ] used a preloaded library with alphabet charac-ters with some different fonts. The pattern matching and sub-stitution (PMS) method [ 15 ] could also handle graphics in the text images. In this method, large bodies of image graphics were divided into some smaller parts, each of which is con-sidered as a pattern. The weighted AND-NOT (WAN) [ 16 ] was proposed as an improvement to the CSM method which introduced better pattern matcher. One of the authors further stated elsewhere that the performance of CSM, PMS and WAN methods decreases when the symbol size is decreased or the quantization noise is increased [ 7 ]. Thus he proposed the combined size independent strategy (CSIS) [ 7 ], which was similar to the PMS method but tried to operate inde-pendently of the symbol size by performing the symbol size normalization. In [ 46 ], a multistage lossy method was pro-posed to which, after some preprocessing based on a his-togram analysis and skew correction, a PM with a rigorous comparison of patterns was applied; thus some of similar patterns may be considered as dissimilar due to the quanti-zation noise. Therefore at the next step, such patterns were found and merged by a multistage structural clustering. This method used the Q-coder for encoding the prototypes and arithmetic coding for encoding the number sequences. In [ 8 ] a multi-dimensional-signal lossy compression based on multi-scale recurrent patterns, referred to as multi-dimen-sional multi-scale parser (MMP), was proposed which was applicable to all kinds of binary images including the textual ones. Although this method did not employ the PM tech-nique but it used a dynamic dictionary, thus it is mentioned in this section. In this method a multi-dimensional signal was recursively segmented into variable X  X ength vectors, and each segment was encoded using expansions and contrac-tions of vectors in a dictionary. The dictionary was updated while the data was being encoded using the concatenations of expanded and contracted versions of previously encoded vectors. The mgtic method [ 1 , 31 ] was based on the pattern matching technique and for encoding the residual patterns it encoded the original image by context-based arithmetic coding which in that the reconstructed image, using the pro-totypes, was used as the context. Also it used a specific two-level coding scheme [ 44 ] for encoding the prototypes in order to have a larger template for context-based arithmetic cod-ing without high learning cost. In addition the numbers were encoded using the PPMC [ 45 ] technique. In [ 48 ] an entropy-based pattern matching was proposed in which the amount of uncertainty or entropy between patterns was used for the matching process. The compression for document image sys-tem (CDIS) [ 49 ] was a PM-based lossy compression that coded the pattern sequences and positions in the image by coding the pattern positions hierarchically; first it divided the text image into blocks and automatically formatted the text within each block by estimating each pattern X  X  position. Then it transmitted the position error (the difference between the pattern X  X  actual and estimated positions) for each pattern in reduced position. CDIS reproduced document images in nearly-lossless mode. In the SPM method [ 26 ] the adaptive context-based arithmetic coding technique was employed for encoding the residual patterns which in that a combination of the corresponding library prototype and pattern was used as the context. This technique was named soft pattern match-ing [ 26 ]. It also used the adaptive context-based arithmetic coding method for encoding the prototypes similar to JBIG1 standard. The SPM method performed some further process-ing for lossy compression. The most efficient process was the selective pixel reversal technique in which the color of poorly predicted pixels was reversed if some conditions had been satisfied. It also used the multi-symbol QM-Coder [ 26 ] for encoding the numbers. The SPM method had rather bet-ter compression performance than mgtic [ 26 ]. It became a proposal in the JBIG2 standard draft [ 50 ]byISO.In[ 51 ]a PM-based lossy compression method was proposed for Chi-nese text images which reduced the library size by using a property of Chinese scripts. Unlike the ASCII set, which only has 256 characters, the Chinese character set consists of a large number of characters, components and radicals which cause the Chinese script to be more complex than other scripts. The strokes which constitute Chinese charac-ters are relatively simple, thus a morphology-based method was proposed to decompose and recompose Chinese charac-ters for pattern matching. In addition, some small-size com-ponents can be inserted into the blank spaces of large-size components. By using this property, the mentioned method reduced the library size. In [ 52 ] a wavelet-domain PM-based compression method was proposed for gray-scale Ottoman script images. Ottoman script is similar to the Farsi/Arabic script. All procedures were performed in the LL sub-band image of transformed input image. In the pattern matching part of this method, the extracted patterns were compared to every location in the image and the matched parts were removed from the image; therefore, this method was able to detect only the full similarities not the partial ones. The par-tially similar patterns constitute noticeable part of Farsi and Arabic text images.

The PM technique uses the template matching for comparison of patterns. There are some methods for template matching of grayscale images, some of which oper-ate in the gray-scale domain, whereas others operate in the bi-level domain. In the first group, some methods make use of the normalized cross correlation function [ 19 , 20 ] and neu-ral networks [ 21 ]. Some methods can determine the degree of rotation of the template for achieving the best match [ 22 ] or can determine the best relative scale [ 23 ] for the template matching operation. In the second group, the bi-level tem-plate matching technique is used which typically uses the relative number of common pixels [ 23 ]ortheerrormap[ 1 ].
Almost all PM-based methods for bi-level text image com-pression, use the weighted error map [ 1 , 31 ]. In the CSM method, first the error map was computed by XORing the two patterns. Then a weighted sum of the map elements were computed and compared with a threshold. The larger was the cluster size of the error map pixels, the larger was the weight [ 6 ]. The WAN method distinguished between black-to-white errors and white-to-black ones. It also used the perimeter of patterns to determine if a match should be attempted. The PMS method rejected a match if any position in the error map was found to have four or more neighbors set to 1. The SPM and mgtic methods used the Hamming distance which is the count of the number of mismatched pixels between the patterns when they are aligned according to the geomet-ric centers of their bounding boxes [ 1 , 26 ] but this distance metric is somewhat sensitive to the spot noise.

In the lossy compression case, all or some of the residual patterns or residual pixels may be ignored and not encoded. Thus it is reasonable to omit the less important ones by pri-oritizing them. Furthermore, prioritizing the residual pixels is useful for progressive compression in which the aim is to encode and transmit the more important residual pixels ear-lier, in order to reduce the necessary waiting time for display-ing the reconstructed image on the screen. In [ 9 , 10 ] a method of prioritizing the residual pixels for progressive encoding was proposed based on the distance of these pixels from their nearest edges.
 The early standards such as ITU-T Group3 ( G 3 ) and Group 4 ( G 4 ) facsimile standards did not use the PM tech-nique and operated on the basis of variants of run-length coding and Huffman codes [ 1 , 26 , 31 ]. In these standards the run time was more important than the compression ratio. The ISO/IEC JBIG1 standard for lossless bi-level image compres-sion used the adaptive context-based arithmetic coding using a special 10-pixel template for defining the context [ 43 ] and had better results than the mentioned standards [ 26 , 31 ]but still did not use the PM technique [ 26 ]. The JBIG2 standard is the best existing PM-based compression standard which has been introduced in the last decade and has better compres-sion performance than all previously mentioned standards [ 17 , 25 , 27 ]. This standard has not specified the encoder but the most efficient existing JBIG2 -based methods, such as jb2 , have used the soft pattern matching technique. The jb2 method is a bi-level text image lossy compression which has been used in the DjVu document image compression method [ 40 ].

Yet none of the mentioned methods or standards has used the library size reduction technique for improving the com-pression efficiency by removing the fully and partially sim-ilar patterns. Also boundary description methods have not being used in conjunction with run-length and arithmetic coding techniques for encoding the patterns. In this paper a method of lossy/lossless compression of printed typeset bi-level text images based on an improved pattern match-ing is proposed which works well for all kinds of scripts, but has the best performance for those scripts which con-tain a relatively high number of touching/attaching letters especially the Farsi/Arabic script. The attaching/touching let-ters produce many different prototypes some of which have simple relation or full/partial similarity to each other. In the proposed method, such prototypes are detected, identified by index vectors and then removed from the library. Then, the remaining prototypes are encoded by the proposed pattern encoder. For encoding the number sequences we have mod-ified and used the multi-symbol QM-coder [ 26 ].

The proposed method has three different levels for lossy compression, each of which further increases the compres-sion ratio. The first level includes employing some chain code domain procedures which aim to improve or preserve the image quality as much as possible in addition to increase the compression ratio. The second level includes the selec-tive pixel reversal technique. The third level includes using the proposed method of prioritizing the residual patterns for encoding which aims to preserve the image quality as much as possible in addition to increase the compression ratio. These three levels cause the proposed method to be more flexible than the existing methods such as mgtic and SPM , because by using all or a number of them, different compression ratios are achievable. 3 The proposed method Stages of the proposed compression method for bi-level printed text images are illustrated in the block diagram of Fig. 6 . At the first stage, the PM technique is applied to the input image using the proposed template matching technique in order to produce the library completely. For this, the image is scanned and processed in a top-to-bottom serpentine order. After obtaining the complete library, the proposed library size reduction technique is performed at the second stage. In this technique, the computed library prototypes are sorted according to their width and then each prototype is compared with all wider ones in order to detect those prototypes which have full/partial similarity, or simple relation, to each other. Then such prototypes are removed from the library after the necessary information has been saved.

If we desire a lossy compression, we use the third stage of Fig. 6 , otherwise, we skip it. The third stage has three lossy compression levels all or some of which may be used. These levels include selective pixel reversal technique, chain code domain processing and using the proposed method of prioritizing the residual patterns for encoding.

Finally at the last stage, the number sequences, proto-types, and the whole or part of the residual patterns are com-pressed and encoded. In this paper, for encoding the number sequences, we have modified and used the multi-symbol QM coder. We use the proposed pattern encoder for encoding the prototypes and residual patterns.

The following sections describe each block of the diagram of Fig. 6 , respectively. The Sect. 3.1 describes the PM with the proposed template matching technique. The proposed pro-cedure for reducing the library size is explained and illus-trated in Sect. 3.2 . The Sect. 3.3 describes the proposed three lossy compression levels, and finally the proposed encoding scheme for numbers and patterns is explained and illustrated in Sect. 3.4 . 3.1 Pattern matching using the proposed template matching For the PM part of the proposed compression method, the input image is scanned and the patterns are extracted and processed in a serpentine order from top to bottom in order to have smaller relative coordinate numbers as much as pos-sible and hence higher compression ratio. Each pattern is compared with the library prototypes to find a match. The library is empty at the beginning thus, the first encountered pattern is always added to the library. For any pattern, if a match occurs, the corresponding index of the matching pro-totype, the relative position of current pattern with respect to the previously processed pattern and the corresponding residual pattern are computed and saved; otherwise, the cur-rent pattern is added to the library as a new prototype and a new index number is assigned.

When comparing any two patterns, the probability of a mismatch in the edge regions is higher than other regions due to reasons such as the quantization noise. This proba-bility is decreased exponentially as we move away from the edges [ 13 ]. Therefore in an error map (or residual pattern), the residual pixels have not equal importance. The nearer are the residual pixels to the edges, the less is their impor-tance and therefore, should be assigned a lower weight in computing the corresponding difference metric. We can con-sider the weight of the residual pixels as an exponential func-tion of their distances from the corresponding nearest edges. For reducing the run time, we have proposed a successive edge detection procedure. After performing each edge detec-tion stage we further move away from the edges; thus the corresponding edge pixels take higher weight.

In the proposed template matching method, before com-paring any two patterns, we compute a weight matrix W for each pattern. To this end, we perform successive edge detec-tion on the pattern or the result of the previous stage. Any pixel whose sum of values of its eight neighbors is nonzero and &lt; 8 is defined as an edge pixel. Figure 7 shows a pattern and the results of three successive edge detection stages. In this figure, the edge pixels are colored as black. The weight matrix elements are zero at the beginning. At each edge detection stage, those weight matrix elements which correspond to the edge pixels and are zero, take the value B k where k is number of the current edge detection stage and B is the base number which is best determined experimentally. The result of the first ( k = 1), second ( k and third ( k = 3) edge detection stages are shown in Fig. 7 b X  X , respectively. After performing N stages and computing the N  X  X h stage weight matrix, W N , the final weight matrix W for the current pattern is computed as W ( i , j ) = W N
After performing each stage, the dimension of the result-ing pattern is further increased. The number of edge detec-tion stages, i.e. N , is chosen such that the dimension of the resulting pattern is approximately N ED percent larger than the original pattern dimension. The value of the parameter N ED is best determined experimentally.

Now, in order to compare any two patterns A and B , we should compute two quantities. The first one is the differ-ence metric. For computing this metric, we at first compute the corresponding residual pattern (or error map), E A , B aligning the patterns according to the geometric centers of their bounding boxes and using the XOR operation. Then we compute the difference metric as Diff A , B = where W A and W B are the weight matrices for patterns A and B , respectively.

The second quantity is the threshold value against which the difference metric of relation (2) should be compared. Because any two similar patterns mainly differ in the edge regions, we can define an adaptive threshold value as follows Threshold = max ( T A , T B ) (3) where values of T A and T B are computed as T T where k T is a parameter which is best determined experi-mentally. Its value is larger than 1 due to the spot noise. As can be seen, this threshold can be computed after computing the W 1 matrix.

Finally, the match or mismatch of the patterns is deter-mined as if Diff A , B  X  Threshold  X  Match , if Diff A , B &gt; Threshold  X  Mismatch .
In our implementation, we only compare those patterns and prototypes which their dimension difference is less than N DD percent of the smaller one. The value of the parameter N DD is best determined experimentally. After applying the PM technique to the input text image, the library is obtained completely. Figure 8 shows a sample bi-level Farsi text image and the corresponding library. In this example some proto-types, like the ones shown in Fig. 8 c, are partially similar. 3.2 Reducing the library size After performing the PM technique to obtain the library, we reduce the number of prototypes by using the proposed library size reduction technique. This technique is based on detecting, saving necessary information, and removing those prototypes which have a simple relation or full/partial simi-larity to each other. In the decoder, first, the original library is reconstructed using the reduced library and the correspond-ing information. The text image is then reconstructed using the resulting library.

In order to reduce the number of prototypes (or library size), we process the prototypes as shown in Fig. 9 .Atthe first stage, we sort the prototypes with respect to their widths and modify the corresponding indices in library and all pre-viously saved library indices for image patterns. For exam-ple, the library of Fig. 8 b is modified as shown in Fig. 10 a. The corresponding modified indices are also shown in this figure. Then, those prototypes which their dimension dif-ference is less than 5% are processed to find if they have a simple relation such as having a symmetry axis. In this paper, two kinds of simple relations are used which include rotation and vertical/horizontal symmetry axis. If any two prototypes have a simple relation to each other then the kind of the relation is properly signaled by a specific code and one of them is removed from the library. For example, in the sorted library of Fig. 10 a the pair-wise prototypes corresponding to the indices (3,4) and (5,6) have vertical and horizontal symmetry axes, respectively. Also the corresponding proto-types to the indices (8,9) can be produced by 180  X  rotation of each other. Thus, the kind of the relation and the prototypes indices are properly encoded using an index vector such as [
Idx 1 , R , Idx 2 ] , and one of the prototypes is then removed from the library. This index vector means that the prototypes with the library indices of Idx 1 and Idx 2 have the relation specified by the predetermined code R . The resulting library is then like the Fig. 10 b.

At the second stage of Fig. 9 , each prototype of the remain-ing library, which we call as the primary prototype , is com-pared to all wider ones, each of which is called the secondary prototype . Any prototype may be used as a primary in a time, and as a secondary in another time. In each comparison, we shift the corresponding geometrical centers of the primary and secondary prototypes in all directions by a predetermined step equal to Step pixels. The value of this parameter is best determined experimentally. The number of shift steps is cho-sen such that the primary prototype can be compared with all parts of the secondary prototype; thus this value is different for each secondary prototype. In each shift, we compute the corresponding error map and error signal. The length of the error signal is equal to the width of the primary prototype and its value at each point is equal to the number of error pix-els in the corresponding column of the error map. Figure 11 shows two prototypes and three possible error maps and the corresponding error signals.

We could define and use the second group of error signals, each of which has a length equal to the height of the primary prototype and its value at each point is equal to the number of error pixels in the corresponding row of the error map. Defining this group of error signals is useful for detecting those prototypes which are fully/partially similar in vertical direction such as the English letters  X  X  X  and  X  X  X  or the letters  X  X  X  and  X  X  X . Almost all similarities in Farsi and Arabic text images occur in horizontal direction; thus, we may not use this group of error signals for these text images.
At the third stage of Fig. 9 , we use the error signals to determine and find if parts of the current primary prototype are fully/partially similar to parts of some secondary proto-types. There may be primary prototypes which parts of them have not repeated in any other prototype. Thus we can only partially remove these primary prototypes.

We could perform the procedure of the third stage either after computing all error signals for each primary prototype or after the second stage has been completely performed. We prefer the former because it consumes noticeably lower mem-ory. Thus, we perform the third stage procedure for every primary prototype.

For each part of the current primary prototype to which a part of a secondary prototype is similar, as shown in Fig. 12 , the corresponding values of the proper error signal are small. By  X  X roper error signal X  we mean the error signal which corresponds to the best matched prototypes. Therefore, the error signals indicate which part(s) of the primary prototype is (are) similar to which part(s) of which secondary proto-type(s). Figure 12 a shows a primary prototype and Fig. 12 b shows two secondary prototypes. As can be seen from Fig. 12 c,d, we can break the primary prototype into two parts each of which is similar to a part of one of the secondary pro-totypes. Thus we can save the necessary information for every part as an index vector [ idx 1 , C 1 , C 2 , Idx 2 , then remove the whole primary prototype from the library. This index vector means that the current part, which starts at the column C 1 and stops at the column C 2 of the prototype with the library index of Idx 1 , is similar to the part which starts at the column C 3 and stops at the column C 4 of the prototype with the library index of Idx 2 . Also we compute and attach the corresponding residual patterns to each other and save the result as the corresponding residual pattern for the current primary prototype.

Corresponding to each current primary prototype there are some error signals D i , j , k ( n ), n = 1 ,..., M , P , j , k = X  S ,..., S , each of which corresponds to a specific shift value. The subscript i is related to the i  X  X h secondary prototype, the parameter P is the number of prototypes, the subscripts j and k correspond to the shift operation in which the geometrical centers of the primary and secondary proto-types have distances equal to j  X  Step and k  X  Step pixels in vertical and horizontal directions, respectively, the parameter S is the maximum number of shift steps and the parameter M is the width of the current primary prototype. The value of the parameter S is such that the primary prototype can be compared to all parts of the secondary prototype; thus it is different for each secondary prototype.

The quantitative procedure of detecting the repeated parts of current primary prototype is illustrated in Fig. 13 .Atthe first stage, we compute the moving average of the computed error signals using a sliding window of a length 2 L as The parameter L is best determined experimentally.
Then at the second stage, the sequence of library indices of the most similar secondary prototypes and also the mini-mum value of the corresponding error signal at each library index are computed respectively as I ( n ) = arg min i MA i , j , k ( n ), for all j , k , (7) and E ( n ) = min MA i , j , k ( n ), for all i , j , k . (8)
The sequence value I ( n ) contains the library index of the most similar secondary prototype for the column n of the current primary prototype. For example if I = [33333555 5 5] it means that the current primary prototype could be con-sidered as the combination of two parts which the first part is mostly similar to a part of the prototype with the library index of 3 and the second part is mostly similar to a part of the prototype with the library index of 5.

At the last stage of Fig. 13 , we compute the substitution efficiency (SE) measure for each specific library index value in the sequence I and compare it with the predetermined threshold T SE , to determine whether it is better to remove the corresponding part(s) of that library index in the primary prototype or not. If so, then we save the corresponding index vector and residual pattern and remove the corresponding part(s). The substitution efficiency measure for the library index i , SE i , is defined as SE where: L i = the average length of the connected subsequences in I which have the value i , which have the value i , to the value i in the sequence I.

An example for computing the SE measure for an assumed primary prototype and the corresponding sequences of I = [ 5555222555533333 ] and E =[ 883242513542011131518 ] is shown in Table 1 . The left first column shows the library indices which exist in the sequence I . The second column shows the number of connected subsequences in the sequence I whichhavethevalue i . The last column shows the com-puted values of SE 2 , SE 3 and SE 5 for this example. As can be seen, that part of the primary prototype which corresponds to the library index of 2, has the highest SE i value in spite of having the smallest width. The more is the value of the SE i measure, the more valuable is to omit the corresponding part(s) from the current primary prototype.

The value of the threshold T SE depends on the type of the encoders selected for encoding the residual patterns and prototypes. If the prototype encoder is more efficient than the residual pattern encoder, it is better to choose larger val-ues. Otherwise, it is better to choose smaller ones because the smaller is the value of the threshold, the larger is the number of matches and therefore the larger is the number of residual patterns. We chose the threshold T SE experimentally by using some training images from our database. For this we obtained the compression ratio curve versus threshold values for each training image, while other parameters had values as in Table 2 , and chose that threshold value which maximized the compression ratio. Finally, we computed the average value of such chosen threshold values and chose it as the value of T SE .

After detecting and saving the necessary information of repeated fully/partially similar parts, they are removed from the corresponding primary prototypes. Thus, some proto-types are completely removed and some others are partially removed. For example the result of applying the library size reduction technique to the library of Fig. 10 bisshownin Fig. 14 a which in that the prototypes corresponding to the library indices of 2, 7, 11, 13, 14, 17, 20 and 21 are com-pletely removed and the prototypes corresponding to the library indices of 19, 22, 23 and 24 are partially removed. The partially removed prototypes contain some separate parts such as the prototype with the library index of 24 in Fig. 14 a. For improving the compression efficiency we attach the sep-arate parts to each other and produce single patterns for each partially removed prototype. The result of attaching the sep-arate parts for the library of Fig. 14 a is shown in Fig. 14 b. No more information is needed to be saved (in order to sig-nal the decoder how to separate the attached parts) because enough information had been included in the index vectors before; thus, attaching the separate parts does not confuse the decoder to produce the original library prototypes.
Finally, the reduced library, sequences of numbers, and the whole or part of the residual patterns are encoded. If we desire lossy compression we may only encode some of the residual patterns. It should be noted that two groups of resid-ual patterns have been produced till now. The first group is produced at the PM stage of the proposed compression method and the second group is produced at the library size reduction stage. The second group has more importance because it affects the library prototypes which frequently occur in the image. Therefore, in the lossy compression, we may deal with both groups equivalently or consider higher weight for the second group. We have selected the latter. For this we have used two different thresholds T RP1 and T RP2 for encoding the first and second group, respectively, which have the relation T RP2 &lt; T RP1 to each other. More details are given in the next section. 3.3 Lossy compression The proposed compression method has three levels for lossy compression each of which further increases the compression ratio. In order to achieve higher compression ratios, we may need to use more levels. The first level includes using a num-ber of chain code domain procedures which include omission of small patterns and holes, omission of inner holes of pat-terns and smoothing the boundaries of image patterns. We only omit the holes which their perimeter is less than a pre-determined threshold T ih . It should be noted that omission of inner holes of patterns, as illustrated in the experimen-tal results, has not much undesired effect on the legibility of Farsi and Arabic text images but it may lower the legibility of English text images somewhat. For smoothing the boundaries of the patterns, we have used a chain-code-based non-linear smoothing filter which detects the outgrowths/indentations and flatten/fill them.

The second level includes selective pixel reversal which reverses the color of poorly predicted pixels in context-based adaptive arithmetic coding [ 26 ]. This level is applicable only in the soft pattern matching mode of the proposed pattern encoder for sparse patterns.

The third level includes using the proposed method of prioritizing the residual patterns for encoding. It aims to pre-serve the image quality as much as possible, in addition to increase the compression ratio. At this level, we prioritize the connected components of each residual pattern for encod-ing by computing the corresponding value of compactness measure (CM) which is defined in the next section. If the CM measure of an assumed connected component is below a threshold T RP , we remove it from the corresponding residual pattern. As mentioned in the previous section, we have con-sidered two different thresholds for encoding two groups of residual patterns. These threshold values, i.e. T RP1 and T are best determined experimentally.

The three proposed lossy compression levels cause the proposed method to be more flexible than the existing methods because by using all or a combination of them, dif-ferent compression ratios are achievable. 3.4 Encoding the number sequences, residual patterns By using the serpentine scanning order to extract and pro-cess the image patterns we can obtain small relative row and column coordinates as much as possible. For example the rel-ative row and column coordinates of the patterns of the first ten lines of Farsi text image in Fig 19 careshowninFig. 15 . As can be seen, these sequences have roughly zero average value, but locally they have a non-zero bias. For improving the compression performance of the multi-symbol QM coder, we can remove this bias by subtracting any new arrived rel-ative coordinate from the average value of N QM previously encoded numbers in current sequence. The value of N should be smaller than the average number of patterns in text lines. We have chosen the 1% of the image width for this parameter. This subtraction is for producing smaller numbers and hence, higher compression ratio; because smaller are the numbers smaller is the length of the output bit-stream of multi-symbol QM-coder [ 25 ]. This technique has improved the compression ratio of multi-symbol QM-coder by 8% in average.

In lossy compression, we may use another technique before the above modification to further increase the com-pression ratio. For this, we divide the relative position num-bers computed in the PM stage of the proposed method by 2 and round the results. This task produces further smaller numbers and higher compression ratio, but at the other hand, produces visual distortion. In this paper, we have not used this technique.

For encoding the patterns, including the prototypes and residual patterns, we use the proposed pattern encoder which has two operation modes of chain coding and soft pattern matching . In the proposed encoder, the diagram of which is shown in Fig. 16 , for encoding the patterns we at first, compute the compactness measure CM which is equal to the ratio of the pattern area to its chain coding effective length. We define the chain coding effective length, as the length of run-length-coded chain code sequence. For run-length cod-ing of the chain code sequence we replace any sequence of N
RL successively occurred specific chain code with an extra symbol like the number 4 followed by the repeated chain code. If a chain code has occurred 2 N RL times, we use the symbol 4 twice followed by the repeated chain code and so on. In lossy compression if we desire to perform chain code domain processing we should at first perform the desired processing and then compute the chain code effective length and the compactness measure. For computing the chain code sequence we use the 4-chain code using the codes illustrated in Fig. 17 c.

If the computed CM measure for the input pattern is below the threshold T C , we use the soft pattern matching technique [ 26 ] to encode the pattern; otherwise, we encode the com-puted run-length-coded chain code sequence using the 1D adaptive context-based arithmetic coding. We have used the last N AC symbols in the chain code sequence to define the context. The threshold T C and the parameter N AC are best determined experimentally.

In computing the chain code sequence of any pattern in the chain coding mode we should save the start pixel coordinates as well so that we can reconstruct the original pattern. Some patterns, like the one shown in Fig. 17 a, have inner holes; in such situations, we should compute the chain code sequences of the inner boundaries and the corresponding start pixels as well. The inner boundaries can be obtained by edge detec-tion, like Fig. 17 b. All start pixel coordinates are computed relative to the last computed one. 4 Experimental results The text image database used in this work has approximately 800 text images which mostly contain printed typeset bi-level Farsi and Arabic text images with somewhat dif-ferent spatial resolutions from 100 to 600 dpi and relatively good quality. By  X  X ood quality X  we mean that serious degra-dations which change the shape of patterns non-uniformly and thus produce more different prototypes do not exist. The most important degradations include text line curvature and high density spot noise. The former causes to produce more different prototypes and the latter lowers the template matching efficiency in providing correct matches. Generally, these degradations lower the compression performance of PM-based compression methods, but we expect it has less undesired effects on the proposed method due to the proposed template matching and library size reduction techniques.
We have classified the images into six classes for better discussion on the compression performance. These classes include mostly graphics, mixed Farsi-graphics, mostly Farsi, mixed Farsi X  X rabic, mostly Arabic and English text images. It should be noted that Arabic and Farsi scripts are similar but some Arabic text images contain some guide marks for better pronunciation. A sample word containing such guide marks is shown in Fig. 18 . The mostly Arabic class of images con-tains such images. Other Arabic text images are included in the mostly Farsi class. For better discussion and illustration of the compression performance of the proposed method, we have chosen a sample text image from each class such that the corresponding lossless compression ratio is near the average compression ratio of the images in the corresponding class. These sample images are shown in Fig. 19 .

Table 2 shows the numerical values of the parameters used in the proposed method, which are chosen experimentally. The proposed compression method is not much sensitive to the incorrect settings of the threshold values. For example, if Eq. (5) fails to provide a correct match or the technique based on the substitution efficiency measure fails due to the incorrect setting of the threshold T SE , the performance is not much decreased because if the Eq. (5) fails in providing a correct match two situations may occur: 1. If two similar patterns are mistakenly recognized as dissimilar ones, then one of them is removed at the library size reduction stage and thus, the compression ratio is not changed noticeably. But if at the library reduction stage these prototypes are not recognized as similar due to the incorrect setting of the threshold T SE , then we should consume more bit budget for encoding the extra prototype. This is the worst case and the compression ratio is decreased somewhat. 2. If two dissimilar patterns are mistakenly recognized as similar ones, then the library size is reduced but at other hand an extra residual pattern is produced; thus, we have a trade off. For determining whether this trade off improves the com-pression ratio or not, we should analysis the encoder(s) used for prototypes and residual patterns. If the prototype encoder is more effective than the residual pattern encoder, then the compression ratio is decreased; otherwise, it is increased. In the proposed method, a same pattern encoder is used for encoding both residual patterns and prototypes. It has two operation modes depending on whether the current pattern is sparse or compact. In current study case, the produced residual pattern is most likely encoded the same way as the prototype. Therefore the compression ratio is not changed noticeably.

The proposed bi-level text image compression method has four main parts including library size reduction technique, proposed encoder and lossy/lossless compression. In this section, the compression performance of each part is eval-uated separately or in a proper combination with others.
We can evaluate the performance of the library size reduction technique by computing each of two quantitative measures including the average number of occurrence of pro-totypes and the average area reduction which were defined before. For example, after applying the library size reduction technique to the libraries of Farsi and English text images of Fig. 19 c, f, the resulting reduced libraries will be as shown in Fig. 20 . The original libraries are shown in Fig. 2 . As can be seen by comparing each library before and after using the mentioned technique, the proposed technique has notice-able effect on reducing the number of prototypes. The aver-age number of occurrence of prototypes is 12.29 and 39.9 for Farsi and English reduced libraries. In other words, the proposed library size reduction technique has improved the average number of occurrence of prototypes by 335.7 and 70% for the Farsi and English libraries, respectively. It is obvious that the proposed technique has improved the PM performance for both text images but it has noticeably higher effect on the Farsi library than the English one.

The average area reduction measure is 16.9 and 29.2 for the Farsi and English reduced size libraries, respectively. Therefore, the proposed library size reduction technique has improved this ratio by 412.1 and 44.6% for the Farsi and English libraries, respectively. Again, the higher effect of the proposed technique on the Farsi library is deduced.
For evaluating the compression performance of the pro-posed encoder, we have compared its compression ratio to that of non-progressive JBIG1 [ 26 ] by directly applying both of them to the six text images of Fig. 19 . Table 3 shows the compression performance (in bit per pixel) of two methods for these images and different spatial resolutions of 100, 200, 300 and 600 dpi. Also in this table the relative run time of these methods is compared which in that the JBIG1 run time is chosen as the reference. In this table, for each combina-tion of the image number, dpi, and the compression method, two numbers are shown. The first (upper) number shows the compression performance in bit per pixel ( bpp ) and the sec-ond (lower) number shows the relative run time in our imple-mentation. As can be seen from this table, the average run time of the proposed encoder is lower than that of the JBIG1 encoder mainly due to the low computational complexity of chain coding [ 47 ] and relatively small context used in the chain coding mode for 1D context-based arithmetic cod-ing. The compression ratio curves of these methods for each image is shown in Fig. 21 versus the four spatial resolu-tions 100, 200, 300 and 600 dpi, respectively. As can be seen from these curves and has been shown in Appendix B, the compression ratio usually increases by increasing the spa-tial resolution. The relative compression ratio curves of the proposed encoder to the JBIG1 versus the six image num-bers are shown in Fig. 22 for the four spatial resolutions. By inspecting these compression performance curves we can say that generally in equal number of patterns, the more is the number of line-drawings and large compact patterns, the more is the compression ratio of the proposed method. Also in equal total area of the patterns, the smaller is the number of patterns, the more is the compression ratio. As can be seen for Farsi/Arabic text images the compression ratio is higher than the English one because the former have larger compact patterns with smaller number of patterns.

The proposed text image compression method has both lossy and lossless modes. The performance of the lossless version of the proposed compression method is compared with that of the lossless JBIG2 standard in Table 4 . Since JBIG2 standard has not exactly specified the encoder unit [ 17 , 25 ], we have used the SPM encoder [ 26 ] for implemen-tation. Table 4 shows the compression performance (in bit per pixel) of the methods for the six text images and differ-ent spatial resolutions of 100, 200, 300 and 600 dpi. Also in this table the relative run time of these methods is compared which in that the lossless JBIG2 run time is chosen as the ref-erence. The units in Table 4 are the same as those of Table 3 . As can be seen from this table, the average run time of the proposed compression method is relatively higher than the lossless JBIG2 mainly due to the library size reduction tech-nique. The compression ratio curves of the mentioned meth-ods for each of the six images is also shown in Fig. 23 versus the four spatial resolutions of 100, 200, 300 and 600 dpi, respectively.

The relative compression ratio curves of the proposed loss-less compression method to the lossless JBIG2 are shown in Fig. 24 versus the six image numbers and for the four spa-tial resolutions. As can be seen the best compression ratio of the proposed method occurs for the mostly Farsi or Arabic text images. For English text image, the compression ratio of the proposed method is still better than that of the lossless JBIG2 . Also the compression ratio for the image class of mixed Farsi X  X rabic is not as much as the image class of mostly Farsi or mostly Arabic mainly due to the script and font style/size variation inside the image which causes to pro-duce more different prototypes with smaller average number of occurrence of prototypes. Although there are not much repetitive patterns in the first and second image, the corre-sponding compression ratios are as high as or more than that of the mixed Farsi X  X rabic images mainly due to the exis-tence of line-drawings. In addition, we have the best lossless compression ratio for the mostly Farsi or Arabic text images as high as 2.4 times and the lowest one for the English text image as high as 1.3 times at 300 dpi.

For evaluating the compression performance of the lossy version of the proposed method, we have employed all pro-posed techniques in Sect. 3.3 except the technique based on dividing the numbers by 2. For the first level of the pro-posed lossy levels, small holes or patterns are removed and the boundaries of patterns are smoothed using a non-linear chain-code-based filter. Also the relatively small inner holes of sub-words are removed. Chain code domain smoothing has noticeable effect on improving the sub-words quality and legibility especially those ones which contain straight bound-aries such as the English letters E , F , L , I and Farsi/Arabic letters , and . The results of applying some lossy proce-dures are shown in Fig. 25 at 300 dpi. Generally by decreas-ing the spatial resolution, the reconstructed image quality is lowered. The advantage of the proposed lossy procedures is that they usually improve or preserve the image quality and legibility especially for Farsi/Arabic scripts. For example removing the inner holes of sub-words in Farsi/Arabic text images preserves their legibility and quality well, as shown in Fig. 25 h, but this is not true that much for the English text image, as shown in Fig. 25 j, because the ratio of the average area of the inner holes to the average area of the correspond-ing sub-words for Farsi/Arabic scripts is smaller than that of printed Latin scripts. The compression performance of the lossy version of the proposed method is compared with a lossy version of the JBIG2 , named jb2 , in Table 5 .The jb2 method is part of the DjVu compound document image compression method [ 40 ]. As can be seen the correspond-ing run times of the methods are not included in this table because their platforms were different. For employing the jb2 method we used DjVu Solo software from LizardTech company.

The compression ratio curves of two methods for each image is shown in Fig. 26 versus the four spatial resolutions 100, 200, 300 and 600 dpi, respectively. Also the relative compression ratio curves of the lossy version of the pro-posed method to the lossy jb2 versus the six image numbers are shown in Fig. 27 for the four spatial resolutions. It can be seen that roughly similar results to the lossless case are obtained. For example the best compression ratio of the pro-posed method occurs for the mostly Farsi or Arabic text images. For English text image, the proposed method com-pression ratio is still better than that of the lossy jb2 . As can be seen, again we have the best lossy compression performance for mostly Farsi or Arabic text images as high as three times and the lowest one for the English text image as high as 1.6 times at 300 dpi. 5 Conclusions In this paper, a lossy/lossless PM-based compression method for printed typeset bi-level text images was presented for archiving and storage purposes which demand for high compression ratio, relatively low reconstruction time and reasonable image quality. Our proposed method works best for bi-level Farsi and Arabic text images, but still works better than the best existing compression methods or stan-dards for other scripts such as printed Latin scripts. In the Farsi/Arabic script, contrary to the printed Latin script, letters normally attach to each other and produce various patterns. Hence some patterns are fully/partially subsets of some others. Detecting such situations and exploiting them to reduce the number of library prototypes has a great effect on the compression efficiency, because the number of occur-rence of such situations in Farsi and Arabic text images is very high. Such attaching characters exist in some other scripts and/or font faces. In addition undesired touching char-acters exist in some text images such as Latin ones. Thus, the proposed compression method is not limited to only Farsi and Arabic text images.
 In the proposed method, three techniques were employed. First, the number of library prototypes was reduced by detect-ing and omitting the fully/partially similar prototypes or those prototypes which have a simple relation to each other. Sec-ond, a new effective pattern encoding scheme was proposed for encoding all kinds of patterns. The proposed encoding scheme has two operation modes of chain coding and soft pattern matching each of which is used for different kinds of patterns. The former is used for compact patterns and the latter is used for sparse patterns. Switching between these modes is done by a measure of input pattern which is equal to the ratio of the pattern area to the effective length of its chain code sequence. In chain coding mode, it uses a combination of chain coding, run-length and 1D adaptive context-based arithmetic coding techniques and in the soft pattern match-ing mode, it employs the soft pattern matching technique. In addition, we have modified and used the multi-symbol QM-coder for encoding the sequences of numbers.

Third, three different levels are proposed for lossy com-pression each of which further increases the compression ratio. All or some of these levels may be used in order to achieve different compression ratios. The first level includes applying some chain code domain procedures such as omis-sion of small patterns and holes, omission of inner holes of letters, and smoothing the boundaries of the patterns. The second level includes using the selective pixel reversal tech-nique, and the third level includes using the proposed method of prioritizing the residual patterns for encoding. These three levels cause the proposed method to be more flexible than the existing compression methods or standards because different compression ratios are achievable.

Experimental results showed that the proposed method works better, as high as 1.4 X 3.3 times in lossy case and 1.2 X 2.7 times in lossless case at 300 dpi, than the best existing compression methods or standards. The maximum compres-sion ratios were achieved for Farsi/Arabic scripts. Appendix A In this Appendix, we aim to show that the compression ratio of the entropy-based compression methods, such as arithme-tic coding technique, is decreased, in average, for Farsi and Arabic text images relative to printed Latin text images.
The entropy measure for each set of symbols S 1 to S N with probabilities p 1 to p N is the average information content of that set and is defined as [ 42 ]: E = X 
The entropy unit is bit . Any entropy-based binary image compression method, such as arithmetic coding, tries to reduce the redundancy of the image so that the number of bits consumed for the image is near to the image entropy as much as possible. In other words the entropy measure determines the maximum achievable compression ratio of the compres-sion methods. As the entropy of an image is increased, the maximum achievable compression ratio is decreased.
In a binary image each black or white pixel may be consid-ered as a symbol and thus we may use the arithmetic coding to encode the image using the probabilities of these two sym-bols. But as is done in the context-based arithmetic coding [ 42 , 45 ], it is better to condition the pixels probabilities to a proper context in which the pixels have occurred, because it improves the prediction precision of symbols probabilities. By using this scheme, we in fact use the redundancy between the neighboring pixels to further compress the binary image. The larger is the context size, the more is the prediction preci-sion of probabilities but the more is the computational com-plexity, therefore a trade off occurs. In addition, in order to reduce the encoding time, the input image is considered as a causal signal and by arriving every new symbol the corresponding output bit-stream is generated and then the corresponding probability is updated. This scheme is referred to as adaptive context-based arithmetic coding [ 42 , 45 ] which is used for example in the JBIG1 standard wherein a three line 10-pixel causal template is used to define the context [ 26 ]. In this case there are 1,024 different contexts or equivalently 1,024 different symbols.

In most printed Latin binary text images, in which the characters are separate, changing the location of each char-acter to produce different words does not change the shape of characters. Thus if we assume that the number of charac-ters in a text image is constant and the context size is small enough, then the frequency of symbols approximately is not changed. In other words the symbols entropy and thus the maximum achievable compression ratio are not changed so much. For example the symbols entropy of the sample binary images shown in Fig. 28 a, b is roughly the same.

Another property of such text images, which is related to the pattern matching compression efficiency, is that the corresponding library prototypes mostly correspond to char-acters. Occurrence of new words in the text does not add new prototypes but only changes the numbers of occurrences, or frequencies of these prototypes.

But if reconsider the above situations for Farsi/Arabic text images the result is different, because in such scripts let-ters usually attach to each other, the corresponding shapes are changed and even extra symbols may be produced. For example in Table 6 some Farsi letters and some correspond-ing words are shown. As can be seen the shape of characters is changed and extra symbols may be produced which mostly contain dots. Thus if we assume the number of letters in a Farsi/Arabic text image is constant and we change the loca-tion of letters in order to produce different words and sen-tences, then the frequency of contexts and consequently the symbols entropy and the maximum achievable compression ratio most likely are changed. In addition, occurrence of new words usually increases the number of library prototypes.
The entropy of symbols in Farsi/Arabic text images is more than that of printed Latin text images in average because the shape of characters in the latter is not changed thus usu-ally some contexts frequently occur and some others rarely occur. But the shape of letters in the former is changed and thus all contexts more or less occur. In other words the vari-ance of symbols probabilities in printed Latin text images is more than that of Farsi/Arabic text images and the number of existing symbols in each typical Farsi/Arabic text image, in average, is more than that of a typical printed Latin text image. Therefore we should examine the effects of probabil-ity variance and number of symbols on the entropy measure separately.

Since the entropy value of some symbols, with a con-stant number, is maximized when the corresponding proba-bilities are equal, thus it can be deduced that as the variance of symbols probabilities is increased, the entropy measure is decreased. Thus the average value of the entropy measure for Farsi/Arabic text images is more than that of Latin text images. For examining the effect of number of symbols on the entropy measure we at first consider a set of N equal-probability symbols. In this case the entropy relation (10) simplifies to log 2 ( N ) which is increased by increasing the number of symbols N . Then we consider a set of N symbols and assign them random probabilities with the total sum of unity and compute the entropy measure. For each specific value of N we perform this procedure 100 times and con-sider the average value of computed entropies as the entropy value for N symbols. A sample averaged-entropy curve ver-sus the parameter N is shown in Fig. 29 . As can be seen, the entropy value generally is increased as the number of symbols is increased.

Form previous paragraph we conclude that the entropy of symbols in Farsi/Arabic text images, in average, is more than that of printed Latin text images. Therefore the compres-sion ratio of the entropy-based compression methods, such as arithmetic coding technique, is decreased in average for Farsi/Arabic text images relative to printed Latin text images. Appendix B Usually by increasing the spatial resolution of binary images, the compression ratio of the entropy-based com-pression methods is increased. For better discussion, we can consider a set of some symbols S 1 to S N with probabilities p 1 to p N and frequencies of C 1 to C N , respectively.
We h ave p
If we assume that the contexts dimensions are small enough then increasing the spatial resolution in fact increases the contexts frequency. In other words the number of occur-rence of the symbols S 1 to S N is increased to k  X  C 1 to k respectively where k is a constant which is larger than 1. So the new probabilities are p
As can be seen, the symbols probabilities have not being changed and consequently the entropy value is increased roughly linearly because the number of symbols is increased linearly, but at the other hand the total number of image pixels is increased in a quadratic manner by increasing the spatial resolution. Thus the compression ratio which is reversely proportional to the ratio of total number of bits consumed for the compressed image to the total number of image pixels, is increased.
 References
