 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA We solve the nuclear norm optimization problem: where f ( X ) is a twice differentiable convex function,  X  &gt; 0 is the regularization parameter, and k X k  X  P known as the trace norm). The nuclear norm regularization promotes a low rank solution, which is a key idea that can be applied to many applications such as recommender sys-tems (Cand ` es &amp; Recht, 2009), dimension reduction in mul-tivariate regression (Yuan et al., 2007), multi-task learning (Argyriou et al., 2008), and multi-label learning (Cabral et al., 2011). The nuclear norm regularization is an ` 1 regu-larization of the singular values of X , and it therefore pro-motes a low rank solution. It is proved that the underly-ing low rank solution can be recovered by solving (1) un-der certain conditions (Cand  X  es &amp; Tao, 2009; Recht et al., 2010).
 There has been much work on developing efficient nu-clear norm minimization solvers (Ji &amp; Ye, 2009; Mazumder et al., 2010; Jaggi &amp; Sulovsky, 2010; Avron et al., 2012), but most of them still fail to solve large-scale problems. (Avron et al., 2012) reports that on the Netflix dataset, one of the state-of-the-art Stochastic Sub-Gradient Descent (SSGD) algorithms cannot achieve 0.95 test Root Mean Square Error (RMSE) in one day, while other non-convex methods (ALS) methods can achieve 0.93 test RMSE in a couple of hours. Similar scalability problems also arise in multi-task learning (Dudik et al., 2012) and multivariate re-gression (Giraud, 2011). This scalability deficiency makes nuclear norm regularization less applicable for large-scale real world problems, despite its strong theoretical guaran-tees.
 In contrast, recently ` 1 -regularized solvers have been well-developed and scaled to ultra-large-scale problems with a trillion parameters (Hsieh et al., 2013). The key tech-nique used in the fastest ` 1 -minimization solvers is to de-tect a small subset of active variables and focus on opti-mizing these (Olsen et al., 2012; Hsieh et al., 2011; Yuan et al., 2012). Since the nuclear norm is equivalent to the ` norm on singular values, it is natural to ask the follow-ing question: can we identify a small active subspace and efficiently minimize the reduced-sized nuclear norm mini-mization problem? In this paper, we propose two new methods to solve large-scale nuclear norm regularized problems using ac-tive subspace selection . Our methods alternate between two phases: firstly we identify the active row and column subspaces ; secondly, within the subspace, the problem can be reduced to a smaller k  X  k nuclear norm minimization problem. We then describe two efficient solvers to solve the reduced problem, alternating minimization method and cone projection Newton descent method , to minimize the sub-problem. We show that the active subspace will never change in a neighborhood of the global minimum, and in practice the subspace converges in very few iterations (10 in our experiments), thus our methods are extremely fast compared to other state-of-the-art methods.
 Applications. The nuclear norm minimization can be ap-plied to many applications where a low rank solution is pre-ferred, and each application uses a different loss function f ( X ) in (1). For example:  X  Matrix Completion : Given a partially observed low  X  Multivariate Regression : Given a data matrix A  X  In the experiments we will show the effectiveness of our algorithm on the two problems described above, and our method can also be extended to solve other nuclear norm regularized problems, including multi-task learning and clustering with missing labels. In this section we give some interesting background infor-mation related to nuclear norm minimization.
 If X has the singular value decomposition X = U  X  V &gt; , then we define the shrinkage operator by where the operation a + = max { 0 ,a } is applied element-wise to the matrix. Also, we denote the pseudo-inverse of X by X  X  .
 It is worthwhile noting that the regression problem (3) with a nuclear norm regularizer can be solved analytically when Theorem 1 (Exact Solvability) . Let A  X  R m  X  m , B,X  X  R m  X  n then if AA &gt; and BB &gt; commutes the minimum of F ( X ) is achieved for X  X  = ( A  X  ) 2 S  X  ( A &gt; B ) . The theorem can be proved by verifying that F ( X ) is con-vex and that 0 is a subgradient at X  X  . A similar result is shown in Theorem 4 of (Yu &amp; Schuurmans, 2011). A par-ticularly important case of Theorem 1 is A = I when the solution simplifies to X  X  = S  X  ( B ) , and this special case has been proved and used in many previous papers (Cai et al., 2010; Mazumder et al., 2010).
 We now describe two first order methods that converge to the global minimum of the matrix completion and regres-sion problems respectively.
 Theorem 2. The iteration X k +1 = S  X  ( X   X  ( A ) +  X 
 X  ( X k )) , where  X  the global minimum of (2).
 See the Appendix 6.1 in the supplement for a sketch of the proof.
 Theorem 3. The iteration X k +1 = S  X /c ( X k  X  1 c  X  f ( X converge to the global minimum of (3) if cI A &gt; A = f ( X ) .
 See the Appendix 6.2 in the supplement for a sketch of the proof. It should be noted that the iteration in Theorem 2 is a special case of Theorem 3 with c = 1 . The vector S  X /c ( X  X  1 c f 0 ( X )) is the proximal gradient (Toh &amp; Yun, 2010; Ji &amp; Ye, 2009) and we shall make use of it later when we explore more efficient methods. Our proposed method iterates between two phases. At each iteration, we identify the active row and column sub-spaces U A ,V A using the power method. Within the ac-tive subspace, the original problem can be reduced to a k  X  k nuclear norm minimization problem with k min( m,n ) . We then propose efficient ways to minimize the sub-problem. Our framework can be summarized in Algorithm 1. 3.1. Active subspace selection In this section we introduce our active subspace selection strategy. Note that any matrix X  X  R m  X  n can be repre-sented as a sum of rank one matrices: when span { u i } i = R m and span { v j } j = R n . Since the solution is low rank,  X  will be sparse. Therefore, the goal Algorithm 1: Our proposed framework Input : regularization parameter  X  , initial X = U  X  V &gt; Output : The optimal solution X  X   X  R m  X  n 1 for iter = 1 , 2 ,... do 2 [  X  U,  X   X  ,  X  V ]  X  ApproxSVD ( X  X  X  X  f ( X )) ; 3 U G  X  X   X  u i |  X   X  ii &gt;  X  } ,V G  X  X   X  v i |  X   X  ii 4 U A  X  QR ([ U G ,U ]) ,V A  X  QR ([ V G ,V ]) ; 5 S  X  argmin S f ( U A SV &gt; A ) +  X  k S k  X  ; 6 [ U S ,  X  ,V S ]  X  ThinSVD ( S ) ; 7 U  X  U A U S , V  X  V A V S , X  X  U  X  V &gt; ; 8 end of the active set selection phase is to eliminate rank-one subspaces which are likely to have zero weights in the final solution, and then focus on the remaining rank one sub-spaces, which we call the active subspace .
 If the SVD of X is U  X  V T , it is shown in (Watson, 1992) that the sub-differential of k X k  X  is  X  k X k  X  = { UV &gt; + W : U &gt; W = 0 ,WV = 0 , k W k 2 Assume U  X  ,V  X  are orthogonal subspace complements to U,V , then the sub-differential with respect to  X  ij can be written  X   X  ij F ( X )  X  [ u &gt; i  X  f ( X ) v j  X   X , u &gt; i  X  f ( X ) v if u i  X  U  X  or v j  X  V  X  (equivalently, u &gt; i X v j = 0 ). We the sub-differential. If we want to update X by a rank one factor  X  uv &gt; , the optimal step size is and it will have the solution  X   X  = 0 whenever 0 is in the sub-differential set. We therefore define the fixed rank one subspace as Therefore, all the rank one subspaces in F have zero weight in the current solution X , and is not likely to change from zero to nonzero. However, fixed subspace elements can still be activated in the next iteration of our algorithm, when we compute a new proximal gradient. We define the active rank one subspace as which is the complementary set of F . We focus on updat-ing X on active rank one subspaces and fix the weights for all the fixed subspaces to be zero. In the following we propose a simple way to eliminate the fixed subspaces: Theorem 4. Assume X = U  X  V &gt; is the reduced SVD of X ( U  X  R m  X  k , V  X  R n  X  k and  X  has positive diagonal val-ues), and S  X  ( X  X  X  X  f ( X )) = U G  X  G V &gt; G (also a reduced SVD). Let U A be an orthonormal basis of span ( U,U G ) , V be an orthonormal basis of span ( V,V G ) , and U  X  A ,V  X  orthonormal complements to U A ,V A , then The proof is in Appendix 6.3. Given U A ,V A , we can form the column basis  X  U = [ U A ,U  X  A ] and row basis  X  [ V
A ,V  X  A ] , and then re-parameterize X by X = where Y  X  R m  X  n . Now solving the original problem is equivalent to solving min Y F (  X  UY  X  V &gt; ) . Assume both U and V A have k columns, then by Theorem 4 only the top k  X  k submatrix of Y belong to the active set and all other elements of Y belong to the fixed set . So after eliminating the fixed set , the problem can be reduced to the following sub-problem: argmin to where  X  f ( S ) = f ( U A SV &gt; A ) , k min( n,m ) . Since the variable in (8) is a small k  X  k matrix, solving (8) is often much cheaper than solving the original problem. We will discuss how to solve (8) in Section 3.3. In addition, we will show in Theorem 7 that U A ,V A are exactly equal to the column and row subspace of the optimal solution in a neighborhood of X  X  , and in this case solving (8) once will achieve the global optimum.
 Empirically we also observe that the active subsapce U
A ,V A converges to the final space quickly. In Figure 1a we compute the similarity between ( U A ) t ( U A at the t -th iteration) and U  X  , which is measured by the smallest sin-gular value of ( U  X  ) &gt; ( U A ) t . If U  X   X  span (( U value will be 1. We can see the value converges to 1 in ten steps. Moreover, the rank of our solutions X 1 ,X 2 ,... does not blow up when the final solution has a small rank, as shown in Figure 1b.
 Relationship to other greedy methods. The family of  X  X reedy algorithms X , including GECO (Shalev-Shwartz et al., 2011), Lifted-CD (Dudik et al., 2012), and GCG (Zhang et al., 2012), have been proposed and achieved state-of-the-art performance on solving nuclear norm regu-larized problems. The greedy algorithms also consider the coordinate decomposition (5), but they construct the active subspace in a greedy manner: at each iteration, they add the top singular vector pair ( u , v ) to the active subspace, and Figure 1: Experiments on the ml100k dataset demonstrates the then solve the problem within this subspace. The draw-back of these greedy algorithms is the difficulty in remov-ing unimportant basis elements. In contrast, our algorithm selects the rank-k subspace anew at each iteration, and thus rapidly removes irrelevant basis elements and converges faster as in the experiments. 3.2. Step 1: Computing active subspace In this section, we discuss an efficient way to compute the active subspace U A ,V A . Note again that U A is the or-thonormal basis of span ( U,U G ) . U is the column basis of the current solution X , and this is always maintained during the process since we always maintain the low rank factor form USV &gt; of X .
 To compute U G and V G , we have to compute S  X  ( X  X   X  f ( X )) , which requires the top k singular vectors of X  X   X  f ( X ) (assuming we know the rank k ). This is the bottle-neck in other proximal gradient based methods (Mazumder cussed in (Mazumder et al., 2010), since X is a low rank matrix and  X  f ( X ) usually have a special form, the ma-trix vector product ( X  X  X  X  f ( X )) v can often be computed efficiently. For example,  X  f ( X ) =  X   X  ( X  X  A ) in ma-trix completion problems with square loss, and  X  f ( X ) = A &gt; AX  X  A &gt; B in multivariate regression problems. There-fore, (Mazumder et al., 2010) apply a Lanczos algorithm to compute the top k eigenvectors in Soft-Impute.
 In our solver, we compute S  X  ( X  X   X  f ( X )) faster than Soft-Impute by use of the following innovations: 1. We observe that S  X  ( X  X  X  X  f ( X )) will not change a 2. In our solver, this step is only used to identify the sub-Also, it is easy to increase the target rank k in the power method. Assume we already compute the top k 1 eigenvec-tors of S  X  ( X  X   X  f ( X )) and we find the k 1 -th singular value is larger than  X  , then we have to increase k 1 . Sup-pose we increase k 1 to k 2 ; we can keep the top k gular vectors, and then run the power method to compute the k 1 + 1 ,...,k 2 -th singular vectors. During the process, we just need to make sure those k 1 + 1 ,...,k 2 -th vectors are orthogonal to the top k 1 singular vectors. Therefore, the time complexity of each step of the power method is O ( |  X  | k 2 ) to computing A u for each new vector u  X  R that is added, and O (( m + n ) k 2 2 ) for orthogonalization. The algorithm is summarized in Algorithm 2.
 Algorithm 2: Power method (ApproxSVD in Algorithm 1) Input : Input matrix A , rank k , initial R  X  R n  X  k Output : Approximate SVD A  X  U  X  V &gt; 1 Y  X  AR ; 2 Q  X  QR ( Y ) ; 3 for t = 1 ,...,T max do 4 Y  X  A ( A &gt; Q ) ; 5 Q  X  QR ( Y ) ; 6 end 7 B 8 [  X 
U,  X  ,V ] = SVD ( B ) ; 9 U = Q  X  U ; 3.3. Step 2: Solving the k  X  k Sub-Problem After selecting the subspace U A ,V A , we need to solve (8). Since the variable S in (8) is a small k  X  k matrix, com-puting the SVD of S is cheap. Moreover, the gradient and Hessian vector product of  X  f ( S ) can also be computed effi-ciently.
 For general matrix-scalar functions, using the chain rule we have  X   X  f ( S ) = and if we define  X  2 f ( X ) to be a R mn  X  mn matrix with Usually by utilizing the structure of  X  f ( Y ) and  X  2 f ( Y ) , combined with ( U  X  V ) vec( D ) = vec( V DU &gt; ) , both gra-dient and Hessian can be computed efficiently. The follow-ing are some examples:  X  Matrix Completion problems:  X  Multivariate regression: In the reduced k  X  k problem, we can utilize the second order information, to achieve faster convergence. In the following we propose two novel algorithms for minimizing the sub-problem (8). For both algorithms, the most time consuming step is to compute the gradient or Hessian vec-tor product, so the time complexity is proportional to that. Algorithm 3: Our proposed algorithm Active ALT Input : Active subspace U A ,V A , initial S Output : Solution S of (8) 1 Z 2 for t inner = 0 , 1 ,... do 4 Solve  X  2 S g ( S,Z ) vec( D ) = vec( G ) by CG 5 S  X  S  X  D 7 end To compute the nuclear norm it is necessary to compute the square root ( S &gt; S ) 1 / 2 . If Z 0 commutes with S Z 0 = S &gt; S ) then the iteration Z k +1 = 1 2 Z k + Z coincides with Newton X  X  algorithm and converges to the square root, (Higham, 1986). This motivated the follow-ing reformulation Lemma 1 (A Convex Function) . Let s ( Z ) = trace( Z + Z  X  1 S &gt; S ) then s is a convex function (strictly convex when S is invertible) with and the infimum is attained when S is invertible for Z = ( S &gt; S ) 1 / 2 . When S is not invertible we can get arbitrarily close to the infimum by approaching ( S &gt; S ) 1 / 2 from inside the cone of positive definite matrices.
 The Lemma follows directly from the expressions of the gradient and Hessian of s .
 Based on Lemma 1, we can rewrite the sub-problem By Lemma 1 it follows that this function is jointly convex in S,Z as stated in the following theorem Theorem 5. g ( S,Z ) is jointly convex on S,Z on the do-main S  X  R k  X  k ,Z 0 .
 Therefore, we can alternatingly minimize S and Z to solve (11). The update rules are described below.
 Update S . When Z is fixed, g ( S,Z ) in (11) is a convex quadratic function in S , so we can update S by Newtons method. When f ( X ) is quadratic (as in matrix completion or multivariate regression), Newtons method converges in one iteration. Each Newton step can be computed by the Conjugate Gradient method (CG), which only requires us to compute Hessian vector products. The gradient is and since  X  2 S tr ( SSZ ) = 1 2 ( Z  X &gt; + Z  X  1 )  X  I , we have  X  2 S g ( S,Z ) vec( D ) =  X  2  X  f ( S ) vec( D ) We can further assume that Z is symmetric which gives  X  2 S g ( S,Z ) vec( D ) =  X  2  X  f ( S ) vec( D ) +  X  vec( DZ When the iteration cannot be computed efficiently, we can use limited memory BFGS (Nocedal &amp; Wright, 1999) with line search instead.
 Closed form solution for multivariate regression prob-lem. Interestingly, when f ( X ) = k AX  X  B k 2 F , we can derive a closed form solution of min S g ( S,Z ) . By setting  X  S g ( S,Z ) = 0 we get Assuming AU is full rank, then this equation is equivalent to SP + QS = K, (12) ( AU )  X  BV . Eq (12) is a Sylvester equation and can be solved in O ( k 3 ) time by forming the SVD of P,Q and K or by using the even more efficient Bartels-Stewart algo-rithm, (Bartels &amp; Stewart, 1972). Since all of them are k by k matrices and k m,n , we can compute the exact solution efficiently.
 Update Z . Lemma 1 shows that Z = mizer of g ( S,Z ) .
 Since (11) is a convex problem and our method is a block coordinate descent method with two blocks, our method is guaranteed to converge (see Proposition 7.2.1 in (Bert-sekas, 1999)). Moreover, Lemma 1 implies that  X  F ( S t ) = min Z g ( S t ,Z ) = g ( S t ,Z t ) , so  X  F ( S 1 ) ,  X  F ( S creasing sequence and converges to the optimum of the sub-problem.
 The details of this approach for solving the matrix comple-tion problem is in Algorithm 3. 3.4. Cone Projection Newton Descent Method Here we consider an alternative method to solve the k  X  k sub-problem that takes advantage of the fact that any norm is linear on at least a one dimensional cone since k tx k = t k x k for t &gt; 0 . In both of the proposed focus problems the smooth part of the objective is a quadratic and there-fore the entire problem becomes a smooth quadratic on the linearization cone. If the cone is only one dimensional this is not of great benefit, but for the ` 1 norm for example the cone is one of the mn dimensional orthants of R m  X  n . The cone is also non-trivial for many other norms such as `  X  and the nuclear norm.
 Theorem 6. Let U  X  R m  X  k and V  X  R n  X  k be orthogonal matrices then the sub-differential of k X k is constant on the cone { X = USV &gt; : S = S &gt; and S 0 } .
 The proof is in Appendix 6.4. The positive definite cone is 2 dimensional, which is nontrivial whenever k &gt; 1 . If we change the asymmetric portion of S then the derivative changes, much in the way it would for an ` 2 norm. The nuclear norm resembles both the ` 1 norm and the ` 2 norm. The ` 1 norm aspect we already saw, while the ` 2 norm is apparent when m = 1 since then k X k  X  = k X k F . We propose to solve the quadratic problem by use of a quasi Newton method such as the conjugate gradient algorithm or limited memory BFGS (L-BFGS), (Nocedal &amp; Wright, 1999). If the optimum over the symmetric matrices is not positive definite we do a back-tracking line search. We ad-ditionally project the entire search line segment onto the cone to ensure convergence. This also encourages further reduction in rank. For a particular point on the line seg-ment the projection consists simply of setting any negative eigenvalues to zero. This is analogous to what was done for the graphical LASSO problem in (Olsen et al., 2012). In our framework (Algorithm 1) there are two key tasks; 1) step 2 : to compute U A by ApproxSVD and 2) step 5 : to solve the k  X  k -size sub-problem. Both tasks incorporate an iterative solver. When both tasks are solved exactly, our algorithm converges to the global optimum (we omit the proof because this is a special case of Theorem 9). As the sequence X t converges to the global optimum X  X  , we show that the active column and row subspace ( U A ,V A will converge to the column and row space of X  X  in finite number of iterations.
 Theorem 7. Assume step 2 and 5 are exact, and then span ( U A ) = span ( U  X  ) , span ( V A ) = span ( V a finite number of iterations, where U  X  ,V  X  are column and row space of the global optimum X  X  .
 The proof is in Appendix 6.5. Note that Theorem 7 holds for any convergent sequence with lim t  X  X  X  X t = X  X  , and the assumption (13) can be shown to happen with very low probability, and was satisfied in our experiments. As long as U A ,V A span the column/row space of X  X  , Algorithm 1 can terminate in one step, so we have the following theo-rem: Theorem 8. If step 2 and 5 are exact and (13) holds, then Algorithm 1 terminates in a finite number of iterations. Since there is no closed-form solution to a general singular value decomposition, we consider the case where singular vectors are identified by the power method, as discussed in Algorithm 2. Assume in each iteration we use the previous U
G ,V G (denoted by ( U G ) t , ( V G ) t ) as the initial subspace for the power method and run the power method for more than one iteration . In general power method cannot con-verge to the top k eigenvalues of A unless V &gt; R is nonsin-gular for the initial guess R  X  R n  X  k , where V  X  R n  X  k is the top k singular vectors of A . This conditions is usu-ally satisfied in practice. Under this condition, we prove the following theorem: Theorem 9. When step 2 is computed by power method with more than one iteration and step 5 is solved exactly, then Algorithm 1 converges to the optimum with an asymp-totic linear convergence rate.
 The proof is in Appendix 6.6. This remarkable result shows that our algorithm converges fast even if the SVD in step 2 is computed inaccurately (i.e., with only one power itera-tion), if we initialize it by the previous U G ,V G . In this section, we compare our proposed nuclear norm solver with other state-of-the-art solvers. All the exper-are much faster than other methods.
 iments are conducted on an Intel Xeon X5355 2.66GHz CPU with 32G RAM. 5.1. Matrix Completion: Comparison with nuclear We compare the following methods:  X  Soft-Impute: the gradient descent method proposed  X  JSH: based on the work by (Jaggi &amp; Sulovsky, 2010;  X  SSGD: a Stochastic Sub-Gradient Descent method  X  Lifted CD: a greedy coordinate descent method pro- X  MMBS: the method iteratively increases the rank and  X  GCG: A Generalized Conditional Gradient method  X  Active ALT: our method with sub-problems solved by  X  Active Newton: our method with sub-problems solved The implementation detail for the competing algorithms are described in Appendix 6.7. We use the real-world recommendation system datasets, MovieLens ( ml100k , ml10m ), Netflix, and Yahoo Music (Dror et al., 2012) as shown in Table 1. The balancing parameter  X  was chosen by 3-fold cross validation on subsamples and the resulting rank k are also shown in Table 1. We compare the methods in terms of objective function value and test data RMSE. The results are shown in Figure 2. Notice the relative er-ror on the y-axis is defined as | ( F ( X )  X  F ( X  X  )) /F ( X where X  X  is the optimal solution. JSH, SoftImpute, and Lifted CD are too slow on the Netflix dataset so we omit the results in Figure 2f and 2c. Since JSH solves a constrained version of (1), we omit the objective function value of JSH in the plots. The results show that our methods are more than 6 times faster than other solvers on large datasets. Nuclear norm regularization is often considered too slow to solve large problems, and another approach is used to solve the non-convex problem: k V k 2 F ) = k X k  X  , so solving (14) is equivalent to solving (1). However, (14) is not jointly convex in U,V , so solvers Figure 3: Comparison to non-convex methods (ALS and LMaFit). The speed of our method is competitive to non-convex methods, and they may not converge to the global optimum. are not guaranteed to converge to the global optimum. We compare our method to non-convex solvers  X  Alternating Least Squares (ALS) and LMaFit (Wen et al., 2012) (a suc-cessive over-relaxation version of ALS) in Figure 3. Since Active ALS has very similar performance to Active New-ton on large dataset, we only compare Active Newton with ALS in the figures. Non-convex solvers (especially ALS) were widely used in the Netflix price because of its scal-ability (Koren et al., 2009). We observe that our method is faster than ALS and LMaFit on ml10m , while on ya-hoo non-convex solvers are faster in the beginning, but converges to an inferior solution (because it may get stuck in saddle points). Therefore, our method is competitive in terms of time, is more stable, and has theoretical guaran-tees. This is the first paper to scale nuclear norm solvers to the yahoo dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared with non-convex solvers. 5.2. Other Applications Next, we apply our method to other problems with nu-clear norm regularization. Since Active Newton and Ac-tive ALT has similar performance, we only show results for Active ALT in this section. We consider m = 3724 stocks, each with daily closing price recorded in 2012 ( l = 200 days) downloaded from Yahoo Finance. Assume p t  X  R m is the stock return of all the m stocks at day t , and we model the change of return by the auto regres-sion model p t = X p t  X  1 . To estimate the transition matrix X  X  R m  X  m , we solve the multivariate regression problem (3) where A = [ p 1 p 2 ... p l  X  1 ] &gt; and B = [ p 2 p Figure 4: Comparison on regression and multi-class prob-lems. Our proposed method is much faster than other nu-clear norm solvers.
 It was shown (Yuan et al., 2007) that a low rank assump-tion of X corresponds to the idea of feature sharing. We set  X  = 5 , which gives us a solution with rank 186 . The model is tested on next 200 days data and evaluated using the root mean square error. The experimental results in Figure 4a show that our method is much faster than other methods. We also test our method on a multi-class classification problem. We use the dataset from the ILSVRC-2010 com-petition. This dataset is a subset of ImageNet (Deng et al., 2009) with roughly 1000 images in each of the 1000 cate-gories. There are 1.2 million training images and 150,000 testing images, the 1000 bag-of-visual-word features pro-vided in the original dataset is used for classification. We model this as a multivariate regression problem, where each row of A is a training data, and each row of B is a unit vector e y i where y i is the label of i -th training data. The nuclear norm regularization is useful and has theoret-ical benefit as shown in (Amit et al., 2007). We solve the nuclear norm regularized multivariate regression problem with  X  = 600 to get a solution X  X  with rank 189. The per-formance of our proposed algorithm and other methods are shown in Figure 4b. Our method achieves 18.57% test ac-curacy in 6 minutes, while no other nuclear norm solver can achieve this accuracy in 4 hours. Our method achieve the final accuracy 21.36% after 0.5 hours. Notice that this ac-curacy is already good since random guess for 1000 classes would just achieve a 0.1% accuracy.
 Acknowledgements We would like to thank Haim Avron and Vikas Sindhwani for providing the SSGD and JSH implementation as well as for valuable discussions. We also thank the anonymous reviewers for their suggestions. C.-J. Hsieh acknowledges support from an IBM PhD fellowship.

