 Data intensive computing has become a central theme in research community and industry. There is an ever growing need to process and analyze massive amounts of data from diverse sources such as telecom call data records, telescope imagery, online transaction records, web pages, stock markets, medical records (monitoring critical health conditions of patients), climate warning systems, etc. Removing redundancy in the data is an important problem as it helps in resource and compute efficiency for downstream process-ing of the massive ( 1 billion to 10 billion records) datasets. In application domains such as IR, stock markets, telecom and others, there is a strong need for real-time data redundancy removal (re-ferred to as DRR ) of enormous amounts of data flowing at the rate of 1 GB/s or more. Real-time scalable data redundancy removal on massive datasets is a challenging problem. We present the design of a novel parallel data redundancy removal algorithm for both in-memory and disk-based execution. We also develop queueing the-oretic analysis to optimize the throughput of our parallel algorithm on multi-core architectures. For 500 million records, our paral-lel algorithm can perform complete de-duplication in 255 s ,on core Intel Xeon 5570 architecture, with in-memory execution. This gives a throughput of 2 M records/s. For 6 billion records, our par-allel algorithm can perform complete de-duplication in less than 4 . 5 hours, using 6 cores of Intel Xeon 5570, with disk-based ex-ecution. This gives a throughput of around 370 K records/s. To the best of our knowledge, this is the highest real-time throughput for data redundancy removal on such massive datasets. We also demonstrate the scalability of our algorithm with increasing num-ber of cores and data.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.2.8 [ Database Management ]: Database applications X  Data mining Algorithms, Design, Performance, Theory Bloom Filter, Data De-duplication, Data and Knowledge Mining, High Performance Computing, Queueing Theory
Data intensive computing has evolved into a central theme in research community and industry. There has been a tremendous spurt in the amount of data being generated across diverse applica-tion domains such as IR, telecom (call data records), telescope im-agery, online transaction records, web pages, stock markets, med-ical records (monitoring critical health conditions of patients), and climate warning systems among others. Processing such enormous data is computationally expensive. Removing redundancy in the data helps in improving resource utilization and compute efficiency especially in the context of stream data, which requires real-time processing at 1 GB/s or higher. We consider the problem of elimi-nating redundant records present in massive datasets in real-time. A record may be considered redundant if there exists another record present in that data stream earlier, which matches exactly (or ap-proximately) with this record. This is also referred to as the data de-duplication (de-dup) problem. Data redundancy removal (DRR) and de-duplication are used interchangeably in this paper. where each call generates call data records ( CDRs ). Each CDR contains details about a particular call such as the calling number, the called number and so forth. Due to errors in CDR generation, multiple copies of a CDR can get generated. Before storing these CDRs in a central data center, one needs to perform de-duplication over around 5 billion CDRs at a very high throughput. Current solutions perform de-duplication using database accesses, thereby, making them slow and inefficient. Hence, there is a strong need to have high throughput de-duplication over 5 to 10 billion records. involve pair-wise string comparisons, leading to quadratic com-plexity. This prohibits real-time redundancy removal over enor-mous ( 1 to 10 billion) number of records. In order to address this computational challenge, Bloom Filters [3] are typically used. Bloom Filters are space-efficient probabilistic data structures that provide fast set membership queries, but with a small false posi-tive rate ( FPR ). In order to achieve high throughput ( 1 data redundancy removal, one needs a scalable parallel Bloom Fil-ter based algorithm. Typical parallel Bloom Filter approaches incur k cache-misses with every record, where k is the number of hash functions computed per record to check the bits of the Bloom Filter array. This leads to a poor cache performance. Moreover, thread pipeline throughput is a critical issue in parallel Bloom Filter de-sign that, if not addressed, can lead to a lower overall performance. For in-memory DRR over billions of records, the memory required by the Bloom Filter array is very high (order of tens of Gigabytes depending on the false positive rate). One approach is to store the Bloom Filter array on the disk and bring parts of it into memory for reading and updates. But, this would lead to a huge fall in the overall DRR throughput (due to disk access overheads). Further, there is a trade-off between the cache performance and memory ef-ficiency [15] in the Bloom Filter design. These issues make parallel real-time data redundancy removal (de-duplication) over billions of records a very challenging problem.
 we optimize our algorithm on parallel multi-core architectures. Emerg-ing and next generation many-core architectures have massive num-ber of hardware threads and multiple levels of the cache hierar-chy. In order to provide real-time DRR, one needs to consider the cache performance at multiple levels as well as the thread pipeline throughput with an increasing number of threads. The parallel al-gorithm needs to provide an appropriate trade-off between cache locality and memory efficiency. Further, for billions of records, it needs to optimize disk accesses in an algorithmic fashion. We design a novel algorithm to improve the throughput of disk based DRR solution apart from using software cache to enhance the through-put.This paper makes the following contributions: memory real-time DRR over massive datasets ( 500 million records). Using cache-aware performance optimizations, our in-memory al-gorithm scales strongly with increasing number of cores. We also present a novel disk access optimized algorithm to perform DRR over 6 billion records. Using software cache based optimizations, our disk based algorithm demonstrates weak scalability and scala-bility with increasing data. velop queueing theoretic models to analyze the throughput, tune the thread distribution across the parallel phases, and tune the pa-rameters involved. This leads to improvement in the thread pipeline throughput and overall parallel DRR performance. core Power6 and 16 -core Intel Xeon 5570 multi-core architectures. Our in-memory algorithm, delivers a throughput of 2 M records/s (around 1 GB/s for 512 byte records) for de-duplication over records with a small false positive rate of 0 . 01 . We also demon-strate strong and weak scalability on both Power6 and Intel multi-core architectures. Our disk based algorithm delivers a throughput of 370 K records/s over 6 billion records, with FPR = 10  X  5 best of our knowledge, this is the best known throughput for data redundancy removal over such enormous number of records.
A Bloom Filter is a space-efficient probabilistic data structure that is widely used for testing membership queries on a set [2]. The efficiency is achieved at the expense of a small false positive rate, where the Bloom Filter may report falsely the presence of an ele-ment in the set. However, it does not report false negatives. Repre-senting a set of n elements by a Bloom Filter requires an array of m (locations) need to be set in the Bloom Filter array (BFA), an oper-ation denoted here by INSERT .These k locations are obtained by the evaluation of k independent hash functions h 1 (  X  ) ,...,h We denote this indexing operation by HASH . If all the locations are already set to 1, then either the element  X  is already a member of the set or a false positive. The probability of the false positive rate [3] for a standard Bloom Filter is: Given n and m , the optimal number of hash functions is given by k =ln2  X  ( m/n ) .

To support a situation where contents of a set change over time, with elements (records) being continually inserted and deleted, Fan et al. [7] introduced counting Bloom Filters, whereby elements can be deleted from a Bloom Filter by using a small counter instead of a single bit at every position. Insertion now requires the corre-sponding counters to be incremented. On the other hand, deletion requires the corresponding counters to be decremented. However, deletion is expensive since an element to be deleted needs to be re-hashed to determine the locations.
 For in-memory DRR, we introduce a variant of the counting Bloom Filter that avoids this expensive re-hashing operation. Ini-tially, let us have  X  sets labeled as S 1 ,S 2 ,S 3 ,...,S elements are distributed. Note that the same element might occur across several sets. Now, consider a situation where an applica-tion requires to process unique elements across  X  consecutive sets. Thus, our task is to detect and remove duplicates of elements over a window of length  X  . Therefore, we take  X  bits to represent a location of the Bloom Filter. To DELETE elements, only the bit representing the set S i needs to be toggled, thus significantly re-ducing the cost incurred due to re-hashing.
Bloom Filters have been proposed for various purposes. These include counting Bloom Filters [7], compressed Bloom Filters [14], space-code Bloom Filters [12], and spectral Bloom Filters [17]. Counting Bloom Filters replace an array of bits with counters in order to count the number of items hashed to a particular location. We design and analyze a variation of the counting Bloom Filter and optimize its performance for parallel multi-core architectures.
Bloom Filters have been broadly applied to network-related ap-plications such as finding heavy flows for stochastic fair blue queue management scheme [8], providing a useful tool to assist network routing, such as packet classification [1], per-flow state manage-ment, and the longest prefix matching [6]. [5] presents the design of parallel Bloom Filters, implemented in hardware, to match patterns in network packet payload. It demon-strates matching 10 , 000 strings on the network data at a line speed of 2 . 4 Gbps using state-of-the-art FPGAs. We perform real-time de-duplication on massive datasets (around 6 billion records) us-ing a novel parallel Bloom Filter algorithm, with performance op-timizations for multi-core architectures. [4] proposes a new design of Bloom Filter in which every two memory addresses are squeezed into one I/O block of the main memory. With the burst-type data I/O capability in the contemporary DRAM design, the total num-ber of memory I/Os involved in the membership query is reduced to half. This leads to a reduction in query delay and an improve-ment in overall performance, with a small increment in the false positive rate. Our in-memory algorithm design supports any desir-able false positive rate, while providing scalable performance with an increasing number of cores. Our disk based algorithm has been optimized using a novel bucketing technique and a software cache to minimize the disk access overhead. [10] proposes a new Bloom Filter structure that can support rep-resentation of items with multiple attributes and allow false posi-tive probability of the membership queries at a very low level. The new structure is composed of parallel Bloom Filters and a hash table to support an accurate and efficient representation for query-ing of items. [9] extends Bloomjoin , the state-of-the-art algorithm for distributed joins, to minimize the network usage for the query execution based on database statistics. [13] discusses how Bloom Filters can be used to speed up name to location resolution process in large scale distributed systems. This approach offers trade-offs between performance (time taken to resolve an object X  X  name to its location) and resource utilization (amount of physical memory to store location information and number of messages exchanged to obtain the object X  X  address). Our parallel Bloom Filter (PBF) design for multi-core architectures can be leveraged to accelerate these network processing applications that use Bloom Filters.
We present here the design of our parallel Bloom Filter for in-memory execution. Instead of processing the n elements together, we take up batches of size N , one at a time, and perform the nec-essary de-duplication operation. Thus, we process  X  = n in total. The PBF consists of three modules namely Pre-Processing (PP) module, Front-End (FE) module, and Back-End (BE) module. We assign  X  ,  X  and  X  threads to each of these modules respectively.
The PP module performs the task of detecting duplicates, within the batch being currently processed, using hash tables in parallel. This module helps in ensuring correctness of the multi-threaded FE and BE modules. To begin with, the PP module assigns to each record in the batch, a unique identification number (UID) by us-ing a cryptographic hash function (such as MD5). The rationale behind this assignment is to avoid costly compare operations on large records. Next, the batch is partitioned into  X  parts, each con-taining N (Fig. 1) gives equal processing load to each of the  X  threads. Each of the  X  threads computes a hash function on the records assigned to it, and stores them in the hash table associated with it. There-after, hash index based partitioning is used to merge the hash tables generated by each thread (Fig. 1). During merge, duplicate records are discarded. Thus, at the end of the PP module, the batch being processed consists of only unique records (within itself).
The FE module is responsible for computing the k independent hash functions of each element by taking UID as the input (Fig. 2). Each of the k hash functions per record is used to access the Bloom Filter array ( BFA ). The BE module takes care of setting the bits, in the BFA, corresponding to the locations generated by the k hash functions. It also reports the duplicates, if any, after accessing all the locations. Each of the  X  threads of the BE module is assigned an equal sized partition of the BFA, to enable simultaneous access. Further, each thread of the BE module has a queue. The location thus generated is enqueued into one of the  X  queues by each of the threads in the FE module, which handles the particular range of the BFA (Fig. 2). It might happen that two different threads vie for the same queue, leading to performance degradation due to requirement of locks. To avoid such situations, we allocate N space to each of the queues. For large values of n and small values of FPR , the resulting array m into the L2 cache, because of large m . Therefore to improve the cache performance, we follow the blocked Bloom Filter approach introduced in [15].
Figure 2: Parallel Bloom Filter Design (  X  =4 ,  X  =2 ,  X  =4
The DELETE operation is performed during transition from one group to another. In the BE module, each of the  X  thread deletes its own portion of the BFA, in parallel. The PP module does the pre-processing operation independent of the remaining modules. After it completes operation on a batch, it signals the corresponding FE module to start the HASH operation. As soon as the FE module starts filling up the  X  queues, it sends a signal to the BE module to start the DELETE and/or INSERT operations. The FE module waits for the BE module to complete before it takes up the next batch.
We present here the design of our parallel Bloom Filter for disk based execution. The Bloom Filter array size for 6 billion records, with FPR = 10  X  5 , is around 22 GB. During Bloom Filter array read and write operations for the in-flowing records, random parts of this array are accessed. If the Bloom Filter array is divided into equal parts and each part is stored as a file on the disk, then due to random files being accessed by each record, there will be a huge number of file accesses. This could result in a huge fall in the DRR throughput using disk based approach. To address this challenge, we propose a novel approach that is described below.

Each record can be considered as a byte-pattern. The input range of byte-pattern of the records is split into Z buckets . It is assumed that each record has the same format and consists of well defined fields. The concatenated value of the first few fields is divided into a set of range of values. Let Z be the cardinality of this set of range of values. With each bucket , a unique range is associated. Thus, Z buckets are generated. Each bucket can only be assigned records, whose first few fields have a value that lies in the range represented by that bucket. In order to assign a record to a bucket, first the range for the record is computed using its first few fields. Then, the record is assigned to the bucket that represents that particular range. The key observation here is that two records in different buckets can never be the same. Hence, we can perform de-duplication in each bucket independently.
 Hence, for each bucket one can have a logically separate Bloom Filter. The size of the Bloom Filter array, m , per bucket, will depend on the number of records, n , that will get assigned to that bucket and also on the FPR desired. If the total input records are uniformly distributed across the buckets, then all buckets can have the same Bloom Filter array size to maintain the FPR desired. In case the distribution of records across the buckets is non-uniform, then one can use an estimated value of max number of records per bucket to determine m . Then, each bucket will guarantee the FPR expected from the solution.

Fig. 3 refers to the overall flow of the algorithm in disk based execution.The records incoming into the system, as a data stream, first go through the Filtering phase. Here, based on the range a record belongs to, it gets assigned to the corresponding bucket. For each bucket, two buffers, each of fixed size N , are maintained. As the records come in, the first buffer for that bucket gets filled. When this buffer is filled, the records in that buffer are ready to be pro-cessed. Further incoming records that belong to the same bucket are assigned to the second buffer. If both buffers are filled, then the full record stream is buffered until one of the buffers for that bucket is available. A buffer is available for re-fill only when all the records within that buffer have been processed for de-duplication, using the Bloom Filter array associated with that bucket.

Whenabufferofsize N for a bucket is ready to be processed, its reference is put into the Ready Queue by the Filter thread. The Monitoring phase scans for filled buffers. When a Bloom Filter module ( BFM ) is available, the filled buffer for that bucket is as-signed (by the Monitor thread) to that Bloom Filter module (BFM) for de-duplication. A Bloom Filter module (BFM) is essentially a multi-threaded in-memory Bloom Filter (Section 4). Each BFM has a certain number of threads (cores) to provide parallel de-duplication (Fig. 1). In the disk based algorithm, the corresponding Bloom Fil-ter array resides on the disk as a separate file. This is first read from the disk and loaded into system memory, before the de-duplication for the records can be performed. The duplicate records found are written to a buffer per BFM. When this buffer overflows, it is flushed to the disk as a duplicate record file per BFM. Note that the records in the duplicate record file per BFM may contain records from any bucket, since the BFM will process different buckets at different point of time, though only one bucket at a time.
The throughput of this bucket-based approach can be optimized gorithm with the software cache. Here, when the Monitor thread intends to process a buffer for a bucket, then it first pre-fetches the Bloom Filter array for that bucket into the software cache. Then, when a BFM is available, it assigns that buffer for processing to that BFM. In order to pre-fetch a Bloom Filter array into the software cache, the Monitor thread must first evict one of the cache entries and write its Bloom Filter array onto the disk. The eviction policy uses the principle of choosing the one with the least expected us-age in the future. Using the entries in the Ready Queue and also by looking at the level of buffer utilization per bucket, the Monitor thread can determine which buckets will be processed in the near future. The Bloom Filter arrays in the software cache that do not belong to such buckets thus become the candidates for eviction.
The parallel time complexity of the proposed de-duplication frame-work can be analyzed by evaluating the complexity of the three sep-arate components: the Pre-Processing stage, the Front-End stage and the Back-End stage.
In the PP step, using the notation indicated in Table 1, MD5 computation requires overall O ( DX ) effort. However, since this much work is carried out in parallel using k 1 threads, therefore the effective cost of MD5 operation can be given by O DX Note that in our case, X =64 bytes. Next, every batch or block of N records, with size X each, is split across k 1 threads, and checked for duplication at each thread, for O NX bound per block ( X =8 bytes, in our setting). Finally, we perform record based partitioning to check for duplicates across sub-blocks of N Since there are O ( k 1 ) sub-blocks per block, hence O cNX records need to be processed for each block across k Thus, O cNX across sub-blocks, in parallel, for each block. Hence, the overall time complexity of the PP step, for the input dataset comprised of O D O DX
The first step in the FE stage consists of computing k Bloom Fil-ter locations for each record of size X . Since the work is divided across f threads, therefore, the resultant complexity is O Next, the enqueue operation is carried out, wherein each of the O ( kD ) locations is checked for its corresponding queue at one of the b BE threads. Therefore, O kbD this task by f threads. Hence, the overall complexity of the FE stage is O kDX
In the BE stage, two operations viz. inserting into and delet-ing (periodically) from the Bloom Filter are carried out. Inserting into the appropriate module or block of the Bloom Filter requires O kD eration, the entire Bloom Filter needs to be traversed; and hence O m of the BE stage is given by O kD
Thus, noting that k 2 = f + b , the overall time of our de-duplication framework is bounded by the maximum of PP, FE, and BE stages.
In this section, we present the analytical analysis for throughput using an abstract queueing theoretic model. Our framework can be modeled using an open system of two queues. The first queue represents the Pre-processing module, while the second queue rep-resents the combined Front-end and Back-end modules. As shown in Fig. 4, records enter into the system in batches, according to an exponential distribution with mean rate  X  1 ; and wait in Q are processed by any of the k 1 threads, each of which has a mean exponential service rate  X  1 . Then, processed batches of records wait in Q 2 for their service by one of the k 2 threads, each of which has a mean exponential service rate  X  2 .
 Figure 4: Queueing Theoretic Model for In-Memory Algorithm Consider a general M/M/k queue Q , where the symbols M , M ,and k denote the (exponential) distribution of the arrivals, (ex-ponential) distribution of departures, and the number of servers re-spectively. Let us define the work in the system at any time t to be the total remaining service time of all the records in Q at t .Let V denote the (time) average work, when records arrive in Q , one-by-one. Then, denoting the average total remaining service time of the records, when a new record enters Q ,by E [ R ] , we obtain the following relation [16]: where, under heavy traffic assumption [11], Now, if instead the records arrive in batch mode, denoting the aver-age record waiting time in Q by W Q and building on the M/G/k model proposed in [16] for single customer arrival, we obtain the following equation for the effective time average work, V : where, the average latency incurred by a record due to records in the same batch, E [ W B ] , is given by [16], Using (7.1), (7.2), and (7.3),
V + E Now, as shown in [16], But, Then, using (7.4) and (7.5), we obtain where, In case of heavy traffic, Then, using (7.6), Differentiating W Q with respect to k , we obtain d Now, the total queue waiting time in the system, Further, the total number of threads, k 1 + k 2 = T , is a constant. Therefore, dk 2 G/M/k 2 queue Q 2 , having a mean arrival rate  X  2 , by an equivalent M/M/k 2 queue with mean  X  2 , we obtain To fi n d k  X  1 and k  X  2 , the optimal value(s) of k 1 and k we set dW Now, since  X  1 denotes the mean of an exponential distribution, therefore,  X  2 T 1 = sum of k 1 exponential random variables, each having a mean ser-vice rate  X  1 = dom variables having the same mean is a gamma random variable,  X  E [
S 2 ]= (7.11) implies Rearranging the terms, It can be shown mathematically that the optimal value of k by,  X  Now, k 1 , being the number of servers for Q 1 , must be an integer. Therefore, we take either k 1 = k  X  1 or k 1 = k  X  1 , choosing a value that yields a lower waiting time.
We now model the behavior of our disk based de-duplication framework. In our setting (Fig. 5), records assumed to be arriv-ing from an exponential distribution with mean  X   X  accumulate into one of the Z buckets, which feed into Q 3 , and wait for batch mode service. With an increase in the number of buckets, the records as-signed to each bucket will decrease. This would lead to a decrease in the size of the Bloom Filter array for the buckets. Hence, the disk read and write operations will happen over lower sized Bloom Filter arrays. This leads to improvement in the service time. Hence, assuming the disk service time (without the cache) to be a random exponential variable X with parameter  X &gt; 0 , we model the im-proved service time due to presence of the cache as, where,  X  and  X  model the improvement in service rate due to soft-ware cache and increasing number of buckets respectively.  X  E [ S 3 ]= 1 Under the heavy traffic assumption [11],  X   X  Substituting  X  3 =  X   X  time in Q 3 ,
Using  X  2 S 3 = Now, each of the buckets, on being served by the disk server, pour records into Q 4 , each of which is processed in single-mode by one of the k 4 Q 4 servers. Thus, the effective arrival rate for Q
E [ N ] E [ [11], we obtain the waiting time in Q 4 , with  X  2  X  0 ,as W It can be shown that  X  4 = E = Solving, we get, and, d X  4 Using (8.2) and (8.3), and estimating  X  ,  X  and  X  , the optimal value of Z is calculated by approximating  X  1 by  X  2 , and then solving It can be shown that using z  X  , a reasonably large value of Z :
We implemented our parallel DRR algorithm using Posix Threads / NPTL(Native Posix Thread Library) API. Random test data, with variable number of records and record sizes, was used to eval-uate the performance and scalability of our algorithm. We per-formed two sets of experiments: ( a ) In-memory execution, where the Bloom Filter array is stored in the system memory, and Disk based execution, where the Bloom Filter arrays reside on the disk, and are read and updated before being written back to disk for processing of each bucket. The in-memory experiments were per-formed on two parallel multi-core architectures: (1) Power6 SMP (Enterprise Linux), with 32 (4GHz) Power6 cores. Each core has a 32KB L1 instruction cache and a 32KB L1 data cache and a 4MB semi-private L2 cache. Two cores share a 32MB L3 cache; and, (2) 16-core Intel with four Quad-core Xeon 5570 chips. Each core has a private L1 instruction and a L1 data cache of size 32KB and a private 256KB L2 cache. Four cores share 8MB L3 cache. For both Power6 and Intel architectures, the affinity of each thread was set to a different core using the thread affinity API in NPTL. The disk based experiments were performed on the Intel Xeon 5570 multi-core architecture only. We use the following notation in this section. X represents the size of a record in bytes; N represents the size of a batch in number of records (per batch in in-memory execution); D represents the total number of records over which to perform de-duplication; D 0 : represents the total number of records used to determine the size of the Bloom Filter array. Note, D to maintain the false positive rate (FPR); B represents the size of a block in bits, used for cache efficient Bloom Filter access and T : represents the number of threads used.
This section presents the strong and weak scalability analysis for in-memory execution with constant X =64 bytes and B = 512 bits. The FPR for all in-memory experiments was kept constant at 0 . 01 . The time measured for in-memory experiments represents the total time for de-duplication.
For strong scalability, we keep both D and D 0 constant, while in-creasing the number of threads, T , from 4 to 32 .Fig.6 ( the variation of the total time (with D = 100 M , D 0 = 500 with increasing number of threads. On Intel Xeon 5570, The total time decreases from 110 s for 4 threads to 51 s for 16 threads. This gives a relative speedup of 2 . 16  X  with 4  X  increase in the number of threads (cores). On Power6, the total time decreases from for 4 threads to 72 s for 32 threads. This gives a relative speedup of 6 . 9  X  with 8  X  increase in the number of threads (cores). This demonstrates the strong scalability of our parallel algorithm. Fur-ther, the Intel Xeon performance for T =16 threads, gives the pro-jected rate (for all 500 M record processing) as 500 M/ (5  X  51)  X  2
M records/s. We can use this extrapolation without loss of ac-curacy, since (on Intel Xeon 5570) for constant D 0 and all other parameters but with increasing D , the time increases linearly.
For weak scalability, we increase D as well as D 0 , from to 100 M (for Power6) while maintaining D = D 0 . At the same time, the number of threads, T , is also increased from For Intel architecture, D = D 0 is varied from 50 M to 200 varies from 4 to 16 .Fig.6 ( b ) displays the variation of the total time with the increasing number of threads. In case of Intel Xeon 5570, the total time increases from 55 s for 4 threads to 79 s for Thus, there is only 1 . 4  X  increase of time with 4  X  increase in the number of threads and the number of records. In case of Power6, the total time increases from 32 s for 4 threads to 53 s for Thus, there is only 1 . 66  X  increase of time with 8  X  increase in the number of cores/threads and the number of records. Thus our par-allel algorithm demonstrates both strong and weak scalability.
Fig. 6 ( c ) illustrates the scalability with an increase in the total number of records, as the number of threads is kept constant. For Intel, T =16 while for Power6, T =32 .Both D and D 0 are increased from 62 . 5 M to 500 M (while maintaining D = Both for Intel Xeon and Power6, the time increase is slightly more than the proportional increase in the number of records. This is because the underlying Bloom Filter array gets doubled in size each time, which leads to a fall in cache performance. Thus, when the number of records to process increases by 2  X  each time, the overall time increases by more than 2  X  .
In this section, we study the performance impact of variation of the number of threads allocated to the Pre-processing phase (de-noted by k 1 ) while keeping the total number of threads, T , as con-stant. We show that our queueing theoretic model (section 7) predicts close to the experimental optimal value of k 1 . We also look at the impact of change in the block size, B bits; change in batch size, N records; and change in size of the record, X bytes. Fig. 7 ( a ) presents the impact of thread allocation on the total time (with T =32 , B = 512 bits, D = 100 M , D 0 = 500 M , X =64 bytes). The horizontal-axis represents the number of threads, k allocated to the pre-processing phase. The remaining threads are allocated to the front-end and the back-end phase to get the best performance. As k 1 varies from 8 to 16 , the total time changes non-monotonically. It first goes down from 77 s to 70 s and then increases to 74 s following which it dips to 72 s . The best value of overall time ( 70 s ) is obtained at k 1 =11 .Fig.7 ( b ) variation of total time with changing value of k 1 ,when T Here, the best time ( 114 . 5 s ) is obtained at k 1 =8 , while the varia-tion is between 119 s at k 1 =6 and 130 s at k 1 =10 .

We compared this with the predicted optimal value for k 1 the queueing theoretic analysis in section 7. For T =32 , we exper-imentally measured the value of the queueing theoretic model pa-rameters,  X  1 =1 . 73  X  10 5 ,  X  2 =4 . 51  X  10 4 ,and  X  1 Using equation (7.12), we get predicted optimal value, k 1 Considering the floor and ceiling of this value we get two predicted optimal values, k 1 =10 and k 1 =11 . The value k 1 =11 matches exactly with the experimentally measured optimal value (Fig. 7 ( a ) ). For T =16 , using experimentally measured values,  X  1 =1 . 82  X  10 5 ,  X  2 =5 . 5  X  10 4 ,and  X  1 =14 . 41  X  10 5 ing equation (7.12), we get predicted optimal value, k 1 Considering the floor and ceiling of this value we get two predicted optimal values, k 1 =8 and k 1 =9 . The value k 1 =8 matches exactly with the experimentally obtained optimal value (Fig. 7 This demonstrates the modeling accuracy of our queueing theoretic model.
 all processing time. The parameters kept constant are: T B = 512 bits, D = 100 M , D 0 = 500 M . As the size of the record increases from 64 bytes to 2048 bytes ( 32  X  increase), the time in-creases from 94 s to 236 s ( 2 . 5  X  increase). This increase happens due to an additional computational processing for the MD5 based 128 -bit hash value generation. This also gives the data processing rate of (2048  X  100 M ) / 236 = 0 . 85 GB/s .
 false positive rate (FPR), with T =16 , D = 200 M , X =64 bytes, Y =8 K . The horizontal axis represents the number of records used to determine the Bloom Filter array size, i.e. D is varied from 50 M to 200 M ,as D is kept constant at 200 D 0 increases from 50 M to 200 M , the FPR drops from 0 0 . 0062 , while the total time increases from 177 s to happens as the time for record addition remains the same, since D is kept constant at 200 M , but the time for record deletion increases since the size of the Bloom Filter array increases with increase in D .
 in the block size, B , is presented in Fig. 8 ( c ) . Here, D D 0 = 500 M , X =64 bytes, Y =8 K .The L 2 cache miss rate reduces from 36% to 30% as the block size, B , reduces from bits to 128 bits. The overall time improves from 155 s to The cache line size of Power6 is 1024 bits ( 128 bytes). With block based cache-efficient technique, there is one cache miss for Bloom Filter array read/update per record. As we reduce, the block size to to 128 bits (below the cache line size), 8 records can potentially be processed with one cache miss. However, during actual execution, not all the 8 blocks within a cache line get used by 8 consecutive records. Hence, the cache miss rate falls as expected but not by a big percentage. Since the size of the Bloom Filter array is large, 625 MB for D 0 = 500 M , the false positive rate does not increase much [15]. If we consider the increase in FPR, then one needs to increase the Bloom Filter array size to maintain the same FPR. Here, the reduction in performance due to increase in Bloom Filter array size (to maintain the same FPR) could offset the gain in per-formance due to reduction in B . If the block size, B , is reduced beyond 128 bits, the increase in the Bloom Filter array size (to maintain the same FPR) causes the overall time to increase. Hence, the optimal value of B is obtained at 128 bits.
This section presents the performance and scalability analysis for disk based execution with constant X =64 bytes and B = 512 bits. For the disk based experiments, FPR was chosen as The time measured for disk based experiments represents the to-tal time including disk access for the Bloom Filter arrays as well as de-duplication performed by the Bloom Filter modules. With threads on Intel Xeon, the total time for disk based de-duplication of 6 billion records is around 4 . 5 hours, using 7 GB physical mem-ory and 22 . 35 GB hard disk space. In comparison to in-memory execution, the disk based solution is 3  X  more memory efficient.
For weak scalability, we increase D , from 50 M to 200 M records, while keeping D 0 =6 billion. At the same time, the number of threads, T , is varied from 6 to 16 .Fig.9 ( a ) displays the variation of the total time with the increasing D . The total time increases from 176 s for 6 threads to 631 s for 16 threads. Thus, there is a 3 . 6  X  increase in time with 4  X  increase in the number of records but with 2 . 7  X  increase in T . Thus, our disk based parallel algo-rithm demonstrates weak scalability.

Fig. 9 ( b ) illustrates the scalability with an increase in the total number of records, with T =6 , D 0 =6 billion. D is increased from 50 M to 200 M . The time increases from 106 s to 756 50 M records, the number of times a bucket is processed is small. The first read for each Bloom Filter array is of zero size (no bit set to one), hence the disk read times are negligible. For 50 total disk read time is very small and hence its overall time is low ( 106 s ). Thus, when the number of records to process increases by 2  X  each time, the overall time increases by more than 2  X 
Fig. 10 illustrates the impact of variation of the number of buck-ets on performance. Here, the number of buckets is varied from to 2048 , with T =6 , N =32 K . The time decreases from 853 to 301 s for the uniform distribution of records. For skewed distri-bution of records, the time decreases from 737 s to 245 s .Asthe number of buckets increases, the Bloom Filter array size decreases, which leads to lesser time in disk reads. This leads to better disk ac-cess times with increase in the number of buckets. Hence, the over-all performance improves. Fig. 11 illustrates the impact of variation of the size of the bucket buffers on performance (with T =6 Z = 1536 ). Here, the size of buckets is varied from 16 K to The time decreases from 753 s to 268 s for the uniform distribution of records. For skewed distribution of records, the time decreases from 546 s to 269 s . As the bucket size increases, for constant, D , the number of disk read / write operations goes down leading to improvement in the overall performance. At bucket size = the number of disk accesses of the buckets with higher number of records (in the skewed distribution) becomes similar to the that of the buckets in the uniform distribution. Hence, the number of disk accesses for both the uniform and the skewed distribution become similar which leads to similar time (around 268 s )atbucketbuffer size = 40 K records. Both the curves (Fig. 10 and Fig. 11) repre-sent trade-off between the memory efficiency and the performance of our disk based approach. As the number of buckets and/or the size of buckets increases, the performance improves at the cost of increased memory consumption.

Real-time data redundancy removal for massive datasets (bil-lions of records) is a very challenging problem. We have pre-sented a novel parallel algorithm for real-time data redundancy removal with optimizations for multi-core architectures and disk based execution. We developed queueing theoretic models for anal-ysis of throughput and determination of parameter values for opti-mal performance. We demonstrated real-time parallel data redun-dancy removal using a random dataset of 500 M records for in-memory (FPR = 10  X  2 )and 6 billion records for disk based execu-tion (FPR = 10  X  5 ). We delivered a de-duplication rate of around 2
M records/second for in-memory execution and 370 K records/s for disk based execution. To the best of our knowledge, these are the best known throughput values for data redundancy removal. For in-memory execution, we also demonstrated strong and weak scal-ability, and scalability with increase in the size of each record and the total number of records. We hope our work would lead to ad-vances into parallel data redundancy removal research. In future, we intend to investigate the scalability on many-core architectures with thousands of threads, and also on clusters of hybrid systems. We would like to express our gratitude to Dr. Ravi Kothari, Asso-ciate Director, IRL for his support and advice. We also acknowl-edge with great appreciation, the help and encouragement extended by Dr. Shivkumar Kalyanaraman, Senior Manager, Next Genera-tion Systems and Smarter Planet Solutions, IRL. [1] F. Baboescu and G. Varghese. Scalable packet clasification. [2] B. H. Bloom. Space/time trade-offs in hash coding with [3] A. Z. Broder and M. Mitzenmacher. Network applications of [4] Y. Chen, A. Kumar, and J. Xu. A new design of bloom filter [5] S. Dharmapurikar, P. Krishnamurthy, T. S. Sproull, and J. W. [6] S. Dharmapurikar, P. Krishnamurthy, and D. Taylor. Longest [7] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. Summary [8] W. Feng, D. Kandlur, D. Sahu, and K. Shin. Stochastic fair [9] T. Hofmann. Optimizing distributed joins using bloom [10] Y. Hua and B. Xiao. A multi-attribute data structure with [11] L. Kleinrock. Queueing Systems, Volume I: Theory . Wiley [12] A. Kumar, J. Xu, J. Wang, O. Spatschek, and L. Li. [13] M. Little, N. Speirs, and S. Shrivastava. Using bloom filters [14] M. Mitzenmacher. Compressed bloom filters. In IEEE/ACM [15] F. Putze, P. Sanders, and J. Singler. Cache-, hash-, and [16] S. M. Ross. Introduction to Probability Models . Academic [17] C. Saar and M. Yossi. Spectral bloom filters. In ACM
