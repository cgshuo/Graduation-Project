 Jun Zhu dcszj@mail.tsinghua.edu.cn As the availability and scope of social networks and relational datasets increase, a considerable amount of attention has been devoted to the statistical analysis of such data, which is typically represented as a graph in which the vertices represent entities and edges repre-sent links between entities. Link prediction is one fun-damental problem in analyzing these social network or relational data, and its goal is to predict unseen links between entities given the observed links. Often there is extra information about links and entities such as at-tributes and timestamps ( Liben-Nowell &amp; Kleinberg , 2003 ; Backstrom &amp; Leskovec , 2011 ; Miller et al. , 2009 ) that can be used to help with prediction.
 Recently, various approaches based on probabilistic models have been developed for link prediction. One class of such models utilize a latent feature matrix and a link function (e.g., the commonly used sig-moid function) ( Hoff , 2007 ; Miller et al. , 2009 ) to de-fine the link formation probability distribution. These latent feature models were shown to generalize laten-t class ( Nowicki &amp; Snijders , 2001 ; Airoldi et al. , 2008 ) and latent distance ( Hoff et al. , 2002 ) models and are thus able to represent both homophily and stochastic equivalence, which are important properties common-ly observed in real-world social network and relational data. The parameters for these probabilistic models are typically estimated with MLE or their posterior distributions are inferred with Monte Carlo method-s. Such techniques have demonstrated competitive re-sults on various datasets. However, to determine the unknown dimensionality of the latent feature space (or latent social space), most of the existing approach-es rely on a general model selection procedure, e.g., cross-validation, which could be expensive by compar-ing many different settings. The work ( Miller et al. , 2009 ) is an exception, which presents a nonparametric Bayesian method to automatically infer the unknown social dimension.
 This paper presents an alternative way to develop non-parametric latent feature relational models. Instead of defining a normalized link likelihood model, we pro-pose to directly minimize some objective function (e.g., hinge-loss) that measures the quality of link predic-tion, under the principle of maximum entropy discrim-ination (MED) ( Jaakkola et al. , 1999 ; Jebara , 2002 ), which was introduced as an elegant framework to in-tegrate max-margin learning and Bayesian generative modeling. The present work extends MED in sever-al novel ways to solve the challenging link prediction problem. First, like ( Miller et al. , 2009 ), we use non-parametric Bayesian techniques to automatically re-solve the unknown dimension of a latent social space, and thus our work represents an attempt towards unit-ing Bayesian nonparametrics and max-margin learn-ing, which have been largely treated as two isolated topics. Second, we present a fully-Bayesian method to avoid tuning regularization constants. By mini-mizing a hinge-loss, our model avoids dealing with a highly nonlinear link likelihood (e.g., sigmoid) and can be efficiently solved using variational methods, where the sub-problems of max-margin learning are solved with existing high-performance solvers. Experimental results on three real datasets appear to demonstrate that 1) using max-margin learning can significantly improve the link prediction performance, and 2) using fully-Bayesian methods, we can avoid tuning regular-ization constants without sacrificing the performance, and dramatically decrease running time.
 The paper is structured as follows. Sec 2 introduces existing latent feature models, as well as a new insight about the connections between these models. Sec 3 presents the max-margin latent feature relational mod-el and a fully-Bayesian formulation. Sec 4 presents empirical results. Finally, Sec 5 concludes. Assume we have an N  X  N relational link matrix Y , where N is the number of entities. We consider the binary case, where the entry Y ij = +1 (or Y ij =  X  1) indicates the presence (or absence) of a link between entity i and entity j . We emphasize that all the la-tent feature models introduced below can be extended to deal with real or categorical Y . Y is not fully ob-served. The goal of link prediction is to learn a model from observed links such that we can predict the val-ues of unobserved entries of Y . In some cases, we may have observed attributes X ij  X  R D that affect the link between i and j .
 In a latent feature relational model, each entity is associated with a vector  X  i  X  R K , a point in a latent feature space (or latent social space). Then, the link likelihood is generally defined as where a common choice of  X  is the sigmoid function, i.e.,  X ( t ) = 1 1+ e  X  t . For the latent distance mod-el ( Hoff et al. , 2002 ), we have For the latent eigenmodel ( Hoff , 2007 ), which gener-alizes the latent distance model and the latent class model for modeling symmetric relational data, we have  X  (  X  i , X  j ) =  X   X  i D X  j , where D is a diagonal matrix . In the above models, the dimension K is unknown a priori , and a model selection procedure (e.g., cross-validation) is needed. The nonparametric latent feature relational model (LFRM) ( Miller et al. , 2009 ) leverages the recent advances in Bayesian nonparametrics to automatically infer the latent dimension. Moreover, LFRM differs from the above models by inferring binary latent features and defining We will use Z to denote a binary feature matrix, where each row corresponds to the latent feature of an entity. For LFRM, we have Z = [  X   X  1 ;  X   X   X  ;  X   X  N ]. Fully-Bayesian inference with MCMC sampling is usually performed for these models by imposing appropriate priors on la-tent features and model parameters. In LFRM, Indian buffet process (IBP) ( Griffiths &amp; Ghahramani , 2006 ) was used as the prior of Z to induce a sparse latent feature vector for each entity.
 Miller et al. discussed the expressiveness of LFRM over latent class models. Here, we provide another support for the expressiveness. For modeling symmet-ric relational data, we usually constrain W to be sym-metric ( Miller et al. , 2009 ). Since a symmetric real matrix is diagonalizable, i.e., there exists an orthogo-nal matrix Q satisfying that Q  X  WQ is a diagonal ma-trix, denoted again by D , we have W = QDQ  X  . Thus we can treat ZQ as the effective real-valued latent fea-tures and conclude that LFRM subsumes the latent egienmodel for modeling symmetric relational data . Now, we present the max-margin latent feature model for link prediction. We first briefly review the basic concepts of MED ( Jaakkola et al. , 1999 ; Jebara , 2002 ). 3.1. MED We consider binary classification, where the response variable Y takes values from { +1 ,  X  1 } . Let X be an input feature vector and F ( X ;  X  ) be a discriminant function parameterized by  X  . Let D = { ( X n ,Y n ) } N n =1 be a training set and define h  X  ( x ) = max(0 , X   X  x ), where  X  is a positive cost parameter. Unlike standard SVMs, which estimate a single  X  , MED learns a dis-tribution p (  X  ) by solving an entropic regularized risk minimization problem with prior p 0 (  X  ) where C is a positive constant; KL( p  X  q ) is the KL di-vergence; R ( p (  X  )) = hinge-loss that captures the large-margin principle un-derlying the MED prediction rule By defining F as the log-likelihood ratio of a Bayesian generative model, MED provides an elegant way to integrate the discriminative max-margin learning and Bayesian generative modeling. MED subsumes SVM as a special case and has been extended to incorporate latent variables ( Jebara , 2002 ; Zhu et al. , 2009 ) and to perform structured output prediction ( Zhu &amp; Xing , 2009 ). Recent work has further extended MED to u-nite Bayesian nonparametrics and max-margin learn-ing ( Zhu et al. , 2011a ; b ), which have been largely treated as isolated topics, for learning better classi-fication models. The present work contributes by in-troducing a novel generalization of MED to perform the challenging relational link prediction. 3.2. MED Latent Feature Relational Model Now, we present the max-margin latent feature model for link prediction. Based on the above discussions, we use the same formulations as the most general LFRM model. Specifically, we represent each entity using a set of binary features and let Z to denote the binary feature matrix, of which each row corresponds to an entity and each column corresponds to a feature. The entry Z ik = 1 means that entity i has the feature k . If the features Z i and Z j are given, we can naturally define the latent discriminant function as where W is a real-valued matrix and the entry W kk  X  is the weight that affects the link from entity i to entity j if entity i has feature k and entity j has feature k  X  . For finite sized matrices Z with K columns, we can define the prior as a Beta-Bernoulli process ( Meeds et al. , 2007 ). In the infinite case, where Z has an infinite number of columns, we adopt the Indian buffet pro-cess (IBP) prior over the unbounded binary matrices as described in ( Griffiths &amp; Ghahramani , 2006 ). Let  X  = { W, X  } be all the parameters. To make this model Bayesian, we also treat  X  as random, with a prior p 0 ( X ). To make prediction, we need to get rid of the uncertainty of latent variables, and we define the effective discriminant function as an expectation Then, the prediction rule is  X  Y ij = sign f ( X ij ). Let I be the set of pairs that have observed links. The hinge loss of the expected prediction rule is Let p 0 ( Z ) be the prior on the latent feature matrix. We define the MED latent feature relational model (MedLFRM) as solving the problem In graphical models, it is well known that introducing auxiliary variables could simplify the inference by con-verting marginal dependence into conditional indepen-dence. Here, we follow this principle and introduce ad-ditional variables for the IBP prior p 0 ( Z ). One elegant way to do that is the stick-breaking representation of IBP ( Teh et al. , 2007 ). Specifically, let  X  k  X  (0 , 1) be a parameter associated with column k of Z . The pa-rameters  X  are generated by a stick-breaking process, that is,  X  1 =  X  1 , and  X  k =  X  k  X  k  X  1 =  X  i  X  Beta(  X , 1). Given  X  k , each Z nk in column k is sampled independently from Bernoulli(  X  k ). This pro-cess results in a decreasing sequence of probabilities  X  , and the probability of seeing feature k decreases exponentially with k on a finite dataset. With this representation, we have the augmented MedLFRM min where p 0 (  X  ,Z,  X ) = p 0 (  X  ) p ( Z |  X  ) p 0 ( X ). We make two comments about the above definition-s. First, we have adopted the similar method as in ( Zhu et al. , 2011a ; b ) to define the discriminant func-tion using the expectation operator, instead of the tra-ditional log-likelihood ratio of a Bayesian generative model with latent variables ( Jebara , 2002 ; Lewis et al. , 2006 ). The linearity of expectation makes our for-mulation simpler than the one that could be achieved by using a highly nonlinear log-likelihood ratio. Sec-ond, although a likelihood model can be defined as in ( Zhu et al. , 2011a ; b ) to perform hybrid learning, we have avoided doing that because the sigmoid link likeli-hood model in Eq. ( 1 ) is highly nonlinear and it could make the hybrid problem hard to solve. 3.2.1. Inference with Truncated Mean-Field The above problem has nice properties. For example, R  X  is a piece-wise linear functional of p and f is linear of  X . While sampling methods could lead to more accurate results, variational methods are usually more efficient and they also have an objective to monitor the convergence behavior. Here, we introduce a simple variational method to explore such properties, which turns out to perform well in practice. Specifically, we make the truncated mean-field assumption where p (  X  k |  X  k ) = Beta(  X  k 1 , X  k 2 ), p ( Z ik |  X  Bernoulli(  X  ik ) and K is a truncation level. Then, problem ( 8 ) can be solved using an iterative procedure that alternates between: Solving for p ( X ): by fixing p (  X  ,Z ), the subproblem can be equivalently written in a constrained form where  X  Z ij = E p [ Z  X  j Z i ] is the expected latent features and  X  = {  X  ij } are slack variables. By Lagrangian duality theory, we have the optimal solution where  X  = {  X  ij } are Lagrangian multipliers. For the commonly used standard normal prior p 0 ( X ), we have the optimal solution where the means are  X  kk  X  = and  X  d = Equivalently, the mean parameters  X  and  X  can be directly obtained by solving the primal problem which is a binary classification SVM. We can solve it with any existing high-performance solvers, such as SVMLight or LibSVM.
 Solving for p (  X  ,Z ): by fixing p ( X ),the subproblem is With the truncated mean-field assumption, we have We defer the evaluation of the KL-divergence to Ap-pendix A. For p (  X  ), since the margin constraints are not dependent on  X  , we can get the same solutions as in ( Doshi-Velez et al. , 2009 ).
 We can solve for p ( Z ) using sub-gradient methods. Let g ( x )  X   X  ; 0 otherwise, we have the subgradient  X  where  X  k  X  ( X   X  k ) denotes the k th row (column) of  X , and I (  X  ) is an indicator function. Let the subgradient equal to 0, and we get the update equation where L  X  k is a lower bound of E p [log(1  X  Appendix A). 3.3. The Fully-Bayesian Model MedLFRM has one regularization parameter C , which normally plays an important role in large-margin clas-sifiers, especially on sparse and imbalanced dataset-s. To search a good value of C , cross-validation is a typical approach, but it could be computationally expensive by comparing many candidates. Under the probabilistic formulation, we could provide an alter-native way to control model complexity automatical-ly, at least in part. Below, we present a fully-Bayesian MedLFRM model by introducing appropriate priors for the hyper-parameters.
 Normal-Gamma Prior : For simplicity, we assume that the prior is an isotropic normal distribution 1 with common mean  X  and precision  X  To complete the model, we use a Normal-Gamma hyper-prior for  X  and  X  : where G is the Gamma distribution,  X  0 is the prior mean,  X  0 is the prior degrees of freedom, n 0 is the prior sample size, S 0 is the prior sum of squared er-rors. We denote this Normal-Gamma distribution by We note that the normal-Gamma prior has been used in a marginalized form as a heavy-tailed prior for de-riving sparse estimates ( Griffin &amp; Brown , 2010 ). Here, we use it for automatically inferring the regulariza-tion constants, which replace the role of C in prob-lem ( 8 ). Also, our Bayesian approach is different from the previous methods that were developed for estimat-ing the hyper-parameters of SVM, by optimizing a log-evidence ( Gold et al. , 2005 ) or an estimate of the gen-eralization error ( Chapelle et al. , 2002 ). Formally, with the above hierarchical prior, we define Bayesian MedLFRM (BayesMedLFRM) as solving For this problem, we can develop a similar iterative algorithm as for MedLFRM. Specifically, the sub-step of inferring p (  X  ,Z ) does not change. For p (  X , X ,  X ), the sub-problem (in equivalent constrained form) is which is convex but intractable to solve directly. Here, we make the mild mean-field assumption that p (  X , X ,  X ) = p (  X , X  ) p ( X ). Then, we iteratively solve for p ( X ) and p (  X , X  ), as summarized below. We defer the details to Appendix B.
 For p ( X ), we have the mean-field update equation  X  d = E [  X  ] +  X   X  1 Similar as in MedLFRM, the mean of  X  can be obtained by solving the following problem where e is a K  X  1 vector with all entries being the unit 1 and E = ee  X  is a K  X  K matrix. Let  X   X  =  X   X  E [  X  ] E and  X   X  =  X   X  E [  X  ] e , we have the transformed problem tive cost. The problem can be solved using an existing binary SVM solver with slight changes to consider the sample-varying costs. Comparing with problem ( 11 ), we can see that BayesMedLFRM automatically infers the regularization constant  X  (or equivalently C ), by iteratively updating the posterior distribution p (  X  ), as explained below.
 The mean-field update equation for p (  X , X  ) is and S W =  X  W  X   X  WE  X  2 2 , S  X  =  X   X   X   X   X  e  X  2 2 . From p (  X , X  ), we can compute the expectation and variance, which are needed in updating p ( X ) Now, we provide empirical studies on several real datasets to demonstrate the effectiveness of the max-margin principle in learning latent feature relational models, as well as the effectiveness of fully-Bayesian methods in avoiding tuning the hyper-parameter C . 4.1. Multi-relational Datasets We report the results of MedLFRM and BayesMedL-FRM on the two benchmark datasets which were used in ( Miller et al. , 2009 ) to evaluate the performance of latent feature relational models. One dataset contains 54 relations of 14 countries along with 90 given features of the countries, and the other one contains 26 kinship relationships of 104 people in the Alyawarra tribe in Central Australia. On average, there is a probability of about 0 . 21 that a link exists for each relation on the countries dataset, and the probability of a link is about 0 . 04 for the kinship dataset. So, the kinship dataset is extremely imbalanced (i.e., much more neg-ative examples than positive examples). To deal with this imbalance in learning max-margin MedLFRM, we use different regularization constants for the positive ( C + ) and negative ( C  X  ) examples. We refer the read-ers to ( Akbani et al. , 2004 ) for other possible choices. In our experiments, we set C + = 10 C  X  = 10 C for sim-plicity and tune the parameter C . For BayesMedLFR-M, this equality is held during all iterations. Depending on the input data, the latent features might not have interpretable meanings ( Miller et al. , 2009 ). In the experiments, we focus on the effective-ness of max-margin learning in learning latent feature relational models. We also compare with two well-established class-based algorithms  X  IRM (i.e., infinite relational model) ( Kemp et al. , 2006 ) and MMSB (i.e., mixed membership stochastic block) ( Airoldi et al. , 2008 ), both of which were tested in ( Miller et al. , 2009 ). In order to compare with their reported results, we use the same setup for the experiments. Specifical-ly, for each dataset, we held out 20% of the data during training and report the AUC (i.e., area under the Re-ceiver Operating Characteristic or ROC curve) for the held out data. As in ( Miller et al. , 2009 ), we consider two settings  X   X  X lobal X  and  X  X ingle X . For the global setting, we infer a single set of latent features for all relations; and for the single setting, we infer indepen-dent latent features for each relation and the overall AUC is an average of the AUC scores of all relations. For MedLFRM and BayesMedLFRM, we randomly initialize the posterior mean of W uniformly in the interval [0 , 0 . 1]; initialize  X  to uniform (i.e., 0 . 5) cor-rupted by a random noise distributed uniformly at the interval [0 , 0 . 001]; and initialize the mean of  X  to be zero. All the following results of MedLFRM and BayesMedLFRM are averages over 5 randomly ini-tialized runs, again similar as in ( Miller et al. , 2009 ). For MedLFRM, the hyper-parameter C is selected via cross-validation during training. For BayesMedLFR-M, we use a very weak hyper-prior by setting  X  0 = 0, n 0 = 1,  X  0 = 2, and S 0 = 1. We set the cost parameter  X  = 9 in all experiments.
 Table 1 shows the results. We can see that in both settings and on both datasets, the max-margin based latent feature relational model MedLFRM sig-nificantly outperforms LFRM that uses a likelihood-based approach with MCMC sampling. Comparing BayesMedLFRM and MedLFRM, we can see that us-ing the fully-Bayesian technique with a simple Normal-Gamma hierarchical prior, we can avoid tuning the regularization constant C , without sacrificing the link prediction performance. To see the effectiveness of la-tent feature models, we also report the performance of logistic regression (LR) and linear SVM on the coun-tries dataset, which has input features. We can see that a latent feature or latent class model generally outperforms the methods that are built on raw input features for this particular dataset.
 Figure 1 shows the performance of MedLFRM on the countries dataset when using and not using input fea-tures. We consider the global setting. Here, we also study the effects of truncation level K . We can see that in general using input features can boost the per-formance. Moreover, even if using latent features only, MedLFRM can still achieve very competitive perfor-mance, better than the performance of the likelihood-based LFRM that uses both latent features and input features. Finally, it is sufficient to get good perfor-mance by setting the truncation level K to be larger than 40. We set K to be 50 in the experiments. 4.2. Predicting NIPS coauthorship The second experiments are done on the coauthorship data constructed from the NIPS dataset which con-tains a list of all papers and authors from NIPS 1-17. To compare with LFRM, we use the same dataset as in ( Miller et al. , 2009 ), which contains 234 authors who had published with the most other people 2 . To better fit the symmetric coauthor link data, we restrict our models to be symmetric, i.e., the posterior mean of W is a symmetric matrix, as in ( Miller et al. , 2009 ). For MedLFRM and BayesMedLFRM, this symmetry constraint can be easily satisfied when solving the SVM problems ( 11 ) and ( 16 ). To see the effects of the symmetry constraint, we also report the results of the asymmetric MedLFRM and asymmetric BayesMedL-FRM, which do not impose the symmetry constrain-t on the posterior mean of W . As in ( Miller et al. , 2009 ), we train the model on 80% of the data and use the remaining data for test.
 Table 2 shows the results, where the results of LFR-M, IRM and MMSB were reported in ( Miller et al. , 2009 ). Again, we can see that using the discrimina-tive max-margin training, the symmetric MedLFRM and BayesMedLFRM outperform all other likelihood-based methods, using either latent feature or latent class models; and the fully-Bayesian MedLFRM model performs comparably with MedLFRM while avoiding tuning the hyper-parameter C . Finally, the asymmet-ric MedLFRM and BayesMedLFRM models perform much worse than their symmetric counterpart models, but still better than the latent class models. 4.3. Stability and Running Time Figure 2 shows the change of the objective function as well as the change of the AUC scores on test da-ta of the countries dataset during the iterations for both MedLFRM and BayesMedLFRM. For MedLFR-M, we report the results with the best C selected via cross-validation. We can see that the variational infer-ence algorithms for both models converge quickly to a particular region. Since we use sub-gradient descent to update the distribution of Z and the subproblems of solving for p ( X ) can in practice only be approxi-mately solved, the objective function has some distur-bance, but within a relatively very small interval. For the AUC scores, we have similar observations, name-ly, within several iterations, we could have very good link prediction performance. The disturbance is again maintained within a small region, which is reasonable for our approximate inference algorithms. Comparing the two models, we can see that BayesMedLFRM has similar behaviors as MedLFRM, which demonstrates the effectiveness of using fully-Bayesian techniques to automatically learn the hyper-parameter C . Figure 3 presents the results on the kinship dataset, from which we have the same observations. We omit the results on the NIPS dataset for saving space.
 Finally, Figure 4 shows the training time and test time of MedLFRM and Bayesian MedLFRM on each of the three datasets. For MedLFRM, we show the single run with the optimum parameter C , selected via inner cross-validation. We can see that using Bayesian infer-ence, the running time does not increase much, being generally comparable with that of MedLFRM. But s-ince MedLFRM needs to select the hyper-parameter C , it will need much more time than BayesMedLFRM to finish the entire training on a single dataset. We have presented a discriminative max-margin latent feature relational model for link prediction. Under a Bayesian-style max-margin formulation, our work nat-urally integrates the ideas of Bayesian nonparametric-s to automatically resolve the unknown dimensional-ity of a latent social space. Furthermore, we present a fully-Bayesian formulation, which can avoid tuning regularization constants. We developed efficient varia-tional methods to perform posterior inference. Empir-ical results on several real datasets appear to demon-strate the benefits inherited from both max-margin learning and fully-Bayesian methods.
 Our current analysis is focusing on small static net-work snapshots. For future work, we are interested in learning more flexible latent feature relational models to deal with large dynamic networks and reveal more subtle network evolution patterns. We are also inter-ested in developing Monte Carlo sampling methods, which have been widely used in previous latent fea-ture relational models.
 JZ is supported by National Key Foundation R&amp;D Projects 2012CB316301, a Starting Research Fund No. 553420003, the 221 Basic Research Plan for Young Faculties at Tsinghua University, and a Research Fund No. 20123000007 from Microsoft Research Asia.
