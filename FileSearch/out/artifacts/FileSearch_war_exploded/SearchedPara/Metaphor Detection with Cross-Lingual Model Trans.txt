 Lakoff and Johnson (1980) characterize metaphor as reasoning about one thing in terms of another, i.e., a metaphor is a type of conceptual mapping , where words or phrases are applied to objects and actions in ways that do not permit a literal inter-pretation. They argue that metaphors play a fun-damental communicative role in verbal and writ-ten interactions, claiming that much of our every-day language is delivered in metaphorical terms. There is empirical evidence supporting the claim: recent corpus studies have estimated that the pro-portion of words used metaphorically ranges from 5% to 20% (Steen et al., 2010), and Thibodeau and Boroditsky (2011) provide evidence that a choice of metaphors affects decision making.

Given the prevalence and importance of metaphoric language, effective automatic detec-tion of metaphors would have a number of ben-efits, both practical and scientific. Language pro-cessing applications that need to understand lan-guage or preserve meaning (information extrac-tion, machine translation, dialog systems, senti-ment analysis, and text analytics, etc.) would have access to a potentially useful high-level bit of in-formation about whether something is to be under-stood literally or not. Second, scientific hypothe-ses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: hu-mans may disagree whether a particular expres-sion is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain-and context-
Previous work has focused on metaphor identi-fication in English, using both extensive manually-created linguistic resources (Mason, 2004; Gedi-gian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detec-tion into other languages in which few resources may exist. Our work makes the following con-tributions: (1) we develop a new state-of-the-art English metaphor detection system that uses con-ceptual semantic features, such as a degree of ab-ate new metaphor-annotated corpora for Russian fer (McDonald et al., 2011; T  X  ackstr  X  om et al., 2013; Kozhenikov and Titov, 2013), we provide sup-port for the hypothesis that metaphors are concep-tual (rather than lexical) in nature by showing that our English-trained model can detect metaphors in Spanish, Farsi, and Russian. Our task in this work is to define features that dis-tinguish between metaphoric and literal uses of two syntactic constructions: subject-verb-object examples of a prototypical metaphoric usage of each type:  X  SVO metaphors. A sentence containing a  X  AN metaphors. The phrase broken promise
Motivated by Lakoff X  X  (1980) argument that metaphors are systematic conceptual mappings, we will use coarse-grained conceptual , rather than fine-grained lexical features, in our classifier. Con-ceptual features pertain to concepts and ideas as opposed to individual words or phrases expressed in a particular language. In this sense, as long as two words in two different languages refer to the same concepts, their conceptual features should be the same. Furthermore, we hypothesize that our coarse semantic features give us a language-invariant representation suitable for metaphor de-tection. To test this hypothesis, we use a cross-lingual model transfer approach: we use bilingual dictionaries to project words from other syntactic constructions found in other languages into En-glish and then apply the English model on the de-rived conceptual representations.
Each SVO (or AN) instance will be represented by a triple (duple) from which a feature vector concatenation of the conceptual features (which we discuss below) for all participating words, and to generate the feature vector for the SVO triple ( car , drink , gasoline ) , we compute all the features for the individual words car , drink , gasoline and combine them with the conjunction features for the pairs car drink and drink gasoline .

We define three main feature categories (1) ab-stractness and imageability, (2) supersenses, (3) unsupervised vector-space word representations; each category corresponds to a group of features with a common theme and representation.  X  Abstractness and imageability. Abstract- X  Supersenses. Supersenses 7 are coarse se- X  Vector space word representations. Vec-In this section we describe a classification model, and provide details on mono-and cross-lingual implementation of features. 3.1 Classification using Random Forests To make classification decisions, we use a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned from many inde-pendent subsamples of the training data. Given an input, each tree classifier assigns a probabil-ity to each label; those probabilities are averaged to compute the probability distribution across the ensemble. Random forest ensembles are partic-ularly suitable for our resource-scarce scenario: rather than overfitting, they produce a limiting value of the generalization error as the number is required. In addition, decision-tree classifiers learn non-linear responses to inputs and often out-Our random forest classifier models the probabil-ity that the input syntactic relation is metaphorical. If this probability is above a threshold, the relation is classified as metaphoric, otherwise it is literal. We used the scikit-learn toolkit to train our classifiers (Pedregosa et al., 2011). 3.2 Feature extraction Abstractness and imageability. The MRC psy-cholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained among other data, 4,295 words rated by the de-grees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space representations. More specifically, we calculate the degree of abstractness and imageability of all English items that have a vector space representa-tion, using vector elements as features. We train two separate classifiers for abstractness and im-ageability on a seed set of words from the MRC database. Degrees of abstractness and imageabil-ity are posterior probabilities of classifier predic-tions. We binarize these posteriors into abstract-concrete (or imageable-unimageable) boolean in-mance of these classifiers, tested on a sampled held-out data, is 0.94 and 0.85 for the abstractness and imageability classifiers, respectively. Supersenses. In the case of SVO relations, we incorporate supersense features for nouns and verbs; noun and adjective supersenses are used in the case of AN relations.

Supersenses of nouns and verbs. A lexical item can belong to several synsets, which are associ-ated with different supersenses. Degrees of mem-bership in different supersenses are represented by feature vectors, where each element corre-sponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the super-sense noun.body . The value of the feature corre-sponding to this supersense is 3 / 33  X  0 . 09 .
Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-they produce a vector with a classifier posterior probabilities corresponding to degrees of mem-bership of this word in one of the 13 semantic for nouns and verbs. For example, for a word calm the top-2 categories (with the first and second highest degrees of membership) are adj.behavior and adj.feeling .
 Vector space word representations. We em-ploy 64-dimensional vector-space word represen-Vector construction algorithm is a variation on traditional latent semantic analysis (Deerwester et al., 1990) that uses multilingual information to produce representations in which synonymous words have similar vectors. The vectors were trained on the news commentary corpus released 3.3 Cross-lingual feature projection For languages other than English, feature vectors are projected to English features using translation which is a proprietary resource, but any bilingual dictionary can in principle be used. For a non-English word in a source language, we first ob-tain all translations into English. Then, we av-erage all feature vectors related to these transla-tions. Consider an example related to projection of WordNet supersenses. A Russian word  X  X  X  X  X  X  is translated as head and brain . Hence, we select all the synsets of the nouns head and brain . There are 38 such synsets (33 for head and 5 for brain ). Four of these synsets are associated with the su-persense noun.body . Therefore, the value of the feature noun.body is 4 / 38  X  0 . 11 . In this section we describe a training and testing dataset as well a data collection procedure. 4.1 English training sets To train an SVO metaphor classifier, we employ 3,737 manually annotated English sentences from the Wall Street Journal (Birke and Sarkar, 2007). Each sentence contains either literal or metaphori-cal use for one of 50 English verbs. First, we use a dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) relations. Then, we fil-ter extracted relations to eliminate parsing-related errors, and relations with verbs which are not in the TroFi verb list. After filtering, there are 953 metaphorical and 656 literal SVO relations which we use as a training set.

In the case of AN relations, we construct and make publicly available a training set contain-ing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and culled the col-lected metaphors, by removing duplicates, weak metaphors, and metaphorical phrases (such as drowning students ) whose interpretation depends on the context. 4.2 Multilingual test sets We collect and annotate metaphoric and literal test sentences in four languages. Thus, we compile eight test datasets, four for SVO relations, and four for AN relations. Each dataset has an equal number of metaphors and non-metaphors, i.e., the datasets are balanced. English ( EN ) and Russian (
RU ) datasets have been compiled by our team and are publicly available. Spanish ( ES ) and Farsi (
FA ) datasets are published elsewhere (Levin et al., 2014). Table 1 lists test set sizes.
 Table 1: Sizes of the eight test sets. Each dataset is balanced, i.e., it has an equal number of metaphors and non-metaphors. For example, English SVO dataset has 222 relations: 111 metaphoric and 111 literal.
 We used the following procedure to compile the EN and RU test sets. A moderator started with seed
Then she used the SketchEngine, which pro-vides searching capability for the TenTen Web cor-quently co-occurred with words from the seed lists. From these sentences, she removed sen-tences that contained more than one metaphor, and sentences with non-SVO and non-AN metaphors. Remaining sentences were annotated by several native speakers (five for English and six for Rus-sian), who judged AN and SVO phrases in con-text. The annotation instructions were general:  X  X lease, mark in bold all words that, in your opin-ion, are used non-literally in the following sen-tences. In many sentences, all the words may be used literally. X  The Fleiss X  Kappas for 5 English and 6 Russian annotators are: EN -AN = .76, RU -AN = .85, EN -SVO = .75, RU -SVO = .78. For the fi-nal selection, we filtered out low-agreement ( &lt; .8) sentences.

The test candidate sentences were selected by a person who did not participate in the selection of the training samples. No English annotators of the test set, and only one Russian annotator out of 6 participated in the selection of the training samples. Thus, we trust that annotator judgments were not biased towards the cases that the system is trained to process. 5.1 English experiments Our task, as defined in Section 2, is to classify SVO and AN relations as either metaphoric or lit-eral. We first conduct a 10-fold cross-validation experiment on the training set defined in Section 4.1. We represent each candidate relation using the features described in Section 3.2, and evalu-ate performance of the three feature categories and their combinations. This is done by computing an accuracy in the 10-fold cross validation. Experi-mental results are given in Table 2, where we also provide the number of features in each feature set. AbsImg 20 0.73  X  16 0.76  X  Supersense 67 0.77  X  116 0.79  X  AbsImg+Sup. 87 0.78  X  132 0.80  X  VSM 192 0.81 228 0.84  X  All 279 0.82 360 0.86 Table 2: 10-fold cross validation results for three feature categories and their combination, for clas-sifiers trained on English SVO and AN training sets. # FEAT column shows a number of features. ACC column reports an accuracy score in the 10-fold cross validation. Statistically significant dif-ferences ( p &lt; 0 . 01 ) from the all-feature combina-tion are marked with a star.

These results show superior performance over previous state-of-the-art results, confirming our hypothesis that conceptual features are effective in metaphor classification. For the SVO task, the cross-validation accuracy is about 10% better than that of Tsvetkov et al. (2013). For the AN task, the cross validation accuracy is better by 8% than the result of Turney et al. (2011) (two baseline methods are described in Section 5.2). We can see that all types of features have good perfor-mance on their own (VSM is the strongest feature type). Noun supersense features alone allows us to achieve an accuracy of 75%, i.e., adjective super-sense features contribute 4% to adjective-noun su-persense feature combination. Experiments with the pairs of features yield better results than in-dividual features, implying that the feature cate-gories are not redundant. Yet, combining all fea-tures leads to even higher accuracy during cross-validation. In the case of the AN task, a difference between the All feature combination and any other combination of features listed in Table 2 is statis-tically significant ( p &lt; 0 . 01 for both the sign and the permutation test).

Although the first experiment shows very high scores, the 10-fold cross-validation cannot fully reflect the generality of the model, because all folds are parts of the same corpus. They are col-lected by the same human judges and belong to the same domain. Therefore, experiments on out-of-domain data are crucial. We carry out such exper-iments using held-out SVO and AN EN test sets, described in Section 4.2 and Table 1. In this ex-periment, we measure the f -score. We classify SVO and AN relations using a classifier trained on the All feature combination and balanced thresh-olds. The values of the f -score are 0.76, both for SVO and AN tasks. This out-of-domain experi-ment suggests that our classifier is portable across domains and genres.

However, (1) different application may have different requirements for recall/precision, and (2) classification results may be skewed towards hav-ing high precision and low recall (or vice versa). It is possible to trade precision for recall by choos-ing a different threshold. Thus, in addition to giving a single f -score value for balanced thresh-olds, we present a Receiver Operator Characteris-tic (ROC) curve, where we plot a fraction of true positives against the fraction of false positives for 100 threshold values in the range from zero to one. The area under the ROC curve (AUC) can be in-terpreted as the probability that a classifier will as-sign a higher score to a randomly chosen positive example than to a randomly chosen negative ex-ROC curve is a dashed diagonal line. A bad classi-fier has an ROC curve that goes close to the dashed diagonal or even below it. Figure 1: ROC curves for classifiers trained using different feature sets (English SVO and AN test sets).

According to ROC plots in Figure 1, all three feature sets are effective, both for SVO and for AN tasks. Abstractness and Imageability features work better for adjectives and nouns, which is in line with previous findings (Turney et al., 2011; Broadwell et al., 2013). It can be also seen that VSM features are very effective. This is in line with results of Hovy et al. (2013), who found that it is hard to improve over the classifier that uses only VSM features. 5.2 Comparison to baselines In this section, we compare our method to state-of-the-art methods of Tsvetkov et al. (2013) and of Turney et al. (2011), who focused on classifying SVO and AN relations, respectively.

In the case of SVO relations, we use software and datasets from Tsvetkov et al. (2013). These datasets, denoted as an SVO -baseline, consist of 98 English and 149 Russian sentences. We train SVO metaphor detection tools on SVO relations extracted from TroFi sentences and evaluate them on the SVO -baseline dataset. We also use the same thresholds for classifier posterior probabilities as Tsvetkov et al. (2013). Our approach is different from that of Tsvetkov et al. (2013) in that it uses additional features (vector space word representa-tions) and a different classification method (we use random forests while Tsvetkov et al. (2013) use logistic regression). According to Table 3, we ob-tain higher performance scores for both Russian and English.
 Table 3: Comparing f -scores of our SVO metaphor detection method to the baselines.
In the case of AN relations, we use the dataset (denoted as an AN -baseline) created by Turney et al. (2011) (see Section 4.1 in the referred pa-per for details). Turney et al. (2011) manu-ally annotated 100 pairs where an adjective was one of the following: dark , deep , hard , sweet , and worm . The pairs were presented to five human judges who rated each pair on a scale from 1 (very literal/denotative) to 4 (very non-literal/connotative). Turney et al. (2011) train logistic-regression employing only abstractness ratings as features. Performance of the method was evaluated using the 10-fold cross-validation separately for each judge.

We replicate the above described evaluation procedure of Turney et al. (2011) using their model and features. In our classifier, we use the All feature combination and the balanced thresh-old as described in Section 5.1.

According to results in Table 4, almost all of the judge-specific f -scores are slightly higher for our system, as well as the overall average f -score.
In both baseline comparisons, we obtain perfor-mance at least as good as in previously published studies. 5.3 Cross-lingual experiments In the next experiment we corroborate the main hypothesis of this paper: a model trained on En-Table 4: Comparing AN metaphor detection method to the baselines: accuracy of the 10-fold cross validation on annotations of five human judges. glish data can be successfully applied to other languages. Namely, we use a trained English model discussed in Section 5.1 to classify literal and metaphoric SVO and AN relations in English, Spanish, Farsi and Russian test sets, listed in Sec-tion 4.2. This time we used all available features.
Experimental results for all four languages, are given in Figure 2. The ROC curves for SVO and AN tasks are plotted in Figure 2a and Figure 2b, respectively. Each curve corresponds to a test set described in Table 1. In addition, we perform an oracle experiment, to obtain actual f -score values for best thresholds. Detailed results are shown in Table 5.

Consistent results with high f -scores are ob-tained across all four languages. Note that higher scores are obtained for the Russian test set. We hy-pothesize that this happens due to a higher-quality translation dictionary (which allows a more accu-rate model transfer). Relatively lower (yet rea-sonable) results for Farsi can be explained by a smaller size of the bilingual dictionary (thus, fewer feature projections can be obtained). Also note that, in our experience, most of Farsi metaphors are adjective-noun constructions. This is why the AN FA dataset in Table 1 is significantly larger than SVO FA . In that, for the AN Farsi task we observe high performance scores.

Figure 2 and Table 5 confirm, that we ob-tain similar, robust results on four very differ-ent languages, using the same English classi-fiers . We view this result as a strong evidence of language-independent nature of our metaphor de-tection method. In particular, this shows that pro-posed conceptual features can be used to detect se-lectional preferences violation across languages.
To summarize the experimental section, our metaphor detection approach obtains state-of-the-Figure 2: Cross-lingual experiment: ROC curves for classifiers trained on the English data using a combination of all features, and applied to SVO and AN metaphoric and literal relations in four test languages: English, Russian, Spanish, and Farsi. art performance in English, is effective when ap-plied to out-of-domain English data, and works cross-lingually. 5.4 Examples Manual data analysis on adjective-noun pairs sup-ports an abstractness-concreteness hypothesis for-mulated by several independent research studies. For example, in English we classify as metaphoric dirty word and cloudy future . Word pairs dirty diaper and cloudy weather have same adjectives. Yet they are classified as literal. Indeed, diaper is a more concrete term than word and weather is more concrete than future . Same pattern is ob-served in non-English datasets. In Russian,  X  X  X  X - X  X  X  X  X  X  X  X  X  X  X   X  X ick society X  and  X  X  X  X  X  X  X  X  X  X   X  X mpty sound X  are classified as metaphoric, while Table 5: Cross-lingual experiment: f -scores for classifiers trained on the English data using a com-bination of all features, and applied, with optimal thresholds, to SVO and AN metaphoric and literal relations in four test languages: English, Russian, Spanish, and Farsi.  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X ick grandmother X  and  X  X - X  X  X  X  X  X  X  X  X   X  X mpty cup X  are classified as literal. Spanish example of an adjective-noun metaphor is a well-known m  X  usculo econ  X  omico  X  X conomic muscle X . We also observe that non-metaphoric ad-jective noun pairs tend to have more imageable ad-jectives, such as literal derecho humano  X  X uman right X . In Spanish, human is more imageable than economic .

Verb-based examples that are correctly clas-sified by our model are: blunder escaped no-tice (metaphoric) and prisoner escaped jail (lit-eral). We hypothesize that supersense features are instrumental in the correct classification of these examples: &lt; noun.person,verb.motion &gt; is usually used literally, while &lt; noun.act,verb.motion &gt; is used metaphorically. For a historic overview and a survey of common approaches to metaphor detection, we refer the reader to recent reviews by Shutova et al. (Shutova, 2010; Shutova et al., 2013). Here we focus only on recent approaches.
Shutova et al. (2010) proposed a bottom-up method: one starts from a set of seed metaphors and seeks phrases where verbs and/or nouns be-long to the same cluster as verbs or nouns in seed examples.

Turney et al. (2011) show how abstractness scores could be used to detect metaphorical AN phrases. Neuman et al. (2013) describe a Concrete Category Overlap algorithm, where co-occurrence statistics and Turney X  X  abstractness scores are used to determine WordNet supersenses that corre-spond to literal usage of a given adjective or verb. For example, given an adjective, we can learn that it modifies concrete nouns that usually have the supersense noun.body . If this adjective modifies a noun with the supersense noun.feeling , we con-clude that a metaphor is found.

Broadwell et al. (2013) argue that metaphors are highly imageable words that do not belong to a discussion topic. To implement this idea, they extend MRC imageability scores to all dic-tionary words using links among WordNet super-senses (mostly hypernym and hyponym relations). Strzalkowski et al. (2013) carry out experiments in a specific (government-related) domain for four languages: English, Spanish, Farsi, and Russian. Strzalkowski et al. (2013) explain the algorithm only for English and say that is the same for Span-ish, Farsi, and Russian. Because they heavily rely on WordNet and availability of imageability scores, their approach may not be applicable to low-resource languages.

Hovy et al. (2013) applied tree kernels to metaphor detection. Their method also employs WordNet supersenses, but it is not clear from the description whether WordNet is essential or can be replaced with some other lexical resource. We cannot compare directly our model with this work because our classifier is restricted to detection of only SVO and AN metaphors.

Tsvetkov et al. (2013) propose a cross-lingual detection method that uses only English lexical re-sources and a dependency parser. Their study fo-cuses only on the verb-based metaphors. Tsvetkov et al. (2013) employ only English and Russian data. Current work builds on this study, and incor-porates new syntactic relations as metaphor candi-dates, adds several new feature sets and different, more reliable datasets for evaluating results. We demonstrate results on two new languages, Span-ish and Farsi, to emphasize the generality of the method.

A words sense disambiguation (WSD) is a re-lated problem, where one identifies meanings of polysemous words. The difference is that in the WSD task, we need to select an already existing sense, while for the metaphor detection, the goal is to identify cases of sense borrowing. Studies showed that cross-lingual evidence allows one to achieve a state-of-the-art performance in the WSD task, yet, most cross-lingual WSD methods em-ploy parallel corpora (Navigli, 2009). The key contribution of our work is that we show how to identify metaphors across languages by building a model in English and applying it X  without adaptation X  X o other languages: Spanish, Farsi, and Russian. This model uses language-independent (rather than lexical or language spe-cific) conceptual features. Not only do we estab-lish benchmarks for Spanish, Farsi, and Russian, but we also achieve state-of-the-art performance in English. In addition, we present a comparison of relative contributions of several types of fea-tures. We concentrate on metaphors in the con-text of two kinds of syntactic relations: subject-verb-object (SVO) relations and adjective-noun (AN) relations, which account for a majority of all metaphorical phrases.

Future work will expand the scope of metaphor identification by including nominal metaphoric re-lations as well as explore techniques for incor-porating contextual features, which can play a key role in identifying certain kinds of metaphors. Second, cross-lingual model transfer can be im-proved with more careful cross-lingual feature projection.
 We are extremely grateful to Shuly Wintner for a thorough review that helped us improve this draft; we also thank people who helped in creating the datasets and/or provided valuable feedback on this work: Ed Hovy, Vlad Niculae, Davida Fromm, Brian MacWhinney, Carlos Ram  X   X rez, and other members of the CMU METAL team. This work was supported by the U.S. Army Research Labo-ratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533.

