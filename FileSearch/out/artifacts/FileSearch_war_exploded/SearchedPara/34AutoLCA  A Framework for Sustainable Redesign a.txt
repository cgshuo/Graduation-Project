 MANISH MARWAH and AMIP SHAH , HP Labs LAYNE T. WATSON and NAREN RAMAKRISHNAN ,VirginiaTech Increasingly, environmental sustainability is becoming an important criterion of prod-uct design and competitive differentiation. For example, about 75% of enterprise cus-tomers surveyed were expected to use some type of environmental criteria in their buying decisions [Plummer et al. 2008]. A common approach to quantify broad envi-ronmental impacts is the method of life cycle assessment (LCA) [Baumann and Tillman 2004], which takes a comprehensive view of multiple environmental impacts, such as, greenhouse gas emissions, resource consumption, toxicity, carcinogenicity, etc., across the entire life cycle of a product (from cradle to grave). LCA can, for example, be used to answer questions like which is more environmentally sustainable, an e-reader or an old-fashioned book? [Goleman and Norris 2010], or how does the environmental footprint of a Kindle TM compare with that of a Nook TM ?
However, performing LCA is not a straightforward process. A typical system may easily consist of several thousand input variables; for example, an environmental eval-uation of a server involves creating an inventory of all its components, usually down to parts, such as ICs, resistors, capacitors, fans, screws, heat sink, etc.; estimating their mass (or volume); and finally, manually mapping each component to representative entries in an environmental impact factor database. Since this process is so labor in-tensive, it is a challenge to estimate the environmental footprint of any realistic system, and designing to reduce environmental footprint requires deep domain expertise and experience.

Thus, two significant problems exist in achieving sustainable design on a large scale within reasonable cost and time bounds. First, an approach that allows for automation of the assessment process that eliminates the need for detailed manual data collection and inventory creation is desired. Second, a system that automatically provides a limited, narrow set of reasonable design recommendations for a given product would be very helpful.

In this article, we propose a framework (AutoLCA) composed of a palette of data mining algorithms X  X terative k -medoids clustering, classification, disparate cluster-ing, and constrained optimization algorithms X  X o address these problems. To un-derstand how these algorithms interoperate, it is helpful to view objects/nodes (i.e., components, parts, and processes) as existing in two domains X  X he substitutability do-main and the impact factor domain. The former is indicative of how replaceable one node is with another, captured by the node text description. For example, it might be possible to replace a particular kind of capacitor with another kind. The sec-ond domain corresponds to the impact factors of the nodes, which relates to their environmental sustainability. For instance, it has been claimed anecdotally that a Google search uses the equivalent energy of boiling a kettle of water, and thus while these two elements have widely different descriptions, they have similar sustainability characteristics.

First, we use iterative clustering methods in the substitutability domain to organize bill of material (BOM) data that is typically available for products in a company. An excerpt from a printed circuit board (PCB) BOM, listing a few components with attributes such as a part number and a short, unstructured text description is shown in Table I. Although a BOM provides a comprehensive listing of the components, it does not include their environmental impacts. We induce a naive Bayes classifier to map BOM nodes to nodes in an environmental impacts (EI) database. Nodes in the EI database have at least two attributes: a short, text description of the node, and a set of impact factor values. Table II shows an example of a few nodes in an EI database. The transformation of BOM nodes to the EI database achieves two purposes: (1) an automated mechanism for environmental assessment of BOM components to identify the components where redesign efforts should be focused; and (2) recommendation of sustainable design alternatives based on disparateness between the substitutability and impact factor domains, as shown in Figure 1.

To validate our approach, we applied it to real data for four different products to (1) estimate environmental footprint of the BOM components, (2) identify the top contrib-utors to a particular impact (carbon emissions), and (3) suggest design alternatives for the top impact contributors, which on implementation could reduce the carbon foot-print as much as high as 36%. The products we used are a printed circuit board (PCB) for an enterprise computer, a desktop computer (without screen), a laser jet printer, and a color laser jet printer. Although in some cases, the improvement is modest (as low as 1%), the millions of such products routinely purchased across the globe could add up to constitute a sizable contribution to sustainability. Our contributions are the following.  X  X e formulate sustainable design as a series of clustering X  X lassification X  X lustering tasks. We address the task of finding a set of more eco-friendly alternative com-ponents by a disparate clustering formulation between two domains in the EI database.  X  X e demonstrate how AutoLCA simplifies estimating the environmental footprint of the BOM by more than an order of magnitude over current methods.  X  X e validate our techniques on real data from a large computer manufacturer. A common tool used for environmental design is life-cycle assessment (LCA) [Baumann and Tillman 2004]. In this approach, detailed inventories are compiled for a given product across the material extraction, manufacturing, transportation, use, and dis-posal phases of a product or service. These inventories are aggregated, often by mass or energy, to obtain a comprehensive view of resources going into the product or system across the life-cycle. The aggregated inventory is then translated into environmental outputs using a set of known impact factors across predefined environmental impact categories [Glasson et al. 2005]. Once these impacts of a given product are known, the designer then tries to uncover alternative designs that might result in a lower environmental footprint. While simple in concept, sustainable product design is often challenging due to the requirements for extensive inventory data. For many complex products, the inventory could easily exceed several thousand components [Vigon 1994]. As a result, the time and cost associated with collecting accurate data across the entire life-cycle is often prohibitive. A few researchers have applied data mining techniques for sustainabile design and LCA [Patnaik et al. 2009; Marwah et al. 2011; Ramakrishnan et al. 2012; Sundaravaradan et al. 2011a, 2011b]. Sousa and Wallace [2006] develop an automated classification system to support LCA. Park and Seo [2006] propose a knowledge-based approximate life cycle assessment system to assess the environmental impacts of product design alternatives. However, neither of these consider product bill of material data.

The framework we have developed is analogous to transfer learning methods, where knowledge gained in one domain can be used to solve a similar or different prob-lem in another domain. Transfer learning methods have received attention in recent years [Ling et al. 2008; Davis and Domingos 2009; Kuhlmann and Stone 2007] due to the fact that solutions of real-life problems require diverse knowledge from other domains. They have been studied for cross-language text classification [Ling et al. 2008], to transfer knowledge within a number of domains including molecular biol-ogy, Web, and social networks [Davis and Domingos 2009], and in gaming domains [Kuhlmann and Stone 2007]. Since bill of material nodes and environmental impact factors are in two different domains, we solve the problem of finding design alter-natives in one domain (BOM) by transfering required knowledge from another (en-virnomental impact). In fact, the problem is actually one of transferring negative infor-mation, because we desire to identify nodes that are substitutable for each other but have different environmental impact factors, so that we can design more sustainable alternatives.

Our research goal of finding design alternatives also has parallels to work in identifying more than one clustering solution, including alternative clustering [Qi and Davidson 2009; Dang and Bailey 2010], subspace clustering [Agrawal et al. 1998; Cheng et al. 1999], nonredundant clustering/views [Cui et al. 2007; Gondek and Hofmann 2005], associative clustering [Sinkkonen et al. 2004; Kaski et al. 2005], meta-clustering [Caruana et al. 2006; Zeng et al. 2002], and consensus clustering [Monti et al. 2003; Strehl and Ghosh 2003]. A key distinguishing feature of our work is the formulation of information-theoretic functions for clustering alternatives using a uniform contingency table framework. While contingency tables have been employed elsewhere [Sinkkonen et al. 2002; Brohee and van Helden 2006], they have been used primarily as criteria to evaluate clusterings. The few works [Nadif and Govaert 2005; Greenacre 1988] that do use contingency tables to formulate objective criteria use them in the context of a specific algorithm, such as co-clustering or block clustering, whereas we use them to find design alternatives through disparate clustering. This work can also be viewed as a form of relational clustering [Hossain et al. 2010, 2013], because we use data from two different domains, the substitutability domain and the impact factor domain, to model the  X  X lternativeness X  property of two clusterings. Let B be a collection of bill of material nodes where B = X  i { p i  X  b i } , i = 1 ,..., n b are associated part numbers and short text descriptions of the nodes of the collection. The i th bill of material node is expressed as B i = { p i  X  b i } .Let E be a database that contains the environmental impacts of a collection of n e nodes. The attributes of E can be divided into two sets, t and e j , j = 1 ,..., l e where t is the text description attribute and e j  X  X  are the impact factor attributes. t forms a text description dataset T and l e impact factor attributes form a real-valued impact factor matrix X of size n e  X  l e .We use t s and x s , respectively, to refer to the text description and impact factor vector of the s -th node E s of E .

Recall that we address two main problems here: (1) environmental assessment of B i  X  X , where  X  X nvironmental assessment X  can be defined as the process of estimating different environmental impacts (such as carbon emissions, toxic effluents, energy use, etc.) for a given product; and (2) discovering more sustainable design alternatives to B i  X  X  with the largest footprint, which we reduce to finding alternatives to corresponding E i  X  X . Figure 2 shows the overall architecture of AutoLCA, which consists of two main parts. The top figure shows how we build a classifier to map BOM nodes to the environmental database and then prepare a list of design alternatives of all components. At first we cluster the BOM nodes using an iterative k -medoids clustering algorithm, then we map each medoid to the environmental database to map all node of each cluster. We train a Naive Bayes classifier on top of the mappings so that we can automatically map any new BOM to the environmental DB nodes. Finally, we use disparate clustering to find functionally similar components with disparate environmental impact factors, which provide candidates for more sustainable design recommendations.
 Figure 2 (bottom) shows how we can find an alternative design of a specific product. At first, we use the classifier we trained in the figure at top to map each node of the bill of materials of the product to the environmental DB nodes. Then we use a constrained optimization fit to find environmental footprint of each component. We also find the design alternatives from the list generated earlier. We suggest replacements for the components that have high environmental footprint with similar but more environment friendly components to design a more sustainable product.

We describe AutoLCA in detail in the following three sections.
Since BOM nodes do not have impact factor information, they need to be mapped to an environmental impact factor database. To this end, we used a real BOM dataset containing about 6.8K nodes, obtained from a large computer manufacturer, to build a classifier. Table I shows some sample bill of material nodes. Along with the short description, every bill of material node is associated with a part number. Similar nodes typically have a common prefix in their part numbers. They also have similar tokens in their description string. For example, a capacitor generally starts with  X  X ap X  or  X  X apacitor X , a resistor with  X  X es X  or  X  X esistor X , and an inductor with  X  X dut X  or  X  X nductor X . We main-tain a list of synonyms (Table III) to combine semantically equivalent terms. Further, these descriptions often contain different units such as those for capacitance, voltage, inductance, etc. The same unit can appear in different forms. For example, capacitance can have units pf (pico-Farad), uf (  X  -Farad), mfd (milli-Farad), etc. The appearance of any of these terms denotes a capacitor. To combine similar units to a single term we maintain unit lists (Table IV). We used several measures to calculate the dissimilarity between two bill of material nodes. All these measures involve two basic dissimilarity calculations: (1) D LC S  X  X ased on longest common substring (LCS), and (2) D LC P  X  X ased on longest common prefix (LCP) between two strings, defined by (1) Dissimilarity based on whole strings . (2) Token-based Euclidean distance. For this and the next metric, we tokenize the (3) Token-based dissimilarity. In this measure, we tokenize each of the bill of material
We analyzed all three distance measures ( d 1 , d 2 ,and d 3 ) using clustering results with feedback from an expert. Based on the analysis, we selected d 3 as the dissimilarity measure for the k -medoids clustering algorithm. Section 7.1 describes the details of this selection procedure. Mapping each of the bill of material nodes to the environmental database is a daunting task, for example, a typical BOM can contain several hundred to a few thousand nodes. Since we desire to minimize the human effort in this process, we propose mapping clusters instead of mapping the individual bill of material nodes. We use an iterative k -medoids clustering algorithm to bring similar bill of material nodes together in the same group. We use the k -medoids algorithm instead of k -means for two reasons. (1) The bill of material database contains short text descriptions, so it is very hard to (2) A long mean prototype in a vector form does not provide the expert an intuition Instead of manually mapping all the nodes of the bill of material database to the environmental database, the expert only maps the medoids of the clusters. This reduces the effort of mapping by orders of magnitude.

We construct the clusters iteratively using a k -medoids clustering algorithm. We il-lustrate this iterative clustering process in Alg. 1. We, at first, generate clusterings with different number of clusters and select the best clustering using overall the Aver-age Silhouette coefficient (ASC) (steps 2 and 3). We then set a sum of squared distance (SSD) and an ASC threshold to select candidate clusters from the selected clustering for the next iteration (step 4). SSD measures the cohesion of the clusters and ASC is a measure that takes both cohesion and separation into account. Lower SSD and higher ASC values are better. We do not set any absolute value of SSD as a threshold, rather we focus on a plot of SSD where the horizontal axis refers to clusters sequenced by the ascending order of their corresponding SSD. We calculate the rate of increase of SSD (slope of the curve) at every point and find a threshold where it increases rapidly. However, we set a fixed ASC threshold of 0 to select clusters for the next iteration. Any negative ASC value of a cluster indicates overlap with other clusters, and we make the nodes of those clusters to be candidates for the next level of clustering.
The number of clusters k can vary from iteration to iteration. In our experiment, a domain expert provided the maximum number of clusters k m that would be acceptable for manual mapping. As illustrated in Alg. 1, at every iteration, we attempt to generate around 50% of the left number of clusters. Instead of strictly generating 50% of the left clusters at any certain iteration, we generate a range of number of clusters around 50%. For example, in the first iteration, we generate clusterings with different numbers k . We generated clusterings with different k within this range and selected the one with the highest ASC.

The purpose of the bill of material node clustering is not to present a good set of clusters of the entire dataset, but rather to present a subset of clusters that contain enough varieties of nodes in different clusters so that one medoid can represent an entire group of bill of material nodes. An expert maps the medoids of the clusters of the bill of material nodes to the environmental database. At the end of all iterations, we had 80 clusters with 80 representative medoids. An expert manually mapped all of these 80 medoids to the nodes of environmental database. Mapping only the medoids results in a reduction of almost two orders of magnitude compared to mapping all the bill of materil nodes. Section 7.2 provides the detailed results. The purpose of clustering the bill of material nodes is to reduce the human effort of manual mapping to the environmental database. As stated earlier, all the clusters generated by the iterative k -medoids clustering algorithm are given to an expert. The expert manually maps the clusters medoids to the environmental database. Note that the expert can avoid mapping a cluster if he/she feels that the medoid is not a represen-tative one for that cluster. k b  X  e n , where k b is the number of cluster-medoids mapped to e n unique environmental DB node. After the mapping, we consider that each of the BOM nodes of each cluster also maps to the same environmeltal DB node as the corre-sponding medoid. Now that we have a number of such mappings, we build a classifier on top of it. Any new bill of material node can be mapped to an environmental DB node using this classifier. We use a naive Bayes classifier for this purpose.

Sections 7.3 and 7.4 depict experimental results on automatic mapping and how terms class probabilities can be used to characterize each cluster. Here we formulate a confidence score for the naive Bayes classification-based automatic mapping we use. Let all the T terms of all the descriptions of the training set be ordered in descending order of their conditional probabilities ( P ( t | e )) of being mapped to a particular environmental node e . The confidence score of a bill of material node b ,if b is mapped to environmental DB node e and binary vector B records presence/absence of each of the T terms in b , is computed by the following formula:
Note that this confidence score S ( e ) of a bom of being mapped to environmental node e ranges from 0 to 1.0, where 0 indicates that none of the terms of the bom node match with the important terms of the description of e and 1.0 indicates a complete match. In practice, a complete match is rare, because the number of terms in a BOM node  X  b T . However the match can be very close to 1.0 becasue of the negative exponent used in the formula for higher order indices. In Section 7.3, we show that our classification technique maps BOM nodes to EI nodes with high confidence. In Section 4, we described how BOM nodes can be mapped to e n unique nodes in the environmental database. In this stage of the framework, we use these e n nodes as seeds to form clusters around them. We select a set of neighbors for each of these e n environmental nodes based on their text descriptions. We might obtain k e clusters which is less than e n because some of the seeds can be neighbors of each other. We select only those nearest neighbors that contain at least the first token of the seed node. A text cluster with a seed related to  X  X apacitor X  is shown in Table V. It shows that all the elements of the cluster are related to capacitor. We restrict the neighbor search by applying the constraint that the neighbors should contain at least the first token of the seed node because we desire the text-description based clusters to be cohesive enough to contain similar as well as substitutable elements.

Now that we have the clusters based on neighbors and text descriptions, we seek to generate an impact factor clustering that is cohesive in the impact factor space as well as dissimilar from the text-based clustering. Note that the text based k e clusters group functionally similar components. We seek to identify a second clustering that would contain components possessing similar impact factors, at the same time, we desire this second clustering to be disparate from the text clustering. The disparateness between functionality (text) based clustering and environmental impact based clustering would provide us the design alternatives. Let T be the text dataset with k e clusters and X be the corresponding environmental impact factor dataset. Each of the text descriptions of T is associated with one vector of X forming an implicit one-to-one relationship. Let us consider that there are l x impact factors, that is, x s  X  R l x .

Since we desire to find design alternatives for each of the clusters of T , we expect the elements of each cluster of T to be scattered over multiple clusters of X .Let C ( t ) and C ( x ) be the cluster indices, that is, indicator random variables, corresponding to T and X and let k be the corresponding number of clusters. Thus, both C ( t ) and C ( x ) takes values in { 1 ,..., k } .

Let m i , X be the prototype vector for cluster i in impact factor vector-set X . m i , X  X  X  are precisely the quantities we wish to estimate/optimize for, but in this section, assume they are given. Let v ( x s ) i be the cluster membership indicator variables, that is, the to the cluster membership indicator variables for the text dataset T . Thus, k i = 1 v ( x s ) i =
Ideally, we would like a continuous function that tracks these hard assignments to a high degree of accuracy. A standard approach is to use a Gaussian kernel to smooth out the cluster assignment probabilities: where D = max s , s || x s  X  x s || 2 , 1  X  s , s  X  n is the pointset diameter.  X / D is the width of the Gaussian kernel. Notice that D is completely determined by the data but  X  is a user-settable parameter, and precisely what we can tune. T and X are two different but related datasets of the environmental database E .As stated earlier, the relationships between them is one-to-one. At this point, we have the clusters of T and we seek to identify a clustering based on the impact factors. One straightforward approach is to apply k -means clustering on X . In an ideal case, this ap-proach will not provide any difference between the text clusters and the impact factor vector-set clusters. Figure 3 (left) shows this ideal situation, where every element of a text cluster is mapped to the elements of exactly one cluster of the impact factor clus-tering. This situation does not provide any information regarding design alternatives. In contrast, Figure 3 (right) shows a scenario where the elements of a text cluster are distributed among multiple impact factor clusters. The benefit of this second clustering is that we can find design alternatives for the nodes of the text clustering. For example, in Figure 3 (right), we find that capacitors 7011 and 7012 can be used as alternatives of capacitor 7013. Similarly, resistor 7068 has two alternatives 7069 and 7070. The disparateness between the text clustering and the impact factor vector-set clustering provides us the information about design choices.

Now, we desire to identify a clustering in X which maintains its own locality but is different than a given set of assignments in the text dataset T . We formulate this problem in a disparate clustering fashion. The assignments learnt from the nearest neighbors of the medoids of T will be used to influence the clustering of X in addition to the local memberships of X .

We construct a k  X  k contingency table to capture the relationships between entries in clusters across T and X . We simply iterate over the implicit one-to-one relationships between T and X : we suitably increment the appropriate entry in the contingency table s th text description and impact factor vector, respectively.

We also define w i . = k j = 1 w ij ,w . j = k i = 1 w ij , where w i . and w . j are the row-wise and column-wise counts of the cells of the contingency table, respectively. Table VI shows two contingency tables before and after our disparate clustering framework is applied. Note that each of these contingency tables capture the relationships between two clusterings.

We will find it useful to define the distributions of the row-wise and column-wise random variables:
The row-wise distributions represent the conditional distributions of the clusters in T given the clusters in X ; the column-wise distributions are also interpreted analogously. Now that we have a contingency table, we must evaluate it to see if it reflects dis-parateness of the two clusterings. Ideally, we expect that the contingency table would be uniform in a perfect disparate clustering if the clusters of T are of equal size. Therefore in the ideal case of our objective criterion, we compare the row-wise and column-wise distributions from the contingency table entries to the uniform distribu-tion. If the clusters of T are not of equal size, then the relationships of X should be compared with the cluster distribution of T . Note that the distribution of the relation-ships of T is still uniform, since the clustering of X is unknown. We use KL-divergences to define the objective function (lower values are better): where U is the uniform distribution over k clusters and U T is the probability distri-bution of the elements of k text clusters, and the second two terms help guard against degenerate solutions.

We minimize this objective function over the mean prototypes of the impact factor dataset X . For this optimization purpose, we adopt an augmented Lagrangian formu-lation with a quasi-Newton trust region algorithm. In order to estimate the environmental impact of components of a BOM, they need to be mapped to nodes in the EI DB. The naive Bayes classifier addressed in the previous section, provides this transformation. However, one pending challenge with the mapping is that the units specified on the BOM and EI DB may differ. For example, most product BOMs specify the number of repeating instances for a particular part number, while the environmental DB nodes may be specified per mass (kg) or volume. Considering a given BOM as a tree with the product or part node as the root and its components as its children, we can address this problem by recognizing the following property X  X he parent impact is approximately equal to the sum of the impacts of the children. Knowing the quantity, in compatible units (usually weight or volume), of any EI node (parent or any child) in the product allows this can be formulated as a constrained optimization problem: where W = abs( Ax  X  I p ), I p is the impact factor vector of the parent node, A is a matrix with l e rows ( l e is the total number of impact factors) and N c columns ( N c is the number of child nodes), and is a coefficient or weight associated with the parent node. Note that x is a vector-valued variable of length N c . Minimization of x allows the environmental impact of all the BOM components to be estimated. The en-vironmental impact L j of the j th factor can be computed as L j = N c i = 1 A j , i x i , where A , i corresponds to the ( j , i )th cell of matrix A and x i corresponds to the i th element of x . We draw a pie chart for an impact factor to illustrate the corresponding envi-ronmental footprint. Section 7.6 describes some of these pie charts for a number of products. In this section, we present a comprehensive evaluation of AutoLCA against a broad range of qualitative and quantitative measures. We apply AutoLCA on real data from a large computer manufacturer. The dataset we use has a total of 6,812 nodes from thirteen different bill of materials. The environmental database has 3,949 nodes with more than 200 impact factors. Then, we specifically apply AutoLCA to four different BOMs, evaluate them on the basis of carbon impact of the child nodes, present a list of substitutes resulting from disparate clustering to an expert, and finally, estimating the potential carbon savings from substitutions that may be feasible based on the recommendations of the expert.

The specific questions we seek to answer in this section are as follows. (1) What is the performance of the distance measures introduced in Section 4.2? (2) Does the iterative k -medoids clustering algorithm provide cohesive clusters? (3) How well does automatic classification perform in mapping bill of material nodes (4) How can we characterize the BOM clusters? (Section 7.4) (5) Can disparate clustering bring functionally similar (substitutable) but environ-(6) Can AutoLCA recommend viable alternatives for product components resulting in We analyzed all three distance measures ( d 1 , d 2 ,and d 3 ) introduced in Section 4.2 using clustering results with feedback from an expert. We generated 50 clusters with these three dissimilarity measures and the expert then marked BOM nodes wrongfully placed in each cluster. We observed that the whole string based dissimilarity d 1 and the token &amp; LCS based dissimilarity d 3 have small number of clusters with large error. On the other hand, the Euclidean distance d 2 results in a number of clusters with large error. A distribution analysis on the number of erroneous BOM nodes in the clusterings using three similarity measures lead us to choose either d 1 or d 3 as our dissimilarity measure because the very few clusters with erroneous BOM nodes can become candidates for  X  X e-clustering X  and the rest of the clusters are cohesive enough to keep as they are. The total percentages of erroneous BOM nodes in these three clusterings with d 1 , d 2 ,and d 3 are 29.5%, 32.2%, and 18.8%, respectively. We finally select d 3 because it provides clusters with the lowest amount of erroneous elements. We perform an SSD and ASC analysis at the end of each iteration of the iterative k -medoids clustering. Note that some of the very high SSD and negative ASC clusters are removed at the end of each iteration which become candidates for the next iteration of the algorithm. Our observation is that at the end of every iteration, there are a few clusters with high SSD and negative ASC that contain a heterogeneous mix type of bill of material nodes. The algorithm stops iterating if all (or a desired percentage) of these nodes can be assigned to any of the previous good clusters. For our dataset with 6,812 bill of material nodes, the algorithm stopped after the third iteration leaving 864 nodes. These 864 nodes did not have enough resemblance with the elements of the existing clusters and they did not have enough structure to form a new clusters either. As stated earlier in Section 4.3, at the end of all iterations we had 80 clusters with 80 representative medoids which were manually mapped to the environmental DB nodes by an expert. Eighty medoids generated by our iterative k -medoids clustering were mapped to 30 unique environmental DB nodes by an expert. For the classification purpose, we con-sider that each of the BOM nodes of each cluster also maps to the same environmental DB node as the corresponding medoid. We build a naive Bayes classifier on top of this mapped data. We have a total of 1,932 terms in 5,948 bill of material descriptions of the 80 clusters. All these terms become features for the naive Bayes classifier.
We found that the accuracy of the classifier improves (around 4%) with the use of synonyms along with the units. More importantly, it is essential to keep the synonyms in the same bucket for characterization (described in the following subsection). We evaluate the mapping process using cross validation technique. We calculated the accuracies using different number of folds ranging from two to eight with varying number of suggested mappings ranging from one to five. The plot of Figure 4 (left) shows that the classifier has more than 95% accuracy when we use 3-to 8-folds with five suggested mappings from a pool of 30 options. With the lowest amount of training (2-fold), the accuracy is still more than 90% with five suggestions. The accuracy is more than 80% with any number of folds for more than one suggestions. The accuracies of the mappings are fairly high for a pool of thirty class labels.

We applied naive Bayes classification on 5,948 nodes of all the clustered data and computed confidence score (Section 4.5) for each of the nodes. Figure 4 (right) shows the distribution of the confidence scores computed over all 5,948 nodes. We observed that around 90% of the bill of material nodes have confidence score greater than 0.6 which indicates high confidence in automatically mapping the bill of material nodes using naive Bayes classification. The most frequently occurring terms in a BOM cluster allow us to characterize it by associating (tagging) that cluster with those terms. In fact, this can assist an expert in the manual mapping process. After we have the medoids (clusters) mapped and the classifier trained, we can characterize the different classes using the terms with high class probabilities. These terms could also be shown to the user to justify the catego-rization of a new BOM node to a particular EI DB node. The top left plot of Figure 5 shows the list of terms in the horizontal axis ordered based on their probabilities of being mapped to EI DB node 7013 (capacitor). All the BOM clusters mapped to EI DB node 7013 can be characterized by the terms,  X  X apacitor X ,  X  X ap X ,  X  X apacitance X ,  X  X olt-age X , etc. In the same figure, we show three more similar plots for inductor, integrated circuit, and resistor related clusters. We have a total of thirty such plots (only four of them are shown in Figure 5) to characterize 80 BOM clusters.
 For the disparate clustering phase of the experiments, the experts selected 46 impact factors of their interest out of more than 200 for ease of analysis and verification. As described in Section 5, we construct text description clusters based on neighbors in the EI database. We seek to identify a second clustering that would contain components possessing similar impact factors, at the same time, we desire this second cluster-ing to be disparate from the text clustering. We obtained a total of 170 nodes around 30 seeds. (These 30 seeds are the unique environmental DB nodes used as class labels in Section 7.3). The disparateness between functionality (text) based clustering and envi-ronmental impact based clustering would provide us the design alternatives. Table VII shows some text-based clusters ( C 1 ) and the disparate cluster IDs ( C 2 ) of the corre-spondign nodes. In Table VII, two integrated circuits (7016 and 7015) are shown to be in the same text-based cluster but they are in two different clusters in the impact factor-based disparate clustering. Similarly, capacitor 7014 has two alternatives (7010 and 7013), all of which are in different impact factor cluster than capacitor 7014. Figure 6 shows distributions of percentage difference of the impact factors between capacitor 7014 and two of its alternatives. A positive difference indicates that the alternative has lower impact whereas a negative difference indicates that a replacement with the alter-native would result in a higher impact at that specific factor. Note that 7010 has higher positive difference than 7013. On the other hand, 7013 has higher negative difference than 7010 indicating that the impact factors of these two alternatives are very diverse. We apply AutoLCA to four specific products: a printed circuit board (PCB) for an enterprise computer, a desktop computer (without screen), a laser jet printer, and a color laser jet printer. The BOM of these products is mapped to EI nodes using the classifier. In order to determine the quantity of each component in the products (which in turn is used to assess the environmental impact of the components), we solve the constrained optimization problem described in Section 6. The results are summarized in Table VIII and show good fits.

After determing the component impact factors, these can be used to perform a carbon hotspot analysis, where essentially a Pareto list of the biggest environmental contribu-tors to the parent footprint is generated so that a designer or LCA practitioner can zoom in on where further efforts should be focused. Figure 7 shows such a hotspot analysis of the four products for the impact of carbon emissions. Both for PCB and desktop, the largest contributors are integrated circuits (ICs) (85% for PCB and 33% for desktop; note that for desktop, the printed wiring board itself would contain ICs, so essentially this percentage would be actually higher). This is followed by discrete components such as transistors, resisters, capacitors. This is quite similar to the results uncovered through a manual LCA, where the ICs were found to have the largest contributions. For the printers, the largest contributor is the toner, which again matches manual studies. However, aluminum is higher than expected, particularly for the laser jet printer. The contribution of epoxy (plastics) also matches well. A designer desiring to further re-duce the carbon footprint of any of these products can correctly identify where efforts should be focused. This is important, because it becomes possible for someone who has no LCA or environmental background to automatically obtain feedback regarding the sustainability of their design.
 Next, we aim to provide suggestions for more sustainable design of the four products. For each product, we look for child nodes that belong to the same cluster in C 1 (text-based clusters) but in different ones in C 2 (disparate clusters). Such nodes are likely similar in functionality but different in their environmental impacts, making them good candidates for substitution in order to reduce environmental footprint. A list of such substitutions were presented to an expert to advise on their feasibility, based on which the savings in the carbon footprint of each product was estimated as shown in Table IX. While redesign of desktop results in a potential savings of 36% in the carbon footprint, for the PCB and the printers, the savings are in the range of 1 X 2%. The reason for low savings in the printers is that the main contributors to carbon footprint, namely, toner, epoxy and chromium steel do not have good substitutes in the database.

The fact that the savings are not large for the printers is very useful from a prac-titioner X  X  standpoint X  X t essentially means that Auto-LCA can be used as a tool to expediantly decide where further Design for Environment (DfE) efforts should be fo-cused. For example, it could help a large company to decide which class of its prod-ucts it should focus on first (e.g., desktops vesus printers). Thus Auto-LCA is use-ful in identifying environmental impact hotspots both within a product and across products. We have proposed a framework for environmental assessment and redesign of prod-ucts from their bill of materials. Sustainable redesign is formulated as a transfer learning problem to map between diverse attribute spaces. A detailed case study on several complex electronic products shows that our framework can expeditiously deter-mine the environmental hotspots in products providing valuable information on which components to focus on for redesign. For the four products considered, our redesign recommendations based on disparate clustering can reduce carbon footprint from 1% to 36%, enabling companies to decide which class of products is most suitable for envi-ronmental redesign. In future we will expand our work to other domains to be able to further assess the compatibility of the discovered design alternatives.

