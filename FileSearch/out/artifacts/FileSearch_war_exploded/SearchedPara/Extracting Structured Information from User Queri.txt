 When search is against structured documents, it is bene-ficial to extract information from user queries in a format that is consistent with the backend data structure. As one step toward this goal, we study the problem of query tag-ging which is to assign each query term to a pre-defined category. Our problem could be approached by learning a conditional random field (CRF) model (or other statistical models) in a supervised fashion, but this would require sub-stantial human-annotation effort. In this work, we focus on a semi-supervised learning method for CRFs that utilizes two data sources: (1) a small amount of manually-labeled queries, and (2) a large amount of queries in which some word tokens have derived labels , i.e. , label information au-tomatically obtained from additional resources. We presen t two principled ways of encoding derived label information i n a CRF model. Such information is viewed as hard evidence in one setting and as soft evidence in the other. In addition to the general methodology of how to use derived labels in semi-supervised CRFs, we also present a practical method on how to obtain them by leveraging user click data and an in-domain database that contains structured documents. Evaluation on product search queries shows the effectivenes s of our approach in improving tagging accuracies.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Search process ; I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms, Experimentation Semi-supervised learning, conditional random fields, info r-mation extraction, metadata
The World Wide Web is a large reservoir of information that is still growing at a rapid rate. Unlike data found in a database, the vast majority of the Web documents are only semi-structured, making information retrieval a challeng ing task. In recent years, there has been a surge of interest in extracting structured information from Web documents and converting the extracted data into database objects [2, 16, 20, 17]. Moreover, many vertical search engines crawl data directly from a relational database, and their indexes con-tain highly structured information that is less ambiguous i n nature. Such data representations, if appropriately utili zed, can greatly help enhance search experience.

When search is against structured data, it is beneficial to extract information from user queries that is explicitly re p-resented in a structured form. In this work, we study the problem of query tagging as one step toward this goal. More specifically, we view a query as a sequence of word tokens; given a set of pre-defined fields (or states), our aim is to as-sign each word token a label indicating which field it belongs to. In particular, we focus our attention on tagging prod-uct search queries, since this is one representative domain where structured information can have a substantial influ-ence on search experience. Below is an example showing a product search query annotated with field information. The complete schema of field definition is given in Table 1 with details given in Section 5.
For our task, it is possible to construct (from a database) field-dependent lexicons that enumerate possible values fo r each field. However, a pure lexicon-lookup based approach is insufficient largely due to the presence of ambiguous words. For example, the word  X  X ed X  is in most cases seen as At-tribute , but can mean Brand in the context of  X  X ed hat X . Another challenge is that users may formulate queries us-ing out-of-lexicon words. For these reasons, we take a sta-tistical approach to our problem for its known success in sequential labeling tasks. In particular, we use condition al random fields (CRFs) [10] that model probabilistic depen-dencies between two consecutive fields and between fields and observations. Indeed, despite the fact that user querie s are commonly viewed as bags of words, we found that the field ordering in product search queries does display statis -tically significant patterns that can help sequential label ing.
If queries fully-annotated with field information were avai l-Table 1: A example schema that defines fields in product search queries. The top and bottom rows contain fields that correspond to product metadata and product-independent keywords respectively. able in abundance, our problem could be approached by learning a CRF model in a supervised fashion. However, supervised training data will hardly ever be sufficient in thi s space as the volume of web search queries is extremely large. Often times, while it is expensive to ask human annotators to label a large amount of queries, it is relatively easy to ob -tain X  X abels X  for some word tokens from additional resource s. We use the term derived labels to refer to partial labeling information obtained in this fashion. This should be distin -guished from manual labels which are acquired via human annotation. Note that derived labels may be available only for a subset of word tokens in a query, and thus cannot be directly used as supervised training data. Moreover, such information is often noisy depending on the data source.
In this work, we investigate a semi-supervised learning method for CRFs that utilizes two sources of information: (1) a small amount of manually-labeled queries, and (2) a large amount of queries with derived labels obtained in an unsupervised fashion. Our goal, then, is to make use of these two data sources to learn a better CRF model. Our contribution is two-fold. First, we assume the availabilit y of the second data source and explore two principled ways of incorporating such information into conditional random fields. In one setting, derived labels are considered as hard evidence on the corresponding state variables. In the sec-ond setting, derived labels are used as soft evidence in the form of a feature function, expressing preferences over val -ues of state variables. Both approaches are expressed in a graphical model framework, and their respective optimiza-tion objectives are discussed.

Our second contribution is to present a practical method that generates derived labels for a large amount of queries with minimum human supervision. We demonstrate our ap-proach in the product search domain. Suppose that we have access to user click data in the form of ( query , product title ) pairs, and to a product database that contains product ti-tles and their corresponding field data. Then we are able to associate queries with their relevant field data. This enabl es us to automatically attach labels to some word tokens in queries, and to use them as derived labels in the proposed semi-supervised learning paradigm.

We evaluate our approach in the task of tagging product search queries, while our method is general enough to be applied to other domains (such as local search and sports search). Experiments show that our proposed approach can significantly increase tagging performance on practical da ta.
There are not many works on query tagging that we are aware of. The only slightly relevant work is by [3] on part-of-speech tagging of queries. Therefore, we review related works mostly from a machine learning perspective.
A good number of semi-supervised methods for CRFs have been published in recent years. One school of approaches makes the standard assumption that, in addition to a small amount of labeled data, there exists a large amount of unla-beled data. In this setting, it is natural to apply self-training [18] which trains a seed model using the labeled data, and it-eratively uses high-confidence predictions on the unlabele d data to expand the training set. This approach, however, lacks a theoretical justification for optimality, unless ce rtain non-trivial conditions are satisfied [1].

A common challenge to applying semi-supervised learn-ing to CRFs is that the entire state sequences of unlabeled data are hidden. Since CRFs have a maximum conditional-likelihood objective, the Expectation-Maximization (EM) used in generative models [8] is not directly applicable. To solve this problem, entropy minimization , originally pro-posed by [7] and extended to CRFs by [9], aims to maxi-mize the conditional likelihood of labeled data while mini-mizing the conditional entropy of unlabeled data. Another approach, referred to as JESS-CM [15], embeds a generative model (HMMs) into the CRF framework with an objective that can be iteratively optimized.

A second group of works makes an additional assumption that alternative label resources can be utilized in learnin g; and our work falls into this category. When unlabeled data have partial labeling information, it becomes possible to o p-timize a CRF model using the EM algorithm. This is essen-tially the first approach we explore in this work that treats derived labels as hard evidence. The idea is also akin to the methodology proposed by [14] to integrate hidden variables in CRFs. Another work, generalized expectation [6, 12], uses aggregated  X  X erived label X  X nformation to regularize the c on-ditional distributions of a state variable given individual features. Our second approach bears a resemblance to their idea in that derived labels are viewed as soft evidence to bias the values of state variables. The key difference is that we use such information in the context of an input sequence to provide local, as opposed to global, preferences over sta te hypotheses. In this regard, our method is analogous to the use of virtual evidence in directed graphical models [4]. But our goal is to optimize a discriminative model, instead of a generative model, that incorporates such evidence.
Additionally, there are a fairly large amount of works on using additional resources for semi-or un-supervised info r-mation extraction in general. Here we only mention the most relevant few. In [5], a database is used to create artificiall y-annotated training data or to train a language model for an HMM-based sequence labeler. A similar approach is used by [19] for CRF-based text segmentation. Both works resort to relational tables to create field labels for text segments. I n our work, we additionally make use of click data to maxi-mally reduce ambiguity in this process.
Linear-chain CRFs have been widely used in sequential la-beling tasks such as part-of-speech tagging and informatio n extraction [10, 13]. We choose to apply CRFs to our task for its ability of incorporating arbitrary features functi ons on observations without complicating the training. For-mally, we let x = ( x 1 , x 2 , . . . , x T ) denote an input query of T terms, and y = ( y 1 , y 2 , . . . , y T ) the corresponding state (field) sequence. Each y t can take on a pre-defined categor-ical value. We further augment a state sequence with two special states: Start and End , represented by y 0 and y T +1 respectively. The conditional probability p ( y | x ) is given by p ( y | x ;  X ) = The partition function Z ( x ;  X ) normalizes the exponential form to be a probability distribution. f k ( y t  X  1 , y t feature functions, and  X  = {  X  k } are their corresponding weights. There are typically two types of features used in first-order, linear-chain CRFs: transition features and emis-sion features . A transition feature is a binary function that indicates whether a transition ( y t  X  1 = i, y t = j ) occurs, i.e. , An emission feature is a binary function that indicates whet her a observation-dependent feature co-occurs with state j . For example, a unigram feature function is defined as where w represents a unigram. In a more general form,  X  ( x t = w ) can be replaced with an arbitrary function on x . Different forms of this function would express different characteristics of the input query.

Given a set of manually-labeled queries { ( x ( i ) , y ( i ) we can estimate model parameters in a supervised fashion. This training paradigm is expressed in the graphical model language in Figure 1(a). Note that the decoding graph for CRFs is the same as Figure 1(a) except that the entire state sequence becomes hidden. In supervised training, we aim to estimate  X  that maximizes the conditional likelihood of training data while regularizing model parameters: The objective can be optimized using stochastic gradient descent, generalized iterative scaling, or other numerica l op-timization methods.
When applying CRFs to query tagging and particularly product search query tagging, it is curious to ask if such a model is suitable for solving our problem. A major ques-tion one may raise is that CRFs assume a probabilistic de-pendency between two consecutive states, which may not have a strong presence in queries. If that were the case, an instance-based classifier that tags each word token inde-pendently would be more appropriate. We thus randomly selected 4.5K product search queries across different cate-gories, and had them manually labeled based on the schema in Table 1. Interestingly, we found that the distribution of transition features is rather skewed in this dataset. For ex -ample, ( y t  X  1 = Type , y t = End ) occurred in 80% of the queries, meaning that most queries in this dataset end with the field Type . This suggests that CRFs could be more effective than instance-based models in tagging product search queries, a s will be supported by experimental evidence in Section 6.
In addition to transition features that are implicitly as-sumed by CRFs, we introduce three types of emission fea-tures for our task. First, we use ngram features including both unigrams and bigrams. A unigram feature has been de-fined in Equation (3); and a bigram feature can be defined in a similar way. Specifically, for a given state variable in the linear chain, we use the current word and its preceding word to form a bigram, i.e. , We can also use the bigram that consists of the current word and its following word or use both types of bigrams simul-taneously, but did not observe significant performance dif-ference with these alternatives. The use of bigrams offers contextual information that is helpful in word disambigua-tion. Consider the examples in Table 1. The word  X  X uy X  is typically seen as BuyingIntent , but most likely means Mer-chant in the context of  X  X est buy X  (same with the earlier example  X  X ed X  and  X  X ed hat X ). When used in a CRF model, each ngram feature is assigned a separate weight, providing fine-grained information for sequential labeling. However , this can cause overfitting if the training data is sparse.
To improve the generalization ability of our model, we introduce a second type of features referred to as regular expression (regex) features : where x t  X  r means that x t matches the regular expres-sion r . For example, sd700 , sd800 and sd850 all match the regular expression  X  X a-z]+[0-9]+ X  (in the pattern matchin g language). This can be useful in representing word tokens that correspond to fields like Model and Attribute . Further-more, we introduce lexicon features which are given by Here L denotes a lexicon of words or phrases, and this fea-ture is activated if x t occurs in that lexicon. Field-dependent lexicons, e.g. , a Brand lexicon, can be extracted from a prod-uct database, enumerating possible values for each field. Th e advantage of using such features is that they generalize to words that do not occur in the training data.
The last section gave an overview of CRFs in a supervised learning paradigm. While it is not always feasible to have a large amount of manually-labeled data, it is often easy to automatically obtain derived labels for some word tokens. Here we make the following distinction between manual and derived labels:
As one can imagine, derived labels can be valuable to learning since they may offer information, often in a vast amount, complementary to that provided by manual labels. We will leave the discussion of how to obtain derived labels for a specific domain in Section 5. In this section, we present a general methodology of incorporating derived labels in CRFs for semi-supervised learning. For this we assume the availability of two data sources: (1) a set of manually-labe led ples with derived labels, denoted by { ( x ( i ) , z ( i ) goal, then, is to learn a CRF model leveraging these two sources of information.
One natural solution is to treat derived labels the same as manual labels, and use them as hard evidence on state variables. Specifically, for queries in the second data sour ce, we assume that a state variable is observed with value y t z , if z t 6 = null , and is hidden otherwise. This training setting is depicted in the graphical model in Figure 1(b).
We let y ( i ) o denote the set of observed state variables, and let y ( i ) h denote the complement set of state variables that are hidden. Our goal is to maximize the conditional likelihood of the incomplete data , i.e. , log p ( y o | x ), which is not directly optimizable. However, we can apply the EM algorithm that iteratively maximizes its low bound. Our learning objectiv e, therefore, is given by The first term is the same as Equation (4). The second term denotes the expected conditional likelihood of auto-label ed data. This is akin to the optimization objective of hidden-state CRFs [14]. In the E-step, we compute the posterior on the current model  X  g . This can be efficiently computed using the Forward-Backward algorithm. In both the forward and backward paths, the values of the observed state vari-ables are committed to their derived labels. In the M-step, we fix the posteriors and update  X  that maximizes Equa-tion (8). This step can be solved using stochastic gradient descent. The gradient has a similar form as that of J 1 except for an additional marginalization over y h .

There are a number of implementation issues worth at-tention. First, our semi-supervised learning objective is no longer convex due to the existence of hidden variable. The initialization of model parameters, therefore, becomes cr iti-cal to learning performance. Here we initialize the model by performing supervised learning on manually-labeled data, but we extract emission features from both data sources. Secondly, since queries are typically short, computation i s in general not an impediment to exact inference. As for the stopping criterion, we empirically found that running 2-3 epochs of the EM algorithm gives reasonably good results.
By taking derived labels as hard evidence, we are facing the risk that some state variables may take on erroneous val-ues. The second solution we explore in this work is to use derived label information as soft evidence . For queries in the second data source, we view the entire state sequence as hidden variables, but use derived labels to provide extra ev -idence in inference. This is achieved by creating a sequence of soft evidence nodes z t , t = 1 , 2 , . . . , T , in parallel to hid-den state nodes y t . This training setting is encoded in the graphical model in Figure 1(c).

Since all state variables are hidden, the learning objectiv e in Equation (8) is no longer applicable. Instead, we pro-pose to optimize log p ( z | x ). In this case, we can apply the EM algorithm in the same fashion as in Section 4.1 which iteratively optimizes an expected conditional likelihood : where the conditional probability in the second term is de-fined as p ( y , z | x ;  X ) =
Z 0 ( x ;  X ) Here Z 0 ( x ;  X ) is a normalization function (obtained by sum-ming the numerator over both y and z ); s ( y t , z t ) is a soft evidence feature with a pre-defined weight  X  . The informa-tion of derived labels is thus incorporated in the model via this feature function. To use z t as a  X  X rior X  of y t , we choose the following function form,
To understand the impact of the soft evidence feature on p ( y | x , z ;  X  g ) and hence on training, we re-write this poste-rior probability using the Bayes rule. It is easy to see that p ( y | x , z ;  X  g ) has the same exponential form as p ( y , z | x ;  X  in Equation (10) except that it has a different normalization function. This means that (a) if x t does not have an de-rived label, i.e. , z t = null , the soft evidence function assigns equal values (zero) to all state hypotheses and the poste-rior probability solely depends on transition and emission features f k ; (b) when there does exist an derived label, the function assigns a relatively large value to the state hypot h-esis that agrees with the derived label. In other words, the soft evidence function regularizes a hidden state towards t he value of the corresponding derived label. The larger  X  is, the more influence this feature has on the posterior probability . At one extreme where  X  = 0, the derived label informa-tion is completely ignored in training. At the other extreme where  X   X  +  X  , all state hypotheses would have extremely small posterior probabilities except the one consistent wi th the derived label, which is equivalent to using derived labe ls as hard evidence.

As one last remark, the soft evidence feature is only used in training. Once trained, the CRF model in Equation (1) is used to predict the state sequences for unseen queries.
So far we have assumed the availability of derived labels in our proposed semi-supervised learning approaches. In practice, however, the feasibility of acquiring such data i n a large amount is critical to the success of our approach. In this work, we give one practical example that automatically obtains such information for product search queries, while our method can be applied to many other domains.

First of all, we define the fields to be extracted from prod-uct search queries as in Table 1, and call such a set of fields a target schema . Note that there may well be other ways of defining such a schema. The first four fields, i.e. Brand , Model , Type , and Attribute , correspond to product metadata we can find from a relational database. The other fields, on the other hand, represent words that frequently occur in product search queries but are not directly related to prod-uct metadata. For example, Mechant represents a physical or online store that sells products; SortOrder represents a way of sorting product listings; BuyingIntent and Research-Intent correspond to intents to buy and research products respectively; and Other covers everything else. Since the second set of fields do not exist in a product database, they can only be obtained via manual labeing.

Given the target schema, our goal is to obtain derived labels for some word tokens in queries, which are to be used in our proposed semi-supervised CRFs. Briefly speaking, our method leverages product search click-through data in conjunction with a product database. Intuitively, the clic k data links queries to relevant products, which are then link ed to the corresponding metadata via a relational database. With the association between queries and product metadata, it is much easier to predict the field information for some word tokens using simple heuristics. Figure 2 gives a high-level diagram of our approach, while the rest of this section describes each step in detail. The click data is extracted from the query log of Live Search . When a user clicks on a document after issuing a query, a click event ( query , document ) will be recorded in the query log. In particular, we extract click events where the document is known to be a product listing page. To this end, we mine the URL pattern of the product listing pages of an online shopping website http://shopping.msn.com, an d select the click events in which their document URLs match such a pattern. Note that the click data could be drasti-cally increased if more shopping websites were considered. Figure 2: Diagram of obtaining derived labels for product search queries We further extract product titles from the selected product listing pages. This can be easily achieved by extracting the corresponding field in these pages, which are typically well structured. In this way, a click event is represented in the form of ( query , product title ). Alternatively, such pairs can be directly obtained if the query log of a product search engine is readily available.
At the second stage, we leverage a relational database that contains structured information of product listings, incl ud-ing product titles and other metadata such as Manufacturer , Color and Weight . There are over a few hundred metadata-related fields in the product database that we have, and this set of fields will be referred to as a source schema . With both the click data and the product database, we are able to associate user queries with their relevant metadata via fuzzy match of product titles. Specifically, given a click event ( query , product title ), we look for the database entry that has the most similar product title, and attach the cor-responding metadata to the query if the similarity is above certain threshold. In this work, we use tf-idf based cosine similarity and use an empirically-chosen threshold 0.75 to select ( query , metadata ) pairs. The reason we use fuzzy match instead of exact match is to increase the coverage of user queries for which we can obtain metadata.
Next, we convert the metadata represented in the source schema to that in the target schema defined in Table 1. In fact, Table 1 was created as a simplified version of the source schema in the first place. For example, Color , Weight and Dimension in the database all correspond to Attribute in the target schema. As a first attempt to extract structured information from queries, we use the simplified schema ( i.e. , target schema) in order to reduce human annotation effort. The mapping is deterministic. Thus it is straightforward to convert the metadata form as shown in Figure 2.
Given ( query , metadata ) pairs where the metadata corre-sponds to field information represented in the target schema . We apply the following heuristics to generate derived label s for some word tokens in the queries: Note that none of the fields in the second row of Table 1 exist in our product database. Consequently, the word tokens of these fields are bound to have null labels, e.g. , the word  X  X heap X  in Figure 2. In this regard, a pure unsupervised approach is inadequate to label such fields correctly.
We evaluate our approach on the task of tagging prod-uct search queries based on Table 1, where two evaluation metrics are used. (1) Sentence accuracy is the percentage of  X  X entences X  (queries in our case) that are correctly tagged , meaning that the entire decoded state sequence has to be correct. (2) Word accuracy is the percentage of word to-kens that are correctly tagged. Note that there is another widely-used evaluation metric, entity accuracy , which was found following a similar trend as the first two metrics and thus was not reported in this work.
Our experiment data was collected from a 3-month query log of www.live.com. We selected queries that clicked on product listing pages of shopping.msn.com. The selected queries (presumably with product search intent) were fur-ther classified into 20 product categories. This was done by an automatic query classifier that was built based on [11]. Our evaluation focuses on the largest two categories, i.e. , clothing-shoes and computing-electronics .

Recall that two data sources are needed in our frame-work: (1) manually-labeled queries and (2) queries with de-rived labels. To obtain the first source of data as well as to collect test data for evaluation, we randomly sampled product search queries in each category and asked human annotators to label them. Specifically, a user interface was created where each query was presented along with search results from two major search engines. Then a human an-notator browsed through both results before assigning the 9 labels in Table 1 to word tokens. We collected 4K queries as supervised training data for clothing-shoes , and another 900 queries (2.5K tokens) as test data. For computing-electronics , the training and test data were 15K and 700 (2K tokens) respectively. We labeled a drastically larger numb er of training-set queries for the second category, only to stu dy the impact of the amount of such data on semi-supervised learning performance. Due to resource limit, each query was labeled only once. However, we had 400 and 1300 queries from these two categories labeled by a secondary annotator. The inter-rater agreement is around 80% at query level and 91% at token level for both categories.

Next, we followed our procedure in Section 5 to collect the second source of data. On one hand, we extracted 500K and 250K ( query , product title ) pairs for clothing-shoes and computing-electronics respectively based on Section 5.1. On the other hand, we had a relational database that contains 20M ( product title , metadata) pairs across different cate-gories. Applying the fuzzy match of product titles and fol-lowing the auto-labeling heuristics, we obtained 50K and 20K queries with derived labels (at least one word token must be labeled) for these two categories respectively.
We used three types of emission features that have been defined in Section 3. In addition to ngram features that can be automatically extracted from queries, 7 regex features were defined to represent variations of digit-letter combin a-tions. Furthermore, 4 lexicons were extracted from the prod -uct database, corresponding to Brand , Model , Type and At-tribute respectively. For example, for all fields in the source schema that were mapped to Attribute in the target schema, their values were aggregated to be the Attribute lexicon.
In our evaluation, we use two different feature settings for all experiments: (a) ngram features only, including both unigram and bigram features, and (b) ngram + regex + lexicon features. The former setting completely relies on training data, requiring no feature engineering efforts fro m human. The latter setting has a better generalization abil-ity, but the design of regex features requires some degree of human intervention.

Under the above feature settings, we compare the follow-ing methods for query tagging: In all models above, the regularization parameter  X  2 is cho-sen via cross-validation.

We first inspect tagging performance when we vary the amount of supervised data while maintaining the same set of auto-labeled data. We only experimented with computing-electronics since only for this category we obtained a large number of manually-labeled queries. Figure 3 shows the sentence/word accuracies when we gradually increase the number of manually-labeled queries from 500 to 15K and when we use all 20K queries with derived labels. While the first two supervised approaches (MaxEnt and CRF) only use the first data source, the three semi-supervised learnin g methods additionally use the second data source.

There are several observations from Figure 3: (1) Adding regex and lexicon features works better than using ngram features alone (by comparing the top two plots with the bottom two), especially with a relatively small amount of manually-labeled samples. The difference becomes trivial, though, when the supervised data is significantly increased . This makes sense since regex and lexicon features general-ize better, and thus are especially beneficial when super-vised data is limited. (2) Supervised CRF performs signif-icantly better than supervised MaxEnt in both feature set-tings. This confirms that transition features are helpful in sequential labeling of queries. (3) Semi-supervised CRFs with derived label information greatly improve over super-vised training and self-training. The improvement is rela-tively large when supervised data is limited. Finally, (4) treating derived labels as soft evidence is in general super ior to treating them as hard evidence. This is mainly because derived label information is noisy in our task.

In a second set of experiments, we investigate the per-formance of our best approach, i.e. semi-supervised CRF with soft evidence , when fixing the amount of supervised data and exponentially increasing the second data source. We conducted this experiment for both clothing-shoes and computing-electronics . We use all 4K manually-labeled queries for the former category, and 3K queries for the latter cate-gory just to be comparable. As shown in Table 2, the ac-curacy generally improves when more queries with derived labels are used. The gain is especially large for clothing-shoes . This is largely because word ambiguity is more se-vere a problem in this category (which also accounts for its relatively poor baseline performance). Adding auto-label ed training samples, therefore, is more likely to help.
This work presented semi-supervised CRFs that incor-porate derived label information from additional resource s. Two principled approaches were explored to encode derived labels into CRFs, one as hard evidence and the other as soft evidence expressed by a feature function. Their respec -tive optimization objectives were discussed. Furthermore , we presented a practical method on how to obtain derived la-bels by leveraging user click data and an in-domain database in the context of tagging product search queries. Evaluatio n shows that our semi-supervised learning CRFs with derived labels are effective in improving tagging performance com-pared with both supervised training and self-training. In addition, we found that treating derived labels as soft ev-# derived ngram features ngram+regex+lex # derived ngram features ngram+regex+lex Table 2: Tagging accuracies of semi-supervised CRFs with soft evidence, where we use the same sets of manually-labeled samples and vary those with de-rived labels. The first row corresponds to supervised learning results. idence performed the best in our task. In the future, we would like to apply the soft evidence approach to manually-labeled data as well, since this data source can also be noisy . It is also important to show the benefit of query tagging to information retrieval. [1] S. Abney. Understanding the Yarowsky algorithm. [2] A. Arasu and H. Garcia-Molina. Extracting structured [3] C. Barr, R. Jones, and M. Regelson. The linguistic [4] J. Bilmes. On soft evidence in Bayesian networks. [5] S. Canisius and C. Sporleder. Bootstrapping [6] G. Druck, G. Mann, and A. McCallum. Learning from [7] Y. Grandvalet and Y. Bengio. Semi-supervised [8] T. Grenager, D. Klein, and C. Manning. Unsupervised [9] F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and [10] J. Lafferty, A. McCallum, and F. Pereira. Conditional [11] X. Li, Y.-Y. Wang, and A. Acero. Learning query [12] G. S. Mann and A. McCallum. Generalized [13] D. Pinto, A. McCallum, X. Wei, and W. B. Croft. [14] A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and [15] J. Suzuki and H. Isozaki. Semi-supervised sequential [16] P. Viola and M. Narasimhand. Learning to extract [17] T.-L. Wong, W. Lam, and T.-S. Wong. An [18] D. Yarowsky. Unsupervised word sense disambiguation [19] C. Zhao, J. Mahmud, and I. Ramakrishnan.
 [20] J. Zhu, B. Zhang, Z. Nie, J.-R. Wen, and H.-W. Hon.
