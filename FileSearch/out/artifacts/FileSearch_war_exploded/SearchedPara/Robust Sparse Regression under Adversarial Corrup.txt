 h (  X  on  X   X  has objective value  X  S = { n 1 + 1 ,...,n + n 1 } ,  X   X  = { 2 ,...,k,k + 1 } has smaller objective value. y matrix. The proof is given in the appendix.
 Lemma 1. If n &amp; k 3 log p , we have with high probability. then the objective value is If S X  X  =  X  , we have S = A and thus 1 + 1 k 1  X  n 1 n (  X  2 e + 2). Combining (1), (2) and (3) concludes the proof. we have with probability at least using Lemma 5.1 in [1]. Chernoff bound, and is given in the next subsection. random variables. It follows directly from Eq. (74) in [2]. variance at most 1 n . Then we have with high probability for some absolute constant c 2 .
 n 1 entries). This will affect the statement of the theorem by a constant of 2. write h ( j ) as We estimate each term in the above sum. Combining pieces, we have for all j = 1 ,...,p , to constant factors). Therefore, we have require row corruption model. Let  X  Z = max i Z i . By definition of sub-Gaussianity, we have It follows from Markov Inequality that so a union bound gives Taking t = 4 selector. We will use Lemma 2 and Lemma 3 given in the last section. b S is the vector with ( b S ) i = b i for i  X  S and ( b S ) i = 0 for i /  X  S . Combining with (5), we obtain  X  We thus obtain Now note that Using (8) and the k -sparsity of  X   X  , we can bound the first term with we decompose f j as It follows from a union bound that Combining pieces, we obtain Rearranging terms, we get Combining the above inequality with (10), (12) and (5), we obtain which concludes the proof of the theorem. Define F and f as before. Using (8) and (11), we have Dantzig selector. Applying Lemma 1 in [3], we obtain It follows that where  X  RE is defined as Combining this with (14), we obtain It then follows from (13), (8), (9) and our choice of  X  and  X  that This concludes the proof of the theorem.  X   X  This means for t large enough, k  X  t  X   X   X  k q .  X   X   X   X   X  descend method also obeys the error bounds in Theorem 4.

 Yudong Chen ydchen@utexas.edu Constantine Caramanis constantine@utexas.edu Shie Mannor shie@ee.technion.ac.il Department of Electrical Engineering, Technion, Haifa 32000, Israel Linear regression in general, and sparse linear regres-sion in particular, seek to express a response vari-able as the linear combination of (a small number of) covariates. They form one of the most basic pro-cedures in statistics, engineering, and science. More recently, regression has found increasing applications in the high-dimensional regime, where the number of variables, p , is much larger than the number of mea-surements or observations, n . The key structural prop-erty exploited in high-dimensional regression, is that the regressor is often sparse, or near sparse, and as re-cent research has demonstrated, in many cases it can be efficiently recovered, despite the underdetermined nature of the problem (e.g., (Chen et al., 1999; Can-des &amp; Tao, 2005)). Another common theme in large-scale learning problems  X  particularly problems in the high-dimensional regime  X  is that we not only have big data, but we have dirty data. Recently, attention has focused on the setting where the output (or response) variable and the matrix of covariates are plagued by erasures, and/or by stochastic additive noise (Rosen-baum &amp; Tsybakov, 2011; Loh &amp; Wainwright, 2012; Chen &amp; Caramanis, 2013). Yet many applications may suffer from persistent errors, that are ill-modeled by stochastic distribution. Indeed, many problems, par-ticularly those modeling human behavior, may exhibit maliciously corrupted data.
 This paper is about extending sparse high-dimensional regression to be robust to this type of noise. We call this deterministic or cardinality constrained robust-ness, because rather than restricting the magnitude of the noise, or any other such property of the noise, we merely assume there is a bound on how many data points, or how many coordinates of every single co-variate, are corrupted. Other than this number, we make absolutely no assumptions on what the adver-sary can do  X  the adversary is virtually unlimited in computational power and knowledge about our algo-rithm and about the authentic points. There are two basic models we consider. In both, we assume there is an underlying generative model: y = X X   X  + e , where X is the matrix of covariates and e is sub-Gaussian noise. In the row-corruption model , we assume that each pair of covariates and response we see is either due to the generative model, i.e., ( y i ,X i ), or is corrupted in some arbitrary way, with the only restriction that at most n 1 such pairs are corrupted. In the distributed cor-ruption model , we assume that y and each column of X , has n 1 elements that are arbitrarily corrupted (ev-idently, the second model is a strictly harsher corrup-tion model). Building efficient regression algorithms that recover at least the support of  X   X  accurately sub-ject to even such deterministic data corruption, greatly expands the scope of problems where regression can be productively applied. The basic question is when is this possible: how big can n 1 be, while still allowing correct recovery of the support of  X   X  ? Many sparse-regression algorithms have been pro-posed, and their properties under non adversarial ob-servations are well understood; we survey some of these results in Section 3. Also well-known, is that the performance of standard algorithms (e.g., Lasso, Orthogonal Matching Pursuit) breaks down even in the face of just a few corrupted points or covariate co-efficients. As more work has focused on robustness in the high-dimensional regime, it has also become clear that the techniques of classical robust statistics such as outlier removal preprocessing steps cannot be applied to the high-dimensional regime (Donoho, 1982; Huber, 1981). The reason lies in the high dimensionality. In this setting, identifying outliers a priori is typically im-possible: outliers might not exhibit any strangeness in the ambient space due to the high-dimensional noise (see (Xu et al., 2013) for a further detailed discus-sion), and thus can be identified only when the true low-dimensional structure is (at least approximately) known; on the other hand, the true structure cannot be computed by ignoring outliers. Other classical ap-proaches have involved replacing the standard mean squared loss with a trimmed variant or even median squared loss (Hampel et al., 1986). First, these are non convex optimization problems, and second, it is not clear that they provide any performance guaran-tees, especially in high dimensions.
 Recently, the works in (Laska et al., 2009; Nguyen et al., 2011; Li, 2011) have proposed an approach to handle arbitrary corruption in the response variable . As we show, this approach faces serious difficulties when the covariates are also corrupted, and is bound to fail in this setting. One might modify this approach in the spirit of Total Least Squares (TLS) (Zhu et al., 2011) to account for noise in the covariates (discussed in Section 3), but it leads to non convex problems. Moreover, the approaches proposed in these papers are the natural convexification of the (exponential time) brute force algorithm that searches over all subsets of covariate/response pairs (i.e., rows of the measure-ment matrix and corresponding entries of the response vector) and subsets of the support (i.e., columns of the measurement matrix) and then returns the vector that minimizes the regression error over the best selection of such subsets. Perhaps surprisingly, we show that the brute force algorithm itself has remarkably weak performance. Another line of work has developed ap-proaches to handle stochastic noise or small bounded noise in the covariates (Herman &amp; Strohmer, 2010; Rosenbaum &amp; Tsybakov, 2010; 2011; Loh &amp; Wain-wright, 2012; Chen &amp; Caramanis, 2013). The corrup-tion models studied by these authors, however, are dif-ferent from ours which allows arbitrary and malicious noise; those results seem to depend crucially on the assumed structure of the noise and cannot handle the setting in this paper.
 More generally, even beyond regression, in, e.g., robust PCA and robust matrix completion (Chandrasekaran et al., 2011; Candes et al., 2009; Xu et al., 2012; Chen et al., 2011; Lerman et al., 2012), recent robust re-covery in high dimensions results have for the most part depended on convex optimization formulations. We show in Section 4 that for our setting, convex-optimization based approaches that try to relax the brute-force formulation fail to recover support, with even a constant number of outliers . Accordingly, we develop a different line of robust algorithms which uti-lize non-convex operations.
 In summary, to the best of our knowledge, no robust sparse regression algorithm has been proposed that can provide performance guarantees, and in particu-lar, support recovery, under arbitrarily and maliciously corrupted covariates and response variables.
 We believe that robustness is of great interest both in practice and in theory. Modern applications of-ten involve  X  X ig but dirty data X , where outliers are ubiquitous either due to adversarial manipulation or to the fact some samples are generated from a model different from the assumed one. It is thus desirable to develop robust sparse regression procedures. From a theoretical perspective, it is somewhat surprising that the addition of a few outliers can transform a simple problem to a hard one; we discuss the difficulties in more detail in the subsequent sections.
 Paper Contributions: We first present two negative results that are somewhat surprising. We show that a broad class of convex optimization-based methods fail in our setting. This is in sharp contrast with the strong performance of these methods when only y is corrupted. Moreover, even a natural brute force algo-rithm has limited performance. On the positive side, we propose a general framework for high-dimensional robust regression, based on a simple idea: since global outlier rejection in high dimensions is (generally) im-possible, we do it locally in low dimensions, by replac-ing the key vector operations used by all algorithms, the inner product, with its robust counterpart: the trimmed inner product (we define this precisely be-low). The idea is simple, and while the trimming op-eration is non-convex, it is computationally tractable. We consider three popular algorithms for sparse recov-ery: Thresholding Regression, Lasso, and the Dantzig selector. We show how our idea applies to each, and then analyze the resulting robust counterparts of these three algorithms. Our main theorems provide bounds on the number of corrupted points each can tolerate, while still guaranteeing support recovery and/or small ` errors. In particular, all three algorithms are guar-anteed to have small ` 2 errors in the setting where both response variables and covariate variables are ar-bitrarily corrupted; we are unaware of any other al-gorithm with such guarantees in this high-dimensional and arbitrary-error-in-variable regime. We consider the problem of sparse linear regression. The unknown parameter  X   X   X  R p is assumed to be k -sparse ( k &lt; p ), i.e., has only k nonzeros. The ob-servations take the form of covariate-response pairs ( x i ,y i )  X  R p  X  R , i = 1 ,...,n + n 1 . These pairs, if not corrupted, would obey the following linear model here e i is additive noise and p  X  n . The actual ob-served pairs are corrupted by one of the models below. Definition (Row Corruption) . Out of these n + n 1 pairs, up to n 1 of them are arbitrarily corrupted, with both x i and y i being potentially corrupted.
 Definition (Distributed Corruption) . We allow arbi-trary corruption of any n 1 elements of each column of the covariate matrix X and of the response y . In particular, the corrupted entries need not lie in the same n 1 rows in the second model. Clearly this in-cludes the first model as a special case.
 Note that in both models, we impose no assumption whatsoever on the corrupted pairs . They might be un-bounded, non-stochastic, and even dependent on the authentic samples. They are unconstrained other than in their cardinality  X  the number of rows or coefficients corrupted. We illustrate both of these corruption mod-els pictorially in Figure 1.
 Goal: Given these observations { ( x i ,y i ) } , the goal is to obtain a reliable estimate  X   X  of  X   X  with bounded error  X   X   X   X   X  tal question, therefore, is to understand in each given model, given p,n, and k , how many outliers ( n 1 ) can an estimator handle? As we show in Sections 4 and 5, models that consider corruption only in y are fundamentally easier, and in particular, algorithms successful in that regime fail in the more difficult one we consider. Note that in the dis-tributed corruption model, an equivalent model with corruptions only in y might require all entries in y to be corrupted  X  an absurd setting to hope for a solution. Under the high-dimensional setting p  X  n , there is a large body of literature on sparse recovery when there is no corruption. It is now well-known that recov-ery of  X   X  is possible only when the covariate matrix X satisfies certain conditions, such as the Eigenvalue Property (Bickel et al., 2009). Various ensembles of random matrices are shown to satisfy these conditions with high probability. Many estimators have been pro-posed, most notably Basis Pursuit (a.k.a. Lasso) (Tib-shirani, 1996; Donoho et al., 2006), which solves an ` -regularized least squares problem as well as Orthogonal Matching Pursuit (OMP) e.g., (Tropp, 2004), which is a greedy algorithm that esti-mates the support of  X   X  sequentially. Both Lasso and OMP, as well as many other estimators, are guaran-teed to recover  X   X  with good accuracy when X is well-conditioned, and the number of observations satisfies n &amp; k log p. (Here we mean there exists a constant c , independent of k,n,p , such that the statement holds. We use this notation throughout the paper.) Most existing methods are not robust to outliers; for example, standard Lasso and OMP fail even if only one entry of X or y is corrupted. One might consider a natural modification of Lasso in the spirit of Total Least Squares, and solve where E accounts for corruption in the covariate ma-trix, and k X k  X  is a norm. When E is known to be row sparse (as is the case in our row-corruption model), one might choose k X k  X  to be k X k 1 , 2 or k X k 1 ,  X  1 ; the work in (Zhu et al., 2011) considers using k X k  X  = k X k F (sim-ilar to TLS), which is more suitable when E is dense yet bounded. The optimization problem (1) is, how-ever, non convex due to the bilinear term E X  , and we are not aware of any tractable algorithm with provable performance guarantees.
 Another modification of Lasso accounts for the corrup-tion in the response via an additional variable z (Laska et al., 2009; Li, 2011; She &amp; Owen, 2010): We refer to this approach as Justice Pursuit (JP) after (Laska et al., 2009). Unlike the previous approach, the problem in (2) is convex. In fact, it is the natural convexification of the brute force algorithm: where k u k 0 denotes the number of nonzero entries in u . It is easy to see (and well known) that the so-called Justice Pursuit relaxation (2) is equivalent to mini-mizing the Huber loss function plus the ` 1 regularizer, with an explicit relation between  X  and the parameter of the Huber loss function (Fuchs, 1999). Formulation (2) has excellent recovery guarantees when only the response variable is corrupted, delivering exact recov-ery under a constant fraction of outliers. However, we show in the next section that a broad class of convex optimization-based approaches, with (2) as a special case, fail when the covariate matrix X is also cor-rupted. In the subsequent section, we show that even the original brute force formulation is problematic: while it can recover from some number n 1 of corrupted rows, that number is order-wise worse than what our algorithms guarantee. Neither the brute force algo-rithm above, nor its relaxation, JP, are appropriate for our second model for distributed corruption . For standard linear regression problems in the classi-cal scaling n p , various robust estimators have been proposed, including M -, R -, and S -estimators (Huber, 1981; Maronna et al., 2006), as well as those based on ` -minimization (Kekatos &amp; Giannakis, 2011). Many of these estimators lead to non-convex optimization problems, and even for those that are convex, it is un-clear how they can be used in the high-dimensional scaling with sparse  X   X  . Another difficulty in apply-ing classical robust methods to our problems arises from the fact that the covariates, x i , also lie in a high-dimensional space, and thus defeat many outlier de-tection techniques that might otherwise work well in low-dimensions. Again, for our second model of cor-ruption, outlier detection seems even more hopeless. We consider a broad class of convex optimization-based approaches of the following form: Here R is a radius parameter that can be tuned. Both f (  X  ) and h (  X  ) are convex functions, which can be in-terpreted as a loss function (of the residual) and a regularizer (of  X  ), respectively. For example, one may take f ( v ) = min z k v  X  z k 2 2 +  X  k z k 1 and h (  X  ) = k  X  k which recovers the Justice Pursuit (2) by Lagrangian duality; note that this f ( v ) is convex by (Fuchs, 1999). The function f (  X  ) can also be any other robust convex loss function including the Huber loss function. We assume that f (  X  ) and h (  X  ) obey a very mild condi-tion, which is satisfied by any non-trivial loss function and regularizer that we know of. In the sequel we use [ z ; z 2 ] to denote the concatenation of two column vec-tors z 1 and z 2 .
 Definition (Standard Convex Optimization (SCO) Condition) . We say f (  X  ) and h (  X  ) satisfy the SCO Condition if lim  X   X  X  X  f (  X v ) =  X  for all v 6 = 0 , variant under permutation of coordinates.
 We also assume R  X  h (  X   X  ) because otherwise the for-mulation is not consistent even when there are no out-liers. The following theorem shows that under this assumption, the convex optimization approach fails when both X and y are corrupted. We only show this for our first corruption model, since it is a spe-cial case of the second distributed model. As illus-trated in Figure 1, let A and O be the (unknown) sets of indices corresponding to authentic and corrupted observations, respectively, and X A and X O be the authentic and corrupted rows of the covariate matrix X = [ x 1 ,...,x n + n 1 ] &gt; . The vectors y A and y O fined similarly. Also let  X   X  be the support of  X   X  . With this notation, we have the following.
 Theorem 1. Suppose f and h satisfy the SCO Condi-tion. When n 1  X  1 , the adversary can corrupt X and y in such a way that for all R with R  X  h (  X   X  ) , any optimal solution does not have the correct support. The proof is given in the supplement. Our proof pro-ceeds by using a simple corruption strategy. Certainly, there are natural approaches to deal with this spe-cific example, e.g., removing entries of X with large values. But discarding such large-value entries is not enough, as there may exist more sophisticated corrup-tion schemes where simple magnitude-based clipping is ineffective. We illustrate this with a concrete ex-ample in the simulation section, where Justice Pursuit along with large-value-trimming fails to recover the correct support. Indeed, this example serves merely to illustrate more generally the inadequacy of a purely convex-optimization-based approach.
 More importantly, while the idea of considering an un-bounded outlier is not new and has been used in clas-sical robust statistics and more recently in (Yu et al., 2012), the above theorem highlights the sharp con-trast between the success of convex optimization (e.g., JP) under corruption in only y , and its complete fail-ure when both X and y are corrupted. Corruptions in X not only break the linear relationship between y and X , but also destroy properties of X necessary for existing sparse regression approaches. In the high di-mensional setting where support recovery is concerned, there is a fundamental difference between the hardness of the two corruption models. The brute force algorithm (3) can be restated as: it considers all n  X  k submatrices of X and picks the one that gives the smallest regression error w.r.t. the corresponding subvector of y . Formally, let X S  X  denote the submatrix of X corresponding to row indices S and column indices  X , and let y S denote the subvector of y corresponding to indices S . The algorithm solves Suppose the optimal solution is  X  S ,  X   X  ,  X   X  . Then, the al-gorithm outputs  X   X  with  X   X   X  =  X  and  X   X   X  c = 0. Note that this algorithm has exponential complexity in n and k , and S c can be considered as an operational definition of outliers. We show that even this algorithm has poor performance and cannot handle large n 1 .
 To this end, we consider the simple Gaussian design model, where the entries of X A and e are independent zero-mean Gaussian random variables with variance 1 n and  X  e n , respectively. The 1 n factor is simply for nor-malization and no generality is lost. We consider the setting where  X  2 e = k and  X   X   X   X  = [1 ,..., 1] &gt; . If n existing methods (e.g., Lasso and standard OMP), and the brute force algorithm as well, can recover the sup-port of  X   X  with high probability provided n &amp; k log p . Here and henceforth, by with high probability we mean with probability at least 1  X  p  X  2 . However, when there are outliers, we have the following negative result. Theorem 2. Under the above setting, if n &amp; k 3 log p and n 1 &amp; 3 n k +1 , then with probability at least 1  X  p the adversary can corrupt X and y to make the brute force algorithm fail to output the correct support  X   X  . The proof is given in the supplement. We believe the condition n &amp; k 3 log p is an artifact of our proof and is not necessary. This theorem shows that the brute force algorithm can only handle O n k outliers. In the next section, we propose a simple, tractable algorithm that outperforms this brute force algorithm and can handle O n  X  As described above, standard tools such as convexity alone, or outlier rejection, do not fare well. Our ap-proach does not try to accurately identify outliers; in-stead, we replace standard computations with robust counterparts less susceptible to manipulation. In par-ticular, we replace the inner product with a more ro-bust version: the trimmed inner product. While sim-ple, this is the corner stone to our results, and we describe it in Algorithm 1.
 Algorithm 1 Trimmed Inner Product  X  a,b  X  n 1 Input: a  X  R N , b  X  R N , n 1 Compute q i = a i b i , i = 1 ,...,N .
 Sort {| q i |} and select the smallest ( N  X  n 1 ) ones. Let  X  be the set of selected indices.
 Output: h = P i  X   X  q i .
 The next sections show how this simple idea can take non-robust algorithms, and yield tractable algorithms with provable robustness properties. 6.1. Robust Thresholding Regression The first algorithm we consider is Thresholding Re-gression (a.k.a. Sure Screening, and Marginal Regres-sion). Standard TR estimates the support of  X   X  by selecting the columns of X which have large (in abso-lute value) inner products with the response vector y . If the sparsity level k of  X   X  is known, then one may select the top k columns. To successfully recover the support of  X   X  , standard TR relies on the fact that for well-conditioned X , the inner product h ( j ) =  X  y,X j  X  is close to  X  j . When outliers are present, TR fails be-cause the h ( j ) X  X  may be distorted significantly by mali-ciously corrupted x i  X  X  and y i  X  X . To protect against out-liers, we compute h ( j ) using the more robust trimmed inner product. This leads to our Robust Thresholding Regression algorithm (RoTR) (Algorithms 2).
 Algorithm 2 Robust Thresholding Regression Input: X,y,k,n 1 .
 For j = 1 ,...,p , compute h ( j ) =  X  y,X j  X  n 1 . Sort {| h ( j ) |} and select the k largest ones. Let  X   X  be the set of selected indices.
 Set  X   X  j = h ( j ) for j  X   X   X  and 0 otherwise. Output:  X   X  We note again that (a) this algorithm is no more computationally taxing than ordinary TR; (b) we are not performing outlier detection (i.e., identifying cor-rupted rows in X and y ) X  rather, mitigating the strength of the adversary to skew each step.
 Our algorithm requires two parameters, n 1 and k . We discuss how to choose them after we present the per-formance guarantees below. 6.1.1. Performance Guarantees for RoTR We are interested in finding conditions for ( p,k,n,n 1 ) under which RoTR is guaranteed to recover  X   X  with correct support and small error. We consider the fol-lowing sub-Gaussian design model. Recall that a ran-dom variable Z is sub-Gaussian with parameter  X  if E [exp( tZ )]  X  exp( t 2  X  2 / 2) for all real t . Definition. We say that a random matrix X  X  R n  X  p is sub-Gaussian with parameter ( 1 n  X  , 1 n  X  2 ) if: 1) each row x &gt; i  X  R p is sampled independently from a zero-mean distribution with covariance 1 n  X  , and 2) for any unit vector u  X  R p , the random variable u &gt; x i is sub-Gaussian with parameter 1  X  Definition (Sub-Gaussian design) . We assume the true design matrix X , before corruption, is sub-Gaussian with parameter ( 1 n  X  x , 1 n  X  2 x ) , and the additive noise e is sub-Gaussian with parameter 1 n  X  2 e . Note that the sub-Gaussian model covers the case of Gaussian, Bernoulli, and any other distributions with bounded support. For RoTR, we consider the special case with independent columns, i.e.,  X  x = I . The following theorem (proof in supplement) charac-terizes the performance of RoTR, and shows that it can recover the correct support even when the num-ber of outliers scales with n . In particular, this shows RoTR can tolerate an O (1 / or row-wise outliers.
 Theorem 3. Under sub-Gaussian design with  X  x = I and the row or distributed corruption model, the fol-lowing hold with probability at least 1  X  p  X  2 . (1) The output of RoTR satisfies the ` 2 bound:  X   X   X   X   X  (2) If the nonzero entries of  X   X  satisfy |  X   X  k  X   X  k 2 2 /n log p 1 +  X  2 e / k  X   X  k 2 2 , then RoTR correctly identifies the support of  X   X  provided In particular, our algorithm is order wise stronger than the brute force algorithm, in terms of the number of outliers it can tolerate while still correctly identifying the support (compared with Theorem 2). A few re-marks are in order. 1. We emphasize that knowledge of the exact number of outliers is not needed  X  n 1 can be any upper bound of the number of outliers. The theo-rem holds even if there are less than n 1 outliers. Of course, this would result in sub-optimal bounds in the estimation due to over-conservativeness. In practice, cross-validation could be useful here. 2. We note that essentially all robust statistical proce-dures we are aware of have the same character noted above. This is true even for the simplest algorithms for robustly estimating the mean. If an upper bound is known on the fraction of corrupted points, one com-putes the analogous trimmed mean. Otherwise, one can simply compute the median, and the result will have controlled error (but will be suboptimal) as long as the number of corrupted points is less than 50%  X  something which, as in our case, and every case, is always impossible to know simply from the data. 3. In a similar spirit, the requirement to know k can also be relaxed. For example, if we use some k 0 &gt; k instead of k , then the theorem continues to hold in the sense that RoTR identifies a superset (with size k 0 ) of the support of  X   X  , and the ` 2 error bound holds with k replaced by k 0 . In addition, standard procedures of estimating the sparsity level (e.g., cross-validation) may potentially be applied in our setting. 6.2. Robust Dantzig Selector and Lasso We now consider the robustified versions of the Dantzig selector and Lasso, given in Algorithm 3 and 4, respectively. In both, the key difference is the use of the trimmed inner product.
 Algorithm 3 Robust Dantzig Selector Input: X,y, X , X ,n 1 .
 Compute for all i,j = 1 ,...,p , Use linear programming to solve and output: Algorithm 4 Robust Lasso Input: X,y,R,n 1 .
 Compute  X   X  and  X   X  using (5).
 Use Projected Gradient Descent to solve and output: Note the k  X  k 1 term on the R.H.S. of the constraint. It accounts for the effect of the corruption in X be-ing multiplied by  X  ; a similar formulation appears in (Rosenbaum &amp; Tsybakov, 2010; 2011) under a dif-ferent context. The optimization in Robust Lasso is non-convex because  X   X  might have negative eigenvalues. Nevertheless, we can still use the following projected gradient descent method (Loh &amp; Wainwright, 2012): here P R is the ` 2 -projection onto the ` 1 -ball of radius R , and  X  is the step size. The theoretical guarantees below hold for the output of projected gradient descent as well (see the supplementary material for details). 6.2.1. Performance Guarantees We have the following guarantees for Robust Dantzig selector and Lasso. Here we allow for a general  X  x . Theorem 4. Under the sub-Gaussian Design Model, suppose the following is satisfied: If we choose the following parameters: 1. for robust Dantzig selector:  X  = 16 n 1 log p n  X  2 2. for robust Lasso: R = k  X   X  k 1 ; then with probability at least 1  X  p  X  2 , the outputs of Robust Dantzig selector and robust Lasso both satisfy the following ` 2 and ` 1 error bounds Some remarks are in order: (1) Both algorithms require some tuning parameters (  X  ,  X  , and R ), for which the theorem gives the  X  X pti-mal X  values, in the sense that we get the best possible bounds. But this requires knowing the statistics of the noises (  X  e and n 1 ), the true design matrix (  X  x ), and the true solution ( k  X   X  k 2 or k  X   X  k 1 ). If we set these pa-rameters larger than their optimal values, then we can still get errors bounds, but they will be sub-optimal; we omit the details here due to space constraint. Note that the same is true for standard Dantzig selector and Lasso, and their modified versions in (Rosenbaum &amp; Tsybakov, 2011; Loh &amp; Wainwright, 2012). (2) If  X  x = I , then, in order for the ` 2 error to be bounded, the requirement for n 1 is worse than RoTR by a factor of support recovery for the brute force algorithm). This is because Robust Dantzig and Lasso do not use the knowledge of  X  x being diagonal when constructing  X   X . (3) If we use  X   X  = I in the above case, or more generally  X   X  = E [ XX &gt; ], it is easy to prove that Robust Lasso essentially becomes (an ` 1 relaxation of) RoTR. We report some results for RoTR, robust Dantzig se-lector, and Lasso on synthetic data. The performance recovery (the number of non-zero locations of  X   X  that are correctly identified). For the robust Dantzig selec-tor and Lasso, we estimate the support using the loca-tions of the largest k entries of  X   X  . We also consider an refinement of robust Dantzig selector/Lasso (dubbed R-Dantzig-rnd/Lasso-rnd) where we re-calculate the entries in the estimated support by least squares using  X   X  and  X   X  , and set all the other entries to zero. For comparison, we apply standard Lasso and JP (Laska et al., 2009; Li, 2011) to the same data. We search for the values of the tradeoff parameters  X , X  that yield the smallest ` 2 -errors. Furthermore, we test JP with two different pre-processing procedures, both of which aim to detect and correct the corrupted en-tries in X directly. The first one, dubbed JP-fill, finds the set E of the largest n 1 n portion of the entries of X , and then scales them to have unit magnitude. The second one, dubbed JP-row, discards the n 1 rows of X that contain the most entries in E .
 We first consider the case where X has indepen-dent columns. The authentic rows ( X A ,y A ) are gen-erated under the sub-Gaussian Design model using Gaussian distribution with  X  x = I , p = 4000 ,n = 1600 ,k = 10 and  X  e = 2, with the non-zero ele-ments of  X   X  being random  X  1. The corrupted rows ( X O ,y O ) are generated by following procedure: Let  X   X  = arg min X sion n 1  X  k , and y O = X O  X   X  (  X   X   X  ). For i = 1 ,...,n a ( n  X  k )-vector with i.i.d. standard Gaussian entries. The results are shown in Figure 2. One observes that RoTR, robust Dantzig selector and Lasso all perform better than Lasso and JP for both metrics, especially when the number of outliers is large. The ` 2 -errors of Lasso and JP flatten out because they return near-zero solutions. Pre-processing procedures do not sig-nificantly improve performance of JP, which highlights the difficulty of outlier detection in high dimensions. We next consider X with correlated columns. The data is generated using  X  e = 1, ( X  x ) ii = 1 for all i , and ( X  x ) ij = 0 . 4 for all i 6 = j ; other parameters are the same as before. The results are shown in Figure 3. The output of RoTR becomes unreliable (as expected), but robust Dantzig selector and robust Lasso remain stable and compare favorably with the other methods. C. Caramanis and Y. Chen were supported by NSF Grant EECS-1056028 and DTRA grant HDTRA 1-08-0029. S. Mannor was partially supported by the Israel Science Foundation under grant No 920/12.
 Bickel, P.J., Ritov, Y., and Tsybakov, A.B. Simul-taneous analysis of lasso and dantzig selector. The Annals of Statistics , 37(4):1705 X 1732, 2009.
 Candes, E.J. and Tao, T. Decoding by linear program-ming. IEEE Transactions on Information Theory , 51(12):4203 X 4215, 2005.
 Candes, E.J., Li, X., Ma, Y., and Wright, J. Ro-bust principal component analysis? Arxiv preprint arXiv:0912.3599 , 2009.
 Chandrasekaran, V., Sanghavi, S., Parrilo, S., and
Willsky, A. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization , 21 (2):572 X 596, 2011.
 Chen, S.S., Donoho, D.L., and Saunders, M.A. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing , 20(1):33 X 61, 1999.
 Chen, Y. and Caramanis, C. Noisy and missing data regression: Distribution-oblivious support recovery. In ICML , 2013.
 Chen, Y., Xu, H., Caramanis, C., and Sanghavi, Sujay. Robust matrix completion with corrupted columns. In ICML , 2011.
 Donoho, D. L. Breakdown properties of multivariate location estimators, qualifying paper, Harvard Uni-versity, 1982.
 Donoho, D.L., Elad, M., and Temlyakov, V.N. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Infor-mation Theory , 52(1):6 X 18, 2006.
 Fuchs, J.J. An inverse problem approach to robust regression. In Proceedings of ICASSP , volume 4, pp. 1809 X 1812. IEEE, 1999.
 Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., and
Stahel, W.A. Robust statistics: the approach based on influence functions , volume 114. Wiley, 1986. Herman, M.A. and Strohmer, T. General deviants: An analysis of perturbations in compressed sensing.
IEEE Journal of Selected Topics in Signal Process-ing , 4(2):342 X 349, 2010.
 Huber, P. Robust Statistics . Wiley, New York, 1981. Kekatos, V. and Giannakis, G.B. From sparse signals to sparse residuals for robust sensing. IEEE Trans-actions on Signal Processing , 59(7), 2011.
 Laska, J.N., Davenport, M.A., and Baraniuk, R.G. Ex-act signal recovery from sparsely corrupted measure-ments through the pursuit of justice. In Asilomar Conference on Signals, Systems &amp; Computers , 2009. Lerman, G., McCoy, M., Tropp, J.A., and Zhang, T.
Robust computation of linear models, or how to find a needle in a haystack. arXiv:1202.4044 , 2012. Li, Xiaodong. Compressed sensing and matrix comple-tion with constant proportion of corruptions. Arxiv preprint arXiv:1104.1041 , 2011.
 Loh, P.L. and Wainwright, M.J. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Annals of Statistics , 40(3):1637 X 1664, 2012.
 Maronna, R.A., Martin, R.D., and Yohai, V.J. Robust statistics . Wiley, 2006.
 Nguyen, N.H., Tran, T., et al. Exact recoverability from dense corrupted observations via l 1 minimiza-tion. Arxiv preprint arXiv:1102.1227 , 2011.
 Rosenbaum, M. and Tsybakov, A.B. Sparse recovery under matrix uncertainty. The Annals of Statistics , 38(5):2620 X 2651, 2010.
 Rosenbaum, M. and Tsybakov, A.B. Improved matrix uncertainty selector. arXiv:1112.4413 , 2011.
 She, Y. and Owen, A. B. Outlier Detection Using
Nonconvex Penalized Regression. arXiv:1006.2592 , 2010.
 Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) , pp. 267 X 288, 1996. Tropp, J.A. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Infor-mation Theory , 50(10):2231 X 2242, 2004.
 Xu, H., Caramanis, C., and Sanghavi, S. Robust PCA via outlier pursuit. IEEE Transactions on Informa-tion Theory , 58(5):3047 X 3064, 2012.
 Xu, H., Caramanis, C., and Mannor, S. Outlier-Robust PCA: The High Dimensional Case. IEEE Transactions on Information Theory , 59(1), 2013. Yu, Y., Aslan, O., and Schuurmans, D. A polynomial-time form of robust regression. In NIPS , 2012. Zhu, H., Leus, G., and Giannakis, G.B. Sparsity-cognizant total least-squares for perturbed compres-sive sampling. IEEE Transactions on Signal Pro-
