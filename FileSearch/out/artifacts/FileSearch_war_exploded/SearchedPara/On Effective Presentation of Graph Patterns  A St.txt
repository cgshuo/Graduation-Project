 In the past, quite a few fast algorithms have been developed to mine frequent patterns over graph data, with the large spectrum covering many variants of the problem. However, the real bottleneck for knowledge discovery on graphs is nei-ther efficiency nor scalability, but the usability of patterns that are mined out. Currently, what the state-of-art tech-niques give is a lengthy list of exact patterns, which are undesirable in the following two aspects: (1) on the micro side, due to various inherent noises or data diversity, exact patterns are usually not too useful in many real applications; and (2) on the macro side, the rigid structural requirement being posed often generates an excessive amount of patterns that are only slightly different from each other, which easily overwhelm the users.

In this paper, we study the presentation problem of graph patterns, where structural representatives are deemed as the key mechanism to make the whole strategy effective. As a solution to fill the usability gap, we adopt a two-step smoothing-clustering framework, with the first step adding error tolerance to individual patterns (the micro side), and the second step reducing output cardinality by collapsing multiple structurally similar patterns into one representative (the macro side). This novel, integrative approach is never tried in previous studies, which essentially rolls-up our at-tention to a more appropriate level that no longer looks into every minute detail. The above framework is general, which may apply under various settings and incorporate a lot of extensions. Empirical studies indicate that a compact group of informative delegates can be achieved on real datasets and the proposed algorithms are both efficient and scalable.
The work was supported in part by the U.S. National Sci-ence Foundation grants IIS-05-13678 and BDI -05-15813 and Office of Naval Research grant N00014-08-1-0565. Any opin-ions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.
 Categories and Subject Descriptors: H.2.8 [ Database Management ]: Database Applications  X  Data Mining General Terms: Algorithms Keywords: frequent graph pattern, structural representa-tive, smoothing-clustering
Frequent graph pattern (subgraph) mining is an impor-tant data mining task, due to the prevalence of graph data in real applications (e.g., chemical compounds, XML and so-cial networks), and also because graph is the most expressive data structure for modeling purposes. Mining substructures that frequently occur (for at least min sup times) can help people get insight into the graphs, which has been used for indexing [23], classification [5] and many other applications. Recently, quite a few fast algorithms are developed to mine frequent patterns over graph data, with the large spectrum covering many variants of the problem. However, the real bottleneck for knowledge discovery on graphs is neither ef-ficiency nor scalability, but the usability of patterns that are mined out, which hampers the deployment of effective individual and global analysis.

There are two sources of issues that lead to the above usability gap. First, on the micro side, due to various in-herent noises or data diversity (e.g., data collection errors, insertions, deletions and mutations as the data evolves), ex-act patterns are usually not too useful in many real applica-tions. Indeed, it is often crucial to allow imprecise matchings so that all potentially interesting patterns can be captured. For example, in biological networks, due to some underly-ing process, occasionally we may observe two subnetworks N 1 and N 2 , which are quite similar in the sense that, after proper correspondence, discernable resemblance can be ob-served between respective vertices, e.g., with regard to their molecular compositions, functions, etc., and the interactions within N 1 and N 2 are nearly identical to each other. Exact patterns certainly cannot fit into such scenarios.
Second, on the macro side, there are usually too many number of patterns. Compared to less complicated data structures, the curse of combinations is even worse for graphs. Given a graph with n nodes, it has O ( n 2 ) possible edges, where a difference of just one edge can make two patterns not exactly matchable. Having such a large set of patterns that are only slightly different from each other, the mining result becomes lengthy but redundant, which easily over-whelms the users.
If we take a closer look at the above two difficulties: the limited expressiveness of individual patterns in a micro sense and the huge volume of mining results in a macro sense, they are actually both related to the same underlying problem, that is, the level on which we perform analysis is too low. The exact scheme treats a graph mining task with all struc-tural details, which not only limits the amount of informa-tion any single pattern can cover, but also makes the output cardinality too big to be handled.

Thus, the solution we propose for this problem is to roll-up our attention to a more appropriate level through structural representatives , where two issues are simultaneously tack-led. Following above discussions, it can be achieved by a so-called smoothing-clustering framework, i.e., we may con-sider working in both the micro direction and the macro direction. Note that, this is in vivid contrast to some previ-ous studies that only solve one side of the problem, but not both (cf. related work in Section 2).

In the micro direction, based on graph structures, a given pattern is usually very similar to a few look-alikes (e.g., they might be identical except  X  edges). For many real applica-tions, it is often unnecessary and sometimes even inappropri-ate to assume the restrictive exact setting: This leads us to the introduction of an error tolerance  X  ,whichcanhelpblur the rigid boundaries among individual patterns. By such a smoothing procedure, patterns within the  X  threshold are now treated as equivalent, whose supports can be further aggregated to reflect the approximation thus achieved.
In the macro direction, we want to show as few patterns as possible so that the users X  reviewing efforts are minimized. As we blur the exact boundary in smoothing, it now becomes very likely that structurally similar patterns will also have substantial overlaps with regard to their smoothed supports, an ideal property for cardinality reduction purposes. Now, instead of displaying all patterns, we perform another clus-tering step, so that the cluster centers are taken as structural representatives for the others, while the support information is also well-preserved.

The remainder of this paper is organized as follows. Re-lated work is discussed in Section 2. Then, for ease of pre-sentation, we start from a relatively simple and easy-to-understand setting, where frequent induced subgraphs are mined in the transaction database setting, to carefully study the smoothing-clustering framework, with Section 3 focus-ing on the smoothing step and Section 4 focusing on the clustering step. After laying out major algorithms, we fur-ther generalize our proposed methods to the non-induced case, the single graph case and many other extensions in Section 5. Experimental results are reported in Section 6, and conclusions are given in Section 7.
There is a wealth of literature devoted to mining frequent patterns over graph data, such as [11, 3, 16, 12]. Com-pared to them, our major aim in this study is to enhance the usability of patterns that are mined out. Previously, there have been researches that either relax the rigid struc-tural requirement or reduce output cardinality by exact rep-resentatives (which are often associated with terminologies such as mining approximate patterns or pattern compres-sion/summarization); however, as far as we know, ours is the first that considers the possibility of integrating both aspects and conquering them at once.

Following similar concepts in frequent itemset mining [2, 17], the data mining community has proposed algorithms to mine maximal [9] and closed [22] subgraphs from the database. The notion of closeness is proposed to deliver all information about the frequen t patterns in a lossless way, where both structure and support are fully preserved. In contrast, motivated by Apriori, the maximal approach only focuses on those biggest ones, because all other frequent pat-terns must be contained by them. Apart from these two extremes, [20, 13] suggest a road in the middle, which dif-fers from the maximal scheme in that it incorporates sup-port consideration, and also differs from the closed scheme because sub-patterns and super-patterns are not required to have exactly the same support so that they can be collapsed. As we can see, there is one thing in common for all these methods, i.e., the graph structures are taken as a bottom line, which does not allow any relaxation. In this way, what we get is just a compressed output, whose patterns are still exact in the sense that no structural difference is tolerated.
On the other hand, especially in the area of Bioinformat-ics where noises are inevitable in the analysis of biological networks, algorithms [18, 4] have been proposed that are able to find those patterns in data whose appearances can be slightly different for each occurrence. They usually bear the name of mining approximate patterns, but  X  X he stone only kills one bird X , because the result cardinality is not re-duced: Users are still viewing a lengthy but redundant list, except that each pattern is now supported by many impre-cise matchings in the data.

There are other related studies that try to pick some (say top-k ) delegates from the mined patterns [6, 14, 21, 19]. However, either the selection criterion being applied is not what we want [6, 14], e.g., [6] delivers the top-k frequent patterns, or the scheme is specifically designed for simple settings [21, 19], e.g., [21] assumes an independent  X  X ag-of-words X  model and summarizes patterns in the form of itemset profiles, which is hard, if not impossible, to be gen-eralized to the graph scenario, without incurring a lot of complicated dependence modeling. Recently, [7] designs a randomized algorithm to mine maximal frequent subgraphs, which are then compressed to a few orthogonal representa-tives based on pattern structures. However, (1) it does not consider support information like all maximal schemes, and (2) the mining procedure still disables structural relaxation (i.e., no smoothing is done, in the terminology of this paper), a very important aspect as we observe.
In this paper, we will use the following notations: For agraph g , V ( g ) is its vertex set, E ( g )  X  V ( g )  X  its edge set, L isafunctionmappingavertextoalabel, and I : V ( g )  X  V ( g )  X  X  0 , 1 } is an indicator such that I ( v 1 ,v 2 )=1if( v 1 ,v 2 )  X  E ( g ), and I ( v 1 ,v 2 For now, there are no edge labels, i.e., an edge is just a plain connection; further extensions will be included in Section 5.
How to specify structural matchings between graphs is a core concept for knowledge discovery on graphs. Given a pattern graph p and a data graph d , the central theme of graph mining is to find out whether there is a subgraph of d that can be matched with p , and if the matching exists, where does it happen. As we discussed in above, previous methodologies usually make the assumption that an exact match must be established between p and d so that it can be identified, which is inevitably restricted in many real ap-plications. Now, by Definition 1, we can relax this rigid constraint up to a  X  -threshold so that as long as there are no more than  X  interconnections among vertices (i.e., one has the edge while the other does not) that differ between p and d , the small error is neglected.

Definition 1. (Approximate Graph Isomorphism). For two graphs g and g , we say that g and g are approximately isomorphic if there exists a bijective mapping f : V ( g ) the following symmetric difference: is less than or equal to  X  . Here, L /I and L/I are the label/indicator functions of g and g , respectively, while  X  is apredetermined error tolerance parameter.

There are three things we want to emphasize. First, g and g must have the same number of vertices (through bi-jective mapping), because we are defining approximate graph isomorphism here. When it comes to approximate subgraph isomorphism, which is defined in Section 4, the vertex set cardinalities can be different.

Second, the vertex labels of g and g are exactly identical under the correspondence f (first condition), i.e., labels are characteristic values, which cannot be substituted. We will discuss further extensions in Section 5, which tries to remove this constraint.

Third, the symmetric difference of corresponding indica-tor functions is bounded by  X  (second condition). Actually, for two graphs g and g , there might be multiple bijective mappings that are also label-preserving. Let these mappings comprise a set F ( g ,g ), then we can define the structural dif-ference between g and g to be: which is denoted as diff ( g ,g ), with a special case of i.e., when the vertex labels of g and g cannot be matched. Being a useful reminder, it is easy to verify that the approx-imate graph isomorphism concept we just gave is equivalent to diff ( g ,g )  X   X  .

Definition 2. (Smoothed Support). Given a set of n fre-quent patterns that are mined from the graph database D : P = { p 1 ,p 2 ,...,p n } , p i  X  X  original supporting transaction set  X  D p i includes every d  X  X  such that p i is an induced subgraph of d . Due to the  X  error tolerance that was intro-duced in above, the smoothed supporting transaction set of p is defined as: Here, |  X  D p | is cal led the original support of p , and is denoted as d sup ( p ) (of course, we have d sup ( p i )  X  min sup ,since p is frequent); while |D p | is cal led the smoothed support of p , and is denoted as sup ( p ) .

The core concept of smoothing is to blur the exact bound-aries among individual patterns, which is done by aggregat-ing respective supporting transactions. Based on the pat-tern set P got from graph mining algorithms, we identify all patterns that are st ructurally similar to p i , i.e., all p such that diff ( p i ,p j )islessthanorequalto  X  ,andcombine corresponding  X  D p j  X  X  into D p i . By doing this, the support of each frequent pattern is further enhanced by those of its approximately isomorphic counterparts.

Based on the idea of smoothing, Definition 2 augments the support concept of traditional graph mining technologies, which is no longer subject to precise matches. However, as careful readers might already notice, we did emphasize the phrase X  X nduced graph X  X n above descriptions, which is due to the reason that: Throughout Definition 1, when calculating the structural difference of g and g , all edges among a given set of vertices are taken into consideration  X  Such semantics is directly related to the induced notion of graph theory, which we formally give in below.

Definition 3. (Induced Subgraph). For two graphs g and g , g is said to be an induced subgraph of g if there exists an injective mapping f : V ( g )  X  V ( g ) , s.t.,: first,  X  v  X  V ( g ) ,L ( v )= L ( f ( v )) ; and second,  X  ( v 1 E ( g ) , ( f ( v 1 ) ,f ( v 2 ))  X  E ( g ) ,and vice versa . The purpose of choosing induced subgraphs is to make Definition 1 (calculation of structural difference) and Defini-tion 2 (decision of smoothed support) consistent. By writing the first definition, we are imposing the following opinion re-garding whether two graphs are look-alikes or not, i.e., we will only deem them as similar if the two sets of nodes are interacting in nearly the same way as each other. This re-quires us to extract all edges among a given set of vertices, which conforms to the induced scenario. However, whether to stick with such an induced scheme does depend on real applications. For some cases, where we are not necessar-ily interested in every interconnection in between, the non-induced setting might be more appropriate, because it does not require the pattern as a subgraph to cover every edge that appears in its data graph counterpart. Fortunately, despite all these considerations with respect to user inten-tions, the methodology we propose in this paper is general enough to handle both cases, which will be further discussed in Section 5.

Example 1 Suppose we have a graph transaction database as depicted in the top part of Fig.1, i.e., there are 10, 2, 2, 2 copies of d 1 ,d 2 ,d 3 and d 4 , respectively. Set min sup =2, we can find 7 frequent induced subgraphs, with their original supports listed in the second row of Fig.2. When  X  is 1, i.e., any one-edge difference is gracefully tolerated, the smoothed supports of the same set of patterns are shown in the third row of Fig.2. For example, p 1 can absorb the supporting transactions of p 2 ,p 3 ,p 4 and essentially enhance its support to 10+2+2+2=16.
After examining the micro aspect, where smoothing is ap-plied to resolve the structural restrictiveness of exact pat-terns, we now turn to the macro aspect of the problem. Unfortunately, even though a pattern X  X  support is no longer confined to the number of times it precisely appears in the data, there are yet too many number of patterns. Interest-ingly, if we think from the following perspective, the smooth-ing step actually hints a way for us to cut down result car-dinality in this section: The fundamental basis for us to introduce some structural tolerance is that, by specifying a parameter  X  , users are indeed willing to accept all minor variations within this threshold, i.e., they essentially treat two patterns p 1 ,p 2 as  X  X quivalent X  if diff ( p 1 ,p 2 ) based on the same rationale, since p 1 , p 2 are deemed as in-terchangeable, can we just retain one of them in the final output and remove the other? Our answer to this question is yes, which leads to the clustering step we are going to elaborate.
Figure 1: Example Graph Database and Patterns
Example 2 Still looking at Fig.1 and Fig.2, the struc-tures of p 2 ,p 3 ,p 4 are quite similar to that of p 1 (within a  X  of 1), where p 5 ,p 6 ,p 7  X  X  structures can also be represented by p 1 because p 1 contains them. Meanwhile, as we can see, p ,p 3 ,...,p 7  X  X  smoothed supports are all very close to p All these lead us to propose { p 1 : sup ( p 1 )= 16 } as an effec-tive presentation for the whole set of 7 patterns. Note that, without the smoothing step, the original supports of p 2 ,p are far away from that of p 1 , and thus in the exact sense, it will be better to add these patterns as three additional repre-sentatives.

From the above example, we can see that, there is possibil-ity for us to represent a pattern p 1 by another pattern p that the whole pattern set can be shrunk to a small group of delegates. Towards this objective, we will take a clustering approach. As one might expect, some qualifying conditions need to be satisfied, which justify a good presentation of p by p 2 . In this sense, we can cast the degree of such good-ness to a distance function d ( p 1 ,p 2 ), and produce a much less number of clusters according to the patterns X  mutual distances. Then, as a result of this processing, the cluster centers will be taken as final representatives and passed on to the users.

Considering that patterns produced by graph mining al-gorithms are indeed a combined list of topological structures and enumerated supports, in order to delegate p 1 by p 2 so that p 1 can be removed from the final output, conditions from the following two aspects must be satisfied.  X  Structure Representability : In terms of graph topology, we  X  Support Preservation : In terms of significance measure, In the first place, let us look at structure representability. As we motivated in Example 2, there are two cases here: (1) If two patterns g , g comprise the same set of labeled ver-tices, and their mutual interactions are nearly identical to each other, i.e., the structural difference given in Definition 1 turns out to be less than  X  , then we can use one of g , g to represent the other; and (2) Different from the circumstance in (1), if pattern g has more vertices than pattern g ,anatu-ral consideration is that g can represent g if g is  X  X ontained X  in g , because the structure of g gives us full information about the structure of g . AccordingtoDefinition3,this means that g is an induced subgraph of g , and some previ-ous works have also leveraged such subpattern-superpattern relationships in compressing/summarizing itemset patterns [20, 21]. Finally, these two cases can be unified, which gener-alize to a new concept: approximate subgraph isomorphism .
Definition 4. (Approximate Subgraph Isomorphism). For two graphs g and g , we say that g is an approximate sub-graph of g if there exists an induced subgraph g of g such that diff ( g ,g )  X   X  , i.e., g and g are approximately iso-morphic to each other.

For the rest of this paper, we will write g  X  induce  X  g if g is an approximate subgraph of g in the induced sense. Obviously, g  X  induce 0 g (i.e., no tolerance) degenerates to the simple rigid case of exact induced subgraph.

Looking at Definition 4, for any two graphs g and g (with-out loss of generality, suppose | V ( g ) | &lt; | V ( g ) ists a bijective, label-preserving mapping f between V ( g ) and a subset of V ( g ), then the mutual relationship between g and g can always be decomposed into: (1) a structural dif-ference part between g and g , and (2) a subgraph contain-ment part between g and g . Now, since containment-based representation is not subject to any further conditions, we can disregard the second part from structure representabil-ity considerations, i.e., as long as the first part is bounded by a user-acceptable level  X  , there would be no problem to represent g by g in terms of graph topology.

Definition 5. (Structure Representable). For two graphs g and g ,the structural representability of g in place of g (i.e., use g as the delegate for g ) is defined as: where | V ( g ) | = | V ( g ) | means that g and g have the same number of vertices, g  X  induce 0 g means that g is an in-duced subgraph of g ,and R S stands for r epresentability on s tructure. We say that g is structure representable by g if R S ( g ,g ) is less than or equal to the predetermined error tolerance  X  .

In above, g , as an induced subgraph of g , is specifically chosen so that the structural difference diff ( g ,g ) is mini-mized. This in fact suggests the most efficient way of editing (i.e., through edge insertions/deletions) g to make it become an induced subgraph of g .
We now move on to support preservation, which is a bit more straightforward. Depending on different user inten-tions, i.e., what kind of information is expected to be con-veyed by support, we have multiple choices here. Note that, all support mentioned here has already been smoothed.
First, if support is simply treated as a number that quan-tifies pattern frequencies in the dataset, then we do not need to worry much except the support itself. We can say that g  X  X  support is well-preserved by g if where P S stands for p reservation on s upport, and is an-other tolerance parameter, whose function is comparable to that of  X  .

Second, as one might think, other than purely numeric frequencies, pattern mining results are usually associated with a set of occurrence lists, which link each pattern to those places where it appears in the data. In the scenario of graph transaction databases, this means that people may often want a pattern X  X  support to be conveyed in the sense of its supporting transactions. Having this in mind, we can lay down another definition and prescribe that g  X  X  support is well-preserved by g if Here, P S ( g ,g ) takes the form of a Jaccard distance between the supporting transactions of g and g .

Interestingly, if we ignore different transaction IDs and treat D g / D g as a set of sup ( g ) /sup ( g )ones: { 1 , 1 ,..., 1 which essentially means that users are not concerned about the specific locations where the pattern occurs, then indicating that the above two definitions can indeed be con-solidated under the same framework.

Example 3 Let us consider a variation of Examples 1-2, which is depicted in Fig.3: Now, instead of 10, 2, 2, 2 copies of d 1 ,d 2 ,d 3 ,d 4 , we have 10, 10, 10, 10 of them respectively. Based on the second option in above, i.e., support preser-vation is calculated based on supporting transactions, Fig.4 shows P S ( p j ,p i ) for each pair of patterns in a symmetric matrix. Compared to the previous situation, it now becomes less desirable to delegate p 2 ,p 3 ,p 4 by p 1 because of their dif-ferences on support: Looking at corresponding entries in the matrix, P S ( p 2 ,p 1 )= P S ( p 3 ,p 1 )= P S ( p 4 ,p 1 longer a small number. In this sense, we choose to present the whole set of 7 patterns by where p 5 ,p 6 ,p 7 are still delegated by p 1 , after introducing p ,p 3 ,p 4 as three additional representatives.

We may interpret this result as follows. If we denote la-bel a as  X  X IUC X , label b as  X  X BM Research X  and label c as  X  X orthwestern X , while each of d 1 ,d 2 ,d 3 ,d 4 corresponds to a sample of three researchers taken from these three institu-tions, then an edge between two vertices can be thought as an indication of close collaborating relationship. For exam-ple, 10 copies of d 4 in Fig.3 means that there are 10 samples out of 40 where the UIUC person and the Northwestern per-son both work closely with the IBM person, though they do not collaborate much with each other. For the case of Ex-amples 1-2, the majority of samples from three institutions have pairwise research connections among them (correspond-ing to pattern p 1 ). In comparison, the situation is much more diverse here, and intuitively this will necessitate more representatives.
Now, we are ready to formalize the problem of present-ing graph patterns through structural representatives, which is stated in Definition 6. Intuitively, clustering is a nat-ural choice for such representative selection tasks: Based on aforementioned structure representability and support preservation, which are formulated as distance functions, we can partition all patterns into a bunch of clusters and pick their centers to approximately delegate the rest. Here, structural tolerance  X  ,numberofclusters k , and support preservation threshold are all user-set parameters, which can control the clustering process and in turn reflect the amount of approximation that is desired.

Definition 6. (Presenting Graph Patterns through Struc-tural Representatives). Given a set of n frequent patterns that are mined from the graph database D : P = { p 1 ,p 2 and their associated supports, find a subset of representatives R X  X  such that for each p  X  X  , there exists at least one pattern r  X  X  satisfying: first, p is structure-representable by r ; and second, p  X  X  support is well-preserved by r .
In below, we focus on actual implementations of the clus-tering step. Given the pattern set { p i } and their smoothed supporting transactions {D p i } , we develop two algorithms to partition P , which are constructed from different perspec-tives. The first one is called -bounded clustering, it inherits bottom-up ideas and uses a parameter to control the local tightness of each cluster. The second one applies k -medoids clustering, it has top-down properties and uses a parameter k to control the global number of clusters.
As we suggested in above, in order to guarantee tight-ness, a parameter can be put on the clustering X  X  local side, which directly bounds the maximal  X  X adius X  of each clus-ter. Intuitively, the larger is and the more amount of ap-proximations that are allowed, the less number of clusters. Now, with fixed threshold , we say that a pattern p j can be delegated by another pattern p i ,if: (1) p j is structure representable by p i (see Definition 5), and (2) p j  X  X  support is well-preserved by p i , i.e., P S ( p j ,p i )  X  .
When these two conditions are satisfied, p i is a qualified structural representative for p j , which essentially creates an -cluster C ( p i )centeredat p i :
C ( p i )= { p j | p j  X  X  ,P S ( p j ,p i )  X  , C ( p i ) covers all instances in P that can be removed from the pattern set as long as p i itself is retained, while the degree of information loss for doing so is upperbounded by .
At this stage, having n -clusters C ( p i ) for each pattern p  X  X  , our task is transformed into the selection of n &lt;n -clusters so that these n cluster centers can represent the whole pattern set. Obviously, each p i  X  X  is contained in at least one cluster C ( p i ), while it might also exist in some other clusters. Now, in order to promote result cardinality reduction and thus expose users to the least amount of in-formation under a specific approximation level ,wewantto choose as few -clusters as possible, if they can fully cover all the patterns in P .

Treating the total number of n patterns as a universal collection and each -cluster as a set that contains several elements, we resort to set cover for the implementation of -bounded clustering. As a classical NP-Complete problem [8], a well-known approximation algorithm for set cover is to greedily choose a set that contains the largest number of uncovered elements at each stage. Algorithm 1 transfers this strategy to our scenario here.
 Algorithm 1 The -bounded Clustering Input: the pattern set P ,the -clusters { C ( p i ) } . Output: picked cluster centers R . 1: P r = P ; 2:
R =  X  ; 3: while P r =  X  do 4: select the -cluster C ( p i ) that currently contains 5: R = R X  X  p i } ; 6: P r = P r  X  C ( p i ); 7: return R ;
In -bounded clustering, a pattern can exist in multiple clusters and thus be delegated by more than one structural representative that is selected; meanwhile, sometimes users may also feel difficult to specify an appropriate tightness , if they are not so familiar with the data. Thus, as an alternative method, we develop the k -medoids clustering al-gorithm, which is comparable to its counterpart in general unsupervised analysis, where users only need to designate the number of representatives k they want to see, and each pattern is assigned to only one cluster.

Define a distance function d ( p i ,p j ) between two patterns p ,p j  X  X  as d ( p i ,p j )= we have the k -medoids clustering strategy described in Al-gorithm 2. At first, k patterns M = { m 1 ,m 2 ,...,m k } randomly selected from P to act as the initial medoids; and then in each step, one medoid pattern is swapped with an-other non-medoid pattern, which aims to reduce the follow-ing distance-to-medoids: The above procedure is iteratively continued, until reaching alocalminimumof D ( M ). Previously proposed k -medoids algorithms mainly differ in the way they perform swaps be-tween steps: PAM [10] picks a pair reducing D ( M )themost at each step, which includes an extensive examination of ev-ery possible swapping; CLARANS [15] refuses to do this, it will continue as soon as it finds something that can decrease the objective function. We will follow CLARANS X  X  idea in this paper, because it has been shown that the method gives similar clustering quality but is far more efficient than PAM due to its randomized search, which is ideal for large-scale applications.
 Algorithm 2 The k -medoids Clustering Input: the pattern set P , the number of clusters k , Output: k cluster medoids M = { m 1 ,m 2 ,...,m k } . 1: start from k patterns in P to initialize M ; 2: j =0; 3: consider a random pair m  X  M and p  X  M to be swapped, let M = M  X  X  p } X  X  m } ; 4: if D ( M ) &lt;D ( M ) then 5: M = M , goto step 2; 6: else 7: j = j +1; 8: if j &lt; maxtocheck then goto step 3; else return M ;
In this section, we discuss some variants and extensions of the smoothing-clustering framework that has been proposed.
Our previous discussions have been using Definition 1 to quantify the structural difference diff ( g ,g ) between two pat-terns g and g . Since all edges among a set of vertices are taken into consideration, it actually corresponds to the in-duced notion of graph theory. However, as we mentioned in Section 3, for some application scenarios, it might not be appropriate to stick with this setting if the users are ac-tually expecting some non-induced scheme, which has been followed by quite a few knowledge discovery tasks on graphs.
In a similar fashion, we can define approximate subgraph isomorphism for the non-induced scenario. For two graphs g and g ,wesaythat g is an approximate subgraph of g in the non-induced sense if there exists an injective , label-preserving mapping f : V ( g )  X  V ( g ) such that the follow-ing asymmetric difference : is less than or equal to  X  .

Due to the non-induced nature, the above asymmetric dif-ference only considers edges that are present in g but not evident distinction with respect to the induced scenario. Ac-tually, if there are no such edges, then the calculated asym-metric difference would be 0  X  consistent with the fact that g is now an exact non-induced subgraph of g . Under this ex-treme case, the dissimilarities between g and g are reflected through support, but not through structure ( g have some ad-ditional vertices/edges compared to g , but they belong to the containment part and are thus not treated as structural differences): In fact, if their supports also coincide, then g can well represent g without losing any information, which corresponds to the concept of closeness in mining theory.
Compared to g  X  induce  X  g ,wecanwrite g  X   X  g if g is an approximate subgraph of g in the non-induced sense. For the smoothing step, we need to enhance each p j  X  X  support by that of p i if p i  X   X  p j and | V ( p i ) | = | V ( p clustering step, g is said to be structure representable by g if g  X   X  g . These are all we need to modify in the original framework.
Conceptually, there are no difficulties to extend our frame-work to the single graph case, because its starting point is a set of patterns plus their supports, which are given by mining algorithms in both scenarios: We can still get these raw pat-terns, compute the smoothed supports and perform cluster-ing. There is only one complication: Unlike the transaction database setting, where support equals the number of graph transactions that contain some pattern, a common strategy adopted here is to count the maximal number of edge-disjoint embeddings [12], i.e., different occurrences must not overlap, so that the anti-monotonicity is still guaranteed. Now, as we work with support preservation, it could be complex to adopt the second option, which relies on well-defined sup-porting transactions; while for the first option that reflects numeric supports, P S ( g ,g ) takes the form of where a is an adjusting factor equivalent to g  X  X  support in g : Since a pattern can appear multiple times in a single graph, it is straightforward to imagine that g  X  X  support would be around a times sup ( g )ifweuse g to delegate g .
In above, we have been using a uniform structural toler-ance  X  in all situations. However, intuitively, shall we allow a greater tolerance for larger patterns than for smaller pat-terns? For example, in a graph with 4 edges, a difference of 2 edges means 50% approximation; while in a graph with 20 edges, it is only 10%. To reflect this idea, when consid-ering approximate subgraph isomorphisms g  X  induce  X  g and g  X   X  g in either the smoothing step or the clustering step, instead of a constant, we can let  X  be an increasing function of g  X  X  size, i.e., which is called a size-increasing structural tolerance . Here, two interesting choices could be  X  1 =  X  | V ( g ) | and  X   X  |
E ( g ) | ,where  X  is a percentage constant, with  X  1 being tolerance proportional to the number of vertices and  X  2 being tolerance proportional to the number of edges.
Remember that, when two graphs g and g  X  X  structural dif-ference was characterized in Definition 1, the configuration is restricted in the following two senses. First, we stipu-late that, even for two graphs to be approximately isomor-phic, their vertex labels should still match exactly, which was meant to focus our discussions on the difference with regard to interconnecting topology. This is plausible if the label, as a categorical attribute, represents a strict classifi-cation of node entities, where two classes are not compatible to each other at all. However, based on different application scenarios, the above setting might be too rigid, e.g., in a pro-tein network, though protein A is not identical to protein B , mutual substitutions can still happen between them under certain situations. Second, concerning edges, we have only differentiated two possibilities, i.e., there is an edge or not. In general, edges are also associated with labels, which in-corporate relevant information, e.g., there could be l e types of edges and l e +1 different cases: type-1, type-2, ... ,type-l plus null ,where null means no edge exists.

Formally, we can define two distance matrices: (1) dist v an l v  X  l v matrix, with l v being the number of different vertex labels, and (2) dist e [ i, j ], an ( l e +1)  X  ( l trix. These two matrices are symmetric, have zero diagonals, whose ( i, j ) th entry is a quantification of the dissimilarity be-tween the i th and j th vertex/edge label. In practice, they can be determined by domain experts, reflecting their knowl-edge on the difficulty of having a corresponding replacement: For example, it might be easier for an edge with label 1 to change its label to 2 than to null , which means to completely diminish that edge; and this will result in a smaller penalty score for dist e [1 , 2] than for dist e [1 , null ]inthematrix.
With all above preparations, we can rewrite the formula in Definition 1 as: diff ( g ,g )= min where the label function L /L is now applied on both ver-tices and edges. More clearly, for an edge between u and v , its label is L ( u, v ), with L ( u, v )= null if there is no edge. This new way of modeling structural difference can be easily inserted into our current framework, which is also straight-forward to be extended to the non-induced case.
In this section, we provide empirical evaluations on our algorithm of presenting graph patterns through structural representatives. We use two kinds of datasets in this study, one real dataset and one synthetic dataset. All experiments are done on a Microsoft Windows XP machine with a 3GHz Pentium IV CPU and 1GB main memory. Programs are compiled by Visual C++. Chemical Compounds. The first, AIDS anti-viral screen dataset contains the graph structures of chemical compounds, which is publicly available on the Developmental Therapeu-tics Program X  X  website. It consists of more than 40,000 molecules, and has been used as an important benchmark for various knowledge discovery tasks on graphs. In this data, vertices are labeled with Carbon (C), Hydrogen (H), Oxygen (O), etc.; while an edge exists if there is a corre-sponding chemical bond. Based on the state-of-art graph mining tools, we use a minimum support of 8% and gener-ate 5,304 frequent non-induced subgraphs, which are taken as the input P for our smoothing-clustering framework. Experiment Settings. Clearly,givensuchalargeamount of patterns that are nonetheless redundant, it is very hard for any users to make sense of them. To maintain consistency with the mining algorithm, we adopt the non-induced setting to characterize structural differences (see Section 5.1), while support preservation is computed based on supporting trans-actions (instead of pure frequency numbers), which are well-defined here. Note that, when performing the smoothing and clustering steps, we have to compare each pair of pat-terns so that structure representability and support preser-vation can be decided, especially for the smoothing step, where the determination of approximate subgraph isomor-phism is somehow costly. We implement a few heuristics to accelerate this. First, we make sure that p j  X  X  vertex label set iscontainedinthatof p i before any real matching is done, because otherwise p j will not be structure representable by p i anyway. Second, in computing diff ( g ,g ), which is the sum over a series of non-negative numbers, we designate an upperbound of  X  max and stop whenever the addition reaches this value; the result thus achieved can be reused for many rounds of clustering as long as the approximation parameter  X  is less than  X  max .

Fig.5 gives some sample representatives we get from this dataset, which are shown on the right-hand side; on the left-hand side, it lists a few original patterns that are delegated by them. In order to make the whole picture easy to read, we omit all Hydrogens from the drawing, while a vertex without label is by default a Carbon, e.g., the  X  X  X  with one edge is in fact an  X  X H X , and the (unmarked)  X  X  X  with two edges is in fact a  X  X H 2  X . There are some chemical properties associated with each cluster, which are further epitomized by the corresponding representative: The hexagon shape of patterns on the first row is typical of hexose (e.g., glucose, galactose belong to this category), while the pentagon shape of patterns on the second row is typical of pentose (e.g., ribose, desoxyribose belong to this category).

Fig.6 is an examination on the effect of smoothing. Here, patterns are grouped into bins based on their number of edges, which is shown on the x -axis; meanwhile, we plot the average support of patterns within each bin, which is shown on the y -axis. Clearly,  X  = 0 corresponds to the original supports that are not smoothed. As we can see, even using  X  = 1 will largely boost the curve, which means that the pattern set is highly redundant where a lot of frequent sub-graphs are so similar to each other (the difference is only one edge). In comparison, the gap between  X  =1and  X  =2is much smaller, hinting that  X   X  = 1 could be a reasonable set-ting for the chemical compound dataset. To this extent, we will focus more on the  X  = 1 case for the rest of Section 6.1. There is only one thing that might seem counter-intuitive: For  X  =1and  X  = 2, sometimes the support slightly goes up as the pattern size increases. Actually, this can happen after smoothing: An easy-to-think example is Fig.1, where p is an induced subgraph of p 1 , but sup ( p 1 ) &gt;sup ( p
Fig.7 depicts -bounded clustering. It is easy to imagine that, as the threshold becomes larger, less and less cluster centers will suffice to represent the whole pattern set. Also, the large gap between  X  =1and  X  = 0 indicates that struc-tural relaxation and thus structural representative is a nec-essary mechanism to achieve effective pattern presentation. Empirically,  X  =0 . 15 or 0 . 2 could be a suitable parameter to work with, because from 0 to  X  , the number of clusters reduces significantly, while beyond that range, the gain to further increase is less obvious.
Fig.8 depicts k -mediods clustering, where maxtocheck is set as 2000. Similar to tests on -bounded clustering, we want to examine the relationship between number of clusters k and clustering quality, which is reflected by the following measure of average error: where M is the final set of medoids. The general trend is quite similar to Fig.7 (suggesting k  X  = 300 as a suit-able parameter setting), except that the support preserva-tion threshold there bounds on the maximal distance from each pattern to its cluster center, while the error E defined here calculates an average, which is somehow smaller. We decide to omit the  X  = 0 curve, because for such a zero struc-tural difference case, k on the x -axis must be set very large so that each pattern is structure representable by at least one medoid, and this will dwarf the more interesting  X  =1 curve.
Fig.9 draws two distributions for the original patterns P and final representatives R , respectively. Like in Fig.6, pat-terns are binned based on their number of edges, with its y -axis now showing the number of patterns for each bin. -bounded clustering with  X  =1and =0 . 15 is applied. The downward tendency of R with respect to P is very clear. Moreover, an interesting observation is that, the degree of descending is even bigger for the curve X  X  mid-part, which corresponds to the high-frequency area if we look at the dis-tribution of P . This demonstrates the effectiveness of our algorithm in the dense region of data.
Fig.10 describes what happens when a size-increasing struc-tural tolerance is used. Based on Section 5.3, we adopt the following function: where  X  is set at 0.125. -bounded clustering is tested. Due to the rounding that is applied,  X  starts from 0 for one-edge patterns and gradually augments to 1 (at 4-edge), 2 (at 12-edge), and so on. Looking at Fig.9, the fraction of patterns with less than 4 edges is pretty small, which means that in overall, the constraint imposed by the scheme here is less rigid than that of a uniform  X  = 1. This might explain the non-uniform curve X  X  lower position in Fig.10. Generator Description. The synthetic graph generator follows a similar mechanism as that was used to generate transaction itemset data [1], where we can set the number of graphs ( D ), average size of graphs ( T ), number of seed patterns ( L ), average size of seed patterns ( I ), and number of distinct vertex/edge labels ( V/E ). First, a set of L seed patterns are generated randomly, whose size is determined by a Poisson distribution with mean I ; then, seed patterns are randomly selected and inserted into a graph one by one until the graph reaches its size, which is the realization of a Poisson random variable with mean T . Due to lack of space, readers are referred to [11] for further simulation details.
The data set we take is D10kT20L200I10V( l v )E1, i.e., 10,000 graphs with 20 vertices on average, which are gen-erated by 200 seed patterns of average size 10; the number of possible vertex and edge labels are set to l v and 1, re-spectively. Other experiment settings are as same as those in Section 6.1. We vary l v from 6, 8, 10, 12, up to 15, and use a fixed min sup = 6% to mine frequent subgraphs from these five datasets, which are further processed by smooth-ing and -bounded clustering (use =0 . 2). The result is de-picted in Fig.11, where in order to account for the fact that the original pattern set X  X  cardinality |P| is different for each dataset, we normalize the number of final representatives by dividing |P| , which is called reduction ratio and shown on the y -axis. Based on the upward trend of curves, it can be seen that the benefit achieved by our algorithm shrinks as l becomes larger, which is natural, because in the normal case, vertex labels must be exactly matched (even increasing the error tolerance  X  cannot change this), and more labels will surely increase the diversity of data. Section 5.4 offers a solution to alleviate this effect, if distinct labels are not absolutely non-substitutable.

Taking D( |D| )T20L200I10V10E1, we also tested the effi-ciency of our algorithms over five synthetic datasets by vary-ing the number of transactions |D| from 5,000, 10,000, up to 25,000, which is shown in Fig.12. Here, a fixed min sup = 6% is used and around 1,500 frequent subgraphs are gener-ated for each dataset. The total running time is composed of two parts: (1) the smoothing time (use  X  =1),which
Figure 11: Performance w.r.t. Number of Labels enhances the support of each pattern by those of its look-alikes, and (2) the clustering time (use =0 . 2for -bounded clustering and k = 500 , maxtocheck = 2000 for k -medoids clustering) that produces a bunch of final representatives. It can be seen that the implementation is highly efficient, which can finish in tens of seconds, while both of our algo-rithms are linearly scalable.
We examine the presentation problem of graph patterns and solve the usability issue raised at the beginning of this paper. Instead of too many ex actpatternsthatarenotso meaningful, a compact group of informative delegates are generated and shown to the users. Structural representa-tives play a key role in this novel, integrative approach, which essentially rolls-up our attention to a more appropri-ate level that no longer looks into every minute detail. The smoothing-clustering framework is nearly universal, which can handle many variants of the problem (induced/non-induced, transaction database/single graph) and incorporate other extensions as well. Finally, empirical studies confirm the effectiveness and efficiency of our proposed algorithms. As a promising future direction, since the method described here belongs to the post-processing category, we are now working on advanced algorithms that can directly mine such representative patterns from data. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] R. J. Bayardo. Efficiently mining long patterns from [3] C. Borgelt and M. R. Berthold. Mining molecular [4] C. Chen, X. Yan, F. Zhu, and J. Han. gapprox: [5] M. Deshpande, M. Kuramochi, N. Wale, and [6] J. Han, J. Wang, Y. Lu, and P. Tzvetkov. Mining [7] M. Hasan, V. Chaoji, S. Salem, J. Besson, and [8] D. S. Hochbaum, editor. Approximation Algorithms [9] J. Huan, W. Wang, J. Prins, and J. Yang. Spin: [10] L. Kaufman and P. J. Rousseeuw, editors. Finding [11] M. Kuramochi and G. Karypis. Frequent subgraph [12] M. Kuramochi and G. Karypis. Finding frequent [13] Y. Liu, J. Li, and H. Gao. Summarizing graph [14] T. Mielik  X  ainen and H. Mannila. The pattern ordering [15] R. T. Ng and J. Han. Clarans: A method for [16] S. Nijssen and J. N. Kok. A quickstart in frequent [17] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [18] R. Sharan, S. Suthram, R. M. Kelley, T. Kuhn, [19] C. Wang and S. Parthasarathy. Summarizing itemset [20] D. Xin, J. Han, X. Yan, and H. Cheng. Mining [21] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing [22] X. Yan and J. Han. Closegraph: mining closed [23] X. Yan, P. S. Yu, and J. Han. Graph indexing: A
