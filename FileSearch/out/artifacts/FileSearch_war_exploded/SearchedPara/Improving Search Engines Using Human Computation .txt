 Work on evaluating and improving the relevance of web search engines typically use human rele vance judgments or clickthrough data. Both these methods look at the problem of learning the mapping from queries to web pages. In this paper, we identify some issues with this approa ch, and suggest an alternative approach, namely, learning a ma pping from web pages to queries. In particular, we use human co mputation games to elicit data about web pages from players that can be used to improve search. We describe three human computa tion games that we developed, with a focus on Page Hunt, a si ngle-player game. We describe experiments we conduc ted with several hundred game players, highlight some interesting aspects of the data obtained and define the  X  X indability X  metric. We also show how we automatically extract query alterations for us e in query refinement using techniques from bitext matching. The data that we elicit from players has several other app lications including providing metadata for pages and identifying ranking issues. H.3.m [Information Retrieval]: Mi scellaneous; H.5.3 [HCI]: Web-based Interaction Measurement, Design, Experi mentation, Human Factors. Web Search, Human Computati on Games, Findability, Query Alterations, Relevance Web search engines have become an integral part of our everyday lives. It is clearly important to evaluate the  X  X oodness X  of search engines to help improve the s earch experience of users and advertisers, and build traffic and revenue. Web search today typically retrieves several document s relevant to queries issued by users. Hence evaluation methods are usually based on measuring the relevance of documents to queries. One common method of evaluating relevance is to use large hand-annotated evaluation corpora where query/document pairs are assigned relevance judgments. Another method is to use implicit measures of relevance, such as identifying clicks on results. Although the click-based method is subject to noisy data issues, the amount of data available is potentially li mitless. Compared to the hundreds of queries used in typical TREC collections, the vast quantities of clickthrough data available with search engines can support a much more extensive evaluation. Both these evaluation methods (human relevance methods and click log methods) are based on pages surfaced by a search engine. However, some pages may never get surfaced for a variety of reasons, including bad ranking, indexing problems, network issues, improper deduplic ation of URLs, improper spam detection, etc. If a page is not surfaced (or is surfaced with insufficiently high rank), it will not enter the pool of results to be evaluated; nor will it figure in clickthrough data since no one can click on it. To avoid problems in finding pages relevant to a given query, we suggest an alternative approach, looking in the other direction: given a web page, find the queries that will effectively surface this web page. This can potentially pr ovide us with new insights into the relevance pipeline. If startin g with queries focuses on how effectively search engines can retrieve documents relevant to specific queries, starting with we b pages focuses on the findability of the pages indexed by the search engines. Howeve r, it would be time-consuming, expens ive and infeasible to study every web page, and somehow obtain querie s to find these web pages. We propose instead to use games to collect relevant data. There has been a lot of interest in using games to transform businesses, helping them become more competitive and profitable. Edery and Mollick [4] provide an excellent overview of this landscape. Chapter 5 of McDonald et al. [12] describes how productivity games have been used in software testing. Recently, the idea of employing human computation to solve research tasks was popularized with the introduction of the ESP game [18]. As reported in [19], more than 200 million hours are spent each day playing computer and video games in the U.S. Indeed, by age 21, it is said that the average American has spent more than 10,000 hours playing such games. The idea behind human computation games is to harness the time and energy spent *This work was done when the authors were on summer internships at Microsoft Research. on playing games to solve computational problems that are otherwise difficult to tackle. The ESP game, developed by von Ahn et al. [18], brings web users together to participate in a large-scale image labeling problem in the guise of a guess ing game. Other  X  X ames with a Purpose X  [17, 19] have been devel oped to locate objects in images [23], collect common-sense facts [22], improve accessibility of the Web [21] and classify the query intent of users [8]. Similar games have been devel oped to tag images [1], and collect data for automated directory assistance [1 4]. Such games have become so popular that there are now toolk its, such as the HCToolkit from Law et al. [7], to simplify and automate the process of making a human computation game. von Ahn and Dabbish [19] categorize human computation games into output-agreement games, i nversion-problem games, and input-agreement games. In outpu t-agreement games (e.g. the ESP game) two players are given the same input and must produce outputs based on the input; to wi n the game, both players must produce the same output. The inversion-problem games are a generalization of Peekaboom [23] , Phetch [20] and Verbosity [22]. In these games, in each round, one player is assigned to be "guesser". The describer produces outputs that are sent to the guesser based on the input. The winning condition is that the describer. Finally, the input -agreement games represent a generalization of games like TagATune [9]. In each round, both the players are given inputs that are known by the game to be the same or different; but the players do not know if they are same or different. The players are instruct ed to produce outputs describing their input. Both players win if they both correctly determine whether they have been given the same or different inputs. Whenever we gather data from the web, we must be concerned with quality control. The games de scribed above typically involve two players who collaborate to find  X  X rue X  answers for each task; truth is determined by their agreement on results. To prevent collusion and data corruption issues, players are matched randomly. When there are an odd number of players, these games typically back-off to using data from previously recorded (two-person) games to simulate one of the players. This may degrade the gaming experience and thus the data quality. Also, these collaborative games tend to elicit tags/descriptions that are very general, since it is easier to agree on generalities than specifics. Partial remedies include the us e of taboo words and specificity scores to encourage more specific labels, but this can still be a problem. Weber et al. [24] recently showed that the labels in ESP Game can be deduced from other labels present using language modeling. They developed a robot which played the ESP game, and with no information from the images , agreed with a randomly assigned partner 69% of the time on all images, and 81% when they had some off-limit (taboo) wo rds assigned to them. Weber et al. have a range of palliative suggestions, including changing scoring, changing the timing m echanism, motivating players, hiding taboo terms etc. In this paper, we seek to develop an efficient and effective human computation model for improving web search. We avoid the issues mentioned above primarily by using a single-player model. We focus primarily on Page Hunt, a single-player game where the player attempts to devise a query that will surface a given page in the top N results from a search engine; thus  X  X ruth X  is determined by the search engine used in the game. The data that we elicit from players has several applications including providing metadata for pages, providing query alterations for use in query re finement, and identifying ranking issues. We describe an experi ment we conducted with over 10,000 game players, and high light some interesting aspects of the data obtained. We define a new metr ic we call  X  X indability X , and automatically extracting query alte rations using the technique of bitext matching. The overarching research question that we tackle is: Can we use human computation games to get us eful data and develop an end-to-end process to improve web s earch? This bigger question can be broken into the following questions:  X  Games have to be (by definition) fun to play and so in turn,  X  Is the data we get from Page Hunt comparable to data we get  X  Finally, how do we get useful data from this? Can we define Once we decided to use human computation games, we came up with a set of design goals. We wanted a simple, fun game that people want to play (rather than be forced to play!). We wanted it to have a great user experience, be visually appealing, desirably be whimsical and definitely not be staid. We wanted it to be a web experience (rather than a downloa ded game), with little or no server load. We wanted the game to be simple to understand and describe. We decided to use time limits to encourage quick play. Our goal was to design our game to avoid any bias in data collection. As we started work, we design ed and developed three related games, which we descri be in this section. However, as mentioned earlier, in this paper we will focus primarily on Page Hunt, a single-player game where play ers hunts down given web pages with their queries. We will briefly describe the other two games that we developed: Page Race and Page Match. The basic idea of Page Hunt is to show the player a random web page, and get the player to come up with a query that would bring engine. Fig. 1 shows a screenshot of this game. The web page being  X  X unted X  is shown in the background. The player types in queries and looks at results returned in the floating operating panel, currently in the lower right corner. This panel turns almost transparent when not in focus. The border of the panel relays important feedback informatio n through animation and color changes. Game play goes thus: 1. The player gets a random web page with URL U. The player can see just the Web page, but not the URL of the page. 2. The player views the page, and types in a word (or a phrase) as the current query Q. 3. The system retrieves the top N search results for this query from a search engine and displays them. For each result, the rank, title, description or snippet and the URL are displayed. Against each result, a big check mark is shown if the match is successful, and a big cross is displayed if it is not. We can use any web search engine to get these results . In Page Hunt, we get results from the Bing search engine (www.bing.com) using their public SOAP API. 4. This match is successful if the URL U is in the top N results for a given query Q. Players get point s based on the rank of U in the search engine in the background de termines if the player wins or loses. We check for exact or near-exact URL matching, with some normalization (e.g. to handle http://a.com pointing usually to the same site as http://a.com/default.h tm or http://a.com/default.aspx). But we do not attempt any sort of content matching to identify matching URLs. 5. If query Q does not lead to success, the player edits the query to change the phrase or add a word/phrase. If the query leads to success, the player X  X  score gets updated, and the game advances to the next web page. 6. This is repeated for each page till the player quits or hits a fixed time limit for the game. with no penalty; we do not force the player to provide tags for each page. If they see a page which is rendered badly or is bad in any way, they can mark it as bad, again with no penalty. If several players tag a page as bad, the page is reviewed and removed if appropriate. At each step, we record the following: the player X  X  screen name (not connected to the player X  X  re al identity in any way; the game can also be played anonymously), the web page id, the query that was tried, whether it was right and if so at what rank position, the time, and the points the player got for this query. The terms (queries) that the player provides at each step acts as a tag or label for the page. When the player gets it right, the label is valuable; but even when it is wrong, the label provided could be useful. At the end of each game, the player gets the option to review each page, the queries tried and the results obtained. This can be valuable in training people how to query better. We want players to use appropriate query terms to get to the given page; we do not want random seque nces of words from the page used as queries. For this reason, a transparent overlay on the web page prevents players from cutting and pasting long phrases from the page as queries. Players may still type in long phrases from the page, but since sear ch engines typically have a query length limit, players soon learn to be discriminating. The quality of the game depend s on the web pages displayed. Although search engine companie s have abundant clickthrough data, not all the web pages ha ve enough adequate metadata associated with them. The web pages we use in Page Hunt are those that have been identified by a search engine as requiring special treatment. Some are pages with very little text, or pages which search engines tend not to surface; other pages are about celebrities, weather etc. As we will see later, not all pages are easy to  X  X unt down X   X  some are easier than others. It is easy to adapt the game to the player X  X  skill level with an appropriate choice of web pages. Single player human computation games such as Page Hunt are generalized in Fig. 2(a). Based on our game design goals, we chose to implement Page Hunt using Microsoft Silverlight, since it gave us a good platform for development, access to animation and interactivity, and browser independence. Page Hunt also includes several features to increase fun in game playing, such as timed response, score keeping, top-players list, frequent queries (a concept si milar to taboo queries), and occasional bonus points. Because player engagement is important in this context, we provide ad ditional details on these points. Setting limits on game sessions introduces challenge into a game in the form of a timed response [ 10, 11]. In Page Hunt, we set the time limit for each round as 3 minutes. The time limit and time remaining are displayed throughout the game, pushing players to keep trying queries quickly, one after the other. Page Hunt provides a simple scor e based on the rank of the URL U in the result set. Top players who login into the game can see their names on the leader board. We do not have the concept of taboo queries in Page Hunt, where specific terms are completely disallowed. However, for a small fraction of web pages displaye d, we show frequent queries associated with the page. If players get stuck on such a page, they can issue these frequent queries . However, they get bonus points if they avoid using these frequent terms. This encourages forward movement in the game and prevents players from getting frustrated with the game. In Page Hunt, the randomness comes out in two of its features: (1) the web pages displayed are randomly selected for each player, and (2) during scoring, the play er X  X  winning score is randomly doubled for a page with a probability of 10% and randomly tripled with a probability of 5%. Thes e random bonuses increase the fun factor of the game, and act as an incentive to play again. Page Race, a two-player competitive game, is a natural extension of Page Hunt. Fig. 3 shows the operating panel of this game. In this game, a player is randomly matched with another. Both players are shown the same random web page. As in Page Hunt, the players type in a word (or a phrase) as the current query after viewing the content of this web page. The player who first brings up the web page within the top N s earch results wins the points. In this game, the players review the last query issued by them and issued by their competitor and the related search results. Each player can thus learn from his/her competitors X  queries and related search results. Page Match is a collaborative game (see Fig. 3 for a screenshot) where two players are randomly paired up. Both players are shown a web page each. These pages are the same with a 50% probability, and different but related with a 50% probability. The players type in a word (or a phrase) as the current query Q after viewing the content of their web page. The players then compare their search results with the search results generated by their partners, and then assess whether their web pages are the same or different. If both players correctly determine and agree whether the pages they have been given are the same or different, both players win points. The Page Match and Page Race games were built with an early version of the HCToolKit [7], and the games share some code components. Fig. 2(b) and Fig. 2(c) depict generalized versions of the Page Race and Page Match games. As mentioned above, the focus of our experiment is the Page Hunt game. We released the game to the web, we conducted a pilot study with over 10,000 web users. On the game page, we told the users about the game, described th e rules, and gave them a web link to try the game. We seeded the system with 698 URLs for which better labels were required. We did not offer any inducements (gifts or payments ) since we did not want these inducements to bias their play. T hus participation was completely voluntary. In the next section, we present evidence showing that Page Hunt was fun to play and that people provide valuable research data while playing. In this section, we consider the research issues listed in Section 1.2, and provide answers to each of them. During this experiment, over 10,000 people on the web played Page Hunt over a period of ten days, generating over 123,000 non-null labels on the 698 URLs in the system. On average every player contributed over 12 labels, and every URL has over 170 labels. Players sent comments like  X  X ddictive X ,  X  X  love the app. It has a Passing it along ... X . The players who were at the top of the leader board kept playing to keep their names on the leader board. Judging from these numbers, and fro m the comments we got, this game seems to be fun. In an experiment of this nature, it is important to check if the queries we elicit from the players are comparable to the queries that they would have posed to a standard web search engine. From the pilot data, we took a random sample of successful 100 &lt;query, web page URL&gt; pairs. Two of us analyzed these 100 pairs and categorized the queries as OK, Over-specified or Under-specified. OK queries are just what one needs to get the associated http://www.nlm.nih. gov/medlineplus/hives.html. The query without the words  X  X tart here X  is enough to get this page in the top 5 results. Similarly, since we would need more query words than [nelson county] to get to the specific page http://www.nelsoncounty.com/dir ectory/discuss/msgReader$340. So the query [nelson county] is treated as an under-specified query for this page. In our analysis, 78% of the queries were OK, 15% were over-specified and 7% were under-specified. Because the web pages being  X  X unted X  are in front of the players as they create their queries, we expected the queries to be more like  X  X nown item X  searches than exploratory searches. We expected queries to be over-specified in general. But, contrary to our expectations, the queries are predominantly (78%) similar to queries given to web search engi nes, while only slightly above a fifth were over-specified or under-specified. From the data collected, we find that some of the URLs are easily found (or  X  X unted X  down), while some others are difficult to get in the URLs. The X-axis of this figure shows findability ranges of URLs. A 100% findability level indi cates that this URL can be easily located, since in every instance where a player was presented with this URL, the player was able to successfully identify a query that brought up this URL to the top 5 search results. On the other hand, if a URL has a 0% findability level, presented with this pagfe was the player able to identify a query that successfully brought up this UR L to the top 5 search results. From Fig. 4, we can see that about 10.0% of the 698 URLs in our database have &gt;70% findability while roughly the same (11.3%) have &lt;10% findability. findability levels of URLs, we eval uate the impact of the length of URLs. In Fig. 5(a) and Fig. 5(b), we show how the length of a URL impacts the URL findability levels. Here the length of a URL is measured as the number of characters it contains, not including  X  X ttp:// X . From Fig. 5(a), we draw the conclusion that as the length of URL links increases, the URLs are ha rder to locate through search engines. Fig. 5(b) shows the distribution of the number of characters in each URL. Note that findability could have other uses. If we ran a game like Page Hunt continuously , assuming a reasonably large set of web pages, and a random, changing set of players, the average findability of the pages could prov ide valuable insights into search engine accuracy. The findability metric can be used to evaluate the overall and comparative goodness of search engines. On most search engines, if you search for, say, [Wash. DC], the query is internally modified as if it were equivalent to a search on [Wash. DC] ORed with [Washington, DC]. This idea of altering the user X  X  query using other term s which may have been intended is called query alteration. The query alterations that are used in a search engine may be obtained usi ng a variety of sources. In this section, we describe one way to learn query alterations from Page Hunt game data. Consider a web page like http:// www.labor.state.ny.us/ . When we tried this on our pilot players, we got a number of queries that successfully retrieved this page, some of which are shown in Table 1. It is easy to see that the query NYSdol should have  X  X ew york state department of labor X  as a possible alteration, that  X  X epartment X  should have  X  X ept X  as an alteration etc. But how can we automatically extract these alterations? Table 1. Queries for the NY State Dept of Labor page We use the intuition that different queries for the same page can be viewed as different paraphr ases in the same language, or translations of the same concept in different languages. There has been work in statistical machine translation (MT) to learn such paraphrases or matching segments across document pairs (hence  X  X itext matching X ). We use bitext matching, as described below, to learn alterations from our game data. In recent years, statistical approaches to machine translation that learn translational equivalents from bitexts have shown great promise as effective translation techniques. Modern statistical methods learn translational equivale nts directly from parallel data with little to no outside supervision; in many cases, only a tokenizer for the source and targ et language along with generic learning algorithms suffice to produce high quality translations [13]. The technology originally developed to learn transformations from one language or domain to another has applications outside of translati on. For instance, statistical MT techniques have shown promising results in the task of generating monolingual paraphrases [15]. Learning label or query transformations is a natural next step. The labels collected during the game begin in a somewhat different format: for each URL, we have a set of possible labels. We can coerce this data into a reasonable bitext format by simply selecting all pairs: given a URL U and a set of queries Q ( U ), each pair of queries x 1 , x 2 in Q ( U ) where x 1 monolingual constructed bitext. As in the above example of paraphrasing, the source and targ et languages are identical; we hope to learn transformations that carry approximately the same semantic content using di fferent lexical items. Given this bitext, we first iden tify a correspondence between the words in each query pair. This will be used later in extracting single-word or multi-word phrases. As described in [16], the process of finding the most likely alignment between two sequences can be seen as a Hidden Markov Model (HMM). We treat one side of the parallel corpus as the source side, and train a generative model that produces the hidden word alignment and the target side of the parallel corpus using Expectation Maximization (EM) [3]. For each label pair, let s source tokens, t 1 to t n be the target tokens, and a hidden alignment, where a i is a value ranging between 0 and m . If a = k , then target word i corresponds to source work k . Position 0 is a special position indicating the null word. Using the notation s to represent the sequence s i , s i+1 , ..., s j , we can write the probability of a target sequence and word alignment as: Basically we assume that there is some jump distribution modeling the position of the source word generating target word i given the position of the source word generating target word i -1 . Also for each source word, we maintain a multinomial distribution over possible target words that are its translation. These distributions are learned from th e parallel text using EM as described above. In practice, wo rd alignment qua lity is improved significantly if we initialize th e translation distributions using some simpler model such as IBM X  X  Model 1 [2]. Therefore, we begin by running five iterations of Model 1, and follow up with five iterations of the H MM model described here. Next we extract so-called phrase pairs from the bitext. Note that these phrase pairs are just arbitr ary word sequences, not linguistic constituents. We begin by running the word alignment algorithm in both directions: first we select one side of the corpus as the source, train word alignment model, and extract the most likely alignments for each sentence pair; then we flip the assignment and train the models again. The word alignments are combined using a heuristic named Grow-Diag-Final as described in [6]. Let the heuristically-combined word a lignment be represented as a relation  X  : we say s  X  t if the word s is aligned to the word t . For &lt; s range (i.e., there exists some x  X  [ i, j ] and y  X  [ k, l ] such that s  X  t y ), and (2) words inside the source phrase only align to words inside the target and vice versa (for all s x  X  t y we have x  X  [ i, j ] iff y  X  [ k, l ]). As an example, consider the pair  X  new york department labor  X  and  X  new york state department of labor  X , and assume that all and only the identical words are aligned; thus  X  state  X  and  X  of  X  are not aligned. From this pair, we would extract the phrase pairs  X  new york / new york state  X ,  X  department labor / department of labor  X , and  X  department / department of  X , amongst many other phrase pairs. However, the phrase pair  X  department labor / department of  X  would not be extracted because some words inside the phrase on one side are aligned to words outside the phrase on the other side. We extract all phrase pairs consis tent with the word alignment from each query pair. To gauge the quality of these phrase pairs, we estimate a probability distribution by aggregating counts across the bitext. Let c  X  [ S, T ] be the count of times we observed the phrase pair S , T from any query pair in the bitext. Then we can define a probability distribution ba sed on the relative frequency of the phrase pairs ( T | S ) = c ( S , T ) /  X  T' For all web pages (URLs) in the Page Hunt pilot data, we extracted the queries that corresponded to winning trials, generated all pairs of queries as bitext data, and applied the algorithm described above to extract all phrase pairs. This results in a large table of alterations (phrase pairs) with weights corresponding to the likelihood of each alteration. Table 2 shows some of the examples. If we take the top scoring alterations, we find that they can be categorized into 4 bins: Search engines already handle th e alterations in the first two categories well. The last two categories, especially the conceptual alterations, look very promising. In Fig. 6, we also plot the distri butions of the query alterations in terms of the categories. From this data, we see that Category 1 and Category 4 alterations are the most frequent alterations we learn from the data. Our plan is to garner additional data, improve the bitext ranking process, and run our bitext code on the new data to generate alterations. These alterations can then be evaluated by themselves by human judges, and then used in a search engine to see if they cause an improvement in relevance measures. In this paper, we present a pa ge-centric approach to improving Web search using human comput ation games. We provided a detailed description of the Page Hunt game, and briefly described Page Race and Page Match, two vari ants on the theme. To the best of our knowledge, Page Hunt is the first single-player non-collaborative human computation game. 
Fraction of Alterations We showed that the query data we obtain from the Page Hunt game is not very different from queries used on search engines. We discussed the finda bility metric, which has implications on various aspects of a search engi ne including indexing, ranking and deduplication. This notion can be used to complement result-relevance metrics and implicit measures of search engine goodness. We also described a process to extract query alterations from game data, using the idea of bitext matching. There is considerable scope for further work, including evaluations of the utility of query altera tions gleaned by this method. We plan to continue work in this area, and to improve our games e.g. by filtering players at the beginning of the game, incorporating player skill levels, dissuading cheating, etc. In the future, we also plan to conduct an equivalent of eye-tracking analysis from the data we collect. Eye-tracking analysis can help us with information retrieval tasks related to Web search ranking [25], query expansion [26], etc. However, such experiments not only require expensive equipments like eye-tracking facilities but also require users who participate in the research to be physically present. Moreover, the equipments also require calib ration, and are sometimes not accurate enough. Since in our games, players need to scan the whole web page to summarize info rmation, our ga mes potentially record a huge amount of data for analysis. An example is shown in Table 3 and Fig. 7. Table 3 shows the query logs of URL number 1008 issued by user number 308 in one session, and Fig. 7 shows th e content of web page number 1008. We mark the queries on the related location of the web page in time sequence. From this figure, we can observe very clearly the track of this player's attention and also discover how this player extracts useful informa tion from the web page. We can use this information to improve ra nking based on page features, and also help users improve their web pages. In any human computation game, the data comes from the players who gave us data while playin g our games. We thank them for their help and the data they provided. [1] P. N. Bennett, D.M. Chicke ring, A.Mityagin. Learning [2] P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer. [3] A. Dempster, N. Laird, a nd D. Rubin. Likelihood from [4] D. Edery and E. Mollick. Changing the Game: How Video [5] N. Ford, T. D. Wilson, A. Foster, D. Ellis, and A. Spink. [6] P. Koehn, F. J. Och, and D. Marcu. Statistical phrase-based [7] E. Law, A. Mityagin, and M. Chickering. Building human-[8] E. Law, A. Mityagin, and M. Chickering. Intentions: A [9] E. Law, L. von Ahn, R. B. Dannenberg, and M. Crawford. [10] T. W. Malone. What makes things fun to learn? Heuristics [11] T. W. Malone. Heuristics for designing enjoyable player [12] M. McDonald, R. Musson and R. Smith. The Practical [13] NIST 2008 Machine Translation Evaluation. [14] T. Paek, Y.-C. Ju, and Chris Meek. People watcher: A game [15] C. Quirk, C. Brockett, and W. B. Dolan. Monolingual [16] S. Vogel, H. Ney, and C. Tillmann. HMM-based word [17] L. von Ahn. Games with a purpose. IEEE Computer, [18] L. von Ahn and L. Dabbish. Labeling images with a [19] L. von Ahn and L. Dabbish. Designing games with a [20] L. von Ahn, S. Ginosar, M. Kedia, and M. Blum. [21] L. von Ahn, S. Ginosar, M. Kedia, R. Liu, and M. Blum. [22] L. von Ahn, M. Kedia, and M. Blum. Verbosity: a game for [23] L. von Ahn, R. Liu, and M. Blum. Peekaboom: a game for [24] I. Weber, S. Robertson and M. Vojnovic. Rethinking the [25] E. Agichtein, E. Brill, and S. Dumais. Improving web search [26] G. Buscher, A. Dengel, and L. van Elst. Query expansion [27] L. Azzopardi, V. Vinay. An Evaluation Measure for Higher [28] L. Azzopardi, V. Vinay. Accessibility in Information 
