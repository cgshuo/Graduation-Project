 Outlier detection is one of the most important data mining techniques with vital importance in many application domains including credit card fraud detection, network intrusion detection, environment monitoring, etc. Hawkins [4] defines an outlier as an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism.
 Most of the earliest outlier detection techniques were given by statistics [6]. However, most statistical techniques are univariate, and in the majority of tech-niques, the parameter of distribution is difficult to determine. In order to over-come these problems several distance-ba sed approaches for outlier detection have been proposed in data mining [5], [11], [14].

Due to the increasing usage of sensors, RFIDs and similar devices for data collection these days, data contains cer tain degree of inherent uncertainty. The causes of uncertainty may include limitation of equipments, absence of data and delay or loss of data in transfer. In order to get reliable results from such data, uncertainty needs to be considered in calculation. In this work we study the problem of top-k distance-based outlier detection on uncertain data following the Gaussian distribution.
 In the following, uncertainty of data is modelled by the most commonly used PDF, i.e., the Gaussian distribution. Since the distance between uncertain data objects is very costly to compute, we intr oduce a populated-cell list (PC-list) based top-k outlier detection technique. PC-list is a sorted list of non-empty cells of a d -dimensional grid, where grid is used to index our data. Using PC-list, our top-k outlier detection algorithm needs to consider only a fraction of the dataset objects and hence quickly identifies candidate objects for top-k outliers. Furthermore an approximate top-k outlier detection algorithm is also presented to increase the efficiency of our outlier detection algorithm.
 The rest of the paper is organized as follows. Sec. 2 surveys the related work. Sec. 3 formally defines the top-k distance-based outlier d etection on uncertain datasets. The PC-list, the top-k algorithm and the approximate top-k algorithm are presented in Sec. 4. Sec. 5 contains a n extensive experimental evaluation that demonstrates the efficiency and scalability of proposed techniques. Sec. 6 concludes our paper. Distance-based outliers detection approach was introduced by Knorr, et al. in [5]. They defined a point p to be an outlier if at most M points are within D -distance of p . They also presented a cell-based approach to efficiently compute the distance-based outliers. [9] formulated distance-based outliers as the top-t data points whose distance to their  X  th nearest neighbour is largest. Angiulli et al. in [10] gave a slightly different definition of outliers than [9] by considering the average distance to their k nearest neighbours. Besides, there are some works on the detection of distance-based outlie rs over stream data including [13], [14] and [15]. These works are based on the Knorr, et al. definition of distance-based outliers. Furthermore, [13] gave an approximate algorithm to reduce the memory space required by its exact counterpart. Later on [14] extended [13] work by adding the concepts of multi-qu ery and micro-cluster based distance-based outlier detection. A g eometric approach of outlie r detection has also been proposed in [2]. The proposed solution is only suitable for identifying abnormal nodes from the cluster of nodes placed nearby and not valid for the problem when the measurements of a single node is classified as outliers, based on the nodes past measurements. However all these a pproaches were given for deterministic data and could not handle uncertain data.

Recently a lot of research has focused on managing, querying and mining of un-certain datasets [12], [7]. The problem of outlier detection on uncertain datasets was first studied by Aggarwal, et al. in [12]. They represented an uncertain object by a PDF. They defined an uncertain object o to be a density-based (  X ,  X  ) outlier, if the probability of o existing in some subspace of a region with density at least  X  is less than  X  . However, their work focuses on detecting outliers in subspaces. In practise, an outlier in subspace is not necessarily an outlier in full space as argued in [11]. [7] also proposed a distance-based outlier detection algorithm on uncertain datasets, which was later extended in [8] for probabilistic data streams. However in their works, an object X  X  existential un certainty is consider ed rather than repre-senting an object by a PDF as in our work.

In [1], we proposed a cell-based approach of distance-based outlier detection on uncertain data. According t o [1], an uncertain object o is a distance-based outlier if the expected number of objects lying within its D -distance is not greater than M = N (1  X  p ), where N is the number of objects in the dataset and p is the fraction of objects that lies farther than D -distance of o . In practise parameter p is difficult to determine and is dependent on N . An arbitrary value of p may results in a very few or a lot of outliers for different N . Moreover from [1], we cannot obtain the outlier X  X  ranking. Therefore in this work, we propose PC-list based approach of the top-k distance-based outlier det ection, which can always obtain k strongest outliers along with their ranking, provided k  X  N . The very first definition of distance-base d outlier detection on deterministic data was given by Knorr, et al. in [5]. They defined distance-based outliers as follows. Definition 1. An object o in a dataset DB is a distance-based outlier, if at least fraction p of the objects in DB lies greater than distance D from o .
 In this work, our focus is the detection of the top-k outliers on a dataset whose objects X  attribute values are uncertain. This paper assumes that the uncertainty is given by the Gaussian distribution. The Gaussian distribution is chosen for representing uncertainty, because in statistics the Gaussian distribution (or the normal distribution) is the most important and the most commonly used.
In this paper, k -dimensional un certain objects o i are considered, with at-tribute (  X  Namely, the vector tion values) of object o i . The complete database cons ists of a set of such objects, G DB = { o 1 , ..., o N } ,where N = |G DB | is the number of uncertain objects in G
DB . 3.1 Top-k Distance-Based Outliers in Uncertain Data We naturally extend Definition 1 for the top-k distance-based outliers on uncer-tain datasets as follows.
 Definition 2. The top-k distance-based outliers are the k uncertain objects in the dataset G DB for which the expected number of objects lying within D -distance is smallest.
 The objects that lie within D -distance of an object o are called D -neighbours of o and the set of D -neighbours of o is denoted by DN ( o ). In order to find the top-k distance-based outliers in G DB , the distance between uncertain objects needs to be calculated, which is given by another distribution known as the Gaussian dif-ference distribution [3]. Let random vectors with means respectively. Then, the probability that o j  X  DN ( o i ). Then, where R is a sphere with centre ( derivation of Pr ( o i ,o j ,D ), please refer our previous work [1]. Furthermore, we will use Pr (  X , D )todenote Pr ( o i ,o j ,D ) when there is no confusion, where  X  is an ordinary Euclidean distance between the means of o i  X  X  DB and o j  X  X  DB . Computing this probability is usually very costly, and we have to avoid this computation as much as possible.

The Naive approach of the top-k outlier detection given in Alg. 1 uses Nested-loop. In order to find whether an object o i  X  X  DB is a top-k outlier, we need to compute its expected D -neighbours ( EN ( o i )). Computation of EN ( o i )for an object o i  X  X  DB requires evaluation of N expensive distance functions. During the computation of EN ( o i ), if expected D -neighbours become greater than threshold  X  , o i is an inlier and the computation of EN ( o i )isstopped.On the other hand, if EN ( o i )islessthanorequalto  X  , o i is added to candidate list of outliers C obj , along with its expected D -neighbours. The C obj is kept sorted in ascending order of D -neighbours X  column and the top-k objects in it are selected as outliers. In the worst case, this approach requires O ( N 2 ) evaluations of distance function, which is very expensive. The Naive approach requires a lot of computation time to detect top-k outliers even from a small dataset due to the costly distance calculation. To overcome this problem we propose a PC-list-based approach of the top-k outlier detection. PC-list is an array of non-empty cells of a d -dimensional grid containing uncertain data objects o  X  X  DB . The PC-list helps in detection of the top-k distance-based outliers by identifying the cells containing candidate outliers.
 Lemma 1. Let o i ,o j  X  X  DB be two d -dimensional uncertain objects following the Gaussian distribution and  X  denotes an ordinary Euclidean distance between the means of o i and o j . Then for t  X  X  , denoting the number of standard de-viations required to enclose a large probability ( say &gt; 99%) of a d -dimensional Gaussian difference distribution, following statements hold. (a) If  X   X  D  X  t X  ,Pr ( o i ,o j ,D )  X  1 . (b) If  X   X  D + t X  ,Pr ( o i ,o j ,D )  X  0 .
 Algorithm 1. The top-k Naive Approach where  X  is the standard deviation of the Gaussian difference distribution in any one dimension (assuming that the standard deviation is uniform in all the dimensions).
 Proof. The number of standard deviations s needed to enclose a given probabil-ity for a d -dimensional random variable X following the Gaussian distribution can be obtained using the expression Pr { d M ( X, X  )  X  s } = G d ( s 2 ) [19], where d
M ( X, X  )= ( X is the CDF of the chi-squared distribution with d -degrees of freedom. Here we are interested in computing the di stance between two uncertain objects o and o j following the Gaussian distribution. This distance is given by another Gaussian distribution known as the Gaussian difference distribution [3]. Hence if t denotes the value of s , such that Pr { d M ( X, X  )  X  t } covers a large area of the Gaussian distribution (say &gt; 99%), then for  X   X  D  X  t X  ,Pr ( o i ,o j ,D )  X  1 and for  X   X  D + t X  ,Pr ( o i ,o j ,D )  X  0 4.1 Structure In order to find the top-k distance-based outliers from an uncertain dataset using the PC-list, we first quantize each object in G DB ,toa d -dimensional space that is partitioned into cells of length l (The cell length is discussed in Sec. 4.3). Let C indices. The layers ( L 1 , ..., L n )of C  X  1 ,..., X  d  X  X  are the neighbouring cells of C  X  1 ,..., X  d , as shown in Fig. 1 and are derived as follows. L denote C  X  1 ,..., X  d when there is no confusion.
Let R D  X  t X  ( C ) denotes a region formed by D  X  t X  C  X  X  . The region R o C  X  t X  ( C ) is the count of objects within cells in region R D  X  t X  ( C ) (including C itself). Then the PC-list ( PC ) is a sorted list containing C ( C )and C D  X  t X  ( C ) for each non-empty cell C  X  X  as shown in Fig.2. The tuples in the PC-list are sorted in an ascending order of C D  X  t X  ( C ) column. The idea behind sorting is that outliers tend to exist in sparse regions. Sorting tuples in the PC-list, lets us identify cells in sparse regions. 4.2 Cell Bounds In order to identify cells C  X  PC , containing only inliers or candidate top-k outliers, their bounds on the expected D -neighbours are used. A cell C can be pruned as an inlier cell if the minimum expected D -neighbours for any object in C is greater than threshold  X  (  X  is discussed shortly). Similarly a cell can be identified as containing top-k outliers if the maximum expected D -neighbours for any object in C is less than  X  . Since the Gaussian distribution is unbounded, the PC-list need to be considered for the computation of bounds of C  X  PC . To compute cell bounds, the minimum and the maximum ordinary Euclidean distances between cells are required. Beside distance between cells, object count of each C  X  PC and precomputed Pr (  X , D ) values for  X  ranging from the minimum to the maximum ordinary Euclidean distances between cells in G are also required for the computation of C  X  PC bounds. The precomputed values are stored in a look-up table to be used by the top-k outlier detection algorithm. Distance between Cells: Let C p and C q are two cells in PC with indices  X  denote the minimum and the maximum ordinary Euclidean distances between C p and C q respectively. Distance between C p and C q depends on their positions in the grid G and can be derived as follows.

 X  Now we can obtain bounds for cells in the PC-list using pre-computed Pr (  X , D ) values and the information available in the PC-list. Let LB ( Pr ( C p ,C q )) and UB ( Pr ( C p ,C q )) denote Pr (  X , D ) values at minimum  X   X   X  max ( C p ,C q )and maximum  X   X   X  min ( C p ,C q ) respectively. Then for a C  X  PC , LB ( C )= ( C  X  PC LB ( Pr ( C, C ))  X  X  ( C )and UB ( C )=( C  X  PC UB ( Pr ( C, C ))  X  X  ( C )).
Let R D + t X  ( C ) denotes the region formed by cell C  X  X  as shown in Fig. 1. Region R D + t X  ( C ) is chosen in such a way that for contribution in the bounds for C  X  X  is done by the cells in region R D + t X  ( C ), we redefine the bounds for C  X  PC , to reduce the number of pre-computations and bounds computation time, as follows.
 Number of Pre-computations: Since the bounds are pre-computed for the cells in region R D + t X  ( C ), Pr (  X , D ) values are computed only for the neighbour-ing layers within D + t X  distance of a cell. For require 2 D + t X  l ! pre-computations. Two more pre-computations are required for the cell C itself and the objects that lie greater than D + t X  distance of a cell. Hence the total number of pre-computations required are only 2 D + t X  l ! +2. 4.3 Candidate Outlier Cells Let C cell is a list containing candidate outlier cells from PC ,sortedinas-cending order of UB ( C ). Let C k  X  C cell is a cell with the minimum upper bound containing the k th object. A C  X  PC is a candidate outlier cell whenever Cell Pruning and  X  Updation: For a C  X  PC ,if LB ( C ) &gt; X  , C cannot contain any of the top-k outliers and can be pruned. On the other hand, if LB ( C )  X   X  , C may contain the top-k outlier. C is added to C cell , such that C cell remain sorted of its UB ( C ) attribute. Set  X  = UB ( C k )andremove C from C cell , such that LB ( C ) &gt; X  , as they cannot contain the top-k outliers. Stopping Condition: The PC-list is scanned from top to bottom for candidate outlier cells. During the scanning, if a C  X  PC is found such that Pr ( D  X  t X , D )  X  C  X  t X  ( C ) &gt; X  , neither C nor any cell after it in PC-list can contain outliers. Hence the PC-list scanning can be stopped at this point.
 Cell Length l : Due to the complexity of our distance function, it is not possible to derive a single cell length l suitable for all the combinations of D and variances. Very small cell length increases the num ber of cells in the Grid exponentially and the time required to construct the PC-list. A good starting point of the cell length that we found through experiments is the standard deviation, i.e., l =  X  . Algorithm 2. The Top-k Distance-based Outliers 4.4 Outlier Detection Algorithms In this section, we present t wo algorithms to detect top-k distance-based out-liers from uncertain datasets. The first algorithm computes accurate expected D -neighbours for all the un-pruned objects, however the second algorithm ap-proximates the expected D -neighbours to reduce the algorithm computation cost.
 Top-k Algorithm: The algorithm 2 first maps dataset objects to appropriate grid cells and creates the PC-list in lines 4 and 5 respectively. Since the PC-list is sorted in the ascending order of its C D  X  t X  ( C ) column, it guarantees that cells in the sparse regions of the grid G are at the top of the PC-list. Hence the can-didate outlier cells are expected to be at the top of the list. We scan the PC-list and add the candidate outlier cells in C cell until the stopping condition on line 8 becomes true. The number of objects in C Cell may be greater than k , hence we calculate expected D -neighbours EN ( o ) of candidate objects to find the top-k outliers and their ranking. The o is then added to the C obj (set of candidate outlier objects) along with its EN ( o ). The objects in C obj are sorted in ascend-ing order of EN ( o ) column. As the k th object X  X  EN ( o ) is found, threshold  X  is set (refer line 10 of Alg.1). During the calculation of EN ( o ), if for some o , EN ( o ) becomes greater than  X  ,then o can not be among the top-k outliers and is removed from furth er consideration.
 Approximate Top-k Algorithm: In the top-k algorithm, the minimum num-ber of distance function computations required for the evaluation of kEN ( o ) is kN , however the candidate outlier objects which require the evaluation of EN ( o ), may be greater that k . When the distance function is expensive to com-pute (as in our case), computation of even kEN ( o ) is very expensive. According to our distance function, the major contribution in the evaluation of EN ( o )is done by the nearer objects. Hence EN ( o ) for each un-pruned o can be approx-imated with high accuracy by co nsidering objects only within D + t X  distance of o according to Lemma 1, rather than cons idering all the objects in dataset. Rest of the algorithm is same as that of Alg.2.
 Maximum Approximation Error: For any o  X  X  DB , maximum approxima-tion error (  X  max ) happens if all the o  X  X  DB \ o are at a distance slightly greater than D + t X  from o . Hence  X  max =( N  X  1)  X  Pr ( D + t X  +  X ,D ), where  X   X  R is a very small real value to make distance greater than D + t X  .

For example for t =9, d =2and N =10 5 objects,  X  max  X  10  X  5 .  X  max depends mainly on t . In practice t  X  6 gives sufficiently accurate EN ( o )for d =2and3. For higher d values, we need to increase t value according to Lemma 1. We conducted extensive experiments on synthetic and real data to evaluate the effectiveness and scalability of our proposed algorithms. All algorithms were implemented in C++, GNU compiler. Al l experiments were performed on a system with an Intel Core 2 Duo CPU E8400 3.00GHz CPU and 2GB main memory running Ubuntu 12.04 OS. All programs run in main memory and no I/O cost is considered. We use two synthetic datasets and two real datasets for our experiments. Synthetic datasets, unimodal Gaussian (UG) and trimodal Gaussian (TG) are 2-dimensional and are generated using BoxMuller method [16]. A 3-dimensional unimodal Gaussian (UG3D) dataset is also generated for the evaluation of our proposed approaches on 3-dimensional data. A shorthand notation  X  DatasetName + DatasetSize (e.g. UG5k to denote 5,000 tuples of unimodal Gaussian dataset) is used in figures. As for real-world data, we use two datasets: ADAPTE and SDSS. ADAPTE is obtained from CISL Research data archive [17] and SDSS is obtained from Sloan Digital Sky Survey [18]. ADAPTE consists of about 1,851 maximum and minimum tem perature values collected from the National Polytechnic Institute of Mexico and National Meteorological System. SDSS dataset contains 10,136 Right Ascension (or  X  X A X ) and Declination (or  X  X ec X ) coordinates of stars and galaxies.

All the datasets are normalized to have a domain of [0,1000] on every di-mension. For each point z in a dataset, we create an uncertain object o ,whose uncertainty is given by Gaussian distribution with mean z and standard devi-ation  X  in all the dimensions. Unless specified, following parameter values are used: D = 100,  X  = 10, l =10and k =0 . 1% of the respective dataset size. For approximate top-k algorithm, we considered objects only within D +6  X  dis-tance of each un-pruned object o . Pre-computation time is not included in the measurements. We first conduct experiments to evaluate the efficiency of our proposed algorithms presented in Sec.4.4. Fig. 3a compares the execution times of Naive and proposed algorithms on UG dataset. Our proposed algorithms are several times faster than its Naive counterpart due to their strong pruning ca-pability as can be observed from Fig.3b. Stopping condition discussed in Sec.4.3 helps identify candidate outlier cells very quickly. Fig.3c shows the percentage of cells considered in the PC-list to identify candidate outlier cells. The percentage is comparatively higher for trimodal Gaussian dataset because the dataset is relatively sparse and hence results in larger number of candidate outlier cells. Moreover the approximate top-k algorithm is thousands of times faster than the ordinary top-k algorithm, due to the reason discussed in Sec.4.4. From theoret-ical analysis in Sec. 4.4 and experiments we found that the approximate top-k algorithm gives an accuracy of up to several decimal digits in the evaluation of EN ( o ) and hence the outliers obtained from both the algorithms are same. Therefore in the rest of this section, we will evaluate only approximate top-k algorithm.

Graphs in Fig.4 show the effect of varyin g different parameters on the execu-tion times. It is obvious from the graphs in Figs. 4a, 4b and 4c that smaller cell lengths require lower execution times. However very small cell length increases the number of cells exponentially and therefore the execution time of the al-gorithm. Therefore we recommend the use of cell length equal to the standard deviation as discussed in Sec. 4.3. In Fig. 4c, k is very small due to the small size of ADAPTE dataset and therefore pruning time dominates the algorithm exe-cution time. Consequently as the number of cells decreases due to the increase in cell length, algorithm execution time d ecreases. Next we perform experiments by varying the parameter  X  .As  X  increases, the uncertainty of the object also in-creases. This increase in uncertainty results in smaller Pr ( o i ,o j ,D )valuesevenif o and o j are located nearby. Hence the number of distance function evaluations required increases for un-pruned objects, which results in higher execution times as can be observed from Figs. 4d, 4e and 4f. Figs. 4g, 4h and 4i show the effect of varying parameter D . For each un-pruned o from the PC-list-based pruning, increase in D resultsinanincreaseinthe D -neighbours which need to be consid-ered for the approximation of EN ( o ). Therefore it increases the execution time of the overall algorithm for larger values of D .FromFigs.4j,4kand4l,increase in k results in an increase in execution time of algorithm, which is quite obvious behaviour of our algorithm. Figure 5 shows similar effects of varying different parameters on three dimensional dataset. In this work, the top-k distance-based outlier detection approach on uncertain datasets of the Gaussian distribution based on the PC-list is proposed. PC-list helps identify candidate outlier objects very quickly without considering all the objects in dataset. Moreover an approximate top-k outlier detection approach is presented to further reduce the algorithm computation cost. An extensive empirical study on real and synthetic da tasets demonstrate t he effectiveness and scalability of our proposed approaches.
 Acknowledgement: This work has been partly supported by Grant-in-Aid for Scientific Research(A)(#24240015A).

