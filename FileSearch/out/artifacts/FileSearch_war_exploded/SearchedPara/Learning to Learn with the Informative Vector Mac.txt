 Neil D. Lawrence neil@dcs.shef.ac.uk U.K.
 John C. Platt jplatt@microsoft.com Recently, there has been a large amount of interest in the machine learning community in learning the kernel (equivalently, the covariance) of a Gaussian Process (GP), for either regression or classication (Williams, 1998). By learning the kernel from data, the accuracy of the GP can be improved. Typically, a GP kernel is parameterised by some vector  X  , where regardless of the value of  X  , the covariance will always be valid (a positive denite Mercer kernel).
 Depending on the availability of data, the parameters  X  can be learned one of three dierent ways: 1. The labelled training set The same data-set that is used to train the linear parameters of a GP model can also be used to estimate the kernel parameters  X  . This is the most common situation. Several dierent approaches have been used to estimate the kernel, in-cluding maximum likelihood estimation and hierarchi-cal Bayesian learning (Williams, 1998). If the labelled training set is small, then these methods may not es-timate the covariance accurately. 2. Unlabelled data If unlabelled data is available, then it can be used to dene a kernel. Examples of such work is the Fisher kernel (Jaakkola et al., 1999) or the work of Seeger (2002), where a generative model of unlabelled data can be used to create a kernel that can work well. In the context of GPs, however, it is unclear whether such a kernel estimates the true covariance of the GP that the target function is drawn from. 3. Labelled data from related tasks Using labelled data from related tasks is known as learning to learn or multi-task learning (Baxter, 1995; Thrun, 1996; Caruana, 1997). In the context of GPs, we assume that these related functions are drawn from the same GP and use the related data to t the parameters  X  . Minka and Picard (1997) were the rst to link multi-task learning with tting a GP covariance. If the re-lated task data is plentiful, we can t the parameters with low variance. This paper addresses the multi-task learning problem.
 There are two main issues in multi-task learning with GPs. The rst is computational eciency : CPU time as a function of training set size. Computing the gra-dient of the log-likelihood of the data from M tasks with respect to  X  requires solving M linear systems, each with dimension N m , where N m is the number of training points in the m th task. Thus, multi-task learning with GPs can be computationally daunting. The second issue is statistical eciency : accuracy as a function of training set size. Approximating maximum likelihood may slow down convergence to the correct parameter vector. Since it is always possible to speed up maximum likelihood by throwing away data, any proposed algorithm exhibit better statistical eciency than randomly sub-sampling the data.
 This paper proposes the multi-task informative vec-tor machine (MT-IVM), which is a computationally ecient algorithm to t a GP covariance to multiple tasks. MT-IVM adapts the IVM algorithm (Lawrence et al., 2003) to the multi-task case. Similar to the IVM, the MT-IVM selects a subset of data from the tasks. The data-points are selected by a simple, fast heuristic: choose the points that minimise the entropy of the posterior process. This heuristic tends to choose points that are maximally informative. MT-IVM is computationally ecient and more accurate than max-imum likelihood with sub-sampled data. This is the rst multi-task GP algorithm to be tested for both statistical and computational eciency. 1.1. Structure of Paper In order to describe MT-IVM, we rst review Gaus-sian Processes in section 2 and the original IVM in section 3. In section 4, we extend the IVM to multiple tasks by mapping the multi-task GP back to a sin-gle larger GP. Section 5 presents experimental results: we show that MT-IVM is better than random sub-sampling for multi-task learning of GPs. Section 5.2 shows that we can get faster training by treating speaker-independent phoneme recognition as a multi-task problem (one task per speaker) rather than one unied task. In this section we briey review Gaussian processes, introducing the notation that we will use in the follow-ing sections. Consider a simple latent variable model for data where the observations, y = [ y 1 . . . y N ] T , are independent from input data, X = [ x 1 . . . x N ] T , given a set of latent variables, f = [ f 1 . . . f N ] T . The prior distribution over the latent variables is given by a GP, with covariance function, or `kernel', K which is pa-rameterised by the vector  X  and evaluated at the points given in X . This relationship is shown graphically in Figure 1.
 The joint likelihood of the data can be written as where p ( y n | f n ) gives the relationship between the la-tent variable and our observations and is sometimes referred to as the noise model. For the least squares re-gression case (see e.g. (Williams &amp; Rasmussen, 1996)) the marginalised likelihood can be computed with-out approximations. Unfortunately when the noise model is non-Gaussian this marginalisation is not pos-sible. One solution to this problem is to approximate each factor of the noise model, p ( y n | f n ) with a Gaus-sian distribution, N precision  X  n . If this is done in a sequential man-ner through minimisation of a KL divergence the ap-proach is known as assumed-density ltering (ADF), see e.g . Csat X  (2002). Thus we start with the GP prior q ( f | X ,  X  ) = p ( f | X ,  X  ) and we include the n th factor of the noise model  X  p 1 ( f )  X  q 0 ( f ) p ( y n | x n ) posterior distribution. This posterior is then approxi-mated by minimising the KL divergence between  X  p 1 ( f ) and a Gaussian distribution q 1 ( f ) , A further data-point is then included and the approxi-mation is repeated. This is continued until all data has been included. The tractability of the approximation relies on the normalisation constant for  X  p i ( f ) , where By dierentiating ln Z i with respect to  X  i  X  1 ,n (the n element of  X  N  X  Similarly we dierentiate the log partition with respect to  X  i  X  1 ,n , the n th diagonal element of  X  i  X  1 , to nd and dening  X  in =  X   X  in + 1 Using this notation, under the ADF scheme, the fol-lowing update equations hold, 2.1. Classication Noise Model For classication we consider the probit noise model, where  X  (  X  ) is the cumulative Gaussian 2 and y n  X  { 1 ,  X  1 } , . The partition function is again found by taking the expectation of p ( y n | f n ) under the marginal distribution q i  X  1 ( f ) , y (1 +  X  i  X  1 ,n )  X  1 2 . Performing the necessary deriva-tives to obtain g in and  X  in we have 3 and In practice we wish to summarise our approximation to the likelihood in terms of  X  n and z n . This relationship can easily be found by replacing y n with z n in (2) and (3) and re-arranging to obtain Updates for  X  ters of q ( f ) , are then as those given in (4) and (5). The assumed density ltering (ADF) approach out-lined above assumes that all data-points will be made use of in determining the model. One problem with this is that including all N data-points gives the algo-rithm O that if we wish to nd the parameters of the kernel,  X  , by gradient based optimisation of the (log) likelihood, each gradient evaluation will be O tant to nd a method for reducing this complexity. In this section we will review the informative vector machine (IVM) which aims to nd a sparse represen-tation of the data-set, thereby reducing the computa-tional cost (Lawrence et al., 2003). Then, in section 4, we return to the the concept of learning from multi-ple tasks and show how it can be done eciently with Gaussian processes. 3.1. Data-point Selection with the IVM The informative vector machine approach to greedily obtaining a sparse representation of the data-set gives an approximation to the solution in O tions, where d is the number of data-points included in the sparse representation. We will denote the set of these `active points' with I and the set of those which are not included with J .
 The data-points in I are greedily selected using a simple criterion inspired by information theory: the change in entropy of the posterior process. For an in-dividual point, n , as a candidate for the i th inclusion this entropy change is given by where  X  i  X  1 ,n is the n th diagonal element from  X  i  X  1 Other criteria (such as information gain) are also straightforward to compute.
 The idea behind the IVM is to greedily minimise the entropy of the true posterior through incorporating data-points that most reduce the entropy in a sequen-tial manner. We note from (10) that in order to score each point we need to keep track of  X  i , the diagonal of  X  i  X  1 , and  X  in . If these can be eciently computed or stored then we will have an ecient algorithm. Note that maintaining  X  i  X  1 in memory would require O  X  is to seek an ecient representation of the posterior covariance. From (5) it is clear that  X  i has a particular structure, where successive outer products are added to the original prior covariance  X  0 = K to form the current covariance. This can be represented by where the k th row of M is given by  X   X  k,n and n k represents k th included data-point. Naturally, if we are not storing  X  i  X  1 , we will not be able to represent (for instance) s i  X  1 ,n can be computed from M i  X  1 and K . where k n a tion will require O (( i  X  1) N ) operations and domi-nates each point inclusion resulting in an algorithm of complexity O From (4) and (5) it can be seen that the diagonal of  X  ,  X  i , can be updated by and the mean output vector can be updated by Storage requirements are dominated by M i which at maximum is an N  X  d matrix.
 The overall algorithm for the IVM is given in Algo-rithm 1.
 Algorithm 1 The standard IVM algorithm.
 Require: d a number of active points. For classica-tion z = 0 and  X  = 0 . For regression substitute ap-propriate target values. Take  X  0 = diag ( K ) ,  X  = 0 ,
J = { 1 , . . . , N } , I = { X } , M 0 is an empty matrix. for i = 1 to d do end for Optimisation of kernel parameters can now be achieved through maximisation of the approximation to the marginal likelihood, where B = diag (  X  ) . The dependence of the likelihood on y is indirect and through z and  X  . Gradients of the log-likelihood with respect to kernel parameters,  X  may be computed and used in a non-linear optimiser such as scaled conjugate gradients to develop an esti-mate  X   X  .
 This completes our review of the IVM, we now describe how the same principles may be applied when learning from multiple tasks. In this section we will extend the IVM approach to handle the situation where we have multiple indepen-dent tasks. We assume that we are given M training sets from tasks which are independent given a vector of parameters  X  and an input matrix X m (see Figure 2). We model the target data for each task, y m , as a GP so that the probability distribution for the matrix Y , whose columns are y m , is where each p ( y m | X m ,  X  ) is a Gaussian process. The entire likelihood can be considered to be a Gaus-sian process over a vector y which is formed by stack-ing columns of Y , y = matrix is then and we write This establishes the equivalence of the multi-task GP to a standard GP. Once again we obtain an estimate,  X   X  , of the parameters by optimising the log-likelihood with respect to the parameters  X  . These gradients require the inverse of K and, while we can take advantage of its block diagonal structure to compute this inverse, we are still faced with inverting N m  X  N m matrices, where N m is the number of data-points associated with task m : so we look to the IVM algorithm to sparsify the GP specied by (14).
 It is straightforward to show that the new posterior co-a Gaussian with a block-diagonal covariance matrix, Note that k inclusions in total does not mean k in-clusions for each task. The i in (15) represents the number of inclusions for each task. The value of i will vary with m , but we prefer to drop this dependence to avoid further cluttering of our notation. Each block of the posterior covariance is where the rows of M ( m ) The means associated with each task are given by and updates of  X  ( m ) From (16) and (17) it is obvious that the updates of q M models. Point selection, however, should be per-formed across models, allowing the algorithm to select the most informative point both within and across the dierent tasks. We must also need to maintain an active, I ( m ) , and an inactive, J ( m ) , set for each task. The details of the algorithm are given in Algorithm 2.
 The eect of selecting across tasks, rather than select-ing independently within tasks is shown by a simple experiment in Figure 3. Here there are three tasks, each contains 30 data-points sampled from sine waves Algorithm 2 The multi-task IVM algorithm.
 Require: d the number of active points. for m = 1 to M do end for for k = 1 to d do end for dierent distributions for the input data: in the rst it was sampled from a strongly bimodal distribution; in the second it was sampled from a zero mean Gaus-sian with standard deviation of 2 and in the third task data was sampled uniformly from the range [  X  15 , 15] . An MT-IVM with d = 15 and an RBF kernel of width 1 was trained on the data. The data-points that the MT-IVM used are circled. Note that all but six of the points came from the third task. The rst and second task contain less information because the input data is less widely distributed, thus the MT-IVM algorithm relies most heavily on the third task. This toy example illustrates the importance of selecting the data-points from across the dierent tasks.
 To determine the kernel parameters, we again max-imise the approximation to the likelihood, We show the eectiveness of the MT-IVM on two dif-ferent tasks. First, in section 5.1, we take a GP with a covariance function with known parameters. We hide the parameters from the MT-IVM, then generate a se-ries of tasks from the GP, feed the training data for the tasks to the MT-IVM, and measure how quickly the MT-IVM converges to the true results. The second test, in section 5.2, uses the MT-IVM as a classica-tion algorithm on a small, real data-set for phoneme recognition. The traditional way to view phoneme recognition is to gather a data-set from many speak-ers, throw away the speaker labels, then train a single speaker-independent model for the phones. Given the MT-IVM, we can treat speakers as tasks, which are in-dependent given the GP covariance for the phonemes. We then test to see if the MT-IVM converges faster than a speaker-independent GP model trained with IVM. 5.1. Regression with the Multi-task IVM The rst test generates data from an articial M -task GP with a covariance function parameterised by  X   X  ij is the Dirac delta function and We generate `training tasks' from a Gaussian process with  X  = [1 , 1 , 100 , 0] . For each task: the number of input dimensions was 4 and N m = 2000 ; half the input vectors, X m , were sampled independently from a spherical Gaussian distribution with mean in each direction of 1 and variances of 0.125; the other half were sampled from a Gaussian with the same covari-ance but a mean in each direction of -1; the y m values were sampled from a GP parameterised by  X  .
 To optimise the MT-IVM we initialised  X   X  = [10 , 10 , 10 , 10] and selected an active-set. The likeli-hood of the active-set was then optimised with respect to the parameters using scaled conjugate gradients un-til convergence or a maximum of 50 iterations. The active-set was then reselected using the new estimate for  X   X  . This process was repeated ve times. We compared the MT-IVM approach with random sub-sampling of the the data-set. To optimise in this case we simply maximised the likelihood of the sub-sampled data using scaled conjugate gradients for a maximum of 200 iterations.
 to 1000 at intervals of 100. For uniform sub-sampling of 50 samples.
 The quality of the parameter estimates was measured using the KL divergence evaluated for a separate set of test points: Ten runs were made for each sub-sample or active set size. Results are presented in Figure 4. In the top gure, we simultaneously test for both statistical and computational eciency by plotting the KL divergence as a function of CPU time. Note that with about 170 seconds of training time the MT-IVM is already close to a KL divergence of zero, whereas for sub-sampling even with over 1500 seconds of training time the mean of the KL divergence is still some distance from zero. This dierence is explained by the statistical eciency of the algorithm (bottom of Figure 4) because the MT-IVM actively selects data-points it achieves far lower KL divergences with fewer data-points. There is a small time penalty associated with the active point-selection, but it is insignicant when compared to its associated benets. 5.2. Classication with the Multi-task IVM In this section we turn to a speech example from the UCI repository (Blake &amp; Merz, 1998) The data consists of 15 dierent speakers saying 11 dierent phonemes 6 times each (giving 66 training points for each speaker). Our aim will be to learn kernel pa-rameters that are appropriate for classifying the dif-ferent phonemes. To this end, we will consider that each speaker is independent given the kernel parame-ters associated with the phoneme, i.e. we treated each speaker as a separate task. We used 14 of the speak-ers to learn the kernel parameters for each phoneme giving 14 tasks. Model evaluation was then done by taking one example of each phoneme from the remain-ing speaker (11 points) and using this data to construct a new Gaussian process model based on those kernel parameters. Then we evaluated this model's perfor-mance on the remaining 55 points for that speaker. This mechanism was used for both an MT-IVM model and an ADF trained GP where points were randomly sub-sampled.
 To demonstrate the utility of the multi-task framework we also built a IVM based Gaussian process model on the 14 speakers ignoring which speaker was associated with each data point (924 training points). The kernel parameters were optimised for this model and then the model was evaluated as above.
 For enough information to be stored by the kernel about the phonemes it needs to be `parameter rich'. We therefore used a kernel which could scale each in-put k  X  ij is the Dirac delta function, D = diag (  X  6 . . .  X  6+ K ) and K is number of elements in each input vector x . This lead to a total of 15 parameters for the kernel.
 The results on the speech data are shown in Figure 5. The convergence of MT-IVM to  X  10% error is roughly 10 times faster than the IVM. The MT-IVM takes ad-vantage of the assumed independence to train much faster than the regular IVM. While this data-set is relatively small, the structure of this experiment is im-portant. One reason for the popularity of the combi-nation HMM-Gaussian mixture model for modelling in speech is the ease with which these generative models can be modied to take account of an individual speak-ers this is known as speaker-dependent recognition. Up until now it has not been clear how to achieve this with discriminative models. The approach we are sug-gesting may be applied to large vocabulary word recog-nisers and used in speaker-dependent recognition. In order to eciently train multi-task Gaussian pro-cess regression and classication, we have generalised the informative vector machine to the multi-task case. We modelled the multi-task training sets as being in-dependent, given the parameters of the covariance for all of the tasks. Given this model, the covariance for the multi-task data is block diagonal, where each block arises from each task. We then apply the standard IVM algorithm to nd the maximum likelihood esti-mate of the underlying parameters of the Gaussian process. The IVM algorithm does not compute the full covariance matrix for all tasks. Rather, it eciently selects points from all of the tasks, by a heuristic that every point should minimise the entropy of the poste-rior process.
 We have shown that multi-task IVM is more ecient (from a combined statistical/computational eciency standpoint) than random sub-sampling. This e-ciency is shown by tests on articial regression data and a phoneme recognition data-set. We also showed that re-expressing a speaker-independent phoneme task as multiple speaker-dependent tasks makes train-ing much more ecient.
 A software implementation of the MT-IVM is available from http://www.dcs.shef.ac.uk/~neil/mtivm/ . Baxter, J. (1995). Learning internal representations.
Proc. COLT (pp. 311320). Morgan Kaufmann Pub-lishers.
 Blake, C. L., &amp; Merz, C. J. (1998). UCI repository of machine learning databases.
 Caruana, R. (1997). Multitask learning. Machine Learning , 28 , 4175.
 Csat X , L. (2002). Gaussian processes iterative sparse approximations . Doctoral dissertation, Aston University.
 Jaakkola, T. S., Diekhaus, M., &amp; Haussler, D. (1999).
Using the Fisher kernel method to detect remote protein homologies. 7th Intell. Sys. Mol. Biol. , 149 158.
 Lawrence, N. D., Seeger, M., &amp; Herbrich, R. (2003).
Fast sparse Gaussian process methods: The infor-mative vector machine. Advances in Neural Infor-mation Processing Systems (pp. 625632). Cam-bridge, MA: MIT Press.
 Minka, T. P., &amp; Picard, R. W. (1997). Learning how to learn is learning with point sets. Web. Revised 1999, available at http://www.stat.cmu.edu/minka/ . Seeger, M. (2002). Covariance kernels from bayesian generative models. Advances in Neural Information Processing Systems (pp. 905912). Cambridge, MA: MIT Press.
 Thrun, S. (1996). Is learning the n -th thing any easier than learning the rst? In (Touretzky et al., 1996), 640646.
 Touretzky, D. S., Mozer, M. C., &amp; Hasselmo, M. E. (Eds.). (1996). Advances in neural information pro-cessing systems , vol. 8. Cambridge, MA: MIT Press. Williams, C. K. I. (1998). Prediction with Gaussian processes: From linear regression to linear predic-tion and beyond. Learning in Graphical Models . Dordrecht, The Netherlands: Kluwer.
 Williams, C. K. I., &amp; Rasmussen, C. E. (1996). Gaus-sian processes for regression. In (Touretzky et al.,
