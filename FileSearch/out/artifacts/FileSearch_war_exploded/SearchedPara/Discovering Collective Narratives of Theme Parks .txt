 We present an approach for generating pictorial storylines from large collections of online photo streams shared by vis-itors to theme parks ( e.g . Disneyland), along with pub-licly available information such as visitor X  X  maps. The story graph visualizes various events and activities recurring across visitors X  photo sets, in the form of hierarchically branching narrative structure associated with attractions and districts in theme parks. We first estimate story elements of each photo stream, including the detection of faces and support-ing objects, and attraction-based localization. We then cre-ate spatio-temporal story graphs via an inference of sparse time-varying directed graphs. Through quantitative eval-uation and crowdsourcing-based user studies via Amazon Mechanical Turk, we show that the story graphs serve as a more convenient mid-level data structure to perform photo-based recommendation tasks than other alternatives. We also present storybook-like demo examples regarding explo-ration, recommendation, and temporal analysis, which may be most beneficial uses of the story graphs to visitors. I.4.9 [ Image processing and computer vision ]: Appli-cations Photo storylines; summarization and exploration of multi-media data; user modeling
Current technical advances, including widespread avail-ability of mobile photo taking devices and ubiquitous net-work connectivity, are changing the way we tell our sto-ries. Personal storytelling is becoming more data-driven and  X  c  X  vision-oriented ; people can simply capture their memorable moments by a stream of images, and then spontaneously reuse them to deliver their own stories. In addition, with the emergence of social networking, the sharing of such vi-sual stories is becoming effortless. For example, even in a single day, tens of thousands of people visit Disneyland , and much of them take and are willing to share large streams of photos that record their experiences with families or friends. Each photo stream tells a slightly different story from its own point of view, but by aggregating them, it is likely that common storylines of Disneyland experience emerge. The storylines can summarize a variety of visitors X  activity pat-terns such as popular paths in the theme parks and temporal changes of the flow in and out of the attractions.

In this paper, the term story refers to personal or collective narrative , instead of a fictional story as is the case in novels, games, or films. The personal narrative describes accounts of specific events that have been personally experienced [9], whereas the collective narrative is regarded as a collection of personal narratives from different people that share the similar experiences. We consider a photo stream of each vis-itor as a visual instantiation of his or her personal narrative, and the aggregation of photo streams as that of collective narrative, assuming that photographers take pictures when they encounter scenes or events that they want to remember or tell a story about. We define a photo stream as a set of images that are taken in sequence by a single photographer within a fixed period of time ( e.g . one day).

Fig.1 summarizes the problem statement. Our goal is to develop an approach for creating and exploring spatio-temporal storylines from large collections of photo streams contributed by visitors to Disneyland , along with its public information like visitor X  X  map. We also use meta-data of im-ages such as timestamps and GPS information if available, although they are often noisy and missing ( e.g . only 19% of photo streams in our dataset has GPS information). Tak-ing advantage of computer vision techniques, we represent the visual contents of images in the form of story elements ( e.g . human faces, supporting objects, and locations), and automatically extract shared key moments and put them together to create a story graph , which is a structural sum-mary of branching narratives that visualize various events and/or activities recurring across the input photo sets. To show the usefulness of story graphs, we leverage them to per-form photo-based exploration, recommendation, and tempo-ral analysis tasks. For example, once we have story graphs summarizing people X  X  experiences on their Disneyland trips, we can recommend pieces of those experiences to new visi-tors, suggesting attractions, touring plans, or best ways to experience a given attraction at a particular time of the day.
As a problem domain, we focus on theme parks , more specifically Disneyland , for the following reasons. First, we can easily obtain abundant visual data, because visiting a theme park is a special leisure activity where much of vis-itors are willing to take and share the pictures of their ex-periences 1 . Second, stories play an important role in theme parks for the purpose of user modeling, personalization, and recommendation. Theme parks provide different sets of at-tractions and entertainment, only parts of which are experi-enced by individual visitors. Thus a large diversity of possi-ble stories exist according to who visits when. Families with children may prefer to play with the characters their kids like. Such storylines may differ from those of young adults who may enjoy more adventurous rides ( e.g . roller coasters). Finally, our story extraction and exploration in theme parks can be easily extended to other leisure and entertainment domains, such as city tours [15] and museum tours [30], as long as they have sufficient sets of photo streams. Similar to theme parks, cities also consist of multiple attractions that are more sparsely distributed and events, attendance of which differs by preference, time and visitor types.
Our story graphs can be used as mid-level data structure for a variety of applications, including virtual exploration, path planning, travel recommendations, and temporal min-ing tasks, among many others. Usually theme park opera-tors, including Disney, do not disclose any statistics on the use of attractions. Thus, our story graphs can indirectly hint the visitors X  behavior patterns. Compared to guided routes and lists of events by travel agencies, our visual exploration using story graphs directly builds upon consumer-generated pictures, and thus reflects candid and spontaneous peer cos-tumers X  experiences in the original visual forms. It could be more synergetic to integrate our results with the official Apps of theme parks ( e.g . My Disney Experience mobile app of Disneyland ). Not only for visitors, our story graphs can benefit theme park operators by providing an automatic tool for monitoring the popularities of attractions. We also focus on revealing in-depth branching stories at each attraction, which are not easily mined from other sensor modalities. For example, although a GPS tracker can more accurately
For example, Disneyland is reported as the most geo-tagged location in the world on Instagram in 2014, as shown in http://time.com/3618002/most-instagram-places-2014/. localize the visitors X  paths, it cannot correctly discover the visitors X  activities that occur inside individual attractions.
To conclude, we summarize the contributions of this pa-per as follows. First, to the best of our knowledge, our work is the first approach for creating spatio-temporal story graphs from large sets of online photo streams, especially in the domain of theme parks where the discovery of under-lying stories is of a particular importance for user model-ing, personalization and recommendation. Through quanti-tative evaluation and crowdsourcing-based user studies with Amazon Mechanical Turk, we show that the story graphs serve as a convenient and mid-level data structure to which many applications are supported by simple algorithms ap-plied, including photo-based exploration, recommendation, and temporal analysis. Second, we develop a scalable algo-rithm for creating semantic story graphs, consisting of two components: story element estimation and graph inference. The story element estimation includes the detection of faces and supporting objects, and attraction-based localization. The graph inference algorithm builds a time-varying sparse story graph directly from estimated story elements of photo streams, while addressing several key challenges of large-scale nature of our problems, including global optimality, easy parallelization, and linear computation time.
We discuss some representative previous studies from four lines of research that are closely related to our work.
Story extraction from Web data . In recent web min-ing research, much work has been done to build and visual-ize storylines from online text corpora such as news articles and scientific papers [1, 5, 22, 26]. The work of [5, 22] ad-dresses the problem of extracting diverse story threads from large document collections. In [1], a probabilistic model is presented to group incoming news articles into tempo-rary but tightly-focused storylines, to identify prevalent top-ics/keywords and evolving structure of stories. In [26], the NIFTY is developed for real-time tracking of memes ( e.g . short textual phrases that travel and mutate on the Web) from blog posts and news articles. From this line of research, our work differs that we leverage image collections instead of text data, to discover and explore the storylines of user data.

Story extraction from visual data . The story has been used as an important concept to build interactive video summarization or editing [7, 29], in which the objective is to summarize input video clips in a concise form ( e.g . static images, shorter videos, or language sentences) while preserv-ing key moments of the videos and allowing users to intu-itively explore or edit them. In this category of work, input videos usually contain a small number of specified actors in fixed scenes. In contrast, our main technical challenge is the extraction of shared moments from photo streams that are independently taken by multiple visitors at different time.
The story extraction has been also investigated in the im-age domain as well. In [19], storyline-based summarization is discussed for a small private photo album. However, it is tested with only a small set of about 200 images from a sin-gle user X  X  photo collection. This work is also related to our previous work [12] that leverages a large set of Flickr images to create photo storylines. However, [12] exploits only tem-poral information between images, whereas we additionally consider spatial information that synergetically interplays with time. Thus, our approach also involves an image local-ization task. Second, the algorithm of [12] defines the nodes of graphs using low-level image features only, whereas we here leverage high-level semantic story elements, including the detection output of faces and supporting objects.
Data mining for tourism . As massive amounts of travel data are available and the location-based systems prolifer-ate, the data mining techniques begin offering significant benefits on tourism analytics in various ways. Some no-table examples include customized tour recommendations in urban areas [6], discovery of international human mobil-ity [25], recommendation of travel packages with considera-tion of both tourists X  interests and travel costs [4], to name a few. Geo-referenced Flickr pictures are also leveraged for this purpose. In [14], representative and diverse travel routes are discovered from the histories of tourists X  geotagged pho-tos for landmarks. In [31], a diversity ranking method for such trajectory patterns is proposed. Our main novelty is that we take advantage of computer vision techniques to un-derstand image contents, and thus we can possibly use all the images no matter whether GPS information is available or not. In addition, our idea of story-based summarization and exploration is unique in this line of work.
 Exploration of large collections of tourists X  images . With recent popularity of online image sharing, there has been a significant amount of effort for intuitively exploring large collections of unstructured tourists X  photos. The work of [10] organizes the Flickr images of landmarks by analyz-ing the associated location information, tags, and contents. One of early pioneering work is Photo Tourism [24], which calibrates geo-tagged photos of tourist landmarks in a 3-D space, and enables users to interactively browse them. This work has been extended later in many different di-rections. In [23], tourists X  paths around the landmarks are discovered for better navigation of the 3-D scene. In [21], a semantic navigation is established between regions of 3D landmark models and their corresponding reference texts of Wikipedia. In [15], online geo-tagged images are leveraged to create tours of the world X  X  tourist sites on Google Maps. Compared to this line of research, our work differs in that we aim to build storylines, which illustrate a series of spatio-temporal events or episodes without requiring availability of accurate geometric information as input. Therefore, our sys-tem can bypass the time-consuming step of reconstructing 3-D models.
 Table 1: The number of photo streams and images (  X  10 3 ) of the Disneyland dataset according to the park locations. Total numbers of images and photo streams are (2,513,367, 27,780), respectively. (CA: California, FL: Florida, PA: Paris, TK: Tokyo, HK: Hong Kong, HI: Hawaii, N/A: unknown or noisy).
 Table 2: The attraction statistics of Disney Califor-nia Adventure ( DCA ) and Disneyland Park ( DP ). The numbers in parentheses are about character dining .
The input of our approach is two-fold: a set of visitors X  photo streams and side information of the parks.

Photo streams . We download 3,602,727 unique images from Flickr by querying multiple Disneyland related key-words. Then, using the timestamp and user information associated with each image, we obtain 27,780 photo streams that contain more than 30 images. The input set of photo streams is denoted by P = { P 1 ,  X  X  X  ,P N } , where N is the number of photo streams. We sort the pictures of each photo stream by timestamps. Next we classify the photo streams according to the park locations where they are recorded: { California , Orlando , Tokyo , Paris , Hong Kong , Hawaii , N/A } . The N/A label is tagged for the photo streams that are not taken in any Disney parks or whose locations are unknown. We use GPS information when available; in our sets, 5,257 (19%) of photo streams include GPS informa-tion. Next we exploit the location keywords ( e.g . Califor-nia , Florida ) in the text data ( e.g . tags and titles) associ-ated with images. Table 1 summarizes the statistics of our Disneyland dataset.
 We apply our algorithm to the set of each park separately. To make our discussion easier and more coherent, we hence-forth focus on the two parks at California : Disney California Adventure Park and Disneyland Park .

Side information about attractions . Disneyland parks consist of multiple districts, each of which includes a sets of attractions, dining, and other entertainment ( e.g . pa-rades and stage shows). Table 2 summarizes the statistics obtained from Disney X  X  official maps. For simplicity, we hereafter use the term attractions to indicate attractions, dining, and entertainment events without distinction. We denote the set of attractions by L . For dining, we con-sider a character dining as a separate attraction, and all the others as a single restaurant attraction. The character dining ( e.g . Ariel X  X  Grotto ) is a restaurant where visitors can take pictures with characters ( e.g . Ariel the little mer-maid ). For each attraction, we download at maximum 1,000 top-ranked images from Google and Flickr by querying the attraction name. We use the attraction images as training data for various tasks of story element estimation, such as image localization and supporting object detection, which will be presented in Section 4. We also obtain the GPS Figure 2: Examples of images and GPS coordinates for four attractions in the Fantasyland district of Disneyland park . The attractions are identified by Disney X  X  official map, and images are retrieved by Google and Flickr image search. The GPS coordi-nates are obtained from Google Maps. coordinates of attractions from Google Maps, except for en-tertainment events that do not happen in a specific location ( e.g . Mickey X  X  Soundsational Parade ). Fig.2 shows sampled training images and GPS coordinates of four attractions in the Fantasyland district. Indeed, each attraction builds on a unique theme, whose visual contents are clearly discrimina-tive with other attractions. Thus, Google and Flickr images found by attraction names yield an excellent repository that captures various canonical views of the attractions.
The output of our algorithm is two-fold. First, we extract story elements from all images of the photo streams using computer vision techniques. Along with two low-level image features denoted by v (section 4.1), we define four types of story elements as story-related, high-level descriptors of images in the context of theme parks: { faces, time, location, supporting objects } , which are represented by four vectors: { f , t , c , o } . We will explain each of them in section 4.2 X 4.4.
Based on the estimated story elements over photo streams, the second output is the story graph G = ( O , E ). The ver-tices O correspond to dominant image clusters of the photo streams, and the edge set E links the vertices that sequen-tially recur in many photo streams. More rigorous definition will be shown in Section 5.
We discuss our low-level image description and story ele-ment estimation for the input photo streams P .
For low-level image description, we use dense feature ex-traction with vector quantization, which is one of standard methods in recent computer vision research. We densely ex-tract HSV color SIFT [17] and histogram of oriented edge (HOG) features [3] on a regular grid of each image at steps of 4 and 8 pixels, respectively. We form 300 visual words for each feature type by applying K-means to randomly se-lected descriptors. Finally, the nearest word is assigned to every node of the grid. As image or region descriptors, we build L 1 normalized spatial pyramid histograms to count the frequency of each visual word at three levels [16]. We de-fine the image descriptor v by concatenating the two spatial pyramid histograms of color SIFT and HOG features.
Since much of visitors X  images contain faces as foregrounds, we define a high-level image descriptor that encodes pres-ence/absence of faces and their spatial layout in the image. For example, we may cluster the images that have similar face sizes and positions ( e.g . clustering family photos), or bypass the localization step for the images that are fully oc-cupied by faces. For face detection, we use the Fraunhofer engine [20], which returns bounding boxes containing faces with confidence scores. Based on the detection, we compute the following three types of face features. (i) Histogram of face areas: we first make 10 bins in the range of [0, where h and w are image height and width respectively, and count the frequencies of squared areas of detected faces for each bin. (ii) Histogram of face locations: we split the image into 9 evenly split tiles ( i.e . 3 per dimension), and count the detected face centers in each spatial bin. (iii) Histogram of pairwise face distances: We compute pairwise distances be-tween centroids of all detected faces and make a histogram of 10 bins in the range of [0, diagonal length]. As a final face descriptor vector f  X  X  29 , we concatenate all the three histogram after L 1 normalization.

The timestamps of images can be trivially obtained from meta-data provided by Flickr. We use month and hour information to define time features because the events in theme parks are seasonally and daily periodic. We define the month feature t m ( i )  X  R 12 of image I in which each bin i has a value of Gaussian weighting t m ( i ) = g ( i  X  m )  X  exp(  X  ( i  X  m ) 2 / X  m ), where m is the month of image I and  X  m = 1 . 5. Its basic idea is that if an image is taken in May, the feature values for nearby months like April and June ( i.e . t (4) or t m (6)) are non-zeros as well. The same Gaussian weighting is used for the hour feature with  X  h = 2. Finally, the two features are L 1 -normalized and concatenated into the time descriptor t .
It is important to find out which attraction each image is likely to be taken, because visitors X  activities and stories can be modularized according to attractions. Thus, we per-form the attraction-based localization, whose objective is to determine the likelihood of each image over attractions. Mathematically, we assign a probability vector c i  X  R L to each image of photo stream P n = { I 1 ,...,I N n } , where N is the number of images in P n and L is the number of at-tractions ( i.e . L = |L| ). For notational simplicity, we let
We run localization of each photo stream separately be-cause all photo streams are taken by different users indepen-dently of one another. We use a Conditional Random Field (CRF) model to infer the conditional distribution over the attraction labels of each photo stream P n . The strength of CRF is the flexibility to easily incorporate various pieces of evidence related to localization as energy terms in a single Figure 3: An example of attraction-based localiza-tion. (a) An input photo stream with meta-data of timestamps, GPS coordinates, and text tags. (b) The appearance potential of each image is computed by visual similarity with attraction images. The text potential is defined by the tf-idf measure between tags of the image and attraction names. (c) GPS potential is obtained from a normal distribution on the distances between the image and attractions. unified model. Since each photo stream is a time-ordered se-quence of images, it can be modeled by a linear-chain CRF. The conditional probability of the attraction labels c given a photo stream P n is defined by log P ( c | P n ) = X where Z ( P n ) is the partition function. In the following, we discuss each term of Eq.(1) in detail.

Appearance potential . The  X  ( c l i ,I i ) represents the likelihood that image I i is visually associated with attraction l . (See Fig.3(b)). Intuitively,  X  ( c l i ,I i ) has a high value if I is visually similar to the images of attraction l . Since the training images of attractions are available, the estimation of  X  ( c l i ,I i ) can be accomplished by any image classifier or their combination. In this paper, the potential is computed by summing the normalized scores of two classifiers, a KNN classifier and a linear SVM [28]. For SVM classifiers, we learn multiple classifiers per attraction because the training images of each attraction may contain multiple views; we first split the attraction image set into k different groups us-ing K-means clustering with k = min(2 . 5 a separate classifier per cluster.
 GPS potential . For the 19% of photo streams where GPS information is available, we define the GPS potentials as a normal distribution:  X  ( c l i ,I i ) = N ( x l c ; x and x i are the GPS coordinates of attraction l and image I respectively. (See Fig.3(c)). We set the standard deviation  X  = 0 . 07 km. That is, if a attraction l is 3  X  (= 0 . 21 km) distant away from I i , l is very unlikely to be assigned to I . This GPS term is ignored for the photo streams without GPS information.

Text potential . We also take advantage of text infor-mation when available, as shown in Fig.3(b). We use the tf-idf measure (Term Frequency Inverse Document Frequency). Figure 4: Examples of detected regions of interest encoding supporting objects of different meanings.
 Let us denote the text associated with image I i by D i , which is a list of words of the title and tags. For each term q  X  D we compute the tf measure tf q,i , which is the number of oc-currences of q in D i , and the idf measure idf q = log( N/df where df q is the number of images containing q , and N is the total number of images. Then, for each attraction l , we generate a word list using its name, denoted by q potential  X  ( c l i ,I i ) of image I i to attraction l is computed by
Edge potential . The  X  ( c l i ,c m i ,  X  i ) is defined by the like-lihood that a pair of consecutive images I i and I i +1 are as-sociated with attraction l and m respectively, given  X  i that denotes the elapsed time between I i and I i +1 . The edge potential depends on both spatial connectivity between at-tractions and visitors X  walking speed in the park. We define the edge potential as follows. We first build a connectivity graph between attractions G c = ( L , E c ) based on the official park map and the GPS coordinates. Each attraction in L is linked to its k closest attractions, and the edge weight is given by the walking time between a attraction pair, which is computed by dividing the distance by the human walking speed s w . we set s w = 1km/h. Inspired by a human mo-bility study [8], we assume that a visitor stays in the same location with a probability  X  or moves to one of neighbor attractions with 1  X   X  .  X  is sampled from a truncated ex-ponential distribution:  X   X  exp(  X   X   X  i ). With probability 1  X   X  of moving to another attraction, we use the Gaussian model for a transition likelihood from attraction l to m :  X  ( c l i ,c m i ,  X  i ) = 1 where  X  is proportional to  X  i ; with a longer  X  i attractions can be reached.

Inference . Since our model for the localization is a linear-chain CRF, exact inference is possible. We use the Viterbi algorithm to obtain the most probable attraction assignment of the photo stream c  X  = [ c  X  1 ,...,c  X  n l ]. We also compute the posterior marginals of location labels c using the forwards-backwards algorithm.
We detect the regions of interest (ROI) that may include important supporting objects of images for the following two reasons. First, it can be used as another high-level image descriptor, as shown in Fig.4, because supporting objects il-lustrate the main theme of the images, such as rides, meals, or interaction with characters. Second, it can be exploited Figure 5: A small piece of story graph between and inside four attractions of Bug X  X  Land district of Dis-ney California Adventure Park. Each image repre-sents the central image of each vertex. for user interaction; we allow users to choose ROIs to re-trieve the images that share the similar supporting objects, as another control for story exploration. We will introduce examples in Section 7.

For ROI detection, we first learn a set of ROI classifiers of each attraction as follows. We first sample 10 rectangu-lar windows per training image of each attraction using the objectness metric [2], and build a similarity graph between the windows of all training images, using histogram intersec-tion on the color SIFT and HOG features v of the windows. By applying the diversity ranking and clustering algorithm of [13] to the similarity graph, we discover k representative and discriminative exemplar windows, and cluster windows by associating each window with its closest exemplar. We set k = min(2 . 5 each exemplar, we learn KNN and linear SVM classifiers as done for the appearance potential of localization in section 4.3. For a positive training set, we use the exemplar and its 10 nearest neighbors. For a negative training set, we ran-domly sample 200 windows from the other attractions. As a result, we have k number of classifiers per attraction. We repat this ROI classifier learning for all attractions.
Next we leverage the learned classifiers to detect the ROIs for the images of photo streams. As a pre-processing step, we first sample 100 windows per image using the object-ness metric. Our assumption here is that at least some of candidate windows correspond to good potential supporting objects. We then remove the windows if they largely overlap with face detection ( i.e . if more than 50% of the window area overlaps with any face region), due to redundancy with face detection. Then, if the image is localized as attraction l , we apply a set of learned classifiers for attraction l , and choose one single best window per image as the ROI. In this section, we discuss how to create a story graph G = ( O , E ) from the results of story element estimation. We build story graphs hierarchically. As shown in Fig.5, the upper-level story graph is defined between attractions, each of which subsumes small story subgraphs that repre-sent sequences of activities inside attraction. That is, in the storyline graph G = ( O , E ), the vertex set can be decom-posed into the ones per attraction ( i.e . O = S l  X  X  O l the edge set E consists of two groups: E = E L  X  X  I where E
L defines the edges inside each attraction, and E I includes edges between attractions.
Since many input images are redundant, it is inefficient to build a story graph over individual images. Hence, the vertices O are preferentially defined as image clusters . The vertex set O l for attraction l is obtained as follows. We first represent each image by concatenating the set of vectors { v , f , o , t } , each of which represents the low-level image fea-ture, the face descriptor, the ROI descriptor, and the time descriptor, respectively. We include the ROI descriptor o to encourage clustering the images that share similar support-ing objects. In order to define the vertex set O attraction, we first build a similarity matrix between the images localized at attraction l , by applying the histogram intersection to the above descriptors. We then discover k ex-emplars and clusters using the same diversity ranking and clustering algorithm of [13] with k = min(2 the k clusters constitues the vertex set O l for attraction k . We repeat this process for each attraction l  X  X  . We denote the number of vertices by M ( i.e . M = |O| = P l  X  X  |O l
As a result of clustering for the vertex definition, each image is now associated with a vertex membership vector x  X  R M , which has only a single nonzero element x v = 1 if the image is a member of vertex v  X  O . Therefore, each photo stream P n = { I 1 ,...,I N n } can be represented by the membership vectors P n = { x 1 ,..., x N n } .
The edge set E  X  V  X  V , whose adjacency matrix is denoted by A  X  R M  X  M , includes directed edges between the vertices that sequentially co-occur across many photo streams. In order for the story graphs to be practical, we enforce the edge set E to satisfy the following two proper-ties. (i) E should be sparse . If there are too many edges in the graph, the narrative structure is unnecessarily com-plex. Thus, we retain only a small number of strong story branches per node. ( i.e . A has only a few nonzero ele-ments). (ii) E should be time-varying ; E smoothly changes over time in a day t  X  [0 ,T ]. It means that the preferred transitions between attractions can change over time in the theme parks. For example, in many visitors X  photo streams, the images of attraction Mad Tea Party are likely to be followed by dining images around lunch time, by fireworks images at night, or by nearby attraction images like Pixie Hollow in other time. Hence, we infer individual A t every 30 minutes while changing t from 8AM to 12AM to densely capture such time-varying popular story transitions.
In our previous work [12], we formulate a maximum likeli-hood estimation for inferring { A t } given input photo streams P = { P 1 ,  X  X  X  ,P N } , and develop an optimization algorithm that has several appealing properties for large-scale prob-lems, including global optimality, easy parallelization, and linear computation time. We here skip the details of opti-mization, which can be found in [12]. Instead we denote edge reconstruction procedure by { A t } T t =1 = GraphInf ( P ,T ).
Although we use the similar algorithm for creating edges of graphs with our previous work [12], the story graphs of Table 3: Results of image localization. We report top-1/5 attraction accuracies, and top-1 district ac-curacies. We test our CRF-based approach with dif-ferent combinations of terms: ( T ), ( A ), ( G ), and ( E ) indicate appearance, GPS, text, and edge potential. this work are different from those of [12] in two respects. First, the graphs of [12] are built based on only temporal information between images, whereas here we consider both time and spatial information. Second, the vertices of the graphs of [12] are based on low-level image features only ( e.g . SIFT and HOG features), whereas we define vertices over story elements of high-level semantic image descrip-tion, including face and object detection. In section 6.2, we empirically compare these two story graphs for the image recommendation tasks.
We evaluate the accuracy of our story element estimation for image localization (Section 6.1). Then, we quantitatively compare the performance of image recommendataion using our story graphs with other candidate methods (Section 6.2).
We evaluate the performance of our approach for estimat-ing the locations of images, which may be considered as the most important story elements. The task here is to find out the attraction that a given image is likely to be taken. We select 108 attractions and restaurants as the classes for lo-calization. We obtain groundtruth by letting human experts to annotate 3,000 images of photo streams. We randomly sample 2,000 images out of them and perform a localization experiment, which is repeated ten times.

For baselines, we implement two vision-based methods, which are solely based on the visual contents of images. We use the training images of each landmark to learn linear-SVM and KNN classifiers, which are denoted by ( VSVM ) and ( VKNN ), respectively. In addition, in order to quantize the contribution of terms of our CRF-based localization frame-work in Eq.(1), we also measure the variation of localization accuracies by changing the combination of the four terms; we use ( T ), ( A ), ( G ), and ( E ) to indicate appearance, GPS, text, and edge potential, respectively.

Table 3 summarizes the results of our experiments. We also report chance performance ( Rand ) to show the diffi-culty of the localization task. Our CRF-based approach outperforms the vision-based methods; the accuracy of our full model ( A+T+E+G ) is higher than the best vision-based method ( VKNN ) by 61.6% and 26.7% in terms of the top-1 attraction and those district metric, respectively. We also make three observations from the results. First, the visual content of images is a strong clue for localization. Second, text associated with Flickr images is noisy, and hence pro-vides little to enhance localization accuracy. Third, the edge potential improves the localization accuracy by enforcing temporal constraints between consecutive images.

We note that the task is very difficult, with chance perfor-mance of under 1%. In many cases, even human experts feel difficulty in localizing images as content may match mul-tiple locations ( e.g . Mickey can be observed virtually at any location). We believe overall accuracy can be improved significantly by pre-filtering the images, however, in this ex-periment, our focus is on assessing the contributions of in-dividual terms toward our CRF localization objective, not absolute performance. Hence, the tendency is important in-stead of absolute numbers in this experiment.
We evaluate the ability of our story graphs to perform photo recommendation, which is one of key practical ap-plications of story graphs. We carry out the following two image sequence prediction tasks: (I) predicting next likely images given a short image sequence, and (II) filling in a missing part of a novel photo stream. The first task simu-lates the scenario where a visitor takes a small number of pictures during his trip, and the task suggests up the most likely next pictures of attractions by analyzing the photo sets of other users who had similar experience. The sec-ond task can show the pictures of alternative paths taken by other visitors. It helps the visitor compare similarity and differences of her trip with others. Fig.6 shows examples of the two prediction tasks.

We use the similar experimental protocol of [12]. We first randomly select 80% of photo streams as a training set and the others as a test set. We reduce each test photo stream into uniformly sampled 100 images, since consecutive images can be often very similar. For task (I), we randomly divide a test photo stream into two disjoint parts. Then, the goal is, given the last 10 images of the first part and next 10 query time points t q = { t q 1 ,...,t q 10 } , to retrieve the 10 images that are likely to appear at t q from the training set. The actual images at t q are used as groundtruth. For task (II), we randomly crop out 10 images in the middle of each test photo stream. Then, the goal is to predict the likely images for the missing part given the time points t q and five images before and after the missing slots. Since training and test sets are disjoint, each algorithm can only retrieve similar (but not identical) images from training data at best.
As a result of inference of story graphs, we obtain a set of { A t } , which can be regarded as a state transition matrix be-tween vertices of the graph at different time t . We adopt the state space model (SSM) to perform all prediction tasks [18]. For example, we can predict next k likely story vertices using the forward algorithm, and infer the best k paths between any two vertices using the top-k Viterbi algorithm.
We compare our approach with the method of [12] and the four baselines used in [12]. The first baseline ( Page ) is a Page-Rank based image retrieval, which is one of most successful methods to retrieve a small number of canonical images. The second baseline ( HMM ) is based on the HMM, which has been popularly used for modeling tourists X  se-quential photo sets. The third ( Clust ) is a clustering-based summarization on the timeline [11], in which images on the timeline are grouped into 10 clusters using K-means at every 30 minutes. The forth baseline is the method of [12], denoted by ( Temp ), whose key differences from our approach are (i) use of low-level features only (vs. story elements in our Figure 7: Results of our method and four base-lines for the two prediction tasks. The PNSR val-ues are as follows: [ ( Ours ) , ( Temp ), ( HMM ), ( Page ), [ 9 . 69 , 9 . 58 , 9 . 38 , 9 . 39 , 9 . 29] for task (II). approach) for the node definition, and (ii) temporal infor-mation only (vs. spatio-temporal information) for the edge definition. We measure the performance in two different ways: quantitative similarity measures and crowdsourcing-based user studies via Amazon Mechanical Turk (AMT).
Quantitative results . We evaluate the prediction accu-racy by measuring the similarity between predicted images and groundtruth images of test photo streams. For a similar-ity metric, we resize the images into 32  X  32 tiny images [27] to focus on holistic views instead of minor details, and com-pute peak signal-to-noise ratio (PSNR) between them. A higher value indicates that the two images are more similar.
Fig.7 shows comparison results between our method and four baselines for task (I) and (II). Our algorithm signifi-cantly outperforms all the competitors. That is, predicted images by our method are more similar to groundtruths than those by other baselines. For example, our PSNR perfor-mance gains (in dB) over the best baseline ( Temp ) are 0.131 and 0.106 for the two prediction tasks, respectively. All the numbers can be found in the caption of Fig.7.

User studies via AMT . Actual users X  satisfaction is the most important measure of recommendation. Thus, we here evaluate the performance of image recommendation using user studies via Amazon Mechanical Turk (AMT). We use the same experimental setup with the previous tests, except that the number of images to be estimated is reduced to five for easy evaluation by human subjects. We show given images and five empty slots as a question ( e.g . the first row of Fig.6), and then show a pair of image sequences predicted by our algorithm and one of baselines in a random order, while algorithm names are hidden. A turker is asked to Figure 8: Results of pairwise preference tests via AMT between our method and baselines. The number should be higher than 50% to validate the superiority of our method. The average prefer-ences of our method over ( Temp ), ( HMM ), ( Page ), and ( Clust ) are [ 61 . 2 , 65 . 1 , 73 . 9 , 68 . 1 ] for task (I), and [ 58 . 8 , 63 . 5 , 66 . 4 , 76 . 2 ] for task (II). choose one of them that is more likely to come at the empty slots. We obtain such pairwise comparison for each test set from at least five different turkers. In the second row of Fig.6, we show the groundtruth ( i.e . hidden actual images), and predicted images by our algorithm and three baselines.
Fig.8 shows the quantitative results of pairwise AMT pref-erence tests between our method and four baselines. The number indicates the mean percentage of responses that choose our prediction as a more likely one than that of each baseline. Hence, numbers should be higher than 50% to validate the superiority of our algorithm. Our algorithm significantly outperforms all the baselines; for example, our algorithm ( Ours ) gains 65.1% and 63.5% of votes over the baseline ( HMM ) in the two tasks.
Once we build a story graph as a data-driven summary of visitors X  activities in the parks, we can leverage it to-ward several interesting applications. We present prelim-inary storybook-like demos regarding exploration, recom-mendation, and temporal analysis, all of which may be ben-eficial uses of the story graphs to visitors.

Exploration . The first intuitive application is spatio-temporal exploration of the parks. As shown in Fig.9, we can hierarchically explore the stories between or inside at-tractions, with a single click of controls. In Fig.9.(a), the map on the left shows the current attraction and next avail-able ones that are popularly visited after the current attrac-tion. A user can proceed to a nearby attraction by clicking one of the blue controls. On the right panel, we show a part of story subgraphs at the current attraction ( i.e . the Main Street USA ). Here each photo shows the first image of each story subgraph, whose detailed view is presented in Fig.9.(c). Every story subgraph illustrates a popular coher-ent story thread, for example, from top to bottom, Fig.9.(c) shows Walt Disney Statue , Sleeping Beauty Castle , trans-portation at the main street, character experiences, and pa-rades. Fig.9.(b) shows an example of image retrieval using regions of interest (ROI). By clicking an ROI, we retrieve the closest neighbors using a simple KNN search. For retrieved images, we also show next most popular transitions.
Recommendation . Fig.10 shows an example of predict-ing the location of a novel image and suggesting next en-gaging paths. Since we organize images according to at-tractions in our story graph, we can easily localize the new images using a simple KNN search, as shown in Fig.10.(a). Based on the retrieved images, we suggest several attrac-tions from which the exploration begins. The query image includes Mickey X  X  Fun Wheel , which is viewable from most attractions in the Paradise Pier district. As done in the exploration example of Fig.9, a visitor is allowed to choose one of nearest attractions for further exploration, or have our system suggest a popular one for them. If a user chooses California Screamin X  , as shown in Fig.10.(b), we suggest two popular next available attractions, which are Ariel X  X  Grotto and King Triton X  X  Carousel . On the bottom, we preview the initial segments of story subgraphs of the two attractions.
Temporal analysis . Fig.11 shows an example of tem-poral analysis that benefits from our story graphs. Every two hours on the timeline, we present exemplar images of two central vertices of the story graph at the Mickey X  X  Fun wheel . The ranking scores of vertices at time t can be easily obtained by column-wise summing A t . As shown in Fig.11, even at a single location, a variety of events happen at dif-ferent time throughout the day, for example, Goofy Instant Concert , Pixar Play Parade , Electrical Parade , and World of Color from 2PM to 10PM. Our story graph can help visi-tors plan their itinerary beforehand by providing an intuitive way to promptly preview the popular events.
We presented an approach for creating story graphs from large collections of online photo streams shared by visitors to theme parks. We formulate the story reconstruction as a two-step procedure of story element estimation and infer-ence of sparse time-varying directed graphs. To demonstrate the usefulness of the story graphs, we leverage them to per-form photo-based exploration, recommendation, and tempo-ral analysis tasks. Through quantitative evaluation and user studies via AMT, we show that our algorithm outperforms other alternatives for two image prediction tasks.

There are several promising future directions that go be-yond the current paper. First, to be more practical for gen-eral users, we can leverage guidebooks to infuse semantic meaning for exploration and recommendation. Second, per-sonalization of story graphs is another interesting direction, in order to deliver relevant guidelines to visitors based on the group sizes, people X  X  age and gender, and visiting sea-sons. Finally, it would be also interesting to implement our technique as a function of Disneyland official mobile apps ( e.g . Disneyland Explorer and My Disney Experience ). of the two attractions, Ariel X  X  Grotto and King Triton X  X  Carousel . hours. Each image represents the central image of each vertex.
