 E.E. Regentova, S. Latifi, D. Chen, K. Taghva, D. Yao Abstract. Techniques are presented to directly process JBIG-encoded document images. Two experimental pro-cessing pipelines are designed to evaluate the perfor-mance of the methods from the application perspec-tive. They are document segmentation for obtaining the global layout and the form processing system for form type identification and the form dropout. The JBIG cod-ing context is employed to perform horizontal smear-ing and connected-component detection concurrently in the course of decoding the base layer of the JBIG im-ages. It is shown that, using a simple segmentation al-gorithm, the global layout is identified 50 times faster compared to the case of processing the full resolution images. In addition, an original solution is presented for form type identification by use of the Hough transform of the JBIG base layer images, thus expediting it by a factor of 16 in the designed form dropout system. Ad-vantages of the compressed domain processing include fast procedures, reduced memory requirements, and the possibility of hardware implementation.
 Keywords: Connected components  X  Segmentation  X  Form dropout 1 Introduction Document images are usually compressed before being exchanged and archived. Most compression methods are concerned mainly with image distortion, bit rate, and coding complexity. However, document analysis algo-rithms that commonly handle raster images are usually independent from the design of image compression al-gorithms. Traditional document analysis algorithms de-compress the document image completely before feature extraction. The overall system performance can be im-proved by study of joint image compression and feature extraction since there should be considerable synergy be-tween them.
 niques, such as ITU (formerly CCITT) Group III and Group IV [1] and JBIG (Joint Bi-level Image Expert Group) [2], what can possibly be done in the compressed domain? There are several advantages in pursuing as much functionality as possible in the compressed do-main. First, since there are fewer data than in the orig-inal uncompressed image, it is possible to reduce over-all computational cost. Second, most stored document materials are compressed. By analyzing images in the compressed domain, one can avoid the overhead of de-coding and re-encoding already compressed images. In fact, many compression algorithms act as some sort of information filter or content transformer, which can pro-vide good preprocessing for subsequent image analysis. Algorithms that extract features during the compression or decompression process also result in significant com-putational savings.
 more attention than direct manipulation of text data, al-though the scope of the existing work is limited to run-length coding and its derivatives. A few papers consider direct processing of the binary images of a document compressed using coordinate coding of run data. Shima et al. [3] developed an algorithm that rotates an image in two steps, utilizing the fact that rotating an image can be performed by shifting the image horizontally and then vertically. The authors reported that the perfor-mance of the run-length-encoded version improved by a factor of 12.8, compared to the uncompressed version. In another work, Ronse and Devijiver [4] presented a two-path implementation of the connected-components algo-rithm that manipulates efficiently the run-length encod-ing of a binary image. The first top-down path identifies equivalent classes. The second bottom-up path merges equivalent classes and assigns a label to each class. This implementation, however, does not guarantee the opti-mality of the labels of equivalence classes. Shima et al. have proposed a fast labeling algorithm by sorting and tracking runs and the label propagation to the connected runs [5]. To speed up the propagation step, each scan line is divided into optimal blocks, and runs in each scan line are block sorted. The authors have reported a speedup of 16.7 compared to the conventional pixel-based approach. tem for skewed documents by detecting the skew angle of documents using the Hough transform on the grayscale  X  X urst image X  of run-length-encoded binary images. The interline spacing is found by computing the 1D FFT along the column in the accumulator array correspond-ing to the correct skew angle and taking the inverse of the frequency with the largest magnitude. Pavlidis [7] has proposed a hybrid algorithm for vectorization of documents. Characters with well-defined strokes are detected by analyzing the line adjacency graph (LAG) and performing polygonal approximation. Complex ar-eas are vectorized by sequential thinning on pixels. An improvement of the hybrid algorithm is used in the fea-ture extractor and in the document recognition system of [8]. A transformation of the LAG called compress-LAG (c-LAG) is employed in the analysis.
 using the X  X  Y tree representation of an image. The doc-ument is classified into nested rectangles corresponding to meaningful blocks. This information is used in assign-ing labels to the blocks. Wang and Srihari have devel-oped a newspaper understanding system for run-length-encoded documents [10]. Homogeneous rectangles are ex-tracted by using algorithms like a run-length smoothing algorithm or recursive X  X  Y cuts. These rectangles are further classified using statistical textual features and feature space decision techniques.
 Group III and IV compressed images have been devel-oped. For example, Spitz has presented an algorithm for skew estimation [11] and another for logotype detection [12]. Hull and Cullen have developed a method for de-termining document similarity and equivalence [13]. In [14], Regentova et al. develop an algorithm for analyz-ing a sequence of codes in a 2D scheme for connection detection in two adjacent lines and attained about 3 X 4 times speedup.
 are few; however, Kanai and Bagdanov have presented a projection-profile-based skew estimation algorithm for JBIG compressed images [15]. Their method takes ad-vantage of both the JBIG coding template and resolu-tion reduction to develop a compressed-domain process-ing algorithm for skew estimation. The algorithm takes the fiducial reduction strategy used for processing ITU Group IV compressed images as proposed by Spitz in [12] and speeds up the estimation process and its accuracy. standard suggests computing at lower-resolution levels. The resolution reduction method used in JBIG employs a sophisticated approach to computing target pixels of lower-resolution images based on linear summation of weighted neighboring pixels. Also, some exceptions are introduced to provide the reduction of edge smearing ef-fects generally encountered in filtering operations and preserve dithered and periodic patterns. As a result, not only does the layout of a page remain apparent, but lines and edges are discernible as well. This property can be employed particularly for document layout anal-ysis and line detection. Processing lower-resolution im-ages bursts the economy of computations associated with high-resolution document images. Additionally, if it is possible to derive features in the course of decoding, an additional time saving is attained, resulting in an overall performance increase.
 resolution images is introduced and illustrated with examples. Smearing for structural segmentation and connected-component detection are performed concur-rently with the decoding. The Hough transform is carried out on the base layer of JBIG-encoded form documents to extract features needed for form characterization and form type identification. Applications of preprocessing techniques are discussed to evaluate the fidelity of low-resolution JBIG images, There are segmentation rules for obtaining the global layout and the form dropout. tion 2 provides an overview of the JBIG standard to the extent necessary for further reading. In Sect. 3, we in-troduce the general concept of compressed-domain pro-cessing. In Sect. 4, first, we design two preprocessing algorithms for implementing smearing and connected-component extraction in the course of decoding the base-layer images. Using the preprocessing results, we further obtain the global layout of eight standard ITU docu-ments by performing a simple segmentation algorithm based on the top-down and the bottom-up strategies. The advantages of extracting features of form documents using the parameter space of the Hough transform of the base-layer images and the application of such preprocess-ing for the form type identification and the form dropout are discussed in Sect. 5. Section 6 concludes the paper. 2 JBIG compression scheme The JBIG standard can be viewed as a combination of two algorithms, a progressive transmission scheme and a lossless compression algorithm. The compression algo-rithm is based on the context-based prediction method, so-called prediction by partial matching (PPM) followed by arithmetic encoding. Prediction is made on a two-or three-line template of the pixels, as shown in Fig. 1. The pixel to be coded is marked X , while the pixels to be used for the template are marked O or A . The O and A pixels are previously encoded pixels and are available to both encoder and decoder. The A pixel can be thought of as a floating member of the neighborhood. Its placement depends on the input being encoded. Each pixel is encoded using one of 1024 parallel arithmetic en-coders/decoders according to the context (pattern) made by its ten neighbor pixels. The context is used to index the array of arithmetic coders/decoders, and a sequence of pixels is coded using the corresponding contexts. The JBIG standard defines two kinds of contexts: one for the base, or lowest, resolution layer (as shown in Fig. 1) and the other for the differential, or higher, resolution layers. Differential-layer templates comprise pixels from both lower-and higher-resolution layers.
 image, or it can be used in progressive mode. An ap-proximate quality of the base low-resolution layer can be improved over time by transmitting the differential layers and successively doubling the resolution. The user can choose the base layer and a number of differential ones.
 JBIG standard produce documents of decent quality. The value of target pixel T of the lower resolution layer is calculated based on linear weighting of the neighboring nine higher-resolution and three lower-resolution pixels, colored in Fig. 2 as light squares and dark circles, re-spectively. The expected value of a target pixel is 4.5. It is chosen to be black if the value is 5 or more and white if it is 4 or less.
 pixels are defined in four ancillary tables with different number entries for edge, lines, dither, and periodic pat-tern preservation. According to the exception, the polar-ity of the computed pixels is reversed. Some examples of exception handling are shown in Fig. 3. The last two are designed to preserve some shading patterns and patterns arising from halftoning. The overall result of the thresh-olded linear sum can be represented as a table of 4096 bits, one for each possible configuration of 12 pixels, and exceptions can be incorporated into it. The resolution reduction is the simple table lookup procedure symmet-rical for all layers and all bit planes, if it is implemented on halftone images. Readers interested in the details of the arithmetic coder employed in JBIG can find more information in [2] and [16]. 3 Performing operations in the compressed domain By compressed-domain processing we mean image ma-nipulation on compressed data without decoding, with minimal decoding only, or by performing some data anal-ysis in the course of decoding. In image processing, there are algorithms that find patterns of pixels in adjacent rows of the image raster to get some useful informa-tion for feature extraction. If the patterns can match the JBIG coding template, there comes a synergy. We can decode the image and execute the algorithm con-currently instead of running the algorithm after the im-age is decoded. This means sharing resources and avoid-ing repeated operations, hence saving storage space and speedup computation.
 are lost, yet some are preserved. Some results are even easier to obtain through a low-resolution image. In fact, there are algorithms that first introduce resolution re-duction as a type of preprocessing. Recall that the JBIG compression algorithm supports a progressive scheme that offers  X  X ree X  low-resolution images. With the JBIG resolution reduction scheme, one can expect that the re-sulting lowest-resolution image suitable for image anal-ysis will be of  X  X ood X  quality.
 processing is to be evaluated in terms of fidelity of data for performing the end task. Therefore, we have designed two experimental processing pipelines  X  one for docu-ment segmentation and the other for the form dropout  X  and we test the performance of each. 4 Global layout analysis Document segmentation is performed for obtaining re-gions of text, graphics, or halftone pictures. Segmen-tation can be performed in different ways. Physical or structural analysis yields the document layout by group-ing components such as separated lines or paragraphs into contiguous blocks. Functional analysis performs la-beling of the blocks to assign a functional property such as title, paragraph, equation, footnote, graphic, etc. Such a detailed analysis is limited to commercial document systems. Traditionally, there are two philosophies for physical segmentation: top-down and bottom-up. The adherents of the former strategy find large components first and then smaller ones. For example, a page is di-vided into one or more column blocks of text. Then, each column is divided into paragraphs, each paragraph into text lines, and so on. Techniques that employ bottom-up analysis detect the connected components and merge them into characters, then words, then text lines, and so on. Several methods based on these two philosophies are reported. For example, top-down analysis often uses variants of the run-length smoothing algorithm based on the  X  X mearing X  of the text by joining characters into blobs[17].
 bottom-up analyses, which employ the features of the connected components as well as generic properties of documents [18 X 20]. In general, bottom-up techniques are computationally expensive because of the large number of components in the standard documents, typically in the range of 1000 X 5000 per page.
 take advantage of both presented philosophies. For ex-ample, Kida et al. [21] applied the top-down technique based on the horizontal and vertical profiles to obtain a general view of page layout, and then completed seg-mentation with the help of connectivity analysis. adopted. Top-down analysis yields a structural layout, and the subsequent bottom-up analysis produces the connected components, whose features are used for clas-sification of structural blocks as either textual or non-textual ones. We benefit from using the coding logic to eliminate recurrent operations, reduced number of com-ponents, and the denoising effect of JBIG base-layer im-ages.
 JBIG lowest-resolution image, horizontal smearing is performed, and the connected components database (CCDB) is built in a form suitable for further analy-sis. Then, vertical smearing is performed, and structural blocks are obtained by analyzing the continuities of the black runs in the adjacent rows and columns. A two-means classifier separates two classes of blocks based on the features of the connected components residing in the blocks. Finally, the blocks are labeled as text or nontext, respectively. 4.1 Horizontal smearing The algorithm starts by decoding the JBIG file into the lowest-resolution layer. While restoring the layer, the al-gorithm analyzes the sequence of coding context used to decode incoming pixels. The automaton shown in Fig. 4 detects a white run of a predefined length. Each time it detects a white run that is longer than T , it changes the corresponding portion of the black image into white. That is, we smear the binary image by filling in the pix-els between any two black pixels that are less than a certain threshold distance T apart. We assume T&gt; 3 in order to match the JBIG context, which is reasonable and practical since two components that are three pixels apart or less should belong to the same region. In Fig. 6, a shaded box in a context displays a black pixel while an empty box indicates a white one;  X  X  X  means  X  X on X  X  care X  and  X   X   X  denotes position of the decoded pixel. run that is longer than three has been encountered. Once a context with a text pattern like k appears, which indi-cates the beginning of a white run that is longer than three pixels, the machine moves to state B and out-puts  X 1 X  to have the X coordinate of the current pixel recorded. Otherwise, it remains in state A. In state B, if a context with a pattern like m appears, which indicates the end of a white run that is longer than 3, the machine moves to state A and outputs  X 1 X  to have the Y coordi-nate of the current pixel recorded. Otherwise, it remains in state B. Based on the values of X and Y , the length of the run and coordinates of its start and endpoints can be computed easily.
 program the automaton. Each table contains 1024 en-tries, each of which consists of two bits: one for the state assignment and the other for the output value. The im-plementation of the above machine is particularly easy. Only a small additional memory is needed.
 image of ITU document 5 is displayed. 4.2 Connected components detection The connected-component-detection and labeling tech-niques have different implementations. Recently, fast techniques have been introduced. Among them, the propagation-type labeling based on the block sorting of runs, introduced by Shima et al. [5], offers high compu-tational speed.
 JBIG base layer and its preprocessing allows only for propagation-type labeling for runs. An image is scanned line by line from top to bottom, and the runs pertinent to the same component receive the labels of their con-nected counterparts from the previous lines. Our algo-rithm finds the patterns of pixels in two adjacent rows of the raster image. The JBIG coding templates are ex-ploited to match the patterns, which exhibit the connec-tion between pixels in two adjacent rows. The composite labels (strings) are assigned to the black runs. The string starts with a unique label generated for each black run and contains the label(s) of the black runs connected to it in the previous line. The example in Fig. 6 explains the labeling procedure, where rectangles are the black runs and the strings of numbers are their labels. needed to assign one unique label to each run pertinent to the component, for example one of a minimum num-ber. In the example shown in Fig. 6, label  X 1 X  will be assigned to each black run of the component. To form a database suitable for further analysis, we need, while detecting components, to maintain a record of the end-points of the black runs and their labels. Some geomet-rical properties can then be calculated and used for clas-sification.
 laps and touches of a black run in the current and previ-ous lines and assigning a unique label of the black runs in the previous line to its counterpart in the current scanning line if a connection is detected. To implement the above-mentioned steps while decoding, we construct a table with 1024 entries that corresponds to the 1024 JBIG coding contexts. Each entry is assigned four bits named S, F, R, and A. These bits are interpreted for ex-ecution as follows :S X  X o record the leftmost coordinate of the black run in the decoding line and assign a new label; F  X  to record the rightmost coordinate of the black run in the decoding line ;R X  X o read out a unique label (L) of the black run in the previous run; A  X  to assign the label (L) to the record of the black run in the current line.
 of JBIG coding contexts. A bit value of 1 invokes the corresponding operation; an  X  x  X  in the context denotes a bit value of 0 or 1, and  X   X   X  denotes the position of the decoded pixel. The operations are implemented with respect to the Y coordinate of the currently decoded pixel, i.e., S and R operations get the coordinate Y  X 2, F the coordinate Y  X 3. Operations are implemented in the listed order. In fact, Fig. 7 presents the truth table of the logical functions. That means the operations can be performed using a simple combinational logic if the hardware implementation is regarded. For software re-alization, one can appreciate what instruction is to be performed in response to the control (flag) bit. and labeling algorithm both on full-and lowest-resolution images. The number of components detected in the eight standard ITU documents of full resolution (f cc) and JBIG layer (l cc) are presented in Table 1. The image quality can be objectively evaluated by the per-centage of components retained in the base-layer images. As could be expected, documents with a larger text font such as handwritten documents or with thicker graphic lines or table frames, show relatively better quality. Time estimates for processing full-resolution (1728  X  2376 pix-els) and fourth-level images show more than 50 times speedup (Table 2). 4.3 Document segmentation Next, we perform vertical smearing of the restored base-layer image. In our experiments, the threshold is chosen to be equal to six pixels. In Fig. 8, we demonstrate the horizontally and vertically smeared version of ITU docu-ment 5. Thereafter, an  X  X R X  logical operation is applied to vertically and horizontally smeared images to extract macro components corresponding to layout segments. up fashion for assigning a unique label for all runs be-longing to the same component, and geometrical prop-erties of components such as width, height, and area are calculated. In our experiments, the logarithm of the ratio of the normalized widths to the page area and the nor-malized height are used as classification features. The segments resulting from the smearing and the features of the connected components residing in the segments are passed on to the two-means classifier. Two classes differentiated by the classifier are to be labeled further by their functional content. If the ratio of the variance to the mean value of the area is greater than three, then the block represents text; otherwise, it will be considered as a group of consolidated nontextual blocks. Figure 10a X  h illustrates the classification results for eight ITU test images whose JBIG images are provided in Fig. 9a X  X . Blocks are merged, and the results are presented in con-tinuous rectangular areas.
 it is of limited accuracy. For example, a logotype at the top of Fig. 9a is classified as a text block. The table in Fig. 9c is labeled as a nontext region. Its borders appear merged with the characters due both to the resolution reduction and the smearing. In the same way, the cap-tions of Figs. 2, 3, and 4 and the symbols/characters inside the graphical blocks in ITU document 5 (Fig. 9e) run together with the graphics. These drawbacks em-anate not only from the resolution reduction but also from the settings for the smearing threshold, and they are common for the top-down techniques.
 all document analyses are provided for full-resolution (f anal) and the JBIG base-layer processing (l anal) that demonstrates approximately 50 times speedup. 5 Form processing Among the many tasks associated with form processing, form type identification and form dropout are the most frequently performed operations in automated form pro-cessing systems. Applications include extraction of text data for OCR, efficient coding using text compressors, database retrieval, and many others. In this section we perform form type identification and form dropout. ber of horizontal and vertical lines that form rectangular boxes, so-called cells, intended for filling with handwrit-ten or typed text. Lines and their properties are used widely for form type identification in many systems. For example, in [22], authors use length, width, and posi-tion of horizontal and vertical lines. In [23], the block adjacency graph (BAG) is employed for finding frame lines and separating them from the text. Line crossings are used in [24]. In [25], authors obtain form signatures from the intersection of horizontal and vertical lines and use them for form type retrieval. In this section, we first analyze how connected components can be used to ob-tain form or text data and then design a system for form type identification and form dropout using the Hough transform.
 created as described in Sect. 4.2, we are able to differen-tiate between black runs belonging to the form lines and the character strokes. Three parameters of the connected components are used  X  component widths, heights, and run lengths. Figure 11b demonstrates the result of such filtering of the third layer of the form image in Fig. 11a originally scanned at 200 dpi. The speedup obtained due to the base-layer image processing is of a factor of 7.8 for the given example. However, there is a problem when handwritten characters touch or cross the form lines; otherwise, the procedure performs equally well as on the original image. The image retains 67% (186 out of 276 components of the original image). Further resolution re-duction, however, leads to up to 71% degradation. Under image distortions caused by occasional document rota-tion during scanning or lowering of the resolution, images are no longer suitable for the above-described process-ing, particularly because form lines appear broken. The Hough transform [26] can be instrumental for line de-tection under the mentioned conditions. Since it is time consuming, the processing of lower-resolution images is promising for both time saving and higher performance. 5.1 Hough transform of JBIG base-layer images One can analytically describe a line segment using a parametric notion, i.e.,  X  = x cos  X  + y sin  X , (1) where  X  is the length of a normal from the origin to the line and  X  is the orientation of  X  with respect to the X axis. Viewed in the parameter space, collinear lines be-come readily apparent as they yield curves that intersect at a common ( r,  X  ) point. The transform is implemented by quantizing the Hough parameter space into finite in-tervals, so-called accumulator cells that form the accu-mulator array. Each image pixel at ( x, y ) is transformed into a discretized (  X ,  X  ) curve, and the accumulator cells along this curve are incremented. Resulting peaks in the accumulator array represent a straight line encountered in the image.
 age is taken as an origin of the coordinate system. For M  X  N images, the Hough space (  X ,  X  ) is constructed with  X  spanning from 0  X  to 180  X  , and an increment of 1  X  in  X  .  X  is spanned from  X   X  max to  X  max , where  X  max = ( M 2 + N 2 ). The minimum increment in  X  is also taken to be 1, and each pixel in the raster image with coordinates ( x, y ) is mapped into a parametric curve by Eq. 1. By that we can represent lines in the scope of 0  X   X  360  X  . This is due to the fact that the lower part of the Hough space corresponds to negative  X  , which means that the  X  value of the related line is  X  + 180  X  . Making use of this symmetry, we can decrease the dimension of the Hough space.
 closed in rectangular boxes formed by form lines such that the width/height of a box is always longer than that formed by the text. This means that in the Hough space, a cell corresponding to relatively long horizontal/vertical form lines has a higher magnitude than that formed by text lines in the image space.
 an accumulator cell of size  X  X  X  X  X  at a specific location defined by (  X ,  X  ). The value of the accumulator cell re-flects the line length, i.e., the longer the line in the im-age, the higher the value of its corresponding cell in the accumulator array. Thus, a local maximum in the pa-rameter space represents a line in the image. Both form lines and text strokes are detectable by the Hough trans-form. However, the number of votes for characters is rel-atively smaller. A proper threshold allows for removing the character strokes. The accumulator cell is retained if it is greater than the threshold; otherwise it is zeroed. The threshold T is set depending on the maxima A max encountered in the accumulator array, i.e., T = k  X  A max With a large k , only major lines are retained. Further-more, we use 0 . 4  X  A max for vertical and 0 . 6  X  A max izontal lines. This is done because lines formed by text in the vertical direction are weaker than those formed by horizontal text lines.
 ing procedure. The locations of the cells at (  X ,  X  ) with nonzero values are calculated and recorded in the loca-tion list. Each black pixel is obtained for the fixed  X  with  X  s found in the location list. If the recalculated  X  matches its counterpart in the list, the pixel is retained; other-wise it is discarded. Since the location list contains only cells of significant values, i.e., corresponding to the form lines; pixels of image components other than lines are eliminated. The thresholding procedure, however, fails to detect short lines. Figure 12a X  X  shows the input form, its parameter space, the thresholded accumulator array, and the lines detected by the procedure, respectively. to the system. Thus an input form can be identified by retrieving the template database. An appropriate form description and matching procedure are necessitated. Ei-ther one must be invariant to geometrical transforma-tions introduced by scanning. Further steps include im-age coregistration, that is, transformation of coordinates of the input form, accessing form fields, extracting or editing text data, or, alternatively, form subtraction. 5.2 Form type identification We form scale-and translation-invariant global feature vectors by using the angle position of the lines. The lines counted within a 360  X  range are taken as feature vectors. To evaluate matching, we compute the distance between the feature vectors of the input form and the database vectors. Suppose the feature vector in the database is L T and that of the input form is L I ; then the absolute difference between them, i.e., D L = 0, indicates for us the similarity of features. However, if the form is rotated, then for two identical forms D L is no longer zero.  X  3  X  , we can resolve the above-mentioned problem by cyclically rotating the query feature vector, that is, ro-tating an input form within the allowed range. The dis-tance can then be calculated as D where j indicates the index of the template in the database and k is the rotation angle. For k correspond-ing to the rotation angle of the input form, the above distance is 0 for a matching template.
 for limited databases, like the one used in this research, and consisting of 35 types. But it is likely that in the large databases there can be two or more forms that produce an identical global descriptor. Therefore, we ex-tend the feature vector by additional invariant features. Let us normalize  X  by its mean value  X  as below:  X  n =  X  i / X , where  X  = there can be a number of neighboring nonzero accumu-lator cells. That indicates a thick line in the image space. In our computations, a value corresponding to the high-est magnitude is taken from the list. This feature, in fact, is the normalized distance from the line to the image ori-gin. In the same way, we also normalize the thresholded values of the local maxima of accumulator cells, i.e., A , by their mean to obtain the normalized line lengths, A n . solute distances, i.e., where N is the total number of lines and subscripts T and I indicate the template and the input forms, respec-tively.
 tification invariant to translation and scaling. The final identification step can be formulated as follows. For in-coming forms, a template is found among forms found to be identical by the global descriptor such that the distances D  X  and D A are minimal. Identification was performed correctly with the test set, extended by an-other 20 forms that are of the same global layout as 5 forms of the original collection but of different scaling and translation parameters.
 ear function of the number of lines. It is negligible if compared to the transform, which becomes particularly extensive for high-resolution images. In Table 4, we give the time estimates of the Hough transform for original-resolution and base-layer images. The time savings are about 16 times for processing the third (base)-layer JBIG images originally scanned at 300 dpi.
 forms, that is, to perform coordinate translation for aligning the input and template forms and subtracting. If we assume that only rotation, translation, and scaling are introduced by the scanner, then a linear conformal transform suffices. We need at least two pairs of coordi-nates of so-called control points to calculate parameters of the transform from the system of two equations: x = s  X  cos(  X  )+ s  X  sin(  X  )+ T x , y = s  X  y  X  sin(  X  )+ s  X  cos(  X  )+ T y , (2) where s is a scaling coefficient,  X  is a rotation parameter, T x and T y are shift parameters in the X and Y directions, respectively, and ( x, y ) and ( x ,y ) are the coordinates of a template and an input form, respectively.
 used. Corner points are found among other cross points, i.e., points that belong to two lines at the same time. Suppose (  X  1 , X  1 ), (  X  2 , X  2 ) are two local maxima in the thresholded accumulator array. For a pixel at ( x i ,y i ), the calculated values of  X  at  X  1 , and  X  2 are If  X   X  1 =  X  1 and  X   X  2 =  X  2 , then ( x i ,y i ) is a cross point. Detection of the corner points is particularly simple be-cause frame lines are characterized by the maximum  X  , and their direction is established based on their  X  value. be performed by Eq. 2. Figure 13 shows a text retained after registering the input form presented in Fig. 12a with its template and dropping the form.
 further image registration and text dropout can be per-formed on higher-resolution images to attain quality of data sufficient for OCR.
 construction of the Hough space discussed in Sect. 5.1 is no longer feasible. Experiments show that text and form lines cannot be reliably distinguished under the men-tioned conditions. Smaller increment values for  X  , such as 0 . 2  X  , would be more appropriate. However, the trans-form becomes prohibitively long. For example, for the image used as an exhibit in this section, time increase is up to 20 s. 6 Discussion and conclusions In this paper, we take advantage of two features of JBIG compression to obtain effective methods for document image analysis  X  context modeling and progressive cod-ing. Apparently, these two features are independent of other aspects of the JBIG compression method. There-fore, for any context-model-based compression method, the implementation of some preprocessing steps and computation of certain features concurrently is possible if the techniques are based on the analysis of some pat-terns of pixels that match the coding context templates. tion is still preserved in the reduced-resolution images such that, with minimal decoding only, feature extrac-tion can be performed in the compressed domain. We have tested different preprocessing algorithms and anal-ysis tasks to evaluate the quality of processing from the application perspective. In particular, document seg-mentation for obtaining the global layout and the form dropout are analyzed. We estimate also the time savings for different stages of the processing pipeline, stressing the most extensive phases. Reduction of time complex-ity varies and attains 50 times, when we perform oper-ations concurrently with the decoding on the reduced datasets. A number of preprocessing algorithms are de-signed, i.e., smearing and connected-component extrac-tion are performed concurrently with decoding and form type identification and dropout are implemented on the low-resolution JBIG images. It is shown that once the lowest layer is preprocessed, further analysis can be car-ried out on the higher-resolution images when the quality of data for OCR is concerned.
 document processing such as possible losses of informa-tion due to resolution reduction. Both the number and the shape of connected components degrade such that the quality of data is insufficient for performing charac-ter recognition. The quality of the third layer of form documents originally scanned at low resolution such as 100 dpi does not allow for performing form type iden-tification and form dropout if documents occasionally undergo geometrical transformations during scanning. resolution processing and have demonstrated an im-plementation of the compressed-domain image process-ing technology for JBIG compression. Apparently, these techniques can be extended to other compression meth-ods with certain features identical to the JBIG algo-rithm. One is, for example, group 3/4 coding; another is wavelet-based compression. Performing some operations in parallel with decoding, particularly for the standard techniques, also offers advantages in hardware realiza-tion, and thus additional time savings.
 References
