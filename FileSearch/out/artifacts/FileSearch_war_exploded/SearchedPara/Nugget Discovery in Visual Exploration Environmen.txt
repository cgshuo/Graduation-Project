 Queries issued by casual users or specialists exploring a data set often point us to important subsets of the data, be it clusters, outliers or other meaningful features. Capturing and caching such queries (henceforth called nuggets) has many potential benefits, including the optimization of the system performance and the search experience of users. Un-fortunately, current visual exploration systems have not yet tapped into this potential resource of identifying and shar-ing important queries. In this paper, we introduce a query consolidation strategy aimed at solving the general prob-lem of isolating important queries from the potentially huge amount of queries submitted. Our solution clusters redun-dant queries caused by exploration-style query specification, which is prevalent in data exploration systems. To measure the similarity between queries, we designed an effective dis-tance metric that incorporates both the query specification and the actual query result. To overcome its high com-plexity when comparing queries with large result sets, we designed an approximation method, which is efficient while still providing excellent accuracy. A user study conducted on multivariate data sets comparing our proposed technique to others in the literature confirms that the proposed distance metric indeed matches well with users X  intuition. As proof of feasibility, we integrated our proposed query consolida-tion solution into the Nugget Management System (NMS) framework [22], which is based on a visual exploration sys-tem XmdvTool. A second user study indicates that both the efficiency and accuracy of users X  visual exploration are enhanced when supported by NMS.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Measurement  X  This work is supported under NSF grant IIS-0414380. Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. Query redundancy, query consolidation, distance metrics.
Identifying the queries in which users are most interested is a critical problem for the information and knowledge man-agement community. Such queries may be good indicators of users X  common interest or reflect experts X  insights about noteworthy features in the data. Generally, figuring out these important queries could benefit both the underlying system and the users. On the one hand, knowing the po-tential queries that users may raise in the future enables caching and optimized execution of incoming queries [10]. By reusing the results of important queries, query caching techniques can save both the query execution time and data transmission cost. On the other hand, being informed about the important queries raised by other users may be helpful for the users. For example, a basketball fan who is searching for sports equipment from an Internet search engine could benefit from knowing that the current most popular query for sports equipment is about:  X  X -MAC 6 X , the latest ver-sion of a series of basketball shoes. Efforts to provide users such help are called collaborative querying [8], which aims to help users to formulate queries by harnessing the collective knowledge of other searchers.

Visualization systems [18, 15, 13] traditionally facilitate knowledge discovery by conveying the information to users via graphical depictions. Developers of such systems have paid, little, if any, attention to the potential gains for visual exploration by identifying and exploiting important queries. To symbolize the value of such important queries, we call problem of isolating  X  X uggets X  from the potentially huge volume of queries submitted during users X  exploration.
Query redundancy arises as users with common interest may submit queries with similar but not identical specifica-tions. Storing either specifications or results of such redun-dant queries may not only cause unnecessary memory or disk consumption, but may also make the search for queries of potential interest become increasingly time-consuming. In particular, query redundancy in visual exploration systems tends to be caused by their exploration-style query specifi-cation mechanism. This is because their users utilize visual interaction techniques, such as sliders or selecting windows, to specify the queries that roughly express their interest. changeably in the remainder of this paper. In such an exploration environment, users are more inter-ested in the query results fed back by the systems, rather than the exact specifications of particular queries, which a re usually required to be typed in SQL-style systems. For ex-ample, a person may search for local restaurants through a life-guide system by manipulating sliders that specify th e distance and price. Then, similar queries, such as  X  X ithin 5 miles, $20 -30 per person X ,  X  X ithin 5.3 miles, $23 -32 per person X  or  X  X ithin 5.5 miles, $18 -31 per person X , may be generated during exploration until the satisfactory resul ts are found. Previous work [20, 19] addressed the query re-dundancy caused by ambiguous language expressions mainly in the web and search engine context. However, little effort has focused on how to solve the redundancy caused by such exploration-style query specification mechanisms, which i s a general problem for information exploration systems and particularly common in visual exploration systems.
To solve this problem, we propose to consolidate redun-dant queries by combining queries based on their similarity . Our query consolidation solution targets range queries ove r multi-dimensional data, a major query type in current mul-tivariate visualization systems [18, 11]. The technical ch al-lenge we address in this paper is measuring the similarities between range queries in multivariate environments.
In our solution, we compare two queries by first compar-ing their query specifications. The queries having similar selective ranges will be assigned a smaller distance betwee n each other in terms of query specification. Considering that even queries with similar query specification may lead to very different query results, we further compare the results of queries. We propose an algorithm, which we call Exact Transformation Measure (ETM), to measure the similarity between the results of two queries. The main idea of ETM is to measure the distance between two datasets by the cost of transforming one into the other. Two queries measured to be similar in both query specification and results will be assigned a small distance in our solution. To overcome the high time complexity of ETM when comparing queries with large result sets, we design an approximation method for it, which we show to offer good performance efficiency while still providing excellent accuracy.

To evaluate the effectiveness of our metric, we have per-formed a user study to compare our proposed measure with other distance metrics. The results of our user study show that our metric outperforms other distance metrics in terms of matching with users X  intuition. In most cases, the dis-tances computed by our proposed metrics are identical or very close to those provided by the users  X  while other tech-niques from the literature tend to fair worse.

As further proof of feasibility, we have incorporated our query consolidation solution into the Nugget Management System (NMS), a framework for analysis-guided visual ex-ploration. NMS extracts, consolidates, and maintains im-portant queries submitted to visualization systems. NMS is currently implemented within XmdvTool [18], a freeware multivariate visualization system. User studies were per-formed to compare the users X  efficiency and accuracy of fin-ishing tasks on real datasets, with and without the help of NMS. Our user studies confirmed the effectiveness of NMS and thus also our query consolidation solution. The over-all workflow of NMS is introduced in a recent paper [22], while in this work we concentrate on the details of query consolidation, which is one of the key components of NMS.
The main contributions of this work are: 1) We designed a query consolidation solution that effectively reduces the potential redundancy among important queries collected. 2) We developed distance metric (QD + ETM) to capture the distance between two nuggets. ETM can be used as a standard similarity metrics to compare the subsets of multi -dimensional datasets. 3) We performed user study to com-pare different nugget distance metrics. The results of the user study indicated that (QD + ETM) works best in terms of matching with users intuition. 4) We found the Coupon Collection (CC) algorithm to be a good approximation for Hungarian Assignment (HA) method, when applied to im-plement ETM. 5) We integrated our query consolidation solution into the framework of NMS, and our user study demonstrates that NMS is able to enhance both the effi-ciency and accuracy of visual exploration.

The remainder of this paper is organized as follows: Sec-tion 2 gives the preliminaries. Section 3 introduces the tec h-niques used in query consolidation. Our proposed distance metric between two queries is discussed in Section 4. In Sec-tion 5, we describe a user study comparing different distance metrics. The approximation algorithm to HA is discussed in section 6. We introduce the application of query consolida-tion for exploration support in Section 7. The related work and our final conclusion appear in Section 8 and 9.
Query specification, one of the most important interac-tion techniques for information management systems, is als o being widely used in visualization systems to let users lo-cate and isolate information of interest. Typical querying techniques used in visualization systems include brushing , filtering and dynamic querying [1]. In this work, we con-centrate on multivariate data and thus on queries that im-pose multi-dimensional constraints on such data (e,g., mul ti-dimensional range queries). This query type is based on the brushing technique [11], a primary interaction technique f or multivariate visualization systems.
 A specific query Q in our case can be defined as: Q = Select D.A 0 , D.A 1 , ..., D.A n From D Where D.A 0 = [ A A .b h ] , D.A 1 = [ A 1 .b l : A 1 .b h ] , ..., D.A n = [ A { D.A 0 , D.A 1 , ..., D.A n } X  attributes of D , A x .b l are the lower and the upper bound of the query ranges on attribute A x , [ A x .b l : A x .b h ] means  X  X rom A x .b
This query type is independent from the display methods in visualization systems, such as Parallel Coordinates, Sc at-terplots and Glyphs [18]. Without loss of generality, we use Parallel Coordinates, which is a widely used display method for multivariate data, to demonstrate our queries in this pa -per. In Parallel Coordinates, each dimension corresponds t o an axis, and the N axes are organized as uniformly spaced vertical lines. A data element in the N-dimensional space manifests itself as a connected set of points, one on each axis. Points lying on a common line or plane create readily perceived structures in the image. Figure 1 shows an ex-ample of a four dimensional dataset displayed with Parallel Coordinates. Figure 2 demonstrates a specific range query over this data. Visually a range query appears as a blue band across the axes, which represents the query X  X  selectiv e ranges on each dimension, and the red (highlighted) lines indicate the selected records (result) of the query. Users specify different queries by adjusting the lower and upper bounds of the blue band (selection ranges).
 Figure 1:  X  X ris X  dataset displayed with Parallel Coor-dinates
Generally, the term nugget refers to some valuable infor-mation in the dataset isolated by a query. In this paper, a nugget is defined by a range query Q (see in Section 2.1) over a dataset D as well as the result of this query, dataset Q(D). A more extensive range of nugget types will be considered in our future work.

To extract a nugget, users of NMS can explicitly save a given query and label it by a persistent nugget name. Similar techniques for explicit indication of users interest can al so be found in other visualization systems [7, 13]. However, sinc e visual exploration is generally an intensive process that m ay require continuous user interaction, a non-intrusive nugg et extraction method would clearly be preferable. For this rea -son, NMS also provides the functionality to automatically extract nuggets during a user X  X  exploration. We use  X  X is-iting time X  [4] as the main indicator of a user X  X  interest. Specifically, a nugget is extracted if a user spends a long time  X  X isiting X  (querying over and looking at) a subpart of the dataset. For example, if a subpart isolated by a query shown in Figure 2 has received a long attention time from a single user or repeatedly visited by one or more users, NMS will conclude that it is a potential nugget.

However, nugget extraction, as the initial effort to identif y the important queries, could cause the problem of  X  X ugget redundancy X . Its solution, as the topic of this work, will be discussed in Sections 3, 4 and 5.
The main reason that causes  X  X ugget redundancy X  is that in visual exploration, users utilize visual interaction te ch-niques, such as sliders or selecting windows, to specify the queries while observing the impact on the linked data dis-play, rather than by explicitly typing exact queries as typi -cally done in SQL query systems. By using these exploration-style query specification mechanisms, many similar but not identical queries may be generated even when users are query -ing over the same features in the dataset. Another cause for nugget redundancy is the automatic nugget extraction in NMS. During visual exploration, a user may continuously yet only slightly adjust the query specification. Extractin g all of these similar queries as different nuggets will lead to a nugget pool with many redundant nuggets. Figure 3 shows an example of three similar nuggets, which actually capture the same pattern in the dataset.

Figure 3: Clustering three similar nuggets
Now we introduce our solution of nugget consolidation, which keeps the nugget pool to a modest size yet main-tains high representativeness. Several different techniqu es, such as sampling, filtering and clustering achieve this goal . In our solution, we choose clustering, a widely used tech-nique to group similar queries [20, 19], and generate a rep-resentative for each group. This is because clustering tech -niques combine the features of all the group members when constructing a representative, while filtering and samplin g techniques tend to pick a group member as their representa-tive. Since in many cases, it is hard to tell which nugget is surely the most important one in a group (even if the impor-tance of nuggets can be expressed by a certain mechanism in our system, nuggets with similar importance may be very common), constructing a representative which  X  X peaks X  for every nugget in a group makes more sense than just pick-ing one. And since the importance (indicated by  X  X isiting time X ) can be used as the weight in the clustering process, the representative generated will primarily reflect the fea -ture of the dominant (most important) nugget, if any. One example of nugget clustering is shown in Figure 3.
Clustering aims to group objects based on their similari-ties. Thus, we have to develop a suitable distance metric for multi-dimensional nuggets. This metric will be discussed i n Sections 4 and 5. With a proper distance metric, any generic clustering algorithm [23, 9] can be applied to conduct nugge t clustering. Further discussion on the specific clustering a l-gorithm used in our system can be found in [21].
Since nuggets are defined by both queries and their re-sults, naturally nuggets defined by similar queries should have shorter distance between each other than those defined by rather different queries. Thus we face the problem of how to quantify the similarity of queries. Fortunately, thi s problem has been studied in previous work [19, 20]. They use following major principle (summarized in Formula 1) to measure the query similarity (QS).
 where QS(A,B) represents the query similarity between nuggets A and B, and QA and QB are the qualifiers of these two queries. We adopt this basic idea as a starting point for the design of our similarity measure. However, several issu es have to be refined. First, we focus our attention on metrics for query similarity on a single dimension. Two main types of domains are considered as discussed below.
A discrete domain composed of nominal values is easy to handle. Because of the discrete property, a direct use of Formula 1 solves the problem. For example, given two queries over the nominal domain, QA: select * from X where X.origin = { Japan, US, Germany } , QB: select * from X where X.origin = { Japan, US, Italy } , we just need to count the number of elements that fall into the intersection and the union of these two sets and then we get | QA  X  QB | = | Japan, US | = 2, | QA  X  QB | = | Japan, US, Germany, Italy | =4, and thus QS ( A, B ) = 2 / 4 = 0 . 5. This strategy of count-ing elements can also be used in numeric discrete domains.
Intuitively, a straightforward variant of the previous  X  X o unt method X  can also be used for continuous domains. The intersection and union of two range queries are no longer expressed by a count of the elements, but rather by the  X  X ength X  of overlap and total coverage. For example, given QA: select * from X where X.height =[5.25:5.85], QB: se-lect * from X where X.height=[5.45:6.15], then we have QA  X  QB = 5 . 85  X  5 . 45 = 0 . 40 , QA  X  QB = 6 . 15  X  5 . 25 = 0 . 9, QS=0.4/0.9=0.44.
Although the major principle of Formula 1 still holds for continuous domains, a more careful consideration regardin g the continuity of the domain may be needed. A problem arises, for example, in a domain of size from 0 to 1000, if we decide that two range queries over [1.00:2.00] and [1.50:2. 50] respectively have some similarities, should we assert that two queries over [1.00:2.00] and [2.00:2.50] are totally di s-similar just because they do not happen to overlap each other? An example will illustrate this concern better.
As shown in Figures 4 and 5, queries 1 and 2 on the left most dimension  X  X eight X  are [3051.73:3318.68] and [3327.0 2: 3527.23] respectively. We note that even though they do not overlap, visually the nuggets defined by them are quite similar. So, in order for our metric to capture the broader semantics of similarity, we have developed a more general algorithm that handles both types of domains, while still keeping the essence of Formula 1. In this algorithm, the domain will be divided into discrete bins. If some part of a query falls into a bin, we call the bin an  X  X ccupied bin (ob) X  of the query. Finally, we utilize the  X  X ccupied bin count strategy X  (obcs) when comparing two queries.

As shown in Figures 6 and 7, both overlap and non-overlap cases are handled by our new algorithm. In Figure 6, QA  X  QB = | QA.ob  X  QB.ob | = |{ b 2 , b 3 , b 4 , b 5 , b 6 }| QA  X  QS=5/7=7.1. In Figure 7, QA  X  QB = | QA.ob  X  QB.ob | = |{ b 6 }| = 1 , QA  X  QB = | QA.ob  X  QB.ob | = |{ b 6 }| = 1 , QS=1/1=1. In practice, we could set QS to be less than 1 for non-overlap cases, because after all they are not perfec t matches.
 Figure 6: Over-laping case
The  X  X ccupied bin count strategy X  can be used as a uni-form query similarity metric for range queries on a single di -mension. The discrete domain uses each discrete value as its bin, while the continuous domain divides into bins first. We note that the problem of bin partitioning itself is a general topic [14]. Although we use a simple equal-bin-size method to make our examples as understandable as possible, other partitioning methods could equally be plugged in.
In most cases, datasets are multi-dimensional, and so are the queries defining our nuggets. Thus, we have to extend the previous metric defined for a single dimension to now be applicable to multiple dimensions. Different metrics, such as minimum, maximum, Manhattan or Euclidean Distance, could be applied to accomplish such a extension. Moreover, if users could reveal the importance of each dimension, we could use  X  X eighted X  single-dimension distances when com-bining them to form the multi-dimensional distances. In order to best capture the  X  X isual similarity X  of two queries , we pick minimum single-dimensional query similarity among all the dimensions of two multi-dimensional queries to rep-resent the query similarity between them in our work. A more detail discussion on this choice can be found in [21].
Finally, when we X  X e successfully acquired normalized quer y similarities (between 0-1), we can easily calculate the que ry distances (QD) as shown in Formula 2.

QD ( A, B ) = 1  X  QS ( A, B ) (2)
Nuggets are not only characterized by their queries (pro-file), but also by the results of the queries obtained when applying the queries to a particular dataset (content). As Figure 8: A nugget cap-turing a cluster in the  X  X ris X  dataset shown in Figures 8 and 9, two nuggets may be rather dif-ferent in terms of actual data content, although they are defined by queries with very similar specifications. The for-mer nugget contains a cluster, while the latter is empty. Clearly, we need to enhance the capability of our metrics to capture the similarity between  X  X ontents X  of the nuggets . Now, the problem we must solve is actually a general data analysis problem. That is how to measure the distance be-tween two subsets of a multi-dimensional dataset. Previous works to tackle such problems [2, 6, 12, 5] can be generally classified into two main categories, statistical and transf orm-cost approaches. Below, we explain why we chose the latter, and then introduce a proposed algorithm extending a basic transform cost algorithm.
Since traditional statistical methods, such as average and deviation, cannot fully capture the characteristic differe nce between two subsets, a more sophisticated method based on histograms, Histogram Difference Measures (HDM), has been developed. HDM is used to measure data abstrac-tion quality [6] and for approximate query processing of databases [2]. It compares the histograms of two sets of data, meaning the distributions of data points.

However, for multi-dimensional histograms, the number of bins grows exponentially when the number of dimen-sions increases. Thus the complexity can easily reach an unaffordable level even with a modest number of bins and dimensions. For example, if we have 10 dimensions and isons. As a much cheaper alternative, the integration of single-dimensional histograms first compares histograms o n each dimension separately and then integrates the results into a normalized result. But such integration cannot truly reflect the distribution of data points in many cases. For example, dataset A { a 1( length = 1 , width 1 =) , a 2( length = 10 , width = 10) } and dataset B { b 1( length = 1 , width = 10) , b 2( length = 10 , width = 1) } ( Each a i is a tuple in A, and length and width are two dimensions of A) will be mea-sured to be exactly the same by this method, since they have the same distribution on each individual dimension. In ac-tuality, these two datasets have very different data records .
As a general notion, Transform Cost methods have been shown to be effective in a wide range of different areas, such as  X  X dit Distance X  in string matching [12], and  X  X IFF X  in change detection to HTML and XML files [5]. In the Trans-form Cost Approaches, distance between two objects is ex-pressed as the minimum cost of transforming one object to another. A well known algorithm that relies on Transform Cost is the Nearest Neighbor Measure (NNM). When com-paring two datasets, NNM aims to move each data point (record) in one set to its nearest neighbor in the other set. It then calculates the accumulative distance that all the data points have moved. Generally, it is more precise than the Statistical Approach, because it deals with each data point rather than only general statistics of datasets. But NNM appears to work better for measuring the quality of representativeness due its n-to-1 mapping strategy, mean-ing a data point in one dataset may be mapped as near-est neighbor for more than one data point in the other set. For example, given two datasets: dataset A { a 1( length = 1) , a 2( length = 100) , a 3( length = 100) , ..., a 99( length = 100) , a 100( length = 100) } and dataset B { b 1( length = 1) , b 2( length = 100) } would be measured to be exactly the same, for each element in set A finds a 0 distance nearest neighbor in set B. In short, NNM is a population-insensitive algorithm. It may lead to bad comparison results in our case, because comparing nuggets with different populations is going to be the norm in our work.
To solve this problem, we propose a new algorithm called the Exact Transformation Measure (ETM). ETM not only overcomes the population-insensitivity but also is more ef -fective in capturing visual similarity of two datasets.
Before discussing the specific algorithm, let us first formu-late the problem: Given dataset O, | O | = m , and datasets A and B, A  X  O, B  X  O, | A | = a, | B | = b, 0  X  a  X  b  X  m, | A  X  B | = l, | B | X  X  A  X  B | = n , tuples in O can be viewed as data points distributed in the value space based on their values in different dimensions. We transform A to be exactly equal to B with minimum cost.
 To solve such a problem, simply moving data points in A to their nearest neighbors in B will fail in many cases, because it is neither globally optimal nor sensitive to pop-ulation. Thus, in order to achieve the transformation with minimum cost, we define the following operations:
By using  X  X ove X  and  X  X dd X , we are guaranteed to always be able to transform A to B, since A always has a smaller or equal sized population to that of B. However, simply re-lying on  X  X ove X  and  X  X dd X  will impose  X  X orced matches X , which may not always lead to the capture of the real dis-tance between two datasets. Figure 10 shows an example of two 2-dimensional datasets where moving and adding are not sufficient to make a cost effective transformation plan.
As shown in Figure 10, given dataset A (shown as white points) and B (shown as black points), by using  X  X ove X  and Figure 10: Trasforming A to B with moving and adding operation only  X  X dd X  only, we have to match some data points in A with data points in B that are far away from them.

In the worst case, the existence of a few  X  X utlier X  data points that do not have a  X  X ear neighbor X  close to them will deprive opportunities for many of other data points to be matched with their real nearest neighbors. To deal with this disadvantage of  X  X orced matches X , we introduce anothe r type of operation, namely,  X  X elete X .
With the help of the  X  X elete X  operation, we no longer need to suffer from  X  X orced matches X . We choose to  X  X elete X , if moving a data point incurs too much global cost. This would allow us to achieve a more cost-effective transformation, as shown in Figure 11.
 Cost Models:
To make an optimal transformation plan with the mini-mum cost, we first study the cost model of each operation.
Cost of moving a data point x to y is equal to the distance between x and y. Here, we adopt the Euclidean Distance (normalized, between 0-1), the most widely used distance measure between objects in a multi-dimensional space.
Since Cost(A[x,y]) is usually an estimated value rather than any physical distance, in most of the Transform Cost works [12, 5], a single COA (cost of adding, which is inde-pendent from the position where the point will be added) is used for each transformation. In this work, we adopt this single COA strategy, while developing a new method of estimation. Considering that a point is directly added to a certain position, the adding process is composed of two steps: generation (generating a point at a random position) and moving (moving the point to a certain position). Thus, COA can be expressed in the following way: a) Generating Cost (GC): It is not hard to see that generating a new element for A would cause a greater  X  X u-tation X  to it, when | A | is small. For instance, when | A | = 0, generating a new data point for A will thoroughly change it, while if | A | =100,000, such a generation can hardly make a noticeable difference. So, we correlate GC with the cardi-nality of A:
MPD: the maximum possible distance between two datasets is equal to 1. We add 1 to the divider to handle the case that a=0. b) Moving Cost (MC): When a new data point is gen-erated for A, it has a random position. Thus, since we can-not truly calculate its distance from the position it should be moved to, we use the average distance between two datasets (centroid to centroid) to estimate the MC needed for moving it to this certain position.

Generally, COA as an estimated value has a positive asso-ciation with the average distance between two datasets and negative association with the cardinality of A. It should be more expensive than most of the Cost(M[x,y]) in a transfor-mation. For normalization, we set the upper limit to 1.
Similar to GC, the change cost of deleting is associated with the cardinality of A and unrelated to the position where the deleted data point lies. The difference here is that we do not need to handle the cases where a=0, because we can delete a data point only if it exists. So, we use Cost Of Delete (COD) to express each Cost [D(x)] in a transformation: Making Transformation Plan:
Having defined the cost models, we now establish our solu-tion for finding an optimal (most cost-effective) transforma -tion plan. The Hungarian Assignment Method [17], which was designed for finding minimum cost bipartite matches, provides a good approach to solve this problem. The al-gorithm takes an n  X  n matrix as input. Each row in the matrix represents a data point in A, and each column rep-resents a data point in B. Then each entry is filled with the distance between the row and the column it belongs to. The algorithm returns a minimum cost match in O ( n 3 ) time.
Let us see a simple example of how it works. Given a 2D (0 , 6) , b 3 (9 , 9) } . We know the domain for both dimensions is (0-10), then the input matrix will be as shown in Figure 12. After a series of matrix manipulations, the output matrix will have exact one  X 0 X  in each row and each column, which stands for the  X  X atch X  of the two data points. For example, in the output matrix below, since there X  X  a  X 0 X  appearing at the entry[ a 1 , b 1 ], data point a1 should be moved to b (Figure 13). Further details of the Hungarian Assignment Method can be found in [17].

To be able to use the Hungarian Assignment Method in all the cases, we still have to tackle two issues: 1) Dummy Points: When two subsets have different numbers of members ( a &lt; b ), an input matrix with distances between points only would not be a square matrix required by the Hungarian Assignment Method as input. To deal with this, we add dummy points to A to produce a square input matrix. The distance between a dummy point d i and any real data point in B should be equal to COA, because when the algorithm eventually makes a match between d i and b i , then this means a new point will be added to A at the same position where b i lies, and thus it costs COA. Figure 12: Input matrix Figure 13: Output ma-2) Incorporating Adding and Deleting: When mov-ing a i to b i is more expensive then deleting it and adding a new data point to where b i lies, we choose the latter  X  X elet-ing + adding X  strategy instead of moving. In the input matrix, if the original value of an entry Cost ( M [ a i , b COA + COD , we use COA+COD to replace it.

Once the output matrix has been produced, by simply summing all the values in the input matrix entries, which match entry locations with a  X 0 X  in its output matrix, and dividing the sum by the population of B, we get the Data Distance ( DD [ X, Y ]) between two nuggets X and Y, where A and B are the datasets contained by them. Finally, we combine the Query Distance ( QD [ X, Y ]) and Data Distance ( DD [ X, Y ]) to present the Nugget Distance ( ND [ X, Y ]) between any pair of nuggets X and Y. ND [ X, Y ] =  X   X  QD [ X, Y ]+  X   X  DD [ X, Y ] (  X  +  X  = 1) (6) Since QD and DD are both normalized (between 0 to 1), ND will be normalized as well.
Now, we discuss several experimental studies conducted to show the effectiveness of the proposed distance metric.
Users: The users engaged in this user study were volun-teers, including WPI students, faculty and staff.
Datasets: Three real datasets were employed in our user study. They are the  X  X ris X  dataset (4 dimensions, 150 record s); the  X  X ars X  dataset (7 dimensions, 392 records); and the  X  X aup X  dataset (14 dimensions, 1161 records).

Nuggets: We designed twenty pairs of nuggets based on the three real datasets we mentioned above. In particular, seven nuggets each were based on  X  X ris X  and  X  X ars X , and the other six were extracted from  X  X aup X . These designed nuggets are good examples of real nuggets which users could find in their explorations, because they covered most of the pattern types we discussed in this work and have varying sizes. The smallest nugget we used in this user study was based on the  X  X ris X  dataset. It had a very small selective range on all the four dimensions and contained only two data records. In contrast, the largest one, which was based on the  X  X aup X  dataset, had large selective ranges on all 14 dimensions and contained 543 data records.
 Questions: The user study consisted of 20 questions. Each required users to indicate a distance between a pair of nuggets. Particularly, all the distances were scaled by the integers from 0 to 10 presented by eleven radio buttons. The suggestive semantics of each integer were also shown under the radio buttons. Specifically, 0-1 means  X  X ery sim-ilar X , 2-4 means  X  X imilar X , 5-7 means  X  X nsimilar X  and 8-10 means  X  X otally different X . The sequence of the questions was randomly arranged, but once it was arranged, it was kept identical for all users.

Experimental Methodology: This user study was car-ried out in a web-based environment. A web page which carried the instructions and all the questions was posted on the Internet. A brief instruction for the user study was give n before the specific questions were presented to the user. Dur -ing the user study, users could answer questions in any order and reanswer any previously answered question. However, they had to answer all the twenty questions before they could submit their answers.

Experimental Strategy: We applied each individual distance metric (one query distance metric QD and three data distances HDM, NNM and ETM) and all the com-bined distance metrics to compute the distances between distances given by the users with those computed by each of the metrics. For this, we introduced a function  X  X if X  to express the difference between the distances given by a metric versus by a user. For each user U , each distance metric M and a certain pair of nuggets N x , N y , we compute we assign different amounts of credit, called accuracy credi t, to each metric based on the difference between the distances given by this metric and by the user. Concretely, we give 3 credits to a metric if Dif=0, 2 credits if Dif = 1, and 1 credit if Dif = 2. If Dif &gt; 2, no credit was given to the metric, meaning that the distance metric fails to match with the user X  X  intuition. For all 20 pairs of nuggets and all 20 users (totally 400 distances given), we calculate the se accumulative credits for each of the metrics. We also use pie charts, which we call  X  X if Distribution Charts X  to show us the exact number and percentage of each  X  X atch category X .
First, we analyze the inter-user agreement on the dis-tances between the 20 pairs of nuggets employed in our eval-uation. This is important, because if there is no agreement shown in user-given distances, comparing our metrics with them will be meaningless. Figure 14 shows the distribu-tion of the user-given distances for the 20 pairs of nuggets. From Figure 14, we observe that for most of the questions, both the ranges (difference between maximum and mini-mum) and standard deviations of the user-given distances are relatively small. For all the 20 questions, the average range and the average standard diviation are 3.4 and 0.67 respectively. Considering that the domain for the distance s range from 0 to 10, we observe that the users X  intuitions about the distances between nuggets are not random or to-tally different, but reach a good agreement. This implies that the if any of metrics could match well with these user-given distances, it can be expected to reasonably mimic the intuition of the general users. original QD + ETM solution, which employs the Hungarian Assignment Method. We use  X  X D + ETM (CC) X  to present another version of our QD + ETM solution (discussed later in this section), which is implemented by an approximation algorithm to Hungarian Assignment Method Figure 14: Distances given by 20 users for 20 pair of nuggets
Figure 15 shows the accumulative credits earned by each metric. Our accuracy credit strategy counts all the  X  X atche s X  Figure 15: Accumulative credits earned by each dis-tance metric for all 400 cases and sums up the Accuracy Credit earned by each distance metric for all the 400 cases. Generally, a metric that matche s well with more users in more questions will earn higher accu-earns much higher accumulative credit than any other met-ric. This indicates that it matches the users X  intuition bes t among all distance metrics.
 Figure 16: Dif distribution of QD, QD+ETM (HA) and QD+ETM (CC)
Figures 16 shows the distribution of  X  X if X  X  for several distance metrics. We observe that QD + ETM (HA) has 128 (32%)  X  X erfect matches X  ( Dif = 0) with users X  rat-ings for the 400 distances. It also has 196 (49%) Dif = 1 matches, 44 (11%) Dif = 2 matches, while only 32 (8%)  X  X on-matches X  ( Dif &gt; 2). It is much better than any other distance metric in terms of more  X  X ood matches X  and less  X  X on-matches X , even compared with the second best one (QD). Based on the comparison results above, we learn that will be discussed later QD + ETM (HA) captures the distances between nuggets best among all the metrics we discussed in this work.
The best quality usually comes at the price of the highest cost. Since the Hungarian Assignment (HA) Method used in ETM has O ( n 3 ) complexity, where n is the number of points that appear in the larger subset but not in the smaller subset, it is not always practical performance-wise when we try to compare nuggets with huge populations. Figure 17 shows the CPU time used by all the distance metrics when measuring distances for the 20 pairs of nuggets we mentioned earlier. We can see that QD+ETM (HA) has the highest cost in terms of maximum, minimum and average CPU time used, which means the metric is best at capturing users X  intuition but worst in terms of time efficiency.
 Figure 17: CPU time by different distance metrics
To address this, we now propose to employ a much cheaper approximation algorithm of HA instead of the full-fledged HA. It is the Coupon Collection (CC) Algorithm, which has O ( nln ( m )) complexity, where n has the same meaning as that in HA and m is the size of the original dataset. By using CC, we do not construct a global optimal transforma-tion plan by conducting complicated matrix operations as we did with HA. Instead, we  X  X ove X  each non-overlapping data point in one nugget to its nearest neighbor in another. Once a data point from one nugget has been moved to a data point in the other nugget, the latter data point is  X  X c-cupied X  and can no longer  X  X ccept X  moving from any other data point. Thus if the nearest neighbor of a data point is  X  X ccupied X , this data point has to be moved to its second nearest neighbor, and so on. This continues until a data point finds an unoccupied neighbor, or the data point has to be  X  X eleted X  in the transformation plan.

To compare the CPU time cost of QD+ETM (CC) with the costs by other distance metrics and also its performance in terms of matching users X  intuition, we use QD+ETM(CC) to measure the distances between the same 20 pairs of nuggets used in the earlier user study.

As shown in Figure 17, from the maximum, minimum, av-erage and also standard deviation of CPU time cost, we learn that QD+ETM(CC) is the second fastest distance metric among all those we have discussed and only slightly slower than the cheapest metric (QD). Here, we also need to point out that, although QD+ETM (CC) saves much CPU time in average case compared with QD+ETM (HA), QD+ETM (CC) is not guaranteed to be faster than QD+ETM (HA) in all cases. This is because the complexity of CC, O ( nln ( m )), is related to the size of the original datasets, but the com-plexity of HA, O ( n 3 ), is only related to the non-overlap pop-ulation in the larger nugget. CC can be slower than HA when m is extremely large, while n is extremely small. So, one could choose to use either of these two metrics based on comparing ln ( m ) and n 2 . If the former wins, we pick QD+ETM (CC), else we pick QD+ETM (HA).

When comes to the performance of QD+ETM (CC) in terms of matching users X  intuition, we found that among all the 20 pairs of nuggets we used in our user study, QD+ETM (CC) gave the exactly same answer with QD+ETM (HA) on 18 pairs. For the remained 2 pairs of nuggets, QD+ETM (CC) versus QD+ETM (HA) has a difference equal to 1 and another has a difference equal to 2.

As shown in Figure 15, the accumulative credits earned by QD+ETM (CC) is very close to those of QD+ETM (HA) and much higher than any other metric. From Figure 16, we observe that QD+ETM has 110 (28%)  X  X erfect matches X  ( Dif = 0) with users X  ratings for the 400 distances. It also has 210 (52%) Dif = 1 matches, 49 (12%) Dif = 2 matches, while only 32 (8%)  X  X on-matches X  ( Dif &gt; 2).
To verify the usefulness of the proposed solution, we have integrated Nugget Consolidation into our NMS framework and then evaluated its effectiveness in aiding users during data exploration. More precisely, we have extended Xmdv-Tool, a freeware multivariate visualization system [18], w ith the services of nugget extraction, consolidation, and main -tenance to provide nugget support during visual data ex-ploration. In our system, nugget consolidation first remove s the redundant nuggets generated during nugget extraction. Then, the  X  X ugget representatives X  produced by nugget con-solidation can be retrieved by the users and fed into the nugget maintenance stage. Nugget maintenance expels the out-of-date nuggets or those extracted by misinterpreting users interest from our system. While we provide auto-mated algorithm for these services, we also offer interfaces that enable users to manipulate each individual nugget. For example, a user can modify the query specification of a nugget or attach her own understanding as annotation to a nugget. By doing so, NMS may form a nugget pool that presents users X  discoveries, interest and expertise. This well-organized nugget pool will be used to guide users X  explo-ration. Figure 18 shows a screen shot of our prototype sys-tem. More details of our prototype NMS system can be found in [22].

To compare users X  efficiency and accuracy when solving tasks with and without the help of NMS, we randomly di-vided 12 users, all WPI students, into 4 groups, 3 users per group. All the 4 groups were asked to finish the same 5 knowledge discovery tasks, which were based on 3 real datasets, while only three groups (group 2, 3, 4) were sup-ported by NMS. All the users were encouraged to finish the tasks as quickly and correctly as possible. Details of the experimental setup and methodology of this user study can be found in [22]. Figure 19 shows the time used by each individual user and also each group.

As shown in Figure 19, groups 2, 3, and 4 (with NMS) spent noticeably less time (around 50 percent) than group 1 (without NMS). Such time savings was due to the second Figure 18: A screen shot from our NMS prototype when looking for clusters hidden in the dataset Figure 19: Comparison of users X  efficiency in differ-ent groups and the third users, given that the first users all worked from scratch. However, once the nuggets were extracted during the exploration by the first users, the exploration processe s of the second and the third users largely benefited from the nugget pool. Thus, we showed that NMS may greatly im-prove users time efficiency when solving knowledge discovery tasks. Our preliminary evaluation also shows that NMS en-hances users X  accuracy of finishing these tasks. More detail s of these user studies to NMS can be found in [22]. [3] introduces a technique for mining a collection of user transactions with an Internet search engine to discover clu s-ters of similar queries and similar URLs. A collaborative query system that helps users with query formulation by finding previously submitted similar queries through min-ing web logs is described in [8]. Similar works include [19, 20]. These efforts that aim to mine important queries from query logs have similar goals to us. However, they focus on keyword-based queries in web searches, which is differ-ent from our work in terms of both the query type and the visual specification context.

Although these studies on keyword-based queries in web searches cannot be directly applied to our context, their efforts to measure the similarity between queries parallel ours. As pointed out by [8], when measuring the simi-larity between two queries, we should not only compare their  X  X erms X , which means the query specifications, but also compare the results of them. We use the same princi-ple to measure the similarity between queries in our work, although the specific methods we designed to conduct com-parisons are different. Previous studies on similar queries [19] also provided us the basic idea of comparing the query specification. We extended this basic idea to handle differ-ent types of domains and to capture the  X  X isual similarity X  between queries in our context.

Our work is also closely related to visualization and inter-action techniques [18, 15, 13], because our NMS framework is designed to facilitate visual exploration. The query typ e we target is based on a common querying mechanism for multivariate visualization systems, called a brush [11]. F i-nally, the framework of NMS has good potential to support visual analytics as introduced in [16, 13].
As the main contribution of this work, we present a query consolidation solution, which consolidates redundant que ries caused by exploration-style query specification, a problem commonly found in visual exploration systems. Our solu-tion clusters queries based on their similarity. To solve th e challenge of measuring the similarity between queries, we have developed a collaborative distance metric (QD+ETM), which compares both specifications and results of two querie s. Our user study conducted on visual queries over real dataset s shows that: 1) When the ETM is implemented with the Hungarian Assignment Method (HA), QD+ETM (HA) agrees best with users X  intuition on distances between nuggets. 2) Coupon Collection (CC) algorithm has been shown to be a good approximation to HA when computing ETM. It has almost the same accuracy as HA, and it costs significantly less (on average 80%) CPU time than HA for the nuggets on three real datasets we tested. 3) Query Distance (QD) only, as a cheap metrics, works well in many cases. When picking the distance metric, QD can always be carried out before conducting any data comparison. If two nuggets have huge query distance then the data comparison is no longer nec-essary, because the two nuggets will surely be dissimilar. I f two nuggets have a small query distance and we aim to form as precise clusters as possible, we have to consider data dis -tance. This choice can be made by comparing ln ( m ) and n 2 . If the former is smaller, we pick QD+ETM (CC), otherwise we pick QD+ETM (HA). Further, we outline our effort in realizing some nugget support by incorporating query con-solidation into a freeware visual exploration system. Our preliminary evaluation indicates that it enhances both the efficiency and accuracy of users X  visual exploration. Acknowledgements: We thank Jing Yang, Zaixian Xie, Qingguang Cui, Charudatta Wad, Do Quyen Nguyen, and other members of the XMDV team at WPI for the develop-ment of XmdvTool, which formed the testbed for this work. We also thank members of DSRG at WPI for listening to talks and providing input on this work. [1] C. Ahlberg and B. Shneiderman. Visual information [2] B. Babcock, S. Chaudhuri, and G. Das. Dynamic [3] D. Beeferman and A. L. Berger. Agglomerative [4] M. Claypool, P. Le, M. Wased, and D. Brown.
 [5] G. Cobena, S. Abiteboul, and A. Marian. Detecting [6] Q. Cui, M. O. Ward, E. A. Rundensteiner, and [7] G. D, Z. M.X, and A. V. Interactive visual synthesis of [8] L. Fu, D. H.-L. Goh, S. S.-B. Foo, and J.-C. Na. [9] S. Guha, R. Rastogi, and K. Shim. Cure: an efficient [10] D. Lee and W. W. Chu. Semantic caching via query [11] A. Martin and M. Ward. High dimensional brushing [12] C.-C. Pan, K.-H. Yang, and T.-L. Lee. Approximate [13] P.E.Keel. Collaborative visual analytics: Inferring [14] D. Scott. On optimal and data-based histograms. In [15] B. Shneiderman. Tree visualization with tree-maps: A [16] J. J. Thomas and K. A. Cook. Illuminating the Path: [17] K. Tranbarger and F. P. Schoenberg. The hungarian [18] M. Ward. Xmdvtool: Integrating multiple methods for [19] J. Wen, J. Nie, and H. Zhang. Clustering user queries [20] J.-R. Wen and H. Zhang. Query clustering in the web [21] D. Yang. Analysis-guided exploration of multivariate [22] D. Yang, E. A. Rundensteiner, and M. O. Ward. [23] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH:
