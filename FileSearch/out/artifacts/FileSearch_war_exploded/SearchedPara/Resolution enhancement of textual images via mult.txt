 ORIGINAL PAPER Rim Walha  X  Fadoua Drira  X  Frank Lebourgeois  X  Christophe Garcia  X  Adel M. Alimi Abstract Resolution enhancement has become a valuable research topic due to the rapidly growing need for high-quality images in various applications. Various resolution enhancement approaches have been successfully applied on natural images. Nevertheless, their direct application to tex-tual images is not efficient enough due to the specificities that distinguish these particular images from natural images. The use of insufficient resolution introduces substantial loss of details which can make a text unreadable by humans and unrecognizable by OCR systems. To address these issues, a sparse coding-based approach is proposed to enhance the resolution of a textual image. Three major contributions are presented in this paper: (1) Multiple coupled dictionaries are learned from a clustered database and selected adaptively for a better reconstruction. (2) An automatic process is devel-oped to collect the training database, which contains writing patterns extracted from high-quality character images. (3) A new local feature descriptor well suited for writing specifici-ties is proposed for the clustering of the training database. The performance of these propositions is evaluated qualita-tively and quantitatively on various types of low-resolution textual images. Significant improvements in visual quality and character recognition rates are achieved using the pro-posed approach, confirmed by a detailed comparative study with state-of-the-art upscaling approaches.
 Keywords Resolution enhancement  X  Sparse coding  X  Histogram of structure tensors  X  Writing pattern  X  Textual image  X  Character recognition 1 Introduction Spatial resolution that refers to pixel density or the number of pixels per unit area defines the quality of an image. In fact, a high-resolution (HR) image means that the image is represented over a fine spatial grid carrying more informa-tion and thus fine details within it. For a low-resolution (LR) image, the spatial grid is relatively coarse and results in a loss of detail. The need for HR images continues to progress in many computer vision applications in order to achieve better performance in pattern recognition and image analysis. How-ever, high-quality textual images are not always available. This is mainly due to the presence of wide collections of LR images in digital archives, the high cost, and even the inher-ent limitations of the hardware technology for early digitiza-tion projects. In addition, poorly resolved images could also be result from a LR scanning process that has already been chosen to satisfy high-speed access, low-bandwidth trans-mission, reduction of storage cost, and even ease of process-ing large volumes. Moreover, LR textual images taken by a smartphone or a tablet camera result mainly from the prolif-eration and extensive use of today X  X  mobile devices.
The use of a digital imaging process with insufficient res-olution considerably affects the quality of textual images [ 18 , 32 , 46 , 66 ].Itpresentsasourceofdegradationsthatharms the legibility of text for the observer, especially when the font size is small and spatial resolution is poor. In addition, even if the observer can read a LR textual image, document analy-sis systems encounter serious problems interpreting this type of text. In fact, low resolution is usually accompanied by a loss in the object X  X  topology, transforming, for example, the character  X  X  X  to the character  X  X  X  as illustrated in Fig. 1 b. Moreover, spaces between adjacent characters present in the HR image are often deleted in the LR image. For instance, the two successive characters  X  X  X  and  X  X  X  in the HR image in Fig. 1 a have been converted by the character  X  X  X  in the LR image, as shown in Fig. 1 b. Considerable distortions occur when scanning a textual image that contains fine structures, why the low quality of degraded characters poses serious challenges to optical character recognition (OCR) systems. In fact, their performance depends critically on the image quality, and most of them have been designed to be applied on clean images scanned with at least 300 dpi. These systems cannot be applied directly to degraded images [ 31 ]. Recent research has attempted to directly identify or recognize LR textual images [ 19 , 42 , 48 ].

The problem of insufficient resolution can be overcome through the use of a resolution enhancement process that is useful in recovering a HR image from one or more observed LR images. This process could resolve some imperfections of hardware devices. Moreover, it offers a better visual-ization and interpretation of these poor-quality images and could also guarantee a better utilization of the high-definition display capabilities. In the literature, two kinds of reso-lution enhancement are distinguished: multi-input single-output (MISO) and single-input single-output (SISO) reso-lution enhancement. The first type requires as input multiple successive LR images that are observed from the same phys-ical scene with sub-pixel motions. The second type of resolu-tion enhancement, known as SISO, reconstructs a HR image from a single LR input image. It is more difficult to resolve this type of task because the available information is not suf-ficient to describe the original scene. The MISO resolution enhancement task, however, is not within the scope of this work. In this paper, we address the SISO resolution enhance-ment task. Interpolation-based approaches cannot add details in the image. Indeed, no pure SISO approach would be able to increase the resolution unless some kind of additional infor-mation is included in the problem.

To solve the SISO task, various approaches have been proposed in the literature. Most of them have been success-fully applied to natural images, and their simple application to textual images is not direct. This can be explained by the substantial difference that distinguishes natural images from textual images [ 17 ]. Textual images are thus a dis-tinct class of images, and specific techniques suited for it should be defined. Various approaches which are pro-posed for the resolution enhancement of textual images include a character-based training step in order to enforce text specificities [ 4 , 7 , 24 ]. Most of them have demonstrated good results. Nevertheless, their performances depend heav-ily on the training samples, and they work best especially when the same size and type font of the input LR image are included in the training set. Resolution enhancement approaches that include a patch-based training step, as opposed to character-based training step, can overcome the previous limit [ 12 , 21 , 50 , 53 , 54 ]. This study aims to exam-ine the resolution enhancement of poorly resolved textual images by using a patch-based training step. In this paper, we investigate the use of sparse coding which has been proven to be an effective solution to various reconstruction tasks and a useful technique for patch-based approaches. The underlying idea of this technique is to represent an image patch as a sparse linear combination of elements from a suitably chosen dictionary. Motivated by the key role of the dictionary in sparse coding, we propose a multiple learned dictionaries-based resolution enhancement approach that is adapted to the specificities of writing patterns and includes two phases: the learning and the reconstruction phases. The first phase concerns the learning of multiple coupled dictionaries from a clustered training LR/HR patch-pair database. To improve the unsupervised clustering of this database, an intelligent clustering method is applied and a new local feature descriptor, referred to as histogram of structure tensors (HoST), is introduced making it pos-sible to capture the local information of an image patch. Via the proposed descriptor, the clustering performance is improved and the learning phase can provide more appro-priate dictionaries representing each cluster. Given multiple learned dictionaries, a sparse coding-based reconstruction phase is designed in order to adaptively select the appro-priate dictionary that is useful for improved recovery of each local patch. The results achieved by the proposed res-olution enhancement approach are evaluated visually and quantitatively on various printed and handwritten LR tex-tual images, and interesting results have been achieved. In this paper, we also investigate the effect of the proposed resolution enhancement approach on character recognition performance.

The rest of this paper is organized as follows: Sect. 2 presents a brief review of related research proposed for the resolution enhancement of a single-input image. Details of the proposed multiple dictionaries-based magnification approach are presented in Sect. 3 . Experiments and com-parative studies with results generated by other existing approaches are provided in Sect. 4 . Finally, conclusions and future perspectives are given in Sect. 5 . 2 Related research Resolution enhancement of a given LR image has been stud-ied for a long time. Various interpolation-based approaches have been proposed in the literature. Non-adaptive interpo-lation methods apply a convolution on the image by using kernel functions such as linear, cubic, or higher functions. These methods are simple and fast, but they introduce arti-facts such as blur and blocking artifacts along edges. A num-ber of adaptive interpolation methods have been suggested in order to reduce the degradations generated by these early methods. For instance, Li and Orchard [ 30 ] developed a new edge-directed interpolation (NEDI), which is based on the local covariance and the geometric duality to estimate the HR covariance from the LR counterpart. Zhang and Wu [ 65 ] introduced an edge-guided upscaling method through direc-tional filtering and data fusion.

Resolution enhancement can be considered as an ill-posed inverse problem. A number of methods, such as those reported by [ 6 , 33 ], have been developed to regularize this problem with different priors. For example, Lukin et al. [ 33 ] proposed using bilateral total variation regularization. Various learning-based resolution enhancement approaches have been developed to model the relationship between the input LR image and the output HR image from a training database. For instance, Sun et al. [ 49 ] proposed the gradi-ent profile prior capturing the local edge structure from the training database, which can maintain the sharpness of the resulting image. Dalley et al. [ 12 ] developed an exemplar-based approach that mapped blocks of the LR image into predefined HR blocks. More recently, learning approaches based on the sparse coding technique have attracted increas-ing interest due to its effectiveness in various reconstruction tasks such as inpainting [ 20 , 51 ], denoising [ 23 , 28 , 36 ], and resolution enhancement [ 52 , 60 , 62 , 63 ]. The underlying idea of this technique is that an image patch could be sparsely represented from an appropriately chosen dictionary. The choice of the dictionary is key for the successful application of the sparse coding-based approaches. For instance, Yang et al. [ 61 ] constructed prototype dictionaries by randomly sampling raw patches from training images. In [ 1 , 29 ], dic-tionary learning algorithms were developed to reduce the complexity of sparse coding under prototype dictionaries. In [ 55 , 60 ], learning methods were developed for training two coupled dictionaries to be applied to the resolution enhance-ment task. Recently, the multiple dictionaries-based upscal-ing approach was introduced. For example, the authors of [ 62 ] and [ 15 ] learned several dictionaries from K -means clus-tered patches. These works have been successfully applied to natural, human face, and synthetic aperture radar images.
Most of the existing resolution enhancement approaches have illustrated their results on natural images. The direct application of these methods on textual images is not trivial. This can be explained by the particular specificities that dis-tinguish textual images from natural ones, explaining why specific approaches suited for textual images were devel-oped. For instance, Namane and Sid-Ahmed [ 39 ] proposed a method that is based on detecting the borders of the character, scalingthecontourandthenmagnifyingbyusingcubicspline interpolation. Hobby and Ho [ 24 ] developed a magnification approach to improve the quality of binary textual images sup-plied from faxes. The flowchart of their method consists in two main processes. First, segmented bitmaps of the same glyph scattered across the input image are grouped into the same cluster. Second, the bitmaps are averaged to generate a binary HR model for each glyph, and then, all occurrences of that glyph are substituted with the new bitmap.
In[ 50 ],aniterativeupscalingmethodwasdesignedspecif-ically for textual images. For each LR block of pixels, this method tries to solve a nonlinear optimization prob-lem that minimizes the bimodal, smoothness, and average (BSA) scoring function. Kim [ 26 ] proposed a learning-based approach for the magnification of binary textual images. He began by collecting a training database of LR/HR image patch pairs in order to design a binary windowed zoom operator via the K-nearest neighbor technique. This oper-ator defined an image transformation function to estimate the color of an output pixel from the color of its neighbors in the input image.

Dalley et al. [ 12 ] adopted a full Bayesian formulation to predict blocks of the LR grayscale image from HR blocks existing in a training database. Fan et al. [ 21 ] proposed a res-olution enhancement approach based on the neighbor embed-ding principle [ 10 ] assuming that a HR image can be gen-erated from a set of optimally weighted image patch pairs. These latter are retrieved from a large training database by the K-nearest neighbor search. Luong and Philips [ 34 , 35 ] proposed a resolution enhancement approach based on the presence of repeating characters in the input image to recon-struct each LR character. This approach started with char-acter segmentation, which is usually not easy in LR textual images. More recently, Yan et al. [ 59 ] presented a learning-based approach by adopting a Bayesian formulation via the MRF. Caner and Haritaoglu [ 9 ] developed an approach based on multiple models, called shape-DNA models that consist of statistics on shape information in image patches and map-pings between HR training image patches and their LR ver-sions. Each model corresponds to a specific resolution level, font type, and size. Recently, Walha et al. [ 52 ] developed a sparse coding-based resolution enhancement method that uses only two coupled LR/HR dictionaries. The sparse rep-resentation of a LR image patch from the first dictionary is applied to generate a HR image patch from the second dic-tionary. 3 The proposed sparse coding-based resolution enhancement approach In this section, we propose a sparse coding-based resolution enhancement approach that learns multiple coupled dictio-naries and reconstructs each local patch adaptively. More precisely, it includes two phases: the learning phase and the reconstruction phase. The key idea of the first phase is to find more appropriate dictionaries adapted to the particular specificities of characters and learned from a well-clustered training LR/HR patch-pair database. To improve the unsu-pervised clustering of this database, we propose to apply an intelligent clustering method and to use a new local feature descriptor capturing the local information of a given patch. The second phase guides the reconstruction in selecting the appropriate dictionary to better recover each local patch. An overview of the proposed approach is depicted in Fig. 2 , and details are presented in the following subsections. 3.1 Learning phase 3.1.1 Database collection The choice of the training database is a key for success-ful learning-based approaches. In this paper, we propose to collect a database that describes the specificity of writing and will allow us to enhance the spatial resolution of textual images. We begin by collecting several high-quality char-acter images. In fact, these images are generated automati-cally using a freely available library called FreeType. 1 graphic library provides a portable and efficient engine for renderingfonts.Viathislibrary,acollectionofapproximately 2,480 high-quality printed character images is produced by discretizing vector fonts (TrueType fonts). Such character images cover a large variety of sizes, styles (italic, non-italic) and fonts (serif, sans serif) currently used in textual docu-ments, signs, labels, bills, etc.
 After that, several patch images are selected from each high-quality character image. We note that we are interested only in patches localized along character edges because the shape of edges is a highly valuable reference to describe characters. Consequently, a minimum number of black pixels must be present in each extracted patch image. In addition, a thresh-old that corresponds to the maximum level of intersection allowed between patch images is used (Fig. 3 ). This leads to thecollectionofagenericHRpatchimagetrainingsetinclud-ing numerous writing patterns that differ greatly due to their shapes, sizes, orientations, and positions in the patch images. Figure 4 shows samples of the HR training set. LR patch images that are generated by blurring and down-sampling the corresponding high-quality patch images constitute the LR training set.

After collecting the training database of HR/LR patch-pair images, we turn to the clustering of these patch images. In addition to the choice of the appropriate clustering technique and its parameters setting, it is very important to involve effective features that allow patterns to be clearly distin-guished and thus improve the clustering performance. The emergence of pattern clustering in various studies on tex-tual images introduces the need for efficient features that focus on the writing specificities. For this reason, this paper investigates the feature extraction issue for the clustering of writing patterns and we propose a new structure tensor-based local feature descriptor, referred to HoST. In the following, we describe the proposed descriptor used for improving the clustering of the training database. 3.1.2 Histogram of structure tensors: HoST The proposed HoST dsecriptor is based on the structure ten-sor, a highly useful concept in differential geometry because it can provide substantial information in a local neighbor-hood. This concept has been used for estimating the cur-vature in oriented patterns, detecting complex symmetries [ 58 ], restoring document images by diffusion [ 16 , 17 , 57 ], and detecting local features such as the Harris corner detector [ 27 ]. In this paper, we propose a new local feature descriptor based on the structure tensor concept.

Given an image I , the first-order derivative structure ten-sor at a certain pixel is defined by the product of gradients (Eq. 1 )[ 14 ].
 T = X  I  X  X  I t (1) where  X  is the first-order derivative in the gradient field:  X 
I = In practice, gradients need to be calculated on a slightly smoothed image to avoid the singular points of the input image [ 2 ]. Therefore, a Gaussian convolution kernel G  X  is applied to image I . In addition, Weickert [ 57 ] suggested using a second Gaussian convolution kernel G  X  in order to enhance the local consistency of neighboring tensors. This leads to the generation of the smoothed structure tensor T T  X  = G  X   X   X  ( G  X   X  I )  X  ( G  X   X  I ) t (3) In differential geometry, the structure tensor T  X   X  is defined as a2  X  2 symmetric matrix: T where J 12 = J 21 . The structure tensor T  X   X  could be decom-posed in an orthonormal basis formed by two orthogonal eigenvectors  X  + and  X   X  associated, respectively, with two positive eigenvalues  X  + and  X   X  : T  X  =  X   X   X   X   X   X  t  X  +  X  +  X   X  +  X  t + (5) where  X  The eigenvectors  X  + and  X   X  indicate the local orientations of the variations in the image, while the eigenvalues  X  + and  X   X  that are formally given by Eq. ( 6 ) measure the associated derivative energy (i.e., the magnitude of these variations). Geometrically, the structure tensor T  X   X  represents an ellipse whose radii are  X  + and  X   X  , and its principal axis  X  + forms an angle  X  with the horizontal axis. The formula for calculating the angle  X  is written as follows:  X  = 1 The eigenvalue analysis determines the shape of the structure tensor (isotropic or anisotropic) in the principal directions. The geometric interpretation of the structure tensor T  X   X  is depicted in Fig. 5 .

The basic idea of the HoST descriptor is that a local pattern could be described by the distribution of the structure tensor orientation and shape. The structure tensor at each pixel is characterized by two useful parameters :  X  + and  X  + . In fact, the direction of  X  + indicates the most prominent orienta-tion in a local neighborhood. Furthermore, the eigenvalue  X  Hereafter, we use ( X  + ) i , (  X  + ) i and ( X  ) i as references to the principal axis, the largest eigenvalue and the angle with the horizontal axis of the structure tensor ( T  X   X  ) i at a given pixel i , respectively.

Theorientationfieldisveryusefulappearanceinformation because it describes the changing direction within a spatial region. To take advantage of this information, we suggest gathering the structure tensor orientations of the pixels in a spatial region via a histogram representation. To incorporate the derivative energy in the most prominent orientation, we also suggest calculating for each structure tensor orientation ( X  ) (  X  + ) i divided by the sum of the two eigenvalues. This allows us to capture the dominant orientations in a local region tak-ing into account the local structure shape.

Like [ 11 ], votes are weighted by using bilinear interpola-tion to reduce the aliasing effect. More precisely, for each structure tensor orientation ( X  ) i , the two nearest orienta-by coefficients w i , j inversely proportional to the distance between the given orientation ( X  ) i and its neighboring bin tion ( X  ) i of a given pixel i contributes a weighted vote that will be accumulated into the corresponding orientation bins b j of the HoST histogram (Eq. 8 ).
 HoST [ b j ]= where V
The implementation of the proposed HoST descriptor is summarized by the following steps: Step 1 Compute the matrix of the structure tensor ( T  X   X  Step 2 Estimate the eigenvector ( X  + ) i and the eigenvalue Step 3 Calculate the angle ( X  ) i corresponding to the most Step 4 Compute the weights w i , j based on the distance Step 5 Accumulate the weighted vote into the corresponding
For a closer look at HoST design, Fig. 6 shows a few examples of enlarged patch images extracted from the train-ing database and on which structure tensors are drawn in the form of ellipses. In addition, Fig. 7 illustrates the corre-sponding HoST representation of each patch image shown in Fig. 6 . According to these two figures, it is clear that writ-ing patterns are accurately described by the orientations and shapes of the structure tensors which are the key concept of the proposed HoST descriptor.

In conclusion, the proposed HoST descriptor has two major advantages. On one hand, it captures the dominant ori-entations in a local spatial region taking into account the local shape of the edge structure. In fact, it is based on the struc-ture tensor that represents a very valuable concept for char-acterizing the local shape. On the other hand, the use of the histogram concept makes the proposed descriptor effective and useful when a reduced feature representation is required.
In this study, the proposed local feature descriptor HoST is addressed to the clustering of writing patterns included in the training database. The effectiveness of the proposed HoST descriptor is evaluated in Sect. 4.2 and compared with different local feature descriptors currently used in the liter-ature. 3.1.3 Learning multiple coupled dictionaries An  X  X ntelligent X  version of the K -means algorithm, referred to i K -means [ 38 ], is selected to partition the training database into C clusters in an unsupervised fashion because we have no prior knowledge on the number of clusters. The advantage of the i k -means method is that it automatically determines the number of clusters C and initial cluster centers for K -means using the anomalous pattern (AP) algorithm.

Sparse coding is an unsupervised method based on learn-ing a succinct high-level representation (called a dictionary D ) from the training data Y given only unlabeled inputs. Based on sparse coding principle, learning a dictionary can be mathematically formulated by the following minimization problem: D = argmin where  X  is the sparse representation of the training samples Y over the dictionary D , and the constant  X  is a regulariza-tion parameter used for balancing sparsity of the solution and fidelity of the approximation to Y . In the literature, dic-tionary learning problem is usually solved by alternatively optimizing  X  via a sparse coding step ( D is kept constant) and D via a dictionary update step (  X  is kept constant) until convergence.

Two coupled HR and LR dictionaries are generally learned for sparse coding-based resolution enhancement task [ 52 , 60 , 61 ]. To find more appropriate dictionaries describing the specificities of writing, we suggest learning multiple cou-pled HR and LR dictionaries from the clustered training data-base. Given the LR/HR patch pairs { Y i l , Y i h , i = 1 each cluster i , two coupled LR/HR dictionaries { D i l , 1 ... C } should be learned and the above minimization prob-lem becomes: D and D for the LR and HR patch spaces of each cluster i , respec-tively. To solve these minimization problems, we use a cou-pled dictionary learning method whose goal is to have the same sparse representation for each LR/HR patch pair [ 55 ]. The generation of the multiple coupled dictionaries enables the reconstruction phase, which is described in the following subsection. 3.2 Reconstruction phase The reconstruction phase aims to recover a HR image X from the input LR image B . It is performed using the multi-ple learned coupled dictionaries generated from the previous phase.
 First, the input LR image is up-sampled by the bicubic inter-polation. The interpolated image is considered as the LR image to be processed patch by patch from the upper-left corner with overlapping between adjacent patches. Second, the mean pixel value m is subtracted from each LR patch y Considering the centers of the C training clusters described in the previous phase, for each patch y j we suggest seeking the K nearest centers. Specifically, Euclidean distances are calculated between the input patch y j and the cluster centers p . Only the K nearest clusters are then chosen according to: min y j  X  p i 2 2 , i = 1 ... C (13) Using this strategy, we guide the reconstruction of y j to choose the K -coupled LR/HR dictionaries { D k l , D k h 1 ... K } learned from the selected K nearest clusters. Based on the sparse coding theory, the LR patch y j can be coded as a sparse linear combination of atoms from each LR dictionary D k l . This leads to the generation of K sparse representations {  X  k , k = 1 ... K } for the same patch y can be mathematically formulated as: (
P where  X  k is the sparse representation of y j over D k l and defines an allowable reconstruction error. Several algorithms have been proposed in the literature to solve ( P 1 ) [ 1 , 29 ]. In our implementation, the feature-sign search algorithm [ 29 ]is selected because of its efficiency and significant acceleration. Then, we select only the representation  X  that minimizes the local reconstruction error according to the appropriate LR dictionary D l :  X  = min The key idea is to guide the reconstruction to adaptively select the appropriate dictionary in order to better recover each local patch. The optimal solution  X  is then applied to generate a local HR patch x j from the corresponding HR dictionary based on: x j = D h  X  + m . Subsequently, the ini-tial HR image X 0 is obtained by simply averaging the values in the overlapped regions to enforce compatibility between adjacent patches.

To ensure consistency with the input LR image, we use the iterative back-projection method to apply a global recon-struction constraint by solving the following equation: X = argmin In fact, this constraint assumes that an observed LR image B consists of a blurred and downsampled version of a HR image X of the same scene: AHX = B , where A and H represents a downsampling operator and a blurring filter, respectively. 4 Experiments and evaluations In this section, we will evaluate our suggestions concerning the local feature descriptor and the resolution enhancement of textual images. The metrics to be used for this assessment will be presented in Sect. 4.1 . Then, quantitative as well as qualitative analysis will be described in detail in Sects. 4.2 and 4.3 . In the following experimental study, the best results are in bold. 4.1 Evaluation metrics 4.1.1 Metrics for unsupervised clustering When the ground-truth data partition is available, it is possi-ble to compare it with the partition proposed by the clustering method based on many indices such as the adjusted Rand, and the Jaccard and Folkes X  X allows indices [ 5 ]. However, when the ground-truth data partition is not available, another type of index is used to estimate the quality of clustering by measuring the cohesion (intra-variance) and separation (inter-variance) of the clusters. We focus on the second type of index because the correct partition of the training data-base used in this study is not available. In the following, we present the principle of each index used in this paper.
Given a dataset X ={ x 1 ,..., x N } composed by N ele-ments, the goal of a clustering is to partition X into K groups:  X  ={ c and the centroid of the dataset as c k and X , respectively. We will denote the Euclidean distance between x i and x j d ( x
Calinski and Harabasz [ 8 ] proposed a clustering validity index, referred to as the Calinski X  X arabasz (CH) index. It estimates the cohesion based on the distances from an ele-ment in a cluster to its centroid and the separation based on the distance from the centroids to the global centroid. The higher the CH index value, the better the clustering solution is. CH index is defined as follows: CH = where | c k | is the cardinality of the cluster c k . Davies and Bouldin [ 13 ] proposed another clustering validity index, referred to as the Davies X  X ouldin (DB) index, which is calculated as follows: DB = where S ( c k ) = In contrast to the CH index, the minimal value of DB index indicates the best clustering solution.

The silhouette is another popular cluster validity index [ 44 ]. For each observation in a dataset, the average distance to all observations in the same cluster and the average dis-tance to all observations need to be computed. This silhouette measurement gives information of how well each observa-tion lies within its cluster. Thus, the average of silhouette measurements (Sil) over the entire dataset provides an indi-cator of how appropriately the data has been clustered. It is defined as follows: Sil = where a ( x i , c k ) = b ( The silhouette measurement ranges from  X  1 to +1. A good clustering solution provides a result close to 1. Arbelaitz et al. [ 3 ] demonstrate that CH, DB, and Sil are within the group of indices that shows better behavior for clustering validation than other indices. 4.1.2 Image quality metrics Thestudyofresolutionenhancementrequirestheimagequal-ity metrics necessary to indicate the fidelity of the recov-ered image I r to the original ground-truth image I o .This can be done with objective measurements that should reflect human eye preference. Here, we chose to use the most pop-ular objective measurements, such as the root mean square error (RMSE), the peak signal-to-noise ratio (PSNR), the structural similarity index (SSIM) [ 56 ], and another recently proposed measurement known as the feature similarity index (FSIM) [ 64 ]. The RMSE calculates the average of the squares of the errors between the original ground-truth image I o the estimated image I r of common size m  X  n as follows: RMSE = The PSNR, which is the most commonly used image qual-ity metric, is measured via the ratio between the maximum possible value of a pixel MAX and the errors introduced by reconstruction: PSNR = 20  X  log 10 The SSIM measurement [ 56 ] incorporates implicit percep-tual models that reflect the characteristics of the human visual system. It is defined by local statistics comparing contrast, luminance, and structure. Given two image patches x and y extracted from the same spatial location from two images being compared, the SSIM metric is calculated as follows: SSIM ( x , y ) = where  X  x and  X  y are the mean intensities,  X  2 x and  X  2 variances,  X  xy is the covariance of x and y , and C 1 and C are small real constants used to stabilize the division with weak denominator. Resultant values of SSIM vary between 0 and 1, and the value 1 is attainable only in the case of two identical images.

The FSIM [ 64 ] definition is based on the image gradient magnitude and the significance of a local structure for the human visual system. The best FSIM value is expected to be close to 1.
 4.2 Evaluation of the proposed local feature descriptor 4.2.1 HoST: case study for pattern clustering In this section, we evaluate the proposed HoST descriptor for the clustering of writing patterns. In fact, each image patch of the database presented above is described by an HoST representation. After that, the i K -means method is applied to gather similar patterns in the same cluster.

HoST is designed to be non-oriented in contrast to the histogram of oriented gradients (HOG) descriptor [ 11 ]. Con-sequently, the orientation bins of the histogram are spaced over [ 0  X   X  180  X  ] . Table 1 summarizes the effect of varying the number of orientation bins on pattern clustering perfor-mance.TheresultsareinvestigatedintermsoftheDBandCH indices. According to Table 1 , we can see that sampling the structure tensor orientations into four bins achieves the lower DB result, the higher CH result, and thus the best clustering accuracy. The number of clusters determined by the AP algo-rithm is 12 when using four orientation bins. Increasing the number of orientation bins decreases the pattern clustering performance.

Given that the HOG descriptor has become increasingly popular in computer vision and pattern recognition appli-cations, we compare it with the proposed HoST descriptor. Before that, we study the effect of varying the number of ori-entation bins with the HOG descriptor. Tests are performed for the same sizes of orientation bins as in the previous exper-iment, but these bins are spaced over [ 0  X   X  360  X  ] . The HOG descriptor is designed to be oriented, and the gradient direc-tions have an effect on writing patterns [ 37 ]. For instance, a histogram of four orientation bins spaced over [ 0  X   X  180 in the HoST descriptor corresponds to a histogram of eight orientation bins spaced over [ 0  X   X  360  X  ] for the HOG descrip-tor. To make a fair comparison, the same clustering method i K -means is used. According to Table 2 , the best clustering accuracy is achieved when using eight orientation bins in the HOG descriptor. This confirms that 45  X  is the appropriate orientation bin size to cluster the writing patterns, as shown in the above experiment. In the following experiments, we use the optimal number of orientation bins in the case of the HOG descriptor and the HoST descriptor.
To examine the effectiveness of the proposed HoST descriptor, we compare it with the HOG descriptor and other existing local feature descriptors on the pattern clustering task. The features include pixel intensity, first order and sec-ond order of gradients, and local binary patterns (LBP) [ 40 ]-based descriptors. The results of this comparative study are listed in Table 3 . Figure 8 also compares our descriptor with those cited above in terms of silhouette measurements. At the sametestingsettings,Table 3 showsthattheclusteringperfor-mance is significantly improved when the proposed feature descriptor is applied to represent the image patches of the dataset. Indeed, pattern clustering using the HoST descriptor achieves the best results in terms of mean of the silhouette and the DB index. In addition, it greatly outperforms the tests applying the other feature descriptors involved in this study in terms of the CH index.

Furthermore, the results of clustering plotted in Fig. 8 via graphic representations of silhouette measurements illus-trate again that the use of the proposed feature descriptor improves pattern clustering performance. In fact, the silhou-ette measurement is negative when the average distance of one element to the others in the same group is greater than the average distance to elements in other groups. This unde-sirable characteristic is clearly observed in Fig. 8 a X  X  where the silhouette measurements are plotted for pattern clustering using, respectively, pixel intensity, first-order gradients, first order and second order of gradients, LBP, and HOG-based descriptors. Figure 8 f showing the results of clustering using the proposed HoST descriptor includes the lowest number of silhouette measurements that are negative. 4.2.2 HoST: case study for textual image resolution Basedontheaboveclusteringresults,thetrainingwritingpat-tern database is divided into 12 clusters using the proposed HoST descriptor. Coupled LR/HR dictionaries are learned from each cluster. Figure 9 b displays some of the HR dictio-naries generated by using the HoST descriptor. To compare with the application of the other descriptor, Fig. 9 a plots HR dictionaries learned from the same number of clusters and determined using the intensity-based descriptor. It can be noted that most appropriate dictionaries are found by apply-ing our descriptor, which allows gathering patterns with sim-ilar dominant orientations in the same cluster.

To evaluate quantitatively the effect of using the pro-posed HoST descriptor on the performance of our resolu-tion enhancement approach, we upscaled a LR textual image whose spatial resolution is 150 dpi. Table 4 compares the PSNR, RMSE, SSIM, and FSIM results of the upscaled images recovered using pixel intensity, gradients, LBP, HOG, and HoST-based descriptors. In this experiment, 128 atoms areusedperdictionaryandonlyonenearestclusterisselected during the reconstruction of the input LR patches. Accord-ingtoTable 4 , we can see that the proposed HoST descrip-tor succeeds in improving the performance of our resolu-tion enhancement approach. In fact, the highest measurement results are achieved when incorporating it into our approach. 4.3 Evaluation of the proposed resolution enhancement In this subsection, the experimental results achieved by applying the proposed resolution enhancement approach on a variety of LR textual images are given. To investigate the per-formance of the proposed approach, we compare it with other existing approaches including the nearest-neighbor interpo-lation, the spline interpolation, the bilinear interpolation, the bicubic interpolation [ 25 ], the methods of Li and Orchard NEDI [ 30 ], Zhang and Xiaolin [ 65 ], Shan et al. [ 47 ], Freed-man and Fattal [ 22 ], Yang et al. [ 60 ], Walha et al. [ 52 ], and Dong and Zhang [ 15 ]. The results are evaluated both visu-allyandquantitativelyintermsofimagequalityandcharacter recognition performance.
 We note that, in our resolution enhancement approach, the same dictionaries which were trained on a large database of writing patterns generated via FreeType library are used in all experiments. 4.3.1 Experimental results: textual image quality Experiment 1: Magnification of low-resolution printed tex-tual images As a first set of experiments, we apply the proposed approach for the upscaling of a LR textual image whose spatial resolu-tion is 150 dpi that was generated by blurring and downsam-pling the HR ground-truth image. This image contains a 10 pt text size with different styles (italic, non-italic, bold, non-bold). Table 5 quantitatively compares the PSNR, RMSE, SSIM, and FSIM results of the upscaled images recovered by different magnification approaches. The table shows that the images reconstructed by the proposed approach achieve higher measurement results than the other methods tested in this study. For a closer look, we specify regions in the input LR image and in the upscaled images and we enlarge them as shown in Fig. 10 . Compared to the input LR image in Fig. 10 a, improvements in visual quality are clearly noted in the result produced by each resolution enhancement method. In fact, more details are present, and characters are much more readable. However, as can be clearly seen in Fig. 10 b X  X , the bilinear, bicubic and spline interpolations produce blurry images and do not preserve sharpness along the edges. In addition, although the resolution enhancement approaches of Li and Orchard [ 30 ], Yang et al. [ 60 ], Freedman and Fattal [ 22 ], Dong and Zhang[ 15 ], and Shan et al. [ 47 ] generate good results when applied to natural images, their application to a textual image is limited (Fig. 10 e X  X , i, j, respectively). In fact, the characters in the resulting textual images are sub-stantially harmed by several degradations that affect local patterns and structures such as edges. On the other hand, when we visually compare the results in Fig. 10 b X  X  versus those in Fig. 10 k, it can be observed that the image recovered by the proposed approach is clearer and sharper at edges than images generated by the other approaches tested in this study and that have significant artifacts appearing near edges. Con-sequently, they are not as effective when applied to textual images. This can be explained by the substantial difference that distinguishes natural images from textual images.
A second set of experiments aims to enhance the spatial resolutionofadocumentimageprovidedbytheTobaccodoc-ument collections. 2 The input degraded image whose resolu-tion is 75 dpi was generated by blurring and downsampling the original image by a factor of two. Different magnifica-tion approaches are compared visually in this experiment. For better visualization, regions that are selected from the resulting images are enlarged and given in Fig. 11 . These figures demonstrate that the results generated by the pro-posed resolution enhancement approach outperform all the other resulting images.
 Experiment 2: Magnification of textual images taken by a mobile device This experiment concerns the resolution enhancement of a textual document image taken by a mobile device and whose resolution is 96 dpi. The distance between the mobile device and the document was about 0.5 m. In Fig. 12 , enlarged regions are extracted from the input image and from other images recovered by different resolution enhancement approaches. It can be noted that the image recovered by our approach (Fig. 12 f) is clearer and sharper at the char-acter edges. Thus, the resulting image has better visual qual-ity than those produced by the bilinear, bicubic [ 65 ], and [ 30 ] approaches. We should note that the input acquired image (Fig. 12 a) presents further degradation due to illumi-nation problems; nevertheless, our resolution enhancement approach provides promising results.
 Experiment 3: Magnification of low-resolution handwritten textual images The proposed resolution enhancement approach is not con-finedtotheprocessingofprintedtextualimages.Handwritten textual images can also be processed. In fact, the use of vari-ous elementary writing patterns in the learned dictionaries is of a great benefit to recover even this particular kind of docu-ment images. In addition, the proposed reconstruction phase occurs at a patch image level and not on the character image level as is done in previous works [ 9 , 12 , 41 ]. Therefore, our approach provides substantial flexibility for different kinds of writing (printed and handwritten). For instance, Fig. 13 shows enlarged regions extracted from results generated by the application of different magnification methods on a LR handwritten textual image scanned at 75 dpi. Tests are per-formedfortheupscalingfactor4.Figure 13 provestheuseful-ness of the proposed approach when applied to enhance the resolution of a handwritten textual image. Promising results are achieved when compared to those of other upscaling methods and to the ground-truth image scanned at 300 dpi. Experiment 4: Magnification of textual images with different scripts Most of the learning-based approaches proposed in the liter-ature for the resolution enhancement of textual images have demonstrated good results. Nevertheless, their enlargement results depend heavily on the training character images and they work better, especially when the same size and type font of the input LR image are included in the training samples [ 9 , 12 , 41 ]. As mentioned in Sect. 3 , the learning phase of the proposed resolution enhancement approach is based on a large database of various writing patterns and the recon-struction phase occurs at a patch image level and not on the character image level as is done in previous studies [ 9 , 12 , 41 ]. Therefore, our approach provides substantial flexibility for different scripts.
 As confirmation, the current experiment investigates the magnification of a LR document image containing Arabic handwritten text. The tested image was scanned at 75 dpi. The results shown in Fig. 14 illustrates the effectiveness of our approach, which is able to enhance the spatial resolution of any script. In fact, the resulting image in Fig. 14 f is sharper at the character edges and has better visual quality than the images upscaled by bicubic interpolation, and the methods designed by [ 30 ], and [ 47 ], which generate significant arti-facts near the edges, as noticed in Fig. 14 b X  X .

Another experiment was conducted for enhancing the spa-tial resolution of a LR textual image containing a manu-script written in Irish script. This image was taken from the bleed-through degraded document image database [ 45 ]. In this experiment, the input image is enlarged from 75 dpi to 600 dpi (i.e, upscaling factor 8) using different methods. Some enlarged regions extracted from the recovered images are shown in Fig. 15 . The effectiveness of the proposed res-olution enhancement approach in reconstructing Irish script is clear. In fact, the visual quality of the result (Fig. 15 e) is significantly improved when compared to the results of bicu-bic interpolation (Fig. 15 b) and the approaches reported by [ 30 ] (Fig. 15 c) and [ 47 ] (Fig. 15 d). More precisely, the result achievedbyourapproachcontainsfewerartifactssuchasblur and noise, preserves important character edges, and thus is theclosesttotheground-truthimageshowninFig. 15 f.More-over, Table 6 quantitatively compares the PSNR, RMSE, SSIM, and FSIM results of the upscaled images recovered by different resolution enhancement methods involved in this experiment. The table shows that the highest measurement results are achieved by using the proposed approach. Experiment 5: Discussion on the parameters of the proposed resolution enhancement approach In this experiment, we investigate the influence of two parameters on the performance of the proposed resolution enhancement approach. The first parameter is the number of the selected nearest clusters K for which an input LR patch will be assigned in the reconstruction phase. Tests are performed on the LR image used above (Fig. 10 a) for the upscaling factor 2. We vary the value of K from 1 to 6, and we investigate the quality of the reconstructed images. The results given in Table 7 shows that the best RMSE, PSNR, and SSIM values are achieved by selecting the three nearest clusters. When the value of K is greater than 3, the proposed approach outputs very similar results. This can be explained by the usefulness of the proposed HoST descriptor, which improves the clustering of the training database; therefore, it allows faster convergence to the appropriate dictionary dur-ing the proposed reconstruction scheme.
 The second parameter studied in this experiment is the num-ber of dictionary atoms. Based on the previous test, we set K = 3. According to Table 8 , it can be seen that the mag-nification results are slightly improved when increasing the number of atoms from 128 to 256. On the other hand, we can see that the computational complexity increases with the number of dictionary atoms. Since the increase in resolution enhancement results did not compensate for the computa-tional complexity when varying the number of atoms from 128 to 256, a decision was made to use only 128 atoms per dictionary.

It should be noted that in the first three experiments pre-sented above to evaluate the performance of the proposed res-olution enhancement approach, we used 128 atoms in each dictionary and we set K = 3. As shown in this experimental study, even if we use other values for these two parameters, our proposed resolution enhancement approach succeeds in achieving the best results when compared visually and quan-titatively to other existing approaches. 4.3.2 Experimental results: character recognition As seen in the above-described experimental study, the eval-uation of textual image quality after the resolution enhance-ment process was conducted visually and quantitatively with PSNR, RMSE, SSIM, and FSIM measurements. Unlike on natural images, the effect of the magnification process on textual images could also be evaluated by assessing the automatic character recognition performance. In fact, the improved character recognition rate provided by the same OCR engine is surely related to improved image quality. Thus, the goal of this section is to study the response of an OCR engine before and after enhancing the spatial resolution of textual images using different magnification approaches. To ensure a fair comparison, Tesseract 3.02, which is con-sidered one of the most accurate open source OCR engines, is used for all tests.
 In this study, the results are investigated in terms of character OCR accuracy percentage defined as: C  X  L C  X  100, where C is the total number of the correct characters and L is the Leven-shteindistance,whichisthetotalminimumcostoftransform-ing one string into the other using the following edit opera-tions: insertions, deletions, and substitutions [ 43 ]. The first experiment concerns the recognition of a LR textual image provided by the  X  X obacco X  document collections before and after magnification with the proposed approach and other upscaling approaches, including nearest-neighbor interpo-lation, bilinear interpolation, spline interpolation, bicubic interpolation [ 25 ], and the methods of Li and Orchard [ 30 ], Zhang and Xiaolin [ 65 ], Shan et al. [ 47 ], Freedman and Fattal [ 22 ], and Dong and Zhang [ 15 ].
 The spatial resolution of the input image is 72 dpi, and the tests are performed for the magnification factor 2. The text included in this image contains 1,672 characters. As shown in Table 9 , a character recognition performance gap exists between using resolution enhancement before the recogni-tion step and without using it. In fact, recognizing a LR tex-tual image presents several challenges to any OCR software. For instance, the character recognition percentage achieved when applying the Tesseract engine to the input LR image is 15.79%. On the other hand, the accuracy of this engine is sig-nificantly improved when the LR image is processed with the proposed resolution enhancement approach and we succeed in increasing the character recognition rate to 71.81%. Com-pared to the recognition results using the resolution enhance-ment approaches involved in this study, our approach pro-vides the best character recognition rate. In particular, this learning-based approach outperforms bicubic interpolation, which is one of the most commonly used magnification meth-ods, increasing the character recognition rate by more than 10%.

Another experiment was conducted to examine the use-fulness of the proposed resolution enhancement approach in improving character recognition performance. A validation set of ten document images scanned at 100 dpi was used in this experiment. They are labeled from 1 to 10. The content of these images is diverse because they are extracted from different documents and they contain different text sizes (9, 10, 12, 14, 20), styles (italic, non-italic, bold, non-bold), and fonts (serif, sans serif). Some regions are extracted from these images and illustrated in Fig. 16 . The quality of characters, which are considerably affected by the use of a scanning process with LR, can be clearly seen. For all these images, the OCR accuracy and the total number of recognition errors before and after the magnification are detailed in Table 10 . As globally shown in Fig. 17 , the resolution enhancement process provides significant improvements in terms of the characterrecognitionrate.Inaddition,theproposedapproach allows reaching much better recognition results than bicubic interpolation. For instance, the character recognition percent-age of the image labeled 5 rises from 18.97 to 61.26% after magnification with bicubic interpolation and to 85.98% after upscaling with our approach. This is due to the improved abil-ity of this approach in preserving writing patterns and thus in distinguishing characters.

Figure 18 details the effect of the resolution enhance-ment on an extract of this type of image. This figure presents extracts of the OCR results obtained on the input LR image scanned at 100 dpi, the images recovered by bicubic inter-polation and the proposed resolution enhancement approach, and the ground-truth image scanned at 200 dpi. For a closer visualization, enlarged regions extracted from these different images are also given. 5 Conclusions and perspectives In this paper,we have proposed a new sparse coding-based approach for the resolution enhancement of LR textual images using multiple coupled dictionaries and adaptive sparse representation selection. An unsupervised cluster-ing of writing patterns and a novel local feature descrip-tor, referred to HoST, have been incorporated into the learn-ing of the coupled dictionaries in order to find more appro-priate descriptor adapted to the particular specificities of writing. We have proven the efficiency of the proposed approach, which was successfully applied to a variety of printed and handwritten LR textual images with different scripts. In addition, the approach was evaluated visually and quantitatively and compared to other existing magnification methods. Moreover, the experimental results demonstrate the usefulness of this proposed approach for improving character recognition performance.

An extension to this work would be to apply the proposed resolution enhancement approach to LR texts embedded in video. Furthermore, we intend to investigate the application of the proposed HoST descriptor to other computer vision tasks such as image matching and object detection. References
