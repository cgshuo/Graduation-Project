 } We propose a simple method for converting many stan-dard measures of retrieval performance into metasearch al-gorithms. Our focus is both on the analysis of retrieval mea-sures themselves and on the development of new metasearch algorithms. Given the conversion method proposed, our ex-perimental results using TREC data indicate that system-oriented measures of overall retrieval performance (such as average precision) yield good metasearch algorithms whose performance equals or exceeds that of benchmark techniques such as CombMNZ and Condorcet.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Retrieval models Theory, Algorithms, Experimentation Metasearch, Retrieval Evaluation
Metasearch is the well-studied process of fusing the ranked lists of documents returned by a collection of systems in re-sponse to a given user query in order to obtain a combined list whose quality equals or exceeds that of any of the under-lying lists. Many metasearch techniques have been proposed and studied, and for the purposes of comparison, we con-sider two benchmark techniques in this work: CombMNZ and Condorcet. CombMNZ [1, 2] is based on combining the normalized scores given to each document by the un-derlying systems, while Condorcet [3] is based on viewing the metasearch problem as a multi-candidate election where the documents are candidates and the systems are voters expressing preferential rankings among the candidates.
Retrieval systems are evaluated using a number of stan-dard measures of performance such as average precision, R-precision, and precisions at various cutoffs. These evaluation
We gratefully acknowledge the support provided by NSF grant CCF-0418390.
 measure implicitly assign weights to the relevances of docu-ments at various ranks. For example, precision-at-cutoff 10, PC(10), implicitly assigns a weight of 1/10 to the relevances of each of the top 10 documents in a list and a weight of 0 to the relevances of all remaining documents, and as such PC(10) can be calculated by multiplying the weights implic-itly associated with each document by their 0-1 relevances.
We now consider two synergistic facts: (1) evaluation mea-sures aim to assess how well a system retrieves relevant doc-uments, as measured and evaluated by the aforementioned implicit weights and (2) retrieval systems aim to retrieve rel-evant documents as well as possible. As such, our hypoth-esis is that evaluation measures will assign  X  X igh X  weights to relevant documents when applied to the lists generated by  X  X ood X  retrieval systems. Thus, evaluation measures can be used to identify likely relevant documents in a list, as determined by the measure X  X  implicit weights. Applying such a measure to many lists and combining the weights as-signed to documents appropriately, one can assign  X  X onsen-sus X  weights to the documents collectively retrieved in mul-tiple lists, rank these documents by their consensus weights, and thus obtain a metasearch list.
We now formalize the ideas presented above to convert measures of retrieval performance to metasearch algorithms. Precisions at standard cutoffs. Consider precision-at-cutoff k , PC( k ), for any integer k . PC( k ) implicitly assigns a weight of 1 /k to each of the top k documents in a list and a weight of 0 to every remaining document. Given multiple lists, a document may be assigned multiple weights, depend-ing on how the document is ranked in each of the underlying lists. To obtain a consensus weight for a document, one can simply compute the average weight assigned to the docu-ment across the underlying lists. To obtain a consensus metasearch list, one can then simply rank the documents according to these average scores (breaking ties arbitrarily). R-precision. By definition, R-precision is PC( R ), where R is the total number of relevant documents for the query. As such, one can convert R-precision to a metasearch al-gorithm as described above. However, such a conversion does not yield a true metasearch algorithm because it de-mands a priori knowledge of R . In practice, one would need to estimate (or be given) R , a non-trivial task in a typical metasearch setting. However, we include R-precision in this discussion since it is an often cited and robust measure of overall retrieval performance. Average precision. Average precision does not yield im-plicit weights associated with ranks quite as obviously as do precisions-at-cutoffs and R-precision; however, one may compute such implicit weights as follows. By definition, av-erage precision is the average of the precisions at all rele-vant documents. Given a list of documents, one normally assumes that the precisions at all unretrieved relevant docu-ments are zero. As such, one can compute average precision as follows, where N is the length of the retrieved list, rel ( i ) is the 0-1 relevance of the document at rank i , and R is the number of relevant documents for the query.
 Thus, average precision effectively assigns an implicit weight R  X  i to each pair of ranks ( i, j ), for all 1  X  j  X  i  X  N . To compute the implicit weight associated with each rank r , we simply sum the weights associated with all pairs involving r , yielding X where H k is the k -th harmonic number. Finally, we note that R is seemingly necessary to calculate these weights, yielding similar problems for metasearch as described above for R-precision. However, unlike the situation for R-precision where knowledge of R was necessary to determine which documents would receive a non-zero weight, here R simply acts as a uniform scaling factor applied to all weights. As such, it is unnecessary for metasearch (i.e., ranking) pur-poses: we simply weight each document at rank r in a list with the value (1 + H N  X  H r ), compute a consensus weight for each document by averaging the weights assigned over all lists, and rank the documents according to these average scores to obtain a metasearch list.
We tested the metasearch algorithms associated with av-erage precision, R-precision, and precisions at standard cut-offs using data from TRECs 5, 6, 7, 8, and 9. For each metasearch algorithm, each TREC, and each of the 50 queries in that TREC, we used the metasearch algorithm in question to combine all of the lists submitted for that query in that TREC. We evaluated these metasearch lists using average precision and averaged these AP values across the queries in a TREC to obtain the mean average precision (MAP) val-ues reported in Table 1. The table also contains MAP values for the benchmark CombMNZ and Condorcet algorithms for the purposes of comparison.

We first note that the metasearch algorithms associated with average precision and R-precision outperformed those algorithms obtained from precision-at-cutoff k for any k . Our hypothesis is that system-oriented measures of overall retrieval performance tend to implicitly weight the complete set of relevant documents more highly than user-oriented measures such as precisions at standard cutoffs. Second, we note that the performance of the metasearch algorithms cor-responding to average precision and R-precision often equals or exceeds the performance of benchmark techniques such as CombMNZ and Condorcet. The AP and RP results shown in Table 1 constitute statistically significant improvements with respect to CombMNZ and Condorcet when labeled with a  X  and/or  X  , respectively.
We have described a generic methodology for converting a measure of retrieval performance to a metasearch algorithm, and we have demonstrated such conversions for the measures average precision, R-precision, and precisions at standard cutoffs. Our conclusion is that system-oriented measures such as average precision and R-precision tend to implic-itly weight relevant documents appropriately for metasearch purposes, yielding algorithms whose performance equals or exceeds benchmark techniques such as CombMNZ and Con-dorcet. We intend to explore the use of this methodol-ogy to convert other measures of retrieval performance to metasearch algorithms and to further study and evaluate the quality of retrieval performance measures in general. [1] E. A. Fox and J. A. Shaw. Combination of multiple [2] J. H. Lee. Analyses of multiple evidence combination. [3] M. Montague and J. A. Aslam. Condorcet fusion for
Sign test of significance; 90% confidence level.
