 1. Introduction
Web search engines are changing people X  X  life, and continuously enhancing the accuracy (relevance) of 1999 ) resort to empirical methods in ranking model construction. These include content based methods such as
BM25 ( Robertson, 1997 ) and link based methods such as PageRank ( Page, 1998 ). As more and more informa-features. The approach of employing machine learning techniques to address the problem naturally emerges as an effective solution and several methods have been proposed along the direction. Typical methods include
Joachims, 2002 ), and RankNet ( Burges et al., 2005 ), which are based on boosting, support vector machines and neural networks, respectively. From the machine learning perspective, ranking, in which given a query and its associated documents we are to rank the documents as correctly as possible, also becomes a new branch of supervised learning, in addition to classification, regression, and density estimation ( Vapnik, 1998 ).
However, it should be noted that the aforementioned machine learning methods were not proposed directly nen, 2000, 2002 ). All the IR criteria are on the query-level; specifically, given two queries, no matter how in terms of the evaluation measures.

In order to solve this problem, we propose employing query-level loss functions in learning of ranking func-tions for IR.

In this paper, we first discuss what kind of properties a good query-level loss function should have. Then we a ranking list and the corresponding ground truth with respect to a given query. With the new loss function, we further derive a learning algorithm, RankCosine, which learns a generalized additive model as ranking function.

Next, we discuss whether it is possible to extend the document or document-pair level loss functions of the existing methods (ranking SVM, RankBoost, and RankNet) to the query-level.

We used two public datasets and one web search dataset to evaluate the effectiveness of our method. Exper-query-level.
 query-level loss function should have. We then give an example of query-level loss function, cosine loss, and the query-level. Conclusions and future work are given in Section 7 . 2. Related work In recent years many machine learning technologies ( Burges et al., 2005; Crammer &amp; Singer, 2002; Dekel, applied to the problem of ranking for information retrieval. Some early work simply tackled this problem documents. However, in real-world applications, the degree of relevance of a document to a query can be dis-lems, many other methods were proposed including ranking SVM, RankBoost, and RankNet.

Herbrich et al. (2000) and Joachims (2002) took an SVM approach to learn a ranking function and pro-posed the ranking SVM algorithm. The basic idea of ranking SVM is the same as the conventional SVM: min-imizing the sum of empirical loss and regularizer. The difference is that the constraints in ranking SVM are defined on partial-order relationships within document pairs. The optimization formulation of ranking SVM is shown as follows 1 : in the framework of structural risk minimization, and empirically the effectiveness of ranking SVM has been et al., 2007 ) have also been proposed.
 Freund et al. (2003) adopted the boosting approach to ranking and proposed the RankBoost algorithm. Similarly to ranking SVM, RankBoost operates on document pairs. Suppose d i q d j denotes that document d that the model asserts d i q d j . Then the loss for a document pair in RankBoost is defined as Consequently, the total loss on training data in RankBoost is defined as the sum of losses from all document pairs: The advantages of RankBoost include that it is easy to implement the algorithm and it is possible to run the algorithm in parallel. The effectiveness of RankBoost has also been verified.
 Neural networks have also been applied to ranking recently. Burges et al. (2005) proposed the Rank-ranking function. Similarly to ranking SVM and RankBoost, training samples of RankNet are also document Similarly, the total loss in RankNet is defined as the sum of all document pairs RankNet has been successfully applied to web search. Further improvements on RankNet can be found in Matveeva, Burges, Burkard, Laucius, and Wong (2006), Tsai, Liu, Qin, Chen, and Ma (2007), Cao, Qin, Liu, Tsai, and Li (2007) .

One major problem ranking SVM, RankBoost, and RankNet have is that the loss functions used are not in accordance with the IR evaluation measures. We will elaborate on this in more details in the next section. 3. Query-level loss functions for information retrieval the classification approach ( Nallapati, 2004 ), the loss function is defined on the document level. The loss functions of ranking SVM, RankBoost, and RankNet are defined on the document-pair level. These loss func-teria for IR such as MAP and NDCG which are defined on the query-level. This motivates us to propose the use of query-level loss functions, as will be discussed below. 3.1. Why is query-level loss function needed
As mentioned above, the loss functions in ranking SVM, RankBoost, and RankNet are not in accordance with the evaluation criteria in IR. This may penalize the accuracies of the learning algorithms.
Let us consider a simple example. Suppose there are two queries q 1 and q 2 with 40 and five documents, respectively. In the extreme case of using complete partial-order document pairs for training, we can get have the same number of document pairs for training, the document-level loss function and the query-level loss can lead to the same result. However, this assumption does not hold in real-world scenarios. Therefore it is better to define loss function on the query-level when training ranking functions. 3.2. What is a good loss function
One may ask what properties a good query-level loss function should have. Here we list some properties, and discuss the necessities.

We first give explanations on the notations. Suppose n ( q ) denote the number of documents for a query q to two ranking lists to a non-negative real number. That is, (1) Insensitive to number of document pairs.
 This has been made clear through the example in Table 2 . In this regard, the loss functions of ranking SVM, RankBoost and RankNet are not good loss functions.
 This property can be expressed formally as (2) Important to rank top results correctly.

In contemporary IR, precision is often considered more important than recall, because information to be searched is usually abundant. Many IR evaluation criteria embody the requirement of conducting accurate ranking on the top of lists. For example, a ranking error at the 5th position is more harmful than a ranking error at the 95th position. A good loss function should also reflect this property.
 We give a formal definition of this property: Property 2. Suppose the ground truth rank list of query q is where d  X  j  X  i means document d i is ranked at position j. s f1 (q) and s f2 (q) are two ranking lists generated by ranking functions f1 and f2 Then a good query-level loss function L should satisfy The loss functions of ranking SVM, RankBoost and RankNet have a similar tendency. The reason is that if a definitely irrelevant document is ranked high, it will violate many constraints regarding to ranking of docu-(3) Upper bound.

Query-level loss function should not be easily biased by difficult queries. For this purpose, a natural requirement is that the loss for each query should have an upper bound. Otherwise, queries with extremely large losses will dominate the training process.
 The formal definition of this property is Property 3. For any f 2 F ; q 2 Q , there should exist a constant C, such that
We can see that the document-pair level loss functions of RankBoost, ranking SVM, and RankNet do not have an upper bound. For RankBoost, because the exponential loss is used, the value of the loss func-tion can be extremely large. We can make a similar conclusion for the hinge loss of ranking SVM. For
RankNet, the loss function does not have an upper bound either, according to the analysis in Burges et al. (2005) . 4. RankCosine
In this section, we propose a new loss function which retains all the aforementioned properties. 4.1. Cosine loss is the output of the k th document given by the learning machine.
 Next, we define the ranking loss for query q as follows: loss.
 The goal of learning then turns out to minimize the total loss function over all training queries where the loss function is defined in (6) .
 Now we show that the cosine loss has all the three properties defined above.

Firstly, since bound of cosine loss is For any k 1&gt;0and k 2 &gt; 0, we get Therefore, Property 1 is satisfied with " C &gt; 1 for the cosine loss.
 Secondly, we can put more emphasis on training of top results by setting an appropriate ground truth label.
For example, we can set the scores of ground truth using an exponential function. Specifically we set the
Thirdly, the cosine loss function has an explicit lower bound and upper bound: 4.2. Learning algorithm
In this section, we explain how to optimize the cosine loss function. We choose to employ a generalized additive model as the final ranking function: and d is the dimension of feature vector: For simplicity, we assume the ground truth for each query has already been normalized: Then we re-write (6) as irani, 1998 ), to train the parameters in the ranking function. Let us denote H k ( q )as Given H k 1 ( q ) and h k ( q ), (11) can be re-written as a k as follows: where W 1, k ( q ) and W 2, k ( q ) are two n ( q )-dimension weight vectors with the following definitions. weak learners and their combination coefficients, and thus the final ranking function.
The time complexity of RankCosine is O( m  X  k  X  T  X  n max ), where m denotes number of training queries, k denotes number of weak learner candidates, T denotes number of iterations, and n max denotes maximum num-ber of documents per query. RankBoost also adopts a boosting algorithm for learning of ranking function and much lower than that of RankBoost. 5. Experiments
To verify the benefit of using query-level loss functions, we conducted experiments on three datasets. We describe the details in this section. 5.1. Settings We adopted three widely used evaluation criteria in our experiments: mean precision at n (P@ n )( Baeza-Sormunen, 2002 ). All of them are widely used in IR.

In our experiments, we selected three machine learning algorithms for comparison: RankBoost, RankNet, and ranking SVM. For RankBoost, the weak learners had a binary output of 0 or 1. For RankNet, we used Layer-RankNet respectively. For ranking SVM, we used the tool of SVMlight ( Joachims, 1999; Joachims, 2002 ). make a comparison with traditional IR approaches, we also chose BM25 ( Robertson, 1997 ) as baseline. 5.2. Experiments with TREC web track data We tested the performance of RankCosine using the dataset from the TREC web track ( Voorhees &amp; Harman, 2005 ).
 5.2.1. Dataset The TREC Web Track (2003) dataset contains web pages crawled from the .gov domain in early 2002. There are totally 1,053,110 pages. The query set is from the topic distillation task ( Craswell, Hawking, Wilkinson, &amp; Wu, 2003 ) and there are in total 50 queries. The ground truths of this task are provided by the TREC committee as binary judgments: relevant, or irrelevant. The number of relevant pages per query ranges from 1 to 86.

We mentioned that usually the number of documents can vary largely according to queries and this phe-nomenon can be verified with this dataset. For instance, from Fig. 2 , we can see that the number of relevant documents per query has a non-uniform distribution: about two thirds queries have less than 10 relevant doc-uments. If we use a document-pair level loss function, two thirds of the queries will be penalized. In other words, two thirds of the queries will not contribute to the learning process as much as they should.
We extracted 14 features from each document for the learning algorithms. These features include content based features (BM25, MSRA1000), web structure based features (PageRank, HostRank), and their combi-nations (relevance propagation features). Some of them are traditional features (BM25, PageRank) and some are new features (HostRank, relevance propagation features). The details of the features are described in Table 3 . 5.2.2. Results
We conducted fourfold cross validations for the learning algorithms. We tuned the parameters of BM25 in
From Fig. 3 a, we can see that RankCosine outperforms all the other algorithms in terms of MAP, while the SVM, RankBoost, and RankNet) have similar learning abilities for information retrieval. Note all the learning algorithms outperform BM25. The MAP value of BM25 is about 0.13, which is comparable to the value reported in Qin, Liu, Zhang, Chen, and Ma (2005) . RankBoost gets the lowest MAP value of 0.18 among all the learning algorithms, and this is still much higher than BM25. RankCosine, which obtains an MAP value of 0.21, improves upon BM25 for about 70%. This suggests that learning to rank is a promising approach for search, as it can leverage the information from various features.
 From Fig. 3 b, we can see that RankCosine outperforms all the other algorithms, from P@1 to P@7. From P@8 to P@10, RankCosine is still much better than most of the other algorithms, except TwoLayer-RankNet. An interesting phenomenon is that RankCosine achieves more improvements at top when compared with the other algorithms, for example, more than four precision point 4 improvement for P@1, about two precision point improvements for P@5. Since correctly conducting ranking on the top is more important, this tendency is desirable anyway.

Fig. 3 c shows the result in terms of NDCG, and again RankCosine can work better than the other algorithms. 5.3. Experiments with OHSUMED data
We also conducted experiments with the OHSUMED data ( Hersh, Buckley, Leone, &amp; Hickam, 1994 ), 2000 ). 5.3.1. Data
OHSUMED is a dataset of documents and queries on medicine, consisting of 348,566 references and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. Different evant X  X , and  X  X  X ot relevant X  X .

We adopted 30 features, similar to those defined in Nallapati (2004) . Table 4 shows some examples of the features. They include tf (term frequency), idf (inverse document frequency), dl (document length), and their combinations. BM25 score is another feature, as proposed in Robertson (1997) . We took log on the feature iments. Stop words were removed and stemming was conducted in indexing and retrieval. Note the features of this dataset are different from those of the TREC data, since OHSUMED is a text document collection with-out hyperlink.

When calculating MAP, we defined the category of  X  X  X efinitely relevant X  X  as positive and the other two cat-egories as negative. 5.3.2. Results
In this experiment, we also conducted fourfold cross validation for learning algorithms, and tuned the averaged over four trials. From the figure, we can see that RankCosine outperforms all the other algorithms, from NDCG@1 to NDCG@10. On the other hand, the performances of the three learning methods (Rank-Net, ranking SVM, and RankBoost) are similar. Therefore, we can draw the same conclusion as in the pre-vious experiment.

Comparing Fig. 3 b with c and Fig. 4 b with c, we may observe the differences between the corresponding evaluation criteria. P@ n only considers the number of relevant documents at top n positions, and ignores the distribution of relevant documents. For example, the following two rank lists get the same P@4 value. Different from P@ n , NDCG@ n is sensitive to the ranked positions of relevant documents. For example, the NDCG@4 value of list B is much higher than that of list A.
 (A) {irrelevant, irrelevant, relevant, relevant, ... }. (B) {relevant, relevant, irrelevant, irrelevant, ... }. 5.4. Experiments with web search data
To verify the effectiveness of our algorithm, we also conducted experiments with a dataset from a commer-cial web search engine. 5.4.1. Data
This dataset contains over 2000 queries, with human-labeled judgments. The queries were randomly sam-pled from the query log of the search engine. Human labelers were asked to assign ratings (from 1 which means  X  X rrelevant X  to 5 which means  X  X efinitely relevant X ) to those top-ranked pages for each query. Each failed, a meta labeler was asked to give a final judgment. Not all the documents were labeled due to resource limitation.

We randomly divided the dataset into a subset for training and a subset for testing. There were more than have less than 400 pairs; and the remaining queries have from 400 to over 1400 pairs. If we use document-pair ries with more pairs.

The features are also from the search engine, which mainly consists of two types: query-dependent features and query-independent features. Query-dependent features include term frequencies in anchor text, URL, title, and body text, while query-independent features include page quality, number of hyperlinks and so on. In total, there are 334 features.

Since there are five levels of judgment for this dataset, only NDCG is suitable for the evaluation. 5.4.2. Result
The accuracies of the learning algorithms are shown in Fig. 6 . From the figure, we can see that RankCosine achieves the best result in terms of all NDCG scores, beating the other algorithms by 2 X 6 NDCG 5 points (which corresponds to about 4 X 13% relative improvements). TwoLayer-RankNet achieved the second best result, followed by the group of ranking SVM, RankBoost and linear-RankNet. The results indicate that RankCosine (and using query-level loss function) is the approach one should take for search.
We conducted t -tests on the results of RankCosine and TwoLayer-RankNet. The p -values from NDCG@1 to NDCG@10 with respect to the confidence level of 98% are shown in Table 5 . As can be seen, all the p -val-ues are small, indicating that the improvements of RankCosine over TwoLayer-RankNet are statistically significant. 5.4.3. Robustness to query variance
As indicated above, the number of document pairs can vary largely according to queries. We investigated the impact of this variance on the performances of ranking algorithms. For this purpose, we randomly sam-tested the ranking performances on the test in the same way as before. Since the five training sets were gen-erated by random sampling, we can observe variances in number of documents across queries. Taking dataset 1 and 2 as example (see Fig. 7 a), we can see that distributions of pair numbers in the two datasets are very ances of queries.

The performances of each algorithm with respect to the five training sets are shown in Fig. 7 b. As can be accuracy of 0.576 on the third dataset and the lowest accuracy of 0.557 on the fifth. The results of ranking
SVM also change dramatically over different training sets. The results indicate that both RankBoost and rank-ing SVM are not very robust to query variances. The results of two-layer RankNet are more stable. In con-ument-pair level loss functions. 6. Query normalization
As shown in Section 5 , the query-level loss function performs better than the document-pair level loss func-tions. One may argue that employing a document-pair level loss function while conducting normalization on problem in details. Specifically, we introduce query normalization to loss functions of ranking SVM, Rank-Boost, and RankNet, and look at the corresponding ranking performances.
 For ranking SVM, we can modify its loss function (Eq. (1) ) as below where # q denotes number of document pairs for query q . Similarly, we can modify the loss function of Rank-Boost as follows: and modify the loss function of RankNet (Eq. (5) ) as follows:
With the above modifications, we can still find suitable optimization procedures for performing the learning tasks. We have conducted experiments on the methods and Fig. 8 shows the results using the same setting as above.

From Fig. 8 , we find that (1) When compared with their original algorithms, normalized RankBoost and normalized linear-RankNet (2) The results of normalized TwoLayer-RankNet are worse than those of the original TwoLayer-RankNet
From the above experiments, we can come to the conclusion that it is non-trivial how to improve the exist-ing ranking algorithms by query normalization. Note that both RankCosine and RankBoost employ boosting techniques in learning, therefore the difference between the two should be mainly from the loss functions. 7. Conclusions and future work
Applying machine learning techniques to ranking in information retrieval has become an important research problem. In this paper, we have investigated how to improve ranking accuracies of machine learning methods by employing suitable loss functions. Our contributions include: (1) We have pointed out that query-level loss functions are more suitable for information retrieval, when (2) We have defined the cosine loss function as an example of query-level loss functions, and derived the (3) Through empirical study, we have showed that it is difficult to extend the loss functions of the existing
Experimental results on the datasets of TREC web track, OHSUMED, and a commercial web search engine all show that our proposed query-level loss function can significantly improve the search accuracy.
Future work includes investigation on the relationship between query-level loss functions and information on the generalization ability of the RankCosine algorithm.
 Acknowledgements We would like to thank Xiu-Bo Geng for her discussion and comments for this work.
 References
