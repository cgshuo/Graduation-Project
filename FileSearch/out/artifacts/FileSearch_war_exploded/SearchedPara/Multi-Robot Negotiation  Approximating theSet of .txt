 We model the multi-agent planning problem as a general-sum stochastic game with cheap talk: the agents observe the state of the world, discuss their plans with each other, and then simultaneously select their actions. The state and actions determine a one-step reward for each player and a distribution over the world X  X  next state, and the process repeats. While talking allows the agents to coordinate their actions, it cannot by itself solve the problem of trust: the agents might lie or make false promises. So, we are interested in plan-ning algorithms that find subgame-perfect Nash equilibria . In a subgame-perfect equilibrium, every deviation from the plan is deterred by the threat of a suitable punishment, and every threatened punishment is believable. To find these equilibria, planners must reason about their own and other agents X  incentives to deviate: if other agents have incentives to deviate then I can X  X  trust them, while if I have an incentive to deviate, they can X  X  trust me. In a given game there may be many subgame-perfect equilibria with widely differing payoffs: some will be better for some agents, and others will be better for other agents. It is generally not feasible to compute all equilibria [1], and even if it were, there would be no obvious way to select one to implement. It does not make sense for the agents to select an equilibrium without consulting one another: there is no reason that agent A X  X  part of one joint plan would be compatible with agent B X  X  part of another joint plan. Instead the agents must negotiate, computing and proposing equilibria until they find one which is acceptable to all parties.
 This paper describes a planning algorithm and a negotiation protocol which work together to ensure that the agents compute and select a subgame-perfect Nash equilibrium which is both approximately Pareto-optimal (that is, its value to any single agent cannot be improved very much without lowering the value to another another agent) and approximately fair (that is, near the so-called Nash bargaining point ). Neither the algorithm nor the protocol is guaranteed to work in all games; however, they are guaranteed correct when they are applicable, and applicability is easy to check. In addition, our experiments show that they work well in some realistic situations. Together, these properties of fairness, enforceability, and Pareto optimality form a strong solution concept for a stochastic game. The use of this definition is one characteristic that distinguishes our work from previous research: ours is the first efficient algorithm that we know of to use such a strong solution concept for stochastic games.
 Our planning algorithm performs dynamic programming on a set-based value function: for P players, at a state s , V  X  V ( s )  X  R P is an estimate of the value the players can achieve. We represent V ( s ) by sampling points on its convex hull. This representation is conservative , i.e., guarantees that we find a subset of the true V  X  ( s ). Based on the sampled points we can efficiently compute one-step backups by checking which joint actions are enforceable in an equilibrium.
 Our negotiation protocol is based on a multi-player version of Rubinstein X  X  bargaining game. Players together enumerate a set of equilibria, and then take turns proposing an equilibrium from the set. Until the players agree, the protocol ends with a small probability after each step and defaults to a low-payoff equilibrium; the fear of this outcome forces players to make reasonable offers. 2.1 STOCHASTIC GAMES A stochastic game represents a multi-agent planning problem in the same way that a Markov Decision Process [2] represents a single-agent planning problem. As in an MDP, transitions in a stochastic game depend on the current state and action. Unlike MDPs, the current (joint) action is a vector of individual actions, one for each player. More formally, a general-sum stochastic game G is a tuple ( S, s start ,P,A,T,R, X  ). S is a set of states, and s start  X  S actions. We deal with fully observable stochastic games with perfect monitoring, where all players can observe previous joint actions. T : S  X  A  X  P ( S ) is the transition function, where P ( S ) is the set of probability distributions over S . R : S  X  A  X  R P is the reward factor. Player p wants to maximize her discounted total value for the observed sequence for player p is a function  X  p : S  X  P ( A p ). A stationary joint policy is a vector of policies  X  =(  X  1 ,..., X  P ), one for each player. A nonstationary policy for player p is a function  X  p :(  X   X  t =0 ( S  X  A ) t  X  S )  X  P ( A p ) which takes a history of states and joint actions and produces a distribution over player p  X  X  actions; we can define a nonstationary joint policy analogously. For any nonstationary joint policy, there is a stationary policy that achieves the same value at every state [3].
 The value function V  X  p : S  X  R gives expected values for player p under joint policy  X  .The value after observing history h .) A vector V is feasible at state s if there is a  X  for which V  X  ( s )= V , and we will say that  X  achieves V .
 We will assume public randomization : the agents can sample from a desired joint action distribution in such a way that everyone can verify the outcome. If public randomization is not directly available, there are cryptographic protocols which can simulate it [4]. This assumption means that the set of feasible value vectors is convex, since we can roll a die at the first time step to choose from a set of feasible policies. 2.2 EQUILIBRIA While optimal policies for MDPs can be determined exactly via various algorithms such as linear programming [2], it isn X  X  clear what it means to find an optimal policy for a general sum stochastic game. So, rather than trying to determine a unique optimal policy, we will define a set of reasonable policies: the Pareto-dominant subgame-perfect Nash equilibria. A (possibly nonstationary) joint policy  X  is a Nash equilibrium if, for each individual player, playing the game. Nash equilibria can contain incredible threats , that is, threats which the agents have no intention of following through on. To remove this possibility, we can define the subgame-perfect Nash equilibria . A policy  X  is a subgame-perfect Nash equilibrium if it is a Nash equilibrium in every possible subgame: that is, if there is no incentive for any player to deviate after observing any history of joint actions.
 policy which is not Pareto dominated by any other policy is Pareto optimal . 2.3 RELATED WORK Littman and Stone [5] give an algorithm for finding Nash equilibria in two-player repeated games. Hansen et al. [6] show how to eliminate very-weakly-dominated strategies in par-tially observable stochastic games. Doraszelski and Judd [7] show how to compute Markov perfect equilibria in continuous-time stochastic games. The above papers use solution con-cepts much weaker than Pareto-dominant subgame-perfect equilibrium, and do not address negotiation and coordination. Perhaps the closest work to the current paper is by Braf-man and Tennenholtz [8]: they present learning algorithms which, in repeated self-play, find Pareto-dominant (but not subgame-perfect) Nash equilibria in matrix and stochastic games. By contrast, we consider a single play of our game, but allow  X  X heap talk X  beforehand. And, our protocol encourages arbitrary algorithms to agree on Pareto-dominant equilibria, while their result depends strongly on the self-play assumption. 2.3.1 FOLK THEOREMS In any game, each player can guarantee herself an expected discounted value regardless of what actions the other players takes. We call this value the safety value . Suppose that there is a stationary subgame-perfect equilibrium which achieves the safety value for both players; call this the safety equilibrium policy.
 Suppose that, in a repeated game, some stationary policy  X  is better for both players than the safety equilibrium policy. Then we can build a subgame-perfect equilibrium with the same payoff as  X  : start playing  X  , and if someone deviates, switch to the safety equilibrium policy. So long as  X  is sufficiently large, no rational player will want to deviate. This is the folk theorem for repeated games : any feasible value vector which is strictly better than the safety values corresponds to a subgame-perfect Nash equilibrium [9]. (The proof is slightly more complicated if there is no safety equilibrium policy, but the theorem holds for any repeated game.) There is also a folk theorem for general stochastic games [3]. This theorem, while useful, is not strong enough for our purposes: it only covers discount factors  X  which are so close to 1 that the players don X  X  care which state they wind up in after a possible deviation. In most practical stochastic games, discount factors this high are unreasonably patient. When  X  is significantly less than 1, the set of equilibrium vectors can change in strange ways as we change  X  [10]. Figure 1: Equilibria of a Rubinstein game with  X  =0 . 8. Shaded area shows feasible value player 1 moves first, left-hand circle when player 2 moves first. The Nash point is at . 2.3.2 RUBINSTEIN X  X  GAME Rubinstein [11] considered a game where two players divide a slice of pie. The first player offers a division x, 1  X  x to the second; the second player either accepts the division, or refuses and offers her own division 1  X  y, y . The game repeats until some player accepts an offer or until either player gives up. In the latter case neither player gets any pie. Rubinstein discount factor 0  X   X &lt; 1 and an appropriate time-independent utility function U p ( x )  X  0, then rational players will agree on a division near the so-called Nash bargaining point . This is the point which maximizes the product of the utilities that the players gain by cooperating, U ( x ) U 2 (1  X  x ). As  X   X  1, the equilibrium will approach the Nash point. See Fig. 1 for an illustration. For three or more players, a similar result holds where agents take turns proposing multi-way divisions of the pie [12]. See the technical report [13] for more detail on the multi-player version of Rubinstein X  X  game and the Nash bargaining point. The Rubinstein game implicitly assumes that the result of a failure to cooperate is known to all players: nobody gets any pie. The multi-player version of the game assumes in addition that giving one player a share of the pie doesn X  X  force us to give a share to any other player. Neither of these properties holds for general stochastic games. They are, however, easy to check, and often hold or can be made to hold for planning domains of interest.
 So, we will assume that the players have agreed beforehand on a subgame-perfect equilibrium  X  In addition, for games with three or more players, we will assume that each player can unilaterally reduce her own utility by any desired amount without affecting other players X  utilities.
 Given these assumptions, our protocol proceeds in two phases (pseudocode is given in the technical report [13]. In the first phase agents compute subgame-perfect equilibria and take turns revealing them. On an agent X  X  turn she either reveals an equilibrium or passes; if all agents pass consecutively, the protocol proceeds to the second phase. When an agent states a policy  X  , the other agents verify that  X  is a subgame-perfect equilibrium and calculate its payoff vector V  X  ( s start ); players who state non-equilibrium policies miss their turn. At the end of the first phase, suppose the players have revealed a set  X  of policies. Define is the set of feasible excess vectors.
 In the second phase, players take turns proposing points u  X  U along with policies or mixtures of policies in  X  that achieve them. After each proposal, all agents except the pro-poser decide whether to accept or reject. If everyone accepts, the proposal is implemented: everyone starts executing the agreed equilibrium.
 Otherwise, the players who accepted are removed from future negotiation and have their utilities fixed at the proposed levels. Fixing player p  X  X  utility at u p means that all future proposals must give p exactly u p . Invalid proposals cause the proposer to lose her turn. To achieve this, the proposal may require p to voluntarily lower her own utility; this requirement is enforced by the threat that all players will revert to  X  dis if p fails to act as required. If at some point we hit the chance of having the current round of communication end, all remaining players are assigned their disagreement values. The players execute the last proposed policy  X  (or  X  dis if there has been no valid proposal), and any player p for whom level. (Again, failure to do so results in all players reverting to  X  dis .) Under the above protocol, player X  X  preferences are the same as in a Rubinstein game with utility set U : because we have assumed that negotiation ends with probability after each message, agreeing on u after t additional steps is exactly as good as agreeing on u (1  X  ) t now. So with sufficiently small, the Rubinstein or Krishna-Serrano results show that rational players will agree on a vector u  X  U which is close to the Nash point argmax u  X  U  X  p u p . In order to use the protocol of Sec. 3 for bargaining in a stochastic game, the players must be able to compute some subgame-perfect equilibria. Computing equilibria is a hard problem, so we cannot expect real agents to find the entire set of equilibria. Fortunately, each player will want to find the equilibria which are most advantageous to herself to influence the negotiation process in her favor. But equilibria which offer other players reasonably high reward have a higher chance of being accepted in negotiation. So, self interest will naturally distribute the computational burden among all the players.
 In this section we describe an efficient dynamic-programming algorithm for computing equi-libria. The algorithm takes some low-payoff equilibria as input and (usually) outputs higher-payoff equilibria. It is based on the intuition that we can use low-payoff equilibria as en-forcement tools: by threatening to switch to an equilibrium that has low value to player p , we can deter p from deviating from a cooperative policy.
 In more detail, we will assume that we are given P different equilibria  X  pun 1 ,..., X  pun P ;we the only equilibrium we know; or, we can use any other equilibrium policies that we happen to have discovered. The algorithm will be most effective when the value of  X  pun p to player p is as low as possible in all states.
 We will then search for cooperative policies that we can enforce with the given threats  X  pun p . We will first present an algorithm which pretends that we can efficiently take direct sums and convex hulls of arbitrary sets. This algorithm is impractical, but finds all enforceable value vectors. We will then turn it into an approximate algorithm which uses finite data structures to represent the set-valued variables. As we allow more and more storage for each set, the approximate algorithm will approach the exact one; and in any case the result will be a set of equilibria which the agents can execute. 4.1 THE EXACT ALGORITHM Our algorithm maintains a set of value vectors V ( s ) for each state s . It initializes V ( s )to a set which we know contains the value vectors for all equilibrium policies. It then refines V by dynamic programming: it repeatedly attempts to improve the set of values at each state by backing up all of the joint actions, excluding joint actions from which some agent has an incentive to deviate.
 value of  X  if s were the start state.) We also need the values of the punishment policies for Initialization for s  X  S
V ( s )  X  X  V | V dis p ( s )  X  V p  X  R max / (1  X   X  ) } end Repeat until converged for iteration  X  1 , 2 ,... for s  X  S end end their corresponding players, V pun p ( s )  X  V to be the value to player p of playing joint action a from state s and then following  X  pun p forever after.
 From the above Q dev p values we can compute player p  X  X  value for deviating from an equilib-rium which recommends action a in state s :itis Q dev p ( s, a ) for the best possible deviation a , since p will get the one-step payoff for a but be punished by the rest of the players starting on the following time step. That is, V p ( s, a ) is the value we must achieve for player p in state s if we are planning to recommend action a and punish deviations with  X  pun p : if we do not achieve this value, player p would rather deviate and be punished.
 Our algorithm is shown in Fig. 2. After k iterations, each vector in V ( s ) corresponds to a k -step policy in which no agent ever has an incentive to deviate. In the k + 1st iteration, the first assignment to Q ( s, a ) computes the value of performing action a followed by any k -step policy. The second assignment throws out the pairs ( a,  X  ) for which some agent would want to deviate from a given that the agents plan to follow  X  in the future. And the convex hull accounts for the fact that, on reaching state s , we can select an action a and future policy  X  at random from the feasible pairs. 2 Proofs of convergence and correctness of the exact algorithm are in the technical report [13].
 Of course, we cannot actually implement the algorithm of Fig. 2, since it requires variables whose values are convex sets of vectors. But, we can approximate V ( s ) by choosing a finite V ( s ) is then approximated by the convex hull of { V ( s, w ) | w  X  W } .If W samples the P -dimensional unit hypersphere densely enough, the maximum possible approximation error will be small. (In practice, each agent will probably want to pick W differently, to focus her computation on policies in the portion of the Pareto frontier where her own utility is relatively high.) As | W | increases, the error introduced at each step will go to zero. The approximate algorithm is given in more detail in the technical report [13]. Figure 3: Execution traces for our motion planning example. Left and Center: with 2 witness vectors , the agents randomize between two selfish paths. Right: with 4 X 32 witnesses, the agents find a cooperative path. Steps where either player gets a goal are marked with  X  . Figure 4: Supply chain management problem. In the left figure, Player 1 is about to deliver part D to the shop, while player 2 is at the warehouse which sells B . The right figure shows the tradeoff between accuracy and computation time. The solid curve is the Pareto frontier for s start , as computed using 8 witnesses per state. The dashed and dotted lines were computed using 2 and 4 witnesses, respectively. Dots indicate com-puted value vectors;  X  marks indicate the Nash points. We tested our value iteration algorithm and negotiation procedure on two robotic planning domains: a joint motion planning problem and a supply-chain management problem.
 In our motion planning problem (Fig. 3), two players together control a two-wheeled robot, with each player picking the rotational velocity for one wheel. Each player has a list of goal landmarks which she wants to cycle through, but the two players can have different lists of goals. We discretized states based on X, Y,  X  and the current goals, and discretized We discretized time at  X  t =1 s , and set  X  =0 . 99. For both the disagreement policy and all punishment policies, we used  X  X lways stop , X  since by keeping her wheel stopped either player can prevent the robot from moving. Planning took a few hours of wall clock time on a desktop workstation for 32 witnesses per state.
 Based on the planner X  X  output, we ran our negotiation protocol to select an equilibrium. Fig. 3 shows the results: with limited computation the players pick two selfish paths and randomize equally between them, while with more computation they find the cooperative path. Our experiments also showed that limiting the computation available to one player allows the unrestricted player to reveal only some of the equilibria she knows about, tilting the outcome of the negotiation in her favor (see the technical report [13] for details). For our second experiment we examined a more realistic supply-chain problem. Here each player is a parts supplier competing for the business of an engine manufacturer. The man-ufacturer doesn X  X  store items and will only pay for parts which can be used immediately. Each player controls a truck which moves parts from warehouses to the assembly shop; she pays for parts when she picks them up, and receives payment on delivery. Each player gets parts from different locations at different prices and no one player can provide all of the parts the manufacturer needs.
 Each player X  X  truck can be at six locations along a line: four warehouse locations (each of which provides a different type of part), one empty location, and the assembly shop. Building an engine requires five parts, delivered in the order A, { B, C } ,D,E (parts B and C can arrive in either order). After E , the manufacturer needs A again. Players can move left or right along the line at a small cost, or wait for free. They can also buy parts at a warehouse (dropping any previous cargo), or sell their cargo if they are at the shop and the manufacturer wants it. Each player can only carry one part at a time and only one player can make a delivery at a time. Finally, any player can retire and sell her truck; in this case the game ends and all players get the value of their truck plus any cargo. The disagreement various numbers of witnesses. The more witnesses we use, the more accurately we represent the frontier, and the closer our final policy is to the true Nash point.
 All of the policies computed are  X  X ntelligent X  and  X  X ooperative X : a human observer would not see obvious ways to improve them, and in fact would say that they look similar despite their differing payoffs. Players coordinate their motions, so that one player will drive out to buy part E while the other delivers part D . They sit idle only in order to delay the purchase of a part which would otherwise be delivered too soon. Real-world planning problems involve negotiation among multiple agents with varying goals. To take all agents incentives into account, the agents should find and agree on Pareto-dominant subgame-perfect Nash equilibria. For this purpose, we presented efficient planning and negotiation algorithms for general-sum stochastic games, and tested them on two robotic planning problems.

