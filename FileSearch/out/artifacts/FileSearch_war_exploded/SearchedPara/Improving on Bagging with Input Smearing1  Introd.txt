 Ensembles of multiple prediction models, generated by repeatedly applying a base learning algorithm, have been shown to often improve predictive perfor-mance when compared to applying the base learning algorithm by itself. En-semble generation methods differ in the processes used for generating multiple different base models from the same set of data. One possibility is to modify the input to the base learner in different ways so that different models are generated. This can be done by resampling or reweighting instances [1, 2], by sampling from the set of attributes [3], by generating artificial data [4], or by flipping the class labels [5]. A different possibility is to modify the base learner so that different models can be generated from the same data. This is typically done by turning the base learner into a randomized versio n of itself, e.g. by choosing randomly among the best splits at each node of a decision tree [6]. This paper investigates an ensemble learning method that belongs to the former category. We call it  X  X nput smearing X  because we randomly modify the attribute values of an in-stance, thus smearing it out in instance space. We show that, when combined with bagging, this method can improve on using bagging alone, if the amount of smearing is chosen appropriately for each dataset. We show that this can be reliably achieved using internal cross-validation, and present results for classifi-cation and regression problems.

The motivation for using input smearing is that it may be possible to increase the diversity of the ensemble by modifying the input even more than bagging does. The aim of ensemble generation is a set of classifiers such that they are si-multaneously as different to each other as possible while remaining as accurate as possible when viewed individually. Independence X  X r  X  X iversity X  X  X s important because ensemble learning can only improve on individual classifiers when their errors are not correlated. Obviously these two aims X  X aximum accuracy of the individual predictors and minimum correlation of erroneous predictions X  X onflict with each other, as two perfect classifie rs would be rather similar, and two max-imally different classifiers could not at the same time both be very accurate. This necessary balance between diversit y and accuracy has been investigated in various papers including [7], which a mong other findings reported that bagged trees are usually much more uniform than boosted trees. But it was also found that increasing levels of noise lead to much more diverse bagged trees, and that bagging starts to outperform boosted trees for high noise levels.

Commonly the attribute values of the examples are not modified in any way in the ensemble generation process. One ex ception to this  X  X ule X  is called  X  X utput smearing X  [5], which modifies the class labels of examples by adding a controlled amount of noise. In this paper we inves tigate the complimentary process of ap-plying  X  X mearing X  not to the output variable, but to the input variables. Initial experiments showed that smearing alone could not consistently improve on bag-ging. This lead us to the idea of combining smearing and bagging, by smearing the subsamples involved in the bagging process. The amount of smearing en-ables us to control the diversity in the ensemble, and more smearing increases the diversity compared to bagging alone. However, more smearing also means that the individual ensemble members become less accurate. Our results show that cross-validation can be used to reliably determine an appropriate amount of smearing.

This paper is structured as follows. In Section 2 we discuss previous work on using artificial data in machine learning and explain the process of  X  X nput smearing X  in detail. Section 3 presents our empirical results on classification and regression datasets, and Section 4 disc ussed related work. Section 5 summarizes our findings and points out directions for future work. One way of viewing input smearing is that artificial examples are generated to aid the learning process. Generating meaningful artificial examples may seem straightforward, but it is actually not that simple. The main issue is the problem of generating meaningful class values or labels for fully artificially generated examples. Theoretically, if the full joint distribution of all attributes including the class attribute were known, examples could simply be drawn according to this full joint distribution, and their class labels would automatically be meaningful. Unfortunately this distribution is not available for practical learning problems.
This  X  X abelling X  problem is the most likely explanation as to why artificially generated training examples are rarely used. One exception is the approach re-ported in [8]. This work is actually not concerned with improving the predictive accuracy of an ensemble, but instead tries to generate a single tree with similar performance to an ensemble generated by an ensemble learning method. The aim is to have a comprehensible model with a similar predictive performance as the original ensemble. The method generates artificial examples and uses the induced ensemble to label the new examples. It has been shown that large sets of artificial examples can lead to a large single tree capable of approximating the predictive behaviour of the original ensemble.

Another exception is the work presented in [9], which investigates the prob-lem of very skewed class distributions in inductive learning. One common idea is oversampling of the minority class to even out the class distribution, and [9] takes that one step further by generating new artificial examples for the minority class. This is done by randomly selecting a pair of examples from the minority class, and then choosing an arbitrary point along the line connecting the original pair. Furthermore the method makes sure that there is no example from the ma-jority class closer to the new point than any of the minority examples. The main drawback of this method is that it is very conservative, and that it relies on near-est neighbour computation, which is of questionable value in higher-dimensional settings. In the case of highly skewed class distributions such conservativeness might be appropriate, but in more general settings it is rather limiting.
Finally, the Decorate algorithm [4] creates artificial examples adaptively as an ensemble of classifiers is being built. It assigns labels to these examples by choosing those labels that the existing ensemble is least likely to predict. It is currently unclear why this method works well in practice [4].

We have chosen a very simple method for generating artificial data to improve ensemble learning. Our method addresses the labelling problem in a similar fashion as what has been done for skewed class distributions, taking the original data as the starting point. However, we then simply modify the attribute values of a chosen instance by adding random attribute noise. The method we present here combines bagging with this modification for generating artificial data. More specifically, as in bagging, training examples are drawn with replacement from the original training set until we have a new training set that has the same size as the original data. The next step is new: instead of using this new dataset as the input for the base learning algorithm, we modify it further by perturbing the attribute values of all instances by a small amount (excluding the class attribute). This perturbed data is then fed into the base learning algorithm to generate one ensemble member. The same process is r epeated with differ ent random number seeds to generate differen t datasets, and thus different ensemble members.
This method is very simple and applicable to both classification and regres-sion problems (because the dependent variable is not modified), but we have not yet specified how exactly the modification of the original instances is per-formed. In this paper we make one simplification: we restrict our attention to datasets with numeric attributes. Although the process of input smearing can be applied to nominal data as well (by changing a given attribute value with a certain probability to a different value) it can be more naturally applied with numeric attributes because they imply a notion of distance. To modify the nu-meric attribute values of an instance we simply add Gaussian noise to them. We take the variance of an attribute into account by scaling the amount of noise based on this variance (using Gaussian noise with the same variance for every attribute would obviously not work, given that attributes in practical datasets are often on different scales). More specifically, we transform an attribute value a original into a smeared value where  X  a is the estimated global standard deviation for attribute a original ,and p is a user-specifiable parameter that determines the amount of noise to add. The original class value is left intact.

Usually the value of the smearing parameter is greater than zero but the optimum value depends on the data. Cross-validation is an obvious method for finding an appropriate value in a purely data-dependent fashion, and as we will see in the next section, it chooses quite different values depending on the dataset. In the experiments reported below we employed internal cross-validation in conjunction with a simple grid search, evaluating different values for p in a range of values that is explored in equal-size steps. As it turns out, there are datasets where no smearing ( p = 0) is required to achieve maximum accuracy.
Another view of input smearing is that we employ a kernel density estimate of the data, placing a Gaussian kernel on every training instance, and then sample from this estimate of the joint distribution of the attribute values. We choose an appropriate kernel width by evaluating the cross-validated accuracy of the resulting ensemble (and combine the smearing process with bagging) but an alternative approach would be to first fit a kernel density estimate to the data by some regularized likelihood method, and then use the resulting kernel widths to generate a smeared ensemble. A potential drawback of our method is that the amount of noise is fixed for every attribute (although it is adjusted based on the attributes X  scales). It may be that performance can be improved further by introducing a smearing parameter for every attribute and tuning those smearing parameters individually. Using an approach based on kernel density estimation may make this computationally feasible.

Note that, compared to using bagging alone, the computational complexity remains unchanged. Modifying the attribute values can be done in time linear in the number of attributes and instances. The cross-validation-based grid search for the optimal smearing parameter increases the runtime by a large constant factor but it may be possible to improve on this using a more sophisticated search strategy in place of grid search.

Figure 1 shows the pseudo code for building an ensemble using input smear-ing. The process for making a prediction (as well as the type of base learner employed) depends on whether we want to tackle a regression problem or a classification problem. In the case of re gression we simply average the predicted numeric values from the base models to derive an ensemble prediction. In the case of classification, we average the class probability estimates obtained from the base models, and predict the class for which the average probability is max-imum. (In the experiments reported in the next section we use exactly the same method for bagging.) In this section we conduct experiments on both classification and regression problems to compare input smearing to bagging. As a baseline we also present results for the underlying base learning algorithm when used to produce a single model. The main parameter needed for input smearing, the noise threshold p , is set automatically using cross-validation, as explained above. We will see that this automated process reliably chooses appropriate values. Consequently input smearing competes well with bagging. 3.1 Classification Our comparison is based on 22 classification problems from the UCI reposi-tory [10]. We selected those problems tha t exhibit only numeric attributes. Miss-ing values (present in one attribute of one of the 22 datasets, the breast-w data) are not modified by our implementation of smearing.

Input smearing was applied in conjunction with unpruned decision trees built using the fast REPTree decision tree le arner in Weka. REPTree is a simple tree learner that uses the information gain heuristic to choose an attribute and a binary split on numeric attributes. It avoids repeated re-sorting at the nodes of the tree, and is thus faster than C4.5. We performed ten iterations to build ten ensemble members. Internal 5-fold cross-validation was used to choose an appropriate parameter value for the smearing parameter p for each training set. To identify a good parameter value we used a simple grid search that evaluated values 0, 0.05, 0.1, 0.15, 0.2, 0.25, and 0.3. This automated parameter estimation adds a large computational overhead, but prevents the user from bad choices, and might also provide valuable insights into both the data as well as the example generation process.

Table 1 lists the estimated classificatio n accuracy in percent correct, obtained as averages over 100 runs of the stratified hold-out method. In each run 90% of the data was used for training and 10% was used for testing. The corrected resampled t -test [11] was used to perform pairwise comparison between algo-rithms.

Apart from the results for input smearing, the table also lists results for bagging, unpruned decision trees genera ted using REPTree, and pruned C4.5 trees. It also shows the average parameter value chosen by the internal cross-validation, and the standard deviation for each of the statistics across the 100 runs. Bagging was applied in conjunction with the same base learner and the same number of iterations as input smearing.

Analyzing the results of Table 1, we see that  X  X nput smearing X  can improve the predictive accuracy of single trees for about half of all the datasets, and also significantly outperforms bagging four times. More importantly, it never performs significantly worse than any of the other algorithms. The average values chosen for p varyfrom0upto0 . 29. Given that the latter value is quite close to the upper boundary of the range that we searched in our experiments, it may be possible that larger values would result in further improvements for the datasets where such a large value was chosen. For all datasets except one a non-zero parameter value is chosen, with spambase being the sole exception. We can only speculate why smearing does not work for this dataset. Most likely the noise generation process is not appropriate for this dataset, which consists solely of counts of word occurrences. These are non-negative and generally follow a power law [12]. A more specialized distribution like the Poisson distribution may be more appropriate for smearing in this case. Alternatively, the input variables could also be preprocessed by a logarithmic transformation, which is common practice in statistics for dealing with counts.

One method for analysing the behaviour of a modelling technique is the so-called bias-variance decomposition (see e.g. [13]), which tries to explain the total prediction error as the sum of three different sources of error: bias (i.e. how close is the average model to the actual function?), variance (i.e. how much do the models X  guesses  X  X ounce around X ?), and intrinsic noise (the Bayes error).
Using the specific approach described in [13], a bias-variance decomposition was computed for all the classification datasets used above for both input smear-ing and bagging. We would expect that i nput smearing exhibits a higher bias than bagging on average, as it modifies the input distribution of all attributes. To verify this hypothesis, the relative contribution of bias compared to variance was computed for both methods on each dataset. More specifically, we computed 3.2 Regression Classification is not the only application of input smearing. In the following we investigate its performance when applied in conjunction with a state-of-the-art tree learner for regression problems. This comparison is based on a collection of 23 regression problems [14] that are routinely used as benchmarks for evaluating regression algorithms.

We employed the same evaluation framework as in the classification case: ensembles are of size ten and random train/test splits of 90%/10% are repeated 100 times (in this case without applying stratification, of course). Performance is measured based on the root relative squared error. A value of zero would indicate perfect prediction, and values larger than 100 indicate performance worse than simply predicting the global mean of the class-values obtained from the training data. Unpruned M5 model trees [15], generated using the M5 X  model tree learner in Weka [16], were used as the base lear ner for input smearing and bagging, and we compare to single unpruned and pruned M5 model trees. Again, the noise parameter p was determined automatically by internal five-fold cross-validation using a grid search on the values 0, 0.05, 0.1, 0.15, 0.2, 0.25, and 0.3.
Again, analyzing the results of Table 2, we see that input smearing almost always improves prediction over single mo del trees. However, it is significantly worse than a single pruned tree on three datasets. Compared to bagging, signif-icant improvements are achieved 39% of the time, with only one significant loss. As with classification, the average smearing parameter values chosen by cross-validation are well below 0 . 3 in most cases, except for one dataset (2dplanes), where an even larger parameter value m ay have been chosen if it had been avail-able. Again there is one dataset where zero is chosen consistently. As we are not familiar with the actual meaning of the attributes in this dataset (ailerons), we cannot make such strong claims as for the spambase dataset, but at least one third of all attributes in this dataset again appear to be based on counts, and another third of all attributes is almost constant, i.e. clearly not normally distributed either. Inspecting the attribute distributions for the only other two datasets with smearing parameter values close to 0 (house-8L and triazines) reveals that in both datasets a majority of attributes again is not normally dis-tributed. In this section we discuss related work but restrict our attention to ensemble generation methods. We do not repeat the discussion of methods that have already been discussed in Section 2. In t erms of ensemble generating methods we only list and discuss methods that modify the data in some way.  X  Bagging [1] has its origin in bootstrap sampling in statistics, which produces  X  Dagging [17] is an alternative to baggi ng that combines classifiers induced  X  Output smearing [5] adds a controlled amount of noise to the output or  X  Random feature subsets [3, 18] work particularly well for so-called stable al-We have described a new method for ensemble generation, called input smear-ing, that works by sampling from a kernel density estimator of the underlying distribution to form new training sets, in addition to resampling from the data itself like in bagging. Our experimental results show that it is possible to obtain significant improvements in predictive accuracy when applying input smearing instead of bagging (which can be viewed a s a special case of input smearing in our implementation). Our results also show th at it is possible to use cross-validation to determine an appropriate amount of smearing on a per-dataset basis.
Input smearing using Gaussian noise is not necessarily the best choice. An avenue for future work is to investigate the effect of other distributions in input smearing, and to choose an appropriate distribution based on the data. Such a more sophisticated approach should also make it possible to generalize input smearing to other attribute types and structured input.

