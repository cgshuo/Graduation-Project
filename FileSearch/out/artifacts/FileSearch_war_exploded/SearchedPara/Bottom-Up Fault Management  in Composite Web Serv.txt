 Service-oriented architecture (SOA) has recently emerged as a promising approach for application integration [1,14]. It utilizes services (commonly Web services) as the building blocks for developing software systems distributed within and across organi-zations. The primary value of SOA is the ability to (1) reuse pre-developed, autono-mous, and independently provided resources (e.g., legacy applications, sensors, data-bases, storage devices, COTS products) as Web services and (2) combine pre-existing services, called participants , into higher level services, called composite services , which perform more complex functions [9,15]. unavoidable faults during their lifetime. The relationship between a fault and failure is given in ISO/CD 10303-226 document, where a fault is defined as an abnormal condition or defect at the component, equipment, or sub-system level which may lead to a failure. In their seminal work, Avizienis et al. [2] state that faults cycle between paper, we focus on faults that propagate from participants to composite services. We refer to these faults as bottom-up . Two examples of bottom-up faults are (1) a shut-down scheduled by a participant X  X  provider for maintenance; and (2) an update to a participant X  X  policy (e.g., new message parameters added to a WSDL specification) that may affect the way that participant is consumed. Hence, composite services must rapidly detect and handle faults in their participants to avoid run-time failures and maintain consistency. they are either guaranteed message delivery or notified if a message was not delivered (e.g., because of a server unavailability). In the latter case, composite services be-time that fault occurred. This may decrease the availability of composite services. Besides, users X  requests are pending as long as the composite service did not recover from the fault (e.g., by replacing the faulty participant with an equivalent one). This calls for a framework in which composite services are able to detect and handle bot-tom-up faults as soon as those faults occur in their participants. posite services. The proposed framework uses soft-state signaling to propagate faults from participants to composite services. Soft state denotes a type of protocols where state (e.g., whether a server is alive) is constantly refreshed by periodic messages; state which is not refreshed in time expires [8]. This is in contrast to hard-state where installed state remains installed unless explicitly removed by the receipt of a state-teardown message. Advantages of the soft-state approach include implicit error re-covery and easier fault management, resulting in high availability [16]. Soft state was introduced in the late 1980s and has been widely used in various Internet protocols soft-state for fault management in composite services. The major contributions of this paper are summarized below:  X  We introduce a bottom-up fault model for composite services. The model in- X  We propose a soft-state protocol for bottom-up fault propagation.  X  We conduct experiments to assess the performance of the proposed framework. The rest of this paper is organized as follows. In Section 2, we describe the bottom-propagation. In Section 4, we present experiments to assess the performance of the proposed approach. In Section 5, we give a brief survey of related work. We finally provide concluding remarks in Section 6. In this section, we describe our model for bottom-up fault management. We first provide a categorization of bottom-up faults. Then, we define the notion of partici-pant X  X  state. Finally, we introduce a peer-to-peer topology for bottom-up fault management. 2.1 Bottom-up Fault Taxonomy A fault management approach must refer to a taxonomy that describes the different types of faults that composite services are ex pected to be able to manage. We identify communication system is assumed to be failure-free : there is no creation, alteration, loss, or duplication of messages. However, node faults are still possible. A node fault occurs if the servers (e.g., application serv er, Web server) hosting a participant are out of action. Logical faults are initiated by service providers; this is in contrast to physi-cal faults which are out of service providers X  control. We categorize logical faults as status change , participation refusal , and policy change . status of its service. The status may be changed through freeze or stop. In the freeze fault, providers shut down their services for limited time periods (e.g., for mainte-providers make their services permanently unavailable (e.g., a company going out-of-business). composition. The way participation decision is made varies from a Web service to another. We give below four techniques on how such decisions could be made: In the rest, we assume the existence of a pre-defined function Agreed2Join() used by services to decide whether they are willing to participate in compositions. Each ser-vice may provide its own definition and implementation of the Agreed2Join() func-tion. However, the way is Agreed2Join() defined is out of the scope of this paper. a broad definition of policy, encompassing all requirements under which a service may be consumed. We adopt the categorization proposed in [11] considering policies as either vertical or horizontal. The ver tical category refers to application domain-dependent policies (e.g., shipping policy in business-to-business e-commerce). The horizontal category refers to policies that are applicable across domains. It is com-posed of three sub-categories: functional, non-functional, and valued-added. Func-tional category describes the operational features of a Web service (e.g., in WSDL [1,14], OWL-S [10]). Non-functional category relates to Quality of Service metrics. The value-added category brings "better" environments for Web service interactions. It refers to a set of specifications for supporting optional (but important) requirements for the service (e.g., security, privacy, negotiation, conversation). Changes in the with WS i . Hence, they should be considered as logical faults. For instance, CS j invo-cations to WS i may lead to run-time failures if WS i provider changed the input mes-sage required by WS i (e.g., new message parameters added, changes made to data types). 2.2 State of a Service Soft-state signaling enables the propagation of bottom-up faults from participants to composite services. The main idea of this class of signaling is that the state of each participant is periodically sent to the composite service. The composite service will then use the received state to determine whether there was any physical or logical fault in the participant. Several questions need to be tackled when designing a soft-will give answers to these questions in the rest of this section and paper. Physical faults and status changes are detected by composite services in an implicit manner; if a node fault occurs or a participant is stopped/frozen, then the composite service will not receive a state from that par ticipant. The participation refusal fault is explicitly communicated by participants if they are not willing to be part of a compo-sition. track of policy changes, each participant WS i maintains a data structure called State i ChangeStatus is equal to True if policy changes have been made to WS i . Several changes may occur in WS i during a time period; details about these changes are stored defined by a couple (C,S) where C is the category of the policy and S is the scope of the change. The initial values of ChangeStatus and ChangeDetails are False and  X  , respectively. If a change (C,S) is detected on WS i , State i .ChangeStatus is set to True and (C,S) is added to State i .ChangeDetails. The content of State i is periodically communicated to composite services. If a composite service does not receive State i after a certain period of time, then it assume s a physical fault or status change in WS i . Otherwise, the composite service reads State i to find out about the changes made to WS i . mentioned in Section 2.1, it refers to functional, non-functional, value-added, or do-main (i.e., vertical) categories. Policie s are specified in XML-based Web service languages/standards (e.g., WSDL [1], WS-Security [14]). The scope of a change defines the subject to which that change was applied. It includes details about (i) the location of the modified policy specification and (ii) the element that has been up-dated within that specification. The specification location is given by the URI of the XML file that stores the specification. The updated element is identified by the XPath lowing WSDL file located in  X  X ttp://www.ws.com/sq.wsdl X : WSDL document. The category and scope of the change are defined as follows: ticipant WS i . Definition: The state, denoted State i , of a participant WS i is defined by (ChangeS-tatus,ChangeDetails) where:  X  ChangeStatus = True  X  changes have been made to WS  X  ChangeDetails = {(C,S) / C and S are the category and scope of a change in  X  Initially do: State  X  At the occurrence of a change (C,S) in WS 2.3 Fault Coordinators In the proposed framework, fault management is a collaborative process between architectural modules called fault coordinators . Each Web service (participant or composite) has a coordinator associated to it. This peer-to-peer topology distributes control and externalizes fault management , hence creating a clear separation between the business logic of the services and fault management tasks. state receivers (SS-R). Each participant (resp., composite service) has a sender (resp., receiver) attached to it. A sender SS-S i maintains the State i data structure. To keep tached to SS-S i ) participates in CS j (attached to SS-R j ) then SS-R j  X  Receivers(SS-S i ). period is determined by the  X  SSS timer maintained by SS-S i . A receiver SS-R j main-tains two data structures: Senders(SS-R j ) and  X  SSR . Senders(SS-R j ) is the set of send-SS-S i  X  Senders(SS-R j ).  X  SSR is a timer used by SS-R j to process Refresh() messages received from its senders. propagation , and fault reaction . The sequence diagram in Fig. 4 depicts the relation-ship between these tasks:  X  Fault detection : SS-S  X  Fault propagation : SS-S  X  Fault Reaction : Finally, SS-Rj and/or CSj execute appropriate measures to react to In this section, we describe the algorithms executed by senders and receivers for propagating bottom-up faults. We assume that WS i (with SS-S i as attached sender) participates in CS j (with SS-R j as attached receiver). As mentioned in Section 2.1, we assume that there is no creation, alteration, loss, or duplication of messages. The propagation protocol adapts the well-know soft-state signaling described in [8,16] to service-oriented environments. It enables the propagation of participation refusal and policy changes faults to receivers. Physical faults and status changes are implic-itly propagated to receivers if the latter do not get Refresh() messages from faulty senders. 3.1 Soft-State Sender Algorithm Table 1 gives the algorithm executed by SS-S i . SS-S i receives two types of messages Agreed2Join() function to figure out whether WS i is willing to participate in CS j (see Section 2.1). If Agree2Join(SS-R j ) returns False, SS-S i sends the Decision2Join(SS-S ,False) message to SS-R j . Otherwise, SS-S i adds SS-R j to its receivers. If SS-R j is S sends its decision to SS-R j through the Decision2Join(SS-S i ,True) message. At any time, SS-S i may receive a Leave() message from SS-R j (lines 11-13). This message indicates that CS j is no longer using WS i as a participant. In this case, SS-S i removes SS-R j from its receivers. receivers at the end of  X  SSS cycle includes all changes that have occurred during that cycle. At the end each period (denoted by  X  SSS timer), SS-S i sends a Refresh() message to each one of its receivers (lines 18-23). This message includes State i as a parameter, hence notifying SS-R j about all policy changes that occurred in WS i during the last  X  SSS period. SS-S i then reinitializes State i and restarts its  X  SSS timer. 3.2 Soft-State Receiver Algorithm The aim of SS-R j protocol is to detect faults in senders. For that purpose, SS-R j main-tains a local table called SR-Table j . SR-Table j allows SS-R j to keep track of Refresh() messages transmitted by senders. It contains an entry for each SS-S i that belongs to Senders(SS-R j ). Each entry contains two columns: A temporary node failure in SS-S i may prevent SS-S i from sending Refresh() to SS-R j during a  X  SSR cycle. In this case, SS-R j may want to give SS-S i a second chance for variable (positive integer) Max-Retry j . If SS-R j does not receive Refresh() from SS-S i Max-Retry j is set by CS j composer and may vary from a composite service to another. The smaller is Max-Retry j , the more pessimistic is CS j composer about the occurrence of faults in participants. two types of messages from SS-S i : Decision2Join() and Refresh(). Table 2 gives the algorithm executed by SS-R j . Whenever a new participant WS i is added to CS j , SS-R j SS-R j sends a Leave(SS-R j ) message to SS-S i and removes SS-S i entry from SR-Table j (lines 4-7). At the reception of Decision2Join(SS-S i ,True), SS-R j adds SS-S i to the list of senders (lines 8-16). It also creates a new entry for SS-S i in SR-Table j and initializes the Refreshed and Retry columns of that entry to False and 0, respectively. Decision2Join(SS-S i ,False), SS-R j calls the React() procedure to pro cess the participa-tion refusal fault issued by SS-S i (see Section 2.3).  X  SSR timer (lines 21-32), SS-R j checks if it received Refresh() from each of its senders. If SS-R j received Refresh() from SS-S i , it re-initializes the Refreshed and Retry increments SR-Table j [SS-R j ,Retry] If SR-Table j [SS-S i ,Retry] equals Max-Retry j (i.e., cles), SS-R j assumes a physical (node) fault in SS-S i and hence, calls the React() pro-cedure to process that fault. SS-R j finally restarts its  X  SSR timer. 3.3 Example Let us consider a Web service WS 3 (with a soft-state sender SS-S 3 ) that participates in spectively). We assume that  X  SSR1 =  X  SSR2 = 2  X   X  SSS3 . Fig 5 depicts the interactions between SS-S 3 and SS-R 1 /SS-R 2 . State 3 . SS-R 1 and SS-R 2 process those changes by calling their React() procedure at fresh() sent by SS-S 3 . At this same time, SS-S 3 sends Refresh() to both receivers with the parameters (False,  X  ) since no changes have been detected in the second SS-S 3 tively. At t 33 , State 3 .ChangeStatus equals True and State 3 .ChangeDetails contains SS-R 1 and SS-R 2 note the reception of the Refresh() sent by SS-S 3 . At times t 5 and t 6 , changes have been detected in the corresponding SS-S 3 cycle. At t 7 , SS-R 1 and SS-R 2 note the reception of the Refresh() sent by SS-S 3 . Let us now assume a server failure receive Refresh() from SS-S 3 during the last SS-R 2 cycle. If Max-Retry 2 is equal to 1, SS-R 1 and SS-R 2 conclude that SS-S 3 failed and hence call the React() function. We conducted experiments to assess the different parameters that may impact the performance of the proposed protocol. We used Microsoft Windows Server 2003 (operating system), Microsoft Visual Studio 8 (development kit), UDDI Server, IIS Server, and SQL Server. We ran our experiments on Intel(R) processor (1500MHz) and 512MB of RAM. Soft-state senders and receivers have been developed in C#. We created twenty (20) receivers and fifty (50) senders, and registered them in UDDI. Each receiver has ten (10) senders randomly selected among the existing senders. In following two parameters: fault propagation time and false faults . 
Fault propagation time is the first performance parameter we analyze in our study time it took to the receiver to detect a fault in its senders). Fig. 6 compares the aver-out of 10) of participants within a composite service failed. We focused on physical node faults; these are created by physically stopping the services corresponding to propagation time. in their senders. Fig. 7 depicts the relationship between false faults and timer differ-bigger is  X  SSS (compared to  X  SSR ), the larger is the number of false faults. These faults correspond to cases where Refresh() messages are sent after the end of the corre-sponding  X  SSR cycles. Mechanisms that support fault management in software systems have been around for a long time. Workflow has traditionally been used to deal with faults in business processes. Faults in workflows have usually been modeled as exceptions. SOAs consider faults as the rule, and any solution to fault management would need to treat them as such. Traditional software engineering solutions for fault management (e.g., exceptions and run-time assertion checking) have hard-coded, internal, and applica-adaptation logic throughout the application, making it costly to modify and maintain [4]. These factors have actuated research dealing with the concept of self-healing systems. A comprehensive survey of major self-healing software engineering approaches is presented in [5]. However, such approaches focus on  X  X raditional X  applications not service-oriented. The peculiarity of faults, interaction models, and architectural style in SOAs as well as the autonomy, distribution, and heterogeneity of services make these approaches difficult to apply in SOAs. 
Current techniques for coping with faults in SOAs allow developers to include constructs in their service specifications (e.g., fault elements in SOAP and WSDL, exception handling in BPEL). Such techniques are static, ad hoc, and make service design complex [13]. Efforts have recently been made to add self-healing capabilities to SOAs (e.g., [6]). However, these efforts mostly focus on monitoring service level agreements and quality of service. In this paper, we proposed a soft-state approach for managing bottom-up faults in composite services. The approach includes techniques for fault detection, propaga-tion, and reaction. We then focused on describing a fault propagation protocol. The protocol inherits the advantages of the soft -state concept: implicit error recovery and easier fault management resulting in high availability. A future extension of our pro-tocol is to combine soft-state with hard-state signaling. For instance, if WS i provider is scheduling a shut-down, SS-S i sends an explicit Shutdown() message to SS-R j ; this allows SS-R j to differentiate between physical faults and status changes and hence, detect status changes as soon as they occu r. Another possible extension is to transmit Refresh() reliably (e.g., via the use of ACK timers) and integrate a notification mechanism through which receivers inform senders about their view (in terms of failure) on those senders. 
