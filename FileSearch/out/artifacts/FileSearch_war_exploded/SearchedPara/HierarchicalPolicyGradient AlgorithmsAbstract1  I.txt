 Mohammad Gha vamzadeh mgh@cs.umass.edu Sridhar Mahadev an mahadev a@cs.umass.edu Value function-based reinforcemen t learning (VFRL) has been extensiv ely studied in the mac hine learning literature. However, there are only weak theoretical guaran tees on the performance of these metho ds on problems with large or con tinuous state spaces. An alternativ e approac h to VFRL is to consider a class of parameterized stochastic policies, compute the gradien t of a performance function with resp ect to the parameters, and impro ve the policy by adjusting the parameters in the direction of the gradien t. This approac h is kno wn as policy gradien t reinforcemen t learning (PGRL) (Marbac h, 1998; Baxter &amp; Bartlett, 2001), and have receiv ed much recen t atten tion as a means to solv e problems with con tinuous state spaces. The main motiv ations for this are : 1) PGRL algo-rithms are theoretically guaran teed to con verge to lo-cally optimal policies, and 2) it is possible to incorp o-rate prior kno wledge into these metho ds via appropri-ate choice of the parametric form of the policy . However, in real-w orld high-dimensional tasks, in whic h the performance function is parameterized us-ing a large num ber of parameters, the PGRL metho ds migh t sho w poor performance by becoming stuc k in local optima. Moreo ver, PGRL algorithms are usually slower than VFRL metho ds, due to the large variance of their gradien t estimators. The above two reasons migh t mak e the application of these algorithms prob-lematic in real-w orld domains. A possible solution is to incorp orate prior kno wledge and decomp ose the high-dimensional task into a collection of mo dules with smaller and more manageable state spaces and learn these mo dules in a way to solv e the overall problem. Hierarc hical value function-based RL metho ds (Parr, 1998; Dietteric h, 1998; Sutton et al., 1999) have been dev elop ed using this approac h, as an attempt to scale RL to large state spaces.
 In this pap er, we de ne a family of hier archic al pol-icy gradient (HPG) algorithms, for scaling PGRL metho ds to high-dimensional domains. In HPG, sub-tasks involved in decision making (subtasks with more than one child), whic h we call non-primitive subtasks, are de ned as PGRL problems whose solution in-volves computing a locally optimal policy . Eac h non-primitive subtask is form ulated in terms of a param-eterized family of policies, a performance function, a metho d to estimate the gradien t of the performance function, and a routine to update the policy parame-ters using the performance gradien t.
 We accelerate the learning of HPG algorithms by for-mulating higher-lev el subtasks, whic h usually involve smaller and more manageable state and nite action spaces, as VFRL problems, and lower-lev el subtasks with in nite state and/or action spaces as PGRL prob-lems (Morimoto &amp; Doya, 2001). We call this family of algorithms hier archic al hybrid algorithms. The e ec-tiveness of our prop osed algorithms is demonstrated using a simple taxi-fuel problem as well as a more com-plex con tinuous state and action ship steering task. We introduce the hierarc hical task decomp osition us-ing a con tinuous state and action space ship steering problem (Miller et al., 1990, see Figure 1). A ship starts at a randomly chosen position, orien tation and turning rate and is to be maneuv ered at a constan t speed through a gate placed at a xed position. Equation 1 gives the motion equations of the ship, where T = 5 is the time constan t of con vergence to de-sired turning rate, V = 3 m=sec is the constan t speed of the ship, and = 0 : 2 sec is the sampling interv al. There is a time lag between changes in the desired turning rate and the actual rate, mo deling the e ects of a real ship's inertia and the resistance of the water. At eac h time t , the state of the ship is given by its position x [ t ] and y [ t ], orien tation [ t ] and actual turn-ing rate _ [ t ]. The action is the desired turning rate of the ship r [ t ]. All four state variables and also the action are con tinuous and their range is sho wn in Ta-ble 1. The ship steering problem is episo dic. In eac h episo de, the goal is learning to generate sequences of actions that steer the cen ter of the ship through the gate in the minim um amoun t of time. The sides of the gate are placed at coordinates (350,400) and (450,400). If the ship moves out of bound ( x &lt; 0 or x &gt; 1000 or y &lt; 0 or y &gt; 1000), the episo de terminates and is considered as a failure.
 We applied both at PGRL and actor-critic (Konda, 2002) algorithms to this task without achieving a good performance in reasonable amoun t of time (Figure 7). We believ e this failure occurred due to two reasons, whic h mak e this problem hard for RL algorithms. First, since the ship cannot turn faster than 15 de-grees/sec, all state variables change only by a small amoun t at eac h con trol interv al. Thus, we need a high resolution discretization of the state space in order to accurately mo del state transitions, whic h requires a large num ber of parameters for the function appro x-imator and mak es the problem intractable. Second, there is a time lag between changes in the desired turn-ing rate r and the actual turning rate _ , ship's position x;y and orien tation , whic h requires the con troller to deal with long dela ys.
 However, we successfully applied a at policy gradien t algorithm to simpli ed versions of this problem sho wn in Figure 2, when x and y change from 0 to 150 in-stead of 0 to 1000, the ship alw ays starts at a xed position with randomly chosen orien tation and turn-ing rate, and the goal is reac hing a neigh borho od of a xed point. It indicates that this high-dimensional non-linear con trol problem can be learned using an ap-propriate hierarc hical decomp osition. Using this prior kno wledge, we decomp ose the problem into two lev-els using the task graph sho wn in Figure 3. At the high-lev el, the agen t learns to select among four di-agonal and four horizon tal/v ertical subtasks. At the low-lev el, eac h low-lev el subtask learns a sequence of turning rates to achiev e its own goal. We use symme-try and map eigh t possible subtasks of the root to only two subtasks at the low-lev el, one asso ciated with four diagonal subtasks and one asso ciated with four hori-zon tal/v ertical subtasks as sho wn in Figure 3. We call them diagonal subtask and horizontal/vertic al subtask. As illustrated above for the ship steering task, the de-signer of the system uses his/her domain kno wledge and recursiv ely decomp oses the overall task into a col-lection of subtasks that are imp ortan t for solving the problem. This decomp osition is represen ted by a di-rected acyclic graph called task graph , as sho wn in Fig-ure 3. Mathematically , the task graph is mo deled by decomp osing the overall task MDP M , into a nite set of subtask MDPs f M 0 ;::: ;M n g . Eac h subtask MDP M i mo dels a subtask in the hierarc hy and M 0 is the root task and solving it solv es the entire MDP M . Eac h non-primitiv e subtask i is de ned over the MDP M i using state space S i , initiation set I i , set of termi-nal states T i , action space A i , transition probabilit y function P i , and rew ard function R i . Eac h primitiv e action a is a primitiv e subtask in this decomp osition, suc h that a is alw ays executable and it terminates im-mediately after execution. From now on in this pap er, we use subtask to refer to non-primitive subtasks. If we have a policy i for eac h subtask i in this mo del, it gives us a hier archic al policy = f 0 ;::: ; n g . A hierarc hical policy is executed using a stac k discipline, similar to subroutine calls in programming languages. After decomp osing the overall problem into a set of subtasks as describ ed in section 2, we form ulate eac h subtask as a PGRL problem. Our focus in this pap er is on episo dic problems, so we assume that the overall task ( root of the hierarc hy) is episo dic . 3.1. Policy Form ulation We form ulate eac h subtask i using a set of randomized stationary policies i ( i ) parameterized in terms of a taking action a in state s under the policy corresp ond-ing to i . We mak e the follo wing assumption about this set of policies.
 A1: For every state s 2 S i and every action a 2 bounded rst and second deriv ativ es. Furthermore, i ( s;a; i ) is bounded, di eren tiable and has bounded rst deriv ativ es.
 Since every time we call a subtask i in the hierarc hy, it starts at one of its initial states ( 2 I i ) and terminates at one of its terminal states ( 2 T i ), we can mo del eac h instan tiation of subtask i as an episo de as sho wn in Figure 4. In this mo del, all terminal states ( s 2 T i ) transit with probabilit y 1 and rew ard 0 to an absorbing state s i . We mak e the follo wing assumption for every subtask i and its parameterized policy i ( i ). A2 (Subtask Termination): There exists a state s i 2 S i suc h that, for every action a 2 A i , we have R i ( s i ;a ) = 0 and P i ( s i j s i ;a ) = 1, and for all sta-tionary policies i ( i ) and all states s 2 S i , we have P i ( s i ;N j s; i ( i )) &gt; 0, where N = j S i j . Under this mo del, we de ne a new MDP M i i for sub-task i with transition probabilities and rew ards R i i ( s; i ) = R i ( s; i ), where i ( s ) is the probabilit y that subtask i starts at state s . Let P i i be the set of all transition matrices i ( s 0 j s; i ( i )). We have the follo wing result for sub-task i .
 Lemma 1: Let assumptions A1 and A2 hold. Then for every P i i 2 P i i and every state s 2 S i , we have P Lemma 1 is equiv alen t to assume that the MDP M i i is recurr ent , i.e. the underlying Mark ov chain for every policy i ( i ) in this MDP has a single recurren t class and the state s i is recurren t state. 3.2. Performance Measure De nition We de ne weighte d rewar d-to-go i ( i ) as the perfor-mance measure of subtask i form ulated by parameter-ized policy i ( i ), and for whic h assumption A2 holds, as where J i ( s; i ) is the rew ard-to-go of state s , where T = min f k &gt; 0 j s k = s i g is the rst future time that state s i is visited. 3.3. Optimizing the Weigh ted Rew ard-to-Go In order to obtain an expression for the gradien t r i ( i ), we use MDP M i i de ned in section 3.1. Using Lemma 1, MDP M i i is recurr ent . For MDP M i i , let i i ( s; i ) be the steady state probabilit y distribution of being in state s and let E i ; i [ T ] be the mean recurrence time, i.e. E i ; i [ T ] = usual action-v alue function.
 Using MDP M i i , we can deriv e the follo wing prop osi-tion whic h gives an expression for the gradien t of the weighte d rewar d-to-go i ( i ) with resp ect to i . Prop osition 1: If assumptions A1 and A2 hold
The expression for the gradien t in prop osition 1 can be estimated over a renewal cycle 1 as where t m is the time of the m th visit at the recurren t estimate of Q i .
 From Equation 2, we obtain the follo wing pro cedure to update the parameter vector along the appro ximate gradien t direction at every time step. where k is the step size parameter and satis es the follo wing assumptions.
 A4: k 's are deterministic, nonnegativ e and satisfy A5: k 's are non-increasing and there exists a positiv e integer p and a positiv e scalar A suc h that P n + t k = n ) At p 2 n for all positiv e integers n and t . We have the follo wing con vergence result for the itera-tive pro cedure in Equation 3 to update the parameters. Prop osition 2: Let assumptions A1, A2, A4 and A5 hold, and let ( i k ) be the sequence of parameter vectors generated by Equation 3. Then, i ( i k ) con verges and lim k !1 r i ( i k ) = 0 with probabilit y 1. Equation 3 pro vides an unbiased estimate of r i ( i ). For systems involving a large state space, the interv al between visits to state s i can be large. As a con-sequence, the estimate of r i ( i ) migh t have a large variance. Sev eral approac hes have been prop osed to reduce the variance in these estimations and deliv er-ing faster con vergence. For instance, in one approac h, we can replace s i with S i , a subset of state space con-taining s i , and reset z i when s 2 S i . This metho d can be easily implemen ted for eac h subtask by de ning S i = T i S f s i g . Another approac h uses a discoun t factor in rew ard-to-go estimation. However, these metho ds introduce a bias into the estimate of r i ( i ). For both approac hes, we can deriv e a mo di ed version of Equation 3 to incremen tally update the parameter vector along the appro ximate gradien t direction. After decomp osing the overall task to a set of sub-tasks as describ ed in section 2, and form ulating eac h subtask in the hierarc hy as an episo dic PGRL prob-lem as illustrated in section 3, we can use the update Equation 3 and deriv e a hierarc hical policy gradien t al-gorithm (HPG) to maximize the weighte d rewar d-to-go for every subtask in the hierarc hy. Algorithm 1 sho ws the pseudo code for this algorithm.
 G i ( s 0 j s;a ) in lines 10 and 16 of the algorithm is the in-ternal rew ard whic h can be used only inside eac h sub-task to speed up its local learning and does not propa-gate to upp er levels in the hierarc hy. Lines 11 16 can be replaced with any other policy gradien t algorithm to optimize the weighte d rewar d-to-go , suc h as (Mar-bac h, 1998) or (Baxter &amp; Bartlett, 2001). Thus, Al-gorithm 1 demonstrates a family of hierarc hical policy gradien t algorithms to maximize the weighte d rewar d-to-go for every subtask in the hierarc hy.
 Algorithm 1 A hierarc hical policy gradien t algorithm 1: Function HPG(T ask i , State s ) 2: ~ R = 0 3: if i is a primitiv e action then 4: execute action i in state s , observ e state s 0 and 5: return R ( s 0 j s;i ) 6: else 7: while i has not terminated ( s 6 = s i ) do 8: choose action a using policy i ( s; i ) 9: R = HPG(T ask a , State s ) 10: observ e result state s 0 and internal rew ard 13: else 15: end if 17: ~ R = ~ R + R 18: s = s 0 19: end while 20: end if 21: return ~ R 22: end HPG The above form ulation of eac h subtask has the fol-lowing limitations: 1) Compact (parameterized) rep-resen tation of the policy limits the searc h for a policy to a set whic h is typically smaller than the set of all possible policies. 2) Gradien t-based optimization al-gorithms as a searc h metho d for a policy , nd a so-lution whic h is locally , rather than globally , optimal. Thus, in general, the family of algorithms describ ed above con verges to a recursively local optimal policy . If the policy learned for every subtask in the hierarc hy coincides with the best policies, then these algorithms con verge to a recursively optimal policy .
 Despite all the metho ds prop osed to reduce the vari-ance of gradien t estimators in PGRL algorithms, these algorithms are still slower than VFRL metho ds, as we will sho w in the simple taxi-fuel exp erimen t in sec-tion 5.1. One way to accelerate learning of HPG algo-rithms is to form ulate those subtasks involving smaller state spaces and nite action spaces, usually located at the higher-lev els of the hierarc hy, as VFRL prob-lems, and those with large state spaces and in nite ac-tion spaces, usually located at the lower-lev els of the hierarc hy, as PGRL problems. This form ulation can bene t from the faster con vergence of VFRL metho ds and the power of PGRL algorithms in domains with in nite state and/or action spaces at the same time. We call this family of algorithms, hier archic al hybrid algorithms and illustrate them in our ship steering ex-perimen t. In this section, we rst apply the hierarc hical policy gradien t algorithm prop osed in this pap er to the well-kno wn taxi-fuel problem (Dietteric h, 1998) and com-pare its performance with MAX Q-Q, a value-based hi-erarc hical RL algorithm, and at Q-learning. Then we turn to a more complex con tinuous state and action ship steering domain, apply a hierarc hical hybrid al-gorithm to this task and compare its performance with at PGRL and actor-critic algorithms. 5.1. Taxi-F uel Problem A 5-b y-5 grid world inhabited by a taxi is sho wn in Figure 5. There are four stations, mark ed as B(lue), G(reen), R(ed) and Y(ello w). The task is episo dic. In eac h episo de, the taxi starts in a randomly chosen location and with a randomly chosen amoun t of fuel (ranging from 5 to 12 units). There is a passenger at one of the four stations (chosen randomly), and that passenger wishes to be transp orted to one of the other three stations (also chosen randomly). The taxi must go to the passenger's location, pick up the passenger, go to the destination location and drop o the pas-senger there. The episo de ends when the passenger is dep osited at the destination station or taxi goes out of fuel. There are 8,750 possible states and sev en prim-itiv e actions in the domain, four navigation actions, Pic kup action, Drop o action, and Fillup action (eac h of these consumes one unit of fuel). Eac h action is deterministic. There is a rew ard of -1 for eac h action and an additional rew ard of 20 for successfully deliv er-ing the passenger. There is a rew ard of -10 if the taxi attempts to execute the Drop o or Pic kup actions il-legally and a rew ard of -20 if the fuel level falls below zero. The system performance is measured in terms of the average rew ard per step. In this domain, this is equiv alen t to maximizing the total rew ard per episo de. Eac h exp erimen t was conducted ten times and the re-sults averaged.
 Figure 6 compares the prop osed hierarc hical policy gradien t (HPG) algorithm with MAX Q-Q (Dietteric h, 1998), a value-based hierarc hical RL algorithm, and at Q-learning. The graph sho ws that the MAX Q-Q con verges faster than HPG and at Q-learning, and HPG is sligh tly faster than at Q-learning.
 The hierarc hical policy gradien t algorithm used in this exp erimen t is the one sho wn in Algorithm 1, with one policy parameter for eac h state-action pair ( s;a ). As we exp ected, the HPG algorithm con verges to the same performance as MAX Q-Q, but it is slower than its value-based coun terpart. The performance of HPG can be impro ved by better policy form ulation and us-ing more sophisticated policy gradien t algorithms for eac h subtask. The slow con vergence of PGRL meth-ods motiv ates us to use both value and policy based metho ds in a hierarc hy and study how to de ne more expressiv e policies for eac h subtask. We address the former using the hier archic al hybrid algorithms in the next section and leave the latter for future work. 5.2. Ship Steering Problem In this section we apply a hier archic al hybrid algorithm to the ship steering task describ ed in section 2 and compare its performance with at PGRL and actor-critic algorithms.
 The at PGRL algorithm used in this section uses Equation 3 and CMA C function appro ximator with 9 four dimensional tilings, dividing the space into 20 20 36 5 = 72000 tiles eac h. The actor-critic algorithm (Konda, 2002) also uses the above func-tion appro ximator for its actor, and 9 ve dimensional tilings of size 5 5 36 5 30 = 135000 tiles, for its critic. The fth dimension of critic's tilings is for con tinuous action.
 In hierarc hical hybrid algorithm, we decomp ose the task using the task graph in Figure 3. At the high-level, the learner explores in a low-dimensional sub-space of the original high-dimensional state space. The state variables are only the coordinates of the ship x and y on the full range from 0 to 1000. The actions are four diagonal and four horizon tal/v ertical subtasks similar to those subtasks sho wn in Figure 2. The state space is coarsely discretized into 400 states. We use the value-based Q( ) algorithm with -greedy action selection and replacing traces to learn a sequence of diagonal and horizon tal/v ertical subtasks to achiev e the goal of the entire task (passing through the gate). Eac h episo de ends when the ship passes through the gate or moves out of bound. Then the new episo de starts with the ship in a randomly chosen position, orien tation and turning rate. In this algorithm, is set to 0.9, learning rate to 0.1 and starts with 0.1 remains unc hanged until the performances of low-lev el subtasks reac h to a certain level and then is decreased by a factor of 1.01 every 50 episo des.
 At the low-lev el, learner explores local areas of the high-dimensional state space without discretization. When the high-lev el learner selects one of the low-lev el subtasks, the low-lev el subtask tak es con trol and ex-ecutes the follo wing steps as sho wn in Figure 2. 1) Maps the ship to a new coordinate system in whic h the ship is in position (40,40) for the diagonal sub-task and (40,75) for the horizon tal/v ertical subtask. 2) Sets the low-lev el goal to position (140,140) for the diagonal subtask and (140,75) for the horizon-tal/v ertical subtask. 3) Sets the low-lev el boundaries to 0 x;y 150. 4) Generates primitiv e actions until either the ship reac hes to a neigh borho od of the low-level goal, a circle with radius 10 around the low-lev el goal (success), or moves out of the low-lev el bounds (failure).
 The two low-lev el subtasks use all four state variables, however the range of coordination variables x and y is 0 to 150 instead of 0 to 1000. Their action vari-able is the desired turning rate of the ship, whic h is a con tinuous variable with range -15 to 15 degrees/sec. The con trol interv al is 0 : 6 sec (three times the sampling interv al = 0 : 2 sec ). They use the policy gradien t learning algorithm in lines 11-16 of Algorithm 1 to up-date their parameters. In addition, they use a CMA C function appro ximator with 9 four dimensional tilings, dividing the space into 5 5 36 5 = 4500 tiles eac h. One parameter w is de ned for eac h tile and the parameterized policy is a Gaussian: where N = 9 4500 = 40500 is the total num ber of tiles and i is 1 if state s falls in tile i and 0 otherwise. The actual action is generated after mapping the value chosen by the Gaussian policy to the range from -15 to 15 degrees=sec using a sigmoid function.
 In addition to the original rew ard of -1 per step, we de-ne internal rew ards 100 and -100 for low-lev el success and failure, and a rew ard according to the distance of the curren t ship orien tation to the angle between the curren t position and low-lev el goal ^ given by where 30(deg) gives the width of the rew ard function. When a low-lev el subtask terminates, the only rew ard that propagates to the high-lev el is the summation of all -1 rew ards per step. In addition to rew ard receiv ed from low-lev el, high-lev el uses a rew ard 100 upon suc-cessfully passing through the gate.
 We train the system for 50000 episo des. In eac h episo de, the high-lev el learner (con troller located at root ) selects a low-lev el subtask, and the selected low-level subtask is executed until it successfully termi-nates (ship reac hes the low-lev el goal) or it fails (ship goes out of the low-lev el bounds). Then con trol returns to the high-lev el subtask ( root ) again. The follo wing results were averaged over ve sim ulation runs. Figure 7 compares the performance of the hierarc hical hybrid algorithm with at PGRL and actor-critic al-gorithms in terms of the num ber of successful trials in 1000 episo des. As this gure sho ws, despite the high resolution function appro ximators used in both at al-gorithms, their performance is worse than hierarc hical hybrid algorithm. Moreo ver, their computation time per step is also much more than the hierarc hical hy-brid algorithm, due to the large num ber of parameters to be learned.
 Figure 8 demonstrates the performance of the system in terms of the average num ber of low-lev el subtask calls. This gure sho ws that after learning, the learner executes about 4 low-lev el subtasks (diagonal or hori-zon tal/v ertical subtasks) per episo de.
 Figure 9 compares the performance of the hierarc hical hybrid, at PGRL and actor-critic algorithms in terms of the average num ber of steps to goal (averaged over 1000 episo des). This gure sho ws that after learning, it tak es about 220 primitiv e actions (turn actions) for hierarc hical hybrid learner to pass the gate. Although at algorithms should sho w a better performance than hierarc hical algorithm in terms of the average num ber of steps to goal ( at algorithms should nd the global optimal policy , whereas hierarc hical hybrid algorithm con verges just to recursiv e optimal solution), Figure 9 sho ws that their performance after 50000 episo des is still worse than the hierarc hical hybrid algorithm. Figures 10 and 11 sho w the performance of the diago-nal and horizon tal/v ertical subtasks in terms of num-ber of success out of 1000 executions, resp ectiv ely. Finally , Figure 12 demonstrates the learned policy for two sample initial points sho wn with big circles. The upp er initial point is x = 700, y = 700, = 100 and _ = 3 : 65 and the lower initial point is x = 750, y = 180, = 80 and _ = 7 : 9. The low-lev el subtasks chosen by the agen t at the high-lev el are sho wn by small circles. This pap er com bines the adv antages of hierarc hical task decomp osition and PGRL metho ds and describ es a class of hier archic al policy gradient (HPG) algo-rithms for problems with con tinuous state and/or ac-tion spaces. To accelerate learning in HPG algorithms, we prop osed hier archic al hybrid algorithms, in whic h higher-lev el subtasks are form ulated as VFRL and lower-lev el subtasks as PGRL problems. The e ectiv e-ness of these algorithms was demonstrated by applying them to a simple taxi-fuel problem and a con tinuous state and action ship steering domain.
 The algorithms prop osed in this pap er are for the case that the overall task is episo dic . We also form ulated these algorithms for the case that the overall task is continuing , but we do not include the results for space reasons. In this case, the root task is form ulated as a continuing problem with the aver age rewar d as its performance function. Since the policy learned at root involves policies of its children, the type of optimalit y achiev ed at root dep ends on how we form ulate other subtasks in the hierarc hy. We investigated di eren t notions of optimalit y in hierarc hical average rew ard, rep orted in our previous work (Gha vamzadeh &amp; Ma-hadev an, 2001; Gha vamzadeh &amp; Mahadev an, 2002), for HPG algorithms with continuing root task. Although the prop osed algorithms give us the abilit y to deal with large con tinuous state spaces, they are not still appropriate to con trol real-w orld problems in whic h the speed of learning is crucial. However, policy gradien t algorithms give us this opp ortunit y to accel-erate learning by de ning more expressiv e set of pa-rameterized policies for eac h subtask. The results of ship steering task indicate that in order to apply these metho ds to real-w orld domains, a more expressiv e rep-resen tation of the policies is needed.

