 1. Introduction articles ... ) has fostered the need for efficient text mining tools. Information retrieval, text filtering and classification, and information extraction technologies are rapidly becoming key components of modern infor-mation processing systems, helping end-users to select, visualize and shape their informational environment.
Information retrieval technologies seek to rank documents according to their relevance with respect to users queries, or more generally to users informational needs. Filtering and routing technologies have the potential to automatically dispatch documents to the appropriate reader, to arrange incoming documents in the proper folder or directory, possibly rejecting undesirable entries. Information extraction technologies, including auto-matic summarization techniques, have the additional potential to reduce the burden of a full reading of texts or messages. Most of these applications take advantage of (unsupervised) clustering techniques of documents or of document fragments: the unsupervised structuring of documents collections can for instance facilitate its indexing or search; clustering a set of documents in response to a user query can greatly ease its visualization; considering sub-classes induced in a non-supervised fashion can also improve text classification ( Vinot &amp; Yvon, 2003 ), etc. Tools for building thematically coherent sets of documents are thus emerging as a basic tech-nological block of an increasing number of text processing applications.

Text clustering tools are easily conceived if one adopts, as is commonly done, a bag-of-word representation of documents: under this view, each text is represented as a high-dimensional vector which merely stores the counts of each word in the document, or a transform thereof. Once documents are turned into such kind of numerical representation, a large number of clustering techniques become available ( Jain, Murphy, &amp; Flynn, 1999 ) which allow to group documents based on  X  X  X emantic X  X  or  X  X  X hematic X  X  similarity. For text clustering tasks, a number of proposal have recently been made which aim at identifying probabilistic ( X  X  X oft X  X ) theme-document associations (see, e.g., Blei, Ng, &amp; Jordan, 2002; Buntine &amp; Jakulin, 2004; Hofmann, 2001 ). These probabilistic clustering techniques compute, for each document, a probability vector whose values can be interpreted as the strength of the association between documents and clusters. As such, these vectors can also serve to project texts into a lower-dimensional space, whose dimension is the number of clusters. These prob-abilistic approaches are certainly appealing, as the projections they build have a clear, probabilistic interpre-tation; this is in sharp contrast with alternative projection techniques for text documents, such as latent semantic analysis (LSA) ( Deerwester, Dumais, Landauer, Furnas, &amp; Harshman, 1990 ) or non-negative matrix factorization (NMF) techniques ( Shahnaz, Berry, Pauca, &amp; Plemmons, 2006; Xu, Liu, &amp; Gong, 2003 ).
In this paper, we focus on a simpler probabilistic model, in which the corpus is represented by a mixture of multinomial distributions, each component corresponding to a different  X  X  X heme X  X  ( Nigam, McCallum, Thrun, &amp; Mitchell, 2000 ). This model is the unsupervised counterpart of the popular  X  X  X aive Bayes X  X  model for text classification (see, e.g., Lewis, 1998; McCallum &amp; Nigam, 1998 ). Our main objective is to analyze the estima-tion procedures that can be used to infer the model parameters, and to understand precisely the behavior of these estimation procedures when faced with high-dimensional parameter spaces. This situation is typical of the bag-of-word model of text documents but may certainly occur in other contexts (bioinformatics, image processing ... ). Our contribution is thus twofold:
We present a comprehensive review of the model and of the estimation procedures that are associated with this model, and introduce novel variants thereof, which seem to yield better estimates for high-dimensional models, and report a detailed experimental analysis of their performance.

These analyses are supported by a methodological contribution on the delicate, and often overlooked, issue of performance evaluation of clustering algorithms (see, e.g., Halkidi, Batistakis, &amp; Vazirgiannis, 2001 ).
Our proposal here is to focus on a  X  X  X ure X  X  clustering tasks, where the number of themes (the number of dimensions in the  X  X  X emantic X  X  space) is limited, which allows in our case a direct comparison with a refer-ence (manual) clustering.

This article is organized as follows. We firstly introduce the model and notations used throughout the paper. Dirichlet priors are set on the parameters and we may use the expectation-maximization (EM) algo-rithm to obtain maximum a posteriori (MAP) estimates of the parameters. An alternative inference strategy uses simulation techniques (Markov Chain Monte Carlo) and consists in identifying conditional distributions from which to generate samples. We show, in Section 2.3 , that it is possible to marginalize analytically all continuous parameters (thematic probabilities and theme-specific word probabilities). This result generalizes an observation that was used, in the context of the latent Dirichlet allocation (LDA) model by Griffiths and
Steyvers (2002) . We first examine what the consequences of this derivation are for supervised classification tasks. We then describe our evaluation framework and highlight, in a first round of experiments, the impor-tance of the initialization step in the EM algorithm. Looking for ways to overstep the limitations of EM by incremental learning, we present an algorithm based on a progressive inclusion of the vocabulary. We then discuss the application of Gibbs sampling to this model, reporting experiments which support the claim that, in our context, the sampling based approach is more robust than EM alternatives. Eventually, we present a comparison of the results with the performances of k -means in this context. 2. Basics words in the document depends on the value of a latent variable associated with each text, the theme ,we use a multinomial mixture model with Dirichlet priors on the parameters.
 sities follow another distribution, called  X  X  X irichlet-multinomial X  X  and how this fact proves useful for both clas-sification and unsupervised learning. 2.1. Multinomial mixture model ber of themes, that is, the number of components in the mixture model. Since we use a bag-of-words represen-tation of documents, the corpus is fully determined by the count matrix C  X  X  C wd  X  w  X  1 ... n is used to refer to the word count vector of a specific document d . The multinomial mixture model is such that:
Note that the document length itself (denoted by l d ) is taken as an exogenous variable and its distribution is not accounted for in the model. The notations a  X  X  a 1 ; a 2 ; ... ; a n t =1, ... , n T ) are used to refer to the model parameters, respectively, the mixture weights and the collection of theme-specific word probabilities.
 in this context is natural because it is the conjugated distribution associated to the multinomial, a property which will be instrumental in Section 2.3 .

Now, as the prior distributions are Dirichlet, the posterior distribution is proportional to (disregarding terms that do not depend on a or b ):
Maximizing this expression is in general intractable. We first consider the simpler case of supervised inference in which the themes T  X  X  T 1 ; ... ; T n Bayes classifier (with Laplacian smoothing). In Section 2.3 , we turn to the so-called fully Bayesian inference an alternative classification rule for unlabeled documents which is connected to the Dirichlet-multinomial (or Polya) distribution. Both of these approaches have counterparts in the context of unsupervised inference which will be developed in Sections 3.2 and 3.5 , respectively. 2.2. Naive Bayes classifier
When T is observed, the log-posterior distribution of the parameters given both the documents C and their themes T has the simple form: up to terms that do not depend on the parameters, where S t is the number of training documents in theme t and K wt is the number of occurrences of the word w in theme t .
 Taking into account the constraints teriori estimates have the familiar form: where K t  X 
In the following, we will denote quantities that pertain to a test corpus distinct from the training corpus C etc. The Bayes decision rule for classifying an unlabeled test document, say C H d , then consists in selecting the theme t which maximizes The above formula corresponds to the so-called naive Bayes classifier, using Laplacian smoothing for word and theme probability estimates ( Lewis, 1998; McCallum &amp; Nigam, 1998 ). 2.3. Fully Bayesian classifier
An interesting feature of this model is that it is also possible to integrate out the parameters a and b under their posterior distribution allowing to evaluate the Bayesian predictive distribution ( Minka, 2003; Mosimann, 1962 ).

To see this, consider the joint distribution of the observations C , the latent variables T and the parameters a and b : As the above quantity, viewed as a function of a and b 1 ; ... ; b n ization constants of the Dirichlet distributions, yielding:
Now, if we single out the document of index d assuming that the document C d itself has been observed but that the theme T d is unknown, elementary manipulations yield: where T d is the vector of theme indicators for all documents but d , C d denotes the corpus deprived from document d , and K d wt is the quantity K d wt  X  the predictive distribution as defined in (5) .
 constant in (7) is indeed computable as it only involves summation over the n T themes. As another practical implementation detail, note that the calculation of (7) can be performed efficiently as the special function C (or can thus be tabulated beforehand.
 compare this approach with the use of the naive Bayes classifier in Section 2.4 below. Eq. (7) is also useful in the context of unsupervised clustering where it provides the basis for simulation-based inference procedures to be examined in Section 3.5 . 2.4. Supervised inference (4) usually adopted in supervised text clustering.
 allow for easier comparison with the naive Bayes classification rule in (4) , we rather denote by C the training corpus, T the associated labels and C H d the test (unlabeled) document. Using these notations, (7) becomes:
Comparing with (4) we get, after simplification of the Gamma functions,
Leaving aside the offset difference on the hyperparameters (due to the non-coincidence of the mode and the expectation of the multinomial distribution), the two formulas are approximately equivalent if: never gets statistically significant on common text classification benchmarks such as Reuters 2000 ( Reuters, 2000 ), 20-Newsgroups ( Lang, 1995 ) and Spam Assassin ( Mason, 2002 ), even when varying the number of training documents or size of vocabulary. Given that the naive Bayes classifier is known to perform worse than not seem to be the method of choice for supervised text classification tasks. This negative result does not come satisfied.

We now turn to the unsupervised clustering case, where the fully Bayesian perspective will prove more useful. 3. Unsupervised inference
When document labels are unknown, the multinomial mixture model may be used to create a probabilistic clustering rule. In this context, the performance of the method is more difficult to assess. We therefore, start this section with a discussion of our evaluation protocol (Section 3.1 ). For estimating the model parameters, we first consider the most widespread approach, which is based on the use of the expectation-maximization (EM) algorithm. It turns out that in the context of large-scale text processing applications, this basic approach is plagued by an acute sensitivity to initialization conditions. We then consider alternative estimation proce-dures, based either on heuristic considerations aimed at reducing the variability of the EM estimates (Section 3.4 ) or on the use of various forms of Markov chain Monte Carlo simulations (Section 3.5 ) and show that these techniques can yield less variable estimates. 3.1. Experimental framework
We selected 5000 texts from the 2000 Reuters Corpus ( Reuters, 2000 ), from five well-defined categories (arts, sports, health, disasters, employment). In a pre-processing step, we discard non-alphabetic characters such as punctuation, figures, dates and symbols. For the baseline experiments, all the words found in the train-ing data are taken into account. Words that only occur in the test corpus are simply ignored. All experiments are performed using 10-fold cross-validation (with 10 random splits of the corpus).

As will be discussed below, initialization of the EM algorithm does play a very important role in obtaining meaningful document clusters. To evaluate the performance of the model, one option is to look at the value of the log-likelihood at the end of the learning procedure. However, this quantity is only available on the training data and does not supply any information regarding the generalization abilities of the model. A more mean-ingful measure, commonly used in text applications, is the perplexity. Its expression on the test data is: It quantifies the ability of the model to predict new documents. The normalization by the total number of word occurrences l w in the test corpus C w is conventional and used to allow comparison with simpler prob-abilistic models, such as the unigram model, which ignores the document level. For the sake of coherence, we will also compute perplexity, rather than log-likelihood, on the training data: their variations are in fact sim-ilar as these quantities are identical up to the normalization constant and the exponential function.
Likelihood or perplexity measures are used to assess the goodness-of-fit. A large number of other quantities have been proposed in the literature to evaluate more directly the performance of clustering algorithms ( Halk-idi et al., 2001 ). When a reference classification is available, several similarity measures can be computed, the most natural one being the number of cooccurrences of documents between  X  X  X quivalent X  X  clusters in two clus-terings. To perform such an evaluation, we must have a way to establish the best mapping between clusters in two different clusterings. Provided that the two clusterings have the same size, this can be done with the so-called Hungarian method ( Frank, 2004; Kuhn, 1955 ), an algorithm for computing the best weighted matching in a bi-partite graph. The complexity of this algorithm is cubic in the number of clusters involved. Once a one-to-one mapping between clusters is established, the score we consider is the ratio of documents for which the two clusterings  X  X  X gree X  X , that is, which lie into clusters that are mapped by the Hungarian method. Lange,
Roth, Braun, and Buhmann (2004) describes in more detail how this method can be used to evaluate clustering algorithms.
 terings with different number of classes and especially the cases where one class in clustering A is split into two classes in clustering B . There exist other information-based measures, such as the Relative Information Gain , that do not suffer from this limitation but present other drawbacks, such as undesirable behaviors for distri-butions close to equiprobability. We do not consider those here, as cooccurrence scores obtained with the base quantified in terms of mutual information). 3.2. Expectation-maximization algorithm distribution given in (2) . The resulting maximization program is unfortunately not tractable. It is, however, possible to devise an iterative estimation procedure, based on the expectation-maximization (EM) algorithm.
Denoting, respectively, by a 0 and b 0 the current estimates of the parameters and by T d the latent (unobserv-able) theme of document d , it is straightforward to check that each iteration of the EM algorithm updates the parameters according to: where the normalization factors are determined by the constraints:
In the remainder of this section, we present the results of a series of experiments based on the use of the EM algorithm. We first discuss issues related to the initialization strategy, before empirically studying the influ-ence of the smoothing parameters. The main findings of these experiments is that the EM estimates are very unstable and vary greatly depending on the initial conditions: this outlines a limitation of the EM algorithm, i.e. its difficulty to cope with the very high number of local maxima in high dimensional spaces. In compar-ison, the influence of the smoothing parameter is moderate, and its tuning should not be considered a major issue. 3.2.1. Initialization parameters a and b and the values of the posterior probabilities P( T d = t j C ; a , b ) using formulas (9) X (11) .
Therefore, the EM algorithm can be initialized either from the E-step, providing initial values for the param-eters, or from the M-step, providing initial values for the posterior probabilities (that is, roughly speaking, an initial soft clustering). There are various reasons to prefer the second solution:
Initializing on posterior probabilities can be done without any knowledge of the model: for instance, it can be performed without knowing the vocabulary size. Section 3.4 will show why this is a desirable property.

Consequently, in the rest of this article, we will only consider initialization schemes that are based on the posterior theme probabilities associated with each document. A good option is to make sure that, initially, all clusters significantly overlap. Our  X  X  X irichlet X  X  initialization consists in sampling, independently for each doc-ument, an initial (fictitious) configuration of posterior probabilities from an exchangeable Dirichlet distribu-tion. In practice, we used the uniform distribution over the n T -dimensional probability simplex (Dirichlet with parameter 1). As the EM iterations tend to amplify even the smaller discrepancies between the components, the variability of the final estimates was not significantly reduced when initializing from exchangeable Dirich-let distributions with lower variance (i.e., higher parameter value).
 To get an idea about the best achievable performance, we also used the Reuters categories as initialization. We establish a one-to-one mapping between the mixture components and the Reuters categories, setting for each document the initial posterior probability in (9) to 1 for the corresponding theme. Fig. 1 displays the cor-responding perplexity on the training and test sets as a function of the number of iterations. Results are aver-aged over 10-folds and 30 initializations per fold and are represented with box-and-whisker curves: the boxes are drawn between the lower and upper quartiles, the whiskers extend down and up to  X 1.5 times the inter-quartile range (the outliers, a couple of runs out of the 300, have been removed).

The variations are quite similar on both (training and test) datasets. The main difference is that test perplex-ity scores are worse than training perplexity scores. This classical phenomenon is an instance of overfitting. Due to the way the indexing vocabulary is selected (discarding words that do not occur in the training data), this effect is not observed for the unigram model. 1
The most striking observation is that the gap between both initialization strategies is huge. With the Dirich-let initialization, we are able to predict the word distribution more accurately than with the unigram model but much worse than with the somewhat ideal initialization. This gap is also patent for the cooccurrence scores initialization on test data.

Given that the Dirichlet initialization involves random sampling, it is worth checking how the performance change from one run to another. We report in Fig. 2 the values of training perplexity and test cooccurrence scores for various runs on the first fold. 2 As can be seen more clearly on this figure, the variability from one initialization to another is very high for both measures: for instance, the cooccurrence score varies from about 0.4 to more than 0.7. This variability is a symptom of the inability of the EM algorithm to avoid being trapped in one of the abundant local maxima which exist in the high-dimensional parameter space. 3.2.2. Influence of the smoothing parameter
Fig. 3 depicts the influence of the smoothing parameter k b 1 in terms of perplexity and cooccurrence scores. We do not consider here the influence of k a 1, which is, in our context, always negligible with respect to the sum over documents of the themes posterior probabilities. For the Reuters categories initialization, there is almost no difference in perplexity scores for small values of k b 1 (i.e. when k b 1 6 0.2). The per-formance degrades steadily for larger values, showing that some information is lost, probably in the set of rare words (since smoothing primarily concerns parameters corresponding to very few occurrences). Similarly, for the Dirichlet initialization, the variations in perplexity are moderate for smoothing values in the range 0.01 X 1, yet there is a more distinguishable optimum, around 0.2. Using some prior information about the fact that word probabilities should not get too small helps to fit the distribution of new data, even for words that are rarely (or even never) seen in association with a given theme.

These observations are confirmed by the observation of the test cooccurrence scores. First, except when using very large (5 or more) values of the smoothing parameters, which yields a serious drop in performance, the categorization accuracy is rather insensitive to the smoothing parameter for the Reuters categories initial-formance are again moderate, with however a higher optimum value around 0.75. A possible explanation of this observation that more smoothing improves categorization capabilities (even if it slightly degrades distri-bution fit) is that the model is so coarse and the data so sparse that only quite frequent words are helpful in categorizing; the other words are essentially misleading, unless properly initialized. This suggests that remov-ing rare words from the vocabulary could improve the classification accuracy.

All in all, changing the values of k a 1 and k b 1 does not make the most important differences in the results, as long as they remain within reasonable bounds. Thus, in the rest of this article, we set them, respec-tively, to 0 and 0.1. 3.3. EM and deterministic clustering
A somewhat unexpected property of the multinomial mixture model is that a huge fraction of posterior probabilities (that a document belongs to a given theme) is in fact very close to 0 or 1. Indeed, when starting from the Reuters categories, the proportion of texts classified in only one given theme (that is, with probability one, up to machine precision) is almost 100%. As we start from the opposite point of  X  X  X xtreme fuzziness X  X , this the documents are categorized with almost absolute certainty. This suggests that in the context of large-dimen-sional textual databases, the multinomial mixture model in fact behaves like a deterministic clustering algorithm. ing version of the EM algorithm, in which the E-step uses deterministic rather than probabilistic theme assign-ments. This algorithm can be seen as an instance of a k -means algorithm, where the similarity between a text d 2 {1, ... , n D } and theme (or cluster) t 2 {1, ... , n T } is computed as:
Up to a constant term, which only depends on the document, the first term is the Bregman divergence ( Baner-jee, Merugu, Dhillon, &amp; Ghosh, 2005 ) between a theme specific distribution and the document, viewed as an empirical probability distribution over words. This measure is computed for every document and every theme, and each document is assigned to the closest theme. The reestimation of the parameters b wt is still performed according to (11) , where the posterior  X  X  X robabilities X  X  are either 0 or 1. The weight a t simply becomes the pro-portion of documents in theme t and b wt the ratio of the number of occurrences of w in theme t over the total number of occurrences in documents in theme t .
This algorithm was applied to the same dataset, with the same initialization procedures as above. At the end of each iteration, we compute the cooccurrence score between the probabilistic clustering produced by EM and the hard clustering produced by this version of k -means.
 behavior of EM can be partly explained by the large dimensionality of the space of documents. 3 This assump-tion has been verified with experiments on artificially simulated datasets, which are not reported here for rea-son of space. 3.4. Improving EM via dimensionality reduction the EM algorithm and should alleviate the variability phenomenons observed in the previous section. After studying the effect of dimensionality reduction, we propose a novel strategy based on iterative inference. 3.4.1. Adjusting the vocabulary size rare words or the frequent words. In this section, we experimentally assess these strategies, by removing con-secutively tens, hundreds and thousands of terms from the indexing vocabulary. The words that are discarded are simply removed from the count matrix. 4 Results presented in Fig. 4 suggest that the performance of the model with the Dirichlet initialization can be substantially improved by keeping a limited number of frequent words (900 out of 40,000).
When varying the size of the vocabulary, perplexity measurements are meaningless, as the reduction of dimensionality has an impact on perplexity which is hard to distinguish from the variations due to a possible better fit of the model. The test cooccurrence score, on the other hand, is meaningful even when with varying vocabulary sizes. Fig. 5 plots the test cooccurrence scores at the end of the 30th EM iteration as a function of the vocabulary size. For the sake of readability, the scale of the x-axis is not regular but rather focuses on the interesting parts: the interval between 100 and 3000 words, which corresponds to keeping only the frequent words, and the region above 40,000 (out of a complete vocabulary of 43,320 forms), which corresponds to keeping only the rare words. This choice is motivated by the well-known fact that most of the occurrences (and therefore most of the information) are due to the most frequent words: for instance, the 3320 most fre-quent words account for about 75% of the total number of occurrences.

The upper graph in Fig. 5 shows that removing rare words always hurts when using the Reuters categories initialization. In contrast, with the Dirichlet initialization, considering a reduced vocabulary (between 300 and 3000 words) clearly improves the performance. The somewhat optimal size of the vocabulary, as far as this specific measure is concerned, seems to be around 1,000. Also importantly, the performance seems much more stable when using reduced versions of the vocabulary, an effect we did not manage to achieve by adjusting the smoothing parameter. We will come back to this in the next section. It suffices to say here that the best score obtained with the Dirichlet initialization is still far behind the performance attained with the Reuters catego-ries initialization. This agrees with our previous observation that even the rarest word are informative, when properly initialized.

Less surprisingly, on the lower portion of the graph, one can see that removing the frequent words almost always hurts the performance. It is only in the case of the Reuters categories initialization that the removal of the 100 most frequent words actually yields a slight improvement of performance. Then the score steadily decreases with the removal of frequent words. The score is almost 0.2 (random agreement) with 20,000 rare words, which is not surprising, as, in this case, the vocabulary mainly contains words occurring only once (so-called hapax legomena ) in the corpus, reducing texts to at most a dozen of terms. 3.4.2. Iterative estimation procedures end-user should get sufficiently reliable results without having to run the program several times and/or to worry about evaluation measures. 3.4.2.1. Incremental vocabulary. The first idea is to take advantage of our previous observations that reducing the dimension of the problem seems to make the EM algorithm less dependent on initial conditions. This sug-gests to obtain robust posterior probabilities using a reduced vocabulary, and to use them for initializing new rounds of EM iterations, with a larger vocabulary. Proceeding this way allows us to circumvent the problem of initializing the b parameters corresponding to rare words, as we start from the other step of the algorithm (the
M-step). When the vocabulary size is increased, the probabilities associated with new words are implicitly ini-tialized on their average count in the corpus, weighted by the current posterior probabilities. This iterative procedure has the net effect of decomposing the inference process into several steps, each being sufficiently stable to yield estimates having both a small variance and good generalization performance.
 vocabulary, save the values of posterior probabilities at the end of the 15th iteration, and use these values to initialize another round of 15 EM iterations, using the full vocabulary. Our earlier results obtained using the full vocabulary are also reported for comparison. The influence of the initial vocabulary size is important: as it is increased, the maximal score gets somewhat better but the results are more variable.

These results can be improved by making the estimation process more gradual, thus reducing the variability of our estimates. Such experiments are reported in Fig. 7 where we use four different steps. Proceeding this way, both the maximal and minimal scores lie within an acceptable range of performance. It is clear from these experiments that the choice of the successive sizes of vocabulary is particularly difficult, being a tight compro-mise between quality and stability. It remains to be seen how to devise a principled approach for finding such appropriate vocabulary increments. 3.4.2.2. Multiple restarts. Another usual approach in optimization problems where the large number of local optima yields unstable results is to perform multiple restarts and pick up the best run according to some cri-terion. From this point of view, a sensible strategy is to choose the vocabulary size yielding the best maximum performance (for instance, Fig. 6 suggests that starting with 800 words is a reasonable choice), run several tri-als and select the parameter set yielding the best cooccurrence score on the test data. For lack of this infor-mation (as would be the case in a real-life study, where no reference clustering is available), a legitimate question to ask is whether the training perplexity could be used instead as a reliable indicator of the quality of parameter settings. The answer is positive, as is shown in Fig. 8 .

This figure reports results of the following experiments: after 15 EM iterations using a reduced vocabulary of 800 words, we consider the complete vocabulary for another 15 additional EM iterations. Training set per-plexity is computed at the end of iteration 15 and at the end of iteration 30. These measurements are repeated 30 times for each of the 10-folds. The test cooccurrence scores (somewhat representing the quality of cluster-in the area of the best runs (low perplexity values X  X arge cooccurrence scores) we are interested in. Selecting the run with lowest perplexity yields acceptable performance in both cases and the correlation is even stronger on the full vocabulary (it is therefore worth performing 15 more iterations with all the words).

In summary, we have presented in this section two inference strategies which significantly improve over a basic implementation of the EM algorithm:
Split the vocabulary in several bins (at least 4) based on frequency; run EM on the smallest set and itera-tively add words and rerun EM.
 Discard rare words, run several rounds of EM iterations, keep the run yielding the best training perplexity.
Rigouste, Cappe  X  , and Yvon (2005b) reports experiments which show that these strategies can be combined, yielding improved estimates of the parameters. 3.5. Gibbs sampling algorithm
In this section, we experiment with an alternative inference method, Gibbs sampling. The first subsection presents the results obtained with the most  X  X  X aive X  X  Gibbs sampling algorithm, which is then compared with a Rao-Blackwellized version relying on the integrated formula introduced in (7) . 3.5.1. Sampling from the EM formulas
To apply Gibbs sampling, we first need to identify sets of variables whose values may be sampled from their joint conditional distribution given the other variables. In our case, the most straightforward way to achieve this is to use the EM update Eqs. (9) X (11) . Hence, we may repeatedly:
Sample a theme indicator in {1, ... , n T } for each document from a multinomial distribution whose param-eter is given by the posterior probability that the document belongs to each of the themes; Sample values for a , b which, conditionally upon the theme indicators, follow Dirichlet distributions; Compute new posterior probabilities according to (9) .
 Fig. 9 displays the evolution of the training perplexity and the test cooccurrence score for 200 runs of the Gibbs sampler (ran for 10,000 iterations on onefold), compared to the regular EM algorithm and the iterative inference method described in Section 3.4 . The performance varies greatly from one run to another and, occa-sionally, large changes occur during a particular run. This behavior suggests that, in this context, the Gibbs sampler does not really attain its objective and gets trapped, like the EM algorithm, in local modes. Hence, one does not really sample from the actual posterior distribution but rather from the posterior restricted to a  X  X  X mall X  X  subset of the space of latent variables and parameters. Results in terms of perplexity and cooccur-rence scores are in the same ballpark as those obtained with the EM algorithm, several levels below the ones obtained with the ad hoc inference method of Section 3.4.2 . 3.5.2. Rao-Blackwellized Gibbs sampling
There is actually no need to simulate the parameters a and b , as they can be integrated out when consid-ering the conditional distribution of a single theme given in (7) . We then obtain an estimate of the distribution of the themes T of all documents by applying the Gibbs sampling algorithm to simulate, in turn, every latent theme T d , conditioned on the theme assignment of all other documents. This strategy, which aims at reducing the number of dimensions of the sampling space, is known as Rao-Blackwellized sampling, and often produces to the Gibbs sampling algorithm described in Griffiths and Steyvers (2002) for the LDA model (using the iden-tity C ( a +1)= a C ( a )).

Fig. 10 displays the training perplexity and the test cooccurrence scores for 30 independent random initial-izations of the Gibbs sampler, compared to the same references as in the previous section. We plot results obtained on 200 samples, each corresponding to 10,000 complete cycles on onefold. The Gibbs sampler out-performs the basic EM algorithm for almost all runs. Its performance is in the same range as the iterative method, albeit much more variable (the cooccurrence score lies in the range 70 X 95%). The sampler trajectories also suggest that the Gibbs sampler is not really irreducible in this context and only explores local modes.
This alternative implementation of the Gibbs sampling procedure is obviously much better than our first, arguably more naive, attempt: not only does it yield consistently better performance, but it is also much faster. Thanks to the tabulation of the Gamma function, the deterministic computations needed for both versions of the sampler are comparable. But the Gibbs sampler based on the EM formulas requires generating n T +1 Dirichlet samples (with respective dimensions n W and n T ) for a rough total of approximately n W n T Gamma distributed variables for the M-step, and n D samples from n T -dimensional discrete distributions for the E-step. In comparison, the Rao-Blackwellized Gibbs sampling only requires n D n T -dimensional samples from discrete distributions. The difference is significant: our C-coded implementation of the latter algorithm runs 20 times faster than the vanilla Gibbs sampler. 3.6. Comparison with k-means
The k -means algorithm ( MacQueen, 1967 ) remains one of the reference methods for unsupervised cluster-ing tasks, being both easy to use and prone to good generalization performance. For textual applications, k -means has been shown to be faster and more accurate than other basic clustering algorithms, including hierarchical methods ( Steinbach, Karypis, &amp; Kumar, 2000 ). Therefore, we believe that it is a reasonable baseline for comparing document clustering algorithms.

When applying k -means to text clustering, it is usual to multiply the count vectors by the inverse document frequency (idf): This transform has the net effect of reducing the influence of function words, which tend to occur in almost all documents. It is also common practice to normalize these vectors, in order to neutralize the effect of the doc-ument length. Hence, it is natural to compare the resulting vectors using the cosine distance. If we denote by b t the clusters centroids, the distance between the document d and the theme t is: Initialization of k -means is very similar to the procedure used for EM and is based on some randomly gen-erated configuration of the posterior probabilities to avoid initializing centroids. In contrast to the EM initial-ization however, the initial classification is made deterministic: each document is assigned to the most likely theme.

For the sake of readability, we will only display results related to the first 15 iterations. Subsequent itera-tions do not seem to change the relative position of the various curves. Performance is measured using the cooccurrence measure on test data: as k -means is not strictly speaking a probabilistic generative model, there is no natural way to compute the associated perplexity.
 The comparison with EM is reproduced on Fig. 11 . After a few iterations, the k -means algorithm with Dirichlet initialization reaches a better score than EM with the same initialization. For this test set, the k -means approach clearly outperforms EM, albeit at the cost of a slight increase in variability. This result seems to support the view that the cosine distance operating on tf-idf representations is more appropriate than the Kullback divergence criterion defined in (12) . 5 One should, however, note that the cooccurrence score is erence clustering. It is likely that on more difficult corpora, with overlapping categories, the difference between k -means and EM would be less significant.

Fig. 12 compares the performances of k -means with those obtained using Rao-Blackwellized Gibbs sam-pling. On average, the latter is slightly better than the former, even though both are quite close after conver-gence (as said before, more iterations do not yield any significant change). However, as the variabilities are very similar, there is no guarantee that on a particular run Gibbs sampling classification will be preferable to that output by k -means.

As for the comparison with the iterative inference algorithm, the results reported in the previous sections suggest that this methodology would yield better and more stable results than k -means. However, preliminary experiments show that k -means could also benefit from the same kinds of dimensionality reduction technique, which suggests that an iterative algorithm based on k -means would achieve the same kind of performance. 4. Conclusion
In this article, we have presented several methods for estimating the parameters of the multinomial mixture model for text clustering. A systematic evaluation framework based on various measures allowed us to under-stand the discrepancy between the performance typically obtained with a single run of the EM algorithm and the best scores we could possibly attain when initializing on a somewhat ideal clustering.

Based on the intuition that the high dimensionality incurred by a  X  X  X ag-of-word X  X  representation of texts is directly responsible for this undesirable behavior of the EM algorithm, we have analyzed the benefits of reduc-ing the size of the vocabulary and suggested a heuristic inference method which yields a significant improve-ment in comparison to the basic application of the EM algorithm. We believe that this methodology could also be used in conjunction with other clustering algorithms facing problems with high-dimensional data, such as, for instance, the k -means algorithm.
 We have also investigated the use of Gibbs sampling, and proposed two different approaches. The Rao-Blackwellized version, which takes advantage of analytic marginalization formulas clearly outperforms the other, more straightforward, implementation. Performance obtained with Gibbs sampling are close to the ones obtained with the iterative inference method, albeit more dependent on initial conditions.
Altogether, these results clearly highlight the too often overlooked fact that the inference of probabilistic models in high-dimensional spaces, as is typically required for text mining applications, is prone to an extreme variability of estimators.

This work is currently extended in several directions. Further investigations of the multinomial mixture model are certainly required, notably aiming at (i) analyzing its behavior when used with very large numbers (several hundreds) number of themes, as in Blei et al. (2002) ; (ii) investigating model selection procedures to see how they can help discover the proper number of themes; (iii) reducing the overall complexity of the train-ing: both the EM-based and the Gibbs sampling algorithm require to iterate over each document, an unreal-istic requirement for very large databases.

Another promising line of research is to consider alternative models: the multinomial mixture model can be improved in multiple ways: (i) its modeling of the count matrix is unsatisfactory, especially as it does not take consider alternative, albeit more complex models of the counts; (ii) the one document-one theme assumption is also restrictive, pleading for alternative models such as LDA ( Blei et al., 2002 ) or GAP ( Canny, 2004 ): pre-liminary experiments with the former model however suggest that it might be faced with the same type of vari-ability issues as the multinomial mixture model ( Rigouste, Cappe  X  , &amp; Yvon, 2006 ).
 Acknowledgements This work has been supported by France Te  X  le  X  com, Division R&amp;D, under Contract No. 42541441. References
