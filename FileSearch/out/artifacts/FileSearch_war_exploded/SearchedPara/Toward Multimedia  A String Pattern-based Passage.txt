 With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed. The well-known Informedia pro-ject (Wactlar, 2000) and TREC-VID track (Over et al., 2005) are the two famous examples. Although text-based question answering (QA) has become a key research issue in past decade, to support mul-timedia such as video, it is still beginning. 
Over the past five years, several video QA stud-ies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abun-dant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and human-made rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined key-words. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (English-to-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other hand, Zhang and Nunamaker (2004) made use of the simple TFIDF term weighting schema to retrieve the manual-segmented clips for video caption word retrieval. They also manually developed the ontology to im-prove system performance. 
In this paper, we present a new string pattern matching-based passage ranking algorithm for video question answering. We consider that the passage is able to answer questions and also suit-able for videos because itself forms a very natural unit. Lin et al. (2003) showed that users prefer pas-sage-level answers over short answer phrases since it contains rich context information. Our method makes use of the string pattern searching in the suffix trees to find common subsequences between a passage and question. The proposed term weight-ing schema is then designed to compute passage score. In addition, to avoid generating over-length subsequence, we also present two algorithms for re-tokenization and weighting. An overview of the proposed videoQA system can be shown in Figure 1. The video processing com-ponent recognizes the input video as an OCR docu-ment at the first stage. Second, each three consecutive sentences were grouped into a passage. We tokenized the Chinese words with three grained sizes: unigram, bigram, and trigram. Simi-larly, the input question is also tokenized to uni-gram, bigram, and trigram level of words. To re-duce most irrelevant passages, we adopted the BM-25 ranking model (Robertson et al., 2000) to re-trieve top-1000 passages as the  X  X nput passages X . Finally, the proposed passage ranking algorithm retrieved top-N passages as answers in response to the question. In the following parts, we briefly in-troduce the employed videoOCR approach. Section 2.2 presents the sentence and passage segmentation schemes. The proposed ranking algorithms will be described in Section 3. 2.1 Video Processing Our video processing takes a video and recognizes the closed captions as texts. An example of the input and output associated with the whole video processing component can be seen in Figure 2. The videoOCR technique consists of four important steps: text detection, binarization, frame tracking, and OCR. The goal of text detection is to locate the text area precisely. In this paper, we employ the edge-based filtering (Lyu et al., 2005) and slightly modify the coarse-to-fine top-down block segmen-tation methods (Lienhart and Wernicke, 2002) to find each text component in a frame. The former removes most non-edge areas with global and local thresholding strategy (Fan et al., 2001) while the latter incrementally segments and refines text blocks using horizontal and vertical projection pro-files. 
The next steps are text binarization and frame tracking. As we know, the main constituent of video is a sequence of image frames. A text com-ponent almost appears more than once. To remove redundancy, we count the proportion of overlap-ping edge pixels between two consecutive frames. If the portion is above 70%, then the two frames were considered as containing the same text com-ponents. We then merge the two frames by averag-ing the gray-intensity for each pixel in the same text component. For the binarization stage, we em-ploy the Lyu X  X  text extraction algorithm (Lyu et al., 2005) to binarize text pixels for the text compo-nents. Unlike previous approaches (Lin et al., 2001; Chang et al., 2005), this method does not need to assume the text is in either bright or dark color (but assume the text color is stable). At the end of this step, the output text components are prepared for OCR. 
The target of OCR is to identify the binarized text image to the ASCII text. In this paper, we de-veloped a na X ve OCR system based on nearest neighbor classification algorithms and clustering techniques (Chang et al., 2005). We also adopted the word re-ranking methods (Lin et al., 2001, strategy 3) to improve the OCR errors. 2.2 Sentence and Passage Segmentation In this paper, we treat all words appear in the same frame as a sentence and group every three consecu-tive sentences as a passage. Usually, words that occur in the same frame provide a sufficient and complete description. We thus consider these words as a sentence unit for sentence segmentation. An example of a sentence can be found in Figure 2. The sentence of this frame is the cascading of the two text lines, i.e.  X  X peed-up to 17.5 thousand miles per hour in less than six minutes X  For each OCR document we grouped every three continuous sentences with one previous sentence overlapping to represent a passage. Subsequently, we tokenized Chinese word with unigram, bigram, and trigram levels. 
Searching answers in the whole video collection is impractical since most of them are irrelevant to the question. By means of text retrieval technology, the search space can be largely reduced and limited in a small set of relevant document. The document retrieval methods have been developed well and successfully been applied for retrieving relevant passages for question answering (Tellex et al., 2003). We replicated the Okapi BM-25 (Robertson et al., 2000), which is the effective and efficient retrieval algorithms to find the related segmented passages. For each input question, the top-1000 relevant passages are input to our ranking model. Tellex et al. (2003) compared seven passage re-trieval models for text QA except for several ad-hoc approaches that needed either human-generated patterns or inference ontology which were not available. In their experiments, they showed that the density-based methods (Lee et al., 2001) achieved the best results, while the BM-25 (Robertson, 2000) reached slightly worse retrieval result than the density-based approaches, which adopted named entity taggers, thesaurus, and WordNet. Cui et al. (2005) showed that their fuzzy relation syntactic matching method outperformed the density-based methods. But the limitation is that it required a dependency parser, thesaurus, and training data. In many Asian languages like Chi-nese, Japanese, parsing is more difficult since it is necessary to resolve the word segmentation prob-lem before part-of-speech (POS) tagging, and pars-ing (Fung et al., 2004). This does not only make the parsing task harder but also required to train a high-performance word segmentor. The situation is even worse when text contains a number of OCR error words. In addition, to develop a thesaurus and labeled training set for QA is far time-consuming. In comparison to Cui X  X  method, the term weight-ing-based retrieval models are much less cost, portable and more practical. Furthermore, the OCR document is not like traditional text articles that have been human-typed well where some words were error predicted, unrecognizable, and false-alarm. These unexpected words deeply affect the performance of Chinese word segmentation, and further for parsing. In our experiments (see Table 2 and Table 3), we also showed that the use of a well-trained high-performance Chinese word seg-mentation tool gave the worse result than using the unigram-level of Chinese word (13.95% and 13.92% relative precision and recall rates dropped for language model method). To alleviate this problem, we treat the atomic Chinese unigram as word and present a weighted string pattern matching algorithm. Our solution is to integrate the suffix tree for finding, and encod-ing important subsequence information in trees. Nevertheless, it is known that the suffix tree con-struction and pattern searching can be accom-plished in linear time (Ukkonen, 1995). Before introducing our method, we give the following no-tations. 
A common subsequence represents a continuous string matching between P and Q. We further im-pose two symbols on a subsequence. For example, Sub i P means i -th matched continuous string (com-mon subsequence) in the passage, while Sub j Q in-dicates the j -th matched continuous string in the question. The common subsequences can be ex-tracted through the suffix tree building and pattern searching. For example, to extract the set of Sub i P , we firstly build the suffix tree of P and incremen-tally insert substring of Q and label the matched common string between P and Q. Similarly, one can apply a similar approach to generate the set of Sub j Q . By extracting all subsequences for P and Q, we then compute the following score (see equation (1)) to rank passages. The first term of equation (1)  X  X W_Density(Q, P) X  estimates the question word density degree in the passage P, while  X  X W_Weight(Q, P) X  meas-ures the matched question word weights in P.  X  is a parameter, which is used to adjust the importance of the QW_Density(Q, P). Both the two estima-tions make use of the subsequence information for P and Q. In the following parts, we introduce the computation of QW_Density(Q,P) and QW_Weight(Q, P) separately. The time complex-ity analysis of our method is then discussed in the tail of this section. 
The QW_Density(Q, P) is designed for quantify-ing  X  X ow dense the matched question words in the passage P X . It also takes the term weight into ac-count. By means of extracting common subse-quence in the question, the set of Sub j Q can be used to measures the question word density. At the be-ginning, we define equation (2) for weighting a subsequence Sub j Q . i.e., the number of words in Sub j Q .  X  1 is a parameter that controls the weight of length for Sub j Q . In this paper, we consider the long subsequence match is useful. A long N -gram is usually much less am-biguous than its individual unigram. The second term in equation (2) estimates the  X  X iscriminative power X  (DP) of the subsequence. Some high-frequent and common words should be given less weight. To measure the DP score, we extend the BM-25 (Robertson et al., 2000) term weighting schema. Equation (3), (4), and (5) list our DP scor-ing functions. 0.75, 500 respectively (Robertson et al., 2000). frequency of Sub j Q in question Q and passage P. Equation (4) computes the inverse  X  X assage fre-quency X  (PF) of Sub j Q as against to the traditional inverse  X  X ocument frequency X  (DF) where N p is the total number of passages. The collected Dis-covery video is a small but  X  X ong X  OCR document set, which results the estimation of DF value unre-liable. On the contrary, a passage is more coherent than a long document, thus we replace the DF es-timation with PF score. It is worth to note that some Sub j Q might be too long to be further re-tokenized into finer grained size. We therefore propose two algorithms to 1): re-tokenize an input subsequence, and 2): compute the DP score for a subsequence. Figure 3, and Figure 4 list the pro-posed two algorithms. 
The proposed algorithm 1, and 2 can be used to compute and tokenize the DP score of not only Sub j Q for question but also Sub j P for passage. As seeing in Figure 4, it requires DP information for different length of N -gram. As noted in Section 2.2, the unigram, bigram, and trigram level of words had been stored in indexed files for efficient re-trieving and computing DP score at this step. By applying algorithm 1 for the set of Sub j Q , we can obtain all retokenized subsequences (TSub j ). We then use the re-tokenized subsequences to compute the final density score. Equation (6) lists the QW_Density scoring function. T_CNT is the total number of retokenized subse-quences in Q, which can be extracted through ap-plying algorithm 1 for all Sub j Q . Equation (7) merely counts the minimum number of words be-tween two neighboring TSub i , and TSub i +1 in the passage.  X  2 is the parameter that controls the im-pact of distance measurement. 
The density scoring can be thought as measuring  X  X ow much information the passage preserves in response to the question X . On the contrary, the QW_Weight (second term in equation (1)) aims to estimate  X  X ow much content information the pas-sage has given the question X . To achieve this, we further take the other extracted common subse-quences, i.e., Sub j P into account. By means of the same term weighting schema for the set of Sub j P the QW_Weight is then produced. Equation (8) gives the overall QW_Weight measurement. where the DP score of the input subsequence can be obtained via the algorithm 2 (Figure 5). S_CNT is the number of subsequence in P. The parameter  X  is also set as equal as equation (2). 
In addition, the neighboring contexts of a sen-tence, which contains high QW_Density score might include the answers. Hence, we stress on either head or tail fragments of the passage. In other words, the passage score is determined by computing equation (1) for head and tail parts of passage. We thus extend equation (1) as follows. 
Instead of estimating the whole passage, the two divided parts: P 1 , and P 2 are used. We select the maximum passage score from either head (P 1 ) or tail (P 2 ) part. When the passage contains only one sentence, then this sentence is indispensable to be used for estimation. 
Now we turn to analyze the time complexity of our algorithm. It is known that the suffix tree con-struction costs is linear time (assume it requires O( T ), T : the passage length for passage and O( T X  ), T X  : the question length for question). Assume the search time for a pattern in the suffix trees is at most O( h log m ) where h is the tree height, and m is the number of branch nodes. To generate the sets of Sub j Q and Sub j P , it involves in building suffix trees and incrementally searching substrings, i.e., O(( T + T X  )+( T + T X  )( h log m )). Intuitively, both algo-rithm 1, and algorithm 2 are linear time algorithms, which depends on the length of  X  X ommon X  subse-quence, i.e., at most O(min( T , T X  )). Consequently, the overall time complexity of our method for computing a passage is O(( T + T X  )(1+ h log m )+ min( T , T X  )). 4.1 Evaluation We should carefully select the use of videoQA col-lection for evaluation. Unfortunately, there is no benchmark corpus for this task. Thus, we develop an annotated collection by following the similar tasks as TREC, CLEF, and NTCIR. The Discovery videos are one of the popular raw video sources and widely evaluated in many literatures (Lin et al., 2001; Wu et al., 2004; Lee et al., 2005). Totally, 75.6 hours of Discovery videos (93 video names) were used. Table 1 lists the statistics of the Dis-covery films. 
The questions were created in two different ways: one set (about 73) was collected from previ-ous studies (Lin et al., 2001; Wu et al., 2004) which came from the  X  X roject: Assignment of Dis-covery X ; while the other was derived from a real log from users. Video collections are difficult to be general-purpose since hundreds hours of videos might take tens of hundreds GB storage space. Therefore, general questions are quite difficult to be found in the video database. Hence, we provide a list of short introductions collected from the cover-page of the videos and enable users to browse the descriptions. Users were then asked for the system with limited to the collected video top-ics. We finally filter the (1) keyword-like queries (2) non-Chinese and (3) un-supported questions. Finally, there were 253 questions for evaluation. For the answer assessment, we followed the TREC-QA track (Voorhees, 2001) and NTCIR to annotate answers in the pool that collected from the outputs of different passage retrieval methods. Unlike traditional text QA task, most of the OCR sentences contain a number of OCR error words. Furthermore, some sentence did include the answer string but error recognized as different words. Thus, instead of annotating the recognized transcripts, we used the corresponding video frames for evaluation because users can directly find the answers in the retrieved video clips and recognized text. Among 253 questions, 56 of which did not have an answer, while 368 passage&amp;frame segments (i.e., answer patterns) in the pool were labeled as answers. On averagely, there are 1.45 labeled answers for each question. 
The MRR (Voorhees, 2001) score, precision and pattern-recall are used for evaluation. We measure the MRR scores for both top1 and top5 ranks, and precision and pattern-recall rates for top5 retrieved answers. 4.2 Results In this paper, we employed six top-performed yet portable ranking models, TFIDF, BM-25 (Robert-son et al., 2000), INQUERY, language model (Zhai and Lafferty, 2001), cosine, and density-based (Lee et al., 2001) approaches for compari-son 1 . For the language model, the Jelinek-Mercer smoothing method was employed with the parame-ter settings  X  =0.5 which was selected via several trials. In our preliminary experiments, we found that the query term expansion does not improve but decrease the overall ranking performance for all the ranking models. Thus, we only compare with the  X  X ure X  retrieval performance without pseudo-feedback. 
The system performance was evaluated through the returned passages. We set  X  1 =1.25,  X  2 = 0.25, and  X  =0.8 which were observed via the following parameter validations. More detail parameter ex-periments are presented and discussed later. Table 2 lists the overall videoQA results with different ranking models. 
Among all ranking models, the proposed method achieves the best system performance. Our ap-proach produced 0.596 and 0.654 MRR scores when evaluating the top1 and top5 passages and the precision rate achieves 0.208. Compared to the second best method (language model), our method is 10.16% better in relatively percentage in terms of MRR(top1) score. For the MRR(top5) score, our method is 7.39 relative percentage better. In terms of the non-answered questions, our method also covers the most questions (253-69=184) compared to the other ranking models. Overall, the experi-ment shows that the proposed weighted string pat-tern matching algorithm outperforms the other six methods in terms of MRR, non-answered question numbers, precision and pattern recall rates. 
Next, we evaluate the performance with adopt-ing a trained Chinese word segmentation tool in-stead of unigram level of word. In this paper, we employed the Chinese word segmentation tool (Wu et al., 2006) that achieved about 0.93-0.96 re-call/precision rates in the SIGHAN-3 word seg-mentation task (Levow, 2006). Table 3 lists the overall experimental results with the adopted word segmentation tool. In comparison to unigram grained level (Table 2), it is shown that the use of word segmentation tool does not improve the videoQA result for most top-performed ranking models, BM-25, language model, INQUERY, and our method. For example, our method is relatively 17.92% and 16.57% worse in MRR(Top1) and MRR(Top5) scores. In terms of precision and pat-tern-recall rates, it drops 14.91, and 16.94 relative percentages, respectively. For the TFIDF method, the MRR score is almost the same as previous re-sult whereas it decreased 30.34%, and 8.71% pre-cision and pattern-recall rates. On averagely, the four models, BM-25, language model, INQUERY, and our method dropped at least relatively 10% in MRR, precision, and pattern-recall rates. In this experiment, our ranking algorithm also achieved the best results in terms of precision and pattern recall rates while marginally worse than the TFIDF for the MRR(top5) score. 
There are three parameters:  X  ,  X  1 ,  X  2 , in our rank-ing algorithm.  X  controls the weight of the QW_Density(Q, P), while  X  1 , and  X  2 were set for the power of subsequence length and the distance measurement. We randomly select 100 questions for parameter validations. Firstly, we tried to verify the optimal  X  1 via different settings of the remain-ing two parameters. The best  X  1 is then set to verify  X  2 via various  X  values. The optimal  X  is subse-quently confirmed through the observed  X  1 and  X  2 values. Figure 5, 6, 7 show the performance evaluations of different settings for the three pa-rameters. 
As shown in Figure 5, the optimal settings of (  X  1 =1.25) is obtained when and  X  2 =0.25, and  X  =0.75. When  X  1 is set more than 1.5, our method quickly decreased. In this experiment, we also found that large  X  2 negatively affects the perform-ance. The small  X  2 values often lead to better rank-ing performance. Thus, in the next experiment, we limited the  X  2 value in 0.0~3.0. As seeing in Figure 6, again the abnormal high or zero  X  2 values give the poor results. This implies the over-weight and no-weight on the distance measurement (equation (7)) is not useful. Instead, a small  X  2 value yields to improve the performance. In our experiment,  X  =0.25 is quite effective. Finally, in Figure 7, we can see that both taking the QW_Density, and QW_Weight into account gives better ranking re-sult, especially QW_Density. This experiment in-dicates that the combination of QW_Density and QW_Weight is better than its individual term weighting strategy. When  X  =0.8, the best ranking result (MRR = 0.700) is reached. 
Next, we address on the impact of different number of initial retrieved passages using BM-25 ranking models. Due to the length limitation of this paper, we did not present the experiments over all the compared ranking models, while we left the further results at our web site 2 . For the three pa-rameters, we select the optimal settings derived from previous experimental results, i.e.,  X  =0.8,  X  =1.25,  X  2 =0.25. Figure 8 shows the experimental results with different number of initial retrieved passages. When employing exactly five initial re-trieved passages, it can be viewed as the re-ranking improvement over the BM-25 ranking model. As seeing in Figure 8, our method does improve the conventional BM-25 ranking approach (MRR score 0.690 v.s. 0.627) with relatively 10.04% MRR value. The best system performance is MRR=0.700 when there are merely 20 initial re-trieved passages. The ranking result converges when retrieving more than 40 passages. Besides, we also continue the experiments using only top-20 retrieved passages on the actual 253 testing questions. The ranking performance is then further enhanced from MRR=0.654 to 0.663 with 1.37% relatively improved. More and more users are interested in searching for answers in videos, while existing question answer-ing systems do not support multimedia accessing. This paper presents a weighted string pattern matching-based passage ranking algorithm for ex-tending text QA toward video question answering. We compare our method with six top-performed ranking models and show that our method outper-forms the second best approach (language model) in relatively 10.16 % MRR score, and 6.12% pre-cision rates. 
In the future, we plan to integrate the other use-ful features in videos to support multi-model-based multimedia question answering. The video-demo version of our videoQA system can be found at the web site ( http://140.115.112.118/bcbb/TVQS2/ ). 
