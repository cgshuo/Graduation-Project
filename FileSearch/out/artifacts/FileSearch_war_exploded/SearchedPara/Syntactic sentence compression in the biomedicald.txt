 Jimmy Lin  X  W. John Wilbur Abstract We explore a syntactic approach to sentence compression in the biomedical domain, grounded in the context of result presentation for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles, a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a corpus of manually com-pressed examples from MEDLINE: an automatic evaluation using B LEU and a summative evaluation involving human assessors. Experiments show that a syntactic approach to sentence compression is effective in the biomedical domain and that the presentation of compressed article titles supports accurate  X  X  X nterest judgments X  X , decisions by users as to whether an article is worth examining in more detail.
 Keywords Sentence compression Extrinsic evaluation PubMed MEDLINE Genomics IR 1 Introduction Sentence compression has previously been identified as a key component in document summarization. Indeed, the ability to convey the substance of a piece of text, but in a smaller amount of space, is one hallmark of a good summarization system. Although both syntactic and statistical approaches have been employed to tackle this problem, previous attempts largely focus on newswire text. This work differs from previous studies of sentence compression in two important ways: First, we explore an application of syntactic trimming techniques in the biomedical domain. Other than brief mentions by Ruch et al. ( 2003 ) and Lu et al. ( 2006 ) in the context of GeneRIF extraction, this work represents the first systematic attempt, to our knowledge, at tackling sentence com-pression in this highly-specialized domain. Second, we couch sentence compression within the context of result presentation in an information retrieval task. This framing of the problem provides an extrinsic, task-based evaluation grounded in real-world user scenarios.

We present a sentence compression algorithm for article titles from MEDLINE based on syntactic trimming rules that operate over parse trees. This approach was adopted due to its proven effectiveness in previous summarization tasks and the paucity of training data in the biomedical domain. Intrinsic evaluations show that trimmed titles generated by our system are competitive with manually-compressed gold standards, both in terms of automatic metrics (B LEU ) and human judgments of content and fluency. In addition, we evaluated the ability of our compressed titles to facilitate  X  X  X nterest judgments X  X  X  X he decision by a user regarding whether or not an article is worth examining in response to an information need. We found little difference between original and automatically compressed titles, indicating that from a task viewpoint, our system can effectively support users X  decisions while reducing the amount of text they must read.

This article is organized as follows: We begin by describing the task model that underlies our explorations in Sect.  X  X  X otivation X  X . Previous work is reviewed in Sect.  X  X  Related work X  X . Efforts to develop appropriate data resources are outlined in Sect.  X  X  Resource development X  X . The syntactic trimming algorithm is detailed in Sect.  X  X  X  syn-tactic approach to compression X  X . Evaluation is broken up into two sections: Sect.  X  X  X utomatic evaluation X  X  covers automatic evaluations, while Sect.  X  X  X anual evalu-ation X  X  covers manual evaluations. We discuss the significance of our work in Sect.  X  X  X iscussion X  X  and future plans in Sect.  X  X  X uture work X  X  before concluding. 2 Motivation Our work is situated in the context of the PubMed search engine, 1 a freely-accessible gateway to the MEDLINE bibliographic database maintained by the US National Library of Medicine (NLM). This database is viewed by medical professionals, biomedical researchers, and many other users as the authoritative source of information related to the health sciences. MEDLINE contains over 15 million references to articles from approxi-mately 5,000 journals in 37 languages, dating back to the 1960s. In 2006, over 623,000 new citations were added to the database, and it currently grows at a rate of 2 X 4,000 citations daily. The subject scope of MEDLINE is biomedicine and health, broadly defined to encompass those areas of the life sciences, behavioral sciences, chemical sciences, and bioengineering needed by health professionals and others engaged in basic research and clinical care, public health, health policy development, or related educational activities. MEDLINE also covers life sciences vital to biomedical practitioners, researchers, and educators, including aspects of biology, environmental science, marine biology, plant and animal science as well as biophysics and chemistry. 2
Each MEDLINE citation includes basic metadata information such as the title of the article, name of the authors, name of the publication, publication type, date of publication, language, etc. Of the entries added over the last decade or so, approximately 79% have English abstracts written by the authors of the articles.

PubMed is a freely-accessible Web search engine that provides access to the MEDLINE database, developed by the National Center for Biotechnology Information (NCBI) at NLM. The system provides an array of query operators that allow users to query in specific data fields (title, author, etc.) and leverage Medical Subject Headings (MeSH), drawn from NLM X  X  controlled vocabulary thesaurus. MeSH terms are assigned manually by trained human indexers with the assistance of automated systems (Aronson et al. 2004 ).

A recently-revised functionality in PubMed is the  X  X  X elated Links X  X  feature. When the user examines a MEDLINE abstract, the right panel of the browser is automatically populated with titles of articles that may also be of interest, as determined by a probabi-listic content similarity algorithm (Wilbur 2005 ) X  X n short, PubMed implicitly issues a query for related articles whenever a user pulls up a MEDLINE abstract to examine in detail. The goal of this feature is to unobtrusively suggest other interesting citations to facilitate knowledge discovery. The screenshot in Fig. 1 shows the arrangement of the PubMed search interface.

Article titles serve as document surrogates for presentation in the  X  X  X elated Links X  X  panel. Other space in the current PubMed interface is reserved for additional features that faced with a tradeoff between quality and quantity: either show more related articles in less detail or show fewer related articles in more detail. In the simple case, this tradeoff can be implemented by controlling the amount of associated metadata that is displayed and truncating the title based on a fixed length quota. This work explores a more sophisticated solution based on linguistic analysis X  X e hypothesize that sentence compression tech-niques can potentially deliver the best of both worlds: by shortening article titles in a linguistically meaningful way, the interface could deliver much of the same substance in a smaller amount of space.

Ideally, the output of a compression algorithm should be both indicative and informative X  properties often discussed in the context of document summarization (Afantenos et al. 2005 ). Indicative summaries suggest the content of an underlying information object without nec-essarily giving away details X  X ften the aim is to entice users, or at least alert them to the presence of a particular information object. Movie trailers and book jackets are good examples. In contrast, an informative summary is meant to represent (and sometimes replace) the original object. Take the case of summarizing news articles: indicative summaries might mention the entities involved in a particular event, but not actually say what happened . An informative summary might focus on what happened, but neglect to mention the roster of participants. See (Kan et al. 2001 ) for an example of an attempt to introduce this distinction into summarization systems. In our application, document surrogates for related articles (displayed in the  X  X  X elated Links X  X  panel) should ideally satisfy both properties X  X e indicative as to capture users X  interest and be informative in order to convey sufficient substance.

Ultimately, the related links feature in PubMed is designed to guide users to other articles of interest X  X o support information seeking or knowledge discovery. This under-lying task model guides our work and provides a framework for extrinsic evaluation. The goal of the document surrogates (i.e., compressed article titles) is to facilitate interest judgments, or user decisions on whether or not to examine a particular citation. We can realistically measure the effectiveness of a compression algorithm by its ability to support such decisions.

In this context, the notion of  X  X  X nterest judgment X  X  differs from the more traditional judgment of relevance that forms the basis of most retrieval applications. Ultimately, retrieval systems aim to deliver information that addresses users X  information needs. However, relevance is a multi-faceted consideration that takes into account a multitude of factors X  X ee (Mizzaro 1998 ) as a starting point into the rich body of literature on rele-vance. We do not believe it is possible to assess an article X  X  relevance from only the title (or any short surrogate), and hence it would not be meaningful to examine relevance directly in our task context. However, an important intermediate step is the decision to examine an abstract in more detail X  X hich we call an interest judgment. Such a decision will then cause a user to bring up more details about the article (abstract text, authors, and other metadata) in order to make a more informed decision about relevance. The related links feature in PubMed is exactly designed to elicit such interest.

Furthermore, our definition of  X  X  X nterest X  X  opens the door to different types of relations beyond relevance X  X  citation may be interesting, not because it is potentially relevant to the present information need, but because it raises questions that the user may not have previously considered. These types of serendipitous connections underlie many significant breakthroughs in the life sciences, and PubMed aspires to assist in this process of knowledge discovery by drawing links where none previously existed. 3 Related work Our approach to sentence compression is most similar to the work of Zajic et al. ( 2004 )in that both use a series of linguistically-motivated trimming rules to remove inessential fragments from the parse tree of a sentence X  X dditional details can be found in (Dorr et al. 2003 ; Zajic et al. 2007 , in press). This approach has proven to be highly effective at generating very short summaries of single newswire documents (i.e., the headline gener-ation task), as evidenced by its performance at the DUC 2004 summarization evaluation. Topiary, the University of Maryland X  X  system which integrates  X  X  X arse-and-trim X  X  tech-niques with topic term extraction, was among the highest scoring systems for all tasks on all measures X  X n some cases, even beating the performance of humans in terms of auto-matic metrics. Other systems that make use of similar techniques include (Mani et al. 1999 ; Jing 2000 ), and more recently, (Blair-Goldensohn et al. 2004 ; Conroy et al. 2005 ). In our approach, sentence compression is achieved by removing elements X  X o attempt is made to reorder material within a sentence. Since our task does not involve multiple sentences, there is no opportunity to generate output that combines fragments from dif-ferent sources, for example, including one sentence as a relative clause inside another (Mani et al. 1999 ). Thus, we conceive of sentence compression solely as the task of selecting sentential elements (words, phrases, clauses, etc.) to remove.

Sentence compression has also been tackled with supervised machine learning tech-niques using a noisy-channel model. Verbose text can be viewed as the output of passing the original compressed text through a  X  X  X oisy channel X  X  that inserts additional inessential content. Given the verbose text, the system X  X  task is to reconstruct the original message. The problem can be modeled in terms of simple word-level features, as in (Banko et al. 2000 ), or in terms of parse tree structures, as in (Knight and Marcu 2000 ; Turner and Charniak 2005 ). One downside of these statistical approaches is the need for annotated training data to learn model parameters. On the other hand, since trimming rules are able to exploit human linguistic insight, far less data is required for system development. Nevertheless, both methods can be viewed as complementary.

Another approach to generating very short summaries is to extract a list of topic descriptors indicative of content; examples include (Bergler et al. 2003 ; Zhou and Hovy 2003 ; Wang et al. 2005 ). The output of such techniques consists of, for the most part, noun phrases X  X s such, they are useful for telling a user what the important entities are, but less useful for conveying what actually happened. In other words, system output is indicative, but often not informative.

Although document summarization techniques have principally been applied to news-wire text, there is a body of research that deals specifically with the summarization of medical documents X  X ee (Afantenos et al. 2005 ) for a survey. A noteworthy example is PERSIVAL (McKeown et al. 2003 ; Elhadad et al. 2005 ), which leverages patient records to generate personalized summaries. In the genomics domain, automatic summarization techniques have also been applied to extracting GeneRIFs, concise phrases describing the function of genes (Ruch et al. 2003 ; Ling et al. 2006 ; Lu et al. 2006 ). In particular, Ruch et al. ( 2003 ) and Lu et al. ( 2006 ) both briefly mention methods for removing inessential elements from GeneRIFs, which are similar in spirit to our sentence compression tech-niques. In comparison to these cited articles, both on summarization of newswire and biomedical text, what sets our work apart is a focus on sentence compression as a tool to facilitate knowledge discovery in the context of an information retrieval system.
An important part of summarization research focuses on methodologies for evaluating system output, which can be broadly classified into two categories. In an intrinsic evalu-ation, system output is directly evaluated in terms of some set of norms X  X or example, fluency (Minel et al. 1997 ), coverage of key ideas (Paice 1990 ; Brandow et al. 1995 ), or similarity to an  X  X  X deal X  X  summary (Kupiec et al. 1995 ). In particular, the last criterion has been operationalized in R OUGE (Lin and Hovy 2003 ), an automated metric that compares system output to a number of human-generated  X  X  X eference X  X  summaries. The primary difficulty, however, lies in establishing an ideal reference (or a set of such texts) X  X um-maries are generated for different purposes, and the human-centric nature of the task means that there is more than one  X  X  X orrect answer X  X . Operationally, this results in low interan-notator agreement on tasks such as sentence extraction (Salton et al. 1997 ).

In contrast to intrinsic evaluations, extrinsic evaluations attempt to measure how summarization impacts some other task. Developing realistic usage scenarios is chal-lenging, but often the  X  X  X oodness X  X  of a summary can only be meaningfully operationalized in its  X  X  X sefulness X  X  for a particular task. One might, for example, measure how summaries impact question answering (Morris et al. 1992 ; Mani et al. 2002 ) or relevance judgments (Dorr et al. 2005 ). One possible hypothesis is that summaries allow users to make quicker decisions (since they have to read less), without compromising the quality of those deci-sions. Along these lines, our work is grounded in an information retrieval task, which allows us to assess the potential real-world impact of our sentence compression techniques. 4 Resource development Since we are not aware of any existing resources for the sentence compression task in the biomedical domain, we devoted significant effort to creating a corpus of annotated examples. Our collection consists of article titles that have been manually shortened by domain experts. Instead of randomly sampling citations from the MEDLINE database, we leveraged the test collection developed from the TREC 2005 genomics track (Hersh et al. 2005 ), which fits well with our task model (finding articles of interest in the context of an ongoing search for information).

One salient feature of the TREC 2005 genomics track evaluation is its use of generic topic templates (GTTs) to capture users X  information needs, instead of the typical free-text title, description, and narrative combinations used in other ad hoc retrieval tasks. The GTTs consist of semantic types, such as genes and diseases, that are embedded in common genomics-related information needs, as determined from interviews with real biologists. In total, five templates were developed, with 10 fully-instantiated topics for each X  X xamples are shown in Fig. 2 . Note that in some cases, the actual topics deviated slightly from the template structure (in order to accommodate real requests).

The genomics track employed a 10-year subset of the MEDLINE database (1994 X  2003), which totals 4.6 million citations, or approximately a third of the size of the entire MEDLINE database at the time it was collected in 2004. Each citation is identified by a unique pmid. In total, 32 groups submitted 59 runs to the task (both manual and automatic), which insured a rich, diverse pool of results. Relevance judgments were provided by an undergraduate student and a Ph.D. researcher in biology.

First, we randomly sampled 200 titles from the known list of relevant citations, and another 200 titles from the known list of irrelevant citations. These were then merged and randomized, producing a total of 400 titles, half of which were relevant according to the original assessors in the TREC task. We adopted this sampling process in order to obtain a good balance X  X  truly random sampling of the citation pool would yield a much larger fraction of irrelevant documents.

Next, these titles were presented to two human annotators X  X oth were subject domain experts otherwise uninvolved with the project. Throughout this paper, we will refer to these annotators as  X  X  X o X  X  (Ph.D. in bioinformatics, B.S. in molecular biology) and  X  X  X y X  X  (Ph.D. in human genetics). They were provided both the original title and the information need that the citation was retrieved to address (i.e., the template with concept instantiations). Both annotators were asked to generate a compressed version of the title by removing unimportant elements from the full title. After this was done, the 400 annotated pairs were divided into a development set and a held-out test set.

We attempted to align the resource development process with our task model as much as possible. To start, the TREC genomics track employed a subset of the MEDLINE database, which gave us a degree of confidence that findings could be directly applied to PubMed. Human generation of the compressed titles occurred in the context of infor-mation needs, much like the task setup in related article search (i.e., browsing the  X  X  X elated Links X  X  panel in PubMed). Furthermore, the needs are those typical of a par-ticular user population, since they were generalized from interviews with real biomedical researchers. We believe that this collection encapsulates the original end-to-end task with great fidelity, thus enabling the results of laboratory experiments to be applicable in real-world environments.

Characteristics of our annotated data are shown in Table 1 . We show the average compression ratio and the average length reduction (with standard deviation) both in terms of characters and words. Compression ratio is computed as the length of the compressed sentence divided by the length of the original sentence, averaged across the entire data set of 200 sentences. Thus, the smaller the number, the shorter the output is. We note that there appear to be more opportunities for compression in the development set (given the lower compression ratios). Abstract titles in the development set averaged 102.3 characters, or 13.34 words; abstract titles in the test set averaged 104.0 characters, or 13.60 words. 5 A syntactic approach to compression We adopt a syntactic approach to sentence compression through the use of linguistically-motivated trimming rules that remove fragments of parse trees. Our work employs the Stanford Parser (Klein and Manning 2003 ). 3 Although it was not originally designed to parse text in the biomedical domain, experimental results show that a syntactic approach is nevertheless effective for compressing abstract titles in the biomedical domain (more on this in Sect.  X  X  X iscussion X  X ). The techniques described here are not tied to a particular parser and will function with any system that utilizes the Penn Treebank conventions.
From the development set, we came up with seven linguistically-motivated rules X  many of these are similar to those discussed in (Zajic et al. 2004 ). These rules are described below, arranged roughly in increasing order of complexity. An example of each is shown in Table 2 ; those examples illustrate the application of each rule in isolation.  X  Subtitles: Subtitles, denoted with a colon or consecutive dashes, are removed. This is  X  Determiners (DT): Determiners are removed. All terminals in the parse tree assigned  X  Participial and Gerund phrases (VBG): Participial and gerund phrases are removed.  X  Serial PPs: In a sequence of prepositional phrases (sharing a common ancestor), all but  X  Nested PPs: Any prepositional phrase that is embedded three or more levels deep is  X  Conjoined NPs: For conjoined noun phrases, the second conjunct is removed. This rule  X  Simple NPs: Adjectives in NPs are removed, unless their heads are  X  X  X ightweight X  X . For
The above trimming rules were formulated after examining sentence pairs in the development set and recognizing opportunities to remove inessential fragments of titles based on syntactic structure. This was not accomplished in a systematic way, and no doubt there are other opportunities that can be exploited. Nevertheless, as our evaluations show, these rules provide the basis for an effective sentence compression algorithm. The order in which the trimming rules are sequentially applied in our final implementation was guided by the evaluation of individual rules, described in the next section. 6 Automatic evaluation Although end-to-end task-based evaluations provide the best method for assessing information systems, the large amount of manual effort typically required for such evaluations precludes using them for system development. In human language technologies, researchers often employ a paradigm based on automatic metrics for system development, capped with a summative evaluation. Our work follows in this model. This section describes a series of experiments that characterize the effectiveness of our syntactic trimming techniques using automated methods. A manual evaluation is detailed in Sect.  X  X  X anual Evaluation X  X . 6.1 Evaluation methodology Automatic evaluation methods are attractive because they enable quick experimental turnaround, thereby facilitating rapid exploration of the solution space. An easily quanti-fiable performance metric provides researchers with an objective function over which they can optimize. Once an automatic metric has been validated X  X hat is, demonstrated to correlate with human preferences X  X he measure can be exploited for system development.
In the language processing community, researchers have developed a family of auto-matic metrics based on the idea of comparing system output to one or more human-generated references. Similarity to these  X  X  X old standards X  X , according to different content overlap metrics, can quantify the quality of system output X  X his represents a well-estab-lished evaluation methodology in the language processing community. Two such commonly-used metrics are B LEU (Papineni et al. 2002 ) for machine translation and R OUGE (Lin and Hovy 2003 ) for document summarization. Both rely on computing n -gram overlap between system output and human references (manually-translated sentences and human-generated summaries, respectively), but the details differ. In general, B LEU is a precision-oriented metric that places heavy emphasis on fluency, i.e., checking to make sure that the system output is  X  X  X ood English X  X . This is important in evaluating machine translations since automated systems have a tendency to produced garbled sentences. R
OUGE , a metric developed for document summarization tasks, on the other hand, focuses on recall of content, i.e., the presence or absence of certain topic terms. Because most modern document summarization systems are extractive, generation of disfluent output is not as severe a problem. 4 Thus, summarization metrics emphasize the inclusion of key facts or concepts in the system output, as measured in terms of n -gram overlap.

Given these considerations, we decided that B LEU is the more appropriate metric for our sentence compression task X  X rimarily because we are concerned with the fluency of system output. Conceptually, sentence compression can be viewed as  X  X  X ranslating X  X  from verbose English into succinct English. According to previous work (Lin 2004 ), R OUGE -1 recall cor-relates best with human judgments on the headline generation task X  X onstructing a very short summary (less than 75 characters) from a single newspaper article. This is the closest sum-marization analog to our task, but we believe that R OUGE -1 is not an appropriate metric. The measure focuses on unigram overlap, i.e., the presence of content words in system output, and is not sensitive to fluency considerations; a grammatical sentence and a random sequence of the same words would receive identical scores. This characteristic does not fit with our desire to assess the grammatical correctness of system output, which is an important concern since removing certain portions of the parse tree may yield ungrammatical output. B LEU ,onthe other hand, is better able to model fluent English text since it takes into account n -grams of different lengths. Although the metric considers only the surface properties of machine output and lacks even a rudimentary model of syntax and semantics, it has proven highly effective in guiding research in machine translation. Despite its deficiencies, B LEU has enabled rapid progress in translation technology over the past few years.

To get a sense of how much the assessors agreed with each other, we computed B LEU scores, using one as the  X  X  X ystem output X  X  and the other as the reference. These results are shown in Table 3 . Note that these values are lower than many of the B LEU scores reported for our compression algorithm because only one set of references is used; in all other experiments, system output is evaluated against both sets of human references. 6.2 Application of individual rules We first examined each rule in isolation. Table 4 shows results for all rules expect for the simple NP rule, which requires an additional parameter and is therefore discussed sepa-rately. For each rule, we note the number of titles that triggered the rule (out of 200 in each set) in the second column of the table. This value quantifies the prevalence of each phenomena. The next four columns show average compression ratio and average length reduction (with standard deviation), both at the character and word levels. These values are computed over affected titles only (the set of abstract titles that triggered the rule). The B
LEU scores were computed across the entire data sets (all 200 sentences), using both  X  X  X o X  X  and  X  X  X y X  X  as the references.

How is one supposed to interpret these results? The effectiveness of each rule is quantified in two ways: the amount of compression achieved and how  X  X  X ood X  X  the resulting output is (compared to human-generated references using the B LEU metric). Thus, each rule represents a tradeoff along these two dimensions. Leaving the abstract titles untrimmed (the first row in Table 4 ) represents a baseline. Not surprisingly, application of the trim-ming rules in most cases raises the B LEU score, indicating that the results are closer to the references than the original full title in terms of n -gram content. In most cases, gains observed in the development set carried over to the held-out test set, although the mag-nitude of the improvements were smaller (but recall that the human-annotated gold standards suggest fewer opportunities for trimming in the test data). In general, the scope of the rules (i.e., number of affected titles) and the degree of compression were comparable.

The performance of the simple NP trimming rule in isolation is shown in Fig. 3 . Corpus statistics required for the idf calculation were extracted from the 10-year MEDLINE collection used in the TREC 2005 genomics track. We varied the idf threshold from 2.0 to 10.0 in increments of 0.5, and obtained a plot that relates the B LEU score to the average compression ratio at the character level. For this graph, average compression ratio is computed on all the titles, even those that were unaffected by the rule (since the threshold controls how many titles trigger the rule). The corresponding graph for average com-pression ratio at the word level looks nearly identical, and is not shown here. It is interesting to note that no threshold actually increases the B LEU score above the baseline (no compression), indicating that users engage in more complex behavior than simply removing modifiers of noun phrases based on idf values. Trends observed in the devel-opment set carry over to the held-out data, although the rule yields less compression.
Given that each rule represents a tradeoff between output quality and compression ratio, how can one assemble a complete compression algorithm for MEDLINE abstract titles? In response, we examine sequential application of the trimming rules, which yields curves that characterize the tradeoffs mentioned. In Sect.  X  X  X anual evaluation X  X , we report results on manual evaluation of system output at two specific points on this tradeoff curve. 6.3 Sequential application of multiple rules After examining each rule in isolation, we experimented with applying the rules sequen-tially. The rules were ordered based on B LEU scores on the development set (Table 4 ): subtitle, DT, VBG, nested PP, serial PP, conjoined NP, and finally simple NP. Results are displayed in Fig. 4 ; the top plot shows compression ratios computed at the character level, and the bottom plot shows compression ratios computed at the word level. The horizontal dotted lines denote the B LEU scores of the original (untrimmed) article titles. The left tails of the curves represent the application of the simple NP rule with different idf thresholds (we varied the parameter from 2.0 to 10.0 in increments of 0.5). The labels indicate the points at which each rule is applied (except for the simple NP rule to reduce clutter). In these plots, points closer to the upper left hand corner are  X  X  X etter X  X , in the sense that we desire large reductions in length while maintaining fidelity to the human generated references.

We notice that successive application of individual rules is subjected to a  X  X  X iminishing returns X  X  effect. The amount of compression achieved with the application of multiple rules is strictly less than the sum of the compression achieved by individual rules. This occurs because earlier rules can eliminate opportunities for later rules to apply. Take the example of prepositional phrases or conjoined NPs inside a gerund phrase. Since the VBG rule applies earlier, the entire phrase would have already been eliminated.
 The same general characteristics are observed in both the development and test sets: B
LEU scores initially rise and then drop as the rules more and more aggressively trim away parts of the structure. Untrimmed sentences (right edges of the graphs) serve as a base-line X  X ut note that it is possible for our compression algorithm to perform worse if too much is removed from the abstract titles. The dip in the performance curve on the test set can be attributed to the relative performance of the PP trimming rules: one was found to be more effective in the development set, but the reverse turned out to be true in the test set.
How can one interpret these results, especially since B LEU scores do not correspond to any quantity that humans have an intuition for? There are two responses to this question: relative differences in B LEU are meaningful X  X n that they tell us if one variant is  X  X  X etter X  X  than another (in terms of matching human references).

However, the more appropriate response is to acknowledge the limitations of automatic scoring metrics. Ultimately, our goal is to develop information systems that are useful for humans, and one way of operationalizing  X  X  X sefulness X  X  is in terms of task performance. Therefore, we believe that the question  X  X  X ow good is a B LEU score of 0.452? X  X  is not pertinent. Rather, we must ask if particular techniques can better assist humans in accomplishing real-world tasks. System development, as guided by automatic metrics, only serves as a stepping stone to extrinsic task-based evaluations. In the next section, we report results from exactly such a study. 7 Manual evaluation In our initial experiments, B LEU primarily served as a formative tool to guide system development. We then conducted a summative task-based evaluation to assess the usefulness of our sentence compression algorithm. At the same time, we also collected human judgments about the intrinsic quality of the compressed output.

Recall from Sect.  X  X  X otivation X  X  that we situate sentence compression in the context of related article search in PubMed X  X n particular, as a method for efficient presentation of compression algorithm is to support interest judgments, that is, a user X  X  decision to examine a citation in detail. The default condition is to show the full title, which serves as a baseline. If the output of our sentence compression algorithm is able to provide the same level of decision support (in terms of accuracy of interest judgments), but with a smaller amount of text, then we can claim to have improved on the baseline. The ability to convey much of the same content in fewer words can be leveraged in two different ways: PubMed can use a smaller screen area for  X  X  X elated Links X  X , thereby freeing up space or other content elements, or PubMed can display more related articles in the same amount of on-screen space.

Three subject domain experts uninvolved with system development were recruited as assessors. Two of them ( X  X  X o X  X  and  X  X  X y X  X ) were the same individuals involved in creating our training and test sets. The third individual ( X  X  X a X  X ) was not involved in any other aspect of the project.

Our experiments involved 100 article titles randomly sampled from the development set. 5 For each title, we randomly assigned one of four conditions to the trimmed output (25 examples each), described below. The average compression ratio (at the character level) of each condition is shown in the second column of Table 5 .  X  Lo: Manually compressed titles by the annotator  X  X  X o X  X .  X  Ly: Manually compressed titles by the annotator  X  X  X y X  X .  X  Variant A : Application of the following rules to the original titles: subtitle, DT, VBG,  X  Variant B: Variant A plus the application of the simple NP rule, with an idf threshold of
The assessment proceeded in two rounds. In the first round, assessors were provided with the information need and the trimmed title of the abstract. Naturally, they did not know the source of the trimmed titles (i.e., which condition). Assessors were asked to rate the fluency of the title on a scale of 1 X 5 (1 = worst, 5 = best). They were also asked if they would read the citation in response to the information need (i.e., a judgment of interest).

In the second round, assessors were provided with the information need, the original title, the trimmed title, and their interest judgment from the first round (i.e., response to  X  X  X ould read abstract? X  X ). They were asked to rate the content of the trimmed title, in terms of capturing essential elements from the full title, on a scale of 1 X 5 (1 = worst, 5 = best). In addition, they were asked if they would now read the citation given the full title.

This two-phase setup was designed to evaluate both the intrinsic quality of the com-pressed titles and their effectiveness in a task context. Fluency and content judgments characterize the inherent quality of the compressed titles, while the interest judgments ground our evaluation in a real-world scenario. We were especially concerned about differences in interest judgments from round one and round two. If both responses were the same, we can conclude that the article title was shortened successfully, i.e., the process did not interfere with task performance. A  X  X  X es to no X  X  flip provides evidence that the com-hand, provides evidence that essential elements from the title were mistakenly removed. 6
Fluency and content judgments from our human assessors are provided in Table 5 , which shows ratings given by all three assessors on all four conditions (mean and standard deviation). Results suggest that variant A is not any more disfluent than the human-compressed gold standards: two of the three assessors actually placed the machine-gen-erated output ahead of one of the gold standards. Note, however, that variant A titles were on average longer than human references. Machine-generated output of comparable compression (variant B) was found to be consistently less fluent than the other conditions. Similar trends are observed for content ratings: variant A appears to be as good as human output, whereas variant B is less so. Overall, there appears to be much variability in these judgments, suggesting that differences exist in the assessors X  interpretation of the task. In particular, we note that Lo preferred Ly X  X  output to Lo X  X  own, in terms of both fluency and content. Similarly, Ly preferred Lo X  X  output to Ly X  X  own (again, both fluency and content). We currently have no reasonable explanation for this observation.

How does the quality of trimmed titles affect users X  task performance? The answer can be found in the tally of flips in interest judgments, as shown in Table 6 . These results appear to suggest that compressed titles have relatively minimal impact on users X  ability to decide whether they want to examine a citation. There does not appear to be much dif-ference between variant A and either one of the human-generated compressions, although variant B titles caused more flips.

Another way of organizing the results is to compare users X  interest judgments on the full title (from the second round) with their judgments on compressed titles (from the first round). This is similar to the consistency test employed in the TIPSTER SUMMAC evaluations (Mani et al. 2002 ). Results can be analyzed in terms of the contingency table shown in Table 7 , from which we can compute standard aggregate statistics:
Results of this analysis are presented in Table 8 . According to the three assessors, variant A titles perform on par with the human-compressed versions, although humans are able to achieve more compression. Variant B titles, which are approximately the same length as the human-generated references, appear to perform worse in terms of precision, recall, and F -score. Note that the absolute performance achieved by human-compressed titles and the output of variant A is very high X  X lmost perfect in many cases. This confirms our basic premise that inessential fragments from MEDLINE article titles can be removed without affecting the substance of what is conveyed.

What are the implications of our findings? Our syntactic compression algorithm is able to shorten abstract titles by approximately 30% without noticeably affecting task perfor-mance (variant A). This translates into less material for the user to read, or alternatively, over 40% more content per unit area. In the same space that it takes to display five full abstract titles, we can now display seven. We believe this is a significant result because it provides users access to more potentially interesting articles without requiring them to read more text. 8 Discussion At a broader level, we believe that this work is significant in two ways. First, the appli-cation of syntactic compression techniques in the biomedical domain raises interesting questions about the domain portability of existing language processing tools. Second, we view this work as a case study highlighting the importance of task-based evaluations in grounding summarization technology. This section elaborates on both points. 8.1 Domain adaptation (Or lack thereof) Issues surrounding the portability of text processing algorithms have recently gained interest in the research community. Due to the availability of corpora and other resources, most modern statistical tools are trained on newswire text, and hence specialized for processing text from that genre, even though many other types of text are worth exploring. Thus, an important consideration in the development of language technologies is its ability to generalize across different domains and genres of text. In this work, we demonstrate that sentence compression techniques originally developed for news articles can be effectively applied to compress MEDLINE article titles. In some ways, this result is somewhat sur-prising, as we explain below.

First, the biomedical domain offers significant challenges to off-the-shelf parsers. The lexical overlap between MEDLINE abstracts and typical news corpora is surprisingly small (Smith et al. 2005 ), which creates challenges for parsers X  X ee, for example, (Clegg and Shepherd 2005 ; Grover et al. 2005 ; Lease and Charniak 2005 ). As a specific example, prepositional phrase attachment is problematic for statistical parsers trained on newswire text, since noun phrase heads are often unknown lexical items in the biomedical domain. Consider examples taken from Table 2 : (1) Electroporation-mediated interleukin-12 gene therapy [ PP for hepatocellular carci-(2) Semiquantitative immunoblot analysis [ PP of nm23-H1 and -H2 isoforms [ PP in
These abstract titles are typical of those in MEDLINE X  X haracterized by sequences of consecutive prepositional phrases. The examples above are annotated with actual structures assigned by the Stanford Parser, which are correct in both cases. The attachment of PPs, whether to the immediately preceding noun head X  X he case with example (2) X  X r another head higher up in the structure X  X he case with example (1) X  X s a complex decision that often requires semantic knowledge. Often, the Stanford Parser is incorrect in its choice of PP attachment point, as illustrated by the following parse: (3) Induction [ PP of cell cycle arrest and morphological differentiation [ PP by Nurr1 and
Rather, the correct structure should be: (4) Induction [ PP of cell cycle arrest and morphological differentiation ][ PP by Nurr1 and
Nevertheless, our nested and serial PP rules appear to be insensitive to these errors because they were engineered by examining Stanford Parser output. Since the parser appears to make systematic errors, we are still able to capture generalizations X  X lbeit these rules may not represent linguistically valid generalizations in the biomedical domain. In example (3), the serial PP rule removes the PP  X  X  X n dopamine MN9D cells X  X , which appears to yield a reasonable compression.

Second, MEDLINE article titles are quite different from sentences that occur in newswire text X  X ost of the time, titles are not even complete sentences. Since titles are often noun phrases or verb phrases, our approach must not only cope with out-of-domain effects (most notably, unknown lexical items), but also stylistic differences. Experiments suggest that the syntactic trimming approach is also capable of handling such diver-gences, given a set of rules developed specifically for the biomedical domain. It is noteworthy that respectable performance is achieved in our application without any domain adaptation. 8.2 Grounding summarization in real-world tasks This work serves as a case study illustrating the importance of grounding summarization tasks in real-world user scenarios. To a human, a fluency score of four (out of five) is not particularly meaningful, and neither is a 0.314 B LEU score. However, quantifying per-formance in terms of decision-making accuracy on compressed titles (as compared to the full titles) is informative because it illustrates how summarization techniques assist the user X  X  end task, that of knowledge exploration and information gathering. In general, we believe that information presentation issues provide a general framework for task-based evaluation of summarization systems; see also, (Mani et al. 2002 ; Dorr et al. 2005 ) for similar setups.

We would like to end this section with a discussion of our underlying task model. In most retrieval tasks, the assumption is that the user issues a query to a search engine and obtains a ranked list of documents that are potentially relevant. This output then serves as the starting point to browsing, selection, examination, and query reformulation mecha-nisms that may ultimately lead to the satisfaction of the information need. However, this traditional query-centered model does not describe the only possible pattern of user-system interactions. In particular, PubMed attempts to draw connections between articles in MEDLINE by unobtrusively displaying titles that may be of interest. This mechanism provides users with another device for exploring the information space. In fact, previous simulation studies have shown that such a feature can improve performance, as measured by traditional ranked retrieval metrics (Wilbur and Coffee 1994 ; Smucker and Allan 2006 ). Although we focus primarily on the related links features in PubMed, our syntactic compression techniques are equally applicable to other components in the retrieval envi-ronment, e.g., summarizing ranked lists so that more results can be displayed on any given Web page. 9 Future work With respect to syntactic compression in the biomedical domain, there are two distinct threads of future work worth exploring X  X mprovements to the compression algorithm and application of similar techniques to related problems. We briefly discuss each.

The simple NP rule is currently the only rule that is parameterized X  X n this case, an idf threshold. The same idea could be applied to other rules. 7 For example, the system could use a threshold to determine if the head of a prepositional phrase was  X  X  X ightweight X  X , and then factor in this evidence to determine if the PP was removable. In the same way, the conjoined NP rule could also be subjected to this modification.

However, the introduction of parameterized rules adds additional complexity to rule ordering. Currently, trimming rules are applied sequentially in a fixed order (based on individual performance in isolation). This neglects possible interaction effects between rules, which would certainly increase with the introduction of parameters. Zajic ( 2007 ) explored a solution to a similar problem by allowing multiple simultaneous rule applica-tions, and then developing a mechanism to select among the multiple compressed candidates. We believe that the same idea can be applied here.

Beyond applications in information retrieval, sentence compression techniques can also be used for other tasks in the biomedical domain. The extraction of GeneRIFs is one such possibility. 8 GeneRIFs are concise phrases describing a function of a gene, explicitly linked to the Entrez Gene database. The extraction and linking of these descriptions is important for biologists and other researchers, since such information would otherwise be scattered in many disparate articles and sources. GeneRIFs by design are limited to 255 characters, 9 so brevity is highly desired. As previously discussed, summarization techniques have been successfully applied to extracting GeneRIFs (Ling et al. 2006 ; Lu et al. 2006 ) X  X hese systems could additionally benefit from the syntactic compression techniques discussed in this article, to eliminate inessential material from the extracted phrases. 10 Conclusion Like much previous work, this paper examines the task of sentence compression. However, our perspective is novel in two different ways: we explore the problem in the domain of biomedicine and within the context of an information retrieval task. The contributions of this study are two-fold: first, we demonstrate that the syntactic trimming approach, which has proven effective in the newswire domain, is portable to the biomedical domain. Sec-ond, our work highlights the importance of extrinsic evaluations and grounding summarization in real-world tasks. It is our hope that we can eventually transition sum-marization technology into the PubMed search engine.
 References
