 Query auto-completion (QAC) facilitates user query composition by suggesting queries given query prefix inputs. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by selecting suggestions of QAC.
Users X  preference of queries can be inferred during user-QAC interactions, such as dwelling on suggestion lists for a long time without selecting query suggestions ranked at the top. However, the wealth of such implicit negative feedback has not been exploited for designing QAC models. Most existing QAC models rank suggested queries for given prefixes based on certain relevance scores.
We take the initiative towards studying implicit negative feed-back during user-QAC interactions. This motivates re-designing QAC in the more general  X (static) relevance X (adaptive) implicit negative feedback X  framework. We propose a novel adaptive model adaQAC that adapts query auto-completion to users X  implicit neg-ative feedback towards unselected query suggestions. We collect user-QAC interaction data and perform large-scale experiments. Empirical results show that implicit negative feedback significantly and consistently boosts the accuracy of the investigated static QAC models that only rely on relevance scores. Our work compellingly makes a key point: QAC should be designed in a more general framework for adapting to implicit negative feedback.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query Formulation Query Auto-Completion; Implicit Negative Feedback
Query auto-completion (QAC) helps user query composition by suggesting queries given prefixes. As illustrated in Fig. 1, upon Part of the work was completed at Yahoo! Labs.
 Figure 1: A commercial search engine QAC. Given prefixes  X  X ac X  and  X  X ace X , popular  X  X acebook X -related queries are sug-gested to users after being ranked by certain relevance scores. a user X  X  keystroke, QAC displays a suggestion list (or list ) below the current prefix . We refer to queries in a suggestion list as sug-gested queries or query suggestions . A user can select to submit a suggested query; a user can also submit a query without selecting query suggestions. In 2014, global users of Yahoo! Search saved more than 50% keystrokes when submitting English queries by se-lecting suggestions of QAC.

Typically, a user favors and submits a query if it reflects the user X  X  query intent in a query composition. However, predicting query intent is challenging. Many of the recently proposed QAC models rank a list of suggested queries for each prefix based on dif-ferent relevance scores, such as popularity-based QAC (using his-torical query frequency counts) [2], time-based QAC (using time information) [32, 35], context-based QAC (using previous query information of users) [2], personalized QAC (using user profile in-formation) [31], time-and-context-based QAC (using both time and previous query information of users) [5].

The aforementioned models use different relevance features but do not fully exploit user-QAC interactions, such as users X  dwell time on suggestion lists and ranked positions of suggested queries by QAC. When users do not select query suggestions at keystrokes of compositions, users implicitly express negative feedback to these queries. Hence at such keystrokes, the user-QAC interaction infor-mation is users X  implicit negative feedback to unselected queries. We aim at complementing relevance features with implicit nega-tive feedback to improve the existing QAC models.
 We start with a motivating example.

Motivating Example: Consider a user who wants to query Ap-ple Inc. X  X   X  X acetime X  with a popularity-based QAC [2]. When the user types  X  X ac X ,  X  X acebook X  is ranked at the top in the suggestion list because it is most popular in historical query logs. The user dwells for a long time to examine the suggested query  X  X acebook X  but does not select it because it is not  X  X acetime X . However, in the next keystroke  X  X  X , popularity-based QAC still makes  X  X acebook X  top in the list because it is still the most popular query that matches the prefix  X  X ace X . Fig. 1 depicts our interactions with a commercial search engine QAC known to depend on relevance scores only.
Here the user implicitly expresses negative feedback to  X  X ace-book X :  X  X acebook X  is the top query suggestion, and the user dwells on the suggestion list for a long time without selecting this query. Hence, based on such implicit negative feedback, the user may not favor this unselected query. Can QAC be more accurate and demote  X  X acebook X  properly given the prefix  X  X ace X ?
Our Approach: To the best of our knowledge, no existing QAC adapts its ranking of query suggestions to implicit negative feed-back. We refer to a QAC model as static QAC , if its ranking of suggested queries does not adapt to implicit negative feedback in a query composition. Examples include popularity-based QAC, time-based QAC, and context-based QAC.

We go beyond static QAC by designing QAC in the new and more general  X (static) relevance X (adaptive) implicit negative feed-back X  framework. In this framework, we propose a novel adaQAC model that adapts QAC to implicit negative feedback. adaQAC reuses the relevance scores of queries from static QAC to pre-index top-N queries. In a single query composition, adaQAC re-ranks these N queries at every keystroke based on users X  implicit nega-tive feedback. Personalized learning for every different user with batch inference is employed by adaQAC, and adaQAC can be ex-tended by un-personalized learning and online inference.
Our Contributions: This work has many distinctions from re-lated research in QAC, negative feedback, and dynamic informa-tion retrieval; we present detailed discussions on such distinctions in  X 5. Our contributions are summarized as follows.  X  To the best of our knowledge, this is the first study on im-plicit negative feedback in user-QAC interactions. We find that the strength of implicit negative feedback to unselected query sugges-tions can be inferred, and a simple model fails ( X 2).  X  We go beyond static QAC under a general  X (static) relevance X  (adaptive) implicit negative feedback X  framework: we propose a novel adaQAC model that adapts QAC to implicit negative feed-back using personalized learning with batch inference, including un-personalized learning and online inference extensions ( X 3).  X  We collect user-QAC interaction data from a commercial search engine and perform large-scale experiments. We show that implicit negative feedback significantly and consistently boosts the accu-racy of the investigated static QAC models ( X 4).
We study QAC log data from a commercial search engine and discuss several motivational observations on implicit negative feed-back in user-QAC interactions.

Terminology: In general, on search engines, queries are submit-ted upon users X  selection from suggestion lists. Below are the other used terms.

Query composition (Composition) : The duration of composing and submitting a single query. It starts from the keystroke of a new query X  X  first character, or from the keystroke starting to edit a previous query. It ends when a query is submitted.

Dwell time : The time dwelled on a suggestion list. It is the time gap between two immediate keystrokes in a query composition. Position : The ranked position of a query in a suggestion list by QAC. Position 1 means being ranked highest or at the top, while 10 corresponds to being ranked lowest or at the bottom.

QAC log data : Our collected user-QAC interaction data from Ya-hoo! Search. There are 2,932,035 query compositions via desktops and they are sampled over five months in 2014. A composition has the prefixes, timestamps and suggested queries of every keystroke, and the submitted query. More details of the data are in  X 4.1. Due to the proprietary nature of the data, some details are omitted in data descriptions and figures.
Typically, a user favors and submit a query that reflects the user X  X  query intent in a query composition. We make the following as-sumption.

A SSUMPTION 1. In a query composition, a user submits a query if and only if the user favors it.

When a suggestion list is displayed, a user may examine or ig-nore a suggested query [21]. If a user ignores and does not select a suggested query, whether the user favors this query is unknown. If a user examines a suggested query but does not select it, there are two usual cases: (1) the user does not favor it; (2) the user still favors the suggestion but the user thinks selecting it from the list is less convenient than typing. In spite of possibly complicated cases, under Assumption 1 we make the following assumption.

A SSUMPTION 2. In a query composition, suppose a user fa-vors a suggested query. For the user, the likelihood of selecting this query is proportional to the likelihood of examining the query.
Suppose a user examines a suggested query in a composition with a higher likelihood. From Assumption 2, if the user favors the query, the user selects it with a higher likelihood. Otherwise, if the user does not select a suggested query after examining the query, it hints that the user may not favor this query. Under Assumption 1, this user may not submit this unfavored query in the composition. Hence, the examined but unselected query may be demoted at the subsequent keystrokes in the same composition; it allows the user X  X  favored query to rank higher in the composition.

Therefore, in a composition when a user does not select a sug-gested query, it may be helpful to know whether the user examines the unselected query. In other words, if the user examines an uns-elected query with a higher likelihood, this query may be demoted more heavily at the subsequent keystrokes of the composition.
For an unselected query suggestion, although whether a user ex-amines it, is not observed, user-QAC interactions can be observed. Such interaction information includes user behavior (dwell time) and settings (position) that are observed during the interactions.
Implicit negative feedback from a user to an unselected query suggestion is observed user-QAC interaction information, when the query is suggested to the user upon keystrokes of a composition. In other words, a user can implicitly express negative feedback to an unselected query  X  X acebook X :  X  X acebook X  is the top query sugges-tion, and the user dwells on the list for long without selecting it.
We claim that implicit negative feedback can be strong or weak, and its strength cannot be directly observed thus has to be inferred. The properly inferred implicit negative feedback strength may be used to properly demote unselected query suggestions. Recall the discussion that  X  X f the user examines an unselected query with a higher likelihood, this query may be demoted more heavily X . Some implicit negative feedback may indicate the likelihood of a user X  X  examination of an unselected query suggestion. Hence, such feed-back is of interest. Important examples are dwell time and position.
If a user dwells on a suggestion list for a longer time, the user may have more time to carefully examine the suggested queries.
On the other hand, if a user dwells for a shorter time, more likely the suggested queries are ignored; thus, even if these queries are unselected, whether the user favors them is unknown. (a) Distributions of dwell time from (b) The distribution shows that different users may have different typing speed;
Fig. 2(a) elucidates the distributions of the 0.1-second dwell time bin between 0 and 3.1 seconds of 10 randomly sampled users from QAC log data 1 . Dwell time t (in seconds) falls in the bin [ t,t + 0 . 1) . As the peak shows the most frequent dwell time bin of a user, it may suggest the user X  X  comfortable typing speed: if the peak falls in the bin of a longer dwell time, the user X  X  general typing speed is slower. The observed heavy-tails of the distributions manifest that longer dwell time is generally rarer, and the peak can characterize the user X  X  typing speed. Thus, in Fig. 2(a), the two peak clusters may imply two broad groups of users: User 1 and 2 generally type slower than the rest.

Fig. 2(b) zooms out from 10 users X  dwell time distributions to all the users X  implied comfortable typing speed with a distribution for dwell time of the peaks in Fig. 2(a). It demonstrates that different users may have different typing speed. Hence, inference of implicit negative feedback strength by dwell time should be personalized.
We study dwell time and position of unselected query sugges-tions that are not submitted by users.

The suggested queries at all the keystrokes in query composi-tions are collected. Then, suggested queries at the final keystrokes in query compositions are excluded because users may select a sug-gested query at the final keystroke: only the percentage of unse-lected queries that are not submitted by users is of interest. Suppose a dwell time threshold T DT and a position threshold T are set up. Consider all the suggested queries Q ( T DT ,T both in the list that is dwelled for no shorter than T DT at positions no lower than T P (dwell time  X  T DT and position  X  T P ). Given T DT and T P ,  X  q  X  Q ( T DT ,T P ) , the percentage of occurrences of q that are not submitted by users at the end of query compositions is recorded. The recorded results are illustrated in Fig. 2(c), with 300 different combinations of T DT and T P where T DT  X  X  0 . 1 , 0 . 2 ,..., 3 . 0 } and T P  X  X  1 , 2 ,..., 10 } .
Recall Assumption 1 that a user submits the favored query in a composition. The percentage of users X  unselected query sugges-
Binning masks details of the data for its proprietary nature. tions that are not favored by them, can be interpreted by the corre-sponding color in Fig. 2(c). As discussed in  X 2.1, implicit negative feedback strength may indicate how to demote unselected queries. For a more accurate QAC, the demotion should properly reflect the likelihood of not favoring or submitting an unselected query: such likelihood is higher with a longer dwell time and a higher position, as shown in Fig. 2(c). Thus, the results in Fig. 2(c) support the hypothesis that dwell time and position are important to infer the strength of implicit negative feedback.

From Fig. 2(c), when a position threshold T P is fixed, a dwell time threshold T DT better differentiates the likelihood of not fa-voring or submitting an unselected query, when 0 &lt; T DT This agrees with the results in Fig. 2(a) X 2(b) that, longer dwell time is generally rarer.
Following the findings in Fig. 2(c), it is tempting to extend an ex-isting QAC model by filtering out all the suggested queries based on dwell time and position thresholds. Thus, we set up a base-line model Filtering QAC to filter out all the suggested queries by using fixed dwell time and position thresholds in the subsequent keystrokes of a query composition. For instance, for T DT T
P = 3 , any previously suggested queries with positions higher than or equal to 2 and dwell time longer than or equal to 3 seconds are not suggested anymore in the subsequent keystrokes of the same query composition. To ensure a higher ranking accuracy, the results of Filtering QAC are tuned among T DT  X  X  0 . 1 , 0 . 2 ,..., 3 . 0 } and T P  X  X  1 , 2 ,..., 10 } .

However, experimental results ( X 4.2) show this simple model fails to significantly boost the static QAC models. Motivated by the findings from large commercial search engine QAC log data in  X 2, we propose a novel adaQAC model that adapts query auto-completion to implicit negative feedback.
We describe the system design of adaQAC to rank the suggested queries for a given prefix. A toy example with two queries  X  X ace-book X  and  X  X acetime X  that match prefixes  X  X ac X  and  X  X ace X  at top positions is used to illustrate the idea. Fig. 3 explains the system design and data flow of adaQAC: it has two stages.

Stage 1 (Pre-indexing): For a given prefix, top-N query sug-gestions with the highest relevance scores of static QAC are pre-indexed: the higher score, the higher position. In Fig. 3, for the prefix  X  X ace X , the top-2 ( N = 2 ) queries  X  X acebook X  and  X  X ace-time X  are pre-indexed by static QAC based on the historical query frequency counts.

Stage 2 (Re-ranking): adaQAC re-ranks these top-N queries based on the implicit negative feedback strength inferred from user-QAC interaction information in the same composition. To illus-trate Stage 2, upon a keystroke  X  X  X  following the prefix  X  X ac X  from a user, the front-end interface takes the current prefix  X  X ace X  as an input and immediately fetches the pre-indexed queries  X  X ace-book X  and  X  X acetime X . Suppose when  X  X acebook X  was ranked high-est in the suggestion list at the prefix  X  X ac X , the user dwells for a long time but does not select it. With this observation, sup-pose adaQAC is able to infer the user X  X  implicit negative feedback strength. Thus, adaQAC updates the ranking score of  X  X acebook X  and re-ranks the top 2 ( N = 2 ) queries  X  X acebook X  and  X  X acetime X . With re-ranking,  X  X acetime X  is now at Position 1, after  X  X acebook X  is demoted to Position 2.

The number of the pre-indexed top queries N can be set to a small positive integer in a production, such as 10 in our experi-ments. With a small constant value N , sorting N queries based on the updated ranking scores can be achieved in constant time [7].
We highlight that, adaQAC is designed in a more general  X (static) relevance X (adaptive) implicit negative feedback X  framework . The  X  X elevance X  component is the relevance score of a query for a user that can be obtained from an existing static QAC model, such as popularity-based QAC; the other  X  X mplicit negative feedback X  component adapts QAC to implicit negative feedback.

The  X (static) relevance X (adaptive) implicit negative feedback X  framework is more general for both reusing existing static QAC re-search and adapting QAC to the newly discovered implicit negative feedback. In this framework, adaQAC is not constrained to employ a certain relevance score: in  X 4 we investigate several different rel-evance scores with different parameter values in these scores.
Consider a user u  X  U , where U is the set of all adaQAC users, at the k -th keystroke in a query composition c  X  C ( u ) , where C ( u ) Table 2: Feature descriptions of the adaQAC model. The im-plicit negative feedback feature vector x ( k ) ( u,q,c ) , from a user u to a query q at a keystroke k in a query composition c , con-tains the following information collected from the beginning of c to the ( k  X  1) -th keystroke in c .
 is the query composition set of u . adaQAC suggests a ranked list of queries in Q according to the ranking scores determined by a prob-abilistic model. The probabilistic model is based on a combination of the relevance score and the inferred strength of implicit negative feedback. For a query q that matches the prefix at the keystroke k in the query composition c , the relevance score of q for the user u is denoted as r ( k ) ( u,q,c ) .

Implicit negative feedback from the user u to the query q at the k -th keystroke in the query composition c is represented by a fea-ture vector x ( k ) l  X  1 ( u,q,c ) , where l is the number of features. The strength of implicit negative feedback is based on x and its associated implicit negative feedback feature weight vec-tor  X  l  X  1 ( u ) for u .  X  l  X  1 ( u ) is a column vector indexed by u from the implicit negative feedback feature weight matrix  X  l  X  m all the users in U . Here m is the number of users in U .
In a query composition c , prefixes with the corresponding sug-gestion lists are referred to by sequential keystroke indices k  X  { 1 , 2 ,...,K ( c ) } , where K ( c ) is the number of keystrokes in a query composition c . For instance, for a query composition c start-ing from an empty string with three keystrokes  X  X ac X  ( K ( c ) = 3 ), the prefix  X  X ac X  with the suggestion list in the left of Fig. 1 can be referred to by k = 3 in c or simply K ( c ) in c . Table 1 briefly summarizes the main notations.
Table 2 lists the features used by adaQAC to fit in the  X  X mplicit negative feedback X  component. Dwell time and positions are stud-ied in  X 2.3. Likewise, the other features also indicate how likely users examine query suggestions.

Based on  X 2.2, such as the observation that different users may have different typing speed, personalized learning is used:  X  ( u ) is to be learned separately for each u  X  U to form  X  ( U ) .
We model preference p ( k ) ( u,q,c ) for a query q of a user u at a keystroke k in a query composition c , by a generalized additive model [10]:
In Equation 1, the preference model p ( k ) ( u,q,c ) is able to re-flect a user u  X  X  preference for a query q after the implicit negative feedback x ( k ) ( u,q,c ) is expressed to q before the k -th keystroke in a query composition c . With the associated feature weights  X  ( u ) personalized for u ,  X  &gt; ( u ) x ( k ) ( u,q,c ) encodes the strength of im-plicit negative feedback to q from u with personalization.
When a user u submits a query q  X  ( c ) at the final keystroke K ( c ) in a query composition c , c ends. The likelihood of the observa-tions on the submitted query in a query composition together with implicit negative feedback in Table 2 is to be maximized. Hence, we define a probabilistic model for a submitted query q  X  at K ( c ) in c with a softmax function that represents a smoothed version of the  X  X ax X  function [3, 38]: where Q ( k ) ( r,u,c,N ) represents the set of top N queries ranked by r ( k ) ( u,q,c ) . Its union with { q  X  ( c ) } ensures proper normal-ization. Likewise, adaQAC predicts the likelihood that a query q  X  Q ( k ) ( r,u,c,N ) to be submitted by a user u at any k in c by
In practice, the simpler form p ( k ) ( u,q 0 ,c ) in Equation 3 is used for re-ranking in Stage 2 of adaQAC ( X 3.1) after  X  ( u ) in Equation 1 is inferred. If a query q never appears in any suggestion list before a keystroke k in a query composition c , x ( k ) ( u,q,c ) is a zero vector and the user u  X  X  preference for q is the same as the relevance score r ( k ) ( u,q,c ) . Here k , c are used to refer to the prefix at k in c and suggested queries must match the prefix. However, if u expresses possibly stronger implicit negative feedback to q before k in c , say q is dwelled longer and at a higher position for several times, then the corresponding weights in  X  ( u ) updates preference for q of u at k in c with a lower p ( k ) ( u,q,c ) value; while possibly weaker im-plicit negative feedback may correspond to shorter dwell time and a lower position. The strength of the expressed implicit negative feedback determines the level of penalizing u  X  X  preference for q in p ( k ) ( u,q,c ) , which affects how to re-rank in Stage 2 of adaQAC. This agrees with the earlier discussions on using proper implicit negative feedback strength to properly demote an unselected query suggestion ( X 2).

We highlight that, the preference model p ( k ) ( u,q,c ) in Equa-tion 1 is designed in the more general framework as discussed in  X 3.2. The  X (static) relevance X  component is r ( k ) ( u,q,c ) , and  X  &gt; ( u ) x ( k ) ( u,q,c ) acts as  X (adaptive) implicit negative feedback X .
In Equation 1  X  ( u ) is inferred with batch inference. The likeli-hood for all compositions C ( u ) of a user u should be maximized.
By Equation 2 and 4, a constrained optimization problem out of minimizing negative log-likelihood with L 2 regularization (to avoid overfitting) is obtained as
There is a one-to-one correspondence between the parameters v in Equation 5 and  X   X  R + , and the corresponding un-constrained optimization problem is: where  X  is the regularizer weight parameter. As there is no closed-form solution for the optimization problem in Equation 6 due to non-linearity of the softmax function [3], iterative batch inference by gradient descent is used. We refer to an adaQAC model us-ing personalized learning with batch inference as adaQAC-Batch . Details for inferring  X  ( u ) are in Appendix A.
The objective function of negative log-likelihood for softmax functions with L 2 regularization in Equation 6 is strongly convex [28]. Hence, the inference is guaranteed to converge to the global optimum [29]: adaQAC-Batch can be inferred precisely. As we know, for a strongly convex objective function f ( x ) whose optimal value is achieved with x = x  X  , the number of iterations to get to accuracy | f ( x  X  )  X  f ( x ) |  X  takes a O (ln( 1 )) time [4]. Our ex-periments in  X 4.3 reinforce that, adaQAC-Batch converges quickly and reaches the global optimum within a constant number of itera-tions.
Suppose the relevance scores of queries for users, which depend on static QAC, are available. During the training phase for a user u ,  X  ( u ) is inferred with the constructed feature vectors. Assum-ing the number of queries in a suggestion list and the number of top queries for re-ranking ( N in  X 3.1) are fixed small constants, the feature construction has a time complexity of O ( lK ( c )) , where l is the feature vector size and K ( c ) is the number of keystrokes in a query composition c . Since the inference algorithm in Ap-pendix A converges within a constant number of steps ( X 3.4.3), it takes a O ( l 2 | C ( u ) | ) time with a constant factor corresponding to the number of convergence steps or a predefined value. Here | C ( u ) | is the number of query compositions for a user u . Note that the features in Table 2 are all distributive functions: the result derived by applying the function to aggregate values is the same as that derived by applying the function on all the data without partitioning. To explain, let x ( k ) i ( u,q,c ) be DwellT-M DwellT-M ( k +1) ( u,q,c ) can be updated by simply taking the larger value of DwellT-M ( k ) ( u,q,c ) and the dwell time at k + 1 in c , if q appears in the suggestion list. With a fixed small constant value N ( X 3.1), the suggestion at each keystroke takes a O ( l ) time. A nice property of personalized learning is scalability. As adaQAC-Batch infers  X  ( u ) for each individual user u , the inference is par-allel for different users on big query log data.

In particular, in the Hadoop MapReduce framework, the  X  ( U ) inference phase of our experiments is conducted in parallel for dif-ferent users by different Reducer nodes.
For a user u , adaQAC-Batch requires training data related to u to infer the feature weight  X  ( u ) . Now we consider a more challenging cold-start scenario where u is a new user without related data for training. Two ways of extensions can address the challenge.
The first way is to infer the feature weights from all the existing users excluding the new user. To maintain scalability on Hadoop MapReduce, a gradient descent variant with averaging is used [38]. This un-personalized approach does not differentiate one user from another, and is referred to as adaQAC-UnP .

Because only one feature weight vector is stored and shared by all the users, adaQAC-UnP is cheap in storage. adaQAC-Batch can be extended to an online inference style. For a new user, first, assign the un-personalized learning output to ini-tialize the feature weights; then, keep update the feature weights with more observations of the user X  X  interactions with QAC. We call this personalized online learning style extension adaQAC-Online . Stochastic gradient descent is used for the online infer-ence. It is similar to batch inference with the constrained optimiza-tion problem out of minimizing negative log-likelihood with L 2 regularization in Equation 5 replaced by Details for inferring  X  ( u ) are in Appendix B.
 Cost Analysis: adaQAC-Online costs more storage than adaQAC-UnP due to maintaining different weights for all the users. As shown in  X 4.4, adaQAC-Online trades its storage cost for slightly higher accuracy than adaQAC-UnP. Compared with adaQAC-Batch, the inference of adaQAC-Online takes a O ( tl 2 ) time, where t is the number of observations and l is the feature vector size. Gen-erally adaQAC-Online takes less time than adaQAC-Batch in in-ference and has the same storage requirement for maintaining dif-ferent feature weights for all the users. Comparing with adaQAC-Batch, adaQAC-UnP takes the same order of time with less storage requirement as it maintains only one feature weight vector that is shared by all the users.
We evaluate the proposed adaQAC-Batch and its two extensions adaQAC-UnP and adaQAC-Online on QAC log data.
Data: We describe important details of our collected QAC log data. Due to the proprietary nature of the data, some details are omitted. The QAC log data are collected from Feb 28 to Jul 28, 2014 and all the queries are submitted via desktops. If a query is submitted by more than two different users, its corresponding query composition is used for evaluation. As adaQAC-Batch requires training data for the feature weight inference, all the users with fewer than 100 query compositions during the given five-month range are filtered out. After the filtering, users are randomly sam-pled and their 2,932,035 query compositions constitute the evalu-ation data. There are in total 481,417 unique submitted queries. All the query compositions have their anonymized user IDs and the submitted queries. In one composition, the prefixes, timestamps and suggested queries of every keystroke are collected.

The training, validation and testing data are split with a ratio of 50%/25%/25% in an ascending time order: the first half of a user X  X  query compositions are used for training; the second and third quar-ters are for validation and testing respectively. The validation data are only used for parameter tuning. As adaQAC infers implicit neg-ative feedback from user-QAC interactions in query compositions, in  X 4.2 X  X 4.5 we experiment on the prefixes at the last keystroke of query compositions to use more interaction information. The average length of query prefixes is 8.53 characters.

The data standardization procedure is transforming data to zero mean and unit variance. All the feature values in Table 2 and the relevance scores are standardized.

Measures for Accuracy: Mean reciprocal rank (MRR) is the average reciprocal of the submitted query X  X  ranking in a suggestion list. It is a widely-adopted measure to evaluate the ranking accuracy of QAC [2, 21, 15, 31]. Success Rate@top-k (SR@ k ) denotes the average percentage of the submitted queries that can be found in the top-k suggested queries on the testing data, and was also used to evaluate the QAC ranking accuracy [15]. In general, a higher MRR or SR@ k indicates a higher ranking accuracy of QAC [2, 21, 15, 31, 5]. Paired-t test is used to validate the statistical significance of the accuracy improvement ( p &lt; 0 . 05 ).
Following the  X (static) relevance X (adaptive) implicit negative feed-back X  framework ( X 3.2), we investigate relevance scores from pop-ular static QAC with different parameter settings to compare the accuracy of adaQAC-Batch, Filtering QAC, and static QAC.
The relevance scores reuse the existing research: MPC [2, 15, 21, 31], Personal(-S) [2, 5, 31], and TimeSense(-S) [5, 32, 35, 27].  X  MPC: Most Popular Completion (MPC) ranks suggested que-ries for a prefix based on the historical popularity of a query. A more popular query gets a higher rank. Despite its simplicity, it was found competitive by various studies [2, 15, 21, 31].  X  Personal: Personal QAC for distinguishing different users can achieve better accuracy [2, 5, 31]. Although personal information may take many different forms, the Personal relevance score in this work is an equal-weighted linear combination of the MPC score and the standardized personal historical query frequency counts.  X  Personal-S: It is the Personal relevance score with an optimal combination with different weights of the MPC score and the stan-dardized personal query frequency counts. The optimal weights achieving the highest accuracy are tuned on validation data. Tun-ing to the optimal weights makes Personal-S more competitive.  X  TimeSense: Time is useful in QAC [5, 32, 35]. Hence, Time-Sense is the same as Personal except that the personal historical static QAC (MPC) by 21.2% in MRR.
 Figure 4: Convergence (left) and regularizer weight (right) study for adaQAC-Batch (TimeSense-S). Plots are similar for the other relevance scores. adaQAC-Batch converges quickly and is not sensitive to the chosen regularizer weight near its optimum. query frequency counts is replaced by the all-user popularity counts of a query in the 28-day time window before a query composition.  X  TimeSense-S: It is the same as Personal-S except that Personal is replaced by TimeSense.

For brevity, we denote  X  X tatic QAC employing the MPC rel-evance score X  as  X  X tatic (MPC) X . Similar notations are used for QAC models employing any relevance score.

Parameters values are tuned to achieve the highest accuracy on validation data. Unless otherwise stated we set the number of it-erations to 40 (adaQAC-Batch and adaQAC-UnP) and the regular-izer weight to 0.01. Personal-S and TimeSense-S both combine a MPC score with the optimal weight  X  and the other score with the weight 1  X   X  . The optimal weights in Personal-S (  X  = 0 . 34 ) and TimeSense-S (  X  = 0 . 42 ) achieve the highest MRR for static QAC.
In  X 2.4 we set up Filtering QAC with relevance scores, by ad-ditionally filtering out all the suggested queries with certain dwell time thresholds ( T DT ) and position thresholds ( T P ) in the subse-quent keystrokes in a composition. To ensure higher competitive-ness, the model is tuned among the 300 threshold value combina-tions in  X 2.4. We set T DT = 0 . 9 and T P = 1 .

Table 3 presents the accuracy comparison of static QAC, Fil-tering QAC, and adaQAC-Batch. The simple Filtering QAC model fails to outperform the corresponding static QAC with the same rel-evance scores significantly. For each same relevance score, adaQAC-Batch exploiting the added implicit negative feedback information significantly and consistently boosts the accuracy of these static QAC models that only use relevance scores. With more accu-rate relevance scores such as Personal and TimeSense, adaQAC-Batch is more accurate. Given the relevance scores with differ-ent parameter settings (Personal vs. Personal-S and TimeSense vs. TimeSense-S), the accuracy of adaQAC-Batch slightly varies de-pending on the accuracy of the relevance scores for the chosen pa-rameter values.

The newly-discovered implicit negative feedback is promising in boosting the accuracy of the existing static QAC models.
Here we set the number of iterations and regularizer weight to different values for the parameter study on the validation data. adaQAC-Batch (TimeSense-S) is tested. The results for the other relevance scores are similar.

Convergence: Fig. 4 (left) shows the evaluation measures against the number of iterations. The results reinforce the fact that, adaQAC-Batch converges quickly and the precise global optimum can be reached within a constant number of iterations ( X 3.4.3).
Regularizer Weight: Fig. 4 (right) plots the evaluation mea-sures of adaQAC-Batch (TimeSense-S) with regularizer weights that are varied around the optimum 0.01. adaQAC-Batch is not sensitive to different regularizer weights near the optimum. This property shows that the accuracy of adaQAC-Batch has less depen-dence on the chosen regularizer weight value.
Motivated by the more challenging cold-start scenario where there is a lack of training data for new users, we evaluate the two adaQAC extensions adaQAC-UnP ( X 3.5.1) and adaQAC-Online ( X 3.5.2).
For a user u , the un-personalized learning is performed by learn-ing from training data related to all the users excluding u , and the learned feature weights are fed into adaQAC-Online for u as the initial feature weights. Neither adaQAC-UnP nor adaQAC-Online uses the training data related to u .

Table 4 shows that, both adaQAC-UnP and adaQAC-Online sig-nificantly and consistently boost the accuracy of static QAC for each relevance score. The mean measure values of adaQAC-UnP and adaQAC-Online are slightly lower than those of adaQAC-Batch for the same relevance score. This slight difference can be justified by the added benefits of the more expensive personalized learning with batch inference of adaQAC-Batch.

It was pointed out that ( X 3.5.2), adaQAC-Online costs more stor-age than adaQAC-UnP due to maintaining different weights for all the users. The slight difference between the mean of the mea-sure values of adaQAC-Online and adaQAC-UnP in Table 4 shows that, adaQAC-Online trades its storage cost for slightly higher ac-curacy than adaQAC-UnP. In addition to the benefits for addressing the cold-start challenge, according to the cost analysis in  X 3.5.2, an important practical implication from the results of Table 4 is, adaQAC-UnP and adaQAC-Online can be good substitutes for the more expensive adaQAC-Batch if time and storage budgets are lim-ited in the real-world productions. adaQAC-Online (MPC) significantly boosts static QAC (MPC) by 20.3% in MRR. relevance scores are able to detect more outliers with the raised minimum bars.
We study the model accuracy on different users using the Box-and-Whisker plots. With each data instance being the MRR on one user, Fig. 5 shows the minimum (bottom bar), quartiles (box edges), median (middle of the box), maximum (top bar) after removal of the detected outlier users (empty circles).
 In general, model comparison using medians and quartiles of MRR agrees with the results in Table 3 X 4 and reaffirms the boosted accuracy by the added implicit negative feedback.

Note that, all the models perform poorly on a few users. Models with the MPC relevance score fail to detect any outlier, and have minimum bars close to 0. The other models still perform poorly on certain users with MRR close to 0. These users are detected as the outliers. The outlier users may behave inconsistently, sub-mit rare queries, or the collected data related to them are noisy or incomplete due to unknown reasons.

To explain, models with the MPC relevance have a larger MRR variance (implied by a longer box in Fig. 5) so outlier users cannot be easily detected. It is easier to see when comparing adaQAC-Online (MPC) with Static (Personal): they have close medians but the lower-variance Static (Personal) is able to detect a few outliers and raise its minimum bar after their removal. When the relevance score is more accurate with a lower variance, adaQAC is able to de-tect more outliers thus raises the minimum bar by further improving the MRR on the majority of the users.

Hence, even though the implicit negative feedback research is promising, further research on more accurate relevance scores is still required.
Now we consider another challenging scenario where testing is based on all possible prefixes in query compositions. Table 5 re-ports MRR of static QAC, adaQAC-Static and adaQAC-Online for prefixes with varying lengths at every keystroke in query composi-tions. Both adaQAC-Batch and adaQAC-Online still significantly and consistently boost the accuracy of static QAC under all prefix lengths for each relevance score.

The MRR gap between adaQAC-Batch and adaQAC-Online is subtle and both are more accurate when prefixes are of  X  X iddle X  lengths. That is, when the prefixes are short, the collected implicit negative feedback features probably contain little useful informa-tion to improve the re-ranking in Stage 2 of adaQAC ( X 3.1). When prefixes get longer, more user-QAC interaction information is ob-tained to make adaQAC more accurate in the adaptive re-ranking stage. However, when prefixes are longer, the QAC problem be-comes less challenging due to a reduction of the matched queries: static QAC employing relevance scores are more accurate and it is harder to further improve the accuracy, even though the implicit negative feedback information may be richer. adaQAC has advantages over static QAC. We describe the fol-lowing cases of Yahoo! Search, and hope that this work can inspire ongoing studies in a broader research community.

Disambiguation: When users have clear query intent and pre-fer disambiguated queries, adaQAC generally outperforms static QAC. Typically, users may prefer queries of the form  X  X ntity name + attribute X  to  X  X ntity name only X . Suppose a user wants to know the showtime of lefont sandy springs. When the user composes the query during the keystrokes  X  X efon X , the entity name  X  X efont sandy springs X  is the top suggestion. The user does not select it because an entity name query may result in diverse search results. So, the query  X  X efont sandy springs X  receives implicit negative feedback. When the prefix becomes  X  X efont X ,  X  X efont sandy springs X  is de-boosts static QAC (MPC) by 17.1% in MRR under all prefix lengths. moted by adaQAC and  X  X efont sandy springs showtime X  gets pro-moted.

Query Reformulation: When users prefer new queries when reformulating older queries, adaQAC generally outperforms static QAC. Suppose a user wants to query  X  X etroit lions X  after query-ing  X  X etroit red wings X . When the user reformulates the query from  X  X etroit red wings X  to  X  X etroit r X  by consecutively hitting Backspace,  X  X etroit red wings X  is ranked highest but the user does not select it. So, the query  X  X etroit red wings X  receives implicit negative feedback. Hence, when the prefix becomes  X  X etroit X  after the user hits two more Backspace,  X  X etroit red wings X  is demoted by adaQAC; some other queries, such as  X  X etroit lions X , are pro-moted accordingly.

Smoothing  X  X ver-Sense X : Certain relevance scores may be sen-sitive to specific signals: TimeSense is sensitive to time. Studies showed users may have query intent for new or ongoing events [1, 16, 20]. In Yahoo! Search, we investigate the QAC results re-sponded by the time-sensitive component. When a user wants to query an earlier event  X  X ussia attack georgia X , the time-sensitive QAC keeps ranking a more recent event  X  X ussia attack ukraine X  highest during keystrokes  X  X ussia att X . Instead, adaQAC receives users X  implicit negative feedback to  X  X ussia attack ukraine X  hence demotes it, and raises  X  X ussia attack georgia X  up to the top.
Query Auto-Completion (QAC): Numerous QAC models have been developed in recent years, such as popularity-based QAC us-ing historical frequency counts [2], time-based QAC using time in-formation [32, 35], context-based QAC using previous query infor-mation of users [2], personalized QAC learning from user profile information [31]. The relevance scores investigated in our work make use of the existing research, such as MPC [2, 15, 21, 31], Personal(-S) [2, 5, 31], and TimeSense(-S) [5, 32, 35, 27]. More recent QAC methods also predicted the probability that a suggested query would be clicked by users based on user models [18, 21], determined suggestion rankings based on query reformulation pat-terns [15], or combined information such as time and previous queries from users [5]. Furthermore, user interactions with QAC just began to be explored. Mitra et al. discussed user-QAC in-teractions from perspectives such as word boundaries, fraction of query typed, and keyboard distance [26]. Hofmann et al. identified common behavior patterns of user-QAC interactions [11].

Other aspects of QAC have also been studied, such as space ef-ficient indexing [13] and spelling error toleration [6, 14, 8, 36].
However, none of the aforementioned work aimed at inferring implicit negative feedback from user-QAC interactions, or adapting QAC to such feedback. We take these initiatives and show that QAC can adapt to implicit negative feedback and be more accurate.
Negative Feedback: Relevance feedback is useful for improv-ing information retrieval models, but further improving it using negative feedback was considered challenging [30, 25]. Recently, more efforts on negative feedback research was made in document retrieval tasks. Wang et al. found negative relevance feedback useful to improve vector-space models and language models [34]. Hong et al. proposed a hierarchical distance-based measure to dif-ferentiate the opposite intent from the true query intent [12]. Zhang and Wang studied language models with negative feedback through positive and negative document proportion on query classification [39]. New models using negative relevance feedback were also developed in TREC [22]. In particular, negative feedback was also found useful to retrieve documents for difficult queries [33, 17, 24].
However, these negative feedback studies focus only on docu-ment retrieval tasks. The richer interaction information, presented in the QAC settings, such as dwell time and positions, is not avail-able in general document retrieval settings.

Dynamic IR: Recent work have gone beyond existing IR tech-niques to incorporate dynamics in session search [9, 23]. In this task, added or removed terms compared with the other queries of the same search session will update term weights to retrieve docu-ments for the completed query [9, 23]. There are important differ-ences between such research and ours. First, search and QAC are different problems. Second, adaQAC emphasizes adapting dynam-ics over a single query composition rather than multiple queries over a search session. Third, adaQAC does not assign weights to characters, prefixes or terms of a query. Other dynamic IR work was surveyed in a tutorial by Yang et al. [37].
We studied interactions between users and QAC where users im-plicitly express negative feedback to suggested queries. Under the more general  X (static) relevance X (adaptive) implicit negative feed-back X  framework, our proposed adaQAC model can reuse the exist-ing static QAC research and adapt QAC to implicit negative feed-back using personalized learning with batch inference. Extensions with un-personalized learning and online inference were also pre-sented. We collected user-QAC interaction data from a commercial search engine. Large-scale empirical results showed that implicit negative feedback significantly and consistently boosts the accu-racy of the investigated static QAC models.

Let f h  X  ( t ) ( u ) i be the objective function in Equation 6, where where and  X  i = 1 , 2 ,...,l , as E ( q ) ,
In the experiments,  X  (0) ( u ) in Equation 7 is randomly sampled from:
The feature weight  X  (0) ( u ) is initialized as the un-personalized learning weight ( X 3.5.1). After each query composition c , the fea-ture weight is updated as in Equation 7 X 10 with Equation 9 re-placed by and  X  is discounted by a factor of 0.9 after each update as an an-nealing procedure [19].
