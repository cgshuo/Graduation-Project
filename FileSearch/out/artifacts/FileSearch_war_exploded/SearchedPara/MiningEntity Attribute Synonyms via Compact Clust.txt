 Entity attribute values, such as  X  X ord of the rings X  for movie.title or  X  X nfant X  for shoe.gender, are atomic components of entity ex-pressions. Discovering alternative surface forms of attribute values is important for improving entity recognition and retrieval. In this work, we propose a novel compact clustering framework to jointly identify synonyms for a set of attribute values. The framework can integrate signals from multiple information sources into a similarity function between attribute values. And the weights of these signals are optimized in an unsupervised manner. Extensive experiments across multiple domains demonstrate the effectiveness of our clus-tering framework for mining entity attribute synonyms.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Entity Search Entity Attribute Synonyms, Compact Clustering
The Web contains a wealth of structured data, such as various entity databases, web tables, etc. There is a growing trend in com-mercial search engines to match unstructured user queries to these structured data sources. However, user expressions of such enti-ties often do not match the canonical specifications from the data providers. For example, in the movie domain, the full title  X  X he lord of the rings: the return of the king X  can be specified by users as  X  X otr 3 X ,  X  X otr: return of the king X , or  X  X he return of the king X . For shoes, people may describe the standard gender value  X  X nfant X  as  X  X aby X  or  X  X oddler X . Thus, entity synonym identification, the dis-covery of alternative ways people describe entities, has become a critical problem to bridge the above mentioned gap between data providers and consumers.

Traditionally, entity synonym research has focused on finding synonyms of named entities, where the entity itself is completely specified by the referent string. Here we are interested in finding Copyright c  X  2013 ACM 978-1-4503-0757-4/11/07 ...$15.00. synonyms of entity attribute values (also referred to as entity at-tribute synonyms throughout this paper). While the attribute values can be entity mentions, they can also be arbitrary strings (adjec-tives, verbs, etc). In fact, our problem definition is a generalization of finding named entity synonyms, because the named entity ex-pression is often just an attribute of the entity. Fig. 1 illustrates an example of such general cases collected from a product title and two user issued queries. Here  X  X anon X  is a named entity, but it also matches attribute digital-camera.brand . And  X 12.1 mega pixel X  is an attribute value; but it cannot be interpreted as a stand-alone en-tity. As seen in Fig. 1, there are a lot of variations in describing the same attribute values. Successful identification of their surface forms will enable better query intent understanding and better nor-malization of products from different providers, etc . In the case the attribute value itself is an entity mention, our problem setup is the same as traditional entity synonym finding. Previous research has addressed the synonym identification problem from multiple perspectives. For example, [10, 4] tried to reconcile different ref-erences to the same database record. Other works identified alter-native forms of a query for web search by measuring query sim-ilarity [6, 12], query-document click-graphs [1] and query-entity click-graphs [9]. For non-entity attribute values (arbitrary strings), there are also research efforts from the Natural Language Process-ing community on finding semantic synonyms based on distribu-tional similarity [14, 15], syntactic patterns [11, 5] et. al .
However, two major challenges remain. First, finding synonyms without context can not handle semantic ambiguity. There are re-cent research attempts to identify synonyms with additional con-text, such as paragraph context in [22]. But for structured database, such information is not always available. Second, previous ap-proaches usually focus on utilizing a single signal, such as distribu-tional similarity [14, 15], syntactic patterns [11, 5], or query-entity clicks [9]. Some recent works explored more information sources [17, 7]. However, the weights for combining these information sources are usually manually tuned largely based on experience.
In this work we focus on finding synonyms for a set of entity attribute values simultaneously. Our problem setup is a general-ization of the entity synonym identification problem, in which the input can be an entity mention or an arbitrary string. To address the deficiencies of existing approaches discussed above, we propose a compact clustering model that enables the integration of multi-ple heterogeneous information sources. Our main contributions are summarized as follows:  X  Joint synonym mining from a set of attribute values . Most previous synonym identification methods search for synonyms one entity at a time. However, processing a set of entity attribute values simultaneously has several advantages. First, as the values are from the same attribute, they exhibit distinctive contextual pat-terns. Mining such patterns allows us to define a novel categori-cal pattern similarity function to tackle the ambiguity problem.
Second, joint modeling of multiple attribute values also provides prior knowledge about the relationship among candidates. For example, for entity mention  X  X ord of the rings 2 X ,  X  X ord of the rings 3" could be identified as synonym mistakenly. But if we learn synonyms from those two values jointly, this error could be corrected easily because of the awareness of  X  X ord of the rings 3".  X  Integrating multiple information sources. Synonym values generally exhibit similarities in more than one aspect. Some syn-onym values only differ in a few characters, due to spelling errors or morphological differences. Also, queries that differ only in synonym values tend to have clicks on similar sets of documents.
In addition, synonym values generally have similar surrounding contexts within queries and documents. Among these signals, some are more important than others in determining synonym re-lations. Furthermore, the relative importance of these signals also depends on the domain: a feature that is crucial in the movie do-main might be only marginal in the camera domain. Therefore automatic determination of the weights of different information sources is critical. In this work we propose to automatically learn these weights via compact clustering  X  a novel clustering proce-dure that maximizes the similarity of points within a cluster.
There is a rich body of work on the general topic of automatic synonym discovery. This topic can be divided into sub-areas, in-cluding finding word synonyms, entity/attribute synonyms, and re-lated query identification. Identifying word level synonyms from text is a traditional topic in the NLP community. Such synonyms can be discovered using simple dictionary based methods such as WordNet; distributional similarity based methods [14, 15]; and ap-proximate string matching approaches [18]. In this work we focus on finding entity attribute synonyms, which usually have more do-main context than plain words.

Researchers have employed several similarity metrics to find syn-onyms from web data. Such similarities include distributional sim-ilarity [14, 15], coclick similarity [9, 7], pointwise mutual informa-tion [21], and co-occurrence statistics [3]. Unlike these works, our work introduces a novel similarity metric called categorical pattern similarity for jointly finding synonyms from a set of attributes.
Although several similarity metrics have been introduced to find synonyms, most previous approaches use only a single metric. [7] tries to combine multiple metrics, however they manually choose a set of thresholds for individual metrics, leading to a precision ori-ented approach. Instead, our approach combines the metrics with weights, and learns these weights automatically in an unsupervised manner. In this sense our work is also related to the previous works on semi-supervised metric learning [23, 20, 2]. We differ from these works in that our metric learning approach is embedded in the compact clustering framework.
In our problem setup, the input consists of (1) a set of canonical entity attribute values from a domain; (2) candidate synonyms of the canonical attribute values. And the output is the true synonyms of this set of attribute values. As there are multiple interpretations of  X  X lternative expressions X , we focus on synonyms that convey the equivalent meaning of the canonical value in the (implied) domain, including semantic alterations, abbreviations, acronyms, permuta-tions, spelling errors, etc . For example, for the input  X  X BM X , the synonyms include  X  X nternational Business Machines X ,  X  X ig blue X ,  X  X BM corporation X  etc .

Formally, given a set of K semantically distinct values V { v 1 , v 2 , ..., v K } from an unspecified entity attribute, where each value v  X  V is represented by a canonical string expression, such as  X 5d mark iii X  for camera.model . From a set of N candidate syn-onym values X = { x 1 , . . . , x N } , we can define an oracle mapping F : X  X  V  X  X  v 0 } , which assigns each candidate value x to its unique canonical synonym value v , or if x is not a synonym of any value in V , to the special background value v 0 . Note that we assume each candidate synonym expression maps to at most one canonical value. Now, we can define the synonym identification problem as follows: Definition 1 : For each canonical attribute value v  X  V , find the subset X v = { x  X  X |F ( x ) = v } , representing the set of synonym expressions for value v .

Note that we assume values in V are semantically distinct and homogeneous. This assumption is reasonable in several applica-tion scenarios. For instance, for product providers such as eBay and Amazon, a set of distinct and homogeneous canonical attribute values can be easily obtained from product catalog. The homoge-neous assumption implies that the inputs are from the same domain, which can be leveraged for mining their synonyms collectively.
As mentioned in the introduction, most previous synonym identi-fication methods search for synonyms one input at a time. However, such strategy have two major drawbacks. First, without modeling the attribute values jointly, it X  X  very difficult to tackle the ambi-guity problem since the category context implied by a set of at-tribute values is lost. Second, this strategy doesn X  X  leverage the prior knowledge multiple attribute values bring to the candidates. In order to take advantage of a set of canonical attribute values, we propose to identify synonyms of attribute values by a clustering model with multiple similarity kernels called compact clustering . In this model, attribute values X = { x 1 , x 2 , ..., x N as data points. And points are connected with others with similarity function f . Data points form clusters such that points in the same cluster are considered synonyms. In this section we first define the similarity kernel functions. Then we introduce a basic model by motivating the concept of cluster compactness. By addressing the limitations of this model, we propose several extensions that lead to the standard compact clustering model.
In our clustering framework, data points (attribute values) are re-lated to each other in different aspects. For example, in the do-main of movie.title , two titles are similar if people click on the same set of documents after querying for these titles. Two titles also are similar if they follow similar lexical distribution. In fact, there are heterogeneous types of information that can be leveraged to infer the synonym relationship. Suppose from information type t , the similarity of points x i and x j is defined as a similarity kernel f ( x i , x j )  X  [0 , 1] , and each type of similarity kernel is associ-ated with a weight w t reflecting its relative importance, then the overall distance between x i and x j can be defined by these similar-ity kernels. Here we define the distance between x i and x combination of the similarity kernels with weights: where f t ( x i , x j )  X  [0 , 1] is the similarity kernel of x culated based on evidence from information source t  X  X  1 , ..., T tween x i and x j .  X  is a constant whose value is set to 2 in this work. And w t  X  0 are the weights needed to be learned, following constraint
The special choice of  X  is to make the optimal w t easier to solve under the above constraint, as introduced in previous work [8]. In the following, we specifically define four similarity kernels accord-ing to four types of information. Note that our framework is not restricted to these kernels. In fact our model can support arbitrary number of similarities from different information sources. 1. Categorical pattern similarity . This is a novel similarity ker-nel which leverages a set of attribute values simultaneously. A key insight is that canonical values in the same category should share common lexical or semantic patterns. Table. 1 illustrates the pattern distribution over 50 attribute values from shoe.brand . These patterns are found by extracting the left and right lexical context from a set of search queries. It clearly shows that the brand names are much more likely to appear at the beginning of a query (#EMPTY# pattern on the left); and the word  X  X hoes X  is frequently following the brand name. By mining this context, we are able to discover categorical patterns, which would otherwise be impossi-ble had we looked for synonyms one attribute value at a time due to data sparseness. Specifically, given data points x i , x and right categorical pattern distributions  X   X  l ,  X   X  canonical attribute values, we define the categorical pattern simi-larity between x i and x j as: where Jaccard ( X  i ,  X   X ) is the average Jaccard similarity of the left context and right context between x i (  X  i;l ,  X  i;r gory (  X   X  l ,  X   X  r ):
Jaccard ( X  i ,  X   X ) = 1 Note that the categorical pattern similarity kernel is large only if both x i and x j share similar context distributions with the categor-ical patterns, which is especially effective for excluding the am-biguous candidate strings. For example, for the canonical value  X  X pple X  in the domain of IT companies (implied by inputs  X  X p-ple X ,  X  X BM X , etc .), a candidate  X  X pple fruit X  will have very low categorical pattern similarity because this candidate has very dif-ferent query context.
 2. Coclick similarity. Two attribute values are similar if users click on similar documents when they issue queries containing the two attribute values (proxy queries). Let the set of proxy queries of x i be Q i = { q i 1 , q i 2 , ..., q i n i } . For each query q clicks on a set of documents, which is denoted as  X  l = { where M is the total number of documents. And let the accumula-tion of these clicks be: Then for points x i and x j , we define their coclick similarity as the cosine similarity of  X  i and  X  j : 3. Lexical context similarity. Under the distributional similar-ity assumption [16], two strings will carry similar meaning if they share similar context. We observe that for true synonyms, the two attribute values will share common left and right context in web search queries. However this similarity is different from the cat-egorical pattern similarity in that the lexical context similarity is more specific to a particular attribute value while the categorical pattern similarity is related to the patterns of a set of values. We define the lexical context similarity of x i and x j as the Jaccard similarity of their left and right context: 4. Pseudo document similarity. This similarity kernel has been successfully applied to finding entity synonyms [7]. It essentially measures the similarity between two attribute values based on the number of co-occurrences in the query-clicked pseudo document pairs. Please refer to [7] for more detail.
After defining the overall distance function and similarity ker-nels, we now describe the formulation of the clustering model. As for a clustering model, we must specify the cluster centers. For the attribute synonym finding problem it X  X  natural to nominate the canonical attribute values as the cluster centers since they should be close to their synonyms. Moreover, synonymous attribute val-ues should be close with each other in a cluster and far away from other clusters, which motivates our compact clustering model. Formally, in the basic model we aim at minimizing the following objective function: subject to: The above objective function is the sum of within-cluster disper-sions. In Eq. (7), the first term is the overall within-cluster dis-tances of the normal clusters, and the second term is the within-cluster distances in the background cluster. Such formulation is to make the resulting clusters more compact. Note that in our model there is no need to represent data points with explicit feature vec-tors, instead, we only require that d ( x i , x j )  X  0 . The notations of variables in the formula are listed below: Rationale of the objective function: The above objective func-tion is similar to K-medoids[13]. The advantage of this framework compared to K-means is that the distance function between data points can be defined in arbitrary form. However, there are im-portant differences between our basic model and the K-medoids model: firstly, the first K medoids in our model are fixed to the canonical attribute values, assuming they are best representatives of these clusters. Secondly, in our model the distance between points is a weighted distance function, which is very different from the standard K-medoids model. Thirdly, in our model we add a back-ground cluster in order to attract the random points.

Although the basic compact clustering model can partition the data points into synonym clusters, it suffers from the following lim-itations: (1) Using a single fixed representative for a cluster may be problematic. First, the canonical value is not always the most pop-ular or most representative. It may have idiosyncrasies that are not shared by other members of the cluster. Second, because the sim-ilarity features are noisy, if we only compare a candidate against the canonical value, a noisy feature may bias it towards an incor-rect cluster. (2) Manually setting the constant  X  is very difficult. (3) No measurement of uncertainties of a point belonging to the background.
Generally, Limitation 1 can be addressed by employing a flexi-ble representative or a small set of representatives for each cluster. However it X  X  not desirable to have flexible medoids since in our problem setup the canonical values are good representatives and it is more robust to include them into the medoids. Therefore we pro-pose to use a small subset of points, including the canonical value, to form a new pseudo-medoid. The subset is viewed as a committee that determines which other points belong to the cluster. A similar idea of clustering with committees of points has been successfully applied to the document clustering problem [19]. Specifically, in our new proposal, we form the new pseudo-medoid by including the L  X  1 most similar values to the canonical value as well as the canonical value itself.

To address Limitation 2, we propose to randomly select  X  pro-portion of points from the background cluster, and estimate  X  by taking the average of the distance from x to this random subset. Results show that the final synonyms are stable with respect to dif-ferent setting of  X  .

We address Limitation 3 by introducing a prior probability p that a given point x belongs to the background cluster. If we further assume x follows a uniform prior distribution for normal clusters, then the prior probability of x belonging to a normal cluster is
Based on these new proposals, we present the standard compact clustering model by minimizing the updated objective function: subject to Eq. (8). Where z  X  k is the pseudo-medoid, A is the sub-set of random points in the background cluster, whose size is con-trolled by the parameter  X  . And the prior probability p is a tunable parameter. The standard compact clustering model aims at induc-ing more compact clusters. In the standard model there are three sets of unknown variables: R , Z  X  and W , which are dependent on each other. There is no exact solution to solve all of them at the same time. Instead we solve this optimization problem by iteratively solving the following minimization problems: 1. Fix Z  X  =  X  Z  X  and W =  X  W ; find the best R that minimizes 2. Fix W =  X  W and R =  X  R ; find the best medoids Z  X  that 3. Fix Z  X  =  X  Z  X  and R =  X  R ; solve the best parameters W that Sub-problem 1 (cluster assignment) can be solved by: { where { For sub-problem 2 , we update the pseudo-medoids of first K clus-ters by including up to the top L  X  1 most similar values to the canonical value as well as the canonical value itself: z F or the background cluster, there is no need to calculate the updated medoid. We follow the basic ideas from weighted K-means [8] to solve sub-problem 3 . Because after fixing R and Z , Eq . (9) is a convex quadratic function, we apply the Lagrange Multiplier method and obtain a closed form solution to W (not shown due to the page limitation). Intuitively, a larger weight is assigned to a feature function which makes the clusters more compact.
To test the effectiveness of our proposed compact clustering model, we first make direct comparison of our model to the baselines on the traditional setting that the attribute values are also entities men-tions. We then conduct another set of experiments on the setting that the attribute values are arbitrary strings. After that, we will show results in cases where the attribute values have ambiguous senses. Furthermore, we investigate the relative importance of sim-ilarity kernels.
In order to evaluate the proposed models, we have collected sev-eral attribute synonym datasets from multiple categories (see Table 2). Specifically, 3 datasets are constructed to test the traditional en-tity synonym finding. 3 other sets are selected to test the synonym identification where the attribute values don X  X  look like entity men-tions. Furthermore, we have collected a set of ambiguous attribute values to discuss the challenging issue of ambiguity. Because ob-taining a set of ambiguous values from a single category is hard, we get the results from 5 datasets, select 18 such ambiguous values and then label them. In terms of evaluation metrics, We evaluate our system based on the standard expected precision, expected re-call and expected F1 measure.
 1. Individual features . Individual features are included as base-lines so as to reveal their strength and weakness on identifying en-tity attribute synonyms both in the form of entity mentions as well as arbitrary strings. Synonyms are identified by single attribute value at a time. We try several settings and manually choose the best thresholds for these feature functions. 2. Chakrabarti-2012 . We also include a strong baseline proposed by Chakrabarti et. al [7], which identifies entity synonyms by com-bining multiple similarity scores with manually tuned thresholds. We consider it a state-of-the-art multi-feature, single value at a time approach. For a fair comparison, this system works on the same set of query log and clickthroughs as our approach for calculating sim-ilarities. We collect final outputs in the form of an unordered list of synonyms for each input attribute value via the system interface provided by the authors of [7]. 3. Clustering with Fixed Weights . In order to reveal the effective-ness of the kernel weights learning, we add a baseline that uses the same clustering model, yet with fixed (equal) kernel weights.
We first evaluate the performance of the compact clustering model on attribute values that are also entity mentions. Among the three test datasets, movie.title and shoe.brand are from popular domains while doctor.specialty is from tail domain. Table 3 shows the ex-pected precision, recall, and F1 scores. Firstly, the results show consistently across three datasets that using single feature doesn X  X  achieve competitive results. Specifically, categorical pattern has somewhat good precision but suffers from very low recall. pseudo document similarity is a relatively robust method achieving bal-anced precision and recall. However it fails to get competitive per-formance compared to methods combining multiple features such as Chakrabarti-2012 and our model. Secondly, the Chakrabarti-2012 approach achieves relatively high on precision but low on recall, confirming its precision orientated nature. Thirdly, learn-ing synonyms jointly in our clustering framework clearly demon-strates advantages: it achieves better F1 scores than Chakrabarti-2012 across three datasets by simply fixing the weights to be all equal. Finally, our proposed compact clustering model is consis-tently obtaining balanced precision and recall, resulted in best F1 scores. It clearly outperforms the baseline of clustering with fixed weights, showing the benefit of automatic weight learning. More-over, its F1 score also consistently outperforms Chakrabarti-2012 . In fact, in two of the three datasets, the statistical T-Test indicates that there is statistically significant difference between our model and Chakrabarti-2012 at confidence level p = 0 . 01 . This reveals the effectiveness of our proposed model that identifies synonyms jointly with kernel weights automatically tuned.

We then compare the results on attribute values that don X  X  look like entity mentions. Such values include interesting instances like infant, women in shoe.gender , thriller in movie.genre , 2 year in babyclothing.age . We summarize the results in Table 4. As ex-pected, categorical pattern , pseudo document behave similarly as in the previous experiment, confirming using them individually is not effective in both forms of attribute values. Also, the clustering with fixed weights performs slightly better than Chakrabarti-2012 . Further, the compact clustering model achieves significantly better results than Chakrabarti-2012 across three datasets. We list some interesting cases that our proposed model identifies successfully but Chakrabarti-2012 fails. For example, for thriller, Chakrabarti-2012 finds  X  X ichael jackson thriller X  as its synonym while com-pact cluster doesn X  X . In fact,  X  X ichael jackson thriller X  is not the synonym of thriller in the particular domain of movie.genre . And our model identifies  X  X cary X  as its synonym, which is more appro-priate. The superior performance of compact clustering might be due to two reasons: first is that we aggregate all referent strings of the attribute value as proxies, therefore resulting in more robust estimate of similarity measures. And second, the joint modeling of multiple attribute values from the same implied domain effectively handles the ambiguity problem, which we will further discuss be-low.

Ambiguous synonyms handling is important for finding domain specific synonyms. Here we compare our model to Chakrabarti-2012 on a set of attribute values that are ambiguous. They include {jordan, coach, lv} from shoe.brand , {app, sun, adobe} from it-company.name , {aarp, advantage, aim} from insurance.provider , {thriller} from movie.genre , {matrix} from movie.title . Results on Table 5 clearly indicate that compact clustering is much more effec-tive than Chakrabarti-2012 on handling ambiguous attribute val-ues. Interestingly, the baseline of clustering with fixed weights also significantly outperforms Chakrabarti-2012 in this case, suggest-ing that joint modeling of multiple attribute values is particularly effective for ambiguous synonyms handling.

Our proposed model is able to learn the weights of similarity kernels. In this experiment we look into the learnt weights to see whether they reflect the relative importance of the similarity ker-nels. For this purpose, we have conducted the ablation test, in which we remove one similarity kernel at a time and run the model. We also report the weights learnt without removing any kernels. Results on three domains are shown in Table 6. These results indicate that pseudo document similarity seems to play relatively higher importance than other kernels. For example, both in movie.title and doctor.specialty , it carries the highest weights; and the F1 mea-sures drop to the lowest when removing this kernel (the lowest F1 is marked in bold). Interestingly, the categorical pattern similarity plays an important role in shoe.brand . Note that in this domain there are more ambiguous inputs (6 values) than other domains, suggesting the importance of categorical pattern similarity for dis-ambiguation.
For the problem of finding entity attribute synonyms, we pro-pose a compact clustering framework to simultaneously identify synonyms for a set of attribute values. In this framework, mul-tiple sources of information are integrated into a kernel function and synonyms are learned via unsupervised clustering. We have also proposed a novel similarity kernel called Categorical Pattern Similarity, which has proven to be effective for improving the per-formance of the compact clustering model. Extensive experiments demonstrate the effectiveness of our clustering framework over pre-vious approaches for identifying entity attribute synonyms, both in the cases where they are entity mentions or are arbitrary strings. We have also demonstrated the effectiveness of our model for am-biguity handling for identifying domain specific synonyms.
Further, besides attribute value synonym identification, our un-supervised framework of simultaneously modeling multiple inputs and integrating multiple kernels can be potentially applied to other applications, such as looking for related queries, product recom-mendation, question paraphrasing et. al.
