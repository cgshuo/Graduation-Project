 Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as  X  X ompare products X ,  X  X lan a tr avel X , etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data fo r learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated traini ng data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different traini ng datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process. Algorithms, Experimentation User intent, search engine, classification. The classical relevance based search strategies may fail in satisfying the end users due to the la ck of consideration on the real search intents of users. For example, when different users search with the same query  X  X anon 5D X  under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search resu lts about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users X  search queries is becoming a crucial problem for both Web search and behavior target ed online advertising. Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impo ssible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rule s, can help gene rate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions ma y help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as  X  X anon 5d compare with Nikon D300 X , he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground tr uth. First, the c overage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is, Without laborious human labe ling work, is it possible to train user search intent cla ssifier using the rule-generated training data, which are generally noisy and biased? Given  X  sets of rule-generated training datasets  X   X   X ..., X 1,2 X , , how to train the classifier  X  X  X : X  on top of these biased and noisy training data sets with good performance? Suppose we have  X  sets of rule-generated training data  X   X ...,1,2 , which are possibly noisy and biased, and a set of unlabeled user behavioral data  X   X  . Each data sample in the training datasets is represented by a triple  X  X   X  X  X   X ,  X  X  X   X |...,1,2  X  | , where  X   X  X  X  stands for the feature vector of the  X  sample in the training data  X   X  ,  X   X  X  X  is its class label and  X | total number of training data in  X   X  . On the other hand, each unlabeled data sample, i.e. the user search session that could not be covered by our rules, is represented as  X  X   X  X  X   X ,  X  X  X   X ,  X |...,1,2  X  |. Suppose for any  X  X  X  , all the features constituting the feature space are represented as a set  X  X  X  X   X  Suppose among all the features F, some have direct correlation to the rules, that is they are used to generate the training dataset  X  These features are denoted by  X   X   X  , X  X  which constitute a subset correlation to the rules used for generating training dataset  X  Given a classifier  X : X   X   X  X  , where  X   X   X  X  is any subset of F , we use  X   X  to represent an untrained classifier and use  X  the classifier trained by the training data  X   X  . Suppose  X  means to train the classifier  X   X  by training dataset  X  features  X   X   X  X  , we have  X   X   X   X  X   X   X  X   X   X |  X   X  , . X ..., X 1,2  X  For the  X   X  X  X  using features F . We assume for each output result of trained classifier  X   X   X  , it can output a confidence score. Let where  X   X  X  X   X  is the class label of  X   X  X  X  assigned by  X  the corresponding confidence score. After generating a set of training data  X   X  , , X ..., X 1,2  X  based on rules, we first train the classifier  X   X  by  X  independently. Then we can get a set of K classifiers, Note that the reason why we use  X   X  to train classifier on top of  X  instead of using the full set of features F is that  X  from some rules correlated to  X   X   X  , which may overfit the classifier  X   X  if we do not exclude them . After each classifier  X  by  X   X  , we use  X   X   X  to classify the training dataset  X  assumption of CLF is that the confidently classified instances by classifier  X   X   X  , X ..., X 1,2 X , have high probability to be correctly classified . Based on this assumption, for any  X   X  X  X  confidence score of the classification is larger than a threshold, i.e.  X  from the class label assigned by the rule, i.e.  X   X   X  X  X  is considered as noise in the training data  X   X   X  X  X   X  is the label of  X   X  X  X  assigned by classifier,  X  class label in tr aining data, and  X   X  X  X  is the true class label, which is not observed. We exclude it from  X   X  and put it into the unlabeled dataset  X   X  . Thus we update the training data by Then we use the classifier  X   X   X  , , X ..., X 1,2  X  unlabeled data  X   X  independently. Based on the same assumption that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to  X  if the confidence score of the classification is larger than a threshold, i.e.  X   X  X  X  &gt;  X   X  , where  X   X   X   X  X   X  X  X   X  X  include  X   X  X  X  into the training dataset. In other words, Through this way, we can gradually reduce the bias of the rule-generated training data. On the other hand, some unlabeled data are added into the training datasets. Suppose the  X   X , X   X   X   X  X   X 1|  X   X  X   X  is the probability of a data sample to be involved in the training data  X   X  at the iteration n conditioned on this data sample is represented as a feature vector  X  and  X   X   X 1 X   X  is the probability of any data sample in D is considered as a training data sample. It can be proved that after n iterations using CLF, for each training dataset, we have  X  The remaining questions are when to stop the iteration and how to train the classifier after iteration stops. In this work, we define the iteration stopping criteria as  X  X f |{  X   X  X  X   X |  X  X  X   X  X  or the number of iterations reaches N, then we stop the iteration X . After the iterations stop, we obtain K updated training datasets with both noise and bias reduc ed. Finally, we merge all these training datasets into one. Thus we can train the final classifier as In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search e ngine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user beha viors as ground truth for results validation. We name this dataset as the  X  Real User Behavioral Data  X . The n -gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the propos ed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training da taset or the combination of all rule-generated training datasets to train the same classification model can give us a set of cla ssifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as  X  Baseline  X  in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of co-training algorithm. It also starts from the rule-generated training data for classification and th us has the same experiments configuration as CLF. The classi fication method selected in CLF is the classical Support Vector M achine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline. One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users X  search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we pr opose to iteratively reduce the bias of the training data and contro l the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework. [1] Russell, D.M., Tang, D., Ke llar, M. and Jeffries, R. 2009. 
