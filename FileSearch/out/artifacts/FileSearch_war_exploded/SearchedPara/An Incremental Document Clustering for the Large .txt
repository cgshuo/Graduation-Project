 Various methods and techniques for processing unstructured data such as textual data are introduced in the field of information retrieval (IR). Basically, the main purpose of these techniques is to construct a large set of well-categorized documents automatically for effective searching and browsing [1]. For this purpose, document clustering and classification are actively studied [2] since they can play an important role in helping an information retrieval system with a huge number of documents. Given a predefined set of document classes, document classification is identifying the appropriate class of a particular document [3]. Traditionally, the document classification is carried out manually. In order to assign a document to an appropriate class manually, a user should analyze the contents of the document. Therefore, a large amount of human effort would be required. There has been some research work on automatic document classification. One approach is learning appropriate text classifiers by machine learning techniques [4, 5] based on a training data set containing positive and negative examples. The accuracy of a resulting classifier is highly dependent on the fitness of the training data set. However, there are lots of terms and various classes of documents. In addition, many new terms and concepts are introduced everyday. Consequently, it is quite impossible to learn a classifier for each document class in such a manner. 
In order to group a set of related documents automatically, clustering techniques [6, 7, 8] have been widely employed. The attractiveness of these cluster techniques is that they can find a set of similar data objects as a cluster directly from a given data set without relying on any predefined information such as training examples provided by domain experts [6, 7]. In most cases of such an application, a set of new documents is incrementally added to the data set. 
This paper proposes an incremental document clustering method. The characteristics of a document are represented by a set of keywords that are extracted by evaluating the term weight of each word in the document. The term weight of each keyword for a document indicates the relative importance of the keyword in the document. Given a finite data set of documents, most document clustering algorithms use a TF*IDF function [17] to find the term weight of a word in a document. The term frequency (TF) of a word in a document is the number of occurrences of the word in the document. The inverse document frequency (IDF) of a word is the number of documents containing the word and it indicates how commonly the word is used in the documents of the data set. When the IDF of a word is high, the usage of the word is localized to a small number of documents in the data set. However, the TF*IDF function is not suitable for an incremental document clustering algorithm due to the following reasons: (1) A word with a relatively low document frequency tends to have a high term weight, so that a large number of document clusters can be generated potentially. (2) As the number of documents in a data set becomes larger, increased specially when most of the documents contain a small number of words as in web documents. This is because the TF of a word in a document becomes small relative to its IDF. Furthermore, if document clustering should be performed in an continuously. For these reasons, a normalized inversed document frequency (NIDF) is used instead in this paper. 
Given an initial set of documents, the initial clusters of similar documents are found by a seed document clustering method called SCUP (Seed Clustering Using Participation and cohesion) in this paper. The SCUP algorithm is a kind of an average link method of hierarchical agglomerative clustering [9, 10]. In a hierarchical agglomerative clustering algorithm, two clusters of the highest similarity are merged in each step. However, there may be a more similar cluster in the future when a set of new documents is incrementally added. Accordingly, the accuracy of a cluster can be degraded in the future. To resolve this problem of a hierarchical agglomerative clustering algorithm, this paper proposes two similarity measures: a cluster cohesion rate and a cluster participation rate. The cluster participation rate is examined to merge a new document with current set of clusters. By using the cluster participation rate, the accuracy of end cluster can be guaranteed at any time. In addition, the hierarchical agglomerative clustering algorithm generally requires a great amount of memory space since it is proportional to the square of the number of documents in a data set [11]. In order to minimize the usage of memory space, the SCUP algorithm produces dendrogram. The resulting dendrogram of the SCUP algorithm is used by an incremental document clustering algorithm (IDC) proposed in this paper in order to construct the category tree of identified clusters. Consequently, as a new document is incrementally added to the data set, the most appropriate cluster for the document can be found in the IDC algorithm based on the category tree efficiently. 
Figure 1 illustrates the overall procedure of the proposed seed clustering SCUP algorithm. The SCUP algorithm is composed of the following steps. First, the keywords of each document in the initial set of documents are selected by the TF*NIDF method. Second, the proposed SCUP algorithm is performed to generate a set of initial clusters. Finally, a category tree for the resulting clusters is generated to be used by the incremental document clustering (IDC) algorithm for a new document. 
This paper is organized as follows. Section 2 describes related works. Section 3 introduces how the term weight of a word for a document in a dataset of documents is calculated to choose the keywords of the document. In Section 4, the proposed SCUP algorithm is presented in detail. Furthermore, the structure of a category tree for identified clusters is described. In Section 5, an incremental document clustering (IDC) algorithm is proposed. Section 6, several experiment results are comparatively analyzed to illustrate the various characteristics of the proposed algorithms. Finally, Section 7 draws overall conclusions. Current document clustering techniques can be broadly classified into two categories: partitional and hierarchical methods. The popular K-Means [12] and K-Medoid [13] methods are partitional methods which maintain k cluster representatives and assign each document to the cluster with its representative closest to the document. The strong point of the K-Means algorithm is that it is relatively scalable and efficient because it has low time complexity. However, a noise document can substantially influence the generation of a cluster, so it may be difficult to produce a correct result in some cases. The K-Medoid algorithm selects the centers of k clusters initially, and The hierarchical method such as BIRCH [14] confined hierarchy of the clusters in a data set to a tree-like structure. In BIRCH, a CF (Clustering Feature) tree which is used to summarize cluster representations is generated dynamically. After the CF tree is built, any clustering algorithm such as a typical partitioning algorithm is then used. The memory requirement problem of K-Means is resolved and thus the method is suitable for a large data set. However, these algorithms[12, 13, 14] are not suitable to solve the document clustering problem since the number of clusters in a set of documents is usually unknown to user [2]. 
Scatter/Gather [15] is a document browsing system based on the clusters of similar documents. It proposes two clustering techniques: buckshot algorithm and fractionation algorithm. The buckshot algorithm selects sample documents to find a set of clusters and assigns remaining documents to the identified clusters. The fractionation algorithm distributes documents into a fixed number of buckets. 
For the hierarchical method, there are two main approaches: divisive clustering and agglomerative clustering. The divisive cl ustering approach regards all documents in a single document is a cluster. The agglomerative clustering approach performs in reverse order. In [9, 10], document clustering algorithms based on the hierarchical agglomerative clustering approach are proposed. Each document itself is regarded as a cluster initially and the similarity between two clusters is examined for every pair of clusters. Subsequently, the most similar pair of two clusters is repeatedly merged until a predefined number of clusters are left. There are three different schemes in a hierarchical agglomerative clustering algorithm, namely a single link, a complete link and an average link [12]. A single link method measures the similarity of two clusters based on the largest document similarity among the similarity of two documents contained in each cluster. On the contrary, a complete link method uses the smallest document similarity between two documents in distinct clusters. Accordingly, unless most documents of two clusters are similar, two clusters are not merged. In the average link method, the similarity of two clusters is determined by the average document similarity instead. Among the three methods, clusters generated by the average link method are most accurate since every document of one cluster is examined to each document of the other cluster. However, researchers have found an agglomerative clustering algorithm is not suitable for maintaining clusters of an incrementally growing set of documents [16]. For each word used in a document of an initial data set, its term weight is calculated to choose the keywords of the document. For this purpose, the TF*IDF (Term Frequency Inversed Document Frequency) [17] is used widely to reflect the importance of a specific word in a document. According to the TF*IDF method, the weight ij tfidf of a word j w in a document i d is defined as follows: where N is the total number documents in a data set and the term frequency ij tf denotes the frequency of a word j w occurred in a document i d . In addition, the document frequency j df denotes the number of documents that the word j w appears in the data set. Equation (1) means that the possibility of a specific word representing the key concept of a particular document is proportional to the frequency of the word in the document. As the same time, it is also inversely proportional to the number of documents that contain the word. In other words, a word can be one of keywords for a document if it appears frequently in a small number of documents in a data set. 
However, as the total number of documents N becomes larger, the effect of the inversed document frequency on a term weight is increased. This is because the term frequency of a word in a document is usually in a certain range specially for a short document. On the other hand, the IDF has the range of [0, lnN], and hence the value of the IDF is greatly influenced by the total number of documents in a data set. Furthermore, when new documents are incrementally added to a data set continuously, the number of documents N is continuously increased. In order to avoid This paper introduces a TF*NIDF (Term Frequency Normalized Inversed Document Frequency) function in which the maximum value of the IDF is normalized within a follows the target range of its normalized inversed document frequency (NIDF) j nidf is following function makes the value of y ln be the range ] , 0 [  X  . 
Based on the above function y , the term weight ij tfnidf of a word j w in a document i d is defined by Equation (2). A word in a document is chosen as a keyword of the document if the term weight TF*NIDF of the word is larger than the average term weight of words in the document. Since the number of words in each document can be different, the range of the term frequency TF of a word in each document is not the same. In other words, when a word appears frequently in a long document, the TF of the word becomes large. As a result, its term weight can become large even though its NIDF is relatively small. To prevent this, the length of a document should also be normalized. 
To normalize the number of words in a document, the maximum frequency normalization [18] can be considered. In this method, it uses the ratio of the frequency of each word in a document over the most frequently used word in the document. However, this can cause a problem when the frequency of a specific word is exceptionally large. This paper uses a cosine normalization which has been widely used in a vector space model. In the cosine normalization, given a vector = , each element of the vector is divided by a cosine normalization normalize the length of a document based on the frequencies of all words in a document together. In this paper, the normalized term weight of a keyword j k in a document i d containing n distinct words represented by the cosine normalization is denoted by ) , ( j i k d t as follows: Given an initial data set of documents, the SCUP algorithm finds the initial clusters of the data set. Although the SCUP algorithm can be solely used as a clustering method hierarchical agglomerative clustering algorithm [10] but uses different similarity measures defined in Definition 1 and Definition 2. Definition 1. Document Similarity Given two documents i d and j d with their keyword sets i K and j K respectively, their document similarity measure ) , ( j i d d s is defined as follows: The above document similarity measure can provide the rate of similarity between only two documents. As a similarity measure for all the documents of a cluster, a cluster cohesion measure is defined in Definition 2. A cluster cohesion measure indicates how tightly the documents of a cluster are related in terms of their keywords. It is the average of the document similarities of all pairs of documents in a cluster. Definition 2. Cluster Cohesion cluster cohesion rate ) ( C h of the cluster C is defined as follows: In the conventional agglomerative approach, a cluster is forced to be merged with another cluster until a predefined number of clusters are left. However, the SCUP algorithm is intended to be used in an incrementally growing set of documents. Consequently, clusters should be carefully merged. In other words, a cluster should not be merged with another cluster unless the documents of the two clusters are similar enough to be merged. If a cluster can not find another cluster that is eligible to be merged in the current set of clusters, it should not be merged. This is because there may be a more similar cluster in the clusters of incrementally added documents in the future. For this purpose, a cluster participation measure between two clusters of documents is defined in Definition 3. The union of the document keyword sets of all documents in a same cluster is named as the cluster keyword set of the cluster. Definition 3. Cl uster Participation Given two clusters m C and n C of documents with cluster keyword sets m CK and defined as follows: Given a minimum cluster participation rate MinClPar and a minimum cluster cohesion rate MinClCoh , two clusters m C and n C are eligible to be merged into one cluster mn C which contains all the documents of the two clusters if the following conditions are satisfied. (i) MinClPar C C CP and MinClPar C C CP m n n m  X   X  ) | ( ) | ( and (ii) MinClCoh C h mn  X  ) ( dendrogram [9] of the SCUP algorithm is used as a category tree. It is widely used to represent the hierarchical cluster structure of a data set. It is generated by keeping merging two similar clusters repeatedly until all documents of the data set are grouped into one cluster. A node of a category tree represents a category. It contains its category keywords which are the union of the cluster keyword sets of all the clusters specific cluster. Most conventional document clustering algorith ms [10, 19] are not intended to be used in an incrementally growing set of documents. Therefore, whenever a set of new documents is added incrementally, all documents in the enlarged data set should be reclustered from scratch. To avoid this, this section presents an incremental document clustering algorithm (IDC) based on the resu lt of the SCUP algorithm presented in Section 4. When a new document is added to a data set of documents, among the current clusters, the most appropriate cluster is identified by traversing the category tree of the clusters starting from the root node of the category tree. The node participation rate of a new document l d for a node N in the category tree defined in Definition 4 is used to traverse the tree. Definition 4. Node Participation in the Category Tree Given a new document m d with its keyword set m K and a node N of a category tree, let NK denote the set of category keywords in the node N . The node participation rate 
For a newly added document d , starting from the root node of a category tree, a document d recursively searches down to its corresponding leaf node based on the node participation rate of each node in its path from the root node. Figure 2 illustrates how a newly added document is incrementally clustered. 
Whenever visiting a node of the category tree for a new document d , among the children of the node N , the one with the highest node participation rate for the document is identified. If the highest node participation rate is greater than or equal to a predefined minimum node participation rate, the corresponding child node is visited. Otherwise, the document is regarded as a noise document temporarily. This traversal is performed repeatedly until a document d visits a leaf node. When a leaf node is document d is greater than or equal to a predefined minimum cluster cohesion rate. If the above condition is not satisfied, the document is regarded as a noise document too. When a considerable number of noise documents are collected, the SCUP method is performed to generate a set of new clusters from the set of noise documents and the category tree is modified accordingly. Figure 3 shows how to insert a set of new documents incrementally. 
On the other hand, when a document in a cluster is deleted, if the updated cluster cohesion rate of the cluster becomes less than a minimum cluster cohesion rate, the documents of the cluster are reclustered by the SCUP algorithm to partition the documents into groups of more similar documents. To illustrate the performance of the proposed method, several experiment results are presented in this section. Among news categories provided in  X  X ahoo X , documents in 10 different domains such as business, science, politics and society are extracted as a data set of documents to be used in these experiments. For each domain, the average number of documents is 1026 and the average number of words in a document is 800. 
In Figure 4, the clustering result of the SCUP algorithm is compared with that of the hierarchical agglomerative clustering algorithm (HAC). To show the relative effectiveness of the proposed clustering algorithm the same similarity measures as described in Section 4 is used for the hierarchical agglomerative clustering algorithm. The resulting number of clusters generated by each algorithm is compared in Figure 4-(a). The average number of documents in a cluster is compared in Figure 4-(b). In addition, the average cluster cohesion rate is compared in Figure 4-(c). The number of clusters generated by the proposed SCUP algorithm is much smaller than that by the hierarchical agglomerative clustering algorithm. However, their order is reversed in terms of the average number of documents in a cluster. However, the average cluster cohesions of two algorithms are almost the same. 
About 10000 documents in the business domain of Yahoo are used to illustrate the performance of the proposed IDC algorithm. The proposed IDC algorithm requires a minimum cluster participation rate additionally. When the value of a minimum cluster participation rate is set to 0.2, the IDC algorithm shows the best result. When the value of a minimum cluster participation rate is set 0.2, in Figure 5, the result of the IDC algorithm is composed with the HAC algorithm. Since the HAC clustering is not an incremental algorithm, all documents of the data set are clustered together at the same time in terms of the number of generated clusters and the average number of documents in a cluster by varying the value of a minimum cluster cohesion rate. 
Given a set of document clusters } , ...... , , { 2 1 m p p p HC = generated by the HAC algorithm, let ) , ( HC q sim i denote the ratio of the number of common documents of a of documents in the cluster i q . The cluster j p includes most documents belonging Equation (8). 
Based on this, the similarity ) , ( HAC IDC sim between the result of the HAC algorithm and that of the IDC algorithm is defined by Equation (9). Hence, the difference between the result of the HAC algorithm and that of the IDC algorithm is defined by Equation (10). 
In Figure 6, the difference is illustrated when the values of a minimum cluster participation rate and a minimum cluster cohesion rate are varied from 0.5 to 0.9. As the value of a minimum cluster cohesion rate becomes higher, the results of the two algorithms become more similar. By varying the number of documents, the processing times of the HAC algorithm and the IDC algorithm are compared in Figure 7. As the number of documents is increased, the processing time of the HAC algorithm is increased more rapidly since the HAC algorithm is not incremental. A TF*NIDF function is introduced to overcome the weak points of the TF*IDF function since the SCUP algorithm should be performed in an incremental way. This paper proposes the SCUP algorithm to find the initial clusters of similar documents in rate. This paper introduces a category tree for incremental hierarchical document clustering, so that it is used by the incremental document clustering (IDC) algorithm added document is examined to be clustered to the most appropriate cluster in the category tree. By comparing the IDC algorithm with the HAC algorithm, the cluster accuracy of the IDC algorithm is more similar relatively to the HAC algorithm. However, the processing time of the IDC algorithm is faster than that of the HAC algorithm when the number of document is increased. 
