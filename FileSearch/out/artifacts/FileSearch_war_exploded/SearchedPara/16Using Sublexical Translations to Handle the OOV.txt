 Many sentences are submitted to machine translate (MT) systems every day, and an increasing number of translation services are available between various source and target language pairs. For example, both Google Translate 1 and Windows Live Translator 2 can promptly translate a block of te xt or a web page. Among different MT systems, there are three common components: the translation model, the language model, and the reordering model. The trans lation model performs transformations from source phrases (or syntax-based rules) to corresponding target ones by querying the phrase table (or syntax-based translation rules). On the other hand, the language model and the reordering model evaluate the fluency and the word orders of the gen-erated target language translation, respectively. While the translation and reordering model are trained on bilingual corpora, the language model is usually trained on monolingual corpora.

Due to the creativity and diversity of nat ure languages, not all source words are known to MT systems specifically their translation models (i.e., phrase tables or syntax-based translation rules), in which case most of the current systems treat them as out-of-vocabulary (OOV) words and typically leave them untranslated. However, leaving the unknown OOVs untouched in the output translation may degrade the over-all translation quality since the lexical choices and reordering around the OOVs may be negatively impacted, not to mention the influence on a human X  X  understanding of the target translation. The problem of OOV could be better handled if a model recog-nized and translated the constituents of an OOV word.

In general, the causes of unknown words can be mainly categorized into following types. OOVs result from segmentation error in the source language (e.g.,  X  X  X  X  is erroneously split into two words  X  and  X  X  which leads to an  X  X  ) ). Another source of OOVs can be attributed to name entity such as person name, location, and organiza-tion. Finally, OOVs may originate from low-frequency abbreviations (e.g.,  X  X  X  athletic association) and combination forms (e.g., eyebank, widebody) of common words (e.g., bank, body). In this article, we focus on handling OOVs of the last type, abbreviation, and combination, which accounts for one f ourth of OOV cases according to our OOV analysis in Section 4.

Consider the Chinese sentence  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (the muscle strength of wang yan X  X  upper limbs has regained by two levels). If MT systems do not cover  X   X  X   X  (upper limbs) in the translation model, typically it, an OOV, will be sent out untouched to the output. Better result might be obtained by first finding translations for the OOV X  X  constituents such as  X  X pper X  (for the  X  part) or  X  X imbs X  (for the  X  part), and combining these sublexical translations (i.e.,  X  X pper X  and  X  X imbs X ) to yield the ref-erence translation:  X  X pper limbs X . We can leverage wildcard queries  X   X   X  X nd X *  X   X , where * stands for any Chinese character, to find theses sublexical translations. Intu-itively, by extracting and combining the translations of OOVs X  sublexical constituents (i.e., Chinese ideographs), we could correctly translate the unknown words.
We present a method that automatically retrieves translations of OOVs X  con-stituents from existing bilingual resource in a MT system that, if combined, are ex-pected to translate OOVs. An example sublexical translation process for the unknown  X   X  X   X  (upper limbs) is shown in Figure 1. Wildcard search would find possible sublex-ical translations, in this case,  X  appeal, rise, upper, and surface  X  X orthequery X   X  * X  and  X  body, extremity, and limbs  X  for the query  X *  X   X   X , but some are not appropriate for (or in the context of) the unknown  X   X  X   X  (upper limbs). Our model constrains the choices of the sublexical translations and removes unlikely ones by analyzing a collection of monolingual and bilingual lexical databases. We describe the process in more detail in Section 3.

At runtime, for an OOV in a source sentence, our model retrieves a limited number of sublexical translations for its constituents and generates an ordered list of its trans-lation candidates based on bilingual lexical correspondences and monolingual fluency. The ordered candidate list returned by the proposed model can provide translation choices for human translators directly or can be incorporated into MT decoders to ease the negative impact of OOVs on translation quality.

The rest of this article is organized as follows. We review the related work in the next section. Then we present our method for automatically learning to find sublex-ical translations and to rank assembled translation candidates for OOVs expected to broaden the translation coverage of MT systems (Section 3). As part of our evaluation, Section 4 describes the experimental settings based on our OOV analysis while Sec-tion 5 compares the performance of a state-of-the-art MT system with or without our OOV model. Finally, Section 6 concludes this article. Machine Translation (MT) has been an area of active research. Recently, the research of translating out-of-vocabulary (OOV) words in MT systems has received much atten-tion. In our work we address an aspect of OOV translation utilizing wildcard query searches and MT system X  X  existing resources.

More specifically, we focus on translating t he source words without direct transla-tion equivalents in the bilingual resources of MT systems by combining translations of their constituents, or sublexical translations, retrieved using wildcard queries. To reduce the number of OOVs, adding more pa rallel data to MT systems seems feasible but may not be always possible for some language pairs. Therefore, interesting ap-proaches were presented to generate additional parallel sentences from comparable, non-parallel, monolingual corpora by bootstrapping and machine learning [Fung and Cheung 2006; Munteanu and Marcu 2005]. In general, the accuracy of aligning these texts remains a challenge for these studies. In contrast, we will show how to trans-late OOVs by retrieving and combining their sublexical translations based on existing parallel data.

More recently, some work has begun to ext ract translations for OOV words from external knowledge sources such as dictionaries and the Web. Unknown words were replaced with their definitions or translations in manually-compiled dictionaries [Eck et al. 2008; Vilar et al. 2007], or with translations mined from large-scale Web data [Cao and Li 2002; Nagata et al. 2001]. Although the Web-based approach avoids the low translation coverage issue facing the di ctionary approach, its translation accuracy highly depends on the type of OOVs (accuracy for name entities is very high while common nouns tend to be low). In our method, our translation candidates are within MT systems X  training corpus not from external knowledge.

Recent work has been done on translating different OOV cases: name entities (NE), compounds, and morphological variants. Knight and Graehl [1997] introduced a statistical machine transliteration model to tackle proper names while Hassan and Sorensen [2005] presented a NE translating approach that combines NE translation and transliteration in a single framework. On the other hand, Cao and Li [2002] and Tanaka and Baldwin [2003] focused on translating compound words, especially noun phrases, via statistical approach and translation templates. Furthermore, in some languages (e.g., Arabic) where morphological variants are a major cause of OOVs, much work has been described to transform these variants into in-vocabulary word forms [Arora et al. 2008; Koehn and Knight 2003; Langlais and Patry 2007; Yang and Kirchhoff 2006]. In contrast, we focus on translating OOVs resulting from abbrevia-tions of source phrases (e.g.,  X   X  athletic association) or combination forms of com-mon words (e.g.,  X  X  blood bank). These two types cover some portion of name entities and noun-noun (NN) and adjective-noun (AN) compounds (e.g.,  X   X  border trade (NN) and  X  X  X  new regulations (AN)).

In the studies more closely related to our work, Marton et al. [2009] and Mirkinet al. [2009] presented paraphrase models that replace OOV words with in-vocabulary equivalents for translation. Paraphrases in Marton et al. [2009] are learnt based on word alignments computed over a large additional set of bitexts, while Mirkinet al. [2009] paraphrased OOVs using entailment rules that are derived from monolingual corpora as well as manually compiled synonym thesaurus. These studies are similar in spirit to our work. However, we do not directly address the problem via paraphrasing. In our model, the wildcard translation searches of OOVs constituents might retrieve their source-language paraphrases with translations. Instead of using the source para-phrases, we directly use the target translations.

Recently, Li and Yarowsky [2008] proposed an unsupervised method for finding the mappings between full-form phrases and their abbreviations that are OOV words. The main difference from our current work is that in their approach they need a reverse MT system and they focus on solving OOVs of proper names or name entities. For example, they first identify a target-language name entity  X  X ong Kong Governor X , translate it via the reverse MT system to source-language full-form  X   X  X  X   X  X  X   X , and finally incorporate its abbreviations, say  X   X  X  X   X , and  X  X ong Kong Governor X  into MT system. Our approach generates translation candidates for OOVs of abbreviations and combination forms, which co ver both common words (e.g.,  X  X  became famous,  X  X  X  border trade, and  X  X  X  new regulations) and proper names. In addition, our wildcard searches for translations of OOVs X  constituents from existing resources of MT systems can be viewed as finding the translations of their full-form words.

In contrast to the previous research, we present a model that automatically trans-forms the exact-match search for translations of an OOV word into a sequence of wild-card queries for sublexical translations, with the goal of maximizing the probability of finding suitable translations for the OOV word. We provide good translation sug-gestions for OOV words by combining their sublexical translations and filtering out improbable ones with monolingual and bilingual consideration. Submitting natural language sentences with OOV words (e.g.,  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X ) to machine translation systems does not work very well. MT systems typically generate the corresponding target-language translations by matching exact words or phrases in their translation models (i.e., phrase tables or syntax-based translation rules). Unfortunately, OOVs have no matches and MT systems would ignore them and leave them untouched. Leaving unknown source words untranslated or directly copying them to the translation output often leads to incorrect lexical choices and word orders s urrounding them, subsequently degrading the translation performance and humans X  understanding of the target translation. To translate an OOV, a promising approach is to automatically transform the exact-match lookup into a set of wildcard searches that are expected to find the common sublexcial translations of the OOV (i.e., translations of the OOV X  X  constituents). We focus on finding translations of an OOV word from existing bilingual resources (e.g., phrase table) via constituent or sublexical lookups. These translation candidates are ranked and returned as the output of the model. The returned candidates can be examined by human translators directly, or passed on to MT decoders (e.g., Moses) to ease the impact of OOVs. Sublexical wildcard searches tend to lead to ambiguity. Thus, it is crucial that sublexical translation choices be constrained to confident trans-lations. At the same time, the set of the OOV X  X  translation candidates cannot be so large that it overwhelms the users or the subsequent (typically computationally ex-pensive) decoders. Therefore , our goal is to return a reasonable-sized set of translation candidates that, at the same time, contain suitable translations of the OOV word. We now formally state the problem that we are addressing.

Problem Statement. We are given a database of translation equivalents TE (e.g., MT phrase tables) trained on a parallel corpus C (e.g., Hong Kong Parallel Text), a large-scale target-language corpus CT (e.g., English Gigaword), and an out-of-vocabulary word O . Our goal is to generate a ranked list of translation candidates that are likely to provide suitable translations for O . For this, we identify the constituents o 1 , ..., o m of O , retrieve, and evaluate the translations of o i from TE by partial matching o i .The retrieved translations of o i , if combined, are likely to translate O .

In the rest of this section, we describe our solution to this problem. First, we define a strategy for finding sublexical translations, translations of a constituent of the given OOV (Section 3.2). This strategy relies on a set of reformulated wildcard searches in replace of the original exact-match inferred from the OOV analysis on the development data (which we will describe in detail in Section 4.2). In this section, we also describe our method for automatically pruning unlikely sublexical translations. Finally, we show how our model assembles the sublexical translations of the OOV and ranks the assembled translation candidates at runtime according to bilingual and monolingual consideration (Section 3.3). For a given OOV, we attempt to find the relevant translations of its constituents which, if combined, are expected to convey the meaning of the OOV. Our constituent translat-ing process is shown in Figure 2. 3.2.1 Retrieving Possible Sublexical Translations. In the first stage of the sublexical trans-lating process (Step (1) in Figure 2), we retrieve possible translations for a constituent of an OOV from a bank of translation equivalents TE . Specifically, we transform the traditional exact-match search for an OOV X  X  translations into a sequence of wildcard sublexical lookups for constituents X  translations. By doing so, the OOV may be decom-posed and translated. For example, while the search for the translations of the OOV  X   X  X   X  (upper limbs) in the existing bilingual resources (e.g., MT phrase tables) is un-successful, the reformulated wildca rd sublexical searches, that is,  X   X  * X  and  X *  X   X , are not. Better yet, the extracted constituent translations,  X  X pper X  from  X   X  * X  and  X  X imbs X  from  X *  X   X  can be combined to translate the OOV word. Figure 3 shows the algorithm for retrieving translations for a constituent of an OOV.

In Step (1) of the algorithm we initiate TransCol to collect possible translations of a constituent c of an OOV O . Based on the constituent c of O and its position in O , we formulate wildcard queries for c  X  X  translations [Step (2)]. We will describe how to formulate search queries according to position in detail in Section 4.3. In Step (3), based on each query search, we retrieve possible sub lexical translations from the translation equivalents TE and append them to TransCol .

Take the constituent  X   X   X  X nd X   X   X  X ftheOOV X   X  X   X  for example. The wildcard query  X   X   X  X nd X *  X   X  generated by the algorithm in Figure 3 yields the set of bilin-3.2.2 Extracting and Restraining Sublexical Translations. In the second stage of the trans-lating procedure (Step (2) in Figure 2), we extract the sublexical translations based on redundancy, and constrain the translation words to frequently-seen ones with a view to composing proper translations for the OOV.

The input to this stage is the possible translations of a constituent obtained from the previous stage, represented by &lt; source word , target phrase &gt; pairs. The output of this stage is a set of &lt; source word , target N-gram &gt; pairs, in which target N-gram may cover different surface forms (e.g.,  X  X imb X  and  X  X imbs X ) of a lemma (e.g.,  X  X imb X ).
The method for extracting and selecting frequent sublexical translations, that is, { &lt; source word , target N -gram &gt; } , involves generating target-language N-grams in tar-get phrases , constraining the choices of target words by consulting a target-language lexical database, and filtering out infrequent words. Each step is discussed in more detailed below.

For each &lt; source word , target phrase &gt; pair, we first generate target N -grams from the target phrase .Take &lt;  X   X   X   X ,  X  X our limbs X  &gt; for instance. Target N -grams include  X  X our X ,  X  X imbs X , and  X  X our limbs X . Second, we constrain content words (e.g., nouns, adjectives, and verbs) in the target N -grams to those seen in a large lexical database (e.g., WordNet). Obviously, if a word is unseen in a lexicon, it is probably not a good translation. Third, we prune infrequent N -grams. To compare fairly, the occurrence count is accumulated over different inflect ed word forms. For example, the occurrence counts for unigrams  X  X ppeals(29) X ,  X  X ppealed(2) X ,  X  X ppeal(93) X , and  X  X ppealing(3) X  sharing the same lemma  X  X ppeal X  are folded to give the lemma a count of 127, where the integer in parentheses denotes the frequency. For the purpose, we use a lemma-tizer [Bird et al. 2008] in our implementation. The method described above would yield frequent and common sublexcial translations in the form of &lt; source word , target N-gram &gt; . See Table I for an example. Notice that the target N -grams would cover many inflectional forms (e.g.,  X  X imb X  and  X  X imbs X ) for the translations of the constituent (e.g.,  X   X   X ), which is generally beneficial to the subsequent sentence-level translation task. 3.2.3 Pruning Less Probable Sublexical N-gram Translations. In the third and final stage of the procedure [Step (3) in Figure 2], we prune less probable sublexical translations of OOVs based on bilingual associations. In the previous step, to extract the translations for the constituent c , each translation pair &lt; source word , target phrase &gt; is trans-formed into inflectional N -gram pairs &lt; source word , target N-gram &gt; .Some N -grams, however, are less related to c . Take the word-phrase pair &lt;  X   X   X   X ,  X  X our limbs X  &gt; re-trieved for the query  X *  X   X  (limb) for example. Since the unigrams,  X  X our X  and  X  X imbs X , are common and frequent, &lt;  X   X   X   X  X  X our X  &gt; and &lt;  X   X   X   X   X  X imbs X  &gt; will both be consid-ered as valid N -gram translations. While  X  X imbs X  is an appropriate translation for the constituent  X   X   X ,  X  X our X , whose Chinese translati ons are typically associated with  X   X   X , is probably not. To reduce computational effort and achieve better translation accu-racy, we filter out less probable sublexical N -gram translations before combining an OOV X  X  sublexical translations at runtime.

The input to this stage is a set of &lt; source word , target N-gram &gt; pairs from the previous stage where the source word contains a querying constituent c . The output of this stage is a subset of these N -gram translation pairs expected to have strong constituent-translation associations.

First, we exploit a bilingual dictionary (e.g., bilingual WordNet) to build reference bilingual associations. In the following, we describe two approaches for construct-ing/registering reference associations. Sample associations yielded by the two ap-proaches are shown in Table II.  X  All-constituent approach. For each entry &lt; source phrase, target phrase &gt; in the dic-tionary, we build bilingual associations between all constituents in the source phrase and all N -grams in the target phrase . That is, once a source-language constituent co-occurs with a target-language N -gram, an association between them is registered.
Take the dictionary entry &lt;  X   X  X   X ,  X  X ind limb X  &gt; for instance. We first identify two constituents  X   X   X  X nd X   X   X  X n X   X  X   X  and build associations between the constituents and N -grams in  X  X ind limb X , that is,  X  X ind X ,  X  X imb X , and  X  X ind limb X . Finally, we register six constituent-translation associations for this entry (See Table II).  X  Salient-constituent approach. In this approach, associations are only registered be-tween the salient constituent of the source phrase and N -grams of the target phrase for each dictionary entry &lt; source phrase , target phrase &gt; . We define that a con-stituent of a source phrase is a salient constituent if it is most associable to the target phrase . Formally speaking, the salient constituent c  X  for the &lt; source phrase , target phrase &gt; is chosen satisfying where c stands for the constituent in the source phrase and Count (  X  ) for the occur-rence count in the dictionary. Note that the set of the bilingual associations gen-erated by this approach is a subset of those generated by all-constituent. Take the entry &lt;  X   X  X   X ,  X  X ind limb X  &gt; for example. After consulting the dictionary which shows Count ( X   X   X , X  X ind limb X ) and Count ( X   X   X ,  X  X ind limb X ) are 1 and the occurrence of  X   X   X ,  X   X   X , and  X  X ind limb X  are 1073, 201, and 1, respectively, we choose the constituent  X   X   X  with higher dice value with the target phrase  X  X ind limb X  as the salient constituent and register associations ( X   X   X   X ,  X  X ind X ), ( X   X   X ,  X  X imb X ), and ( X   X   X ,  X  X ind limb X ) (see Table II).
Once the reference bilingual associations are constructed, we are ready to filter out less likely sublexical translations. Specifically, we only retain the translation pair &lt; source word , target N-gram &gt; of a query constituent c if and only if ( c , target N-gram ) is seen in the reference associations. Take the sublexical N -gram translation pair &lt;  X   X  X   X ,  X  X our X  &gt; from search query  X   X   X  for instance. It will be pruned according to the associations in Table II (Table II does not contain reference association ( X   X   X ,  X  X our X )).
Notice that, different approach, that is, all-constituent and salient-constituent ap-proach, can be leveraged for different application since all-constituent aims at high recall and salient-constituent high precision. Take the dictionary entry &lt;  X   X  X  X   X ,  X  X ring X  &gt; and &lt;  X   X  X   X ,  X  X oap X  &gt; , for example. While the approach of salient constituent under-generates a bilingual association for the entry &lt;  X   X  X  X   X ,  X  X ring X  &gt; (both  X   X   X  X nd  X   X   X  are highly associable to  X  X ring X ), all-constituent approach over-generates an in-direct association between  X   X   X  (fragrant) and  X  X oap X  for &lt;  X   X  X   X ,  X  X oap X  &gt; .Inourim-plementation, we first refer to all-constituent associations to prune and maintain high recall rate. If there are still too many translation candidates, we then resort to the salient-constituent associations to prune more aggressively. After pruning, the output of this stage is a set of translation pairs expected to have strong constituent-translation associations.

Note that the reference bilingual associations in this stage could be modeled as soft constraints and the frequencies of the regis tered associations could be used as feature scores for runtime candidate ranking. Once the confident sublexical translations of an OOV are found, our model then gen-erates and ranks translation candidates for the OOV word using the procedure in Figure 4.
 For each constituent c of the given OOV O , we first retrieve its probable translations SubTrans , including inflected forms, from existing translation equivalents TE using the method described in Section 3.2 [Step (1a)]. SubTrans is a list of &lt; source word , target N-gram &gt; pairs where source word contains a constituent of O . Then, we use bidirectional conditional probability to measure the association strength between the constituent c and its translation in SubTrans , and record such info rmation at corre-sponding position [Step (1b)]. Following the format in Step (1a), elements in CandList N-gram )). Conditional (sublexical) probabilities P sub ( target N-gram | c )and P sub ( c | target N-gram ) are trained on large parallel corpus in which the unit of token in the source language is constituent not word.
 Take an OOV word  X   X  X   X  (upper limbs) for instance. We first obtain Sub-per) and  X   X   X  (limbs), respectively. Then, we calculate the two-way association strengths between the constituents and their extracted ( N -gram) translations and summarize these in CandList (See Table III).

After acquiring the translations of each constituent of the given OOV, we combine these constituent translations to generate the OOV X  X  translation candidates. Although the translation scope of an OOV is much smaller than that of a whole sentence, reorder-ing of the OOV X  X  sublexical translations can still happen. For example, the reference translation of  X   X  X  X   X  is  X  X ir adjustment X  where the translation  X  X djustment X  and  X  X ir X  of the constituent  X   X   X  X nd X   X   X , respectively, are inverted. For this, both straight and inverted translation candidates are generated during the combination of the sublexical translations.

In Step (2), we initialize Straight and Inverted to collect the OOV X  X  translation can-didates by composing its sublexical translations in straight and inverted order. During candidate generation, Straight and Inverted iteratively cover more span of the OOV [Step (3)] collecting constituent translations and multiplying sublexical translation probabilities at the same time. For each assembled translation candidate TransCand , its translation probability P trans is estimated by the product of two-way conditional probabilities of the sublexical translation pairs as: where c i denotes a constituent of O and target N-gram i , j the sublexical translation for c i composing TransCand . Notice that in this step we concatenate source word  X  X  generating parts of TransCand as source phrase , represented by ( O , &lt; source phrase , TransCand &gt; , P trans ( TransCand )). Therefore, in the end we obtain not only the esti-mated translation probability of a translation candidate of the OOV O but the source phrase that generates the (composed) translation candidate (note that the source phrase may be a paraphrase, possi bly a full-form paraphrase, of O ). Consider one sub-lexical translation tuple in CandList [1], ( X   X   X , &lt;  X   X   X   X ,  X  X pper X  &gt; ,0.02  X  0.56), and one in CandList [2], ( X   X   X   X , &lt;  X   X  X   X ,  X  X imbs X  &gt; ,0.05  X  0.01), of the OOV  X   X   X   X  (upper limb). Step (3) would generate a straight translation candidate ( X   X  X   X , &lt;  X   X   X   X  X   X   X ,  X  X p- X  X imbs upper X , (0.02  X  0.56) 1 / 2 (0.05  X  0.01) 1 / 2 ).

Apart from bilingual information, we further leverage monolingual information to prune and estimate assembled translation candidates. In Step (4), we first prune less probable word combinations in the target language, and, for those which survive the pruning, incorporate target-language language model probabilities P TML ( TransCand ) into Straight / Inverted . For pruning, we calculate MI value, long been regarded as a good estimate of the likelihood of a word sequence, of each bigram w 1 and w 1 in an assembled translation candidate using After merging straight and inverted cases, we rank translation candidates based on bilingual translation probabilities and target-language language model [Step (5)]: where  X  i is the feature weight and  X  i equals to 1. Target-language language model plays an important role in ranking the translation candidates especially when straight and inverted translation candidates are all taken into consideration, fluency of the composed translation candidates.

Finally, the N top-ranked candidates whose probabilities ( P ) exceed a threshold  X  are returned as the likely translations of the given OOV. Notice that  X  will be tuned on development data for better translation quality. An example translation process for the OOV  X   X  X   X  (upper limbs) is shown in Figure 1. The OOV handling model was designed to find the translations of OOV words from ex-isting bilingual resources that are likely to help human translators or MT systems ease the negative impact of OOVs. As such, the OOV model will be trained and evaluated over translation task on top of an existing MT system. More specifically, we incor-porated the OOV model into an existing Chinese-to-English MT system and carried out the evaluation process. Furthermore, since the goal of the model was to generate a set of good translation candidates for an OOV, we evaluated the model on the sen-tences with OOVs to which it provides translation suggestions. In this section, we first introduce our underlying MT system, Moses, and describe its capability of plugging in OOVs X  translations as well as the data sets in our experiments, including training and development data (Section 4.1). Then, Section 4.2 describes how we formulate the wildcard query in substitute of the original exact-match search for translations of OOV words based on the observation from the development data. Finally, we report the parameter settings of our model in Section 4.3. The proposed OOV model focused on translating OOV words by exploiting existing bilingual resources (e.g., phrase table) in a MT system. Therefore, our model was built on top of a statistical MT system which accepts translation suggestions of our OOV model. In our experiments, we used the state-of-the-art phrase-based SMT system, Moses [Koehn et al. 2007], as our underlying decoder. It provides easy XML markup for plugging in external translation knowledge/suggestions without changing any com-ponent within such as translation or language model. In this article, we leveraged the markup language to incorporate OOVs X  translation candidates into Moses. Table IV shows an example sentence and its XML version with OOV translations.

To train Moses X  translation model, we used Hong Kong Parallel Text (LDC2004T08) and Xinhua News Agency (LDC2007T09). Chinese sentences within were word seg-mented by the CKIP Chinese word segmentation system [Ma and Chen 2003]. Com-mon settings were used to train Moses: GIZA++ [Och and Ney 2003] was used for word alignment, grow-diagonal-final [Koehn et al. 2005] for bi-direction word align-ment combination, and phrase extraction heuristics in Koehn et al. [2005] for bilingual phrase pair acquisition. Moreover, we exploited Xinhua news from English Gigaword Third Edition (LDC2007T07) and SRILM toolkit [Stolcke 2002] to build trigram lan-guage model used in decoding.
 In our OOV model, on the other hand, we leveraged WordNet 3.0 [Miller 1995] and Sinica BOW [Huang et al. 2004], a manually translated Chinese version of WordNet, to filter sublexical translations (see Section 3.2). To prune less probable translation can-didates of OOVs at runtime (see Section 3.3), we utilized Web 1T 5-gram First Edition (LDC2006T13) for MI calculation. As for the runtime candidate ranking (see Section 3.3), we exploited the same parallel corpora and target-language corpus used for train-ing Moses to estimate translation probabilities between source-language constituents and target-language words ( P trans ) and target-language fluency ( P TLM ), respectively. To study the problem of translating OOV words, we used NIST MT-08 test set consist-ing of 1,273 unknown words in 637 sentences out of a total of 1,357 sentences. Among these 1,273 distinct OOV words, we first inspected the number of OOVs with respect to their lengths, that is, the number of characters (See Table V). One could see from Table V that OOVs of two characters accounted for more than half of the OOV cases 3 . As a result, we focused on translating two-character unknown words. To further an-alyze the distribution of OOV types, formulate appropriate wildcard query replacing the exact-match search, and determine good bilingual resources for sublexical trans-lation retrieval, we randomly selected 100 sentences containing only two-character OOVs from MT-08 set. OOVs in these 100 sentences were classified into 10 types (shown in Table VI) according t o their reference translations manually extracted from reference sentences. Figure 5 shows an example source sentence containing an OOV  X   X  X   X  (upper limbs) and its four reference translations with the OOV X  X  translations underscored. Since our model was designed to retrieve and combine translations of an OOV X  X  constituents, it targets specifically at translating OOV words of the Combina-tion Form which accounts for one fourth of the OOVs (taking up the same proportion as Paraphrase , an active area in the MT OOV research). See Table VI for more details.
Intuitively, there are four ways to formulate the query for sublexical translations of a two-character OOV c 1 , c 2 of the type Combination Form . Table VII shows that the first and the second 4 query formulation of adding wildcard * could retrieve most relevant translations for this OOV catego ry. Therefore, our model adopted these two query formats for finding sublexical translations.
Additionally, to determine the applicability of various bilingual resources for find-ing sublexical translations, we compared the translation hit rates of the Combination Form OOVs using different resources based on the aforementioned two query formats. Among resources, Lin Yutang X  X  Chinese-English Dictionary 5 , LDC translation lexicon (LDC2002L27), character-based phrase table and word-based phrase table, hit rates were 0.64 6 , 0.68, 0.60, and 0.88, respectively. Word-based phrase table resulted in the highest translation coverage, thus chosen as our bilingual resource for sublexical translation retrieval. Apart from its high hit rate, there are other advantages in us-ing word-based phrase table: it includes different inflectional word forms and is more domain-relevant to the NIST MT test set. In this subsection, we describe the pilot experiment with the development data set of 50 sentences randomly selected from NIST MT-08 test set. Half of the sentences contained OOVs of Combination Form, and there was at least one OOV per sentence. We used this data set to fine-tune the two parameters in our system: the number of translation candidates returned by the OOV model N and the filtering threshold  X  used to prune less probable translation candidates at runtime, thus differentiating OOVs of Combination Form from those that are not.

Since the OOV model was designed to provide OOVs X  translation suggestions to hu-man translators or MT systems, the size of the returned translation candidates could not be so large that it overwhelms the user or decreases the efficiency of the subse-quent translation task. Therefore, our goal was to return reasonable-sized translation candidates for an OOV word with its correct translations ranked higher. To choose a suitable N , we leveraged the sentences with combination-form OOVs in the devel-opment set and evaluated the performance of translating these OOVs using Mean Reciprocal Rank ( MRR ). Here, MRR is defined as a measure of how much effort is needed for a user to locate the first correct translation for the given OOV in the ranked list of translation candidates. The MRR value lies between 0 and 1, where 1 im-plies that the first-ranked candidates are always the correct translations of the OOVs. Table VIII summarizes the hit rate X  X  and MRR  X  X  at different values of N  X  X  for the 25 combination-form OOV words in the development set. We eventually set N to 10 con-sidering translation coverage, MRR , and the time complexity at decoding.

Moreover, a threshold  X  on the probability of the translation candidate was used for pruning and determining the applicability of our model on OOV words (Some OOVs are not suitable for the solution in this paper). To select a suitable  X  ,wecompared translation results of the development data at different levels of filtering thresholds (Figure 6). As indicated in Figure 6, when the threshold was larger than -7, very few translation candidates were considered, leading to not much performance difference from the underlying Moses. On the other hand, when the threshold was lower than -13, more noisy translations were incorporated, leading to a decline in translation quality. We chose -12, the best performing threshold (probably achieved balanced performance between translation accuracy and coverage), as our threshold to prune less probable translation candidates or to activate our OOV model.

In summary, we set the runtime filtering threshold  X  to -12, and the maximal num-ber of the returned translation candidates N to 10 in our experiments which will be described in the following section. In this section, we report the results of the experimental evaluation using the method-ology described in the previous section. Firs t, in Section 5.1 we report the translation performance of the underlying MT system, Moses, with and without our OOV model based on BLEU score [Papineni et al. 2002]. In Section 5.2 we discuss the differ-ences between the translations generated by the two systems and point out the future improvement of our OOV model. During the evaluation, we used NIST MT-06 test set, containing 1,664 sentences, for testing. In this test set, there were 933 distinct unknown words scattered in 859 sentences, and its number of OOVs with respect to OOVs X  lengths was much alike to that in our developing set (see Table IX): two-character OOVs also accounted for half of the OOV cases. In the experiment, our OOV model generated translation candidates for 170 distinct two-character OOV words in 351 sentences. Recall that the parameter  X  in the OOV model determines its applicability, that is, whether the combination way of translation is suitable (or acceptable) for the OOV in question. And the produced translation candidates were incorporated into the underlying Moses decoder using XML markup.

Table X shows the overall performance of the underlying Moses and the CST system (i.e., Moses with combined sublexical translations). Although the difference in BLEU score between them is not very significant, the improvement in brevity penalty (BP) is noticeable. That is, the CST system generated sentences closer to the reference translations in length. A slightly better BLEU score implies the additional words (i.e., translations) provided by our OOV model achieved better or at least similar translation accuracy compared to the underlying Moses system.

On the other hand, if we look at the performance of the 351 sentences in the test set for which our OOV model provided translation candidates, the CST system signifi-cantly 7 outperformed Moses in BLEU and encouragingly improved the brevity penalty relatively by 4.4% (see Table XI). The lower BLEU scores in Table XI (vs. BLEU scores in Table X) indicate that these 351 sentences were more difficult to translate, probably due to the existence of OOV words.
 The experimental results show that we were able to translate some portion of the OOV words without degrading the performance of an existing machine translation system and the translation quality of the sentences was substantially improved with our automatic translation suggestions for OOV words. In this subsection, we examine example English translations for OOV words provided by our model. We also point out future direction for our model.

Table XII(a) shows four examples in which the reference translations of OOVs were ranked high by our OOV model (bold-faced) and the underlying decoder chose the cor-rect translations for the OOVs. One may find that the combined words in the OOVs X  translations belong to different part-of-speech (POS) sequences:  X  X orean war X  (AN),  X  X order trade X  (NN),  X  X ecame famous X  (VA), and  X  X ew regulations X  (AN). This indicates wildcard search for sublexical translations of our model could handle combination-form OOVs of various POS combinations. In addi tion, as suggested by Example 4, accurate OOV translations indeed have an impact on the lexical choices of OOVs X  surrounding words or even on the overall fluency (re-ordering).
 Additionally, Table XII(b) displays three example sentences where translations of OOV words partially match their reference translations. In Example 6, although our OOV model ranked the correct translation  X  X hree cars X  at the top choice, the decoder chose  X  X hree trucks X  for the OOV probably based on local consideration of the target language model. Moreover, BLEU may underestimate the performance of the CST system: for Example 7,  X  X alary payment X , though does not match the reference, may still be an acceptable translation.

Examples 8-10 in Table XII(c) illustrate that there is room for future improvements of our system. In Example 8 and 9, despite the fact that our model found the cor-rect sublexical translations  X  X now fall X  and  X  X now storm X  for the OOV  X   X  X  X  X   X  X nd  X   X  X  X  X   X , respectively, their referenc e translations are in the form of one-word compounds  X  X nowfall X  and  X  X nowstorm X . Therefore , we would like to accommodate such cases by adding such compounds into our candidate lists. Furthermore, in the last example in Table XII(c), we observed the need for sense disambiguation of constituents (i.e., Chi-nese characters). In this case, the character  X   X   X  X nOOV X   X   X   X   X  may be associated with many senses, like  X   X   X   X  (class),  X   X  X  X   X (flight), X   X   X   X   X (shift),and X   X  X   X (schedule),re-sulting in many possible choices of translations. Since it is difficult to disambiguate such constituents without the help of contextual information of the OOV word, we plan to incorporate OOVs X  contexts (other than OOVs themselves) into our module as features for better OOV translation quality. We have introduced a method for generating translation suggestions for OOV words from a MT system X  X  existing bilingual resource (i.e., phrase table) via sublexi-cal/constituent translations. The promisi ng and encouraging result indicates that as a preprocessing step before a state-of-the-art phrase-based decoder, Moses, our OOV model genuinely provides good translations for unknown words and that out-of-vocabulary words are in fact in-vocabulary on the sublexical level at least for languages such as Chinese. Apart from the future improvements we point out in Section 5.2 for our model, we would like to incorporate paraphrasing techniques such as [Marton et al. 2009] and [Mirkinet al. 2009] for better OOV translation quality and coverage.
 Many sentences are submitted to machine translate (MT) systems every day, and an increasing number of translation services are available between various source and target language pairs. For example, both Google Translate 1 and Windows Live Translator 2 can promptly translate a block of te xt or a web page. Among different MT systems, there are three common components: the translation model, the language model, and the reordering model. The trans lation model performs transformations from source phrases (or syntax-based rules) to corresponding target ones by querying the phrase table (or syntax-based translation rules). On the other hand, the language model and the reordering model evaluate the fluency and the word orders of the gen-erated target language translation, respectively. While the translation and reordering model are trained on bilingual corpora, the language model is usually trained on monolingual corpora.

Due to the creativity and diversity of nat ure languages, not all source words are known to MT systems specifically their translation models (i.e., phrase tables or syntax-based translation rules), in which case most of the current systems treat them as out-of-vocabulary (OOV) words and typically leave them untranslated. However, leaving the unknown OOVs untouched in the output translation may degrade the over-all translation quality since the lexical choices and reordering around the OOVs may be negatively impacted, not to mention the influence on a human X  X  understanding of the target translation. The problem of OOV could be better handled if a model recog-nized and translated the constituents of an OOV word.

In general, the causes of unknown words can be mainly categorized into following types. OOVs result from segmentation error in the source language (e.g.,  X  X  X  X  is erroneously split into two words  X  and  X  X  which leads to an  X  X  ) ). Another source of OOVs can be attributed to name entity such as person name, location, and organiza-tion. Finally, OOVs may originate from low-frequency abbreviations (e.g.,  X  X  X  athletic association) and combination forms (e.g., eyebank, widebody) of common words (e.g., bank, body). In this article, we focus on handling OOVs of the last type, abbreviation, and combination, which accounts for one f ourth of OOV cases according to our OOV analysis in Section 4.

Consider the Chinese sentence  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (the muscle strength of wang yan X  X  upper limbs has regained by two levels). If MT systems do not cover  X   X  X   X  (upper limbs) in the translation model, typically it, an OOV, will be sent out untouched to the output. Better result might be obtained by first finding translations for the OOV X  X  constituents such as  X  X pper X  (for the  X  part) or  X  X imbs X  (for the  X  part), and combining these sublexical translations (i.e.,  X  X pper X  and  X  X imbs X ) to yield the ref-erence translation:  X  X pper limbs X . We can leverage wildcard queries  X   X   X  X nd X *  X   X , where * stands for any Chinese character, to find theses sublexical translations. Intu-itively, by extracting and combining the translations of OOVs X  sublexical constituents (i.e., Chinese ideographs), we could correctly translate the unknown words.
We present a method that automatically retrieves translations of OOVs X  con-stituents from existing bilingual resource in a MT system that, if combined, are ex-pected to translate OOVs. An example sublexical translation process for the unknown  X   X  X   X  (upper limbs) is shown in Figure 1. Wildcard search would find possible sublex-ical translations, in this case,  X  appeal, rise, upper, and surface  X  X orthequery X   X  * X  and  X  body, extremity, and limbs  X  for the query  X *  X   X   X , but some are not appropriate for (or in the context of) the unknown  X   X  X   X  (upper limbs). Our model constrains the choices of the sublexical translations and removes unlikely ones by analyzing a collection of monolingual and bilingual lexical databases. We describe the process in more detail in Section 3.

At runtime, for an OOV in a source sentence, our model retrieves a limited number of sublexical translations for its constituents and generates an ordered list of its trans-lation candidates based on bilingual lexical correspondences and monolingual fluency. The ordered candidate list returned by the proposed model can provide translation choices for human translators directly or can be incorporated into MT decoders to ease the negative impact of OOVs on translation quality.

The rest of this article is organized as follows. We review the related work in the next section. Then we present our method for automatically learning to find sublex-ical translations and to rank assembled translation candidates for OOVs expected to broaden the translation coverage of MT systems (Section 3). As part of our evaluation, Section 4 describes the experimental settings based on our OOV analysis while Sec-tion 5 compares the performance of a state-of-the-art MT system with or without our OOV model. Finally, Section 6 concludes this article. Machine Translation (MT) has been an area of active research. Recently, the research of translating out-of-vocabulary (OOV) words in MT systems has received much atten-tion. In our work we address an aspect of OOV translation utilizing wildcard query searches and MT system X  X  existing resources.

More specifically, we focus on translating t he source words without direct transla-tion equivalents in the bilingual resources of MT systems by combining translations of their constituents, or sublexical translations, retrieved using wildcard queries. To reduce the number of OOVs, adding more pa rallel data to MT systems seems feasible but may not be always possible for some language pairs. Therefore, interesting ap-proaches were presented to generate additional parallel sentences from comparable, non-parallel, monolingual corpora by bootstrapping and machine learning [Fung and Cheung 2006; Munteanu and Marcu 2005]. In general, the accuracy of aligning these texts remains a challenge for these studies. In contrast, we will show how to trans-late OOVs by retrieving and combining their sublexical translations based on existing parallel data.

More recently, some work has begun to ext ract translations for OOV words from external knowledge sources such as dictionaries and the Web. Unknown words were replaced with their definitions or translations in manually-compiled dictionaries [Eck et al. 2008; Vilar et al. 2007], or with translations mined from large-scale Web data [Cao and Li 2002; Nagata et al. 2001]. Although the Web-based approach avoids the low translation coverage issue facing the di ctionary approach, its translation accuracy highly depends on the type of OOVs (accuracy for name entities is very high while common nouns tend to be low). In our method, our translation candidates are within MT systems X  training corpus not from external knowledge.

Recent work has been done on translating different OOV cases: name entities (NE), compounds, and morphological variants. Knight and Graehl [1997] introduced a statistical machine transliteration model to tackle proper names while Hassan and Sorensen [2005] presented a NE translating approach that combines NE translation and transliteration in a single framework. On the other hand, Cao and Li [2002] and Tanaka and Baldwin [2003] focused on translating compound words, especially noun phrases, via statistical approach and translation templates. Furthermore, in some languages (e.g., Arabic) where morphological variants are a major cause of OOVs, much work has been described to transform these variants into in-vocabulary word forms [Arora et al. 2008; Koehn and Knight 2003; Langlais and Patry 2007; Yang and Kirchhoff 2006]. In contrast, we focus on translating OOVs resulting from abbrevia-tions of source phrases (e.g.,  X   X  athletic association) or combination forms of com-mon words (e.g.,  X  X  blood bank). These two types cover some portion of name entities and noun-noun (NN) and adjective-noun (AN) compounds (e.g.,  X   X  border trade (NN) and  X  X  X  new regulations (AN)).

In the studies more closely related to our work, Marton et al. [2009] and Mirkinet al. [2009] presented paraphrase models that replace OOV words with in-vocabulary equivalents for translation. Paraphrases in Marton et al. [2009] are learnt based on word alignments computed over a large additional set of bitexts, while Mirkinet al. [2009] paraphrased OOVs using entailment rules that are derived from monolingual corpora as well as manually compiled synonym thesaurus. These studies are similar in spirit to our work. However, we do not directly address the problem via paraphrasing. In our model, the wildcard translation searches of OOVs constituents might retrieve their source-language paraphrases with translations. Instead of using the source para-phrases, we directly use the target translations.

Recently, Li and Yarowsky [2008] proposed an unsupervised method for finding the mappings between full-form phrases and their abbreviations that are OOV words. The main difference from our current work is that in their approach they need a reverse MT system and they focus on solving OOVs of proper names or name entities. For example, they first identify a target-language name entity  X  X ong Kong Governor X , translate it via the reverse MT system to source-language full-form  X   X  X  X   X  X  X   X , and finally incorporate its abbreviations, say  X   X  X  X   X , and  X  X ong Kong Governor X  into MT system. Our approach generates translation candidates for OOVs of abbreviations and combination forms, which co ver both common words (e.g.,  X  X  became famous,  X  X  X  border trade, and  X  X  X  new regulations) and proper names. In addition, our wildcard searches for translations of OOVs X  constituents from existing resources of MT systems can be viewed as finding the translations of their full-form words.

In contrast to the previous research, we present a model that automatically trans-forms the exact-match search for translations of an OOV word into a sequence of wild-card queries for sublexical translations, with the goal of maximizing the probability of finding suitable translations for the OOV word. We provide good translation sug-gestions for OOV words by combining their sublexical translations and filtering out improbable ones with monolingual and bilingual consideration. Submitting natural language sentences with OOV words (e.g.,  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X ) to machine translation systems does not work very well. MT systems typically generate the corresponding target-language translations by matching exact words or phrases in their translation models (i.e., phrase tables or syntax-based translation rules). Unfortunately, OOVs have no matches and MT systems would ignore them and leave them untouched. Leaving unknown source words untranslated or directly copying them to the translation output often leads to incorrect lexical choices and word orders s urrounding them, subsequently degrading the translation performance and humans X  understanding of the target translation. To translate an OOV, a promising approach is to automatically transform the exact-match lookup into a set of wildcard searches that are expected to find the common sublexcial translations of the OOV (i.e., translations of the OOV X  X  constituents). We focus on finding translations of an OOV word from existing bilingual resources (e.g., phrase table) via constituent or sublexical lookups. These translation candidates are ranked and returned as the output of the model. The returned candidates can be examined by human translators directly, or passed on to MT decoders (e.g., Moses) to ease the impact of OOVs. Sublexical wildcard searches tend to lead to ambiguity. Thus, it is crucial that sublexical translation choices be constrained to confident trans-lations. At the same time, the set of the OOV X  X  translation candidates cannot be so large that it overwhelms the users or the subsequent (typically computationally ex-pensive) decoders. Therefore , our goal is to return a reasonable-sized set of translation candidates that, at the same time, contain suitable translations of the OOV word. We now formally state the problem that we are addressing.

Problem Statement. We are given a database of translation equivalents TE (e.g., MT phrase tables) trained on a parallel corpus C (e.g., Hong Kong Parallel Text), a large-scale target-language corpus CT (e.g., English Gigaword), and an out-of-vocabulary word O . Our goal is to generate a ranked list of translation candidates that are likely to provide suitable translations for O . For this, we identify the constituents o 1 , ..., o m of O , retrieve, and evaluate the translations of o i from TE by partial matching o i .The retrieved translations of o i , if combined, are likely to translate O .

In the rest of this section, we describe our solution to this problem. First, we define a strategy for finding sublexical translations, translations of a constituent of the given OOV (Section 3.2). This strategy relies on a set of reformulated wildcard searches in replace of the original exact-match inferred from the OOV analysis on the development data (which we will describe in detail in Section 4.2). In this section, we also describe our method for automatically pruning unlikely sublexical translations. Finally, we show how our model assembles the sublexical translations of the OOV and ranks the assembled translation candidates at runtime according to bilingual and monolingual consideration (Section 3.3). For a given OOV, we attempt to find the relevant translations of its constituents which, if combined, are expected to convey the meaning of the OOV. Our constituent translat-ing process is shown in Figure 2. 3.2.1 Retrieving Possible Sublexical Translations. In the first stage of the sublexical trans-lating process (Step (1) in Figure 2), we retrieve possible translations for a constituent of an OOV from a bank of translation equivalents TE . Specifically, we transform the traditional exact-match search for an OOV X  X  translations into a sequence of wildcard sublexical lookups for constituents X  translations. By doing so, the OOV may be decom-posed and translated. For example, while the search for the translations of the OOV  X   X  X   X  (upper limbs) in the existing bilingual resources (e.g., MT phrase tables) is un-successful, the reformulated wildca rd sublexical searches, that is,  X   X  * X  and  X *  X   X , are not. Better yet, the extracted constituent translations,  X  X pper X  from  X   X  * X  and  X  X imbs X  from  X *  X   X  can be combined to translate the OOV word. Figure 3 shows the algorithm for retrieving translations for a constituent of an OOV.

In Step (1) of the algorithm we initiate TransCol to collect possible translations of a constituent c of an OOV O . Based on the constituent c of O and its position in O , we formulate wildcard queries for c  X  X  translations [Step (2)]. We will describe how to formulate search queries according to position in detail in Section 4.3. In Step (3), based on each query search, we retrieve possible sub lexical translations from the translation equivalents TE and append them to TransCol .

Take the constituent  X   X   X  X nd X   X   X  X ftheOOV X   X  X   X  for example. The wildcard query  X   X   X  X nd X *  X   X  generated by the algorithm in Figure 3 yields the set of bilin-3.2.2 Extracting and Restraining Sublexical Translations. In the second stage of the trans-lating procedure (Step (2) in Figure 2), we extract the sublexical translations based on redundancy, and constrain the translation words to frequently-seen ones with a view to composing proper translations for the OOV.

The input to this stage is the possible translations of a constituent obtained from the previous stage, represented by &lt; source word , target phrase &gt; pairs. The output of this stage is a set of &lt; source word , target N-gram &gt; pairs, in which target N-gram may cover different surface forms (e.g.,  X  X imb X  and  X  X imbs X ) of a lemma (e.g.,  X  X imb X ).
The method for extracting and selecting frequent sublexical translations, that is, { &lt; source word , target N -gram &gt; } , involves generating target-language N-grams in tar-get phrases , constraining the choices of target words by consulting a target-language lexical database, and filtering out infrequent words. Each step is discussed in more detailed below.

For each &lt; source word , target phrase &gt; pair, we first generate target N -grams from the target phrase .Take &lt;  X   X   X   X ,  X  X our limbs X  &gt; for instance. Target N -grams include  X  X our X ,  X  X imbs X , and  X  X our limbs X . Second, we constrain content words (e.g., nouns, adjectives, and verbs) in the target N -grams to those seen in a large lexical database (e.g., WordNet). Obviously, if a word is unseen in a lexicon, it is probably not a good translation. Third, we prune infrequent N -grams. To compare fairly, the occurrence count is accumulated over different inflect ed word forms. For example, the occurrence counts for unigrams  X  X ppeals(29) X ,  X  X ppealed(2) X ,  X  X ppeal(93) X , and  X  X ppealing(3) X  sharing the same lemma  X  X ppeal X  are folded to give the lemma a count of 127, where the integer in parentheses denotes the frequency. For the purpose, we use a lemma-tizer [Bird et al. 2008] in our implementation. The method described above would yield frequent and common sublexcial translations in the form of &lt; source word , target N-gram &gt; . See Table I for an example. Notice that the target N -grams would cover many inflectional forms (e.g.,  X  X imb X  and  X  X imbs X ) for the translations of the constituent (e.g.,  X   X   X ), which is generally beneficial to the subsequent sentence-level translation task. 3.2.3 Pruning Less Probable Sublexical N-gram Translations. In the third and final stage of the procedure [Step (3) in Figure 2], we prune less probable sublexical translations of OOVs based on bilingual associations. In the previous step, to extract the translations for the constituent c , each translation pair &lt; source word , target phrase &gt; is trans-formed into inflectional N -gram pairs &lt; source word , target N-gram &gt; .Some N -grams, however, are less related to c . Take the word-phrase pair &lt;  X   X   X   X ,  X  X our limbs X  &gt; re-trieved for the query  X *  X   X  (limb) for example. Since the unigrams,  X  X our X  and  X  X imbs X , are common and frequent, &lt;  X   X   X   X  X  X our X  &gt; and &lt;  X   X   X   X   X  X imbs X  &gt; will both be consid-ered as valid N -gram translations. While  X  X imbs X  is an appropriate translation for the constituent  X   X   X ,  X  X our X , whose Chinese translati ons are typically associated with  X   X   X , is probably not. To reduce computational effort and achieve better translation accu-racy, we filter out less probable sublexical N -gram translations before combining an OOV X  X  sublexical translations at runtime.

The input to this stage is a set of &lt; source word , target N-gram &gt; pairs from the previous stage where the source word contains a querying constituent c . The output of this stage is a subset of these N -gram translation pairs expected to have strong constituent-translation associations.

First, we exploit a bilingual dictionary (e.g., bilingual WordNet) to build reference bilingual associations. In the following, we describe two approaches for construct-ing/registering reference associations. Sample associations yielded by the two ap-proaches are shown in Table II.  X  All-constituent approach. For each entry &lt; source phrase, target phrase &gt; in the dic-tionary, we build bilingual associations between all constituents in the source phrase and all N -grams in the target phrase . That is, once a source-language constituent co-occurs with a target-language N -gram, an association between them is registered.
Take the dictionary entry &lt;  X   X  X   X ,  X  X ind limb X  &gt; for instance. We first identify two constituents  X   X   X  X nd X   X   X  X n X   X  X   X  and build associations between the constituents and N -grams in  X  X ind limb X , that is,  X  X ind X ,  X  X imb X , and  X  X ind limb X . Finally, we register six constituent-translation associations for this entry (See Table II).  X  Salient-constituent approach. In this approach, associations are only registered be-tween the salient constituent of the source phrase and N -grams of the target phrase for each dictionary entry &lt; source phrase , target phrase &gt; . We define that a con-stituent of a source phrase is a salient constituent if it is most associable to the target phrase . Formally speaking, the salient constituent c  X  for the &lt; source phrase , target phrase &gt; is chosen satisfying where c stands for the constituent in the source phrase and Count (  X  ) for the occur-rence count in the dictionary. Note that the set of the bilingual associations gen-erated by this approach is a subset of those generated by all-constituent. Take the entry &lt;  X   X  X   X ,  X  X ind limb X  &gt; for example. After consulting the dictionary which shows Count ( X   X   X , X  X ind limb X ) and Count ( X   X   X ,  X  X ind limb X ) are 1 and the occurrence of  X   X   X ,  X   X   X , and  X  X ind limb X  are 1073, 201, and 1, respectively, we choose the constituent  X   X   X  with higher dice value with the target phrase  X  X ind limb X  as the salient constituent and register associations ( X   X   X   X ,  X  X ind X ), ( X   X   X ,  X  X imb X ), and ( X   X   X ,  X  X ind limb X ) (see Table II).
Once the reference bilingual associations are constructed, we are ready to filter out less likely sublexical translations. Specifically, we only retain the translation pair &lt; source word , target N-gram &gt; of a query constituent c if and only if ( c , target N-gram ) is seen in the reference associations. Take the sublexical N -gram translation pair &lt;  X   X  X   X ,  X  X our X  &gt; from search query  X   X   X  for instance. It will be pruned according to the associations in Table II (Table II does not contain reference association ( X   X   X ,  X  X our X )).
Notice that, different approach, that is, all-constituent and salient-constituent ap-proach, can be leveraged for different application since all-constituent aims at high recall and salient-constituent high precision. Take the dictionary entry &lt;  X   X  X  X   X ,  X  X ring X  &gt; and &lt;  X   X  X   X ,  X  X oap X  &gt; , for example. While the approach of salient constituent under-generates a bilingual association for the entry &lt;  X   X  X  X   X ,  X  X ring X  &gt; (both  X   X   X  X nd  X   X   X  are highly associable to  X  X ring X ), all-constituent approach over-generates an in-direct association between  X   X   X  (fragrant) and  X  X oap X  for &lt;  X   X  X   X ,  X  X oap X  &gt; .Inourim-plementation, we first refer to all-constituent associations to prune and maintain high recall rate. If there are still too many translation candidates, we then resort to the salient-constituent associations to prune more aggressively. After pruning, the output of this stage is a set of translation pairs expected to have strong constituent-translation associations.

Note that the reference bilingual associations in this stage could be modeled as soft constraints and the frequencies of the regis tered associations could be used as feature scores for runtime candidate ranking. Once the confident sublexical translations of an OOV are found, our model then gen-erates and ranks translation candidates for the OOV word using the procedure in Figure 4.
 For each constituent c of the given OOV O , we first retrieve its probable translations SubTrans , including inflected forms, from existing translation equivalents TE using the method described in Section 3.2 [Step (1a)]. SubTrans is a list of &lt; source word , target N-gram &gt; pairs where source word contains a constituent of O . Then, we use bidirectional conditional probability to measure the association strength between the constituent c and its translation in SubTrans , and record such info rmation at corre-sponding position [Step (1b)]. Following the format in Step (1a), elements in CandList N-gram )). Conditional (sublexical) probabilities P sub ( target N-gram | c )and P sub ( c | target N-gram ) are trained on large parallel corpus in which the unit of token in the source language is constituent not word.
 Take an OOV word  X   X  X   X  (upper limbs) for instance. We first obtain Sub-per) and  X   X   X  (limbs), respectively. Then, we calculate the two-way association strengths between the constituents and their extracted ( N -gram) translations and summarize these in CandList (See Table III).

After acquiring the translations of each constituent of the given OOV, we combine these constituent translations to generate the OOV X  X  translation candidates. Although the translation scope of an OOV is much smaller than that of a whole sentence, reorder-ing of the OOV X  X  sublexical translations can still happen. For example, the reference translation of  X   X  X  X   X  is  X  X ir adjustment X  where the translation  X  X djustment X  and  X  X ir X  of the constituent  X   X   X  X nd X   X   X , respectively, are inverted. For this, both straight and inverted translation candidates are generated during the combination of the sublexical translations.

In Step (2), we initialize Straight and Inverted to collect the OOV X  X  translation can-didates by composing its sublexical translations in straight and inverted order. During candidate generation, Straight and Inverted iteratively cover more span of the OOV [Step (3)] collecting constituent translations and multiplying sublexical translation probabilities at the same time. For each assembled translation candidate TransCand , its translation probability P trans is estimated by the product of two-way conditional probabilities of the sublexical translation pairs as: where c i denotes a constituent of O and target N-gram i , j the sublexical translation for c i composing TransCand . Notice that in this step we concatenate source word  X  X  generating parts of TransCand as source phrase , represented by ( O , &lt; source phrase , TransCand &gt; , P trans ( TransCand )). Therefore, in the end we obtain not only the esti-mated translation probability of a translation candidate of the OOV O but the source phrase that generates the (composed) translation candidate (note that the source phrase may be a paraphrase, possi bly a full-form paraphrase, of O ). Consider one sub-lexical translation tuple in CandList [1], ( X   X   X , &lt;  X   X   X   X ,  X  X pper X  &gt; ,0.02  X  0.56), and one in CandList [2], ( X   X   X   X , &lt;  X   X  X   X ,  X  X imbs X  &gt; ,0.05  X  0.01), of the OOV  X   X   X   X  (upper limb). Step (3) would generate a straight translation candidate ( X   X  X   X , &lt;  X   X   X   X  X   X   X ,  X  X p- X  X imbs upper X , (0.02  X  0.56) 1 / 2 (0.05  X  0.01) 1 / 2 ).

Apart from bilingual information, we further leverage monolingual information to prune and estimate assembled translation candidates. In Step (4), we first prune less probable word combinations in the target language, and, for those which survive the pruning, incorporate target-language language model probabilities P TML ( TransCand ) into Straight / Inverted . For pruning, we calculate MI value, long been regarded as a good estimate of the likelihood of a word sequence, of each bigram w 1 and w 1 in an assembled translation candidate using After merging straight and inverted cases, we rank translation candidates based on bilingual translation probabilities and target-language language model [Step (5)]: where  X  i is the feature weight and  X  i equals to 1. Target-language language model plays an important role in ranking the translation candidates especially when straight and inverted translation candidates are all taken into consideration, fluency of the composed translation candidates.

Finally, the N top-ranked candidates whose probabilities ( P ) exceed a threshold  X  are returned as the likely translations of the given OOV. Notice that  X  will be tuned on development data for better translation quality. An example translation process for the OOV  X   X  X   X  (upper limbs) is shown in Figure 1. The OOV handling model was designed to find the translations of OOV words from ex-isting bilingual resources that are likely to help human translators or MT systems ease the negative impact of OOVs. As such, the OOV model will be trained and evaluated over translation task on top of an existing MT system. More specifically, we incor-porated the OOV model into an existing Chinese-to-English MT system and carried out the evaluation process. Furthermore, since the goal of the model was to generate a set of good translation candidates for an OOV, we evaluated the model on the sen-tences with OOVs to which it provides translation suggestions. In this section, we first introduce our underlying MT system, Moses, and describe its capability of plugging in OOVs X  translations as well as the data sets in our experiments, including training and development data (Section 4.1). Then, Section 4.2 describes how we formulate the wildcard query in substitute of the original exact-match search for translations of OOV words based on the observation from the development data. Finally, we report the parameter settings of our model in Section 4.3. The proposed OOV model focused on translating OOV words by exploiting existing bilingual resources (e.g., phrase table) in a MT system. Therefore, our model was built on top of a statistical MT system which accepts translation suggestions of our OOV model. In our experiments, we used the state-of-the-art phrase-based SMT system, Moses [Koehn et al. 2007], as our underlying decoder. It provides easy XML markup for plugging in external translation knowledge/suggestions without changing any com-ponent within such as translation or language model. In this article, we leveraged the markup language to incorporate OOVs X  translation candidates into Moses. Table IV shows an example sentence and its XML version with OOV translations.

To train Moses X  translation model, we used Hong Kong Parallel Text (LDC2004T08) and Xinhua News Agency (LDC2007T09). Chinese sentences within were word seg-mented by the CKIP Chinese word segmentation system [Ma and Chen 2003]. Com-mon settings were used to train Moses: GIZA++ [Och and Ney 2003] was used for word alignment, grow-diagonal-final [Koehn et al. 2005] for bi-direction word align-ment combination, and phrase extraction heuristics in Koehn et al. [2005] for bilingual phrase pair acquisition. Moreover, we exploited Xinhua news from English Gigaword Third Edition (LDC2007T07) and SRILM toolkit [Stolcke 2002] to build trigram lan-guage model used in decoding.
 In our OOV model, on the other hand, we leveraged WordNet 3.0 [Miller 1995] and Sinica BOW [Huang et al. 2004], a manually translated Chinese version of WordNet, to filter sublexical translations (see Section 3.2). To prune less probable translation can-didates of OOVs at runtime (see Section 3.3), we utilized Web 1T 5-gram First Edition (LDC2006T13) for MI calculation. As for the runtime candidate ranking (see Section 3.3), we exploited the same parallel corpora and target-language corpus used for train-ing Moses to estimate translation probabilities between source-language constituents and target-language words ( P trans ) and target-language fluency ( P TLM ), respectively. To study the problem of translating OOV words, we used NIST MT-08 test set consist-ing of 1,273 unknown words in 637 sentences out of a total of 1,357 sentences. Among these 1,273 distinct OOV words, we first inspected the number of OOVs with respect to their lengths, that is, the number of characters (See Table V). One could see from Table V that OOVs of two characters accounted for more than half of the OOV cases 3 . As a result, we focused on translating two-character unknown words. To further an-alyze the distribution of OOV types, formulate appropriate wildcard query replacing the exact-match search, and determine good bilingual resources for sublexical trans-lation retrieval, we randomly selected 100 sentences containing only two-character OOVs from MT-08 set. OOVs in these 100 sentences were classified into 10 types (shown in Table VI) according t o their reference translations manually extracted from reference sentences. Figure 5 shows an example source sentence containing an OOV  X   X  X   X  (upper limbs) and its four reference translations with the OOV X  X  translations underscored. Since our model was designed to retrieve and combine translations of an OOV X  X  constituents, it targets specifically at translating OOV words of the Combina-tion Form which accounts for one fourth of the OOVs (taking up the same proportion as Paraphrase , an active area in the MT OOV research). See Table VI for more details.
Intuitively, there are four ways to formulate the query for sublexical translations of a two-character OOV c 1 , c 2 of the type Combination Form . Table VII shows that the first and the second 4 query formulation of adding wildcard * could retrieve most relevant translations for this OOV catego ry. Therefore, our model adopted these two query formats for finding sublexical translations.
Additionally, to determine the applicability of various bilingual resources for find-ing sublexical translations, we compared the translation hit rates of the Combination Form OOVs using different resources based on the aforementioned two query formats. Among resources, Lin Yutang X  X  Chinese-English Dictionary 5 , LDC translation lexicon (LDC2002L27), character-based phrase table and word-based phrase table, hit rates were 0.64 6 , 0.68, 0.60, and 0.88, respectively. Word-based phrase table resulted in the highest translation coverage, thus chosen as our bilingual resource for sublexical translation retrieval. Apart from its high hit rate, there are other advantages in us-ing word-based phrase table: it includes different inflectional word forms and is more domain-relevant to the NIST MT test set. In this subsection, we describe the pilot experiment with the development data set of 50 sentences randomly selected from NIST MT-08 test set. Half of the sentences contained OOVs of Combination Form, and there was at least one OOV per sentence. We used this data set to fine-tune the two parameters in our system: the number of translation candidates returned by the OOV model N and the filtering threshold  X  used to prune less probable translation candidates at runtime, thus differentiating OOVs of Combination Form from those that are not.

Since the OOV model was designed to provide OOVs X  translation suggestions to hu-man translators or MT systems, the size of the returned translation candidates could not be so large that it overwhelms the user or decreases the efficiency of the subse-quent translation task. Therefore, our goal was to return reasonable-sized translation candidates for an OOV word with its correct translations ranked higher. To choose a suitable N , we leveraged the sentences with combination-form OOVs in the devel-opment set and evaluated the performance of translating these OOVs using Mean Reciprocal Rank ( MRR ). Here, MRR is defined as a measure of how much effort is needed for a user to locate the first correct translation for the given OOV in the ranked list of translation candidates. The MRR value lies between 0 and 1, where 1 im-plies that the first-ranked candidates are always the correct translations of the OOVs. Table VIII summarizes the hit rate X  X  and MRR  X  X  at different values of N  X  X  for the 25 combination-form OOV words in the development set. We eventually set N to 10 con-sidering translation coverage, MRR , and the time complexity at decoding.

Moreover, a threshold  X  on the probability of the translation candidate was used for pruning and determining the applicability of our model on OOV words (Some OOVs are not suitable for the solution in this paper). To select a suitable  X  ,wecompared translation results of the development data at different levels of filtering thresholds (Figure 6). As indicated in Figure 6, when the threshold was larger than -7, very few translation candidates were considered, leading to not much performance difference from the underlying Moses. On the other hand, when the threshold was lower than -13, more noisy translations were incorporated, leading to a decline in translation quality. We chose -12, the best performing threshold (probably achieved balanced performance between translation accuracy and coverage), as our threshold to prune less probable translation candidates or to activate our OOV model.

In summary, we set the runtime filtering threshold  X  to -12, and the maximal num-ber of the returned translation candidates N to 10 in our experiments which will be described in the following section. In this section, we report the results of the experimental evaluation using the method-ology described in the previous section. Firs t, in Section 5.1 we report the translation performance of the underlying MT system, Moses, with and without our OOV model based on BLEU score [Papineni et al. 2002]. In Section 5.2 we discuss the differ-ences between the translations generated by the two systems and point out the future improvement of our OOV model. During the evaluation, we used NIST MT-06 test set, containing 1,664 sentences, for testing. In this test set, there were 933 distinct unknown words scattered in 859 sentences, and its number of OOVs with respect to OOVs X  lengths was much alike to that in our developing set (see Table IX): two-character OOVs also accounted for half of the OOV cases. In the experiment, our OOV model generated translation candidates for 170 distinct two-character OOV words in 351 sentences. Recall that the parameter  X  in the OOV model determines its applicability, that is, whether the combination way of translation is suitable (or acceptable) for the OOV in question. And the produced translation candidates were incorporated into the underlying Moses decoder using XML markup.

Table X shows the overall performance of the underlying Moses and the CST system (i.e., Moses with combined sublexical translations). Although the difference in BLEU score between them is not very significant, the improvement in brevity penalty (BP) is noticeable. That is, the CST system generated sentences closer to the reference translations in length. A slightly better BLEU score implies the additional words (i.e., translations) provided by our OOV model achieved better or at least similar translation accuracy compared to the underlying Moses system.

On the other hand, if we look at the performance of the 351 sentences in the test set for which our OOV model provided translation candidates, the CST system signifi-cantly 7 outperformed Moses in BLEU and encouragingly improved the brevity penalty relatively by 4.4% (see Table XI). The lower BLEU scores in Table XI (vs. BLEU scores in Table X) indicate that these 351 sentences were more difficult to translate, probably due to the existence of OOV words.
 The experimental results show that we were able to translate some portion of the OOV words without degrading the performance of an existing machine translation system and the translation quality of the sentences was substantially improved with our automatic translation suggestions for OOV words. In this subsection, we examine example English translations for OOV words provided by our model. We also point out future direction for our model.

Table XII(a) shows four examples in which the reference translations of OOVs were ranked high by our OOV model (bold-faced) and the underlying decoder chose the cor-rect translations for the OOVs. One may find that the combined words in the OOVs X  translations belong to different part-of-speech (POS) sequences:  X  X orean war X  (AN),  X  X order trade X  (NN),  X  X ecame famous X  (VA), and  X  X ew regulations X  (AN). This indicates wildcard search for sublexical translations of our model could handle combination-form OOVs of various POS combinations. In addi tion, as suggested by Example 4, accurate OOV translations indeed have an impact on the lexical choices of OOVs X  surrounding words or even on the overall fluency (re-ordering).
 Additionally, Table XII(b) displays three example sentences where translations of OOV words partially match their reference translations. In Example 6, although our OOV model ranked the correct translation  X  X hree cars X  at the top choice, the decoder chose  X  X hree trucks X  for the OOV probably based on local consideration of the target language model. Moreover, BLEU may underestimate the performance of the CST system: for Example 7,  X  X alary payment X , though does not match the reference, may still be an acceptable translation.

Examples 8-10 in Table XII(c) illustrate that there is room for future improvements of our system. In Example 8 and 9, despite the fact that our model found the cor-rect sublexical translations  X  X now fall X  and  X  X now storm X  for the OOV  X   X  X  X  X   X  X nd  X   X  X  X  X   X , respectively, their referenc e translations are in the form of one-word compounds  X  X nowfall X  and  X  X nowstorm X . Therefore , we would like to accommodate such cases by adding such compounds into our candidate lists. Furthermore, in the last example in Table XII(c), we observed the need for sense disambiguation of constituents (i.e., Chi-nese characters). In this case, the character  X   X   X  X nOOV X   X   X   X   X  may be associated with many senses, like  X   X   X   X  (class),  X   X  X  X   X (flight), X   X   X   X   X (shift),and X   X  X   X (schedule),re-sulting in many possible choices of translations. Since it is difficult to disambiguate such constituents without the help of contextual information of the OOV word, we plan to incorporate OOVs X  contexts (other than OOVs themselves) into our module as features for better OOV translation quality. We have introduced a method for generating translation suggestions for OOV words from a MT system X  X  existing bilingual resource (i.e., phrase table) via sublexi-cal/constituent translations. The promisi ng and encouraging result indicates that as a preprocessing step before a state-of-the-art phrase-based decoder, Moses, our OOV model genuinely provides good translations for unknown words and that out-of-vocabulary words are in fact in-vocabulary on the sublexical level at least for languages such as Chinese. Apart from the future improvements we point out in Section 5.2 for our model, we would like to incorporate paraphrasing techniques such as [Marton et al. 2009] and [Mirkinet al. 2009] for better OOV translation quality and coverage.
