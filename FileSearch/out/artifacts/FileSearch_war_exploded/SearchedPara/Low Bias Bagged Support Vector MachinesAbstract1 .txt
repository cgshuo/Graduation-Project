 Giorgio Valentini valentini@dsi.unimi.it INFM, Istituto Nazionale per la Fisica della Materia, Italy. Thomas G. Dietterich tgd@cs.orst.edu Bias X  X ariance theory provides a way to analyze the behavior of learning algorithms and to explain the properties of ensembles of classifiers (Friedman, 1997; Domingos, 2000; James, 2003). Some ensemble meth-ods increase expressive power of learning algorithms, thereby reducing bias (Freund &amp; Schapire, 1996). Other ensemble methods, such as methods based on random selection of input examples and input fea-tures (Breiman, 2001; Buhlmann &amp; Yu, 2002) reduce mainly the variance component of the error. The de-composition of the error into bias and variance can guide the design of ensemble methods by relating mea-surable properties of algorithms to the expected per-formance of ensembles (Valentini &amp; Dietterich, 2002). In particular, bias X  X ariance theory can tell us how to tune the individual classifiers in an ensemble so as to optimize the overall performance of the ensemble. In this paper, we apply bias X  X ariance analysis to di-rect the tuning of SVMs to optimize the performance of bagged ensembles. Specifically, since bagging is primarily a variance-reduction method, and since the overall error is (to a first approximation) the sum of bias and variance, this suggests that SVMs should be tuned to minimize bias before being combined by bag-ging. We propose a variant of bagging, that we call Lobag ( Lo w bias bag ging), that estimates the bias of the SVM classifiers, selects low-bias classifiers, and then combines them by bootstrap aggregating. Previous work with other classifiers is consistent with this approach. For example, several studies have reported that bagged ensembles of decision trees often give better results when the trees are not pruned (Bauer &amp; Kohavi, 1999; Dietterich, 2000). Un-pruned trees have low bias and high variance. Simi-larly, studies with neural networks have found that they should be trained with lower weight decay and/or larger numbers of epochs before bagging to maximize accuracy of the bagged ensemble (Andersen et al., 2001).
 Unlike most learning algorithms, support vector ma-chines have a built-in mechanism for variance reduc-tion: from among all possible linear separators, they seek the maximum margin classifier. Hence, one might expect that bagging would not be very effective with SVMs. Previous work has produced varying results. On several real-world problems, bagged SVM ensem-bles are reported to give improvements over single SVMs (Kim et al., 2002). But for face detection, Buciu et al. (2001) report negative results for bagged SVMs. A few other authors have explored methods for tuning SVMs in ensembles. Collobert et al. (2002) proposed solving very large scale classification problems by us-ing meta-learning techniques combined with bagging. Derbeko et al. (2002) applied an optimization tech-nique from mathematical finance to reduce the vari-ance of SVMs.
 The paper is structured as follows. In the following section we discuss differences and commonalities be-tween Lobag and bagging, then we describe how to compute the bias X  X ariance decomposition of the error. In Sect. 4, we outline the main results of an extended analysis we performed on bias X  X ariance decomposition of the error in random aggregated and bagged ensem-bles of SVMs, showing the effect of base learner ag-gregation on bias and variance. Then we present the Lobag algorithm and some numerical experiments to compare lobag ensembles of SVMs versus single SVMs and bagged ensembles of SVMs. An outline of future developments of this work concludes the paper. Let D be a set of m points drawn identically and inde-pendently from U according to P , where U is a popula-is the joint distribution of the data points in U , with x  X  R d .
 Let L be a learning algorithm, and define f D = L ( D ) to be the predictor produced when L is applied to training set D . f D produces a prediction f D ( x ) = y . 2.1. Random Aggregating Suppose that a sequence of learning sets { D k } is given, each drawn i.i.d. from the same underlying distribu-tion P . According to (Breiman, 1996), we can ag-gregate the f D trained with different samples drawn from U to get a better predictor f A ( x , P ). For re-gression problems t j  X  R and f A ( x , P ) = E D [ f D ( x )], where E D [  X  ] indicates the expected value with respect to the distribution of D . In classification problems, t  X  C  X  N and f A ( x , P ) = arg max j |{ k | f D is the plurality vote of the individual predictors. Be-cause the training sets D are randomly drawn from U , we call the procedure for building f A random aggre-gating .
 If T and X are random variables having distribution P , the expected squared loss EL for the single predic-expected squared loss EL A for the aggregated predic-tor is EL A = E T, X [( T  X  f A ( X )) 2 ]]. Developing the square it is easy to see that EL = E T [ T 2 ] + E X [ E D [ f 2 D ( X )]]  X  2 E T [ T ] E EL A = E T [ T 2 ]+ E X [( E D [ f D ( X )]) 2 ]  X  2 E T [ T ] E and hence EL A  X  EL as The reduction of the error depends on the instability of the prediction (Breiman, 1998), that is, on how un-equal the two sides of eq. 2 are. This in turn is related to the variance V ( X ) of the base predictor: The degree to which eq. 3 is greater than 0 depends on how unequal the two sides of eq. 2 are. For classifi-cation problems with unstable predictors, random ag-gregating can lower the expected error, but with poor predictors, unlike regression, aggregation can worsen the accuracy (Breiman, 1996). 2.2. Bagging: Bootstrap Aggregating Base learners of bagged ensembles f B are trained on repeated bootstrap samples { D b } from D . Hence Bag-ging is an approximation of random aggregating, for at least two reasons. First, bootstrap samples are not in-dependent data samples: they are drawn from a data set D that is in turn a sample from the population U . Second, bootstrap samples are drawn from D accord-ing to the uniform probability distribution, which is only an approximation of the unknown true distribu-tion P . For these reasons, there is no guarantee that bagging provides a good enough approximation to f A to produce variance reduction.
 Random aggregating removes all variance, leaving only bias and noise. Hence, if bagging is a good approxi-mation to random aggregating, it will also remove all of the variance. Therefore, to minimize the overall er-ror, bagging should be applied to base learners with minimum bias. 2.3. Lobag We propose to tune SVMs to minimize the bias and then apply bagging to reduce (if not eliminate) vari-ance, resulting in an ensemble with very low error. The key challenge, then, is to find a reasonable way of tun-ing SVMs to minimize their bias. The bias of SVMs is typically controlled by two parameters. First, re-call that the objective function for (soft margin) SVMs has the form: k w k 2 + C of weights computed by the SVM and the  X  i are the margin slacks, which are non-zero for data points that are not sufficiently separated by the decision bound-ary. The parameter C controls the tradeoff between fitting the data (achieved by driving the  X  i  X  X  to zero) and maximizing the margin (achieved by driving k w k to zero). Setting C large should tend to minimize bias. The second parameter that controls bias arises only in SVMs that employ parameterized kernels such as the polynomial kernel (where the parameter is the de-gree d of the polynomial) and RBF kernels (where the parameter is the width  X  of the gaussian kernel). In previous work we showed that in gaussian and poly-nomial SVMs, bias depends critically on these param-eters (Valentini &amp; Dietterich, 2002).
 The basic idea of Lobag is to perform a systematic search of values of C and either d or  X  and experimen-tally estimate the bias of the resulting SVMs to find the parameter values to minimize the estimated bias. These parameter settings are then employed to create a bagged SVM ensemble. 3.1. Bias and Variance for Classification We employ the definitions of bias and variance for gen-eral loss functions developed by Domingos (2000). Let L ( t, y ) be the loss received when the true label is t and the hypothesis predicts y . Then the optimal pre-diction , y  X  , for point x is defined to be the value of y that minimizes E t [ L ( t, y )] : For the usual 0/1 loss function, y  X  = t . The main prediction y m at point x is defined as the class label that would give the minimum expected loss over all training sets D : It expresses the  X  X entral tendency X  of a learner. The bias B ( x ) of the learning algorithm at point x is the loss of the main prediction relative to the optimal prediction: L ( y  X  , y m ). For 0/1 loss, this will either be zero if the main prediction is correct or one if the main prediction is wrong. Hence, for a given learning algorithm L and any point x , the bias will either be 0 or 1. We will refer to the points x where the bias is zero as the  X  X nbiased points X . We will call the other points the  X  X iased points X .
 The variance V ( x ) is the expected loss of the in-dividual predictions relative to the main prediction: V ( x ) = E D [ L ( f D ( x ) , y m )].
 These definitions of bias and variance do not give an additive decomposition of the error. Instead, for bi-ased points, the expected 0/1 loss can be written as B ( x )  X  V ( x ), whereas for unbiased points, the expected 0/1 loss can be written as B ( x ) + V ( x ). The intuition here is that if the learning algorithm is biased at point x , then increased variation (away from the main pre-diction) will increase the chance of classifying x cor-rectly. But if the learning algorithm is unbiased at x , then increased variation (away from y m ) will increase the probability of misclassifying x .
 We can aggregate the bias and variance over a test data set D of m points as follows. The average bias B is The unbiased variance, V u is the average variance of the unbiased points, where m u denotes the number of unbiased points. The biased variance V b is the average variance of the biased points: where m b = m  X  m u is the number of biased points. Define the net variance V n = V u  X  V b . Then we obtain the overall decomposition that the expected 0/1 error on the test data set as B + V n . 3.2. Measuring Bias and Variance We propose to apply out-of-bag procedures (Breiman, 2001) to estimate the bias and variance of SVMs trained with various parameter settings. The proce-dure works as follows. First, we construct B boot-strap replicates of the available training data set D (e. g., B = 200): D 1 , . . . , D B . Then we apply a learning algorithm L to each replicate D b to obtain an hypoth-esis f b = L ( D b ). For each bootstrap replicate D b , let T b = D\ D b be the ( X  X ut-of-bag X ) data points that do not appear in D b . We apply hypothesis f b to the ex-amples in T b and collect the results.
 Consider a particular training example ( x , t ). On the average, this point will be in 63.2% of the bootstrap replicates D b and hence in about 36.8% of the out-of-bag sets T b . Let K be the number of times that ( x , t ) was out-of-bag; K will be approximately 0 . 368 B . The optimal prediction at x is just t . The main prediction y m is the class that is most frequently predicted among the K predictions for x . Hence, the bias is 0 if y m = t and 1 otherwise. The variance V ( x ) is the fraction of times that f b ( x ) 6 = y m . Once the bias and variance have been computed for each individual point x , they can be aggregated to give B , V u , V b , and V n for the entire data set D . Before presenting our experimental tests of Lobag, we first summarize the main results of an extensive ex-perimental analysis of the bias and variance of ran-dom aggregated and bagged ensembles of SVMs. The experiments involved the training and testing of more than 10 million SVMs. We employed several two-class data sets, both synthetic and  X  X eal X . Most of them are from the UCI repository (Merz &amp; Murphy, 1998). In all cases, we used relatively small data sets (100 examples) bootstrapped from a relatively large data set and reasonably large test sets to perform a reliable evaluation of bias and variance.
 Fig. 1 shows results typical of those found in our exper-iments. The first surprise was that SVMs have high bias when the RBF width  X  is set very small. This is counter-intuitive, since one would expect very low bias in these cases. In order to understand how the parameter settings affect the bias, we analyzed the re-lationships between  X  , generalization error, training error, bias and the ratio of support vectors with re-spect to the total number of input examples (Fig. 2). For very small values of  X  , overfitting problems arise: the training error is very small (about 0), while the number of support vectors is very high, and the er-ror and bias are also high (Fig. 2). Indeed with very small  X  values, the discriminant function learned by the SVMs is essentially flat everywhere except in the immediate neighborhood of each of the training data points (Fig. 3 a). Hence, the learned decision surface is very inflexible, and this gives high bias. By enlarging  X  , we obtain a wider response on the input domain. The discriminant function computed by the SVM be-comes smoother (Fig. 3 b), as the  X  X umps X  around the support vectors become wider. This permits SVM to make better predictions on new examples. At the same time, the number of support vectors decreases, as do the bias and the generalization error (Fig. 2). Al-though  X  is the parameter that most affects the RBF-SVMs, the regularization parameter C also plays an important role: it can counter-balance the increased bias due to large values of  X  (Fig. 2). However, if its value is too small, the bias can increase independently of the other learning parameters.
 Another important observation is that for single SVMs, the minimum of the error and the minimum of the bias are often achieved for different values of the tuning parameters C , d , and  X  . This is what spurred us to consider the Lobag approach, which seeks the latter rather than the former.
 The main purpose of the experiments was to analyze the effect of random aggregation (the theoretical ideal) and bagging. We found that random aggregation of SVMs gives relative error reductions of between 10 and 70% (depending on the data set). This reduction is slightly larger for high values of the C parameter and is due primarily to the reduction of the unbiased vari-ance. Indeed in all data sets, the relative reduction of the unbiased variance amounts to about 90%, while bias remains substantially unchanged (Fig. 1). Note that the error of the ensemble is reduced to the bias of the single SVM, while net and unbiased variance are almost entirely eliminated.
 Fig. 4 shows typical results of our experiments with bagging. In this case we also observed a reduction of the error, but it was not as large as with random aggre-gated ensembles. In particular, unlike random aggre-gating, net and unbiased variance are not reduced to 0: in our experiments, we obtained a smaller reduction of the average error (from 0 to 20%) due to a lower de-crease of the net-variance (about 35% on the average against a reduction of over 90% with random aggre-gated ensembles), while bias remained unchanged or increased slightly. The Lobag algorithm accepts the following inputs: (a) a data set D = { ( x i , y i ) } n i =1 , with x i  X  R and y i  X  { X  1 , 1 } , (b) a learning algorithm L (  X  ,  X  ), with tuning parameters  X  , and (c) a set A of possible set-tings of the  X  parameters to try. Lobag estimates the bias of each parameter setting  X   X  X  , chooses the set-ting  X   X  that minimizes the estimated bias, and applies the standard bagging algorithm to construct a bag of classifiers using L (  X  ,  X   X  ). The remainder of this section provides a high-level pseudo-code for Lobag . 5.1. The Bias X  X ariance decomposition This procedure estimates the bias X  X ariance decompo-sition of the error for a given learning algorithm L and learning parameters  X  .
 The learning algorithm L returns a hypothesis f  X  = L ( D ,  X  ) using a learning set D , and it is applied to mul-tiple bootstrap replicates D b of the learning set D in order to generate a set F  X  = { f b  X  } B b =1 of hypotheses f b The procedure returns the models F  X  and the estimate of their loss and bias. For each set of learning param-eters  X   X  A it calls Evaluate BV , a procedure that provides an out-of-bag estimate of the bias X  X ariance decomposition.
 Procedure [ V, F ] BV decomposition ( L , A , D , B ) Input : Output : bias are the estimated loss and bias of the model trained through the learning algorithm L with algo-rithm parameters  X  . begin procedure end procedure . 5.2. The overall Lobag algorithm Using the procedure BV decomposition , we can imple-ment a version of the Lobag algorithm that exhaus-tively explores a given set of learning parameters in order to build a low bias bagged ensemble.
 Algorithm Lobag exhaustive Input: Output: begin algorithm end algorithm .
 The procedure Select model selects the model with the minimum bias and/or minimum loss and returns the parameter values  X  B and  X  L that correspond re-spectively to the model with minimum bias and mini-mum loss. Then the Lobag and bagged ensembles are chosen through the procedure Select ensemble : the Lobag ensemble has base learners with the minimum estimated bias, while the bagged ensemble has base learners with the minimum estimated loss. In order to speed up the computation, we could design variants of the exhaustive Lobag algorithm. For example, we could apply multidimensional search methods, such as the Powell X  X  method, to select the tuning values that minimize bias. We performed numerical experiments on different data sets to test the Lobag ensemble method using SVMs as base learners. We compared the results with single SVMs and classical bagged SVM ensembles.
 6.1. Experimental setup We employed two synthetic data sets ( P2 and a two-class version of Waveform ) and 5  X  X eal X  data sets ( Grey-Landsat , Letter (reduced to the two-class prob-lem of discriminating between the difficult letters B and R), Letter with 20% noise added, Spam , and Musk ). Most of them are from the UCI reposi-tory (Merz &amp; Murphy, 1998).
 We employed small D training sets and large T test sets in order to obtain a reliable estimate of the gen-eralization error: the number of examples for D was set to 100, while the size of T ranged from a few thou-sand for the  X  X eal X  data sets to tens of thousands for synthetic data sets. Then we applied the Lobag al-gorithm described in Sect. 5, setting the number of samples bootstrapped from D to 100, and computing an out-of-bag estimate of the bias X  X ariance decompo-sition of the error. The selected lobag, bagged, and single SVMs were finally tested on the separated test set T . The experiments were repeated 5 times using 5 different training sets randomly drawn from each data set.
 The C++ classes and applications we used to perform all the experiments are included in the NEURObjects library (Valentini &amp; Masulli, 2002). 6.2. Results Table 1 shows the results of the experiments. We com-pared lobag, bagging, and single SVMs with respect to 20 classification tasks: 7 data sets and 3 kernels (gaus-sian, polynomial, and dot-product) have been tested considering all the possible combinations, except for P2 , for which we did not apply the dot-product kernel (because it was obviously inappropriate). We repeated each classification task 5 times, using randomly-drawn training sets of size 100 for each data set, obtaining in this way 100 (20  X  5) outcomes for each method. For each pair of methods, we applied McNemar X  X  test (Di-etterich, 1998) to determine whether there was a sig-nificant difference in predictive accuracy on the test set.
 On nearly all the data sets, both bagging and Lobag outperform the single SVMs independently of the ker-nel used. The null hypothesis that Lobag has the same error rate as a single SVM is rejected at or below the 0.05 significance level in 58 of the 100 cases. Similarly, the null hypothesis that bagging has the same error rate as a single SVM is rejected at or below the 0.05 level in 39 of the 100 cases. Most importantly, Lobag generally outperforms standard bagging: it is signifi-cantly better than bagging in 32 of the 100 cases, and significantly inferior only twice (Tab. 2).
 If we consider the win-tie-loss results per classifica-tion task (that is a method wins if it achieves signif-icant better results according to McNemar X  X  test in the majority of the corresponding experiments), lobag consistently outperforms single SVMs in 18 of the 20 tasks, and it never loses. Moreover, lobag wins with respect to bagging in 12 cases, and it loses only once (Tab. 2). While both lobag and bagging outperform single SVMs with dot-product kernels, lobag shows sig-nificantly better results than bagging with respect to single SVMs if gaussian or polynomial kernels are used. This is confirmed also by the direct comparison lobag and bagging: we can observe a larger difference in fa-vor of lobag especially with gaussian and polynomial kernels (Tab. 2). This paper has shown that despite the ability of SVMs to manage the bias X  X ariance tradeoff, SVM perfor-mance can generally be improved by bagging, at least for small training sets. Furthermore, the best way to tune the SVM parameters is to adjust them to mini-mize bias and then allow bagging to reduce variance. In future work, we plan to apply Lobag to DNA microarray gene expression data which has very few training examples, but each example has very high di-mension.
 In our experiments, we did not consider noise. As a result, noise is folded into bias, and bias itself is overestimated. Even if the evaluation of noise in real data sets is an open problem (James, 2003), we plan to evaluate the role of the noise in synthetic and real data sets, in order to develop variants of Lobag specific for noisy data.
 This work was partially funded by grant ITR 0085836 from the US National Science Foundation and by INFM, unit`a di Genova.

