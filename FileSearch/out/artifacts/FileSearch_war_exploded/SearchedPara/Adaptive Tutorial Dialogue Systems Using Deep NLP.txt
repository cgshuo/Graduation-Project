 Intelligent tutoring systems help students improve learning compared to reading textbooks, though not quite as much as human tutors (Anderson et al., 1995). The specific properties of human-human di-alogue that help students learn are still being stud-ied, but the proposed features important for learn-ing include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner X  X  level, and engagement/affect. Some tuto-rial dialogue systems use NLP techniques to analyze student responses to  X  X hy X  questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on short-answer questions and canned feedback. The result-ing dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004).
We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue sys-tems to improve the interaction. The systems are built using the Information State Update approach (Larsson and Traum, 2000) for dialogue manage-ment and generic components for deep natural lan-guage understanding and generation. Tutorial feed-back is generated adaptively based on the student model, and the interpretation is used to process explanations and to differentiate between student queries and hedged answers phrased as questions. The systems are intended for testing hypotheses about tutoring. By comparing student learning gains between versions of the same system using different tutoring strategies, as well as between the systems and human tutors, we can test hypotheses about the role of factors such as free natural language input, adaptivity and student affect. The B EE D IFF tutor helps students solve symbolic differentiation problems, a procedural task. Solu-tion graphs generated by a domain reasoner are used to interpret student actions and to generate feed-back. 1 Student input is relatively limited and con-sists mostly of mathematical formulas, but the sys-tem generates adaptive feedback based on the notion of student performance and on the dialogue history.
For example, if an average student asks for a hint on differentiating sin ( x 2 ) , the first level of feedback may be  X  X hink about which rule to apply X , which can then be specialized to  X  X se the chain rule X  and then to giving away the complete answer. For stu-dents with low performance, more specific feed-back can be given from the start. The same strat-egy (based on an initial corpus analysis) is used in producing feedback after incorrect answers, and we intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a single diagnosis and generation techniques are used to produce appropriate discourse cues. For example, when a student repeats the same mistake, the feed-back may be  X  X ou X  X e differentiated the inner layer correctly, but you X  X e still missing the minus sign X . The two clauses are joined by a contrast relationship, and the second indicates that an error was repeated by using the adverbial  X  X till X . The B EETLE tutor is designed to teach students ba-sic electricity and electronics concepts. Unlike the B
EE D IFF tutor, the B EETLE tutor is built around a pre-planned course where the students alternate reading with exercises involving answering  X  X hy X  questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-cises there is no structured sequence of steps that the students should follow, but students need to name a correct set of objects and relationships in their re-sponse. We model the process of building an answer to an exercise as co-constructing a solution, where the student and tutor may contribute parts of the an-swer. For example, consider the question  X  X or each circuit, which components are in a closed path X . The solution can be built up gradually, with the stu-dent naming different components, and the system providing feedback until the list is complete. This generic process of gradually building up a solution is also applied to giving explanations. For example, in answer to the question  X  X hat is required for a light bulb to light X  the student may say  X  X he bulb must be in a closed path X , which is correct but not complete. The system may then say  X  X orrect, but is that every-thing? X  to prompt the student towards mentioning the battery as well. The diagnosis of the student an-swer is represented as a set of correctly given objects or relationships, incorrect parts, and objects and re-lationships that have yet to be mentioned, and the system uses the same dialogue strategy of eliciting the missing parts for all types of questions.
Students often phrase their answers tentatively, for example  X  X s the bulb in a closed path? X . In the context of a tutor question the interpretation process treats yes-no questions from the student as poten-tially hedged answers. The dialogue manager at-tempts to match the objects and relationships in the student input with those in the question. If a close match can be found, then the student utterance is interpreted as giving an answer rather than a true query. In contrast, if the student said  X  X s the bulb connected to the battery? X , this would be interpreted as a proper query and the system would attempt to answer it.
 Conclusion We demonstrate two tutorial dialogue systems in different domains built by adapting di-alogue techniques from task-oriented dialogue sys-tems. Improved interpretation and generation help support adaptivity and a wider range of inputs than possible in scripted dialogue.

