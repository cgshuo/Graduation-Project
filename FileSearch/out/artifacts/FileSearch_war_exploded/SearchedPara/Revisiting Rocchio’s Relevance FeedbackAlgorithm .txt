 In information retrieval (I R), relevance feedback (RF) can improve query rep-resentation by taking feedback informat ion into account. A classical relevance feedback algorithm was proposed by Rocchio in 1971 [14] for the SMART re-trieval system [14]. It takes a set of documents as the feedback set. Unique terms in this set are ranked in descending order of TF-IDF weights. A number of top-ranked candidate terms are then added to the original query, and finally, documents are returned for the expanded query. Better performance can always be expected in many IR task.

Many other relevance feedback techni ques and algorithms have been devel-oped, mostly derived from Rocchio X  X  algorithm [1,3,12,13]. Feedback documents can be obtained by many possible means. I n general, there are explicit evidence, such as the labeled relevant documents from real users, or implicit evidence, such as the click-through data. Obtaining the feedback information involves ex-tra efforts, e.g. real user relevance judgment, and is usually expensive. For every given query, the corresponding feedback information is not necessarily available. An alternate solution is query expansion (QE), also called pseudo-relevance feed-back (PRF), which uses the top-ranked documents in the initial retrieval for the feedback [5]. Its basic idea is to extract expansion terms from the top-ranked documents to formulate a new qu ery for a second round retrieval.

Despite the marked improvement broug ht by Rocchio X  X  relevance feedback algorithm (RocRF), the overall retrieval performance of RocRF over the vec-tor space model (VSM) [15] is still not satisfying enough as shown in [16]. In particular, with the recent development of feedback methods [7,16], within the language modeling framework [11], such a combination of RocRF with VSM can no longer catch up with the state-of-the-art methods. In [8], Lv et al. system-atically compare five state-of-the-art approaches for estimating query language models in ad-hoc retrieval, in which an instantiation of the relevance model [7], called RM3 , not only yields effective retrieval performance in both precision and recall metric, but also performs robustly with different feedback document set sizes.

The aim of this paper is to investigate ways to improve PRF for probabilistic models. Our main contributions are as follows. First, we propose a new PRF paradigm, called RocDFR , by revisiting and adopting the classical Rocchio X  X  rel-evance feedback framework within a di vergence from randomness (DFR) prob-abilistic framework [1]. Second, we further enhance the robustness of RocDFR by introducing a quality-biased feedback method, called QRocDFR . Finally, ex-tensive experiments also show that the retrieval performance of our proposed RocDFR and QRocDFR significantly outper form the state-of-the-art relevance model within the language modeling framework.

The remainder of this paper is organized as follows. Section 2 surveys previous work on PRF. Section 3 introduces our proposed PRF method adapted for the DFR framework. Section 4 describe our exp erimental methodology, and Section 5 reports the experimental results and provides the related discussion. Finally, Section 6 concludes on the work and suggested future research directions. 2.1 Rocchio X  X  Relevance Feedback Method Rocchio X  X  algorithm [14] is a classic framework for implementing (pseudo) rele-vance feedback via improving the query representation. It models a way of incor-porating (pseudo) relevance feedback in formation into the vector space model (VSM) in IR. In case of pseudo relevance feedback, Rocchio X  X  method (RocRF) has the following steps: 1. All documents are ranked for the given query using a particular Information 2. An expansion weight w ( t, D R ) is assigned to each term appearing in the set 3. The vector of query terms weight is finally modified by taking a linear combi-2.2 The DFR Probabilistic Framework Divergence from Randomness (DFR) is a componential framework that measures the relevance of documents following the probabilistic paradigm [1]. In the DFR framework, the weight of a document d for a given query term t is given by: where IG is the information gain, which is given by a conditional probability of success of encountering a further toke n of a given word in a given document on the basis of the statistics on the retrieved set. Prob(tf) is the probability of observing the document d given tf occurrences of the query term t .  X  log 2 Prob ( tf ) measures the amount of information that term t carries in d . qtw is the query term weight component. Similarly to the query model in language modeling [11], qtw measures the importance of individual query terms. In the DFR framework, the query term weight is given by: where qtf(t) is the query term frequency of t , namely the number of occurrences of t in the query. qtf max is the maximum query term frequency in the query. For example, if all query terms appear only once in the query, which is usually the case for short queries, qtf max is 1 and qtw(t) is also 1 for each query term. When PRF is applied, qtw(t) is updated according to each query term X  X  importance in the pseudo relevance documents, so that informative query terms can be differentiated from the non-informative ones.

The other two components, namely information gain (IG) and information amount (  X  log 2 Prob ( tf )), can be approximated by di fferent statistics such as Poisson distribution and Bose-Einstein statistics, etc. [1]. In this paper, we apply the DPH instantiation of the DFR framework, which is based on the hyper-geometric approximation of the term distribution [2,6]. Using the DPH model, the relevance score of a document d for a query Q can be found in [2]. It is of note that no parameter tuning is required to optimize DPH, and we can rather focus on studying PRF.

The DFR framework provides a componential paradigm to which relevance feedback methods can be easily embedded. In the next section, we describe our method of combining the classical Rocchio X  X  relevance feedback method with the DFR framework. In this section, we first demonstrate how to employ the Rocchio X  X  algorithm (Ro-cRF) within the DFR probabilistic framework. The resulting feedback method is denoted by RocDFR. Next, We propose a quality-biased feedback method to enhance the robustness of RocDFR. 3.1 Adopting RocRF for DFR We propose to integrate Rocchio X  X  relev ance feedback (RocRF) method into the divergence from randomness (DFR) probabilistic weighting framework. The pro-posed method updates the query term weight component of the DFR framework by considering an expansion term X  X  importance in the pseudo relevance set. Our proposed method, called RocDFR , has the follows steps: 1. For a given query, performs the first-pass retrieval and considers the R high-2. Assigns an expansion weight w ( t, D R ) to each unique term t in D R ,where 3. Finally, the T most weighted expansion terms are added to the query. In Above we have proposed a standard deployment of the classical RocRF in the DFR probabilistic framework. One of the obstacles that prevents RocRF X  X  from its application to probabilistic models is the robustness problem. The retrieval performance of RocDFR could be highly sensitive to its parameters, in particular the size of the pseudo relevance set. In the next section, we propose a quality-biased feedback method that aims to improve the robustness of RocDFR. 3.2 Quality-Biased PRF The RocDFR method is simple and comput ationally efficient, and is usually ef-fective when its parameters are properly optimized, as shown later in our exper-iments. However, as RocDFR treats each f eedback document equally, regardless of their quality, its retrieval performanc e is likely to be sensitive to the size of the pseudo feedback document set. In particular, as shown in our experiments in Section 5.2, when the feedback document set size is very large, the retrieval performance is seriously degraded by the low-quality feedback documents on some test collections. To this end, we propose a quality-biased pseudo relevance feedback (PRF) method, denoted by QRocDFR , to promote expansion terms in the high-quality documents, and penalize those in the low-quality documents.
In our proposed QRocDFR method, a qua lity-biased factor is introduced to the query term weight updating formula (i.e. Equation 1) as follows: where q ( d r ) is the quality score of feedback document d r in the R highest ranked documents in the first-pass retrieval. The document quality score is given by the sum of the expansion weight of the original query as q ( d r )= t  X  Q w ( t, d r ).
In practice, our proposed QRocDFR method is very flexible since the weight of each feedback document can be deter mined in different ways. If a uniform quality score is assigned to all feedba ck documents, RocDFR can be viewed as a special case of QRocDFR. In this paper, we choose to infer the document quality by the expansion weight of the original query terms in the document. The rationale behind our choice of document quality score is that a high-quality feedback document is highly related to the query topic, and hence likely to yield the original query terms. Therefore, ou r document quality score is given by the sum of the expansion weight of the original query as q ( d r )= t  X  Q w ( t, d r ). 4.1 Test Collections and Evaluation In this section, we describe three represe ntative test collections used in our ex-periments: disk4&amp;5 (no CR), WT10G, and GOV2. These three collections are different in both size and genre. The TREC tasks and topic numbers associated with each collection are presented in Table 1.

In all our experiments 1 ,weonlyusethe title field of the TREC queries for retrieval. In the process of indexing and querying, each term is stemmed us-ing Porter X  X  English stemmer, and standard stopwords are removed. The MAP (Mean Average Precision) performance measure for top 1000 documents is used as evaluation metric. 4.2 Baseline Models and Parameter Training In order to evaluate the performance of our proposed methods, the baseline used is DFR-RF , the DFR relevance feedback method [1] implemented in the Terrier retrieval platform [9]. The DFR-RF method considers the R highest ranked documents as the pseudo feedback document set D R . Differently from RocDFR, DFR-RF views the R feedback documents as a whole bag of words, and measures the divergence of an expansion term X  X  distribution in D R from its distribution in the entire document collection. As show n by previous experiments, the DFR-RF method X  X  effectiveness is highly sensi tive to the feedback document set size R [6]. Thus, we expect our proposed methods to outperform DFR-RF in terms of both effectiveness and robustness.

In addition, we also compare our proposed methods with relevance model [7], which is a representative and state-of-the-art approach for estimating query lan-guage models within language modeling framework[8]. Relevance models do not explicitly model the relevant or pseudo-relevant document. Instead,they model a more generalized notion of relevance R . The formula of RM1 is: The relevance model p ( w | R ) is often used to estimate the feedback model  X  F , and then interpolated with the original query model  X  Q in order to improve its estimation. The interpolated version of relevance model is called RM3 .
In order to find the optimal parameter setting for fair comparisons, we use the training method in [4] for both the baseline and our proposed approach. In particular, first, for the smoothing parameter  X  in LM with Dirichlet prior, we sweep over values from 300 to 1500 with an interval of 50. Second, for the lin-ear combination parameter  X  of (Q)RocDFR, and the interpolation parameter  X  of RM3 , we sweep over values in the range of (0 . 0 , 0 . 1 ,..., 1 . 0). For the num-ber of expansion terms, we sweep over values in (10 , 20 , 30 , 40 , 50). To evaluate the baseline and our proposed approach, we use 2-fold cross-validation ,in which the TREC queries are partitioned by the parity of queries number on each collection. Then, the parameters learned on the training set are applied to the test set for evaluation purpose. 5.1 Performance of Basic Retrieval Models Table 2 presents the results of the basic retrieval models. In genereal, LM slightly outperforms DPH on all the test collectio ns. However, there is no signicant dif-ference observed, according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. It is of note that the results from LM are obtained under optimal parameter, while DPH is a parameter-free model. In this sense, DPH is still a good basic retrieval model. In addition, it is fair to make cross-comparison of dif-ferent PRF methods in language modeling framework and the DFR framework respectively, since the basic retrieval models are comparable. 5.2 Comparison of the PRF Methods Tables 3, 4 &amp; 5 present the comparisons of the PRF methods over different num-bers of feedback documents. A star and a  X + X  indicate a statistically significant improvement over the baselines of DFR-RF and RM3 respectively, according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level.

Generally, as we can see from Tables 3 4 5, QRocDFR is at lease compara-ble with DFR-RF and RM3 on all the test c ollections. In some cases, QRocDFR significantly outperforms both DFR-RF and RM3, especially when a larger num-ber of documents are used for PRF. In addition, QRocDFR and RM3 are more robust with respect to the parameter R , compared with DFR-RF and RocDFR. This is because both QRocDFR and RM3 take into account the quality of the feedback documents in some ways. When low-quality documents are used for PRF, it is likely that noisy terms will be introduced into the query such that the IR performance can be decreased.

A more clear picture of the robustness is depicted in Figure 1. While the performance of RM3 is the most robust on all the three collection, QRocDFR achieves better performance than RM3 does in terms of the MAP measure. It is also of note that when a larger number of documents are used for feedback, QRocDFR still achieves relatively good performance. In addition, on the GOV2 collection, we can see that DFR-RF an d RocDFR perform stably over param-eter R , although DFR-RF and RocDFR view each document the same. This is probably because most of the top 50 documents are positive for PRF. On WT10G and disk4&amp;5, when R approaches 50, the MAP values of both DFR-RF and RocDFR drop dramatically. In the contrast, QRocDFR and RM3 remain stably to some extent. It indicates that the quality of the feedback documents is importance in the process of PRF.
 5.3 Influence of the Control Parameter  X  Recall that we incorporate the feedback information to derive a better represen-tation of the query as shown in Equation (4). How much we rely on the feedback information is controlled by the parameter  X  . In our preliminary experiments, we found that the control parameter  X  in Equation (4) plays an important role in obtaining good performance. In this section, we empirically study the influ-ence of this parameter on all the test coll ections. In particular, Figure 2 depicts its influence over different numbers of feedback documents. In this set of exper-iments, we use 50 terms to expand the original query since QRocDFR archives very good performance under this setting generally. The MAP at  X  =0isac-tually the baseline of DPH without query expansion. It is of note that when  X  takes very large value, it approaches the performance resulting from using only the feedback information.

As we can see from Figure 2, in general, the IR performance can always be boosted when the feedback documents are used to expand the original query. Although the setting of  X  can affect the retrieval performance significantly, it is always safe to set  X  to a value around 1 on all our test collections. Similar results can be observed over other numbers of feedback documents, which are not presented as the problem of space limitation.
 In this paper, we have revisited the Rocchio X  X  relevance feedback method and integrated it into the divergence from randomness (DFR) probabilistic retrieval models. Moreover, we have proposed a quality-biased feedback method, called QRocDFR, which takes into account the quality of the feedback document. Ex-tensive experiments on three represent ative TREC test collections show that QRocDFR is not only effective, but also robust with respect to the number of feedback documents. To some extent, we h ave shown that the classical Rocchio X  X  relevance feedback method can be successfully applied to probabilistic models, and the resulting retrieval performance is at lease as good as those in the existing literature of pseudo relevance feedback methods.

In the future, we plan to study how to automatically determine the control parameter  X  . Although it is often relatively safe to set  X  to a value around 1, an automatic way to optimizing  X  for each individual query sounds more interesting. We also plan to use machine learning techniques to estimate the quality of the feedback documents.
 This research is jointly supported by NSERC of Canada, the Early Re-searcher/Premier X  X  Research Excellence Award, the Natural Science Foundation of China (No.60673039 and 60973068) , the National High Tech Research and Development Plan of China (No.2006AA01Z151), National Social Science Foun-dation of China (No.08BTQ025), the Project Sponsored by the Scientific Re-search Foundation for the Returned Overseas Chinese Scholars, State Education Ministry and The Research Fund for the Doctoral Program of Higher Education (No.20090041110002).

