 It is critical for retail enterprises to select good sites or loca-tions to open their stores, especially in curren t comp etitiv e retail mark et. However, evaluating the goodness of sites in real business applications is a complex problem. That is, how to judge whether the mark et around a store site is good? We don't know the exact mechanism of how a site can be good and it is hard to have correct \site goodness values" as supervised labels. The Retail Outlet Site Evalu-ation (ROSE) tool is designed to learn the site evaluation model by integrating city geographic &amp; demographic data and two kinds of expert knowledge: sample preference and feature preference. The feature preference information can help greatly reduce the required number of sample prefer-ences. It enables our application practicable because it is almost impossible to give such amoun t of sample preference pairs manually by experts when ranking hundreds of data points. In the experimen t and case study part, we show that the ROSE tool can achieve good results and useful for users to do site evaluation work in real cases.
 H.3.0 [ Information Storage and Retriev al ]: General; H.3.4 [ Information Systems Application ]: Systems and Software| Performanc e evaluation Algorithms, Experimen tation Ranking, Semi-sup ervised learning, Activ e learning, Prefer-ence constrain ts
Physical stores are always the most importan t sales chan-nel for most retailers, such as Walmart and 7-ELEVEN, even some on-line retail companies achieve great successes in web-store. Besides those pure retailers, a large amoun t of other enterprises also have personal consumers, e.g. chain restau-rants (McDonald, KFC), banks, etc. Their stores /branc hes /ATMs also play as the most importan t channel. We say those personal customer facing sales places (stores /restau-rants /branc hes...) as retail outlets. For the business de-velopmen t of those retail enterprises, planning and selecting prop er sites for opening outlets is one of the most critical tasks. For example, if KFC tends to open chain stores in a China city, it will have to  X nd new good sites. If one store was opened at a bad site with only a few consumers, this wrong site selection would waste thousands of dollars investmen t. We must open stores in good sites with enough mark et capacit y. In this task, the primary requiremen t is to judge whether the mark et around the site is good enough. That is, how to evaluate the site goodness.

Evaluating the retail outlet sites is a critical and challeng-ing work to retail enterprises. It usually has below di X culties in real cases. 1. Don't have an e X cien t or automatically metho d to col-2. Don't know the exact mechanism/mo del to judge whether 3. Don't have enough training samples. Experts may
Because of above reasons, it is not so direct to use data analysis approac hes to construct a practical site evaluation model. In fact, many retailers carry out this task using naive scoring model. Experts de X ne some indices and the weights, then the mark et departmen t collects the data val-ues of those indices by mark et survey. For example, for fast food store site evaluation, the indices can be: people tra X c volume, amoun t of living people in 1km around, sales vol-ume of attac hed shopping mall, number of small stores in 300m around, etc. Then the weights are assigned or tuned by experts. They may visit the target site and collect the values of those indices in  X eld. Each index will be given a score according to the data value, and the total weighted score is obtained as the evaluation value. This metho d is too sensitiv e to the weights which are decided by expert knowledge and not supp orted by data.

In this paper, we introduce the ROSE tool which can avoid above three problems and be practically used in site evaluation tasks. For the  X rst problem, ROSE is developed based on GIS (Geographic Information System) platform and extracts the indices (or features) from formatted geo-graphic and demographic data. The data is more reliable, and it doesn't require people to manually collect in  X eld. For the second and third ones, ROSE learns the site evaluation model by using semi-sup ervised learning metho d to integrate the data and expert knowledge. It provides experts to input the goodness comparison between two sites (sample prefer-ence) and the importance comparison between two indices (feature preference). To make it practically and easily used, we use two mechanisms to avoid that it may require experts to input too many preferences: One is to use feature pref-erence to complemen t the information, and the other is to use activ e learning algorithm to guide people to only input most useful preferences. The experimen ts show that ROSE can work well on benchmark data and real-w orld application data.

Section 2 describ es ROSE tool in three key comp onen ts and the kernel algorithms. Section 3 shows the performance on both benchmark data and real-w ork application data. Section 4 introduces related works and the di X erences from them. In the last section 5, we conclude the three key fea-tures why ROSE can be practically used in real-w orld cases.
ROSE can help consultan ts or experts to do site evalu-ation: evaluate how goodness the mark et is around a site. Using this tool, users can construct the site evaluation model from data and evaluate given sites. It is the primary supp ort for developing their store sites: Figure 1: ROSE screen shot. The upper part is geographic data viewer, and the bottom part is op-eration tools. 1. Open new stores. ROSE can help disco ver new good 2. Diagnose existing stores. When diagnosing a store,
ROSE constructs site evaluation model by semi-sup ervised learning using sample/feature preferences and data samples generated from geographic-demographic data. Firstly , it se-lects the set of sites to be evaluated and generates the data pattern around each site as data sample. Secondly , experts input sample preferences and feature preferences, and ROSE will learn the site evaluation model; Thirdly , if the learned model is not precise enough as lack of preference informa-tion, ROSE will guide user to input most informativ e prefer-ence pairs using activ e learning. Both feature preference and activ e learning are used to reduce the requiremen t of prefer-ence amoun t, as it is impossible to ask experts to manually input so many preferences. We will describ e the three mod-ules and their kernel algorithms in following sub sections.
Fig.1 shows the screen of ROSE. The upper part is the mark et data viewer. Users can view geographic and demo-graphic data around a site or anywhere in the city. In the left site table, user can select the types of data they want to view. Most analysis results are also displa yed here in graphic format. The bottom part is the operation tools, such as data import/exp ert, parameter setting, preference input, etc.
As we mentioned in section 1, one of the di X culties is how to get the features of sites. In naive scoring metho d, some features need manually survey in  X eld. It is really time cost when computing hundreds of sites. GIS (Geographic Infor-mation System) data is the city geographic and demographic data stored in GIS format. It is produced by professional data vendor and can be purchased from GIS mark et. Google Map is a kind of GIS data for navigation. For business analy-sis, we can purchase more complete and detailed geographic and demographic data. A typical list of GIS data content is shown in Table 1. It contains the geographic positions of various types of entities and their demographic attributes. "Type" attribute indicates the type of each entity, e.g. for Geographic Entity Demographic Attribute Residen tial Building Type, Area Size, Grade, Household Shopping, it can be shopping mall, supermark et, or chain store. "Grade" is related to the consuming level, e.g. higher "grade" food entities have higher prices. Fig.2 shows a snap-shot of GIS data.

Most mark et factors are based on the physical geographic entities. E.g. people tra X c volume is a variable which is di X cult to be precisely measured, but it should be decided mainly by the geographic entities. More entities, especially more shopping and subway stations will lead to more traf- X c. We can use the features from GIS data to represen t the mark et factors. In ROSE, we de X ne the statistical indices from GIS data to describ e the mark et environmen t around a site rather than the indices which need manually collecting or time cost. The feature de X nition may vary for di X eren t kinds of industries as their di X eren t target customers. For example, for KFC and luxury store, we can de X ne di X eren t features as in Table 2 because their di X eren t customer po-sitioning have di X eren t types of target customers. Luxury store focuses on the high grade facilities with have wealth y customers, while KFC coun ts all people.

The feature values of each site are computed from the GIS data in the neigh borhood area around the site, e.g. the 1km distance area around the site. In applications, the distance value is speci X ed by experts according to the size and type of retail outlets. We also can use all the values in the neigh bor areas with di X eren t sizes as  X nal features, e.g. Total o X ce buildings in 1km, Total o X ce buildings in 500m, etc.
In previous subsection, we obtain the feature values of all candidate sites from GIS data. Each site is a sample, so we get the data matrix. The objectiv e is to learn the ranking value of each sample. In real cases, it is very hard to get su X cien t samples with labeled ranking values, but Table 2: Feature examples for KFC and luxury store.

Total number of high grade Total number of it's much easier for them to provide several pairs of prefer-ences. In ROSE, we use preference learning metho d to solve the problem, and the most importan t: we incorp orate fea-ture preference simultaneously into the learning process to greatly reduce the requiremen t of preferences, which makes it practical in real cases.
Given a set of points X = f x 1 ; x 2 ; : : : ; x n g 2 R goal is to rank these points according to some rules. For example, in the web searc h problem, the points are usually ranked according to their relev ance to the query given by user. In our site evaluation problem, however, it is hard to obtain an ideal site as query point. Instead, the preferences between several sites are easier to obtain which can be used to disco ver the intrinsic ranking rule. Since only the pref-erences of a part of data points are judged by experts, the key problem is how to transfer the preferences to the whole point set to get the orders of all the data points. To achieve this goal, a manifold based metho d is prop osed in this pa-per which propagates the preference information according to the intrinsic data manifold. In order to form ulate the underlying manifold, a undirected weighted graph G ( V; E ) is constructed. V is the set of vertices. Each vertex corre-sponds to a data point in X . E is the set of edge. The weight of edge e ( v i ; v j ) is given by d ( x i ; x j ) where d : X  X X  X ! R denotes a metric on X . The preferences on a part of vertex pairs can be presen ted by a set E d . If ( v i ; v j ) 2 E point x i should be ordered higher than x j . Furthermore, the preferences between some feature pairs are also given by experts which are denoted by a set E f . If ( f i ; f j ) 2 E the i -th feature is more importan t than the j -th one.
The goal is to learn a good ranking function h : X  X ! R which assigns a ranking score h i to each data point x i . Then all the data points can be ranked according to the assigned ranking scores. A good rank function should make few rank mistak es, i.e., the ranking orders obtained from the ranking function should be consisten t with the expert judgemen ts on the set E d . We assume the ranking function is linear: h = w T x i . Then the ranking function should satisfy: The goal is equiv alent to  X nding a direction w . For all the data point pairs in E d , the projection of x i is bigger than x j on this direction. There may be several directions which satisfy this condition. The one which results in the largest projection distance between x i and x j is selected for good generalized capacit y. It is implemen ted following the maxi-mizing minim um margin idea in SVM: Furthermore, the slack variables are introduced for the non-rankable point pair: where  X  1 is a tradeo X  parameter. The framew ork in Eq.(3) is the same as the Ranking SVM metho d[6] which was pro-posed for supervised learning to rank problem. The resulting in ranking function may not be good when the number of the preference pairs is small. This problem is addressed by in-troducing a semi-sup ervised learning techniquewhic h learns from both labeled and unlab eled data. It is implemen ted by optimizing a regularized version of Eq.(3): where h is a vector, the i -th elemen t of which equals to h L is the Laplacian matrix of graph G which is given by: where D ij = d ( x i ; x j ) is the similarit y between x which is given by exp (  X  1 2  X  2 k x i  X  x j k 2 2 ) , S is a diagonal matrix. The i -th diagonal elemen t S ii is the sum of the i -th row of D . As observ ed in Eq(4), the regularizer h T Lh measures the variation of function h on the data manifold. Minimizing this regularizer will result in a smooth ranking function, i.e., the resulting in ranking function will assign similar ranking scores to the similar data points on the data manifold. Then the data structure is involved in to assist the learning of ranking function in this way.

In some real applications, it is easy for experts to judge the importance of the features. This information is also in-volved as assistan t information when the number of labeled data points is limited. We involve this information by con-straining that the more importan t feature has larger weight in each given feature preference pair. Another slack vari-able  X   X  is introduced to allow the deviation from the expert judgemen t of feature importance. The framew ork is  X nally given by: where w i and w j is the i -th and j -th elemen ts of w respec-tively. The constrain t w  X  0 is added to make the resulting in w interpretable. The optimization problem in Eq.(6) is solved by optimizing the dual problem: It is observ ed that the above optimization problem is a con-vex problem which can be solved by existing metho d in [2].
After setting up the evaluation model following above pro-cess, given a new site, we can extract its feature values, then insert it into the data point set just now and apply the same preference constrain ts again to estimate the eval-uation value, or use the transductiv e model to compute the evaluation value directly .
To better reduce the experts' manual work, we use activ e learning to guide experts to input the most useful prefer-ence pairs. In real cases, experts would like to follow the iterativ e process to do this work. They  X rstly input some initial preferences which may be not enough for training a good model, then run the learning process anyway; Then they are guided by activ e learning module to input the most informativ e preferences and run the learning process again. They repeat the iteration till the ranking values become sta-tionary .

We describ e our semi-sup ervised metho d with sample and feature preference constrain ts in last section. It can be viewed as an extension of the SVM metho d[12]. Therefore many well developed techniques of activ e learning for SVM metho d[11] can be used in our metho d. In our paper, we adapt the classical uncertain ty sampling scheme for activ e learning. Uncertain ty sampling is used extensiv ely by ac-tive learning metho ds since it is usually easy to implemen t and perform well in real applications. In our problem, this scheme is used for both unlab eled instance pairs and unla-beled feature pairs to get the pairs with most expected gain. In the uncertain ty sampling, the maximization of expected gain is implemen ted by  X nding the unlab eled sample whose label given by the curren t predict model is most ambigu-ous. This sample will be selected to label by the experts. In SVM metho d, it is equiv alent to  X nd the unlab eled data point which is close to the classi X cation hyperplane given by the curren t model. In [4], the author discussed that this metho d can be casted into a min-max framew ork.

In SVM metho d, the objectiv e function is given as follows: where (1  X  y i h i ) + = max (0 ; 1  X  y i h i ) is the hinge loss. S is the labeled data set. Then the unlab el data with the
Algorithm 1: Activ e semi-sup ervised ranking using Input:
Initial: Build a graph Laplacian from the whole data set, 1: Training an ranking function using the semi-sup ervised metho d in Section 2.2 with E d and E f . 2: Calculate j w  X  T ( x i  X  x j ) j and j w  X  j  X  w  X  i unlab eled sample and feature pairs respectiv ely. 3: Find the sample minimizes j w  X  T ( x i  X  x j ) j ,  X nd the feature pair minimize or maximizes j w  X  j  X  w  X  i j respec-4: Add the select pairs to E d and E f .

Output: h minim um maxim um loss: is selected. It is veri X ed that solving the problem in Eq.(9) is appro ximate to  X nd x j which minimizes j h  X  ( x j ) j . The function h  X  is the model obtained by curren t labeled data. For this problem, we  X rst consider the supervised case. The loss function can be written as follows:
L ( h; E d ; E f ; w ) = X Then we can cast the worst case analysis mentioned above by: where The y ij = 1(  X  1) means the i -th sample ranked higher(lo wer) than the j -th one, y ( f ) ij = 1(  X  1) means the i -th feature ranked higher(lo wer) than the j -th one. E u d is the set of un-labeled sample pairs while E u f is the set of unlab eled feature pairs. w  X  is the curren t model parameters. It selects the un-labeled data and feature pairs, whose preferences are labeled most ambiguously by the curren t ranking model. Given cur-pair to label and the argmin ( f i ;f j ) 2 E u f j w  X  j feature pair to label.

For the semi-sup ervised case, the scheme of activ e learning is the same. We only need to replace the curren t supervised model by the semi-sup ervised model.

In [10], the authors discussed that selecting the feature by certain ty sampling may work better than uncertain ty sam-pling. In our metho d, the certain ty sampling refers to  X nd feature pair which maximize j w  X  j  X  w  X  i j . We test both un-certain ty and certain ty sampling in our experimen t and  X nd the later one performances better. The algorithm is given in Tab.[3].
In the experimen ts, we will compare the e X ciency of us-ing preferences under similar ranking e X ectiv eness among four types of metho ds: Supervised learning Ranking SVM (RSVM)[6], Semi-sup ervised learning with pure sample pref-erence (SP) , Semi-sup ervised learning with sample prefer-ence and feature preference(SP-FP), Semi-sup ervised learn-ing with sample &amp; feature preference and activ e learning(SP-FP-AL). The experimen tal results on both benchmark data set and real word data are shown in this section.
We choose OHSUMED data set from the Letor data set[8], which is a popular data set in ranking task. The OHSUMED data set is selected from the  X ltering track of TREC 2000. New features are then extracted by authors of [8] to adapt the data set to the learning to rank problem. The OHSUMED data set contains 106 queries and 16 ; 416 documen ts. For each query , a subset of 16 ; 416 documen ts is labeled by the TREC committee with three relev ance judgemen ts: "def-initely relev ant", "possibly relev ant", "not relev ant" to the query . Then we can construct a data set for each query by collecting the documen ts which are judged with this query . For the 8-th query , all the judged documen ts are labeled by "not relev ant". We remo ve this query and obtain 105 data sets  X nally .

For each set, we can generate the preference constrain ts for all the documen t pairs based on the relev ance judgemen t. Since all the pairs are labeled, we can generate the partial labeling by suppressing some of these labels. For example, in semi-sup ervised learning, we choose a set of documen t pairs as preference constrain ts randomly . In activ e learning, the documen t pairs are selected according to the mentioned uncertain ty sampling scheme. For the features, we select the feature pairs in the same way. However, we don't have a standard label of the feature preference. We therefore mea-sure the importance of each feature pair by the following manner. For each feature, we rank the documen ts accord-ing to values of this feature. Then the result rank list is compared with the ground truth relev ance judgemen ts. The evaluation NDCG( Normalized Discoun t Cum ulativ e Gain )[5] which is used to measure the similarit y of two rank lists is calculated and assigned as score of this feature. The pref-erence of feature pairs are then judged by the NDCG scores. The feature which is more similar to the ground truth is though t to be more importan t.

For the OHSUMED data set, 45 features are selected from the  X elds of title and abstract of the documen ts. These fea-tures include term frequency , inverse term frequency , BM25[9] and Language model[14] scores. For each query , all the Figure 3: Test performance on OHSUMED when numbers of sample pairs change. judged documen ts will have the same inverse term frequency . Therefore we select the left three types of features in our ex-perimen ts, i.e., features(1-4,8-19,23-34,38-45, 36 features to-tally). The left three types of features are basic rank models in the information retriev al  X eld. Therefore, it is reasonable to rank the documen ts according to the values of each fea-ture and obtain the importance of this feature according to the similarit y of the resulting in rank list to the ground truth.

We test each metho d several times and set the hyperpa-rameters to obtain the best performance of this metho d. For OHSUMED data, the average result of the 105 subsets is used as the  X nal result for each metho d. In  X g.3, we compare the perfomance of RSVM, SP, and SP-FP metho ds under di X eren t numbers of sample prefer-ences and feature preferences. For Ranking SVM, the ran-domly selected sample pairs are used as supervision. The learned model is then used to rank all the data points in the same data set. For SP-FP , we set the number of feature preference pairs to 5 and 20 respectiv ely, e.g. \SP-FP(5)" represen ts the SP-FP metho d with 5 feature preferences. We can observ e that both SP-FP metho ds outp erform the other two metho ds when the number of sample preference pairs is small. With the help of feature preference, we can get better performance even when we input less sample pref-erences. It helps to reduce the requiremen t of the number of sample preferences.

In  X g.4, we  X x the number of sample preferences to 5, 10, 15 and 20, then evaluate the performances of SP-FP meth-ods under di X eren t feature preferences. We can see that when the number of sample preferences is small (5 and 10), the performance will be limited even adding enough feature preferences. It is reasonable because feature preference con-strain t is some like "rougher" than sample preference. Figure 4: Test performance of SP-FP on OHSUMED data when numbers of both sample and feature pairs change.
In this section, we will experimen t the prop osed metho d on the chain store location site data.
ROSE tool is designed to help experts or consultan ts to evaluate the sites of retail outlets, e.g. chain stores. In this real-w orld application, we aim to use ROSE to eval-uate 244 locations of chain stores in a major China city. Firstly , we extract the environmen t pattern of each site us-ing the process describ ed in section 2.1. Fig.5 shows the underla ying GIS data and the positions of those locations to be evaluated (indicated as stars). We may extract a lots of features from the underla y GIS data, while in this case, we extract 11 features, which are mostly related to the lo-cation evaluation, as the environmen t pattern for each sam-ple: f #HighResiden tialP eople (means the number of resi-dential people with higher wealth), #LowResiden tialP eople, #O X ceP eople, #Restauran t, #Hotel, #En tertainmen t, #C-ompan y, #LargeShopping, #SmallShopping, #Hospital, #S-chool g . The value of each feature is the number of this kind of geographic entities in 500 meters around the store. E.g. #Restauran t value of a store is the number of restauran ts in 500 meters around the store. The objectiv e is to learn the ranking values of the store sites using the environmen t patterns and preferences information input by experts. We also test four metho ds RSVM, SP, SP-FP and SP-FP-AL. Each metho d is run for 50 times and the average results are shown in Fig.6 and Fig.7. In this case, the experts visited the sites and manually labeled the ranking. We use the given ranking as the underlying true ranking to compute NDCG scores.

In Fig.6, the number of feature preferences is  X xed to 5 and 20 for SP-FP metho d. with increasing the number of sample preferences, both SP-FP metho ds always get better performances than SP and RSVM. Figure 6: Test performance on C-Store Site Data when numbers of sample pairs change. Figure 7: Test performance on C-Store Site Data when numbers of both sample and feature pairs change.

In Fig.7, the number of sample preference constrain ts changes from 5 to 20 for RSVM, and the number of sample and fea-ture preference constrain ts change from 5 to 20 simultane-ously for SP-FP and SP-FP-AL metho ds. The SP-FP-AL-1 and SP-FP-AL-2 stand for the activ e learning metho d us-ing uncertain ty and certain ty sampling respectiv ely. The performance of the SP-FP-AL-2 increases much faster than the RSVM and the SP-FP metho ds. So, activ e learning technique does further reduce the requiremen t of preference pairs. In this case, the activ e learning with feature cer-tainty sampling works better than the feature uncertain ty sampling. Ranking has attracted much atten tion in recen t years. Many supervised metho ds have been prop osed in the ex-isting works. These metho ds can be divided into two cate-gories according to the form of supervised information: list-wise and pairwise metho ds. In listwise metho ds[7][13], the whole ranking lists of several data sets are used to train the ranking function. The model is  X t to minimize the loss of a whole permutation. In pairwise metho ds, the prefer-ences of several data pairs are given by experts. The loss function is de X ned on the pairs of items. In the literature, people pointed out that the listwise metho ds can optimize the evaluation measures directly which makes it outp erform the pairwise ones. However, in many real applications, the listwise metho ds usually su X er from several di X culties. It is usually di X cult to give the ranking order of the whole data set especially when the data size is large. The complexit y of minimizing a listwise loss function is usually high. For this reason, we use pairwise technique in our metho d.
There are several pairwise ranking metho ds in the existing works such as Ranking SVM[6] and Ranking Boost[3] which are all supervised learning metho ds. In both metho ds, the preferences of a large number of pairs are needed to learn the ranking function. In order to reduce the requiremen t of labeled pairs. A semi-sup ervised pairwise metho d[1] was prop osed which used both labeled and unlab eled data for estimating the underlying data structure, which was used to help learn the ranking function. This metho d dose decrease the demand for labeled data. However, it still needs to input lots of preferences if it wants to achieve good performance in the case of a large amoun ts of samples. In ROSE tool, we further reduce the requiremen t of prior preferences in two practical ways: incorp orating feature preferences and avoiding useless preferences input. First, we add the feature preference constrain t, and train the ranking model simul-taneously with sample and feature preference constrain ts. Second, the activ e learning is used to select the most useful sample and feature pairs, so that the experts do not need to input other useless preferences. There are several papers[10] which used the activ e learning in both data and feature views for clustering and classi X cation problem. In our paper, we concen trate on the ranking problem.
In conclusion, ROSE has below three key features which makes it can be practically used in real-w orld cases: 1. Building on GIS platform to integrate geographic and 2. Using preference learning to avoid unavailable of la-3. Reducing the requiremen t of preferences: incorp orat-This impro ved preference learning metho d is tested in bench-mark and real data. which shows its superior on preference amoun t requiremen t. In fact, ROSE tool has been used by consultan ts in many real projects to help client evaluate their sites. [1] S. Agarw al. Ranking on graph data. ICML , 2006. [2] S. Boyd and L. Vanden berghe. Convex optimization . [3] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [4] S. C. H. Hoi, R. Jin, J. Zhu, and M. R. Lyu.
 [5] K. Jarvelin and J. Kekalainen. Ir evaluation metho ds [6] T. Joachims. Optimizing searc h engines using [7] T. Joachims. A supp ort vector metho d for [8] T. Y. Liu, T. Qin, J. Xu, W. Y. Xiong, and H. Li. [9] S. Robertson. Overview of the okapi projects. Journal [10] V. Sindh wani, P. Melville, and R. D. Lawrence. [11] S. Tong and E. Chang. Supp ort vector machine activ e [12] V. N. Vapnik. Statistic al Learning The ory . Wiley , New [13] Y. Yue, T. Finley , F. Radlinski, and T. Joachims. A [14] C. Zhai and J. La X ert y. A study of smoothing
