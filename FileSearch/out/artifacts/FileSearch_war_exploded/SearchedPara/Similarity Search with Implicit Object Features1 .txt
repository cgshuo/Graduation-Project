 Similarity search is fundamental to many applications involving spatial data analysis. Many research results [1,4,6,8,7,10] have been published in the last decade, where the most popular similarity model is based on a feature vector for each data object. In such a model, each data object, available for similarity search, is represented as a vector, and th e similarity between objects is measured by the distance between the vectors. Such applications include image similarity retrieval [4,10], shape similarity search [6,8] and similarity search on spatio-temporal trajectories [1,7]. The k -nearest neighbor (KNN) search is one of the most important similarity search queries. For a query object q and a query parameter k , KNN is to find the k objects that are most similar to q [5,11]. defined; consequently, the feature vector for each object is not pre-computed and stored in a database. For instance, ornithologists may want to identify similar bird communities for selecting a future res earch target or for behavior predica-tion. A cluster of bird nests is an object. In the application, nest positions are changing regularly and definition of a cluster may vary from time to time because of difference research orientation. Feature groups are represented as groups of polygons. For example, the open water map is a feature group, including lakes, rivers and springs as polygons. Other feature groups are the vegetation map including forests of specific vegetation, the predator distribution map including communities of predatory birds, and man-made structure map including towns, high ways and villages. Moreover, maps of rainfall precipitation and tempera-ture should also be considered; but in these contour maps, each value range could correspond to a feature group. In the application, a cluster of bird nests can be evaluated based on the distances to the nearest feature in each feature group, such as the nearest open water place and the nearest town. Figure 1 illustrates a cluster of nests and a nearby lake r epresented as a feature polygon. analysis, etc.
 of a non-conventional KNN, where the feature vector of an object is not pre-computed, namely SSIOF(Similarity Search with Implicit Object Features). In particular, we study the KNN problem where each object is a set of points in 2-dimensional space, and each object is evaluated against d groups of features to obtain a d -dimensional feature vector. By effect ively characterizing the results X  properties, we develop an efficient and novel R -tree based algorithm to evaluate features of each object. Then, an effect ive filtering technique is developed to prune away objects (clusters) as many as possible before a precise computation. These are the contributions of the paper. Our performance study demonstrates that our techniques are very efficient to process large spatial datasets. inaries. Section 3 and 4 presents our algorithms and the analysis. Experiment results are reported in Section 5. Finally, Section 6 concludes the paper. In this section, we start with formally defining the problem and then introduce some necessary background. 2.1 Statement In a 2-dimensional space, given n clusters C 1 ,C 2 ,...,C n and d categories/groups of features  X  1 , X  2 ,..., X  d .Eachcluster C i is a set of points and each feature is a polygon. We use F j to denote a feature and pt as a point.
 as d ( C i ,F j ), is defined as the average Euclidean distance from each point pt in C to the polygon. Here, the distance between a point and a feature dist ( pt, F ) is the minimum distance between the point and the edges of the feature. The aggregational feature evaluation of a cluster C i with respect to a feature category  X  k is the distance from C i to its nearest polygon in  X  k , denoted as  X  ( C i , X  k ). The problem of Similarity Search in Implicit Feature Space (SSIOF) is to find k most similar clusters to the given cluster C 0 based on the following similarity measure: distance in our paper. 2.2 R -Tree Index R -tree is a widely used index for spatial objects based on B + -trees, which orga-nizes geometric objects by recursively g rouping neighboring objects and repre-senting them by minimum bounding rectangles(MBRs). A node of R -tree cor-responds to a disk page. An intermediate node maintains a set of MBRs and pointers which represent the children nodes, while a leaf node contains a set of spatial objects with their positions in the database. Fig. 2 shows an instance of R-tree. each feature categories and perform our evaluations. Each polygon is represented by its MBR first, then those MBRs is indexed by R  X  -tree. Our proposed algorithm ES for solving the SSIOFproblem contains two major steps: 1) Feature Evaluation: in this step, we try to find all possible feature candi-2) Similarity Search: this step is to compute the k most similar clusters to 3.1 Feature Evaluation Let N C i be the MBR of a cluster C i with 4 edges r 1 ,r 2 ,r 3 and r 4 ;and N F j be the MBR of a feature polygon F j with 4 edges s 1 ,s 2 ,s 3 and s 4 .Weassumethat N C i and N F j do not overlap. We will first define some useful metrics between MBR X  X  for later discussion.
 s ,and U ( r k ,s l ) denotes the maximum distance between two points falling on r k and s l [2]. Thus the minimum of distance between two points contained in N C i and N F j can be expressed as: Pruning with the Lower and Upper Bounds In the SSIOFproblem, the similarity is not measured between points but clusters and polygons, so it X  X  too expensive to compute precise distances on pairs of clusters and features, which makes it n ecessary to use relatively tight lower and upper bounds for pruning. Then precise distances could be computed only on a small set of clusters and features.
 the lower and upper bounds of the distance between these two are : and erage coordinates of all points in C i on each dimension. We use L ( b ( C i ) ,N F j )to denote the minimal distance between b ( C i ) and a point in rectangle N ( F j ). The lower and upper bounds are illustrated in Figure 3 as LB and UB respectively. average of dist ( pt, F ) for all pt  X  C . Based on the inequality: d ( C, F ) is no less than the distance from b ( C ) to some points inside N C ,whichis no less than L ( b ( C ) ,N F ). Lemma 1 shows the correctness of the upper bounds. Lemma 1. For a cluster C i in MBR N C i andafeature F j in MBR N F j ,the upper bound of d ( C i ,F j ) is maxminU ( N C i ,N F j ) .
 Proof. Suppose that N F j is bounded by s l ( l =1 .. 4). Since N F j is the minimal bound rectangle of F j , there must be at least a point of F j on each s l .Thusthe upper bound of dist ( pt, F j )equalsmin l U ( pt, s l ). Consider all points on N C i , the upper bound of d ( C i ,F j )ismax pt  X  C i dist ( pt, F j ), which is no larger than maxminU ( N C i ,N F j ).
 bounds hold. When a feature group is indexed by an R -tree, the lemma still holds if we change N F j to the MBR of an R -tree node. This gives us the opportunity to prune features while traversing the index.
 R-Tree Based Pruning. Making use of the index on each feature group could speed up the process of feature evaluation. Next we will introduce the pruning technique for a feature group  X  k indexed by an R -tree T  X  k , as shown in Algorithm 1. Each node of T  X  k corresponds to a disk page. To lower the disk I/O cost, we traverse T  X  k using the following strategy which allow us to visit each R -tree node at most once.
 cluster C i , we maintain a candidate list L ( C i , X  k ), implemented as a heap. Each list entry e is either the MBR of an non-leaf R -treenodeortheMBRofa feature polygon, corresponding the inter mediate levels and the leaf level in the R -tree. As mentioned above, the lower and upper bounds hold on both kinds of MBRs, denoted as d LB ( C i ,e )and d UB ( C i ,e ). For any pair of entries in the record the minimum of d LB ( C i ,e ), the minimum of d UB ( C i ,e ) and the maximum q ( C i , X  k ) = 0 initially.
 Algorithm 1. Feature Evaluation all clusters, and insert it in all lists. In each iteration from Line 2 to Line 10 in Algorithm 1, the lists are visited in a round-robin fashion. A non-leaf entry with the minimum lower bound d LB ( C i ,e ) is selected for the current list. Here, a non-leaf entry means the corresponding R -tree node is not a leaf node. We replace it its low bound d LB ( C j ,e ) isn X  X  less than  X  UB ( C j , X  k ), the minimum upper bound we verify the list and filter those entries whose lower bounds d LB ( C j ,e )isgreater any non-leaf entry in all lists. 3.2 Similarity Search In this section, we will discuss how to compute the exact distances between pairs of clusters and feature groups based on the generated candidate features for answering the SSIOFqueries. Our goal is the find the cluster most similar to the query C 0 while minimizing the computation complexity. Algorithm 2 presents the overview of the similarity search step.
 group  X  k . Function ComputeExact( C i , X  k ) in Line 1 and 5 computes the exact distance between C i and feature group  X  k . C min is the cluster with minimal bound of similarity between C i and C 0 , as computed from the input as follows.  X  Algorithm 2. Similarity Search and array as shown in Figure 4. The initial bound of similarity between C i and C 0 are computed as shown on the last column. puted for precise similarity, until the n ext lower bound is larger than an already-found result. This sequence can minimize the number of clusters that is pre-cisely computed. Also in Line 6, after each calling of ComputeExact ,thecur-rent lower bound of similarity is refined using the exact distance returned from the function, and is compared with the result, which greatly reduce the number of feature groups need to be computed. For the example in Fig. 4, the cluster C 3 is first chosen since lower bound of Sim ( C 3 ,C 0 ) is the minimal in all clus-ters. Suppose its precise similarity is 3. The next cluster is C 2 . After calling ComputeExact ( C 2 , X  1 ), assume the lower bound of Sim ( C 2 ,C 0 )isupdatedto be 4, which is larger than the current result 3. As a result, C 2 is dropped as it can not be the result. Also the lower bound of Sim ( C 1 ,C 0 ) is larger than the current result 2, and C 1 is eliminated as well and the final result C 3 is returned. Lemma 2. Algorithm 2 gives the correct answer to the similarity search query. Proof. Consider the case that Algorithm 2 returns C i as result and the exact answer is C j where i = j .Thisisimpossiblesinceafter C i is precisely com-puted, the lower bound of Sim ( C j ,C 0 ) must be smaller than Sim ( C i ,C 0 ), in consequence, C j is chosen to be precisely computed and C j should be returned instead of C i .
 ters. Suppose that the cluster returned is C r with result r ,andthereexistsan algorithm A which minimizes the number of chosen clusters. In algorithm A ,a cluster C i such that lower bound of Sim ( C i ,C 0 ) is larger than r must not be visited while all other clusters must be co nsidered for precise computation. This is exactly the case of Algorithm 2. For a cluster C i that Sim LB ( C i ,C 0 ) &gt;r , After processing C r , result is updated to r and C i are dropped.
 Edge Pruning ComputeExact( C i ,  X  k ) is used to compute the exact distances between a cluster C i and a feature group  X  k . It need to calculate all the distance between the points in C i and the candidate features in every feature groups. The brute-force way is to compute the distance between a point and every edge in some feature and choose the minimum one as the distance of the point to the feature. We proposed some techniques that can avoid useless computations and save much more time than the brute-force way. As shown in Figure 5, the rectangle is the minimum bounding rectangle of a certain feature. By extending the four edges of the MBR, we partition the whole space into 8 areas expect the MBR itself. s 1 ,s 2 ,s 3 ands 4 are vertices of the MBR and a, b, c and d are four edges on the feature. p 1 and p 2 are points belonging to some cluster.
 all edges of the feature in the brute-force way. In fact, we can found that the minimum distance from p 1 to the feature must be the minimum distance of p 1 to one of the four edges a, b, c and d .Incaseof p 2 , the minimum distance from p 2 to the feature must be the minimum distance of p 2 to one of the two edges c and d .
 falls in the edge, then we only need to compute such kind of edges that s i s j can be project on. If not, suppose the nearest vertex of MBR to p is s i ,only the edges that s i can be project on are computed. To further reduce the time complexity, edge projections of a feature are computed at most once and then stored in memory for all other points. Also, when the MBR of a cluster is wholly contained in one of the 8 areas, it is no t necessary to check the position of each point any more.
 Extend to k -Clusters The above algorithm is extended to return the k clusters which are most similar to the given cluster C 0 . In Line 8 of Algorithm 2, variant result should be set to the k -th lowest similarity,and k most similar clusters are returned in Line 13. As mentioned in the above section, for a node on the R  X  -tree, we visit it at most once. In each step, the node to be visited is chosen considering only one cluster while ignoring the preference of other clusters. This searching strategy is based on an assumption that the number of clusters is relatively small, since the strategy sacrifices local optimization for each cluster to achieve a better global I/O cost. Since reading disk is much more costly than in-memory computation, our algorithm works well when the number of clusters is not too large. computation is unbearable, we can use following divide-and-conquer strategy which is similar to the Nested Loops Join. We first partition the clusters into several parts by grouping near clusters. Then we use our proposed algorithm on each part of clusters. In this case, if there are n groups of clusters, each node of an R  X  -tree is visited for at most n times. We implemented our proposed ES algorithm and evaluate its performance on synthetic data. We use the algorithm CPM (Compute Proximity Matching) as a benchmark based on [9]. The algorithm CPM solves a problem that is similar to our problem assuming the number of feature polygons is relatively small and there is no spatial index built on the features. It reads the relevant clusters into buffer first, then read features batch by batch into buffer and determine their groups. For each cluster C i and feature group  X  j , it computes the approximate distance between C i and each feature in  X  j for filtering out features that are too computes the approximate similarity for each cluster and filter out clusters that are not the solution. Finally it calculates the exact similarities to the remaining clusters and their associate featu res, and return the query result.
 numberisthesameineachofthe g features groups. The number of points in each cluster is nc and nf gives the number of edges in each feature polygon. In the experiments, average nc is 100 and average nf is 15.
 distributed in the 2-dimensi onal space. The size of rectangles are randomly cho-sen within a limited range. Number of feat ures in each group is determined such that the summary is m . Rectangles corresponding to the clusters or a features group do not intersect with each other. In each of n rectangles, nc points are uniformly generated, based on which the MBR s are computed. This gives the nc clusters. In each of the remaining rectangles, nf points are randomly generated. To generate a simple polygon which is linked by the nf points. We will apply a Graham X  X  scan-like algorithm [3].
 gorithms CPM and ES are implemented using C++, Experiments are run on a Linux machine with 1.8G P4 CPU and 512M memory. For each dataset, we process extensive queries a nd get the average result. 5.1 Scalability Comparison We first compare the algorithms with different number of clusters, ranging from 200 to 1000. 100000 features are clustered in 10 groups. The experiment results are shown in Fig. 6.
 of pages that corresponding to R  X  -tree index and features. CPM does not use index, but reads a large amount of features; ES reads a small number of index pages in the first step and a few features in the second step. It is clear that the I/O cost of ES is much smaller than CPM .
 ES responds in about 42 seconds while CPM needs nearly 5 minutes to get the result.
 of features. Feature number varies from 5000 to 100000. There are 10 feature groups and the number of clusters is set to 200. Results are shown in Fig. 7. Similar with the previous experiments, the first sub-figure shows the I/O cost while the second compares the precessing time. than 6000 disk pages read in memory, compared with large I/O cost and more than 1 minute processing time of CPM . 5.2 Dimensionality Comparison We evaluate our algorithm with different dimensionality of feature space. The number of feature group varies from 2 to 20. 100000 features are categorized to the feature groups and number of clusters is 200. Fig. 8 shows the I/O cost and user time of the two algorithms, which demonstrates the large performance difference between the two algorithms. In this paper, a similarity search problem which is based on an implicit feature space is investigated. By making use of the spatial indexes like R -trees built on the feature categories, we present an eff ective algorithm for the queries, which consists two steps: feature evaluation and similarity search. Experiments show the efficiency of the algorithm on all cases.
 joins a set of clusters to itself, with respect of d different categories of features. Acknowledgment. The research described in this paper was partially sup-ported by ARC Discovery Grant (DP0346004).

