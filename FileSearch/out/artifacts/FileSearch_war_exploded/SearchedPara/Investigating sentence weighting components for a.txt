 1. Introduction
Sentence based summarisation techniques are commonly used in automatic summarisation to produce ument into a list of sentences. Important sentences are then detected by some sentence weighting scheme, and the highly weighted sentences are selected to form a summary. Although researchers know that the sentence extraction techniques often result in summaries that lack coherence, the generated summaries are useful for humans to browse ( Hirao, Isozaki, Maeda, &amp; Matsumoto, 2002; Paice &amp; Jones, 1993 ) and make judgements about.

A sentence weighting scheme can be variously formulated by employing many components and distributing them with different parameters. For example, Term Frequency, Sentence Order and Sentence Length are common components. However, the detail of how to formulate a sentence weighting scheme is rarely discussed and reported in the literature. In this paper, we focus on investigating and comparing effectiveness between Query Term Frequency and Query Term Order, and evaluating the summaries produced with the
ROUGE-1 metric. 2. Sentence extraction and query terms
The early work from Luhn (1958) identified that a significant sentence consisted of a set of significant words. The definition of significant words in his work avoided linguistic implications such as syntax but gave occurrences. The words in Luhn X  X  work are every single word in a document without any pre-processing
These four term types derive four methods for extracting summaries, and also proved that terms contain important clues for producing summaries.

Since people began to frequently search information online, the relationship between terms in a query and documents has become an active research area. Robertson (1990) discussed using term weighting to generate new terms and examine the usefulness of the new terms as a query explanation approach. Tombros and San-derson (1998) proved that users could better judge the relevance of documents if their query terms appeared in the summaries. Manabu and Hajime (2000) combined the use of query terms and lexical chains to produce query-biased summaries. White, Ruthven, and Jose (2003) used a combination of query terms, Edmundson X  X  title and location to determine important sentences. Several studies about query length from 1981 to 1997 erately experienced and experienced searchers, searchers who were familiar with the search topics and those who were not, and humanities scholars have come to the conclusion that an average query length was in the range of 7 X 15 terms. The word term has been defined in Jansen et al. X  X  (2000) work: a term is any unbroken string of characters, and a query length is the number of search terms in a query. They studied query length by using search engine log and indicated that the length of a real query from real users was on average 2.21 terms from the range of 0 X 10 terms, and query length declined from 1981 to 2000. This result was an inspiration for our proposed Query Term Order algorithm. However we decided to use the top five frequent terms in our experiment, reflecting the more recent work of Williams, Zobel, and Bahle (2004) , who selected phrase length from 2 to 7. Five, therefore seemed a reasonable length to use. 3. Query term order examination with DUC
Evidence that automatic summarisation is improved by the use of Term Order in both documents and que-
Order algorithm is to pay attention to the order of a user X  X  query terms. As the previous research showed that query terms are generally short, processing the QTO algorithm for online summarisation is not complex and can generate a set of weighting terms from the input query terms to enhance weighting effectiveness. Although our proposed Query Term Order algorithm proved effective for producing search result summaries with Eng-rithm X  X  effectiveness using different sets of data.

Document Understand Conference ( DUC, 2004 ) data was used for this experiment. The data originated from task 1 of the competition in DUC 2004. It contains 50 English Newswire clusters, each with 10 docu-ments of a similar content. After the competition finished, eight sets of human summaries were provided by DUC for evaluating participants X  systems. The provided human summaries were utilised as the gold stan-dard summaries against which to compare our system produced summaries.

Lack of queries for the QTO algorithm was the first problem that we encountered. Therefore we generated our own queries in order to produce summaries. Term Frequency (TF) was employed to generate queries as it is one of the most common techniques used in automatic query generation ( Somlo &amp; Howe, 2003 ). A list of 235 stopwords was removed from the documents and no stemming technique was used. The stopword list is slightly modified from the stopword list of the Onix Text Retrieval Toolkit ( Onix, 2000 ). Words relating to date or part of a day were also removed, such as Sunday, Monday, Sun, Mon, morning, afternoon and for the 50 DUC clusters.

Each query generated 10 summaries from its related cluster therefore the 50 queries produced 500 summa-ries with each sentence weighting scheme. Six sentence weighting schemes were used (see Section 3.1 ). Sum-mary length was limited to 76 characters, which was a restriction imposed by DUC. We reused it in order to have the same length as the human summaries for the ROUGE evaluation. 3.1. Six sentence weighting schemes We focused on investigating four sentence weighting components namely: Query Term Order (QTO), Query Term Frequency (QTF), Sentence Length (SL) and Sentence Order (SO).

The most important idea in the QTO algorithm is that however a query is processed the order in the ori-ginal query is preserved all the time. Formula (1) shows how the QTO score is calculated, where s represent a number of j segmentations respectively. The segmentations are derived by removing stop words from an input query and taking a sequence of contiguous words between either punctuation or a stop word into some single terms. The second split may be unnecessary if the segmentation already contains a single term only. Therefore t 1 , t 2 , t 3 , ... , t k represents terms from second split, and f of QTO X  X  weighting terms in a sentence respectively. Each weighting term is assigned a score in descending order (i.e. s 1 is assigned j + k , s 2 is j + k 1 ... and t f tion of the segment, and k is the position of the term within the segment. The m is the total number of weight-ing terms, therefore it is equal to j + k .

QTF is used to calculate the frequency of each query term in a sentence. Formula (2) represents how QTF is calculated, where t 1 , t 2 , t 3 , ... , t n represents terms in a query, and f t , t 2 , t 3 , ... , t n respectively. Each of t 1 , t 2 , t is f 1 + f 2 + f 3 + f n .

The Sentence Length (SL) score is shown in formula (3) . Each sentence X  X  length is calculated according to how many spaces ( x ) are in the sentence. For example if x = 1 then the SL = 2, which means the sentence con-tains two words.

Sentence Order is scored in descending order as shown in formula (4) , where y represents the scores. There-fore the earliest sentence is scored highest and the latest sentence is scored 1.

We produced six summarisers for the experiment. They are named A , B , C , D , E and F and described in the following section. In addition, we adjusted parameters  X  in C and F  X  in order to discover the best combination for the weighting scheme. We also tested omitting short sentences with different thresholds from 4 to 10 words ( Kupiec, Pedersen, &amp; Chen, 1995 ).

A. QTO: The single component QTO is used in the A weighting scheme. We do not give any parameter to
B. QTO/SL: We considered using Sentence Length to balance the QTO score in case of a longer sentence C. ( a )(QTO/SL) + ( b )SO: SO was included to expand scheme B into a combination of two components (i.e.
D. QTF: This scheme is used as a comparison with QTO. Each term appearing in a query is treated the
E. QTF/SL: The scheme is used for comparison with the B scheme, and constructed for the same reason
F. ( a )(QTF/SL) + ( b )SO: This is also used to compare with C. 3.2. Evaluation with ROUGE
To evaluate our 6 * 500 summaries produced from the different sentence weighting schemes, we employed the ROUGE metric ( Lin, 2004 ). Although ROUGE contains many metrics, we only used ROUGE-1 for the evaluation. There are two reasons for the decision. The first one is that ROUGE is an extended version of
BLEU, and Papineni, Roukos, and Ward (2001) indicated that the unigram precision yields a score which more closely matches human judgements. Also n -gram precision decays roughly exponentially with n in their experiment. The second reason, illustrated in Fig. 1 , is that DUC 2004 ROUGE evaluation is similar to Pan-pineni X  X  report. The alphabetic numbers A X  X  in Fig. 1 are eight categories of human produced summaries.
ROUGE evaluations show that ROUGE-1 has the highest scores. The scores decline roughly exponentially when the n -gram increases. Even though ROUGE contains N -gram, longest common subsequence and weighted longest common subsequence metrics, ROUGE-1 (unigram) effectively predicts system ranking based on the other scores.

Table 1 shows ROUGE-1 evaluation results of the C scheme in each entry cell, where the first left column shows the a parameter increases from 0.1 to 0.9 while b decreases from 0.9 to 0.1. The top row shows the threshold of each sentence is from 4 words long to 10. The comparison graph is shown in Fig. 2 , where the 3:7 ratio is the highest and 1:9 is the lowest among the nine different a and b ratios in the C scheme. Table 2 shows ROUGE-1 evaluation results of the F scheme. The table structure is the same as Table 1 .
Their results are compared in Fig. 3 , where the 4:6 ratio is the highest and 1:9 is still the lowest one among the nine different a and b ratios in the F scheme.

Table 3 shows the results of all six weighting schemes, where the results for C and F are the highest param-eter ratios taken from Tables 1 and 2 respectively. Fig. 4 shows ROUGE-1 evaluation results, and clearly dem-onstrates that using a single weighting component (i.e. A and D) achieved the worst results. Although the results show that A is slightly worse than D, we can only assume that the use of a term frequency algorithm to generate queries automatically has already given the advantage to Query Term Frequency (the D scheme).
However, the C scheme performed the best, and in addition, using QTO in a combination performed better than without. For example, B clearly shows better results than E, and C is also better than F. We can be almost certain that QTO performs better than QTF. If we group the six weighting schemes into (A, B, C) and (D, E, F) we find that a combination with more weighting components always performs better than fewer results.
 4. Conclusion
In this paper we have examined the importance of the term order in a given query by comparing different sentence weighting schemes for automatic summarisation. The human summaries provided by DUC 2004 were utilised as the gold standard summaries, and compared with system produced summaries. We con-structed six weighting schemes and explained how we adjusted them to avoid imbalanced weighting results in producing summaries. The results were evaluated by the ROUGE-1 metric, and show that using a single component in a weighting scheme yields the worst performance. But using QTO in a combination produced promising results. In particular the C (0.3QTO/SL + 0.7SO) weighting scheme which combines QTO with
Sentence Length and Sentence Order performed the best among the six. Finally, Query Term Frequency (QTF) was shown to be the least useful weighting component.
 References
