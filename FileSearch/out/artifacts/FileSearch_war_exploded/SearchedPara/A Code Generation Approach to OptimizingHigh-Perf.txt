 We present a code-generation-based optimization approach to bringing performance and scalability to distributed stream process-ing applications. We express stream processing applications using an operator-based, stream-centric language called S PADE ,which supports composing distributed data flow graphs out of toolkits of type-generic operators. A major challenge in building such applica-tions is to find an effective and flexible way of mapping the logical graph of operators into a physical one that can be deployed on a set of distributed nodes. This involves finding how best operators map to processes and how best processes map to computing nodes. In this paper, we take a two-stage optimization approach, where an instrumented version of the application is first generated by the S
PADE compiler to profile and collect statistics about the process-ing and communication characteristics of the operators within the application. In the second stage, the profiling information is fed to an optimizer to come up with a physical data flow graph that is deployable across nodes in a computing cluster. This approach not only creates highly optimized applications that are tailored to the underlying computing and networking infrastructure, but also makes it possible to re-target the application to a different hardware setup by simply repeating the optimization step and re-compiling the application to match the physical flow graph produced by the optimizer. Using real-world applications, from diverse domains such as finance and radio-astronomy, we demonstrate the effective-ness of our approach on System S  X  a large-scale, distributed stream processing platform.
 H.2.4 [ Database Management ]: Systems X  Query processing Algorithms, Design, Performance Streaming Systems, Profile Driven Optimization
In an increasingly information-centric world, people and organi-zations rely on time-critical tasks that require accessing data from highly dynamic information sources and generating responses de-rived from on-line processing of data in near real-time. In many ap-plication domains, these information sources are taking the form of data streams , that are time-ordered series of events or sensor read-ings. Examples include live stock and option trading feeds in finan-cial services, physical link statistics in networking and telecommu-nications, sensor readings in environmental monitoring and emer-gency response, and satellite and live experimental data in scientific computing.

The proliferation of high-rate streaming sources, coupled with the stringent response time requirements of stream processing ap-plications force a paradigm shift in how we process streaming data, moving away from the  X  X tore and then process X  model of database management systems toward the  X  X n-the-fly processing X  model of emerging stream processing systems (SPSs). This paradigm shift has created strong interest in SPS-related research, in academia [21, 8, 4, 1, 6, 11] and industry [9, 14, 20, 2, 10] alike.

Due to the large and growing number of users, jobs, and infor-mation sources, as well as the high aggregate rate of data streams distributed across remote sources, performance and scalability be-come key challenges in SPSs. In this paper, we describe a code gen-eration approach to optimizing high-performance distributed data stream processing applications, in the context of the S PADE guage and compiler [10], and the System S distributed stream pro-cessing platform [2].
System S applications are composed of jobs that take the form of data flow graphs . A data flow graph is a set of operators con-nected to each other via streams . Each operator can have zero or more input ports, as well as zero or more output ports. An operator that lacks input ports is called a source , and similarly an operator that lacks output ports is called a sink . Each output port creates a stream, which carries tuples flowing toward the input ports that are subscribed to the output port. An output port can publish to multiple input ports, and dually an input port can subscribe to mul-tiple output ports, all subject to type compatibility constraints. Data flow graphs are allowed to contain feed-back loops, that may form cycles in the graph.

Data flow graphs can be deployed across the compute nodes of a System S cluster. The placement, scheduling, and other resource allocation decisions with respect to data flow graphs are handled autonomically by the System S runtime, whereas they can also be influenced by the users through knobs exposed by System S. Fig-ure 1 illustrates several concepts related to distributed stream pro-cessing and data flow graphs in the context of System S. In this paper, we focus on compile-time optimizations that deal with cre-ation of efficient data flow graphs, which falls under the responsi-bility of the S PADE compiler, as discussed next. Optimizations that relate to the System S stream processing core [2], multi-job opti-mizing scheduler [25], and job management subsystem [13] are not discussed here. S PADE is a rapid application development front-end for System S. It consist of a language, a compiler, and auxiliary support for building distributed stream processing applications to be deployed on System S. It provides three key functionalities:  X 
A language to compose parameterizable distributed stream pro-cessing applications, in the form of data flow graphs. The S language provides a stream-centric, operator-level programming model. The operator logic could optionally be implemented in a lower-level language, like C++, whereas the S PADE language is used to compose these operators into logical data flow graphs. The S
PADE compiler is able to coalesce logical data-flow graphs into physical ones that are more appropriate for deployment on a given hardware configuration, from a performance point of view. This is achieved by fusing several operators and creating bigger ones that  X  X it X  in available compute nodes. As we will discuss shortly, how this coalescing is performed is critical in creating high-performance data stream processing applications.  X 
A type-generic streaming operator model, which captures the fundamental concepts associated with streaming applications, such as windows on input streams, aggregation functions on windows, output attribute assignments, punctuation markers in streams, etc. S
PADE comes with a default toolkit of operators, called the rela-tional toolkit, which provides common operators that implement relational algebra-like operations in a streaming context.  X 
Support for extending the language with new type-generic, con-figurable, and reusable operators. This enables third parties to cre-ate application or domain specific toolkits of reusable operators. For details of the S PADE language, we refer the reader to our S
PADE project overview paper [10].
A major challenge in building high-performance distributed stream processing applications is to find the right-level of granu-larity in mapping operators to processes to be deployed on a set of distributed compute nodes. From now on, we refer to the con-tainer processes that host operators as Processing Elements ,PEs for short. The challenge of creating PE-level flow graphs for de-ployment, out of user-specified operator-level flow graphs, has two aspects, namely flexibility and performance.

From a flexibility point-of-view, one should define an applica-tion using fine-granular operators, so that it can be flexibly de-ployed on different hardware configurations. Monolithic operators that are  X  X oo big X  make it very hard to port an application from a small set of powerful nodes (like a Blade server) to a large set of less powerful nodes (like a BlueGene supercomputer), as it requires the manual process of refactoring the user-code and is a significant burden on the developer. S PADE  X  X  operator-level programming lan-guage is focused on designing the application by reasoning about the smallest possible building blocks that are necessary to deliver the computation an application is supposed to perform. While it is hard to precisely define what an operator is, in most application do-mains, application engineers typically have a good understanding about the collection of operators they intend to use. For example, database engineers typically conceive their applications in terms of the operators provided by the stream relational algebra [5]. Like-wise, MATLAB [18] programmers have several toolkits at their dis-posal, from numerical optimization to signal processing.
From a performance point of view, the granularity of the PE-level graph sets the trade-off between making use of pipelined paral-lelism versus avoiding costly inter-process communication. For in-stance, at one extreme, it is undesirable from a performance stand-point to run 100 operators as 100 PEs on a single processor system, due to the excessive cost of tuple transfers between processes. On the other extreme, running 100 operators as a single PE on a multi-node configuration will make it impossible to take advantage of hardware parallelism that might be available (e.g., from multi-core chips or muptiple nodes). The sweet spot for optimization is really the set of scenarios between these two extremes, where a healthy balance of fusion and pipelined parallelism is ideal in achieving high performance. As an application gets larger, finding such a balance manually becomes increasingly difficult and necessitates automatic creation of PE-level data flow graphs. S PADE  X  X  fusion optimizer can automatically come up with an optimized partition-ing of operators into PEs, in order to maximize the performance for a given hardware configuration.

Another major challenge in creating optimized stream process-ing applications is to understand the computation and communi-cation characteristics of the operators that form a data flow graph. Without such information, it is almost impossible to make informed decisions in the optimization stage. The S PADE compiler addresses this problem by generating instrumentation code under profiling mode, so that resource usage statistics of operators can be cap-tured and fed into the optimizer. Further complicating the prob-lem, such statistical information could potentially be highly skewed when the profiling is performed in non-fused mode. To address such concerns, S PADE  X  X  profiling framework is designed to be flex-ible enough to collect operator-level statistics under arbitrary fusion settings and with little runtime overhead.
In this paper we devise an optimization framework aimed at find-ing an effective and flexible way of mapping the logical graph of operators into a physical one that can be deployed on a set of dis-tributed nodes. Our contributions include:
In summary, the reliance on code generation provides the means for not only creating highly optimized applications that are tailored to the underlying computing and networking infrastructure, but also re-targeting the application to a different hardware configuration by simply repeating the optimization step and re-compiling the appli-cation to match the physical flow graph produced by the optimizer. In most cases, applications created with S PADE are long-running, continuous queries. Hence the long runtimes amortize the build costs. Nevertheless, the S PADE compiler has numerous features to support incremental compilation, reducing the build costs as well. Using real-world applications, from diverse domains such as fi-nance and radio-astronomy, our experimental results demonstrate the effectiveness of our approach on System S.

Finally, it is important to note that, in this paper, the only data flow graph manipulation we consider is coalescing. We do not ex-plore the possible re-orderings of the operators in the data flow graph. Although such optimizations are commonly applied to query trees in relational databases [22], it is not straightforward to extend them to System S, since in the general case, no assump-tions can be made about the semantics of the operators that form a data flow graph. Data flow graphs may contain large numbers of user-defined operators or type-generic operators from third-party toolkits. S PADE uses code generation to fuse one or more operators into a PE (Processing Element). PEs are units of node-level deployment in System S. A PE is a container for operators and runs as a separate process. At any given time, a running PE is tied to a compute node, although it can be relocated at a later time. Multiple PEs can be run on a given compute node, and a single multi-threaded PE can take advantage of different cores on the same compute node. In this section, we look into how PEs are generated and how they operate.
S PADE  X  X  PE generator produces container code that performs the following fundamental functionalities: 1 ) Popping tuples from the PE input queues and sending them 2 ) Receiving tuples from operators within and pushing them 3 ) Fusing the output ports of operators with the input ports of
PEs have input and output ports, just like operators. There is a one-to-one mapping between the PE-level ports and the exposed ports of the operators contained inside the PE. An operator-level output port is exposed if and only if at least one of the following conditions are satisfied: i ) The output port publishes into an input port that is part of an operator outside this PE; ii ) the output port generates an exported stream. Streams can be exported by the ap-plication developer to enable other applications to dynamically tap into these streams at runtime. This is especially useful for incre-mental application deployment, where multiple applications can be brought up at different times, yet can communicate with each other using dynamic stream connections.

Dually, an input port is exposed if and only if at least one of the following conditions are satisfied: i ) The input port subscribes to an output port that is part of an operator outside this PE, ii ) the input port subscribes to an imported stream. Imported streams have asso-ciated import specifications that describe the exported streams they are tapping into. An input port connected to an imported stream needs to be exposed at the PE level, so that the dynamic stream connections can be established during runtime. Ports that are not exposed at the PE level are internal to the PE and are invisible to other PEs or applications.

Unlike operator ports, PE ports are attached to queues. This also points out an important difference between PE-level and operator-level connections. Crossing a connection from a PE output port to a PE input port involves queuing/dequeuing, mar-shalling/unmarshalling, as well as inter-process communication. The latter can involve going through the network, in case PEs are located in different nodes. In contrast, connections between op-erator ports are implemented via function calls and thus are much cheaper compared to connections between PE ports.

Note that the fusion of operators with function calls results in a depth-first traversal of the operator subgraph that corresponds to the partition of operators associated with the PE, with no queuing involved in-between. However, the existence of multiple threads within a PE can complicate this otherwise simple execution model. Next, we explore the details of the execution of operators within a PE. An operator container generated by S PADE is driven by a main PE thread. This thread checks all input queues for tuples, and when a tuple is available from a PE input port, it fetches it and makes a call to the process function of the associated input port of the op-erator that will process the tuple. Depending on the details of the operator logic, the depth first traversal can be shortcut in certain branches (e.g., an operator filtering the tuple or buffering it without generating output tuples) or result in multiple sub-traversals (e.g., an operator generating multiple output tuples). This design entails non-blocking behavior in process functions associated with the in-put ports of an operator.

As noted earlier, S PADE supports multi-threaded operators, in which case the depth-first traversal performed by the main PE thread will be cut short in certain branches and more importantly other threads will continue from those branches, independently. This has an important implication: The process functions associ-ated with input ports of an operator can be executed concurrently, and as a result code for stateful operators should be written in a thread-safe manner. For user-defined operators, S PADE can gener-ate code to automatically protect the process functions to provide thread-safety (as an optimization, such locks are not inserted into the code if an operator is not grouped together with other opera-tors and is part of a singleton PE). As an alternative, finer grained locking mechanisms can be employed by the operator developers.
PEs can contain any type of operators, including source opera-tors. A source operator is special in the sense that its processing is not triggered by the arrival of an input tuple. Source operators have their own driver process functions. As a result, S PADE -generated PE containers start a separate thread for each source operator within the PE. A source operator thread will call the source operator X  X  pro-cess function, which will drive the processing chain rooted at the source operator. The source operator will not release the control back to the calling context until termination.

Since S PADE supports feedback loops in the data-flow graph, an operator graph is not necessarily cycle-free, which opens up the possibility of infinite looping on a cycle within a composite PE (which will result in running out of stack space). The rationale be-hind allowing feedback loops in S PADE is to enable user-defined operators to tune their logic based on feedback from downstream operators (e.g., by refining a classification model). Under operator fusion, S PADE expects feedback links to be connected to non-tuple-generating inputs. This guarantees cycle-free execution under op-erator fusion. However, it is the developer X  X  responsibility to ensure that the feedback links are not connected to tuple-generating inputs.
Figure 2 depicts a PE generated by S PADE via fusion of 6 tors. The threads involved in the execution of the PE logic are illus-trated with circling yellow arrows. The main PE thread is shown next to the input queues. Source operator O 5 has its own thread. O 3 is a multi-threaded operator, and its first output port is driven by its additional thread. Note that the operators O 1 ,O 2 are subject to execution under more than one thread. This particular PE has 3 threads in total.
A powerful profiling framework is a key ingredient for an effec-tive optimization approach. As a result, S PADE  X  X  compiler-based optimization is driven by automatic profiling and learning of appli-cation characteristics.

S PADE  X  X  profiling framework consists of three main compo-nents, namely code instrumentation , statistics collection ,and statistics refinement . Code instrumentation is used at compile-time to inject profiling instructions into the generated spade processing elements. Statistics collection is the runtime process of executing those instructions to collect raw statistics regarding communication and computation characteristics of operators. Statistics refinement involves post-processing these raw statistics to create refined statis-tics that are suitable for consumption by the fusion optimizer.
S PADE  X  X  profiling framework is designed around the following three goals, which are crucial in achieving effectiveness, flexibility, and efficiency:  X  Statistics should be collected at the operator/port level  X  The profiling infrastructure should be fusion transparent  X  The profiled applications should not suffer from the observer
Collecting statistics at the operator/port level enables the fusion optimizer to reason about different fusion strategies, since PE level statistics can be computed by composing operator level ones. Fu-sion transparency provides us with the flexibility to collect the same type of statistics regardless of the fusion mode. This also makes it possible to perform multiple profile/optimize/re-fuse steps to fur-ther revise the fusion strategy. However, in this paper we mostly focus on results from a single iteration of this step. Finally, the profiling performed by S PADE must be light-weight, so as not to change the application behavior under instrumentation.
S PADE instruments the generated containers using submit sig-nals . For each operator-level output port in the container, there is one submit signal which keeps a list of all the calls to the process functions of the downstream operator inputs subscribed to the out-put port. There is also a submit signal for each PE level input port. Submit signals are used as instrumentation points, and are used to collect the following port-level metrics: 1 ) Number of tuples seen by an input/output port 2 ) Size of tuples seen by an input/output port 3 ) CPU time taken to submit a tuple via an output port 4 ) CPU time taken to process a tuple received by an input port In order to measure CPU time it takes to perform a submit call, S
PADE starts a timer when it receives a tuple at the submit signal and stops the timer when all the calls to the process functions are completed. It also starts similar timers for each process function call attached to the submit call. For instance, in Figure 3 the sub-mit signal contains two process calls and as a result employs three timers, one for the submit call as a whole, and one for each of the process calls. These timers are implemented using CPU counters. As we will discuss shortly, S PADE instrumentation does not collect the aforementioned metrics on a per-tuple basis, and uses sampling to decrease the cost of profiling.

S PADE also maintains a separate base CPU time metric for each thread in the container, other than the main PE thread.
For each metric collected at a submit signal, S PADE maintains not one but a number of samples. For instance, we will have several values representing CPU time taken to submit on an output port, each for a different tuple. These values are timestamped to identify them uniquely. As a result, statistical summaries can be computed from these metrics.

S PADE uses two methods to reduce the computation and memory overhead of profiling, namely stride sampling and reservoir sam-pling. Stride sampling is used to decide for which tuples we want to collect metrics. A sampling fraction of s  X  (0 , 1] will result in collecting metrics once in every 1 /s tuples. For these tuples we will collect metrics, but since we cannot keep all metrics in mem-ory in a streaming system we use a reservoir sampling approach to decide which ones to keep. With reservoir sampling each metric value collected has an equal chance of ending up in the reservoir. A reservoir size of S is used to limit the memory overhead. The reservoir sampling algorithm works as follows [24]:
Given a new value, which is the i th one so far ( i  X  0 ), perform the following action: If i&lt;S add the new value into the reservoir, else ( i&gt; = S ) with probability S/i replace a random item from the reservoir with the new value (drop it with probability 1  X 
Note that the stride sampling reduces the computation overhead, whereas the reservoir sampling reduces the memory overhead of profiling. For instance, with s =0 . 001 and S = 500 , we will collect metrics once in every 1000 tuples and we will only keep a random subset of 500 values from the metric values collected so far.
Recall that one of the goals of profiling was to collect metrics at the operator/port level. Fusion makes this task challenging, since the CPU time for processing a tuple on a given input port not only counts the amount of time spent executing the associated operator X  X  logic, but also any downstream processing that may take place. For instance, Figure 4 shows one possible execution flow and the asso-ciated submit and process times. A simple approach for extracting the time spent for executing the operator logic is to subtract the time spent on downstream processing on a given output port, called submit time , from the total time measured for the process call at the input port, called process time . However, this is not possible with-out very heavy instrumentation, since the execution flow following a process call on an input port is operator logic dependent. For in-stance, the tuple processing may result in zero or more submit calls on one or more output ports. Without tracking which submit calls are triggered by which process calls, it is impossible to compute the CPU time spent within a given operator X  X  scope on a per-port basis. Multi-threading further complicates the situation.

Fortunately, one can compute the average amount of CPU time spent by an operator by refining the raw metric values described in Section 3.1. Concretely, we perform a post-processing step after raw metric values are collected during a profiling run to create the following refined statistics: Data transfer rate : For each input (output) port, we compute the rate in bytes/sec, denoted by r i ( r i ) for the i th input (output) port. Similarly, we compute the rate in tuples/sec, denoted by c i th input (output) port.
 CPU Fraction : For each operator, we compute the fraction of CPU it utilizes, denoted by u  X  [0 ,N ] where N is the number of CPUs on a node. The CPU fraction is computed by aggregating the pro-cess and submit time metric values. Before giving the details of its computation, we first introduce some notation.

Let us denote the CPU time spent on processing a tuple on i th input port as t i and similarly, CPU time spent on submitting a tuple on i th output port as t i .Let k denote the number of input ports, and l denote the number of output ports. Recalling that an operator can have additional threads, m of them in the general case, we denote the CPU fraction taken up by the i th thread as b i , which can be trivially computed using the base CPU time metric from Section 3.1 and the associated timestamps.

Finally, we compute the CPU fraction u for an operator at hand, as follows:
Equation 1 is interpreted as follows. First we add up the fraction of the CPU used by any threads that the operator might have. Then for each input port, we add the fraction of the CPU spent for exe-cuting the associated process calls. Finally for each output port, we subtract the fraction of the CPU spent for executing the associated submit calls. The former is approximated by c i  X  t i for the i th input port. c i is the number of tuples processed within a second, where as t i is the average CPU time in seconds, spent for executing a pro-cess call. This average is computed using the N metric values that were stored in the reservoir during the profiling run. The fraction of the CPU spent for executing submit calls is computed similarly ( c i  X  t i for i th output port).
So far we have discussed operator/port-level statistics. However, the fusion optimizer also needs to know about the cost of sending and receiving tuples at the PE boundaries. Consider a simple sce-nario depicted in Figure 5, where we have two operators, O O 2 , connected via a stream, and we consider two alternative fu-sion strategies: two separate PEs versus a single composite PE. For brevity, assume that these operators have a selectivity of not change the size of the tuple as they propagate it. In the first alternative, the cost of processing a tuple is equal to the cost of re-ceiving a tuple, executing the operator logic, and sending it, that is C r + C ( O 1 )+ C s for operator O 1 and C r + C ( O 2 )+ C erator O 2 . However, when we sum up these costs we overestimate the cost of the second alternative, since the actual cost for the latter is C r + C ( O 1 )+ C ( O 2 )+ C s , assuming that the cost of a function call is negligible compared to C r , C s , C ( O 1 ) ,and C
In summary, the fusion optimizer needs to know about the pro-cessing cost involved in sending and receiving tuples to reason about the cost of different fusion strategies. It is important to note that the cost of sending and receiving tuples mostly depend on the rate at which the tuples are being sent/received and their sizes. As a result, S PADE maintains an application-independent mapping of rate (tuples/sec), tuple size (bytes) pairs to CPU fraction mapping ( v : R +  X  N +  X  [0 ,N ] ), which is used for all applications. This mapping needs to be re-adjusted only when the hardware changes
The goal of fusion optimization is to come up with a PE-level data flow graph using the statistics collected as part of the profil-ing step about the communication and computation characteristics of the operators, as well as the application-independent statistics
It can be scaled using the new processor X  X  MIPS value divided by the MIPS value of the processor used to create the mapping. regarding the cost of sending and receiving tuples at the PE bound-aries. Deployment of the resulting PE-level data flow graph should provide better throughput, compared to the na X ve approaches of cre-ating one PE per operator or fusing all operators into one PE, and more importantly compared to manual fusion done by application designers (which is only practical for small-scale applications).
Let O = { O 1 ,  X  X  X  ,O n } denote the set of operators in the data flow graph. Our goal is to create a partitioning, that is a set of parti-tions P = { P 1 ,  X  X  X  ,P m } where each partition is a set of operators ( P i  X  X  ,  X  P i  X  X  ), such that this partitioning is non-overlapping (  X  partition represents a container PE to be generated by the S compiler as described in Section 2.
 There are two main constraints in creating the partitioning First, the total CPU fraction used by a partition should not exceed a system specified threshold, say M axF rac . Let us denote the computational load of a partition P i by CompLoad ( P i ) OperLoad represents the computational load due to executing the operators within a single PE, that is: u ( O j ) is the CPU fraction used by operator O j , as described in Section 3.3. CommLoad represents the communication load in-curred due to sending and receiving tuples at the PE boundaries, which is computed using the rates, tuple sizes, and the container-level statistics described in Section 3.4. Let Rate ( P i PE communication rate for partition P i and Size ( P i ) be the av-erage tuple size. Using the mapping v from Section 3.4, we can compute: We refer to partition as saturated iff its computational load is above M axF rac ,thatis: With these definitions, we can represent the first constraint as: We usually set M axF rac to a value smaller than 1 in order to leave enough slack for the System S scheduler to dynamically adjust PE placements and CPU allocations during runtime, in response to changes in the workload.

Second, the ratio of CPU load due to executing the operator logic within a partition, compared to the overall CPU load for the partition, referred to as the effective utilization and denoted by Ef f ectiveU til , should be greater than or equal to a threshold, say MinUtil . This limits the overhead of inter-PE communica-tion. For instance, if a partition contains a single operator that performs very little work on a per-tuple basis, the time spent by the PE container for receiving and sending tuples will constitute a significant portion of the overall CPU load, resulting in a small Ef f ectiveU til value, which is undesirable. Formally:
EffectiveUtil ( P i )= OperLoad We refer to a partition as underutilized if its effective utilization is below MinUtil ,thatis: Algorithm 1: GreedyFuse Algorithm G (1) P X  X  P i : P i = { O i } X  O i  X  X } (2) while true (3) P c  X  X { P i ,P j } X  X  : Mergable ( P i ,P j ) } (4) if P c =  X  then break (5) { P i ,P j } X  argmax (6) P i  X  P i  X  P j ; P X  X  X  X  P j } (7) Label partitions in P ,as P 1 ,  X  X  X  ,P m Ideally, we should have no underutilized partitions. Formally: Finally, among solutions that satisfy the saturation and utilization constraints, we prefer the one that minimizes inter-PE communi-cation. In other words, the optimization goal is to minimize the objective function
S PADE employs an algorithm named GreedyFuse to create op-erator partitions. This greedy algorithm starts with a partitioning where each operator is assigned to a different partition. At each greedy step, we create a set of candidate merges , where a merge involves fusing two of the existing partitions into a new, bigger one. Each candidate merge is assigned a merge benefit and the one with the highest benefit is applied to complete the greedy step. The algorithm continues until no candidate merges are available.
In order to create the candidates for merging, S PADE fusion op-timizer considers all pairs of underutilized partitions, but filters the pairs that are not connected to each other or would violate the sat-uration constraint when merged. Formally,
Note that at each greedy step, an effort is made to remove under-utilized partitions. However, this scheme does not guarantee that the final partitioning is completely free of underutilized partitions.
The merge benefit is computed as the amount of inter-PE com-munication saved by merging two partitions, so that each greedy step reduces the objective function to be minimized as much as possible. Formally, M ergeBenef it ( P i ,P j )= Rate ( P i )+ Rate ( P j )  X 
Since the merged partitions must be connected by at least one link, each greedy step reduces the aggregate inter-PE communica-tion, unless the rate of communication between the merged parti-tions is equal to zero. Algorithm 1 gives a summary of the Greedy-Fuse algorithm.

S PADE  X  X  fusion optimizer also performs the placement of PEs to compute nodes. The details of the placement scheme are beyond the scope of this paper. In summary, it uses a form of clustering (PEs into nodes) with the goal of minimizing inter-node communi-cation.
In this section, we evaluate the performance of S PADE  X  X  code generation-based fusion and profiling mechanisms, as well as the effectiveness of its fusion optimizer, using both synthetic and real-world workloads and applications.
The experiments presented in this section were performed on a small subset of our System S cluster at IBM T. J. Watson, using up to 4 nodes, where each node has a quad-core 3 GHz Intel Xeon pro-cessor and 4 GB of main memory. The nodes are connected together with a Gigabit Ethernet network. The results reported in the paper are from the steady-state runtime behavior of the applications and are deduced from raw data collected via reservoir sampling. The default sampling rate used for profiling is s =0 . 001 ,andthede-fault reservoir size used is S = 5000 . The default M axF rac and MinUtil values used for the fusion optimizer (see Section 4) are 0 . 5 and 0 . 95 , respectively.

The experimental study is divided into two parts. In the first part, we use synthetic workloads and micro benchmarks to study the trade-offs involved in fusing operators into PEs as well as the impact of profiling on the application performance. In both cases, we use the steady-state throughput of the application as our evalu-ation metric. In other words, the aggregate rate at which the source operators can pump data into the rest of the processing chain is used as our performance metric.

In the second part, we apply the fusion optimizer to two real-world applications, using real-world workloads. The first applica-tion is from the radio-astronomy domain and the second one is from the financial markets domain. For both applications, we perform the profile/optimize/re-fuse step and compare the performance of the resulting optimized application with other alternatives, includ-ing the hand-fused versions that are optimized manually by the ap-plication developers themselves. Benefit of fusion : To showcase the benefit of fusion, we performed a series of micro benchmarks. We used a simple application, which is a series of operators that form a processing chain. The source operator feeds the rest of the processing chain with tuples that it creates on-the-fly. Each operator in the processing chain simply forwards the tuples it receives to the next operator in line, until the tuples finally reach the sink. There is no operator logic involved in the base version of this simple application. We compile this appli-cation with different number of operators, as well as with different levels of fusion and examine the resulting throughput in tuples/sec. All the experiments in this section are performed on a single quad-core node.

The graphs in Figure 6 plot throughput in tuples/second as a function of the number of operators in the processing chain, for dif-ferent numbers of operators per PE. The legend can be read from Figure 8. For instance, if the number of operators in the chain is 16 and the number of operators per PE is 2 (the red line with cir-cle shaped markers), then we have 16 / 2=8 PEs executing the operators, where consecutive pair of operators along the chain are fused into their own PEs. We observe from the figure that, for all chain lengths from 1 to 32 , the fully fused scenario (number of PEs equal to 1 ) always prevails over other alternatives. Even though the node has 4 processors, we are unable to take advantage of pipeline parallelism, since the cost of inter-process communication domi-nates. This is mostly because the effective utilization (as defined in Section 4) is very low for the operators as well as for the fused PEs in the chain, since there is no operator logic involved.
The graphs in Figure 7 plot throughput in tuples/second as a function of the tuple size, for different numbers of operators per PE. The number of operators in the chain is fixed to 16 .Again,fu-sion prevails over other alternatives. It is more interesting to look at the case, where we have sufficient amount of per-tuple processing, so that the per-PE effective utilization starts to improve in partially
Figure 6: Overhead of no-fusion, with vary-fused scenarios, making it possible to exploit pipelined parallelism available from 4 processors. Along these lines, the graphs in Fig-ure 8 plot throughput in tuples/second as a function of the compu-tation size, for different numbers of operators per PE. Computation size is a variable we use to increase the per-tuple processing cost. In this case, we use a dummy loop to perform a busy wait. The larger the computation size, the longer the busy wait. As we ob-serve from Figure 8, for small computation sizes the fuse-all sce-nario still outperforms alternatives, but as the amount of per-tuple processing increases, the partially fused scenarios start outperform-ing the fuse-all scenario. This clearly illustrates the trade-off be-tween making use of pipeline parallelism versus reducing the inter-process communication overhead. Recall that the fusion optimizer is designed to stop fusion once the effective utilization for a PE reaches a predefined threshold, which results in avoiding the over-head of inter-process communication and yet taking advantage of available parallelism. In the next subsection we look at how well the fusion optimizer performs in real-world applications. Cost of profiling: One of the major goals of our profiling frame-work is to be able to collect statistics with as little overhead as possible, so as not to alter the runtime behavior of the application due to instrumentation.

The graphs in Figure 9 plot throughput in tuples/second as a function the number of operators in the chain, for different pro-filing sampling rates. Note that the reservoir size does not have an impact on the performance, but only on the memory consumption. As a result, we are only showing the impact of the profiling sam-pling rate on the performance. In this experiment all operators are fused into one single PE. Recall that the profiling framework is fu-sion transparent and can work under any fusion scenario. Figure 9 shows that s =0 . 001 and no profiling performs almost the same, and s =0 . 01 is very close with only around 10 % overhead. When we move to s =0 . 1 , the overhead increases to around 50 our experiments in the rest of the paper, we use a default value of s =0 . 001 , which incurs almost no overhead. This is quite con-servative, since the results show that s  X  0 . 01 is overall a good setting.
In this section we look at two real-world applications, one from the radio-astronomy domain and another from the financial markets domain. For both applications, we compare the throughput using the following four fusion strategies: NONE  X  No fusion, one PE per operator FALL  X  Fuse-all, one PE hosts all operators FMAN  X  Manual fusion performed by the app. developer FATM  X  Automatic fusion performed by S PADE
It is worth noting that FMAN represents the scenario where the application developer 2 uses the knobs exposed by the S PADE guage to perform the fusion herself, including the placement of PEs to nodes.
 Outlier Detection/Radio-astronomy : This is an application that detects outliers, such as gamma ray bursts, in radio data from outer space. The data flow graph for the application is shown in Figure 12. The application reads its data from a workload file stored on disk. The workload data is collected from the LOIS [17] project, which is the Scandinavian extension to LOFAR [16] radio-telescope currently under construction in northwestern Eu-rope, which consists of a large number of small radio antennas dis-tributed in a large geographical area, instead of having one giant radio-telescope. This application was built using the S PADE guage and the S PADE built-in operators.

Figure 10 shows the throughput achieved from different fusion strategies, relative to the FATM scenario. The NONE scenario uses all 4 nodes, the FMAN scenario uses 2 nodes, and surprisingly the FATM scenario uses a single node. FALL always uses a single node. Recall that each node has 4 cores. The results show that the automatic fusion (shown in Figure 14) performs 70 %, 100% 85 % better compared to NONE, FMAN, and FALL, respectively. Most interestingly, manual fusion performs (shown in Figure 13)
In our experience, application developers are domain area experts, but not necessarily systems optimization experts.
Figure 16: PE-level data flow graph and placement, hand-fused and hand-placed
Figure 17: PE-level data flow graph, auto-fused and hand-placed the worst, which attests to the importance of automatic fusion opti-mization and the inherent difficulty of manually optimizing appli-cations. For large-scale applications it is truly impractical to ex-pect application developers to come up with an effective fusion and placement strategy.
 Bargain Detection/Financial Trading : This is an application that detects bargains, using financial ticker streams containing trade and quote data from a stock exchange (see [3] for more details). The data flow graph for the application, which contains 200 operators, is shown in Figure 15. The application reads its data from a workload file stored on disk. The workload data contains 22 days worth of ticker data from the NY Stock Exchange, for the month of Decem-ber 2005 . Again, the application is built using the S PADE and the S PADE built-in operators.

Figure 11 shows the throughput achieved from different fusion strategies, relative to the FATM scenario. The NONE and FMAN scenarios use all 4 nodes, whereas the FATM scenario uses Figures 13 and 14 show the manual and automatic fusion, respec-tively. The figures also show how PEs are mapped to nodes. Rect-angular blocks with different colors represent different nodes. The results show that the automatic fusion performs 6 , 2 ,and better compared to NONE, FALL, and FMAN, respectively. Al-though we have not studied the optimality of the fusion optimizer, these results are very promising and showcase the strength of our optimization framework. In the relational data processing world, frameworks such as STREAM [4], Borealis [1], TelegraphCQ [8], among others, fo-cus on providing a declarative language or a graphical flow-graph composition scheme to construct data stream processing applica-tions, as well as a data stream processing runtime to deploy and execute these applications. A fundamental difference between S
PADE /System S and these systems is that, S PADE relies on a code-generation framework, instead of having type-generic oper-ator implementations that employ some form of condition interpre-tation and type introspection. The reliance on code generation pro-vides the means for the creation of highly optimized platform-and application-specific code. In contrast to traditional database query compilers, the S PADE compiler outputs code that is very tailored to an application as well as system-specific aspects such as: the under-lying network topology, the distributed processing topology for the application (i.e., where each piece will run), and the computational environment, including hardware and architecture-specific tuning. In most cases, applications created with S PADE are long-running queries. Hence the long runtimes amortize the build costs. None of the existing approaches gives the developer the language constructs or the compiler optimization knobs to write the application in a granular way to truly leverage the levels of parallelism available in distributed settings with modern processor hardware. Unlike tradi-tional database optimization, S PADE does not perform re-orderings on the query graphs, since no assumptions can be made in the gen-eral case, regarding the semantics of the operators contained in the data-flow graph.

On the programming language side, StreamIt [21] is certainly closer to us. But its focus is on implementing stream flows for DSP-based applications. XStream [11] is another similar stream-ing system for DSP-like applications. More recently, the Aspen language [23] shares some commonalities with our work on S  X  the most important being the philosophy of providing a high-level programming language, shielding users from the complexities of a distributed environment. Many distinctions exist, however. S is organized in terms of high-level operators, forming toolkits (e.g., a relational algebra toolkit), that can be extended with additional operators. Most importantly, the S PADE compiler generates code and, hence, can customize the runtime artifacts to the characteris-tics of the runtime environment, including architecture-specific and topological optimizations.

Finally, contrasting S PADE with Hadoop [12]  X  as one represen-tative middleware supporting the map/reduce paradigm  X  or Mary-land X  X  Active Data Repository [15] and DataCutter [7], the key difference is the abstraction level employed for writing applica-tions. These approaches rely on  X  X ow-level X  programming con-structs. Analytics are written from scratch as opposed to relying on built-in, granular, operators. Moreover, map/reduce operations can only be used for computations that are associative/commutative by nature. Pig Latin [19] was recently introduced to improve upon the low-level programming abstractions of map-reduce systems. Pig Latin sits in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. However, we are not aware of any optimization frameworks tied to Pig Latin. Further-more, Pig Latin is not designed as a streaming system, but instead as a front-end to map-reduce engines.
In this paper, we have addressed a major challenge in optimizing distributed stream processing applications, that is finding an effec-tive and flexible way of mapping the logical graph of operators into a physical one that can be deployed on a set of distributed nodes. Using the stream-centric and operator-based S PADE language and its code-generating compiler, we have developed an effective so-lution which relies on a two-staged optimization framework. First an instrumented version of the application is generated in order to profile and learn about the computation and communication char-acteristics of the application. Next, this profiling information is fed to a fusion optimizer that comes up with a physical data flow graph, which is deployable on the System S distributed runtime and is optimized to strike a good balance between taking advantage of parallelism and avoiding costly inter-process communication. We have used real-world applications and workloads to showcase the performance benefit that can be achieved from using S PADE tomatic application optimization framework.
