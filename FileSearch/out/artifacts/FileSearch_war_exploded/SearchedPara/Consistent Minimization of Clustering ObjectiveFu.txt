 Clustering is the problem of discovering  X  X eaningful X  groups in given data. Many algorithms try to achieve this by minimizing a certain quality function Q n , for example graph cut objective functions such as ratio cut or normalized cut, or various criteria based on some function of the within-and between-cluster similarities. The objective of clustering is then stated as a discrete optimization problem. Given a data set X n = { X 1 , . . . , X n } and a clustering quality function Q n , the ideal clustering algorithm should take into account all possible partitions of the data set and output the one that minimizes Q n . The implicit understanding is that the  X  X est X  clustering can be any partition out of the set of all possible partitions of the data set. The algorithmic challenge is to construct an algorithm which is able to find this clustering. We will call this approach the  X  X iscrete optimization approach to clustering X .
 If we look at clustering from the perspective of statistical learning theory we assume that the finite data set has been sampled from an underlying data space X according to some probability measure. The ultimate goal in this setting is not to discover the best possible partition of the data set X n , but to learn the  X  X rue clustering X  of the underlying space. In an approach based on quality functions, this  X  X rue clustering X  can be defined easily. We choose a clustering quality function Q on the set of partitions of the entire data space X , and define the true clustering f  X  to be the partition minimizing Q . In this setting, a very important property of a clustering algorithm is consistency. Denoting the clustering constructed on the finite sample by f n , we require that Q ( f n ) converges to Q ( f  X  ) when n  X  X  X  . The most important insight of statistical learning theory is that in order to be consistent, learning algorithms have to choose their functions from some  X  X mall X  function space only. To mea-sure the size of a function space F one uses the quantity N F ( x 1 , .., x n ) which denotes the number of ways in which the points x 1 , . . . , x n can be partitioned by functions in F . One can prove that in the standard setting of statistical learning theory, a necessary condition for consistency is that E log N F ( x 1 , .., x n ) /n  X  0 (cf. Theorem 2.3 in Vapnik, 1995, Section 12.4 of Devroye et al., 1996). Stated like this, it becomes apparent that the two viewpoints described above are not compatible with each other. While the discrete optimization approach on any given sample attempts to find the best of all (exponentially many) partitions, the statistical learning theory approach restricts the set of candidate partitions to have sub-exponential size. Hence, from the statistical learning theory perspective, an algorithm which is considered ideal in the discrete optimization setting is likely to overfit. One can construct simple examples (cf. Bubeck and von Luxburg, 2007) which show that this indeed can happen: here the partitions constructed on the finite sample do not converge to the true clustering of the data space. In practice, for most cases the discrete optimization approach cannot be performed perfectly as the corresponding optimization problem is NP hard. Instead, people resort to heuristics. One approach is to use local optimization procedures potentially ending in local minima only (this is what happens in the k -means algorithm). Another approach is to construct a relaxation of the original problem which can be solved efficiently (spectral clustering is an example for this). In both cases, one usually cannot guarantee how close the heuristic solution is to the global finite sample optimum. This situation is clearly unsatisfactory: for most clustering algorithms, we neither have guarantees on the finite sample behavior of the algorithm, nor on its statistical consistency in the limit.
 The following alternative approach looks much more promising. Instead of attempting to solve the discrete optimization problem over the set of all partitions, and then resorting to relaxations due to the NP-hardness of this problem, we turn the tables. Directly from the outset, we only consider candidate partitions in some restricted class F n containing only polynomially many functions. Then the discrete optimization problem of minimizing Q n over F n is no longer NP hard  X  it can trivially be solved in polynomially many steps by trying all candidates in F n . From a theoretical point of view this approach has the advantage that the resulting clustering algorithm has the potential of being consistent. In addition, it also leads to practical benefits: rather than dealing with uncontrolled relaxations of the original problem, we restrict the function class to some small enough subset F n of  X  X easonable X  partitions. Within this subset, we then have complete control over the solution of the optimization problem and can find the global optimum. Put another way, one can also interpret this approach as some controlled way of sparsifying the NP hard optimization problem, with the positive side effect of obeying the rules of statistical learning theory. In the following we assume that we are given a set of data points X n = { X 1 , . . . , X n } and pairwise distances d ij = d ( X i , X j ) or pairwise similarities s ij = s ( X i , X j ) . Let Q n be the finite sample quality function to optimize on the sample. To follow the approach outlined above we have to optimize Q n over a  X  X mall X  set F n of partitions of X n . Essentially, we have three requirements on F n : First, the number of functions in F n should be at most polynomial in n . Second, in the limit of n  X   X  the class F n should be rich enough to approximate any measurable partition of the underlying space. Third, in order to perform the optimization we need to be able to enumerate all members of this class, that is the function class F n should be  X  X onstructive X  in some sense. A convenient choice satisfying all those properties is the class of  X  X earest neighbor partitions X . This class contains all functions which can be generated as follows. Fix a subset of m n  X  X eed points X  X s 1 , . . . , X s m among the given data points. Assign all other data points to their closest seed points, that is for all j = 1 , . . . , m define the set Z j as the subset of data points whose nearest seed point is X s j . Then consider all partitions of X n which are constant on the sets Z j . More formally, for given seeds we define the set F n as the set of all functions f : X  X  X  1 , . . . , K } which are constant on the cells of the Voronoi partition induced by the seeds. Here K denotes the number of clusters we want to construct. The function class F n contains K m functions, which is polynomial in n if the number m of seeds satisfies m = O (log n ) . Given F n , the simplest polynomial-time optimization algorithm is then to evaluate Q n ( f ) for all f  X  X  n and choose the solution f n = argmin f  X  X  the resulting clustering the nearest neighbor clustering and denote it by NNC( Q n ) . In practice, the seeds will be chosen randomly among the given data points. In this section we prove that nearest neighbor clustering is statistically consistent for many clustering quality functions. Due to the complexity of the proofs and the page restriction we can only present sketches of the proofs. All details can be found in von Luxburg et al. (2007). Let us start with some notation. For any clustering function f : R d  X  { 1 , . . . , K } we denote by the predicate A ( f ) a property of the function which can either be true or false. As an example, define A ( f ) to be true if all clusters have at least a certain minimal size. Moreover, we need to introduce a predicate A n ( f ) which will be an  X  X stimator X  of A ( f ) based on the finite sample only. Let m := m ( n )  X  n be the number of seeds used in nearest neighbor clustering. To simplify notation we assume in this section that the seeds are the first m data points; all results remain valid for any other (even random) choice of seeds. As data space we use X = R d . We define: F := { f : R d  X  X  1 , . . . , K }| f continuous P -a.e. and A ( f ) true } F e F Furthermore, let Q : F  X  R be the quality function we aim to minimize, and Q n : F n  X  R an estimator of this quality function on a finite sample. With this notation, the true clustering f  X  on the underlying space and the nearest neighbor clustering f n introduced in the last section are given by Later on we will also need to work with the functions As distance function between different clusterings f, g we will use (we need the conditioning in case f or g depend on the data, it has no effect otherwise). Theorem 1 (Consistency of nearest neighbor clustering) Let ( X i ) i  X  N be a sequence of points drawn i.i.d. according to some probability measure P on R d , and m := m ( n ) the number of seed points used in nearest neighbor clustering. Let Q : F  X  R be a clustering quality function, Q Then nearest neighbor clustering as introduced in Section 2 is weakly consistent, that is Q ( f n )  X  Q ( f  X  ) in probability .
 Proof. (Sketch, for details see von Luxburg et al. (2007)). We split the term P ( | Q ( f n )  X  Q ( f  X  ) | X   X  ) consequence of Condition (2) that the first term converges to 0. The main work consists in bounding the second term. As usual we consider the estimation and approximation errors First we bound the estimation error. In a few lines one can show that Note that even though the right hand side resembles the standard quantities often considered in statistical learning theory, it is not straightforward to bound as we do not assume that Q ( f ) = E
Q n ( f ) . Moreover, note that the function class F n is data dependent as the seed points used in the Voronoi partition are data points. To circumvent this problem, we replace the function class F n by the larger class e F n , which is not data dependent. Using symmetrization by a ghost sample (cf. Section 12.3 of Devroye et al., 1996), we then move the supremum out of the probability:
P sup Note that the unusual denominator in Eq. (1) emerges in the symmetrization step as we do not assume Q ( f ) = E Q n ( f ) . The quantity S K ( e F n , 2 n ) denotes the shattering coefficient, that is the maximum number of ways that 2 n points can be partitioned into K sets using the functions in e F n . It is well known (e.g., Section 21.5 of Devroye et al., 1996) that the number of Voronoi partitions of n points using m cells in R d is bounded by n ( d +1) m 2 , hence the number of nearest neighbor clusterings into K classes is bounded by S K ( e F n , n )  X  K m n ( d +1) m 2 . Under Condition (1) of the Theorem we now see that for fixed  X  and n  X  X  X  the right hand side of (1) converges to 0. Thus the same holds for the estimation error. To deal with the approximation error, observe that if A n ( e f  X  ) is true, then e f  X   X  X  n , and by the definition of f  X  n we have The first expression on the right hand side converges to 0 by Condition (2) in the theorem. Using Condition (3), we can bound the second expression in terms of the distance L n to obtain Now we use techniques from Fritz (1975) to show that if n is large enough, then the distance between a function f  X  F evaluated at x and the same function evaluated at NN m ( x ) is small. Namely, for any f  X  X  and any  X  &gt; 0 there exists some b (  X  (  X  )) &gt; 0 which does not depend on n and f such that The quantity  X  (  X  ) has been introduced in Condition (3). For every fixed  X  , this term converges to 0 due to Condition (4), thus the approximation error vanishes. , Now we want to apply our general theorem to particular objective functions. We start with the normalized cut. Let s : R d  X  R d  X  R + be a similarity function which is upper bounded by a constant k -th cluster. Define the empirical and true cut, volume, and normalized cut as follows: cut( f k ) := E X,Y f k ( X )(1  X  f k ( Y )) s ( X, Y ) Note that E Ncut n ( f ) 6 = Ncut( f ) , but E cut n ( f ) = cut( f ) and E vol n ( f ) = vol( f ) . We fix a constant a &gt; 0 , a sequence ( a n ) n  X  N with a n  X  a n +1 and a n  X  a and define the predicates Theorem 2 (Consistency of NNC(Ncut n ) ) Let ( X i ) i  X  N be a sequence of points drawn i.i.d. ac-cording to some probability measure P on R d and s : R d  X  R d  X  R + be a similarity function which is upper bounded by a constant C . Let m := m ( n ) be the number of seed points used in nearest neighbor clustering, a &gt; 0 an arbitrary constant, and ( a n ) n  X  N a monotonically decreasing se-quence with a n  X  a . Then nearest neighbor clustering using Q := Ncut , Q n := Ncut n , and A and A n as defined in (3) is weakly consistent if m ( n )  X  X  X  and m 2 log n/ ( n ( a  X  a n ) 2 )  X  0 . Proof. We will check that all conditions of Theorem 1 are satisfied. First we establish that {| cut n ( f k )  X  cut( f k ) | X  a X  } X  X | vol n ( f k )  X  vol( f k ) | X  a X  } X  X | Applying the McDiarmid inequality to cut n and vol n , respectively, we obtain that for all f  X  e F n Together with m 2 log n/ ( n ( a  X  a n ) 2 )  X  0 this shows Condition (1) of Theorem 1. The proof of Condition (2) is rather technical, but in the end also follows by applying the McDiarmid inequality to vol n ( f ) . Condition (3) follows by establishing that for f  X  X  and g  X  X  n we have In fact, Theorem 1 can be applied to a large variety of clustering objective functions. As examples, consider ratio cut, within-sum of squares, and the ratio of between-and within-cluster similarity: Here n k := P i f k ( X i ) /n is the fraction of points in the k -th cluster, and c k,n := P i f k ( X i ) X i / ( nn k ) and c k := Theorem 3 (Consistency of NNC(RatioCut n ) , NNC(WSS n ) , and NNC(BW n ) ) Let f n and f  X  be the empirical and true minimizers of nearest neighbor clustering using RatioCut n , WSS n , or BW n , respectively. Then, under conditions similar to the ones in Theorem 2, we have RatioCut( f n )  X  RatioCut( f  X  ) , WSS( f n )  X  WSS( f  X  ) , and BW( f n )  X  BW( f  X  ) in probabil-ity. See von Luxburg et al. (2007) for details. It is an obvious question how nearest neighbor clustering can be implemented in a more efficient way than simply trying all functions in F n . Promising candidates are branch and bound methods. They are guaranteed to achieve an optimal solution, but in most cases are much more efficient than a naive implementation. As an example we introduce a branch and bound algorithm for solving NNC(Ncut) for K = 2 clusters. For background reading see Brusco and Stahl (2005). First of all, observe that minimizing Ncut n over the nearest neighbor function set F n is the same as minimizing Ncut m over all partitions of a contracted data set consisting of m  X  X uper-points X  Z 1 , . . . , Z m (super-point Z i contains all data points assigned to the i -th seed point), endowed with the  X  X uper-similarity X  function  X  s ( Z s , Z t ) := P X with n points can be performed by directly optimizing Ncut on the contracted data set consisting of only m super-points. Assume we already determined the labels l 1 , . . . , l i  X  1  X  X  X  1 } of the first i  X  1 set V := A  X  B of all points. By default we label all points in B with  X  1 and, in recursion level i , decide about moving Z i to cluster +1 . Analogously to the notation f k of the previous section, in case K =2 we can decompose Ncut( f ) = cut( f +1 )  X  (1 / vol( f +1 ) + 1 / vol( f  X  1 )) ; we call the first term the  X  X ut term X  and the second term the  X  X olume term X . As it is standard in branch and bound we have to investigate whether the  X  X ranch X  of clusterings with the specific fixed labels on A could contain a solution which is better than all the previously considered solutions. We use two criteria for this purpose. The first one is very simple: assigning at least one point in B to +1 can only lead to an improvement if this either decreases the cut term or the volume term of Ncut . Necessary If neither is satisfied, we retract. The second criterion involves a lower bound  X  l on the Ncut value of Figure 1: Branch and bound algorithm for NNC(Ncut) for K = 2 . The algorithm is initially called with the super-similarity matrix  X  S , i = 2 , f = (+1 ,  X  1 , . . . ,  X  1) , and  X  u the Ncut value of f . all solutions in the current branch. It compares  X  l to an upper bound  X  u on the optimal Ncut value, namely to the Ncut value of the best function we have seen so far. If  X  l  X   X  u then no improvement is possible by any clustering in the current branch of the tree, and we retract. To compute  X  l , assume we assign a non-empty set B +  X  B to label +1 and the remaining set B  X  = B \ B + to label -1. Using the conventions  X  s ( A, B ) = P Z The volume term can be maximally decreased in case vol( A + ) &lt; vol( V ) / 2 , when choosing B + of the volume term is unavoidable; this increase is minimal when we move one vertex only to A + : Combining both bounds we can now define the lower bound  X  l as the product of Eq. (4) and (5). The entire algorithm is presented in Fig. 1. On top of the basic algorithm one can apply various heuristics to improve the retraction behavior and thus the average running time of the algorithm. For example, in our experience it is of advantage to sort the super-points by decreasing degree, and from one recursion level to the next one alternate between first visiting branch g i = 1 and g i =  X  1 . The main point about nearest neighbor clustering is its statistical consistency: for large n it reveals an approximately correct clustering. In this section we want to show that it also behaves reason-ably on smaller samples. Given an objective function Q n (such as WSS or Ncut ) we compare the NNC results to heuristics designed to optimize Q n directly (such as k -means or spectral clustering). As numeric data sets we used classification benchmark data sets from different repositories (UCI repository, repository by G. R  X  atsch) and microarray data from Spellman et al. (1998). Moreover, we use graph data sets of the internet graph and of biological, social, and political networks: COSIN collection, collection by M. Newman, email data by Guimer ` a et al. (2003), electrical power network by Watts and Strogatz (1998), and protein interaction networks of Jeong et al. (2001) and Tsuda et al. (2005). Due to space constraints we focus on the case of constructing K = 2 clusters using the objective functions WSS and Ncut . We always set the number m of seed points for NNC to m = log n . In case of WSS, we compare the result of the k -means algorithm to the result of NNC using the WSS objective function and the Euclidean distance to assign data points to seed points. Table 1: Left: Numeric data. Results for K -means algorithm, NNC(WSS) with Euclidean distance; spectral clustering (SC); NNC(Ncut) with commute distance. The top line always shows the results on the training set, the second line the extended results on the test set. Right: Network data. NNC(Ncut) with commute distance and spectral clustering, both trained on the entire graph. Note that one cannot run K -means on pure network data, which does not provide coordinates. In case of Ncut , we use the Gaussian kernel as similarity function on the numeric data sets. The kernel width  X  is set to the mean distance of a data point to its k -th nearest neighbor. We then build the k -nearest neighbor graph (both times using k =ln n ). On the network data, we directly use the given graph. For both types of data, we use the commute distance on the graph (e.g., Gutman and Xiao, 2004) as distance function to determine the nearest seed points for NNC.
 In the first experiment we compare the values obtained by the different algorithms on the training sets. From the numeric data sets we generated z = 40 training sets by subsampling n/ 2 points. On each training set, we repeated all algorithms r = 50 times with different random initializations (the seeds in NNC; the centers in K -means; the centers in the K -means post-processing step in spectral clustering). Denoting the quality of an individual run of the algorithm by q , we then report the values mean z ( min r q )  X  standarddev z ( min r q ) . For the network data sets we ran spectral clustering and NNC on the whole graph. Again we use r =50 different initializations, and we report min r q . All results can be found in Table 1. For both the numeric data sets (left table, top lines) and the network data sets (right table) we see that the training performance of NNC is comparable to the other algorithms. This is what we had hoped, and we find it remarkable as NNC is in fact a very simple clustering algorithm.
 In the second experiment we try to measure the amount of overfitting induced by the different algo-rithms. For each of the numeric data sets we cluster n/ 2 points, extend the clustering to the other n/ 2 points, and then compute the objective function on the test set. For the extensions we proceed in a greedy way: for each test point, we add this test point to the training set and then give it the label +1 or -1 that leads to the smaller quality value on the augmented training set. We also tried several other extensions suggested in the literature, but the results did not differ much. To compute the test error, we then evaluate the quality function on the test set labeled according to the exten-sion. For Ncut, we do this based on the k -nearest neighbor graph on the test set only. Note that this experiment does not make sense on the network data, as there is no default procedure to construct the subgraphs for training and testing. The results on the numeric data sets are reported in Table 1 (left table, bottom lines). We see that NNC performs roughly comparably to the other algorithms. This is not really what we wanted to obtain, our hope was that NNC obtains better test values as it is less prone to overfitting. The most likely explanation is that both K -means and spectral clustering have already reasonably good extension properties. This can be due to the fact that as NNC, both algorithms consider only a certain subclass of all partitions: Voronoi partitions for K -means, and partitions induced by eigenvectors for spectral clustering. See below for more discussion. In this paper we investigate clustering algorithms which minimize quality functions. Our main point is that, as soon as we require statistical consistency, we have to work with  X  X mall X  function classes F n . If we even choose F n to be polynomial, then all problems due to NP hardness of discrete op-timization problems formally disappear as the remaining optimization problems become inherently polynomial. From a practical point of view, the approach of using a restricted function class F n can be seen as a more controlled way of simplifying NP hard optimization problems than the standard approaches of local optimization or relaxation. Carefully choosing the function class F n such that overly complex target functions are excluded, we can guarantee to pick the best out of all remaining target functions. This strategy circumvents the problem that solutions of local optimization or relax-ation heuristics can be arbitrarily far away from the optimal solution.
 The generic clustering algorithm we studied in this article is nearest neighbor clustering, which pro-duces clusterings that are constant on small local neighborhoods. We have proved that this algorithm is statistically consistent for a large variety of popular clustering objective functions. Thus, as op-posed to other clustering algorithms such as the K -means algorithm or spectral clustering, nearest neighbor clustering is guaranteed to converge to a minimizer of the true global optimum on the underlying space. This statement is much stronger than the results already known for K -means or spectral clustering. For K -means it has been proved that the global minimizer of the WSS objec-tive function on the sample converges to a global minimizer on the underlying space (e.g., Pollard, 1981). However, as the standard K -means algorithm only discovers a local optimum on the discrete sample, this result does not apply to the algorithm used in practice. A related effect happens for spectral clustering, which is a relaxation attempting to minimize Ncut (see von Luxburg (2007) for a tutorial). It has been shown that under certain conditions the solution of the relaxed problem on the finite sample converges to some limit clustering (e.g., von Luxburg et al., to appear). However, it has been conjectured that this limit clustering is not necessarily the optimizer of the Ncut objec-tive function. So for both cases, our consistency results represent an improvement: our algorithm provably converges to the true limit minimizer of K -means or Ncut , respectively. The same result also holds for a large number of alternative objective functions used for clustering.

