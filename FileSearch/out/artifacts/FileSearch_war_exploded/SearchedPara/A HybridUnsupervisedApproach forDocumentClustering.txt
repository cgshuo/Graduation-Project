 We prop ose a hybrid, unsup ervised documen t clustering ap-proac h that com bines a hierarc hical clustering algorithm with Exp ectation Maximization. We dev elop ed sev eral heuris-tics to automatically select a subset of the clusters generated by the rst algorithm as the initial points of the second one. Furthermore, our initialization algorithm generates not only an initial mo del for the iterativ e re nemen t algorithm but also an estimate of the mo del dimension, thus eliminating another imp ortan t elemen t of human sup ervision. We have evaluated the prop osed system on ve real-w orld documen t collections. The results sho w that our approac h generates clustering solutions of higher qualit y than both its individual comp onen ts.
 Categories and Subject Descriptors: H.3.3: Clustering General Terms: Algorithms Keyw ords: Unsup ervised clustering, EM initialization
The work presen ted in this pap er is motiv ated by researc h into text mining and classi cation from large, real-w orld documen t collections. As the amoun t of available data be-comes virtually unlimited, man ual or sup ervised mining ap-proac hes become prohibitiv ely exp ensiv e due to the limited reading and pro cessing speed of the human exp erts. For this reason, we concen trate our researc h only on unsup ervised metho ds. From the larger eld of text mining and classi -cation, this pap er focuses on documen t clustering. Cluster-ing, loosely de ned as the grouping of similar data items, is the keystone of data classi cation. Follo wing our creed, we focus on unsup ervised clustering techniques that do not require lab eled data or human feedbac k.

From the vast arra y of clustering metho ds, iterativ e re-nemen t clustering techniques are extremely popular due to their good performance, relativ e simplicit y, and good the-oretical foundations. By and large the most popular iter-ativ e re nemen t clustering algorithm is Exp ectation Max-imization (EM) [2]. EM iterativ ely: (a) assigns mem ber-ship probabilities for all data items and all clusters, and (b) re-estimates its mo del parameters based on the new assign-men ts.

The EM class of clustering algorithms are not problem free. Lik e all clustering algorithms, they rely on outside sour ces to provide the expected numb er of clusters, k . Hav-ing the human domain exp ert pro vide this information is not feasible when dealing with large documen t collections con taining new, poten tially unkno wn data. Hence, we fo-cus only on automated, unsup ervised metho ds for the es-timation of k . The most popular probabilistic metho d to determine the dimensions of a given mo del is the Bayes In-formation Criterion (BIC) [9]. From all possible mo del di-mensions, BIC chooses the one that maximizes the mo del log likeliho od function. Calinski and Harabasz [1] prop osed maximizing the ratio of inter (or betwe en ) and intra (or within ) cluster distances as a means for estimating k . This last metho d was empirically sho wn to be the most ecien t [5]. However, unless the data is part of a certain family of dis-tributions, all these metho ds are not consisten t. A second problem that is speci c to iterativ e re nemen t clustering al-gorithms is the choic e of the initial model parameters . Being part of the hill-clim bing family of algorithms, iterativ e re-nemen t algorithms con verge to local maxima, whic h may be far from the global maxim um if the choice of the initial point is poor. De facto solutions in most implemen tations of iterativ e re nemen t clustering algorithms use random or user-c hosen starting points [3, 7]. Comparativ e studies indi-cate that other, more complex, initialization metho ds sho w no impro vemen t over random initialization [4].

The work presen ted in this pap er addresses all previously men tioned issues that plague EM and in general iterativ e re nemen t clustering algorithms with the follo wing adv an-tages: 1. It is an integrated metho d that detects both the num-2. It consisten tly outp erforms its constituen ts individu-The pap er is organized as follo ws. Section 2 overviews the prop osed metho d for the selection of the initial mo del for EM. Section 3 introduces sev eral measures that indicate the qualit y of the initial points. These measures are the key of the initialization algorithm. In Section 4 we evaluate the performance of the initialization algorithm and the perfor-mance of a complete clustering solution based on EM. We conclude in Section 5.
Intuitiv ely, the prop osed approac h searc hes through the space of all possible documen t clusters for the best initial mo del for an EM algorithm. Obviously , a direct implemen-tation of this idea is computationally intractable. To reduce the computational overhead of the metho dology we limit the searc h to the clusters con tained in a hierarc hical clustering solution, or dendrogram [10]. Figure 1 overviews the pro-posed clustering metho d.

First, we use a hierarc hical algorithm to generate the col-lection dendrogram. We detail the hierarc hical algorithm used in Section 2.2. From the dendrogram clusters, the next comp onen t generates the candidates for the EM ini-tial mo del as follo ws: 1. All dendrogram clusters are sorted in descending order 2. The top clusters that pro vide a coverage of less or equal 3. The clusters selected in the previous step are post-The above approac h leaves sev eral imp ortan t questions unan-swered: given that multiple possible qualit y measures exist, whic h one yields the best initial mo del? Furthermore, what is the best collection coverage ( ) to be used in an initial mo del? Both questions are answ ered by the next comp o-nen t, whic h selects the best initial mo del from all the gener-ated candidates. The initial mo del candidates are pro duced for all possible qualit y measures (see Section 3) and sev-eral values. The selection algorithm uses a hill-clim bing algorithm to select the mo del that maximizes some global qualit y function. This algorithm is detailed in Section 2.3.
This initialization approac h follo ws our intuition that the \best" clusters pro vided by a hierarc hical clustering approac h crystallize well the categories hidden in a documen t collec-tion, and the iterativ e algorithm is capable of both re ning poten tially incorrect assignmen ts and assigning the missing documen ts. The remaining of this section details the two clustering algorithms used (hierarc hical and EM), and the initial mo del selection algorithm.
The hybrid clustering metho d illustrated here can poten-tially use dendrograms pro duced by any hierarc hical clus-tering algorithm. Due to space limitations, in this pap er we focus only on hierarc hical agglomerativ e clustering (HA C), a simple algorithm previously rep orted to have good perfor-mance on real-w orld collections [10].

The HA C algorithm uses a bottom-up approac h to build the dendrogram: rst, it assigns eac h documen t to its own cluster, and then it rep eatedly merges the two \closest" clus-ters [10]. The key parameter in HA C is the metho d used to measure the inter-cluster distance. We have used the UP-GMA, or group average, distance, whic h we previously found to yield the best performance in the HA C con text: where n i indicates the num ber of documen ts in cluster c n j indicates the num ber of documen ts in cluster c j , and dist ( d r ; d s ) = 1 cos ( d r ; d s ), where cos ( d r between the two documen ts' tf-idf -weigh ted term vectors.
An imp ortan t part of the prop osed system is the selection of the \best" initial mo del from the set of candidates. We have opted for an approac h similar to the work of Calin-ski and Harabasz [1]. Intuitiv ely, Calinski and Harabasz choose the mo del that locally maximizes a normalized ratio of betwe en distanc es (i.e. distances betwe en di eren t clus-ters) and within distanc es (i.e. distances between documen ts within the same cluster) computed for the complete cluster-ing mo del. In other words, a good initial mo del will con tain tigh t clusters that are well separated from eac h other. For-mally , Calinski and Harabasz compute the score C of a given clustering mo del with dimension k as follo ws: where n is the size of the mo del, n i is the size of the i th cluster, centr oid i is the comp osite vector of cluster i , and meta centr oid is the comp osite vector of the whole collec-tion. B and W are the unnormalized betwe en and within distances for the complete clustering mo del.

One issue that is not captured in Equation 2 is that we prefer larger size initial mo dels, in order to avoid trivial cases suc h as mo dels that con tain clusters with few orthogonal documen ts. Suc h clusters are likely to be outliers, whic h are obviously not good initial points for any iterativ e re nemen t clustering algorithm. This preference for larger size mo dels is captured by the actual selection algorithm, whic h searc hes for the rst local maxim um of C as the collection coverage decreases from 100% to 0%. The algorithm is describ ed below: 1. bestM odel = , bestS cor e = 0. 2. For all qualit y measures: 3. Return bestMo del .

The best initial mo del generated from the selected and qualit y measure inheren tly con tains the mo del dimension k and initial points for eac h of the iden ti ed categories.
The EM algorithm nds maxim um likeliho od estimates of its parameters using probabilistic mo dels over incomplete data. EM was theoretically pro ven to con verge to a local maxim um of the parameters' log likeliho od. The EM al-gorithm used in this pap er estimates its mo del parameters using the Naiv e Bayes (NB) assumptions, similar to [6]. The algorithm has the follo wing structure: 1. Initialization: the mo del parameters are estimated us-2. E step: the NB classi er is used to assign probabilistically-3. M step: new mo del parameters, ^ , are estimated using 4. Rep eat the E and M steps until con vergence to a local In our con text, i.e. documen t clustering, the parameters of the generativ e NB mo del, ^ , include the probabilit y of seeing a given category , P ( c j ^ ), and the probabilit y of seeing a word given a category , P ( w j c ; ^ ). In the initialization and maximization steps, these parameters are estimated using Laplace smo othing:
P ( w j c ; ^ ) = where P ( y i = c j d i ) is 1 if the lab el y i of documen t d and 0 otherwise; T F ( w j ; d i ) is the term frequency of word w j in documen t d i ; q is the num ber of categories in the col-lection; n is the num ber of documen ts; and v is the collection vocabulary size.

Using these parameters and the word indep endence as-sumption typical to the Naiv e Bayes mo del, the probabilit y of a documen t d given a category c is estimated as: Using Equation 7 and the Bayes rule, in the exp ectation step the probabilit y that a documen t d has a given category c is calculated as:
The key comp onen t of our clustering approac h is the bat-tery of qualit y measures used to generate initial mo del can-didates from the dendrogram pro duced by the hierarc hical clustering algorithm. We have dev elop ed a set of qualit y form ulas that com bine the follo wing observ ations: Observ ation 1: On minimizing within distances.
 The initial points for the iterativ e re nemen t algorithm should con tain documen ts from only one category . Since docu-men ts in the same category are conceptually closer than documen ts from di eren t categories, a good initial point will have small pairwise distances between documen ts within the corresp onding cluster. Equation 10 introduces W ( c i ), the average of the pairwise distances between documen ts within a cluster c i : where dist ( d r ; d s ) is computed as in Equation 1. Based on the above observ ation, the qualit y measure should favor clus-ters with small W values.
 Observ ation 2: On maximizing betwe en distances.
 The clusters selected to generate the initial parameters for iterativ e clustering should con tain as many documents from one category as possible . Therefore, since a category should be well separated from the other categories in the data, a good initial point will have large distances betwe en its docu-men ts and the rest of the collection. We mo del the distance B ( c i ) between the cluster c i and the rest of the collection as the average of the pairwise distances between documen ts within and outside of cluster c i : The qualit y measure should favor documen ts with large B values.
 Observ ation 3: On maximizing betwe en distances in the neighb orho od .
 Com binations of the W and B measures have been previ-ously used as clustering criterion functions and as a crite-rion for detecting the best mo del dimension k . However, when using W and B for a post-clustering ltering function, sev eral subtleties arise: (i) most clusters will have large B values because all clustering criteria maximize inter-cluster distances, and (ii) W values will have a large variation be-cause the dendrogram includes clusters of all sizes. In this case, cluster comparison functions based on B and W are decided mainly by the W measure.

Nev ertheless, a betwe en measure computed in the cluster vicinity indicates the degree of separation between a clus-ter and only its neigh bors, without the \noise" introduced by the collection mass. We appro ximate the neigh borho od betwe en distance of a cluster c i as the UPGMA distance be-tween the cluster and its dendrogram sibling: Observ ation 4: On minimizing the cluster \growth". Filtering functions based on the W and B qualit y measures have two poten tial dra wbac ks: (i) they will favor small clus-ters, whic h are more compact and better separated from the rest of the collection, and (ii) they will prefer categories represen ted by denser clusters. In the rst case the sys-tem will generate man y categories with smaller coverage. In the second situation the system will miss the informa-tion con tained in the ignored categories. Suc h situations are frequen t in real-w orld documen t collections, whic h include clusters with di eren t densities. Giv en these observ ations, it becomes imp erativ e to explore other cluster prop erties that are indep enden t of the cluster densit y. One suc h prop erty is the cluster growth , G , de ned as the cluster expansion at the last dendrogram join, relative to the internal densit y of its two children. Formally , G ( c i ) is the ratio of the distance betwe en the cluster's two children c i 1 and c i 2 and the aver-age of the pairwise distances between documen ts within the two children: within chil dren ( c i ) = w sum ( c i 1 where w sum ( c i ) is the unnormalized sum of all distances between objects within a cluster c i , and within chil dren ( c is the average distance between objects belonging to the same child of cluster c i . Intuitiv ely, a good initial point will have a small gro wth factor. Large variations of the gro wth factor indicate that the corresp onding cluster is comp osed of two relativ ely distan t children clusters, whic h happ ens when two di eren t categories are joined, close to the top of the dendrogram tree.

Using the observ ations and the distances previously in-troduced in this section we have deriv ed 6 qualit y measures, listed in Table 1. The qualit y measures are generated in a straigh tforw ard manner: we multiply the form ulas that should be maximized ( B or N ) with the inverse of the for-mulas that should be minimized ( W or G ).
 Table 2: Documen t collections used in the evalua-tion
We used ve documen t collections in the evaluation of the prop osed hybrid clustering algorithm. The AP collection is the Asso ciated Press (year 1999) subset of the AQUAINT collection. The documen ts' category assignmen t is indicated by a CA TEGOR Y tag. The LATIMES collection is the Los Angeles Times subset of the TREC-5 collection. The cate-gories corresp ond to the newspap er desk that generated the article [11]. The REUTERS collection is the by now classic Reuters-21578 text categorization collection [8]. Similar to previous work we used the Mo dApte split [6], but, since our algorithm is unsup ervised, we use the test partition directly . The REUTERS-T op10 collection is a subset of the above Mo dApte test partition that includes only the ten most fre-quen t categories [6]. The SMAR T collection was previously dev elop ed and used for the evaluation of the SMAR T infor-mation retriev al system.

Due to memory limitations on our test mac hines, we re-duced the size of the AP and LATIMES collections to the rst 5,000 documen ts (the complete collections con tain over 100,000 documen ts). The collection documen ts were pre-pro cessed as follo ws: (i) stop words and num bers were dis-carded; (ii) all words were con verted to lower case; and (iii) terms that app ear in a single documen t were remo ved [10, 11]. Table 2 lists the collection characteristics after pre-pro cessing. In the two Reuters collections, the assignmen t of documen ts to categories is ambiguous: the mean num ber of categories assigned to a documen t is 1.2 in the REUTERS collection and 1.1 in the REUTERS-T op10 collection.
The qualit y of the clustering solutions was measured using two evaluation measures, purit y and entrop y, widely used to evaluate the performance of unsup ervised clustering algo-rithms [11, 10].

The purity measure evaluates the degree to whic h eac h cluster con tains documen ts from a single category . The over-all purit y is the weigh ted average of all cluster purities: Table 3: Comparison of three clustering algorithms: the hybrid approac h (Hybrid), HAC, and EM with random initialization averaged over 5 runs (EM5) where n j i represen ts the num ber of documen ts from cluster c assigned to category j . Intuitiv ely, the larger the purit y value, the better the clustering solution is.

The second evaluation measure used is the entr opy mea-sure, whic h analyzes the distribution of categories in eac h cluster. The entrop y E of a cluster c i is de ned as: where q represen ts the total num ber of categories in the collection [11]. The overall entrop y is the weigh ted average of all cluster entropies: Because the entrop y measures the amoun t of disorder in a system, the smaller the entrop y value, the better the clus-tering solution is.

Lastly , for the unsup ervised initialization algorithm we rep ort the estimated num ber of clusters k . The closer k is to the num ber of categories in the collection, q , the better the initialization algorithm is. Nev ertheless, in the situa-tions when k 6 = q , we prefer solutions with larger k , because this increases the num ber of true categories reco vered in the clustering solution.
In this section we evaluate the performance of the pro-posed hybrid clustering system. We compare it against its two constituen t algorithms: EM and HA C. For the stand-alone EM we used random initialization and we have aver-aged the results across ve runs (EM5) 1 . Because EM5 and HA C require the num ber of clusters k as an input parame-ter, for both of them we have used the num ber of clusters k detected by the corresp onding instance of the hybrid system.
Table 3 lists the results for the three algorithms and the ve test collections. The hybrid approac h clearly outp er-forms the other two algorithms in all the test collections. 1 The evaluation data is available at: Table 4: Num ber of clusters estimated by the hybrid approac h and the Calinski metho d These results are a strong indication that the qualit y mea-sures used to generate initial mo del candidates and the sub-sequen t selection pro cess are successful.

The qualit y of the clustering solutions generated by the hybrid system is more than satisfactory considering that the prop osed approac h is completely unsup ervise d , including rel-ativ e to the mo del dimension. We analyze the qualit y of the mo del dimensions detected by the hybrid system in the next section and we pro vide a more detailed analysis of the overall system beha vior in Section 4.5.
While the previous section pro ves that the hybrid ap-proac h pro vides better clustering solutions than both HA C and EM with random initialization, one question remains unansw ered: how does our approac h compare against other unsup ervised metho ds for the estimation of the mo del di-mension k ? To answ er this question we have implemen ted the metho d prop osed by Calinski and Harabasz [1], pre-viously rep orted to be the best among a num ber of other unsup ervised algorithms [5].

Calinski and Harabasz generate clustering mo dels using a range of mo del dimensions k and choose the k that lo-cally maximizes Equation 2. To guaran tee that the Calin-ski metho d is directly comparable with our metho d we have used the same clustering algorithm, HA C, for both approac hes. For a larger exp erimen t we have also evaluated Calinski's metho d when its clustering mo dels are generated using EM with random initialization. Similarly with the previous sec-tion, for EM we have averaged the C scores (see Equation 2) across ve runs. When it uses HA C to generate its cluster-ing mo dels, Calinski's metho d is somewhat similar to our approac h with a signi can t di erence: Calinski uses cluster-ing mo dels that include all the documen ts in the collection, whereas we use only documen ts that are part of what we consider the best clusters, since we discard ambiguous doc-umen ts that decrease the qualit y of clustering solution.
Table 4 sho ws the num ber of clusters k estimated by our hybrid approac h for the ve evaluation collections, com-pared with the num ber of clusters estimated by the Calinski metho d using the two clustering algorithms, and the actual num ber of categories q . Table 4 indicates that the results obtained by Calinski's metho d using EM with random ini-tialization are fairly unsatisfactory: the mo del dimension is generally over-estimated and there is no relation between the estimated num ber, k , and the actual num ber of categories, q . On the other hand, when using HA C, Calinski's metho d has the tendency to underestimate k . This issue is more visible in collections with a larger num ber of categories, like the two Reuters collections. For the larger REUTERS col-lection, our approac h generates a k equal to 11. Although this value seems relativ ely far away from the actual num-ber of categories (93), we consider it a good result: the top Table 5: Qualit y measures and collection coverages detected by the unsup ervised initial model selection comp onen t 11 categories in REUTERS cover over 94% of the collection documen ts.
Underestimating the mo del dimension can be a serious dra wbac k for a real-w orld system because it reduces the num ber of categories reco vered in the clustering solution. Poten tially imp ortan t categories may simply be disp ersed among existing clusters instead of getting a cluster of their own, whic h mak es them virtually invisible to the human an-alyst that bro wses the generated clustering solution. We believ e that Calinski's approac h su ers more from this issue because the clustering mo dels used cover all the collection documen ts, including ambiguous documen ts that minimize the distance between distinct (but close) categories. On the other hand, our metho d works only with documen ts that are part of the \best" clusters, i.e. dense clusters that are well separated from the rest of the collection. We have sho wn that this approac h is bene cial for both the detection of the mo del dimension (see Section 4.4) and for the selection of an initial mo del for EM (see Section 4.3).

One question that remains unansw ered is: what is the best performance we can exp ect from suc h an unsup ervised system? To answ er this question we insp ected the qualit y of the clustering solutions generated from all the initial mo del candidates (for all collection coverages and for all qual-ity measures). We found that the upp er limits for purit y ranged from 75% (in LATIMES and REUTERS) to 92% (in SMAR T), and the upp er limits for entrop y ranged from 10% (in REUTERS-T op10) to 40% (in LATIMES). These rela-tively high upp er limits on the system performance indicate that the prop osed qualit y measures generate initial cluster-ing mo dels of good qualit y. Furthermore, the performance of our unsup ervised metho d is, on the average, in the top 14% of all candidate mo dels' purities and top 21% of all en-tropies, whic h is a strong indication of the robustness of our approac h. Although these results are certainly encouraging they indicate that the prop osed metho d can be extended at least with a better performing selection comp onen t.
Lastly , Table 5 lists the qualit y measures and collection coverages selected by the unsup ervised system. Table 5 indicates that, if execution time is an imp ortan t concern, a practical optimization of the prop osed metho d that uses only one qualit y measure ( GW ) would still have acceptable qualitativ e performance.
This pap er introduces a hybrid clustering approac h that extracts the initial mo del for an EM algorithm from the den-drogram generated by a hierarc hical clustering algorithm. The initial EM mo del is extracted in a two-step pro cess. First, a num ber of candidate mo dels are generated by sort-ing the dendrogram clusters according to a battery of qualit y measures and extracting the top-rank ed clusters for various collection coverages. Then, the candidate mo del that lo-cally maximizes a global qualit y score is chosen as the initial mo del for the EM algorithm.

The rst novelty of the prop osed metho d is that only the \best" clusters pro duced by the hierarc hical algorithm are selected as the initial mo del of the exp ectation maximiza-tion algorithm. The second novelty is that our initialization algorithm generates not only an initial mo del but also an estimate of the mo del dimension, thus eliminating another imp ortan t elemen t of human sup ervision.

An instance of our clustering algorithm has been empiri-cally evaluated on ve real-w orld documen t collections. The results sho w that our approac h is alw ays sup erior to both EM with random initialization and the hierarc hical agglom-erativ e algorithm. A comparison of our technique for esti-mating the mo del dimension with the Calinski and Harabasz metho d indicate that the latter prefers simpler mo dels, while our approac h tends to generate more complex mo dels, whic h increases the num ber of categories reco vered in the cluster-ing solution.
This work has been partially funded by the Europ ean pro ject CHIL (IP-506808) and the Spanish Ministry of Sci-ence and Technology pro ject TIN2004-0171-E. Mihai Sur-dean u is a researc h fello w within the Ram on y Cajal pro-gram of this ministry .
