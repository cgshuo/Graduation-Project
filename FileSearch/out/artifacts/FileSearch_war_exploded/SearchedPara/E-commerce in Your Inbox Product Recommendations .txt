 In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropri-ate type of advertising for a given platform has become crit-ical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase his-tory determined from e-mail receipts to deliver highly per-sonalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included show-ing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were success-fully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.
 H.2.8 [ Database applications ]: Data Mining Data mining; computational advertising; audience modeling c  X 
Hundreds of millions of people around the world visit their e-mail inboxes daily, mostly to communicate with their con-tacts, although a significant fraction of time is spent check-ing utility bills, reading newsletters, and tracking purchases. To monetize this overwhelming amount of traffic, e-mail clients typically show display ads in a form of images along-side native e-mail content. Convincing users to exit the  X  X -mail mode X , characterized by a relentless focus on the task of dealing with their mail, in order to enter a mode where they are willing to click on ads is a challenging task. Effective personalization and targeting [11], where the goal is to find the best matching ads to be displayed for each individual user, is essential to tackling this problem, as ads need to be highly relevant to overcome user X  X  inclination to focus nar-rowly on the e-mail task. In addition to financial gains for online businesses [26], proactively tailoring advertisements to the tastes of each individual consumer also leads to an improved user experience, and can help with increasing user loyalty and retention [2].

Inbound e-mails are still insufficiently explored and ex-ploited for the purposes of ad targeting, while arguably rep-resenting a treasure trove of monetizable data. A recent study [13] showed that only 10% of inbound volume rep-resents human-generated e-mails. Furthermore, out of the remaining 90% of traffic more than 22% represents e-mails related to online shopping. Given that a significant percent-age of overall traffic has commercial intent, a popular form of targeted advertising is mail retargeting (MRT), where ad-vertisers target users who previously received e-mails from certain commercial web domains. These e-mails present a strong signal useful for targeting campaigns, as they give a broad picture about each customer X  X  interests and relation-ship with commercial domains. A recent paper [14] proposed to make use of this vast potential and presented a clustering method to generate MRT rules, showing that such rules are more accurate than the ones generated by human experts.
However, in order to go beyond MRT rules that utilize only the fact that users and e-commerce sites communi-cated, advertisers require more detailed data such as pur-chased product name and price that are often part of e-mail body. E-mail clients have been working with e-commerce and travel domains to standardize how e-mails are format-ted, resulting in schemas maintained by the schema.org com-munity 1 . With more and more e-commerce sites using stan-dard schemas, e-mail clients can provide more personalized user notifications, such as package tracking 2 and flight de-tails 3 . In addition, e-mail receipt extraction brings mone-tization opportunity through product advertising to users based on their individual purchase history. Availability of purchase data from multiple commercial e-mail domains puts the e-mail provider in the unique position to be able to build better recommendation systems than those based on any one commercial e-mail domain alone. In particular, unlike e-commerce websites that make recommendations of type:  X  X ustomers who bought X also bought Y  X , e-mail providers can make recommendations of type:  X  X ustomers who bought X from vendor V 1 also bought Y from vendor V 2  X , allowing much more powerful and effective targeting solutions.
In this paper we tell the story of an end-to-end develop-ment of product ads for Yahoo Mail. The effort included developing a product-level purchase prediction algorithm, capable of scaling to millions of users and products. To this end, we propose an approach that embeds products into real-valued, low-dimensional vector space using a neural lan-guage model applied to a time series of user purchases. As a result, products with similar contexts (i.e., their surround-ing purchases) are mapped to vectors that are nearby in the embedding space. To be able to make meaningful and di-verse suggestions about the next product to be purchased, we further cluster the product vectors and model transition probabilities between clusters. The closest products in the embedding space from the most probable clusters are used to form final recommendations for a product.

The product prediction model was trained using a large-scale purchase data set, comprising more than 280 million purchases made by 29 million users, involving 2 . 1 million unique products. The model was evaluated on a held-out month, where we tested the effectiveness of recommenda-tions in terms of yield rate. In addition, we evaluated sev-eral baseline approaches, including showing popular prod-ucts to all users, showing popular products in various user groups (called cohorts , specified by user X  X  gender, age, and location), as well as showing products that are historically frequently purchased after the product a user most recently bought. To mitigate the cold start problem, popular prod-ucts in user X  X  cohort were used as back-fill recommendations for users without earlier purchases. http://schema.org/email, accessed June 2015 yahoomail.tumblr.com/post/107247385566/track-your-packages-now-in-the-yahoo-mail-app, accessed June 2015 venturebeat.com/2014/10/22/yahoo-mail-now-tells-you-about-upcoming-flights-and-events, accessed June 2015
Empirical results show that the proposed product-level model is able to more accurately predict purchases than baseline approaches. In the experimental section we also share results of bucket tests on live traffic, where we com-pared the performance of the baselines in the same ad slot. Our method substantially improves key business metrics, measured in terms of clicks and conversions. Moreover, our product-prediction technique was successfully implemented at large scale, tested in live buckets, and finally launched in production. The system is able to make near-real-time prod-uct recommendations with latency of less than 200 ms . In Figure 1 we show an example of our product recommender, where an offer from walmart.com is suggested after related purchase was made on amazon.com (marked with red).
In this section we describe related work in mail-based ad-vertising, as well as neural language models that motivated our product recommendation approach.
Web environment provides content publishers with a means to track user behavior in much greater detail than in an of-fline settings, including capturing user X  X  registered informa-tion and activity logs of user X  X  clicks, page views, searches, website visits, social activities, and interactions with ads. This allows for targeting of users based on their behavior, which is typically referred to as ad targeting [1]. With the rise of big data applications and platforms, machine learning approaches are heavily leveraged to automate the ad target-ing process. Within the past several years, there has been a plethora of research papers that explored different aspects of online advertising, each with the goal of maximizing the benefits for advertisers, content publishers, and users.
For any machine learning technique, features used to train a model typically have a critical influence on the perfor-mance of the deployed algorithm. Features derived from user events collected by publishers are often used in predict-ing the user X  X  propensity to click or purchase [9]. However, these features represent only a weak proxy to what publish-ers and advertisers are actually interested in, namely, user X  X  purchase intent. On the other hand, commercial e-mails in the form of promotions and purchase receipts convey a strong, very direct purchase intent signal that can enable ad-vertisers to reach a high-quality audience. According to the Direct Marketing Association X  X   X  X ational Client E-mail Re-port X  4 , an e-mail user has been identified as having over 10% more value than the average online customer. Despite these www.powerprodirect.com/statistics, accessed June 2015 encouraging facts, limited work has been done to analyze the potential of features derived from commercial e-mails. A re-cent work [14] was among the first attempts to investigate the value of commercial e-mail data. The authors applied Sparse Principal Component Analysis (SPCA) on the counts of received e-mails in order to cluster commercial domains, demonstrating significant promise of e-mail data source.
Outside of the e-mail domain, information about user X  X  purchase history is extensively used by e-commerce websites to recommend relevant products to their users [22]. Recom-mendation systems predict which products a user will most likely be interested in either by exploiting purchase behavior of users with similar interests (referred to as collaborative filtering [22]) or by using user X  X  historical interaction with other products (i.e., context-based recommendation [29]). Unlike these studies, we are not limited to the data from a single website, as purchases extracted from e-mails enable us to gather information from hundreds of different e-commerce websites that can be exploited to learn better product pre-dictions. To the best of our knowledge, this work represents the first study that offers a comprehensive empirical analy-sis and evaluation of product predictors using e-mail data of such scale and nature.
In a number of Natural Language Processing (NLP) appli-cations, including information retrieval, part-of-speech tag-ging, chunking, and many others, specific objectives can all be generalized to the task of assigning a probability value to a sequence of words. To this end, language models have been developed, defining a mathematical model to capture statis-tical properties of words and the dependencies among them [3, 20]. Traditionally, language model approaches represent each word as a feature vector using a one-hot representation, where a word vector has the same length as the size of a vo-cabulary, and the position that corresponds to the observed word is equal to 1, and 0 otherwise. However, this approach often exhibits significant limitations in practical tasks, suf-fering from high dimensionality of the problem and severe data sparsity, resulting in suboptimal performance.
Neural language models have been proposed to address these issues, inducing low-dimensional, distributed embed-dings of words by means of neural networks [5, 8, 28]. Such approaches take advantage of the word order in text docu-ments, explicitly modeling the assumption that closer words in the word sequence are statistically more dependent. His-torically, inefficient training of the neural network-based mod-els has been an obstacle to their wider applicability, given that the vocabulary size may grow to several millions in practical tasks. However, this issue has been successfully addressed by recent advances in the field, particularly with the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models [23, 24] for learning word representations. These powerful, efficient mod-els have shown very promising results in capturing both syn-tactic and semantic relationships between words in large-scale text corpora, obtaining state-of-the-art results on a plethora of NLP tasks. More recently, the concept of dis-tributed representations has been extended beyond word representations to sentences and paragraphs [10, 21], rela-tional entities [6, 27], general text-based attributes [19], de-scriptive text of images [18], nodes in graph structure [25], and other applications.
In this section we describe the proposed methodology for the task of product recommendations, which leverages in-formation about prior purchases determined from e-mail re-ceipts. To address this task we propose to learn represen-tation of products in low-dimensional space from historical logs using neural language models. Product recommenda-tion can then be performed in the learned embedding space through simple nearest neighbor search.

More specifically, given a set S of e-mail receipt logs ob-tained from N users, where user X  X  log s = ( e 1 ,...,e M )  X  S is defined as an uninterupted sequence of M receipts, and each e-mail receipt e m = ( p m 1 ,p m 2 ,...p mT m ) consists of T purchased products, our objective is to find D -dimensional real-valued representation v p  X  R D of each product p such that similar products lie nearby in the vector space.
We propose several approaches for learning product rep-resentations that address specifics of the recommendations from e-mail receipts. We first propose prod2vec method that considers all purchased products independently. We then propose novel bagged-prod2vec method that takes into ac-count that some products are listed as purchased together in e-mail receipts, which results in better, more useful prod-uct representations. Finally, we present product-to-product and user-to-product recommendation models that make use of the learned representations. prod2vec. The prod2vec model involves learning vec-tor representations of products from e-mail receipt logs by using a notion of a purchase sequence as a  X  X entence X  and products within the sequence as  X  X ords X , borrowing the ter-minology from the NLP domain (see Figure 2 for graphical representation of the model). More specifically, prod2vec learns product representations using the skip-gram model [24] by maximizing the objective function over the entire set S of e-mail receipt logs, defined as follows where products from the same e-mail receipt are ordered ar-bitrarily. Probability P ( p i + j | p i ) of observing a neighboring product p i + j given the current product p i is defined using the soft-max function, where v p and v 0 p are the input and output vector repre-sentations of product p , c is the length of the context for product sequences, and P is the number of unique products in the vocabulary. From equations (3.1) and (3.2) we see that prod2vec models context of product sequence, where products with similar contexts (i.e., with similar neighboring purchases) will have similar vector representations. How-ever, prod2vec does not explicitly take into account that e-mail receipt may contain multiple products purchased at the same time, which we address by introducing a bagged version of prod2vec described below. bagged-prod2vec. In order to account for the fact that multiple products may be purchased at the same time, we propose a modified skip-gram model that introduces a no-tion of a shopping bag. As depicted in Figure 3, the model operates at the level of e-mail receipts instead at the level of products. Product vector representations are learned by maximizing a modified objective function over e-mail se-quences s , defined as follows
L = X Probability P ( e m + j | p mk ) of observing products from neigh-given the k -th product from m -th e-mail receipt reduces to a product of probabilities P ( e m + j | p mk ) = P ( p m + j, 1 ...  X  P ( p m + j,T m | p mk ), each defined using soft-max (3.2). Note that the third sum in (3.3) goes over receipts, so the items from the same e-mail receipt do not predict each other during training. In addition, in order to capture temporal aspects of product purchases we propose to use the directed language model, where as context we only use future prod-ucts [12]. The modification allows us to learn product em-beddings capable of predicting future purchases.

Learning. The models were optimized using stochastic gradient ascent, suitable for large-scale problems. However, computation of gradients  X  X  in (3.1) and (3.3) are propor-tional to the vocabulary size P , which is computationally expensive in practical tasks as P could easily reach millions of products. As an alternative, we used negative sampling approach proposed in [24], which significantly reduces the computational complexity.
Having learned low-dimensional product representations, we considered several possibilities for predicting the next product to be purchased. prod2vec-topK. Given a purchased product, the method calculates cosine similarities to all other products in the vo-cabulary and recommends the top K most similar products. prod2vec-cluster. To be able to make more diverse rec-ommendations, we considered grouping similar products into clusters and recommending products from a cluster that is related to the cluster of previously purchased product. We applied K -means clustering algorithm implemented on the top of Hadoop distributed system where we grouped prod-ucts based on cosine similarity between their low-dimensional representations. We assume that purchasing a product from any of the C clusters after a purchase from cluster c i follows a multinomial distribution Mu (  X  i 1 , X  i 2 ,... X  iC ), where  X  the probability that a purchase from cluster c i is followed by a purchase from cluster c j . In order to estimate parame-ters  X  ij , for each i and j , we adopted a maximum likelihood
Figure 4: User embeddings for user to product predictions approach,  X  ij = # of times c i purchase was followed by c j
In order to recommend a new product given a purchased product p , we first identify which cluster p belongs to (e.g., p  X  c i ). Next, we rank all clusters c j , j = 1 ,...,C , by the value of  X  ij and consider the top ones as top-related clusters to cluster c i . Finally, products from the top clusters are sorted by their cosine similarity to p , and we use the top K products as recommendations.
In addition to product-to-product predictions [16, 17], most recommendation engines allow user-to-product predic-tions as well [15, 30]. Recommendation for a user are typi-cally made considering historical purchases and/or interests inferred using other data sources such as user X  X  online be-havior [15], social contacts [30], and others. In this section we propose a novel approach to simultaneously learn vec-tor representations of products and users such that, given a user, recommendations can be performed by finding K nearest products in the joint embedding space. user2vec. The user2vec model simultaneously learns vec-tor representations of products and users by considering the user as a  X  X lobal context X , motivated by paragraph2vec algorithm [21]. The architecture of such model is illus-trated in Figure 4. The training data set was derived from user purchase sequences S , which comprised users u n and their purchased products ordered by the time of purchase, u n = ( p n 1 ,p n 2 ,...p nU n ), where U n denotes number of items purchased by user u n . During training, user vectors are updated to predict the products from their e-mail receipts, while product vectors are learned to predict other products in their context. For simplicity of presentation and w.l.o.g. in the following we present non-bagged version of the lan-guage model, however we note that it is straightforward to extend the presented methodology to use the bagged version.
More specifically, objective of user2vec is to maximize the log-likelihood over the set S of all purchase sequences, where c is the length of the context for products in purchase sequence of the n -th user. The probability P ( p ni p n,i + c ,u n ) is defined using a soft-max function, where v 0 p ni is the output vector representation of p ni  X  v is averaged vector representation of the product context including corresponding u n , defined as where v p is the input vector representation of p . Similarly, the probability P ( u n | p n 1 : p nU n ) is defined as where v 0 u n is the output vector representation of u n , and  X  v is averaged input vector representation of all the products purchased by user u n ,
One of the main advantages of the user2vec model is that the product recommendations are specifically tailored for that user based on his purchase history. However, disadvan-tage is that the model would need to be updated very fre-quently. Unlike product-to-product recommendations, which may be relevant for longer time periods, user-to-product rec-ommendations need to change often to account for the most recent user purchases.
The experimental section is organized as follows. We first describe data set used in development of our product-to-product and user-to-product predictors. Next we present valuable insights regarding purchase behavior of different age and gender groups in different US states. This informa-tion can be leveraged to improve demo-and geo-targeting of user groups. This is followed by a section on effectiveness of recommendation of popular products in different user co-horts, including age, gender, and US state. Finally, we show comparative results of various baseline recommendation al-gorithms. We conclude with the description of the system implementation at a large scale and bucket results that pre-ceded our product launch.
Our data sets included e-mail receipts sent to users who voluntarily opted-in for such studies, where we anonymized (a) Percentage of purchasing users among all online users
Figure 5: Purchasing habits for different demographics user IDs. Message bodies were analyzed by automated sys-tems. Product names and prices were extracted from e-mail messages using an in-house extraction tool.

Training data set collected for purposes of developing prod-uct prediction models comprised of more than 280 . 7 million purchases made by N = 29 million users from 172 commer-cial websites. The product vocabulary included P = 2 . 1 million most frequently purchased products priced over $5.
More formally, data set D p = { ( u n ,s n ) ,n = 1 ,...,N } was derived by forming e-mail receipt sequences s n for each user u n , along with their timestamps. Specifically, s prising one or more purchased products, and t nm is receipt timestamp. Predictions were evaluated on a held-out month of user purchases D ts p , formed in the same manner as D measured the effectiveness of recommendations using pre-diction accuracy. In particular, we measured the number of product purchases that were correctly predicted, divided by the total number of purchases. For all models, the accuracy was measured separately on each day, based on the recom-mendations calculated using prior days. We set a daily bud-get of distinct recommendations each algorithm is allowed to make for a user to K = 20, based on an estimate of optimal number of ads users can effectively perceive daily [7].
To get deeper insights into behavior of online users based on their demographic background and geographic location, we segregated users into cohorts based on their age, gender, and location, and looked at their purchasing habits. Such information can be very valuable to marketers when consid-ering how and where to spend their campaign budget. We only considered users from the US, and computed statis-tics per each state separately. First, we were interested in differences between male and female shopping behavior for different age ranges. In addition, we were interested in the following aspects of purchasing behavior: 1) percentage of online users who shop online; 2) average number of products bought; 4) average spent per user; and 4) average price of a bought item. Figures 5 and 6 illustrate the results.
In Figure 5 for each gender and age group we show the percentage of online users who shop online, as well as the average prices of bought items. Expectedly, we see that male and female demographics exhibit different shopping patterns. Throughout the different age buckets percent-age of female shoppers is consistently higher than for males, peaking in the 30-34 age range. On the other hand, per-centage of male shoppers reaches its maximum in the 21-24 bucket, and from there drops steadily. In addition, we ob-serve that male users buy more expensive items on average.
Furthermore, in Figure 6 we show per-state results, where darker color indicates higher values. Note that we only show results for different locations and age ranges, as we did not see significant differences between male and female purchasing behavior across states. First, in Figures 6(a) and 6(b) we show the percent of online shoppers in each state, for younger (18-20) and medium-age (40-44) populations, where we can see there exist significant differences between the states. In Figures 6(c) and 6(d) we show the average number of purchased products per state. Here, for both age buckets we see that the southern states purchase the least number of items. However, there is a significant difference between younger and older populations, as in northern states younger populations purchase more products, while in the northeast and the western states this holds true for the older populations. If we take a look at Figures 6(e) and 6(f) where we compare the average amount of money spent per capita, we can observe similar patterns, where in states like Alaska and North Dakota younger populations spend more money than peers from other states, and for older populations states with highest spending per user are California, Washington, and again Alaska. Interestingly, users from Alaska are the biggest spenders, irrespective of the age or metric used. Sim-ilar holds when we consider average price of a purchased item, shown in Figures 6(g) and 6(h).
In this section we evaluate predictive properties of popular products. Recommending popular products to global popu-lation is a common baseline due to its strong performance, especially during holidays (e.g., Christmas, Thanksgiving). Such recommendations are intuitive, easy to calculate and implement, and may serve in a cold start scenarios for newly registered users when we have limited information.
We considered how far back we need to look when calculat-ing popularity and how long popular products stay relevant. Figure 7: Prediction accuracy and longevity of popular products with different lookbacks In Figure 7 we give results for popular products calculated in the previous 5 , 10 , 20, and 30 days of training data D p uated on the first 1 , 3 , 7 , 15, and 30 days of test data D account for ever-changing user purchase tastes, the results indicate that popular products need to be recalculated at least every 3 days with lookback of at most 5 days.
Next, we evaluated the prediction accuracy of popular products computed for different user cohorts, and compared to the accuracy of globally popular products. In Figure 8 we compare accuracies of popular products in different user cohorts, with the lookback fixed at 5 days. We can observe that popular products in gender cohorts give bigger lift in prediction accuracy than popular products in age or state cohorts. Further, jointly considering age and gender when calculating popular products outperforms the popular gen-der products. Finally, including geographic dimension con-tributes to further accuracy lift. Overall, the results indicate that in the cold start scenario the best choice of which popu-lar products to recommend are the ones from the user X  X  (age, Figure 8: Prediction accuracy of popular products for dif-ferent user cohorts gender, location) cohort. The results also suggest that pop-ular products should be recalculated often to avoid decrease in accuracy with every passing day.
In this section we experiment with recommending prod-ucts to users based on neural language models described in Section 3. Specifically, we compare the following algorithms: 1) prod2vec-topK was trained using data set D p , where product vectors were learned by maximizing log-likelihood of observing other products from sequences s , as proposed in (3.1). Recommendations for a given product p i were given by selecting the top K most similar products based on the cosine similarity in the resulting vector space. 2) bagged-prod2vec-topK was trained using D p , where product vectors were learned by maximizing log-likelihood of observing other products from e-mail sequences s as pro-posed in (3.3). Recommendations for a given product p were given by selecting the top K most similar products based on the cosine similarity in the resulting vector space. 3) bagged-prod2vec-cluster was trained similarly to the bagged-prod2vec model, followed by clustering the prod-uct vectors into C clusters and calculating transition proba-bilities between them. After identifying which cluster p longs to (e.g., p i  X  c i ), we rank all clusters by their transition probabilities with respect to c i . Then, the products from top clusters are sorted by cosine similarity to p i , where top K from each cluster are used as recommendations ( P K K ). Example predictions of bagged-prod2vec-cluster com-pared to predictions of bagged-prod2vec are shown in Ta-ble 2. It can be observed that predictions based on the clustering approach are more diverse. 4) user2vec was trained using data set D p where prod-uct vectors and user vectors were learned by maximizing log-likelihood proposed in (3.5) (see Figure 4). Recommen-dations for a given user u n were given by calculating cosine similarity between the user vector u n and all product vec-tors, and retrieving the top K nearest products. 5) co-purchase. For each product pair ( p i ,p j ) we cal-culated frequency F ( p i ,p j ) ,i = 1 ,...,P,j = 1 ,...,P , with which product p j was purchased immediately after product p . Then, recommendations for a product p i were given by sorting the frequencies F ( p i ,p j ) ,j = 1 ,...,P , and retrieving the top K products. Figure 9: prod2vec accuracy with different decay values
Since user u n may have multiple products purchased prior to day t d , separate predictions need to reach a consensus in order to choose the best K products to be shown on that day. To achieve this we propose time-decayed scor-ing of recommendations, followed by choice of top K prod-ucts with the highest score. More specifically, given user X  X  products purchased prior to t d along with their timestamps, recommendations along with their similarity scores, result-ing in the set { ( p j ,sim j ) ,j = 1 ,...,KU n } , where sim de-notes cosine similarity. Next, we calculate a decayed score for every recommended product, where ( t d  X  t i ) is a difference in days between current day t and the purchase time of product that led to recommen-dation of p j , and  X  is a decay factor. Finally, the decayed scores are sorted in descending order and the top K products are chosen as recommendations for day t d .

Training details. Neural language models were trained using a machine with 96GB of RAM memory and 24 cores. Dimensionality of the embedding space was set to d = 300, context neighborhood size for all models was set to 5. Fi-nally, we used 10 negative samples in each vector update. Similarly to the approach in [24], most frequent products and users were subsampled during training. To illustrate the performance of the language models, in Table 1 we give examples of product-to-product recommendations computed using bagged-prod2vec, where we see that the neighboring products are highly relevant to the query product (e.g., for  X  X espicable me X  the model retrieved similar cartoons).
Evaluation details. Similarly to how popular product accuracy was measured, we assumed a daily budget of K = 20 distinct product recommendations per user. Predictions for day t d are based on prior purchases from previous days, and we did not consider updating predictions for day t d using purchases that happened during that day.

Results. In the first set of experiments we evaluated per-formance of prod2vec for different values of decay factors. In Figure 9 we show prediction accuracy on test data D ts p when looking 1 , 3 , 7 , 15, and 30 days ahead. Initial prod2vec pre-dictions were based on the last user purchase in the training data set D p . The results show that discounting of old pre-dictions leads to improved recommendation accuracy, with decay factor of  X  = 0 . 9 being an optimal choice. Figure 10: Prediction accuracy of different algorithms
Next, we evaluate different product-to-product and user-to-product methods and compared them to the popular prod-ucts approach found in Figure 8 to perform the best, namely recommending popular products computed separately for each (state, age, gender) cohort. Results are summarized in Figure 10. We can observe that all neural-based pre-diction algorithms outperformed method that recommends popular products. Further, even though user2vec model had the best overall accuracy on day 1, its prediction power quickly drops after 3 days. On the other hand, variants of the prod2vec model did not exhibit such behavior, and their performance remained steady across the first 7 days. Of all prod2vec models, the bagged-prod2vec-cluster model achieved the best performance, indicating that diversity in predictions leads to better results. Finally, predictions based on co-purchases did not perform as well as neural language models, supporting the  X  X on X  X  count, predict X  claim from [4] where the authors suggest that simple co-occurence ap-proaches are suboptimal.
 Table 3: Results from live A/B testing of product recom-mendations on Yahoo Mail Metric (5% traffic) (5% traffic) (5% traffic) CTR -+8.33% +9.81 %
YR -+7.63 %
Following offline evaluation of different product prediction methods, we conducted additional A/B testing experiments on live Yahoo Mail traffic. We ran two buckets with two different recommendation techniques, both on 5% of Yahoo Mail users. In the first bucket, for users with prior purchases the recommendations were based on the bagged-prod2vec-cluster model, while for users without prior purchases they were based on globally popular products. In the second bucket all users were recommended globally popular prod-ucts. For purposes of a fair bucket test, both models were retrained and refreshed with the same frequency of 7 days.
Both test buckets were evaluated against a control bucket in which products ads were replaced with standard ads from Yahoo Ad serving platform. All three buckets had signifi-cant amount of users with prior purchases. Evaluation was done based on the click-through rate (CTR) computed as ber of clicks on product ads that occurred after the targeted recommendation, divided by the total number of shown (or impressed) product ads. In the control bucket, we computed the same metric for standard ads. For the two product ad buckets we additionally measured the yield rate, calculated purchase of the recommended product. Conversions were attributed to impression for up to 48 hours since the time users clicked or observed a product ad. Figure 11: CTR of predicted versus popular recommenda-tions in live bucket test over time
The results are presented in Table 3. We can make sev-eral observations. First, both popular and predicted bucket showed better CTR numbers than the control bucket, indi-cating that the users prefer product ads over standard ads. Second, the prediction bucket achieved slightly better CTR rates than the popular bucket. Finally, the prediction bucket achieved significantly better YR than the popular bucket. This indicates that many times users click on popular prod-ucts out of curiosity and do not end up buying the product, whereas clicks on targeted products lead to more purchases as they better capture user interests. Overall, the presented results strongly suggest benefits of the proposed approach for the task of product recommendations.

In addition, in Figure 11 we show how CTR rates change over time. Similarly to our offline experiments, it can be observed that popular recommendations become stale much faster than predicted recommendations, as indicated by the steeper CTR drop. They are also more susceptible to novelty bias following model updates, as seen by larger increase in CTR. Another useful finding was that 7 day updates are not sufficient, confirming findings from Figure 7.
In this section we cover the details on our system imple-mentation that led to final product deployment.
Due to product requirements for near-real-time predic-tions, we chose to use product-to-product recommendations, with the bagged-prod2vec-cluster model. The model is up-dated every 5 days with the most recent purchase data. The product vectors are stored on Hadoop Distributed File System (HDFS), and updated via training procedure im-plemented on Hadoop 5 , where we heavily leverage parallel processing with MapReduce. We form purchase sequences using the newest purchased products extracted from e-mail receipts, and incrementally update the product vectors in-stead of training them from scratch.

Popular products were used as back-fill for users who do not have prior purchase history. Following the experimen-tal results, we recalculated popular products every 3 days, with a lookback of 5 days. More specifically, we extracted https://hadoop.apache.org, accessed June 2015 100 most frequently purchased products for every state-age-gender cohort, and showed them in randomized order to users without purchase history.

We implemented a multi-tier architecture in order to pro-ductionize product recommendation capability on Yahoo Mail. For data storage we used a custom, high-performance dis-tributed key-value store (similar to Cassandra 6 ), which per-sists user profiles and product-to-product prediction model. The user profile store utilizes user identifier as a key and stores multiple columns representing user prior purchases as values. User profiles are updated hourly with new products, extracted from e-mail receipts. Each purchase record per-sists in memory with time-to-live (TTL) set to 60 days, after which period it is discarded.

Product-to-product prediction model is stored in a sim-ilar key-value store, where purchased product is used as a key and values are multiple columns each representing pre-dicted products ordered by score. Both user and product-to-product stores are updated in batch without impacting real traffic. Separate process performs all processing of querying user purchases and asynchronously retrieves relevant pre-dictions and offers. It then returns selected product ad back to presentation process implemented in JavaScript and HTML which renders it in the browser. The retrieval pro-cess is configured such that it can fetch offers from multiple e-commerce websites (affiliate partners). The system runs with a service-level agreement of 500ms and can be improved further by caching interactions with affiliate partners.
Given predictions for a certain user, once the user logs into the e-mail client we show a new recommendation after every user action, including clicking on folders, composing e-mail, and searching the inbox. Recommendation are circled in the order of decayed prediction score, as described in Section 4.4. The product ads are implemented in the so-called  X  X encil X  ad position, just above the first e-mail in the inbox (Figure 1).
We presented the details of a large-scale product recom-mendation framework that was launched on Yahoo Mail in the form of product ads. We discussed the recommendation methodology, as well as the high-level implementation de-tails behind our system. To perform recommendations, we described how we employed neural language models capa-ble of learning product embeddings for product-to-product predictions, as well as user embeddings for user-to-products predictions. Several variants of the prediction models were tested offline and the best candidate was chosen for an on-line bucket test. Following the encouraging bucket test re-sults, we launched the system in production. In our ongoing work, we plan to utilize implicit feedback available through ad views, ad clicks, and conversions to further improve the performance of our product recommendation system. [1] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. [2] J. Alba, J. Lynch, B. Weitz, C. Janiszewski, R. Lutz, http://cassandra.apache.org/, accessed June 2015 [3] R. Baeza-Yates, B. Ribeiro-Neto, et al. Modern [4] M. Baroni, G. Dinu, and G. Kruszewski. Don  X  Ot [5] Y. Bengio, H. Schwenk, J.-S. Sen  X ecal, F. Morin, and [6] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, [7] C.-H. Cho and H. J. Cheon. Why do people avoid [8] R. Collobert, J. Weston, L. Bottou, M. Karlen, [9] N. Djuric, V. Radosavljevic, M. Grbovic, and [10] N. Djuric, H. Wu, V. Radosavljevic, M. Grbovic, and [11] D. Essex. Matchmaker, matchmaker. Communications [12] M. Grbovic, N. Djuric, V. Radosavljevic, and [13] M. Grbovic, G. Halawi, Z. Karnin, and Y. Maarek. [14] M. Grbovic and S. Vucetic. Generating ad targeting [15] D. J. Hu, R. Hall, and J. Attenberg. Style in the long [16] J. Katukuri, T. Konik, R. Mukherjee, and S. Kolay. [17] J. Katukuri, R. Mukherjee, and T. Konik. Large-scale [18] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal [19] R. Kiros, R. S. Zemel, and R. Salakhutdinov. A [20] V. Lavrenko and W. B. Croft. Relevance based [21] Q. V. Le and T. Mikolov. Distributed representations [22] G. Linden, B. Smith, and J. York. Amazon.com [23] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [24] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [25] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: [26] D. Riecken. Personalized views of personalization. [27] R. Socher, D. Chen, C. D. Manning, and A. Ng. [28] J. Turian, L. Ratinov, and Y. Bengio. Word [29] Y. Zhang and M. Pennacchiotti. Predicting purchase [30] X. W. Zhao, Y. Guo, Y. He, H. Jiang, Y. Wu, and
