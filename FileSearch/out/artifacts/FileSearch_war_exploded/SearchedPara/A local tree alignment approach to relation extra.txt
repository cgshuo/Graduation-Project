 1. Introduction
Information extraction (IE) aims to convert content from natural language texts to structured forms. A statistical, auto-an example, consider the following example sentence.
 Mr. Stevens succeeds Fred Casey who retired from Occ in June.
 egories for each entity, for instance, PERSON for the first two entities and extraction creates the relational database tuples consisting of meaningful relations involving named entities. For example, assume that we are interested in the relation schema ( PERSON element. Creating such relations from unstructured texts by performing these two tasks would help to develop the further applications, such as question answering and intelligent document searching and indexing.

In spite of the current high standard of state-of-the-art named entity taggers, developing an accurate method for relation specific problem of extracting binary relations, for example the  X  straightforward approach to this problem would be to use a pairwise strategy, where an existing binary relation extraction machinery to deal with incomplete tuples (e.g. POSITION in the previous example).
 addressed by a pattern induction and matching approach (for example, Yangarber, Grishman, Tapanainen, Huttunen, &amp; Hel-tic structures of natural languages. Most prior works has focused on development of a reliable pattern induction method in exactly same forms between pattern and text instance as a matched pair of instance and pattern. The premise of this hard that hard pattern matching between pattern database and instance trees cannot allow us to examine the structural similarity of the trees, so it yields lower coverage of the patterns. For example, consider an example sentence which paraphrases our previous example sentence: The position of Fred Casey who retired from Occ in June was taken over by Mr. Stevens.

Hard pattern matching may fail to extract the right relations of this sentence by using the patterns induced from the for-mer sentence, or may require more training examples to induce more patterns.

To ease this hard constraint, we explore an alignment-based soft pattern matching approach to improve the coverage of in-We evaluate the proposed method on two standard datasets from two different domains: management succession and
Nobel prize. Interestingly, the Nobel prize domain has no supervised training example but does have a small set of seed in-stances; hence we show that our method is easily applicable to a weakly-supervised learning scenario. Our experiments demonstrate that the soft pattern matching approach is beneficial for the considered problem as our system outperforms both baselines, a pairwise approach of binary relation extraction trained with a tree-kernel support vector machine and a hard pattern matching method.

The remainder of this paper is structured as follows. Section 2 describes our system overview to the problem of relation extraction with multiple arguments. Section 3 presents the background of the local tree alignment algorithm and its appli-on soft pattern matching. Section 5 provides an empirical evaluation of the proposed method. Section 6 concludes with an examination of additional related work. 2. System overview
This section formulates the task of relation extraction with multiple arguments and describes our relation extraction sys-tem. Here we assume that named entities and their types are known a priori in the text as this could be done by an accurate named entity tagger. We also suppose that relation schema are defined for the considered domain. Our task is, roughly, to induce a set of patterns completely or partially modeling the relations of n -ary arguments, and to apply these patterns to create a new tuple of such relations.

We start by clarifying our problem of multiple-argument relation extraction. Formally, we define the relation schema ( r , ... , r n ) as any n -ary relation among n entities, where r common type for named entity extraction could be assigned by different relation types; for example, both Stevens and Fred Casey indicate the PERSON entity type, but they have different roles in the relation schema, like a list of entities ( e 1 , ... , e n ) such that either relation ( e The goal of our relation extraction task is to create a set of relation tuples given a schema and a document collection.
Documents are assumed to be unstructured, and a document x is simply a sequence of sentences, i.e. x ={ s s denotes a sentence. The question of how to represent this sentence remains, however. In this work we define a sentence s as a dependency tree , a directed graph T =( v , ), where dependency trees; that is, p is a tree ( v w , w ), where a node to capture abstract information of language, so it can improve the coverage of relation patterns. For example, for two sen-tences which present the same meaning but are paraphrased by active and passive voice verbs, two dependency structures are roughly the same. In contrast, the bag-of-words may fail to capture an abstraction due to the different order of word sequences, and would require a much larger set of data (e.g. all possible paraphrasing) to achieve the same coverage-level. dependency trees was introduced in GuoDong et al. (2005) and a dependency tree-kernel has been successfully applied to focuses on inducing the informative patterns from a given dependency tree collection and applying these patterns to extract the relation tuples.

We now describe the overview of our relation extraction system, in which context patterns are induced and exploited to given a set of training examples. Note that the training example can be either an annotated document or a partially anno-tern learning is a three-step iterative algorithm to induce relation patterns: (i) it extracts numerous candidates of the effective search process.

The considered pattern candidates follow the pattern representation model defined by the system architecture. Recall the tree, in practice we need to constrain the depth and the set of symbols for efficiency. As a result, we use the dependency tree-based pattern representation models developed in previous works ( Yangarber, 2003; Sudo et al., 2001; Greenwood &amp; Stevenson, 2006; Sudo et al., 2003 ). Fig. 2 b shows a sample of candidate patterns.

Relation tuples are inferred from the documents using a tree alignment-based soft pattern matching. Our pattern matcher gation is straightforward to implement by a simple template matching and voting strategy, the technical challenge of tree to resolve a low coverage problem of exact matching between patterns and instance tree. While some possible approaches next section, we will describe an alternative soft pattern matching method for complex relation extraction by introducing a local tree alignment algorithm. 3. Soft pattern matching with local tree alignment
In this section we present the background and generic algorithm of tree alignment, which is popular in computational sub-structures between the set of dependency trees and the pattern database. 3.1. Local tree alignment
In the bioinformatics literature, sequence alignment is a fundamental task that arranges similar regions among sequences aims to find the most similar regions (i.e. sub-structures) within two given sequences, whereas global alignment calculates the optimal alignment spanning the entire lengths of sequences. Since our goal in relation extraction is to induce patterns that model the relations between entities, we focus on local alignment in this paper. A local tree alignment can be imple-mented using tree edit distance, which models sequence pairs by finding a minimal cost sequence of editing operations to transform a tree representation of one sequence into a tree for the other ( Hochsmann, Toller, Giegerich, &amp; Kurtz, dependency trees ( Culotta &amp; Sorensen, 2004 ), but the tree-kernel instead uses an approximation of overlapping sub-structures rather than explicit alignment.

In this paper, we modify the Hochsmann algorithm ( Hochsmann et al., 2003 ), a local tree alignment algorithm for ana-lyzing secondary RNA structures. This algorithm finds the local, close alignment to maximize the tree edit distance-based similarity score for ordered trees. More formally, the Hochsmann alignment algorithm uses dynamic programming; Fig. 4 shows the outline of algorithm and the notation used therein. According to the order, the similarity score S ( F alignment between F 1 and F 2 can be computed as follows: where r ( v , w ) is the similarity score between two given nodes trees, that is, F 1  X  X  T 0 1 ; ... ; T m 1  X  and F 2  X  X  T tively. Then, three editing operations, insertion , deletion , and replacement are defined as: where r indicates the optimal position for deletion and insertion operations. In order to keep the optimal alignment in the back-trace stage, we record the information for operations and positions on the maximum value path. This algorithm runs in O ( j T 1 jj T 2 j deg( T 1 ) deg( T 2 ) (deg( T 1 ) + deg ( T for two input trees T 1 and T 2 , where deg( T ) is the degree of the tree T . 3.2. Soft pattern matching approach using tree alignment
We now apply the generic local tree alignment algorithm described in the preceding section to soft pattern matching for measure structural differences between the pattern and the instance tree. Formally, we define five feature functions as follows: f LEX ( N i ): lexical of the node N i . f ARG ( N i ): whether the node N i is an argument node to be extracted or not. f POS ( N i ): part-of-speech tag of the node N i . f NE ( N i ): named-entity tag of the node N i . f DEP ( N i ): dependency relationship of the node N i to its parent node.
 feature appears and satisfies the specific conditions:
Note that d TYPE is a type checking function to avoid alignment between an argument and an inconsistent unit. Each type of argument to be extracted has a set of constraints that assesses the feasibility of the obtained alignment. For example, an argument node that has the PERSON _ IN relation type should be aligned with a node which is recognized as the entity type.

Next, we apply such arranged features and feature functions to alignment. To model this, we re-define the r function, a that is, where k X is an interpolation parameter for component X . In this paper we empirically set k k tion extraction task.

The pattern matching for relation extraction is accomplished by aligning a pattern p in the database (i.e. model) and an matched instances, O  X f x : confidence  X  p i ; x  X  &gt; h tern matching, so the proposed soft pattern matching method is a general algorithm for pattern matching-based relation extraction. 4. Pattern induction and learning
In this section we present the pattern induction and learning method which iteratively searches the most influential pat-terns among the candidates, and creates the pattern database. Our pattern learning method sequentially runs three steps: candidate extraction, scoring, and searching.

The pattern candidate extraction step performs extraction of the possible candidate patterns from the training data, which consists of a list of pairs for a dependency tree x tree ( v w , w ) which satisfies the following constraints: In particular, we adopt the subtree pattern representation model ( Sudo et al., 2003 ).
 scoring function as the gain of the considered pattern: where p is a pattern to be evaluated, h is a control parameter corresponding to p , P iteration step ( P  X  i  X   X f X  p k ; h k  X g K i k  X  1 and P might be an arbitrary evaluation function, but for simplicity we use two simple measures in this work. The first measure is used when supervised annotation data can be accessed, and the other is for weakly-supervised learning. In a supervised learning scenario, our feature induction algorithm directly optimizes the end performance, whereas in a weakly-supervised one, we approximate this by selecting the highly informative patterns.
 (i.e. dependency tree) we compare the extracted tuples with the reference tuples by the standard F eval ( P ( i ) )= f1 score ( y , extraction ( x , P ( i ) in the previous section. If only a set of seed instances and unlabeled documents are available, a document-centric scoring beled documents including seed instances are often determined as relevant documents. Then, we use a TFIDF-style metric which scores the usefulness of patterns based on the frequency of the matched documents ( Sudo et al., 2003 ). Formally, we relevant documents and df is the number of the documents matched with the patterns. This function should help select the most representative pattern to cover the given documents.
 defined as: grid is defined as all combinations of pattern candidate and its control parameter h . 5. Experiment 5.1. Dataset and setup
Our method was evaluated on two relation extraction domains: management succession ( Grishman &amp; Sundheim, 1996 ) tuples which consists of four arguments: ( PERSON _ IN, PERSON corpus containing 599 training and 100 test documents created by Soderland (1999) . While the relation templates on the for each sentence. This dataset is appropriate to evaluate our method on a supervised learning task, so we use the f1-score measure as our pattern evaluation function. For the MS dataset, we follow the fully-supervised pattern induction policy:
After inducing the pattern database from annotated documents in the training set, we evaluate the relation extraction per-formance on the test documents. Note that each pattern is considered by descending order of pattern score. The performance is measured by the standard precision, recall and F1 score on the tuples obtained by pattern matching between sentences in to the gold standard labels for all arguments are considered as positive instances.
 The Nobel Prize award dataset (NP) aims to extract event relations consisting three arguments, ( et al., 2007 ). The dataset includes 3312 Nobel Prize related documents from New York Times, online BBC and CNN news re-ports. Because of the absence of annotation for target arguments, we adopted the weakly-supervised induction method with a seed instance set. We began with a seed tuple (Ahmed H Zewail, Chemistry, 1999) and applied a bootstrapping method to append the predicted, reliable extraction from unlabeled documents to the seed set. Since there is no reference annotation list of award events from the Nobel Prize official website sult. We compare the extraction result with the award list by precision, recall, and F1 scores.
All sentences in both datasets were processed by standard preprocessing tools: tokenization, part-of-speech tagging, noun phrase chunking, and named entity recognition by OpenNLP tool
Klein, 2007 ), and constituent-to-dependency parse tree conversion by LTH Constituent-to-Dependency Conversion Tool sampled from the preprocessed documents was performed, and it achieved an unlabeled attachment score (UAS) of 90.1% and a labeled attachment score (LAS) of 86.6%.
To quantify the benefits of our method, we compared our the tree alignment-based soft pattern matching method against performed. The binary classifier was the tree-kernel support vector machine which has been known as a state-of-the-art evaluate the pairwise approach with the binary relation extraction model as follows: set is labeled as a positive example if the annotation of n -ary tuples contains such a binary relation.
We train a binary SVM classifier for each binary relation category. We use a dependency tree kernel method with the pub-licly available SVM-Light software.

For each binary relation pair on test set, the binary SVM classifier is performed. Then, we integrate the obtained binary relations into n -ary tuples. When conflicts occurred between extractions, we choose the instance with the higher score.
Our second baseline is the hard pattern matching method, where every h was set to 1.0. 5.2. Results
We show that our proposed method is an reasonable solution for the multiple-argument relation extraction task ( Tables 2 and 3 and Fig. 5 ), and then we provide some more detailed analysis of its behavior.

Table 2 compares our method against the two baseline approaches. While the binary classification method (Pairwise ap-ticular, our soft pattern matching approach outperforms both the pairwise approach and hard pattern matching. Our hypoth-esis in this paper was that the soft pattern matching method based on tree alignment algorithm would improve the coverage matching methods with partial subsets of training documents for the MS dataset. The same randomly chosen subsets of training examples were used with both methods. In this result, the recalls for soft pattern matching method increased sig-nificantly, whereas precisions decreased compared with hard pattern matching. Thus, F1 score, a harmonic mean of precision and recall, improved. Moreover, the result of soft pattern matching with 30% of training examples was approximately equiv-alent to the result of hard pattern matching with 50% of training examples.

Fig. 5 compares the relative performances in precision and recall of the two pattern matching approaches for both do-precision of hard pattern matching were better than soft pattern matching for most iterations, soft pattern matching ap-matching approach could achieve better coverage when few patterns are available.

Fig. 6 shows distributions in coverage of each pattern induced by the soft pattern matching policy for both domains. The number of extractions decreased. This result means that our method starts to induce relatively general patterns with larger coverage first and accumulates more specific patterns as the induction is iterated.
 only considers errors made by our soft pattern matching method. We only report the result on the MS domain, since the extractions on the NP domain were evaluated on each award, which makes the analysis difficult. While sentence detection errors were caused by extracting tuples from non-relevant sentences, argument selection errors occurred when one or more wrong arguments existed on the frame extracted from the relevant sentences. The error analysis shows that a large number the case that the output frame includes both false positive and true positive arguments. It means that most errors causing precision loss in the soft pattern matching method do not exert absolutely negative influences on the overall task.
Although our method yields better coverage results, we still need a method to improve the precision of our system. Ker-nel-based classification methods can provide highly precise extractions. Therefore, we can expect that combining the tree alignment approach with kernel methods, or, alternately, introducing a more sophisticated kernel, should improve both pre-cision and recall for relation extraction of multiple arguments. A combined method of tree alignment and kernel methods remains as future work.

The top-10 patterns induced from our experiments on both domains listed in Fig. 7 suggest that there X  X  also room for improvement in coverage of our method. Although most patterns represent varied contexts in both structure and lexical as-the fourth and sixth patterns in Fig. 7 having the same substructure except verb nodes indicating  X  X amed X  and  X  X lected X  are quite similar in semantics. However, this kinds of patterns have been completely distinguished to each other, because only lexical agreements have been considered to compute node-level similarities in this work. To enhance the coverage of each sions. We can expect that it could also help to achieve better performance of the method with fewer patterns. 6. Related work
Most previous works on dependency tree pattern induction for IE have mainly concentrated on defining pattern repre-sentation models to construct pattern candidates from given resources. Yangarber (2003) introduced the predicate-argu-ment model, which is based on the dependency relationships among a predicate and its arguments. Sudo et al. (2001) proposed the chain model, defined as a chain-shaped path from each relevant argument to the root node in the dependency tree. Greenwood &amp; Stevenson (2006) utilized the linked chain model constructed by chain model patterns sharing the same verb node. The subtree model explored by Sudo et al. (2003) considered all subtrees containing relevant arguments as pat-tern candidates. Since the goal of our method is not to define a novel pattern representation model but to improve the per-formance in pattern matching, we utilized the subtree model known as the best pattern representation model in IE performances ( Stevenson &amp; Greenwood, 2006 ) for constructing pattern candidates.

The measure to evaluate the quality of pattern candidates is another important issue in pattern induction methods. Yan-garber et al. (2000) presented a document-centric pattern scoring approach that assumes documents containing relatively large numbers of relevant patterns probably include additional patterns. Stevenson &amp; Greenwood (2005) showed that pat-similarity-based pattern scoring approach. Because the evaluation for our method started not with faultless manually se-lected seed patterns but with the induced pattern candidates from a given set of seed instances or labeled documents, we preferred the document-centric pattern scoring rather than the semantic similarity-based method, which is highly depen-dent on the certainties of previously accepted patterns.

Regardless of the type of pattern representation model and scoring measure for pattern selection, most previously ex-plored IE approaches based on tree pattern matching methods have utilized a hard pattern matching policy considering only exactly equivalent matched extractions as the results. The main contribution of our work is that we propose a soft pattern matching method which tolerates a certain degree of structural difference in pattern matching. While soft pattern matching
Kim et al., 2008 ), our soft pattern matching method aims to improve the performance of the tree pattern matching-based IE method. The experimental results show that our soft pattern matching method outperforms the hard pattern matching method on both the management succession and the Nobel Prize Awards domains. 7. Summary and future work
In this paper we studied the complex relation extraction problem which contains multiple arguments. We proposed a local tree alignment approach to the problem which can utilize the structural similarity of dependency parse trees to model in the improvement of the coverage of induced patterns. We also present an effective pattern induction and learning policy, where our system could induce the sub-structure patterns as well as the softness parameters for each pattern. We demon-strated our method on two domains, showing that our approach is beneficial for complex relation extraction tasks.
There are many directions we plan to investigate in the future for the problem of complex relation extraction and the local tree alignment approach. A promising and challenging possibility is to consider models which learn the editing costs in the weights for editing operations for our relation extraction task.

The improvement of similarity functions by exploring various knowledge is also expected as a way of making our method their word lexicals are identical or not. To improve the performance of our method, we plan to modify the node-level sim-
The function will be based on the various features including synonyms or hypernyms from Wordnet, hierarchical categories from Wikipedia, and distributional similarities from the Web.

Another direction would be to investigate a method to apply to multiple languages. Our method is constructed on the dependency parse tree, and such a linguistic structure is general across many languages. Thus, one potential application could be multilingual relation extraction, in which the knowledge on one language side could be transferred to other lan-guages; this should help to build relation extraction systems for resource-scare languages.
 Acknowledgements
This work was supported by the Industrial Strategic technology development program, 10035252, Development of dia-log-based spontaneous speech interface technology on mobile platform funded by the Ministry of Knowledge Economy (MKE, Korea).
 References
