 Document Similarity Search (DSS) is to find sim-ilar documents to a query doc in a text corpus or on the web. It is an important component in mod-ern information retrieval since DSS can improve the traditional search engines and user experience (Wan et al. , 2008; Dean et al. , 1999). Traditional search engines accept several terms submitted by a user as a query and return a set of docs that are rele-vant to the query. However, for those users who are not search experts, it is always difficult to ac-curately specify some query terms to express their search purposes. Unlike short-query based search, DSS queries by a full (long) document, which allows users to directly submit a page or a document to the search engines as the description of their informa-tion needs. Meanwhile, the explosion of information has brought great challenges to traditional methods. For example, Inverted List (IL) which is a primary key-term access method would return a very large set of docs for a query document, which leads to the time-consuming post-processing. Therefore, a new effective algorithm is required.

Hashing methods can perform highly efficient but approximate similarity search, and have gained great success in many applications such as Content-Based Image Retrieval (CBIR) (Ke et al. , 2004; Kulis et al. , 2009b), near-duplicate data detection (Ke et al. , 2004; Manku et al. , 2007; Costa et al. , 2010), etc. Hashing methods project high-dimensional ob-jects to compact binary codes called fingerprint s and make similar fingerprints for similar objects. The similarity search in the Hamming space 1 is much more efficient than in the original attribute space (Manku et al. , 2007).

Recently, several hashing methods have been pro-posed. Specifically, SimHash (SH) (Charikar M.S., 2002) uses random projections to hash data. Al-though it works well with long fingerprints, SH has poor discrimination power for short fingerprints. A kernelized variant of SH, called Kernelized Local-ity Sensitive Hashing (KLSH) (Kulis et al. , 2009a), is proposed to handle non-linearly separable data. These methods are unsupervised thus cannot incor-porate prior knowledge for better hashing. Moti-vated by this, some supervised methods are pro-posed to derive effective hash functions from prior knowledge, i.e., Spectral Hashing (Weiss et al. , 2009) and Semi-Supervised Hashing (SSH) (Wang et al. , 2010a). Regardless of different objectives, both methods derive hash functions via Principle Component Analysis (PCA) (Jolliffe, 1986). How-ever, PCA is computationally expensive, which lim-its their usage for high-dimensional data.

This paper proposes a novel (semi-)supervised hashing method, Semi-Supervised SimHash (S 3 H), for high-dimensional data similarity search. Un-like SSH that tries to find a sequence of hash func-tions, S 3 H fixes the random projection directions and seeks the optimal feature weights from prior knowledge to relocate the objects such that simi-lar objects have similar fingerprints. This is im-plemented by maximizing the empirical accuracy on the prior knowledge (labeled data) and the en-tropy of hash functions (estimated over labeled and unlabeled data). The proposed method avoids us-ing PCA which is computationally expensive espe-cially for high-dimensional data, and leads to an efficient Quasi-Newton based solution. To evalu-ate our method, we compare with several state-of-the-art hashing methods on two large datasets, i.e., 20 Newsgroups (20K points) and Open Directory Project (ODP) (2.4 million points). All experiments show that S 3 H gets the best search performance.
This paper is organized as follows: Section 2 briefly introduces the background and some related works. In Section 3, we describe our proposed Semi-Supervised SimHash (S 3 H). Section 4 provides ex-perimental validation on two datasets. The conclu-sions are given in Section 5. Suppose we are given a set of N documents, X = { x i | x i  X  R M } N i =1 . For a given query doc q , DSS tries to find its nearest neighbors in X or a subset X  X   X  X  in which distance from the documents to the query doc q is less than a give threshold. How-ever, such two tasks are computationally infeasible for large-scale data. Thus, it turns to the approxi-mate similarity search problem (Indyk et al. , 1998). In this section, we briefly review some related ap-proximate similarity search methods. 2.1 SimHash SimHash (SH) is first proposed by Charikar (Charikar M.S., 2002). SH uses random projections as hash functions, i.e., where w  X  R M is a vector randomly generated. SH specifies the distribution on a family of hash func-tions H = { h } such that for two objects x i and x j , where  X  ( x i , x j ) is the angle between x i and x j . Ob-viously, SH is an unsupervised hashing method. 2.2 Kernelized Locality Sensitive Hashing A kernelized variant of SH, named Kernelized Locality Sensitive Hashing (KLSH) (Kulis et al. , 2009a), is proposed for non-linearly separable data. KLSH approximates the underling Gaussian distri-bution in the implicit embedding space of data based on central limit theory. To calculate the value of hashing fuction h (  X  ) , KLSH projects points onto the eigenvectors of the kernel matrix. In short, the com-plete procedure of KLSH can be summarized as fol-lows: 1) randomly select P (a small value) points from X and form the kernel matrix, 2) for each hash function h (  X  ( x )) , calculate its weight  X   X  R P just as Kernel PCA (Sch  X  olkopf et al. , 1997), and 3) the hash function is defined as: where  X  (  X  ,  X  ) can be any kernel function.
KLSH can improve hashing results via the kernel trick. However, KLSH is unsupervised, thus design-ing a data-specific kernel remains a big challenge. 2.3 Semi-Supervised Hashing Semi-Supervised Hashing (SSH) (Wang et al. , 2010a) is recently proposed to incorporate prior knowledge for better hashing. Besides X , prior knowledge in the form of similar and dissimilar object-pairs is also required in SSH. SSH tries to find L optimal hash functions which have maximum empirical accuracy on prior knowledge and maxi-mum entropy by finding the top L eigenvectors of an extended covariance matrix 2 via PCA or SVD.
However, despite of the potential problems of nu-merical stability, SVD requires massive computa-tional space and O ( M 3 ) computational time where M is feature dimension, which limits its usage for high-dimensional data (Trefethen et al. , 1997). Fur-thermore, the variance of directions obtained by PCA decreases with the decrease of the rank (Jol-liffe, 1986). Thus, lower hash functions tend to have smaller entropy and larger empirical errors. 2.4 Others Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al. , 1998). LSH performs a random linear projection to map similar objects to similar hash codes. However, LSH suffers from the effi-ciency problem that it tends to generate long codes (Salakhutdinov et al. , 2007). LAMP (Mu et al. , 2009) considers each hash function as a binary par-tition problem as in SVMs (Burges, 1998). Spec-tral Hashing (Weiss et al. , 2009) maintains similar-ity between objects in the reduced Hamming space by minimizing the averaged Hamming distance 3 be-tween similar neighbors in the original Euclidean space. However, spectral hashing takes the assump-tion that data should be distributed uniformly, which is always violated in real-world applications. In this section, we present our hashing method, named Semi-Supervised SimHash (S 3 H). Let X L = { ( x 1 ,c 1 ) ... ( x u ,c u ) } be the labeled data, c  X  { 1 ...C } , x  X  R M , and X U = { x u +1 ... x N } the unlabeled data. Let X = X L  X  X  U . Given the labeled data X L , we construct two sets, attraction set  X  a and repulsion set  X  r . Specifically, any pair ( x are in the same class, i.e., c i = c j , while any pair ( x previews works that attempt to find L optimal hyper-planes, the basic idea of S 3 H is to fix L random hy-perplanes and to find an optimal feature-weight vec-tor to relocate the objects such that similar objects have similar codes. 3.1 Data Representation Since L random hyperplanes are fixed, we can rep-resent a object x  X  X  as its relative position to these random hyperplanes, i.e., where the element V ml  X  X  +1 ,  X  1 , 0 } of V indi-cates that the object x is above, below or just in the l -th hyperplane with respect to the m -th feature, and which, to some extent, reflects the distance from x to these hyperplanes. 3.2 Formulation Hashing maps the data set X to an L -dimensional Hamming space for compact representations. If we represent each object as Equation (4), the l -th hash function is then defined as: where w  X  R M is the feature weight to be deter-mined and d l is the l -th column of the matrix D .
Intuitively, the  X  X ontribution X  of a specific feature to different classes is different. Therefore, we hope to incorporate this side information in S 3 H for better hashing. Inspired by (Madani et al. , 2009), we can measure this contribution over X L as in Algorithm 1. Clearly, if objects are represented as the occurrence numbers of features, the output of Algorithm 1 is just the conditional probability Pr(class | feature) . Finally, each object ( x ,c )  X  X  L can be represented as an M  X  L matrix G : Note that, one pair ( x i , x j ) in  X  a or  X  r corresponds to ( G i , G j ) while ( D i , D j ) if we ignore features X  contribution to different classes.

Furthermore, we also hope to maximize the em-pirical accuracy on the labeled data  X  a and  X  r and
Algorithm 1: Feature Contribution Calculation maximize the entropy of hash functions. So, we de-fine the following objective for ~ (  X  ) s: where N p = |  X  a | + |  X  r | is the number of attrac-tion and repulsion pairs and  X  1 is a tradeoff between two terms. Wang et al. have proven that hash func-tions with maximum entropy must maximize the variance of the hash values, and vice-versa (Wang et al. , 2010b). Thus, H( ~ (  X  )) can be estimated over the labeled and unlabeled data, X L and X U .
Unfortunately, direct solution for above problem is non-trivial since Equation (7) is not differentiable. Thus, we relax the objective and add an additional regularization term which could effectively avoid overfitting. Finally, we obtain the total objective: where g i,l and d i,l denote the l -th column of G i and D i respectively, and  X  ( t ) is a piece-wise linear func-tion defined as: This relaxation has a good intuitive explanation. That is, similar objects are desired to not only have the similar fingerprints but also have sufficient large projection magnitudes, while dissimilar objects are desired to not only differ in their fingerprints but also have large projection margin. However, we do not hope that a small fraction of object-pairs with very large projection magnitude or margin dominate the complete model. Thus, a piece-wise linear function  X  (  X  ) is applied in S 3 H.

As a result, Equation (8) is a simply uncon-strained optimization problem, which can be ef-ficiently solved by a notable Quasi-Newton algo-rithm, i.e., L-BFGS (Liu et al. , 1989). For descrip-tion simplicity, only attraction set  X  a is considered and the extension to repulsion set  X  r is straightfor-ward. Thus, the gradient of L ( w ) is as follows: Note that  X  X  ( t ) / X  X  = 0 when | t | &gt; T g . 3.3 Fingerprint Generation When we get the optimal weight w  X  , we generate fingerprints for given objects through Equation (5). Then, it tunes to the problem how to efficiently ob-tain the representation as in Figure 4 for a object. After analysis, we find: 1) hyperplanes are randomly generated and we only need to determine which sides of these hyperplanes the given object lies on, and 2) in real-world applications, objects such as docs are always very sparse. Thus, we can avoid heavy computational demands and efficiently gener-ate fingerprints for objects.

In practice, given an object x , the procedure of generating an L -bit fingerprint is as follows: it main-tains an L -dimensional vector initialized to zero. Each feature f  X  x is firstly mapped to an L -bit hash value by Jenkins Hashing Function 4 . Then, Algorithm 2: Fast Fingerprint Generation these L bits increment or decrement the L compo-nents of the vector by the value x f  X  w  X  f . After all features processed, the signs of components deter-mine the corresponding bits of the final fingerprint. The complete algorithm is presented in Algorithm 2. 3.4 Algorithmic Analysis This section briefly analyzes the relation between S
H and some existing methods. For analysis sim-plicity, we assume  X  ( t ) = t and ignore the regular-ization terms. So, Equation (8) can be rewritten as follows: where  X  + ij equals to 1 when ( x i , x j )  X   X  a otherwise 0,  X   X  ij equals to 1 when ( x i , x j )  X   X  r otherwise note respectively. Therefore, maximizing above function is equivalent to maximizing the following: Clearly, Equation (12) is analogous to Linear Dis-criminant Analysis (LDA) (Duda et al. , 2000) ex-cept for the difference: 1) measurement. S 3 H uses similarity while LDA uses distance. As a result, the objective function of S 3 H is just the reciprocal of LDA X  X . 2) embedding space. LDA seeks the best separative direction in the original attribute space. In contrast, S 3 H firstly maps data from R M to R M  X  L through the following projection function where r l  X  R M ,l = 1 ,...,L , are L random hyper-planes. Then, in that space ( R M  X  L ), S 3 H seeks a direction 5 that can best separate the data. From this point of view, it is obvious that the basic SH is a special case of S 3 H when w is set to e = [1 , 1 ,..., 1] . That is, SH firstly maps the data via  X  (  X  ) just as S 3 H. But then, SH directly separates the data in that feature space at the direction e .
Analogously, we ignore the regularization terms in SSH and rewrite the objective of SSH as: where W = [ w 1 ,..., w L ]  X  R M  X  L are L hyper-planes and X = [ x 1 ,..., x N ] . Maximizing this ob-jective is equivalent to maximizing the following: where S  X  + = X X  + X T and S  X  X  X  = X X   X  X T . Equa-tion (15) shows that SSH is analogous to Multiple Discriminant Analysis (MDA) (Duda et al. , 2000). In fact, SSH uses top L best-separative hyperplanes in the original attribute space found via PCA to hash the data. Furthermore, we rewrite the projection function  X  (  X  ) in S 3 H as: where R l = diag(sign( r l )) . Each R l is a mapping from R M to R M and corresponds to one embedding space. From this perspective, unlike SSH, S 3 H glob-ally seeks a direction that can best separate the data in L different embedding spaces simultaneously. We use two datasets 20 Newsgroups and Open Di-rectory Project (ODP) in our experiments. Each doc-ument is represented as a vector of occurrence num-bers of the terms within it. The class information of docs is considered as prior knowledge that two docs within a same class should have more similar fingerprints while two docs within different classes should have dissimilar fingerprints. We will demon-strate that our S 3 H can effectively incorporate this prior knowledge to improve the DSS performance.
We use Inverted List (IL) (Manning et al. , 2002) as the baseline. In fact, given a query doc, IL re-turns all the docs that contain any term within it. We also compare our method with three state-of-the-art hashing methods, i.e., KLSH, SSH and SH. In KLSH, we adopt the RBF kernel  X  ( x i , x j ) = exp(  X  0.5 and the other two parameters p and t are set to be 500 and 50 respectively. The parameter  X  in SSH is set to 1. For S 3 H, we simply set the parameters  X  1 and  X  2 in Equation (8) to 4 and 0.5 respectively. To objectively reflect the performance of S 3 H, we eval-uate our S 3 H with and without Feature Contribution Calculation algorithm (FCC) (Algorithm 1). Specif-ically, FCC-free S 3 H (denoted as S 3 H f ) is just a simplification when G s in S 3 H are simply set to D s.
For quantitative evaluation, as in literature (Wang et al. , 2010b; Mu et al. , 2009), we calculate the pre-cision under two scenarios: hash lookup and hash ranking. For hash lookup, the proportion of good neighbors (have the same class label as the query) among the searched objects within a given Hamming radius is calculated as precision. Similarly to (Wang et al. , 2010b; Weiss et al. , 2009), for a query doc-ument, if no neighbors within the given Hamming radius can be found, it is considered as zero preci-sion. Note that, the precision of IL is the propor-tion of good neighbors among the whole searched objects. For hash ranking, all the objects in X are ranked in terms of their Hamming distance from the query document, and the top K nearest neighbors are returned as the result. Then, Mean Averaged Pre-cision (MAP) (Manning et al. , 2002) is calculated. We also calculate the averaged intra-and inter-class Hamming distance for various hashing methods. In-tuitively, a good hashing method should have small intra-class distance while large inter-class distance.
We test all the methods on a PC with a 2.66 GHz processor and 12GB RAM. All experiments repeate 10 times and the averaged results are reported. 4.1 20 Newsgroups 20 Newsgroups 6 contains 20K messages, about 1K messages from each of 20 different newsgroups. The entire vocabulary includes 62,061 words. To evaluate the performance for different feature di-mensions, we use Chi-squared feature selection al-gorithm (Forman, 2003) to select 10K and 30K fea-tures. The averaged message length is 54.1 for 10K features and 116.2 for 30K features. We randomly select 4K massages as the test set and the remain 16K as the training set. To train SSH and S 3 H, from the training set, we randomly generate 40K message-pairs as  X  a and 80K message-pairs as  X  r .
For hash ranking, Figure 1 shows MAP for vari-ous methods using different number of bits. It shows that performance of SSH decreases with the grow-ing of hash bits. This is mainly because the variance of the directions obtained by PCA decreases with the decrease of their ranks. Thus, lower bits have larger empirical errors. For S 3 H, FCC (Algorithm 1) can significantly improve the MAP just as discussed in Section 3.2. Moreover, the MAP of FCC-free S H (S 3 H f ) is affected by feature dimensions while FCC-based (S 3 H) is relatively stable. This implies FCC can also improve the satiability of S 3 H. As we see, S 3 H f ignores the contribution of features to dif-ferent classes. However, besides the local descrip-tion of data locality in the form of object-pairs, such (global) information also provides a proper guidance for hashing. So, for S 3 H f , the reason why its re-sults with 30K features are worse than the results with 10K features is probably because S 3 H f learns to hash only according to the local description of data locality and many not too relevant features lead to relatively poor description. In contrast, S 3 H can utilize global information to better understand the similarity among objects. In short, S 3 H obtains the best MAP for all bits and feature dimensions.
For hash lookup, Figure 2 presents the precision within Hamming radius 3 for different number of bits. It shows that IL even outperforms SH. This is because few objects can be hashed by SH into one hash bucket. Thus, for many queries, SH fails to return any neighbor even in a large Hamming radius of 3. Clearly, S 3 H outperforms all the other methods for different number of hash bits and features.
The number of messages searched by different methods are reported in Figure 3. We find that the number of searched data of S 3 H (with/without FCC) decreases much more slowly than KLSH, SH and SSH with the growing of the number of hash bits. As discussed in Section 3.4, this mainly benefits from the design of S 3 H that S 3 H (globally) seeks a di-rection that can best separate the data in L embed-ding spaces simultaneously. We also find IL returns a large number of neighbors of each query message which leads to its poor efficiency.

The averaged intra-and inter-class Hamming dis-tance of different methods are reported in Table 1. As it shows, S 3 H has relatively larger margin (  X  ) between intra-and inter-class Hamming distance. This indicates that S 3 H is more effective to make similar points have similar fingerprints while keep the dissimilar points away enough from each other.
Figure 4 shows the (training) computational com-plexity of different methods. We find that the time and space cost of SSH grows much faster than SH, KLSH and S 3 H with the growing of feature dimen-sion. This is mainly because SSH requires SVD to find the optimal hashing functions which is compu-tational expensive. Instead, S 3 H seeks the optimal feature weights via L-BFGS, which is still efficient even for very high-dimensional data. 4.2 Open Directory Project (ODP) Open Directory Project (ODP) 7 is a multilingual open content directory of web links (docs) organized by a hierarchical ontology scheme. In our exper-iment, only English docs 8 at level 3 of the cate-gory tree are utilized to evaluate the performance. In short, the dataset contains 2,483,388 docs within 6,008 classes. There are totally 862,050 distinct words and each doc contains 14.13 terms on aver-age. Since docs are too short, we do not conduct feature selection 9 . An overview of ODP is shown in Figure 5. We randomly sample 10 % docs as the test set and the remain as the training set. Furthermore, from training set, we randomly generate 800K doc-pairs as  X  a , and 1 million doc-pairs as  X  r . Note that, since there are totally over 800K features, it is extremely inefficient to train SSH. Therefore, we only compare our S 3 H with IL, KLSH and SH.
The search performance is given in Figure 6. Fig-ure 6(a) shows the MAP for various methods using different number of bits. It shows KLSH outper-forms SH, which mainly contributes to the kernel trick. S 3 H and S 3 H f have higher MAP than KLSH and SH. Clearly, FCC algorithm can improve the MAP of S 3 H for all bits. Figure 6(b) presents the precision within Hamming radius 2 for hash lookup. We find that IL outperforms SH since SH fails for many queries. It also shows that S 3 H (with FCC) can obtain the best precision for all bits. Table 2 reports the averaged intra-and inter-class Hamming distance for various methods. It shows that S 3 H has the largest margin (  X  ). This demon-strates S 3 H can measure the similarity among the data better than KLSH and SH.

We should emphasize that KLSH needs 0.3ms to return the results for a query document for hash lookup, and S 3 H needs &lt; 0.1ms. In contrast, IL re-quires about 75ms to finish searching. This is mainly because IL always returns a large number of ob-jects (dozens or hundreds times more than S 3 H and KLSH) and requires much time for post-processing.
All the experiments show S 3 H is more effective, efficient and stable than the baseline method and the state-of-the-art hashing methods. We have proposed a novel supervised hashing method named Semi-Supervised Simhash (S 3 H) for high-dimensional data similarity search. S 3 H learns the optimal feature weights from prior knowledge to relocate the data such that similar objects have similar fingerprints. This is implemented by max-imizing the empirical accuracy on labeled data to-gether with the entropy of hash functions. The proposed method leads to a simple Quasi-Newton based solution which is efficient even for very high-dimensional data. Experiments performed on two large datasets have shown that S 3 H has better search performance than several state-of-the-art methods. We thank Fangtao Li for his insightful suggestions. We would also like to thank the anonymous review-ers for their helpful comments. This work is sup-ported by the National Natural Science Foundation of China under Grant No. 60873174.
