 Abstract LETOR is a benchmark collection for the research on learning to rank for information retrieval, released by Microsoft Research Asia. In this paper, we describe the details of the LETOR collection and show how it can be used in different kinds of researches. Specifically, we describe how the document corpora and query sets in LETOR are selected, how the documents are sampled, how the learning features and meta infor-mation are extracted, and how the datasets are partitioned for comprehensive evaluation. We then compare several state-of-the-art learning to rank algorithms on LETOR, report possible new research topics that can be supported by LETOR, in addition to algorithm comparison. We hope that this paper can help people to gain deeper understanding of LETOR, and enable more interesting research projects on learning to rank and related topics.
 Keywords Learning to rank Information retrieval Benchmark datasets Feature extraction 1 Introduction Ranking is the central problem for many applications of information retrieval (IR). These include document retrieval (Cao et al. 2006 ), collaborative filtering (Harrington 2003 ), key term extraction (Collins 2002 ), definition finding (Xu et al. 2005 ), important email routing objects, we utilize a ranking model (function) to create a ranked list of the objects. The relative order of objects in the list may represent their degrees of relevance, preference, or importance, depending on applications. Among the aforementioned applications, document example when performing the discussions in this paper.

Learning to rank , when applied to document retrieval, is a task as follows. Assume that there is a corpus of documents. In training, a number of queries are provided; each query is associated with a set of documents with relevance judgments; a ranking function is then created using the training data, such that the model can precisely predict the ranked lists in the training data. In retrieval (i.e., testing), given a new query, the ranking function is used to create a ranked list for the documents associated with the query. Since the learning to rank technology can successfully leverage multiple features for ranking, and can auto-matically learn the optimal way of combining these features, it has been gaining increasing attention in recent years. Many learning to rank methods have been proposed and applied to different IR applications.

To facilitate the research on learning to rank, an experimental platform is sorely needed, which contains indexed document corpora, selected queries for training and test, feature vectors extracted for each document, implementation of baseline algorithms, and standard evaluation tools. However, there was no such an environment and it largely blocked the advancement of the related research. Researchers had to use their own datasets (i.e., different document corpora, different query sets, different features, and/or different eval-uation tools), and thus it was not possible to make meaningful comparison among different methods. This is in sharp contrast with several other fields where research has been significantly enhanced by the availabilities of benchmark collections, such as Reuters 21578 1 and RCV1 (Lewis et al. 2004 ) for text categorization, and UCI (Asuncion et al. 2007 ) for general classification. In order to accelerate the research on learning to rank, we decided to build the benchmark collection LETOR. The construction of such a collection engineering efforts. Thanks to the contributions from many people, we were able to release LETOR and upgrade it for several times.

LETOR was constructed based on multiple data corpora and query sets, which have been widely used in IR. The documents in the corpora were sampled according to carefully designed strategies, and then features and metadata were extracted for each query-docu-ment pair. Additional information including hyperlink graph, similarity relationship, and sitemap was also included. The data was partitioned into five folds for cross validation, and standard evaluation tools were provided. In addition, the ranking performances of several state-of-the-art ranking methods were also provided, which can serve as baselines for newly developed methods.

LETOR has been widely used in the research community since its release. The first version of LETOR was released in April 2007 and used in the SIGIR 2007 workshop on learning to rank for information retrieval ( http://www.research.microsoft.com/users/ LR4IR-2007/ ). At the end of 2007, the second version of LETOR was released, which was later used in the SIGIR 2008 workshop on learning to rank for IR ( http://www. research.microsoft.com/users/LR4IR-2008/ ). Based on the valuable feedback and sugges-tions we collected, the third version of LETOR was released in December 2008. The focus of this paper is on the third version, 2 LETOR 3.0.

The contributions of LETOR to the research community lie in the following aspects. 1. It eases the development of ranking algorithms .
 2. It makes the comparison among different learning to rank algorithms possible . 3. It offers opportunities for new research topics on learning to rank .

The remaining part of the paper is organized as follows. We introduce the problem of learning to rank for IR in Sect. 2 . Section 3 gives a detailed description about LETOR. Section 4 reports the performances of several state-of-the-art learning to rank algorithms on LETOR. We then show how LETOR can be used to study other research topics beyond algorithm comparison in Sect. 5 . Finally limitations of LETOR are discussed in Sect. 6 and concluding remarks are given in Sect. 7 . 2 Learning to rank for IR There are two major approaches to tackle the ranking problems in IR: the learning to rank approach and the traditional non-learning approach such as BM25 (Robertson et al. 2000 ) and language model (Zhai et al. 2001 ).

The main difference between the two approaches lies in that the former can automatically learn the parameters of the ranking function using training data, while the latter usually determines the parameters heuristically. If a ranking model has only several parameters, heuristic tuning can be possible. However, if there are many parameters, it will become very difficult. As more and more evidences have been proved as useful for ranking, the traditional non-learning approach will face challenges in effectively using these evidences. In contrast, the learning to rank approach can make good use of multiple evidences. Therefore learning to rank has been drawing broad attention in both machine learning and IR communities, and many learning to rank algorithms have been proposed recently. Roughly speaking, there are mainly three kinds of algorithms, namely, the pointwise approach (Li et al. 2006 , 2008 ), the pairwise approach (Burges et al. 2005 ; Freund et al. et al. 2008 ; Volkovs et al. 2009 ; Xia et al. 2008 ; Xu et al. 2007 ; Yue et al. 2007 ).
The pointwise approach regards a single document as its input in learning and defines its loss function based on individual documents. According to different output spaces of the regression based algorithms (Li et al. 2006 ).

The pairwise approach takes document pairs as instances in learning, and formalizes the problem of learning to rank as that of pairwise classification. Specifically, in learning it collects or generates document pairs from the training data, with each document pair ranking model by using classification technologies. The uses of support vector machines (SVM), Boosting, and Neural Network as the classification model lead to the methods of Ranking SVM (Herbrich et al. 1999 ), RankBoost (Freund et al. 2003 ), and RankNet (Burges et al. 2005 ). Many other algorithms have also been proposed, such as FRank (Tsai et al. 2007 ), multiple hyperplane ranker (Qin et al. 2007 ) and nested ranker (Matveeva et al. 2006 ).

The listwise approach takes document lists as instances in learning and the loss function Cosine (Qin et al. 2008e ), relational ranking (Qin et al. 2008d ), global ranking (Qin et al. usually referred to as the direct optimization of IR measures. Example algorithms include AdaRank (Xu et al. 2007 ), SoftRank (Taylor et al. 2008 ), SVM-MAP (Yue et al. 2007 ), PermuRank (Xu et al. 2008 ), ApproxRank (Qin et al. 2008b ), and BoltzRank (Volkovs et al. 2009 ). 3 Creating LETOR collection four main steps: selecting document corpora (together with query sets), sampling docu-ments, extracting learning features and meta information, and finalizing datasets. 3.1 Selecting document corpora In the LETOR collection, we selected two document corpora: the  X  X  X ov X  X  corpus and the OHSUMED corpus. These two corpora were selected because (1) they were publicly available (Voorhees et al. 2005 ) and (2) they had been widely used by previous work on ranking in the literature of IR (Cao et al. 2006 ;Craswelletal. 2004 , 2003 ; Robertson et al. 2000 ). 3.1.1 The  X  X  X ov X  X  corpus and six query sets In TREC 2003 and 2004, there is a special track for web IR, named the Web track (Craswell et al. 2004 , 2003 ). The tracks used the  X  X  X ov X  X  corpus, which is based on a January, 2002 crawl of the  X  X  X ov X  X  domain. There are about one million html documents in this corpus.
Three search tasks are defined in the Web track: topic distillation (TD), homepage finding (HP) and named page finding (NP). Topic distillation aims to find a list of entry points of good websites principally devoted to the topic. The focus is to return entry pages of good websites rather than the web pages containing relevant information, because entry pages provide a better overview of the websites. Homepage finding aims at returning the homepage of the query. Named page finding is about finding the page whose name is named page finding. Many papers (Qin et al. 2005 , 2007 ; Xu et al. 2007 ; Xue et al. 2005 ) have been published using the three tasks on the  X  X  X ov X  X  corpus as the basis for evaluation. The following example illustrates the differences among these three tasks (TREC 2004 ). Consider the query  X  X SGS X , which is the acronym for the US Geological Survey.  X  For topic distillation, the query means  X  X  X ind for me homepages of sites describing the  X  For homepage finding, the query means  X  X  X ind for me the URL of the USGS homepage  X  For named page finding, the query may mean find me the URL of a non-homepage,
The numbers of queries in these three tasks are shown in Table 1 . For simplicity, we use acronyms in following sections: TD2003 for the topic distillation query set in TREC2003, TD2004 for the topic distillation query set in TREC2004, NP2003 for the named page finding query set in TREC2003, NP2004 for the named page finding query set in TREC2004, HP2003 for the homepage finding query set in TREC2003, and HP2004 for the homepage finding query set in TREC2004. 3.1.2 The OHSUMED corpus The OHSUMED corpus (Hersh et al. 1994 ) is a subset of MEDLINE, a database on medical publications. It consists of about 0.3 million records (out of over 7 million) from 270 medical journals during the period of 1987 X 1991. The fields of a record include title, abstract, MeSH indexing terms, author, source, and publication type.

A query set with 106 queries on the OHSUMED corpus has been widely used in previous work (Qin et al. 2007 ; Xu et al. 2007 ), in which each query describes a medical search need (associated with patient information and topic information). The relevance degrees of the documents with respect to the queries are judged by human annotators, on three levels: definitely relevant, partially relevant, and irrelevant. There are in total 16,140 query-document pairs with relevance judgments. 3.2 Sampling documents Due to the large scale of the corpora, it is not feasible to judge the relevance of all the documents to a given query. As a common practice in IR, given a query, only some necessary to extract feature vectors from all the documents in the corpora. A reasonable approach is to sample some  X  X  X ossibly X  X  relevant documents, and then extract feature vectors from the corresponding query-document pairs. In this section, we introduce the sampling strategy used in the construction of LETOR.

For the  X  X  X ov X  X  corpus, given a query, the annotators organized by the TREC committee labeled some relevant documents. For the remaining unlabeled documents, the TREC committee treated them as irrelevant in the evaluation process (Craswell et al. 2003 ). Following this practice, in LETOR, we also treated the unlabeled documents for a query as ( 2008 ), we performed the document sampling in the following way. We first used the BM25 model to rank all the documents with respect to each query, and then selected the top 1,000 documents for each query for feature extraction. Note that for some rare queries, less than 1,000 documents can be retrieved. As a result, some queries have less than 1,000 associated documents in LETOR.

Different from the  X  X  X ov X  X  corpus in which unjudged documents are regarded as irrel-evant, in OHSUMED, the judgments explicitly contain the category of  X  X  X rrelevant X  X  and the unjudged documents are ignored in evaluation (Hersh et al. 1994 ). Following this practice, we only sampled judged documents for feature extraction and ignored the unjudged documents. As a result, on average, a query has about 152 documents associated for feature extraction. 3.3 Extracting learning features In IR, given a query, we would like to rank a set of documents according to their relevance and importance to the query. In learning to rank, each query-document pair is represented by a multi-dimensional feature vector, and each dimension of the vector is a feature indicating how relevant or important the document is with respect to the query. For example, the first element of the vector could be the BM25 score of the document with regards to the query; the second element could be the frequency of the query terms appearing in the document; and the third one could be the PageRank score of the docu-ment. In this section, we introduce how the features in LETOR were extracted.

The following principles were used in the feature extraction process of LETOR. 1. To cover as many classical features in IR as possible. 2. To reproduce as many features proposed in recent SIGIR papers as possible, which 3. To conform to the settings in the original documents or papers. If the authors Fig. 1 . Each row represents a query-document pair. The first column is the relevance judgment of the query-document pair; the second column is the query ID, followed with the feature ID and feature values. 3.3.1 Extracting features for the  X  X  X ov X  X  corpus For the  X  X  X ov X  X  corpus, we extracted 64 features in total for each query-document pair, as shown in Table 2 . Note that the id of each feature in this table is the same as in the LETOR dependent on both the query and the document, D means that the feature only depends on the document, and Q means that the feature only depends on the query. Here we would like to point out that a linear ranking function cannot make use of the class-Q features, since these features are the same for all the documents under a query.

Some details about these features are listed as below. 1. We considered five streams/fields (Robertson et al. 2004 ) of a document: body, 3. c ( q i , d ) denotes the number of occurrences of query term q i in document d . Note that 4. Inverse document frequency (IDF) of query term q i was computed as follows, 5. | d | denotes the length (i.e., the number of words) of document d . When considering a 6. The BM25 score of a document d was computed as follows, 7. For the language model features (26 X 40, 62 X 64), the implementation and the 8. For sitemap-based propagation features (41 X 42), we set the propagation rate a = 0.9 9. Because the original PageRank score of a document is usually very small, we 10. For topical PageRank and topical HITS, the same 12 categories as in Nie et al. ( 2006 ) 11. To count the number of children of a web page (for feature 60), we simply used the 12. Extracted title was extracted from the content of a html document (Hu et al. 2005 ), 3.3.2 Extracting features for the OHSUMED corpus For the OHSUMED corpus, there are in total 45 features extracted from the steams/fields feature in this table is the same as in the LETOR datasets. We categorized the features into two classes: Q-D means that the feature depends on both the query and the document, and Q means that the feature only depends on the query.

Some details about these features are listed as below. 1. For the language model features (13 X 15, 28 X 30, 43 X 45), the implementation and 2. For BM25 feature, we set its parameters as k 1 = 1.2, k 3 = 7 and b = 0.75. 3.4 Extracting meta information In addition to the learning features, meta information that can be used to reproduce these learning features and even to create new features is also provided in LETOR. With the help of the meta information, one can conduct research on feature engineering, which is very important to learning to rank. For example, one can tune the parameters in existing learning features such as BM25 and LMIR, or investigate new features.

There are three kinds of meta information. The first is about the statistical information of a corpus; the second is about the raw information of the documents associated with a query; and the third is about the relationship among documents in a corpus. 3.4.1 Corpus meta information An xml file was created to contain the information about the corpus, as shown in Fig. 2 .As streams, the number of (unique) terms in each stream, and so on. One can easily obtain quantities like avgdl based on such information. 3.4.2 Query meta information An xml file was created for each individual query, as shown in Fig. 3 , containing the raw information about the query and its associated documents. Line 3 in the figure indicates the ID of the query, which comes from the original corpus. Other information includes streaminfo, terminfo, and docinfo. 3. The node  X  X  X ocinfo X  X  (line 25 X 40 in Fig. 3 ) plays a role similar to the forward index 3.4.3 Additional meta information As requested by researchers working on learning to rank, we have created several addi-relationship matrix of the corpora. The hyperlink graph and the sitemap information (built by analyzing URLs of all the documents according to some heuristic rules) are specific for SUMED corpus, which describes the similarity between all the sampled documents dependent ranking (Qin et al. 2008c , 2008d ; Zhai et al. 2003 ). 3.5 Finalizing datasets As described in Sect. 3.1 , there are seven datasets in the LETOR collection: TD2003, TD2004, NP2003, NP2004, HP2003, HP2004, and OHSUMED. There are three versions for each dataset: Feature_NULL, Feature_MIN, and QueryLevelNorm. 1. Feature_NULL : Since some documents may not contain query terms in certain streams 2. Feature_MIN : In this version, the  X  X  X ULL X  X  values have been replaced by the minimal 3. QueryLevelNorm : Taking into consideration that the values of different features or
We partitioned each dataset into five parts with about the same number of queries, denoted as S1, S2, S3, S4, and S5, for five-fold cross validation. In each fold, we propose using three parts for training, one part for validation, and the remaining part for test (See Table 4 ). The training set is used to learn ranking models. The validation set is used to tune the hyper parameters of the learning algorithms, such as the number of iterations in RankBoost and the combination coefficient in the objective function of Ranking SVM. The test set is used to evaluate the performance of the learned ranking models. Note that since we conducted five-fold cross validation, the reported performance of a ranking method is actually the average performance over the five trials.

The LETOR collection, containing the aforement ioned feature representations of documents, test sets, can be downloaded from http://www.research.microsoft.com/ * letor/ . 4 Benchmarking the collection On top of the data collections introduced above, standard evaluation tools and the eval-uation results for several representative learning to rank algorithms are also provided in LETOR. The availability of these baseline results can ease the comparison between existing algorithms and newly designed algorithms, and ensure the objectiveness and correctness of the comparison.
 baselines algorithms, and make discussions on the experimental results.
 4.1 Evaluation measures and tools Three widely used measures were adopted for evaluation: precision at position k (Baeza-Yates et al. 1999 ), average precision (AP) (Baeza-Yates et al. 1999 ), and normalized discounted cumulative gain (NDCG) (Ja  X  rvelin et al. 2002 ). 4.1.1 Precision at position k (P@k) P@ k is a measure for evaluating top k positions of a ranked list using two levels (relevant and irrelevant) of relevance judgment: where k denotes the truncation position and
For a set of queries, we averaged the P@ k values of all the queries to get the mean P@ k value. Since P@ k requires binary judgments while there are three levels of relevance judgments in the OHSUMED corpus, we simply propose treating  X  X  X efinitely relevant X  X  as relevant and the other two levels as irrelevant when computing P@ k . 4.1.2 Average precision (AP) Average precision (AP), another measure based on two levels of relevance judgment, is defined on the basis of P@ k : where N indicates the number of retrieved documents and | D ? | denotes the number of compute AP for this query. Then MAP is defined as the mean of AP over a set of queries. 4.1.3 Normalized discount cumulative gain (NDCG) NDCG@ k is a measure for evaluating top k positions of a ranked list using multiple levels (labels) of relevance judgment. It is defined as follows, where k has the same meaning as in Eq. 3 ; N k denotes the maximum 3 of function: and d ( j ) denotes a discount function:
Note that in order to calculate NDCG scores, we need to define the rating of each document. For the OHSUMED dataset, we defined three ratings 0, 1, 2, corresponding to six datasets, we defined two ratings 0, 1 corresponding to  X  X  X rrelevant X  X  and  X  X  X elevant X  X .
To avoid differences in the evaluation results due to different implementations of these measures, a set of standard evaluation tools are provided in LETOR. The tools are written in perl, and can output Precision, MAP and NDCG for the ranking results of a given ranking algorithm, and the significance test results between two given ranking algorithms. We encourage all the users to use the tools. By using a single set of evaluation tools, the experimental results of different methods can be easily and impartially compared. The input to the evaluation tools should be one of the original datasets in the collection, because the evaluation tools (such as Eval-Score-3.0.pl) sort the documents according to their input order when the documents have the same ranking scores. That is, the evaluation tools are sensitive to the order of documents in the input file. 4.2 Baseline ranking algorithms We tested a number of learning to rank algorithms on LETOR and provided the corre-sponding results as baselines. To make fair comparisons, we tried to use the same setting RankBoost and FRank, which adopted non-linear ranking functions by combining multiple binary weak rankers. Secondly, all the algorithms utilized MAP on validation set for model selection. 4.2.1 Pointwise approach As for the pointwise approach, we tested the linear regression based algorithm on LETOR. This algorithm aims to learn a linear ranking function which maps a feature vector to a real value. Since only relevance label is provided for each query-document pair, one needs to map the label to a real value for training. The general rule is that after mapping the real value of a more relevant document should be larger that that of a less relevant document. In our experiments, we used the validation set to select a good mapping from labels to real values. 4.2.2 Pairwise approach Several pairwise ranking algorithms were tested on LETOR, including Ranking SVM, RankBoost, and FRank.

The basic idea of Ranking SVM (Herbrich et al. 1999 ; Joachims 2002 ) is to formalize learning to rank as a problem of binary classification on document pairs, and then to solve the classification problem using Support Vector Machines. The public tool of SVMlight was used in our experiment. We chose the linear ranking function, and tuned the parameter SVMlight.

RankBoost (Freund et al. 2003 ) also formalizes learning to rank as a problem of binary boosting algorithms, RankBoost trains one weak ranker at each round of iteration, and combines these weak rankers to get the final ranking function. After each round, the document pairs are re-weighted by decreasing the weights of correctly ranked pairs and increasing the weights of incorrectly ranked pairs. In our implementation, we defined each weak ranker on the basis of a single feature. With a proper threshold, the weak ranker has binary output {0, 1}. For each round of iteration, we selected the best weak ranker from (number of features) 9 (255 thresholds) candidates. The validation set was used to determine the best number of iterations.

FRank (Tsai et al. 2007 ) is another pairwise ranking algorithm that utilizes a novel loss function called the fidelity loss within the probabilistic ranking framework (Burges et al. 2005 ). The fidelity loss has several nice properties for ranking. To efficiently minimize the fidelity loss, a generalized additive model is adopted. In our experiments, the validation set was employed to determine the number of weak learners in the additive model. 4.2.3 Listwise approach Four listwise ranking algorithms were tested on LETOR, i.e., ListNet, AdaRank-MAP, AdaRank-NDCG, and SVMMAP.

ListNet (Cao et al. 2007 ) is based on a probability distribution on permutations. Spe-defines another distribution based on the ground truth labels. After that, the cross entropy between the two distributions is used to define the loss function. To define the distribution based on the ground truth, ListNet needs to map the relevance label of a query-document pair to a real-valued score. In our experiments, the validation set was used to determine the mapping.
 AdaRank-MAP (Xu et al. 2007 ) manages to directly optimize MAP. The basic idea of AdaRank-MAP is to repeatedly construct  X  X eak rankers X  on the basis of re-weighted dictions. AdaRank-MAP utilizes MAP to measure the goodness of a weak ranker. In our experiments, the validation set was employed to determine the number of weak rankers.
AdaRank-NDCG (Xu et al. 2007 ) manages to directly optimize NDCG. The basic idea of AdaRank-NDCG is similar to that of AdaRank-MAP. the difference lies in that Ada-Rank-NDCG utilizes NDCG to measure the goodness of a weak ranker. In our experi-ments, the validation set was employed to determine the number of weak rankers.
SVMMAP (Yue et al. 2007 ) is a structured support vector machine (SVM) method that optimizes an upper bound of (1-AP) in the predicted rankings. There is a hyper parameter in the loss function of SVMMAP. In our experiments, the validation set was employed to determine the value of the hyper parameter. 4.3 Results The ranking performances of the aforementioned algorithms are listed in Tables 5 , 6 , 7 , 8 , ranking algorithms perform very well on most datasets. Among the four listwise ranking algorithms, ListNet seems to be better than the others. AdaRank-MAP, AdaRank-NDCG and SVMMAP obtain similar performances. Pairwise ranking algorithms achieve good ranking accuracy on some (but not all) datasets. For example, RankBoost offers the best performance on TD2004 and NP2003; Ranking SVM shows very promising results on NP2003 and NP2004; and FRank achieves very good results on TD2004 and NP2004. In contrast, simple linear regression performs worse than the pairwise and listwise ranking algorithms. Its results are not so good on most datasets.

We observe that most ranking algorithms perform differently on different datasets. They may perform very well on some datasets but not so well on the other datasets. To evaluate the overall ranking performance of an algorithm, we used the number of other algorithms that it can beat over all the seven datasets as a measure. That is, performance of i -th algorithm on j -th dataset in terms of measure M (such as MAP), and 1 reference, we call this measure winning number . Figure 4 shows the winning number in terms of NDCG for all the algorithms under investigation. From this figure, we have the following observations 4 1. In terms of NDCG@1, among the four listwise ranking algorithms, ListNet is better 2. In terms of NDCG@3, ListNet and AdaRank-MAP perform much better than the other 3. For NDCG@10, one can get similar conclusions to those for NDCG@3.

Comparing NDCG@1, NDCG@3, and NDCG@10, it seems that the listwise ranking algorithms have certain advantages over other algorithms at top positions (position 1) of documents and make use of the position information of them. In contrast, the loss functions of the pointwise and pairwise algorithms are defined on a single document or a document pair. It cannot access the scores of all the documents at the same time and cannot see the position information of each document. Since most IR measures (such as MAP and NDCG) are position based, listwise algorithms which can see the position information in their loss functions should perform better than pointwise and pairwise algorithms which cannot see such information in their loss functions.

Figure 5 shows the winning number in terms of Precision and MAP. We have the following observations from the figure. 1. In terms of P@1, among the four listwise ranking algorithms, ListNet is better than 2. For P@3, one can get similar conclusions to those for P@1. 4. In terms of MAP, ListNet is the best one; Ranking SVM, AdaRank-MAP, and To summarize, the experimental results show that the listwise algorithms (ListNet, AdaRank-MAP, AdaRank-NDCG, and SVMMAP) have certain advantages over other algorithms, especially for the top positions of the ranking results.

Note that the above experimental results are in some sense still preliminary, since the result of almost every algorithm can be further improved. For example, for regression, we can add a regularization item to make it more robust; for Ranking SVM, if the time complexity is not an issue, we can remove the constraint of -# 5000 to achieve a better convergence of the algorithm; for ListNet, we can also add a regularization item to its loss function and make it more generalizable to the test set. Considering these issues, we would like to call for contributions from the research community. Researchers are encouraged to existing algorithms to LETOR. In order to let others re-produce the submitted results, the contributors are respectfully asked to prepare a package for the algorithm, including 1. a brief document introducing the algorithm; 2. an executable file of the algorithm; 3. a script to run the algorithm on the seven datasets of LETOR.

We believe with the collaborative efforts of the entire community, we can have more versatile and reliable baselines on LETOR, and better facilitate the research on learning to rank. 5 Supporting new research directions So far LETOR has mainly been used as an experimental platform to compare different algorithms. In this section, we show that LETOR can also be used to support many new research directions. 5.1 Ranking models functions, and simply uses a scoring function as the ranking model. It may be one of the study new algorithms using pairwise and listwise ranking functions. Please note the challenge of using a pairwise/listwise ranking function. That is, the test complexity will be much higher than that of using a scoring function. One should pay attention to the issue if he/she performs research on pairwise and listwise ranking functions. 5.2 Feature engineering contains rich meta information, it can be used to study feature related problems. 5.2.1 Feature extraction The performance of a ranking algorithm greatly depends on the effectiveness of the fea-tures used. LETOR contains the low-level information such as term frequency and doc-ument length. It also contains rich meta information about the corpora and the documents. These can be used to derive new features, and study their contributions to ranking. 5.2.2 Feature selection know, the discussions on feature selection for ranking are still very limited. LETOR contains tens of standard features, and it is feasible to use LETOR to study the selection of most effective features for ranking. 5.2.3 Dimensionality reduction features by transforming/combining the original features. Dimensionality reduction has been shown very effective in many applications, such as face detection and signal reduction for ranking. It is an important research topic, and LETOR can be used to support such research. 5.3 New ranking scenarios LETOR also offers the opportunities to investigate some new ranking scenarios. Here we give several examples as follows. 5.3.1 Query classification and query dependent ranking In most previous work, a single ranking function is used to handle all kinds of queries. This may not be appropriate, particularly for web search. Queries in web search may vary largely in semantics and user intentions. Using a single model alone would make com-promises among queries and result in lower accuracy in relevance ranking. Instead, it contains several different kinds of query sets (such as topic distillation, homepage finding, problems of query classification and query dependent ranking (Geng et al. 2008 ). 5.3.2 Beyond independent ranking Existing technologies on learning to rank assume that the relevance of a document is independent of the relevance of other documents. The assumption makes it possible to score each document independently first and sort the documents according to their scores after that. In reality, the assumption may not always hold. There are many retrieval applications in which documents are not independent and relation information among documents can be or must be exploited. For example, Web pages from the same site form a sitemap hierarchy. If both a page and its parent page are about the topic of the query, then it would be better to rank higher the parent page for this query. As another example, simi-larities between documents are available, and we can leverage the information to enhance relevance ranking. Other problems like Subtopic Retrieval (Qin et al. 2007 ) also need utilize relation information. LETOR contains rich relation information, including hyperlink graph, similarity matrix, and sitemap hierarchy, and therefore can well support the research on dependent ranking. 5.3.3 Multitask ranking and transfer ranking Multitask learning aims at learning several related task at the same time, and the learning of the tasks can benefit from each other. In other words, the information provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Transfer learning uses the data in one or more auxiliary domains to help the learning task in the main domain. Because LETOR contains seven query sets and three different retrieval tasks, it is a good test bed for multitask ranking and transfer ranking.
To summarize, although the current use of LETOR is mostly about algorithm com-parison, LETOR actually can be used to support much richer research agenda. We hope that more and more interesting researches can be carried out with the help of LETOR, and the state of the art of learning to rnak can be significantly advanced. 6 Limitations Although LETOR has been widely used, it has certain limitations as listed below. 6.1 Document sampling strategy For the  X  X  X ov X  X  datasets, the retrieval problem is essentially cast as a re-ranking task (for top 1,000 documents) in LETOR. On one hand, this is a common practice for real-world ciency: firstly a simple ranker (e.g., BM25) is used to select some candidate documents, and then a more complex ranker (e.g., the learning to rank algorithms as mentioned in the paper) is used to produce the final ranking result. On the other hand, however, there are also some retrieval applications that should not be cast as a re-ranking task. We will add datasets beyond re-ranking settings to LETOR in the future.

For the  X  X  X ov X  X  datasets, we sampled documents for each query using a cutoff number of 1,000. We will study the impact of the cutoff number on the performances of the ranking algorithms. It is possible that the dataset should be refined using a better cutoff number. 6.2 Features In both academic and industrial communities, more and more features have been studied and applied to improve ranking accuracy. The feature list provided in LETOR is far away from comprehensive. For example, document features (such as document length) are not included in the OHSUMED dataset, and proximity features are not included in all the seven datasets. We will add more features into the LETOR datasets in the future. 6.3 Scale and diversity of datasets LETOR is not yet very large. To verify the performances of learning to rank techniques for real-web search, large scale datasets are needed. We are working on some large scale datasets and plan to release them in the future versions of LETOR.

Although there are seven query sets in LETOR3.0, there are only two document corpora involved. We will create new datasets using more document corpora in the future. 6.4 Baselines Most baseline algorithms in LETOR use linear ranking functions. From Sect. 4.3 , we can see that the performances of these algorithms are not good enough, since the perfect ranking should achieved the accuracy of 1 in terms of all the measures (P@ k , MAP and NDCG). As pointed out in Sect. 3.3 , class-Q features cannot be effectively used by linear ranking func-tions. We will add more algorithms with nonlinear ranking functions as baselines of LETOR. We also encourage researchers in the community to test more non-linear ranking models. 7 Conclusions By explaining the data creation process and the results of the state-of-the-arts learning to rank algorithms in this paper, we have provided the information for people to better understand the nature of LETOR and to more effectively utilize the datasets in their research work.
 We have received a lot of comments and feedback about LETOR after its first release. We hope we can get more suggestions from the research community. We also encourage researchers to contribute to LETOR by submitting their results.

Finally, we expect that LETOR can be just a start for the research community to build benchmark datasets for learning to rank. With more and more such efforts, the research on learning to rank for IR can be significantly advanced.
 References
