 Text classification is a task which automatically assigns an appropriate category for a text according to its content. The t ask formally can be defined as follows. We text and l i is the corresponding label drawn from a set of discrete values indexed by { 1 , 2 ,...,k } . The training data is used to build a classification model h .Then for a given test text t whose class label is unknown, the training model is used to predict the class label l for this text. In recent years, with the rapid explosion of information, text in digital form comes from everywhere. In order to handle a large amount of text, automatically text classification has become not only an important research area, but also a urgent need in different kinds of applications. As a result, automatically text classification has been widely studied in machine learning and information retrieval community. When Chinese becomes more and more popular these years, Chinese text classification will be an important way to deal with the large amount of Chinese text.

The common procedure of text classification can be divided into three parts, text representation, feature selection a nd classification. Generally speaking, be-fore using a classifier, we should present e ach text as a vector in a high dimensional Euclidean space. Commonly, each word or character in text can be viewed as a feature. And all of these features compose of a feature space. Because the feature space is too large and redundant, for better generalization and performance, we usually apply some dimension reduction or feature selection methods to represent the data in a reasonable scale feature space. The last step is to build a classifica-tion model which will be used to predict the class label given new text. Chinese text classification is a little different from English text classification. We usually need one more step before t he common procedure X  X ord segmenta-tion, because Chinese sentences do not delimit words by spaces. Though word segmentation seems a nece ssary step, we think it may bring some potential prob-lems for classification. Obviously, segmentation errors may bring bad influence to the classification. And even the segmented results are totally correct, some useful information may also be lost which incarnate in our experiments later. Another problem is that word segmenta tion cannot recognize new words very well. For example, the word-based classification performance is not so good in Social network message data where so many new words exists [7]. So, a natural idea is to use character-bas ed N-gram instead of words.

In this paper, we propose a framework that adopts a character-based N-gram approach to Chinese text classification and uses regularized logistic regression classifier to solve the sparse problem of the N-gram data. There are two main contributions of our work. Firstly, we demonstrate words segmentation will lose some valuable information for classification, no matter the segmented results are correct or not. We also show that character-based N-gram text representation is more suitable for Chinese text classification. Secondly, for better generalization and interpretation, we introduce the L 1 regularized logistic regression ( L 1-LR) model to classify Chinese text which ca n get better classification performance.
The rest of this paper is organized as follows. In the next section, we will discuss the background. In section 3, we discuss our works on Chinese text clas-sification, including the detail of proposed classification method and analysis of doing so. In section 4, we present several experimental results and qualitative analysis of N-gram based regularized logistic regression in Chinese text classifi-cation followed by conclusions in section 5. In this section, we review some basic information of Chinese text classification, including text representation, feature se lection and classifier. In Chinese text representation, one way is to use word segmentation. For Chinese word segmen-tation, there are two mostly used word segmentation tools: ICTCLAS (Institute of Computing Technology, Chinese Lexical Analysis System) [2] and Stanford Word Segmenter [3]. Additionally, Stanford Word Segmenter has two certain specifications, that is PeKing University criterion (pku) and Chinese Treebank criterion (ctb). Different criterions will produce different results. Another way to represent text is N-gram based approach, which William and John [5] firstly used in English text classification and r eceived good effects. After preprocessing of text content, we should turn the text i nto feature vectors next. The commonly used approach is tf  X  idf [6], which is a weighted technology for text representa-tion. And a high value means the feature (word, character etc. ) is important for one text in corpus. Another way is to use 0-1 weight vector, indicating whether one feature appearance in a document or not.

After the generation of the feature vect ors, feature selection methods will be used to reduce the feature space. In text classification, commonly used feature selection approaches are Gini Index, Information Gain, Mutual Information and  X  -Statistic [4]. All of these four methods aim to find the relationship between features and class labels. According to some criterion, these methods give each feature a value that indicate the importance of this feature in classification. But these methods only pay attention to the relation between features and labels, and ignore the relationship between the features which is also important to classification.
 After the feature selection is performed ,wecanuseittotrainingaclassifier. In text classification, commonly used cla ssifier are SVM, Logistic regression, Decision Tree, Naive Bayes Classifiers and Neural Network Classifiers. Among these, SVM and Logistic regression are basically linear classifier and do well in text classification. Compared with SVM, the loss function of logistic regression is closer to a linear classifier. So, when we add a regularization term to the classifier, logistic regression is more able to reflect the difference among the difference regularized term than SVM. Furthermore, in large-scale sparse case, logistic regression can perform well compared with SVM [8]. Besides, logistic regression does well in many natural language processing applications [9]. In this paper, we will use logistic regression as our classifier. Automatic word segmentation error may influence the performance of Chinese text classification. Even if the segmented results are totally correct, some useful information will also be lost. Luo and Ohyama [1] have studied the impact of word segmentation on Chinese text classi fication. They compared text classifi-cation performance on automatic word segmentation, manual word segmenta-tion and character-based N-gram approach, respectively. And they used support vector machine (SVM) with linear kernel, polynomial kernel and radial basis function as classifiers, respectively. Th e results show that the manual word seg-mentation gets the best performance, and character-based N-gram also gets the better performance than automatic word segmentation. But in real application, it X  X  almost impossible to get manual word segmentation for each text. From their work, they don X  X  explain why N-gram features work or not, and what kinds of N-gram feature are most valuable for classification. In this paper, we will to-tally forget word segmentation and focus on how to effectively extract valuable N-gram features for classification.

Moreover, Zhang and OLES [10] have compared performance of several linear model in text classification, including regularized logistic regression. The results show that regularized logistic regression has a performance that is comparable with other state-of-the-art methods, especially when we have a large scale feature space. And as is well-known, L 1-regularized logistic regression is an outstanding method to generate a sparse model for classification. The sparse model can give us a chance to dig the huge potential of character-based N-gram for classification. Our work can be divided into two parts. First, we use character-based N-gram approach to represent Chinese text classification. Second, we use regularized logistic regression to train Chinese text classifier. 3.1 Text Representation Using character-based N-gram to represent text are dramatically simpler, we could avoid the complicated process for the word segmentation. We just extract a sequence of N consecutive characters. I n practice, we usually use N  X  3. Table. 1 shows an example of N-gram features.

We use tf  X  idf as the feature weight. A tf  X  idf value consists of two parts. One is term frequency (tf), another is inverse document frequency (idf). Term frequency counts the relevant frequency of one feature in a given text. Higher tf  X  idf value of one feature means that it is more important than others. Inverse document frequency indicates whether one feature appears in most of the documents or not. If one feature appears in most of the documents, it is useless for classification and its idf value will be lower. A tf  X  idf value for one feature in a text is computed as follows: where n i,j is the frequency of feature t i in text d j , | D | is the total number of documents.

By using N-gram, we can transfer text i nto feature vectors easily, without any complex word segmentation or other language specific techniques. So, the main measures we adopt are as follows: We use unigram and bigram or unigram, bigram and trigram to represent the text. And tf  X  idf is used as the weight. 3.2 Regularized Logistic Regression In logistic regression [10], we model the conditional probability as follows: Commonly, we use maximum likelihood estimation (MLE) to obtain an estimate of w , which minimizes the following equation: Equation 4 may be ill-conditioned numerically. One way to solve this problem is to use regularization. In regularized approximation, by adding a regularizer to the loss function, it can limit model complexity and tune parameters for better generalization. General form of regularization is as follows: Where L ( y i ,f ( x i )) is the loss function, and R ( f ) is the regularizer,  X   X  0is a coefficient, which aims to adjust the relationship between the two terms. The goal is to find a model f that is uncomplicated (if possible), and also makes the loss function L small.

In this paper, we use logistic regression with 1-norm regularizer for classifica-tion, denote as L 1-regularized ( L 1-LR) logistic regression. And for comparing, we also use logistic regression with 2-norm regularizer for classification, denote as L 2-regularized ( L 2-LR) logistic regression. The objective function of L 1-LR and L 2-LR are shown as follows, respectively: 3.3 Analysis of Using L 1-Regularized Logistic Regression As mentioned before, we choose L 1-regularized logistic regression as the classi-fier. The first reason is that, in character-based N-gram case, the feature space is too large to analysis. And the data sparsity is very serious. The second reason is that most of text classification tasks are linear separable [14]. A simple lin-ear model may perform well compared to complicated models. When compared with SVM, the loss function of logistic regression is closer to a linear classi-fier. If we plus a regularized term to the classifier, logistic regression is more able to reflect the difference among the difference regularized term than SVM. Therefore, we choose to use regularizer logi stic regression as classifier, especially, the L 1-regularized logistic regression. Since L 1-regularization is a sparse model, the feature vector produced by L 1-regularization has few er none-zero features. Due to the properties of 1-norm and 2-norm when searching in the hypothe-sis space, L 1-regularization encourage less features which may be important in classification to be nonzero and the res t features are zero. On the other hand, L 2-regularization is more like a kind of average, it encourage more features to be a small value. With this property, L 1-regularized logistic regression will select some key features from N-gram based feat ure space. These selected features may seem a bit weird by human, but are definitely valuable for classification. Those selected features can help to interpret th e significance of character-based N-gram approach.

Moreover, Andrew Y. Ng [11] proved that using L 1-regularization, the sample complexity (i.e., the number of training examples required to learn well) grows only logarithmically in the number of irrelevant features. But any rotationally invariant algorithm (e.g., L 2-regularized logistic regression) exist a worst case that the sample complexity grows at least linearly in the number of irrelevant features. According to this theorem when the irrelevant features are much more than the training text, L 1-regularization will also achieve a good results. More-over, text classification is the case that much more irrelevant features come from limited amount of text. 4.1 Setup To compare with the previous Chinese text classification, we also implement some common approaches in this paper. As mentioned above, we use ICTCLAS and Stanford Word Segmenter (pku and c tb) as word segmenter, respectively. Top 80 percent word features are selected with four feature selection methods, Gini Index, Information Gain, Mutual Information and  X  2 -Statistic. Then, we use SVM as a baseline which is a state-of-the-art classifier in text classification. We use libsvm [12] as tools to train a SVM classifier.

In N-gram based text classification, we use (1 + 2)-gram (use unigram and bigramfortextrepresentation)and(1+2+3)-gram(useunigram,bigramand trigram for text representation) in experiment. And we use liblinear [13] as tools to train the regularized logistic regression. For solving L 1-LR, the liblinear use newGLMNET algorithm, see [15] for computational complexity and other de-tails. Additionally, we also use regularized logistic regression on segmented text. At last, we use 10-fold cross validation in our experiments.
 4.2 Experiment on Chinese Corpus Fudan Chinese text classification corpus was used in our experiment (it is re-leased on http://www.nlpir.org/download/tc-corpus-answer.rar ). We se-lect 9 classes from this corpus. The total number of documents is 9330. The categories include art, history, space, computer, environment, agriculture, econ-omy, politics and sports. For a better comparison with these three different word segmenter, we list the results of N-gram three times. Table. 2 shows the result of text classification on Fudan corpus. Where the X 2 means that use some word segmenter and  X  2 -Statistic as feature selection. The Gini means that use some word segmenter and Gini Index as feature selection. The IG means that use some word segmenter and Informatio n Gain as feature selection. The MI means that use some word segmenter and Mutual Information as feature selection. The L 1-LR means the L 1-regularized logistic regression. And L 2-LR means the L 2-regularized logistic regression.

From Table. 2, it is obvious that character-based N-gram with L 1-regularized logistic regression does best in Chinese text classification regardless of which word segmenter is used. Our regularized classifier really does better than tra-ditional features selections methods. And in large scale data classification, reg-ularized logistic regression is more effective than SVM. Additionally, the result shows that dealing with the large and sparse text data, L 1-regularized logistic regression is better than L 2-regularized logistic regression. 4.3 Experiment on English Corpus Moreover, in order to further validate the generality of N-gram based regular-ized logistic regression approach. We experiment this method on English text classification. 20-News English text classification corpus was used in our experi-ment (it is released on http://qwone.com/ ~ jason/20Newsgroups/ ). We use 10 classes selected from 20-News corpus. The total number of documents is 10000 (1000 documents for each class). Then, we repeat the previous steps. Note that for English text, we use word-based N-gra m instead of character-based N-gram. The results are shown in Table. 3.

From Table. 3, it is obvious that word-based N-gram with regularized logistic regression does best in English text classification. The result shows that N-gram based regularized logistic regression also performs better than the state-of-the-art approach, in English text classification. 4.4 Accuracy Changes over Nonzero Features Furthermore, we present the variety cur ve of text classification accuracy over the number of the nonzero features. We use Fudan corpus as training data and use spline interpolation to reflect the variation trend. The result is shown in Figure.1. Note thatwe omitthe curve of(1+2+3)-gram,since they arealmost the same.

In Figure. 1, we can find that text classification accuracy grow rapidly with rising of nonzero features and reach the maximum at around 2000 nonzero fea-tures. Then, as the nonzero features con tinued to increase, t he accuracy comes down slightly. This also shows that the sparsity of text data from another side. And a small number of features selected by sparse model are enough to achieve good classification accuracy.
 4.5 Qualitative Analysis At last, in order to qualitative analysis that N-gram based L 1-regularized logis-tic regression can select some key featu res which we cannot obtain through word segmentation. We experiment on a binary -class Chinese text classification prob-lem. We select two class from Fudan Chinese corpus, which composed by 1357 documents labeled as  X  X omputer X  and 1601 documents labeled as  X  X conomy X . In order to reflect the importance of each f eature more directly, we use 0-1 vector instead of tf  X  idf . Thus, the importance of one feature is totally depend on the weight vector w . Then, we use (1 + 2 + 3)-gram and L 1-regularized logistic regression. After the classifier has been tr ained, we select some typical features from top ranked features. These f eatures are shown in Table. 4.

From Table. 4 we can find that most of the top features are N-gram form which cannot be generated by word segm enter. These N-gram features can be divided into three categories. The first category of features presents two separate words. These two words will appear in two classes of text, respectively. But, in one of the two classes, they appear sequentially. Taking  X   X  as an example, this trigram is the suffix of  X   X  (object-oriented). In segmented text, this feature is segmented as  X   X  (oriented) and  X   X  (object). The feature  X   X  (oriented) appears in 264 computer documents and 230 economy documents, respectively. The feature  X   X  (object) appears in 476 computer documents and 406 economy documents, respectivel y. The number of times they appear in these two classes are similar. As a resul t, these two features are not useful in classifying the two classes. But the feature  X   X  appears in 137 computer documents and 0 economy documents, resp ectively. Obviously, this trigram is more useful than  X   X  (oriented) and  X   X  (object) for classifying the two classes, significantly. Take  X   X  as another example which is the prefix of  X  as  X   X  (economic) and  X   X  (development). The feature  X   X  (economic) appears in 155 computer documents and 1548 economy documents, respectively. The feature  X   X  (development) appears in 534 computer documents and 1500 economy documents, respectively. But the feature  X   X  appears in 11 com-puter documents and 1220 economy documents, respectively, which is a much stronger indicator for classification.

The second category of features consist s of only one Chinese character. But this character is usually a prefix or suffix of a group of words. This group of words usually appears in only one of the two classes. Using just one Chinese character to represent a group of words is more effective. Take  X   X  as an example. In economy text, there are lots of words containing the character  X   X . (such as,  X   X  (politics),  X   X (policy), X   X  (government), etc.) But, from Table. 4, the feature  X   X  appears in 80 computer documents and 1453 economy documents, respectively. The number of times they ap pear in these two classes are different very much. It is obviously that one character will suffice.
 The third category of features presents the beginning or end of one sentence. Take  X   X  as an example. In segmented text, they are segmented as  X   X  X nd  X   X . But, from Table. 4, the feature  X   X  appears in 68 computer documents and 1028 economy documents, respectively. It is obviously that  X   X  X suseful in classification. As an explanation, we find that, in economy text, there are lots of quotes in the end of the sentence.

The above analysis shows that N-gram based L 1-regularized logistic regression can get some key features which cannot b e generated by word segmenter. But these information are useful in text classification, indeed. In this paper, we demonstrate the drawbacks of using word segmentation in Chinese text classification. We propose a framework that use character-based N-gram with regularized logistic regression on Chinese text classification. And, we made experiments on Fudan Chinese text classification corpus. Results of different word segmenters and different f eature selection methods are compared. The proposed method gets the best performance. Though quantitative and qual-itative analysis, we further discussed for experiments why the N-gram features work better than word features, and what kinds of N-gram features are valuable for classification.

But there are still some limitations of our method. For example, the regularizer we used doesn X  X  consider the relationship between features. We just use 1-norm or 2-norm as the regularizer. In the future work, we will consider adding structure information in the regularizer for better performance hopefully.

