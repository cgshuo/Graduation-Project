 1. Introduction
During decades the information retrieval (IR) area has provided users with methods and tools for searching interesting pieces of text among huge document collections. However, until very recently these techniques have been implemented apart from databases due to the very different nature of the objects they manage: whereas data is well-structured with well-defined semantics, texts are unstructured and require approximate query processing (Baeza-Yates &amp; Ribeiro-Neto, 1999).

Nowadays, corporate information systems need to include internal and external text-based sources (e.g., web documents) into the information processes defined within the organization. For example, decision support systems would greatly benefit from text-rich sources (e.g., financial news and market research reports) as they can help analysts to understand the histor-ical trends recorded in corporate data warehouses. Opinion forums and blogs are also valuable text-sources that can be of great interest for enhancing the decision making processes. Unfortunately, there are scarce works in the literature concerned with a true integration of data and document retrieval techniques.
 Recent proposals in the field of IR include language modeling (Ponte &amp; Croft, 1998 ) and relevance modeling (Lavrenko &amp; Croft, 2001 ). Language modeling represents each document as a language model. Thus, documents are ranked according to the probability of emitting the query keywords from the corresponding language model. Relevance modeling estimates the joint probability of the query X  X  keywords and the document words over the set of documents deemed relevant for that query. In this paper, we apply the language modeling and relevance modeling approaches to develop a new model that estimates the relevance of the facts stored into a data warehouse with respect to an IR query. These facts are well-structured data tu-ples, whose meaning is described by a set of documents retrieved with the same IR query from a separate text repository. The proposed relevance model is the core of the contextualized warehouse described in P X rez, Berlanga, Aramburu, and Pedersen (2008) . However, the topic of P X rez et al. (2008) was the multidimensional model of the contextualized warehouse, rather than the relevance model. In the current paper, we describe the relevance model in detail, and we compare it with the relevance-based language model techniques that support it. The paper provides a series of experiments over a well-known IR collection in order to demonstrate that the ranking of facts provided by the model is good enough for helping analysts in their tasks. This evaluation is completely new and has not been previously published anywhere. The review of the language modeling and relevance modeling approaches included in this paper is also an original contribution.

The rest of the paper is organized as follows: Section 2 overviews the contextualized warehouse. Section 3 reviews the language modeling and the relevance modeling IR approaches. Section 4 presents the contextualized warehouse relevance model and Section 5 evaluates it. Finally, Section 6 discusses some conclusions and future lines of work. 2. The contextualized warehouse
A contextualized warehouse is a new kind of decision support system that allows users to obtain strategic information by combining sources of structured data and documents. Fig. 1 shows the architecture of the contextualized warehouse pre-sented in P X rez et al. (2008) . Its main three components are a corporate data warehouse, a document warehouse and the fact extractor module. Next, we briefly describe these components: (a) The corporate data warehouse integrates data from the organization X  X  structured data sources (e.g., its different depart-(b) The document warehouse stores the unstructured data coming from internal and external sources. These documents (c) The objective of the fact extractor module is to relate the facts of the corporate warehouse with the documents that
In a contextualized warehouse, the user specifies an analysis context by supplying a sequence of keywords (i.e., an IR query Q like  X  X  X etrol crisis X ). The analysis is performed on a new type of OLAP cube, called R-cube , which is materialized by retrieving the documents and facts relevant for the selected context. Table 1 shows an example R-cube . Each row repre-and month. R-cubes have two special dimensions: the relevance ( R ) and the context ( Ctxt ) dimensions. In the relevance dimen-sion, each fact has a numerical value representing its relevance with respect to the specified context (e.g., how important the fact is for a  X  X  X etrol crisis X ). Thereby the name R-cube (Relevance cube). The context dimension links each fact to the set of documents that describe its context. In the R-cube , each d r j denotes a document whose relevance with respect to the analysis context is r .

The most relevant facts of our example R-cube are the facts f 4 and f 5 , which involve the sales made to Japanese and Korean customers during the months of October and November 1998. By studying the documents associated to these facts, e.g., the most relevant d 7 , we may find out a report on a petrol crisis that affected Japan and Korea during the second half of 1998. Probably, this report could explain why the sales represented by f 4 and f 5 experimented the sharpest drop.
The formal definition of the R-cube  X  X  multidimensional data model and algebra was given in P X rez et al., 2008 . A prototype of a contextualized warehouse was presented in ( P X rez, Berlanga, Aramburu, &amp; Pedersen, 2007 ). This paper presents the IR model of the contexutalized warehouse. Given a context of analysis (i.e., an IR query), we first retrieve the documents of the warehouse by following a language modeling approach. Then, we rely on relevance modeling to rank the facts described in the retrieved documents. Language modeling and relevance modeling establish a formal foundation based on probability theory, which is also well-suited for studying the influence of the R-cubes algebra operations in the relevance values of the facts ( P X rez et al., 2008 ). 3. Language models and relevance-based language models
The work on language modeling estimates a language model m j for each document d j . A language model is an stochastic process which generates documents by emitting words randomly. The documents d j are then ranked according to the prob-ability P  X  Q j m j  X  of emitting the query keywords Q from the respective language model m j (Ponte &amp; Croft, 1998 ).
The calculation of the probability P  X  Q j m j  X  differs from model to model. In Song andCroft (1999) the query Q is represented and the probability P  X  Q j m j  X  is computed by
Song and Croft (1999) propose to approach the probability P  X  q i j m j  X  of emitting the keyword q i from m j by smoothing the relative frequency of the query keyword in the document d j . Their approach avoids probabilities equal to zero in P  X  Q j m j  X  when a document does not contain all the query keywords. They make the assumption that finding a keyword in a document might be at least as probable as observing it in the entire collection of documents, and estimate this probability as follows: words in the document, cwf q i is the number of times that the query keyword q i occurs in all the documents of the collection, and coll size w is the total number of words in the collection. The k factor is the smoothing parameter, and its value is deter-mined empirically, k 2 X  0 ; 1 .

The retrieval model of the contextualized warehouse proposed in this paper also models the queries as sequences of key-words and follows a similar approach to compute the relevance of the documents.

Many well-known IR techniques, such as the relevance feedback, have a very intuitive interpretation in the classical prob-abilistic models ( Robertson, 1997 ). These techniques require modifying a sample set of relevant documents according to the user X  X  relevance judgments. However, they are difficult to integrate into the language modeling framework where there is not such a notion of set of relevant documents.

The work on relevance modeling returns to the probabilistic models view of the document ranking problem, i.e., the esti-modeling is to identify those words that indicate relevance and thus will be effective when comprising a query. These papers make the assumption that in the absence of training data, but given a query Q  X  q 1 ; q 2 ; ... ; q n , the probability P  X  w i w (Lavrenko &amp; Croft, 2001 ), that is
Let M  X f m j g be the finite universe of language models m j that (notionally) generated the documents of the collection. If we assume independence between the word w i and the query keywords Q 1 , the joint probability P  X  w i ; Q  X  can be then com-puted by the total probability of emitting the word and the query keywords from each language model in M : Formula (4) can be interpreted as follows: P  X  m j  X  is the probability of selecting a language model m j from the set M ,
P  X  w the query keywords Q from the same language model. Like in the language modeling approach (Song &amp; Croft, 1999 ), the probability P  X  w i j m j  X  can be estimated by the smoothed relative frequency of the word in the document. See formula (2). By applying the Bayes X  conditional probability theorem, the probability P  X  Q j m j  X  can be computed by Replacing P  X  Q j m j  X  by the previous expression in formula (4), we obtain: Finally, by including formula (6) in the expression (3), the approximation of the probability P  X  w i j R  X  results in
In order to implement the relevance models in an IR system, the set M is restricted to contain only the language models of the k top-ranked documents retrieved by the query Q . The system performs the following two steps (Lavrenko et al., 2002 ): 1. Retrieve from the document collection the documents that contain all or most of the query keywords and rank the doc-language modeling formula proposed in Song and Croft (1999) can be used for this purpose, see formula (1). Let RQ be the set composed of the language models associated with the top r ranked documents. RQ stands for documents Relevant to the Query. 2. Approximate the probability P  X  w i j R  X  of finding a word w i in the ideal set of relevant documents R by the probability P  X  w i j RQ  X  of emitting it from the set of relevant document language models RQ
The main contribution of relevance modeling is the probabilistic approach discussed above to estimate P  X  w i j R  X  using the query alone, which has been done in a heuristic fashion in previous works (Robertson &amp; Jones, 1976 ). This approximation to
P  X  w i j R  X  can be latter used for applying the probability ranking principle. For instance, the authors of Lavrenko and Croft (2001) represent the documents as a sequence of independent words (let w i 2 d j be each one of these words) and propose to rank the documents by
The models of relevance have been shown to outperform base-line language modeling and tf/idf IR systems in TREC ad-hoc retrieval and TDT topic tracking tasks (Lavrenko &amp; Croft, 2001 ). Moreover, relevance modeling provides a theoretically well-founded framework where not only is possible to calculate the probability of finding a word in the set of documents relevant to an IR query, but also to estimate the probability of observing any arbitrary type of object described in this set of relevant documents. For example, in Lavrenko, Feng, and Manmatha (2003) relevance models are applied in image retrieval tasks to compute the joint probability of sampling a set of image features and a set of image annotation words.
The notion of the set RQ of documents relevant to an IR query can be used for representing the context of analysis in a contextualized warehouse. The relevance model presented in this paper adapts this idea to estimate the probability of observing a corporate fact described in the set of documents relevant to the context of analysis. 4. The facts relevance model
In this section, we propose a relevance model to calculate the relevance of a fact with respect to a selected context (i.e., to evant for this context. We will consider that a fact is important in an document if its dimension values are mentioned fre-quently in the document textual contents.

We assume that each document d j describes a set of facts f f i g ; and that the document and its fact set were generated by a model m j that emits words, with a probability P  X  w i j m j  X  , and facts, with a probability P  X  f i j m j  X  . will mean that v k is a dimension value of the fact f i .

The tuple  X  fo 1 ; Japan ; 1998 = 10  X 2 Products Customers Time represents the fact f 4 of the cube characterized by the dimensions Products , Customers and Time , shown in Fig. 1 .

Notice that at this point, we are only concerned about the occurrence of dimension values in the documents, indepen-dently of the hierarchy level which they belong to. Then, in the relevance model, we simply consider a dimension as the flat set that includes all the members of the dimension hierarchy levels, as specified in the corporate warehouse schema. For example, we do not make explicit the mapping of customers into cities, or states into countries in the Customers dimension; we just represent the dimension Customers by the set that comprises all the values of its hierarchy levels (e.g, Customers  X  Customer [ City [ State [ Country ).
 generated the documents relevant to this query. We compute the relevance of a fact f i to the query Q by the probability P  X  f i j RQ  X  of emitting this fact from the set RQ of models relevant to the query, as follows:
That is, we estimate the relevance of a fact by calculating the probability of observing it in the set of documents relevant computed by the language modeling formula (1).
 Definition 3. P  X  f i j m j  X  is the probability of emitting the fact f i from the model m j , which is estimated as follows: dimension values found in the document d j .

The approach discussed above to compute the probability P  X  f i j RQ  X  is based on the relevance modeling techniques pre-sented in Section 3. However, we have adapted these techniques to estimate the probability of facts instead of document words. Next, we point out the major similarities and differences between the two approaches.
 mula (5). By including the expression (5) in the formula (10), we have that of emitting the query keywords from each model in RQ . See formula (13). Notice that the assumption that we make here is equivalent to the one made by the relevance modeling works in formula (4) to calculate the joint probability P  X  k i ; Q  X  .
By considering that the probability P  X  m j  X  is constant, and replacing the probability P  X  Q  X  by the previous expression, we have that formula (12) is equivalent to Notice the similarity between formula (14) and the relevance modeling formula (8) used for computing the probability
P  X  w i j RQ  X  . The difference comes in considering that whereas the ordinary relevance modeling proposals approached the prob-have been previously found in the documents, that is, P  X  f i j R  X  P  X  f i j Q  X  .
 5. Experiments and results
This section evaluates the proposed relevance model with the Wall Street Journal (WSJ) TREC test collection ( Harman, 1995) and a fact database constructed from the metadata available in the documents. In our experiments, we took a set of example information requests (called topics in TREC), determined which is the expected most relevant fact in the response result for each topic, and analyzed the quality of the ranking of the facts provided by our model.

It is important to emphasize that the objective here is not to evaluate the document retrieval performance. The formulas used in our approach for estimating the relevance of a document and building the set RQ of relevant models are based on those of language modeling, that have already been shown to obtain good performance results ( Song&amp; Croft, 1999 ). The final objective of our experiments is to evaluate the proposed facts relevance ranking approach.

Next, we introduce the document collection, the fact database and the topics selected for the experiments. Afterwards, we show how we built the IR queries for the topics and tuned the set RQ . Finally, we study the results obtained when ranking the facts with the relevance model. 5.1. Document collection, fact base and topics
In our experiments we considered the 1990-WSJ subcollection from TREC disk 2, a total of 21,705 news articles published during 1990. The news articles of the WSJ subcollection contain metadata. These metadata comprise, among other informa-tion, the date of publication of the article, and the list of companies reported by the news article. By combining the date of publication and the company list of each article, we built a  X  Date ; Company  X  fact database. For each fact, we also kept the news articles were the corresponding  X  Date ; Company  X  pair was found. Thus, our experiments involved two dimensions, the Date and the Companies dimensions. In the Companies dimension, the companies described by the WSJ articles are orga-nized into Industries , which are in turn classified into Sectors . The correspondence between companies, industries and sectors is based on the Yahoo Finance 2 companies classification.

We selected 16 topics from the TREC-2 and TREC-3 conferences by choosing the topics that have at least 20 documents in the provided solution set of documents relevant for the topic. We made such a restriction to ensure that the set of relevant documents was big enough to find several samples of the dimension values relevant for the query. Furthermore, we exam-ined the textual description of each selected topic in order to determine the industry that is most likely related to the theme of the topic, that is, the industry of the companies that are expected to be found at the top-ranked facts for each topic.
Table 2 shows the topic number, title and expected top-ranked industry of the TREC topic set considered in our experi-ments. For example, as this table shows, the expected most relevant industry for the TREC topic number 198, entitled  X  X  X ene Therapy and Its Benefits to Humankind X , is Biotechnology . 5.2. Building the set RQ
In order to estimate accurately the relevance of the facts, we need an acceptable description of each selected topic in the corresponding set of relevant models RQ . Next, we show how we constructed and tuned the context of analysis (i.e., the set RQ ) for the test topics.

For each topic, we specified a short IR query (less than 4 keywords), and then we retrieved the set of documents relevant to this query, as discussed in Section 3. The smoothing parameter k of formula (2) determines the features of the top-ranked documents, mainly their length (Losada &amp; Azzopardi, 2008 ). Larger documents are usually ranked at the first positions as k decreases. In our case, larger documents are more likely to describe more dimension values than shorter ones, and therefore they can contribute better to contextualize facts. Additionally, it is well-known that short queries require less smoothing than larger ones. For these reasons, we set the smoothing parameter k to 0.1 in our experiments. Nevertheless, a deeper study of the influence of the smoothing method in the result must be carried out in the future.

The query keywords were interactively selected to reach an acceptable precision versus recall figure ( Baeza-Yates &amp; Ribe-iro-Neto, 1999 ). Typically, an  X  X  X cceptable X  retrieval performance is considered to be achieved when the precision is over 40% See for example, the evaluations of Harman (1995), Lavrenko and Croft (2001) and Song and Croft (1999) . Fig. 2 illustrates the average precision values obtained at the 11 standard recall levels for the selected topics. The percentages are over the acceptable margins quoted above. Table 3 details these precision values.

The R-Precision is a useful parameter for measuring the quality of the result set for each individual topic, when the ideal set R of documents judged to be relevant is known (Baeza-Yates &amp; Ribeiro-Neto, 1999 ). Given j R j , the number of documents values obtained for each topic, as well as the resulting average R-Precision. Fig. 3 depicts the corresponding R-Precision histogram.

As stated in Section 3, the set RQ comprises the models associated with the k top-ranked documents of the query. We now turn our attention to tuning the size of the RQ sets. Here, our purpose is to determine the number k of top-ranked documents to be considered in RQ that maximizes the retrieval performance. In this case, we use a different performance measure, called
F-measure ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ), that calculates the harmonic mean of precision and recall. Maximizing the F-measure means finding the best possible combination of precision and recall. We computed the average F-measure for the selected TREC topics with different sizes of the result set. As Fig. 4 shows, the maximum value is 0,4534, reached when the result set contains the 36th top-ranked documents. 5.3. Evaluation of results
Finally, in this section we evaluate the fact relevance ranking results obtained with our model. For each topic, we consid-ered the facts described by the 36 top-ranked documents in the corresponding set RQ . We grouped the facts by industry, and calculated their relevance to the IR query following the approach discussed in Section 4. Tables 5 and 6 show the industries, along with their relevance, at the top of the ranking for each topic.

We can conclude that the results demonstrate the effectiveness of the approach. For all the topics, even for those where the R-Precision was low (see for example the topics 152 and 179), the expected industry is found at the first (81% of the top-ics) or the second (19%) position of the ranking.

Furthermore, the relevance value assigned to facts clearly differentiates the industries that are directly related to the topic of analysis from those that are not so relevant. In almost all cases, the relevance value approximately decreases by one order tribution ) ranked industries are clearly related to the thematic of the topic ( X  X  X il spills X ). The relevance values assigned to these industries (0.6348 and 0.3192, respectively) are significantly greater than the relevance value of the next industry in the ranking ( Industrial Transportation , 0.0460).

We also find an explanation for some of the topics where the ranking was not completely accurate. The top-raked indus-try for the topic number 173 is Airlines , whereas the expected industry Tobacco is found at the second position of the ranking. The reason is that a number of the documents judged to be relevant for this topic report smoking bans on flights. The indus-try at the top of the ranking for the topic 137 is Media , since many media companies also own thematic parks (e.g., Time Warner/ Warner Bros. Entertainment). In fact, in our Companies dimension, the Media industry also comprises these recre-ation and entertainment companies. The second top-ranked industry for this topic is Industrial Metals , which still has a rel-ative high relevance value. Although, this industry initially seemed irrelevant for the topic 137, after reading some of the documents retrieved for this topic, we discovered a group of news relating the vanguardist Japanese company Nippon Steel X  X  diversification strategy on the amusement-park sector. 6. Conclusions
This paper introduces a new relevance model aimed at ranking the structured data (facts) and documents of a contextu-alized warehouse when the user establishes an analysis context (i.e., runs an IR query). The approach can be summarized as follows. First, we use language modeling formulas (Ponte&amp; Croft, 1998 ) to rank the documents by the probability of emitting the query keywords from the respective language model. Then, we adapt relevance modeling techniques ( Lavrenko &amp; Croft, 2001) to estimate the relevance of the facts by the probability of observing its dimension values in the top-ranked documents.

We have evaluated the model with the Wall Street Journal (WSJ) TREC test subcollection and a fact database self-con-structed from the metadata available in the documents. The results obtained are encouraging. The experiments show that our relevance model is able to clearly differentiate the facts that are directly related to the test topics, from those that are not so relevant. We found the expected top-ranked fact in the first or the second position of the ranking for the 16 topics selected. A deeper study of the influence of the smoothing method in our approach remains to be done.

In the prototype of the contextualized warehouse presented in P X rez et al. (2007) , a corporate warehouse with data from the World major stock indices is contextualized with a repository of business articles, selected from the WSJ TREC collection too. The prototype involved a dataset of 1936 (Date, Market, Stock Index value) facts and 132 documents. Although we did not formally evaluate the relevance model of the prototype, we showed some analysis examples where the relevant articles explain the increases and decreases of the stock indexes. Testing the performance of the contextualized warehouse analysis operations with larger datasets and studying query optimization techniques is also future work.
 These papers propose specific techniques for retrieving and classifying opinions expressed in small text fragments (like the posts of a web forum). We are currently working on extending our retrieval model with opinion retrieval techniques in order to contextualize a traditional company X  X  sales data warehouse with documents gathered from web forums, where the cus-tomers review the products/services of the company.
 References
