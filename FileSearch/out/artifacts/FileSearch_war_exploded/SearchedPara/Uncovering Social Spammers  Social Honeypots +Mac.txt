 Web-based social systems enable new community-based opportu-nities for participants to engage, share, and interact. This com-munity value and related services like search and advertising are threatened by spammers, content polluters, and malware dissemi-nators. In an effort to preserve community value and ensure long-term success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deploy-ment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deploy-ment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.
 Categories and Subject Descriptors: H.3.5 [Online Information Services]: Web-based services; J.4 [Computer Applications]: So-cial and behavioral sciences General Terms: Design, Experimentation, Security Keywords: social media, social honeypots, spam
The past few years have seen the rapid rise of Web-based sys-tems incorporating social features  X  from Web-based social net-works (e.g., Facebook, MySpace) to online social media sites (e.g., YouTube, Flickr) to large-scale information sharing communities (e.g., Digg, Yahoo! Answers). These social systems have attracted a tremendous amount of media and research interest [1, 10, 18]. One of the key features of these systems is their reliance on users as primary contributors of content and as annotators and raters of other X  X  content. This reliance on users can lead to many positive effects, including large-scale growth in the size and content in the community, bottom-up discovery of  X  X itizen-experts X , serendipi-tous discovery of new resources beyond the scope of the system designers, and new social-based information search and retrieval algorithms. Unfortunately, the relative openness and reliance on users coupled with the widespread interest and growth of these so-cial systems has also made them prime targets of social spammers .
In particular, social spammers are increasingly targeting these systems as part of phishing attacks [14], to disseminate malware [5] and commercial spam messages [7, 26], and to promote affili-ate websites [17]. In the past year alone, more than 80% of social networking users have  X  X eceived unwanted friend requests, mes-sages, or postings on their social or professional network account X  (Source: Harris Interactive, June 2008). Unlike traditional email-based spam, social spam often contains contextual information that can increase the impact of the spam (e.g., by eliciting a user to click on a phishing link sent from a  X  X riend X ) [7, 12, 14].

Successfully defending against these social spammers is impor-tant to improve the quality of experience for community members, to lessen the system load of dealing with unwanted and sometimes dangerous content, and to positively impact the overall value of the social system going forward. However, little is known about these social spammers, their level of sophistication, or their strategies and tactics. Filling this need is challenging, especially in social networks consisting of 100s of millions of user profiles (like Face-book, MySpace, Twitter, YouTube, etc.). Traditional techniques for discovering evidence of spam users often rely on costly human-in-the-loop inspection of training data for building spam classifiers; since spammers constantly adapt their strategies and tactics, the learned spam signatures can go stale quickly. An alternative spam discovery technique relies on community-contributed spam refer-rals (e.g., Users A, B, and C report that User X is a spam user); of course, these kinds of referral systems can be manipulated them-selves to yield spam labels on legitimate users, thereby obscuring the labeling effectiveness. And neither spam discovery approach can effectively handle zero-day social spam attacks for which there is no existing signature or wide evidence.

With these challenges in mind, we propose and evaluate a novel honeypot-based approach for uncovering social spammers in on-line social systems. Concretely, the proposed approach is designed to (i) automatically harvest spam profiles from social networking communities, avoiding the drawbacks of burdensome human in-spection; (ii) develop robust statistical user models for distinguish-ing between social spammers and legitimate users; and (iii) ac-tively filter out unknown (including zero-day) spammers based on these user models. Drawing inspiration from security researchers who have used honeypots to observe and analyze malicious activ-ity (e.g., for characterizing malicious hacker activity [22], gener-ating intrusion detection signatures [16], and observing email ad-dress harvesters [20]), we deploy and maintain social honeypots for trapping evidence of spam profile behavior, so that users who are detected by the honeypot have a high likelihood of being a spam-mer (i.e., low false positive rate). Over two distinct communities (MySpace and Twitter), we find that the proposed approach pro-vides generalizable and effective social spam detection.
In this section, we present the conceptual framework of the pro-posed honeypot-based approach and outline the research questions motivating our examination of this framework.
In social networking communities like MySpace and Facebook, there are a set of k users U = { u 1 , u 2 , . . . , u k } . Each user u a profile p i . Profiles are self-describing pages that are created and controlled by a given user. For example, users typically include information such as their name, gender, age, and so on in their pro-files. Each community has its own profile format, but most fields in the formats are the same.

The social spam detection problem is to predict whether u a spammer through a classifier c when p i is given. A classifier approximates whether u i is a spammer. To build c , we need to ex-tract a set of m features F = { f 1 , f 2 , . . . , f m } from U . For example, we can extract F u i from p i of u i .

Whereas traditional email spam detection has focused on identi-fying spam messages which are of relatively low individual value to the spammer (and whose identification typically doesn X  X  threaten the ongoing ability of a spammer to send new messages), social spam detection is focused on identifying and eliminating spam ac-counts themselves. This detection is potentially more disruptive to spammers, since these accounts typically represent a more expen-sive investment by the spammer (through email and social media account registrations).
We propose to monitor spammer activity through the creation of social honeypots. We define social honeypots as information system resources that monitor spammers X  behaviors and log their information (e.g., their profiles and other content created by them in social networking communities). Social honeypots and tradi-tional honeypots (e.g., in domains such as network systems and emails [16, 20, 22]) share a similar purpose in that they both mon-itor and log the behaviors of spammers or attackers. However, tra-ditional honeypots typically target network or systems-level behav-ior, whereas social honeypots specifically target community-based online activities.

While social honeypots alone are a potentially valuable tool for gathering evidence of social spam attacks and supporting a greater understanding of spam strategies, it is the goal of this research project to support ongoing and active automatic detection of new and emerging spammers (See Figure 1). In practice, we deploy a social honeypot consisting of a legitimate profile and an associated bot to detect social spam behavior. If the social honeypot detects suspicious user activity (e.g., the honeypot X  X  profile receiving an unsolicited friend request) then the social honeypot X  X  bot collects evidence of the spam candidate (e.g., by crawling the profile of the user sending the unsolicited friend request plus hyperlinks from the profile). What entails suspicious user behavior can be optimized for the particular community and updated based on new observa-tions of spammer activity.

As the social honeypots collect spam evidence, we extract ob-servable features from the collected candidate spam profiles (e.g., number of friends, text on the profile, age, etc.). Coupled with a set of known legitimate (non-spam) profiles which are more popu-lous and easy to extract from social networking communities, these spam and legitimate profiles become part of the initial training set of a spam classifier. Through iterative refinement of the features selected and the classifier used (e.g., SVM), the spam classifier can be optimized over the known spam and legitimate profiles.
Based on these developed classifiers, we can then explore the wider space of unknown profiles. On MySpace alone there are 100s of millions of profiles, of which some unknown fraction are spam. Using the classifiers based on the harvested social honeypot data, we iteratively explore these profiles  X  X n-the-wild X  to detect new spammers that have yet to be identified by a social honeypot directly. In our design of the overall architecture, we include human inspectors in-the-loop for validating the quality of these extracted spam candidates. Instead of inspecting the entirety of all profiles, these inspectors are guided to validate just the few spam candidates recommended by the learned classifiers. Based on their feedback, the spam classifiers are updated with the new evidence and the pro-cess continues. Given the overall architecture, we address three important research challenges in turn in the rest of the paper:  X  Research Challenge #1 [RC1]: Do social honeypots collect ev- X  Research Challenge #2 [RC2]: Can we build classifiers from  X  Research Challenge #3 [RC3]: Finally, can the developed clas-
Based on the overall social honeypot framework, we selected two social networking communities  X  Myspace and Twitter  X  to eval-uate the effectiveness of the proposed spam defense mechanism. Both MySpace and Twitter are large and growing communities and both also support public access to their profiles, so all data collec-tion can rely on purely public data capture.

MySpace Social Honeypot Deployment: In previous research [23], we created 51 generic honeypot profiles within the MySpace com-munity for attracting spammer activity so that we can identify and analyze the characteristics of social spam profiles. To observe any geographic artifacts of spamming behavior, each profile was as-signed a specific geographic location (i.e., one honeypot was as-signed to each of the U.S. states and Washington, D.C.). Each hon-eypot profile tracks all unsolicited friend requests. Upon receiving a friend request, we store a local copy of the profile issuing the friend request, extract all hyperlinks in the  X  X bout Me X  sections (we find that these typically point to an affiliate spam website), and crawl the pages pointed to by these hyperlinks. Based on a four month evaluation period (October 2007 to January 2008), we col-lected 1,570 profiles whose users sent unsolicited friend requests to these social honeypots.

Twitter Social Honeypot Deployment: Similarly, we created and deployed a mix of honeypots within the Twitter community  X  some of them had personal information such as biography, location and so on, while others did not have this personal information. Some social honeypots posted tweets, while others did not post them. We omit some of the concrete details of the Twitter honeypot deploy-ment due to the space constraint. From August 2009 to September 2009, these social honeypots collected 500 users X  data.
After analyzing the harvested spam profiles from MySpace, we find some interesting observations (more fully detailed in [23]): (1) The spamming behaviors of spam profiles follow distinct temporal patterns. (2) The most popular spamming targets are Midwestern states, and the most popular location for spam profiles is Califor-nia. (3) 57.2% of the spam profiles copy their  X  X bout Me X  con-tent from another profile. (4) Many of the spam profiles exhibit distinct demographic characteristics (e.g., age, relationship status, etc.). (5) Spam profiles use thousands of URLs and various redi-rection techniques to funnel users to a handful of destination Web pages. Through manual inspection, we grouped the harvested spam profiles into five categories:  X  Click Traps: Each profile contains a background image that is  X  Friend Infiltrators: These nominally legitimate profiles befriend  X  Pornographic Storytellers: Each of these profiles has an  X  X bout  X  Japanese Pill Pushers: These profiles contain a sales pitch for  X  Winnies: All of these profiles have the same headline:  X  X ey its
Similarly, we discovered various types of spam users in the har-vested data from Twitter. In many cases, spammers inserted mali-cious or spam links into their tweets. Since most Twitter links use a form of URL-shortening, users clicking on these links have no assurances of the actual destination.  X  Duplicate Spammers: These users post a series of nearly iden- X  Pornographic Spammers: Their data such as user image, profile  X  Promoters: These users post tweets about several things such as  X  Phishers: Similar to promoters, these users use a mix of strate- X  Friend Infiltrators: Much like their counterparts on MySpace,
These observations indicate that social honeypots can success-fully attract spammers across fundamentally different communities (MySpace vs. Twitter), and the results suggest that building auto-matic classifiers may be useful for identifying social spam.
We next explore whether there are discernible spam signals in the harvested spam profiles that can be used to automatically dis-tinguish spam profiles from legitimate profiles. Since social hon-eypots are triggered by spam behaviors only, it is unclear if the corresponding profiles engaging in the spam behavior also exhibit clearly observable spam signals. If there are clear patterns (as our observations in the previous section would seem to indicate), then by training a classifier on the observable signals, we may be able to predict new spam even in the absence of triggering spam behaviors.
As part of this empirical evaluation of social spam signals, we consider four broad classes of user attributes that are typically ob-servable (unlike, say, private messaging between two users) in the social network: (i) user demographics: including age, gender, lo-cation, and other descriptive information about the user; (ii) user-contributed content: including  X  X bout Me X  text, blog posts, com-ments posted on other user X  X  profiles, tweets, etc.; (iii) user activity features: including posting rate, tweet frequency; (iv) user connec-tions: including number of friends in the social network, followers, following. For MySpace and Twitter we select a subset of these features to train the classifier.

The classification experiments were performed using 10-fold cross-validation to improve the reliability of classifier evaluations. When a dataset is not large, it is common to use 10-fold cross-validation to achieve statistically precise results. In 10-fold cross-validation, the original sample is randomly divided into 10 equally-sized sub-samples. 9 sub-samples are used as a training set and the remaining one is used as a testing set; the classifier is evaluated, then the pro-cess is repeated for a total of 10 times. Each sub-sample is used as a testing set once in each evaluation. The final evaluation re-sult is generated by averaging the results of the 10 evaluations. In practice, we evaluated over 60 different classifiers in the Weka [24] machine learning toolkit using 10-fold cross-validation with default values for all parameters. Classification results are presented in the form of a confusion matrix as in Table 1.

To measure the effectiveness of classifiers based on our proposed features, we used the standard metrics such as precision, recall, ac-curacy, the F 1 measure, false positive and true positive. Precision is the ratio of correctly predicted users as a class to the total predicted users as the class. For example, the precision (P) of the spammer class in Table 1 is a/ ( a + c ) . Recall (R) is the ratio of correctly predicted users as a class to the actual users in the class. The re-call of the spammer class in the table is a/ ( a + b ) . Accuracy is the proportion of the total number of predictions that were correct. The accuracy in the table is ( a + d ) / ( a + b + c + d ) . F that trades off precision versus recall. F 1 measure of the spammer class is 2 P R/ ( P + R ) . A false positive is when the actual Y class users are incorrectly predicted as X class users. The false positive of the spammer class is c . A true positive is when actual X class users are correctly predicted as X class users. The true positive of the spammer class is a .

To measure the discrimination power between spammers and le-gitimate users of each of the proposed features, we generate a Re-ceiver Operating Characteristics (ROC) curve. ROC curves plot false positive rate on the X axis and true positive rate on the Y axis. The closer the ROC curve is to the upper left corner, the higher the overall accuracy is. The ideal ROC curve includes the coordinate (0, 1), indicating no false positives and a 100% true positive rate.
We randomly sampled 388 legitimate profiles from MySpace (which were labeled by us) and 627 deceptive spam profiles from the 1,570 deceptive spam profiles collected by our social honey-pots. When we sampled the profiles, we considered several condi-tions. Profiles have to be public, and marital status, gender, age, and  X  X bout Me X  content in the profiles have to be valid (i.e., a non-empty value). In addition, we removed duplicated profiles among the 1,570 deceptive spam profiles in the case that a spammer sent a friend request to several social honeypots. The goal of spam clas-sification over the MySpace data is to predict whether a profile is either spammer or legitimate.

We considered several representative user features: number of friends, age, marital status, gender, as well as some text-based fea-tures modeling user-contributed content in the  X  X bout Me X  section. Specifically, we consider a bag-of-words model in which we re-move punctuation, make all letters lowercase, tokenize each word, remove stopwords, and do stemming for each word using the Porter stemmer [19]. We assigned weights to each word based on tf-idf weighting: tf-idf t,d = log(1+ tf t,d )  X  log( N df term frequency of term t in a profile X  X   X  X bout Me X , N is the num-ber of profiles, and df t is the number of profiles which includes term t . We also measured the length in bytes of the  X  X bout Me X  content.

Before evaluating the effectiveness of our classifiers, we investi-gated the discrimination power of our individual classification fea-tures. Recognizing that social spam classification is an example of an adversarial classification problem [11], we wanted to evalu-ate the robustness of our features against an adversarial attack. To show the discrimination power and robustness of each feature, we generated ROC curves for each feature using an AdaboostM1 clas-sifier. The results are shown in Figure 2. Marital status and sex are the least discriminative features, which is unsurprising because they are represented by a small set of predefined values (e.g.,  X  X ar-ried X ,  X  X ingle X ,  X  X ale X , etc.) that will inevitably appear in both legitimate and spam profiles. On the other hand, the bag of words features extracted from  X  X bout Me X  content (AMContent) are the most discriminative. This is a very encouraging result because it means our classifier was able to distinguish between legitimate and spam  X  X bout Me X  content with a high degree of accuracy. There-fore, if spammers begin varying the other features of their profiles (to appear more legitimate), our classifiers will still be effective. Additionally, the  X  X bout Me X  content is the most difficult feature for a spammer to vary because it contains the actual sales pitch or deceptive content that is meant to target legitimate users.
In Table 2, the performance results for the top 10 classifiers are shown. The table clearly shows that all of the classifiers were suc-cessful. Each classifier generated an accuracy greater than 98.4%, an F 1 measure over 0.98, and a false positive rate below 1.6%. Overall, meta-classifiers (Decorate, LogitBoost, etc.) performed better than tree classifiers (FT and J48) and a function-based clas-sifier (SimpleLogistic). The best classifier is Decorate, which is a meta-learner for building diverse ensembles of classifiers. It ob-tained an accuracy of 99.21%, an F 1 measure of 0.992, and a 0.7% false positive rate. We additionally considered different training mixtures of spam and legitimate training data (from 10% spam / 90% legitimate to 90% spam / 10% legitimate); we find that the metrics are robust across these changes in training data.
To evaluate the quality of spam classification over Twitter, we randomly selected 104 legitimate users (labeled by us) from a pre-viously collected Twitter dataset of 210,000 users. We additionally considered two classes of spam users: the 61 spammers and the 107 promoters sampled from 500 users X  data collected by the social honeypots. For each user, we collected the user profile, tweets (sta-tus update messages), following (friend) information and follow-ers X  information. The goal of spam classification over the Twitter data is to predict whether a profile is either spammer, a promoter, or legitimate. When we sampled users X  data, we considered two con-ditions: the profiles did not have a verified account badge and the number of tweets had to be over 0. The verified account badge is one way Twitter ensures that profiles belong to known people (e.g, Shaquille O X  X eal and not an impersonator).

Unlike MySpace profiles which emphasize on longer-form per-sonal information sharing (e.g.,  X  X bout Me X  text) and usually have self-reported user demographics (e.g., age, gender), Twitter accounts are noted for their short posts, activity-related features, and limited self-reported user demographics. For user features, we consider the longevity of the account on Twitter, the average tweets per day, the ratio of the number of following and number of followers, the per-some features of the tweets sent, including:  X  The ratio of the number of URLs in the 20 most recently posted  X  The ratio of the number of unique URLs in the 20 most recently  X  The ratio of the number of @usernames in the 20 most recently  X  The ratio of the number of unique @usernames in the 20 most
Additionally, we measure the average content similarity over all pairs of tweets posted by a user: where the content similarity is computed using the standard cosine similarity over the bag-of-words vector representation tweet content:
We finally considered some text-based features to model the con-tent in the tweets. Since tweets are extremely short (140 characters or less), we consider a bag-of-words model and a sparse bigrams model [9]. We remove punctuation, make all letters lowercase, to-kenize each word in the bag-of-words model and tokenize a pair of words in the sparse bigrams model. The sparse bigrams model generates a pair of words separated by no more than k words. We assigned k = 3 in our system, while k = 0 yields ordinary bigrams. If a tweet is  X  X heck adult page view models X , the sparse bigrams will generate the features  X  X heck adult X ,  X  X heck page X ,  X  X heck view X ,  X  X dult page X ,  X  X dult view X ,  X  X dult models X ,  X  X age view X ,  X  X age models X ,  X  X iew models X . We weighted terms and bigrams using tf-idf weighting as in the previous MySpace classification.
In order to know how much discrimination power each feature has for spammers, promoters and legitimate users, we generated ROC curves of the proposed features using Decorate in Figure 3. Average posted tweets per day ( | tweets | per day ), percentage of bidirectional friends ( bi-friends ), and ratio of number of follow-ing and number of followers ( F-F Ratio ) have low discrimination powers relatively, while ratio of number of unique URLs in recently posted top 20 tweets and number of the tweets ( | unique U RL | per tweet ), ratio of number of @username in re-cently posted top 20 tweets and number of the tweets ( | @ | per tweet ), ratio of number of unique @username in recently posted top 20 tweets and number of the tweets ( | unique @ | per tweet ), and av-erage content similarity between a user X  X  tweets ( tweets similarity ) have good discrimination powers. Account age, text-based features extracted from tweets, and ratio of number of URLs in recently posted top 20 tweets and number of the tweets ( | U RL | per tweet ) have the best discrimination power. Overall, the proposed all fea-tures have positive discrimination power.

To further illustrate, Figure 4(a) presents the cumulative distri-butions of content similarity in tweets posted by each user class. It shows clear distinction among legitimate users, spammers and promoters. The content similarity in tweets of each spammer is the largest compared to the other classes because some of them post almost the same content or even duplicate tweets. Promoters have a goal of promoting something like online business, market-ing and so on; naturally their tweets include common terms like the name of a product. Therefore, the content similarity in their tweets is larger than legitimate users X  one because legitimate users post tweets about their news such as what they are doing, where they are and so on. The content similarity in tweets of legitimate users is the smallest. Figure 4(b) shows the cumulative distribu-tions of the average number of URLs in the tweets of each user. Tweets posted by legitimate users include the smallest number of URLs; not surprisingly, the majority of spammers and promoters post tweets with URLs. The curves of spammers and promoters are overlapped near 1 in the X axis, meaning that promotor and spammer behavior is closely coupled in our dataset.
 Table 3 shows the performance results for the top 10 classifiers. Each of the top 10 classifiers achieved an accuracy greater than 82.7%, an F 1 measure over 0.82, and a false positive rate less than 10.3%. As in the case with MySpace, the meta classifiers (Deco-rate, LogitBoost, etc.) produced better performance than tree clas-sifiers (BFTree and FT) and function-based classifiers (SimpleLo-gistic and LibSVM). The best classifier was Decorate, which ob-tained an accuracy of 88.98% accuracy, an F 1 measure of 0.888, and a 5.7% false positive rate. As in the MySpace analysis, we additionally considered different training mixtures of spam and le-gitimate training data (from 10% spam / 90% legitimate to 90% spam / 10% legitimate); we find that the classification metrics are robust across these changes in training data.

Based on our empirical study over both MySpace and Twitter, we find strong evidence that social honeypots can attract spam be-haviors that are strongly correlated with observable features of the spammer X  X  profiles and their activity in the network (e.g., tweet frequency). These results hold across two fundamentally different communities and confirm the hypothesis that spammers engage in behavior that is correlated with observable features that distinguish them from legitimate users. In addition, we find that some of these signals may be difficult for spammers to obscure (e.g., content con-taining a sales pitch or deceptive content), so that the results are encouraging for ongoing effective spam detection.
So far, we have seen that the deployed social honeypots can col-lect evidence of spam behavior, and that these behaviors are cor-related with spam signals which can support automatic spam clas-sification. In this final study, we explore whether these classifiers can be effectively deployed over large collections of unknown pro-files (for which we have no assurances of the degree of spam or legitimate users). Concretely, we apply the developed classifiers for both MySpace and Twitter over datasets  X  X n-the-wild X  to better understand the promise of social honeypots in defending against new and emerging spam and zero-day spam attacks. For this final study, we considered two large datasets.
 MySpace Dataset: The first dataset is a crawl over MySpace, in-cluding about 1.5 million of public profiles collected in 2006 and 2007. A full description of this dataset and its characteristics is available in [8]. Table 4 summarizes statistics of this dataset. Twitter Dataset: We also collected a large dataset from Twitter for the period September 2 to September 9, 2009. We sampled the pub-lic timeline of Twitter (which publishes a random selection of new tweets every minute), collected usernames, and then used the Twit-ter API to collect each user X  X  recently posted top 20 tweets, plus the user X  X  following (friends) and followers X  information. Table 5 presents statistics of this Twitter dataset. It consists of 215,345 user profiles, 4,040,415 tweets.

In both cases, the collected profiles are unseen to our system, meaning that we do not know ground truth as to whether a pro-file is spam or legitimate. As a result, the traditional classification metrics presented in the previous section would be infeasible to ap-ply in this case. Rather than hand label millions of profiles, we adopted the spam precision metric to evaluate the quality of spam predictions. For spam precision, we evaluate only the predicted spammers (i.e., the profiles that the classifier labels as spam). Spam precision is defined as:
As in the previous section, we trained a classifier over a train-ing set consisting of 388 legitimate profiles (labeled by us) and 627 deceptive spam profiles collected from social honeypots. In the interests of efficiency, we used the LibSVM classifier  X  an imple-mentation of support vector machines  X  which is a widely popular classifier and classifies a large dataset quickly with high accuracy. Its classification time is faster than meta classifiers that proved suc-cessful in the previous experiments. We sampled from the 1.5 mil-lion public profiles a smaller test set of 43,000 profiles. We re-peated this sampling procedure four times so we had four different test sets.

As we classified each of four test sets, a human inspector ver-ified whether the newly found predicted spam was actually spam, added the new instances to the training set, and the process contin-ued. In each test set, LibSVM classifier predicted about 30 users as spammers. In each subsequent iteration, we found that the spam precision increased.

Figure 5 shows the evaluation results of the fourth test set. The left two bars of the figure present spam precision based on sex-ual content. If an unseen profile is classified to a deceptive spam profile by LibSVM, and its  X  X bout Me X  content includes sexual content, it will be considered as a real deceptive spam profile. The right two bars of the figure present spam precision based on adver-tisement content. If predicted spam profile X  X   X  X bout Me X  content includes advertisement content, it will be considered as a real de-ceptive spam profile. Note that there are two results: with postfilter and without postfilter. We found that LibSVM incorrectly predicted spam labels for profiles containing special profile layout links, e.g.,  X  X lick here to get a theme for your myspace X  or  X  X lick here for myspace backgrounds X , which are similar to spammer techniques for inserting links into spam profiles. These types of profile layout links are common on MySpace, which allows users to adjust their profile layouts. To correct these errors, we inserted a  X  X ostfilter X  to check for these standard links and remove their spam labels.
As we can see, using postfilters improved about 40% spam pre-cision in sexual content and about 21% in advertisement content. Detecting spammers whose profiles include advertisement content is easier than detecting spammers whose profile include sexual con-tent. Even with the fairly good results (70% spam precision), the results are significantly worse than what we observed in the pre-vious section over the controlled dataset. We attribute this decline in performance to the time-based mismatch between the harvested social honeypots and the large MySpace dataset. The honeypots were deployed in 2007, but the large MySpace data was crawled in 2006. As a result, the spam signatures developed from the hon-eypots have difficulty identifying spam in an earlier dataset when those spam signature may have not been in use at all. Even with these challenges, the results are fairly good.

As an example, Table 6 illustrates a legitimate-appearing profile in the first part of the  X  X bout Me X  content, but then inserts a URL which links to an external site (usually porn or sexual sites) in the middle part or the last part of the  X  X bout Me X  content.
Unlike the MySpace data mismatch, the social honeypots de-ployed on Twitter pre-date the large Twitter dataset collection. Hence, we are interested in this final experiment to discover if these hon-eypots can effectively detect new and emerging spam. For Twitter classification, we again relied on the training set consisting of 104 legitimate users X  data, 61 spammers X  data and 107 promoters X  data which were used in Section 4. For prediction, we considered two cases: legitimate or spam+promoter. We randomly selected a test set of 1,000 users X  data from the total dataset of about 210,000 users. We repeated this process three times, resulting in three test sets. The feature generator produces the same features used in the previous section. We selected Decorate as a classifier because it showed the best performance in the previous Twitter study. Human inspectors view spam data predicted by the classifier, and then de-cide whether or not they are real spam data. They will add newly found spam data with labels (spam or non-spam) to the training set in order to iteratively improve the classifier X  X  accuracy.
Figure 6 presents spam precision results obtained from the three test sets. In each test set, the Decorate classifier predicted about 20 users as spammers. We assessed whether the predicted spam-mers were real spammers. In the first iteration, spam precision was 0.75, nearly matching the performance of the controlled classifier reported in the previous section. By the third iteration, the spam precision was 0.82. We see in this experiment how the social hon-eypots provide strong ability to discover unknown spam; and as these newly discovered spammers are added to the training set, the classifier becomes more robust (resulting in the improvement from the first to the third iteration).

As an example, Figure 7 shows an example of newly found spam-mer. The spammer X  X  tweets include URLs which link to sex search tool pages. It is interesting that the spammer has 205 followers, meaning that this spammer has successfully inserted himself into the social network without detection. We additionally found that about 20% of the users predicted to be spammers were bots that post tweets automatically using the Twitter API.

Based on this large-scale evaluation of spam  X  X n-the-wild X , we can see how social honeypots can enable effective social spam de-tection of previously unknown spam instances. Since spammers are constantly adapting, these initial results provide positive evidence of the robustness of the proposed approach.
In this paper, we have presented the design and real-world eval-uation of a novel social honeypot-based approach to social spam detection. Our overall research goal is to investigate techniques and develop effective tools for automatically detecting and filtering spammers who target social systems. By focusing on two different communities, we have seen how the general principles of (i) social honeypot deployment, (ii) robust spam profile generation, and (iii) adaptive and ongoing spam detection can effectively harvest spam profiles and support the automatic generation of spam signatures for detecting new and unknown spam. Our empirical evaluation over both MySpace and Twitter has demonstrated the effectiveness and adaptability of the honeypot-based approach to social spam de-tection.

In our continuing research, we are interested to explore a number of extensions. First, to what degree can traditional email and web spam approaches be incorporated into the social honeypot frame-work? For example, email spam and phishing approaches rely-ing on data compression algorithms [6], machine learning [15, 21] and statistics [25] could inform the further refinement of the pro-posed approach. Similarly, web spam approaches have extensively studied the link structure of the web (e.g., [2, 3]); adapting these link-based approaches to the inherent social connectivity of online communities could further improve social spam effectiveness.
Second, we are interested to explore how social honeypots can be augmented by other recent approaches to deal with spam in so-cial systems, including Heymann et al. [13] and Benevenuto et al. [4]. These prior approaches have focused on particular com-munities (e.g., social tagging systems, online video sharing sites); in what ways can their domain-specific techniques be incorporated into the social honeypot approach?
Finally, we are interested to expand and refine the social honey-pots. For example, it may be worthwhile to both scale up the num-ber of social honeypots (say, to the 1000s) and to consider more variation in the demographics and behaviors of the social honey-pot profiles (say, by constructing clique-based social honeypots to measure whether honeypots that are more  X  X onnected X  induce more spammer activity than  X  X oner X  honeypots.). Similarly, we are inter-ested to diversify the domains in which we deploy the honeypots (while respecting the terms of service of each community). Do we find that spammers engage in similar behaviors across domains? And if so, perhaps we can use this cross-domain spammer correla-tion to further improve the effectiveness of social spam detection.
This work is partially supported by a Google Research Award and by faculty startup funds from Texas A&amp;M University and the Texas Engineering Experiment Station.
