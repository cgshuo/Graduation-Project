 Many real-world problems necessitate the use of a classification model to assign items in a collection to target categories or classes. The chief objective of a classification model is to accurately predict the target class for each case in the data. Accordingly, when evaluating a classification model, one desires an accurate assessment of its performance on unseen data. Accurate model assessments are important because they permit candidate models to be meaningfully compared and allow one to determine whether a model will perform at an  X  X cceptable X  level. The notion of acceptable performance may be defined solely by internal concerns (e.g., the benefit of a model must outweigh its implementation cost) or by external factors (e.g., regulators may hesitate to approve a diagnostic test with a high false negative rate). No matter how it is applied, however, sound model assessment is a critical element of any classification task [ 10 ]. There are many ways to quantifiably assess the performance of a classifier. In this work, we quantify classifier performance via a simple linear cost model : where c i is the cost of misclassifying a class i instance,  X  of class i instances in the data, and e i is the error rate on class i instances. This cost model is convenient and is commonly used in cost-sensitive learning. However, cost-sensitive methods generally assume that the parameters  X  c are known and constant (e.g., [ 5 , 12 , 18 , 21 ]), an assumption that is often not borne out in practice [ 14 ]. Zadrozny and Elkan [ 20 ] provide a framework for estimating costs and probabilities when sample data are available, but for the purpose of scenario analysis (i.e., the process of evaluating possible future events for which such information is not readily available) [ 11 ].
 future deployment scenarios engendered by uncertain operating environments. We begin in Section 2 by connecting the current techniques for dealing with uncertain operating environments with a notion of cost. We then demonstrate in Section 3 that, as a result of this connection, there exists an underlying theo-retical relationship between several of these methods that leads to a natural def-inition of the risk of an individual classifier and instance. Further, we find that this risk can be substantially mitigated via a boosting-based algorithm we call RiskBoost . In Section 4 , we demonstrate that RiskBoost outperforms AdaBoost [ 8 ] over a diverse collection of datasets. Finally, we present our conclusions in Section 5 . Consider a binary classification task where we have several cases or instances, each of which may be assigned to one of two categories or classes that are labeled 1 (positive) and 0 (negative). Further, assume that any classifier learned on the training data is capable of producing, for each input vector x , a real-valued belongs to class 1. These scores are mapped to binary classifications by choosing and class 1 (positive) if s ( x )  X  t .
 which can be characterized by a confusion matrix of particular true positive ( tp ), the de facto standard method for evaluating classification models from a confu-sion matrix is the receiver operating characteristic, though alternatives such as the H Measure and cost curves also exist. We first elaborate upon each of these below, after which we define a clear relationship between all three. work is given as Table 1 . For the remainder of the work, we use the term classifier to refer to a specific confusion matrix, whereas classification algorithm or learn-ing algorithm is used to refer to a trained model for which a decision threshold has not been defined. Symbol Description  X  i The proportion of class i instance in test data. c i The cost of misclassifying a class i instance. c A normalized cost ratio, i.e., c = c 0 / ( c 0 + c 1 ). u ( c ) The likelihood distribution over cost ratios. e i The error rate on class i instances. n i The number of class i test instances.
 L i The marginal cost of class i instances. t A classification threshold. ( r ,r 0 i )The i th point on the ROC convex hull. f ( x )The i th line segment on the lower envelope in cost space. tp , fp A true and false positive classification, respectively. tn , fn A true and false negative classification, respectively. tpr , fpr The true and false positive rate, respectively. tnr , fnr The true and false negative rate, respectively. 2.1 Addressing Cost with ROC Curves The Receiver Operating Characteristic (ROC) curve [ 7 , 13 ] forms the basis for many of the techniques that we will discuss in the remainder of this work. An ROC curve is formed by varying the classification threshold t across all possible values. In a binary classification problem, each threshold produces a distinct confusion matrix that corresponds to a two-dimensional point ( r space, where r 1 = fpr and r 0 = tpr .
 A point p 1 in ROC space is said to  X  X ominate X  a point p 2 convex hull of the ROC curve are potentially optimal for some value of c as a point not on the convex hull will be dominated by a point that is on it [ 14 ]. As each point on the ROC convex hull represents classification performance at some threshold t , different thresholds will be optimal under different operating conditions c and  X  i . For example, classifiers with lower false negative rates will be optimal at lower values of c , while classifiers with lower false positive rates will be optimal at higher values of c .
 Now, let p i =( r 1 i ,r 0 i )and p i +1 =( r 1( i +1) ,r ROC convex hull. Then p i +1 will produce superior classification performance to p if and only if the change in the false positive rate is offset by a corresponding change in the true positive rate. That is, if we set  X x i  X y Similarly, given a fixed value for c , we can determine the optimal classifier at a given value of  X  0 . Then for p i +1 to outperform p i , we require that Thus, the ROC convex hull can be used to select the optimal classification thresh-articulated by Provost and Fawcett [ 14 ].
 Relationship Between ROC Curves and Cost. Each point in ROC space corresponds to a misclassification cost that can be specified via our simple linear cost model as Note that only the ordinality (i.e., relative magnitude) of the cost is needed for ranking classifiers. Accordingly, if we assume that the cardinality (i.e, absolute magnitude) of the cost can be ignored, then, as c = c 0 / ( c This formulation will be used frequently throughout the remainder of this work. 2.2 Addressing Uncertain Cost with the H Measure An alternative to the ROC is the H Measure, proposed by Hand [ 9 ] to address shortcomings of the ROC. Unlike the ROC, the H Measure incorporates uncer-tainty in the cost ratio c by integrating directly over a hypothetical probability distribution of cost ratios. As the points on the ROC convex hull correspond to optimal misclassification cost over a contiguous set of cost ratios (see Equa-tion 2 ), then, given known prior probabilities  X  i , the average loss over all cost ratios can be calculated by integrating Equation 4 piecewise over the cost regions defined by the convex hull.
 Relationship Between the H Measure and Uncertain Cost. To incorpo-rate a hypothetical cost ratio distribution, we set c = c the integral by the cost distribution, denoted as u ( c ). The final loss measure is then defined as: The H Measure is represented as a normalized scalar value between 0 and 1, whereby higher values correspond to better model performance. 2.3 Addressing Uncertain Cost with Cost Curves Cost curves [ 6 ] provide another alternative to ROC curves for visualizing classi-fier performance. Instead of visualizing performance as a trade-off between false positives and true positives, they depict classification cost in the simple linear cost model against the unknowns  X  i and c i .
 The marginal misclassification cost of class i can be written as This means that if the misclassification rate of class i instances increases by some amount  X e i , then the total misclassification cost increases by maximum possible cost of any classifier is max = L 0 + L 1 rates are 1. Accordingly, we can define the normalized marginal cost (termed the probability cost by Drummond and Holte [ 6 ]) as pc i the normalized total misclassification cost as norm = / max quantity pc i can be thought of as the proportion of the total risk arising from class i instances, since we have pc 0 + pc 1 = 1, while norm the maximum possible cost that the given classifier actually incurs. EachROCpoint( r 1 i ,r 0 i ) corresponds to a range of possible misclassification costs that depend on the marginal costs L i , as shown in Equation 4 .Wecan rewrite Equation 4 as a function of pc 1 as follows: Thus any point in ROC space translates (i.e., can be transformed) into a line in cost space. Of particular interest are the lines corresponding to the ROC convex lines enclose a convex region of cost space known as the lower envelope. The values of pc 1 for which a classifier is on the lower envelope provide scenarios under which the classifier is the optimal choice.
 One can compute the area under the lower envelope to obtain a scalar esti-mate of misclassification cost. Here, we denote points on the convex hull by ( r ,r denote the corresponding cost lines as f i ( x )= m i x + b and b i is the y -intercept of the i th cost line. The lower envelope is then composed of the intersection points of successive lines f i ( x )and f points p i =( x i ,y i ), which can be calculated as The area under the lower envelope can be calculated geometrically as the area of a convex polygon or analytically as a sum of integrals (the areas under the constituent line segments). For our purposes, it is convenient to express it as follows: The function A (  X  ) represents a loss measure, where higher values of A correspond to worse performance. This area represents the expected misclassification cost section, we discuss the implications of this loss measure. In the previous section, we related several measures of classifier performance to a notion of cost. In this section, we elaborate on the consequences of these con-nections, from which we derive definitions of  X  X isk X  for classifiers and instances. 3.1 Relationship Between Cost Curves and H Measure An interesting result emerges if we assume an accurate estimate of  X  from the training data or from some other source of background knowledge and rewrite this expression into the standard form of an equation for a line, which gives us c = c (  X  0 r 1  X   X  1 (1  X  r 0 )) + (1  X  r 0 ) .
 lope, can similarly be derived as Consequently, the area under the lower envelope can be expressed as: As the endpoints x i are the same as those used in the computation of the H Measure (see Equation 2 ), it follows that the H Measure is equivalent to the area under the lower envelope of the cost curve with uniform u ( c ) and prior probabilities  X  i known. Further, Hand has demonstrated that, for a particular curves X  X re simply specific instances of the simple linear cost model. Rather than debating the relative merits of these specific measures, which is beyond the scope of this work (cf. [ 3 , 9 ] for such discussions), we instead focus on the powerful consequences of adhering to the more general model.
 fier performance, it also provides an avenue for interpreting model performance. In fact, we find that it provides an insight into model performance under hypo-thetical scenarios X  X hat is, a notion of risk X  X hat cannot be explicitly captured by these other measures. We elaborate on this below. 3.2 Interpreting Performance Under Hypothetical Scenarios As a consequence of the relationship between the H Measure and cost curves, we can actually represent the H Measure loss function in cost space. By representing each of which corresponds to a loss function.
 Figure 1 depicts scenario curves for several different likelihood functions alongside a standard cost curve. Each curve quantifies the vulnerability of the classification algorithm over the set of all possible scenarios pc probabilistic beliefs about the likelihood of different cost ratios. The likelihood distributions include: (1) the Beta (2 , 2) distribution u ( c )= gested by [ 9 ]; (2) a Beta distribution shifted so that the most likely cost ratio is proportional to the proportion of minority class instances (i.e., c a truncated Beta distribution where the probability of minority class instances is greater than the probability of majority class instances (i.e., p ( c motivated by the observation that the minority class typically has the highest misclassification cost; (4) a truncated exponential distribution where the param-eter  X  is set to ensure that the expectation of class i is inversely proportional to the proportion of that class in the data (i.e., c i  X  1 / X  which assumes uniform distributions over probabilities and costs.
 From the figure, it is clear that the choice of likelihood distribution can have the area under the curve) and on which scenarios we believe will produce the greatest loss for the classifier. These curves also have intuitive meanings that may be useful when analyzing classifier performance. First, as the cost curve makes no a priori assumptions about the likelihood of different scenarios, it can present the performance of an algorithm over any given scenario. Second, if and when information about the likelihood of different scenarios becomes known, the cost curve presents the set of classifiers the pose the greatest risk (i.e., the components of the convex hull).
 Both interpretations are important. On the one hand, an unweighted cost curve can be used to identify the set of scenarios over which a classifier performs acceptably for any domain-specific definition of reasonable performance. On the other hand, a weighted scenario curve can be used to identify where an algorithm should be improved in order to achieve the maximum benefit given the available information. From the second observation arises a natural notion of risk. 3.3 Defining Risk hull is optimal over some range of cost ratios (see Equation 2 ). From this, we can derive two intuitive definitions: one for the risk associated with individual classifiers and one for the risk associated with individual instances. Definition 1. Assume that classifier C is optimal over the range of cost ratios [ c ,c ] . Then the risk of classifier C is the expected cost of the classifier over the range for which it is optimal: Definition 2. Theriskofinstance x is the aggregate risk over all classifiers that misclassify x .
 We discuss how these definitions may be applied to improve to classifier perfor-mance below. 3.4 RiskBoost: Optimizing Classification by Minimizing Risk Since we can quantify the degree to which instances pose the greatest risk to our classification algorithm, it is natural to strengthen the algorithm by assigning greater importance to these  X  X isky X  instances.
 on the  X  X ardness X  of correctly classifying a particular instance [ 8 ]. Instead, we propose a novel boosting algorithm that reweights instances according to their relative risk, which we call RiskBoost . RiskBoost uses the expected misclassifi-cation loss to reweight instances that are misclassified by the most vulnerable classifier according to both classifier performance and the hypothetical cost ratio distribution. Pseudocode for RiskBoost is provided as Algorithm 1 . Algorithm 1. RiskBoost To evaluate the performance of RiskBoost, we compare it with AdaBoost on 19 classification datasets from the UCI Machine Learning Repository [ 1 ]. We employ as suggested by [ 9 ]. AdaBoost is employed with the AdaBoost.M1 variant [ 8 ]. For both algorithms, we use 100 boosting iterations of unpruned the C4.5 decision trees, which previous work has shown benefit substantially from AdaBoost [ 15 ]. In order to compare the classifiers, we use 10-fold cross-validation. In 10-fold cross-validation, each dataset is partitioned into 10 disjoint subsets or folds such that each fold has (roughly) the same number of instances. A single fold is retained as the validation data for evaluating the model, while the remain-ing 9 folds are used for model building. This process is then repeated 10 times, with each of the 10 folds used exactly once as the validation data. As the cross-the performance results from 100 repetitions of 10-fold cross-validation to gen-erate reliable estimates of classifier performance. Performance is reported as AUROC (area under the Receiver Operating Characteristic). 4.1 Statistical Tests Previous literature has suggested the comparison of classifier performance across multiple datasets based on ranks. Following the strategy outlined in [ 4 ], we first rank the performance of each classifier by its average AUROC. The Friedman test is then used to determine if there is a statistically significant difference between the rankings of the classifiers (i.e., that the rankings are not merely randomly distributed), after which the Bonferroni-Dunn post-hoc test is applied to control for multiple comparisons. 4.2 Results From Table 2 , we observe that RiskBoost performs better than AdaBoost in 14 of the 19 datasets evaluated, with 1 tie. Further, we find that RiskBoost performs statistically significantly better than AdaBoost at a 95% confidence level over the collection of evaluated datasets. The 95% critical distance of the Bonferroni-Dunn procedure for 19 datasets and 2 classifiers is 0.45; consequently, an average rank lower than 1.275 is statistically significant, which RiskBoost achieves with an average rank of 1.21. Similar results were achieved for 10 repetitions of 10-fold cross-validation (where RiskBoost X  X  average rank was 1.11), 50 repetitions (1.26), and 500 repetitions (1.21). 4.3 Discussion For a better understanding of the general intuition behind RiskBoost, Figure 2 shows the progression for AdaBoost and RiskBoost when optimizing the H Measure with the Beta (2 , 2) cost distribution. At each iteration, the RiskBoost global maximum in the figure. Successive iterations of RiskBoost lead to direct cost reductions for this classifier, resulting in a gradual but consistent reduc-tion from peak risk. By contrast, AdaBoost establishes an arbitrary threshold for  X  X ncorrect X  instances. As a result, AdaBoost does not always focus on the instances that contribute greatest to the overall misclassification cost, which ultimately results in the erratic behavior demonstrated by AdaBoost X  X  scenario curves.
 Though RiskBoost offers promising performance over a diverse array of clas-sification datasets, we note that there is an expansive literature on cost-sensitive that can be used to tackle similar problems. A critical feature that sets our work apart from prior efforts, however, is that previous work tacitly assumes that mis-classification costs are known, whereas RiskBoost can expressly optimize mis-classification costs that are unknown and uncertain. Further, we demonstrate that this strategy for risk mitigation actually arises naturally from the frame-work of scenario analysis. We leave further empirical evaluation of RiskBoost with cost-sensitive boosting algorithms as future work.
 Classification models are an integral tool for modern data mining and machine model that will perform well on unseen data, often according to some hypo-thetical future deployment scenario. In doing so, two critical questions arise: First, how does one estimate performance so that the best-performing model can be selected? Second, how can one build a classifier that is optimized for these hypothetical scenarios? approaches for evaluating classifier performance in uncertain deployment sce-narios, we derived a relationship between H Measure and cost curves, two well-known techniques. As a consequence of this relationship, we found that ROC curves, H Measure, and cost curves can be represented as specific instances of a simple linear cost model. We found that by defining scenarios as probabilistic expressions of belief in this simple linear cost model, intuitive definitions emerge for the risk of an individual classifier and the risk of an individual instance. These observations suggest a new boosting-based algorithm X  X iskBoost X  X hat directly mitigates the greatest component of classification risk, and which we find to outperform AdaBoost on a diverse selection of classification datasets.
