 Vector Space Model (VSM) has been at the core of informa-tion retrieval for the past decades. VSM considers the doc-uments as vectors in high dimensional space. In such a vec-tor space, techniques like Latent Semantic Indexing (LSI), Support Vector Machines (SVM), Naive Bayes, etc., can be then applied for indexing and classification. However, in some cases, the dimensionality of the document space might be extremely large, which makes these techniques infeasible due to the curse of dimensionality .Inthispaper,wepro-pose a novel Tensor Space Model for document analysis. We represent documents as the second order tensors, or ma-trices. Correspondingly, a novel indexing algorithm called Tensor Latent Semantic Indexing (TensorLSI) is devel-oped in the tensor space. Our theoretical analysis shows that TensorLSI is much more computationally efficient than the conventional Latent Semantic Indexing, which makes it applicable for extremely large scale data set. Several experi-mental results on standard document data sets demonstrate the efficiency and effectiveness of our algorithm. Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing  X  Indexing methods General Terms: Algorithms, Theory, Experimentation, Performance Keywords: Tensor Space Model, Vector Space Model, Latent Semantic Indexing, Tensor Latent Semantic Indexing
Document indexing and representation has been a fun-damental problem in information retrieval for many years. Most of previous works are based on the Vector Space Model (VSM, [3]). The documents are represented as vectors, and each word corresponds to a dimension. Learning techniques such as Latent Semantic Indexing (LSI, [2]), Support Vector Machines, Naive Bayes, etc., can be then applied in such a vector space. The main reason of the popularity of VSM is probably due to the fact that most of the existing learning algorithms can only take vectors as their inputs, rather than tensors.
 When VSM is applied, one if often confronted with a doc-ument space R n with a extremely large n .Let x  X  R n de-notes the document vector. Let us consider n =1 , 000 , 000 and learning a linear function g ( x )= w T x . In most cases, learning g in such a space is infeasible in the sense of com-putability. For example, when LSI is applied in such a space, one needs to compute eigen-decomposition of a 1 M  X  1 M matrix 1 .

Different from traditional Vector Space Model based docu-ment indexing and representation, in this paper, we consider documents as matrices, or the second order tensors. For a document set with n words, we represent the documents as the second order tensors (or, matrices) in R n 1  X  R n where n 1  X  n 2  X  n . For examples, a 1 , 000 , 000-dimensional vector can be converted into a 1000  X  1000 matrix. Let X  X  R n 1  X  R n 2 denotes the document matrix. Naturally, a linear function in the tensor space can be represented as f ( X )= u T X v ,where u  X  R n 1 and v  X  R n 2 . Clearly, f ( X ) has only n 1 + n 2 (=2000 in our case) parameters which is much less than n (= 1 , 000 , 000) of g ( x ).

Based on the tensor representation of documents, we pro-pose a novel indexing algorithm called Tensor Latent Se-mantic Indexing (TensorLSI) operated in the tensor space rather than vector space. Sharing similar properties as the conventional Latent Semantic Indexing (LSI) [2], TensorLSI tries to find the principal components of the tensor space. Let { u i } n 1 i =1 be a set of basis functions of R n 1 be a set of basis functions of R n 2 . It is easy to show that { u i v T j } forms a basis of R n 1  X  R n 2 . Thus, TensorLSI aims at finding bases { u i } and { v j } such that the projections of the documents onto { u i v T j } can best represent the documents in the sense of reconstruction error.

It would be important to note that, while searching for the optimal bases { u i } and { v j } , we need only to compute the eigen-decompositions of two n 1  X  n 1 and n 2  X  n 2 matrices. This property makes our algorithm particularly applicable for the case when the number of words is extremely large. This work is foundamentally motivated by [4]. For more detailed document analysis using TSM, please see [1].
TensorLSI is fundamentally based on LSI. It tries to project the data to the tensor subspace in which the reconstruc-tion error is minimized. Given a set of document matrices X i  X  R n 1  X  n 2 , i =1 ,  X  X  X  ,m .Let Y i = U T X i V denote its projection in the tensor subspace U X  X  . The reconstruction
Note, we assume that the number of documents ( m )is larger than n .When m&lt;n , it suffices to compute the eigen-decomposition of a m  X  m matrix. Table 1: Complexity Comparison of TensorLSI and LSI error for X i canbewrittenas X i  X  UY i V T .Thus,the objective function of TensorLSI can be described as follows: which is equivalent to the following:
With some algebraic steps [1], we can show that, the op-timal U and V are given by solving the following two eigen-vector problems:
After obtaining the basis vectors { u i } ( i =1 ,  X  X  X  ,n { v j } ( i =1 ,  X  X  X  ,n 2 ), each u i v T j is a basis of the transformed tensor space, and u T i X t v j ( t =1 ,  X  X  X  ,m ) is the coordinate of X t corresponding to u i v T j in this tensor space. When we want to keep the first k principle component of document in the transformed tensor space, we use function f ( u i , v j )= to evaluate the importance of u i v T j with respect to i and j . f ( u i , v j ) reflects the importance of the tensor basis u terms of reconstruction error. When we want to keep the first k principle component in the transformed tensor space, we sort f ( u i , v j ) for all the i and j in decreasing order and choose the first k pairs.
 Table 1 lists the computational complexity comparison of TensorLSI and LSI, more detailed analysis can be found in [1].
In this section, document clustering on Reuters-21578 cor-pus 2 is used to show the effectiveness of our proposed algo-rithm. We chose k -means as our clustering algorithm and compared three methods. These three methods are listed below:
The clustering result is evaluated by comparing the ob-tained label of each document with that provided by the document corpus. The accuracy ( AC ) is used to measure the clustering performance [1].
Reuters-21578 corpus is at http://www.daviddlewis.com/ resources/testcollections/reuters21578/ Figure 1: (a)Clustering accuracy with respect to the number of classes. (b) Computation time (time on dimension reduction plus time on k -means) with re-spect to the number of classes. As can be seen, TensorLSI achieves comparable accuracy with LSI while much faster than LSI.

The evaluations were conducted with different number of clusters, ranging from 2 to 10. For each given cluster number k , 50 tests were conducted on different randomly chosen categories, and the average performance was computed over these 50 tests.

The average accuracy and the computation time (time on dimension reduction plus time on k -means) of three methods are shown on fiure 1. Both LSI and TensorLSI are signifi-cantly better than baseline with respect to both clustering accuracy and computation time. For k =(2 , 3 , 4 , 5 , 6), LSI and TensorLSI achieved almost same clustering accuracy. For k =(8 , 9 , 10). Clustering using LSI is slightly better than clustering using TensorLSI. However, in all cases, the computation time (time on dimension reduction plus time on k -means) of TensorLSI is extremely shorter than that of LSI.
A novel document representation and indexing method has been proposed in this paper, called Tensor Latent Se-mantic Indexing. Different from conventional LSI which considers documents as vectors, TensorLSI considers doc-uments as the second order tensors, or matrices. Based on the tensor representation, TensorLSI tries to find an opti-mal basis for the tensor subspace in terms of reconstruction error. Also, our theoretical analysis shows that TensorLSI can be much more efficient than LSI in time, memory and storage especially when the number of documents is larger than the number of words. [1] D. Cai, X. He, and J. Han. Tensor space model for [2] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [3] G. Salton, A. Wong, and C. S. Yang. A vector space [4] J. Ye. Generalized low rank approximations of matrices.
