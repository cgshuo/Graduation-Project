 ORIGINAL PAPER Chun Lei He  X  Louisa Lam  X  Ching Y. Suen Abstract In document recognition, it is often important to obtain high accuracy or reliability and to reject patterns that cannot be classified with high confidence. This is the case for applications such as the processing of financial docu-ments in which errors can be very costly and therefore far less tolerable than rejections. This paper presents a new approach based on Linear Discriminant Analysis (LDA) to reject less reliable classifier outputs. To implement the rejection, which can be considered a two-class problem of accepting the clas-sification result or otherwise, an LDA-based measurement is used to determine a new rejection threshold. This mea-surement (LDAM) is designed to take into consideration the confidence values of the classifier outputs and the relations between them, and it represents a more comprehensive mea-surement than traditional rejection measurements such as First Rank Measurement and First Two Ranks Measurement. Experiments are conducted on the CENPARMI database of numerals, the CENPARMI Arabic Isolated Numerals Data-base, and the numerals in the NIST Special Database 19. The results show that LDAM is more effective, and it can achieve a higher reliability while maintaining a high recognition rate on these databases of very different origins and sizes. Keywords Document recognition  X  Linear discriminant analysis  X  Rejection criterion  X  Recognition reliability 1 Introduction In document recognition applications, it can be very impor-tant to achieve high levels of accuracy as well as high rec-ognition rates, because even a low percentage of recognition errors can have serious consequences. For example, while OCR algorithms have reported recognition rates in excess of 99% on the numeral databases of MNIST [ 1 , 2 ] and CENP-ARMI [ 3 ], even the resulting low error rates can be extremely costly in applications such as the processing of financial doc-uments. For these applications, errors should be reduced as much as possible, and it is preferable to reject some classifi-cation results in order to achieve a very high reliability while maintaining a high recognition rate as defined by:
To accomplish the objective of high reliability, samples that are recognized with low confidence should be rejected, and various rejection methods depending on classifier out-puts have been implemented in the literature. Some of these strategies are outlined below. 1.1 Rejection strategies in the literature Generally speaking, classifiers can produce abstract (class label only), rank, and measurement level outputs. For clas-sifiers that produce only class-level outputs, rejection can be implemented by majority or plurality voting of several such classifiers. In the case of majority vote, tied votes can result when the number of classifiers is even, which may lead to rejections. The result would be lower recognition and also lower error rates [ 4 , 5 ]. With plurality voting, which does not require a majority, recognition rates higher than those of majority vote can be obtained, but it would also entail a higher error rate. It can be seen [ 5 ] that these two voting methods do produce graphs of similar shapes that fluctuate according to whether the number of voters is even or odd, but the graphs would be located at different recognition levels.
Most recent recognition algorithms produce outputs that consist of target classes together with corresponding confi-dence values (or distance measurements). With these mea-surements, thresholds can be established in the training process and used as criteria for acceptance or rejection of the classification results. A large number of confidence measures have been proposed for different applications and recogniz-ers. For example, this measure can be based on matching the input pattern against stored templates, and it has been applied to face recognition [ 6 ], matching of graphic symbols [ 7 ], and logos [ 8 ].

In the recognition of handwritten numerals with a feed-forward neural network [ 9 ], an output rejection neuron is introduced to catch the patterns possibly misdirected to an incorrect subclassifier. For a hidden Markov model based off-line handwritten word recognition system [ 10 ], three types of rejection strategies dependent on class, hypothesis, and global thresholds are applied at the post-processing level to determine whether the best word hypothesis can be accepted or otherwise. 1.2 Rejection strategy proposed in this work In this work, it is considered that a rejection process could be applied to different recognition algorithms as well as data sets, and it describes the design of such a process based on Linear Discriminant Analysis (LDA). Furthermore, experi-ments are conducted on the recognition outputs when a Sup-port Vector Machine (SVM) classifier is applied to process three databases of handwritten numerals. This rejection pro-cess is applicable to other current recognition algorithms, and is not restricted to any particular type of data, provided the recognition algorithms produce measurement-level outputs.
Most recent recognition algorithms do produce outputs thatconsistoftargetclassestogetherwithcorrespondingcon-fidence values (measurements). Achieving high recognition and reliability with these methods would require them to be capable of assigning generally higher confidences to cor-rect recognition results than to incorrect ones. This confi-dence scoring method may consist of implementing a simple function of appropriate parameters drawn directly from the recognition process, or it may be a learning task in which a classifier is trained to use an array of parameters to distin-guish correct recognitions from misclassifications [ 11 ].
It would appear that the Bayes decision rule embodies a rejection rule, namely, the decision can be based on the maximum confidence value provided this maximum exceeds a certain threshold value. However, this approach did not perform satisfactorily when experiments were conducted on numeral databases because the distribution of incorrectly classified samples is not Gaussian in shape, but remains gen-erally flat with small fluctuations throughout a range of con-fidence values, which means the rejection threshold would be very difficult to establish effectively. This phenomenon is shown in Fig. 1 , in which the incorrectly classified data are shown to be evenly distributed over a wide range of confi-dence values by a LIBSVM classifier [ 12 ].
 Figure 1 shows the results on the training set when LIB-SVM [ 12 ] was applied to the CENPARMI Arabic database of numerals, and it shows that misclassifications can occur even with very high confidence. Consequently, there is no obvious means for establishing an effective rejection threshold.
In this paper, we adopt a novel post-processing approach to the confidence values output by a classifier. This is imple-mented by applying Linear Discriminant Analysis (LDA) [ 13 ] to the measurement level outputs so that samples with low confidence can also have a Gaussian distribution separate from that of the correctly classified data. LDA is a supervised classification method widely used to find the linear combi-nation of features for separating two or more classes. The main idea of LDA is to project high-dimensional data onto a line and perform classification in this one-dimensional space. It provides a linear projection of the data with the outcome of maximumbetween-classvarianceandminimumwithin-class variance. By finding the feature space that can best discrim-inate an object from others, these discriminative methods have been successfully used in pattern classification applica-tions including Chinese character recognition [ 14 ] and face recognition [ 15 ].

Our objective in this research is to better identify sam-ples recognized with low confidence for rejection and thus achieve a higher reliability in handwritten numeral recog-nition. To achieve this objective, we define a novel rejec-tion measurement X  X DA Measurement (LDAM), and we compare it to other rejection measurements, such as First Rank Measurement (FRM) and First Two Ranks Measure-ment (FTRM) in Sect. 2 . Then, we describe the experiments conducted on three databases in Sect. 3 and compare the resultsobtainedfromusingthethreemeasurementsinSect. 4 . We conclude the discussions with some remarks in Sect. 5 . 2 Rejection measurements In considering the outputs of classifiers for the rejection option as a two-class problem, (acceptable or rejected classi-fication), the outputs at the measurement level can be consid-ered as features for the rejection option. In an output vector whose components may represent distances or probabilities, we expect the confidence value (measure) of the first rank (most likely class) to be far distant from the confidence val-ues or measures of the other classes. In other words, good outputs should be easily separated into two classes: the con-fidence value of the first rank and others. In the following discussion, we assume that the classifier outputs the proba-bilities of the patterns for each class, and the considerations would be analogous in the case when the classifier outputs the distances. 2.1 First rank measurement and first two ranks Generally, rejection strategies can be directly applied to the outputs that represent probability estimations (or equiva-lently, distances). In an M -class problem, suppose P ( x { p tor of the given pattern x , with probabilities p i ( x ) descending order. The decision can be based on sgn ( 1 ( x T ), where T and 1 ( x ) = p 1 ( x ) .

If 1 ( x )  X  T 1 , the classifier rejects the pattern and does not assign it to a class (it might instead be passed to a human operator). This has the consequence that on the remaining patterns, a lower error rate can be achieved. This method uses the First Rank Measurement (FRM) [ 16 ].

Using this method, the frequency distribution according to confidence values of samples in the training set is considered and the threshold T 1 is determined accordingly.

However, FRM cannot always identify incorrectly classi-fied patterns, as can be seen from the probability distribution of misclassified samples shown in Fig. 1 .
 To overcome this deficiency of FRM, the First Two Ranks Measurement (FTRM) [ 17 ] has been introduced. It uses the difference between the probabilities p 1 ( x ) and p 2 the first two ranks as a condition for rejection. In FTRM, the measurement function is 2 ( x ) =|| p 1 ( x )  X  p 2 ( x ) || || . || can be any distance measurement, and the decision func-tion is based on sgn ( 2 ( x )  X  T 2 ) , where T 2 is a threshold determined from the training set.
 However, FTRM cannot solve the problem in some cases. pattern may still be accepted, when this pattern should really have been rejected since the top two classes are close to each other in terms of relative distance. 2.2 Linear discriminant analysis measurement To consider the relative difference between the measure-ments in the first two ranks and all other measurements, LDAM is defined and applied. Since rejection in classifi-cation can be considered as a two-class problem (acceptance or rejection), we apply LDA for two classes, to implement rejection.

LDA approaches the problem by assuming that the condi-tionalprobabilitydensityfunctionsofthetwoclassesareboth normally distributed. There are n = n 1 + n 2 observations with d features in the training set, where { x 1 i } n 1 i discrimination assumes two normal distributions: ( x |  X  1 N ( X  maximize the Fisher criterion [ 13 ]: J (w) = tr w T S w w wheretr (  X  ) denotesthetraceofamatrix, S B and S w denotethe between-class scatter matrix and within-class scatter matrix, respectively, and w is the optimal discriminant vector. For the two classes  X  1 and  X  2 , with a priori probabilities p p 2 (it is often assumed that p 1 and between-class scatter matrices can be written as S where 12 is the average variance of the two classes.
Sincewewill usethis principleontheoutputs for therejec-tion option as a one-dimensional application, w,  X  1 and  X  are scalars, and hence the criterion becomes: J (w) = S B and G ( 2 ) ( x ) ={ p 2 ( x ), p 3 ( x ), . . . , p M ( x maximize the separation between the highest confidence and others. Then it follows that  X   X  Thus, in LDA, J (w) =
Then the decision function would be based on sgn ( J (w)  X  T 3 ) , where T 3 is a threshold derived from the training set, and all values are scaled to [0, 1].
In summary, when compared to FRM and FTRM, LDAM should be more comprehensive and informative since it compares the relative difference of the measures in the first two ranks with all other measures. 3 Experimentation 3.1 Databases used in experiments The LDA-based method described above is applied to the rec-ognition results of three different databases: the CENPARMI Arabic Isolated Numerals Database [ 18 ], the CENPARMI numerals database, and the isolated numerals contained in NIST Special Database 19 [ 19 ]. The method has previously been applied to the Arabic Numerals Database [ 20 ], and in the present work, the experimentation has been extended to two other well-known databases of numerals, after which the results from the three databases are considered together.
These three databases have been selected because they have very different sizes and origins. The CENPARMI Arabic Isolated Numerals Database was collected recently (in 2008) as part of a larger database in response to the recent development of research in Arabic handwriting recognition, and it consists of isolated handwritten Arabic numerals. The CENPARMI and NIST numeral databases are well-known and have been tested by researchers for over twenty years; the former was extracted from the ZIP codes of USPS mail-pieces in the early 1980s while the latter consists of numerals collected in the early 1990s from 3,699 forms on which the writers were instructed to print specific numerals in desig-nated boxes.

We understand that, strictly speaking, the samples in the first database should be called  X  X rabic-Indic X  or  X  X astern Arabic X  numerals, while the latter two databases contain  X  X indu-Arabic X  or  X  X uropean X  numerals. For simplicity, we will continue to refer to them as Arabic numerals and numer-als, respectively, in accordance with common usage in this field. 3.2 Distributions of samples in databases The distributions of samples in each of the three databases are given below: (i) The CENPARMI Arabic Isolated Numerals Database (ii) The CENPARMI Numerals Database contains 4,000 (iii) The isolated numerals in NIST Special Database 19 3.3 Samples from the databases In Figs. 2 , 3 , and 4 , we show five samples from each of the 10 classes of numerals from the databases used in the experiments.

For Fig. 2 , the class of the numeral is shown in the first column, its Arabic printed form is shown in the second, fol-lowed by five examples of its handwritten form in the third column. These written samples are shown in the same vertical positions as they appear in the text lines.
 3.4 Preprocessing The images in the CENPARMI and NIST databases of numerals are in binarized form while the Arabic numerals database consists of images in 256 gray levels.

The images in the binary databases are size-normal-ized using moment normalization [ 21 ] to align the centroid (center of gravity) to the geometric center of the normalized plane. The grayscale images were converted to binary ones through the processes of grayscale normalization and size normalization using moment normalization, then binarized with a threshold determined by the algorithm of Otsu [ 22 ]. 3.5 Feature extraction Gradient features were extracted from pseudo gray-scale images [ 23 ]. The Robert filter, which uses the masks 01  X  10 and dient strengths and directions of pixels. The direction of the gradient was quantized to 32 levels with an interval of  X / The normalized character image was divided into 81 ( 9  X  9 blocks. After extracting the strengths and directions in each image, the spatial resolution was reduced from 9  X  9to5  X  by down sampling every two horizontal and every two verti-cal blocks with a 5  X  5 Gaussian filter. Similarly, the direc-tional resolution was reduced from 32 to 16 levels by down sampling with a weight vector 14641 T , to produce a feature vector of size 400 ( 5  X  5  X  16 ) . Moreover, the trans-formation y = x 0 . 4 was applied to make the distribution of thefeaturesGaussian-like.Thefeaturesetsizewasreducedto 400byprincipalcomponentanalysis(KLtransform).Finally, we scaled the feature vectors by a constant factor so that the values of feature components range from 0 to 1.

Gradient features and down sampling are image pro-cessing techniques commonly used in the recognition of handwritten characters from various countries, including Devnagari characters [ 24 ]. 3.6 Classification Support Vector Machines [ 25 ] was chosen as a classifier. SVMs with different kernel functions can transform a non-linear separable problem into a linear separable one by pro-jecting data into the feature space, and then SVMs can find the optimal separating hyperplane. In this research, the experiments were conducted using LibSVM [ 12 ]asclassifier and a Radial Basis Function (RBF) was chosen with kernel k ( x i , x j ) = exp (  X   X  || x i  X  x j || 2 ) . Two parameters need to be determined when using RBF kernels, C &gt; 0 being the penalty parameter of the error term and  X  the kernel parameter.Theseparameterswereoptimallychosenbycross-validation via parallel grid search on the training set. These optimal parameter values were then applied on the test set.
The probability values output by the SVM classifier are then considered by a post-processor which considers whether the recognition result should be rejected according to LDAM. The results from applying this measurement are considered in the next section together with the results obtained from applying FRM and FTRM. 4 Experimental results and analyses For each database, the SVM classifier was trained on the training set and tested on the test set. Then FRM, FTRM, and LDAM were applied as rejection criteria with threshold T = 0 . 05. The results shown in Table 3 were obtained. 4.1 Classification results on three databases It is worth noting that for these three databases, the SVM classifier achieved very similar recognition rates without rejection, varying from 98.10 to 98.48%. This shows the con-sistent behavior of the SVM classifier even when trained on sets of sizes with different orders of magnitude.
When rejection was determined by FRM, no rejections were made on any of the three databases. This was due to the fact that the top choice always had measurement val-ues exceeding the common set threshold of T = 0 . 05. With FTRM, the three methods all had rejections below 0.5% with the same threshold, for a similar reason.

Then when LDAM was applied for rejection, the method was most effective on the NIST database, given that out of 2,807 rejected samples, 732 of them (26.08%) would have been recognition errors. For the CENPARMI Arabic and CENPARMI numerals, the ratios are 17.22 and 18.29%, respectively.

Furthermore, it is remarkable that on three such very dif-ferent databases, the reliabilities achieved with the same SVM classifier and LDA rejection measurement are uni-formly high at around 99.7%. The level and consistency of these results would provide solid support for the validity of the method presented in this work.
 Nevertheless, the very low (or zero) rejection rates by FRM and FTRM do highlight the difficulty of establish-ing effective rejection thresholds with these measurements. A more detailed examination of this problem is presented in the next section. 4.2 Analyses of classification results For an analysis of the rejection behavior with the three mea-surements, the distributions of samples according to mea-surement value from the experiments conducted on the three test sets are shown in Fig. 5 . In each graph, the horizontal axis indicates the values of each measurement (FRM, FTRM, and LDAM), while the vertical axis shows the numbers of samples. The solid lines represent the distributions of errors, and the dotted lines represent the distributions of correctly recognized samples.

For each of the databases, the distributions based on FRM are shown first on the left. Although the numbers of cor-rectlyclassifiedsamples tendtobeincreasingfunctions of the measurements, the errors are distributed more randomly over ranges of confidence values, so it is too difficult to establish an effective rejection threshold. Compared to FRM, FTRM is more discriminating, as the range of measurements here is wider than in FRM, and therefore there is greater separation of the samples according to this measurement. However, the distribution of errors in FTRM also does not provide an effec-tive cut-off point for reducing classification errors.
From these distributions, it is also clear that a rejection threshold of 0.05 would not result in any rejections for the three databases under FRM, while the same threshold would result in significant rejections only in the NIST Database for FTRM.

Figure 5 also shows that LDAM is more discriminating than FRM and FTRM, because the errors plus correctly clas-sified samples with low confidence values are assigned small measurements. This can be seen for all three databases in Fig. 5 , in which the numbers of errors can also be observed to decrease sharply for small values of LDAM. For example, with the CENPARMI Arabic numerals, out of the 95 samples initially wrongly classified without rejections, 78 of them were assigned LDA measurements of less than 0.05, and would therefore be rejected with this threshold. Thus, LDAM enables a more effective reduction of potential errors with the thresholds obtained from the training set.
Figure 6 shows the reliabilities that were achieved with different thresholds on the CENPARMI Arabic Numerals Database, for each of the three measurements. The graphs for the other two databases are almost identical in shape and are therefore omitted. It can be observed that, among the three measurements, LDAM enables the reliability to increase most rapidly for small rejection threshold values, and a reliability close to 1 is achieved with a threshold of 0.05. Beyond this threshold value, the slow increases in reli-ability would be accompanied by declines in the recogni-tion rate, and the trade-off may be decided by the user. For example, with a threshold value of 0.15, the recognition rate and reliability would be 91.58 and 99.81%, respectively, compared with 92.50 and 99.69% for the threshold value of 0.05.

As indicated in Table 3 , after processing the three data sets by the SVM classifier and applying LDAM for rejection, there were 17, 6 and 174 misclassifications in the test sets of the CENPARMI Arabic, CENPARMI, and NIST numeral databases, respectively. All these images are shown in Fig. 7 .
In Fig. 7 , for the Arabic numerals, most (12 out of 17) of the misclassifications are due to the confusing styles that could be used in writing the numerals  X 2 X  and  X 3 X  that would be indistinguishable even to human beings. This can be due to the fact that writers in different regions/countries can write the  X 2 X  and  X 3 X  in identical styles, and this problem appears to be more severe in this data set than the one reported in [ 26 ], where there were confusions between  X 2 X  and  X 3 X  in only 5 samples out of 10,000.

For the other numeral databases, some of the misclas-sifications are very understandable as they may have been the results of incorrect labeling during the process of data collection and preparation. However, the causes of other misclassifications are far from obvious to the human eye and are probably the result of falling on the wrong side of certainthreshold(s)intheautomaticrecognitionprocess.One explanation for some of the errors in the NIST database may have been caused by the mislabeling of certain samples (or by writers X  mistakes in printing the numerals indicated) in the test and training sets. Due to the immense size of the latter (344,307 samples), the effort required to verify and ensure the correct labeling of all data might have been too immense to be practicable. 5 Concluding remarks In pattern recognition, the rejection option can be very useful in preventing misclassifications, especially in applications which require a high reliability. We designed a novel rejec-tion criterion using the LDA Measurement (LDAM), which is based on the principle of Linear Discriminant Analysis. The design of this measurement incorporates information about the relationships among the probabilities in the output vector of each pattern. This measurement was applied to pro-cess the training and test sets of three databases of very dif-ferent sizes. The results indicate that a very consistent and high level of reliability can be achieved in the recognition results. At the same time, we compared LDAM with other measurements such as First Rank Measurement (FRM) and First Two Ranks Measurement (FTRM). The results indi-cate that LDAM achieved a higher reliability than the other measurements when a small threshold was set.

While the method presented in this paper has been imple-mented for handwritten numeral recognition, it is really much more general in nature and can be applied to most pattern recognition contexts in which measurement level outputs are generated.

In the future, we can apply this rejection method to train-ing procedures or multi-classifier systems. Moreover, we can apply this methodology to semi-supervised learning, so that we could reject the data with unreliable classification results produced by supervised learning.
 References
