 Learning users X  preferences is critical to enable personalized recommendation services in various online applications such as e-commerce, entertainment and many others. In this pa-per, we study on how to learn users X  preferences from abun-dant online activities, e.g., browsing and examination, which are usually called implicit feedbacks since they cannot be interpreted as users X  likes or dislikes on the corresponding products directly. Pairwise preference learning algorithms are the state-of-the-art methods for this important prob-lem, but they have two major limitations of low accuracy and low efficiency caused by noise in observed feedbacks and non-optimal learning steps in update rules. As a response, we propose a novel adaptive pairwise preference learning al-gorithm, which addresses the above two limitations in a s-ingle algorithm with a concise and general learning scheme. Specifically, in the proposed learning scheme, we design an adaptive utility function and an adaptive learning step for the aforementioned two problems, respectively. Empirical s-tudies show that our algorithm achieves significantly better results than the state-of-the-art method on two real-world data sets.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Collaborative Recommendation; Implicit Feedbacks; Pair-wise Preference Learning
Recommendation and personalization technology has an extremely wide spectrum of online applications, including e-commerce, entertainment, professional networks, mobile ad-vertisement, etc. Automatically mining and learning user-s X  preferences from their online activities such as browsing and examination records is critical to provide qualified per-sonalized services. Such activities are usually called users X  implicit feedbacks, which is very different from explicit feed-backs like graded ratings in the contest of Netflix $1 million prize because we cannot infer users X  true preferences from implicit feedbacks directly. In this paper, we focus on this important problem of learning users X  preferences from im-plicit feedbacks. Note that implicit feedbacks are usually represented as (user, item) pairs instead of (user, item, rat-ing) triples for explicit feedbacks.

Previous works on preference learning with implicit feed-backs include algorithms based on preference regression [1] and preference paired comparison [5], where the latter usu-ally performs better in empirical studies due to the more relaxed pairwise assumption as compared with that of the former. Specifically, paired comparison is defined on an ob-served (user, item) activity ( u, i ) and an unobserved (user, item) activity ( u, j ), where ( u, i ) can be interpreted that user u has implicitly expressed some preference on item i while ( u, j ) means that such activity is not observed. Paired comparison usually simplifies the hidden relationships and assumes that a user u has a higher preference score on item i than on item j , i.e., ( u, i )  X  ( u, j ) [5]. Note that such t-wo (user, item) pairs or a triple ( u, i, j ) is usually randomly sampled from the database of users X  feedbacks for prefer-ence learning. With the paired comparisons, different forms of loss functions can then be designed and optimized for different purposes.

However, previous works based on paired comparison usu-ally have two major limitations. First, most algorithms adopt the pairwise relationship ( u, i )  X  ( u, j ) without con-sidering the existence of some noisy triples that may not satisfy the pairwise relationships, and thus results in low accuracy. Second, most algorithms randomly sample triples from a huge set of ( u, i, j )s as constructed from the observed implicit feedbacks, which is often of low efficiency due to the resulted non-optimal learning steps. Some works have realized the above two problems and relax the pairwise re-lationships via introducing a new preference score on a set of items [2] instead of on a single item [5], which introduces a more general loss function. Some other works design some advanced sampling strategies for the second issue such as [4].
In this paper, we aim to address the aforementioned t-wo problems in one single algorithm. Specifically, we de-sign a concise and general learning scheme, which is able to absorb different loss functions and sampling strategies as s pecial cases. Furthermore, we design an adaptive utility function and learning step in a pairwise preference learning algorithm, which is thus called APPLE (a daptive p airwise p reference le arning).
We use R = { ( u, i ) } to denote a set of implicit feedbacks or activities from n users and m items. Each (user, item) pair ( u, i ) means that user u has browsed or examined item i , which is usually called an implicit feedback of user u on item i due to the uncertainty of the user X  X  true preference. Our goal is then to exploit the data R in order to generate a personalized ranked list of items from { j | ( u, j ) /  X  R} for each user u .
A pairwise preference learning algorithm usually mini-mizes a tentative objective function f ( u, i, j ) for a randomly sampled triple ( u, i, j ). A triple ( u, i, j ) means that the re-lationship between user u and item i is observed while the relationship between user u and item j is not observed. In order to encourage pairwise competition, the tentative ob-jective function is usually defined on a pairwise preference difference, i.e., f ( u, i, j ) = f ( X  r uij ), where  X  r is the difference between user u  X  X  preferences on item i and item j . A user u  X  X  preference on an item i , i.e.,  X  r ui ically modeled by a set of parameters denoted by  X  , which include user u  X  X  latent feature vector U u  X  R 1  X  d , item i  X  X  la-tent feature vector V i  X  R 1  X  d and item i  X  X  bias b i  X  R . With the model parameter  X  , we can estimate a user X  X  preference on a certain item via  X  r ui = U u V T i + b i .

With a sampled triple ( u, i, j ) and a tentative objective function f ( X  r uij ), the model parameter  X  can then be learned or updated accordingly. The update rule is usually repre-sented as follows, where f ( X  r uij ) can be  X  ln 1 / (1 + e  X   X  r uij ) [5], max(0 , 1  X   X  r uij ) [6] or in other forms, in order to encourage differen-t types of pairwise competitions between an observed pair ( u, i ) and an unobserved pair ( u, j ). Note that  X  X  in Eq.(1) is from a regularization term  X  2 k  X  k 2 u sed to avoid overfitting.
There are two fundamental questions associated with the update rule in Eq.(1), namely (i) how to choose a specif-ic form of the tentative objective function f ( X  r uij ), and (ii) how to sample a triple ( u, i, j ). For the first question, dif-ferent works usually incorporate different loss functions into f ( X  r uij ) with different goals, which will then result in different works sample a triple in a uniformly random manner [5, 6].
Mathematically, the above two questions can be repre-sented by a concise and general learning scheme,
Learning Scheme: (  X  ( u, i, j ) ,  X  ( u, i, j )) := (  X ,  X  ) (2) where (i) the first term  X  ( u, i, j ) denotes the utility of a randomly sampled triple ( u, i, j ), which answers the question of how to sample a triple, and (ii) the second term  X  ( u, i, j ) =
Input : Triples T = { ( u, i, j ) } 1  X  u  X  n, 1  X  i  X  m , and learning scheme (  X  ( u, i, j ) ,  X  ( u, i, j )).
 1: for t = 1 , . . . , T do 2: repeat 3: Randomly sample a triple ( u, i, j ) from T . 4: Generate a random variable  X  rand  X  [0 , 1]. 5: Calculate the utility  X  ( u, i, j ). 6: until  X  rand  X   X  ( u, i, j ) 7: Update model via Eq.(3) with  X  ( u, i, j ). 8: if S-II &amp; mod ( t, K ) = 0 then 9: Update  X  2 via Eq.(7).
 Figure 1: The algorithm of a daptive p airwise p reference le arning (APPLE).  X   X  r u ij is the gradient, which answers the question of how to choose a specific form of the tentative objective function. The update rule in Eq.(1) can then be equivalently written as follows, wise preference learning can also be described by an algo-rithm, which is shown in Figure 1, in particular of lines 5-7. In Figure 1, we can see that the chance of sampling a triple T and  X  ( u, i, j ) is the utility of the randomly sampled triple.
With the general learning scheme in Eq.(2), we can rep-resent a typical pairwise preference learning algorithm in a concise way. For example, the seminal algorithm BPR (Bayesian personalized ranking) [5] can be represented as follows, from which we can see that our learning scheme in Eq.(2) is quite powerful and is able to absorb other pairwise prefer-ence learning algorithms as special cases.

Based on the general learning scheme in Eq.(2), we pro-pose one preliminary learning scheme and two specific learn-ing schemes so as to learn users X  true preferences in a more effective and efficient way.
It is well known [4] that a large preference difference  X  r means that the pairwise competition between ( u, i ) and ( u, j ) of a typical triple ( u, i, j ) has been well encouraged, and thus may not be helpful to use this  X  r uij in the update rule in Eq.(1). This observation motivates us to sample triples with small preference differences. We thus propose a pre-liminary learning scheme with an adaptive utility function without changing the expectation of the learning step, It is easy to show that a smaller  X  r uij will result in a larger u-tility, and the expectation of the learning step |  X  | for ( u, i, j ) F igure 2: The distribution of  X  r uij in different learn-ing stages. |T |  X   X  0  X  |  X  0 | . The advantages of the learning scheme S-0 as compared with S-BPR are, (i) a triple ( u, i, j ) with larger  X  r uij will have a lower chance to be sampled, and (ii) the learning step |  X  0 | is larger than |  X  BP R | , which is assumed to be helpful for the learning efficiency.

In the following sections, we will describe two specific learning schemes based on this preliminary learning scheme.
We assume that a triple ( u, i, j ) with a very small  X  r a higher chance to be noise, especially when the learning process has been conducted for a cerntain time. Specifically, a small  X  r uij often denotes a high chance that user u dislikes item i or user u likes item j , and then we may not encourage the pairwise competition between ( u, i ) and ( u, j ) any more. Hence, such a triple ( u, i, j ) is considered noise for pairwise preference learning. As a response, we design a new utility function,  X  ( u, i, j ) = 1 order to reduce the chance that a triple ( u, i, j ) with lower  X  r uij will be sampled. This new utility function reaches the peak value 0 . 25 when  X  r uij = 0, and becomes smaller as  X  r increases or decreases. In order to constrain the value range to [0 , 1], we obtain our first learning scheme, We can see that the difference between S-I and S-0 is the u-the impact of noisy triples (i.e., triples with small preference difference  X  r uij when the learning process has conducted for some time).
In order to improve the learning efficiency, we may set the learning step |  X  | to be a larger value in the beginning, since most  X  r uij are very small. While in the middle or in the end of the learning process, we shall decrease the learning step |  X  | so as to reach the optimal solution in a smooth manner and thus to achieve high recommendation accuracy. We then propose a more sophisticated learning scheme accordingly in order to benefit both learning efficiency with large |  X  | and recommendation accuracy with small |  X  | .

We first show the distribution of preference difference  X  r of MovieLens1M data (see more information in the Section of Experimental Results) in different learning stages in Fig-ure 2. We can see that the whole distribution will move from the origin to the right, which means that the difference  X  r becomes larger in the learning process as expected by the competition encouragement. Based on this observation, we propose to use the average preference difference to update the value of |  X  | in a certain monotonic decreasing function. Due to the complexity of  X  1 and the fact that  X  r uij  X  0  X  1 when  X  r uij  X  [0 ,  X  ) in Eq.(6), and obtain an estimation x = 2 , y = 1 . 5 via minimizing the KL-divergence between  X  and  X  1 with  X  r uij  X  0. We then have  X  r uij = ln(1 / X   X  to represent the average preference difference, where  X  N = of the k  X  X  iteration in Figure 1. With  X  r uij , we reach our second learning scheme,
S-II: 4 e where g is set as the maximal value of  X  r uij obtained when the learning process converges so as to ensure that |  X  2 larger than 1 in the learning process and is roughly equal to 1 in the end. We can see that the major difference between S-II and S-I is the gradient  X  , which is not static and fixed as  X  1 in Eq.(6), but is dynamic w.r.t. the preference difference. The new gradient  X  aims to achieve better learning efficiency and recommendation accuracy, which is also supported by our empirical studies. In our empirical studies, we use two data sets, including MovieLens1M 1 and Douban 2 .
 MovieLens1M MovieLens1M contains about 1 million triples in the form of (user, movie, rating) with n = 6 , 040 users and m = 3 , 952 movies. In our experiments, we ran-domly take about 50% triples as training data, about 10% triples as validation data, and the remaining about 40% triples as test data. For training data, we keep all triples and take the corresponding (user, movie) pairs as implicit feedbacks. For validation data and test data, we only keep triples with ratings equal to 4 or 5 and take the correspond-ing (user, movie) pairs as implicit feedbacks [3]. Douban We crawled a real implicit data of users X  feedbacks on books from Douban.com in December 2013, which is one of the largest Chinese online social media websites. The Douban data contains about 3 million (user, book) reading records from 10 , 000 users and 10 , 000 books. Similarly, we randomly take about 50% records as training data, about 10% records as validation data and the remaining about 40% records as test data.

We conduct the above  X 50%, 10%, 40% X  splitting proce-dure of each data for 5 times and thus get 5 copies of training data, validation data and test data.
We adopt a commonly used top-k evaluation metric for implicit feedbacks [3], i.e., precision ( P re @ k ). We use k = 5 h ttp://grouplens.org/datasets/movielens/ http://www.douban.com/ in our experiments since most people may only check a few r ecommended items. In our experiments, we study our proposed algorithm AP-PLE with two specific learning schemes in comparison with the state-of-the-art algorithm BPR (Bayesian personalized ranking) [5]. We implement both learning schemes in Eq.(6) and Eq.(7) and that of BPR in Eq.(4) in the same algorith-mic framework in Figure 1 for fair comparison.

For all experiments, we use the validation data and P re @5 to tune the hyperparameters. Specifically, we search the regularization parameter  X  in the range of [0 . 001 , 0 . 5]. For the parameter g of our learning scheme S-II in Eq.(7), we have tried g  X  { 1 , 2 , 3 , 4 } to find an approximation of the maximal value of  X  r uij in Eq.(7). For the learning rate  X  in Eq.(3), we fix it as 0 . 01 [3]. The parameter K is set as 10 5 , and the iteration number T is searched around 10 8 for sufficient convergence. The number of latent features for users and items is fixed as d = 10 for MovieLens1M and d = 20 for Douban.
The recommendation performance of our two learning schemes and S-BPR are shown in Table 1, from which we can see that the overall recommendation performance ordering is S-II  X  S-I &gt; S-BPR. We also conduct significance test and find that S-I and S-II are significantly better than S-BPR on both data sets. The results in Table 1 clearly demonstrates the advan-tages of our proposed learning schemes in our algorithm AP-PLE, in particular of the learning scheme S-II with sophisti-The learning efficiency of our two learning schemes and S-BPR are shown in Figure 3, from which we have a sim-ilar observation, i.e., the overall convergence performance ordering is S-II &gt; S-I &gt; S-BPR. It is interesting to see that this ordering is consistent with that of the learning step of the learning schemes, i.e., |  X  2 | &gt; |  X  1 | &gt; |  X  s that increasing the learning step can indeed improve the convergence performance.
 Table 1: Recommendation accuracy of our pro-posed algorithm APPLE with learning schemes S-I in Eq.(6) and S-II in Eq.(7), and the seminal algo-rithm BPR [5] with S-BPR in Eq.(4) on MovieLen-s1M and Douban data sets.

I n this paper, we propose a novel algorithm called a daptive p airwise p reference le arning (APPLE) for collaborative rec-o mmendation with implicit feedbacks. Our proposed al-gorithm improves the state-of-the-art pairwise preference F igure 3: Learning efficiency of our proposed algo-rithm APPLE with learning schemes S-I in Eq.(6) and S-II in Eq.(7), and the seminal algorithm BPR [5] with S-BPR in Eq.(4) on MovieLens1M and Douban data sets. learning algorithm, i.e., BPR [5], via a concise and gen-eral learning scheme with an adaptive utility function and an adaptive learning step. Empirical studies show that our algorithm performs significantly better than the BPR al-gorithm regarding both the recommendation accuracy and learning efficiency.

For future work, we are mainly interested in generalizing our learning scheme in APPLE to include heterogeneous user feedbacks and social context information.
We thank the support of National Natural Science Foun-dation of China (NSFC) No. 61272303, National Basic Re-search Program of China (973 Plan) No. 2010CB327903, Natural Science Foundation of SZU No. 201436, NSFC No. 61170077, NSF GD No. 10351806001000000, GD S&amp;T No. 2012B091100198, S&amp;T of SZ No. JCYJ20130326110956468 and No. JCYJ20120613102030248, Natural Science Foun-dation of Ningbo No. 2012A610029 and Department of Ed-ucation of Zhejiang Province(Y201120179).
