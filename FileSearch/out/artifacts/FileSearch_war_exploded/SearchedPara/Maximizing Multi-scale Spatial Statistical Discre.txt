 Detecting anomalous events from spatial data has important applications in real world. The spatial scan statistic meth-ods are popular in this area. With maximizing the spatial statistical discrepancy by comparing observed data with a given baseline data distribution, significant spatial overden-sity and underdensity can be detected. In reality, the spatial discrepancy is often irregularly shaped and has a structure of multiple spatial scales. However, a large-scale discrep-ancy pattern may not be significant when conducting fine granularity analysis. Meanwhile, local irregular boundaries of a maximized discrepancy cannot be well approximated with a coarse granularity analysis. Existing methods most-ly work either on a fixed granularity, or with a regularly shaped scanning window. Thus, they have difficulties in characterizing such flexible spatial discrepancies. To solve the problem, in this paper we propose a novel discrepan-cy maximization algorithm, RefineScan . A grid hierarchy encoding multi-scale information is employed, making the algorithm capable of maximizing spatial discrepancies with multi-scale structures and irregular shapes. Experiments on a wide range of datasets demonstrate the advantages of RefineScan over the state-of-the-art algorithms: It always finds the largest discrepancy scores and remarkably better characterizes multi-scale discrepancy boundaries. Theoreti-cal and empirical analyses also show that RefineScan has a moderate computational complexity and a good scalability. H.2.8 [ Database Management ]: Database Applications X  Data Mining Anomalous event detection; spatial scan statistic; multi-scale statistical discrepancy
Anomalous event detection is an important problem in data mining. It has numerous applications in environmen-tal monitoring, epidemic surveillance, criminology research, etc. The spatial scan statistic [9] is one of the most popular methods in detecting anomalous events from spatial dataset-s, where the events are geo-referenced. In such a context, the anomaly is defined as the unusually high (or low) density of events in a geographically bounded region that is unlikely to have occurred only by chance . To quantify such anomalies, a statistical discrepancy score between observed data and a given baseline distribution is evaluated and maximized. A p-value that reflects the unusualness of the maximized dis-crepancy is also computed by hypothesis testing. Such an analysis is also referred to as overdensity / underdensity de-tection [1, 5] or cluster detection [7, 12] where the region maximizing the discrepancy score is called a cluster 1 . Take the epidemic surveillance scenario as an example, the people infected by an epidemic disease in a city are observed data, which are called cases , and the people not infected are called controls . The summation of cases and controls is called pop-ulation , which defines the baseline distribution. Given the locations of each data sample, maximizing the spatial statis-tical discrepancy between the cases and the population can identify city regions with elevated disease risk (overdensity). Suppose there is a high density of population in the city cen-ter, which naturally leads to a high density of disease cases observed there if the probability of getting infected is uni-form throughout the population. However, such a clustering of cases caused by the inhomogeneous baseline distribution is not anomalous. Only the extraordinary clustering effect that exceeds the interpretability of the baseline is anoma-lous, which implies a higher probability of getting infected than expected. The discrepancy maximization is well suited for such anomaly detection applications.

By searching over the geographical space, regions maxi-mizing a discrepancy function (i.e., clusters) can be identi-fied. In reality, the discrepancy can often be highly flexible in its geospatial structure. On one hand, it may embed com-
The difference between the cluster concept here and the one in the classical clustering problem is significant. In the classical clustering problem, the focus is the overall data clustering effect instead of the statistical discrepancy de-fined between observations and a baseline. Therefore, in the classical clustering framework, the statistical significance is usually not considered, and detecting underdensity is impos-sible. More discussions can also be found in [1, 5]. Figure 1: A sample dataset and overdensity regions (significant at 0.05 level) detected by GridScan [5] plex multi-scale structures that cannot be well described at one spatial scale only. Especially, maximized discrepancies obtained at different scales can be far different, which we will discuss later. On the other hand, the shape of the clus-ter can be irregular at different scales. We refer to it as the multi-scale spatial discrepancy , which is ubiquitous in spatial data. Figure 1(a) shows a typical example: a uniform distri-bution of controls is overlaid with four overdensity regions of cases that are of varied scales. The capability of modeling such spatial discrepancies can be critical, since a more accu-rate characterization of anomalies undoubtedly helps achieve stronger situational awareness in real-world applications [17, 10, 2, 8, 5]. However, existing scan statistic based algorithm-s mostly work on simplified data assumptions and often fail when dealing with multi-scale spatial discrepancies. Mainly due to computational reasons, earlier proposed algorithm-s assume that the clusters have regular shapes, e.g., circle [9], ellipse [10], and rectangle [12, 1]. Although the detected clusters can be of varied scales, these algorithms easily fail to detect irregularly shaped clusters, making their applica-tions limited in practice. Recently proposed algorithms can detect irregularly shaped clusters [17, 2, 8, 4, 5], but most of which, if not all, work at a fixed spatial scale. Their common assumption is that the input data are geographically aggre-gated by areas with fixed boundaries. Sizes and shapes of the areas are predefined by either administrative regions or regular grids. Because the spatial granularity is fixed, the spatial discrepancies at finer or coarser granularities cannot be addressed, which is well-known in geographical analysis as the modifiable areal unit problem (MAUP) [15]. In oth-er words, these algorithms solve the single-scale discrepancy maximization problem. Figure 1 demonstrates examples of applying a typical grid-based algorithm, GridScan [5], with two different grid sizes on the sample dataset. We can see that when using 16  X  16 grids (relatively large grid units), a large-scale cluster significant at 0.05 level is detected, and it connects all the local overdensity regions. But apparent-ly, the grids are too coarse to approximate the local shapes. When using 64  X  64 grids (relatively small grid units), the fine-grained overdensity is better characterized but the glob-al shape also becomes fragmented. In addition, because the cluster locating in the upper right of Figure 1(a) disconnects with the others at the given scale, its discrepancy score is too small to be identified as significant (at 0.05 level). The limitation of analyzing a single scale is evident.

Nonetheless, determining an optimal spatial scale for a ge-ographical analysis is difficult in general. In fact, according to the MAUP, the possibly different (and sometimes even contradictory) spatial analysis results obtained at different scales are all correct [15]. Thus, the superiority of any single scale over the others is usually hard to measure. However, specifically in the cluster detection problem, the maximized discrepancy scores obtained at different scales are numeri-cally comparable. We argue that rather than striving to find an  X  X ptimal X  scale, a better strategy dealing with multi-scale discrepancies can be: combining the information gathered from different scales to search for the region globally max-imizing the discrepancy score. To the best of our knowl-edge, no algorithm so far can incorporate multi-scale in-formation for spatial discrepancy maximization. Compared with the single-scale analyses, there are three technical chal-lenges in maximizing a multi-scale spatial discrepancy: (1) Determining boundaries of the discrepancy is much harder than working on a fixed areal aggregation. Because there is no available reference boundary at larger or smaller s-cales, the search space is tremendously enlarged. (2) Due to the MAUP, cluster detection results can be far differen-t at different scales when the discrepancy indeed embeds multi-scale structures. A reasonable way of combining such information has to be developed. (3) Algorithms meeting the two aforementioned challenges will inevitably introduce additional computations. The overall computational com-plexity must be acceptable in practice.

To meet these challenges, in this paper, we propose a nov-el spatial discrepancy maximization algorithm, RefineScan . The main idea is as follows. (1) On top of a base level spatial aggregation (by regular grids in this paper) that rep-resents the finest granularity in which the user is interested, a multi-level grid hierarchy is built for information diffusion across multiple scales. (2) On a higher level of the hier-archy, the grid unit size becomes larger. Region search is initially done on the top level grids to locate large-scale, yet coarse, clusters. (3) The coarse clusters are then iter-atively refined on lower levels until reaching the base level, so that the useful information for maximizing the discrep-ancy is kept and passed on to lower levels. An additional technical challenge is that, due to the mathematical charac-teristics of the employed discrepancy function (i.e., the scan statistic likelihood), there is theoretical possibility of cluster merging and splitting during refinement. In-depth analyses are given from this perspective, and efficient computational methods are developed accordingly. By doing so, the cluster-s detected by RefineScan not only preserve the large scale spatial connectedness that may help globally maximize the discrepancy, but also accurately characterize small scale ir-regular shapes. Experiments will show that compared with existing algorithms, RefineScan performs the best in max-imizing multi-scale spatial discrepancy and characterizing the flexible clusters given ground truths. The computation-al complexity of RefineScan is also moderate: O ( n + N 3 given N  X  N base level grids and a dataset of size n .
The remainder of the paper is organized as follows. Sec-tion 2 details RefineScan and analyzes its computational complexity. Section 3 presents experimental studies. Sec-tion 4 reviews related work. Section 5 concludes the paper.
Table 1 summarizes the notations used in this paper. Kull-dorff X  X  scan statistic [9] is one of the most popular discrep-ancy measures developed in literature. Given a region Z , the likelihood function of the scan statistic, L ( Z ), is defined as the objective function of discrepancy maximization. T-wo widely used probabilistic models for defining L ( Z ) are Bernoulli and Poisson. Formally, when data are typical bi-nary events, the Bernoulli model is used. Then we have L ( Z ) = m Z When the number of cases in data is known as Poisson dis-tributed, the Poisson model is used, and where E ( m Z ) = m n  X  n Z is the expected number of cases in a region under the null hypothesis of no overdensity or un-derdensity. Also note that additional baseline information other than locations can be considered here. For instance, variables such as age and sex in the epidemic surveillance application mentioned in Section 1 can be treated as covari-ates , and E ( m Z ) becomes the covariate adjusted expectation [9, 11]. A cluster is found by arg max L ( Z ). Local optima of L ( Z ) also need to be found, because multiple local clus-ters can exist, which are necessary to be detected in prac-tice. When detecting overdensity, a constraint m Z n is applied, meaning only regions with more cases observed than expected are to be detected. Conversely, m Z n is applied for underdensity detection. Because the distribu-tion of L ( Z ) is unknown, Monte Carlo simulation is usually adopted to calculate the p-value when evaluating the signif-icance of a cluster [9]. Specifically, a number of replicated datasets are generated by randomly reassigning the case and control labels under the null hypothesis. On each replicated dataset, L ( Z ) is maximized, which refers to one simulation. Suppose a cluster Z 0 has a discrepancy score L ( Z 0 its p-value is p = r +1 K +1 , where K is the number of simula-tions, and r is the number of discrepancy scores obtained from the simulations with values no smaller than L ( Z 0 ). In implementations, log L ( Z ) is often used instead of L ( Z ), so we will use log L ( Z ) in algorithm descriptions hereafter. Al-gorithm 1 outlines the overall cluster detection framework. Line 1 is detecting all clusters by RefineScan . Lines 2 X 6 are Monte Carlo simulations. Lines 7 X 9 are computations of p-values. Function I (  X  ) on line 8 is the indicator function.
To handle the multi-scale spatial discrepancy, a grid hier-archy data structure is employed in RefineScan . Suppose N  X  N grids are applied to the data in D as the base level grids. A base level grid cell is either marked or unmarked, indicating its state of whether being a part of a cluster or not. We define that a higher level regular grid cell is the parent of four lower level grid cells, and a lower level cell Algorithm 1: ClusterDetection
Figure 2: An example of three-level grid hierarchy equals one quadrant of a parent cell. In this way, the base level N  X  N grids correspond to N 2  X  N 2 grids on a higher level, namely level one, and further build up N 2 2  X  N 2 2 on level two, and so on. On level l , there are N 2 l  X  N Figure 2 shows a simple example of a three-level grid hierar-chy. On a given level of the grid hierarchy, a cluster is a set of connected cells. In this paper, we consider the immedi-ate neighboring relationship defined by the 8-connectedness (i.e., the first-order Queen spatial contiguity). We regard cells inside a cluster as marked and the rest unmarked . Let U denote all the unmarked cells on a level. Get8NB ( c ) is a function returning all the 8-connected neighboring cells of cell c . The inner-neighbors and outer-neighbors of a cluster are defined as follows.

Definition 1 (Inner-Neighbors). Inner-neighbors ( inNB ) of a cluster Z are the marked cells of Z neighboring U : inNB = { c | c  X  Z, Get8NB ( c )  X  X  6 =  X  X  .

Definition 2 (Outer-Neighbors). Outer-neighbors ( ouNB ) of a cluster Z are the unmarked neighboring cells of Z : ouNB = { c | c  X  X  , Get8NB ( c )  X  Z 6 =  X  X  .
 We use GetInNB ( Z ) and GetOuNB ( Z ) to denote the functions returning all inner-and outer-neighbors of Z , respectively.
The flow of RefineScan is shown in Algorithm 2, which is a two-step procedure. After initializing the grid hierarchy (lines 1 X 3), in the first step (lines 4 X 8), the rough locations of candidate clusters at the largest available scale are identi-fied by region-growing on the top level grids. The procedure of locating a large-scale cluster is LocateCluster (Algorith-m 3). In the second step (lines 9 X 15), each of the large-scale and coarse clusters is refined on lower level grids, such that the spatial structure at a finer level is exploited. The refine-ment is done on each level of the hierarchy in a top-down manner. Before refining on a lower level, all clusters and U are reconstructed by decomposing every component grid u-nit into its four children grids on the lower level. Then, each cluster is iteratively refined by either removing cells from the inner-neighbors, or growing cells from the outer-neighbors. Once the refinement of all clusters finishes on one level, the same operations are repeated on the next available lower lev-el. Apparently, the granularity of a cluster becomes finer as Algorithm 2: RefineScan the level goes down. Meanwhile, because the principle of re-finement is to monotonically enlarge the discrepancy scores, the grids X  spatial connectedness at larger scales is retained also at smaller scales as long as it is helpful for enlarging the score. It can be seen as information diffusions from higher to lower grid levels. In this way, the information of different scales is combined for cluster detection. The procedure of refining a cluster is RefineCluster (Algorithm 5).
The objective here is to roughly and also quickly locate a cluster on the top level grids. It can be inferred that an exhaustive search needs to test 2 |C ( l max ) | alternatives to find the optimal solution, which is computa-tionally inefficient. Therefore, here we adopt a greedy yet effective cluster growing strategy proposed in GridScan [5], which only has a linear time complexity to |C ( l max ) | . Briefly, it first finds a seed cell for a cluster, and then locally grows the cluster by its outer-neighbors. The seed is chosen from a candidate seed set ( candSeedSet ) by maximizing log L ( { c } ). When growing a cluster, only one cell chosen from the outer-neighbors is included (and marked) each time. The cell is found by maximizing a discrepancy gain function G g ( Z,c ).
We first define a function S g ( Z,c ) before formulating G The aforementioned cluster growing is greedy in nature. Therefore, it is possible that a growing cluster Z can ap-proach one or more (at most three under 8-connectedness) existing clusters that have stopped growing in previous it-erations. Let mergClusters = { Z 0 | c  X  GetOuNB ( Z 0 ) ,Z clusters } denote such clusters, where set clusters denotes existing clusters. If there exists a cell c  X  GetOuNB ( Z )  X  GetOuNB ( Z 0 ), then marking c will connect Z and Z 0 . In this case, the supposed discrepancy score of Z by growing c is: It is known that S g ( Z,c ) is not surely greater than both log L ( Z ) and log L ( Z 0 ) [5]. If  X  Z 0  X  mergClusters , s.t. S log L ( Z 0 ), c will not be chosen for growing. This is to re-strict the discrepancy score to monotonically increase. It can be inferred that merging Z with Z 0 by c to obtain a larger size cluster Z 00 = Z  X  { c }  X  Z 0 requires the fol-Algorithm 3: LocateCluster Algorithm 4: AboveMinP lowing two conditions: (1) L ( Z 0 ) &gt; L ( Z 0  X  { c } ), indicat-ing that Z 0 did not grow by c in earlier search, and (2) L ( Z 00 ) &gt; L ( Z 0 ) V L ( Z 00 ) &gt; L ( Z ), indicating that Z a larger discrepancy score than both Z and Z 0 . It is easy to prove that this never happens when L ( Z ) is monoton-ic with m Z n conclusion also holds with reversing the directions of the inequations. However, because L ( Z ) is neither monotonic The discrepancy gain function of growing a cell, G g ( Z,c ), is: When choosing the seed or cell to grow, an additional con-dition is AboveMinP (Algorithm 4). It is to restrict extra large size of a cluster and make the cluster shape compact. A parameter minP is applied to limit the inclusion of cells that contain too less points. Empirical studies have shown that a default minP =1 works well enough in many cases [5].
Given that the clusters and U are decomposed to the cur-rent level grids, refining a cluster Z is also iterative and incremental. In each step, two operations are evaluated: (1) removing a cell from Z  X  X  inner-neighbors, or (2) growing a cell from Z  X  X  outer-neighbors. The one better enlarges Z  X  X  discrepancy score will be applied. Because the discrepancy Algorithm 5: RefineCluster score is monotonically increased, if a large-scale cluster has been roughly located on a higher level, its component grids X  spatial connectedness can still be preserved on lower levels as long as its discrepancy score keeps increasing.

Removing a cell from inner-neighbors is implemented on lines 9 X 18. This may cause a cluster to split into multiple (at most four under 8-connectedness) pieces. Let spltClusters = { Z 0 | T Z 0 =  X  ,Z = S Z 0  X  X  c }} denote the new clusters that will split out of Z once removing c . Z will then become Z Z \{ c }\ spltClusters . The supposed maximized discrepancy score generated by removing c from Z is: Similar to the analysis in Section 2.1.1, S r ( Z,c ) is not always greater than log L ( Z ) or log L ( Z 0 ), where Z 0  X  spltClusters  X  { Z 00 } . Therefore, cluster splitting is probable as well. When judging if removing c can result in cluster splitting, just con-sidering c and its 8-neighbor is necessary but not sufficient. There are 2 8  X  1 = 255 possible situations of the 8-neighbor. The excluded situation refers to that the 8-neighbor cells are all unmarked, which never happens. It can be proved by enumeration that, in 132 out of the 255 situations, s-pltClusters is guaranteed to be empty, which simplifies the computation of (5). Still, there are 123 situations that may cause cluster splitting. Figure 3 illustrates two such typical situations. Nevertheless, such situations do not certainly lead to cluster splitting, because the split parts observed in the 8-neighbor area may still be connected from outside. In this case, finding all the connected components of Z \{ c } is Figure 3: Two typical situations of 8-neighboring cells that may cause cluster splitting if removing the cell in the center (in gray). Dark cells are marked. White cells are unmarked. needed to obtain spltClusters . We restrict that if the clus-ter splitting does not result in at least one new cluster with discrepancy score greater than the original log L ( Z ), the cell will not be removed. The discrepancy gain function of re-moving operation, G r ( Z,c ), is thus defined as: An inner-neighbor maximizing G r ( Z,c ) will be considered to remove from Z . The inner-and outer-neighbors are to be updated incrementally (lines 16 X 18) after the removal.
As an alternative to removing an inner-neighbor, grow-ing an outer-neighbor is also evaluated. Key steps of such a cluster growing (lines 19 X 32) are similar to LocateCluster (Algorithm 3). Once a cell maximizing G g ( Z,c ) is deter-mined to be included in Z , again, probable cluster merging is handled. The inner-and outer-neighbors are also to be updated (lines 23, 24, 27 X 32). Different from LocateClus-ter where inner-neighbors are not considered, a special case of updating inner-neighbors needs to be handled (line 29): Suppose within a cluster, there is a hole with a size of one cell, which must be an outer-neighbor 2 . If this hole is cho-sen for growing, then its 8-neighbor should be removed from inner-neighbors and no insertion is performed, which is d-ifferent from the normal cases (line 31). By comparing the maximized G r ( Z,c ) and G g ( Z,c ), either removing or grow-ing that better improves the discrepancy score is applied at a time. The refinement stops if there is no further improve-ment on the current level (line 8).
We consider the time complexity of detecting one cluster in RefineScan (Algorithm 2). Data aggregation (lines 1 X  2) costs O ( n + N 2 ). Building the grid hierarchy (line 3) The dominant computation on lines 4 X 8 is LocateCluster (Algorithm 3). The complexity of growing one cluster is of the same order of the cluster growing in GridScan given
In the refinement step (lines 9 X 15), the primary com-putations include line 11 that costs O (( N fineCluster (Algorithm 5). Inside RefineCluster , the pri-mary computations are lines 4 X 5. All the rest set operations can have efficient implementation so that their cost is domi-nated. We can infer that the size of inner-or outer-neighbors approximately equals the length of a cluster X  X  perimeter P . On level l , in extreme cases, P l = O (( N 2 l ) 2 ), whereas in most cases, P l = O ( N 2 l ). Calling GetInNB ( Z ) in RefineCluster (line 1) at the first time after cluster growing on the top
It can be proved that such a hole cannot be generated in upper levels since the unit cell size of upper levels must be larger. Therefore, it can only be generated on the current level. A theoretically possible cause can be that, a hole with area larger than a cell is generated by cluster merging, and it is filled by growing with one cell left unmarked. level needs to traverse all outer-neighbors. But afterward-s, if all inner-and outer-neighbors are always kept together with Z , GetInNB ( Z ) only costs O (1). GetOuNB ( Z ) on line 2 always costs O (1) because outer-neighbors are already maintained since LocateCluster . Finding c in (line 4) costs cluster Z (also is the complexity of finding connected com-ponents), and p is the probability of observing the 123 situa-tions that may cause cluster splitting (see Section 2.1.2). We have | spltClusters | X  3 and in most cases it is equal to 0. Be-sides, empirical results show that usually p &lt; 123 255 in normal cases, O (( N 2 l ) 3 ) can be a reasonable cost approxi-mation for line 4, if assuming | Z | is of order O (( N 2 l ilarly, finding c out (line 5) costs O ( P l ) = O ( N 2 l RefineCluster has a complexity of O (( N 2 l ) 3 ). The cost of re-finement on all levels is thus O ( P l max l =0 (( N
Overall, the time complexity of RefineScan is O ( n + N 2 N 3 ) = O ( n + N 3 ). As can be seen, RefineScan can handle very large datasets without difficulty, because the complex-ity is linear to dataset size n with fixed N . Note that N may not be very large in practice (e.g., N =1K is very large already). If n N , the complexity approaches O ( n ).
Now let us compare a naive algorithm that maximizes the multi-scale discrepancy directly on the base level grids by ex-haustive search. Given N  X  N grids, there are 2 ( N 2 ) possible ways of marking them. The algorithm will cost O ( n + 2 ( N to find the global optimum, which is unacceptable in prac-tice. Although RefineScan employs a greedy search that does not guarantee the global optimality, as a trade-off, it-s computational cost is significantly lower. Experiments in the next section will show that RefineScan can have remark-able performance when compared with the state-of-the-art discrepancy maximization algorithms.
In this section, we will experimentally evaluate the perfor-mance of RefineScan using a variety of public datasets. We will also study the impacts of parameters on RefineScan as well as the algorithm X  X  scalability.
Eight overdensity and underdensity detection tasks are conducted on six datasets (see Table 2). More or less, these datasets all contain multi-scale spatial discrepancies, which will be clearly seen later. These datasets were also used as benchmarks in previous studies [3, 4, 5]. All the tasks are overdensity detections except for task 3. Dataset1 and Dataset2 are purely spatial datasets.  X  Emerging  X ,  X  expand-ing  X , and  X  moving  X  are spatio-temporal datasets. We adopt certain data chunks of two weeks from the original datasets, with data collected in the earlier week treated as controls and data collected in the latter week treated as cases. Epi-demic is also a spatio-temporal dataset, and each record is a microblog with time stamp and GPS location. The orig-inal dataset containing totally 1,023,077 microblogs is from the IEEE VAST Challenge 2011 Mini-Challenge 1 3 . Fever and Diarrhea are its subsets, corresponding to two typical subgroups of microblogs that are classified by epidemic key-words fever and diarrhea. In the data chunk of May19day, http://hcil.cs.umd.edu/localphp/hcil/vast11 microblogs containing the certain keywords are treated as cases, and the rest are treated as controls. All the data preparations (including how the data are grouped and la-beled) are exactly the same as in the literature. More details of these datasets, the ground truths, and the pre-processing steps can be found in [3, 5].

Based on the common discrepancy function, log L ( Z ), we compare RefineScan with a typical grid-based algorithm, GridScan [5], Kulldorff X  X  spatial scan statistic [9] using cir-cular and elliptic windows ( SaTScan software [11] as imple-mentation), and Neill X  X  scan statistic [12] using rectangular window on regular grids ( city4 applic software [13] as imple-mentation). In Neill X  X  approach, only overdensity detection with Poisson model is implemented. On Dataset2 and Epi-demic , Poisson model is used for all algorithms. On the other datasets, Bernoulli model is used for all algorithms except Neill X  X  approach. Default minP =1 is used in Re-fineScan . Unless explicitly mentioned, parameters of the candidate algorithms are set to their default values. We use K = 99 Monte Carlo simulations and report clusters signifi-cant at 0.05 level. All experiments are done on a 64-bit Win 7 machine with Intel Core 2 2.66GHz CPU plus 3G memory.
We will first evaluate the visualization results of detected clusters for each task. This verifies the necessity of char-acterizing multi-scale spatial discrepancy. Then, we will e-valuate the numerical results by comparing the maximized discrepancy scores and analyzing the CPU time cost of Re-fineScan . At last, we will analyze the impact of parameters on RefineScan , and study the algorithm X  X  scalability.
On task 1, we use N =64, l max =2 in RefineScan . The grid hierarchy is thus a 3-level structure with 16  X  16 top level grids. As a fair comparison, we also test N =16 and 64, respectively, for both GridScan and Neill X  X  approach. Data distribution of Dataset1 is shown in Figure 1(a). As an il-lustration of the execution process of RefineScan , Figure 4 shows the snapshots of the cluster X  X  shape outputted by Re-fineScan (highlighted grids in the figure) on each level of the grid hierarchy. We can see that at first, RefineScan locates all the overdensity regions on the top level (level 2) with large connected grids, and then refines the overal-l shape with smaller grid units on the lower levels (levels 1 and 0) while keeping the connectedness on larger scales. The top-down procedure of cluster refinement can be clearly seen. The comparison of all candidate algorithms X  results on task 1 are summarized in Figure 5. The results of RefineS-can and GridScan are shown by highlighted grids, SaTScan results are shown by circles and ellipses, and the results of Neill X  X  approach are shown by rectangles in dashed blue line. We can see that RefineScan better characterizes the cluster shape than the others. GridScan cannot well approximate the cluster boundary with large grids although the correct regions are identified. Also, it cannot detect significant over-density at larger scales with too small grids: Some regions (b) Refinement on level 1 (c) Refinement on level 0 Figure 4: Illustration of RefineScan execution on task 1. Clusters are shown with base level grids. The control data are not displayed for clarity. with dense cases easily regarded insignificant. It is note-worthy that, although the finest grids used in RefineScan are the same as in GridScan (64  X  64), their results are far different. The effectiveness of RefineScan in terms of han-dling multi-scale spatial discrepancy is clearly demonstrated. Neither SaTScan nor Neill X  X  approach can work well: Due to the limitation of the regular scanning windows, they cannot recognize the true cluster shape and often wrongly detect a large connected region as multiple (and overlapping) ones. On tasks 2 and 3, we use N =32, l max =2 in RefineScan . Therefore, the grid hierarchy is 3-level with 8  X  8 grids on top. We also test N =8 and 32 for GridScan and Neill X  X  approach. The dataset and cluster results are shown in Figure 6. We can see that the comparisons generate similar conclusions to task 1. RefineScan better characterizes the cluster X  X  overall shape. In contrast, other algorithms either split a connect-ed region into separate ones or report unnecessarily large areas. Still, with the same finest grids (32  X  32), RefineS-can behaves differently from GridScan. The advantage of RefineScan will be further illustrated when examining the maximized discrepancy scores in Section 3.3 (Table 3).
On tasks 4 X 6, we use N =40, l max =1 in RefineScan , mean-ing a grid hierarchy of 2 levels with 20  X  20 grids on top is em-ployed. Results of the other candidates are adopted from [5] Figure 6: Dataset2 and results. Task 2 results are shown in (b)(c)(d) and task 3 results in (e)(f )(g). for comparison. GridScan and Neill X  X  approach use N =20. The results are shown in Figure 7. The data in these datasets are not as dense as in Dataset1 and Dataset2, which makes it difficult to capture the patterns with small grids. Moreover, 20  X  20 is an appropriate setting considering the scales in which the true clusters are generated [3]. The true clusters in the datasets are a circle, a large-scale ring, and an L-shaped area, respectively. Again, we can find that RefineScan cap-tures the correct spatial discrepancy structures, improving the results of GridScan. More detailed numerical improve-ments can be found in Section 3.3 (Table 3). On the other hand, SaTScan and Neill X  X  approach do not work well, even when facing the circular true cluster (see Figure 7(d)). On tasks 7 and 8, we only compare RefineScan with Grid-Scan since the true clusters are notably irregular. 30  X  76 base grids and l max =1 are used in RefineScan . The up-per level grids are thus 15  X  38. In GridScan, 15  X  38 and 30  X  76 grids are used, respectively. Results are visualized in Figure 8. We can see that RefineScan again works better: One cluster for fever and one cluster for diarrhea are detect-ed, which is consistent with the ground truth (c.f. [5]), and the shapes are correctly characterized with plenty of detail-s. GridScan can have satisfactory results with coarse grids. But if using fine grids, the diarrhea cluster breaks into four pieces, and the fever cluster breaks into two parts with much smaller sizes. This again indicates that as a single-scale algo-rithm, GridScan can be sensitive to the grid setting, whereas RefineScan is more robust due to combining and utilizing the information from multiple spatial scales.
The maximized discrepancy scores obtained by each can-didate algorithm on the eight tasks are reported in Table 3. We can find that RefineScan performs the best throughout the experiments. It always finds the largest discrepancy s-core, indicating its remarkable performance in maximizing multi-scale spatial discrepancy from a variety of dataset-s even in presence of irregularly shaped clusters. Since the search strategies of the candidate algorithms differ with each other and are implemented in different languages and pro-gramming models, it is not meaningful to directly compare Table 3: Comparisons of maximized discrepancy s-cores. RefineScan performs the best (bolded). For GridScan and Neill X  X  approach, when two grid sizes are used for a task, the results obtained by coarse grids (smaller N ) are shown in the upper row. their computational time. Therefore, here we only report the CPU time of RefineScan on each task (one run of Algorith-m 2 on the original data without Monte Carlo simulations and single threaded C++ implementation) in Figure 9. We can find that generally, larger dataset size ( n ), the number of base grids ( N ), l max and true cluster size (i.e., the number of marked grids), and a more irregularly shaped multi-scale discrepancy, will result in more CPU time. Overall, the computational cost of RefineScan is moderate.
To analyze the impacts of the primary parameters N and l max on RefineScan , and further verify the algorithm X  X  s-
Figure 9: CPU time of RefineScan on each task calability, we conduct additional experiments on a simulat-ed dataset, D1K [5] that contains 10 3 cases and 10 trols for Bernoulli model based overdensity detection. It is noteworthy that although the reported result here is on a specific dataset, the conclusion coincides with the theo-retical analysis in Section 2.2, and can be generalized to any datasets. We first test the performance of RefineS-can using feasible combinations of N  X  { 8 , 16 , 32 , 64 , 128 } and l max  X  X  0 , 1 , 2 , 3 , 4 , 5 , 6 } . Note that l max log 2 N , and in practice at most log 2 N  X  1 since a search s-tarting with only one grid unit on the top level is meaningless in most cases. Therefore, only the combinations satisfying l max &lt; log 2 N are tested.

The results of maximized discrepancy scores are summa-rized in Table 4. In the table, the largest scores obtained are shown in bold. We can see that in general, when l max is sufficiently large, a larger N leads to a larger discrepancy score. This is natural since a finer base granularity helps better characterize clusters X  shape. Meanwhile, with a fixed N , l max is not necessarily to be as large as log obtain the maximized discrepancy score. This is also rea-sonable, because as long as the grid size on a level of the Table 4: Maximized discrepancy scores with varying l Table 5: CPU time cost (in second) with varying l grid hierarchy is sufficiently large to capture the global scale structure, an even larger grid size at a higher level does no more help. In contrast, if N is too small (e.g., N =8, the first column of Table 4), meaning the base level grids are still too coarse, the algorithm may become unstable due to missing details of the data distribution in the aggregation step. Be-sides, even with a sufficiently large N , if l max is too small (e.g., N = 128 and l max  X  X  0 , 1 , 2 } in Table 4), RefineScan cannot benefit much from the grid hierarchy due to miss-ing data distribution information at larger scales. Hence, RefineScan cannot work well either.
 The CPU time cost of RefineScan (still, one run without Monte Carlo simulations) is summarized in Table 5. Gen-erally, when the dataset size n is constant, larger N and larger l max values naturally result in increased CPU time. Section 2.2 showed that RefineScan has a computational complexity of O ( n + N 3 ), where the O ( n ) complexity is in-troduced only by parsing the dataset for one time aggrega-tion before detecting clusters. Therefore, as validation on the linearity with n is trivial, we omit it here. We only need to verify that with a constant n , the primary computation is linear to N 3 . Accordingly, we choose the CPU time cost obtained by the largest l max for each N (values are bold-ed in Table 5) and study the relationship between the real CPU time and N . Figure 10 shows the samples and a fitted curve from the samples. The curve is obtained by regress-ing the CPU time with variable N 3 using a linear model y = aN 3 + b . When plotting the linear model against N , it becomes a cubic curve. The obtained linear equation is y = 0 . 2872 N 3  X  2376 . 7 with R square 1.0, indicating a per-fect linearity between the CPU time and N 3 .

The results show the performance of RefineScan with changing parameters and verify the theoretical analyses in earlier sections. The guidelines of setting the parameters of RefineScan when applying it to new real-world problem-s can also be given: N should be set as large as possible if more detailed discrepancy structure is of great interest. However, this may lead to a larger computational cost (lin-ear to N 3 given a dataset), and thus a trade-off should be considered in practice. The grid hierarchy plays a key role in successfully characterizing multi-scale discrepancies, while a large enough l max (but may not necessarily close to log 2 depending on the data distribution) is also helpful.
Previous studies on spatial discrepancy maximization for overdensity/underdensity detection either only consider reg-ularly shaped clusters or just focus on a fixed spatial scale. Typical algorithms addressing regular shapes include the s-patial scan statistics methods using circular and elliptic s-canning windows proposed by Kulldorff et al. [9, 10] and the rectangular window based scan statistics proposed by Neill et al. [12, 14] and by Agarwal et al. [1]. Given a dataset with size n , the computational complexity of cluster detec-tion (with excluding data preparation) of Kulldorff X  X  scan statistic methods is at least O ( n 2 ) when running without aggregation and becomes O ( N 2 ) when data are aggregated to N  X  N grids [9, 10]. It can be inferred that a naive search for rectangular regions costs O ( N 4 ) given N  X  N grids. Neill et al. reduces the complexity to O (( N log N ) 2 ) [12]. Agarw-al et al X  X  -approximation algorithm costs O ( 1 N 3 log N ) [1]. Compared with these algorithms, the proposed RefineS-can in this paper is much more flexible and more powerful, and its computational complexity O ( N 3 ) is quite acceptable. There are also algorithms that can detect irregularly shaped clusters, but most of which work on a pre-defined spatial scale. Typical algorithms in this category include GridScan proposed by Dong et al. [5], the random-work based FS proposed by Janeja and Atluri [8], AMOEBA proposed by Aldstadt and Getis [2], a flexible scan statistic proposed by Tango and Takahashi [17], etc. As analyzed in previous sec-tions, because there is no explicit consideration on the multi-scale spatial structure embedded in data, these algorithms cannot well handle multi-scale spatial discrepancy. More-over, their performance can be sensitive to the aggregation. Many of these algorithms employ exhaustive search, making their scalability poor. The lowest computational complexity by far is O ( N 2 ) of GridScan [5], which is linear to the number of grids ( N  X  N ) or say aggregation areas. By comparison, we can see that RefineScan does not introduce heavy addi-tional computations considering the significant performance improvement over these single-scale algorithms.

The traditional clustering algorithms in data mining can also be applied to spatial datasets to discover clustering pat-terns. For instance, DBSCAN proposed by Ester et al. [6] is capable of considering multi-scale information within the classical clustering framework. There are also grid-based algorithms, such as STING proposed by Wang et al. [18] and WaveCluster proposed by Sheikholeslami et al. [16]. A grid hierarchy is also employed in STING to store the multi-scale statistical information for spatial query speed-up. Nonetheless, as already discussed in Section 1, due to the intrinsic difference between the classical clustering prob-lem and the overdensity/underdensity detection problem on which this paper focuses, these algorithms do not work on discrepancy maximization. Not to mention that RefineScan utilizes the grid hierarchy in a different way: The top-down manner of the cluster refinement in RefineScan essentially implements the information diffusion across multiple spatial scales, which is totally different from the speed-up purpose motivating the grid hierarchy in STING.
In this paper, we propose a novel algorithm RefineScan for multi-scale spatial statistical discrepancy maximization. By employing a multi-level grid hierarchy with efficient clus-ter growing and refinement methods, RefineScan is good at maximizing flexible spatial discrepancies even when facing multi-scale and irregularly shaped discrepancy structures. Experiments show that RefineScan outperforms the state-of-the-art algorithms by (1) finding the largest discrepan-cy scores throughout the experiments, and (2) better char-acterizing the true shapes of overdensity and underdensity. The impacts of parameters on RefineScan are also studied. Guidelines of setting the parameters are given accordingly. Given N  X  N base level grids and a dataset of size n , the com-putational complexity of RefineScan is O ( n + N 3 ), where N can be bounded to a small number in practice. It makes RefineScan a scalable and effective algorithm for detecting spatial anomalous events from large datasets. [1] D. Agarwal, A. McGregor, J. Phillips, [2] J. Aldstadt and A. Getis. Using AMOEBA to create a [3] W. Chang, D. Zeng, and H. Chen. A stack-based [4] W. Dong, X. Zhang, Z. Jiang, W. Sun, L. Xie, and [5] W. Dong, X. Zhang, L. Li, C. Sun, L. Shi, and [6] M. Ester, H. Kriegel, J. Sander, and X. Xu. A [7] V. Iyengar. On detecting space-time clusters. In [8] V. Janeja and V. Atluri. Random walks to identify [9] M. Kulldorff. A spatial scan statistic. Comm. Statist. [10] M. Kulldorff, L. Huang, L. Pickle, and L. Duczmal. [11] M. Kulldorff and Information Management Services, [12] D. Neill and A. Moore. Rapid detection of significant [13] D. Neill, A. Moore, K. Daniel, and R. Sabhnani. [14] D. Neill, A. Moore, M. Sabhnani, and K. Daniel. [15] S. Openshaw. The modifiable areal unit problem . Geo [16] G. Sheikholeslami, S. Chatterjee, and A. Zhang. [17] T. Tango and K. Takahashi. A flexibly shaped spatial [18] W. Wang, J. Yang, and R. Muntz. STING: A
