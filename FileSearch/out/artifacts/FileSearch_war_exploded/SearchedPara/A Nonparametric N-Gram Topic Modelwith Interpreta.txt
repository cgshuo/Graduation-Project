 Nonparametric topic models such as Hierarchical Dirichlet Processes ( HDP )[28],when viewed as an infinite-dimensional extension to the fixed-dimension Latent Dirichlet Allocation ( LDA ) model [4] and [3], have gained immense popularity in recent years because in that one does not need to explicitly provide the number of topics apriori. However, a limitation of the HDP model is that it loses important structural information present in the text leading to undesirable effects such as producing ambiguous terms in topics. For example, due to its bag-of-words assumption, HDP discovers unigrams such as  X  X etworks X  in a topic which does not seem to be that insightful. Instead finding n-gram words can convey more interpretabl e meaning to readers [31] and [23], for ex-ample,  X  X eural networks X . Also, word order is important to many aspects of linguistic processing [26] and [21]. Related works in parametric topic modeling, such as the bi-gram topic model ( BTM ) [30], the LDA Collocation model [18] ( LDACOL ), the topical n-gram model [31] ( TNG ), which maintain the order of the words in the document have shown to perform better than the ba g-of-words counterpart models.

Processing documents by keeping the word ordering intact, such as the existing para-metric topic models mentioned earlier, does i ncorporate additional computational bur-den, nonetheless it gives an upper-hand over tr aditional bag-of-words topic models [19]. One useful advantage is to discover more interpretable latent topics. However, one com-mon limitation of these existing parametric n-gram topic models which consider word order is that they require the number of topics to be supplied by the user in advance. In reality a user is completely ignorant about th e number of topics that may uncover the true latent structure of the corpus. It is therefore more reasonable to develop a nonpara-metric model which can automatically infer a desirable number of latent topics via the data characteristics inherent in a collection of text documents.

We develop a new nonparametric topic model, which we name as NHDP ,byex-tending the HDP model so that word order is taken into consideration during the topic discovery process. Our proposed NHDP model not only maintains the document X  X  word order information, but also discovers topical n-gram words based on context. By gen-erating n-gram in topics helps in better topi c interpretation because n-gram words are more insightful to the reader than unigram words [23]. Considering the importance of the word order in nonparametric setting is becoming to attract attention. For example, Goldwater et al. [15] presented two nonparametric word segmentation models where one of the models, called the bigram HDP model, maintains the ordering in text. Related extensions are described in [5] and [14]. They are well catered to the word segmentation task. In their model, contextual dependencies are distributed according to a Dirichlet Process (DP) speci fic to the words in a document and it closely resembles the hierarchical Pitman-Yor processes model [27] and [16]. In [11], the author introduced a nonparametric model that can extract phrasal terms based on the mutual rank relation. This model first extracts phrases and subsequently ranks them. It employs a heuristic measure for the identification of phrasal terms. In [25], the authors introduced the notion of extension pattern, which is a formalization of the idea of extending lexical association measures defined for bigrams. In [33], the authors presented a Bayesian nonparametric model for symbolic chord sequences. Their model is designed to handle n-grams in chord sequences for music information retrieval. Our proposed model is significantly different than the ones mentioned above. First, our model is an n-gram nonparametric topic disco very model capturing word dependencies in text. Consequently, it can generate more interpretable topics.

Some nonparametric language models have been proposed recently which maintain the word order, for example, [27], [32], etc. But there are differences between language models and topic models [29]. For example, language models do not discover topics, which typically is a probability distributio n over words. Also, language models focus on representing local linguistic structure, as expressed by word order [8] whereas topic models focus on finding topics.

Parametric and nonparametric syntax base d models also capture word dependencies using an extra layer of Hidden Markov Model ( HMM ). But they are different from our model in that we do not incorporate a HMM model in our NHDP model to capture word dependencies. For example, in [17] the authors introduced a parametric Bayesian topic model which can not only capture the semantic information inherent in the text, but also capture the syntax in the document by introducing an extra layer of HMM in the model. This model was later extended to a nonparametric setting [10] and [6] where the author introduced HDP model instead of its parametric counterpart, Latent Dirich-let Allocation ( LDA ) [4]. In [13], the authors presented the sticky HDP-HMM model for speaker diarization. Their model segments a piece of audio discourse using an aug-mented HDP-HMM that provides effective control over the switching rate in the audio data. The existing HMM based topic models are designed to capture the syntactic classes such as part-of-speech. In contrast, our mode l does not assume that syntax information is available.

Some existing parametric topic models discover n-gram words. But these models assume that the number of topics is known in advance. We believe that this is a ma-jor shortcoming because the desirable numbe r of topics that describes the collection is typically not known in advance. One approach to solving this problem is to train several models with different numbers of topics and choose the one that performs rea-sonably well according to a performance meas ure [9]. But this is cumbersome and time consuming [10]. Note that selecting less number of topics than what the data can actu-ally accommodate will result in under-fitting w hereas selecting more number of topics will result in over-fitting. The LDA model [4], which is a basic parametric topic model, assumes  X  X xchangeability X  [1] among the words in the document. In [18], the authors proposed an extension to the LDA model, called the LDA Collocation ( LDACOL ) model. This model introduces a set of random variables which capture whether words in order form collocations. Each word has a topic assignment and a collocation assignment. The collocation variable can take on two values, namely, 0 and 1 . If the collocation vari-able is 1 , then the word is generated from the distribution based on just the previous word. Otherwise, the word is generated from a distribution associated with its topic. In this way, the model can generate both unigram and bigram words. Wang et al. [31] extended the LDA Collocation model and proposed the topical n-gram ( TNG ) model which makes it possible to decide whether to form a bigram for the same two con-secutive words depending on their nearby context. However, this model suffers from some drawbacks such as words within a topical n-gram do not share the same topic. Moreover, the topic-specific bigram distributions share no probability mass between each other or with the unigram distributions . These shortcomings were addressed re-cently in another parametric topic model [23 ] based on the Hierarchical Pitman-Yor Processes [27]. However, their model becomes overly complex and it is inefficient for handling large datasets. Wallach in [30] proposed the bigram topic model which is an extension to the LDA and it maintains the word order in the document, but the model only generates bigrams in topics. In [20] Johnson described a connection between prob-abilistic context-free grammars PCFG and the LDA model. This paper shows how the LDA model can be expressed as a PCFG .The LDA model is employed to generate col-locations of words apart from applying the m odel in other natural language processing task. The difference between Johnson X  X  work in [20] and our paper is that we generate word collocations in a nonparametric setting whereas Johnson used the LDA model to generate word collocations. Recently, in [22] t he authors presented a study where they considered bigrams as a single token and used bigrams as features to be given to a topic model. The authors presented extensive experiments how collocations can help improve a topic model in empirical evaluations. Their method has a limitation in that one has to manually supply bigrams to the model rather t han the bigrams automatically discov-ered by the model itself. In [2] the authors presented an application of topic models to recommender systems. The authors presented topic models where the models maintain the ordering of the words in sequence and in turn obtain better empirical results in their experimental analysis. However, their model does not generate n-gram words based on the co-occurrences in the data. In order to circumvent the limitation prevalent in parametric topic models, Teh et al. [28] proposed the Hierarchical Dirichlet Processes ( HDP ) model. This model can be regarded as a nonparametric version of the LDA model [10]. We will mainly describe the HDP model in the context of topic modeling.

HDP is a nonparametric Bayesian model which is a Bayesian model on an  X  -dimensional parameter space. For nonparame tric models, the number of parameters grows with the sample size. Here we give a succinct description of the HDP model whose one of the applications is also topic modeling. Inquisitive readers are requested to consult [28] for more details.

Given a collection of text documents, HDP is characterized by a set of random prob-ability measures G d for each document d in the collection. In addition, a global random probability measure G 0 which itself is drawn from a Dirichlet Process (DP) with the base probability measure H . The global measure G 0 selects all the possible topics from the base measure H , and then each G d draws the topics necessary for the document d from G 0 . The model is defined as: where  X  and  X  are the concentration parameter s that govern the variability around G 0 and G d respectively. The base probability measure H provides the prior distribution for the factors or topics z di . Each z di is a factor corresponding to a single observation w di which is the word at the position i in the document d .

One perspective associated with the HDP mechanism can be expressed by the Chi-nese Restaurant Franchise (CRF) [28] which is an extension of the Chinese Restaurant Process (CRP). In order to describe sharing among the groups, the notion of  X  X ranchise X  has been introduced that serves the same set of dishes globally. When applied to text data, each restaurant corresponds to a document. Each customer corresponds to a word. Each dish corresponds to a topic. A customer sits at a table, one dish is ordered for that table and all subsequent customers who sit at that table share that dish. The dishes are sampled from the base distribution H which corresponds to discrete topic distributions. Multiple tables in multiple restaurants can serve the same dish. The factor values are shared both between and amongst documents. For a complete mathematical derivation of the CRF metaphor, we direct the reader to review [28]. 4.1 Model Description We describe our n-gram nonparametric topic model, called NHDP , which is an exten-sion to the basic HDP model described in Section 3. Unlike the basic HDP model, our proposed NHDP model is no longer invariant to the reshuffling of words in a document.
We introduce a set of binary random variables x which we term as the concatenation indicator variable that assume either of the two values which are 0 or 1. This variable indicates whether two words in consecutive order can be concatenated or not. Note that NHDP uses the first order Markov assumption on the words. There are two assignments per word w di at position i in the document d ,and 1  X  i  X  N d where N d is the number of words (unigrams) in the document d . One assignment is the topic and the other assignment is the concatenation indicator variable x di which relates to whether the word w di can be concatenated with the previous word w d,i  X  1 .If x di =1 ,then w di is part of a concatenation and the word is generated from a distribution that is dependent w di is generated from the distribution associated with its topic. We assume that the first indicator variable x d 1 in a document is observed and set to 1, and only a unigram is allowed at the beginning of the document. In fact, we can also enforce other constraints in the model. Some examples are: no concaten ation is allowed for sentence or paragraph boundary, only a unigram is allowed after a st opword is removed from that position, etc.
Note that NHDP can capture word dependencies in the document. The conditional probability P ( w di | w d,i  X  1 ) can be written as:
We can observe that P ( w di | w d,i  X  1 ,x di =0) can be computed using the basic HDP model. The full definition of our NHDP model is given as follows: 1 G 0 |  X ,H  X  DP (  X ,H ) ; 2 G d |  X , G 0  X  DP (  X , G 0 ) ; 5 if x di =1 then 7 end 8 else
Note that in the definition of our model the hyperprior of  X  is  X  . The hyperprior value of  X  is . Just as in the HDP model described earlier, the distribution F ( z di ) ,isthe Multinomial distribution in line 9 in the above generative process. We can obtain higher order n-grams by concatenating the current concatenated words with the next n-gram based on the value obtained by the next concatenation indicator variable. Although our model does not directly generate topic-wise n-grams, an n-gram can be associated with a topic via a simple post-processing strategy. One strategy is to take the topic of the first term in the n-gram as the topic for the whole n-gram. This technique has been used in [24] for the LDACOL model. Another strategy is to assume the topic of the n-gram as the most common topic occurring in the words involving in that n-gram [24]. 4.2 Posterior Inference Our inference scheme is based on the Chinese Restaurant Franchise scheme [28] with some modifications. In our scheme, we have to handle two different conditions. The first condition is concerned with x di =0 whereas the second condition is concerned with x di =1 . Note that for some observed x di , only z di needs to be drawn.
In the document modeling setting, each document is referred to as a restaurant and words in the document are referred to as customers. The set of documents share a global menu of topics. The words in the documen t are divided into groups, each of which shares a table. Each table is associated with a topic and words around each table are associated with the table X  X  topic.
 The First Condition: The first condition refers to x di =0 . In this setting, most of the modeling will resemble the HDP model as presented in [28], but in our case we need to derive updates for the HDP model for text data.

We will sample t di which is the table index for each word w di at the position i in the document d . We will then sample k dt which is the topic index variable for each table t in d . k d  X  t is the new topic index variable created for a new table. Note that we will only sample the index variables here rather than the distributions themselves [10]. k as ( k dt :  X  d, t ) . In addition, we also define x as ( x di :  X  d, i ). When a superscript is attached to a set of variables or count, for example, ( k  X  dt , t  X  di ), it means that the variables corresponding to the superscripted index are removed from the set or from the calculation of the count. Each word whose x di =0 is assumed to be drawn from F ( z ) whose density is written as f ( . |  X  ) ( f is just one part obtained from F ). This density is the multinomial distribution with the parameter  X  . The likelihood of w di for t di = t given all words in topic k except w di : where h is a probability density function of H and H is a Dirichlet distribution over a one of the global topics with which each tabl e is associated which is indicated with a table-specific topic index k dt . Furthermore, Equation 2 can be simplified as: where n  X  w di ..k is the number of words belonging to the topic k in the corpus whose x di =0 excluding w di . n topic k excluding w di and whose x di is 0. Furthermore, V is the number of words in the vocabulary which is typically fixed and is known. The likelihood of w di for t di =  X  t , where  X  t is the new table being sampled, is written as: where  X  k is the new topic being sampled. m .k is the number of tables belonging to the topic k in the corpus. m .. is the total number of tables in the corpus. f  X  w di  X  scribed in Section 3. Since we follow the standard Chinese Restaurant Franchise sam-pling procedure, the conditional density for t di for Gibbs sampling, the conditional densities for k d  X  t and k dt can be found in [10].
 The Second Condition: The second condition refers to x di =1 . We only need to sample the probability of a topic in a document as the current word w di is generated by the previous word w d,i  X  1 . In order to do this, we proceed as follows: where f  X  w dt k ( w dt ) , which is the conditional density of w dt given all words associated with the topic k leaving out w dt is defined as: number of times the word  X  appears at the table t with the assignment x di =0 . n  X  w dt ..k is the number of words belonging to topic k in the corpus except w dt .
 Sampling the Concatenation Indicator Variables: We present how to sample the values of the indicator variables. The idea is to compute the probabilities of how often two words consecutively occur in sequence. Then based on the probability value, the times word w d,i  X  1 has been drawn from a topic or formed a part of a concatenation respectively and all counts exclude the current case. 0 and 1 are the priors of the where  X  is same as described in Section 4.1. 5.1 Test Collections We used several corpora in our experiments. One corpus is the NIPS 1 collection often used in the topic modeling literature. Note that the original raw NIPS corpus consists of 17 years of conference papers. But we supplemented this corpus by including some new raw NIPS documents 2 and it has 19 years of papers i n total. Our NIPS collection consists of 2,741 documents comprising of 4,536,069 non-unique words and 94,961 words in the vocabulary. The second corpus is the Associated Press (AP) corpus. We have obtained this corpus from the LDA-C 3 package. This corpus consists of 2,243 documents with 38,631 words in the vocabulary. We also use one of the datasets from the 20 Newsgroups corpus for showing qualitative results. We have chosen the computer (indexed as  X  X omp X  available in the corpus) dataset from the 20 Newsgroups corpus. We removed stopwords 4 from the collections, but did not perform word stemming. 5.2 Comparative Methods One of the comparative methods is the basic HDP model proposed in [28]. We also chose the LDACOL model proposed in [18] for our comparative study. This model can be regarded as a parametric version close to our model. In addition to our proposed full NHDP , we also investigated a variant of our model, where we set all x di =1 . We call this model as Bi-NHDP in the experiments. Note that we do not expect the Bi-NHDP model to generate interpretable latent topi cs because it always generates bigrams just like the BTM [30] model. We do not compare with the TNG model [31] because the TNG model performs topic sampling for every word in a bigram. Neither our model nor LDACOL employ this sampling. Also we only chose s trong closely related comparative methods here. It has already been demonstr ated through quantitative analysis in [31] that the LDACOL model is more powerful than the BTM model. Also, in [30] it has been has shown that the BTM outperforms the LDA model in several quantitative experiments. Hence we do not compare our model with the BTM and the LDA models. 5.3 Experimental Setup The number of iterations for the Gibbs sampler for all models is 1,000. We have set  X  =0 . 1 , 0 =0 . 1 ,and 1 =0 . 1 which are the new parameters introduced in our model NHDP . We used a symmetric Dirichlet distribution with parameter of 0.5 for the prior H over topic distributions and the con centration parameters were set as  X   X  Gamma (1 , 0 . 1) and  X   X  Gamma (1 , 1) for our and the HDP models.  X  was set to 0.01. For the LDACOL model, following the notations described in [31], we set  X  =0 . 01 ,  X  =50 /T ,  X  0 =0 . 1 ,  X  1 =0 . 1 and  X  =0 . 1 . Note that T is the number of topics which is pre-defined for the LDACOL model. The same hyperparameter values are also used in the LDACOL implementation available publicly 5 . Since the LDACOL model requires the number of topics to be supplied by the user, we conducted several runs by varying the number of topics and measured the perfo rmance at different number of topics. The best result value was chosen based on all values obtained. 5.4 Qualitative Results We first show how our model generates more interpretable topics with topical n-grams. Here we employed a strategy that using the topic of the first word as the topic of the entire n-gram (refer Section 4.1 for more details). Our main comparative method for qualitative analysis is the HDP model which belongs to a family of nonparametric topic models. We manually chose top five words occurring with high probability in some topics. From Figure 1, we can see that compared to the HDP model, our NHDP model has generated words which are more cohere nt and has provided an extremely salient summary about  X  X eural networks X  in the NIPS collection, media related information obtained from the Associated Press (AP), and computer technology related words from the 20 Newsgroups computer dataset (denoted as  X  X omp Dataset X  in Figure 1). The results show that our model has produced interpretable topics.

Both LDACOL and NHDP can generate bigrams such as  X  X eural networks X , etc. But the merit of NHDP lies in the fact that it does not require the number of topics to be specified explicitly by the user and it is automatically inferred from the data charac-teristics. Moreover, NHDP can produce topical n-grams, not only restricted to bigrams. We thus investigate how well they perform in some typical text analysis tasks, such as document modeling, as described next. 5.5 Document Modeling Several works in the topic modeling literature have used perplexity analysis to compare the generalization ability of the model on unseen data as exemplified in [12]. Evaluation using perplexity is important for our mode l because it is well suited for models where word order in the document is maintained [7]. Generally the documents in the collection are treated as unlabeled. Thus, our goal is den sity estimation. We wish to achieve a high likelihood on the held-out test set. We first train the parameters of the model using a training set and subsequently the unseen data is fed to the learnt model in order to measure its generalization ability. A commonly used metric is the perplexity score. A lower perplexity score indicates better gener alization performance. More formally, for a test set D consisting of D documents, the perplexity score is written as: where w d are the words in the document d and N d is the number of words in the document d .

We conducted perplexity analysis by experimenting on two datasets described earlier (AP and NIPS datasets). Perplexity analysis was conducted by running the Gibbs sam-pler 3 times each with 1,000 iterations and the average of the three perplexity values was taken. For all the datasets, we split th e datasets randomly into two subsets each. One subset is the training set and the other subset is the testing set. We conducted sev-eral runs by varying the split proportion obtaining different amounts of the training set from 30% to 90% in steps of 20%. The purpose is to study how the models perform on different sizes of the training set.

From the results depicted in Figure 2, our model outperforms all comparative models in terms of generalizing on the unseen data. T he computed average perplexity values for our NHDP model are statistically significant compared to the HDP and LDACOL models according to a two-tailed statis tical significance test with p&lt; 0 . 05 in all corpora. The variant of our model, namely, Bi-NHDP does not perform at par with our NHDP model. The LDACOL model also does not generalize well on the unseen data. In Figure 2, we see the effect of varying the training size on different models. For example, in the AP and NIPS datasets, our model generally perfo rms extremely well in different training portions. One can note that even when the training data is less, our NHDP model gener-alizes well on the unseen data. In contrast, HDP loses important structural information in the document. We have presented a new nonparametric n-gram topic model that maintains the order of the words in the document. Word ordering plays a vital role in many linguistic tasks. An important innovation that we introduce i n our work is generating n-gram words in topics where the number of topics need not be specified by the user. We have shown better quantitative performance in generalizing on an unseen data in two document collections. Our model generates more inte rpretable latent topics with n-gram words, whereas the existing nonparametric topic model HDP fails to generate such n-grams which are more insightful to a reader.

In the future, we intend to extend our model in generating n-gram words in topics over time as test collections in general are dynamic and topics change over time. An-other direction which we wish to investigate is to incorporate text segmentation in a nonparametric setting and capturing n-gram words in each segment.

