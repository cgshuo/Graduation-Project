 support vector machine (SVM) has been a widely used and ranking [13] problem. Strictly speaking, a set of training data, consisting of  X  samples is given: belongs to. The soft margin SVM problem [26] is aiming to minimizing the following quadratic convex objective, which is also known as the primal SVM objective :  X  (  X  ) = where the first term is a 2-norm regularizer 12  X  X   X   X  empirical loss function: Reproducing Kernel Hilbert Space (RKHS), satisfying  X (.,.) . The integration of the kernel enables SVM to linear SVM or kernel SVM . For example, a polynomial kernel allows one to model feature conjunctions, while a Gaussian kernel enables us to pick out hyper spheres in solved the linear SVM problems [24] [14] [30]; however, to research problem. predictor  X  accordingly. It is worth noting that our proposed packSVM algorithm embraces the best known learning rate means, with probability at least 1 X  X  X  we can obtain a predictor  X  that is guaranteed to satisfy  X  (  X  )  X  X  X  (  X  if  X   X  is the optimal solution.
 parallelize the above algorithm with the help of a distributed proposed parallel algorithm P-packSVM. Notice that it is naturally difficult to parallelize SGD algorithms in hundreds of processors due to their huge communication cost. The packing strategy non-trivially reduces the communication The time complexity of P-p ackSVM is thus reduced to  X  (  X / X  X  X  X  X  X  ) if using  X  processors. At the same time, P-packSVM uses only  X ( X / X ) space for each processor. packSVM overwhelms the state-o f-the-art PSVM [5] in both than SVM-light. Mean while, its accuracy is comparable to SVM-light. For example, P-p ackSVM trains a CCAT dataset of 800k samples in 761 seconds with a speed-up of 295 times on 512 processors; it trains a CovType dataset of 500k samples in 236 seconds with a speed-up of 416 times on 512 capable of performing well in million scale data set. introducing its sequential implementation and then move differences of our P-packSVM with other contemporary works in this section. Experimental results are then provided in Section IV. Finally we leave several enhancements to our algorithm in Section V and conclude our work in Section VI. Lagrangian multipliers introduces a transformation from the primal objective (2) into its dual form: min the Lagrangian dual variable , also known as the support vector in SVM. The predictor  X  is a superposition of  X ( X  SVM trainers mainly into the following three categories. Interior Point Method (IPM) : minimizing the dual objective is a convex Quadra tic Programming (QP) problem and can be solved via the primal-dual Interior-Point Method [21]. The idea of IPM is to incorporate Newton or Quasi-Newton methods with the number of iterations proportional the memory requirements of IPM are as high as  X  (  X  the computational cost is  X  (  X   X  ) for each iteration. called PSVM. It enables a parallel implementation of IPM and Incomplete Cholesky Factorization [9] (ICF)  X  X  X  X  X  X  where  X  has the dimension  X  X  X  X   X  . [5] empirically showed induced an algorithm with the time complexity of  X ( X   X   X   X  ) for each iteration and the space requirement of  X ( X  kernel SVM on 500 processors in experiments, and they training samples. Sequential Minimal Optimization (SMO) : to make SVM decomposing the large QP problem into an inactive part and an active part  X  a so called  X  X orking set X . Many open source hundred thousand training samples on a single machine. made. For example, Zanghirati and Zanni [28] proposed a parallel implementation of SVM-light, especially effective for Gaussian kernels; Cao et al [3] also parallelized a slightly conducted experiments on up to 32 processors with 60k training samples, claiming a speed-up of approximately 20 times. growing amount of attention had been paid towards stochastic gradient descent algorithms, in which the gradient linear-SVM algorithms. T. Zhang [29] proved that a constant learning rate (no parameter sweep required) in SGD will numerically achieve good accur acy, enabling a running time aggressively adopted a learning rate of 1/ X  X  X  . It turns out this SVM, and even endowed with an inverse time dependency time complexity of  X  (  X  ) . primal objective is slow when an algorithm tries to optimize first two categories. Our proposed method falls into the third category, and thus is born with advantages. We incorporate a distributed hash table to enable the parallelism and the that though in general only the algorithm in the first category can be effectively parallelized, our proposed packing strategy reverses the adversity. descent method to the kernel SVM problem, and provide the result of its convergence analysis. Next, we propose its contemporary works. A. Sequential packSVM packSVM. We adopt the framework discussed in [24], which has the best known learning rate and an additional projection phrase. hinge loss on a single training sample. Based on this idea, we  X  X  X  { 1,... X  } , it picks up a random example  X  X   X  (  X  ) , X   X  , and approximates the empiri cal loss (3) and the objective (2) as the following:  X  (  X  )  X  X  X   X  (  X  )  X  X ax X 0,1 X  X  X   X  (  X  )  X   X   X , X  X  X  X   X  (  X  )  X   X   X  modify the predictor as below in iteration  X  : gradient [24] [23]. We write down the sub-gradient explicitly: superposition of samples  X =  X   X   X   X   X   X  (  X   X  )  X   X  X  X  X  subtraction in (6) simply consists of an overall shrinking and the addition of at most one term. to get closer to the optimum [24]: performing scaling we can simply change the value of  X  and the adding  X  X  X  X +  X   X   X   X   X ( X   X  )  X  . Figure 2 gives the pseudo code of the algorithm presented in Figure 1. analysis in [24], which is a special case of S-packSVM when linear kernel is adopted. Due to limited space, we only follow the idea of [24]. following inequality for some constant  X  : that S-packSVM requires  X = X   X  ( 1/ X  X  X  X  ) iterations to obtain at least 1 X  X  X  , assuming  X   X  to be the optimal predictor. This suggests a total running time of  X   X  (  X / X  X  X  X  ) for S-packSVM, as all commands except Line 5 take a constant running time, entries of  X  in at most  X ( X ) time. B. Parallel packSVM algorithm [29], its sequential behavior does not show a many experimental observations, we find the optimal  X  on time complexity is in square dependence on the number of samples  X  . In this sub-section, we provide the parallel packSVM, called P-packSVM, a nd show that it has some unique advantages in kernel SVM training. Before going into parallelism that packSVM embodies: strategy to overcome the defect. Distributed Hash Table . We enable a distributed hash table Suppose the  X  th processor saves a subset two important operations: 3 can be experimentally shown to overwhelm many contemporary kernel SVM tools and run well on up to further by introducing the following packing strategy. Packing Strategy . Given an integer  X  ,we aim to pack  X  shown in Section IV.C. equation (8) and (9), the calculation from  X  additional term. For the sake of simplicity, we combine them and write the recursive fo rmula implicitly, where  X  calculated from  X   X  X  X  , X   X  X  X  , X   X  X  X  : but  X   X  is dependent on the previous iteration, since  X  can only be calculated in iteration  X  X  X 1 . At first glance, this suggests it is ends. Next, we will show how to calculate  X   X   X  ,... X  simultaneously.  X   X  =  X   X   X  , X  (  X   X  ) X  to terms of  X   X  , X  (  X   X = X ... X + X  X  X 1 , and hide those comp lex coefficients. One can see that although coefficients  X  are unknown at the  X   X  Besides, the pair-wise values  X ( X   X  , X   X  ) for  X  X  X  X &lt; X  X  X  X +  X  X  X 1 can also be pre-processed in a distributed manner. This all needs two communication requests like AllReduce in MPI. We summarize our packing algorithm for  X  consecutive iterations  X ,..., X + X  X  X 1 as follows: packing strategy in Appendix. We remark on the coefficient  X  that it is not the larger the better. As one may see from the assuming  X  to be the feature dimens ion. If this time exceeds acceleration will be undermined. We will practically show that  X  =100 is a good parameter in Section IV.C. C. Comparisons sequential or parallel trainers mentioned in Section II, for the large scale kernel SVM training. pay attention here the former  X  the optimization error. primal. If the algorithm terminates early, the optimization on the primal produce better solution than on the dual. Secondly, algorithm converges to its own objective. IPM and SMO goes closer to the optimal in every step. SGD algorithms do convexity [23] [24] ensures that P-packSVM achieves good accuracy, as we analyzed in Section III.A. Speed on a single machine . SGD algorithms are the fastest for linear SVM [25], and SMO algorithms are generally address the incorporation of ke rnels in SGD, because before the introduction of the best known learning rate in [24], SGD algorithms like [16] take a much longer time than SMO. We will show in Section IV.A that our SGD algorithm, P-packSVM, can achieve similar efficiency as SMO on a single machine. Parallel speed-up . Regarding the parallel capability, we need to consider the following two factors: Compare with PSVM . We pay special attention to the comparisons with our well-matched adversary PSVM. Firstly, in order to achieve an endu rable speed, PSVM forces an Incomplete Cholesky Factoriza tion, lacks theoretical error convergence analysis on P-packSVM guarantees good the dual objective while our P-packSVM directly optimizes on the primal. Thirdly, the parallel speed-up of PSVM cannot achieve the height of P-packSVM, due to Amdahl X  X  law mentioned above. Fourthly, the memory requirement for PSVM is as high as  X ( X   X . X  / X ) , while P-packSVM uses only  X ( X / X ) for each processor, making memory no longer a bottleneck for the algorithm. varying in size from 1,000 to 8,000,000 samples. We use 144 machine is equipped with two 2.5GHz Intel Xeon CPUs with Message-Passing Interface (MPI) as our parallel platform the experiments: the Gaussian  X  X  X  X  kernel kernels like the polynomial kern el, Laplacian kernel, etc. A. Performance Test the accuracy of our proposed P-packSVM against two state-of-the-art SVM trainers: SVM-light [12] and PSVM [5]. For SVM-light we use its default convergence parameters. For dual) threshold to 0.1, and an upper limit of 1000 iterations. Unless otherwise state, we adopt the suggested  X   X  = X   X . X  balance the accuracy and th e efficiency in [5]. Appendix for a detailed configuration). These parameters are program runs three times a nd the mean accuracy, mean small training set splice , neither PSVM no r P-packSVM can achieve reasonable accuracy un der the above configurations, and thus we choose  X   X  =0.1 X  for PSVM and  X = 10 X ,15 X ,20 X  for P-packSVM. Notice that the column  X #pro cessors X  applies to both PSVM and P-packSVM, while SVM-light is a sequential SVM trainer. We conclude that P-packSVM is hundreds of times faster than SVM-light and several times faster than PSVM performing simple multiplication, one can see even in a single machine, our proposed P-packSVM may achieve a similar speed as SVM-light for large scale data. selected in the en tire execution of P-packSVM. The accuracy report in Figure 6 demonstrates that our proposed method can get accuracy very close to SVM-light X  X , and overwhelm the state-of-the-art traine r PSVM on datasets except CovType . Cholesky decomposition  X  makes PSVM not accurate enough for datasets with large-rank kernel matrices, like CCAT . and add the number of correct / incorrect instances together. labels are extremely biased (number of negative samples dominate). Results in Figure 7 show that our proposed P-packSVM can handle this situ ation successfully. In sharp several days). MNIST8m . We emphasize that our proposed method can run against the very large scale dataset MNIST8m with 8 million did not spend extra time choosing the best fit  X  and  X  X  X  X  (see Appendix). To the best of our knowledge, no generic kernel S [ a o  X  9 t e t h r c o B w e p s a T o w p C o t i 1 a n S VM trainer h a 19] used the i n a n accuracy of o n the origina l  X  = X /8, X / 4 9 9.54% and 9 9 e sting set, an d h e last 100,00 r unning time is c ompetitor, ou r o n million scal e B . Convergen c w ith respect t o e xperiment is p rediction acc u s hown in Figu r a round 200,00 T his chart also o f iterations o n w ith a stochas t p ackSVM with C . Scalability o n CovType a n i me with a dif f 1 28, 256, 512. a s the followin g n umerato r in t h p ackS V p roces s runnin g eighth for  X  = runnin g speed-u can be time i n What i both C obtain a CovTy p PSVM for bot h our pr o our P-p Bias t e the obj result i  X = X   X  ( objecti v to this  X   X  ) /2 t stay cl o tested t (about 2 Conve r criteri o lacks a duality example 1000 random samples can be calculated (this needs iteration limit  X  is around the sample size  X  . Extension to other loss . We emphasize that our proposed algorithm can be easily generalized to convex loss functions, slightly change  X  X  X   X  in Equation (7). We have shown that the [30]. We will perform this research in our further work. kernels. Parallel implementation is provided by introducing a The proposed algorithm, P-pack SVM, averagely distributes communications requests. We em phasize that this packing parallel speed-up. proposed algorithm can run much faster than the state-of-the-faster than the sequential trainer SVM-light. For example, P-packSVM trains CovType with 500k samples in 4 minutes and CCAT with 800k samples in 13 minutes. We emphasize that P-packSVM attain s accuracy that is sufficiently close to SVM-light, and prevails over that of PSVM . Hebrew University for his valuable discussions, Teng Gao Peking University for her comparable experiments on PSVM. anonymous reviewers for their fruitful comments. 
