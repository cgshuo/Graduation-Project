 Categories and Subject Descriptors: H.3.4 [Informa-tion Storage and Retrieval]: Systems and Software Perfor-mance Evaluation General Terms: Measurement,Reliability Keywords: Performance Evaluation, Metrics, Uncertainty
Modern information retrieval (IR) test collections violate the completeness assumption of the Cranfield paradigm [2]. In order to maximise the available resources, only a sample of documents (i.e. the pool) are judged for relevance by a hu-man assessor(s). The subsequent evaluation protocol does not make any distinctions between assessed or unassessed documents, as documents that are not in the pool are as-sumed to be not relevant for the topic. This is beneficial from a practical point of view, as the relative performance can be compared with confidence if the experimental con-ditions are fair for all systems. However, given the incom-pleteness of relevance assessments, two forms of uncertainty emerge during evaluation. The first is Aleatory uncertainty, which refers to variation in system performance across the topic set, which is often addressed through the use of sta-tistical significance tests. The second form of uncertainty is Epistemic , which refers to the amount of knowledge (or ignorance) we have about the estimate of a system X  X  per-formance [1]. Epistemic uncertainty is a consequence of in-completeness and is not addressed by the current evaluation protocol. In this study, we present a first attempt at mod-elling both aleatory and epistemic uncertainty associated with IR evaluation. We aim to account for both the vari-ability associated with system performance and the amount of knowledge known about the performance estimate.
IR evaluation can be thought of as a probabilistic reason-ing and decision process. Conventional Bayesian decision process represents knowledge in the form of specified dis-tributions. This is always true even if the source of infor-mation about this knowledge is weak. In this convention, P ( A )+ P (  X  A )=1where P ( A ) represents the probabil-ity in favour of the proposition and P (  X  A )representsthe opposite argument. However, information utilised for this process can be imprecise and/or ambiguous. With respect to IR evaluation, the precision of performance metric can be modelled specifically as the proportion of documents as-sessed for relevance in a systems ranked list, which we define as assessment [1]. By accounting for the epistemic uncer-tainty associated with incomplete collections, we model both performance and knowledge of the experimental conditions when comparing systems. Rather than testing whether  X  X ys-tem A is better than system B under certain conditions X , we refer to these uncertainties with respect to the performance of  X  X ystem A and system B X . The Dempster-Shafer theory of evidence is a framework for representing this situation. of representing and reasoning with imperfect information [3], providing a foundation for a non-Bayesian theory of prob-ability in order to represent epistemic uncertainty. Instead of a Bayesian distribution, the probabilities are represented by a belief function and assigned to sets rather than to sin-gle events. Therefore, supporting evidence is calculated and represented by a belief interval to show how close the ev-idence is to determining the truth of a hypothesis. The framework is designed to address varying levels of precision regarding its set theoretical nature. Evidences from differ-ent sources can be combined without a need for any a-priori distributions.

DS introduces a concept called frame of discernment which contains the set of elements that we are interested in  X  = {  X  1 , X  2 , X  3 , ..,  X  n } . The power set contains all 2 sets. The mass function, denoted by m , is a mapping be-tween the power set to the interval of 0 and 1. The sum of the masses of all the subsets of the power set equals to 1 (see equation 1). Although there are some interpretations in the literature, the mass function does not refer to probability in the classical sense. A probability assignment may not always be possible to all subsets of  X . In this case, an uncertainty measure can be modelled on the set  X . This assignment can be interpreted as the amount of information that we are not able to load to any of the subsets or hypotheses. In the Bayesian viewpoint, this has only one interpretation: the opposing proposition.
DS defines two important representations of uncertainty associated with a set of possible hypotheses; a Belief and a Plausibility function. Belief is a measure the extent to which all evidences implies that the true value is contained in the set under consideration defined in  X . In short, it is the smallest possible probability that is consistent and sup-ported by all available information. Conversely, plausibility is a measure of the extent to which all evidences implies that true value might be in the set under consideration de-fined in  X . In this regard, plausibility is the largest possible probability that is consistent and supported by all available information which cannot be disproved.

These functions may be viewed as lower and upper bounds on probabilities, respectively, where the actual probability is contained in the interval described by these bounds. The lower bound Belief for a set A is defined as the sum of all the basic probability assignments of the proper subsets ( B ) of the set of interest ( A )( Bel ( A )). The upper bound then can be calculated as the sum of all the basic probability assignments of the sets ( A )thatintersectthesetofinterest ( Pl ( A )). Formally, for all A  X   X , It is also possible to obtain one from the other by Pl ( A )= 1  X  Bel (  X  A ) and derive the amount of ignorance by Ign ( A )= Pl ( A )  X  Bel ( A ). Thus, we can write Bel ( A )+ Bel (  X  A ) &lt; = 1, which is one of the main differences between DS and classical probability.

The theory also suggests combination of two or more bod-ies of independent evidences defined with in the same frame of discernment into one body of evidence. Dempster X  X  com-bination rule is defined as DS in IR Evaluation. In IR evaluation, incompleteness is caused by lack of evidences and thus give rise to uncommit-ted belief. This uncommitted mass contains the probability neither belongs to a relevant set or non-relevant set. We regard this as the uncertainty over the experiments which should be considered during the comparison of system per-formance. Therefore, we model relevant, non-relevant and unassessed portions of a result set of a system A which is run over a topic set T = { t 1 , ..., t n } . These result sets can be considered as different and independent sources of evidences during the evidence combination phase.

We now provide a concrete example of how uncertainty can be modelled when evaluating the performance of a sys-tem. In order to demonstrate the reasoning let  X  = { R, N where R represents the relevant portion of the collection which system A returns w.r.t a topic t and N represents non-relevant portion (see Table 1). The masses of m ( { R } )and m ( { N } ) together represents our true belief on the amount of assessment (percentages of R and N retrieved). Conversely, m ( { R, N } ) represents the amount of unassessment (uncom-mitted belief) which is also the frame of discernment  X  it-self. Then, belief intervals for each argument are given below [Bel( { A } ),Pl( { A } )].
 Experiments. We now demonstrate the utility of DS by comparing two systems using the Robust 2005 TREC collec-tion. An analysis by Buckley et al. [2] highlighted a potential bias towards documents containing the topic title keywords in this collection, although relevant documents exist that do not contain these terms. For example, the sab05ror1 run, using existing relevance judgements to generate an op-timal query, reported a large change in performance when removing unique documents pooled by the system from the relevance assessments. In this demonstration (see Figure 1), we compare this system against another run, pircRB05t2 , whichusedanexternalcorporatoexpandthetitlequery.
 The aim is to analyse whether our evaluation model can identify potential problems with this comparison by iden-tifying lower and upper boundaries of performance. The performance of sab05ror1 appears to be better than pir-cRB05t2 . However, as we compare performance over an increasing measurement depth the confidence interval starts to spread, particularly beyond the pool depth. The amount of information about the systems performances decreases which is a potential indication of the epistemic uncertainty. Figure 1: Lower and Upper boundaries for both sys-tems across the measurement depth
We are currently in the progress of completing an in-depth study of this framework, comparing a large number of sys-tems across various test collections and metrics, as well as extending this model for graded relevance assessments.
