 Inderjit S. Dhillon INDERJIT @ CS . UTEXAS . EDU The support vector machine (SVM) (Cortes &amp; Vapnik, 1995) is probably the most widely used classifier in var-ied machine learning applications. For problems that are not linearly separable, kernel SVM uses a  X  X ernel trick X  to implicitly map samples from input space to a high-dimensional feature space, where samples become linearly separable. Due to its importance, optimization methods for kernel SVM have been widely studied (Platt, 1998; Joachims, 1998), and efficient libraries such as LIBSVM (Chang &amp; Lin, 2011) and SVMLight (Joachims, 1998) are well developed. However, the kernel SVM is still hard to scale up when the sample size reaches more than one mil-lion instances. The bottleneck stems from the high compu-tational cost and memory requirements of computing and storing the kernel matrix, which in general is not sparse. By approximating the kernel SVM objective function, ap-proximate solvers (Zhang et al., 2012; Le et al., 2013) avoid high computational cost and memory requirement, but suf-fer in terms of prediction accuracy.
 In this paper, we propose a novel divide and conquer ap-proach (DC-SVM) to efficiently solve the kernel SVM problem. DC-SVM achieves faster convergence speed compared to state-of-the-art exact SVM solvers, as well as better prediction accuracy in much less time than ap-proximate solvers. To accomplish this performance, DC-SVM first divides the full problem into smaller subprob-lems, which can be solved independently and efficiently. We theoretically show that the kernel kmeans algorithm is able to minimize the difference between the solution of subproblems and of the whole problem, and support vectors identified by subproblems are likely to be support vectors of the whole problem. However, running kernel kmeans on the whole dataset is time consuming, so we apply a two-step kernel kmeans procedure to efficiently find the parti-tion. In the conquer step, the local solutions from the sub-problems are  X  X lued X  together to yield an initial point for the global problem. As suggested by our analysis, the coor-dinate descent method in the final stage converges quickly to the global optimal.
 Empirically, our proposed Divide-and-Conquer Kernel SVM solver can reduce the objective function value much faster than existing SVM solvers. For example, on the covtype dataset with half a million samples, DC-SVM can find a globally optimal solution (to within 10  X  6 accu-racy) within 3 hours on a single machine with 8 GBytes RAM, while the state-of-the-art LIBSVM solver takes more than 22 hours to achieve a similarly accurate solu-tion (which yields 96.15% prediction accuracy). More in-terestingly, due to the closeness of the subproblem solu-tions to the global solution, we can employ an early pre-diction approach, using which DC-SVM can obtain high test accuracy extremely quickly. For example, on the cov-type dataset, by using early prediction DC-SVM achieves 96.03% prediction accuracy within 12 minutes, which is more than 100 times faster than LIBSVM (see Figure 3e for more details).
 The rest of the paper is outlined as follows. We propose the single-level DC-SVM in Section 3, and extend it to the multilevel version in Section 4. Experimental comparison with other state-of-the-art SVM solvers is shown in Sec-tion 5. The relationship between DC-SVM and other meth-ods is discussed in Section 2, and the conclusions are given in Section 6. Extensive experimental comparisons are in-cluded in the Appendix. Since training SVM requires a large amount of memory, it is natural to apply decomposition methods (Platt, 1998), where each time only a subset of variables are updated. To speedup the decomposition method, (P  X  erez-Cruz et al., 2004) proposed a double chunking approach to maintain a chunk of important samples, and the shrinking technique (Joachims, 1998) is also widely used to eliminate unimpor-tant samples.
 To speed up kernel SVM training on large-scale datasets, it is natural to divide the problem into smaller subprob-lems, and combine the models trained on each partition. (Jacobs et al., 1991) proposed a way to combine models, although in their algorithm subproblems are not trained independently, while (Tresp, 2000) discussed a Bayesian prediction scheme (BCM) for model combination. (Col-lobert et al., 2002) partition the training dataset arbitrarily in the beginning, and then iteratively refine the partition to obtain an approximate kernel SVM solution. (Kugler et al., 2006) applied the above ideas to solve multi-class problems. (Graf et al., 2005) proposed a multilevel ap-proach (CascadeSVM): they randomly build a partition tree of samples and train the SVM in a  X  X ascade X  way: only support vectors in the lower level of the tree are passed to the upper level. However, no earlier method appears to dis-cuss an elegant way to partition the data. In this paper, we theoretically show that kernel kmeans minimizes the error of the solution from the subproblems and the global so-lution. Based on this division step, we propose a simple method to combine locally trained SVM models, and show that the testing performance is better than BCM in terms of both accuracy and time (as presented in Table 1). More im-portantly, DC-SVM solves the original SVM problem, not just an approximated one. We compare our method with Cascade SVM in the experiments.
 Another line of research proposes to reduce the training time by representing the whole dataset using a smaller set of landmark points, and clustering is an effective way to find landmark points (cluster centers). (Moody &amp; Darken, 1989) proposed this idea to train the reduced sized problem with RBF kernel (LTPU); (Pavlov et al., 2000) used a sim-ilar idea as a preprocessing of the dataset, while (Yu et al., 2005) further generalized this approach to a hierarchical coarsen-refinement solver for SVM. Based on this idea, the kmeans Nystr  X  om method (Zhang et al., 2008) was proposed to approximate the kernel matrix using landmark points. (Boley &amp; Cao, 2004) proposed to find samples with simi-lar  X  values by clustering, so both the clustering goal and training step are quite different from ours. All the above ap-proaches focus on modeling the between-cluster (between-landmark points) relationships. In comparison, our method focuses on preserving the within-cluster relationships at the lower levels and explores the between-cluster information in the upper levels. We compare DC-SVM with LLSVM (using kmeans Nystr  X  om) and LTPU in Section 5. There are many other approximate solvers for the kernel SVM, including kernel approximation approaches (Fine &amp; Scheinberg, 2001; Zhang et al., 2012; Le et al., 2013), greedy basis selection (Keerthi et al., 2006), and online SVM solvers (Bordes et al., 2005). Recently, (Jose et al., 2013) proposed an approximate solver to reduce testing time. They use multiple linear hyperplanes for prediction, so the time complexity for prediction is proportional to the dimensionality instead of number of samples. There-fore they achieve faster prediction but require more train-ing time and have lower prediction accuracy comparing to DC-SVM with early prediction strategy. Given a set of instance-label pairs ( x i ,y i ) ,i = 1 ,...,n, x i  X  R d and y i  X  { 1 ,  X  1 } , the main task in training the kernel SVM is to solve the following quadratic optimization problem: where e is the vector of all ones; C is the balancing pa-rameter between loss and regularization in the SVM pri-mal problem;  X   X  R n is the vector of dual variables; and Q is an n  X  n matrix with Q ij = y i y j K ( x i , x j ) , where K ( x i , x j ) is the kernel function. Note that, as in (Keerthi et al., 2006; Joachims, 2006), we ignore the  X  X ias X  term  X  indeed, in our experiments reported in Section 5, we did not observe any improvement in test accuracy by including the bias term. Letting  X   X  denote the optimal solution of (1), the decision value for a test data x can be computed by P We begin by describing the single-level version of our pro-posed algorithm. The main idea behind our divide and conquer SVM solver (DC-SVM) is to divide the data into smaller subsets, where each subset can be handled effi-ciently and independently. The subproblem solutions are then used to initialize a coordinate descent solver for the whole problem. To do this, we first partition the dual vari-ables into k subsets {V 1 ,..., V k } , and then solve the re-spective subproblems independently min where c = 1 ,...,k ,  X  ( c ) denotes the subvector {  X  p | p  X  V } and Q ( c,c ) is the submatrix of Q with row and column indexes V c .
 The quadratic programming problem (1) has n variables, and takes at least O ( n 2 ) time to solve in practice (as shown in (Menon, 2009)). By dividing it into k subproblems (2) with equal sizes, the time complexity for solving the sub-problems can be dramatically reduced to O ( k  X  ( n k ) 2 O ( n 2 /k ) . Moreover, the space requirement is also reduced from O ( n 2 ) to O ( n 2 /k 2 ) .
 After computing all the subproblem solutions, we concate-nate them to form an approximate solution for the whole problem  X   X  = [  X   X  (1) ,...,  X   X  ( k ) ] , where  X   X  solution for the c -th subproblem. In the conquer step,  X   X  is used to initialize the solver for the whole problem. We show that this procedure achieves faster convergence due to the following reasons: (1)  X   X  is close to the optimal solution for the whole problem  X   X  , so the solver only requires a few iterations to converge (see Theorem 1); (2) the set of sup-port vectors of the subproblems is expected to be close to the set of support vectors of the whole problem (see Theo-rem 2). Hence, the coordinate descent solver for the whole problem converges very quickly.
 Divide Step. We now discuss in detail how to divide problem (1) into subproblems. In order for our proposed method to be efficient, we require  X   X  to be close to the opti-mal solution of the original problem  X   X  . In the following, we derive a bound on k  X   X   X   X   X  k 2 by first showing that  X   X  is the optimal solution of (1) with an approximate kernel. Lemma 1.  X   X  is the optimal solution of (1) with kernel function K ( x i , x j ) replaced by where  X  ( x i ) is the cluster that x i belongs to; I ( a,b ) = 1 iff a = b and I ( a,b ) = 0 otherwise.
 Based on the above lemma, we are able to bound k  X   X   X   X   X  k by the sum of between-cluster kernel values: Theorem 1. Given data points { ( x i ,y i ) } n i =1 with labels y i  X  { 1 ,  X  1 } and a partition indicator {  X  ( x 1 ) ,..., X  ( x n ) } , where f (  X  ) is the objective function in (1) and D (  X  ) = P C 2 D (  X  ) / X  n where  X  n is the smallest eigenvalue of the ker-nel matrix.
 The proof is provided in Appendix 8.2. In order to min-imize k  X   X   X   X   X  k , we want to find a partition with small D (  X  ) . Moreover, a balanced partition is preferred to achieve faster training speed. This can be done by the kernel kmeans algorithm, which aims to minimize the off-diagonal values of the kernel matrix with a balancing nor-malization.
 We now show that the bound derived in Theorem 1 is rea-sonably tight in practice. On a subset (10000 instances) of the covtype data, we try different numbers of clusters k = 8 , 16 , 32 , 64 , 128 ; for each k , we use kernel kmeans to obtain the data partition {V 1 ,..., V k } , and then compute C 2 D (  X  ) / 2 (the right hand side of (4)) and f (  X   X  )  X  f (  X  (the left hand side of (4)). The results are presented in Fig-ure 1. The left panel shows the bound (in red) and the difference in objectives f (  X   X  )  X  f (  X   X  ) in absolute scale, while the right panel shows these values in a log scale. Figure 1 shows that the bound is quite close to the differ-ence in objectives in an absolute sense (the red and blue curves nearly overlap), especially compared to the differ-ence in objectives when the data is partitioned randomly (this also shows effectiveness of the kernel kmeans proce-dure). Thus, our data partitioning scheme and subsequent solution of the subproblems leads to good approximations to the global kernel SVM problem. Figure 1: Demonstration of the bound in Theorem 1  X  our data However, kernel kmeans has O ( n 2 d ) time complexity, which is too expensive for large-scale problems. Therefore we consider a simple two-step kernel kmeans approach as in (Ghitta et al., 2011). The two-step kernel kmeans algo-rithm first runs kernel kmeans on m randomly sampled data points ( m n ) to construct cluster centers in the kernel space. Based on these centers, each data point computes its distance to cluster centers and decides which cluster it be-longs to. The algorithm has time complexity O ( nmd ) and space complexity O ( m 2 ) . In our implementation we just use random initialization for kernel kmeans, and observe good performance in practice.
 A key facet of our proposed divide and conquer algorithm is that the set of support vectors from the subproblems  X  very close to that of the whole problem S := { i |  X   X  i &gt; 0 } . Letting  X  f (  X  ) denote the objective function of (1) with ker-nel  X  K defined in (3), the following theorem shows that when  X   X  i = 0 ( x i is not a support vector of the subprob-lem) and  X  i  X  f (  X   X  ) is large enough, then x i will not be a support vector of the whole problem.
 Theorem 2. For any i  X  X  1 ,...,n } , if  X   X  i = 0 and where K max = max i K ( x i , x i ) , then x i will not be a sup-port vector of the whole problem, i.e.,  X   X  i = 0 . The proof is given in Appendix 8.3. In practice also, we ob-serve that DC-SVM can identify the set of support vectors of the whole problem very quickly. Figure 2 demonstrates that DC-SVM identifies support vectors much faster than the shrinking strategy implemented in LIBSVM (Chang &amp; Lin, 2011) (we discuss these results in more detail in Sec-tion 4).
 Conquer Step. After computing  X   X  from the subproblems, we use  X   X  to initialize the solver for the whole problem. In principle, we can use any SVM solver in our divide and conquer framework, but we focus on using the coordinate descent method as in LIBSVM to solve the whole prob-lem. The main idea is to update one variable at a time, and always choosing the  X  i with the largest gradient value to update. The benefit of applying coordinate descent is that we can avoid a lot of unnecessary access to the kernel ma-trix entries if  X  i never changes from zero to nonzero. Since  X   X   X  X  are close to  X   X  , the  X   X  -values for most vectors that are not support vectors will not become nonzero, and so the algorithm converges quickly. There is a trade-off in choosing the number of clusters k for a single-level DC-SVM with only one divide and conquer step. When k is small, the subproblems have similar sizes as the original problem, so we will not gain much speedup. On the other hand, when we increase k , time complexity for solving subproblems can be reduced, but the resulting  X   X  can be quite different from  X   X  according to Theorem 1, so the conquer step will be slow. Therefore, we propose to run DC-SVM with multiple levels to further reduce the time for solving the subproblems, and meanwhile still obtain  X   X  values that are close to  X   X  .
 In multilevel DC-SVM, at the l -th level, we partition the whole dataset into k l clusters {V ( l ) 1 ,..., V ( l ) k l those k l subproblems independently to get  X   X  ( l ) . In order to solve each subproblem efficiently, we use the solutions from the lower level  X   X  ( l +1) to initialize the solver at the l -th level, so each level requires very few iterations. This allows us to use small values of k , for example, we use k = 4 for all the experiments. In the following, we discuss more insights to further speed up our procedure.
 Adaptive Clustering. The two-step kernel kmeans ap-proach has time complexity O ( nmd ) , so the number of samples m cannot be too large. In our implementation we use m = 1000 . When the data set is very large, the perfor-mance of two-step kernel kmeans may not be good because we sample only a few data points. This will influence the performance of DC-SVM.
 To improve the clustering for DC-SVM, we propose the following adaptive clustering approach. The main idea is to explore the sparsity of  X  in the SVM problem, and sample from the set of support vectors to perform two-step kernel kmeans. Suppose we are at the l -th level, and the current set of support vectors is defined by  X  S = { i |  X   X  i &gt; 0 } . Suppose the set of support vectors for the final solution is given by S  X  = { i |  X   X  i &gt; 0 } . We can define the sum of off-diagonal elements on  X  S  X  S  X  as D S  X   X   X  S P orem shows that we can refine the bound in Theorem 1: Theorem 3. Given data points x 1 ,..., x n and a partition {V 1 ,..., V k } with indicators  X  , Furthermore, k  X   X   X   X   X  k 2 2  X  C 2 D S  X   X   X  S (  X  ) / X  The proof is given in Appendix 8.4. The above observa-tions suggest that if we know the set of support vectors  X  and S  X  , k  X   X   X   X   X  k only depends on whether we can obtain a good partition of  X  S  X  S  X  . Therefore, we can sample m points from  X  S  X  S  X  instead of the whole dataset to perform the clustering. The performance of two-step kernel kmeans depends on the sampling rate; we enhance the sampling rate from m/n to m/ | S  X   X   X  S | . As a result, the performance significantly improves when | S  X   X   X  S | n .
 In practice we do not know S  X  or  X  S before solving the prob-lem. However, both Theorem 2 and experiments shown in Figure 2 suggest that we have a good guess of sup-port vectors even at the bottom level. Therefore, we can use the lower level support vectors as a good guess of the upper level support vectors. More specifically, after com-puting  X   X  l from level l , we can use its support vector set  X  S l := { i |  X   X  l ing the clusters at the ( l  X  1) -th level. Using this strategy, we obtain progressively better partitioning as we approach the original problem at the top level.
 Early identification of support vectors. We first run LIBSVM to obtain the final set of support vectors, and then run DC-SVM with various numbers of clusters 4 , 4 4 ,..., 4 0 (corresponding to level 5 , 4 ,..., 0 for multi-level DC-SVM). We show the precision and recall for the support vectors determined at each level (  X   X  i &gt; 0 ) in iden-tifying support vectors. Figure 2 shows that DC-SVM can identify about 90% support vectors even when using 256 clusters. As discussed in Section 2, Cascade SVM (Graf et al., 2005) is another way to identify support vectors. However, it is clear from Figure 2 that Cascade SVM can-not identify support vectors accurately as (1) it does not use kernel kmeans clustering, and (2) it cannot correct the false negative error made in lower levels. Figure 2c, 2d, 2g, 2h further shows that DC-SVM identifies support vectors more quickly than the shrinking strategy in LIBSVM. Early prediction based on the l -th level solution. Com-puting the exact kernel SVM solution can be quite time consuming, so it is important to obtain a good model us-ing limited time and memory. We now propose a way to efficiently predict the label of unknown instances using the lower-level models  X   X  l . We will see in the experiments that prediction using  X   X  l from a lower level l already can achieve near-optimal testing performance.
 When the l -th level solution  X   X  l is computed, a naive way to predict a new instance x  X  X  label  X  y is: Another way to combine the models trained from k clus-ters is to use the probabilistic framework proposed in the Bayesian Committee Machine (BCM) (Tresp, 2000). How-ever, as we show below, both these methods do not give good prediction accuracy when the number of clusters is large.
 Instead, we propose the following early prediction strategy. From Lemma 1,  X   X  is the optimal solution to the SVM dual problem (1) on the whole dataset with the approximated kernel  X  K defined in (3). Therefore, we propose to use the same kernel function  X  K in the testing phase, which leads to the prediction where  X  ( x ) can be computed by finding the nearest cluster center. Therefore, the testing procedure for early prediction is: (1) find the nearest cluster that x belongs to, and then (2) use the model trained by data within that cluster to compute the decision value.
 We compare this method with prediction by (5) and BCM in Table 1. The results show that our proposed testing scheme is better in terms of test accuracy. We also compare average testing time per instance in Table 1, and our pro-posed method is much more efficient as we only evaluate K ( x , x i ) for all x i in the same cluster as x , thus reducing the testing time from O ( | S | d ) to O ( | S | d/k ) , where S is the set of support vectors.
 Refine solution before solving the whole problem. Be-fore training the final model at the top level using the whole dataset, we can refine the initialization by solving the SVM problem induced by all support vectors at the first level, i.e., level below the final level. As proved in Theorem 2, the support vectors of lower level models are likely to be the support vectors of the whole model, so this will give a more accurate solution, and only requires us to solve a problem with O ( |  X  S (1) | ) samples, where  X  S (1) is the set of support vectors at the first level. Our final algorithm is given in Algorithm 1.
 Algorithm 1 Divide and Conquer SVM Input : Training data { ( x i ,y i ) } n i =1 , balancing parameter Output : The SVM dual solution  X  . for l = l max ,..., 1 do end Refine solution: Compute  X  (0) by solving SVM on { x i |  X  Solve SVM on the whole data using  X  (0) as the initial point; We now compare our proposed algorithm with other SVM solvers. All the experiments are conducted on an In-tel 2.66GHz CPU with 8G RAM. We use 7 bench-mark datasets as shown in Table 3. The cifar dataset can be downloaded from http://www.cs.toronto. edu/  X kriz/cifar.html , and other datasets can be downloaded from http://www.csie.ntu.edu.tw/  X cjlin/libsvmtools/datasets or the UCI data repository. We use the raw data without scaling for two image datasets cifar and mnist8m , while features in all the other datasets are linearly scaled to [0 , 1] . mnist8m is a digital recognition dataset with 10 numbers, so we follow the procedure in (Zhang et al., 2012) to transform it into a binary classification problem by classifying round digits and non-round digits. Similarly, we transform cifar into a binary classification problem by classifying animals and non-animals. We use a random 80%-20% split for cov-type , webspam , kddcup99 , a random 8M/0.1M split for mnist8m (used in the original paper (Loosli et al., 2007)), and the original training/testing split for ijcnn1 and cifar . Competing Methods: We include the following ex-act SVM solvers (LIBSVM, CascadeSVM), approximate SVM solvers (SpSVM, LLSVM, FastFood, LTPU), and on-line SVM (LaSVM) in our comparison: 1. LIBSVM: the implementation in the LIBSVM library 2. Cascade SVM: we implement cascade SVM (Graf 3. SpSVM: Greedy basis selection for nonlinear SVM 4. LLSVM: improved Nystr  X  om method for nonlinear 5. FastFood: use random Fourier features to approximate 6. LTPU: Locally-Tuned Processing Units proposed in 7. LaSVM: An online algorithm proposed in (Bordes 8. DC-SVM: our proposed method for solving the ex-9. DC-SVM (early): our proposed method with the early (Zhang et al., 2012) reported that LLSVM outperforms Core Vector Machines (Tsang et al., 2005) and the bun-dle method (Smola et al., 2007), so we omit those compar-isons here. We apply LIBSVM/LIBLINEAR as the default solver for DC-SVM, FastFood, Cascade SVM, LLSVM and LTPU, so the shrinking heuristic is automatically used in the experiments.
 Parameter Setting: We first consider the RBF kernel K ( x i , x j ) = exp(  X   X  k x i  X  x j k 2 2 ) . We chose the balanc-ing parameter C and kernel parameter  X  by 5-fold cross validation on a grid of points: C = [2  X  10 , 2  X  9 ,..., 2 and  X  = [2  X  10 ,..., 2 10 ] for ijcnn1 , census , covtype , webspam , and kddcup99 . The average distance between samples for un-scaled image datasets mnist8m and cifar is much larger than other datasets, so we test them on smaller  X   X  X :  X  = [2  X  30 , 2  X  29 ,..., 2  X  10 ] . Regarding the parame-ters for DC-SVM, we use 5 levels ( l max = 4 ) and k = 4 , so the five levels have 1 , 4 , 16 , 64 and 256 clusters respec-tively. For DC-SVM (early), we stop at the level with 64 clusters. The following are parameter settings for other methods in Table 2: the rank is set to be 3000 in LLSVM; number of Fourier features is 3000 in Fastfood 1 ; number of clusters is 3000 in LTPU; number of basis vectors is 200 in SpSVM; the tolerance in the stopping condition for LIB-SVM and DC-SVM is set to 10  X  3 (the default setting of LIBSVM); for LaSVM we set the number of passes to be 1; for CascadeSVM we output the results after the first round. Experimental Results with RBF kernel: Table 2 presents time taken and test accuracies. Experimental re-sults show that the early prediction approach in DC-SVM achieves near-optimal test performance. By going to the top level (handling the whole problem), DC-SVM achieves better test performance but needs more time. Table 2 only gives the comparison on one setting; it is natural to ask, for example, about the performance of LIBSVM with a looser stopping condition, or Fastfood with varied num-ber of Fourier features. Therefore, for each algorithm we change the parameter settings and present the detailed ex-perimental results in Figure 3 and Figure 5 in Appendix. Figure 3 shows convergence results with time  X  in 3a, 3b, 3c the relative error on the y-axis is defined as ( f (  X  )  X  f (  X   X  )) / | f (  X   X  ) | , where  X   X  is computed by running LIB-SVM with 10  X  8 accuracy. Online and approximate solvers are not included in this comparison as they do not solve Table 4: Total time for DC-SVM, DC-SVM (early) and LIB-the exact kernel SVM problem. We observe that DC-SVM achieves faster convergence in objective function compared with the state-of-the-art exact SVM solvers. Moreover, DC-SVM is also able to achieve superior test accuracy in lesser training time as compared with approximate solvers. Figure 3d, 3e, 3f compare the efficiency in achieving dif-ferent testing accuracies. We can see that DC-SVM consis-tently achieves more than 50 fold speedup while achieving the same test accuracy with LIBSVM.
 Experimental Results with varying values of C, X  : As shown in Theorem 1 the quality of approximation depends on D (  X  ) , which is strongly related to the kernel parame-ters. In the RBF kernel, when  X  is large, a large portion of kernel entries will be close to 0, and D (  X  ) will be small so that  X   X  is a good initial point for the top level. On the other hand, when  X  is small,  X   X  may not be close to the optimal solution. To test the performance of DC-SVM under dif-ferent parameters, we conduct the comparison on a wide range of parameters ( C = [2  X  10 , 2  X  6 , 2 1 , 2 6 [2 type , webspam and census datasets are shown in Tables 7, 8, 9, 10 (in the appendix). We observe that even when  X  is small, DC-SVM is still 1-2 times faster than LIBSVM: among all the 100 settings, DC-SVM is faster on 96/100 settings. The reason is that even when  X   X  is not so close to  X  , using  X   X  as the initial point is still better than initialization with a random or zero vector. On the other hand, DC-SVM (early) is extremely fast, and achieves almost the same or even better accuracy when  X  is small (as it uses an approxi-mated kernel). In Figure 6, 8, 7, 9 we plot the performance of DC-SVM and LIBSVM under various C and  X  values, the results indicate that DC-SVM (early) is more robust to parameters. Note that DC-SVM (early) can be viewed as solving SVM with a different kernel  X  K , which focuses on  X  X ithin-cluster X  information, and there is no reason to be-lieve that the global kernel K always yields better test ac-curracy than  X  K . The accumulated runtimes are shown in Table 4.
 Experimental Results with polynomial kernel: To show that DC-SVM is efficient for different types of kernels, we further conduct experiments on covtype and webspam datasets for the degree-3 polynomial kernel K ( x i , x j (  X  +  X  x T i x j ) 3 . For the polynomial kernel, the parame-ters chosen by cross validation are C = 2 , X  = 1 for cov-type , and C = 8 , X  = 16 for webspam . We set  X  = 0 , which is the default setting in LIBSVM. Figures 4a and 4c compare the training speed of DC-SVM and LIBSVM for reducing the objective function value and Figures 4b and 4d show the testing accuracy compared with LIB-SVM and LaSVM. Since LLSVM, FastFood and LPTU are developed for shift-invariant kernels, we do not include them in our comparison. We can see that when using the polynomial kernel, our algorithm is more than 100 times faster than LIBSVM and LaSVM. One main reason for such large improvement is that it is hard for LIBSVM and LaSVM to identify the right set of support vectors when us-ing the polynomial kernel. As shown in Figure 2, LIBSVM cannot identify 20% of the support vectors in 10 5 seconds, while DC-SVM has a very good guess of the support vec-tors even at the bottom level, where number of clusters is 256. In Appendix 8.5 we show that the clustering step only takes a small portion of the time taken by DC-SVM. In this paper, we have proposed a novel divide and con-quer algorithm for solving kernel SVMs (DC-SVM). Our algorithm divides the problem into smaller subproblems that can be solved independently and efficiently. We show that the subproblem solutions are close to that of the orig-inal problem, which motivates us to  X  X lue X  solutions from subproblems in order to efficiently solve the original ker-nel SVM problem. Using this, we also incorporate an early prediction strategy into our algorithm. We report ex-tensive experiments to demonstrate that DC-SVM signifi-cantly outperforms state-of-the-art exact and approximate solvers for nonlinear kernel SVM on large-scale datasets. The code for DC-SVM is available at http://www.cs. utexas.edu/  X cjhsieh/dcsvm . This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H also acknowledges support from an IBM PhD fellowship.
 Boley, D. and Cao, D. Training support vector machine using adaptive clustering. In SDM , 2004.
 Bordes, A., Ertekin, S., Weston, J., and Bottou, L. Fast kernel classifiers with online and active learning. JMLR , 6:1579 X 1619, 2005.
 Chang, Chih-Chung and Lin, Chih-Jen. LIBSVM: A li-brary for support vector machines. ACM Transactions on Intelligent Systems and Technology , 2:27:1 X 27:27, 2011. Collobert, R., Bengio, S., and Bengio, Y. A parallel mixture of SVMs for very large scale problems. In NIPS , 2002. Cortes, C. and Vapnik, V. Support-vector networks. Ma-chine Learning , 20:273 X 297, 1995.
 Fine, S. and Scheinberg, K. Efficient SVM training us-ing low-rank kernel representations. JMLR , 2:243 X 264, 2001.
 Ghitta, Radha, Jin, Rong, Havens, Timothy C., and Jain,
Anil K. Approximate kernel k-means: Solution to large scale kernel clustering. In KDD , 2011.
 Graf, H. P., Cosatto, E., Bottou, L., Dundanovic, I., and
Vapnik, V. Parallel support vector machines: The cas-cade SVM. In NIPS , 2005.
 Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton,
G. E. Adaptive mixtures of local experts. Neural Com-putation , 3(1):79 X 87, 1991.
 Joachims, T. Making large-scale SVM learning practical.
In Advances in Kernel Methods  X  Support Vector Learn-ing , pp. 169 X 184, 1998.
 Joachims, T. Training linear SVMs in linear time. In KDD , 2006.
 Jose, C., Goyal, P., Aggrwal, P., and Varma, M. Local deep kernel learning for efficient non-linear SVM prediction. In ICML , 2013.
 Keerthi, S. S., Chapelle, O., and DeCoste, D. Building sup-port vector machines with reduced classifier complexity. JMLR , 7:1493 X 1515, 2006.
 Kugler, M., Kuroyanagi, S., Nugroho, A. S., and Iwata,
A. CombNET-III: a support vector machine based large scale classifier with probabilistic framework. IEICE Trans. Inf. and Syst. , 2006.
 Le, Q. V., Sarlos, T., and Smola, A. J. Fastfood  X  approx-imating kernel expansions in loglinear time. In ICML , 2013.
 Loosli, Ga  X  elle, Canu, St  X  ephane, and Bottou, L  X  eon. Train-ing invariant support vector machines using selective sampling. In Large Scale Kernel Machines , pp. 301 X  320. 2007.
 Menon, A. K. Large-scale support vector machines: algo-rithms and theory. Technical report, University of Cali-fornia, San Diego, 2009.
 Moody, John and Darken, Christian J. Fast learning in net-works of locally-tuned processing units. Neural Compu-tation , pp. 281 X 294, 1989.
 Pavlov, D., Chudova, D., and Smyth, P. Towards scalable support vector machines using squashing. In KDD , pp. 295 X 299, 2000.

Rodr  X   X guez, A. Double chunking for solving SVMs for very large datasets. In Proceedings of Learning , 2004. Platt, J. C. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods -Support Vector Learning , 1998.
 Smola, A., Vishwanathan, S., and Le, Q. Bundle methods for machine learning. NIPS , 2007.
 Tresp, V. A Bayesian committee machine. Neural Compu-tation , 12:2719 X 2741, 2000.
 Tsang, I.W., Kwok, J.T., and Cheung, P.M. Core vector machines: Fast SVM training on very large data sets. JMLR , 6:363 X 392, 2005.
 Wang, Z., Djuric, N., Crammer, K., and Vucetic, S.
Trading representability for scalability: Adaptive multi-hyperplane machine for nonlinear classification. In KDD , 2011.
 Yu, Hwanjo, Yang, Jiong, Han, Jiawei, and Li, Xiaolei.
Making SVMs scalable to large data sets using hierar-chical cluster indexing. Data Mining and Knowledge Discovery , 11(3):295 X 321, 2005.
 Zhang, K., Tsang, I. W., and Kwok, J. T. Improved Nystr  X  om low rank approximation and error analysis. In ICML , 2008.
 Zhang, K., Lan, L., Wang, Z., and Moerchen, F. Scaling up kernel SVM on limited resources: A low-rank lineariza-
