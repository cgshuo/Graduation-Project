 Single-iteration clarification dialogs, as implemented in the TREC HARD track, represent an attempt to introduce in-teraction into ad hoc retrieval, while preserving the many benefits of large-scale evaluations. Although previous ex-periments have not conclusively demonstrated performance gains resulting from such interactions, it is unclear whether these findings speak to the nature of clarification dialogs, or simply the limitations of current systems. To probe the limits of such interactions, we employed a human intermedi-ary to formulate clarification questions and exploit user re-sponses. In addition to establishing a plausible upper bound on performance, we were also able to induce an  X  X ntology of clarifications X  to characterize human behavior. This ontol-ogy, in turn, serves as the input to a regression model that attempts to determine which types of clarification questions are most helpful. Our work can serve to inform the design of interactive systems that initiate user dialogs. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search Process Measurement, Experimentation interactive retrieval, intermediated search, TREC HARD
The one-shot model of information retrieval operational-ized in TREC evaluations represents an oversimplification of real-world information-seeking behavior, and has often been criticized for neglecting the important role of interaction. Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. Indeed, there is empirical evidence that gains in one-shot retrieval performance, as measured by metrics such as mean average precision, do not necessarily translate into higher effectiveness in interactive settings [3, 11]. However, test collections created as a result of TREC evaluations repre-sent an affordable and repeatable way to measure one salient characteristic of retrieval systems. On the other hand, full-blown user studies, while potentially more insightful due to higher-degrees of realism, suffer from problems with afford-ability: the high-cost and time-consuming nature of such ex-periments limit the range of hypotheses that can be consid-ered, the speed at which variables can be explored, and the statistical significance of results. The TREC HARD tracks grew out of the recognition that there are other interesting regions in the solution space of evaluation design which en-code different tradeoffs between insightfulness, affordability, and repeatability of experiments.

The TREC HARD track [2] was originally conceived with a focus on three different ideas: richer specifications of infor-mation needs (i.e., context), finer-grained units of retrieval (i.e., passages), and limited interaction with the user (i.e., clarification dialogs). It was decided that clarification di-alogs represented the most promising avenue in which to advance the state of the art, and they were retained as the sole focus in 2005. Clarification dialogs represent an attempt to reduce both the scope and duration of user X  X ystem in-teractions to allow practical implementation in large-scale evaluations. Instead of arbitrarily complex interface con-trols, interactions were limited to what can be conveyed on an HTML page X  X heckboxes, text input forms, and the like. Instead of repeated iterations, interactions were limited to one single cycle and bounded at three minutes in duration. Due to these factors, the HARD track had a more complex evaluation cycle: participants first submitted a set of base-line runs, along with a set of HTML forms that constituted the clarification dialogs. NIST then presented these pages to assessors, who interacted with them accordingly. The results of these interactions, captured via the CGI protocol, were sent back to the participants, who then created final runs. The goal was to compare pre-and post-clarification runs to quantify the performance gains that can be attributed to user feedback.

Explorations of clarification dialogs have proven to be challenging because most previous experiments confounded the effects of the content of clarification forms, the manner in which they are generated, and the manner in which re-sponses are exploited. Previous results have shown little im-provement in terms of standard ranked-retrieval metrics [2]. Does this finding reveal fundamental limitations about clar-ification dialogs, or are present systems simply unable to effectively capitalize on such interactions?
This paper describes our experiments and subsequent anal-ysis for the TREC 2005 HARD track. To better understand the solution space of single-iteration clarification dialogs, we employed a trained intermediary to gather potentially rel-evant documents, construct clarification forms, and exploit user responses. We had three major goals: We have made headway toward achieving all of these goals. Our HARD experiments begin to quantify the added-value of having a human in the loop and the possible performance gains (Sections 3, 4, and 5). Through post-hoc analysis of clarification questions, we have discovered commonly-occurring patterns of intent, which serve as one possible ontology of clarifications (Section 6). Regression analysis reveals which types are most helpful. We also discuss how these insights can translate into system strategies for initi-ating and exploiting user X  X ystem dialogs (Section 7).
Information need negotiation in the context of a reference interview within a library setting is a complex communica-tion between a specialist and a user [20], which begins with the user describing his or her requirements. Through a series of interactions, both parties arrive at a better understanding of the information need.

Like information need negotiation in reference interviews, clarification dialogs aim at gaining a better understand-ing of the user X  X  requirements. However, the critical dif-ference is that the reference interview usually precedes the initial search process, whereas clarification dialogs in HARD occur after an initial search is performed (this setup is a methodological necessity in order to evaluate pre-and post-clarification performance). Thus, HARD clarification di-alogs involve search results, whereas, in most cases, little is known about actual documents during the reference inter-view. Pre-search information need negotiation is essential in reference transactions because it determines what resources to search, which depends, for example, on answer require-ments such as the format of expected results. This element of source selection has been eliminated in TREC.

As users become more accustomed to searching online electronic resources, the face X  X o X  X ace reference interview is gradually being replaced by other media: initially, the tele-phone, and now, email and online chat. HARD clarification dialogs share many formal properties with email reference interviews: both are asynchronous and depend primarily on text-based interfaces for soliciting user input. Hence, we can benefit from previous work on the email reference inter-view. Abels [1] identified five approaches often used in need negotiation over email: (1) piecemeal, (2) feedback, (3) bom-bardment, (4) assumption, and (5) systematic. Her analysis showed that the systematic approach was most successful and most efficient in terms of the number of messages ex-changed. In the systematic approach, the information spe-cialist responds to a request with a list of open-and closed-ended questions covering all aspects of the topic, arranged in a coherent, logical manner.
 The email interview provides a real-world grounding for HARD clarification dialogs. These interactions are not mere-ly stilted attempts at introducing elements of user studies in traditional ad hoc tasks, but represent realistic abstractions of information-seeking behavior. Given this understanding, we can recast the purpose of the HARD track as a means to better understand the systematic approach to asynchronous need negotiation so that such dialogs can be automatically conducted by a computer. While we acknowledge the work that has been done on analyzing real-time reference encoun-ters (e.g., [15, 17, 18, 23]), asynchronous clarification dialogs represent a qualitatively different form of interaction.
To establish an upper bound on the effectiveness of single-iteration clarification dialogs, we employed a trained inter-mediary 1 in all phases of our HARD experiments. This sec-tion describes our methodology for creating pre-and post-clarification runs.

The intermediary employed the  X  X uilding blocks X  strat-egy [8, 14] with INQUERY to gather relevant documents on behalf of the user (who is also the assessor; we use these two terms interchangeably). First, conceptual facets were identified from the topic statement and captured with a dis-junction of synonymous or related terms using INQUERY X  X   X  X oft X  OR operator. External tools such as Google and Wikipedia were used when appropriate (e.g., as a source for additional query terms). These facets were then system-atically combined into complete queries, most often with IN-QUERY X  X   X  X oft X  AND operator. Schematically, a building blocks query looks like:
The first ten or so hits were examined to determine if the query was  X  X ood X ; if not, the intermediary reformulated the query, taking advantage of additional terms that may have appeared in the top hits and INQUERY X  X  full range of query operators (hard boolean operators, proximity opera-tors, etc.). In addition to adding or replacing query terms basedonthetophits,thenegationoperatorwasfrequently used to disambiguate concepts. For most of the topics, the intermediary went through a few rounds of query reformu-lating until a  X  X ood query X  was constructed. Following that, between 50 and 120 documents in the resulting hit list were manually examined; the actual number depended on the dif-ficulty of the topic, the number of relevant hits, and other factors. Each examined document was assigned one of four judgments (cf. [18]):
A total of three pre-clarification runs were created au-tomatically using the relevance judgments provided by the intermediary. Based on tf.idf scores, 20 terms were selected from the documents marked centrally relevant. These terms were combined with terms from the topic title and topic de-scription using INQUERY X  X  weighted sum operator (weight of 3.0 for title terms, 1.0 for all others). This ranked list was submitted as run B2. Our main run, B1, consisted of CR, PR, and MR documents (in that order), followed by documents in B2 (with duplicates removed). Documents in each of the three piles were simply arranged in the order they were examined in the search process. As an automatic baseline (B3), we submitted an INQUERY run that used ti-tle and description terms as the query, with blind relevance feedback (top 20 tf.idf terms from top 10 hits).

We conceived of the clarification process as a reshuffling of documents between the four piles created by the interme-diary. Clarification questions were explicitly created with one of two goals:
Although a major goal of our research is the development of an  X  X ntology of clarifications X , we consciously adopted an inductive, bottom-up approach. Thus, the intermediary for-mulated questions as appropriate, without reference to any pre-existing ontologies, questions types, or stylized phrasing of questions (e.g., question templates). However, all ques-tions were constructed so that responses could be captured via checkboxes to ensure a consistent user interaction pat-tern. In addition to topic-specific questions, all clarification forms included two generic questions (located at the end of the form):  X  X ny additional search terms? X  and  X  X ny other comments? X  Both were followed by a 70  X  4 text box for free-formed input. As a complete example, Figure 1 shows the topic statement for topic 416  X  X hree Gorges Project X , along with the four clarification questions generated by our inter-mediary. In this example, the second question was targeted at PR documents, while the other questions were targeted at MR documents.

After receiving clarification responses from the user, our intermediary shuffled the document piles based on a more refined understanding of the information need. In addition, the intermediary performed another round of search for sev-eral difficult topics. New results were examined and rele-vant documents were added to CR pile. Creation of our main post-clarification run followed exactly the same pro-cedure as the creation of our pre-clarification run, except with updated piles (run C1). In addition, we submitted two contrastive conditions: C2 used title and description terms from the topic, along with search terms supplied by the user in the clarification forms. The run C3 included additional blind relevance feedback terms to the run C2.
We submitted a total of three pre-clarification and three post-clarification runs for the HARD track. Official results are shown in Table 1:  X  X edian X  is the mean of the per-topic median score of all submitted runs,  X  X est X  is the mean of the best per-topic score of all submitted runs, and  X  X est auto X  is the highest-scoring automatic run. In total, 30 pre-clarification and 92 post-clarification runs were submitted by all participants. For 29 topics (out of 50 total), the B1 pre-clarification run achieved the best mean average precision across all submitted runs; for R-precision, 28 topics. For 20 topics, the C1 post-clarification run achieved the best mean average precision across all submitted runs; for R-precision, 17 topics.

Our intermediary spent an average of 109 minutes per topic searching and assessing documents to create the pre-clarification document piles (max 170, min 35,  X  =29 . 7). This time included analyzing the topic statement, formu-lating a  X  X ood X  query, and performing the relevance assess-ment. For about half a dozen topics, the intermediary had difficulty generating a good query and finding relevant doc-uments; the advice of other team members was sought, but this time is not included in the figures mentioned above. We did not keep detailed time statistics for the process of exploiting clarification responses, but reassessing the docu-ments took approximately ten to thirty minutes per topic.
A total of 89 clarification questions was posed across all 50 topics (discounting the two generic questions present for every topic), for an average of 1.8 questions per topic ( 1 . 47). Topic 341  X  X irport security X  had the most clarifica-tion questions, with seven. Ten topics had no specific clar-ification questions: the intermediary found the topic state-ments to be straightforward. Disregarding the ten topics without specific clarification questions, the average number of questions per topic jumps to 2.2 (  X  =1 . 31).
For thirty-five of the topics, clarification responses in-cluded additional search terms supplied by the user. In fif-teen of the forms, clearly demarked phrases were entered. There was an average of 3.66 additional terms or phrases per topic (  X  =3 . 31),withamaximumoffourteen.

Looking at the difference between B1 and C1, it appears that clarification had only a small impact on MAP (+3.8%) and R-precision (+3.5%). A Wilcoxon signed-rank test re-veals that the difference in MAP is significant at the 1% level, while the difference in R-precision is significant at the 5% level. Figure 2 shows the effects of the clarification di-alog on mean average precision on a per-topic basis. Each pair of closely-spaced bars represents a single topic: the left bar represents the range of the median to best score before clarification; the right bar, after clarification. Boxes indicate the performance of B1 and C1, respectively. The rightmost set of bars represents an average across all topics.
The impact of user-supplied terms can be seen in the per-formance differences between runs B3 and C3, where ti-tle, description, and blind relevance feedback terms were expanded with user-supplied terms from the clarification forms. This resulted in a 4.4% improvement in MAP (sig-nificant at the 5% level).
A topic-by-topic analysis focused on runs B1 and C1 re-vealed ways in which the clarification dialogs helped or hurt. We arbitrarily divided topics into five bins, according to the relative differences between pre-and post-clarification MAP:  X   X  0 . 10, 0 . 05  X   X &lt; 0 . 10,  X  0 . 05  X   X &lt;  X  0 . 10  X   X &lt;  X  0 . 05, and  X &lt;  X  0 . 10. As can be seen in Table 2, eight topics fell in the last two bins, where clarifi-cation decreased MAP by at least 5%.

First, we narrowed our examination to topics for which there were clarification questions (forty topics). This is shown as testset A in Table 2. 2 Considering this reduced set of topics, we observe a gain of 4.3% in terms of MAP (significant at the 1% level). We then manually examined each topic in order to better understand ways in which the clarification dialog helped or hurt.

For many topics, it is easy to see why clarification dialogs improved performance. A better understanding of the user X  X  information need brings the intermediary X  X  relevance judg-ments more in sync with those of the user. The most dra-matic example of this is with topic 362  X  X uman smuggling X , where MAP jumped from 0.405 to 0.643, a gain of 59%. The topic called for reports about incidents of human smuggling for monetary gain. The clarification questions confirmed that the element of monetary gain must be present, and that summaries of smuggling rings and smuggling statistics were not relevant.

Somewhat distressing are seven topics in which the clari-fication dialog resulted in a decrease in MAP of at least 5%. For example, the mean average precision of topic 336  X  X lack bear attacks X  dropped 34% (from 0.466 to 0.309). To the clarification question  X  X oes a document need to mention fre-quency of attacks and cause of attacks and method of con-trol to be considered relevant? X , the assessor answered  X  X es X , indicating that documents with missing facets were not con-sidered relevant. However, analysis of the final qrels show that many documents missing the abovementioned facets were nevertheless marked relevant. In other words, the as-sessor X  X  answer to the clarification question did not match subset B). this the  X  X nconsistent user X  phenomenon.

In fact, examining all eleven topics where clarification di-alogs caused a drop in MAP revealed eight cases of the  X  X n-consistent user X  phenomenon. For these topics, the feedback received was misleading and contradicted the users X  rele-vance criteria as reflected in the final judgments. Results of removing these topics from testset A are shown in Table 2 as testset B. On these topics, clarification dialogs yielded an increase of 7.7% in mean average precision (significant at the 1% level). The table shows that topics in the worst-performing bin (MAP decrease greater than 10%) can all be attributed to this cause. In three of the eleven topics under consideration, the clarification dialog actually clouded the intermediary X  X  understanding of the user X  X  need, mainly due to poorly-formulated clarification questions. One question, for example, asked whether or not  X  X etails X  were necessary. This being a vague term, the intermediary and user ulti-mately had different notions of what  X  X etails X  meant.
What is the cause for this  X  X nconsistent user X  phenom-enon? Ruling out malicious intent, there are at least two possibilities: one points to a methodological flaw, and the other to the nature of information-seeking behavior itself. Due to real-world constraints involved in coordinating the HARD track, documents were not assessed until approxi-mately a month after the clarification questions had been answered (in order to allow ample time for participants to prepare their final runs). During this time, the assessors may have already forgotten their original answers: instabil-ity in relevance criteria over long periods of time could be the source of observed user inconsistencies. This is exacer-bated by the fact that this year X  X  topics did not represent  X  X eal X  information needs, since the topic statements were not constructed by the assessors themselves.

Research in information science, however, suggests that inconsistencies in users X  notions of relevance may be an in-escapable fact of real-world information-seeking behavior. The TREC evaluation methodology assumes a static infor-mation need against which documents are evaluated for rel-evance, when, in truth, information needs are themselves constantly shifting and evolving as users learn more about the subject [4, 20]. Therefore, it is entirely conceivable that the mere act of participating in the clarification dialog al-tered the users X  needs. Since our clarification questions were created based on documents assumed to be relevant by the intermediary, we are already circumscribing the bounds of the user X  X  relevance space. Most of our clarification ques-tions can be considered  X  X eading X , which may influence the assessor to respond in a calculated manner that runs counter the actual criteria used in the assessment! We have dubbed to the underlying need. Thus,  X  X eutral X  questioning is pre-ferred in reference interviews so that the questions posed do notleadtobiasedresponses[7].

In truth, inconsistencies in users X  notions of relevance are most likely caused by a combination of both factors de-scribed above. Unfortunately, the current HARD method-ology conflates these two issues. More carefully-constructed experiments must be conducted to better understand the shifting nature of information needs.
In the process of generating clarification questions, we no-ticed that a number of common patterns began to emerge, even though we did not impose any sort of pre-existing the-ory or ontology in a top-down manner. Analysis of our HARD results included an attempt to induce an  X  X ntology of clarifications X  in a bottom-up manner by observing simi-larities in the intent of clarification questions. This task was undertaken by the first author, who then manually coded all clarification questions according to the induced ontology.
As previously described, we view the clarification dialog as an opportunity to better understand the user X  X  informa-tion need so that peripherally relevant and maybe relevant documents can be sorted into either the centrally relevant or not relevant piles. Questions targeted at the peripherally relevant documents form a coherent class in terms of intent:
Other clarification questions fall naturally into five cat-egories, discussed below. An example of each is found in Table 3.
Returning to the example in Figure 1, the clarification questions would be classified as ACF, RT, RTA, CRC, re-spectively. The distribution of clarification types across all topics, as coded by the first author, is also shown in Table 3.
Given this ontology, can we determine the effectiveness of different clarification questions? We attempted to answer this question by constructing a linear regression model where the number of clarification questions in each category served as the independent variables (predictors) and the relative difference in MAP served as the dependent variable. We fixed the intercept of the regression model to zero, since asking no questions should yield no score difference. We used the 32 topics denoted as testset B in Table 2, which does not contain topics without clarification questions or topics that exhibited the  X  X nconsistent user phenomenon X .
Overall, our regression model was statistically significant, with an R 2 value of 0.66 (adjusted R 2 of 0.56). Regression coefficients for each variable are shown in Table 4, along with their p -values (for convenience, the frequency of each clari-fication question type is also shown). Because the number
Table 4: Regression based on clarification types. of observations (topics) is smaller than one would normally expect for this type of analysis, these results should be taken as indicative, not conclusive. Positive values for all regres-sion coefficients confirm our expectation that asking clari-fication questions correlates positively with increased MAP (to different degrees). Of all categories, AS (acceptability of summaries) and CRC (closely-related concepts) were found to be statistically significant predictors, and have the largest regression coefficients X  X eaning that they seemed to be the most helpful type of clarification questions to ask. How-ever, both question types combined account for less than 20% of all clarification questions. It is interesting that RT (relevance threshold) questions are only moderately helpful, despite their prevalence. This suggests that users X  relevance profiles are rather difficult to probe, and attempts to do so are of limited effectiveness.
Consistent with much previous work in library science, our results show that facet analysis can be a very effective tool for decomposing users X  information needs and can serve as the basis for a dialog strategy. In this section, we discuss how systems can operationalize these insights to facilitate richer user X  X ystem interactions.

Document clustering (e.g., [10, 13]) is one possible way in which facet analysis can be implemented. By noting thedistributionofterms,bothinthequeryandinthe resulting hits, it may be possible to automatically cate-gorize query terms into conceptually-related groups. This then provides an interaction opportunity for the user to ei-ther confirm or further manipulate results of the system X  X  analysis. Hearst [9] has shown that imposing simple con-straints between sub-topics (in a two-pass retrieval scheme) can have a beneficial impact on precision, and this technique represents one simple way to exploit user responses. In fur-ther support of this analysis, Buckley and Harman [5] have discovered that missing facets are a common failure mode of many retrieval systems. Clustering also provides a con-crete method for generating RTA (related topical aspects) questions X  X y identifying groups of documents that share terms both with the query and also with each other. In a way, such techniques as scatter X  X ather [10] can be viewed as a method for browsing related topic aspects.

In general, many types of clarification questions speak for the need to better model linguistic and ontological relations in queries and documents. For example, syntactic analysis can help determine if query terms should be related by con-junction or disjunction. Systems could make this decision directly if there is sufficient evidence, but there remains the opportunity to ask ACF (ambiguously conjoined facet) ques-tions. Conjunctions and disjunctions are merely examples of important linguistic relations that may be present in topic statements. Within the field of question answering, syntac-tic analysis has yielded significant improvements in perfor-mance [6], for example, differentiating between the killer of John Wilkes Booth from the man John Wilkes Booth killed (which have identical  X  X ag of words X  representations after stemming and dropping stopwords). In ad hoc retrieval, term relations often hold the key to high accuracy; for exam-ple, in the topic about  X  X uman smuggling X , the modification relationship between the two terms is critical, especially in a corpus that contains many articles about smuggling in gen-eral (term proximity is no substitute in this case).
Linguistic relations between query terms and terms in the result set are also important; two categories of clarification questions in our ontology (CRC and EC) specifically speak to such relations. Many of them are ontological in nature, and resources such as WordNet can be brought to bear to assist in the retrieval process. Although previous experi-ments with techniques such as query expansion using lexical semantic relations have yielded at best marginal improve-ments (e.g., [21]), they were attempted for the most part without human intervention. Placing the human in the loop has been shown to be an effective method for weeding out bad choices [12]. In a query expansion setting, linguistic and ontological relations can be employed to select candi-date terms, as opposed to using only term frequencies; such interactions are, in fact, examples of CRC and EC questions.
In summary, we believe that our ontology of clarifications canbeusedtoguidethedesignofinteractiveIRsystems.
 In the cases where similar techniques already exist, the on-tology provides a deeper explanation of why they work and how they can be improved.
What have we learned about the nature of interactive re-trieval and clarification dialogs through our human X  X n X  X he X  loop experiments? Somewhat to our relief, we demonstrated that human involvement significantly improves IR perfor-mance. This is by no means an obvious outcome, considering that  X  X anual runs X  in previous TREC evaluations were not considerably better than fully-automated runs (e.g., [22]). The crucial difference is the element of interaction X  X revious manual runs, for the most part, consisted of single-shot re-trieval with human-constructed queries. This setup is un-realistic in that an information-seeker does not attempt to optimize the result set from a single interaction, but rather culls relevant information from multiple iterations.
This work describes an effective strategy for leveraging human expertise and strengths of automated retrieval sys-tems in intermediated searching. Studies of search strate-gies using modern ranked-retrieval systems and how to best combine human and system results represent an underex-plored area in the IR literature, and our conception of need clarification as a process of creating and  X  X huffling X  piles is novel, as far as we are aware. We have gained a better un-derstanding of single-iteration clarification dialogs, and our experiments show that significant gains are possible, even when compared to a strong baseline. Further analysis re-vealed an interesting phenomenon where users X  responses to clarification questions do not appear to be consistent with their relevance judgments X  X ore work is required to assess the significance of this finding.

Finally, this paper provides insights into three impor-tant questions: Can clarification questions be classified into meaningful categories? Are some types more useful than others? How might a system initiate dialogs automatically? Although elicitations during face X  X o X  X ace reference encoun-ters have previously been analyzed and categorized [15, 17, 19, 23], asynchronous clarification dialogs are qualitatively different in nature. Furthermore, the setup of the HARD track allows us to isolate and focus on topical aspects of need negotiation; many of the previously cited works had to contend with categories of communications that did not relate directly to the information need, e.g., printer setup. Admittedly, some amount of realism is sacrificed, but in other ways, this work goes beyond previous studies in that we categorize the ways in which conceptual facets within an information need can be clarified. Finally, the tight coupling between the clarification dialog and system output allows us to quantitatively measure the effect of the interaction and understand the contributions of each question type.
Nevertheless, there are a number of unresolved issues re-lating to our clarification ontology worth mentioning. The categories presented here sprung from the mind of one sin-gle individual; given the same data, would another person come up with similar categories? Even with a fixed ontology, can humans reliably code questions? Even if the ontology is stable and questions can be reliably coded, are there other influencing factors, e.g., type of information need, domain, genre, etc.? For example, AS (acceptability of summaries) questions were found to be very useful, but the suitability of asking such questions would depend on the corpus (some collections have more summaries than others). In the same vein, there are perhaps natural associations between cat-egories of clarification questions and types of information needs. Results from this study supply the foundation for future work on the interaction between information seekers, intermediaries, and systems.
The primary purpose of this paper was to explore the limits of single-iteration clarification dialogs, which can be grounded in asynchronous need negotiation. By involving a human intermediary in our TREC 2005 HARD experi-ments, we determined a plausible upper bound on retrieval effectiveness and gained a better understanding of such in-teractions. Furthermore, this work has yielded an  X  X ntology of clarifications X , which can be leveraged to guide the devel-opment of future systems. We recognize the limitations of this present work, but are excited about the new frontiers in interactive information retrieval that have been identified.
This work has been supported in part by DARPA coop-erative agreement N66001-00-2-8910 and contract HR0011-06-2-0001 (GALE). We X  X  like to thank James Allan for orga-nizing the HARD track and Doug Oard for various engaging discussions. The first author would like to thank Esther and Kiri for their loving support. [1] E. Abels. The e-mail reference interview. RQ , [2] J. Allan. HARD track overview in TREC [3] J.Allan,B.Carterette,andJ.Lewis.Whenwill [4] M. Bates. The Berry-Picking search: User interface [5] C. Buckley and D. Harman. Reliable information [6] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua. [7] B. Dervin and P. Dewdney. Neutral questioning: A [8] S. Harter. Online Information Retrieval: Concepts, [9] M. Hearst. Improving full-text precision on short [10] M. Hearst and J. Pedersen. Reexaming the cluster [11] W. Hersh, A. Turpin, S. Price, B. Chan, D. Kramer, [12] J. Koenemann and N. Belkin. A case for interaction: [13] D. Lawrie, W. Croft, and A. Rosenberg. Finding topic [14] G. Marchionini. Information Seeking in Electronic [15] R. Nordlie.  X  X ser revealment X  X  X  comparison of [16] T. Saracevic. Relevance: A review of and a framework [17] A. Spink, A. Goodrum, D. Robins, and M. Wu.
 [18] A. Spink and T. Saracevic. Interaction in information [19] K. Swigger. Questions in library and information [20] R. Taylor. The process of asking questions. American [21] E. Voorhees. Query expansion using lexical-semantic [22] E. Voorhees and D. Harman. Overview of the Eighth [23] M. White. Questions in reference interviews. J. of
