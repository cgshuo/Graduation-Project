 The objective of this paper is to extend a Malay lexicon with count classifier information for nomi-nal types. This is done under the umbrella of deep lexical acquisition: the process of automatically or semi-automatically learning linguistic structures for use in linguistically rich language resources such as precision grammars or wordnets (Baldwin, 2007).
One might call Malay a  X  X edium-density X  lan-guage: some NLP resources exist, but substantially fewer than those for English, and they tend to be of low complexity. Resources like the Web seem promising for bootstrapping further resources, aided in part by simple syntax and a Romanised ortho-graphic system. The vast size of the Web has been demonstrated to combat the data sparseness prob-lem, for example, in Lapata and Keller (2004).
We examine using a similar  X  X irst gloss X  strategy to Lapata and Keller (akin to  X  X irst sense X  in WSD, in this case, identifying the most basic surface form that a speaker would use to disambiguate between possible classes), where the Web is used a corpus to query a set of candidate surface forms, and the fre-quencies are used to disambiguate the lexical prop-erty. Due to the heterogeneity of the Web, we expect to observe a significant amount of blocking from In-donesian, a language with which Malay is some-what mutually intelligible (Gordon, 2005). Hence, we contrast this approach with observing the cues directly from a corpus strictly of Malay, as well as a corpus-based supervised machine learning approach which does not rely on a presupplied gloss. 2.1 Count Classifiers A count classifier ( CL ) is a noun that occurs in a specifier phrase with one of a set of (usually nu-meric) specifiers; the specifier phrase typically oc-curs in apposition or as a genitive modifier ( GEN ) to the head noun. In many languages, including many South-East Asian, East Asian, and African families, almost all nouns are uncountable and can only be counted through specifier phrases. A Malay exam-ple, where biji is the count classifier ( CL ) for fruit, is given in (1). (1) tiga
Semantically, a lexical entry for a noun will in-clude a default (sortal) count classifier which se-lects for a particular semantic property of the lemma. Usually this is a conceptual class (e.g. HUMAN or ANIMAL ) or a description of some relative dimen-sional property (e.g. FLAT or LONG -AND -THIN ).
Since each count classifier has a precise seman-tics, using a classifier other than the default can co-erce a given lemma into different semantics. For ex-ample, raja  X  X ing X  typically takes orang  X  X erson X  as a classifier, as in 2 orang raja  X 2 kings X , but can take on an animal reading with ekor  X  X nimal X  in 2 ekor raja  X 2 kingfishers X . An unintended classifier can lead to highly marked or infelicitious readings, such as #2 biji raja  X 2 (chess) kings X .

Most research on count classifiers tends to discuss generating a hierarchy or taxonomy of the classi-fiers available in a given language (e.g. Bond and Paik (1997) for Japanese and Korean, or Shirai et al. (2008) cross-linguistically) or using language-specific knowledge to predict tokens (e.g. Bond and Paik (2000)) or both (e.g. Sornlertlamvanich et al. (1994)). 2.2 Malay Data Little work has been done on NLP for Malay, how-ever, a stemmer (Adriani et al., 2007) and a prob-abilistic parser for Indonesian (Gusmita and Manu-rung, 2008) have been developed. The mutually in-telligibility suggests that Malay resources could pre-sumably be extended from these.
 In our experiments, we make use of a Malay X  English translation dictionary, KAMI (Quah et al., 2001), which annotates about 19K nominal lexical entries for count classifiers. To limit very low fre-quency entries, we cross-reference these with a cor-pus of 1.2M tokens of Malay text, described in Bald-win and Awab (2006). We further exclude the two non-sortal count classifiers that are attested as de-fault classifiers in the lexicon, as their distribution is heavily skewed and not lexicalised.

In all, 2764 simplex common nouns are attested at least once in the corpus data. We observe 2984 unique noun X  X o X  X efault classifier assignments. Pol-ysemy leads to an average of 1.08 count classifiers assigned to a given wordform. The most difficult exemplars to classify, and consequently the most in-teresting ones, correspond to the dispreferred count classifiers of the multi-class wordforms: direct as-signment and frequency thresholding was observed to perform poorly. Since this task is functionally equivalent to the subcat learning problem, strategies from that field might prove helpful (e.g. Korhonen (2002)).

The final distribution of the most frequent classes is as follows: Of the 49 classes, only four have a relative frequency greater than 3% of the types: orang for people, batang for long, thin objects, ekor for animals, and buah , the semantically empty classifier, for when no other classifiers are suitable (e.g. for abstract nouns); orang and buah account for almost 70% of the types. 3.1 Methodology Lapata and Keller (2004) look at a set of generation and analysis tasks in English, identify simple surface cues, and query a Web search engine to approximate those frequencies. They then use maximum likeli-hood estimation or a variety of normalisation meth-ods to choose an output.

For a given Malay noun, we attempt to select the default count classifier, which is a generation task under their framework, and semantically most simi-lar to noun countability detection. Specifier phrases almost always premodify nouns in Malay, so the set of surface cues we chose was satu CL NOUN  X  X ne/a NOUN  X . 1 This was observed to have greater cov-erage than dua  X  X wo X  and other non-numeral spec-ifiers. 49 queries were performed for each head-word, and maximum likelihood estimation was used to select the predicted classifier (i.e. taking most fre-quently observed cue, with a threshold of 0). Fre-quencies from the same cues were also obtained from the corpus of Baldwin and Awab (2006).

We contrasted this with a machine learning model for Malay classifiers, designed to be language-independent (Nicholson and Baldwin, 2008). A fea-ture vector is constructed for each headword by con-catenating context windows of four tokens to the left and right of each instance of the headword in the cor-pus (for eight word unigram features per instance). These are then passed into two kinds of maximum entropy model: one conditioned on all 49 classes, and one cascaded into a suite of 49 separate binary classifiers designed to predict each class separately. Evaluation is via 10-fold stratified cross-validation. A majority class baseline was also examined, where every headword was assigned the orang class.
For the corpus-based methods, if the frequency of every cue is 0, no prediction of classifier is made. Similarly, the suite can predict a negative assign-ment for each of the 49 classes. Consequently, pre-cision is calculated as the fraction of correctly pre-dicted instances to the number of examplars where a prediction was made. Only the suite of classifiers could natively handle multi-assignment of classes: recall was calculated as the fraction of correctly pre-dicted instances to all 2984 possible headword X  X lass assignments, despite the fact that four of the systems could not make 220 of the classifications. 3.2 Results The observed precision, recall, and F-scores of the various systems are shown in Table 1. The best F-score is observed for the Web frequency system, which also had the highest recall. The best precision was observed for the corpus frequency system, but with very low recall  X  about 85% of the wordforms could not be assigned to a class (the corresponding figure for the Web system was about 9%). Conse-quently, we attempted a number of back-off strate-gies so as to improve the recall of this system.
The results for backing off the corpus frequency system to the Web model, the two maximum entropy models, and two baselines (the majority class, and the semantically empty classifier) are shown in Ta-ble 2. Using a Web back-off was nearly identical to the basic Web system: most of the correct assign-ments being made by the corpus frequency system were also being captured through Web frequencies, which indicates that these are the easier, high fre-quency entries. Backing off to the machine learn-ing models performed the same or slightly better than using the machine learning model by itself. It therefore seems that the most balanced corpus-based model should take this approach.

The fact that the Web frequency system had the best performance belies the  X  X oisiness X  of the Web, in that one expects to observe errors caused by carelessness, laziness (e.g. using buah despite a more specific classifier being available), or noise (e.g. Indonesian count classifier attestation; more on this below). While the corpus of  X  X lean X , hand-constructed data did have a precision improvement over the Web system, the back-off demonstrates that it was not substantially better over those entries that could be classified from the corpus data. As with many classification tasks, the Web-based model notably outperformed the corpus-based mod-els when used to predict count classifiers of Malay noun types, particularly in recall. In a type-wise lex-icon, precision is probably the more salient evalua-tion metric, as recall is more meaningful on tokens, and a low-precision lexicon is often of little utility; the Web system had at least comparable precision for the entries able to be classified by the corpus-based systems.

We expected that the heterogeneity of the Web, particularly confusion caused by a preponderance of Indonesian, would cause performance to drop, but this was not the case. The Ethnologue estimates that there are more speakers of Indonesian than Malay (Gordon, 2005), and one would expect the Web dis-tribution to reflect this. Also, there are systematic differences in the way count classifiers are used in the two languages, despite the intelligibility; com-pare  X  X ive photographs X : lima keping foto in Malay and lima lembar foto, lima foto in Indonesian. While the use of count classifiers is obligatory in Malay, it is optional in Indonesian for lower reg-isters. Also, many classifiers that are available in Malay are not used in Indonesian, and the small set of Indonesian count classifiers that are not used in Malay do not form part of the query set, so no confu-sion results. Consequently, it seems that greater dif-ficulty would arise when attempting to predict count classifiers for Indonesian nouns, as their optional-ity and blocking from Malay cognates would intro-duce noise in cases where language identification has not been used to generate the corpus (like the Web)  X  hand-constructed corpora might be neces-sary in that case. Furthermore, the Web system ben-efits from a very simple surface form, namely se-CL NOUN : languages that permit floating quantifica-tion, like Japanese, or require classifiers for stative verb modification, like Thai, would need many more queries or lower-precision queries to capture most of the cues available from the corpus. We intend to ex-amine these phenomena in future work.

An important contrast is noted between the  X  X n-supervised X  methods of the corpus-frequency sys-tems and the  X  X upervised X  machine learning meth-ods. One presumed advantage of unsupervised sys-tems is the lack of pre-annotated training data re-quired. In this case, a comparable time investment by a lexicographer would be required to generate the set of surface forms for the corpus-frequency mod-els. The performance dictates that the glosses for the Web system give the most value for lexicographer input; however, for other languages or other lexical properties, generating a set of high-precision, high-recall glosses is often non-trivial. If the Web is not used, having both training data and high-precision, low-recall glosses is valuable. We examine an approach for using Web and cor-pus data to predict the preferred generation form for counting nouns in Malay, and observed greater pre-cision than machine learning methods that do not require a presupplied gloss. Most Web X  X s X  X orpus research tends to focus on English; as the Web in-creases in multilinguality, it becomes an important resource for medium-and low-density languages. This task was quite simple, with glosses amenable to Web approaches, and is promising for automatically extending the coverage of a Malay lexicon. How-ever, we expect that the Malay glosses will block readings of Indonesian classifiers, and classifiers in other languages will require different strategies; we intend to examine this in future work.
 Acknowledgements
