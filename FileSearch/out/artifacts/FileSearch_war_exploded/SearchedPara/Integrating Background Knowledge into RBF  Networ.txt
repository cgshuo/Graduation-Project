 Automatic text classification is a problem applied to natural language texts that assigns a document into one or more predefined categories, based on its content. This is typically accomplished by machine learning algorithms and involves models built on the top of category-labeled training data. With the growth of the Internet and advances of computer technology, more textual documents have been digitized and stored electronically, and digital libraries and encyclopedias have become the most valuable information resources. Recently, the US Library of Congress has started a project named World Digital Library, which aims to digitize and place on the Web significant primary materials from national libraries and other institutions worldwide. digital phenomenon, and it has been widely applied in many areas that include Web page indexing, document filtering and management, information security, business and marketing intelligence mining, customer survey and customer service automation. 
Over the years, a number of machine learni ng algorithms have been used in text classification problems [8]. Among them, na X ve Bayes [6], nearest neighbor [1], decision tree with boosting [7], Support Vector Machines (SVM) [4] are the most cited. As supervised learning algorithms, they all require some labeled training sets and in general, the quantity and quality of the training data have an important impact on their classification effectiveness. In this paper, a text classification model, based on and also integrates additional unlabeled background knowledge to aid in its classification task. In Section 2, an overview of RBF networks is provided. In Section 3, the proposed model is described, and its application on one benchmark email corpus and performance comparisons with two popular text classifiers are presented in Section 4. Some concluding remarks are provided in Section 5. As a popular feed-forward neural network paradigm, the radial basis function (RBF) networks have been applied in many fields in science and engineering which include telecommunication, signal and image processing, time-series modeling, control systems and computer visualization. The architecture of a typical RBF network is presented in Fig. 1. 
The network consists of three layers: an input layer, a hidden layer of nonlinear processing neurons and an output layer. The input to the network is a vector , 1  X   X   X  n x and the output of the network, , 1  X   X   X  m y is calculated according to where , j c j = 1, 2, ..., k are the RBF centers in the input space, ij w are the weights the distance between its associated center and the input to the network is computed. the distance, the stronger the activation. The most commonly used basis function is the Gaussian where  X  is the width parameter that controls the smoothness properties of the basis function. Finally, the output of the network is computed as a weighted sum of the hidden layer activations, as shown in Equation (1). The radial basis function (RBF) networks can be applied for text classification, and for a given document collection, they are used to build models that characterize principal correlations of terms and document in the collection. In this section, an RBF network based text classifier is described in terms of its structure and major components. 
The classifier is constructed by feature selection, document vector formulation and network training. First, it preprocesses input data by a rigorous feature selection simply a symbol in a document. For a given document collection, the dimensionality of feature space is generally very large. The procedure aims to reduce the input feature dimensionality and to remove irrelevant features from network training and deploying. It consists of two feature selection steps: unsupervised and supervised. In the first unsupervised selection step, features such as common words and words with very low and very high frequencies are eliminated from training data. Terms with a conducted on those retained features by their frequency distributions between document classes in a training data set. This supervised selection intends to further identify the features that distribute most differently between document classes and uses the well-known Information Gain [8] as the selection criterion. 
After feature selection, each document is encoded as a numerical vector of values of the selected features. More precisely, each vector component represents a combination of the local and global weights of a retained term and is computed by the log(tf)-idf weighting scheme [5]. 
RBF networks are typically trained by some nonlinear comprehensive algorithms that involve the entire networks. It can be very computationally intensive for large training data. Alternatively, it can be trained by a much less expensive two-stage training process [3]. More specifically, the first training stage determines the RBF centers and widths through some unsupervised algorithms. In our model, a variant of k -means algorithm is used in this stage. It is used to construct a representation of the density distribution in input training data space and is accomplished by clustering each of the document classes in a collection. It is noted that in many cases, documents in a class, though may vary in content, can likely be grouped by topics into a number of clusters where some underlying semantic term and document correlations are effectively the encoded content vectors representing the most important document features and subsequently, they summarize topics within the document classes. With the RBF network parameters being determined and fixed, the second training stage selects the weights for the network output layer. This is essentially a linear modeling problem and is solved by a logistic algorithm in our model. 
There are many text classification problems where unlabeled data are readily available and labeled data may be limited in quantity due to labeling cost or difficulty. For instance, labeling newsgroup articles by interest of a news reader can be quite tedious and time-consuming. As another example, categorization of Web pages into subclasses for a search engine is very desirable. However, given the exponential growth rate of the Web, only a tiny percentage of Web content can realistically be hand-labeled and classified. All of these problems require solutions that can learn accurate text classification through limited labeled training data and additional related unlabeled data (background knowledge). 
The two-stage training process described above particularly facilitates an integration of additional background unlabeled data. The data used in the first training stage, for determining the network basis functions of the hidden layer, are not required to be labeled and in fact, it is done by a clustering algorithm. However, some classification performance is generally achieved if the stage is carried out on a combined training data set that includes both labeled and unlabeled documents. Of course, some labeled data are needed in the second training stage. Overall, the available background unlabeled documents can be used to compensate for insufficient labeled training data and also to further im prove the quality of classification decisions of the proposed model. The model training process can be summarized as follows: 
Model Training Process 1. Select data features on all labeled and unlabeled training data 2. Construct training document content vectors that combine feature local and 3. Cluster labeled content vectors in each document class and background 4. Determine the network output layer weights by the logistic regression 
It should be pointed out that the proposed model that applies both feature selection and the two-stage training is effective in significantly reducing the computational workload for network training and hence, it provides a practical text classifier for applications with large training data. The proposed RBF network based text classification model has been applied to email spam filtering, a special and important two-category text classification problem. In this section, the experiments of the model on the benchmark spam testing corpus PU1 and its comparison with the popular SVM and na X ve Bayes approaches are described. 4.1 Experiment Settings The corpus PU1 [2] is made up of 1099 real email messages, with 618 legitimate and 481 spam. The messages in the corpus have been preprocessed with all attachments, HTML tags and header fields except subject being removed, and with all retained words being encoded into numbers for privacy protection. The experiments are partitioned into ten equally-sized subsets. Each experiment takes one subset for testing and the remaining nine subsets for training, and the process repeats ten times with each subset takes a turn for testing. The performance is then evaluated by averaging over the ten experiments. Various feature sets are used in the experiments ranging from 50 to 650 with an increment of 100. 
The performance of a text classifier can be evaluated by precision and recall. These measurements, however, do not take an unbalanced misclassification cost into consideration. Spam filtering is a cost sensitive learning process in the sense that misclassifying a legitimate message to spam is typically a more severe error than misclassifying a spam message to legitimate. In our experiments, a unified cost sensitive weighted accuracy [2] is used as the performance criterion and it can be defined as legitimate, incorrect legitimate, correct spam, and incorrect spam, respectively, and  X  is a cost parameter. The WAcc formula assumes that the misclassification error on popular  X  = 9 is used. 4.2 Classification Performance with and without Background Knowledge As discussed in Section 3, the proposed classification model is capable of incorporating both labeled and unlabeled data (background knowledge) in its learning effectively, and this can be particularly advantageous for the applications where labeled training data are in short supply. 
The first part of our experiments was to compare classification performance of the model with and without using background knowledge. The experiments were conducted with the training set being further divided into a labeled and an unlabeled subset. Those known email class labels for the data assigned to the unlabeled subset are ignored in the experiments. When the model is trained with background subset is used in the second stage of training. When the model is trained without background knowledge, only the labeled training subset is used in both stages of training. 
Fig. 2 summarizes the average weighted accuracy results (in y-axis) obtained by the model over various feature set sizes (in x-axis) and several combined varies from 900 to 400 with a decrement of 100 whereas the unlabeled training size n1 varies from 89 to 589 with a corresponding increment of 100, and the total combined size of n1 and n2 is 989. It can be observed from Fig. 2 that, as the feature size increases, a trend on accuracy starts with a decent initial value, dips at the size of background knowledge might not be very beneficial as expected. However, for relatively larger feature sets, background knowledge can help improve the quality of classification decisions, and that includes the cases where labeled training sets are relatively small. 4.3 Comparison with Other Text Classifiers The second part of our experiments was to compare the proposed model with two well-known classification algorithms: na X ve Bayes and SVM [9]. The former is a standard implementation of na X ve Bayes and the latter is an implementation of SVM using sequential minimization optimization and a polynomial kernel. Note that the selection and term weighting. 
The experiments also used the stratified 10-fold cross validation procedure. With a labeled training set of size 900, Fig. 3 compares SVM, na X ve Bayes (NB) with two versions of the RBF based model (RBF): one is trained only by a labeled set (marked as 0/900 in the aforementioned notation) and the other is trained by both the labeled set and an additional unlabeled background set of size 89 (marked as 89/900). This setting represents the model training using a relatively small background set and a relatively large labeled set. Clearly, Fig. 3 shows that background knowledge is useful for the RBF model to outperform other algorithms as the feature size gets large enough. 
We also looked at different situations for model training where background data performance comparison of these models on a small labeled training set of size 400. Note that the model RBF (598/400) uses an additional background set of size 598 in the training. Overall, Fig. 4 demonstrates the similar classification outcomes as in Fig. 3, except that SVM seems suffering more noticeably from the reduced labeled training data. Both Fig.3 and Fig.4 have indicated that, with some appropriate feature selection, the training with background knowledge can indeed be beneficial for the RBF model in learning the associations between a class and its constituent documents cases, na X ve Bayes performs very well and its weighted accuracy values stabilize after the feature size reaches 250. Likely, its performance is enhanced by our rigorous feature selection process. The radial basis function (RBF) networks have been successfully used in many applications in science and engineering. In this paper, an RBF-based text classification model is proposed. It is capable of integrating background knowledge into its learning and utilizing valuable discriminative information in training data. The experiments of the model on email spam classification and a comparison with some popular text classifiers have shown that it is very effective in learning to classify documents based on content and represents a competitive text classification approach. As future work, we plan to conduct experiments of the model with general text classification corpora and improve the accuracy and efficiency of the model by further exploring feature-document underlying associations. We also plan to perform some empirical work in comparing the mode l with other more closely related semi-supervised classification algorithms. 
