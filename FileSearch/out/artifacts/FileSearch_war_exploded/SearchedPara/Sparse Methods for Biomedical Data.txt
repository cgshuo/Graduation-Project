 Following recent technological revolutions, the investigation of massive biomedical data with growing scale, diversity, and complexity has taken a center stage in modern data analysis. Although complex, the underlying representations of many biomedical data are often sparse. For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a few genes are relevant to the dis-ease; a gene network is sparse since a regulatory pathway involves only a small number of genes; many biomedical sig-nals are sparse or compressible in the sense that they have concise representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. Sparse methods based on the ` 1 norm have attracted a great amount of research efforts in the past decade due to its sparsity-inducing prop-erty, convenient convexity, and strong theoretical guaran-tees. They have achieved great success in various applica-tions such as biomarker selection, biological network con-struction, and magnetic resonance imaging. In this paper, we review state-of-the-art sparse methods and their appli-cations to biomedical data.
 Sparse learning, structured sparsity, Gaussian graphical model, magnetic resonance imaging Recent technological revolutions have unleashed a torrent of biomedical data with growing scale, diversity, and com-plexity [24; 27; 77; 86; 101]. The wealth of data confronts scientists with an urgent need for new methods and tools that can intelligently and automatically extract useful infor-mation from data and synthesize knowledge [17; 32; 56; 74]. Although complex, the underlying representations of many real-world data are often sparse [32; 38; 41]. For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a small number of them are relevant to the disease; a gene network is sparse since a regulatory pathway involves only a small number of genes; the neural representation of sounds in the auditory cortex of unanesthetized animals is sparse, since the fraction of neurons active at a given instant is small; many biomed-ical signals have sparse representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. The last decade has witnessed a growing interest in the search for sparse representations of data.
 The quest for sparsity is further motivated for various rea-sons. First, sparse representations enhance the interpretabil-ity of the model. For example, in many biological applica-tions, the selection of genes or proteins which are related to the study, is crucial to facilitate the biological interpre-tation [18; 38]. In addition, the resulting gene/protein se-lection might enable a feasible biological validation with a reduced experimental cost. Second, sparseness is one way to measure the complexity of the learning model [84]. Regu-larization is commonly employed to penalize the complexity of a learning model and alleviate overfitting. Regularization based on the ` 0 norm maximizes sparseness, which, however, leads to an NP-hard problem. As a computationally efficient alternative, the ` 1 norm regularization, which also leads to a sparse model, is widely used in many areas including signal processing, statistics, and machine learning [13; 23; 52; 93; 98; 124; 127]. Finally, finding sparse representations has re-cently received increasing attention due to the current burst of research in Compressed Sensing (CS) [4; 6; 16; 25; 26; 102]. CS is a technique for acquiring and reconstructing a signal utilizing the prior knowledge that it is sparse or com-pressible. It encodes a large sparse signal using a relatively small number of linear measurements, and minimizing the ` norm in order to decode the signal. Recent theories [13; 14; 15; 16; 25] assert that one can recover certain signals and images from far fewer samples or measurements than traditional methods.
 In this paper, we review sparse methods for (1) incorpo-rating a priori knowledge on feature structures for feature selection, (2) constructing undirected Gaussian graphical models, and (3) parallel magnetic resonance imaging. Structured Feature Selection. Although sparse learn-ing models based on the ` 1 norm such as the Lasso [98] have achieved great success in many applications, they do not take the existing feature structure into consideration. Specifically, these models yield the same solution after ran-domly reshuffling the features. However, in many applica-tions, the features exhibit certain intrinsic structures, e.g., spatial or temporal smoothness, disjoint/overlapping groups, trees, and graphs [42; 45; 51; 65; 116]. The a priori struc-ture information may significantly improve the classifica-tion/regression performance and help identify the important features. For example, in the study of arrayCGH [99; 100], the features X  X he DNA copy numbers along the genome X  have the natural spatial order, and the fused Lasso, which incorporates the structure information using an extension of the ` 1 -norm, outperforms the Lasso in both classifica-tion and feature selection. In this paper, we review various structured sparse learning models including group Lasso, sparse group Lasso, overlapping group Lasso, tree Lasso, fused Lasso, and graph Lasso.
 Sparse Undirected Gaussian Graphical Models. Undi-rected graphical models explore the relationships among a set of random variables through their joint distribution. The estimation of undirected graphical models has applications in many domains, such as computer vision, biology, and medicine. An instance is the analysis of gene expression data. As shown in many biological studies, genes tend to work in groups based on their biological functions, and there exist some regulatory relationships between genes [19]. Such biological knowledge can be represented as a graph, where nodes are the genes, and edges describe the regulatory rela-tionships. Graphical models provide a useful tool for mod-eling these relationships, and can be used to explore gene activities. One of the popular graphical models is the Gaus-sian graphical model (GGM), which assumes the variables to be Gaussian distributed [5]. In GGM, the problem of learning a graph is equivalent to estimating the inverse of the covariance matrix (precision matrix), since the nonzero off-diagonal elements of the precision matrix represent edges in the graph [5]. In some applications, we need to esti-mate multiple related precision matrices. For example, in the modeling of brain networks for Alzheimer X  X  disease us-ing neuroimaging data [43], we want to estimate graphical models for three groups: normal controls (NC), patients of mild cognitive impairment (MCI), and Alzheimer X  X  patients (AD). These graphs are expected to share some common connections, but they are not identical. It is thus desirable to jointly estimate the three graphs. In this paper, we review sparse methods for estimating a single undirected graphical model and for estimating multiple related undirected graph-ical models and discuss their properties.
 Parallel Magnetic Resonance Imaging. Parallel imag-ing has been the single biggest innovation in magnetic reso-nance imaging in the last decade. It exploits the difference in sensitivities between individual coil elements in a receive array to reduce the number of gradient encodings required for imaging, and the increase in speed comes at a time when other approaches to acquisition time reduction were reach-ing engineering and human limits [59]. In the SENSE-type reconstruction approach, researchers have taken advantage of the sparsity promoting penalties (e.g., wavelets and total variations) to reduce the acquisition time while maintaining the image quality. Key components of sparse learning in-clude the estimation of the coil sensitivity profiles, the design of the sparsity promoting regularization, the development of the sampling pattern that takes advantage of sparse learn-ing, and the efficient optimization of the non-smooth inverse problem. In this paper, we review different components of sparse learning in magnetic resonance imaging.
 The rest of the paper is organized as follows. We review structured sparse learning for feature selection in Section 2. The estimation of sparse undirected Gaussian graphical mod-els is presented in Section 3. We discuss sparse learning in parallel magnetic resonance imaging in Section 4. Finally, we conclude the paper in Section 5. We are given a set of training samples { a i ,b i } a  X  R p denotes the p -dimensional features for the i -th sam-ple, and b i  X  R is its response (numeric for regression, and categorical for classification). In addition, we are given a feature structure, e.g., a group structure, a tree structure, or a graph structure, as part of the input data. We focus on a linear model h : R p  X  R with h ( a ) = x T a , where x  X  is the vector of model parameters. To fit the model with the training samples, we learn the model parameter vector x by solving the following optimization problem: where L ( x ) is a loss function,  X ( x ) is a regularization term encoding the prior knowledge on the input features, and  X  &gt; 0 is the regularization parameter controlling the trade-off between the loss L (  X  ) and the penalty  X (  X  ). The formulation in (1) can be applied for regression, classi-fication, and longitudinal data analysis: The regularization term  X ( x ) in (1) is commonly employed to penalize the complexity of a learning model and allevi-ate overfitting, e.g., the ` 2 -norm regularization used in ridge regression. However, the commonly used ` 2 -norm regular-ization leads to a dense model, i.e., almost all model param-eters in x are non-zero. To enhance the interpretability of the model, a sparse model is desired. One popular sparse model, known as the Lasso, is based on the ` 1 -norm penalty: The Lasso has been applied widely in many biomedical ap-plications [91; 94; 107; 111; 123]. In many applications, the features exhibit certain intrinsic structures, e.g., spatial or temporal smoothness, graphs, trees, and disjoint/overlapping groups. The a priori structure information may significantly improve the classification/regression performance and help to identify the important features. In many applications, the features form a natural group structure. For example, the voxels of the positron emission tomography (PET) images in the Alzheimer X  X  Disease study can be divided into a set of non-overlapping groups accord-ing to the brain regions [43]; in the multi-factor ANOVA source. The features selected by each algorithm are highlighted. problem, each factor may have several levels and can be represented using a group of dummy variables [117]. The selection of group structures has recently received increasing attention in the literature [3; 44; 45; 64; 78; 117; 120]. The pioneer work [117] focused on the non-overlapping group Lasso, i.e., the groups are disjoint. Assume the features are partitioned into k disjoint groups { G 1 ,  X  X  X  ,G k } . The group Lasso formulation uses the ` q, 1 -norm penalty on the model parameters: where k X k q is the ` q -norm with q &gt; 1 (most existing work focus on q = 2 or  X  ) [68], and w i is the weight for the i -th group. The group selection distinguishes the group Lasso from the Lasso which does not take group information into account and does not support group selection. The group Lasso has been applied for regression [55; 80; 117], classi-fication [78], joint covariate selection for grouped classifica-tion [85], and multi-task learning [2; 62; 89].
 The group Lasso does not perform feature selection within each feature group. For certain applications, it is desirable to perform simultaneous group selection and feature selec-tion. The sparse group Lasso (sgLasso) incorporates the strengthens from both Lasso and group Lasso, and it yields a solution with simultaneous between-and within-group sparsity [30; 87]. The sparse group Lasso penalty is based on a composition of the ` q, 1 -norm and the ` 1 -norm: where  X   X  [0 , 1], the first term controls the sparsity in the feature level, and the second term controls the sparsity in the group level. The sparse group Lasso has been applied to analyze multiple types of high dimensional genomic data for biomarker discovery [87].
 Figure 1 illustrates Lasso, group Lasso, and sparse group Lasso; we use four types of data sources including Pro-teomics, GWAS (genome-wide association study), MRI (mag-netic resonance imaging), and PET from the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) database 1 . We con-struct four feature groups, one for each each data source. As shown in the figure, the Lasso does not consider the group (source) information and selects a subset of features from all four groups; the group Lasso selects a subset of the groups http://www.adni-info.org/ (3 in this example) and all features from these 3 groups are selected; the sparse group Lasso simultaneously selects a subset of the groups and a subset of the features within each selected group. In group Lasso [117], the groups are disjoint. Some re-cent work [44; 45; 46; 51; 69; 120] studied the more gen-eral case where the groups may overlap. One motivating example is the use of biologically meaningful gene/protein sets (groups). The proteins/genes in the same groups are related if they either appear in the same pathway, or are semantically related in terms of Gene Ontology (GO) hi-erarchy, or are related from gene set enrichment analysis (GSEA) [97]. The canonical pathway in MSigDB, for ex-ample, has provided 639 groups of genes [97]. It has been shown that the group (of proteins/genes) markers are more reproducible than individual protein/gene markers and the use of such group information improves classification perfor-mance [19]. Groups may overlap -one protein/gene may be-long to multiple groups -and the group Lasso formulation is not applicable. For the general overlapping group patterns, we can make use of the following overlapping group Lasso penalty [120]: where  X   X  [0 , 1], w i &gt; 0 ( i = 1 , 2 ,...,k ), and G the indices from the i -th group of features. The k groups of features are pre-specified, and they may overlap. A different overlapping group Lasso formulation was proposed in [44]. In some applications, the features follow a tree structure. For example, an image can be represented using a tree struc-ture where each leaf node corresponds to a feature (pixel) and each internal node corresponds to a group of features (pixels) based on the spatial locality [69]. In such a case, we can make use of the tree structured group Lasso penalty [46; 51; 69; 120]: where w i j &gt; 0 is a constant weight, and G i j , a node at the depth i , consists of all features in the subtree. Note that any parent node is a superset of its children. Thus, if a specific node is not selected (i.e., its corresponding model coefficient is zero), then all its children will not be selected. It is clear that the tree structured group Lasso is a special case of the overlapping group Lasso with a specific tree structure. ( v ) of (8), the fused Lasso signal approximator. In many applications, the features enjoy certain smooth-ness properties. For example, the adjacent features in the arrayCGH data are close to each other along the genome. Therefore, it is desirable to enforce the model parameters in x to have the structure of smoothness. Such a structure can be induced by the fused Lasso penalty [28; 99]: where  X   X  [0 , 1]. The fused Lasso penalty in (7) shall induce a solution that x i tends to be close or identical to x i +1 i = 1 ,  X  X  X  ,p  X  1. The smoothness structure can also be revealed from the fused Lasso signal approximator [28]: Figure 2 illustrates the fused Lasso signal approximator (8) under different values of  X  with  X  = 0 . 5. We can observe that the solution is piecewise constant. In certain applications, the features form an undirected graph structure, in which two features connected by an edge in the graph are more likely to be selected together. As an exam-ple, many biological studies have suggested that genes tend to work in groups according to their biological functions, and there are some regulatory relationships between genes [60]. This biological knowledge can be represented as a graph, where the nodes represent the genes, and the edges imply the regulatory relationships between genes. Figure 3 shows a subgraph consisting of 80 nodes (genes) of the network de-scribed in [19]. Several recent studies have shown that the estimation accuracy can be improved using dependency in-formation encoded as a graph. Let ( N,E ) be a given graph, where N = { 1 , 2 ,  X  X  X  ,p } is a set of nodes, and E is a set of edges. Node i corresponds to the i -th feature. If nodes i and j are connected by an edge in E , then the i -th feature and the j -th feature tend to be grouped.
 The fussed Lasso penalty in (7) can be extended to a general graph structure; we call it the ` 1 graph Lasso: where the second regularization term penalizes a large de-viation between two model parameters whose correspond-ing nodes are connected in the graph. Intuitively, if two Figure 3: Illustration of a subgraph of the network consisting of 80 nodes. genes/proteins are connected in a network, their model pa-rameters are likely to be close to each other, satisfying the so-called smoothness property on a graph. The ` 1 graph Lasso formulation is computationally expensive to solve. The ` 2 graph Lasso, or the Laplacian Lasso, is an efficient alternative, which uses the following penalty: where L is the Laplacian matrix [7; 20] constructed from the graph. It is known that the Laplacian matrix is positive semi-definite, and captures the underlying local geometric structure of the data. When L is an identity matrix, (10) reduces to the elastic net penalty [126]. Existing efficient al-gorithms for solving the Lasso can be applied to solve the ` graph Lasso by grouping the loss term L ( x ) and the Lapla-cian regularization  X  (1  X   X  ) x T L x together, as the latter is both convex and differentiable.
 Both ` 1 and ` 2 graph Lasso encourage positive correlation between the values of coefficients for the features connected by an edge in the graph. However, in certain applications, two features connected may be negatively correlated. To overcome this limitation, GFlasso employs a different ` 1 ularization over a graph:
 X  GFlasso ( x ) =  X  k x k 1 + (1  X   X  ) X where r ij is the sample correlation between two features [50]. The penalty in (11) encourages the coefficients x i features i,j connected by an edge in the graph to be similar when r ij &gt; 0, but dissimilar when r ij &lt; 0. GFlasso would introduce additional estimation bias due to possible graph misspecification. For example, additional bias may occur when the sign of r ij is inaccurate.
 Another alternative is the so-called graph OSCAR (GOSCAR) penalty given by [110]:
 X  GOSCAR ( x ) =  X  k x k 1 + (1  X   X  ) X where a pairwise `  X  regularizer is used to encourage the co-efficients to be equal [9], but the grouping constraints are imposed on the nodes connected over the given graph. The ` regularizer encourages sparseness. The pairwise `  X  reg-ularizer puts more penalty on the larger coefficients. Note that max {| x i | , | x j |} can be decomposed as The GOSCAR formulation is closely related to OSCAR [9]. The penalty of OSCAR is
 X  OSCAR ( x ) v =  X  k x k 1 + (1  X   X  ) X The ` 1 regularizer leads to a sparse solution, and the ` ularizer encourages the coefficients to be equal. OSCAR can be efficiently solved by accelerated gradient methods, whose key projection can be solved by a simple iterative group merging algorithm [121]. However, OSCAR assumes each node is connected to all the other nodes, which is not suffi-cient for many applications. Note that OSCAR is a special case of GOSCAR when the graph is complete. GOSCAR, incorporating an arbitrary undirected graph, is much more challenging to solve [110].
 The penalty in GOSCAR overcomes the limitation of the Laplacian Lasso that the different signs of coefficients can introduce additional penalty. However, under the `  X  reg-ularizer, even if | x i | and | x j | are close to each other, the penalty on this pair may still be large due to the property of the max operator, resulting in the coefficient x i or x ing over penalized. The additional penalty would result in biased estimation, especially for large coefficients, as in the Lasso case [98]. In GFlasso, when the pairwise sample cor-relation wrongly estimates the sign between x i and x j , an additional penalty on x i and x j would occur, introducing estimation bias. This motivates the following non-convex feature grouping and selection penalty: which shrinks only small differences in absolutes values [110; 125]. As a result, estimation bias is reduced as compared to those convex grouping penalties. Note that the non-convex penalty does not assume the sign of an edge is given; it only relies on the graph structure. Undirected graphical models are commonly used to describe and explain the relationships among a set of variables based on a collection of observations. In the Gaussian case, the graphical Lasso [29] is a popular approach for learning the structure in an undirected Gaussian graphical model [5]. The basic model for continuous data assumes that the obser-vations have a multivariate Gaussian distribution with mean  X  and covariance matrix  X . If the ij th entry of  X  =  X   X  1 zero, then variables i and j are conditionally independent, given the other variables. Here,  X  is called the precision ma-trix. Thus, the problem of identifying the structure of the undirected Gaussian graphical model is equivalent to finding the nonzero entries of  X . In [5], the ` 1 penalty is imposed on the precision matrix to increase its sparsity. The sparse undirected graphical model has been applied to construct biological networks [5] and brain networks [43]. Suppose we have n samples independently drawn from a multivariate Gaussian distribution, and these samples are denoted as y 1 ,  X  X  X  , y n  X  N (  X ,  X ), where y i is a p dimen-sional vector,  X   X  R p is the mean, and  X   X  R p  X  p is the covariance matrix. Let  X  =  X   X  1 be the inverse covariance matrix. The empirical mean is denoted as  X   X  = 1 n P n i =1 and the empirical covariance is denoted as S : It can be shown that under a multivariate Gaussian model, the maximum likelihood estimate of  X  =  X   X  1 can be ob-tained by solving the following maximization problem: where tr ( S  X ) is the trace of S  X , given by the summation of the diagonal entries of S  X . Assume that S is nonsingular. The maximum likelihood estimate of the inverse covariance  X  is  X  = S  X  1 . If the dimensionality is larger than the sample size, i.e., p &gt; n , S is singular. In such a case, regu-larization is commonly applied, and we estimate  X  =  X   X  1 by maximizing the following objective function: where J ( X ) is a penalty function. The graphical Lasso em-ploys the ` 1 penalty and solves the following optimization problem [5]: It is known that a larger value of  X  leads to a sparser  X  that fits the data less well, while a smaller value of  X  leads to a less sparse  X  that fits the data well. Thus, the choice of  X  is an important issue in practical application of the graphical Lasso [63; 79].
 Banerjee et al. [5] employed the interior point method to solve the optimization problem in (17). Friedman et al. [29] developed the graphical Lasso (GLasso) which applied the blockwise coordinate descent method to solve (17). The GLasso fails to converge with warm-starts. To resolve this issue, Mazumder and Hastie [76] proposed a new algorithm called DP-GLasso, each step of which is a box-constrained QP problem. The main challenge of estimating a sparse pre-cision matrix is its high computational complexity. Witten et al. [106] and Mazumder and Hastie [75] independently de-rived a screening rule, which dramatically reduced the com-putational cost especially for large regularization parameter values. Huang et al. [43] derived the monotone property of the graphical Lasso. We first introduce the following definition.
Definition 1. In the graphical representation of the in-verse covariance, if node i is connected to node j by an arc, then node i is called a  X  X eighbor X  of node j . If node i is connected to node k though some chain of arcs, then node i is called a  X  X onnectivity component X  of node k .
 Intuitively, two nodes are neighbors if they are directly con-nected, whereas two nodes belong to the same connectivity component if they are indirectly connected, i.e., the con-nection is mediated through other nodes. In other words, if two nodes do not belong to the same connectivity com-ponent (i.e., two nodes completely separated in the graph), then they are completely independent of each other. Huang et al. [43] showed that the connectivity components have the following monotone property:
Proposition 1. Let C k (  X  1 ) and C k (  X  2 ) be the sets of all the connectivity components of node k with  X  =  X  1 and  X  =  X  , respectively. If  X  1 &lt;  X  2 , then C k (  X  2 )  X  C k Intuitively, if two nodes are connected (either directly or indirectly) at one level of sparseness, they will be connected at all lower levels of sparseness. This monotone property can be used to identify how strongly connected each node k is to its connectivity components [43]. In some applications, we need to estimate multiple related precision matrices. A motivating example is the modeling of brain networks for Alzheimer X  X  disease using neuroimaging data such as PET, in which, we want to estimate graphical models for three groups: normal controls (NC), patients of mild cognitive impairment (MCI), and Alzheimer X  X  patients (AD). These graphs are expected to share some common connections, but they are not identical. Furthermore, the graphs are expected to evolve over time, in the order of dis-ease severity from NC to MCI to AD. Estimating the graphi-cal models separately fails to exploit the common structures among them. It is thus beneficial to jointly estimate the three graphs, especially when the number of subjects in each group is small. There is some recent work on the estimation of multiple precision matrices. Guo et al. [36] proposed to jointly estimate multiple graphical models using a hierarchi-cal penalty. The time-varying graphical models were stud-ied by Zhu et al. [122], and Kolar et al. [53; 54]. Danaher et al. [22] estimated multiple precision matrices simultaneously using a pairwise fused penalty and grouping penalty. Assume we are given K data sets, X ( k )  X  R n k  X  p , k = 1 ,  X  X  X  ,K with K  X  2, where n k is the number of samples of the i th dataset, and p is the number of features. The p features are common for all K data sets, and all samples are independent. Furthermore, the samples within each data set X ( k ) are identically distributed with a p -variate Gaus-sian distribution with zero mean and covariance matrix  X  ( k ) We assume that there are many conditionally independent pairs of features, i.e., the precision matrix  X  ( k ) = ( X  sparse. Denote the sample covariance matrix for each data multiple precision matrices together by solving the following optimization problem [22; 109]: and  X  1 and  X  2 are nonnegative regularization parameters. The ` 1 regularization leads to a sparse solution, and the fused penalty encourages  X  ( k ) to be similar to its neighbors. The optimization in (18) is computationally expensive to solve. Danaher et al. [22] developed a screening rule for the two graph case to speed up the computation. The screening rule was recently extended to the more general case with more than two graphs in [109]. Specifically, Yang et al. [109] considered the problem of estimating multiple graphical models by maximizing a penalized log likelihood with ` 1 and fused regularization as in [22]. The ` larization yields a sparse solution, and the fused regulariza-tion encourages adjacent graphs to be similar. The block-wise coordinate descent method was employed to solve the fused multiple graphical Lasso (FMGL), where each step was solved by the accelerated gradient method [83]. In addition, a screening rule was developed which enabled the efficient estimation of multiple large precision matrices. Specifically, a set of necessary conditions were derived for the solution of FMGL to be block diagonal. These conditions were shown to be sufficient when K  X  3. Yang et al. also performed exten-sive simulation studies; results indicate that these conditions are likely sufficient for any K &gt; 3 as well. Magnetic resonance imaging (MRI) [39; 105] is a medical imaging technique used in radiology to visualize internal structures of the body in detail. As a non-invasive imag-ing technique, MRI makes use of the property of nuclear magnetic resonance to image nuclei of atoms inside the body. MRI has been applied to image the brain, muscles, the heart, cancers, etc. The acquired raw data by an MR scanner are the Fourier coefficients, or the so-called k -space data (see Figure 4 (a) for illustration). The k -space data are typically acquired by a series of phase encodings (each phase encoding cov-ers a given amount of k -space data that are related to the trajectory, e.g., Cartesian sampling, radial sampling). For example, with Cartesian sampling, we need 256 frequency encodings to cover the full k -space of one 256  X  256 image. The time between the repetitions of the sequence is called the repetition time (TR) and it measures the time for ac-quiring one phase encoding. If TR=50 ms, it takes about Figure 4: Illustration of MR image and the k -space data: (a) the full k -space data (displayed in logarithmic scale), (b) the image obtained by applying inverse Fourier transform to (a), (c) the undersampled k -space (displayed in logarith-mic scale), and (d) the image obtained by applying inverse Fourier transform to (c). 12.8 seconds to acquire the full k -space data of one 256  X  256 image with the Cartesian trajectory. With the same TR, it takes about 15.4 minutes to acquire the full k -space of a 256  X  256  X  72 volume. With higher spatial resolution, the time for acquiring the full k -space can be even longer. In addition, in dynamic cine imaging, we are interested in the study of the motion of the object (heart, blood, etc) over time. This leads to an increased number of phase encodings and increased acquisition time, and one usually has to com-promise between spatial resolution and temporal resolution. To save the acquisition time, one has to undersample the k -space, i.e., reducing the number of acquired phase encodings. For example, if the k -space data are acquired every other line, as shown in Figure 4 (c), half of the acquisition time can be saved. The relationship between the acquired k -space data and the image to be reconstructed can be written as where F u is a given undersampled Fourier transform opera-tor, f denotes the MR image, y is the acquired k -space data, and n depicts the noise introduced in the acquisition. Unlike the full k -space scenario, one cannot directly apply the in-verse Fourier transform to the undersampled data acquired in Figure 4 (c), since otherwise an aliased image shown in Figure 4 (d) will be obtained. Parallel imaging [34; 47; 88; 95] has been proven effective for reducing the acquisition time. It exploits the difference in sensitivities between individual coil elements in a receive array to reduce the number of gradient encodings required Figure 5: Illustration of the coil images and the coil sensi-tivity profiles (coil images of 8 channels are shown in the first two rows, and the corresponding coil profiles are shown in the last two rows). for imaging. Figure 5 illustrates parallel imaging with 8 coils. Specifically, the first two rows show the coil images seen by the individual coil/channel, and the last two rows show the coil profiles of these 8 coils. It can be observed that the 8 coils have different sensitivities. Parallel imaging tries to reconstruct the target image with the undersampled k -space data.
 Based on how the coil sensitivities are used, parallel imag-ing can be roughly divided into the following two main categories: 1) the approaches that implicitly make use of the coil sensitivities, represented by GRAPPA [34], and 2) the approaches that explicitly make use of the coil sensi-tivities, represented by SENSE [88]. In the GRAPPA type approaches, one usually estimates the missing phase encod-ing lines with the kernels that are estimated by implicitly using the coil sensitivities. In the SENSE type approach, one models the relationship between the target image and the acquired k -space data as: where y i is the acquired undersampled k -space data by the i -th coil, and S i is the coil sensitivity maps (see the last two rows of Figure 5). The relationships between GRAPPA and SENSE have been studied in the literature [8; 35; 47], and several recent work [57; 58; 72; 73] have shown that GRAPPA and SENSE can be combined to give improved reconstruction performance. The most common way to determine the sensitivity maps is to obtain low-resolution pre-scans. However, when the object is not static, the sensitivity functions are different between pre-scan and under-sampled scans, and this could lead to reconstruction errors. To compensate for this, joint estimation approaches [103; 113] have been proposed. How-Figure 6: Illustration of the gradient of the phantom (shown in Figure 4) along the vertical direction (left) and horizonal direction (right), respectively. ever, these approaches usually have high computation cost and are restricted to the SENSE type reconstruction. The eigen-vector approach proposed in [72] is a very promis-ing approach for sensitivity maps estimation. It tried to build a connection between GRAPPA and SENSE-type ap-proaches, by showing that the Coil Profile used in SENSE can be computed with the GRAPPA-type calibration. Such idea was also used in [57; 58]. It was shown in [72] that the coil sensitivities can be computed as the eigen-vector of a given matrix in the image space corresponding to eigenval-ues  X 1 X  X . Cartesian sampling is the most natural scheme which under-samples the k -space by skipping some lines. In cardiac MR imaging, TSENSE [37; 48] is a well-known approach that is based on time interleaving of k-space lines in sequential images, and there are studies that makes use of variable den-sity to optimize the sampling scheme, e.g., [12]. The Fourier transform associated with the Cartesian sampling can be efficiently computed.
 Spiral and projection (radial) are the most widely used non-Cartesian sampling patterns, among many others. It was observed in several works (e.g., [40]) that the radial sampling exhibits advantages over Cartesian Sampling. The Fourier transform in the non-Cartesian case is much more challeng-ing than the Cartesian one, and gridding is usually employed for performing Non-Uniform FFT [33]. To recover f from (19), it is important to note that our target f has certain structures, with which we can better reconstruct f from the undersampled data y . This is where sparse learning can play a role. Typically, we are interested in computing f by solving the following problem where loss( y ,F u f ) depicts the data fidelity, and  X  ( f ) incor-porates our prior knowledge about the image to be recon-structed.
 For the data fidelity term, a commonly used one is the squared distance between the acquired data and the pre-shown that the usage of self-consistency [57; 58; 73] can benefit reconstruction.
 For  X  ( f ), one needs to take advantage of the structure in the target image f . Figure 6 shows the gradient of the phantom, and it is easy to observe that such gradient is sparse. Cand`es et al. [14] proposed to set  X  ( f ) = k f k TV , showed the effec-tiveness of the sparsity promoting penalty in the scenario of single coil, and proved the exact recovery under the so-called Robust Uncertainty Principles (RIP). Later on, compressed sensing was used widely in the reconstruction of MR images, e.g., [1; 57; 61; 71; 112]. When applying sparse learning to parallel MR imaging, one key task is to develop a suitable  X  (  X  ) that adapts the structure of the image(s) to be recon-structed. Group sparsity [117] has been used for accelerating dynamic MRI [104], and total variation and wavelet trans-formation have also been used for parallel MR imaging [14; 66; 67; 90; 103; 112]. An important and hot research topic is to develop better sparsity promoting penalties that adapt to the images to be reconstructed.
 The efficient optimization of problem (21) is crucial for par-allel imaging. Several popular approaches include conju-gate gradient [40], Newton-type methods [103], Nesterov-type approaches [81; 82; 66; 49], and the alternating direc-tion method of multipliers [1; 10; 31; 112]. In this paper, we review sparse methods for biomedical data in three specific applications. Sparse methods have also been applied to many other applications, e.g., incomplete multi-source data fusion [114] and biological image annotation and retrieval [115]. As with many other data mining and ma-chine learning techniques, the selection of the appropriate sparse method and proper tuning of the associated parame-ters are critical for finding meaningful and useful results. To this end, one needs to understand the data in a domain spe-cific context and understand the strengths and weaknesses of various sparse methods.
 Most existing work on sparse learning focus on prediction, parameter estimation, and variable selection. Very few work address the problem of assigning statistical significance or confidence [11; 118]. However, such significance or confi-dence measures are crucial in biomedical applications where interpretation of parameters and variables is very impor-tant [11]. Most sparse methods in the literature are based on a convex regularizer. Sparse methods based on a non-convex regularizer have recently been proposed and efficient methods based on the difference of convex functions (DC) have been developed [92; 119]. However, their theoretical properties have not been well understood yet, although some recent work demonstrate the advantage of non-convex meth-ods over their convex counterparts [92; 108; 119]. Finally, missing data is ubiquitous in biomedical applications. One important issue that has not been well addressed is how to adapt sparse methods to deal with missing data [70; 96]. This work was supported in part by NSF (IIS-0953662, MCB-1026710, CCF-1025177) and NIH (R01LM010730). [1] M. Afonso, J. Bioucas-Dias, and M. Figueiredo. An [2] A. Argyriou, T. Evgeniou, and M. Pontil. Con-[3] F. R. Bach. Consistency of the group lasso and multi-[4] W. Bajwa, J. Haupt, A. Sayeed, and R. Nowak. Com-[5] O. Banerjee, L. El Ghaoui, and A. d X  X spremont. [6] R. Baraniuk. Compressive sensing. IEEE Signal Pro-[7] M. Belkin and P. Niyogi. Laplacian eigenmaps for di-[8] M. Blaimer, F. Breuer, M. Muller, R. Heidemann, [9] H. Bondell and B. Reich. Simultaneous regression [10] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eck-[11] P. B  X uhlmann. Statistical significance in high-[12] R. Busse, K. Wang, J. Holmes, J. Brittain, and F. Ko-[13] E. Cand`es and J. Romberg. Quantitative robust un-[14] E. Cand`es, J. Romberg, and T. Tao. Robust uncer-[15] E. Cand`es and T. Tao. Near optimal signal recovery [16] E. Cand`es and M. Wakin. An introduction to com-[17] S. Carroll, J. Grenier, and S. Weatherbee. From DNA [18] W. Chu, Z. Ghahramani, F. Falciani, and D. Wild. [19] H. Chuang, E. Lee, Y. Liu, D. Lee, and T. Ideker. [20] F. Chung. Spectral Graph Theory . American Mathe-[21] D. Cox. Regression models and life-tables. Journal of [22] P. Danaher, P. Wang, and D. Daniela. The joint graph-[23] A. d X  X spremont, L. El Ghaoui, M. Jordan, and [24] D. Donoho. High-dimensional data analysis: The [25] D. Donoho. Compressed sensing. IEEE Transactions [26] M. Duarte, M. Davenport, M. Wakin, and R. Bara-[27] J. Fan and J. Lv. A selective overview of variable se-[28] J. Friedman, T. Hastie, H. H  X ofling, and R. Tibshirani. [29] J. Friedman, T. Hastie, and R. Tibshirani. Sparse in-[30] J. Friedman, T. Hastie, and R. Tibshirani. A note on [31] T. Goldstein and S. Osher. The split bregman method [32] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, [33] L. Greengard and J. Lee. Accelerating the nonuni-[34] M. A. Griswold, P. M. Jakob, R. M. Heidemann, [35] M. A. Griswold, S. Kannengiesser, R. M. Heidemann, [36] J. Guo, E. Levina, G. Michailidis, and J. Zhu. Joint [37] M. A. Guttman, P. Kellman, A. J. Dick, R. J. Led-[38] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [39] E. M. Haacke, R. W. Brown, M. R. Thompson, and [40] M. S. Hansen, C. Baltes, J. Tsao, S. Kozerke, K. P. [41] T. Hrom  X adka, M. DeWeese, and A. Zador. Sparse rep-[42] J. Huang, T. Zhang, and D. Metaxas. Learning with [43] S. Huang, J. Li, L. Sun, J. Liu, T. Wu, K. Chen, [44] L. Jacob, G. Obozinski, and J. Vert. Group lasso with [45] R. Jenatton, J.-Y. Audibert, and F. Bach. Struc-[46] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. [47] P. Kellman. Parallel imaging: the basics. In ISMRM [48] P. Kellman, F. H. Epstein, and E. R. McVeigh. Adap-[49] K. Khare, C. J. Hardy, K. F. King, P. A. Turski, and [50] S. Kim and E. Xing. Statistical estimation of corre-[51] S. Kim and E. P. Xing. Tree-guided group lasso [52] K. Koh, S. Kim, and S. Boyd. An interior-point [53] M. Kolar, L. Song, A. Ahmed, and E. Xing. Esti-[54] M. Kolar and E. Xing. On time varying undirected [55] M. Kowalski. Sparse regression using mixed norms. [56] S. Kumar, K. Jayaraman, S. Panchanathan, R. Gu-[57] P. Lai, M. Lustig, B. A. C., V. S. S., B. P. J., and [58] P. Lai, M. Lustig, V. S. S., and B. A. C. ESPIRiT (effi-[59] D. J. Larkman and R. G. Nunes. Parallel magnetic [60] C. Li and H. Li. Network-constrained regularization [61] D. Liang, B. Liu, J. Wang, and L. Ying. Accelerat-[62] H. Liu, M. Palatucci, and J. Zhang. Blockwise co-[63] H. Liu, K. Roeder, and L. Wasserman. Stability ap-[64] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [65] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [66] J. Liu, J. Rapin, T. Chang, A. Lefebvre, M. Zenge, [67] J. Liu, J. Rapin, T. Chang, P. Schmitt, X. Bi, [68] J. Liu and J. Ye. Efficient ` 1 /` q norm regularization. [69] J. Liu and J. Ye. Moreau-Yosida regularization for [70] P. Loh and M. Wainwright. High-dimension regression [71] M. Lustig, D. L. Donoho, and J. M. Pauly. Sparse [72] M. Lustig, P. Lai, M. Murphy, S. Vasanawala, M. Elad, [73] M. Lustig and J. M. Pauly. SPIRiT: Iterative self-[74] M. Marton et al. Drug target validation and identi-[75] R. Mazumder and T. Hastie. Exact covariance thresh-[76] R. Mazumder and T. Hastie. The graphical lasso: [77] S. Megason and S. Fraser. Imaging in systems biology. [78] L. Meier, S. Geer, and P. B  X uhlmann. The group lasso [79] N. Meinshausen and P. B  X uhlmann. Stability selection. [80] S. Negahban and M. Wainwright. Joint support recov-[81] A. Nemirovski. Efficient methods in convex program-[82] Y. Nesterov. Introductory Lectures on Convex Opti-[83] Y. Nesterov. Gradient methods for minimizing com-[84] A. Ng. Feature selection, ` 1 vs. ` 2 regularization, and [85] G. Obozinski, B. Taskar, and M. I. Jordan. Joint co-[86] H. Peng. Bioimage informatics: a new area of engineer-[87] J. Peng, J. Zhu, B. A., W. Han, D.-Y. Noh, J. R. Pol-[88] K. Pruessmann, M. Weiger, M. Scheidegger, and [89] A. Quattoni, X. Carreras, M. Collins, and T. Dar-[90] S. Ramani and J. A. Fessler. Parallel MR image recon-[91] S. Ryali, K. Supekar, D. Abrams, and V. Menon. [92] X. Shen, W. Pan, and Y. Zhu. Likelihood-based se-[93] J. Shi, W. Yin, S. Osher, and P. Sajda. A fast algo-[94] W. Shi, K. Lee, and G. Wahba. Detecting disease-[95] D. Sodickson and W. Manning. Simultaneous acqui-[96] N. St  X adler and P. B  X uhlmann. Missing values: sparse [97] A. Subramanian et al. Gene set enrichment analysis: [98] R. Tibshirani. Regression shrinkage and selection via [99] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and [100] R. Tibshirani and P. Wang. Spatial smoothing and [101] P. Tomancak, A. Beaton, R. Weiszmann, E. Kwan, [102] A. Tropp, A. Gilbert, and M. Strauss. Algorithms for [103] M. Uecker, T. Hohage, K. T. Block, and J. Frahm. Im-[104] M. Usman, C. Prieto, T. Schaeffter, and P. G. [105] M. T. Vlaardingerbroek and J. A. Boer, editors. Mag-[106] D. Witten, J. Friedman, and N. Simon. New in-[107] T. Wu, Y. Chen, T. Hastie, E. Sobel, and K. Lange. [108] S. Xiang, X. Shen, and J. Ye. Efficient sparse group [109] S. Yang, Z. Pan, X. Shen, P. Wonka, and J. Ye. Fused [110] S. Yang, L. Yuan, Y.-C. Lai, X. Shen, P. Wonka, and [111] J. Ye, M. Farnum, E. Yang, R. Verbeeck, V. Lobanov, [112] X. Ye, Y. Chen, and F. Huang. Computational accel-[113] L. Ying and J. Sheng. Joint image reconstruction and [114] L. Yuan, Y. Wang, P. Thompson, V. Narayand, and [115] L. Yuan, A. Woodard, S. Ji, Y. Jiang, Z.-H. Zhou, [116] M. Yuan, V. R. Joseph, and H. Zou. Structured [117] M. Yuan and Y. Lin. Model selection and estimation [118] C.-H. Zhang and S. Zhang. Confidence intervals [119] T. Zhang. Analysis of multi-stage convex relaxation [120] P. Zhao, G. Rocha, and B. Yu. The composite abso-[121] L. Zhong and J. Kwok. Efficient sparse modeling with [122] S. Zhou, J. Lafferty, and L. Wasserman. Time varying [123] J. Zhu and T. Hastie. Classification of gene microar-[124] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-[125] Y. Zhu, X. Shen, and W. Pan. Simultaneous group-[126] H. Zou and T. Hastie. Regularization and variable se-[127] H. Zou, T. Hastie, and R. Tibshirani. Sparse princi-
