 We present a static index pruning method, to be used in ad-hoc document retrieval tasks, that follows a document-centric approach to decide whether a posting for a given term should remain in the index or not. The decision is made based on the term X  X  contribution to the document X  X  Kullback-Leibler divergence from the text collection X  X  global language model. Our technique can be used to decrease the size of the index by over 90%, at only a minor decrease in retrieval effectiveness. It thus allows us to make the index small enough to fit entirely into the main memory of a sin-gle PC, even for large text collections containing millions of documents. This results in great efficiency gains, superior to those of earlier pruning methods, and an average response time around 20 ms on the GOV2 document collection. H.2.4 [ Systems ]: Textual databases; H.3.1 [ Content Analysis and Indexing ]: Indexing methods Experimentation, Performance Information Retrieval, Index Pruning, KL Divergence
Fagin et al. [6] introduced the concept of static index prun-ing to information retrieval. In t heir paper, they describe a term-centric pruning method that, for each term T in the index, only retains its top k T postings, according to the in-dividual score impact that each posting would have if T ap-peared in an ad-hoc search query ( k T may be term-specific, not necessarily constant). By applying this method to an inverted index, its size can be reduced greatly. At query Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. time, this reduction results in improved query processing performance at the cost of a minor decrease in retrieval ef-fectiveness. Their method is static, as it is applied during index construction and is independent of any search queries.
In contrast to Fagin X  X  method, we present a document-centric pruning technique that, for each document D in the text collection, only keeps the postings for the top k D terms in that document (in general, k D will not be constant, but will depend on the document D ). We use D  X  X  Kullback-Leibler divergence from the rest of the collection, and in particular each term X  X  contribution to the document X  X  KL divergence, as a pruning criterion. Conceptually, for every document D in the collection, we perform a pseudo-relevance feedback step, based on Kullback-Leibler divergence scores (described by Carpineto et al. [7]) at indexing time and only keep postings for the top k D feedback terms extracted from that document in the index, discarding everything else. Be-cause pseudo-relevance feedback techniques are very good at finding the set of query terms, given the top search results, this method can be used to very accurately predict the set of queries for which D can make it into the top documents. Only terms appearing in those queries need to be kept in the index.

The resulting pruned index can then either be used stand-alone or in conjunction with the original, unpruned index. In the latter case, the pruned index acts as the primary index; the unpruned index is used as a secondary index and is only consulted when a query term cannot be found in the pruned index.

By using KLD-based document-centric index pruning to build a pruned index for the 10 6 most frequent terms in the GOV2 [8] text collection, it is possible to construct an index whose size is less than 10% of the size of the original index for the collection, but that still contains most of the information necessary to produce high-quality search results for most search queries. The pruned index is small enough to be loaded into memory and can be used to process the vast majority of all search queries. The original unpruned index, stored on disk because it is too large to fit into main memory, only needs to be accessed for queries involving very rare terms.

Following this general scheme, the average response time of the search engine can be decreased substantially, by about 87% in our experiments (from 190 ms down to 24 ms, when run on a single PC). At the same time, precision at 10 doc-uments only drops by 2.5% (from 0.6400 to 0.6240), and precision at 20 documents by 3.4% (from 0.5660 to 0.5470).
In the next section, we give a brief overview of related work. This is followed by a general description of the search engine and the baseline retrieval method used in our ex-periments. In section 4, we present our document-centric approach to index pruning, along with two possible instan-tiations. Our experimental results are presented in section 5. They include a comparison with existing, term-centric pruning methods and with efficiency figures gathered from a wide range of retrieval systems in the TREC 2005 Terabyte track [8], showing the potential of our new method. Section 6 discusses possible ways to build the global language model needed to compute term scores during index pruning.
Throughout this paper, we use the term index to refer to a document-level inverted file (also known as frequency index ), not containing any positional information. Conse-quently, posting refers to a document-level posting, i.e., a pair of the form (document identifier, term frequency). In-dices containing positional information are different in na-ture and not easily accessible to the pruning techniques de-scribed here (de Moura et al. [9] present a possible ap-proach). Our method is not suitable for phrase queries and other query forms that require the existence of positional in-formationintheindex.However,itcanbeusedtoperform bag-of-words search operations, in conjunction with scoring functions such as Okapi BM25 [13] and certain implemen-tations of language-model-based retrieval functions [2]. If a search query requires access to positional index information, a separate index, containing this information, can be used to process the query. Unless such queries are very frequent, the overall savings achieved by our pruning method can still be substantial. Bahle et al. [3], for instance, report, that only 8.3% of the queries they found in an Excite query log were phrase queries.
Traditionally, pruning techniques in information retrieval systems have been dynamic, which means that they were applied at query time in order to reduce the computational cost required to find the set of top documents, given a search query. Moffat and Zobel [10], for example, increase their sys-tem X  X  query processing performance by restricting the num-ber of document score accumulators maintained in memory in a term-at-a-time query processing framework. Persin et al. [11] describe a dynamic pruning technique that is based on within-document term frequencies. They also show how the inverted index can be reorganized (frequency-sorted in-dex) in order to better support this kind of pruning.
Fagin et al. [6] broke away from the dynamic pruning paradigm and introduced the concept of static index pruning to information retrieval. In static index pruning, informa-tion that is not likely to be needed during query processing is discarded during index construction (or immediately follow-ing it), resulting in an inverted file that can be much smaller than the original index for the given text collection. The dis-advantage of static pruning is that the pruning decision has to be made at indexing time, without any knowledge of the queries that are going to be processed. Its advantage is its ability to substantially reduce the size of the index, promis-ing great efficiency gains due to decreased disk acitivity. In their paper, Fagin et al. describe a global, uniform strategy and several local, term-dependent pruning strategies. They assume that search results are ranked by the search engine based on some TF/IDF method and, during pruning, rank each term X  X  postings according to their hypothetical con-tribution to a document X  X  TF/IDF score. In their global strategy, every posting whose contribution to the score of the document it belongs to is smaller than a fixed thresh-old  X  is removed from the index. In their term-dependent strategy,  X  =  X  ( T ) is a function of the term T .Theirrecom-mended method lets  X  ( T )=  X   X  P ( k ) T ,  X   X  [0 , 1], where P is the score of the k th-best posting for the term T .
Based on Fagin X  X  method, de Moura et al. [9] propose a locality-based pruning technique that, instead of only tak-ing the highest-scoring postings into the pruned index, se-lects all postings corresponding to terms that appear in the same sentence as one of the postings selected by Fa-gin X  X  method. Their experimental results indicate that this locality-enhanced version of the pruning algorithm outper-forms the original version. They also discuss how pruning techniques can be applied to positional indices in addition to pure frequency indices.

B  X  uttcher and Clarke [5] present a variation of Fagin X  X  method that is motivated by the goal to reduce the size of the index so far that it fits into memory. For very large text collections containing tens of millions of terms (GOV2: around 50 million different terms), this is only possible if the number of terms taken into the pruned index is limited. Therefore, in their pruning strategy, two parameters n and k can be chosen. The pruning process takes an existing index and creates a pruned index that contains the top k postings (using Fagin X  X  technique to sort each term X  X  posting list) for each of the n most frequent terms in the original index. The original index is kept on disk, while the pruned index is loaded into memory. At query time, the search engine fetches postings from the in-memory index whenever possi-ble, accessing the on-disk index only if a query term cannot be found in the pruned in-memory index. Throughout this paper, we refer to this variant of term-centric index pruning as TCP k n (or simply TCP).

In contrast to the term-centric pruning methods described above, we propose a document-centric strategy, where the decision whether a posting is taken into the pruned index does not depend on the posting X  X  rank within its term X  X  post-ing list, but on its rank within the document it refers to. In some sense, our technique is similar to the feedback mech-anism based on document surrogates that was proposed by Billerbeck and Zobel [4]. The difference is that they use the surrogates for query expansion, while we use them to build the index, and that our term selection method (KL divergence) is different from theirs.

Anh and Moffat [1] have recently presented a very effective pruning technique that is non-static, but requires the index to be in impact-order instead of the traditional document-order. Their technique allows to change the amount of prun-ing dynamically, after the index has been created, and rep-resents an interesting alternative to our pruning method.
The retrieval system used for the experiments described in this paper is based on a compressed inverted file con-taining a document-level posting list for each term found in the text collection. Postings are stored as simple integer values, where the 5 least significant bits represent the term frequency (after applying an exponential transformation for Table 1: Probability that all query terms are among the top f feedback terms of a document randomly chosen from the top k documents retrieved by the BM25 baseline, for various values of f and k . Table 2: Probability that at least one query term is among the top f feedback terms of a document ran-domly chosen from the top k documents retrieved by the BM25 baseline, for various values of f and k . TF values greater than 2 4 ), while all other bits represent the document number the posting refers to. Postings are grouped into blocks and compressed using a byte-aligned en-coding method [14]. Each compressed block contains around 2 16 postings. After compression, the average size of a post-ing is 13.6 bits.
 Baseline Retrieval Method Our basic retrieval method relies on the Okapi BM25 for-mula [13]. Given a search query Q = { Q 1 ,...,Q n } ,con-taining n terms, the score of a document D is: S where f D,Q i is Q i  X  X  frequency within D , dl is D  X  X  length (number of tokens), and avgdl is the average document length in the collection. w Q i is Q i  X  X  IDF weight: w Q i log( N/N Q i ), where N is the number of documents in the collection and N Q i is the number of documents containing the term Q i . For the free parameters, we chose k 1 =1 . 2and b =0 . 5  X  a configuration that was shown to be appropriate for the GOV2 collection used in our experiments [12].
Within this general framewo rk, document scores are com-puted in a document-at-a-time fashion. For each query term, the corresponding posting list is fetched from the index, de-compressed, and the in formation from all n lists is combined in a multiway merge process, using a heap data structure, resulting in a stream of document scores. Document descrip-tors for the top documents encountered so far are kept in memory, again using a heap. Standard optimizations, such as
MaxScore [15], are applied to reduce the computational cost of processing a query. Additional data structures, not described here, allow us to efficiently look up the score im-pact of a posting, based on the document number and the term frequency, and to quickly produce official TREC doc-ument IDs (e.g.,  X  X X255-91-2697243 X ) from internal docu-ment numbers. As a result, queries can be processed with high throughput, at a rate of 5.25 queries per second (190.5 ms per query) for the 50,000 queries used in the TREC 2005 Terabyte efficiency task (with GOV2 as the underlying doc-ument collection).
The general goal of our pruning technique is the same as that in [5]: Reducing the size of the index so far that it completely fits into main memory. The difference is that we follow a document-centric approach instead of a term-centric. Our method is similar to the pseudo-relevance feedback mechanism described by Carpineto et al. [7]. Carpineto X  X  method is based on the Kullback-Leibler diver-gence between the unigram language model defined by an individual document and that defined by the whole text collection. It uses each term X  X  contribution to a document X  X  KL divergence to assign feedback scores to potential expan-sion terms. Given unigram term distributions P and Q , their KL divergence is: where T is the set of all terms in the vocabulary, and P ( T ) and Q ( T )denote T  X  X  probability of occurrence under the distribution P and Q , respectively. In their feedback mech-anism, Carpineto et al. select a set R of pseudo-relevant documents, build a language model M R for each document R  X  X  , and compute the feedback score of each term T appearing in R according to the rule where M  X  is the global language model of the entire text collection.

When conducting some initial experiments with this feed-back method, we noticed that it is very good at finding the original query terms, given a set R of pseudo-relevant doc-uments, even if the set is very small ( |R| =1or |R| =2). For example, if we pick a random topic from the 50 topics used in the ad-hoc retrieval task of the TREC 2005 Terabyte track and select a single random pseudo-relevant document from the top k = 20 documents returned by BM25, then the probability of at least one query term being among the top f = 20 feedback terms is 97.8%; the probability of all query terms being among the top f = 20 feedback terms is 39.9% (for details, see Tables 1 and 2).

Thus, by performing pseudo-relevance feedback on indi-vidual documents in a static fashion at indexing time ,with-out taking an actual search query into account, it is possible to predict the set of query terms for which a given document will end up in the top documents returned the BM25 base-line method. Hence, in our document-centric index pruning strategy, a language model M D is built for every document D being indexed. Each term T in D is assigned a score: Only the top-scoring terms are kept in the index; the rest are discarded. The size of the resulting index depends on how many postings for a given document are allowed into the index and how many are rejected. The final goal is to reduce the size of the index enough so that it can be completely loaded into main memory. Since, for a collection with 50 million different terms (GOV2), this is only feasible if the number of candidate terms is limited, only the 10 6 most frequent terms in the collection are allowed to enter the pruned index, while the less frequent terms are disregarded completely.

We implemented two different instantiations of the general pruning strategy: In our first method, for each document in the collection, the top k terms from the document are put into the index; all other terms in the document are discarded. k is a user-defined constant. Smaller k results in a smaller index, while greater k produces higher-quality search results. Method 2: DCP (  X  ) Rel Our second method is similar to the first. Now, however, the number of terms taken from a document D is not constant, but depends on the number of distinct terms in the docu-ment, denoted by | D | , and a user-defined pruning parame-ter  X  .Fromeachdocument D ,thetop | D | X   X  terms are taken into the index; all other terms are discarded. Smaller  X  results in a smaller index and lower response times. The rationale behind this method is that longer documents usu-ally cover a greater variety of topics than shorter documents. Thus, the set of possible query terms for longer documents is larger than for shorter ones.
For all our experiments, we used the GOV2 text collection, the result of a webcrawl co nducted in 2003/2004, also known as the TREC Terabyte collection [8]. All efficiency figures were obtained by sending the 50,000 efficiency queries used in the TREC 2005 Terabyte track [8] to our search engine and processing all queries in a s equential manner, report-ing the top 20 documents for each search query. Since no relevance judgements are available for this query set, all ef-fectiveness numbers were computed for the title-only queries derived from the 50 ad-hoc topics used in the TREC 2005 TB track (topics 751-800). These 50 queries form a subset of the 50,000 efficiency queries. Our evaluation methodology is the same as that used in the Terabyte track: Precision vs. mean time per query. Like in TREC, P@20 (precision af-ter 20 doc X  X ) serves as our main precision measure. In some places, we also look at other measures, such as P@10 and mean average precision (MAP).
 All experiments were conducted on a PC based on an AMD Athlon64 3500+ processor (2.2 GHz) with 2 GB of RAM and a 7,200-rpm SATA hard drive. The search engine used in our experiments was a modified version of Wumpus 1 For the TREC 2005 Terabyte ad-hoc topics, the baseline retrieval method presented in section 3 achieves a P@20 of 0.5660 (P@10: 0.6400; MAP: 0.3346) at an average response time of 190.5 ms . Unless explicitly stated oth-erwise, we always used the pruned in-memory index and the unpruned on-disk index in parallel, fetching postings from the in-memory index whenever possible and only accessing the on-disk index when a query term could not be found in main memory.

In our first series of experiments, we repeated the prun-ing experiments reported by B  X  uttcher and Clarke [5]. The results shown in Figure 1 are roughly in line with their find-ings. A slight difference is that, in our experiments, retrieval effectiveness does not drop as quickly as in theirs. This is mainly due to better index compression, which allows us to keep more postings in the in-memory index (B  X  uttcher and Clarke report 19.3 GB for their unpruned index, whereas the unpruned index used in our experiments is only 12.9 GB). This also explains why our baseline (190 ms) is faster than their baseline (326 ms). Despite these improvements, for average reponse times below 20 ms, the difference between TCP and the BM25 baseline is still quite large (for an in-memory index of size 1024 MB, decreasing the response time to under 20 ms requires P@2 0 to drop below 0.4800  X  a 15% decrease compared to the baseline).

We then examined the DCP Const pruning strategy. We conducted experiments for various values of the pruning pa-rameter k . Query processing performance is promising, with response times below 25 ms in all cases. Retrieval effective-ness, however, is not even close to that of the BM25 base-line. For k = 18, for instance, DCP Const achieves an average time per query close to 16.5 ms, but P@20 is 23% below the baseline (0.4370 vs. 0.5660), and MAP, even worse, is 45% below the baseline (0.1835 vs. 0.3346). These numbers are similar to those of term-centric pruning for equivalent query response times (details in Figure 2).

Our experiments with the DCP Rel strategy (results shown in Figure 3) had a more reassuring outcome. Like for DCP Const , query times are very low, less than 25 ms for all values of the pruning parameter  X  that we tested. 0.4710 ( k =28 ).
Pruning level Postings decompr. Postings inspected  X  =0 . 04 242380 (7.5%) 71981 (8.0%)  X  =0 . 06 318273 (9.9%) 94671 (10.6%)  X  =0 . 08 381075 (11.8%) 115862 (13.0%)  X  =0 . 10 442592 (13.7%) 135102 (15.1%)  X  =1 . 00 3219483 (100%) 892145 (100%) Table 3: Average number of postings touched per query, for DCP Rel . MaxScore is responsible that the number of postings inspected is not decreasing pro-portionally to  X  .
 However, DCP Rel  X  X  retrieval effectiveness is far superior to that of DCP Const .With  X  =0 . 05 (size of pruned index: 848 MB), DCP Rel processes queries at a rate of 61 queries per second (16.3 ms per query). With 0.5340, precision at 20 documents is only 6% below the baseline. Average precision is still rather low (0.2323; 31% below the baseline), but more competitive than that produced by DCP Const .By increasing  X  to 0.08 (resulting in a pruned index of size 1284 MB, 10% the size of the unpruned index), it is possible to improve the situation. Average precision rises to 0.2569, or 23% below the baseline; the average time per query increases to 20 ms.

For  X  =0 . 1 (index siz e: 1570 MB, 12% of unpruned in-dex), the search produced from the pruned index are virtu-ally indistinguishable from those generated from the orig-inal, unpruned index. P@20 is 3.4% below the baseline (0.5470 instead of 0.5660) and P@10 only 2.5% (0.6240 in-stead of 0.6400). At an average query processing latency of 24 ms (87% below the baseline method), response time is still very low.

The performance gains achieved by DCP Rel and docu-mented by our experimental results have two different ori-gins. Firstly, by keeping the primary index in main memory and only accessing the secondary on-disk index when a query term cannot be found in the primary index, disk I/O is de-creased dramatically. Secondly, the reduced length of the posting lists in the primary index leads to a smaller num-ber of postings that need to be inspected and thus a smaller number of document scores that need to be computed.
Table 3 covers this second aspect. It shows that the num-ber of postings inspected during query processing is de-creased by up to 92% for  X  =0 . 04. The reason why the number of postings that are inspected during query pro-cessing is not proportional to the pruning level  X  is that the query processor employs the MaxScore [15] heuristic to ignore postings that cannot change the ranking of the top search results (here for the top 20). Thus, many of the postings removed by the pruning process would not have been considered by the query processor anyway. The dis-crepancy between the number of postings read from the in-dex and decompressed and the number of postings actually used for query processing purposes (a factor-3.5 difference) stems from the fact that our sea rch engine stores postings in compressed blocks of around 2 16 postings each. Thus, even if only a single posting from a block is needed, the entire block needs to be decompressed. Decreasing the block size might help, but would have other disadvantages, which is why we decided to leave it at 2 16 postings. ad-hoc topics 701-800. For the same response time (18 ms), DCP most recall levels.

Figure 4 combines the results we obtained for the 3 differ-ent pruning methods and compares their retrieval effective-ness at different efficiency levels. While DCP Const performs slightly worse than TCP, the DCP Rel pruning strategy out-performs the two other strategies at every efficiency level we tested. At an average response time of 13.5 ms (  X  =0 . 05), for example, its P@20 (0.4790) is 14% higher than that of TCP (0.4190) and 27% higher than that of DCP Const (0.3780). In addition, Figure 4(a) shows that even for a relatively small in-memory index (about 700 MB), DCP Rel achieves respectable precision (P@20 = 0.5130).
 Statistical Significance We fixed the amount of main memory available for the in-memory index to 1024 MB. With an in-memory index of this size, DCP Const ( k = 21) and DCP Rel (  X  =0 . 062) both lead to an average query response time of 18 ms for the TREC 2005 Terabyte efficiency queries. With TCP, the same re-sponse time can be achieved by building an index containing the k = 24500 best postings for each of the n = 16000 most frequent terms (TCP 24500 16000 ). We analyzed all three prun-ing techniques at this efficiency level, using the ad-hoc top-ics from the 2004 and 2005 TREC Terabyte tracks as test queries. Table 4 provides precision values at several retrieval points for each pruning method. It shows that DCP Rel de-classes DCP Const and TCP at virtually every recall level. Stars indicate significantly worse retrieval effectiveness for DCP Const and TCP, compared with DCP Rel , according to a paired t-test (one star: 95% confidence; two stars: 99% confidence).
 Similarity to Original Search Results Another interesting question is how close the search results produced by DCP Rel are to those produced by the BM25 baseline  X  in other words, to find out whether DCP achieves such high precision because it actually produces the similar search results as the baseline, or because it re-turns different documents which, however, also turn out to be relevant. To evaluate the similarity between the baseline results and the DCP Rel results, we employ the methodol-ogy also used by Fagin et al. [6]. Table 5 shows that, for the TREC 2005 Terabyte ad-hoc topics, the search results produced from the pruned indices are in fact very similar to those produced from the unpruned index. For  X  =0 . 1, the similarity level, as measured by the symmetrical difference between the search results, i.e., the ratio of the intersection and the union of the top 20 documents produced from the pruned and from the unpruned index, is 67%. On average, 77% of the top 20 doc X  X  produced from the unpruned index appear in the top 20 doc X  X  produced from the pruned index. TREC Terabyte We compared the performance of our pruning method to other retrieval systems that participated in the efficiency task of the TREC 2005 Terabyte track (as reported by Clarke et al. [8]). Figure 5 shows that document-centric Figure 5: Comparing term-centric and document-centric index pruning with the official runs submit-ted for the TREC 2005 Terabyte track. All query times are CPU-normalized  X  simulating search en-gines running on a single-CPU system. pruning allows us to protrude into the previously undiscov-ered area in the top-left corner of the graph, demonstrating the potential of DCP. The fastest run submitted for the efficiency task of the Terabyte track had a mean time per query of 55 ms (normalized by #CPUs) and P@20 = 0.3900. At that precision level, DCP Rel needs less that 12 ms per query. However, the evaluation methodology employed in the Terabyte track (response times normalized by #CPUs) is questionable, and the inter-system comparison should be taken with a grain of salt.
 Additional Experiments For all experiments described so far, the pruned in-memory index and the unpruned on-disk index were used in par-allel. The unpruned index served as a backup that could be accessed whenever no postings for a given query term were found in the pruned index. In experiments not re-ported on in detail here, we examined how turning off the on-disk index affects both efficiency and effectiveness. For a DCP Rel -pruned index with  X  =0 . 05, average response time can be decreased from 16.3 ms to 12.8 ms by disabling the on-disk index. On the other hand, this makes P@20 drop from 0.5340 to 0.5290 because  X  X ersenne X  (topic 784) and  X  X eechee X  (topic 791) have no posting list in the pruned in-dex. This confirms our initial assumption that a backup index might be needed to cover the case of query terms with very low collection frequency.
 By looking at the pruned index created by DCP (  X  =0 . 03) we noticed that, although it was only 544 MB large (4.1% of the unpruned index), the posting list for the term  X  X he X  had 3.0 million entries (15% of its l ength in the unpruned index). Our KLD-based pruning techniques seems to favor frequent terms over unfrequent terms. By changing the term score formula from equation 3 to with  X   X  [0 , 1), it is possible to counter this effect. Choosing  X  =0 . 10, for example, cuts the list for  X  X he X  down to 2.37 million entries. It increases P@10 from 0.5520 to 0.5580 and P@20 from 0.4790 to 0.4890, without harming performance. We did not further investigate in this direction. Table 5: Similarity of top 20 search results produced from pruned index to top 20 search results from un-pruned index, for DCP Rel and various pruning levels  X  . Topics: TREC Terabyte 2005.

Finally, we compared our KLD-based term selection func-tion to the formula proposed by Billerbeck and Zobel [4]. Although Billerbeck X  X  objective (query expansion) is differ-ent from ours (index pruning), the techniques are similar, raising the question how their method performs compared to ours. We modified the DCP (  X  ) Rel strategy so that it uses the selection function instead of equation 3. Here, N is the total number of docu-ments in the collection, N T the number of documents con-taining T ,and f D,T the frequency of the term T within the document in question. For equivalent query processing per-formance (17.5 ms per query), Billerbeck X  X  selection function produces a P@20 of 0.4820  X  9.4% lower than the original KLD-based selection function (0.5320). A paired t-test re-ports statistical significance with confidence level 94.3%.
The pruning method described in section 4 demands the knowledge of the global language model of the text collec-tion for which an index is being constructed. This seems to require a two-pass index construction process, where the language model is built during the first phase and the actual index is constructed during the second phase, approximately doubling the total index construction time. Fortunately, if the text collection is very large, it is not necessary to analyze the entire collection during the first phase. A representative subcollection, consisting of randomly selected documents, can be used to build the language model in phase 1, and the whole collection does not need to be read twice.

For all experiments described in the previous section, we used a language model built from a random subcollection comprising 5% of the documents in the GOV2 corpus. Al-tering the size of the subcollection does not substantially change the effectiveness of our technique. For DCP (0 . 062) (index size: 1024 MB), lowering the size of the subcollection from 5% to 1% results in a mild decrease of P@20 (from 0.5310 to 0.5250). Increasing it to 10%, improves P@20 to 0.5340. We also co nducted experiments in which we used a completely different text collection to build the background language model. If TREC disk 5 is used to build the back-ground model, P@20 decreases to 0.5270. If the Medline corpus from the TREC Genomics track is used, P@20 drops to 0.5070. Since the TREC Medline corpus is as far away from GOV2 as it can get, without changing the language of the collection from English to something else, we conclude that our method is very robust with respect to noisy data in the language model.

Another aspect of our approach is data sparseness. We decided to build the language model according to the max-imum likelihood estimate. MLE is very amenable to in-accuracies resultin g from data sparseness. We addressed this problem by restricting the language to the X most fre-quent terms seen in the subcollection (in our experiments: X =10 6 ). This requires us to build an unpruned index in addition to the pruned index so that, at query time, post-ing lists for infrequent terms, which did not make it into the pruned index, can be fetched from the unpruned index. However, this does not imply the necessity of a two-phase index construction process. Instead, two indices can be built in a single pass  X  the full index for the collection, containing all postings, and a smaller, pruned index that may be loaded into memory for query processing.
We have presented a document-centric index pruning method that can be used to dramatically increase the query processing performance of search engines working on static document collections. Our pruning method employs Kullback-Leibler divergence to select terms by performing query-independent pseudo-relevance feedback at index-ing time. We examined two possible implementations of our document-centric pruning method. The first imple-mentation selects a constant number of terms from each document, while the number of terms selected by the second implementation is proportional to the number of distinct terms in the document. Our experimental results show that the proportional pruning method (DCP Rel ), outperforms existing term-centric pruning methods.

Compared to the Okapi BM25 baseline retrieval method implemented in our search engine, DCP Rel can decrease the average response time by 87% (to 24 ms) for the efficiency queries used in the TREC Terabyte track. At the same time, P@20 only decreases by 3.4% (from 0.5660 to 0.5470). Fur-ther speed-ups are possible, but carry the cost of a larger decrease of the search engine X  X  effectiveness: Decreasing the response time by 92.3% (to 14.7 ms) makes P@20 drop by 9.4%, to 0.5130. Compared to term-centric pruning meth-ods, document-centric pruning also has the advantage that the pruning criterion can be chosen independently of the ranking function employed for query processing; there is no obvious, close connection between KL divergence and the incarnation of BM25 we use to rank the searc hresults.
The main part of the savings achieved by our method stems from the decrease (or elimination) of disk transfer operations, which represent a major bottleneck for most re-trieval systems. In distributed search systems that consist of a number of computers large enough to hold the entire unpruned index in memory, the efficiency gains will not be as dramatic as indicated by our experiments. However, even in such cases, our pruning method can be expected to im-prove query efficiency by a factor 5 or so, by reducing the amount of postings that need to be inspected during query processing. [1] V. N. Anh and A. Moffat. Pruned Query Evaluation [2] L. Azzopardi and D. E. Losada. An Efficient [3] D. Bahle, H. E. Williams, and J. Zobel. Efficient [4] B. Billerbeck and J. Zobel. Techniques for Efficient [5] S. B  X  uttcher and C. L. A. Clarke. Efficiency vs. [6] D. Carmel, D. Cohen, R. Fagin, E. Farchi, [7] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. [8] C. Clarke, F. Scholer, and I. Soboroff. Overview of the [9] E.S.deMoura,C.F.dosSantos,D.R.Fernandes, [10] A. Moffat and J. Zobel. Self-Indexing Inverted Files [11] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered [12] V. Plachouras, B. He, and I. Ounis. University of [13] S. E. Robertson, S. Walker, and M. Hancock-Beaulieu. [14] F. Scholer, H. E. Williams, J. Yiannis, and J. Zobel. [15] H. Turtle and J. Flood. Query Evaluation: Strategies
