 Microsoft Corporation Recall is More Subtle than Precision I just returned from the Association for Computational Linguistics X  43rd Annual Meet-ing (ACL-2005). The acceptance rate was 18%. Is this a good thing or a bad thing? they know that as well. ACL-2005 had great precision.
 vious to everyone. If you listen closely, you X  X l hear some grumbling in the halls. And papers, and often strong contenders for the best paper award at EMNLP. I used to be so many years, I am no longer surprised by anything. The practice of setting EMNLP X  X  submission date immediately after ACL X  X  notification date is a not-so-subtle hint: Please do something about the low recall.
 what is happening. ACL reviewing is paying too much attention to abstentions (and objections from people outside the area). If a reviewer isn X  X  qualified to say anything on a particular topic, that X  X  okay. An abstention shouldn X  X  kill a paper.
 The only bad paper is a paper without an advocate. A paper with a single advocate should trump a paper with lots of seconds, but no advocates. Don X  X  average votes. The key votes are the advocates. Negative votes matter only if they convince the advocates to change their votes.
 the classic paper on page rank, a hugely successful paper in terms of citations, perhaps more successful than anything SIGIR ever published. 1. A Model Consider the following model. Suppose we generate the gold standard so that some fraction, a = 20%, of the s = 400 submitted papers should be accepted, and 1  X  a , should be rejected. There are r = 3 reviews for each paper. A review votes either 1 (accept) or 0 (reject). Papers are scored by summing the votes. Some reviews are good ( g = 70%), and some (1  X  g ) are not. A good review votes the same way as the gold standard. A bad review votes randomly, accepting a of the papers and rejecting the rest. The program accept a papers with the best votes, how well do those papers match the gold standard? a=0.2 # acceptance rate s=400 # submissions r=3 # reviews per paper g=0.7 # mixture of good to random reviews gold = rbinom(s, 1, a) good = matrix(rbinom(s*r, 1, g), ncol=r) score = good*gold + (1-good)*matrix(rbinom(s*r,1,a),ncol=r) accept = rank(jitter(apply(score,1,sum)))&gt;(s*(1-a)) precision = sum(accept * gold)/sum(accept) recall = sum(accept * gold)/sum(gold) According to this model, recall can be improved in at least three ways: rate is easy. There is no excuse not to. Last century, we kept the acceptance rate low so everyone could hear every paper in a single plenary session. Given the ever increasing what similar argument, that we can X  X  accept more papers because of some (imagined) constraint involving local arrangements. In fact, ACL-2005 could have accepted more papers than it did; there were empty rooms during much of the meeting. Moreover, local arrangements have obvious financial incentives to come up with creative ways to accept as many papers as possible: more papers  X  more $$ (conference registrations).
If we can accept more papers without hurting (real or perceived) precision too much, we ought to do so.
 though I can X  X  use the model above to justify the magic threshold of 20%, it has been my experience that whenever acceptance rates fall below that magic threshold, it be-576 like 20% change the tone of the grumbing in the halls into an ugly swap meet. Many And it X  X  hard to maintain plausible deniability when everyone knows what everyone knows.
 ought to use them to increase recall by increasing the number of reviews per paper, but realistically, as submissions go up and up and up, we X  X e unlikely to have lots of spare rate), then we could retaliate with Plan B as a deterrent. It the community won X  X  let us to do the right thing (increase acceptance rates), then we can punish them with more and more papers to review until they  X  X ppreciate X  the merits of Plan A.
 Unfortunately, reviewers do what reviewers do. Some reviewers are conservative and some are really conservative and some are really really conservative. Reviewers love safe (boring) papers, ideally on a topic that has been discussed before (ad nauseam). from new blood on new topics). But the survival of the organization depends on new blood. We need to liberalize the process, or else.
 be enough. It is up to the meta-reviewers to make sure that a paper with an advocate reviewers should maintain a reserve of designated tie-breaker reviewers. The reserve should be highly respected (and highly opinionated). They need to work quickly and decisively, without too many abstentions, selecting interesting novel papers, and avoid-ing the safe boring unobjectionable papers that tend to do relatively well on the first round with the first tier of reviewers. 2. Recommendations
Whatever you measure, you get. Precision is a good thing, but so is recall. In addition to reporting submissions, and acceptance rates, conferences ought to report estimates of validation argument. We could give a sample of the papers to another set of reviewers and use their decisions as a gold standard for evaluating the program committee as a whole in terms of precision and recall.
 published elsewhere; we can hope that the accepts will be more heavily cited than the rejects.
 submission) for as long as it takes until they appreciate the merits of Plan A. Meanwhile, rates) and Plan B (increase the reviewing burden). Plan D is truly Machiavellian: more conferences.
