 Question answering (QA) forums such as Stack Ex-millions of questions. The expanding scope and cov-erage of these forums often leads to many dupli-cate and interrelated questions, resulting in the same questions being answered multiple times. By iden-tifying similar questions, we can potentially reuse existing answers, reducing response times and un-necessary repeated work. Unfortunately in most fo-rums, the process of identifying and referring to ex-isting similar questions is done manually by forum participants with limited, scattered success.
The task of automatically retrieving similar ques-tions to a given user X  X  question has recently attracted significant attention and has become a testbed for various representation learning approaches (Zhou et al., 2015; dos Santos et al., 2015). However, the task has proven to be quite challenging  X  for instance, dos Santos et al. (2015) report a 22.3% classification ac-curacy, yielding a 4 percent gain over a simple word matching baseline.

Several factors make the problem difficult. First, submitted questions are often long and contain ex-traneous information irrelevant to the main question being asked. For instance, the first question in Fig-ure 1 pertains to booting Ubuntu using a USB stick. A large portion of the body contains tangential de-tails that are idiosyncratic to this user, such as ref-erences to Compaq pc , Webi and the error message. Not surprisingly, these features are not repeated in the second question in Figure 1 about a closely re-lated topic. The extraneous detail can easily confuse simple word-matching algorithms. Indeed, for this reason, some existing methods for question retrieval restrict attention to the question title only. While ti-tles (when available) can succinctly summarize the intent, they also sometimes lack crucial detail avail-able in the question body. For example, the title of the second question does not refer to installation from a USB drive. The second challenge arises from the noisy annotations. Indeed, the pairs of questions marked as similar by forum participants are largely incomplete. Our manual inspection of a sample set of similar pairs have been annotated by the users, with a precision of around 79%.

In this paper, we design a neural network model and an associated training paradigm to address these challenges. On a high level, our model is used as an encoder to map the title, body, or the combina-tion to a vector representation. The resulting  X  X ues-tion vector X  representation is then compared to other questions via cosine similarity. We introduce sev-eral departures from typical architectures on a finer level. In particular, we incorporate adaptive gating in non-consecutive CNNs (Lei et al., 2015) in or-der to focus temporal averaging in these models on key pieces of the questions. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of per-formance in our setting. Moreover, we counter the scattered annotations available from user-driven as-sociations by training the model largely based on the entire unannotated corpus. The encoder is cou-pled with a decoder and trained to reproduce the ti-tle from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summariza-tion (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost.
We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos San-tos et al., 2015). During training, we directly uti-lize noisy pairs readily available in the forum, but to have a realistic evaluation of the system perfor-mance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for de-velopment and hyper parameter tuning and another for testing. We evaluate our model and the base-lines using standard information retrieval (IR) mea-sures such as Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at n (P@ n ). Our full model achieves a MRR of 75.6% and P@1 of 62.0%, yielding 8% absolute improvement over a standard IR baseline, and 4% over standard neural network architectures (including CNNs, LSTMs and GRUs). Given the growing popularity of community QA fo-rums, question retrieval has emerged as an important area of research (Nakov et al., 2015; Nakov et al., 2016). Previous work on question retrieval has mod-eled this task using machine translation, topic mod-eling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata in-formation for questions. They define each question as a distribution which generates each word (embed-ding) independently, and subsequently use a Fisher kernel to assess question similarities. Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations. Further, we propose a training paradigm that utilizes the entire corpus of unannotated questions in a semi-supervised manner. Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network archi-tectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Compared to our work, these ap-proaches focus on improving various other aspects of the model. For instance, Feng et al. (2015) ex-plore different similarity measures beyond cosine similarity, and Tan et al. (2015) adopt the neural at-tention mechanism over RNNs to generate better an-swer representations given the questions as context. We begin by introducing the basic discriminative setting for retrieving similar questions. Let q be a query question which generally consists of both a title sentence and a body section. For efficiency reasons, we do not compare q against all the other queries in the data base. Instead, we retrieve first a smaller candidate set of related questions Q ( q ) using a standard IR engine, and then we apply the more sophisticated models only to this reduced set. Our goal is to rank the candidate questions in Q ( q ) so that all the similar questions to q are ranked above the dissimilar ones. To do so, we define a similarity score s ( q,p ;  X  ) with parameters  X  , where the simi-larity measures how closely candidate p  X  Q ( q ) is related to question q . The method of comparison can make use of the title and body of each question.
The scoring function s (  X  ,  X  ;  X  ) can be optimized on the basis of annotated data D = ( q i ,p + where p + Q lar to q i . During training, the correct pairs of similar questions are obtained from available user-marked pairs, while the negative set Q  X  from the entire corpus with the idea that the likeli-hood of a positive match is small given the size of the corpus. The candidate set during training is just Q ( q i ) = { p + sets are retrieved by an IR engine and we evaluate against explicit manual annotations.

In the purely discriminative setting, we use a max-margin framework for learning (or fine-tuning) pa-rameters  X  . Specifically, in a context of a particu-lar training example where q i is paired with p + minimize the max-margin loss L (  X  ) defined as where  X  (  X  ,  X  ) denotes a non-negative margin. We set  X  ( p,p + 0 otherwise. The parameters  X  can be optimized through sub-gradients  X  X / X  X  aggregated over small batches of the training instances.

There are two key problems that remain. First, we have to define and parameterize the scoring func-tion s ( q,p ;  X  ) . We design a recurrent neural network model for this purpose and use it as an encoder to map each question into its meaning representation. The resulting similarity function s ( q,p ;  X  ) is just the cosine similarity between the corresponding repre-sentations, as shown in Figure 2 (a). The parame-ters  X  pertain to the neural network only. Second, in order to offset the scarcity and limited coverage of the training annotations, we pre-train the param-eters  X  on the basis of the much larger unannotated corpus. The resulting parameters are subsequently fine-tuned using the discriminative setup described above. 4.1 Non-consecutive Convolution We describe here our encoder model, i.e., the method for mapping the question title and body to a vector representation. Our approach is inspired by temporal convolutional neural networks (LeCun et al., 1998) and, in particular, its recent refine-ment (Lei et al., 2015), tailored to capture longer-range, non-consecutive patterns in a weighted man-ner. Such models can be used to effectively sum-marize occurrences of patterns in text and aggre-gate them into a vector representation. However, the summary produced is not selective since all pat-tern occurrences are counted, weighted by how co-hesive (non-consecutive) they are. In our problem, the question body tends to be very long and full of irrelevant words and fragments. Thus, we believe that interpreting the question body requires a more selective approach to pattern extraction.

Our model successively reads tokens in the ques-tion title or body, denoted as { x i } l i =1 , and trans-forms this sequence into a sequence of states { h i } l i =1 . The resulting state sequence is subse-quently aggregated into a single final vector repre-sentation for each text as discussed below. Our ap-proach builds on (Lei et al., 2015), thus we begin by briefly outlining it. Let W 1 and W 2 denote filter ma-trices (as parameters) for pattern size n = 2 . Lei et al. (2015) generate a sequence of states in response to tokens according to where c t 0 ,t represents a bigram pattern, c t accumu-lates a range of patterns and  X   X  [0 , 1) is a con-stant decay factor used to down-weight patterns with longer spans. The operations can be cast in a  X  X e-current X  manner and evaluated with dynamic pro-gramming. The problem with the approach for our purposes is, however, that the weighting factor  X  is the same (constant) for all, not triggered by the state h t  X  1 or the observed token x t .
 Adaptive Gated Decay We refine this model by learning context dependent weights. For example, if the current input token provides no relevant infor-mation (e.g., symbols, functional words), the model should ignore it by incorporating the token with a vanishing weight. In contrast, strong semantic con-tent words such as  X  X buntu X  or  X  X indows X  should be included with much larger weights. To achieve this effect we introduce neural gates similar to LSTMs to specify when and how to average the observed signals. The resulting architecture integrates recur-rent networks with non-consecutive convolutional models: where  X  (  X  ) is the sigmoid function and represents the element-wise product. Here c (1) t ,  X  X  X  , c ( n ) accumulator vectors that store weighted averages of 1-gram to n -gram features. When the gate  X  t = 0 (vector) for all t , the model represents a traditional CNN with filter width n . As  X  t &gt; 0 , however, c ( n ) becomes the sum of an exponential number of terms, enumerating all possible n -grams within x 1 ,  X  X  X  , x t (seen by expanding the formulas). Note that the gate  X  (  X  ) is parametrized and responds directly to the previous state and the token in question. We refer to this model as RCNN from here on.
 Pooling In order to use the model as part of the discriminative question retrieval framework outlined earlier, we must condense the state sequence to a sin-gle vector. There are two simple alternative pooling strategies that we have explored  X  either averaging meaning representation. In addition, we apply the encoder to both the question title and body, and the final representation is computed as the average of the two resulting vectors.

Once the aggregation is specified, the parameters of the gate and the filter matrices can be learned in a purely discriminative fashion. Given that the avail-able annotations are limited and user-guided, we in-stead use the discriminative training only for fine tuning an already trained model. The method of pre-training the model on the basis of the entire corpus of questions is discussed next. 4.2 Pre-training Using the Entire Corpus The number of questions in the AskUbuntu corpus far exceeds user annotations of pairs of similar ques-tions. We can make use of this larger raw corpus in two different ways. First, since models take word embeddings as input we can tailor the embeddings to the specific vocabulary and expressions in this corpus. To this end, we run word2vec (Mikolov et al., 2013) on the raw corpus in addition to the Wikipedia dump. Second, and more importantly, we use individual questions as training examples for an auto-encoder constructed by pairing the en-coder model (RCNN) with an corresponding de-coder (of the same type). As illustrated in Fig-ure 2 (b), the resulting encoder-decoder architecture is akin to those used in machine translation (Kalch-brenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b) and summarization (Rush et al., 2015).

Our encoder-decoder pair represents a conditional language model P ( title | context ) , where the context can be any of (a) the original title itself, (b) the ques-tion body and (c) the title/body of a similar ques-tion. All possible (title, context) pairs are used dur-ing training to optimize the likelihood of the words (and their order) in the titles. We use the question title as the target for two reasons. The question body contains more information than the title but also has many irrelevant details. As a result, we can view the title as a distilled summary of the noisy body, and the encoder-decoder model is trained to act as a de-noising auto-encoder. Moreover, training a decoder for the title (rather than the body) is also much faster since titles tend to be short (around 10 words).
The encoders pre-trained in this manner are sub-sequently fine-tuned according to the discriminative criterion described already in Section 3. For comparison, we also train three alternative benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analo-gously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our pre-trained RCNN.
 LSTMs LSTM cells (Hochreiter and Schmidhu-ber, 1997) have been used to capture semantic in-formation across a wide range of applications, in-cluding machine translation and entailment recogni-tion (Bahdanau et al., 2015; Bowman et al., 2015; Rockt  X  aschel et al., 2016). Their success can be at-tributed to neural gates that adaptively read or dis-card information to/from internal memory states.
Specifically, a LSTM network successively reads the input token x t , internal state c t  X  1 , as well as the visible state h t  X  1 , and generates the new states c , h t : where i , f and o are input , forget and output gates, respectively. Given the visible state sequence { h i } l i =1 , we can aggregate it to a single vector ex-actly as with RCNNs. The LSTM encoder can be pre-trained (and fine-tuned) in the similar way as our RCNN model. For instance, Dai and Le (2015) recently adopted pre-training for text classification task.
 GRUs A GRU is another comparable unit for se-quence modeling (Cho et al., 2014a; Chung et al., 2014). Similar to the LSTM unit, the GRU has two neural gates that control the flow of information: where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to var-ious NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation and associated filters map local chunks (windows) of the input into a feature representation. Concretely, if we let n denote the filter width, and W 1 ,  X  X  X  , W n the corresponding filter matrices, then the convolu-tion operation is applied to each window of n con-secutive words as follows:
The sets of output state vectors { h t } produced in this case are typically referred to as feature maps. Since each vector in the feature map only pertains to local information, the last vector is not sufficient to capture the meaning of the entire sequence. In-stead, we consider max-pooling or average-pooling to obtain the aggregate representation for the entire sequence. Dataset We use the Stack Exchange AskUbuntu dataset used in prior work (dos Santos et al., 2015). This dataset contains 167,765 unique questions, user-marked similar question pairs. We provide var-ious statistics from this dataset in Table 1. Gold Standard for Evaluation User-marked sim-ilar question pairs on QA sites are often known to be incomplete. In order to evaluate this in our dataset, we took a sample set of questions paired with 20 candidate questions retrieved by a search en-gine trained on the AskUbuntu data. The search en-gine used is the well-known BM25 model (Robert-son and Zaragoza, 2009). Our manual evaluation of the candidates showed that only 5 % of the similar questions were marked by users, with a precision of 79 % . Clearly, this low recall would not lead to a re-alistic evaluation if we used user marks as our gold standard. Instead, we make use of expert annota-tions carried out on a subset of questions.
 Training Set We use user-marked similar pairs as positive pairs in training since the annotations have high precision and do not require additional man-ual annotations. This allows us to use a much larger training set. We use random questions from the cor-pus paired with each query question p i as negative pairs in training. We randomly sample 20 questions as negative examples for each p i during each epoch. Development and Test Sets We re-constructed the new dev and test sets consisting of the first 200 questions from the dev and test sets provided by dos Santos et al. (2015). For each of the above ques-tions, we retrieved the top 20 similar candidates us-ing BM25 and manually annotated the resulting 8K Baselines and Evaluation Metrics We evaluated neural network models X  X ncluding CNNs , LSTMs , GRUs and RCNNs  X  X y comparing them with the following baselines: We evaluated the models based on the following IR metrics: Mean Average Precision (MAP), Mean Re-ciprocal Rank (MRR), Precision at 1 (P@1), and Precision at 5 (P@5). Hyper-parameters We performed an extensive hyper-parameter search to identify the best model for the baselines and neural network models. For the TF-IDF baseline, we tried n -gram feature order n  X  { 1 , 2 , 3 } with and without stop words pruning. For the SVM baseline, we used the default SVM-Light parameters whereas the dev data is only used to increase the training set size when testing on the test set. We also tried to give higher weight to dev instances but this did not result in any improvement. For all the neural network models, we used Adam (Kingma and Ba, 2015) as the optimiza-tion method with the default setting suggested by the authors. We optimized other hyper-parameters with the following range of values: learning rate  X  { 1 e  X  3 , 3 e  X  4 } , dropout (Hinton et al., 2012) probability  X  { 0 . 1 , 0 . 2 , 0 . 3 } , CNN feature width  X  { 2 , 3 , 4 } . We also tuned the pooling strategies and ensured each model has a comparable number of parameters. The default configurations of LSTMs, GRUs, CNNs and RCNNs are shown in Table 3. We used MRR to identify the best training epoch and the model configuration. For the same model con-figuration, we report average performance across 5 Word Vectors We ran word2vec (Mikolov et al., 2013) to obtain 200-dimensional word embeddings using all Stack Exchange data (excluding Stack-Overflow) and a large Wikipedia corpus. The word vectors are fixed to avoid over-fitting across all ex-periments. Overall Performance Table 2 shows the perfor-mance of the baselines and the neural encoder mod-els on the question retrieval task. The results show that our full model, RCNNs with pre-training, achieves the best performance across all metrics on both the dev and test sets. For instance, the full model gets a P@1 of 62.0% on the test set, outper-forming the word matching-based method BM25 by over 8 percent points. Further, our RCNN model also outperforms the other neural encoder mod-els and the baselines across all metrics. This su-perior performance indicates that the use of non-consecutive filters and a varying decay is effective in improving traditional neural network models.
Table 2 also demonstrates the performance gain from pre-training the RCNN encoder. The RCNN model when pre-trained on the entire corpus consis-tently gets better results across all the metrics. Pooling Strategy We analyze the effect of various pooling strategies for the neural network encoders. As shown in Table 4, our RCNN model outperforms other neural models regardless of the two pooling strategies explored. We also observe that simply us-ing the last hidden state as the final representation achieves better results for the RCNN model.
 Using Question Body Table 5 compares the per-formance of the TF-IDF baseline and the RCNN model when using question titles only or when using question titles along with question bodies. TF-IDF X  X  performance changes very little when the question bodies are included (MRR and P@1 are slightly bet-ter but MAP is slightly worse). However, we find that the inclusion of the question bodies improves the performance of the RCNN model, achieving a 1% to 3% improvement with both model variations. The RCNN model X  X  greater improvement illustrates the ability of the model to pick out components that pertain most directly to the question being asked from the long, descriptive question bodies.
 Pre-training Note that, during pre-training, the last hidden states generated by the neural encoder are used by the decoder to reproduce the question ti-tles. It would be interesting to see how such states capture the meaning of questions. To this end, we evaluate MRR on the dev set using the last hidden states of the question titles. We also test how the en-coder captures information from the question bodies to produce the distilled summary, i.e. titles. To do so, we evaluate the perplexity of the trained encoder-decoder model on a heldout set of the corpus, which contains about 2000 questions.

As shown in Figure 3, the representations gener-ated by the RCNN encoder perform quite well, re-sulting in a perplexity of 25 and over 68% MRR without the subsequent fine-tuning. Interestingly, the LSTM and GRU networks obtain similar per-plexity on the heldout set, but achieve much worse MRR for similar question retrieval. For instance, the GRU encoder obtains only 63% MRR, 5% worse than the RCNN model X  X  MRR performance. As a result, the LSTM and GRU encoder do not benefit clearly from pre-training, as suggested in Table 2.
The inconsistent performance difference may be explained by two hypotheses. One is that the per-plexity is not suitable for measuring the similarity of the encoded text, thus the power of the encoder is not illustrated in terms of perplexity. Another hy-pothesis is that the LSTM and GRU encoder may learn non-linear representations therefore their se-mantic relatedness can not be directly accessed by cosine similarity.
 Adaptive Decay Finally, we analyze the gated convolution of our model. Figure 5 demonstrates at each word position t how much input information is taken into the model by the adaptive weights 1  X   X  t . The average of weights in the vector decreases as t increments, suggesting that the information encoded into the state vector saturates when more input are processed. On the other hand, the largest value in the weight vector remains high throughout the input, indicating that at least some information has been stored in h t and c t .

We also conduct a case study on analyzing the neural gate. Since directly inspecting the 400-dimensional decay vector is difficult, we train a model that uses a scalar decay instead. As shown in Figure 4, the model learns to assign higher weights to application names and quoted error messages, which intuitively are important pieces of a question in the AskUbuntu domain. In this paper, we employ gated (non-consecutive) convolutions to map questions to their semantic representations, and demonstrate their effectiveness on the task of question retrieval in community QA forums. This architecture enables the model to glean key pieces of information from lengthy, detail-riddled user questions. Pre-training within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus is integral to the model X  X  success.
 We thank Yu Zhang, Yoon Kim, Danqi Chen, the MIT NLP group and the reviewers for their help-ful comments. The work is developed in col-laboration with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the I YAS project. Any opinions, find-ings, conclusions, or recommendations expressed in this paper are those of the authors, and do not neces-sarily reflect the views of the funding organizations.
