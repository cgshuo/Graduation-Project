 Question Answering (QA) distinguishes itself from other information retrieval tasks in that the system tries to return accurate answers to queries posed in natural language. Factoid QA limits it-self to questions that can usually be answered with a few words. Typically factoid QA systems em-ploy some form of question type analysis, so that a question such as What is the capital of Japan? will be answered with a geographical term. While many QA systems use hand-crafted rules for this task, such an approach is time-consuming and doesn X  X  generalize well to other languages. Ma-chine learning methods have been proposed, such as question classification using support vector ma-chines (Zhang and Lee, 2003) and language mod-eling (Merkel and Klakow, 2007). In these ap-proaches, question categories are predefined and a classifier is trained on manually labeled data. This is an example of supervised learning. In this pa-per we present an unsupervised method, where we attempt to cluster question-and-answer (q-a) pairs without any predefined question categories, hence no manually class-labeled questions are used. We use a statistical QA framework, described in Section 2, where the system is trained with clusters 2.1 Retrieval Model The retrieval model P ( A | X ) is essentially a lan-guage model which models the probability of an answer sequence A given a set of information-bearing features X = { x is constructed by extracting single-word features from Q that are not present in a stop-list of high-frequency words. The implementation of the re-trieval model used for the experiments described in this paper, models the proximity of A to fea-tures in X . It is not examined further here; see (Whittaker et al., 2005) for more details. 2.2 Filter Model The question-type feature set W = { w such as where , in what and when were from the input question Q . We limit ourselves to extracting single-word features. The 2522 most frequent words in a collection of example questions are considered in-vocabulary words; all other words are out-of-vocabulary words, and substituted with h UNK i .
 Modeling the complex relationship between W and A directly is non-trivial. We there-fore introduce an intermediate variable C { c example q-a pairs. In order to construct these classes, given a set E = { t ample q-a pairs, we define a mapping function f : E 7 X  C E which maps each example q-a pair t j for j = 1 . . . | E | into a particular class f ( t Thus each class c all component q-a features from each t ing f ( t cluster of q-a pairs. Finally, to facilitate modeling we say that W is conditionally independent of A given c
P ( W | A ) = where c e type features and example answers for the class c respectively.
 Figure 1: M RR vs. LL (average per q-a pair) for 100 random cluster configurations. where R candidate answer for g
Given a set D = ( d from the q-a pairs in C calculate the log-likelihood as To examine the relationship between M RR and LL , we randomly generate configurations C E , with a fixed cluster size of 4, and plot the result-ing M RR and LL , computed on the same data set D , as data points in a scatter plot, as seen in Fig-ure 1. We find that LL and M RR are strongly correlated, with a correlation coefficient  X  = 0 . 86 .
This observation indicates that we should be able to improve the answer accuracy of the QA system by optimizing the LL of the filter model in isolation, similar to how, in automatic speech recognition, the LL of the language model can be optimized in isolation to improve the speech recognition accuracy (Huang et al., 2001). Using the observation that LL is correlated with M RR on the same data set, we expect that opti-mizing LL on a development set ( LL improve M RR on an evaluation set ( M RR Hence we propose the following greedy algorithm to maximize LL
Configuration LL one-in-each -0.87 0.263 2016 Table 1: LL M RR eval (over all held-out TREC years), and number of clusters (median of the cross-evaluation folds) for the various configurations. 4 years X  data and an evaluation set of one year X  X  data. For each TREC question the top 50 doc-uments from the AQUAINT corpus are retrieved in Section 2 for QA evaluation. Our evaluation metric is M RR tion criterion, as motivated in Section 3.
 Our baseline system uses manual clusters. These clusters are obtained by putting all who q-a pairs in one cluster, all when pairs in a second and all where pairs in a third. We compare this baseline with using clusters resulting from the algorithm described in Section 4. We run this algorithm until there are no further improvements in LL other cluster configurations are also investigated: all q-a pairs in one cluster (all-in-one), and each q-a pair in its own cluster (one-in-each). The all-in-one configuration is equivalent to not using the fil-ter model, i.e. answer candidates are ranked solely by the retrieval model. The one-in-each configura-tion was shown to perform well in the TREC 2006 QA evaluation (Whittaker et al., 2006), where it ranked 9th among 27 participants on the factoid QA task. 5.2 Results In Table 1, we see that the manual clusters (base-line) achieves an M RR clusters resulting from the clustering algorithm give an M RR improvement of 7%. This improvement is sta-tistically significant at the 0.01 level using the Wilcoxon signed-rank test. The one-in-each clus-ter configuration achieves an M RR which is not a statistically significant improvement over the baseline. The all-in-one cluster configura-tion (i.e. no filter model) has the lowest accuracy, with an M RR tion 4. It can clearly be seen that the optimization of LL that LL In this paper we have shown that the log-likelihood of our statistical model is strongly correlated with answer accuracy. Using this information, we have clustered training q-a pairs by maximizing log-likelihood on a disjoint development set of q-a pairs. The experiments show that with these clus-ters we achieve better QA accuracy than using manually clustered training q-a pairs.

In future work we will extend the types of ques-tions that we consider, and also allow for multi-word answers.
 The authors wish to thank Dietrich Klakow for his discussion at the concept stage of this work. The anonymous reviewers are also thanked for their constructive feedback.

