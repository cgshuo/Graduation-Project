 Fen Xia  X  Wensheng Zhang  X  Fuxin Li  X  Ya n w u Ya n g Abstract Ranking problems have recently become an important research topic in the joint field of machine learning and information retrieval. This paper presented a new splitting rule that introduces a metric, i.e., an impurity measure, to construct decision trees for ran-king tasks. We provided a theoretical basis and some intuitive explanations for the splitting rule. Our approach is also meaningful to collaborative filtering in the sense of dealing with categorical data and selecting relative features. Some experiments were made to illustrate our ranking approach, whose results showed that our algorithm outperforms both perceptron-based ranking and the classification tree algorithms in term of accuracy as well as speed. Keywords Machine learning  X  Ranking  X  Decision tree  X  Splitting rule 1 Introduction Ranking problems are one of the most important components in many applications including information retrieval and filtering [ 8 ]. In information retrieval, documents are ranked before being delivered to users. In order to elicit and model user preferences, more precise relevance measures are required rather than simple binary values (relevant/non-relevant). Some resear-chers argue that users X  relevance judgments can be reflected by a continuum of relevance regions from highly relevant, partially relevant to non-relevant [ 17 ]. The number of ratings is different according to users X  requirements and the precision of the application requirement. A collaborative filtering application such as movie recommendation aims to generate and deliver recommendations of movies that users would be likely to enjoy. Initially, users are asked to rate a list of movies that they have seen. An example for possible ratings might be run-to-see, very-good, good, only-if-you-must, and do-not-bother [ 5 ]. Consequently, these ratings are classified according to some similarity measure. Finally, recommendations are given based on users X  rating similarities.

In a broad sense, the ranking aims to predict instances of ordinal scale, i.e., the so-called ordinal regression [ 10 ]. Ranking problems can be regarded as a supervised inductive learning task, which predicts values of an ordinal function for any valid input objects according to some rules learned from a number of training examples (i.e., pairs of input and target out-put). Traditionally, supervised learning researches mainly focus on classification [ 12 , 21 ]and regression problems [ 16 ], and have made significant progresses on learning algorithms and evaluation approaches [ 9 ]. However, ranking problems require higher computing functions, e.g., to order things rather than simply classify them. As a matter of fact, the ranking lies in between classification and regression. In classification, given a training set with associated class labels, the goal is to learn some rules which could assign a new instance a correct label. In the regression, outputs of the training set are real-valued. The ranking is similar to the classification in that both of outputs are a finite set, and to the regression in that there is an ordinal relationship materialized in elements in a finite label set.

Various machine learning algorithms have been adapted to ranking tasks to automate ranking process [ 4 , 6 , 7 ], and to improve ranking accuracy and speed [ 2 , 20 ]. A natural idea settings, a new training set is formed by extracting pairs of examples of different ratings. Then ranking rules are constructed by binary classifiers on the training set [ 10 ]. However, it is time-consuming as the data complexity is increased from O ( n ) to O ( n 2 ) . In regression settings, a proper mapping is used to convert rankings to real values. However, it is hard to determine a proper mapping, and corresponding algorithms might be sensitive to the representation of rankings rather than their pairwise orderings [ 10 , 11 ].

State-of-the-art ranking approaches mainly assume that rankings are coarsely measured latent continuous variables, and model them with intervals on the real line. Based on this assumption, these algorithms seek a direction representing the real line on which examples are projected and a set of thresholds dividing the direction into consecutive intervals. [ 5 ] proposed a perceptron-based algorithm (called Prank) to seek the direction and some thre-sholds to construct ranking rules. This is an online mistake-driven procedure initialized with a direction and a set of thresholds that are adapted each time when a training example is misclassified. This procedure is guaranteed to converge, given a direction and a set of cor-responding thresholds that can correctly rank the training data. [ 11 ] generalized the Prank by running several Prank algorithms in parallel. The outputs were averaged to produce a ranking rule, which showed improved performance over a single Prank algorithm. A practical appli-cation of proceptron-based algorithms was proposed in [ 18 ], which dealt with ranking and re-ranking problems for natural language processing. The algorithm searches for and uses pairs of mis-ranked objects to update weight vectors. Perceptron-based ranking algorithms avoid the increase of data complexity, and outperform algorithms based on pairs (described in [ 10 ]) [ 11 ]. However, perceptron-based methods also have certain shortcomings, e.g., they can only deal with real inputs. When the data are linearly separable, there are many solu-tions, and which one is found depends on starting values. When the data are non-linearly separable, the algorithms will not converge due to the fact that they inevitably produce cycles. The cycles can be long and therefore hard to detect [ 10 ]. As a kernel mapping is needed for non-linearly separable data, and the accuracy of ranking rules is also sensitive to the kernel mapping.

Decision trees can, to some degree, overcome these shortcomings of perceptron-based methods. An intuitive idea to extend decision trees being able to deal with ranking problems implies the formulation of ranking problems as multi-class classification problems. However, degree of their deviations from true ratings. Intuitively, the discrimination can be measured by the impurity on a set. A pair of irrelevant items should cause more impurity than a relevant paper, we formulate this intuitive assumption by a new impurity measure, which introduces a metric on a set. The new impurity measure takes ordinal relationships into account, e.g., makes corresponding splitting rules to prefer child nodes with closer ranking distance, which is theoretically proved in this paper. Based on the new impurity, a decision tree is trained on ranking data sets, which we call Ranking Tree (RT), to assign a new example a ranking label.

The remainder of this paper is organized as follows: Sect. 2 gives a brief description of decision trees. In Sect. 3 , we describe two impurity metrics, that is, the gini impurity and our ranking impurity measure. In Sect. 4 , we provide a theoretical basis for our ranking impurity measure and analyze its capacity in ranking problems. In Sect. 5 , we illustrate experimental results and compare them with results in [ 11 ]. In Sect. 6 , we conclude the paper and give some possible research issues in the near future. 2 Decision tree Decision trees can produce good predictions and easy-to-interpret rules. It can also accept continuous, discrete and categorical inputs, fill missing values, and select relevant features to produce simple rules. It is invariant under strictly monotone transformations of individual inputs. Decision tree learning is a method for approximating discrete-valued target functions. Learned trees can also be represented as sets of if-then rules to improve human readability. Each node in the tree specifies a test of an instance on selected attributes, and each branch descending from that node corresponds to one of the possible values for this attribute. An instance is classified by starting at the root node of the tree, testing the attribute specified by that node, then moving down the selected branch to a new node. This process is iterated until one or several leaf nodes are reached [ 9 ]. Finally, classifications associated with one or several leaf nodes are returned and possibly combined to assign a label to the instance [ 13 ].

Decision tree algorithms contain two important parts, namely the splitting criterion and the pruning criterion. The splitting criterion is used to grow the tree. In each splitting, algorithms check every possible splitting point, and choose the point where a certain impurity measure decreases most. When the process of the growth is finished, the pruning criterion is used to prune the tree in order to increase its generalization ability. The impurity measure differs in different decision tree algorithms. In classification, CART [ 3 ] takes the gini criterion, while ID3 [ 14 ]andC4.5[ 15 ] prefer the entropy criterion. Experimentally, the gini criterion and entropy criterion are statistically indistinguishable [ 1 ]. In this paper, we propose a new impurity measure for the ranking process in decision trees, which can be readily plugged into nearly all decision tree algorithms with little adaptations. 3 The splitting rules One of the most commonly used impurity measures in classification problems is the gini impurity, as follows: 3.1 The gini impurity Definition 1 Given an example set T and let p i = p ( i | T ) be the proportion of examples belonging to class i ,where i  X  X  1 ,..., k } is the class label, the gini impurity (also known as the gini index) is defined as
There are two interesting interpretations of the gini impurity. If an example belongs to class loss on all classes is given by k i = 1 p i ( 1  X  p i ) . Similarly, if each example is coded as 1 Summing over classes k gives the gini impurity again, i.e., k i = 1 p i ( 1  X  p i ) .
A splitting operation divides a set T into two sets T L and T R , namely the left child and the right child of T , respectively. The splitting rule of the gini impurity is to find the best splitting X  X he one that maximizes the quantity of expected decrease in terms of the gini impurity defined as where p ( T L ) and p ( T R ) denote the proportion of examples going to the left child, and the right child, respectively.

To prevent the creation of a degenerated tree, the decrease I must be non-negative. It is easy to prove that the gini impurity is strictly concave [ 3 ].

The gini index is well suitable for classification tasks. In the ranking task, however, the gini index ignores ordinal relationships among class labels. Consider the first interpretation of the gini impurity. Misclassifying an example from class i to any other class produces an equal portion of loss. Whereas, in ranking problems mis-ranking an item further away from its actual ranking means more loss. To deal with such an unbalance loss in ranking situations, we retain another impurity measure for splitting rules to build a ranking tree. 3.2 The ranking impurity We now present our new impurity measure named ranking impurity.
 Definition 2 Givenanexampleset T labeled by a totally ordered label set L ={ L 1 ,..., L k } , let N i ( T ) be the number of elements in T that have label L i , the ranking impurity is given by The ranking impurity can be interpreted as follows. Suppose that a 1  X  T belongs to label L , a 2  X  T belongs to label L 2 ,and L 1 &lt; L 2 . These two examples form a mis-ranked pair if a 1 is ranked after a 2 . The mis-ranked pair can be weighted by the difference between their rankings, that is, L 2  X  L 1 . Since every example has the potential of drawing a weighted mis-ranked pair with each of the examples ranked before it, the maximum potential number of weighted mis-ranked pairs for the example is given by the number of all examples ranked before it. Summing over all examples gives the ranking impurity.
The splitting rule of our ranking impurity searches for the best splitting point that could maximize the quantity of decrease in terms of the ranking impurity. It X  X  defined as
The objective of the splitting rule can be interpreted as to minimize the maximum potential number of weighted mis-ranked pairs in both T L and T R .

The I in ( 4 ) is positive iff neither T L nor T R is empty, as shown in the following proposition: Proposition 1 Iin ( 4 ) is non-negative. It is positive iff neither T L nor T R is empty. Proof Note that N i ( T ) = N i ( T L ) + N i ( T R ) ,then
Thus, I is non-negative. The equality applies only when both N i ( T L ) and N i ( T R ) are zero. Therefore, if neither T L nor T R is empty, I is positive. 4 Theoretical proof In this section we will give a theoretical basis of the proposed ranking impurity ( 3 ), and ranking impurity. Then we will compare it with the gini impurity and point out the deficiency of gini impurity in ranking settings.
 to 0  X  x i  X   X , a ij = 0 for every i , j  X  X  1 ,... k } pair, achieves its extremum value at the boundary.
 Proof This is a continuous quadratic function on a close-bounded region. So it can attain its extremum at stationary points or the boundary. It suffices to show that the extremum cannot be achieved at any stationary point.
 x for every j  X  X  1 ,..., k } . The stationary point is then the solution of the following equation: where a ij = a ji .

Let A k =
We take the Hessian of F ( x 1 , x 2 ,..., x k ) to analyze whether the stationary point is an extreme point, which is applicable to be A k as well. Since tr ( A k ) = 0 , A k must have both positive and negative eigenvalues and cannot be positive definite, thus the stationary point cannot be the extremum.
 Theorem 1 Given an example set T labeled by a totally ordered label set L ={ L 1 ,..., L k } , the right node after a splitting. Suppose that N 1 ( T ) = N 2 ( T ) =  X  X  X  = N k ( T ) =  X  and the examples can arbitrarily go to either the left node or the right node. Then the best splitting sof is achieved at [ we assume N 1 ( T L ) = 0] : k is even: k is odd: and Proof Accordingto( 3 ), it follows that Note N 1 ( T ) = N 2 ( T ) =  X  X  X  = N k ( T ) =  X , and for every i  X  X  1 , ... k } , N i ( T L ) + N ( T R ) =  X  . So for every i, N i ( T R ) =  X   X  x i ,and0  X  x i  X   X  ,if N i ( T L ) = x i .

Reformulate ( 6 )as
According to Lemma 1 ,( 7 ) achieves the maximum on the boundary. Without loss of generality, we add a boundary condition x j = 0. Replacing x j with 0 in ( 7 ), the new function still satisfies the conditions in Lemma 1 . This procedure can be repeated until vertices of the boundary are reached. The maximum is achieved at one of the vertices. Comparison among all vertices gives the result.

Theorem 1 shows that when the number of examples with different labels are the same at node T , examples with closer rankings tend to go together in a splitting.

Now we come to the case where the numbers of examples with different labels are different at a node T . Let us consider the case of k = 3. (Analyses in the splitting for k &gt; 3 can be done in a similar manner.) Theorem 2 Given an example set T labeled by a label set { 1 , 2 , 3 } ,letN i ( T ) =  X  i be the number of elements that have label i in T, s a split, T L the left node and T R the right node after a splitting. Suppose the examples can arbitrarily go to either the left node or the right node. Then the best split will be achieved by the following rules [ similar to Theorem 1 we assume N 1 ( T L ) = 0] : If  X  2  X  2  X  1 and  X  3  X   X  1 as illustrated in Fig. 1 a .
 If  X  2  X  2  X  3 and  X  1  X   X  3 as illustrated in Fig. 1 b .
 N ( T R ) = 0 as illustrated in Fig. 1 c .
 Proof Reformulate ( 6 ) for the case of k = 3. Similar to the proof of Theorem 1 , Lemma 1 proves Theorem 2 .

Fig. 1 Theorem 2 shows that in the case of k = 3, in order to avoid separating out examples with label 2, the number (of examples) with label 2 should be less than either twice of that with label 1, or twice of that with label 3.

Now, let us consider the case of k = 3 for the gini impurity. When the numbers of examples with different labels are the same, e.g., N 1 ( T ) = N 2 ( T ) = N 3 ( T ) =  X  ,it N N more  X  X ure X  than the latter. Roughly speaking, the ranking impurity emphasizes the role of individual examples while the gini impurity emphasizes the role of individual classes. Meanwhile, the former takes ordinal relationships into account. The ranking impurity groups examples with closer ratings together in each splitting step. Computationally, the two impurity measures share the same goal of making leaf nodes pure. However, the process of these two splittings can be very different because of the greedy nature of tree-based algorithms. The ranking impurity is more suitable for ranking processes in decision trees. 5 Experiments and discussion We made some experiments to compare the Ranking Tree with the perceptron-based ranking algorithms and the Classification Tree on the data set used in [ 11 ], including a synthetic data set and several real-world data sets. In our experiments the Classification and Regression Tree (CART) was used as the fundamental decision tree algorithm. The implementation of CART is based on the rpart package in R (refer to http://www.r-project.org ). 5.1 Ranking tree with the synthetic data set Firstly, random points ( x 1 , x 2 ) were generated according to the uniform distribution on the unit square [0,1]  X  [0,1]. Secondly, each point is assigned a ranking y chosen from the set { 1 0 . zero mean and standard deviation of 0.125. We took the average rank loss 1 T T t = 1 | where T is the number of examples in the test set, to measure the performance of algorithms. It quantifies the accuracy of predictive rankings
Similar to [ 11 ], we took 20 Monte-Carlo trials with 50,000 training examples and a separate test set of 1,000 examples. The Ranking Tree and the Classification Tree are applied on these data sets. Cross-validation was made to choose the depth of the tree. Table 1 shows experimental results of our RT algorithms and the Classification Tree comparing with results reportedin[ 11 ]. The lowest value among the results is boldfaced and the results are repre-sented with their corresponding 95% confidence intervals with the Student X  X  t distribution.
Ta b l e 1 shows that perceptron-based algorithms achieve worse performance than algo-rithms based on decision trees do. The main reason might be the difficulty of getting a good kernel mapping in the problem context. As can be seen from Table 1 , the Classification Tree performs as well as the Ranking Tree on the synthetic data. However, the Ranking Tree shows a faster convergence rate with respect to the depth of the tree, as shown in Fig. 2 . This coincides with our previous theoretical analysis of the capacity of the ranking impurity. That is, the Ranking Tree could make better partitions than the Classification Tree in term of accuracy as well as speed. The fast convergence rate is very important when time and costs of obtaining values of features are important.

The processes of the partitioning of Classification Tree and Ranking Tree are illustrated in Figs. 3 and 4 respectively, to investigate the splitting difference between them. Those algorithms were applied to 1,000 synthetic examples. The symbols (e.g.,  X * X ,  X  X  X ,  X + X ,  X  X  X  respectively. From Fig. 3 a, we can see that the first splitting of the Classification Tree is near the boundary of the region. This is clearly a consequence of the gini impurity, which favors smaller pure child nodes. However, the splitting near the boundary surely increases the difficulty of fully classifying the entire data set. The Ranking Tree (Figure 4 a) in this case makes a much wiser initial splitting in the middle of the region, which facilitates further splittings. The following splittings also prove the same story. It can be seen clearly that, when the depth of the tree is small, the Ranking Tree gives a much more regular model than the Classification Tree. This implies better generalization of the Ranking Tree in the sense of Occam X  X  Razor, which can be validated by the results shown in Fig. 2 . 5.2 Ranking with collaborative filtering conditions to compare the Ranking Tree with the perceptron-based ranking algorithms and the Classification Tree. The experimental data is composed of two collaborative filtering data sets: Cystic Fibrosis [ 19 ] and MovieLens data sets. The Cystic Fibrosis data set consists of items where each entry is given as a query-document-rating triple, and the MovieLens data set as user-item-rating triples. We randomly chose a target ranking y t on an item, and then used the remaining ratings as dimensions of an instance vector x t . The detailed experimental setups for each data set are described as below.

The Cystic Fibrosis data set is a set of 100 queries with relevant documents. There are 1,239 documents published from 1974 to 1979 describing Cystic Fibrosis aspects. Each query-document pair has three ratings of highly relevant, marginally relevant and not relevant, which are valued as 3, 2, 1, respectively in our experiments. For each pair, four ratings are assigned. Therefore, a feature vector has three dimensions and a target ranking. The sizes of training set and test set are 4,247 and 572, respectively.

As the EachMovie data set used in [ 11 ] is not available, we took the MovieLens data from the GroupLens Research Project ( http://www.grouplens.org/data/ ). The data set consists of 100,000 ratings (1 X 5) from 943 users on 1,682 movies, with each user rating at least 20 movies. Some similar experiments to [ 11 ] were performed, where only those people (54 persons) who rated over 300 movies were considered. Consider a specific person as the reference to validate ranking results, the dimension of a movie instance vector is 53. The feature vector of a movie is constructed with ratings by the other 53 persons. Given a person, we searched for the first 300 movies rated by her/him, and formed instances by using her/his ratings as target rankings. If one of them has not seen the selected movie, a value  X 3 X  is assigned to it as her/his rating. In our experiments, 210 random movies form the training set and the other 90 movies act as the test set.

We applied the Ranking Tree and the Classification Tree to the two collaborative filte-ring data sets, and compared experimental results with those reported in [ 11 ]. The results were averaged on 500 Monte-Carlo trials, as given in Table 2 . The lowest values among the results are boldfaced and the results are represented with their corresponding 95% confidence intervals with the Student X  X  t distribution. We can find that the Ranking Tree significantly out-performs the other two algorithms (the Classification Tree and the perceptron-based ranking algorithms) on the Cystic Fibrosis data set. Perceptron-based algorithms perform relatively worse than algorithms based on decision trees do. The might be the same reason as in syn-thetic data set that it is difficult to find a good kernel mapping in the problem context. On the MovieLens data, both the Classification Tree and the Ranking Tree prefer trees with fewer nodes. It turns out that stumps (trees with depth 1) perform rather well in the MovieLens case. This might imply that, given sufficient recommenders, it X  X  easy to find a recommender with most similar interests and preferences to a given user.

An important property of tree classifiers is the natural feature selection. A decision tree uses only a small subset of input features to make predictions. Thus decision tree algorithms are vastly different from kernel algorithms in terms of the model space. Our experiments also proved the fact that decision tree models can approximate many realistic ranking problems much better than kernel algorithms. Interpretability, speed and accuracy are important to many realistic problem-solving applications such as the ranking. Compared to the Classifi-cation Tree and the perceptron-based ranking algorithms, the Ranking Tree not only achieves relatively higher accuracy, but also gets faster speed and good interpretability. As a derivation of decision trees, an inherent problem with the Ranking Tree is the instability. Any small change in the data set might result in a very different series of splittings. The major reason for the instability is the hierarchical nature of the splitting process: the effect of errors in early splittings is propagated down to the following splittings. We believe that some ideas relevant to bagging and boosting might be a good solution to the instability problem of the Ranking Tree. 6 Conclusions In this paper, we presented a novel ranking approach based on decision trees, namely the Ranking Tree. With a new impurity measure, the Ranking Tree is an extension of decision trees in the direction of dealing with ranking problems. This paper also provided a theoretical basis for our Ranking Tree algorithms and experimentally validated their effectiveness.
Our algorithms were applied to data sets in Harrington X  X  previous works [ 11 ]. Experimen-tal results showed that the Ranking Tree X  X  performance is closer to the true rankings than other ranking algorithms such as the Classification Tree and the pereptron-based ranking algorithms. We also illustrated the difference between classification and ranking, which pro-ved that the Ranking Tree is more robust than the Classification Tree.

In the current stage we mainly deal with totally ordered ratings. There are many ranking problems where the order structure is much more complex. E.g., an example set might have several subsets, and each of them is defined with a different order. It would be interesting to extend the Ranking Tree to deal with ranking problems in these application scenarios. We also intended to investigate the instability inherited in the Ranking Tree resorting to some ideas relevant to bagging and boosting.
 References Author X  X  biography
