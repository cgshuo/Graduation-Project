 vishy@stat.purdue.edu , sunz@stat.purdue.edu , ntheeraa@cs.purdue.edu Research on Multiple Kernel Learning (MKL) needs to follow a two pronged approach. It is im-portant to explore formulations which lead to improvements in prediction accuracy. Recent trends indicate that performance gains can be achieved by non-line ar kernel combinations [7,18,21], learn-ing over large kernel spaces [2] and by using general, or non-sparse, regularisation [6, 7, 12, 18]. Simultaneously, efficient optimisation techniques need to be developed to scale MKL out of the lab and into the real world. Such algorithms can help in investig ating new application areas and different facets of the MKL problem including dealing with a very large number of kernels and data points. Optimisation using decompositional algorithms such as Seq uential Minimal Optimization (SMO) [15] has been a long standing goal in MKL [3] as the algor ithms are simple, easy to im-plement and efficiently scale to large problems. The hope is t hat they might do for MKL what SMO did for SVMs  X  allow people to play with MKL on their laptops, m odify and adapt it for diverse real world applications and explore large scale settings in term s of number of kernels and data points. Unfortunately, the standard MKL formulation, which learns a linear combination of base kernels subject to l 1 regularisation, leads to a dual which is not differentiable . SMO can not be applied as a result and [3] had to resort to expensive Moreau-Yosida regu larisation to smooth the dual. State-of-the-art algorithms today overcome this limitation by solvi ng an intermediate saddle point problem rather than the dual itself [12,16].
 Our focus, in this paper, is on training p -norm MKL, with p &gt; 1 , using the SMO algorithm. More generally, we prove that linear MKL regularised by certain B regman divergences, can also be trained using SMO. We shift the emphasis firmly back towards solving t he dual in such cases. The l p -MKL dual is shown to be differentiable and thereby amenable t o co-ordinate ascent. Placing the p -norm squared regulariser in the objective lets us efficient ly solve the core reduced two variable optimisation problem analytically in some cases and algori thmically in others. Using results from [4, 9], we can compute the l p -MKL Hessian, which brings into play second order variable s election methods which tremendously speed up the rate of convergence [8]. The standard decompositional method proof of convergence [14] to the global optimum holds with minor modifications. The resulting optimisation algorithm, which we call SMO-MK L, is straight forward to implement and efficient. We demonstrate that SMO-MKL can be significant ly faster than the state-of-the-art specialised p -norm solvers [12]. We empirically show that the SMO-MKL alg orithm is robust with that our algorithm is well suited for learning both sparse, a nd non-sparse, kernel combinations. Furthermore, SMO-MKL scales well to large problems. We show that we can efficiently combine a hundred thousand kernels in approximately seven minutes o r train on fifty thousand points in less than half an hour using a single core on standard hardware whe re other solvers fail to produce results. The SMO-MKL code can be downloaded from [20]. Recent trends indicate that there are three promising direc tions of research for obtaining performance improvements using MKL. The first involves learning non-lin ear kernel combinations. A framework for learning general non-linear kernel combinations subje ct to general regularisation was presented in [18]. It was demonstrated that, for feature selection, th e non-linear GMKL formulation could perform significantly better not only as compared to linear M KL but also state-of-the-art wrapper methods and filter methods with averaging. Very significant p erformance gains in terms of pure classification accuracy were reported in [21] by learning a d ifferent kernel combination per data point or cluster. Again, the results were better not only as c ompared to linear MKL but also baselines such as averaging. Similar trends were observed for regress ion while learning polynomial kernel combinations [7]. Other promising directions which have re sulted in performance gains are sticking to standard MKL but combining an exponentially large number of kernels [2] and linear MKL with p -norm regularisers [6, 12]. Thus MKL based methods are begin ning to define the state-of-the-art for very competitive applications, such as object recognit ion on the Caltech 101 database [21] and object detection on the PASCAL VOC 2009 challenge [19].
 In terms of optimisation, initial work on MKL leveraged gene ral purpose SDP and QCQP solvers [13]. The SMO+M.-Y. regularisation method of [3] wa s one of the first techniques that could efficiently tackle medium scale problems. This was sup erseded by the SILP technique of [17] which could, very impressively, train on a million point pro blem with twenty kernels using paral-lelism. Unfortunately, the method did not scale well with th e number of kernels. In response, many two-stage wrapper techniques came up [2, 10, 12, 16, 18] whic h could be significantly faster when the number of training points was reasonable but the number o f kernels large. SMO could indirectly be used in some of these cases to solve the inner SVM optimisat ion. The primary disadvantage of these techniques was that they solved the inner SVM to optima lity. In fact, the solution needed to be of high enough precision so that the kernel weight gradien t computation was accurate and the algorithm converged. In addition, Armijo rule based step si ze selection was also very expensive and could involve tens of inner SVM evaluations in a single line s earch. This was particularly expensive since the kernel cache would be invalidated from one SVM eval uation to the next. The one big advantage of such two-stage methods for l 1 -MKL was that they could quickly identify, and discard, the kernels with zero weights and thus scaled well with the nu mber of kernels. Most recently, [12] have come up with specialised p -norm solvers which make substantial gains by not solving th e inner SVM to optimality and working with a small active set to bette r utilise the kernel cache. Given a set of base kernels { K k } and corresponding feature maps {  X  k } , linear MKL aims to learn a linear combination of the base kernels as K = P k d k K k . If the kernel weights are restricted to be non-negative, then the MKL task corresponds to learning a standard SVM in the feature space formed by concatenating the vectors The regularisation on the kernel weights is necessary to pre vent them from shooting off to infinity. Which regulariser one uses depends on the task at hand. In this Section, we limit ourselves to the p -norm squared regulariser with p &gt; 1 . If it is felt that certain kernels are noisy and should be discarded then a sparse solution can be obtained by letting p tend to unity from above. Alternatively, if the application demands dense solutions, then larger val ues of p should be selected. Note that the primal above can be made convex by substituting w k for We first derive an intermediate saddle point optimisation pr oblem obtained by minimising only w , b and  X  . The Lagrangian is Differentiating with respect to w , b and  X  to get the optimality conditions and substituting back results in the following intermediate saddle point problem where A = {  X  | 0  X   X   X  C 1 , 1 t Y  X  = 0 } , H k = Y K k Y and Y is a diagonal matrix with the labels on the diagonal. Note that most MKL methods end up optimising either this, or a very similar, saddle point problem. To now eliminate d we again form the Lagrangian the optimal value of  X  k is zero. Our l p -MKL dual therefore becomes and the kernel weights can be recovered from the dual variabl es as Note that our dual objective, unlike the objective in [3], is differentiable with respect to  X  . The SMO algorithm can therefore be brought to bear where two vari ables are selected and optimised using gradient or Newton methods and the process repeated un til convergence.
 Also note that it has sometimes been observed that l 2 regularisation can provide better results than l [6, 7, 12, 18]. For this special case, when p = q = 2 , the reduced two variable problem can be solved analytically. This was one of the primary motivati ons for choosing the p -norm squared regulariser and placing it in the primal objective (the othe r was to be consistent with other p -norm formulations [9, 11]). Had we included the regulariser as a p rimal constraint then the dual would have the q -norm rather than the q -norm squared. Our dual would then be near identical to Eq. (9 ) in [12]. However, it would then no longer have been possible t o solve the two variable reduced problem analytically for the 2 -norm special case. We now develop the SMO-MKL algorithm for optimising the l p MKL dual. The algorithm has three main components: (a) reduced variable optimisation; (b) wo rking set selection and (c) stopping criterion and kernel caching. We build the SMO-MKL algorith m around the LibSVM code base [5]. 4.1 The Reduced Variable Optimisation The SMO algorithm works by repeatedly choosing two variable s (assumed to be  X  1 and  X  2 without loss of generality in this Subsection) and optimising them w hile holding all other variables constant. If  X  1  X   X  1 +  X  and  X  2  X   X  2 + s  X  , the dual simplifies to where s =  X  y 1 y 2 , L = ( s == +1) ? max (  X   X  1 ,  X   X  2 ) : max (  X   X  1 ,  X  2  X  C ) , U = ( s == +1) ? min ( C  X   X  1 , C  X   X  2 ) : min ( C  X   X  1 ,  X  2 ) , a k = H 11 k + H 22 k + 2 sH 12 k , b k =  X  t ( H :1 k + sH :2 k ) and c k =  X  t H k  X  . Unlike as in SMO,  X   X  can not be found analyti-cally for arbitrary p . Nevertheless, since this is a simple one dimensional conca ve optimisation problem, we can efficiently find the global optimum using a var iety of methods. We tried bisection search and Brent X  X  algorithm but the Newton-Raphson method worked best  X  partly because the one dimensional Hessian was already available from the working set selection step. 4.2 Working Set Selection The choice of which two variables to select for optimisation can have a big impact on training time. Very simple strategies, such as random sampling, can have ve ry little cost per iteration but need many iterations to converge. First and second order working set s election techniques are more expensive per iteration but converge in far fewer iterations.
 We implement the greedy second order working set selection s trategy of [8]. We do not give the variable selection equations due to lack of space but refer t he interested reader to the WSS2 method of [8] and our source code [20]. The critical thing is that the selection of the first (second) variable involves computing the gradient (Hessian) of the dual. Thes e are readily derived to be where D has been overloaded to now refer to the dual objective. Rathe r than compute the gradient  X   X  D repeatedly, we speed up variable selection by caching, sepa rately for each kernel, H k  X  . The cache needs to be updated every time we change  X  in the reduced variable optimisation. However, since only two variables are changed, H k  X  can be updated by summing along just two columns of the kernel matrix. This involves only O ( M ) work in all, where M is the number of kernels, since the column sums can be pre-computed for each kernel. The Hess ian is too expensive to cache and is recomputed on demand. 4.3 Stopping Criterion and Kernel Caching We terminate the SMO-MKL algorithm when the duality gap fall s below a pre-specified threshold. Kernel caching strategies can have a big impact on performan ce since kernel computations can dominate everything else in some cases. While a few different kernel caching techniques have been explored for SVMs, we stick to the standard one used in LibSVM [5]. A Least Recently Used (LRU) cache is implemented as a circular queue. Each element in the queue is a pointer to a recently accessed (common) row of each of the individual kernel matri ces. We briefly discuss a few special cases and extensions which im pact our SMO-MKL optimisation. 5.1 2-Norm MKL As we noted earlier, 2-norm MKL has sometimes been found to ou tperform MKL trained with l 1 regularisation [6, 7, 12, 18]. For this special case, when p = q = 2 , our dual and reduced variable optimisation problems simplify to polynomials of degree fo ur Just as in standard SMO,  X   X  can now be found analytically by using the expressions for th e roots of a cubic. This makes our SMO-MKL algorithm particularly effic ient for p = 2 and our code defaults to the analytic solver for this special case. 5.2 The Bregman Divergence as a Regulariser The Bregman divergence generalises the squared p -norm. It is not a metric as it is not symmetric and does not obey the triangle inequality. In this Subsection, w e demonstrate that our MKL formulation can also incorporate the Bregman divergence as a regularise r.
 Let F be any differentiable, strictly convex function and f =  X  F represent its gradient. The objective leads to the following intermediate saddle point problem and Lagrangian Substituting back in the Lagrangian and discarding terms de pendent on just d 0 results in the dual In many cases the optimal value of  X  will turn out to be zero and the optimisation can efficiently b e carried out over  X  using our SMO-MKL algorithm.
 Generalised KL Divergence To take a concrete example, different from the p -norm squared used thus far, we investigate the use of the generalised KL diverg ence as a regulariser. Choosing F ( d ) = P k d k (log( d k )  X  1) leads to the generalised KL divergence between d and d 0 Plugging in r KL in I B and following the steps above leads to the following dual pro blem which can be optimised straight forwardly using our SMO-MKL algorithm once we plug in the gradient and hessian information. However, discussing thi s further would take us too far out of the scope of this paper. We therefore stay focused on l p -MKL for the remainder of this paper. 5.3 Regression and Other Loss Functions While we have discussed MKL based classification so far we can e asily adapt our formulation to handle other convex loss functions such as regression, nove lty detection, etc . We demonstrate this for the  X  -insensitive loss function for regression. The primal, int ermediate saddle point and final dual problems are given by SMO has a slightly harder time optimising D R due to the |  X  | term which, though in itself not differentiable, can be gotten around by substituting  X  =  X  +  X   X   X  at the cost of doubling the number of dual variables. In this Section, we empirically compare the performance of o ur proposed SMO-MKL algorithm against the specialised l p -MKL solver of [12] which is referred to as Shogun. Code, scri pts and parameter settings were helpfully provided by the authors a nd we ensure that our stopping criteria are compatible. All experiments are carried out on a single c ore of an AMD 2380 2.5 GHz processor with 32 Gb RAM. Our focus in these experiments is purely on tra ining time and speed of optimisa-tion as the prediction accuracy improvements of l p -MKL have already been documented [12]. We carry out two sets of experiments. The first, on small scale UCI data sets, are carried out using pre-computed kernels. This performs a direct comparison of the algorithmic components of SMO-MKL and Shogun. We also carry out a few large scale experiment s with kernels computed on the fly. This experiment compares the two methods in totality. In this case, kernel caching can have an effect, but not a significant one as the two methods have very s imilar caching strategies. For each UCI data set we generated kernels as recommended in [ 16]. We generated RBF kernels with ten bandwidths for each individual dimension of the fea ture vector as well as the full feature vector itself. Similarly, we also generated polynomial ker nels of degrees 1, 2 and 3. All kernels matrices were pre-computed and normalised to have unit trac e. We set C = 100 as it gives us a reasonable accuracy on the test set. Note that for some value of  X  , SMO-MKL and Shogun will converge to exactly the same solution [12]. Since this value is not known a priori we arbitrarily set  X  = 1 .
 Training times on the UCI data sets are presented in Table 1. M eans and standard deviations are reported for five fold cross-validation. As can be seen, SMO-MKL is significantly faster than Shogun at converging to similar solutions and obtaining similar te st accuracies. In many cases, SMO-MKL is more than four times as fast and in some case more than ten or twenty times as fast. Note that our test classification accuracy on Liver is a lot lower than Shog un X  X . This is due to the arbitrary choice of  X  . We can vary our  X  on Liver to recover the same accuracy and solution as Shogun w ith a further decrease in our training time.
 Another very positive thing is that SMO-MKL appears to be rel atively stable across a large operating range of p . The code is, in most of the cases as expected, fastest when p = 2 and gets slower as one increases or decreases p . Interestingly though, the algorithm doesn X  X  appear to be s ignificantly slower for other values of p . Therefore, it is hoped that SMO-MKL can be used to learn spar se kernel combinations as well as non-sparse ones.
 Moving on to the large scale experiments with kernels comput ed on the fly, we first tried combining a hundred thousand RBF kernels on the Sonar data set with 208 p oints and 59 dimensional features. Table 1: Training times on UCI data sets with N training points, D dimensional features, M kernels and T test points. Mean and standard deviations are reported for 5 -fold cross validation. 1.10 4.89  X  0.31 58.52  X  16.49 85.22  X  2.96 85.22  X  2.81 26.4  X  0.8 137.2  X  53.8 1.33 4.16  X  0.16 33.58  X  2.58 85.36  X  3.79 85.07  X  2.85 40.8  X  1.3 62.4  X  4.7 1.66 4.31  X  0.19 31.89  X  1.25 85.65  X  3.73 85.07  X  2.85 72.2  X  4.8 100.2  X  3.7 2.00 4.27  X  0.10 27.08  X  7.18 85.80  X  3.74 85.22  X  2.99 126.4  X  4.3 134.4  X  5.6 2.33 4.88  X  0.18 24.92  X  6.46 85.80  X  3.74 85.07  X  2.85 162.8  X  3.6 177.8  X  8.3 2.66 5.19  X  0.05 26.90  X  2.05 85.80  X  3.68 85.22  X  2.85 188.2  X  4.7 188.8  X  5.1 3.00 5.48  X  0.21 27.06  X  2.20 85.51  X  3.69 85.22  X  2.85 192.0  X  2.6 194.4  X  1.2 1.10 2.85  X  0.16 19.82  X  4.02 92.60  X  1.35 92.03  X  1.68 50.0  X  2.7 125.2  X  7.3 1.33 2.78  X  1.18 8.49  X  0.61 92.03  X  1.42 92.60  X  1.86 120.8  X  6.0 217.0  X  23.4 1.66 2.42  X  0.28 10.49  X  2.27 91.74  X  2.08 91.74  X  1.37 200.8  X  4.4 291.4  X  33.0 2.00 2.16  X  0.16 13.99  X  4.68 92.03  X  1.68 91.17  X  2.45 328.0  X  6.6 364.2  X  15.4 2.33 2.35  X  0.25 24.90  X  9.43 92.03  X  1.68 91.74  X  2.08 413.6  X  5.6 412.2  X  6.6 2.66 2.50  X  0.32 33.05  X  3.66 92.03  X  1.68 92.03  X  1.68 430.6  X  4.6 436.6  X  4.3 3.00 3.03  X  0.99 36.23  X  3.62 92.31  X  1.41 91.75  X  2.05 434.4  X  4.8 442.0  X  0.0 1.10 0.53  X  0.03 2.15  X  0.12 62.90  X  9.81 66.67  X  9.91 9.40  X  1.02 39.40  X  1.50 1.33 0.54  X  0.03 0.92  X  0.05 66.09  X  8.48 71.59  X  8.92 24.40  X  2.06 43.60  X  2.42 1.66 0.56  X  0.04 1.14  X  0.23 66.96  X  7.53 70.72  X  9.28 44.20  X  2.23 57.00  X  3.29 2.00 0.54  X  0.04 1.72  X  0.57 66.96  X  7.06 72.17  X  6.94 71.00  X  5.29 78.00  X  2.28 2.33 0.63  X  0.03 2.35  X  0.36 66.38  X  7.36 73.33  X  6.71 82.40  X  2.42 88.20  X  1.72 2.66 0.65  X  0.02 2.53  X  0.44 65.22  X  6.80 72.75  X  7.96 83.20  X  2.32 90.80  X  0.40 3.00 0.67  X  0.03 3.40  X  0.55 65.22  X  6.74 73.91  X  7.28 85.20  X  3.37 91.00  X  0.00 1.10 4.95  X  0.29 47.19  X  3.85 85.15  X  7.99 81.25  X  8.71 91.2  X  6.9 258.0  X  24.8 1.33 4.00  X  0.76 18.28  X  1.63 84.65  X  9.37 87.03  X  6.85 247.8  X  7.7 374.2  X  20.9 1.66 4.48  X  1.63 20.27  X  8.84 88.47  X  6.68 87.51  X  6.28 383.0  X  5.7 451.6  X  12.0 2.00 3.31  X  0.31 31.52  X  5.07 88.94  X  6.00 88.95  X  6.33 661.2  X  10.2 664.8  X  35.2 2.33 3.54  X  0.35 51.83  X  17.96 88.94  X  4.97 88.94  X  5.41 770.8  X  4.4 763.0  X  7.0 2.66 3.83  X  0.38 64.59  X  9.19 88.94  X  4.97 88.94  X  4.97 782.0  X  3.4 789.4  X  2.8 3.00 3.96  X  0.45 70.08  X  9.18 88.94  X  4.97 89.92  X  5.13 786.0  X  4.1 792.2  X  1.1 Note that these kernels do not form any special hierarchy so a pproaches such as [2] are not applica-ble. Timing results on a log-log scale are given in Figure (1a ). As can be seen, SMO-MKL appears to be scaling linearly with the number of kernels and we conve rge in less than half an hour on all hundred thousand kernels for both p = 2 and p = 1 . 33 . If we were to run the same experiment using pre-computed kernels then we converge in approximately sev en minutes (see Fig (1b)). On the other hand, Shogun took six hundred seconds to combine just ten tho usand kernels computed on the fly. The trend was the same when we increased the number of trainin g points. Figure (1c) and (1d) plot timing results on a log-log scale as the number of training po ints is varied on the Adult and Web data sets (please see [1] for data set details and downloads) . We used 50 kernels computed on the
Figure 1: Large scale experiments varying the number of kern els and points. See text for details. fly for these experiments. On Adult, till about six thousand p oints, SMO-MKL is roughly 1.5 times faster than Shogun for p = 1 . 33 and 5 times faster for p = 2 . However, on reaching eleven thousand points, Shogun starts taking more and more time to converge a nd we could not get results for sixteen thousand points or more. SMO-MKL was unaffected and converg ed on the full data set with 32,561 points in 9245.80 seconds for p = 1 . 33 and 8511.12 seconds for p = 2 . We tried the Web data set to see whether the SMO-MKL algorithm would scale beyond 32K p oints. Training on all 49,749 points and 50 kernels took 1574.73 seconds (i.e. less than ha lf an hour) with p = 1 . 33 and 2023.35 seconds with p = 2 . We developed the SMO-MKL algorithm for efficiently optimisi ng the l p -MKL formulation. We placed the emphasis firmly back on optimising the MKL dual rat her than the intermediate saddle point problem on which all state-of-the-art MKL solvers are based. We showed that the l p -MKL dual is differentiable and that placing the p -norm squared regulariser in the primal objective lets us analytically solve the reduced variable problem for p = 2 . We could also solve the convex, one-dimensional reduced variable problem when p 6 = 2 by the Newton-Raphson method. A second-order working set selection algorithm was implemented to speed up convergence. The resulting algorithm is simple, easy to implement and efficiently scales to large p roblems. We also showed how to generalise the algorithm to handle not just p -norms squared but also certain Bregman divergences. In terms of empirical performance, we compared the SMO-MKL a lgorithm to the specialised l p -MKL solver of [12] referred to as Shogun. It was demonstrated that SMO-MKL was significantly faster than Shogun on both small and large scale data sets  X  so metimes by an order of magnitude. SMO-MKL was also found to be relatively stable for various va lues of p and could therefore be used to learn both sparse, and non-sparse, kernel combinati ons. We demonstrated that the algorithm could combine a hundred thousand kernels on Sonar in approxi mately seven minutes using pre-computed kernels and in less than half an hour using kernels c omputed on the fly. This is significant as many non-linear kernel combination forms, which lead to p erformance improvements but are non-convex, can be recast as convex linear MKL with a much lar ger set of base kernels. The SMO-MKL algorithm can now be used to tackle such problems as long a s an appropriate regulariser can be found. We were also able to train on the entire Web data set w ith nearly fifty thousand points and fifty kernels computed on the fly in less than half an hour. O ther solvers were not able to return results on these problems. All experiments were carr ied out on a single core and therefore, we believe, redefine the state-of-the-art in terms of MKL opt imisation. The SMO-MKL code is available for download from [20].
 We are grateful to Saurabh Gupta, Marius Kloft and Soren SSon nenburg for helpful discussions, feedback and help with Shogun.

