 Prateek Jain prajain@microsoft.com Microsoft Research Labs, Bangalore, INDIA Abhradeep Thakurta b-abhrag@microsoft.com Stanford University and Microsoft Research Silicon Valley Campus Modern systems can log and mine a lot of data to learn interesting trends/patterns which can then be used for different tasks. Typically, these logs contain sensitive information from individuals and releasing the learned patterns/model might compromise an individual X  X  pri-vacy. To address this issue, there have been several works that study the problem of privacy preserving learning (Rubinstein et al., 2009; Pathak et al., 2010; Chaudhuri et al., 2011).
 Typically, the goal of these works is to learn a privacy preserving parameter vector w  X  (for example a linear classifier) from the data which also generalizes well on unseen test data. There are several notions of privacy in the literature, but differential privacy (Dwork et al., 2006b) has been one of the most theoretically sound and widespread notion.
 In particular, Chaudhuri et al. (2011); Rubinstein et al. (2009) introduced a general framework for differ-entially private regularized empirical risk minimization (ERM) that guarantees privacy as well as small  X  X x-cess X  error, in addition to the generalization error of the best non-private solution. Popular applications of such methods include private version of Support Vec-tor Machines (SVM) and logistic regression.
 However, the above private algorithms are mostly re-stricted to the  X  X inear X  case only, where each data point lies in a low dimensional vector space and can be explicitly accessed. Moreover, to provide privacy, these methods need to add noise proportional to the dimensionality of the data ( d ). Hence, their gen-eralization error guarantees become weak for high-dimensional datasets. Chaudhuri et al. (2011) and Rubinstein et al. (2009) briefly looked at kernel ERMS where the dimensionality ( d ) can be allowed to be po-tentially infinite. However, their algorithms are re-stricted to the class of translation invariant kernels and does not capture a wide variety of kernels, e.g., polynomial kernels or pyramid match kernel from the computer vision domain (Grauman &amp; Darrell, 2007). In this work, we study the problem of differentially private learning using kernel ERM (kERM), where ac-cess to each data point is through a kernel function only. As the optimal solution to kERMs can only be obtained implicitly and is defined in terms of the training data points themselves, it is not clear how a solution to general kERMs can be released privately (Chaudhuri et al., 2011). To address this issue, we propose three simpler but practical models for privacy preserving kERM. For each model, we provide an algo-rithm that guarantees differential privacy to the train-ing points, while also providing strong  X  X tility X  guar-antees, i.e., error bounds. In contrast to bounds by (Chaudhuri et al., 2011; Rubinstein et al., 2009), our bounds are independent of the dimensionality of the feature space.
 The key insight behind our models is that the end goal of solving kERM for a user is to obtain accurate pre-dictions for its test set. Now, while providing explicit access to the optimum of the kERM (without violating privacy) might be difficult, privacy preserving predic-tions on the test set should still be possible. Here, by  X  X rivacy preserving X  we mean privacy of the training data points. Each of our model tries to exploit this insight in a different setting.
 Our first model (see Figure 1 (a)) is an interactive model, where the user sends its test data to a trusted learner (who solves an ERM over the training points), and the learner sends back predictions over those points. A typical scenario would be an online ad sys-tem where the learner (for example, Google) uses click-logs to predict whether a particular ad is relevant for a given user. See Section 5 for more details. For this model, we adapt an algorithm by Gupta et al. (2011) and show that we can provide accurate privacy pre-serving predictions for a large number (exponentially many in the training set size) of test points. Our second model is  X  X emi-interactive X , where the user sends a small subset of its test data to the trusted learner and the learner returns a differentially private version ample scenario can be a learner using a private dataset from a biology lab to learn a model that it uses to pre-dict a disease over some public dataset (NIH, 2003) (see Section 6.1). Another example can be situations where a user is willing to  X  X ell X  his/her privacy. For this model, we provide an algorithm that takes as in-put a random sample from the test set (or the test distribution) and outputs vate with respect to the training data. We also show that test set, in addition to the error incurred by w  X  . (Here n is the size of the training data.) We stress that the  X  X oodness X  of its distribution) only and might not extend to all the possible input points. We demonstrate the practical validity of our theoretical bounds by performing ex-periments with the polynomial kernel.
 Our third model is a non-interactive model where the learner is oblivious to the test data but still sends a that is private and guarantees accurate predictions for all possible test points which are in the feature space representation of the domain points. While this model is similar to the model of Chaudhuri et al. (2011), there is a subtle distinction. In the traditional model, we require the difference in predictions | X  w  X   X  to be small for all v while in our model we require | X  w  X   X  space representation of domain points. The kERM approach of Rubinstein et al. (2009); Chaudhuri et al. (2011) addresses this problem to some extent, but their methods are restricted to translation invariant kernels and essentially reduces the kERM to a linear ERM. Our algorithm for the non-interactive model can also be applied to standard linear ERMs. Here, our sam-ple complexity is O ( d 1 / 3 ) compared to O ( d ) samples required by (Chaudhuri et al., 2011; Rubinstein et al., 2009).
 Finally, we provide empirical validation of our semi-interactive method on benchmark datasets using poly-nomial kernels (where existing methods do not apply directly). Our results show that for reasonably small  X  0 . 1, our method achieves accuracy similar to the non-private baseline classifier. Additionally, in the set-tings where the dimensionality of the problem is large, and where algorithms of (Chaudhuri et al., 2011) and ours are applicable, experimentally our algorithm we outperform them. 1.1. Contributions: We consider the problem of differentially private kernelized learning and study it under three practical models. Our algorithms for the first two models are computationally efficient but for the third model they can have exponential time complexity for some kernel functions.
 Interactive : Our interactive model is useful for several learning tasks faced by online systems like ad-systems, recommendation systems. We provide an efficient algorithm that can accurately predict for exponentially many test points, in terms of error bound and training points.
 Semi-interactive : Our semi-interactive model is useful when public test sets are available. Here, we provide an efficient differentially private algorithm with additional generalization error that is indepen-dent of the dimensionality of the data.
 Non-interactive : Finally, we provide a privacy pre-serving algorithm with generalization error bound for the standard learning model but where kernel function is restricted to a function of low-dimensional vector spaces. Although our algorithm for this setting might not be computationally efficient in general, but for the case of linear kernels we can prove it to be efficient. Recently, there has been a lineage of results which try to understand the privacy implications of learning al-gorithms (Rubinstein et al., 2009; Chaudhuri et al., 2011; Pathak et al., 2010; Williams &amp; McSherry, 2010; Chaudhuri &amp; Hsu, 2011; Kifer et al., 2012; Jain et al., 2012). Typically, the generalization error of these algo-rithms depends polynomially on the dimensionality of the parameter vector. One notable exception is the method by (Kifer et al., 2012), that achieves loga-rithmic dependence on the dimensionality when the underlying dataset satisfies Restricted Strong Convex-ity (RSC) property. Another notable exception is the work of (Chaudhuri et al., 2011), which shows that if the underlying kernel function is translation invariant, then their method outputs a differentially private pa-rameter vector contrast, our algorithms for releasing private parame-ter vectors applies to general RKHS where one might not have a translation invariant reproducing kernel. Another relevant work is by Hall et al. (2012) that pro-vides a method to release function values in RKHS in a differentially private manner. Their techniques can be directly applied to our models but a direct adaptation of their technique suggests significantly worse utility guarantees. For example, using their technique with our interactive model, one can make accurate differen-tially private predictions for only quadratically many (in the number of training points) test points while our method can predict for exponentially many test points. Blum et al. (2008) also proposed differentially private techniques for estimating model parameters (or the optima to the ERM). However, their method as-sumes that the class of possible parameter vectors has a finite number of candidate parameter vectors. Ad-ditionally, the running time of the algorithm by Blum et al. (2008) is exponential in the size of the output. Finally, Gupta et al. (2011); Hardt &amp; Rothblum (2010); Hardt et al. (2010) used online learning tech-niques to interactively answer linear queries where the error in each query only depends logarithmically on the number of queries. Our algorithms adapt tech-niques by these methods, for our kERM problem, to provide accurate predictions/parameter vectors in our interactive and semi-interactive models. Privacy : In this work, we select the notion of differ-ential privacy (Definition 1) for guaranteeing privacy of each of the input data point. Differential privacy is a well studied privacy notion and has emerged as a well accepted definition of privacy for statistical data analysis (Dwork, 2006; 2010). Intuitively, the defini-tion requires that perturbing an individual data point should not lead to any noticeable change in the distri-bution over the space of possible outputs of a random-ized algorithm A .
 Definition 1 (Differential privacy (Dwork et al., 2006b;a)) . An algorithm A is ( , X  ) -differentially pri-vate if for any two datasets G , G 0  X  ( X X Y ) n s.t. G and G 0 differ in exactly one data point, and for all measur-able sets O  X  Range ( A ) , the following holds: Here ( X  X Y ) is the domain of the data entries. Convex Optimization and Kernel Methods : We assume that the loss functions are convex and are of the form: ` : R d  X   X  ( X  X Y )  X  R , where X is the input domain and Y is the target output domain. For a given prediction function f : X  X  R d  X   X  R , the loss function is alternatively represented as ` ( f ( w , x ) ,y ), d  X  refers to the output dimensionality of the feature we assume that there exists a kernel function K which can efficiently compute the inner product of any  X  ( x ) Additionally, we assume that the feature map  X  is L  X  -Lipschitz and the loss function ` is L -Lipschitz in the first parameter. We denote vectors in bold ( w ) while matrices with capital letters ( X ). In this section, we study the problem of differentially private learning using regularized empirical risk mini-mization (ERM) in kernel space. ERM is a canonical learning method for several concept classes; regular-ization helps in avoiding over-fitting to the training data and guaranteeing good generalization error. In general, a regularized ERM can be written as the fol-lowing optimization problem: where f : X  X  R d  X   X  R is the prediction function, ( x i ,y i )  X  X  X  Y are training samples from a fixed (unknown) distribution P , i.e., ( x i ,y i )  X  P . X is the input domain, while Y is the prediction domain. For example, Y = { X  1 , +1 } for classification problem. Here, ` is the (surrogate) convex loss function for the predictions f (  X  ,  X  ).
 Typically, f is selected to be a linear prediction func-cal applications have complex decision boundaries that cannot be captured by linear predictors. In fact for several applications (e.g., computer vision) obtaining a vector representation for data points itself might not be possible. A common approach to circumvent this problem is by using  X  X ernel trick X . That is each x i is mapped to a higher dimensional feature space and f is required to be linear in that mapped feature space. That is, f ( w , x i ) =  X  w , X  ( x i )  X  , where  X  : X  X  and d  X  is the dimensionality of the mapped feature space. Also, K ( x , v ) =  X   X  ( x ) , X  ( v )  X  is assumed to be an efficiently computable kernel function.
 In this work, we restrict our ERM formulation to L 2 regularization, i.e., r ( w ) = 1 2 k w k 2 2 . Formally, we con-sider the following general ERM: w  X  = arg min where d  X  is the dimensionality of mapped feature space,  X  &gt; 0 is the regularization parameter. Note that w  X  is dependent on each of the data point and observing it might lead to privacy compromise of an individual point ( x i ,y i ). Hence, the goal is to provide predictions that are similar to predictions using op-timal w  X  while preserving privacy of each individual point. To guarantee privacy, we use the well-known notion of differential privacy (Definition 1). Recently Chaudhuri et al. (2011); Rubinstein et al. (2009) proposed algorithms for approximately solv-ing (2) while preserving differential privacy. However, their algorithms are either restricted to linear decision functions ( f ( w , x ) =  X  w , x  X  ) (which has polynomial dependence of the error on the dimensionality of the feature space) or non-linear decision functions using a restricted class of translation-invariant kernels, e.g., the Gaussian kernel.
 In contrast, we address the above mentioned prob-lem of privately learning from data points in arbitrary RKHS. Now, w  X  is a d  X  dimensional vector, where d  X  can potentially be very large and might not even be representable explicitly. Kernel methods exploit the fact that w  X  is of the form w  X  = P i  X  i  X  ( x i ) and hence maintain  X  i , 1  X  i  X  n to store/use w  X  . As observed by Chaudhuri et al. (2011), releasing w  X  in this case is hard, since the learner does not have explicit access to w  X  and in fact relies on each training point (or sup-port vector) x i to compute prediction values. Hence, the  X  X raditional X  model of releasing a differentially pri-vate X might be too restrictive for this problem.
 In the next section, we consider an interactive model that does not release w  X  explicitly but provides predic-tions similar to w  X  while preserving privacy. Then in Section 6.1, we provide a semi-interactive model where a differential private version of w  X  is released but ac-curacy in its predictions are guaranteed only for the data coming from a fixed distribution. Finally in Sec-tion 6.2, we provide a method for our non-interactive model where the learner releases a differentially pri-vate version of w  X  with guaranteed accurate predic-tions (compared to w  X  ) over entire input domain X . However, here, we need to restrict input space X and also unlike our other methods, this method might re-quire exponential (in n ) amount of computation time. See Figure 1 for a block schematic of our three models. In this section, we describe our interactive model for releasing privacy preserving predictions for given test points. Our model consists of three parties: a dataset, a learner, and a user. A trusted learner obtains su-pervised data from the dataset and learns model pa-rameters w  X  by solving (2). Then, user sends its test points to the learner (in online/batch mode) and the learner provides predictions that preserve privacy of each training point and are also close to the predic-tions obtained using w  X  .
 Example Scenario 1 : Consider the case of an spon-sored search ad system where the ad delivery engine needs to find the relevance of a particular ad for a given user query. Now, typically ad engines use stored user-click logs to learn a classifier for such tasks. In the context of our differentially private interactive model, the goal of the ad engine (learner) would be to pre-dict relevance of a given ad, user query pair (user test point) accurately while preserving privacy of the train-ing data, i.e, user-click logs (dataset).
 Similar privacy preserving learning scenarios can be found in several other online systems as well, such as recommendation system, social networks etc.
 Example Scenario 2 : Consider two hospitals A and B . Hospital A sends its labeled data (health profile of its patients) to a trusted research lab (learner) that learns a classifier using the supplied data by A . Then, Hospital B sends its unlabeled data to the trusted lab (learner), which returns back reasonably accurate la-bels for the test data from B while guaranteeing pri-vacy to each data point from A .
 We now formalize our model. The dataset provides labeled samples G = { ( x 1 ,y 1 ) ,..., ( x n ,y n ) } to the learner using which learner estimates w  X  by solving (2). The user provides its test points over T rounds. During each round, the user provides the learner a test point z t  X  X for which the learner predicts differentially private w.r.t. G . The learner should en-sure that with high probability,  X  t , |  X  , where  X  is a fixed parameter.
 For this problem, we adopt a recent technique from the literature of private interactive dataset release: itera-tive dataset construction (IDC) method (Gupta et al., 2011). Gupta et al. (2011) show that their IDC based method can be used to release accurate answers to lin-ear queries over a private dataset. In the context of our problem, the private  X  X ataset X  is w  X  and the goal is to answer linear queries a high level, the algorithm at each step maintains a dif-ferentially private version w t of the true dataset w  X  . For a given query z t , algorithm first tries to answer using w t , i.e., is inaccurate w.r.t. w  X  , i.e., | X  w t , z t  X  X  X  X  w  X  , z then added noise, i.e., calibrated noise  X  t . Also, in this case, the algorithm updates w t so that it gets  X  X loser X  to w  X  .
 Main Algorithmic Idea: In the case of learning with kernels, we cannot explicitly maintain w t . However, it is easy to see that w t obtained using IDC updates is of the form w t = P t  X  =1  X   X   X  ( z  X  ). Further, each z public (to the user). Hence, to maintain differentially private w t , we need to maintain differentially private  X   X  X  only. Algorithm 1 (Algorithm PINP) provides a pseudo-code of our method for releasing predictions in the case of (non-linear) kernels. Below, we provide both privacy as well as utility (i.e., accuracy in predic-tion w.r.t. w  X  ) guarantees for PINP.
 Theorem 2 (Privacy Guarantee) . Let w  X  be the op-timal solution to (2) and let gorithm 1 for a given point z t . Then, each out-put differentially private w.r.t. input training samples G = { ( x 1 ,y 1 ) ,..., ( x n ,y n ) } .
 See supplementary material for a proof of Theorem 2. Utility analysis of Algorithm 1 Theorem 3 (Error bound) . Let prediction by Algorithm 1 for the t -th step test point z . Let w  X  = P n i  X  i  X  ( x i ) be the optimal solution to (2) with k w  X  k 2  X  C . Then, with probability at least 1  X   X  , for each prediction 1 is bounded by: The above theorem shows that the error in Algorithm 1 depends only logarithmically on the number of test points ( T ), while increasing the number of training points ( n ) rapidly decreases error incurred. See Ap-pendix A.2 for a proof sketch of the above theorem. Note that we assume a differentially private bound C on k w  X  k 2 . A simple bound of C = 2 LR  X   X  follows di-rectly using optimality of w  X  . Otherwise, C can also be estimated differentially privately by adding a small assume that k w  X  k 2 is available publicly and hence can be used by our algorithms without violating privacy. In the previous section we presented an algorithm each of the test samples Z = { z 1 , z 2 ,..., z T } within O ( p (log T ) /n ) error while guaranteeing privacy for each of the n training samples ( G ). However, in this model the  X  X earner X  itself needs to answer each test query which might be undesirable for both  X  X earner X  and the  X  X ser X .
 In some applications, it would be useful for the  X  X earner X  to release a differentially private version of w  X  so that it would have similar generalization error as w  X  on test samples of the  X  X ser X . As mentioned earlier in Section 4, the learner maintains w  X  only im-plicitly in terms of training data points ( x i ,y i ). Hence, it seems impossible for the learner to privately release w  X  unless it makes assumption about the test samples or makes assumptions about the input space X and/or the kernel space K . (Chaudhuri et al., 2011) took the later approach for differentially private kernel ERM, i.e., they assumed that X is a finite dimensional vector space and K is translation invariant. However, for several applica-tions one or both of the above mentioned constraints may not be satisfied.
 In this section we propose two different models for Algorithm 1 Private Interactive Non-linear Predic-tion Algorithm (PINP) Require: Optimum of (2): w  X  = P n i =1  X  i  X  ( x i ); Test 1: Set the bound on the number of updates: B  X  2: Noise parameter: 0  X  n X  3: Set  X  0  X  0 // Implicitly assume , w 0 = 0 4: for t  X  X  1 ,  X  X  X  ,T } and counter &lt; B do 5: b a t  X  X  w  X  , X  ( z t )  X  +  X  t = P n i =1  X  i K ( z t 7: if | b d t | &gt;  X  then 8:  X  t  X   X  sign( b d t ) and counter  X  counter + 1 9: Output prediction: b y t = b a t 10: else 11:  X  t  X  0 12: Output prediction: b y t = P t  X  =1  X   X  K ( z  X  , z t 13: end if 14: end for this problem: 1) Test Data Dependent Learner ( Semi-interactive model ) and 2) Test Data Independent Learner ( Non-interactive model ). In the first model (see Section 6.1), we assume that the learner is pro-vided with a small set of random samples from the test data and then the learner provides a differentially private set. In this model, we do not make any assumption about the input space X or the kernel function K . Our second model is agnostic to the test data, i.e., it does not seek any test samples, but it needs to as-sume X to be a low-dimensional vector space (i.e., the dimensionality has to be lesser than O ( n 2 ), see Sec-tion 6.2 for more details). Furthermore, unlike our al-gorithm for the semi-interactive setting, our algorithm (for the non-interactive setting) can potentially take exponential amount of time in the dimensionality of X . One notable exception is the case of linear ker-nels, where we can provide an efficient algorithm for our non-interactive setting as well. Also note that, for linear kernels, non-interactive setting is essentially the standard setting of differentially private ERM intro-duced by (Chaudhuri et al., 2011). 6.1. Test Data Dependent Learner (Semi-interactive model) In this section, we propose our test data dependent model for differentially private release of the optimal solution w  X  to the regularized kernel based ERM (2). In this model, we assume that the user sends a small subset Z = { z 1 ,..., z T } of its test data points Q = { q 1 ,  X  X  X } to the learner and the learner in turn sends b w which is: 1) differentially private w.r.t. training data G and 2) has small excess error on the test data in addition to the error incurred by w  X  . Figure 1 (b) shows our semi-interactive model.
 Example scenario: This scenario is motivated by the HapMap project of the National Institute of Health (NIH). HapMap (NIH, 2003) is a public dataset that contains genetic data from four populations with African, Asian, and European ancestry. Several ge-nomic research labs, with great effort, collect labeled genetic data using which they can learn a good (non-linear) classifier (for say a particular disease). In the context of our semi-interactive model, the private ge-netic data of the lab is the private dataset and the lab is the learner . While the lab (learner) would like to (or is required to) predict disease for new data points (test points), it wouldn X  X  like to violate privacy of its own data. Hence, a privacy preserving mechanism would be required to release a reasonably accurate (non-linear) classifier. While this task for general kernels seems infeasible, one can exploit the publicly available HapMap data to release differentially private classifier that guarantees good accuracy on sample points  X  X im-ilar X  to the HapMap dataset. That is, the HapMap dataset would be the sample from test set that our model requires.
 Our Solution : The main idea of our solution is to learn a differentially private  X  w that gives  X  X im-ilar X  prediction to w  X  on the dataset Z while also preserving privacy. We then use standard stochas-tic optimization based generalization error guarantees to argue for excess error incurred by  X  w for the en-tire test domain. To this end, we first ensure that each prediction over Z is differentially private by adding appropriate noise b t for all t . We then learn  X  w by solving a simple least squares problem:  X  w = arg min that b t typically has to scale as O ( mentioned approach; one can use algorithm given in the previous section to add only O (log T ) noise per b t However, the analysis is more involved and it doesn X  X  provide significantly stronger utility guarantees. Below, we show that serves privacy of training data G as well as incurs small additional error on Q compared to w  X  .
 Privacy: The privacy guarantee follows directly from the composition property of differential privacy (Dwork et al., 2010); see supplementary material. Theorem 4. Algorithm 2 is ( , X  ) -differentially pri-Algorithm 2 Test Data Dependent Learner (TDDL) Require: Optimal solution to (2): w  X  = 1: Sample random entries b 1 ,  X  X  X  ,b T i.i.d. from 2: Output  X  w = arg min . vate.
 Utility: The theorem below shows that given a fixed set Q , the learner can supply a differentially private that incurs small loss in additional to the loss incurred by w  X  . See supplementary material for a proof. Theorem 5 (Error bound) . Let Q = { q 1 ,..., q | Q | } be the test set, let Z = { z 1 ,  X  X  X  , z T } be sam-pled uniformly at random from Q . If T = O ( kCk 2 n X  ) / ( LR  X  p log(1 / X  )) and w  X   X  C in Al-gorithm 2, then w.p. 1  X   X  , Here y q i refers to the label for the test point q i . We can also easily generalize our result from finite set Q to the general distribution P from which Z = { z 1 , z 2 ,..., z T } is sampled. See Theorem 13 (Ap-pendix B.3) for an exact statement.
 Next, we provide a generalization error bound for our privacy preserving algorithm when both training and test points are sampled from the same distribution. Corollary 6 (Generalization Error Bound) . Let both training and test samples be i.i.d. samples from P . Let C = { w : k w k 2  X  2 LR  X  / X  } , where R  X  and  X  are as defined in Algorithm 2, T = O ( n ) / ( p log(1 / X  )) , tained using Algorithm 2, we have (w.p. 1  X   X  ): Note that the above sample complexity of our Algo-rithm 2 is independent of the dimensionality. In con-trast, existing privacy preserving learning methods like (Chaudhuri et al., 2011) has polynomial dependence on the dimensionality. Naturally, key difference is that we have an additional requirement of a small test data set sample. Another way to understand this difference is that our method guarantees that | ` ( is small only if z is sampled from P . However, these algorithms can guarantee that | ` ( small for all z  X  R d . 6.2. Test Data Independent Learner (Non-interactive model) In the previous section, we proposed a model where the user needs to send a random sample of its test data to obtain a differentially private perform well on all of the test set. However, this model is not applicable in scenarios where the user cannot or does not want to share his test data with a trusted third party, i.e., with the learner .
 In this section, we analyze the later setting where the user does not have access to unlabeled samples. Fig-ure 1 (c) shows our model. The model is same as traditional differentially private ERM model (Chaud-huri et al., 2011), where a learner sends a differentially private version of the optimum w  X  ( i.e. , For this model, our approach is similar to that in Sec-tion 6.1 except that the algorithm itself generates a se-quence of test samples Z = { z 1 ,  X  X  X  , z T } . The goal is that the generated samples z 0 t s should be able to distin-guish the current iterate w t = P t i =1  X  i  X  ( z i ) from the underlying parameter vector w  X  . That is, z t  X  X  force Al-gorithm 1 to make updates to its differentially private model parameters w t . Furthermore, we require to find z  X  X , in a differentially private manner. To this end, we use the exponential mechanism, a well-studied tech-nique in the privacy literature (McSherry &amp; Talwar, 2007). Broadly speaking, we sample z t from a dis-That is, probability of sampling a z is higher if | X   X  ( z ) , w t  X  w  X   X  X  is large, i.e., if z can  X  X istinguish X  w t and w  X  . Also, to ensure differential privacy, the distribution introduces appropriate randomness. See Algorithm 3 for a pseudo-code of our approach. Note that, we assume that the input space is restricted to be a vector space, i.e., each z t  X  R d , k z t k 2  X  1. Now, we provide the privacy and utility guarantees. Theorem 7. Algorithm 3 is ( , X  ) -differentially pri-vate.
 Broadly, our proof follows by combining the analysis of exponential mechanism (McSherry &amp; Talwar, 2007) and the privacy proof of Algorithm 1 (Theorem 2). See supplementary material for a proof sketch. Next we show that the predictions obtained using Algorithm 3 is  X  X imilar X  to the ones obtained using w  X  for all points z  X  X  , where X = { z  X  R d s.t. k z k 2  X  1 } is the input space.
 Algorithm 3 Test Data-independent Learner (TDIL) Require: Optimal solution to (2): w  X  = 1: Bound on number of updates: B  X  n 100 LR 2: Set  X  0  X  0 ,  X  X  X  , X  B  X  0, w 0  X  0 3: Let S be  X  -net on the ball { z : k z k 2  X  r  X  } . 4: for t = { 1 ,  X  X  X  ,B } do 6: b d t =  X  w  X  , X  ( z )  X  X  X  P t i =0  X  t  X  ( z ) T  X  ( z 7: if | b d t | &gt;  X  then 8:  X  t  X   X  sign( b d t ). Output: b a t  X   X  w  X  , X  ( z t 9: else 10:  X  i  X  0. Output : P t i =0  X  i  X  ( z i ) T  X  ( z t ). 11: end if 13: end for 14: Output: b w = w B .
 Theorem 8 (Error bound) . Let L  X  be the Lips-chitz constant for the feature map  X  , X = { z  X  R d s.t. k z k 2  X  1 } and w  X  be the optimal solution to (2) . Then, with probability at least 1  X   X  , the output of Algorithm 3 ( for all z  X  R d and k z k 2  X  1 . C 1 &gt; 0 is a global constant.
 See supplementary material for a proof. We note that although the dependence of the generalization error is worse with respect to the size of the training set (i.e., n 2 / 3 as compared to n in (Chaudhuri et al., 2011)), the error in our case depends only on the dimensionality ( d ) of the low-dimensional space ( X ) instead of the di-mensionality ( d  X  ) of the kernel space as in (Chaudhuri et al., 2011).
 We would like to stress that Algorithm 3 may not be always computationally efficient. However, for the spe-cial case of linear kernels, an efficient version can be constructed via sampling from a uniform mixture of two log-concave distributions. In this section, we present experimental evaluation of our TDDL method for the semi-interactive model (Al-gorithm 2). The goal of these experiments is to demon-strate that our TDDL method, while guaranteeing pri-vacy, is practical and do not deteriorate accuracy sig-nificantly for reasonable privacy requirements. Addi-tionally, we demonstrate that our algorithm is signif-icantly more accurate than (Chaudhuri et al., 2011) for high-dimensional datasets in the traditional linear kernel setting.
 For our first experiment, we applied our method to the CoverType dataset (see supplementary material for re-sults on the URL Reputation dataset). We used non-linear SVM as our classifier and trained them using 500 K training examples. We selected a third-degree polynomial kernel as our kernel function, K ( x , y ) = tion invariant and hence the method of (Chaudhuri et al., 2011) does not apply for this problem. We used LibSVM (Chang &amp; Lin, 2011) for training SVMs and report results averaged over 5 runs. The penalty pa-rameter for SVM training was set to be C = 0 . 001. Figure 2 (a) plots our methods results for the Cover-Type dataset. Baseline accuracy of the non-private learner is 72 . 3%. Note that for  X  0 . 1, our method performed similarly to the baseline method.
 Next, we study our algorithms in the linear but high-dimensional model. For this, we selected the KDD Cup 2010 dataset that contains data points embedded in around 2 M dimensions. We selected 20000 train-ing points randomly and learned a linear regression function. Figure 2 (b) compares ` 2 regression error ( k X w  X  y k 2 ) incurred by our method with the Objec-tive Perturbation method of (Chaudhuri et al., 2011) and the non-private least squares method. Clearly, our method achieves significantly smaller error when com-pared to the method of (Chaudhuri et al., 2011). The reason is that our method adds noise whose magni-tude is independent of the dimensionality d , while for (Chaudhuri et al., 2011) the norm of the noise varies linearly with d . We also stress that our method as-sumes semi-interactive model where it has access to a small subset of the test set; in contrast, (Chaudhuri et al., 2011) doesn X  X  exploit such a subset and hence as such is at a disadvanatage.
 Blum, Avrim, Ligett, Katrina, and Roth, Aaron.
A learning theory approach to non-interactive database privacy. In STOC , pp. 609 X 618. ACM, 2008.
 Chang, Chih-Chung and Lin, Chih-Jen. LIBSVM: A library for support vector machines. ACM Trans-actions on Intelligent Systems and Technology , 2: 27:1 X 27:27, 2011. Software available at http://www. csie.ntu.edu.tw/ ~ cjlin/libsvm .
 Chaudhuri, Kamalika and Hsu, Daniel. Sample com-plexity bounds for differentially private learning. Journal of Machine Learning Research -Proceedings Track , pp. 155 X 186, 2011.
 Chaudhuri, Kamalika, Monteleoni, Claire, and Sar-wate, Anand D. Differentially private empirical risk minimization. JMLR , 12:1069 X 1109, 2011.
 Dwork, Cynthia. Differential privacy. In ICALP (2) , 2006.
 Dwork, Cynthia. Differential privacy in new settings. In SODA , 2010.
 Dwork, Cynthia, Kenthapadi, Krishnaram, Mcsherry,
Frank, Mironov, Ilya, and Naor, Moni. Our data, ourselves: Privacy via distributed noise generation. In In EUROCRYPT , pp. 486 X 503. Springer, 2006a. Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in pri-vate data analysis. In TCC , pp. 265 X 284. Springer, 2006b.
 Dwork, Cynthia, Rothblum, Guy N, and Vadhan,
Salil. Boosting and differential privacy. In FOCS , 2010.
 Dwork, Cynthia, Naor, Moni, and Vadhan, Salil. The privacy of the analyst and the power of the state. In FOCS , 2012.
 Grauman, Kristen and Darrell, Trevor. The pyramid match kernel: Efficient learning with sets of features.
Journal of Machine Learning Research , 8:725 X 760, 2007.
 Gupta, Anupam, Roth, Aaron, and Ullman, Jonathan. Iterative constructions and private data release. CoRR , abs/1107.3731, 2011.
 Hall, Rob, Rinaldo, Alessandro, and Wasserman,
Larry A. Differential privacy for functions and func-tional data. CoRR , abs/1203.2570, 2012.
 Hardt, Moritz and Rothblum, Guy N. A multiplica-tive weights mechanism for privacy-preserving data analysis. In FOCS , 2010.
 Hardt, Moritz, Ligett, Katrina, and McSherry, Frank.
A simple and practical algorithm for differentially private data release. CoRR , abs/1012.4763, 2010. Hsu, Justin, Roth, Aaron, and Ullman, Jonathan. Dif-ferential privacy for the analyst via private equilib-rium computation. CoRR , abs/1211.0877, 2012.
 Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep. Differentially private online learning. In COLT , 2012.
 Kifer, Daniel, Smith, Adam, and Thakurta,
Abhradeep. Private convex empirical risk minimiza-tion and high-dimensional regression. In COLT , 2012.
 McSherry, Frank and Talwar, Kunal. Mechanism de-sign via differential privacy. In FOCS , pp. 94 X 103. IEEE, 2007.
 NIH. The international hapmap project. Nature , 426: 789 X 796, 2003.
 Pathak, Manas A., Rane, Shantanu, and Raj, Bhik-sha. Multiparty differential privacy via aggregation of locally trained classifiers. In NIPS , pp. 1876 X 1884, 2010.
 Rubinstein, Benjamin I. P., Bartlett, Peter L., Huang,
Ling, and Taft, Nina. Learning in a large func-tion space: Privacy-preserving mechanisms for svm learning. CoRR , abs/0911.5708, 2009.
 Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and Sridharan, Karthik. Stochastic Convex Op-timization. In Proceedings of the Conference on Learning Theory (COLT) , 2009.
 Williams, Oliver and McSherry, Frank. Probabilis-tic inference and differential privacy. In NIPS , pp.
