 We present a simple way to improve document retrieval for question answering systems. The method biases the retrieval system toward documents that contain words that have appeared in other documents containing answers to the same type of question. The method works with virtually any retrieval system, and exhibits a statistically significant performance improvement over a strong baseline. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation, search process. Algorithms, Experimentation. Keywords: Document retrieval; question answering; document priors. of a question by performing document retrieval to find documents that are likely to contain an answer to the question. Retrieval results are either used directly, or forwarded to a passage retrieval system. Some systems ( e.g., Cui et al. [2]) place strong emphasis on the retrieval phase, performing sophisticated processing on the question and document set, and using complex algorithms to assess matches; others use traditional approaches to retrieval, relying on subsequent processing stages to narrow the search. document contains an answer to a question might be assessed before the question is seen. That is, we wanted to know whether certain documents have higher prior probability of containing question answers than others. The mathematics of many language modeling approaches to information retrieval easily admits the use of document priors (see e.g., Miller et al. [4]). However, this approach works only with certain retrieval systems, and may demand a complex training phase. retrieval system X  X  similarity metric is to augment the query to include terms that might appear in documents that have higher prior probability of relevance. For example, if we expect a document containing the word north to have a higher-than-average probability of being relevant to a WHERE question, we For our purposes, we treat correct and non-exact as relevant, and all others as non-relevant. non-relevant), we want to extract terms that are prominent in the relevant documents but not in the non-relevant documents. To do so, we first consider each document set separately, extracting words that appear frequently in that set, but relatively infrequently in the collection as a whole. We use a home-brew  X  X ffinity X  statistic for this purpose [3], but other measures, such as mutual information or the Dice coefficient, might work as well. The result is an ordered list of scored terms. We then score each term top scoring terms as the expansion terms. Sample augmentation terms are show in Table 1. Some augmentation terms are clearly sensible, such as north , near and miles for WHERE questions. Others, such as rock and ago , make less intuitive sense. Using retrieved but non-relevant documents to provide terms that should not appear in the augmentation lists eliminates most question-specific terms. However, a fair amount of noise remains in these augmentation term lists. Nonetheless, we used the unaltered lists in our experiments. -2004 data, and tested the approach using data from the TREC-2005 Document Ranking subtask of the Question Answering track [6]. This latter task used fifty questions, all with answer documents in the collection. Assessors generated binary relevance judgments for the pool of top documents from 77 submitted runs. type as described above. We then build a new query, comprising the terms from the original query, plus the expansion terms for the selected question type. We weight query terms at a ratio of 25:1 relative to the expansion terms. We used three-fold cross-validation on TREC-2002, -2003 and -2004 data to measure performance using ratios of 25:1, 100:1 and 250:1, with 25:1 proving to be the best of the three. A more accurate tuning of this parameter might lead to significant additional gains from the technique. Finally, we processed the augmented query normally; we used the HAIRCUT retrieval system [3] with unstemmed words as indexing terms, a unigram language model with  X  =0.5, and no blind relevance feedback. technique. Mean average precision without augmentation was 0.3364. This is a reasonably strong baseline; if entered into the TREC-2005 Document Ranking task, these results would have ranked sixth. When augmentation words were added to the queries, mean average precision rose to 0.3528. Augmentation thus produced a 4.8% relative improvement, which would have ranked fourth if entered in TREC-2005. This result is statistically significant, using a Wilcoxon test, to well below the 0.01 level. The percentage of questions with at least one relevant document in the top ten rose from 0.86 to 0.90. These gains were achieved with a 
