 Sparse coding has attracted significant attention in recent years because it has been shown to be pirically observed that high-dimensional sparse coding plus linear classifier is successful for image classification tasks such as PASCAL 2009 [7, 15].
 The empirical success of sparse coding can be justified by the oretical analysis [17], which showed ing (LCC), represents a new class of effective high dimensio nal non-linear function approximation methods with sound theoretical guarantees. Specifically, L CC learns a nonlinear function in high dimension by forming an adaptive set of basis functions on th e data manifold, and it has nonlinear approximation power. A recent extension of LCC with added lo cal tangent directions [16] demon-strated the possibility to achieve locally quadratic appro ximation power when the underlying data manifold is relatively flat. This also indicates that the non linear function approximation view of sparse coding not only yields deeper theoretical understan ding of its success, but also leads to im-proved algorithms based on refined analysis. This paper foll ows the same idea, where we propose a scheme.
 The algorithm derived from this approach has some advantage s over the single-layer approach, and can also be extended into multi-layer hierarchical systems . Such extension draws connection to deep belief networks (DBN) [8], and hence we call this approa ch deep coding network. Hierarchi-sis function, and multi-layer basis functions provide a nat ural way to zoom into each single basis for finer local details  X  this intuition can be reflected more r igorously in our nonlinear function approximation result. Due to the more localized zoom-in eff ect, it also alleviates the problem of overfitting when many basis functions are needed. Second, it is computationally more efficient than flat coding because we only need to look at locations in the sec ond (or higher) layer corresponding duces many zero coefficients, the hierarchical structure si gnificantly eliminates many of the coding approach, our proposal of multi-layer coding requires fitti ng many small models separately, each with a small number of parameters. In particular, fitting the small models can be done in parallel, e.g. using Hadoop, so that learning a fairly big number of cod ebooks can still be fast. This section reviews the nonlinear function approximation results of single-layer coding scheme in the traditional sparse coding scheme called local coordina te coding (LCC), our analysis will rely on a similar modification.
 Consider the problem of learning a nonlinear function f ( x ) in high dimension: x  X  R d with large d . While there are many algorithms in traditional statistics that can learn such a function in low suffer the so called  X  X urse of dimensionality X . The recentl y popularized coding approach addresses Coordinate Coding can take advantage of the underlying data manifold geometric structure in order to learn a nonlinear function in high dimension and alleviat e the curse of dimensionality problem. The main idea of LCC, described in [17], is to locally embed po ints on the underlying data manifold into a lower dimensional space, expressed as coordinates wi th respect to a set of anchor points. The main theoretical observation was relatively simple: it was shown in [17] that on the data manifold, a nonlinear function can be effectively approximated by a glo bally linear function with respect to the local coordinate coding. Therefore the LCC approach turns a very difficult high dimensional nonlin-ear learning problem into a much simpler linear learning pro blem, which can be effectively solved is effective because the method naturally takes advantage o f the geometric information. In order to describe the results more formally, we introduce a number of notations. First we denote by kk the Euclidean norm (2-norm) on R d : Definition 2.1 (Smoothness Conditions) A function f ( x ) on R d is (  X ,  X ,  X  ) Lipschitz smooth with respect to a norm kk if and and where we assume  X ,  X ,  X   X  0 .
 These conditions have been used in [16], and they characteri ze the smoothness of f under zero-th, first, and second order approximations. The parameter  X  is the Lipschitz constant of f ( x ) , which  X  when k x  X  x  X  k is small,  X  measures how well f ( x ) can be approximated by a constant function,  X  measures how well f ( x ) can be approximated by a linear function in x , and  X  measures how well becomes smaller (locally when k x  X  x  X  k is small) if we use a higher order approximation. Similar to the single-layer coordinate coding in [17], here we define a two-layer coordinate coding as the following.
 C R second layer coordinate-coding pairs that refine the first la yer coding for every first-layer anchor point v  X  C 1 .
 The performance of LCC is characterized in [17] using the fol lowing nonlinear function approxima-tion result.
 Lemma 2.1 (Single-layer LCC Nonlinear Function Approximat ion) Let (  X  1 , C 1 ) be an arbi-We have for all x  X  R d : where w This result shows that a high dimensional nonlinear functio n can be globally approximated by a linear function with respect to the single-layer coding [  X  1 [ w bounds directly suggests the following learning method: fo r each x , we use its coding [  X  1 R learning method such as SVM, where [ w optimal coding can be learned using unlabeled data by optimi zing the right hand side of (1) over unlabeled data.
 In the same spirit, we can extend the above result on LCC by inc luding additional layers. This leads to the following bound.
 Lemma 2.2 (Two-layer LCC Nonlinear Function Approximation ) Let (  X , C ) = { (  X  1 , C 1 ) } X  Lipschitz smooth function. We have for all x  X  R d :  X  0 . 5  X  k x  X  h  X  1 ,C 1 ( x ) k + 0 . 5  X  X where w where w Similar to the interpretation of Lemma 2.1, bounds in Lemma 2 .2 implies that we can approximate a nonlinear function f ( x ) with linear function of the form where [ w minimizing the right hand side of (2) or (3).
 Compare with the single-layer coding, we note that the secon d term on the right hand side of (1) of the single-layer coding scheme (with a quadratic error te rm) becomes quadratic approximation power of the two-layer coding scheme (with a cubic error term ). The first term on the right hand then the error terms k x  X  h to the second term on the right hand side of (1). In such case th e two-layer coding scheme can the second layer uses local PCA instead of another layer of no nlinear coding. However, the bound in Lemma 2.2 is more refined and specifically applicable to nonli near coding. The bound in (2) shows the potential of the two-layer coding scheme in achieving hi gher order approximation power than single layer coding. Higher order approximation gives mean ingful improvement when each | C 2 ,v | large, then achieving higher order approximation does not l ead to meaningful improvement. In such of one-level coding scheme in (1). This is the situation wher e the 1st layer is mainly used to par-tition the space (while its approximation accuracy is not im portant), while the main approximation power is achieved with the second layer. The main advantage o f two-layer coding in this case is to save computation. This is because instead of solving a sin gle layer coding system with many parameters, we can solve many smaller coding systems, each w ith a small number of parameters. This is the situation when including nonlinearity in the sec ond layer becomes useful, which means that the deep-coding network approach in this paper has some advantage over [16] which can only approximate linear function with local PCA in the second lay er. We shall discuss the computational algorithm motivated by L emma 2.2. While the two bounds (2) Therefore we have to consider a mixed effect. Instead of mini mizing one bound versus another, we shall use them to motivate our algorithm, and design a method that accommodate the underlying intuition reflected by the two bounds. 3.1 Two Layer Formulation In the following, we let C 1 = { v  X  individual codebook at the second layer. We take a layer-by-layer approach for training, where the second layer is regarded as a refinement of the first layer, whi ch is consistent with Lemma 2.2. In the first layer, we learn a simple sparse coding model with all data: where  X  is some constant, e.g., if all X 1. For convenience, we not only enforce sum-to-one-constra int on the sparse coefficients, but also impose nonnegative constraints so that P (2) and (3): Note that neither sum to one or 1-norm regularization of coef ficients is needed in the derivation of (2), while such constraints are needed in (3). This means add itional constraints may hurt perfor-mance in the case of (2) although it may help in the case of (3). Since we don X  X  know which case is the dominant effect, as a compromise we remove the sum-to-one constraint but put in 1-norm to the following formulation for the second layer: where  X  the codings on both layers, the sparse representation of X where s is a scaling factor balances the coding from the two differen t layers. 3.2 Multi-layer Extension The two-level coding scheme can be easily extended to the thi rd and higher layers. For example, at the third layer, for each base v mization: 3.3 Optimization The optimization problems in Equations (4) to (6) can be gene rally solved by alternating the follow-ing two steps: 1) given current cookbook estimation v , compute the optimal sparse coefficients  X  ; 2) given the new estimates of the sparse coefficients, optimi ze the cookbooks.
 Step 1 requires solving an independent optimization proble m for each data sample, and it can be computationally very expensive when there are many trainin g examples. In such case, computa-tional efficiency becomes an important issue. We developed s ome efficient algorithms for solving the optimizations problem in Step 1 by exploiting the fact th at the solutions of the optimization problems are sparse. The optimization problem in Step 1 of (4 ) can be posed as a nonnegative quadratic programming problem with a single sum-to-one equ ality constraint. We employ an ac-optimal solutions are very sparse, the active set method oft en gives the exact solution after a few dozen of iterations. The optimization problem in (5) contai ns only nonnegative constraints (but not the sum-to-one constraint), for which we employ a pathwise p rojected Newton (PPN) method [3] that optimizes a block of coordinates per iteration instead of one coordinate at a time in the active set method. As a result, in typical sparse coding settings (f or example, in the experiments that we (e.g. 2048 dimension) nonnegative quadratic programming p roblem in milliseconds.
 Since the dual problem contains only nonnegative constrain ts, we can still employ projected Newton fairly mild conditions [3]. The computational cost in Step 2 is often negligible compared to the computational cost in Step 1 when the cookbook size is no more than a few thousand.
 in (5), the second-layer sparse coding is decomposed into L can be naturally parallelized. In our implementation, this is done through Hadoop. 4.1 MNIST dataset We first demonstrate the effectiveness of the proposed deep c oding scheme on the popular MNIST benchmark data [1]. MNIST dataset consists of 60,000 traini ng digits and 10,000 testing digits. In with codebook of size 64. For each of the 64 bases in the first la yer, a second-layer codebook was learned  X  the deep coding scheme presented in the paper ensur es that the codebook learning can be done independently. We implemented a Hadoop parallel pro gram that solved the 64 codebook learning tasks in about an hour  X  which would have taken 64 hou rs on single machine. This shows that easy parallelization is a very attractive aspect of the proposed deep coding scheme, especially for large scale problems.
 Table 1 shows the performance of deep coding network on MNIST compared to some previous an extra layer yields significant improvement on classificat ion; e.g. for L LCC method in [16] may also be regarded as a two layer method bu t the second layer is linear); the two-layer coding scheme here significantly improves the per formance with classification error rate of In fact, for the single-layer coding, our experiment shows t hat further increasing the codebook size will cause overfitting (e.g., with L contrast, the performance of two-layer coding still improv es when the second-layer codebook is as large as 512 (and the total codebook size is 64  X  512 = 32768 , which is very high-dimensional when high-dimensional representation is preferred in the c ase of using sparse coding plus linear classifier for classifications.
 Figure 1 shows some first-layer bases and their associated se cond-layer bases. We can see that the basis can come from Digit 7 , Digit 9 or even Digit 4 . Then, its second-layer bases help to further very informative. The zoomed-in details contained in deepe r bases significantly help a classifier to Table 1: The classification error rate (in % ) on MNIST dataset with different sparse coding schemes. Figure 1: Example of bases from a two-layer coding network on MNIST data. For each row, the The colorbar is the same for all images, but the range it repre sents differs from image to image  X  generally, the color of the background of a image represent z ero value, and the colors above and below that color respectively represent positive and negat ive values. 4.2 PASCAL 2007 The PASCAL 2007 dataset [6] consists of 20 categories of imag es such as airplanes, persons, cats, Among different methods, one particularly effective appro ach is to use sparse coding to derive a codebook of low-level features (such as SIFT) and represent an image as a bag of visual words [15]. Here, we intend to learn two-layer hierarchical codebooks i nstead of single flat codebook for the bag-of-word image representation.
 on each image using four scales, 7  X  7 , 16  X  16 , 25  X  25 and 31  X  31 with stepsize of 4. Then, the SIFT descriptors from all images (both training and vali dation images) were utilized to learn first-layer codebooks with different dimensions, L layer codebook, for each basis in the codebook, we learned it s second-layer codebook of size 64 by solving the weighted optimization in (5). Again, the seco nd-layer codebook learning was done in parallel using Hadoop. With the first-layer and second-la yer codebooks, each SIFT feature was coded into a very high dimensional space: using L Table 2: Average precision (in % ) of classification on PASCAL07 dataset using different spar se coding schemes. in total is 1024 + 1024  X  64 = 66 , 560 . For each image, we employed 1  X  1 , 2  X  2 and 1  X  3 spatial pyramid matching with max-pooling. Therefore in th e end, each image is represented by a counterpart.
 We would like to point out that, although we simply employed m ax-pooling in the experiments, it may not be the best pooling strategy for the hierarchical cod ing scheme presented in this paper. We as an open problem and is one of our future work. where a two-layer coding scheme is derived based on theoreti cal analysis of nonlinear functional approximation that extends recent results for local coordi nate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical m ultiple-layer manner. There are two main advantages of multi-layer coding: it can potentially achie ve better performance because the deeper layers provide more details and structures; it is computati onally more efficient because coding are decomposed into smaller problems. Experiment showed that t he performance of two-layer coding can significantly improve that of single-layer coding.
 For the future directions, it will be interesting to explore the deep coding network with more than two layers. The formulation proposed in this paper grants a s traightforward extension from two layers to multiple layers. For small datasets like MNIST, th e two-layer scheme seems to be already very powerful. However, for more complicated data, deeper c oding with multiple layers may be an large categories such as human, bikes, cups, and so on; then f or the human category, the second-layer coding may find difference among adult, teenager, and s enior person; and then the third layer may find even finer features such as race feature at different a ges.
 [1] http://yann.lecun.com/exdb/mnist/. [2] Samy Bengio, Fernando Pereira, Yoram Singer, and Dennis Strelow. Group sparse coding. In [3] D P. Bertsekas. Projected newton methods for optimizati on problems with simple constraints. [4] Dimitri P. Bertsekas. Nonlinear programming . Athena Scientific, 2003. [5] David Bradley and J. Andrew (Drew) Bagnell. Differentia ble sparse coding. In Proceedings [6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The [7] Mark Everingham. Overview and results of the classificat ion challenge. The PASCAL Visual [8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimens ionality of data with neural net-[9] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng . Efficient sparse coding algo-[10] Michael S. Lewicki and Terrence J. Sejnowski. Learning overcomplete representations. Neural [12] B.A. Olshausen and D.J. Field. Emergence of simple-cel l receptive field properties by learning [13] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Pack er, and Andrew Y. Ng. Self-taught [14] Marc Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sp arse feature learning for deep [15] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Li near spatial pyramid matching [16] Kai Yu and Tong Zhang. Improved local coordinate coding using local tangents. In ICML X  09 , [17] Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In
