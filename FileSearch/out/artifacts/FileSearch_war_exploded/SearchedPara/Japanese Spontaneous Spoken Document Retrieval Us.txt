 The search and retrieval of a document is generally conducted by matching keywords document, the document is regarded to be relevant to the input query. A fundamental problem of information retrieval (IR) is term mismatch . A query is usually a short and incorrect relevance ranking of documents with regard to the information need expressed in the query. 
For spoken document retrieval (SDR), it faces a new problem besides the term technology, the transcript produced by the speech recognition process always contains errors. In SDR, terms misrecognized will not match the query and the document rep-similar to the term mismatch. Here, we call this problem as the term misrecognition. 
Advanced users need tools that can find underlying concepts and not just search for been demonstrated effective in the tasks of spoken document retrieval. Chen [2] pro-posed a word topic model (WTM) to explore the co-occurrence relationship between alternative to the existing models, i.e. PLSA. 
Non-negative matrix factorization (NMF) [3] is also an approach in latent semantic space. It is a type of dimension reduction t echnique, and has distinct features of pre-tion (SVD) [4], the NMF uses non-negativity constraints; the decomposition is purely based representation. Also, the NMF computation is based on a simple iterative proc-ess, it is therefore advantageous for applications involving data sparseness, like large derived latent semantic space. We adopt the NMF-based document topic model (DTM) approach for spontaneous spoken document retrieval (SDR) in this study. Since the approaches of latent semantic indexing are based on the semantic relations, investigating the effectiveness of this DTM for SDR. The comparisons are conducted keyword matching. 
The rest of this paper is organized as follows: In Section 2, based on our previous formation retrieval, and explain the method to construct the topic model by using the factorized matrices of the NMF, and show how to compute relevance of target docu-method and the conventional tf-idf-based VSM. Finally, in Section 5, we present our with the term misrecognitions. The system presented here operates in two phases combining speech-based processing and text-based processing. 
In the speech-based processing phase, the spoken documents are transcribed by an automatic speech recognizer (ASR). The transcription of the ASR is in the form of an N-best list, in which the top N hypotheses of the ASR results are stored in the recog-hypotheses, and to compensate the effectiveness of term misrecognitions. frequency tf , which is defined as the number of a term occurs in a document and the inverse document frequency idf , are the two fundamental parameters. For the N-best, described as follows: Let D be a document modeled by a segment of the N-Best. P(w|o,D) is defined as the the occurrence of w in the N-Best. follows. Where the tf X  is the conventional term frequency, and is defined as follows: of w , as shown in the following equation: Here, N is the total number of documents contained in the corpus. C is the entire document set of the corpus. 
The term-document matrix A for NMF is finally built by using tf idf . By using the processing of NMF, a topic model is constructed, and is used for computing the rele-vance of target documents to the input query. 3.1 Document Topic Model and Information Retrieval ment D can be expressed as P(D|Q) . By applying the Bayes theorem, it can be trans-formed into: With the invariability of P(Q) over all documents, and assuming that document prob-ability P(D) has a uniform distribution, ranking the documents by the P(D|Q) can be words generated by the document : Each individual document D can be interpreted as a generative document topic model ated with this topic model: the probability of a latent topic given a document and the probability of word in a latent topic. So the probability of a query word i w generated by D is expressed by document model generated by D is thus represented by In this study, we compare the retrieval performance of the NMF with the conventional vector space vector (VSM) where the similarity between the query Q and docu-ment D is computed by following equation: 3.2 Link NMF to Topic Model Let A be the matrix produced in section 2 to stand for relationships among the terms = forms a normalized table to approximate the joint probability p(w,d) of term w , and document d . 
NMF is a matrix factorization algorithm [3] that finds the positive factorization of a given positive matrix. Assume that the given document corpus consists of K topics. The general form of NMF is defined as: negative matrices, G and H with dimension k m  X  and dimension n k  X  respectively. So from the equation (11), the joint probability p(w,d) can be expressed by To normalize G by  X  = can be rewritten as: Each entity ) 1 ( g w that would be generated by a latent topic k. , that is ) | ( k w P
Each entity n , k h of the matrix latent topic k , that is ) | P(M The ) | ( Based on the above equations, the equation (9 ) for relevance can be computed by the matrices G and H, the factorized matrices of the NMF. 4.1 Experimental setups Japanese) is the result of a Japanese national project on  X  X pontaneous Speech Corpus 1,400 speakers of various ages. About 95% of the CSJ corpus is devoted to spontane-ous monologues, such as academic presentations and public speaking, including man-ual transcriptions. This test collection is developed by the Japanese Spoken Document document retrieval systems. This collection consists of a set of 39 textual queries, the ognition (ASR) system, allowing retrieval of 2702 spoken documents of the CSJ. The large vocabulary continuous ASR system use an engine in which the acoustical model is trained by a corpus in the domain of travel [8], but the language model is trained by the manually-built transcript of the CSJ corpus. The word accuracy of the recognition consideration. For examples, for keyword sequence of  X   X  X  X  X   X  X  X   X  X   X  which corresponds to query text  X  X HN101801]  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ( search the utterances about the purposes or ef- X  pendent on the first keyword  X   X  X  X  X  (pet) X . 
For the data structure of test transcript, three types of transcripts of the same spoken documents are used for evaluations. (2) 1-best (denoted as 1bst ). (3) Manual transcript (denoted as tran ). The mean average precision ( MAP ) is used as the performance measure in this study. 4.2 Experimental Results 4.2.1 Retrieval Performance with Topic Number dimensional semantic space. 4.2.2 Effectiveness on different Data Type VSM methods. In this experiment, the number of topics was selected to be 1000. the 1-best one is incorrect [7]. In this paper, we proposed a NMF-based document topic model to explore the Japa-retrieval performance of the NMF-based topic model is found to be steadily improved ciently large, the NMF-based model outperforms the conventional tf-idf-based VSM. However, this fact also reveals that the merit of NMF-based topic model for retrieval is conditional on the number of topics. 
We show that as in the case of the VSM, the N-best is also effective to compensate for the misrecognition for the proposed NMF-based model. Moreover, its improve-ment (6.2%) from 1-best to N-best is also larger than the VSM(3.4%). This achieve-analyzing individual queries, the retrieval improvement mainly happens in those con-[HN101801] mentioned in above, its MAP is changed from 0.051 (VSM) to 0. 128 (NMF). 
In future work, the comparison of the NMF-based topic model to other topic mod-els such as PLSA, LDA will be analyzed in detail. Acknowledgments. This work was partly supported by a Grant-in-Aid for Scientific Research on Priority Areas in Japan as a part of Cyber Infrastructure for the Informa-tion Explosion Era, under Grant No. 19024074. 
