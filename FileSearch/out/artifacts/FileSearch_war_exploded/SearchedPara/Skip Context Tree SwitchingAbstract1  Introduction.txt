 Google DeepMind Franklin and Marshall College The sequential prediction setting, in which an unknown en-vironment generates a stream of observations which an al-gorithm must probabilistically predict, is highly relevant to a number of machine learning problems such as statistical language modelling, data compression, and model-based reinforcement learning. A powerful algorithm for this setting is Context Tree Weighting (CTW, Willems et al., 1995), which efficiently performs Bayesian model averag-ing over a class of prediction suffix trees (Ron et al., 1996). In a compression setting, Context Tree Weighting is known to be an asymptotically optimal coding distribution for D -Markov sources.
 A significant practical limitation of CTW stems from the fact that model averaging is only performed over predic-tion suffix trees whose ordering of context variables is fixed in advance. As we discuss in Section 3, reorder-ing these variables can lead to significant performance im-provements given limited data. This idea was leveraged by the class III algorithm of Willems et al. (1996), which performs Bayesian model averaging over the collection of prediction suffix trees defined over all possible fixed vari-able orderings. Unfortunately, the O (2 D ) computational requirements of the class III algorithm prohibit its use in most practical applications.
 Our main contribution is the Skip Context Tree Switching (SkipCTS) algorithm, a polynomial-time compromise be-tween the linear-time CTW and the exponential-time class III algorithm. We introduce a family of nested model classes, the Skip Context Tree classes, which form the ba-sis of our approach. The K th order member of this family corresponds to prediction suffix trees which may skip up to K runs of contiguous variables. The usual model class associated with CTW is a special case, and corresponds to K = 0 . In many cases of interest, SkipCTS X  X  O ( D 2 K +1 running time is practical and provides significant perfor-mance gains compared to Context Tree Weighting.
 SkipCTS is best suited to sequential prediction problems where a good fixed variable ordering is unknown a priori . As a simple example, consider the record aligned data de-picted by Figure 1. SkipCTS with K = 1 can improve on the CTW ordering by skipping the five most recent symbols and directly learning the lexicographical relation. While Context Tree Weighting has traditionally been used as a data compression algorithm, it has proven useful in a diverse range of sequential prediction settings. For ex-ample, Veness et al. (2011) proposed an extension (FAC-CTW) for Bayesian, model-based reinforcement learning in structured, partially observable domains. Bellemare et al. (2013b) used FAC-CTW as a base model in their Quad-Tree Factorization algorithm, which they applied to the problem of predicting high-dimensional video game screen images. Our empirical results on the same video game domains (Section 4.2) suggest that SkipCTS is par-ticularly beneficial in this more complex prediction setting. We consider the problem of probabilistically predicting the output of an unknown sequential data generating source. Given a finite alphabet X , we write x 1: n := x 1 x 2 ...x X n to denote a string of length n , xy to denote the concate-nation of two strings x and y , and x i to denote the concate-nation of i copies of x . We further denote x &lt;n := x and the empty string by . Given an arbitrary finite length string y , we denote its length by | y | . The space of proba-bility distributions over a finite alphabet X is denoted by P ( X ) . A sequential probabilistic model  X  is defined by a sequence of probability mass functions {  X  i  X  P ( X i ) } that satisfy, for any n  X  N , for any string x 1: n the constraint P x subscript to  X  n is always clear from its argument, we hence-forth write  X  ( x 1: n ) for the probability assigned to x We use  X  ( x n | x &lt;n ) to denote the probability of x tional on x &lt;n , defined as  X  ( x n | x &lt;n ) :=  X  ( x provided  X  ( x &lt;n ) &gt; 0 , from which the chain rule  X  ( x Q We assess the quality of a model X  X  predictions through its cumulative, instantaneous logarithmic loss P models M , we define the regret of  X  with respect to M as Our notion of regret corresponds to the excess total loss suffered from using  X  in place of the best model in M . In our later analysis, we will show that the re-gret of our technique grows sublinearly and therefore that lim n  X  X  X  R n (  X , M ) /n = 0 . In other words, the average instantaneous excess loss of our technique with respect to the best model in M asymptotically vanishes. 2.1. Bayesian Mixture Models One way to construct a model with guaranteed low regret with respect to some model class M is to use a Bayesian mixture model where w  X  &gt; 0 are prior weights satisfying P  X   X  X  w  X  = 1 . It can readily be shown that, for any  X   X  X  , we have which implies that the regret of  X  MIX ( x 1: n ) with respect to M is bounded uniformly by a constant that depends only on the prior weight assigned to the best model in M . For example, the Context Tree Weighting approach of Willems et al. (1995) applies this principle recursively to efficiently construct a mixture model over a doubly-exponential class of tree models.
 A more refined nonparametric Bayesian approach to mix-ing is also possible. Given a model class M , the switch-ing method (Koolen &amp; de Rooij, 2013) efficiently main-tains a mixture model  X  SWITCH over all sequences of mod-els in M . We review here a restricted application of this technique based on the work of Veness et al. (2012) and Herbster &amp; Warmuth (1998). More formally, given an indexed set of models {  X  1 , X  2 ,..., X  k } and an index se-quence i 1: n  X  X  1 , 2 ,...,k } n let be a model which predicts at each time step t according to the model with index i t . The switching technique implic-itly computes a Bayesian mixture over the exponentially many possible index sequences. This mixture is efficiently computed in O ( k ) per step by using where, for t  X  1 ...n , we have that and in the base case w  X , 0 := 1 /k for each  X   X  M . It can be shown (Veness et al., 2012) that for any  X  i 1: n we have times the index sequence switches models. In particular, if a single model performs best throughout,  X  SWITCH only in-curs an additional log n cost compared to a Bayesian mix-ture model using a uniform prior. The switching method is a key component of the Context Tree Switching algo-rithm, which we review in Section 2.3, as well as our new SkipCTS algorithm. In practice, especially when the mod-els in M are both adaptive and of varying complexity, the ability to rapidly switch between models often leads to an empirical performance improvement (Erven et al., 2007). A more comprehensive overview of switching strategies can be found in the work of Koolen &amp; de Rooij (2013). 2.2. Prediction Suffix Trees A Prediction Suffix Tree (Ron et al., 1996; Figure 2) is a type of variable order Markov model. Informally, a pre-diction suffix tree combines a collection of models using a data-partitioning tree, whose purpose is to select which particular model to use at each time step.
 Given finite strings c := c 1 ...c m  X  X m and x 1: n := x ...x n  X  X n , we say that c is a suffix of x 1: n if x n  X  i c m  X  i for all i  X  { 0 ,...,m  X  1 } , and that c is the context of x 1: n if it is a suffix of x &lt;n . We also write T { i  X  N : c is a suffix of x &lt;i , 1  X  i  X  n } to denote the set of time indices occurring in context c , and denote by x 1: n :=  X  x i : i  X  T c ( x 1: n )  X  the subsequence of x matches context c . Furthermore, given an alphabet X and an upper bound on the maximum Markov order D  X  N , let  X  X := S D i =0 X i , with X 0 := { } . A set S  X   X  X is called a proper suffix set over  X  X if for any finite string x there is exactly one c  X  S such that c is a suffix of x . We also write S ( x ) to denote the matching context c  X  X  of a string x . The key property of S (  X  ) is that given x 1: n , it defines a partition of the time indices { 1 ,...,n } in the sense that the collection of sets {T c ( x 1: n ) } c  X  X  is mutually exclusive and exhaustive.
 A prediction suffix tree is a tuple ( S ,  X ) where S is a proper suffix set over  X  X and  X  := {  X  c } c  X  X  is a set of sequen-tial probabilistic models, with each model being associated in S corresponding to x &lt;t , then the t th symbol is predicted the sequential probabilistic model 2.3. Context Tree Switching Context Tree Switching (CTS, Veness et al., 2012) is a re-cent extension of Context Tree Weighting (CTW, Willems et al., 1995) that retains the strong theoretical properties of CTW but performs better in practice. Let X := { 0 , 1 } be the binary alphabet, D  X  N be arbitrary but fixed, and let C D be the collection of all binary prediction suffix trees ( S ,  X ) of depth less than or equal to D . Let  X  CTS ( x denote the probability assigned to x 1: n by CTS. The regret R n (  X  CTS , C D ) of CTS with respect to C D is upper bounded by where  X  D ( S ) := |S| X  1 + |{ c : c  X  S , | c | 6 = D }| is the description length of S ,  X ( S ) := max c  X  X  | c | , and Notice that the bound makes explicit an Occam bias to-wards smaller prediction suffix trees. The last summand in Equation 2 arises from having to learn the parameters of |S| unknown Bernoulli distributions. This, combined with the fact that  X  D ( S ) = O ( |S| ) , causes CTS to perform well whenever a small prediction suffix tree is sufficient to de-scribe the data.
 Algorithm. CTS is best understood as a recursive appli-cation of the switching technique described in Section 2.1. As depicted in Figure 3, for each context c  X   X  X we define a switching model between two components: a base estima-tor  X  which predicts the substring x c 1: n and a split estima-tor which subdivides c into 0 c and 1 c and predicts x c 1: n querying the corresponding switching models for the fur-ther partitioned data. The latter operation is well-defined by the partitioning property of proper suffix sets: x n be-then assigns a probability to x 1: n  X  X n using its top-level switching model, i.e.  X  CTS ( x 1: n ) :=  X  ( x 1: n ) . The algorithmic core of CTS is a context tree data struc-ture: a perfect binary tree of depth D whose nodes cor-respond to all possible strings in  X  X . Each node c  X  stores a base estimator  X  c as well as the data-dependent quantities  X  c ,  X  c , and  X  c . Informally,  X  c ( x &lt;t correspond to w  X ,t  X  1 in Equation 1, while  X  c corresponds to  X  SWITCH ( x 1: t ) . CTS incrementally maintains these quan-tities as follows. Given a new symbol x t and its associ-ated history x &lt;t , CTS updates the D + 1 nodes along the left unchanged. CTS performs a post-order traversal along this path, first updating each node X  X  base estimator and the other quantities as follows. At the leaf c = x t  X  D : t  X  1  X  ( x 1: t ) . At the internal nodes, the following updates oc-cur:  X   X   X  signed to x t by the recursively-defined split estimator, with x 0 := x t  X  X  c | X  1 . Every node c is initialized with  X  c  X  ( ) = 1 2 , except for leaf nodes where we set  X  c ( ) = 1 and  X  c ( ) = 0 to reflect the fact that no further splitting occurs at depth D . 2.4. Choice of Base Estimator If the alphabet is binary, a natural choice of base model is the KT estimator (Krichevsky &amp; Trofimov, 1981). This estimator probabilistically predicts each symbol according to a Beta-Binomial model, using a Beta ( 1 2 , 1 2 ) prior over the unknown parameter. The regret of the KT estimator with respect to any Bernoulli process is known to be at most 1 2 log n + 1 . Non-binary alphabets can be handled in a number of ways. The most direct approach is to use a Dirichlet-Multinomial model with a Dirichlet ( 1 2 ) prior over X , leading to the multi-alphabet KT estimator, whose re-gret is O ( |X| log n ) (Tjalkens et al., 1993). When |X| is large and only a small fraction of the possible symbols are observed, this approach is inefficient (e.g. Volf, 2002). A recently developed solution to this problem is the Sparse Adaptive Dirichlet (SAD) estimator (Hutter, 2013). The SAD approach enjoys regret guarantees close to those of the multi-alphabet KT restricted to the subalphabet A X  X  of symbols which occur in x 1: n . In Section 4 we describe a large experiment whose performance is much improved by the use of a SAD-like estimator. In this section we generalize CTS to partial context matches to produce the Skip Context Tree Switching (SkipCTS) al-gorithm. We begin with some notation describing partial context matches, then describe how the SkipCTS algorithm incorporates these. Finally we provide a bound on the re-gret of SkipCTS with respect to the set of all bounded K -skip prediction suffix trees.
 To gain some intuition as to why ignoring irrelevant vari-ables matters, consider what happens internally in the con-text tree in the presence of irrelevant variables. As the right-hand side of Figure 4 shows, in the worst case, the data used to train the base models can be dispersed uniformly across 2 m bins. On the other hand, SkipCTS can ignore the in-tervening variables and directly aggregate all the available data into a single bin (Figure 4, left). In the particular case where the base estimator is the KT estimator (with regret O (log n ) for memoryless sources), we see that SkipCTS can enjoy an exponential reduction in regret compared to CTS. 3.1. Partial Context Matches We begin with some notation. Let ? denote a wildcard sym-bol and let Y := X  X  X  ? } be the wildcard extension of X . We say that a string a  X  X m matches b  X  Y m if for all 1  X  i  X  m such that b i 6 = ? , we have a i = b i . For ex-ample, if X is the set of uppercase letters then BAY , BOY BUY , etc. all match B ? Y . Given a finite string x , we say that c  X  Y m is a suffix of x iff x = yc 0 for some c 0  X  X such that c 0 matches c . We call a string c k -skip contiguous if it contains at most k contiguous runs of ? symbols. For example, 0 ?? 1 ? 0 is 2-skip contiguous. Finally, we denote the set of all k -skip contiguous strings of length i by Y and the union of all such sets for i = 0 ...D as  X  Y k . 3.2. Algorithm The SkipCTS algorithm is parametrized by a maximum depth D  X  N and a number of allowable skips K  X  N . The key idea is to maintain a context tree whose nodes cor-respond to all possible contexts c  X   X  Y K . To do so, we gen-eralize the CTS update equations described in Section 2.3, leading to a recursive switching model which chooses be-tween not only a base estimator and a split estimator, but also between a variable number of recursively defined skip estimators. Effectively, these additional estimators corre-spond to ignoring one or more symbols in the context. As we will show in Section 3.3, the addition of these new mod-els allows us to obtain a competitive regret bound with respect to the set of all bounded K -skip prediction suffix trees.
 In designing SkipCTS, one additional subtle issue arises: some of the contexts in  X  Y K are redundant for the pur-pose of sequential prediction. To see this, consider two contexts from  X  Y K , c = 010 and c 0 = ? 010 . Given any string x 1: n  X  X n , both contexts induce the same substring x sidered by our algorithm. More generally, the contexts c  X   X  Y K for which c = ? l c 0 with l  X  N are equivalent. We refine the algorithm by considering only the set of rep-resentative contexts , i.e. the contexts c such that c = xc where x  X  X . In other words, representative contexts are those contexts which do not contain trailing wildcard sym-bols. This results in a more efficient algorithm than if we were to consider all possible contexts in  X  Y K . To avoid no-tational clutter, from here onwards we will use the notation  X  Y
K to denote the set of representative contexts. For a given string x , we call a representative context c which is a suffix of x a representative suffix .
 For every c  X   X  Y K we incrementally maintain a base esti-ber of  X  c,  X  quantities depends on c as follows. Define  X  ( c ) as the number of contiguous runs of ? symbols in c . For D  X  X  c | otherwise. The  X  c, 0 term then corresponds to the split estimator, while the remaining  X  c,  X  terms correspond to r ( c )  X  1 skip estimators; in particular, when r ( c ) = 1 no skip estimators are updated for c .
 Upon observing a new symbol x t , SkipCTS first updates  X   X  ( x 1: t ) for all c  X   X  Y K representative suffixes of x | c | = D . The remaining representative suffixes of x &lt;t updated recursively as where  X  t = 1 / [ r ( c )( t + 1)] , b c,t :=  X  c ( x t | x is the probability assigned to x t by the l th skip estimator, which is defined as where c 0 := ? l c and x 0 := x 0 t  X  X  c 0 | X  1 . As with CTS, any node not corresponding to a representative suffix of x &lt;t left unchanged. The probability  X  S KIP CTS ( x 1: n ) output by SkipCTS is the probability assigned by the root switching model  X  ( x 1: n ) .
 The particular update structure of SkipCTS, i.e. the con-texts c for which  X  c ( x 1: n ) are updated, depends on both K and D . As with CTS, we set  X  c ( ) = 1 whenever | c | = D . For | c | &lt; D , we set  X  c ( ) = 1 2 and 1.  X  c, 0 ( ) = 1 2 if  X  ( c ) = K ; 2. otherwise It can be verified that for any c  X   X  Y K we have  X  c ( ) + P l =0  X  c,l ( ) = 1 . In general, we may freely initialize the non-zero  X  c and  X  c terms, provided they are nonnegative and sum to one. Because  X  ( c ) in Equation 3 ranges from 0 to K and | c | from 0 to D , performing one update requires O ( D 2 K +1 ) operations  X  effectively the number of repre-sentative suffixes c  X   X  Y K which match x &lt;t . In particular, note that when K = 0 we recover the original Context Tree Switching algorithm of Veness et al. (2012). 3.3. Regret Analysis In this section we provide a bound on the regret of Skip Context Tree Switching with respect to any bounded K -skip prediction suffix tree ( S ,  X ) , where K is fixed and S is a proper suffix set over  X  Y . At a high level, Lemma 1 bounds the regret induced by the base estimators at the leaves of ( S ,  X ) . Lemma 2 bounds the regret contributed from a sin-gle level of internal nodes in the tree, and Lemma 3 applies a recursive argument to combine Lemmas 1 and 2. Theo-rem 1 finally uses Lemma 3 to bound the regret of SkipCTS with respect to an arbitrary K -skip prediction suffix tree. Lemma 1. For any proper suffix set S over  X  Y K and for any x Proof. Let c t := S ( x &lt;t ) be the context in S which is a suffix of x &lt;t , which is guaranteed to be unique as S is a proper suffix set. By combining Equations 3 and 4 we have  X  Equation 4 as hence and the desired result follows by recalling that {T c ( x 1: n ) } c  X  X  is a partition of { 1 ,...,n } . We now define the various kinds of decision points (e.g. split only; split or skip) within the context tree. Definition 1. Let S be a proper suffix set over  X  Y . A string c  X   X  Y is a choice point in S whenever c is a strict suffix of some c 0  X  X  and c = xc 0 , where x  X  X  .
 Definition 2. Let S be a proper suffix set over and for c  X   X  Y let E S ( c ) := { l  X  N : ? c is a strict suffix of some c 0  X  S} . We call  X  S := { ( c,l ) : c is a choice point in S ,l  X  X  S ( c ) } the set of choice nodes of S .
 Figure 5, for example, is described by S = { 0 ? 0 , 1 ? 0 , 01 , 11 } , of which 0 and 1 are choice points. The set of choice nodes corresponding to S is  X  S := { (0 , 1) , (1 , 0) } . Intuitively, these choice nodes correspond exactly to the nodes with more than one child.
 Definition 3. Let c := c 1 c 2 ...c m  X  Y m . We define the effective length of c as ` ( c ) := |{ i  X  { 1 ,...m } : c and for a set V of such strings define ` ( V ) := max c  X  X  Recall that our aim is to bound  X  log  X  ( x 1: n ) . Having bounded the product of terms at the leaves, Q c  X  X   X  c ( x we now derive a similar bound for the internal nodes. Lemma 2. Let S be a proper suffix set over  X  Y K , and let  X  S d := { ( c,l )  X  and any x 1: n  X  X  n , we have that
Y Proof. For any ( c,l )  X   X  S , and any t  X  { 1 ,...,n } , let c Equation 6 as which, following a similar approach to the proof of Lemma 1 leads to the desired result.
 Lemma 3. For every proper suffix set S over  X  Y K , with denoting the set of choice nodes of S , we have Proof. For any c  X   X  Y K and l  X  r ( c ) ,  X  c ( x 1: n  X  S is a proper suffix set, at any time step t at most one c  X   X  S  X  S of effective length d  X  ` ( S ) matches x &lt;t By recursively applying Lemma 2 to the right hand side of Equation 3 for every c  X   X  S , and keeping the left hand side whenever c  X  X  , we obtain The result then follows since  X  S =  X  ` ( S )  X  1 d =0  X  We are now in a position to bound the regret of Skip Con-text Tree Switching with respect to any skipping predic-tion suffix tree structure that can be obtained by pruning the SkipCTS context tree.
 Theorem 1. Let  X  S denote a D  X  N bounded k -skip pre-diction suffix tree ( S ,  X ) , where S is a proper suffix set over  X  Y k and  X  := {  X  c } c  X  X  is a set of sequential prob-abilistic models inside the SkipCTS context tree. For any x 1: n  X  X n , the regret of SkipCTS run with parameters D and K  X  k with respect to  X  S is bounded as Proof. (Sketch) Beginning with R n (  X  S KIP CTS , {  X  S apply Lemma 3, then Lemma 1, and finally simplify using  X  If the data is generated by some unknown prediction suffix tree and the base estimators are KT estimators, the above regret bound leads to a result that is similar to the regret bound for CTS given by Veness et al. (2012), save for two main differences. First, recall that C D , the collection of models considered by CTS, is a subset of C D,K , the collec-tion of models considered by SkipCTS (with equality only when K = 0 ). Our bound thus covers a broader collec-tion of models. Second, for a proper suffix set S defined over X , i.e. a no skip prediction suffix tree, the description length  X  K D ( S ) under SkipCTS with K &gt; 0 is necessarily larger than its CTS counterpart. While these differences negatively affect our regret bound, we have seen in Sec-tion 3 that we should expect significant savings whenever the data can be well-modelled by a small K -skip predic-tion suffix tree. We explore these issues further in the next section. We tested the Skip Context Tree Switching on a series of prediction problems. The first set of experiments uses a popular data compression benchmark, while the second set of experiments investigates performance on a diverse set of structured image prediction problems taken from an open source Reinforcement Learning test framework. A reference implementation of SkipCTS is provided at: http://github.com/mgbellemare/SkipCTS . 4.1. The Calgary Corpus Our first experiment evaluated SkipCTS in a pure compres-sion setting. Recall that any algorithm which sequentially assigns probabilities to symbols can be used for compres-sion by means of arithmetic coding (Witten et al., 1987). In particular, given a model  X  assigning a probability  X  ( x to x 1: n  X  X n , arithmetic coding is guaranteed to produce a compressed file size of essentially  X  log 2  X  ( x 1: n ) . We ran SkipCTS (with D = 48 , K = 1 ) and CTS (with D = 48 ) on the Calgary Corpus (Bell et al., 1989), an established compression benchmark composed of 14 dif-ferent files. The results, provided in Table 1, show that SkipCTS performs significantly better than CTS on certain files, and never suffers by more than a negligible amount. Of interest, the files best improved by SkipCTS are those which contain highly-structured binary data: GEO , OBJ 1, and OBJ 2. For reference, we also included some CTW ex-periments, indicated by the CTW and and SkipCTW rows, that measured the performance of skipping using the orig-inal recursive CTW weighting scheme; here we see that the addition of skipping also helps. Table 1 also provides results for CTW  X  , an enhanced version of CTW for byte-based data (Willems, 2013). Here both CTS and SkipCTS outperform CTW, with SkipCTS providing the best results overall. Finally, it is worth noting that averaged over the Calgary Corpus, the bits per byte performance of SkipCTS is superior ( 2 . 10 vs 2 . 12 ) to DEPLUMP (Gasthaus et al., 2010), a state-of-the-art n -gram model. While SkipCTS is consistently slightly worse for text data, it is significantly better on binary data. It is also worth pointing out that no regret guarantees are yet known for DEPLUMP. 4.2. Atari 2600 Frame Prediction We also tested our algorithm on the task of video game screen prediction. We used the Arcade Learning Environ-ment (Bellemare et al., 2013a), an interface that allows agents to interact with Atari 2600 games. Figure 6 depicts the well-known P ONG , one of the Atari 2600 X  X  flagship games. In the Atari 2600 prediction setting, the alphabet X is the set of all possible Atari 2600 screens. Because each screen contains 160  X  210 7-bit pixels, it is both imprac-tical and undesirable to learn a model which predicts each x t  X  X atomically. Instead, we take a similar approach to that of Bellemare et al. (2013b): we divide the screen into 16  X  16 blocks and predict each block atomically using SkipCTS or CTS combined with the SAD estimator.
 Each block prediction is made using a context composed of the symbol value of neighbouring blocks at previous 11 variables. In this setting, skipping irrelevant variables is particularly important because of the high branching fac-tor at each level. For example, when predicting the motion of the opponent X  X  paddle in P ONG , SkipCTS can disregard horizontally neighbouring blocks and the player X  X  action. We trained SkipCTS with K = 0 and 1 on 54 Atari 2600 games. Each experiment consisted of 10 trials, each lasting 100,000 time steps, where one time step corresponds to 4 emulated frames. Each trial was assigned a specific random seed which was used for all values for K . We report the average log-loss per frame over the last 4500 time steps, corresponding to 5 minutes of real-time Atari 2600 play. Throughout our trials actions were selected uniformly at random from each game X  X  set of legal actions.
 The full table of results is provided as supplementary mate-rial. For each game we computed the improvement in log-loss per frame and determined whether the difference in loss was statistically significant using the Wilcoxon signed rank test. As a whole, SkipCTS achieved lower log-loss than CTS in 54 out of 55 games; all these differences are significant. While SkipCTS performed slightly worse in E
LEVATOR A CTION , the difference was not statistically significant. The average overall log-loss improvement was 9.0% and the median, 8.25%; improvements ranged from -2% (E LEVATOR A CTION ) to 36% (F REEWAY ). SkipCTS with K = 1 processed on average 34 time steps (136 frames) per second, corresponding to just over twice the real-time speed of the Atari 2600. We further ran our algorithm with K = 2 and observed an additional, sig-nificant increase in predictive performance on 18 games (up to 21.7% over K = 1 for T IME P ILOT ). On games where K = 2 is unnecessary, however, the performance of SkipCTS degraded somewhat. As discussed above, this behaviour is an expected consequence of the larger  X  K D ( S ) . We have seen that by allowing context trees to skip over variables, SkipCTS can achieve substantially better perfor-mance over CTS in problems where a good variable order-ing may not be known a priori . Theoretically we have seen that SkipCTS can, in the extreme case, have exponentially lower regret. Empirically we observe substantial benefits in practice over state of the art lossless compression algo-rithms in problems involving highly structured data (e.g. the GEO problem in the Calgary Corpus). The dramatic and consistent improvement seen across over 50 Atari predic-tion problems indicate that SkipCTS is especially impor-tant in multi-dimensional prediction problems where issues of variable ordering are naturally exacerbated.
 The main drawback of SkipCTS is the increased compu-tational complexity of inference as a result of the more expressive model class. However, our experiments have demonstrated that small values of K can make a sub-stantial difference. Furthermore, the computational and memory costs of SkipCTS can be alleviated in practice. The tree structure induced by the recursive SkipCTS up-date (Equations 3 X 5) can naturally be parallelized, while the SkipCTS memory requirements can easily be bounded through hashing. Finally note that sampling from the model remains a O ( D ) operation, so, for instance, planning with a SkipCTS-based reinforcement learning model is nearly as efficient as planning with a CTS-based model.
 Tree-based models have a long history in sequence predic-tion, and the persistent issue of variable ordering has been confronted in many ways. The main strengths of SkipCTS are inherited from CTW  X  efficient, incremental, and exact Bayesian inference, and strong theoretical guarantees on asymptotic regret. Other approaches with more represen-tational flexibility lack these strengths. In the model based reinforcement learning setting, some methods (e.g. McCal-lum, 1996; Holmes &amp; Isbell, 2006; Talvitie, 2012) extend the traditional predictive suffix tree by allowing variables from different time steps to be added in any order, or by al-lowing the tree to excise portions of history, but these meth-ods are not incremental and do not provide regret guaran-tees. Bayesian decision tree learning methods (e.g. Chip-man et al., 1998; Lakshminarayanan et al., 2013) could in principle be applied in the sequential prediction setting. These typically allow arbitrary variable ordering, but re-quire approximate inference to remain tractable. In this paper we presented Skip Context Tree Switching, a polynomial-time algorithm which efficiently mixes over se-quences of prediction suffix trees that may skip over K con-tiguous runs of variables. Our results show that SkipCTS is practical for small K and can produce significant empirical improvements compared to members of the Context Tree Weighting family (even with K = 1 ) in problems where irrelevant variables are naturally present.
 The authors would like to thank Alex Graves, Andriy Mnih and Michael Bowling for some helpful discussions.
 Bell, Timothy, Witten, Ian H., and Cleary, John G. Mod-eling for text compression. ACM Computing Surveys (CSUR) , 21(4):557 X 591, 1989.
 Bellemare, Marc G., Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research , 47, June 2013a. Bellemare, Marc G., Veness, Joel, and Bowling, Michael. Bayesian learning of recursively factored environments.
In Proceedings of the Thirtieth International Conference on Machine Learning , 2013b.
 Chipman, Hugh A., George, Edward I., and McCulloch,
Robert E. Bayesian CART model search. Journal of the American Statistical Association , 93(443):935 X 948, 1998.
 Erven, Tim Van, Grunwald, Peter, and de Rooij, Steven.
Catching up faster in Bayesian model selection and model averaging. In NIPS , 2007.
 Gasthaus, Jan, Wood, Frank, and Teh, Yee Whye. Lossless compression based on the sequence memoizer. In Data Compression Conference (DCC) , 2010.
 Herbster, Mark and Warmuth, Manfred K. Tracking the best expert. Machine Learning , 32(2):151 X 178, 1998. Holmes, Michael P. and Isbell, Jr, Charles Lee. Looping suffix tree-based inference of partially observable hidden state. In Proceedings of the 23rd International Confer-ence on Machine Learning , pp. 409 X 416, 2006.
 Hutter, Marcus. Sparse adaptive Dirichlet-multinomial-like processes. In Proceedings of the Conference on Learning Theory (COLT) , 2013.
 Koolen, Wouter M. and de Rooij, Steven. Universal codes from switching strategies. IEEE Transactions on Infor-mation Theory , 59(11):7168 X 7185, 2013.
 Krichevsky, R. and Trofimov, V. The performance of uni-versal encoding. IEEE Transactions on Information The-ory , 27(2):199 X 207, 1981.
 Lakshminarayanan, Balaji, Roy, Daniel M., and Teh,
Yee Whye. Top-down particle filtering for Bayesian de-cision trees. In Proceedings of the 30th International Conference on Machine Learning , 2013.
 McCallum, Andrew K. Reinforcement learning with selec-tive perception and hidden state . PhD thesis, University of Rochester, 1996.
 Ron, Dana, Singer, Yoram, and Tishby, Naftali. The power of amnesia: Learning probabilistic automata with vari-able memory length. Machine learning , 25(2):117 X 149, 1996.
 Talvitie, Erik. Learning partially observable models using temporally abstract decision trees. In Advances in Neu-ral Information Processing Systems (25) , 2012.
 Tjalkens, Tj. J, Shtarkov, Y.M., and Willems, F.M.J. Con-text tree weighting: Multi-alphabet sources. In 14th
Symposium on Information Theory in the Benelux , pp. 128 X 135, 1993.
 Veness, Joel, Ng, Kee Siong, Hutter, Marcus, Uther,
William T. B., and Silver, David. A Monte-Carlo AIXI approximation. Journal of Artificial Intelligence Re-search , 40:95 X 142, 2011.
 Veness, Joel, Ng, Kee Siong, Hutter, Marcus, and Bowling,
Michael H. Context tree switching. In Data Compres-sion Conference (DCC) , pp. 327 X 336, 2012.
 Volf, P. Weighting techniques in data compression: Theory and algorithms . PhD thesis, Eindhoven University of Technology, 2002.
 Willems, Frans M., Shtarkov, Yuri M., and Tjalkens,
Tjalling J. The context tree weighting method: Basic properties. IEEE Transactions on Information Theory , 41:653 X 664, 1995.
 Willems, Frans M., Shtarkov, Yuri M., and Tjalkens,
Tjalling J. Context weighting for general finite-context sources. IEEE Transactions on Information Theory , 42 (5):1514 X 1520, 1996.
 Willems, Frans M. J. CTW website. http://www.ele. tue.nl/ctw/ , 2013.
 Witten, Ian H., Neal, Radford M., and Cleary, John G.
Arithmetic coding for data compression. Communica-
