 We describe a data mining syst em to detect frauds that are camouflaged to look like normal activities in domains with high number of known relationships. Examples include accounting fraud detection for rating and i nvestment, insider attacks on corporate networks, and health care insurance fraud. Our goal is to help analysts who are overw helmed with information about companies or on-line system access logs or insurance claims to focus their attentions on features that cause damage in the future. We focused on accounting fraud where the task is to detect the subset of companies that were potentially committing accounting fraud within the total population of public companies that file quarterly and annual filings with the Securities and Exchange Commission (SEC). Using (a) Repr esentation of changes, (b) A mix of decision tree learning, locally weighted logistic regression, k-means clustering, and constant regression in a two phase pipe line, we developed models th at rank companies based on the probability of forecasting future damaging performance. The learned models were tested ex tensively over four years with public data available from SEC fili ngs and private data available from rating companies and investme nt firms. Cross validation experiments and analyst based va lidation of private experiments were found to show that the appr oach performed as well as or better than domain experts and di scovered new relationships that domain experts did not use on a regular basis. Finally, the detections preceded public knowledge of such problems by six to eighteen months. H.2.8 [Database Management]: Data base Applications  X  Data Mining; 1.2.6 [Artificial Intelligence]: Learning General Terms : Algorithms, Design, Experimentation. Keywords : Accounting fraud, On-line intrusion analysis. In many domains, compensatory beha vior is used to fix short term problems which are indicative of l ong term failures. For example, accounting fraud can make compan y financials look good today while masking problems that will show up in the future. Often it Is hard to tell the difference between a well performing company and a company that is committing accounting fraud to make it look like it is performing well. In order to tell the difference, we have to predict the direction of change of net income in the future. We call this camouflaged fraud detection. The key characteristics of camouflaged fraud detection are:  X  Detecting a set and factors that contribute in a significant  X  Forecasting whether that key variable will change positively All of the domains of interest i nvolve information overload for the analyst and innumerable domain rela tionships that are constantly changing, making detection difficult and forecasting of exceptional future performance very hard. For example, in the past, accountants have used ratios such as Inventory Turnover Ratio or Property Plant and Equi pment Turnover Ratio to measure how well a company is being run. However, given the extensive management of financial stat ements by public companies, researchers have recently focused on different measures that can be used to detect compensatory behavior, such as factors that identify earnings recognition management. The changing nature of the domain relationships combined with discovery of new relationships that need to be an alyzed results in missed threats, inappropriate staffing of analysts or detections that miss emerging threats. This paper describes the techniques we developed and tested through cross validation using public and private data across a variety of public companies. We also briefly describe our experiments in the on-line intrusion analysis domain. We believe that data mining tec hniques, in order to add value, need to discover new relationships that layer on top of existing, known relationships (such as ratio s) while at the same time be auditable by domain experts. Fo r example, accounting ratios used for income statement analysis or diagnostic procedures used to identify faults in machines are examples of domain relationships that are well founded and setup the existing domain structures. New patterns or rules discovered ha ve to fit within the existing knowledge structure in order for it to be accepted by domain experts. Additionally, in these domains, th e number of training positives is often small. The techniques us ed should be robust against potential bias in small training positives. Furthermore, the volume of data used during detection or forecasting could be significantly larger. Therefore, the techniques used for detection or forecasting needs to scale with respect to volum e of data used for detection. In the accounting fraud domain, over a three year period, we extensively cross-validated learne d models developed using five years of corporate annual and quarterly filings. Additionally, the learned models and the detected companies were extensively reviewed with domain experts, and compared with the results in published research. The results of the evaluation showed that our models performed as well as pub lished research [9] and better than domain experts. Additionally, the results of detection were available between six to eighteen months before analysts using existing techniques were able to detect the same companies. Most notably, our models found new dom ain relationships that were previously not used by existing detection processes. We formulate the problem as: Given a time-series of corporate fundamentals data extracted from yearly and quarterly filings by public companies (a) Can we use a unifying framework for existing and newly discovered m odels in Financial Statement Analysis that detect exceptions in corporate fundamentals, (b) Do the variables used to detect companies in (a) predict the direction of change of future value of net-income hence identifying accounting fraud, (c) Does the implementation scale for large number of variables, and (d) Can domain experts control the machine learning process and audit the results? The last point requires explana tion. In the financial accounting world, the accountants who forecast future revenue/income based on corporate fundamentals do not accept  X  X lack-box X  approaches. While several Wall Street companies and academic researchers have tried using techniques such as neural networks to address the performance forecasting problem, to our knowledge, none of them have succeeded. We developed a data mining sy stem called Performance Miner X  and ran it on 5 years of corporate annual and quarterly filings (10K reports). Each company would make four filings per year (excluding restatements), and over a 5 year period we had access to multivariate time series of 20 observations per company. We performed many experiments fo r exception detection, exception detection combined with forecasting of future net income, domain expert audit of newly discovere d relationships, and comparisons of the results with other techniques published in the financial income statement analysis domain to answer the questions posed above. In this paper, we descri be the approach we took and the results we obtained. This paper is organized as follows: Section 2 presents related approaches to addressing this problem. Section 3 describes the domain re quirements and the approach we took in more detail. Section 4 presents results of cross-validation and forecasting accuracy tests. Section 5 describes limited experiments we performed in the on-line intrusion domain using the same approach to test its ge nerality across various application domains. Section 6 briefly describe s the system we implemented, and Section 7 presents our conc lusions and future research possibilities. Our research combines work performed in three different areas in data mining and statistics: Anom aly detection, leveraging prior knowledge, and time-series forecasting. Our approach to detection is similar to the use of Cla ssification And Regression Trees (CART) [1] in that both our approach and CART produce explainable trees. However, our approach to detection extends explainable trees to deal with small training set sample sizes, the need to scale for large volume of data, and integration with clustering and regression to de tect camouflaged compensatory behaviors. Furthermore, our appr oach of using locally weighted logistic regression at the leaf nodes creates an innovative approach for dealing with new patterns that emerge after the decision tree has been created. While research performed in anomaly detection has used si mple outlier techniques based on Euclidian geometry to determine outliers [3], our approach is differentiated by its focus on us ing change and calculations of change to detect outliers. Additi onally, while research has focused on learning new rules that are di fferent from known rules [7], our research focused on layering new relationships on well known domain relationships. While tim e series mining looking for deviants is also a well researched area [6], our focus on change and direction of change adds to traditional time series analysis. Finally, in the financial investme nt domain much work has been performed in applying statistical and data mining techniques to perform time series analysis. Ho wever, to our knowledge, we are not aware of work that has used the combination of techniques described in this paper to detect accounting fraud based on predicting future performance of companies as measured by net income. Our system detects camouflaged fraud in two stages: In the following, we describe the stages sequentially with the common element between them being the domain representation. Therefore, we begin with the domain representation. Our objective was to detect a collection of companies in a particular period in time (year or quarter) that may be manipulating their financial fundame ntals. We used 20 quarters of time series data for approximate ly 8000 public companies. Given only the annual filings are audite d, our experiments were most reliable when using the annual filings which are audited before submission. Consider the annual or quarterly filing of public companies and derived signals we use. The raw va lues are represented as a tuple T = ( C 1 (t),, ... C n (t), B 1 (t), ...,B n (t), I 1 (t), ... I for each reporting period t . That is, T 1 represents a tuple of fundamental time-varying features, where C 1 , C 2 , .. are cash flow features, B 1 , B 2 , .... are balance sheet features, and I income statement features, for period t . There may be many such tuples for different periods base d on quarterly or annual filings. Domain specific ratios that are used to detect exceptions are derived from components of T t and are augmented as a separate tuple. For example, in orde r to detect overstated accounts receivable we use ratio of accounts receivable vs. outstanding days for payment of sale [5]: let where r i (t) are derived ratios for reporting period t . These domain ratios define a domain-specific space in which to detect simple threshold based violations. Acco rding to accounting rules, these ratios are bounded by a high-dimensional box that can be used to isolate exceptions. Therefore, these ratios could be considered to be basic features used for detection. However, these ratios are not enough to isolate all exceptions. This is because organizations that manage their books often adjust their books so that they do not violate well known values for ratios in a single year, but do it over a period of several years. Additionally, emerging trends within a sector are often missed in these analyses. In order to deal with both of these problems, we model accounting behavior us ing differences in time, X t , R t  X  R t -1 ), which we call change features. Additionally, in order to perform peer comparison of companies, each of the change features are also used to bin companies using percentiles. Companies are grouped into three nested classes using Standard Industry Classi fication (SIC) code, Sector, and Industry classifications. Thus a given company has three types of peers: peers by SIC, peers by Sector and peers by Industry. For each company, the empirical distribution of each component of X is computed for these three levels of peers. Each feature in X is expanded to include these three percentiles. We experimented with several binning techniques and chose percentile of the change vector as the most reliable detection mechanism that was also explainable to domain expert s. We represented a total of 383 features across all the above tuples for each filing by a company. We use supervised learning of decision trees to detect exceptions. The supervised learning set is assembled based on well known exceptions such as earning surprises, bankruptcies, SEC action on a company, etc. We use three different approaches to generating these decision trees based on the training set. First, a decision tree can be built from scratch by invoking the tree generation routine for the root node, which is initially assigned the entire training set. Second, a domain expert may manually create a decision tree either by incrementally adding splitting rules to selected leaf nodes, or third, by recursively growing the tree, starting from a selected tree leaf using automatic generation. The feature types could be numeric, string, or categorical. We describe the automatic generation of decision trees below. The generation of a decision tree is performed in two stages:  X  A decision tree is initially generated based on a procedure  X  Within each leaf of the resulting decision tree, logistic During the detection stage, when new samples are filtered through the decision tree, items that are cl assified into the leaves of the decision tree are further classified using locally weighted logistic regression at the leaf level. To automatically generate a splitting rule for a given leaf node in the decision tree, the available features are ranked according to a discriminability score, which in turn is based on the discriminability achieved by optimal placement of the split point. For a candidate splitting feature, the score for a split point position is calculated as a weighted sum of the sensitivity and selectivity achieved by the resulting split of the training set items associated with the node: Sensitivity is defined as the proportion of detected positives divided by all positives, and selectivity is defined as the proportion of detected negatives divided by all negatives. The sensitivity weight w is a user-sp ecified parameter. Other user-specified parameters state the minimum discriminability score allowed for splitting a node, and the maximum number of tree levels to generate at a time. Once a new splitting rule has been auto-generated for a leaf node, it receives two child nodes, and the original leaf node X  X  filing entries are split among the child nodes according to the splitting rule. In the  X  X ll-or-none X  leaf-labeling scheme, each child node is labeled  X  X ositive X  or  X  X egative X . By default, each child node is independently labeled positive if and only if it contains more positive entries than negative entr ies. If the user supplies an optional sensitivity bias parameter, then each child node is labeled positive if and only if its positive entry count, multiplied by the sensitivity bias, exceeds its negative entry count. In the  X  X ll-or-none X  leaf-labeli ng scheme described above, an SEC filing is classified as positive if and only if its traversal through the decision tree ends at a  X  X ositive X  tree node. This simple classification scheme is effective when the trainings sets are fairly balanced. For very imbalanced training sets, in which the positive examples form a tin y minority, it is difficult to achieve a reasonable level of sensitivity without drastically increasing the frequency of false positives. Instead of pruning the tree for generalization as in CART, we use locally weighted logistic regre ssion. In addition to providing the basis for comparing detected items across the leaves of a decision tree, our method also provides a way to associate a probability with the detected item, to allow for a softer decision criterion rather than the all or nothing decision criterion. Locally weighted logistic regression is used to assign a probability value to the detected item based on the item X  X  distance from positive and negative training items in the releva nt tree leaf. This probability is expressed as a confidence factor ranging from 0 to 1, which is reported along with the detection results to the user. We chose locally weighted logistic regression because the detected items in a decision tree X  X  leaf node can form several clusters. Global logistic regre ssion produces a monotonic sigmoid function, which is used to pred ict the probability of an event occurring, given a set of independe nt input variables. Logistic regression works well in the presence of noise, due to its smoothing characteristics. Howe ver, its monotonicity makes it insensitive to local clusters of positives or negatives. In the domains we were interested in, the data sometimes form two or more distinct clusters as shown in Figure 1. Figure 1: clustering of positives, shows the scatter plot for a decision tree node X  X  entries, viewed in the space of the features net-income and current-liabilities (showing SIC-percentiles for the features X  rates-of-change). The training positive (red) filings form loose clusters in the lower-left corner, to the right of center, and (more loosely) near the uppe r-left corner. Each of these clusters is made up of several companies X  filings. Locally weighted logistic regre ssion is a useful classification technique when local clusters, such as those in Figure 1, are present in the distribution of data . As a memory-based form of logistic regression, the sigmoid is calculated at the time of the query, using an inverse-distance formula to weight each training set entry according to its proximity to the query entry. This gives locally weighted logistic regre ssion a nearest-neighbor behavior that respects the local distribution of data. Additionally, locally weighted logistic regression ha s better smoothing characteristics than nearest-neighbor techniques. A Boolean application parameter indicates whether or not the decision trees should classify data at the leaf level via locally-weighted logistic regression, in place of the  X  X ll-or-none X  positive/negative leaf labeling sc heme. A logistic regression probability threshold parameter also specifies the minimum probability of positive-class membership required for the decision tree to classify an input data as a positive instance. Lowering the probability threshold effectivel y increases the decision tree X  X  sensitivity, at the expense of sel ectivity. As shown in the cross-validation results below, the loca lly weighted logistic regression approach appears to be preferable when the training set of positives and negatives is drastically imbalanced. The forecasting of value and sign is the 2 nd stage of the process in our system. In the training phase, two types of training are used to find the best candidates for use in forecasting. The first set of candidates is derived using the previous step explained above. In addition, the training examples are clustered using k-means clustering [4]. Each cluster is labeled as  X + X  or  X - X , to indicate whether the earnings of similar companies may be expected to increase or decrease in the following year. After the clusters are labeled, the clusters for which earnings increas e or decrease cannot be reliably predicted are discarded. In the testing phase, the clusters are repopulated with test filings, and th e cluster labels derived in the training phase predict whether the test filing company X  X  earnings can be expected to increase or d ecrease in the following year. By repeating this experiment over a period of few time periods (e.g., years), we create a time-series data of exceptional data items. We then perform a constant regression fit to be able to predict future values of the variables. Constant regression calculates the mean value and standard deviation of a function (e.g. change in net income) for a set of data points (e.g. the filings in a classifier leaf). For instance, the 2004 filings in a given classifier leaf might have an average increase in normalized net income (normalized by dividing by total assets) of 10%. Then we could predict that the net-income value of 2005 filings that fall into the same leaf to increase by 10%. The inaccuracy of the prediction will be related to the standard deviation. We present our experimental re sults following the description above:  X  Experimental results as they apply to exception detection,  X  Experimental results as they apply to forecasting future value While we performed several diffe rent studies with public and private data, due to confidentiality requirements, we present the cross validation experiments we did with public data. Cross-validation experiments we re performed to test the effectiveness of automatically-generated decision trees, and to compare the degree of discriminability achieved by the  X  X ll-or-none X  leaf labeling scheme versus the locally-weighted logistic regression approach. For each experiment, the experimental data set was randomly partitioned into N subsets, partitioning the positive and negative items separately. N combinations of N  X  1 data subsets were formed by combining N  X  1 training sets at a time, leaving N corresponding holdout sets for evaluating the decision trees generated with the training sets. Experimental results are shown for N = 4 and N = 10. The traini ng examples were labeled based on SEC action that was a result of accounting fraud. Three experimental data sets were used, each containing the 49 positive filing examples. The 49 positive filings were from companies spanning 19 SICs, which together contained a total of 1647 companies. The negative filings for the three experimental data sets were chosen by ra ndomly sampling 5%, 10%, and 20% of those 1647 companies. For each experimental data set, cross-validation was performed using both the  X  X ll-or-node X  leaf labeling scheme, as well as the locally-weighted logistic regr ession scheme [2], for filing classification at the leaf-level. To produce varying sensitivity and selectivity behavior, the sensitivity bias parameter was varied when using the all-or-none leaf-l abeling scheme and the logistic regression probability threshold parameter was varied for the logistic regression scheme. As de scribed previously, in the all-or-none leaf-labeling scheme, a ne wly-generated leaf node was labeled positive if and only if its positive entry count, multiplied by the sensitivity bias, exceeded its negative entry count. For logistic regression, the probability threshold specifies the minimum probability of positive-class membership required for the decision tree to classify an input filing as a positive instance. The tables below show the aver age sensitivity, selectivity, and odds ratio results for each cross-validation experiment. The comparative effectiveness of  X  X ll-or-none X  leaf labeling versus logistic regression depends on th e degree of imbalance between the positive and negative data sets. The all-or-none scheme is fairly effective for experiments whose negative class examples make up 5% and 10% of the 1647 companies, with the positive examples forming 0.21 and 0.12 of the experimental data sets, respectively. However, for the experiments involving 20% of the companies, with positives forming only 0.063 of the data sets, the all-or-none scheme fails to yiel d reasonable sensitivity levels, due to the scarcity of positives. The locally-weighted logistic regression scheme yields comparatively better sensitivity, while retaining fairly high selectivity levels and odds ratios. 
Table 1: All-or-none leaf labelin g, with 5% of the companies 4 1.5 0.3060 0.8702 2.9595 10 0.5 0.1399 0.9628 4.2380 10 1.0 0.3800 0.8868 4.9460 10 1.5 0.4599 0.8540 5.177 Table 2: Logistic Regression with 5% of the companies (49 N 4 0.8 0.3253 0.9187 5.4949 10 0.2 0.5850 0.7295 3.9150 10 0.5 0.3650 0.8160 2.5787 10 0.8 0.3050 0.9078 4.3598 Table 3: All-or-none leaf labelin g, with 10% of the companies N 4 0.8 0.3253 0.9187 5.4949 10 0.2 0.5850 0.7295 3.9150 10 0.5 0.3650 0.8160 2.5787 10 0.8 0.3050 0.9078 4.3598 Table 4: Logistic Regression with 10% of the companies (49 N 4 1.5 0.2660 0.9073 3.5321 10 0.5 0.0800 0.9914 10.1333 10 1.0 0.1600 0.9594 4.6132 10 1.5 0.1600 0.9594 4.6132 
Table 5: All-or-non leaf labelin g, with 20% of the companies N 4 0.8 0 .2051 0.9274 3.2820 10 0.2 0 .2850 0.8492 2.2538 10 0.5 0 .2050 0.8956 2.2008 10 0.8 0 .0800 0.9247 1.0905 Table 6: Logistic Regression With 20% of the companies (49 N 4 1.5 0.0993 0.9523 2.2694 10 0.5 0.0000 1.0000  X  10 1.0 0.0850 0.9727 3.1733 10 1.5 0.0850 0.9727 3.1733 The technique for predicting the direction of change in company earnings was tested using financ ial metrics derived from the 10-K filing fundamentals of 4960 compan ies, spanning four industrial sectors. The filings used in th e experiments were chosen from the years 2000, 2001, and 2002. The features for clustering were chosen by manually comparing experimental results of the previous step for several alternative feature sets. Each experiment was performed for a sector of companies, with each company represented as a point in a 6-dimensional feature space. In the training phase, the companies were clustered using year-2000 filing metrics, and each cluster was labelled as  X + X  or  X - X  to indicate whether the majority (some P% &gt; 50%) of the companies in the cluster had an increase or a decrease in earnings in the year 2001. The clusters were then  X  X runed X  according to how reliably a company X  X  membership in a cluster predicted the sign of its earnings change in 2001. Given a user-specified  X  X runing threshold X  T%, the method discarded any clusters failing to satisfy P%  X  T%. The experiments were performed for four different pruning thresholds, with the higher thresholds resulting in more discarded clusters, and yielding more reliable predictions for fewer companies. In the testing phase, the clusters were repopulated with filings from the year 2001, by assigning each 2001 filing to the cluster with the closest centroid in the financial metric space. Predictions of company earnings increases or decreases were then made only for companies whose filings were assigned to non discarded clusters. For those companie s, the anticipated 2002 earnings change sign was predicted by the label ( X + X  or  X - X ) of the cluster occupied by the company X  X  2001 filing. The results of the experiments are shown in the tables below, by industrial sector. Four pruning thresholds ranging from 60% to 75% were used for each sector X  X  experiments. As the pruning threshold increases, the remaining clusters predict the sign of the earnings change with increasing reliability, but for a diminishing number of test companies. In the sector 8 experiments, for example, a pruning threshold of 60% yields a prediction reliability rate of 61% for 52% of the companies, while a pruning threshold of 75% yields an impressive reliability rate of 70%, but for only 6% of the companies. The reliability of the technique compares favourably with Penman X  X  S-Score technique, which similarly predicts the sign of a company X  X  change in earnings. The effect of increasing the pruning threshold in the current approach is analogous to widening the range of screened-out companies around S-Score 0.5, which increases the relia bility of the predictions but decreases the number of companies for which a prediction is made. Penman X  X  results s how an out-of-sample prediction reliability of 63% obtained for a screen of companies for which S &lt; 0.4 or S &gt; 0.6, representing 27-28% of the companies. By comparison, with the current approach, an out-of-sample prediction reliability of 67% was obtained for health care sector companies belonging to clusters of reliability  X  65% (the pruning threshold), representing 27% of the companies in the sector. 
Table 7: Financial Sector ( 2002 Filings: 1197. Percentage of Pruning 
Threshold Percent 
Predicted Percent 
Correct 
Table 8: Healthcare Sector (2002 Filings: 788. Percentage of Pruning 
Threshold Percent 
Predicted Percent 
Correct Table 9: Technology Sector (2002 Filings: 1569. Percentage of filings with positive earnings change: 58%) Pruning 
Threshold Percent 
Predicted Percent 
Correct Table 10: Technology Sector (2002 Filings: 1569. Percentage We implemented the above me thodology for camouflaged fraud detection in a client/server system called Performance Miner. The Performance miner consisted of a Ja va based client system that supported interactive experimentati on, a server that indexed the data to be analysed during tr aining, and a Web browser based AJAX client that supported ad-hoc searching of the data stored in the server to support interactive knowledge discovery. In addition, we also developed a data flow based filtering system called JADCEA in which learned models could be deployed to support detection with high volumes of data Performance Miner interacted with the server using an XML protocol over the HTTP transport. This allowed the server to be deployed in a different location than where the Performance Miner client was deployed and allowed it to be hosted by IT departments with remote deployment of the client. The top level user interface of Performance Miner is presented in Figure 2. Figure 2: Top level user interface for Performance Miner The interactivity provided by Performance Miner was appreciated by our users. In several of our evaluations, the end-users appreciated the speed with which they were able to identify exceptional companies. Furthermore, our users also appreciated the Browser based AJAX user interface, as it allowed them to explore all the data without being limited to a particular methodology as prescribed in the Performance Miner client above. One of several tabs of th e AJAX user interface is presented below. Many of the users with which we tested the system -first got comfortable using the Web browser based user interface before moving on to using the Performan ce Miner client. The unlimited drill down supported by the Web browser based user interface also increased confidence in the system as many senior accountants needed to see the raw data to feel comfortable about the system. JADCEA was used to deploy the classifiers learned using Performance Miner and was also used to perform in-stream, real-time calculations necessary to perform detection. In addition to the Financial Statement Analysis domain, we also used the same approach in on-line intrusion detection with a large security hosting service company. The same technique was used to detect on-line intrusions where Security Operations Center (SOC) personnel had developed si mple probabilistic models to detect intrusions and attacks. We used their probability based models as previously known domain relationships, validated their predictability, and learned additional domain relationships that were used to detect emerging attacks. The results were reviewed with domain experts and were iden tified as performing as well as or better than existing relationships they had detected. We believe that the techniques we developed can further extend on-line intrusion detection by identifying compensatory behaviour by attackers trying to camouflage their activities. Our research to-date suggests several avenues for future work. First, we only make very limited use of the vector space we create  X  to detect direction of change. However, vector calculus and affine based geometric techniques ha ve been used very effectively in dynamical systems [8]. We be lieve that combining dynamical systems representations with appropriate machine learning techniques can result in a new breakthrough in data mining applications to systems whose be havior changes over time is of concern to end users. Second, different types of ev aluations we conducted with different user populations indicat e that domain experts may be able to label softer exceptions th an the hard labeling (SEC action or known penetration) we used. Furt hermore, this labeling is often driven by application requirement s. While rating analysts may be interested in various ways a company may be committing accounting fraud or misrepresenti ng its fundamentals that may then lead to bankruptcies, portfo lio managers are interested in optimizing their portfolio in the context of expected future exceptional market performance. In the first case, the rating analysts are able to label the training examples using criteria such as earnings persistence or bankruptci es while in the later the users are able to label based on significant events of the company such as earnings restatement or earning surprises. However, each of these types of labeling will have different impacts on the detection phase vs. the forecasting phase. We would like to extend our research approach to d eal with these types of labeling. Third, while we focused our research in the financial domain on just company fundamentals, database s exist that provide historical stock market performance of public companies 1 . While we  X  X RSP X , The Center for Research in Security Prices at the 
University of Chicago, http://gsbwww.uchicago.edu/research/crsp/ performed simple experiments with this database, we would like to explore the potential for extending our forecasting techniques using historical market performan ce of public companies. We also would like to normalize our detection variables based on historical analysis of market performance to remove anomalies that may have resulted because of the period we tested our system where a number of companies were misrepresenting their fundamentals. Finally, while we did limited tests with the data flow engine for high volume data sets, we would like to extend the approach of classifier creation and testing to large data sets with appropriate feedback to help the analyst modify the newly learned relationships based on performance with live detection data. We acknowledge the help provided by John Handley of Xerox Labs. His suggestions were very useful in clarifying the presentation of the key ideas. 
Part of the research reported in this paper was sponsored in part by DARPA SBIR Contract No: DAAH01-03-C-R299. This work reports on research that was performed at Crystaliz, Inc. while the authors were employees of Crystaliz, Inc . 
