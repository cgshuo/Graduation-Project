 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al Performance, exp erimen tation
Relev ance feedbac k (RF) aims to overcome query and doc-umen t disagreemen ts on vocabulary , and user and system disagreemen ts on relev ancy for a given information need. It can be implemen ted as an interactiv e (IRF) or automatic (pseudo-relev ance feedbac k -PRF) pro cess and relies on the initial documen t ranking returned by the system for the original query . The underlying assumption is that the ini-tial retriev al will yield the relev ant documen ts to re ne the query[4, 7]. It is exp ected that they will pro vide new terms, suc h as synon yms, that will impro ve the original query .
The feedbac k performance of retriev al systems dep ends on man y factors and a great variabilit y in precision occurs from topic to topic [1]. On average, the overall system per-formance impro ves after feedbac k, but for some topics the average precision achiev ed is actually lower than the initial run, particularly in PRF.

This e ect may be caused by the use of non-relev ant doc-umen ts (in the case of PRF) or due to the use of a single feedbac k parameter set for all topics [2]. In this work we presen t a new alternativ e explanation for lowered topic per-formance: under speci c feedbac k parameter settings, rele-vant documen ts may act as \poison pills"[9] and harm topic precision after feedbac k.
There are man y possible parameter settings for relev ance feedbac k. For instance, one may prefer use passages rather than documen ts. The optimal num ber of documen ts from the initial run considered is not kno wn in adv ance neither the num ber of terms to be added to the query . Harman [4] discussed term re-w eigh ting and the num ber of query terms. In the case of PRF the documen ts used for feedbac k are normally selected from the top-k rank ed in the initial run but they could be the cen troids of clusters from these top-k documen ts or the top-k interspaced documen ts. Shen and Zhai [8] discussed di eren t strategies in the selection of the feedbac k documen ts. Billerb eck and Zob el [2] review ed pa-rameter setting issues in PRF.

The di eren t retriev al mo dels may need di eren t strate-gies, suc h as Rocchio metho d [7] in the Vector Space Mo del and the div ergence minimisation approac h of language mo d-els for information retriev al [10]. Naturally , wrong choices will harm e ectiv eness and there is no de nitiv e metho d to choose PRF parameters. The problems are similar in IRF except for relev ant documen ts as this information is supplied by the user. However, since the num ber of documen ts that the user is able to read is limited, the documen t selection is constrained [3].
We used TREC 6, 7 and 8 adhoc trac k for evaluation, a total of 150 topics, along with the kno wn relev ant docu-men ts. We used the runs from four systems participating in the RIA workshop [5]: Clarit (TF-IDF), CMU (Lem ur-language mo del), Sabir (SMAR T-V ector Space) and Cit y (Ok api-probabilistic). They represen t distinct information retriev al mo dels as we try to examine the searc h space of parameters/alternativ es in an ad hoc retriev al task. The feedbac k mec hanisms used by these systems are also distinct among them.

For eac h of these systems we used a single relev ant doc-umen t per topic as the only feedbac k and, because of the relev ance, we exp ected an increase in precision on the corre-sponding topic. Surprisingly , 299 documen ts used for feed-bac k made the precision drop in all four systems and we refer to these documen ts as bad relev ant documen ts.
Figure 1 depicts the num ber of topics that these docu-men ts a ect; a total of 47 topics out of the 150 have one or more bad relev ant documen t. The drop in performance is substan tial, in some cases the average precision is 0.10 lower than the baseline. They represen t a small fraction of all rele-vant documen ts; we plot the ratio of bad relev ant documen ts per topic in Figure 2. Overall, 5.33% of the relev ant docu-men ts performed poorly for single documen t feedbac k. This num ber is small thus the chances of selecting one of these documen ts is low. However, we also look at the rank dis-tribution of these documen ts, as depicted in Table 1, where the num ber of bad and relev ant documen ts are listed in dif-feren t rank levels in the initial retriev al for the four systems. Some of the bad and relev ant documen ts do not app ear in the top-1000 retriev ed documen ts of the four systems run. An insp ection on the various depth levels sho w that bad documen ts occur in higher percen tage in the early rankings. Figure 1: Histogram of the number of poison pills per topic Figure 2: Fraction of bad rel documen ts per topic Table 1: Rate of bad docs at various levels(4 sys-tems)
Further analysis is necessary to understand the nature of these bad documen ts. In particular, we are attempting to characterise a bad documen t. Using one documen t for feedbac k is not usual in PRF, but it is not unrealistic in IRF. Some hypothesis may be explanatory to these poison pills:
Documen t talks about sev eral topics. In this case, the feedbac k mec hanism may be selecting a term from another topic in the documen t. Used TREC documen ts are not nor-mally multi-topic.

Binary relev ance judgemen ts. Some documen ts may be marginally relev ant, be on topic but with emphasis in an-other asp ect of it [6].

Feedbac k parameters not set for single docs. The num ber of documen ts and terms to be added to the query may need a di eren t setting in the case of a single documen t. However, preliminary tests on multi-do cumen t feedbac k runs have in-dicated that while the problem is atten uated, the presence of bad documen ts will still lower the average precision.
Feedbac k may need non-relev ant documen ts. In general pseudo-relev ance feedbac k does assume relev ancy for top-k documen ts. Dunlop [3] further suggest that documen ts that do not matc h the query could be used.

More feedbac k documen ts are needed. The feedbac k mec h-anisms may not be getting enough information to expand queries.
Feedbac k impro ves retriev al in average over man y topics but not necessary for all topics individually . A possible rea-son for some topics to have a drop in performance is the existence of bad relev ant documen ts. We have presen ted some numerical evidence that some relev ant documen ts are harmful for feedbac k when used alone. Further analysis will be performed using more relev ant documen ts for feedbac k, including one bad and other relev ant documen ts. [1] N. Alema yehu. Analysis of performance variation [2] B. Billerb eck and J. Zob el. Questioning query [3] M. D. Dunlop. The e ect of accessing nonmatc hing [4] D. Harman. Information Retrieval: Data Structur es [5] D. Harman and C. Buc kley . The NRRC Reliable [6] K. J X arv elin and J. Kek X  al X ainen. Cum ulated gain-based [7] J. Rocchio. The SMAR T Retrieval System: [8] X. Shen and C. Zhai. Activ e feedbac k -UIUC [9] R. H. Warren and T. Liu. A review of relev ance [10] C. Zhai and J. La ert y. Mo del-based feedbac k in the
