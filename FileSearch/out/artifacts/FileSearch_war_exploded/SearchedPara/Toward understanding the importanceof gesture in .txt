 REGULAR PAPER Brian Corrie  X  Margaret-Anne Storey Abstract In this paper, we explore the importance of gesture in distributed, sci-entific collaboration. In particular, we are interested in the impact that distance has when remote collaborators are working together with digital artifacts that are complex (and often visual) in form, such as data that results from complex scien-tific simulations. We call this artifact-centric collaboration. In order to understand such collaborations, we performed a longitudinal ethnographic study of a group of collaborating scientific researchers. We observed a single research group during its regular research meetings, performing over 18 h of observations spanning a 5-month period. In this paper, we present a detailed analysis of two meetings where artifact interaction is prominent, one where all participants are collocated and the other where participants are distributed. Our analysis consists of a detailed coding of the artifact-centric interactions in both meetings as well as an analysis of these interactions. We conclude the paper with a summary of our findings, including a set of guidelines that can be used to inform the design of collaboration software that supports distributed, artifact-centric collaboration.
 Keywords Distributed  X  Collaboration  X  CSCW  X  Multimodal  X  Gesture  X  Scientific visualization Categories and Subject Descriptors H.5.3 [ Information Interfaces and Presentation ]: Group and Organization Interfaces X  computer-supported cooperative work, collaborative computing. H.5.2 [ Information Interfaces and Presentation]: User Interfaces X  input devices and strategies, interaction styles General Terms Performance  X  Experimentation  X  Design  X  Human Factors 1 Introduction Scientific research is rapidly becoming a global endeavor. Today X  X  complex sci-entific problems not only require a wide range of technologies to solve them, but they also require a wide range of expertise. More and more often, researchers are working with collaborators at institutions that are located across the country and/or around the world. In addition, the increasing amount of scientific data that is available to scientific researchers, using high-resolution instruments and/or in-creasingly complex computational simulations, means that the complexity of the information spaces that scientists need to explore is increasing dramatically [29]. though computational simulation, data reduction techniques, data mining, and knowledge extraction are all important tools to today X  X  computational scientist, ultimately insight comes from the scientist X  X  interpretation of this data. Thus, the collaborative exploration of complex scientific data is becoming an increasingly important tool for the scientific research community.
 set of relatively unique problems. Not only is the scientific data the focus of the collaboration, but the data is typically complex in structure, dynamic in nature (e.g., changes over time), and poorly understood (little a-priori knowledge about the data is available). Thus, knowledge extraction, visualization, and the interac-tive exploration of these complex information spaces are key tasks for distributed scientific collaboration.
 are naturally multimodal [2, 21, 22]. People bring  X  X hings X  to meetings (physical objects such as a paper document and digital artifacts such as a data set) and refer to these objects on a regular basis. In particular, the coupling of deictic statements with gestures is an important component of most face-to-face collaborations (e.g., look at this, it was this big). Gestures are often used for other purposes, such as pointing to indicate the next speaker or to enhance a statement that is being made, but gestures are elevated in importance when an object is the focus of the collabo-ration. In face-to-face meetings that involve the design process, up to 14 gestures per minute have been recorded [2]. The question then arises  X  what happens to these communication modalities when collaboration is performed at a distance? Indeed, in their 1995 paper, Bekker calls for a  X  ... concerted empirical attack on the question of what happens to gestures during design meetings [when the users are not co-located] X  [2].
 scientific collaboration. In particular, we are interested in collaboration that in-volves computer-mediated interaction with, and manipulation of, complex sci-entific data (or digital artifacts). We call this artifact-centric collaboration. The research methodology we use for exploring this domain is an iterative one that includes three phases: 1. We perform observational studies of collaborating scientific researchers in a 2. We use the results of these studies to help develop tools that improve the qual-3. We test the effectiveness of the developed tools using both qualitative and longitudinal observational study of an active scientific research group. 1.1 Challenges in artifact-centric collaboration Despite Bekker X  X  call for action [2], relatively little effort has gone toward ex-collaboration. Early work in this area led to an understanding that providing more than just the traditional  X  X alking head X  could be very important in collaborative environments [11, 20, 27]. The paper by Nardi et al. [20] entitled  X  X oving away from talking heads: the use of video as data in neurosurgery, X  shows that the use of video as data (as opposed to the talking head) can be very important. Tang and Minneman [27] use video as data directly to support distributed drawing. Gaver et al. [11] shows that when video of both a person and a physical object are made available, users often ignore the  X  X alking head X  and attend to the video of the ob-ject being discussed.
 for Networked Multimedia Applications (ETNA) [19], which can be used for the analysis of audio and video in collaborative environments. The ETNA taxonomy is an important step in that it moves toward a categorization of collaboration tasks and how audio and video can be used to meet the collaboration needs for those tasks. This includes a classification of the use of video as data. Understanding the specific collaboration needs for a given type of collaboration task is essen-tial to providing a successful collaboration environment. Although task has been shown to be important in the Computer-Supported Cooperative Work (CSCW) research community for some time [18], very little of the CSCW research is clas-sified against such a taxonomy of tasks.
 nize the information needs for that task (the information that needs to be com-municated in order for the collaboration to be successful). Through understanding the information needs, it is then possible to determine how to communicate that information with a set of communication modalities (audio, video, gesture, etc.). 1.2 Object-centric collaboration Before exploring artifact-centric collaboration in detail, it is helpful to explore the area of object-centric collaboration. We define distributed object-centric collabo-ration as collaboration that focuses on real, physical objects. The computer medi-ates the collaboration by communicating information about the object to remote users, typically using a video feed of the actual object. One of the most concen-trated efforts in understanding distributed object-centric collaboration has been carried out by Fussell and colleagues [10, 15, 16, 23, 24]. This series of papers focuses on collaboration tasks that involve a physical object, such as repairing a bicycle ( object -centric). centric and other types of collaboration. In particular, they state: 1. Gesture support in CSCW is different from gesture support in human X  2. Gestures may play a role as both an HCI and an HHI (human X  X uman interac-3. There are few theoretical guidelines to direct researchers in the construction of area of gestural interaction has occurred in an HCI context as opposed to an HHI context. It is not clear how much of the gesture research that has been carried out in the HCI community can be applied to human-to-human object and artifact-centric collaboration. 1.3 Artifact-centric collaboration Artifact-centric collaboration is similar to object-centric collaboration in that the focus of the collaboration is about  X  X  thing. X  We define artifact-centric collabo-ration as collaboration that focuses on digital objects or artifacts (objects repre-sented digitally on a computer). One of the key differences in this type of col-laboration is that manipulation of the artifact occurs through interaction with the computer (as opposed to interaction with the physical object itself). Like object-centric collaboration, artifact-centric collaboration is naturally multimodal and re-search indicates that the communication of gestures is required to perform this task effectively [2, 21]. Ou implies that the spatial physicality of the object is what makes multimodal interaction with gestures important [24]. It is our belief that the human X  X bject/artifact interaction is what requires this multimodal interaction, not the physical nature of the object itself.
 interaction to the same level as that of object-centric collaboration. Although this may seem logical, even for relatively simple objects like a spreadsheet, complex multimodal interactions that include aural, visual, and gestural interactions are common. This holds true for the design process [2] and our research presented here supports this in the domain of scientific collaboration. In addition, recall that we are considering scientific artifacts that are structurally complex, possi-bly changing over time (due to the time varying nature of the phenomena they represent), and poorly understood (little a-priori knowledge about the artifact is available). Such an artifact is of similar complexity (if not more complex) to that of a physical object, and therefore the information that needs to be communicated is likely to be similar in nature and complexity. Previous research in distributed, artifact-centric collaboration has included a number of techniques, including the use of telepointers [13], avatars [1], and video overlays [6, 7, 27, 28] to commu-nicate information. The goal of this research is to step back from these specific implementations and try to understand the fundamental interactions required in artifact-centric scientific collaboration. 2 Preliminary findings with CoTable Our current focus on gesture and its importance in artifact-centric collaboration stems from our previous work that explored gestural interaction on distributed, collaborative tabletop display systems [6, 7]. This work explored distributed artifact-centric collaboration through the implementation of a distributed, collab-orative, video editing application on a touch sensitive tabletop display. Although not the direct focus of this research, our findings from these experiences were important in informing our current research direction. 2.1 CoTable There are four main components to the Collaborative Table (CoTable) system: 1. a tabletop gestural input device (the Mitsubishi DiamondTouch (DT) table [9] 2. a display that shows the video of the remote collaborator and a video of the 3. a two camera video system that captures the video of the remote user and a 4. an audio system.
 in Fig. 2, and the overhead application video is shown in Fig. 3. The components connect to remote systems using standard networking infrastructure. 2.2 VideoBench The video editing application (VideoBench) is an interactive application for the manipulation and editing of video clips. The original application supported colo-cated collaboration and used gesture as an interaction mechanism. It allowed the user to play (play, pause, rewind, stop) and edit (cut, splice, resize) a set of video clips using gestures on the DT table.
 tributed application. The user X  X  gestures were communicated to the remote site through the drawing of an icon that represented where the remote user was touch-ing the table (including a gestural trace that disappears over time). Gestures were also communicated to the remote user through a video of the top of the table (Fig. 3).
 from this research follows. A more complete description of this work is available in refs. [6, 7]. 2.3 CoTable experiences Although we did not perform a formal user study with this system, we did gain some valuable experience from using it in our lab. Everyone that used the system stated that being able to see the remote user X  X  gestures was important to complet-ing the task. Gestures were communicated by the overhead video feed (Fig. 3)or through the use of gestural traces. Two trends emerged: 1. Users used the overhead video feed extensively, often instead of looking at the 2. Users adapted quickly to the application and made extensive use of gestural gesture may be one of the most critical communication mediums for artifact-centric collaboration. In order to understand this domain more effectively, we un-dertook a more detailed study of artifact-centric collaboration. 3 Studying artifact-centric collaboration 3.1 Methodology The study of group work is complex and must take into consideration a range of factors that affect the collaboration process. These factors include the group X  X  characteristics (including size, history, and member characteristics), the local sit-uation (including the technologies that are available for the group), the task being performed (including its nature and difficulty), and the organizational context in which the group is working [22]. Like Olson, we believe that research into group work should take a mixed methods approach [8], spanning research in the lab (through quantitative laboratory studies) and research in the field (through quali-tative and exploratory studies). We believe that it is important to study groups in a naturalistic environment in order to understand the influences of environmental factors on group work.
 1. We perform exploratory studies that increase our knowledge of group work, 2. We use the knowledge gained from these studies to inform the development 3. We test the effectiveness of these tools, and therefore the validity of the the-system (described in Sect. 2) raised some important questions about how collab-orators communicate using gesture when working together remotely. Our current approach to exploring these questions is through a longitudinal ethnographic study of an existing work group as they collaborate in a natural work environment. We report on the results of this Phase 1 study as follows. 3.2 Observational studies The goal of this research is to gain a better understanding of how scientific col-laborators interact with digital artifacts during meetings. We are interested in ob-serving both collocated and distributed research groups. This is important, as we want to understand how collocated researchers work with digital artifacts and what information is lost when these collaborations occur at a distance. We attempt to gain this understanding using naturalistic, longitudinal observational studies in the field. We observe research groups during normal work meetings in both collocated and distributed settings. Meetings are recorded on video tape for later analysis. A single observer records the meeting, manipulating the camera to focus on specific activities as appropriate. The observer also takes notes on any interesting events that occur during the meeting. The observer does not participate in the meeting. 3.3 Coding Analysis of the data gathered from these studies used an open and emergent coding scheme similar to that suggested by Grounded Theory [12]. As actions occurred during the meeting, the coder annotated the action with a variety of information. This included an event identifier, an event code, an event subcode, the time the event took place, the channel used to communicate the event, and any additional comments about the event. Although the coding scheme we developed emerged during the course of this study, it was influenced by a number of similar efforts. It was based roughly on a gesture coding schema from the Department of Linguistics at Goteborg University in Sweden [4] with influence from the work of [2, 26]. Structural codes marked moments in the meeting where the structure or phase of the meeting changed. For example, a meeting might initially provide an update on the status of a project (a description phase) and then move to a question and answer period (a discussion phase). Structural codes were used to divide a meeting into phases. Action codes annotated actions made by participants in the meeting. There were a wide range of action codes used, including verbal utterances, gestures, body language, facial expressions, and physical actions (such as writing or typing). ing the analysis. For example, verbal utterances might be classified as a statement, a question, a response to a question, verbal feedback, or referring to an artifact. Gestures were coded in a similar way, with coding differentiating between ges-tures that pointed at physical objects, gestures that pointed at digital artifacts on the screen, gestures that referred to a person, and gestures that were used for em-phasis ( X  X t was this big X  and indicating size with your hands). We currently have approximately 75 code/subcode pairs in our coding scheme. Not all codes were used at every meeting, and our coding was refined as we recognized subtleties in the interactions that needed to be captured. As the coding changed, events from previous meetings were checked to determine if the new codes applied to those events. In addition to the code and subcode of an event, the communication chan-nel was also noted if it was ambiguous. A communication channel is the mech-anism through which the event is communicated. For example, a gesture may be communicated by pointing a finger or by pointing with a computer mouse. commentary. Comments might include information about actions that were per-formed, elaboration about the utterance issued, etc. Once coding was complete, an analysis of the coding was carried out. The goal of this analysis was to identify themes, features, or events of interest that emerge from the meeting. This analysis is described in more detail in Sect. 5.1. We used a single coder that analyzed all meetings, which maintained consistency in coding, at the risk of personal bias. Because the coding process was an emergent process and evolved during the ob-servational study, it was deemed that having a single coder develop the scheme was important in maintaining consistency. 4 The study The goal of our study was to understand the impact of distance on artifact-centric collaboration. Because of the recent growth in the size of scientific data sets, and the resulting complexity in the artifacts of interest, distributed scientific collabo-ration is an ideal domain for this research. 4.1 Subjects Our study was a longitudinal study of a small research group of computational sci-entists. The group consisted of 14 members (six females and eight males) and met once or twice a week to work on a variety of projects. Projects revolved around the mathematical modeling of complex processes and phenomena. Meetings varied in their purpose and content, including planning sessions for new projects, open dis-cussion sessions, presentations to the group, and focused research meetings. 4.2 Technology environment Group members traveled often during the observational period, and therefore at-tendance at group meetings from remote locations was often necessary. The group used a variety of collaboration tools. An audio communication channel was always used, and was provided using either an analog phone, IP (network) based audio collaboration tools (Skype [17], iChat [3], or AccessGrid [5]). Video of remote group members was sometimes used, using either iChat or AccessGrid.
 This included papers that were being presented or discussed, spreadsheets that contained the results of computational simulations, graphs or other visualizations of the simulation and modeling results, the source code of the computational sim-ulation itself (for development or explanatory purposes), and digital sketches of brainstorming documents for project planning and development. Artifacts, and how they were manipulated, are discussed in more detail in Sect. 5.1. Shared docu-ments between remote participants were typically provided using Virtual Network Computing (VNC) [25]. Using VNC allowed a remote user to see another user X  X  desktop, including mouse motion. It was also possible for remote collaborators to take control of the remote desktop and manipulate artifacts as if they were on their local desktops.
 members. The room had two 62 X  plasma displays mounted on the wall, each with a Smart Technologies touch screen overlay (SmartBoard) that allowed users to draw directly on the screen. The room had a permanent computer that drove the The permanent computer was connected to, and controlled, the SmartBoards. The room also contained sophisticated AV components, including an acoustic echo canceller (providing good quality full duplex audio) and video cameras for send-ing video streams to remote collaborators. 4.3 Observed meetings Our study included the observation of 10 meetings of approximately one and a half hours each over a 5-month period, for a total of approximately 15 h of raw data. The meetings ranged in topic from casual discussions through to intense analysis of computational models and the modeling results. Meetings were coded as described in Sect. 3.3.
 and optimizing complex systems. The system under investigation during our ob-servational study was an organization that had encountered bottlenecks in dealing with its clients. The organization wanted to gain a better understanding of how clients were processed, understand where the bottlenecks in the system were, ex-plore what-if scenarios by changing the operation of the system, and ultimately optimize the system to meet target goals. The research group, starting with data that had been gathered about the system over time, created mathematical mod-els of the system that could then be used to understand, simulate, and optimize the system. The models were validated by running the models and comparing the results to the observed data. In addition, multiple models of the system (using different modeling approaches) were created and validated against each other. meetings that we observed) that were of particular interest. Meeting three (M3) and meeting four (M4) were data and model analysis meetings and were therefore highly artifact-centric. In addition, M3 was a distributed meeting while M4 was collocated, providing an interesting contrast to how the users interacted with each other. We focus on these meetings because they highlight two areas in which we are interested: (1) how users interact with complex artifacts, and (2) the impact of distance on artifact-centric collaboration.
 the organization. Before the meeting, one of the members of the group had per-formed an initial analysis of the data set and created an initial model that simulated the system. M3 was a distributed meeting with one of the group members travel-ing overseas at the time of the meeting. The remainder of the group members that participated (six of them) met in the room shown in Fig. 4. The group used an overseas telephone connection to provide audio to the remote participant. A spreadsheet was sent to the group by the remote participant before the meeting. One of the group members transferred the spreadsheet to the local computer in the room and the spreadsheet was displayed on one of the two plasmas. The spread-sheet contained several visualizations of the data being discussed (in the form of graphs) as well as the raw data from the computational simulation. The remote user connected to the local computer using VNC, and was therefore able to see what was on the desktop of the local computer.
 people with one additional group member. The same data and model were dis-cussed at this meeting, although it had been further refined since M3. In addition, one of the other participants had developed a second model of the system for comparison and validation purposes. The goal of this meeting was to understand in more detail both the data set and the models of the system that had been devel-oped.
 laptop display was mirrored across both plasmas so that the entire group could see the artifacts of interest more readily. The group also used the local computer in the room for brainstorming and sketching. This computer was connected to the SmartBoards in the room, allowing group members to write directly on the screen with digital ink. The group could switch from the laptop display to the local computer display at the press of a button (on a small video switch). 5 Analysis of observational data Both Meetings M3 and M4 were coded using the coding scheme described in Sect. 3.3, with a focus on events that involved interaction with artifacts. Utterances that did not directly involve artifacts were not coded. All gestures and utterances that referred to artifacts or objects were coded, including gestures that referred to ob-jects or artifacts indirectly (such as statements  X  X t was this big X ) and body language and facial expression events that were deemed pertinent to the communication. 5.1 Observed gestural interactions After performing a detailed analysis of the coding for a number of meetings (in-cluding M3 and M4), we discovered that the low-level events that were coded could be grouped into composite events that had meaning above the individual events. Question and answer pairs, gesture and utterance pairs, and gesture, utter-ance, and action triads are all potentially interesting composite interaction events. In addition, we differentiated between gestural events when they occurred phys-ically (someone physically points), on the computer (someone points with the mouse), or with the SmartBoard. We then analyzed this information over time to determine the structure and flow of the meeting and to expose themes and patterns in how users interact with digital artifacts.
 events. In particular, we have defined two types of important high-level artifact interaction events  X  explicit and implicit artifact events. An explicit artifact event occurs when the following criteria are met: 1. An utterance event occurs. 2. A gesture event occurs. 3. The utterance and the gesture events are generated by the same individual. 4. The utterance and gesture events occur at approximately the same time. 5. The utterance refers to an artifact. 6. The utterance is deictic in nature (that is, the utterance makes an explicit ref-utterance is not deictic in nature. That is, only criteria six is different in the above list. For example, an utterance of  X  X he total is 42 X  while pointing to the number 42 in a spreadsheet would be an implicit artifact event. The key information is implied in the utterance itself (that the total is 42), while the gesture adds emphasis or elaboration (which 42 is being discussed, since there may be more than one in the spreadsheet) but not necessarily meaning. There is no deictic utterance during an implicit artifact event.
 following reason. In order for an individual to understand an explicit artifact event, both the utterance and the gesture must be communicated to other individuals (in particular, to remote participants in a distributed collaboration). Without the ges-tural component of the event, an explicit artifact event has little or no meaning. Implicit artifact events have enough meaning encoded in the utterance to deter-mine meaning without the gestural component (although the gestural component may add important information to the communication). Given that we are inter-ested in the impact of distance on artifact-centric collaboration, it was deemed that this distinction was of critical importance. We need to understand the frequency and usage patterns of these types of high-level communication events.
 atomic artifact gestures and spatial gestures. Atomic artifact gestures (as opposed to composite artifact gestures) are those gestures that refer to an artifact but are not accompanied by an utterance. These gestures typically occur when participants are about to generate either an explicit or implicit artifact event but the flow of communication within the group prevents the utterance from taking place. Spatial gestures are those gestures that contain information of a spatial nature that refer to an artifact or object ( X  X t was this big X ).
 marily because the majority of gestural interactions fell into these two categories, with 19 implicit and 17 explicit artifact events (out of a total of 41 artifact-related gesture events) recorded during M3 and 72 implicit and 88 explicit artifact events (out of a total of 171 artifact-related gesture events) recorded during M4. The ar-tifact gesture events that do not fall into these two categories (5 in M3 and 11 in M4) are either atomic or spatial artifact gestures.
  X  Highlighting a single cell in a spreadsheet with the mouse combined with the  X  Physically pointing at a number from the output of a computational simulation  X  Circling a feature in a graph using one of the SmartBoard pens combined with  X  Moving the mouse toward a column that represents the values for the first year  X  Writing a formula combined with the utterance  X  dn / dt = r  X  s , right X . Note 5.2 Meeting 3 analysis M3 was distributed, with one participant at a remote site overseas (a hotel room). The meeting lasted 1 h and 15 min. The main topic of the meeting was the dis-cussion of the data set and a mathematical model of the system that attempted to model the organization X  X  processes. The model was instantiated as a computer simulation and produced numerical results. These results could be viewed in a number of ways, numerically and graphically. The following paragraphs explore M3 in detail, with each paragraph denoting a specific phase of the meeting. The start time of each phase of the meeting is noted at the start of the paragraph, with the interactions that took place during each phase described briefly in the para-graph body. 0:00:00  X  The first phase of the meeting was technical setup. Although the re-0:32:12 -Once a connection was established, the meeting rapidly moved into a 0:39:24  X  One of the local participants asked  X  ... can you walk down the 0:41:51  X  R took control of the application and mouse and continued to explain 0:48:31  X  L1 took control of the application and loaded a document that con-0:54:18  X  The phase changed into a discussion of the raw data. This phase re-1:08:30  X  L1 requested R to clarify an artifact utterance, at which time R began 1:12:57  X  The artifact-centric portion of the meeting was completed and the
Summary: By the end of the meeting, the research group appeared to have a basic 5.3 Meeting 4 analysis M4 took place 5 days later and was a similar meeting in basic structure to M3. The goal of the meeting was to explore further the system being modeled and to validate the model that was being developed. One of the other participants had developed a second, independent mathematical model for the system, and this model was also explored in the meeting. The main difference between M4 and M3 in terms of meeting composition was that all participants in M4 were collocated. One additional member joined the group and the remote user from M3 was now on site (denoted as L2 in the following description). The phases of M4 are explored in more detail as follows: 0:00:00  X  The meeting started with a description phase where L1 (the same L1 0:14:00  X  L2 opened the spreadsheet containing the raw data and the original 0:18:44  X  L2 started to describe the data in some detail, with the goal of validat-0:22:46  X  L2 opened a text document that contained the raw output from the 0:26:36  X  L2 opened a document that contained the code for the computational 0:33:55  X  The group X  X  focus turned to a different approach to modeling. In order 0:37:56  X  The group switched back to the laptop display, with L2 in control of the 0:48:41  X  At this point, the interaction became very dynamic. The group 0:50:53  X  The group switched back to the laptop and the first model (3 explicit, 0:56:12  X  The group switched back to the SmartBoard (6 explicit, 3 manipula-0:57:00  X  The final artifact-centric phase of the meeting had the group exploring 1:05:22  X  L2 identified the problem in the model and explained what the issue 1:13:51  X  The artifact-centric portion of the meeting was finished and the group
Summary: The meeting appeared to be successful. The group had a clear under-6 Findings Based on our initial investigations with the CoTable environment [6, 7], we dis-covered that users were likely to use communication channels in innovative and surprising ways. In particular, for visually rich and complex tasks they consider gesture as a critical part of their work process. In fact, users found innovative and often surprising ways to communicate the interactions that they require to accom-plish a task. Our investigations here support that finding in many ways. In both M3 and M4, participants made extensive use of the mouse, the SmartBoard, and physical gestures to communicate information about artifacts. 6.1 Meeting 3 Many of the problems that participants had at the beginning of M3 are typical of today X  X  collaboration environments. Networking difficulties, firewalls, software compatibility, user configuration, and a variety of other factors often confound the most astute of collaboration experts. Although not the topic of this paper, clearly we have a lot of work to do on the design and implementation of these tools. De-spite these problems, once a connection was made M3 rapidly focused on the main task. The participants immediately started exploring the data set and the results of the computational model created by the remote user (R). The artifact provided by R before the meeting (the spreadsheet with the relevant 2D graphs) was used extensively during this meeting, with R using it to enhance his description of the model.
 occurred during each minute of M3, starting at minute 40 and ending at minute 75. For example, minute 42 in Fig. 7 represents the 1-min period of M3 from the end of minute 41 to the end of minute 42. As the figure shows, during this pe-riod three explicit and three implicit artifact events occurred, giving a total of six artifact interaction events for that time period. The graph starts at minute 40, as no gestural interactions occurred before that time in the meeting. Recall that the collaborators in this meeting encountered significant connection problems with the remote participant and that the meeting proper did not start until minute 32. The gesture events that are shown in Fig. 7 are the composite explicit and implicit artifact events as described in Sect. 5.1. These artifact events occur when a sin-gle individual makes an utterance and points to an artifact on the screen at the same time. Explicit artifact events are those that contain a deictic utterance ( X  X hat one X ) combined with a gesture while implicit artifact events are those where the utterance implies meaning ( X  X he total is 42 X ) and the gesture adds to the commu-nication through a supporting motion (pointing at the cell in the spreadsheet with the number 42).
 M3 was one of the first meetings that the group held in which complex interaction with artifacts involved remote participants. Initially, L1 manipulated the artifacts at the direction of R. When requested by R, L1 started using explicit and implicit artifact events to refer to artifact features (minute 40). Rapidly, it was realized that R should have control of the artifact (minute 42). Once the participants realized that artifact interaction was possible by both local and remote participants, this interaction continued until minute 54 (as shown in Fig. 7). Like our experiences with CoTable, participants adapted to the system very quickly.
 output, interaction switched from using the mouse as a pointing device to physi-cally pointing at the artifact on the screen (minute 51 and 54). This appeared to be a natural interaction for the local participants but because R was a remote partic-ipant, he could not see any of these interactions. Thus, R had to rely on only the utterances to follow the discussion. It appeared that the local participants did not realize that R was missing critical information required for communication and R did not explicitly state that there was a problem. At this stage, the meeting started to discuss details that did not require interaction and R no longer used the mouse to point at artifacts on the screen. When the group went back to discussing the spreadsheet and continued with the model explanation, it took some time before R returned to using the mouse as a pointing device. When R referred to artifact fea-tures without gesturing, the lack of artifact interaction caused confusion among the participants. It took an explicit question from one of the participants about which artifact R was referring before R started using the mouse to perform gestural in-teraction again. Once mouse-based gestural interaction started again, it continued until the meeting ended (see minute 68 X 72 in Fig. 7).
 of the data set and the model that was used to simulate the physical system. Thus, the meeting appeared to be a success. Given the complexity of the data set and the model being used to simulate the system, artifact interaction was clearly an important component of this meeting. This importance is demonstrated by the fact that the main contributor to the meeting was the remote user and that when R did not use explicit and implicit artifact events the group rapidly got confused. 6.2 Meeting 4 M4 was different from M3 in two key ways. First, there was no remote collabora-tor, allowing us to observe how participants interacted with the artifacts in a collo-cated environment. In addition, the participants had been exposed to the model 5 days previously and therefore were familiar with the modeling approach, the data, and the modeling results. Figure 8 is similar to Fig. 7 but displays the number of explicit and implicit artifact interactions for M4 rather than M3.
 mote participant (R in Meeting 3, L2 here) started giving a detailed description of the raw data and the model X  X  output (minute 20 X 26 in Fig. 8). Artifact inter-action was extensive through this phase of the meeting, using the mouse almost exclusively as the pointing device. We see up to 11 artifact-related gestures per min during this phase of the meeting, close to the 14 gestures per min reported by Bekker et al. [2]. This high artifact interaction rate appears to be due to two fac-tors. First, L2 is rapidly describing a complex set of data and is therefore moving around the artifact quickly, highlighting and pointing at a wide range of artifact features. Second, this phase is primarily a one-way communication. That is, L2 is simply describing the data and is therefore not pausing other than to get acknowl-edgement and feedback from the group members.
 gestures typically being taken up by discussion or switching between artifacts. The drop in artifact interaction at minute 26 and 27 in Fig. 8 (a 2-min period with only one gesture event) is due to the opening of a new document and the brief discussion that followed about what the artifacts in that document represent. The lull at minute 29 and 30 is due to a side discussion that did not involve artifacts. tionship between meeting phase and the type of gestural interaction being used. It is clear from Fig. 8 that there are interaction spikes throughout the meeting. These spikes appear to be related to both technology usage (mouse versus SmartBoard interaction) and meeting phase (descriptive versus discussion phases). Based on our analysis, it appears that explicit artifact interaction is used heavily in descrip-tive phases of the meeting (when one person is describing something to someone else), while implicit artifact interaction is used in both description and discussion phases. The phases from minute 31 X 33, 38 X 41, and 55 X 58 in Fig. 8 are descriptive phases of the meeting, making use of almost entirely explicit interactions. These phases of the meeting span multiple minutes (3, 4, and 4 min, respectively), using only explicit interactions. These phases use mouse, mouse, and SmartBoard-based interactions, respectively. The spike from minute 33 to minute 36 is an artifact creation phase on the SmartBoard and therefore has a relatively high number of implicit interactions (speaking while writing/gesturing at the SmartBoard). These results indicate that the function of task may have an effect on the type of gesture being used.
 46 is primarily a one-way description with moderate group interaction. This phase uses mouse-based artifact interaction and is similar to the interactions that occur from minute 20 to minute 26. The spike of both explicit and implicit interactions at minute 48 X 51 is a short discussion phase where participants use the SmartBoard to solve a mathematical equation. This leads to an important insight into a potential problem with the computer model that is represented by the spreadsheet. It is at this point that a true discrepancy between the two mathematical models being discussed seems evident. Up until this moment, it was conjecture that there was a problem and it was unclear where the problem was. The calculation as defined on the SmartBoard indicates that the problem is in the computational model. some detail. The exact problem is identified at minute 64, with the discovery pro-cess at its peak around minute 59 and 60. At this point in time, most of the group is actively engaged in the discovery process and three of the participants are actively pointing out artifact details in rapid succession.
 interactivity builds to the discovery of the problem in the model. As the meeting progresses, it slowly transitions from a description of a complex model, through an investigation of an odd feature of the model, to the discovery of a major problem with the model and its eventual solution. This process starts at minute 20, when the group notices a discrepancy in the model. This is the beginning of artifact interaction (see Fig. 8). As the meeting progresses and the participants become more involved in the discovery process, the number of physical gestural interac-tions (pointing physically with the body) increases. Before minute 28, there are no physical gestural interactions. From minute 28 to minute 63, physical interactions steadily increase in number, peaking at minute 60 (see Fig. 9).
 also peaks around minute 60. Typical descriptive gestural interaction consists of a single participant performing most of the gestures. An example of this is shown from minute 20 to minute 26 in Fig. 10 , where there is a large block of artifact interactions being performed by a single participant (L2). This is clearly not the case during the intense discovery phase of M4 (from minute 48 to minute 63). During this period, there are 21 gestural turn taking changes between participants (or more than one turn taking change per minute). Clearly, this discovery pro-cess is a very dynamic and interactive process that needs to be well supported by collaboration tools.
 Figs. 9 and 10 is also noticeable when watching the video recordings of the meeting. An individual X  X  engagement level progresses from sitting sedately in his/her chair to several people standing in front of the SmartBoard interact-ing with each other and the relevant artifacts in a very dynamic and active manner. 7 Discussion 7.1 Implications for tool design One of the clear indications from these studies is the importance of supporting meeting processes with our collaboration tools. We switch between tasks quickly and easily. This is clearly revealed in our study, as meetings change from light-hearted discussion to intense analysis in very short periods of time. Even within the domain of artifact-centric collaboration, switching between descriptive dia-logue and interactive dialogue happens often and quickly and our results suggest that different artifact interaction styles may need to be supported for different phases of artifact-centric collaboration tasks.
 use innovative and often unexpected techniques to communicate gestures. Al-though our studies did not reveal any unexpected communication techniques, the frequency and ease with which the remote collaborators started communicating using gesture is indicative of this ability to adapt. It also indicates the importance of gesture in artifact-centric collaboration, as such an adaptation would be unlikely to occur if the adaptation were not required to accomplish the task.
 gestural interaction. In particular, a very large proportion of the artifact gestures that we observed were gesture/utterance pairs. There were approximately equal numbers of explicit and implicit artifact events. It is worth noting that in a remote situation explicit artifact events are meaningless unless both the deictic utterance is heard and the gesture is seen. It is, therefore, very important that distributed, artifact-centric collaboration tools support explicit gestural interaction effectively. well, there was some hesitation in using the remote desktop capabilities for com-municating gestures. In some cases, participants resorted to verbally describing the location of an artifact feature, even when they had very recently used the mouse to point at such a feature. These moments often resulted in cumbersome dialogues ( X  X he top one?, X   X  X o, the next one X ) rather than simple explicit artifact gestures. Fortunately, familiarity and comfort with the tools seemed to increase rapidly, and as the meetings progressed participants appeared to be more willing to inter-act with the artifacts. Once participants started using the mouse as a gestural and pointing tool, explicit and implicit artifact interactions were used extensively and naturally.
 physical interaction. Although these results are preliminary, there appears to be a strong link between engagement in the meeting and the amount of physical in-teraction (pointing with your arm, touching the screen). M4 went from a sedate descriptive phase (where a single individual did most of the interaction) through to an exploratory phase where participants were trying to understand the results of the model and simulation. The meeting ended in an intense and interactive problem-solving phase with most of the group involved and with several of them standing at the SmartBoard physically interacting with the screen. As an observer in the meeting room, one could see the energy of the participants building as the meeting progressed. As individuals became more engaged in the problem solv-ing process, actions moved from the computer to the physical environment (as showninFig. 9). As the physical environment is our natural environment this may come as no surprise, but this suggests that direct interaction technologies like the SmartBoard and the DiamondTouch table may provide avenues for much fruit-ful research for supporting gestural interaction in artifact-centric collaboration. In addition, gestures like these are very difficult to communicate to remote collabo-rators, indicating that research in the area of communicating physical gestures in distributed collaboration would be highly beneficial.
 interaction that took place. M3 had only 36 artifact interactions (19 implicit and 17 explicit) while M4 had 160 artifact interactions (72 implicit and 88 explicit). It is tempting to point to the fact that M3 was distributed and M4 was collocated as the main reason for this difference, but this is difficult to do. Although the two meetings were focused on understanding the model and the data it was trying to interpret, there were many other differences. The beginning of M4 was a review of the model that was already discussed in M3. People had seen the data before so the review went quickly, resulting in a set of rapid artifact interactions. In addition, M4 was exploratory in nature while M3 was descriptive, resulting in more participants being involved in M4. Finally, the fact that an important and fundamental problem with the model was identified halfway through the meeting meant that most of the group was very involved. All of these factors contributed to the difference in the number of artifact interactions. More research is required to determine the impact of distance on artifact-centric collaboration. 7.2 Limitations The goal of this research was to gain a better understanding of how scientific collaborators interact with digital artifacts during meetings. Our lab-based explo-rations of artifact-centric collaboration using the CoTable system (described in Sect. 2) raised some important questions about how collaborators communicate using gesture when working together remotely. Our approach in trying to explore these questions was to perform a longitudinal ethnographic study of an existing work group as they collaborate in a natural work environment. It is our belief that ethnographic studies such as these are an important tool in growing our under-standing of distributed, artifact-centric collaboration. With that said, such studies are only the first step in providing a thorough exploration of this area. Our study identifies questions that need to be answered, but does not answer any of these questions directly. Further studies are required to explore these areas in more de-tail.
 order to study our user community in their natural working environment. We use an open and emergent coding scheme to code relevant events during our meetings. The coding approach we used is similar to that suggested by Grounded Theory [12] with influences from the relevant literature on gestural coding schemas [4] and models of gestural interaction [2, 26]. Such an approach relies on qualitative judgments from the coder. Because our coding scheme emerged during the meet-ings (that is, we did not have a complete coding scheme before our observations began) we used a single coder for the meetings discussed in this paper. This al-lowed us to maintain consistency in both the development of the coding scheme as well as in the coding itself, at the risk of introducing personal bias. Now that our coding scheme has been defined, it is possible to have multiple coders perform analysis of our meetings and to perform consistency checks across coders. For our future studies, this is the intention. 8 Conclusions This study is the first step in our human-centered design process, with the goal of creating effective and useful tools for artifact-centric collaboration. In this paper, we present the results of our analysis of a collaborating mathematical modeling research group in both collocated and distributed situations. Although these ob-servations are part of a longer-term study, they provide us with important insights that can help to inform the development of more effective tools for artifact-centric collaboration. These insights include:  X  Collaboration tools need to support meeting processes. They must be able to  X  Artifact-centric collaboration is an important component for some, but not all,  X  Gestural interaction appears to be important in artifact-centric collaboration.  X  Users adapt quickly to their environments. When gestural interaction is sup- X  Most gestural interactions are coupled with verbal utterances. It is, therefore,  X  Different users utilize gestures in different ways. Some users appear to be more  X  When participants are highly engaged in an artifact-centric task, they appear to  X  When participants are highly engaged in an artifact-centric task, more than one our continuing observational studies and the development of collaboration tools for artifact-centric collaboration. Although this work provides useful insights into gestural interaction and artifact-centric collaboration, much work remains. In par-ticular, our future work includes the application of the guidelines presented earlier in developing collaboration tools for artifact-centric collaboration and the testing of the validity of these guidelines in their effectiveness for developing these tools. References Author Biographies
