 Finding a proper distribution of translation probabilities is one of the most important factors impacting the e ff ectiveness of a cross-language information retrieval system. In this paper we present a new approach that computes translation probabilities for a given query by using only a bilingual dictionary and a monolingual cor-pus in the target language. The algorithm combines term associ-ation measures with an iterative machine learning approach based on expectation maximization. Our approach considers only pairs of translation candidates and is therefore less sensitive to data-sparseness issues than approaches using higher n-grams. The learned translation probabilities are used as query term weights and inte-grated into a vector-space retrieval system. Results for English-German cross-lingual retrieval show substantial improvements over a baseline using dictionary lookup without term weighting. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Cross-Language Retrieval, Query Formulation Term Weighting, Term Co-Occurrence measures, Translation Dis-ambiguation
Web-accessible documents are becoming increasingly available in languages other than English. Users that are able to read docu-ments in more than one language can benefit from cross-language retrieval, where the information need is expressed in a user X  X  native language (the source language) and a ranked list of documents is returned in another language (the target language).

The two most straightforward techniques for cross-language re-trieval are: (1) Automatically translate all documents in the collec-tion into the source language and then apply monolingual retrieval on the translated document collection; and (2) Automatically trans-late the user-posed query into the target language and then apply monolingual retrieval with the translated query on the original doc-ument collection in the target language.

The second approach, query translation, is by far the most com-mon cross-lingual retrieval approach. However, Chen and Gey [3] showed that translating the entire document collection outperforms query translation and also that a combination of query translation and document translation can lead to further improvements in re-trieval e ff ectiveness. Nevertheless, most approaches use only query translation because document translation is very time consuming and requires re-indexing of the entire collection each time the au-tomatic translation system is modified to produce a new target col-lection.

Query translation requires access to some form of translation dic-tionary. Three approaches may be used to produce the translations: 1. Application of a machine translation system to translate the 2. Use of a dictionary to produce a number of target-language 3. Use of a parallel corpus to estimate the probabilities that
Each approach has advantages and disadvantages. The machine-translation approach is likely to produce the best translation results. On the other hand, this approach assumes that the input is a syn-tactically well-formed unit, i.e., a declarative sentence (such as I am looking for documents that discuss the Cuba crisis and Pres-ident Kennedy X  X  role ), or a question (such as What was President Kennedy X  X  role in the Cuba crisis? ). Machine translation systems rely, to some degree, on the linguistic context to decide what the most likely translation should be. The problem is that most users don X  X  pose their query as a well-formed sentence, but as a list of keywords (e.g., President Kennedy , Cuba crisis ), disregarding any word order or other syntactic constraints.

The other shortcoming of many machine translation systems X  especially commercial systems X  X s that they only return the most likely translation, where  X  X ost likely X  is defined in terms of the internal algorithm of the translation system. But this does not mean that there are no other equally good X  X r, by some other objective standard, maybe even better X  X lternative translations.
A simple alternative to a full-fledged machine translation is a dictionary lookup approach that uses a bilingual machine-readable dictionary. Dictionary lookup will not provide a single best trans-lation, but a set of possible translations for the query terms. The advantage of this approach is that it does not require syntactic well-formedness, i.e., it can translate a simple unordered list of key-words. The disadvantage is that the translations are normally not prioritized, as most dictionaries do not include information about how likely it is that a word w translates into w 0 versus w cases where such information is available, these preferences are ranked, not quantitatively expressed, and they typically refer to global preferences disregarding any contextual information from the topic.

The third approach is to use a parallel corpus to estimate proba-bilities of translations between source-and target-language words. A parallel corpus is a collection of source-language documents cou-pled with a target-language (human) translation of each document. Sentences are typically aligned to indicate the correspondence be-tween the original sentence and the translated sentence. This ap-proach merges the advantages of the machine-translation frame-work with those of the dictionary-lookup approach in that the paral-lel corpus allows one to compute translation probabilities based on the frequency of co-occurrences between a source-language word and a target-language word in the parallel corpus. Most success-ful statistical machine-translation systems exploit parallel corpora (e.g., [20, 27]).

On the other hand, there are several drawbacks to the parallel-corpus approach. First, although parallel corpora are available for many of the European languages as well as for Arabic and Chi-nese, there are many languages for which there are still no parallel corpora large enough to estimate translation probabilities. Second, most of the parallel corpora belong to a rather specific domain, such as the Europarl corpus, 1 which contains the proceedings of the European parliament in 11 languages for the years 1996 X 2003. This introduces a bias toward the domain of the parallel corpus and makes the learned translation probabilities less reliable for other domains. A third disadvantage is that the translation probabilities induced from parallel corpora are typically based on single-word mappings, although recent template-based statistical methods fa-cilitate the acquisition of phrase translations [20].

In this paper, we propose an approach that does not require a parallel corpus to induce translation probabilities. We use a more sophisticated approach to exploiting context to compute transla-tion probabilities. Instead of using n-grams, we use co-occurrence statistics between all translation candidates for the sentence in ques-tion. In order to compute translation distributions for the foreign words in a sentence, our approach only requires two resources: a machine-readable dictionary (without any rankings or frequency statistics) and a monolingual corpus in the target language. The ap-proach is novel in that uses an iterative expectation-maximization based algorithm for computing translation probabilities from these resources for any given information need. It also allows us to use a minimal cross-lingual component (a bilingual dictionary, not a bilingual corpus) which addresses the issue of scarce parallel re-sources for certain languages. In addition, the approach is much easier to adapt to new domains as it does not require a parallel cor-pus for the domain at hand, but only a monolingual domain-specific corpus, which is much easier to obtain.

This paper is organized as follows: The next section describes some of the problems that arise in determining correct translations. Section 3 describes our novel approach for computing translation
The Europarl corpus is freely available from http://www.isi. edu/  X  koehn/europarl/ .
 Figure 1: Frequency distribution of number of translation in the dictionary. probabilities. Section 4 provides the details of the experimental setting and the evaluation itself. Section 5 provides an overview of approaches that are related to our approach. In Section 6, we draw some conclusions and give an outlook on future research.
Many words or phrases in one language can be translated into another language in a number of ways. For instance, the English word penalty can be translated as Elfmeter (as in soccer) or Strafe (as in punishment), and the choice of the translation depends on the context in which penalty occurs. Translation ambiguity is very common. Figure 1 shows the frequency distribution of dictionary entries with their corresponding number of translations in the D dictionary, an English-German machine readable dictionary.
One way to address the word-choice problem is to apply word-sense disambiguation on the source-language sentence and then use only those translation candidates that are associated with the ap-propriate word sense. Unfortunately, word-sense disambiguation is a non-trivial enterprise and for most languages the appropriate resources, e.g., ontologies like WordNet [10], do not exist. Also sense-annotated corpora that are used to train a word-sense disam-biguation system are rare in foreign languages, and the process of building them is very laborious.

Our alternative approach to modeling context for the problem of word selection is to use co-occurrences between terms. For in-stance, the simultaneous occurrence of the terms w 1 and w as a co-occurrence if they appear within a certain window, where a window can be a particular number of words, a sentence, a para-graph, or a document. Co-occurrences are more flexible than linear n-grams as they do not put any constraints on adjacency or word order. Because of the higher degree of flexibility, it makes sense to base lexical selection on co-occurrence rather than on n-grams. For instance, given three source language terms s 1 , s 2 , and s can translate into t 1 , 1 , . . . t 1 , 3 , s 2 can translate into t can translate into t 3 , 1 , one compares all possible triples and selects the pair of terms that co-occur most frequently as the most likely translation of s 1 and s 2 :
Using term co-occurrence for selecting the target-language trans-lations for two source-language terms is simple. Although the ex-pansion of this approach to three, four, or more source terms may seem trivial, computing co-occurrence statistics for a larger num-ber of terms induces a data-sparseness issue, similar to the sit-uation for higher n-grams in language modeling. To overcome the data-sparseness problem we could use very large corpora for counting co-occurrence frequencies or we could apply smoothing techniques. Although large corpora for English are available (e.g., the Gigaword corpus 3 ), it is doubtful that these provide frequency counts for a su ffi cient number of co-occurrences of four or more terms. An obvious alternative is to use Internet search engines to compute frequencies for larger sizes of co-occurring terms. Re-searchers have shown that the Internet can be used to address the problem of data-sparseness for bi-grams [14] but it is unclear to what extent this approach will resolve the issue of data-sparseness for higher n-gram models.
 The other approach for tackling data sparseness is smoothing. Several smoothing techniques have been developed and many of them are successfully applied in language modeling [4]. The prob-lem is that smoothing techniques are generally evaluated with re-spect to bi-or tri-gram models. It is unclear to what extent these techniques scale up successfully to models using a larger context such as four or five terms.
In the previous section we noted that using co-occurrence fre-quencies for all possible translations is not only computationally expensive, but also su ff ers from the problem of data sparseness. In this section we propose an algorithm that overcomes this prob-lem by using co-occurrences only between pairs of possible trans-lations.

Assume there are three source terms in a source language sen-tence, s 1 , s 2 , and s 3 , and each of these has a number of translations in the target language. For instance, s 1 can be translated as t t lated as t 3 , 1 . As mentioned above, one way to select the appropriate translations for s 1 and s 2 is to use the co-occurrence frequencies of all possible translations and then select the one with the high-est co-occurrence count. In the case of two terms this might be feasible, but if the source sentence contains three or more terms, one runs into the problem of data-sparseness. That is, it is likely that most co-occurrence counts are zero, thus rendering such an approach useless for selecting a possible translation.

Instead of looking at the co-occurrences between the possible translations of all source terms at the same time, we propose to examine pairs of terms in order to gather partial evidence for the likelihood of a translation in a given context.
Gigaword is distributed by the Linguistic data Consortium: http: //ldc.upenn.edu .

When looking at individual pairs only, disambiguation is done locally. Consider Figure 2, where each link between two translation candidates indicates that we consider the co-occurrence frequency of that pair of translation candidates. Here, the link strength be-tween two translations is computed in terms of some co-occurrence based association measure. Note that there are no links between translation candidates for the same source term.
Assume that t i , 1 occurs more frequently with t j , 1 than any other pair of candidates between a translation for s i and s j . In this local context, t i , 1 would be the preferred translation for s On the other hand, assume that t i , 1 and t j , 1 do not co-occur with t at all, but t i , 2 and t j , 2 do. Given the co-occurrence information from other local contexts, the question is, which should be preferred: (1) t
One solution is simply assign a weight to each translation candi-date by adding up all the co-occurrence frequencies with translation candidates of other source terms, and for each source term choose the translation candidate with the highest weight.

The disadvantage of this approach is that the probability that a target word is a translation for a certain source word is not taken into account when the link strength between two candidates is com-puted in the first place.

Our approach combines the link-strength computation with the prior probability of a translation given the other words in the query by computing them iteratively, in a fashion similar to the Expecta-tion Maximization (EM) algorithm [7].

Initially, all possible translations of a source term are considered equally likely, where we associate with each translation candidate a weight w T (  X | X  ) that it is indeed the appropriate translation. That is, w s . For all terms in the source sentence, the sets of translations are initialized this way. Next, each translation candidate t i , l to each translation candidate t j , m where i , j , i.e., di lations of the same source term are not linked to each other. This situation is depicted in Figure 2 for a source sentence with three terms, where s i has three translations, s j has two, and s
In addition to the term weights, the links between translation can-didates are also weighted. The link weight w L ( t i , l , t translations is computed by some measure of association strength based on the co-occurrence frequency of t i , l and t j , m provide the definitions of some association measures.

The first step towards recomputing the term weights is to ini-tialize the weights. Since we use a bilingual dictionary that does not contain any information about the translation probability of a certain source word or phrase, the weights are initialized making the weakest possible assumption, i.e., using a uniform distribution. Assuming that t is a translation candidate for s i , i.e. t  X  tr ( s initialization is defined as: Initialization Step:
After all translation candidates have been initialized appropri-ately, each term weight is recomputed based on two di ff erent in-puts: the weights of the terms that link to the term; and the respec-tive link weight. This formulated as follows: Iteration Step: where inlink ( t ) is the set of translation candidates that are linked to t . For instance, in Figure 2, inlink ( t i , 2 ) = { t j , 1
After each term weight has be re-computed, term weights are normalized so that all weights associated with translation candi-dates of the same source word sum up to 1: Normalization Step: Following this, steps (2) and (3) are repeated. The iteration stops if the changes in term weights become smaller than some predefined threshold  X  . More formally, let w n T be the vector of all term weights in the network for iteration n and let | V | 1 be the L 1 V : where V k is the k th element in the vector, and | V k | is the absolute value of V k . Then, the iteration stops if | w n T  X  w n
Note that the algorithm described above can also be considered a modification of the PageRank algorithm [21], allowing for nodes in the network to be clustered.

There are a number of ways to compute the association strength between two terms. We focus here on three alternatives: Point-wise mutual information , Dice coe ffi cient , and Log Likelihood Ra-tio . The point-wise mutual information between to terms t and t defined as follows [5]: where p ( t , t 0 ) is the probability that the terms t and t same document. Thus, as this value gets larger, the joint prob-ability of t and t 0 occurring together is increasingly larger than the combined probability of them occurring together individually. The probabilities in (5) are estimated by counting the number of (co-)occurrences and dividing this result by the number of text win-dows that were used in the whole corpus.

Measuring association strength in terms of mutual information has some shortcomings. For instance, given two pairs of terms ( t , t 0 1 ) and ( t 2 , t 0 2 ), where both pairs of terms always co-occur with each other, but the pair ( t 1 , t 0 1 ) is less frequent than the pair ( t the pair ( t 1 , t 0 1 ) will have a higher mutual information value than the formation is widely used for measuring association strength (see, europe europa 1.0000 1.0000 1.0000 trade branche 0.0833 0.2072 0.2040 trade handel 0.0833 0.1312 0.1376 trade gewerbe 0.0833 0.1059 0.1018 trade geschaeft 0.0833 0.1001 0.1016 trade abschluss 0.0833 0.0930 0.0998 trade handeln 0.0833 0.0774 0.0760 trade beruf 0.0833 0.0624 0.0572 trade trade 0.0833 0.0476 0.0478 trade handwerk 0.0833 0.0474 0.0466 trade eintauschen 0.0833 0.0430 0.0430 trade handel treiben 0.0833 0.0425 0.0424 trade schachern mit etw. 0.0833 0.0416 0.0416 union union 0.2000 0.4678 0.4554 union gewerkschaft 0.2000 0.1748 0.1893 union vereinigung 0.2000 0.1271 0.1264 union verbindung 0.2000 0.1220 0.1214 union verein 0.2000 0.1081 0.1072 trade union gewerkschaftlich 0.5000 0.5415 0.4664 trade union gewerkschaft 0.5000 0.4584 0.5335 Table 1: The first two iterations of the term re-weighting algo-rithm. e.g., [18]), and we therefore include it as one possibility for com-puting the link weight.

Alternatively, the link weight can be computed using the Dice coe ffi cient, which is defined as follows: where freq ( t , t 0 ) is the number of times t and t 0 co-occur. One ad-vantage of the Dice coe ffi cient is that its value ranges between 0 and 1 (where 1 is perfect co-occurrence), whereas mutual information has no upper bound.
 The last measure of association strength we consider here is the Log Likelihood ratio, which compares two hypotheses:
Hypothesis H 1 states that the probability of both w 2 and w curring together is the same as the probability that w 2 occurs with-out w 1 . In other words, H 1 formalizes independence between w and w 1 . H 2 states that the two probabilities are not the same and hence w 2 and w 1 do not occur independent of each other.
The log likelihood is then defined as [8]: where p , p 1 , and p 2 are defined as in H 1 and H 2 above, c frequency or word w 1 , c 2 is the frequency of word w 2 , c frequency of both words occurring together, N is the number of tokens in the corpus, and L ( k , n , x ) = x k (1  X  x ) n  X  k
Computing the translation probabilities iteratively can help re-solve some of the translation ambiguities properly. Consider Ta-ble 1. The source topic is Trade Unions in Europe . During the Documents source Years Size No. documents Frankfurter Rundschau 1994 320 MB 139,715 Der Spiegel 1994 X 1995 63 MB 13,979 SDA German 1994 144 MB 71,677 SDA German 1995 141 MB 69,438 Total: 668 MB 294,809 Table 2: Documents used for the English-German bilingual task at CLEF 2003. first iteration, gewerkschaftlich is the preferred translation for Trade Union , but during the second iteration, gewerkschaft , which is the proper translation, has a higher translation probability.
This section describes our evaluation of the term re-weighting algorithm described above. We present the set-up of our experi-ment (test data, morphological processing, retrieval model, statisti-cal significance tests) and our experimental results.
Below we describe our experimental set-up, specifically: the test data used; the morphological processing required for the original source-language topics and the target-language queries; the model underlying our retrieval system (a standard vector space model); and our choice of statistical significance tests. The document collection used in our experiments consists of the CLEF 2003 English to German bilingual data. Specific documents are listed in Table 2.

The document collection contains 60 topics, four of which were removed by the CLEF organizers, as no relevant documents were found in the collection, leaving us with 56 topics. Each topic has a title, a description, and a narrative field. For our experiments, we used only the title field to formulate the queries. Although it is common practice in CLEF and TREC to use both the title and description to formulate the queries, title-only queries are more re-alistic, as most queries posed by actual users, web queries in partic-ular, tend to be short (i.e., two to four terms). Table 3 shows some of the topics that were used, in combination with the correspond-ing topic in the target language (as formulated by one of the CLEF assessors).
Morphological normalization is required for the original source-language topics as well as for the translated (target-language) queries and documents.

First, source-language words (English, in our case) from the orig-inal topic are normalized to match entries in the bilingual dictio-nary. The words in the topic may bear morphological inflection such as tense information for verbs and plural information for nouns. Since the dictionary only contains base forms, the words in the top-ics must be mapped to their respective base forms as well. Here, we used TreeTagger [25], which is a part-of-speech tagger that also provides the lemma (or base form) for each word. This form of morphological normalization is less aggressive than a rule-based stemmer, such as Porter X  X  stemmer [23].

Since the target language is German X  X  morphologically more complex language than English X  X dditional normalization steps are required. The translation candidates need not be mapped to their Table 3: Example topics (title field only) used for the English-German bilingual task at CLEF 2003.
 Figure 3: Intermediate results of the query formulation pro-cess. base forms because they are taken from the bilingual dictionary which contains only the base forms. On the other hand, com-pounds are very frequent in German and it has been shown that de-compounding can improve retrieval e ff ectiveness substantially [19].

Instead of de-compounding, we use character n-grams, an ap-proach that yields almost the same retrieval performance as de-compounding. Specifically, it has been shown that using 5-grams leads to the best performance among n-gram approaches, almost equalling the performance of a de-compounding approach, see [12]. Thus, we split all tokens in documents and translated queries into 5-grams, without crossing word boundaries, for all mono-lingual and cross-lingual runs.

For the runs involving term weights, we must decide how to as-sign weights to 5-gram substrings. In our experiments, we simply gave the substrings the same weight as the original term. In addi-tion to n -gram splitting, all words in the target language (including the translations) were mapped to lower case. Figure 3 shows the intermediate results of the query formulation process for one of the topics in the CLEF 2003 test set.
The model underlying our retrieval system is the standard vector space model. All our mono-and bi-lingual runs were based on the Lnu.ltc weighting scheme [2]. That is, to compute the similarity between between a query ( q ) and a document ( d ): sim ( q , d ) = (8)
In our experiments, we used a slope ( sl ) of 0.1. The pivot ( pv ) was set to the average number of unique words per document. The parameter uw d refers to the number of unique words in document d .

Neither the mono-lingual run nor the dictionary baseline include term weights X  X oth used the document similarity measure in (8).
In order to integrate term weights, the formula in (8) was modi-fied. Weighted document similarity is defined in (9): sim w ( q , d ) = (9)
X where the weight of term i is computed as described in Section 3. There are many techniques for drawing statistical inferences. The paired t-test is probably the best-known technique (see, e.g., [16]). Many of the inference techniques make certain assumptions about the data to which they are applied. The most common as-sumption, which also underlies the paired t-test, is that the data is taken from a population which is normally distributed. In the set-ting of retrieval this means that for a number of queries, the di ences between two methods are normally distributed. Whether this assumption holds for text retrieval has been the subject of debate in retrieval evaluation [26].

To determine whether the observed di ff erences between two re-trieval approaches are statistically significant and not just caused by chance, we used the bootstrap method, a powerful non-parametric inference test [9]. The method was previously applied to retrieval evaluation [24, 28]. The basic idea of the bootstrap is to simu-late the underlying distribution by randomly drawing (with replace-ment) a large number of samples of size N from the original sample of N observations. These new samples are called bootstrap sam-ples ; we set the number of bootstrap samples to 2,000 as using the standard size of 1,000 has been shown to be a less reliable approach to inducing a normal distribution [6].

The mean and the standard error of the bootstrap samples allow computation of a confidence interval for di ff erent levels of confi-dence (typically 0.95 and higher). We compare two retrieval meth-ods a and b by one-tailed significance testing. If the left limit of the confidence interval is greater than zero, we reject the null hypoth-esis, stating that method b is not better than a , and conclude that the improvement of b over a is statistically significant, for a given confidence level. Analogously, if the right limit of the confidence interval is less than zero, we conclude that method b performs sig-nificantly worse than a .

In the results reported below, we indicate improvements at a con-fidence level of 90% with  X  4  X  and at a confidence level of 95% with  X   X . Analogously, decreases in performance at a confidence level of 90% are marked with  X  O  X  and at a confidence level of 95% with  X   X . No mark-up is used if neither an increase nor a decrease in performance is significant at either of the 90% or 95% confidence levels.
 Run MAP Rel. impr. Perc. Mono-ling.
 Mono-lingual 0 . 3171  X   X  Unweighted baseline 0 . 1708  X  53 . 9% Mutual Inf. weighted 0 . 1972 + 15 . 5% 62 . 2% Dice weighted 0 . 1994 + 16 . 7% 62 . 9% Log-likel. weighted 0 . 2013 + 17 . 6% 4 63 . 5% Table 4: Experimental results for the di ff erent association mea-sures.
 Figure 4: Absolute di ff erences in average precision between the baseline and Log Likelihood weighted run for the individual queries.
For evaluating the e ff ectiveness of our term re-weighting algo-rithm, we compared five runs against each other. First, we deter-mined the performance ceiling by using manually translated top-ics provided by the CLEF organizers. As a baseline, we used the English-German bilingual dictionary without any weights assigned to the translation, i.e. all translation candidates were considered equally likely. All cross-language runs, i.e. all runs except the mono-lingual run, used the D  X  X  X  X  English-German dictionary.
As described in Section 3, we used three di ff erent association measures for our term re-weighting algorithm: Mutual Information, Dice Coe ffi cient, and Log Likelihood Ratio. Table 4 lists the mean average precision (MAP), relative improvements over the baseline, and percentage of the German mono-lingual e ff ectiveness for all five runs. The frequencies of term occurrences and co-occurrences were computed on the German corpus of the CLEF 2003 document collection.

The results in Table 4 show that retrieval using term re-weighting outperforms the baseline. On the other hand, the improvement was weakly statistically significant for the run using log likelihood ratio as the association measure. Figure 4 shows the absolute di in average precision between Log Likelihood re-weighting and the baseline for individual topics.

Despite that the mean average precision of Log Likelihood re-weighting is substantially higher than the baseline, individual aver-age precision decreases for a number of queries. One explanation for some of the decreases is the treatment of unknown words dur-ing translation. In our experiment, 6% of all English query terms were not in the dictionary. Unknown words are treated as if they were proper names, and therefore the original word from the source language is included in the target language query. Although this fall-back strategy works well in many cases, there are also cases where it harms retrieval performance. For instance, in the English source topic in (10) the word Women is falsely considered a proper noun by the part-of-speech tagger and, therefore, not mapped to its base form woman . The plural form women is not in the bilingual dictionary, although the singular form woman is. Because this word is assumed to be a proper noun during query translation, the original source word women is used as translation.

Although faulty translations of this type a ff ect both the baseline system and the run using term weights, the latter is a ff severely. The reason is that the conditional translation weight is 1 (i.e., the source-word women only has one translation candidate) which induces a weight of 1 for women , whereas all the other terms in the query receive lower weights (the other source words had more than one translation candidate). 4 As a consequence the term woman dominates the document similarity, and most top-ranked documents contain women as the only matching term.

To deal with out-of-vocabulary terms properly, additional ma-chinery (outside of the scope of this paper) is necessary since they strongly bias retrieval results.
Cross-language retrieval has a long history in the broader field of information retrieval. In addition, techniques for word-sense dis-ambiguation and phrase-based machine translation have recently evolved to the point where they are ripe for investigation in the con-text of cross-language retrieval, although others have not yet used these to the extent that we have.

For example, Pirkola X  X  [22] approach does not consider disam-biguation during query formulation at all. Pirkola uses structured queries to cluster together all translations of a word or phrase in the source topic. Disambiguation takes place implicitly during re-trieval. The underlying assumption is that top-ranked documents contain at least one translation for the majority of the clusters and, since all the translations occur in the same document, they are likely to be appropriate translations. Although this assumption makes sense and is appealing in its simplicity, Pirkola X  X  method is sensi-tive to skewed translations for retrieval systems using inverted doc-ument frequencies for term weighting. Given some source word (or phrase) w i , if one translation w i , j has a very high inverted document frequency score, it can bias document similarity toward this transla-tion, and therefore cause the top-ranked documents to contain only few translations of other source words from the topic. This bias re-duces the e ff ect that co-occurrence with translations of other source words has on selecting an appropriate translation.

The work by Jang et al. [13] is closely related to ours in that they also use a word-association measure, mutual information in their case, to re-compute translation probabilities for cross-language re-trieval. Their approach di ff ers from ours in two ways: First, their system only considers mutual information between consecutive terms in the query. For example, for a query of the form w 1 , w only consider mutual information scores of the form MI ( w
Moreover, Beijing also had two translations, viz. Peking and Bei-jing . and MI ( w 2 , k , w 3 , l ), where w n , i is a translation candidate of w sensitivity to word order seems somewhat problematic in the con-text of keyword-based query formulation which should allow for free-word order. Second, they do not compute the translation prob-abilities in an iterative fashion. Thus, their approach does not bene-fit from the power of multiple iterations, as in our approach, where disambiguated information from a previous iteration induces more accurate decisions in the current iteration.

Adriani X  X  approach [1] is similar to the approach by Jang et al. in that her approach also only uses the maximum similarity scores between translation candidates for di ff erent query terms. Similar to the approach by Jang et al. her approach does not benefit from using multiple iterations.

Gao et al. [11] use a decaying mutual-information score in com-bination with syntactic dependency relations. The decay factor is based on the average distance between two words in the target lan-guage. In our model, we did not consider distances between words, but simply counted the number of times two words occur in the same document. Integrating a distance factor might be beneficial to our approach. The dependency model used in [11] requires the topics to bear some form of syntactic structure, e.g., verb-argument or noun-modifier relations. Unfortunately, simple keyword-based topics (such as the title field in the CLEF topics) are not in this form, but are typically just simple lists of noun phrases. For that reason we did not try to carry out any deeper linguistic analysis between the words in the topic.

Maeda et al. [17] compare a number of co-occurrence statistics with respect to their usefulness for improving retrieval e ness. As in our own approach, they consider all pairs of pos-sible translations of words in the query X  X ot just co-occurrences of consecutive words. On the other hand, Maeda et al. use co-occurrence information to select translations of words from the topic for query formulation, instead of re-weighting them. If the association strength between two translation candidates, measured by co-occurrence statistics (e.g., mutual information) is greater than some pre-defined threshold, both translation candidates are included in the query; otherwise they are excluded. This approach has two potential shortcomings: First, it requires a proper estimation of the threshold on some development data set of topics. Second, it does not result in a probability distribution over the possible translations of a word in the source topic. By contrast, our approach allows for a more fine-grained estimation of the usefulness of a particular translation in the context of the given topic.

The work by Kikui [15] is also closely related to our work as it relies, in addition to a dictionary, only on monolingual resources in the target language in order to estimate translation weights. This approach computes the coherence between possible combinations of translation candidates of the source terms. A shortcoming of this approach is that the set of all possible combinations needs to be considered, which can be rather larger, depending on the number of query terms and translation candidates.
This paper has introduced a new algorithm for computing topic-dependent translation probabilities for cross-language information retrieval. These translation probabilities were integrated as term weights into a vector-space retrieval system.

Experimental results for English to German cross-language re-trieval showed that our approach improves retrieval e ff ectiveness significantly compared to a baseline using bilingual dictionary lookup. For estimating translation probabilities we experimented with dif-ferent term association measures: Mutual Information, Dice Coef-ficient, and Log Likelihood Ratio. The experimental results show that Log Likelihood Ratio has the strongest positive impact on re-trieval e ff ectiveness, although the di ff erences in performance be-tween the three measures are relatively small. An important advan-tage of our approach is that it only requires a bilingual dictionary and a monolingual corpus in the target language to compute the translation probability distributions for a given topic.
An issue that remains open at this point is the computation of query terms that are not covered by the bilingual dictionary. In our approach, we have set the query term weight to be the condi-tional translation probability, which causes translations of unknown words to bias the document similarity computation. We plan to in-vestigate ways of addressing this problem.
 This research was supported in part by Army Research Lab Cooper-ative Agreement DAAD190320020, NSF Medium ITR Grant IIS-0326553, and DARPA TIDES Cooperative Agreement N66001-00-2-8910. We also would like to thank Doug Oard for helpful com-ments. [1] M. Adriani. Using statistical term similarity for sense [2] C. Buckley, A. Singhal, and M. Mitra. New retrieval [3] A. Chen and F. C. Gey. Combining query translation and [4] S. F. Chen and J. Goodman. An empirical study of [5] K. Church and P. Hanks. Word association norms, mutual [6] A. Davison and D. Hinkley. Bootstrap Methods and Their [7] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood [8] T. Dunning. Accurate methods for the statistics of surprise [9] B. Efron. Bootstrap methods: Another look at the jackknife. [10] C. Fellbaum, editor. WordNet: An Electronical Lexical [11] J. Gao, J.-Y. Nie, H. He, W. Chen, and M. Zhou. Resolving [12] V. Hollink, J. Kamps, C. Monz, and M. de Rijke.
 [13] M.-G. Jang, S. H. Myaeng, and S. Y. Park. Using mutual [14] F. Keller and M. Lapata. Using the web to obtain frequencies [15] G. Kikui. Term-list translation using mono-lingual word [16] L. Kitchens. Exploring Statistics: A Modern Introduction to [17] A. Maeda, F. Sadat, M. Yoshikawa, and S. Uemura. Query [18] C. Manning and H. Sch  X  utze. Foundations of Statistical [19] C. Monz and M. de Rijke. Shallow morphological analysis in [20] F.-J. Och and H. Ney. The alignment template approach to [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] A. Pirkola. The e ff ects of query structure and dictionary [23] M. Porter. An algorithm for su ffi x stripping. Program , [24] J. Savoy. Statistical inference in retrieval e ff ectiveness [25] H. Schmid. Probabilistic part-of-speech tagging using [26] C. van Rijsbergen. Information Retrieval . Butterworths, 2nd [27] A. Venugopal, S. Vogel, and A. Waibel. E ff ective phrase [28] J. Wilbur. Non-parametric significance tests of retrieval
