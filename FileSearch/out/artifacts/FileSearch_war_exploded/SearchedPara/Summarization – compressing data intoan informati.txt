 REGULAR PAPER Varun Chandola  X  Vipin Kumar Abstract In this paper, we formulate the problem of summarization of a data set of transactions with categorical attributes as an optimization problem involv-ing two objective functions  X  compaction gain and information loss. We propose metrics to characterize the output of any summarization algorithm. We investi-gate two approaches to address this problem. The first approach is an adaptation of clustering and the second approach makes use of frequent itemsets from the association analysis domain. We illustrate one application of summarization in the field of network data where we show how our technique can be effectively used to summarize network traffic into a compact but meaningful representation. Specifically, we evaluate our proposed algorithms on the 1998 DARPA Off-Line Intrusion Detection Evaluation data and network data generated by SKAION Corp for the ARDA information assurance program.
 Keywords Summarization  X  Frequent itemsets  X  Categorical attributes 1 Introduction Summarization is a key data mining concept which involves techniques for find-ing a compact description of a data set. Simple summarization methods such as tabulating the mean and standard deviations are often applied for data analysis, data visualization and automated report generation. Clustering [ 13 , 23 ] is another data mining technique that is often used to summarize large datasets. For exam-ple, centroids of document clusters derived for a collection of text documents [ 21 ] can provide a good indication of the topics being covered in the collection. The clustering-based approach is effective in domains where the features are contin-uous or asymmetric binary [10, 23] and hence cluster centroids are a meaningful description of the clusters. However, if the data has categorical attributes, then the standard methods for computing a cluster centroid are not applicable and hence clustering cannot directly be applied for summarization. 1 One such application is in the analysis of netflow data to detect cyber attacks.
 has different features such as the IPs and ports involved, packets and bytes trans-ferred (see Table 1 ). An important characteristic of netflow data is that it has a mix of categorical and continuous features. The volume of netflow data which a network analyst has to monitor is huge. For example, on a typical day at the Uni-versity of Minnesota, more than one million flows are collected in every 10 min window. Manual monitoring of this data is impossible and motivates the need for data mining techniques. Anomaly detection systems [4, 9, 17, 22] can be used to score these flows, and the analyst typically looks at only the most anomalous flows to identify attacks or other undesirable behavior.
 or thousands of highly ranked flows that require the analyst X  X  attention. But due to the limited time available, analysts look at only the first few pages of results that cover the top few dozen most anomalous flows. If many of these most anomalous flows can be summarized into a small representation, then the analyst can analyze a much larger set of anomalies than is otherwise possible. For example, Table 2 shows 17 flows which were ranked as most suspicious by the MINDS Anomaly Detection Module [ 9 ] for the network traffic analyzed on January 26, 2003 (48 h after the Slammer Worm hit the Internet) for a 10 min window that contained 1.8 million flows. These flows are involved in three anomalous activities X  slammer worm related traffic on port 1434, flows associated with a half-life game server on port 27,016 and ping scans of the inside network by an external host on port 2048. If the data set shown in Table 2 can be automatically summarized into the form showninTable 3 (the last column has been removed since all the transactions con-tained the same value for it in Table 2 ), then the analyst can look at only three lines to get a sense of what is happening in 17 flows. Table 3 shows the output summary for this data set generated by an application of our proposed scheme. We see that every flow is represented in the summary. The first summary S 1 represents flows {
T a single external host and targeting several internal hosts. The second summary S 2 represents flows servers made by an internal host. The third summary, S 3 represents flows { T 11 , T marization has the potential to reduce the size of the data by several orders of magnitude.
 categorical features. We view summarization as a transformation from a given data set to a smaller set of individual summaries with an objective of retaining the maximum information content. A fundamental requirement is that every data item should be represented in the summary . 1.1 Contributions Our contributions in this paper are as follows:  X  We formulate the problem of summarization of transactions that contain cate- X  We investigate two approaches to address this problem. The first approach is  X  We present an optimal but computationally infeasible algorithm to generate  X  We illustrate one application of summarization in the field of network data 2 Characterizing a summary Summarization can be viewed as compressing a given set of transactions into a smaller set of patterns while retaining the maximum possible information. A trivial summary for a set of transactions would be itself. The information loss here is zero but there is no compaction. Another trivial summary would be the empty set , which represents all the transactions. In this case, the gain in compaction is maximum but the summary has no information content. A good summary is one which is small but still retains enough information about the data as a whole and also for each transaction.
 associated weight vector W such that each W i  X  W represents the weight of the feature F i  X  F . A set of transactions T , such that | T |= m , is defined using these features, and each T i  X  T has a specific value for each of the n features. Formally, a summary of a set of transactions can be defined as follows: Definition 1 (Summary) A summary S of a set of transactions T , is a set of indi-vidual summaries { S 1 , S 2 ,..., S l } such that (i) each S j represents a subset of T and (ii) every transaction T i  X  T is represented by at least one S j  X  S. summary S , these transactions are replaced by the individual summary that covers them. As we mentioned before, computing the centroid for data with categorical attributes is not possible. For such data, a feature-wise intersection of all trans-actions is a more appropriate description of an individual summary. Hence, from now on, an individual summary will be treated as a feature-wise intersection of all Ta b l e 4 . The data set shown is a set of eight transactions that are described by six categorical features and two continuous features (see Table 1 ). Let all the features have equal weight of 1 8 . One summary for this data set is shown in Table 5 as a set of three individual summaries. The individual summary S 1 covers transactions {
T 1 , T 2 , T 3 , T 4 , T 8 transaction, T 8 .
 following metrics: Definition 2 (Compaction gain for a summary) Compaction Gain = m l . ( Re-call that m =| T | and l =| S | . ) S Definition 3 ( Information loss for a transaction represented by an individual summary ) For a given transaction T i  X  T and an individual summary S j  X  S that covers T i ,loss ij = n q = 1 W q  X  b q ,where,b q = 1 if T iq  X  S j and 0 otherwise. be the weighted sum of all features that are absent in the individual summary. Definition 4 (Best individual summary for a transaction) For a given transac-tion T i  X  T , a best individual summary S j  X  S is the one for which loss ij is minimum.
 lost for every transaction with respect to its best individual summary. T  X  T 4 are best covered by individual summary S 1 and each has an information loss of 4 8 . Transactions T 5  X  T 7 are best covered by individual summary S 2 and each has an information loss of 5 8 . T 8 is represented by S 1 and S 3 .For T 8 and S 1 , information loss = 4  X  1 8 = 1 2 , since there are four features absent in S 1 .For T 8 and S 3 , information loss = 0 since there are no features absent in S 3 . Hence, the best individual summary for T 8 will be S 3 .Thus,wegetthat Information Loss for S follow an optimality tradeoff curve as shown in Fig. 1 such that increasing the compaction results in increase of information loss. We denote this curve as ICC ( Information-loss Compression-gain Characteristic )curve.
 tion algorithm. The beginning and the end of the curve are fixed by the two trivial solutions discussed earlier. For any summarization algorithm, it is de-sirable that the area under its ICC curve be minimal. It can be observed that getting an optimal curve as shown in Fig. 1 involves searching for a solu-tion in exponential space and hence not feasible. But a good algorithm should be close enough to the optimal curve like 1 and not like 2 in the figure shown.
 optimization problem since it involves two orthogonal objective functions. So a typical objective of a summarization algorithm would be  X  for a given level of compaction find a summary with the lowest possible information loss. 3 Summarization using clustering In this section, we present a direct application of clustering to obtain a summary for a given set of transactions with categorical attributes. This simple algorithm involves clustering of the data using any standard clustering algorithm and then replacing each cluster with a representation as described earlier using feature-wise intersection of all transactions in that cluster. The weights W are used to calculate the distance between two data transactions in the clustering algorithm. Thus, if  X  C is a set of clusters obtained from a set of transactions T by clustering, then each cluster produces an individual summary which is essentially the set of feature-value pairs which are present in all transactions in that cluster. The number of clusters here determine the compaction gain for the summary.
 while Steps 3 and 4 generate the summary description for each of the individual clusters. For illustration consider again the sample data set of eight transactions in T , T 4 , T 8 } and C 2 ={ T 5 , T 6 , T 7 } .Table 6 shows a summary obtained using the clustering-based algorithm.
 of behavior in the data because they are captured well by the clusters. However, this approach performs poorly when the data has outliers and less frequent pat-terns. This happens because the outlying transactions are forced to belong to some cluster. If a cluster has even a single transaction which is different from other clus-ter members, it degrades the description of the cluster in the summary. For exam-ple, let us assume that another transaction T 9 asshowninTable 7 is added to the data set shown in Table 4 and clustering assigns it to cluster C 1 .
 ence of this outlying transaction makes the summary description very lossy in terms of information content. Thus this approach represents outliers very poorly, which is not desirable in applications such as network intrusion detection and fraud detection where such outliers can be of special interest.
 4 An optimal summarization algorithm In this section, we propose an exhaustive search algorithm (shown in Fig. 3 )which is guaranteed to generate an optimal summary of given size l for a given set of transactions, T . The first step of this algorithm involves generating the powerset of T ( = all possible subsets of T ), denoted by C . The size of C will be 2 | T | .The second step involves searching all possible subsets of C (2 2 | T | subsets) to select a subset, S which has following properties Property 1 (1) | S |= l , the size of this subset is equal to desired compaction level (2) The subset S covers all transactions in T (a set cover of T ) (3) The total information loss for S with respect to T is minimum over all other We denote the optimal summary generated by the algorithm in Fig. 3 by S . The optimal algorithm follows the optimal ICC curve as shown in Fig. 1 . 5 A two-step approach to summarization using frequent itemsets The optimal algorithm presented in Sect. 4 requires searching in a 2 2 | T | space (for four transactions it would require searching a set of 65,536 subsets), which makes it computationally infeasible even for very small data sets. In this section, we pro-pose a methodology which simplifies each of the two steps of the optimal algo-rithm to make them computationally more efficient. We first present the following lemma.
 Lemma 1 Any subset of T belonging to the optimal summary, S must belong to C ,whereC c denotes a set containing all closed frequent itemsets 2 generated with a support threshold of 2 and T itself.
 Proof This can be easily proved by contradiction. Suppose the optimal summary contains a subset S i  X  C  X  C c . Thus there will be a subset S j  X  C c which  X  X loses X  S , which means S j  X  S i and support ( S i ) = support ( S j ) . We can replace S i with S j in S to obtain another summary, S  X  such that all the transactions represented by S in S are represented by S j in S  X . Since S j  X  S i , the information loss for these transactions will be lower in S  X . Thus S  X  will have lower information loss than S for the same compaction which is not possible since S is an optimal summary. The result from Lemma 1 ensures that we can still obtain the optimal summary from the reduced candidate set. But to obtain an optimal solution we still need to search from the powerset of C c , which is still computationally infeasible. Higher values of the support threshold can be used to further prune the number of possible candidates, but this can impact the quality of the summaries obtained.
 the exponential search by greedily searching for a good solution. This does not guarantee the optimal summary but tries to follow the optimal ICC curve (refer to Fig. 1 ). The output (a subset of C c ) of Step 2 of our proposed algorithms satisfiy Property 1 .1 and Property 1 .2 mentioned in Sect. 4 but is not guaranteed to satisfy Property 1 .3.
 paction level while the information loss associated with this subset is approxi-mately minimal can be approached in two ways.  X  The first approach works in a top-down fashion where every transaction be- X  The second approach works in a bottom-up fashion by starting with T as the In this paper, we will discuss only the bottom-up approach for summarization. An algorithm based on the top-down approach is presented in an extended technical report [ 8 ]. Both of these approaches build a summary in an iterative and incremen-tal fashion, starting from the original set of transactions T as the summary. Thus from the ICC curve perspective, they start at the left hand corner ( compaction = 1, loss = 0 ). At each iteration the compaction gain increases along with the infor-mation loss. Each iteration makes the current summary smaller by bringing in one or more candidates into the current summary. 6 A bottom-up approach to summarization  X  the BUS algorithm The main idea behind the BUS algorithm is to incrementally select best candidates from the candidate set such that at each step, for a certain gain in compaction, min-imum information loss is incurred. The definition of a  X  X est X  candidate is based on a heuristic decision and can be defined in several different ways as we will describe later.
 closed frequent itemsets of T . As mentioned earlier, choosing a support threshold of two transactions while generating the frequent itemsets as well as the trans-actions themselves ensures that we capture patterns of every possible size. The rest of the algorithm works in an iterative mode until a summary of desired com-paction level l is obtained. In each iteration a candidate from C c is chosen using the routine select best . We have investigated several heuristic versions of select best which will be described later. The general underlying principle for designing a select best routine is to ensure that the selected candidate incurs very low infor-mation loss while reducing the size of summary. After choosing a best candidate, C best , all individual summaries in current summary which are completely covered 3 by C best are removed and C best is added to the summary (let the new summary be denoted by S c ).
  X  1, where size ( C best ) denotes the number of individual summaries in S c com-pletely covered by C best . This is denoted by gain ( C i , S c ) and represents the com-paction gain achieved by choosing candidate C i for a given summary S c . replacing the summaries covered by C best , we need to consider the transactions which will consider C best as their best individual summary (refer to Definition 4 ) in the new summary. For all such transactions, the difference in the loss when they were represented in S c by their best individual summaries and the loss when they are represented by C best in S c is the extra loss incurred in choosing C best .Thisis denoted by loss ( C i , S c ) .
 designed. Each of these approaches make a greedy choice to choose a candidate from the current candidate set C c which would lead to a locally optimal solution but does not ensure that the sequence of these choices will lead to a globally optimal solution . 6.1 Method 1 defined above, to score the candidates and choose one to be added to the cur-ignored (since adding them to the current summary would not result in any com-paction gain in the new summary). The remaining candidates are ordered using the candidate with highest gain ( C i , S c ) is returned as the best candidate . ation by bringing in the candidates with lowest information loss.
 ner might eventually result in a sub-optimal solution. A simple counter-example showninFig. 6 proves this. Let us consider a simple data set which has four trans-a unit weight). The aim is to obtain a summary of size 2 for this data set using the BUS algorithm. As shown in the figure, the candidate set C c contains nine possi-ble candidates. The BUS algorithm considers the transaction set, T as the initial summary, S c . The first iteration uses Method 1 and (see Fig. 7 ) chooses candi-date C 5 as the best candidate, generating a new S c . The candidate set is rescored as shown in the figure. The next iteration chooses (see Fig. 7 ) C 9 as the best candidate and reduces the size of S c to 2. The right side table in Fig. 7 shows an-other size 2 summary for the same data set which has a smaller information loss. This shows that this method to score candidates might not result in an optimal solution. 6.2 Method 2 This method (as shown in Fig. 9 ) also uses the quantities gain ( C i , S c ) and loss ( C i , S c ) as in Method 1, but in a different way. All candidates with gain ( C i , S c ) equal to or less than 1 are ignored (since adding them to the cur-rent summary would not result in any compaction gain in the new summary). The remaining candidates are ordered using gain ( C i , S c ) . From among the candidates is returned as the best candidate .
 metry between information loss and compaction gain . The general idea behind Method 2 is that at any iteration, the candidates which cover least number of in-dividual summaries in the current summary S c , will also incur the lowest possible loss. Similar to Method 1, this method for choosing the best candidate does not guarantee a globally optimal summary. Consider the transaction data set contain-ing six transactions as shown in Fig. 10 . Figures 11  X  13 show the working of the BUS algorithm using Method 2 to obtain a summary of size 3 for this data set. Figure 13 ( right ) shows an alternative summary S c , which is of same size as S c but has a lower information loss. 6.3 Method 3 The third method to determine the best candidate makes use of a parameter k s , which combines the two quantities gain ( C i , S c ) and loss ( C i , S c ) to obtain a sin-gle value, denoted by score ( C i , S c ) . The method is shown in Fig. 14 .Thefirst step calculates the score for each candidate using k s . The candidate with high-est score is chosen as the best candidate. Initially k s is chosen to be 0. This fa-vors the candidates with very small information loss. If all candidates with the highest score have size ( C i , S c )  X  1, then k s is incremented by a small value,  X  . This allows larger candidates to have higher score by offsetting the larger infor-mation loss associated with them, and thus be considered for selection into the summary.
 sult in selection of larger candidates initially. Using this method again does not guarantee an optimal solution and depends on the initial value of k s and  X  . 6.4 Method 4 Each of the above three methods require the rescoring of gain ( C i , S c ) and loss ( C i , S c ) for each candidate after each iteration. The Method 4, shown in Fig. 15 does not require these computations but makes use of the quantity feature loss ( C i ) which refers to the weighted sum of the features missing in a candidate. Note that this quantity does not change over the iterations and hence is computed only once. The method to determine the best candidate uses parameter , which defines a upper threshold on feature loss ( C i ) . Only those candidates which have feature loss ( C i ) less than or equal to this threshold are considered for selection. Out of these the candidate with largest value of gain ( C i , S c ) ( &gt; 1) is selected as the best candidate. Initially is chosen as 0. Thus only candidates with 0 features missing are considered. If all candidates which fall under thresh-old have gain ( C i , S c )  X  1, then is incremented by a small value,  X  .Thisallows larger candidates to be considered for selection into the summary. 6.5 Discussion All the four methods discussed above select a candidate which they consider is the best with respect to a heuristic. We presented examples for first two methods where this choice might not always lead to a globally optimal solution .Thelast two methods make use of a user defined parameter and a wrong choice of this parameter can lead to suboptimal solutions. We have investigated each of these approaches and evaluated them on different network data sets (described in next section) and observed that all of them perform comparably in terms of the ICC curve characteristics with respect to each other. 7 Experimental evaluation and results In this section, we present the performance of our proposed algorithms on network data. We compare the performance of BUS with the clustering-based approach to show that it performs better in terms of achieving lower information loss for a given degree of compaction. As we had mentioned earlier, the clustering-based algorithm captures the clusters in the data and summarizes the transactions be-longing to those clusters well. But the presence of infrequent patterns can ruin the cluster descriptions and result in high information loss. The experimental results presented in this section highlight this fact by choosing different data sets which have different characteristics in terms of the natural clustering in the data. We also illustrate the summaries obtained for different algorithms to make a qualitative comparison between them. The algorithms were implemented in GNU-C ++ and were run on the Linux platform on a 4-processor intel -i686 machine. 7.1 Input data We ran our experiments on four different artificial data sets as listed in Table 8 . The first two data sets, AD 1and AD 2 were artificially generated such that they contained five clusters such that each cluster contained the same transaction repli-cated 200 times. AD 1 contained only these pure clusters. AD 2 contained outliers injected with respect to each of the cluster. The SKAION and DARPA data sets were generated by DARPA [ 15 ] and SKAION corporation [ 1 ], respectively, for the evaluation of intrusion detection systems. The DARPA data set is publicly available and has been used extensively in the data mining community as it was used in KDD Cup 1999. The SKAION data was developed as a part of the ARDA funded program on information assurance and is available only to the investiga-tors involved in the program. Both these data sets have a mixture of normal and attack traffic. The DARPA data set was a subset of the week 4, Friday, training data containing only attack related traffic corresponding to the following attacks X  warezclient, rootkit, ffb, ipsweep, loadmodule and multihop .
 bution. We measure the distribution of the data using the lof (local outlier fac-tor) score (see [ 6 ]). The distribution for lof scores for AD 1 data set is shown in Fig. 16 a. Since all transactions belong to one of the five clusters, all transactions have a lof score of 1. Similar plot for data set AD 2inFig. 17 a shows that some of the transactions have a higher lof score since they are outliers with respect to the clusters. Figure 18 a gives the distribution of the lof (local outlier factor) score (see [ 6 ]) for the transactions in the SKAION data set. The lof score reflects the outlierness of a transaction with respect to its nearest neighbors. The transactions which belong to tight clusters tend to have low lof scores while outliers have high lof scores. For the SKAION data set we observe that there are a lot of transactions which have high outlier scores. The lof distribution for the DARPA data set in Fig. 18 e shows that most of the transactions belong to tight clusters, and only a few transactions are outliers. 7.2 Comparison of ICC curves for the clustering-based algorithm and BUS We ran the clustering-based algorithm by first generating clusters of different sizes using the CLUTO hierarchical clustering package [ 14 ]. For finding the similar-ity between transactions, the features were weighted as per the scheme used for evaluating the information loss incurred by a summary. We then summarized the clusters as explained in Sect. 3 . For BUS, we present the results using frequent itemsets generated by the a priori algorithm with a support threshold of 2 as the candidates. The BUS algorithm was executed using Method 1 (see Sect. 6.1 ). 4 The different features in the data and the weights used are given in Table 9 .These weights reflect the typical relative importance given to the different features by network analysts. The continuous attributes in the data were discretized using equal depth binning technique with a fixed number of intervals ( = 75) and then used as categorical attributes. Figures 16 b and 17b show the ICC curves for the clustering-based algorithm and BUS on data sets AD 1and AD 2, respectively. Since AD 1 contains five pure clusters, both schemes show no information loss till the compaction gain is 200 (summary size = 5). For compaction more than 200, the information loss increases sharply, since the five clusters were chosen to be distinct from each other. Hence, no larger summary could be found which could merge any two clusters efficiently. In the second data set AD 2, there are 100 out-lying transactions. The figure shows that the performance of the clustering-based approach degrades rapidly as the compaction gain is increased. This happens be-cause some of the outlying transactions are forced to belong to the natural clusters, which makes the cluster description very lossy, and hence incurs a large informa-tion loss for all members of that cluster.
 BUS on the DARPA and SKAION data sets, respectively. From the two graphs we can see that BUS performs better than the clustering-based approach. We also observe that the difference in the curves for each case reflects the lof score distri-bution for each data set. In the SKAION data set there are a lot of outliers which are represented poorly by the clustering-based approach while BUS handles them better. Hence, the difference in the information loss is very high. In the DARPA data set, most of the transactions belong to well-defined clusters which are rep-resented equally well by both the algorithms. Thus, the difference in information loss for the two algorithms is not very high in this case.
 quent patterns and outliers in the data, we plot the information loss for transactions which have lost a lot of information in the summary. Figure 18 c shows the differ-ence in the ICC curves for the transactions in the DARPA data set which have lost more than 70% information. The graph shows that for BUS, none of the transac-tions lose more than 70% information till a compaction gain of about 220, while for the clustering-based approach, there are considerable number of transactions which are very poorly represented even for a compaction gain of 50. A similar result for the SKAION data set in Fig. 18 g shows that BUS generates summaries in which very few transactions have a high loss, which is not true in the case of the clustering-based approach.
 transactions which have lost less than 70% of information for the DARPA data set. This plot illustrates the difference in behavior of the two algorithms in terms of summarizing the transactions which belong to some frequent pattern in the data. The clustering-based approach represents these transactions better than BUS. A similar result can be seen for the SKAION data set in Fig. 18 h. 7.3 Qualitative analysis of summaries In this section, we illustrate the summaries obtained by running the clustering-based algorithm (see Table 10 ), and BUS using frequent itemsets (see Table 11 )on the DARPA data set described above. This data set is comprised different attacks launched on the internal network by several external machines. The tables do not contain all the features due to the lack of space. However, the information loss was computed using all the features shown in Table 9 .
 S 1 and S 3 correspond to the icmp and udp traffic in the data. Summaries S 2 , S 4 and S 6 represent the ftp traffic on port 20, corresponding to the warezclient, load-module and ffb attacks which involve illegal ftp transfers. S 5 represents traffic on port 23 which correspond to the rootkit and multihop attacks. The rest of the summaries, S 7  X  S 10 , do not have enough information as most of the features are missing. These cover most of the infrequent patterns and the outliers which were ignored by the clustering algorithm. Thus, we see that the clustering-based algo-rithm manages to bring out only the frequent patterns in the data. The summary obtained from BUS gives a much better representation of the data. Almost all the summaries in this case contain one of the IPs (which have high weights), which is not true for the output of the clustering-based algorithm. Summaries S 1 and S 2 rep-resent the ffb and loadmodule attacks since they are launched by the same source IP. The warezclient attack on port 21 is represented by S 3 .The ipsweep attack, which is essentially a single external machine scanning a lot of internal machines on different ports, is summarized in S 6 . S 5 summarizes the connections which cor-respond to internal machines which replied to this scanner. The real advantage of this scheme can be seen if we observe summary S 9 which is essentially a single transaction. In the data, this is the only connection between these two machines and corresponds to the rootkit attack. The BUS algorithm preserves this outlier even for such a small summary because there is no other pattern which covers it without losing too much information. Similarly, S 10 represents five transactions which are icmp replies to an external scanner by five internal machines. Note that these replies were not merged with the summary S 5 but were represented as such. Thus, we see that summaries generated by BUS algorithm represent the frequent as well as infrequent patterns in the data. 8 Related work Compression techniques such as zip, mp3, mpeg, etc. also aim at reduction in data size. But compression techniques are motivated by system constraints such as pro-cessor speed, bandwidth and disk space. Compression schemes try to reduce the size of the data for efficient storage, processing or data transfer. Summarization, on the other hand, aims at providing an overview of the data, thereby allowing an analyst to get an idea about the data without actually having to analyze the entire data.
 tion of frequent itemsets [ 2 , 5 , 7 , 11 , 19 , 20 ]. However, their final objective is to approximate a collection of frequent itemsets with a smaller subset, which is dif-ferent from the problem addressed in this paper, in which we try to represent a collection of transactions with a smaller summary.
 nity, and has been addressed mostly as a natural language processing problem which involves semantic knowledge and is different from the problem of summa-rization of transaction data addressed in this paper. Another form of summariza-tionisaddressedin[ 12 , 16 ], where the authors aim at organizing and summarizing individual rules for better visualization while not addressing the issue of summa-rizing the data.
 and Karypis [ 24 ]. This paper proposes an algorithm (SUMMARY) to find a set of frequent itemsets (based on a support threshold), which is called a summary-set , for a given set of transactions. The summary-set is found by determining the longest frequent itemset which covers a transaction, for each transaction and then taking union of all such longest frequent itemsets. The summary-set is then used to determine clusters for the given data set by treating each member of the summary-set as a cluster such that all transactions which considered that mem-ber as their longest representation belong to the same cluster. The authors claim that the summary-set determined using the SUMMARY algorithm is a good sum-mary of the entire data set. Indeed, this method can be viewed as one instance of the top-down approach for computing summaries. However, there are several shortcomings associated with it as discussed below  X  The summary-set does not guarantee to cover every transaction belonging to  X  This method does not try to explicitly tradeoff compaction gain for information 9 Concluding remarks and future work The two schemes presented for summarizing transaction data sets with categor-ical attributes demonstrated their effectiveness in the context of network traffic analysis. A variant of our proposed two-step approach is used routinely at the University of Minnesota as a part of the MINDS system to summarize several thousand anomalous netflows into just a few dozen summaries. This enables the analyst to visualize the suspicious traffic in a concise manner and often leads to the identification of attacks and other undesirable behavior that cannot be captured using widely used intrusion detection tools such as SNORT.
 tions in the data set are equally important. In several applications, the transactions might have different levels of importance. In such cases, it will be desirable for higher ranked transactions to incur low information loss while lower ranked trans-actions can tolerate a little higher information loss.
 the network flows are ranked based on their anomaly scores. The main challenge that arises in adapting our proposed summarization techniques to this problem is how to incorporate the knowledge of ranks while scoring the candidates to achieve the above stated objective. We are currently investigating a few possible approaches in this direction.
 References
