 Ctra. Campo Real Km. 0.200, 28500 Arganda del Rey The goal of the research described here is to develop a multistrategy classifier system that can be used for document cat egorization. The system automatically discovers classification patterns by applying several empirical learning met h ods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by ap plying evolutionary techniques as well. The sys tem relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the th e matic domain. Feature selection, multistrategy learning, genetic al gorithms. Text categorization can be applied in any context requiring document organization or selective and adaptive document dispatching. Assigning thematic categories to documents is essential to the efficient management and retrieval of in formation and know l edge [25]. This paper focuses on the task of classifying incoming documents in several non -disjoint categories.
 Although the growth of electronically stored text has led to the development of machine learning methods prepared to exploit ungrammatical text, most of these methods are based on a single strategy and work well in concrete domains [9], [16]. The richness and redundancy of the information present in many digital doc u ments make a multistrategy learning approach especially suitabl e [8]. However, most current multistrategy systems for text classification [7], [10] combine statistical and symbolic algorithms in a predefined manner by using a common feature extraction stage and thus a shared feature set. These systems solve the proble m by different empirical methods and usually take the most conf i dential method.
 Certain learning algorithms are more suitable for some thematic categories than for others [10], showing different classification results due to the different types of i n format ion present in each domain. The performance of an algorithm depends on the features or attributes chosen to represent the information [12], [15], [26]. Choosing the right feature set is critical to the successful induction of classific a tion models [20]. Co nventional approaches use a general method based on statistical measurements and stemming procedures for creating the feature set or vocabulary of the problem, which is independent of the learning algorithm and the thematic d o main [18]. In [2] several expe riments we re carried out to monitor the actual intera c tion between feature selection and the performance of some linear classifiers.
 The algorithm used and the features selected are always the key points at design time, and many experiments are needed to s elect the final algorithm and the best suited feature set. Moreover, once the algorithm and features are set, the achieved solution may prove unsatisfactory due to possible losses of relevant information when ma p ping from documents to the feature set.
 The main goal of the HYCLA (HYbrid CLAssifier) system presented here is to maximize classification performance by considering all the types of information contained in documents regardless of their thematic domain. With this aim, the cla s sification system reli es on a hybrid architecture that tackles two main issues: optim i zation of document representation and integration of the results of several classifiers. The term hybrid has a double meaning here . On one hand, it symbolizes the multistrategy nature of the e mpirical learning approach to text categorization. On the other, it refers to the genetic search carried out to find the vocabulary of the problem and integrate the individual predictions of the learners.
 HYCLA learns classification models from imbalanced document samples. The documents can be imbalanced for two reasons: 1) some thematic categories have many preclassified documents, while others do not; and 2) there are thematic categories that only contain one or two types of inform a tion. Feature selectio n methods based on a particular statistical measurement favor some thematic categories over others depending on the characteristics of the ranking statistical technique and the thematic categories. The genetic feature selection proposed in this paper treat s all categories the same because it considers several statistical measurements, thus obviating the kind of imbalance that stems from a different distribution in the number of documents per category.
 HYCLA distinguishes several types of text information pr esent in documents and builds a classification model for each type of information. When it classifies a document, the final document category is obtained by the genetic combination of the decisions made by all the models. Since there are many web domains c ontaining thematic categories that lack some part of information, the documents belonging to these categories could yield worse results in classification. The goal of the genetic combination is to smooth out this other kind of imbalance by optimizing the c ontribution each classification model makes towards assigning the final category to a document. The HYCLA system has been validated using two types of digital or electronically stored text: scientific/technical papers and hypertext documents belonging t o several categ o ries.
 The following section surveys the architecture capabilities in detail. Section 3 di s cusses the empirical evaluation of this approach, and final sections present the co n clusions and point the way to future work on this subject. HYCLA operates in two stages, learning and integration. In the learning stage, learners apply an evolutionary technique to obtain their own feature set, and then they are trained to obtain their classification model. In the integration stag e, indivi d ual learned models are evaluated on a test set , and the predictions made are co m bined in order to achieve the best classification of test documents. The subsections below describe the modules and procedures of this system.
 The underlying architec ture of HYCLA can be instantiated to approach a different text mining task by upgra d ing its modules. This step is common to all the learners. The system receives a sample of documents of different thematic categories that is divided in to two sets, the training set which contains two -thirds of the documents and the test set which contains one -third of the documents. The task here is to scan the text of the sample and produce the list of the words or vocabulary contained in the documen ts . Figure 1 shows the analogies found between the parts of scientific and hypertext documents. These documents usually present redundant information in all four of their text parts [1]. Based on this idea, when the system receives a t raining sample of scie n tific/hypertext documents whose first line is the title/ url ( u niform r esource l ocator) of the document, four vocabularies are generated from every document: one containing the title/ url words, a second containing all the words from t he from abstract/meta -text, a third with the contents/plain words, and a fourth containing the words from the references/hyperlinks. Every vocabulary is smaller in size than the vocabulary obtained from the original document. Whenever all the documents lac k some portion of the text, the corresponding vocabulary is empty.
 The preprocessing step begins by removing those words found in the vocabularies that belong to a stop list consisting of words without semantic content [12], [15] and applying stemming proc edures [22]. After that, the frequency of occurrence of every valid word is calculated, and this value is increased depending on the word format (for example, the frequency is ten times higher for a word found in the title, nine times higher for a word fou nd in the subtitle, and so on). Words recurring below a frequency thres h old are not reliable indicators and are removed.
 Due to the high dimensionality of the preprocessed vocabularies the first task of HYCLA is to reduce the feature space size with the l owest loss of classification performance. For each preprocessed vocabulary, once the number of documents from every category containing the terms of the vocabulary is known, several information stati s tical measurements are calculated: 1) information gain: how many information bits are needed to predict a category depending on the presence/absence of a word in a document [26]; 2) mutual information: words occurring only in a document belon g ing to a certain category are the most relevant for this category [26 ]; 3) document frequency: words occurring more frequently are the most valuable; 4) chi square: how independent a word and a category are [24]; 5) crossover entropy: similar to information gain, but considering the presence of a word in a document [24]; an d 6) odds -ratio: the relevance of a word for a category depends on the occurrence and non -occurrence of the word in the category [20]. The values of some of these six measurements depend heavily on the distribution of documents into them atic categories. An imbalanced document sample contributes to strengthen the value of only some of these statistical measurements. The words of all of the vocabularies are sorted by the six measurements , and only the k v highest ranked words of each vocabu lary are retained. In [25], [20] a detailed analysis of the optimal value of k v is discussed. The authors show that the relation between some measures of classification performance and k depends heavily on the particular statistical measurement chosen and whether the classifier is binary (disjoint categories) or multicategory (non -disjoint categories). Since the genetic feature selection carried out by HYCLA considers several statistical measurements, the way to avoid these dependences is to adopt a value of k v of approximately 30% of the size of the preprocessed vocabulary. All the statistical measurements achieve their maximum performance classification with this value of k v The k v words of each vocabulary ranked by each measurement form a view. If several views of a vocabulary are identical, then only one of them is considered. The set of views of a vocabulary will be the initial feature subsets of a learner.
 Although some information is lost in any one feature subset, the multiple views of every i nitial vocabulary will make for a better overall performance. In scie n tific/hypertext documents, there are four possible vocabularies and six possible views associated with each vocabulary. Since documents contain diff erent kinds of information, the multistrategy approach suggests that each learner solves a part of the problem with a different incoming information from the same sample. Each learner can learn to classify documents with regard to the feature subsets obtai ned from the preprocessing step. The filtering feature selection by ranking techniques is independent of the learning method that will use the selected features. The performance of a learner using filtered features is very sensitive to the score criterion of the ranking technique. In order to avoid this situation, HYCLA adopts what has been called the wrapper approach [26], [13], in which final feature selection depends on the inductive algorithm used.
 When a learner receives a feature set, it carries out t he following tasks: 1. Empirical learning 2. Testing. The learner applies the inferred model to a test set and calculates several measures of classification performance. Genetic algo rithms are search algorithms based on the natural evolution process. They have been successfully applied to optimization and machine learning problems [3], [11]. Starting from an initial population of individuals or chromosomes repr e senting tentative solut ions to a problem, a new generation is created by combining or modifying the best individuals of the previous generation. The process ends when the best solution is achieved or aft er a fixed number of gener a tions. The application of genetic algorithms to text feature selection involves establis h ing the representation of chromosomes, defining crossover and mutation operators fitted to chromosome representation and document domain, and defining the fitness function used to determine the best chromosomes of a population. The goal of genetic feature selection is to solve the kind of imbalance derived from a different distribution in number of documents per category Each view computed from an original vocabulary in the preprocessing ste p is a chromosome. Chromosome length is fixed at k v . Each gene is a word of the vocab u lary. Population size matches the number of different views of a vocabulary. For example, if the input vocabulary is { bye, see_you, hello, good_morning, good_afternoon }, then { see_you, bye, good_afternoon } and { see_you, good_afternoon, hello } are two chr o mosomes that could be obtained by applying chi -square and crossover entropy tec h niques, respectively, with k v = 3. The crossover operator exchanges the last thir d of the genes of two chromosomes to create a new offspring. The typical size of a chromosome in text domains is about one or two thousands genes, and about the first two -thirds of words are almost i n cluded in all the chromosomes, although at different pla ces within this fragment. In order to avoid obtaining duplicated genes that furnish no new information, only the last third of chromosomes should be exchanged in the crossover operation. For e x ample, if the parents were: Since the size of chromosome is equal to three and the number of genes of last third is equal to one, then the new offspring would be: The mutation operator modifies ten percent of the genes from a randomly selected place p in a chromosome by switching them with other possible words from the vocab u lary. For example: The proportion of chromosomes involved in crossover and mutation operations is determined by crossover and mutation probabilities, which are set empirically. In Section 3, the values of these paramete rs are shown. The results of the application of any genetic operator can produce new chromosomes containing repeated words. Since just the first occurrence of every word within a chromosome will be consi d ered, genetic search can yield not only an optimal f eature set, but also a smaller number of features. The learner obtains a model for every chromosome of a certain generation. The fi t ness function of a chromosome is a measurement of the model performance co m puted on a test sample represented relative to the chromosome. This test sample is a subset of the general test set, and it is composed of relevant, noiseless documents in order to prevent the system from wasting too much time computing the fitness fun c tion. The calculation of t he fitness function uses about 30% of the documents from the initial test set. All learners use the same test sample, which is then barred from further consideration in order to avoid learning overfitted final categorization mo d els. Previous research work on wrapper feature selection using genetic algorithms have defined a composed fitness function as a weighted sum of other fitness fun c tions corresponding to different optimization objectives [21]. Because the wrapper approach is very time -consuming, such research has used neural networks as the sole inductive algorithm for evaluating chromosomes and calculating their fitness as an estimate of precision on a test sample. The resulting classifier is more independent of the document sample and shows a lower c lassification performance. In HYCLA, the learners deal with a population of fixed size with six chrom o somes at most. The initial population is already formed by good feature sets, and the number of generations needed to reach the final feature set is small . When a learner obtains a feature set, the set of training documents is represented relative to that feature set, and then the learner applies its inductive algorithm to learn a classification model. Since there could be four kind s of redundant inform a tion in documents, the system can run four learners: the abstract/meta information learner, the reference/link information learner, the contents/plain information learner and the title/url information learner. The selected learning me thods embo d ied in learners are: ? ? Na X ve Bayes [9] for the plain text learner, since the plain text ? ? Decision trees [9] for the abstract/meta text and the title/ url ? ? Rule discovery [5], [6], [7] for the reference/link learn er. In When a learner obtain s a classification model, whether the feature set is a tentative one obtained from a certain generation of the genetic algorithm or the optimal one, obtained from the last generation, the model is applied to a test set, and several pr e dictive measurements can then be calculated: recall or percentage of documents for a category correctly classified, precision or percentage of predicted documents for a category correctly classified, and F -measure , which can be viewed as a function made up of the recall and pr ecision measurements. The value of F -measure is the fitness value used by the genetic algorithm for chromosomes representing tentative feature sets. In order to classify a document, the different kinds of information belonging to the document are represented according to the learned vocabularies of every learner, and then every learner applies its model to make a prediction. Abstract/meta and refe r ence/link texts usually give accurate information about the category of a document. Howev er, there are many documents that lack both these kinds of information, and the system then has to rely on the prediction made by the plain text and url learners. There are two options for obtaining the final classification prediction of a doc u ment: ? ? To ta ke the model with the best performance results for ? ? To take a combination of the models as the final solution. HYCLA performs a weighted integration of the individual predictions, and it d e termines the weight of each learner together with that of the other learners by using a g e netic algorithm. This genetic integration contributes to improve the results in classification performance of imbalanced thematic categories without some type of information. The genes of a chromosome represent the weights, between 0 and 1, of the predi c tions made by the different learners. Chromosome lengt h matches the number of learners involved in the problem. The initial population is made up of chromosomes whose genes take values from the set [0.0, 0.2, 0.4, 0.6, 0.8, 1], allowing all possible combinations of these values, and an additional chromosome whose genes are the values of F -measure obtained by each learner in the tes t ing stage.
 The crossover operato r allows the genes of two parent chromosomes, taken from a randomly selected place, to be exchanged. The mutation operator increases or r e duces the weight of a randomly selected gene by a quantity between [ -0.10...0.10]. The fitness functio n evaluates ever y chromosome on a labeled test set by combi n ing chromosome weights. Each learner predicts a category for a document with a weight equal to the gene in the chromosome that represents the learner. When several learners predict the same category, the average of their weights is calculated. The final predicted category for a document will be the one predicted by the learner or learners with the highest weight. For example, for the following chr o mosome: If the predictions of the learners were: The highest weight is 0.84, and so the resulting prediction assigns the document to Cat e gory 1. The fitness function value of a chromosome is the value of F -measure achieved by the chromosome for the full test set of d ocuments. The stop criterion of the genetic search could be a thr eshold value of the fitness function, i.e. a classific a tion precision of 97%, or a certain number of generations.
 The computational cost of this genetic search is very low , since the classifi cation of test documents has been performed by the learners in the model testing stage. HYCLA has been evaluated on three text collections. These collections are described below, followed by a review of the experimental settin gs and re sults. Reuters -21578 is a collection of newswire article texts that appeared in 1987. The entire collection has 21,578 texts belonging to 135 topic categories. In order to evaluate the performance of HYCLA, the sample taken into account is composed of cat egories with more than 100 examples. The selected example set has a size of 12,066 documents belonging to 23 different categories. These documents were a r ranged into three disjoint subsets (see Table 1): a training set, Trainin g1, with 6,014 documents, and two test sets, Test11, with 2,614 documents, and Test12, with 1,708 documents. Another collection of 7,161 text documents was collected by a program that aut o matically downloads web documents. The documents belong to three di fferent cat e gories, and were arranged into three disjoint subsets (see Table 2): a training set, Training1, with 5,008 documents, and three test sets, Test11 , Test12 and Test13, with 1,416, 346 and 391 documents, respectively. The  X  X OISE X  category is co m po sed of error pages and ra n domly downloaded pages.
 The third collection is composed of 2,442 documents belonging to five domains defined from the Yahoo Directory. These documents were arranged into three disjoint subsets (see Table 3): Training1, with 1,121 documents, and two test sets, Test11 and Test12, with 561 and 561 documents, respectively. The first kind of experiment allowed the classification performance of several feature selection methods to be comp ared and showed the improvement achieved by evol u tionary selection method used by HYCLA. The statistical methods used were: info r mation gain (I.G.), document frequency (D.F.), chi square (CHI 2 ), crossover entropy (C.E.), mutual information (M.I.) and odds -ratio (O.R.). The values of crossover and mutation pro b abil i ties used for the evolutionary feature selection were both 0.4. This experiment was performed on the two first imbalanced collections: Reuters -21578 collection contains categories with many docum ents while others do not and HTML collection downloaded from the web contains documents that lack some part of text information.
 A Na X ve Bayes classifier was trained on both collections using Training1 . Test set Test12 was used by the genetic algorithm to evaluate the fitness function, and test set Test11 was used to evaluate the classification accuracy measurements of the learned models. The final size of the vocabulary of Training1 on Reuters -21578, after removing the words from a stop list, was 16,806. The value of k v chosen for running the fe a ture sele c tion methods was 5,041. In the HTML collection, only the words contained in the HTML tag  X &lt;META&gt; X  were taken into account. The vocabulary size, after r e moving stop -list words, was 9,364, and t he value of k 3,000. The performance of each method is reported below, in Table 4 for the Reuters texts and in Table 5 for the downloaded web pages. The numerical values Pr , Rc and F represent precision, recall and F -measure (F = (2 * pre cision * recall) / (precision + recall)) normalized between zero and one, respectively. In Table 4, the columns show these values obtained by each feature selection method in each category. The last row presents the macro averaged values. In Table 5, the r ows show the precision, recall and F -measure values obtained by each feature selection in each category. The last column shows the macro averaged values. The values of the evolutionary feature selection method are the average values from running the geneti c algorithm five times. In both t a bles boldface indicates the best values in each category. The results indicate that each statistical feature selection method behaves the best only in certain categories. The genetic feature selection method yields the be st ave r age F -measure value in all categories on both collections. This fact reflects that the genetic feature selection method is more independent of the distribution of the e x amples into categories than the other sele ction methods, and therefore more robust for handling imbalanced data. In the categories where the genetic method does not give the best performance, it does yield the second or third best value. Moreover, the best average F -measure implies the best ratio between precision and recall. The other feature selection methods have a certain leaning towards one or the other of these two measurements. The experiments show that a significant departure from the a p proaches that utilize universal feature selection yiel ds better r e sults. The second kind of experiment was set up to compare the performance of the predi c tions of every individual learner and the genetic combination of these predictions. A compari son of genetic combination of predictions and a voting combination is also reported. The voting combination proposes to assign the category predicted by a majority of individual learners to an incoming document.
 There are four learners for the four differe nt types of information taken into a c count in HTML documents, url text, meta -text, plain text and hyperlink text.
 The experiments were performed on the free collection downloaded from the web shown in Table 2 and the Yahoo collection shown in Table 3. Tabl e 6 and Table 8 present the size of the initial vocabularies of Training1 after removing stop -list words and the value of k v for each type of inform a tion. free collection, Test 13 is the set used to calculate the fitness function i n the g e netic combination of predictions, and Test11 is the set used to calculate the perfor m ance measurements. On the Yahoo dataset, the url text of documents has not been included since it gives no relevant information. The access to these web documents was performed through links in the Yahoo Directory. Every link activates a specific url that runs a program leading the Internet browser to such web documents. The url text of each downloaded web document contains the url of the program as well a s the non -informative arguments inherent to the document.
 The values of crossover and mutation probabilities used for the evol u tionary integration were 0.7 and 0.4, respectively.
 Table 7 and Table 9 show the precision, recall and F -measure values of every learner and of the genetic and voting combination in every category. Last rows indicate the average values. Boldface indicates the best performance values in every category. The genetic pe r formance values are the average values foun d by running the genetic algorithm five times.
 This experiment shows that the average F -measure of genetic integration is the best result. Individual learners obtain good results only in certain categories. The voting combination of learner predict ions obtains very poor results in all categories. The genetic combination of learner predictions behaves better and more smoothly than individual predictions and voting combination in all cat e gories.
 META Vocab u lary The architecture pre sented in this paper is a combination of a variable number of learners. Learners may be added or removed depending on the specific text categor i zation task. This modularity makes the system adaptable to any particular context. The genetic feature selectio n method takes advantage of each statistical selection method used. This method works quite well for all categories, regardless of the di s tribution of the documents in the training sample. Moreover, statistical feature sele c tion methods display text -domain dependence, and the evolutionary method makes this dependence smoother.
 The division of HTML documents into four types of text has shown that some words have a greater importance in a certain piece of text than in the full text with no partition. The appl ication of different learners to each type of information allows the system to be independent of text domain without loss of accuracy. The genetic integration of the predictions of the learners yields good results in classification perfor m ance. Currently work in this area is mainly focused on the design and development of a genetic algorithm devoted to discovering the classification models of different cat e gories of documents. The entire text classification task could be carried out by a genetic algorithm alone. Simplicity, uniformity and intelligibility would be the main features of the resulting categorization system.
 Classifying a new document would mean measuring the distance between the suitably represented document and the chromosome or mod el being evaluated. The definition of the distance measurement and genetic operators are the key points of this research. The individuals that are revealed as the best would be the optimal classific a tion models. Part o f this work was supported by the Spanish Ministry of Science and Techno l ogy under project FIT -070000 -2001 -193 and by Optenet, S.A. [1] Attardi G., Gulli A., Sebastiani F.: Automatic Web Page [2] Brank, J., Groblenik, M., Milic -Frayling, N., Mladenic, D.: [3] Castillo, M X . D. del, Gas X s, J., Garc X a -Alegre, M.C.: Genetic [4] Castill o, M X . D. del, Barrios, L. J.: Knowledge Acquisition [5] Castillo, M X . D. del, Sesmero, P.: Perception and VOTING [6] Cohen, W.: Text categorization and relational learning. [7] Craven, M., DiPasquo, D., Fr eitag, D., McCallum, A., [8] Doan, A., Domingos, P., Halevy, A.: Learning to Match the [9] Dumais, S. T., Platt, J., Heckerman, D., and Sahami, M.: [10] Freitag, D.: Multistrategy Learning for Information [11] Goldberg, D.: Genetic Algorithms in Search , Optimization [12] Grobelnik, M., Mladenic, D.: Efficient Text Categorization. [13] John, G.H., Kohavi, R., Pfleger, K.: Irrelevant Features and [14] Langdon, W. B., Buxton, B. F.: Genetic Programming for [15] Lewis, D.: Feature selection and feature extraction for text [16] Lewis, D., Ringuett e, M.: A Comparison of Two Learning [17] Michalski, R.S., Carbonell J.G., Mitchell T.M.: A theory and [18] Mladenic, D.: Feature Subset Selection in Text -Learning. [19] Mladenic, D., Grobelnik, M.: Feature selection for [20] Mladenic, D., Grobelnik, M.: Feature selection for [21] Oliveira, L. S.: Feature Selection Using Multi -Objective [22] Porter, M.F.: An algorithm for suffix stripping. Program, [23] Quinlan J. R.: C4.5: Programs for Machine Learning. San [24] Sebastiani, F.: Machine Learning in Automated Text [25] Yang, Y., Pedersen, J.P .: A Comparative Study on Feature [26] Yang, J. and Honavar, V.: Feature subset selection using a 
