 REGULAR PAPER Jason D. Van Hulse  X  Taghi M. Khoshgoftaar  X  Haiying Huang Abstract Analyzing the quality of data prior to constructing data mining models is emerging as an important issue. Algorithms for identifying noise in a given data set can provide a good measure of data quality. Considerable attention has been devoted to detecting class noise or labeling errors. In contrast, limited research work has been devoted to detecting instances with attribute noise, in part due to the difficulty of the problem. We present a novel approach for detecting instances with attribute noise and demonstrate its usefulness with case studies using two different real-world software measurement data sets. Our approach, called Pair-wise Attribute Noise Detection Algorithm (PANDA), is compared with a nearest neighbor, distance-based outlier detection technique (denoted DM) investigated in related literature. Since what constitutes noise is domain specific, our case stud-ies uses a software engineering expert to inspect the instances identified by the two approaches to determine whether they actually contain noise. It is shown that PANDA provides better noise detection performance than the DM algorithm. Keywords Data quality  X  Noise detection  X  Data cleaning  X  PANDA 1 Introduction Data quality is a critical issue whenever data mining techniques are applied to real-world data sets. Organizations have realized the value of information and now consider data as an asset of the company [ 2 ]. Important decisions are made every day based on the data stored within a company X  X  databases, from understanding the behaviors of profitable customers to determining the likelihood of future loss on new business contracts. However, the decisions made are only as good as the data upon which they are based. High-quality data is defined as data that is fit for use by data consumers [ 26 ]. Characteristics of high-quality data belong to four categories: Intrinsic, Accessibility, Contextual, and Representational.
 relates to accuracy and objectivity of the data source. Quality issues in the Ac-cessibility category are related to the ability to retrieve the necessary data in a timely and efficient manner. Contextual problems include missing, incomplete, or improperly defined data. The Representational category includes issues related to interpretability, ease of understanding, and the concise and consistent represen-tation of data. Data quality issues are often multifaceted and complex, and it is crucial for information management departments to build applications that sup-port the goal of achieving high-quality data within an organization. Errors in data can often be found when multiple data sources are merged [ 10 , 11 ]. Records pro-cessed within different systems may have different data formats or representations [ 7 ]. When such databases are merged, two records referring to the same entity may not match, resulting in duplicate entries or missing data. An important area of data quality research involves detecting duplicate or near duplicate records in a database.
 very large databases that would be difficult for humans to discover otherwise. The knowledge derived from these algorithms is based on the data within the database being analyzed, and low-quality data can obscure important patterns that may exist. In a classification problem, for example, it has been shown that the presence of errors in the training data set does indeed lower the predictive accuracy of a learner on test data [ 13 , 15 , 29 , 30 ]. In the machine learning context, noise can dramatically slow the convergence of a learning algorithm, or even cause it to converge to a suboptimal set of hypotheses [ 5 ].
 issue, there are generally two types of noise in a given data set [ 3 ]: class noise and attribute noise. Class noise or labeling errors occur when an instance belongs to the incorrect class. Class noise can be attributed to several causes, including subjectivity during the labeling process, data entry errors, or the absence of some representative attributes. In contrast, attribute noise reflects erroneous values for one or more attributes (independent variables) of the data set. The complexity of detecting class noise is relatively lower than that of detecting attribute noise. Subsequently, very limited attention has been given to attribute noise. Our study focuses on detecting instances with attribute noise.
 Generally, there are three types of such methods: robust learners, data polishing, and noise filtering. Learners are considered robust if they are less likely to be influ-enced by the presence of noise in data. A good example of a robust learner is the C4.5 decision tree algorithm [ 22 ]. The C4.5 learner employs pruning strategies to remove statistically insignificant portions of the tree to form the final model. However, if the noise level is relatively high, even a robust learner may have poor performance. Many learners are sensitive to data noise and require data prepro-cessing to address the problem. Noise filtering identifies noisy instances which can be eliminated from the training data [ 3 , 9 , 17 , 30 ]. Finally, data polishing corrects the noisy instances prior to training a learner [ 27 ]. Though considerably tedious, such an approach is viable when the data set is small.
 stances with noise in one or more attributes. Abbreviated as PANDA for Pa i r w i s e Attribute Noise Detection Algorithm , the proposed approach can be used with any number of (numeric or binary) attributes, including the class attribute (dependent variable). PANDA is evaluated in case studies using two different software mea-surement data sets, and is compared with a nearest neighbor, distance-based outlier detection technique (abbreviated DM) presented in related literature [ 23 ]. tribute noise, this study is unique because it employs a software engineering ex-pert 1 to validate the instances identified by the two techniques. To our knowledge, this is the first study to present an algorithm for detecting instances with attribute noise without using the class label.
 detection algorithm may not realize domain-specific exceptions and other allow-able instances. An instance identified as noisy by a given technique may not actu-ally be noise for the given domain. Therefore, it is important to note the distinc-tions between outliers, noise, and exceptions. An observation is called an outlier if it appears to be inconsistent with the remainder of a set of data, or which deviates so much from other observations so as to arouse suspicions that it was generated by a different mechanism [ 25 ]. In [ 20 ], outliers are defined as  X  X bjects deviating from the major distribution of the data set X .
 values that reside on the extremes of the general distributions observed for the given attributes in the data set. In contrast, instances may be considered noisy when the values of one or more attributes of an instance are corrupted or incorrect relative to the values of the other attributes. However, an instance without errors may appear noisy when one or more of its attributes does not follow the general distributions observed for the given attributes. These exceptions therefore often appear as noisy instances. Such instances are very difficult to detect without the input of a domain expert. In our study, the software engineering expert plays an important role in categorizing the instances detected by PANDA as either excep-tions or instances with attribute noise.
 related work in the noise detection domain. Section 3 describes the noise detection algorithm proposed in this paper. Section 4 describes the software measurement data sets used in the case studies, while Sect. 5 discusses the obtained results. We conclude in Sect. 6 and highlight some future research directions. 2 Related work As mentioned earlier, noise can occur in either the dependent variable (class noise) or independent variables (attribute noise). Related literature on noise detection has primarily concentrated on the class noise problem [ 30 ]. Various techniques have been developed to identify instances with class noise with a high degree of accuracy [ 16 ]. In contrast, very few methods are available for detecting instances with attribute noise, largely due to the high complexity of the problem. In this section, some related works on noise detection are briefly discussed including association rules, noise filtering, and data polishing. A discussion on some outlier detection techniques is presented in Sect. 3.2 , including the DM algorithm [ 23 ] which is considered in our comparative study.
 of data errors. Data errors are identified when ordering relations between attributes prevalent in the remainder of the data set do not hold for the observation in ques-tion. For example, an observation with date of birth greater than date of death would be declared as noise. This is a useful tool, but is rather limited in scope. identification or correction of noisy instances, and suffer from the limitation that they cannot be applied to a data set without the class variable. The assumption usually made is that an observation that is difficult to classify correctly is likely to be noise. Of course, it may also be the case that the instance is part of a target concept that is difficult to learn. For the most part, these algorithms do not treat any observations that are predicted correctly. It is not necessarily true, however, that these observations do not contain noise.
 stance is tagged as noise if a given number of classifiers are unable to correctly classify the instance. A majority filter is defined as a system which categorizes an instance as noise if out of the five classifiers built, three or more fail to correctly classify that instance. In a recent study, we investigated a very large ensemble filter (25 classifiers) and partitioning filters for noise filtering of software measurement data [ 14 , 17 ].
 target theory is built using a rule induction algorithm such as CN 2[ 4 ]tocor-rectly classify all instances in the data set. Instances that maximize the reduction in the Minimum Description Length (MDL) cost of encoding the target theory are removed, until the cost cannot be reduced any further.
 rect noise [ 27 ]. Given a data set which contains N independent variables and 1 dependent variable, a total of N modified classifiers are constructed, where each independent variable is switched in turn with the dependent variable. For instances that are misclassified by the base classifier T , the number of attribute misclassifi-cations over the N modified classifiers are recorded. In the polishing phase, using the predictions for each attribute from the N classifiers, the predicted attribute val-ues are inserted into the instance. If the base classifier T is able to correctly predict the class of this modified instance, then the polishing procedure is complete and a noisy instance has been cleansed. The complexity of this algorithm is high and increases significantly as the number of independent variables increases. shows better results for independent variables that cannot be predicted by using the class and the other independent variables (so-called predictive-but-unpredictable attributes). A set of rules predicting the class variable are learned from the data. If an instance fails to satisfy at least one rule, it is identified as noise. The instance can then be corrected by finding a rule that minimizes the number of changes necessary to make the instance consistent with the rule set.
 variables to be non-numeric. In addition, all of these techniques operate in a su-pervised learning environment, disqualifying their use when the class label is un-available. In contrast, the noise detection algorithm proposed in this paper can be used with or without the class variable. If the class variable is available, it can be treated as another attribute. The advantages of the proposed technique include: 1. Erroneous observations can be examined prior to collecting data for the de-2. It can be applied to the test data set (with unknown class labels) as well as the tion technique was evaluated by injecting simulated noise into the data sets, which were primarily obtained from the UCI Repository of machine learning databases [ 21 ]. There are two important problems with this evaluation approach. First, it is unknown whether the underlying data set into which simulated noise is injected is noise-free or clean. Injecting additional noise into a data set that may already contain noise makes the analysis of the effectiveness of the algorithms difficult. real-world data set of the given domain. Obtaining a completely clean data set prior to noise injection is a difficult task and the injection of noise into such a data set to simulate real-world conditions is an open research issue. In our case studies, we consider real-world data sets which inherently contain noisy instances (i.e., we do not inject noise into the data set). A domain expert is needed to inspect and identify instances that are actually noise. The noise detection techniques can then be evaluated with respect to their performance in detecting those instances. 3 Methodology The proposed noise detection algorithm, PANDA, seeks to identify those instances with a large deviation from normal given the values of a pair of attributes. When a set of instances have similar values for one attribute, large deviations from normal for the second attribute may be considered suspicious. The larger the deviation, the stronger the evidence that the instance is noisy. One has to be careful in in-terpreting the aforementioned assumption because the variance and distribution of the two attributes may be different. In our study, we address this issue by stan-dardizing all the attributes in the given data set. The output of PANDA is a list of instances ordered from most noisy to least noisy. Each instance is assigned an output score (Noise Factor), which is used to rank the instance relative to the other instances in the data set. After obtaining a noise ranking, some of the instances may be discarded from the data set, which would result in a cleaner data set with which to perform additional analysis. Removing noisy instances, however, is not required and the user can apply any type of domain-specific treatment using the noise ranking produced by PANDA.
 titioned, the conditional mean and standard deviation of the second attribute are computed relative to the discretized value of the first attribute. Large deviations from normal by the second attribute relative to the instances with the same parti-tioned value for the first attribute have a higher contribution to the output score. Based on these calculations, the value of each attribute is replaced by the stan-dardized value. This process is repeated for each combination of ordered attribute pairs, and the results are aggregated to an output score representing the amount of noise present in the instance.
 into the data set, which should be considered during the respective analysis. As discussed in the case studies, we partition the attributes using several bin sizes and select the median ranking. Selecting the median ranking among multiple bin sizes helps minimize the possibility of noise being introduced due to the discretization process.
 noise detection performance of PANDA with a nearest neighbor distance-based outlier detection algorithm (DM) [ 23 ]. The details of the DM algorithm are pre-sented in Sect. 3.2 . 3.1 Noise detection algorithm The detailed procedure for PANDA is shown in Fig. 1 . The parameters s ikj are initialized to 0 on line 1. The binning algorithm implemented in SAS [ 24 ]isused  X  j is integer-valued, the number of partitions of attribute  X  x  X  j . SAS uses an equal frequency binning algorithm, where, in the absence of tied values for the attribute, each partition will have the same number of instances. An exception is made for instances with the same value for the attribute at the boundary of the partition. As partitions are required to be disjoint, all instances with the same attribute value are placed into the same partition. While other binning procedures exist, we chose the SAS binning algorithm for its simplicity. PANDA can be implemented with other partitioning algorithms as well, and an investigation related to the same will be considered in our future work.
 and standard deviation of the non-partitioned attributes x  X  k , k = j , relative to each bin  X  x  X  j = 0 ,..., L  X  1 is calculated (line 6). The standardized value for attribute value x ik relative to the partitioned attribute value for instance i ,  X  x ij , is calculated in line 8. The standardized value is calculated by subtracting the mean value of attribute x  X  k relative to the partitioned attribute value  X  x ij from the attribute value for instance i , x ik . The absolute value of this quantity (as the sign of the difference is not important) is divided by the standard deviation of attribute x  X  k relative to MAX (lines 18) measures for each observation are calculated.
 as potential noise because an observation with a large standardized attribute value indicates a deviation from normal for that attribute. Based on the noise ranking calculated by PANDA, noisy instances may be removed from the data set, al-though this process is not required. In our case studies, we instead present the noise ranking produced by PANDA to a domain expert for evaluation. In direct elimination (Fig. 3 ), all the desired observations will be removed from the out-put data set after a single iteration of PANDA. In iterative elimination (Fig. 2 ), the user can specify the number of iterations to run the algorithm, eliminating a portion of the total noise in each pass. Iterative elimination has the potential ad-vantage that noise detected prior iterations will not affect the ranking that results from the current iteration, although our experiments demonstrated similar results for both methods.
 pose m is the number of attributes in the data set and n is the number of instances. Computing the mean and standard deviation for each pair of attributes in the data set and standardizing each attribute value has complexity O ( m 2 n ) . In many real-world data sets, the number of attributes is much smaller than the number of instances. Additionally, as PANDA is part of the preprocessing tasks of a data mining project, the calculations can be performed offline in a batch environment. Therefore, we can conclude that the running time of PANDA is not a significant concern, especially for small-and medium-sized data sets. 3.2 Comparing with a distance-based algorithm Outlier detection techniques were first developed within a statistical framework. For example, given the data is sampled from an underlying normal distribution, any observation more than 3 standard deviations from the mean might be consid-ered an outlier. A major difficulty with applying this theory to real-world data sets is that the distribution function is unknown, and is usually very difficult to deter-mine. The work introduced in [ 18 ] provides a generalized definition of an outlier, of which the criteria mentioned earlier is a special case. An observation x in a data set D is an outlier if at least a fraction p of the objects in D are further than d away from x .
 the authors find outliers utilizing a modified distance-based metric to cope with difficulties in determining the optimal value for d . D k ( p ) is defined as the distance from point p to its k th nearest neighbor. The  X  observations with the largest values for D k ( p ) for a given value of k are considered outliers. We denote this algorithm as DM. The basic algorithm implementing DM has complexity O ( n 2 ) ,where n is the number of instances in the data set. The authors introduce a partition-based algorithm, which obtains an order of magnitude reduction in running time given a wide range of parameters used in experiments on simulated data.
 be used either with or without the class variable and can handle numeric attributes. DM and PANDA also have comparable complexity. Although DM is specifically designed for outlier detection, no other noise detection algorithm (excluding class noise detection) has been proposed that can be used for this comparison. One difficulty with distance-based techniques, however, is that they rely heavily on some distance function, which looses effectiveness as the number of dimensions of the feature space increases [ 1 ].
 4 Description of the case study data sets 4.1 JM1-2445 data set The software measurement data set JM1-2445 was derived from a NASA software project written in C. Software metrics data was collected at the function level; hence, a program module (instance) was defined as a function or subroutine. Upon removing inconsistent instances (identical values for the software attributes but different class labels), the data set was reduced from 10,883 to 8850 observations, resulting in a data set called JM1-8850 [ 17 ].
 rics, Halstead Metrics (both derived and basic), Line Count and Branch Count Metrics. See [ 6 ] for a discussion on types, characteristics, and use of software metrics. The 8 derived Halstead metrics were not used in our study, leaving 13 software metrics remaining. These 13 software product metrics are shown in Ta b l e 1 , where LOC represents the lines of code in a program module. nfp (not fault prone) and fp (fault prone). An instance was labeled fp if it contained at least one fault and nfp otherwise. The JM1-2445 data set used in our case study was constructed from JM1-8850 using clustering techniques and expert input.
 vised clustering technique ( k -means) as part of a related study [ 29 ]. Descriptive statistics such as the mean and standard deviation were computed for both the en-tire data set and each cluster. The software engineering expert then labeled ( nfp and fp ) those clusters (and consequently, instances within the clusters) for which he was completely confident. The expert-assigned labels of instances in those clus-ters were then matched with their actual labels, and instances that did not have matching labels were removed from the clusters. These instances were inspected by the expert, and it was verified that their class labels did not match the class labels the instances should have had based on their attribute values. As a result, 2445 instances with no class noise remained in the clusters labeled by the expert. We denote this data set as JM1-2445, which consists of 2210 nfp instances and 235 fp instances. JM1-2445 is a real-world data set that was preprocessed (using clustering and expert input) to remove any instances with class noise. Instances with naturally occurring attribute noise, however, remain in the JM1-2445 data set. 4.2 CCCS data set The CCCS data set is a large military command, control, and communications system written in Ada [ 12 ]. CCCS contains 282 instances, where each instance is an Ada package consisting of one or more procedures. CCCS contains 8 indepen-dent variables or attributes along with an additional attribute nfaults indicating the number of faults attributed to the module during the system integration and test phases and during the first year of deployment. The software metrics in the CCCS data set are listed in Table 2 . One hundred and forty-six of the 282 modules in the CCCS data set contain no faults. 5 Empirical results 5.1 Case Study 1: JM1-2445 data set As mentioned earlier, the outlier detection algorithm (DM) derived in [ 23 ]was compared to PANDA. D k ( p ) is defined as the distance from point p to its k th nearest neighbor. The instances with the largest values for D k ( p ) for a given value of k are considered outliers. In DM, the number of nearest neighbors, k ,isapa-rameter that is set by the user. In this case study, all variables or attributes were standardized, and distance was calculated using the L 2 (Euclidean) norm. The DM algorithm was executed 10 times, with a different value for k from 1 to 10 at each iteration. The final ranking was determined as the median ranking of the 10 iterations to minimize any potential bias when a single value for k is used. introducing additional noise or using a lucky/unlucky binning size), PANDA was executed 11 times using the SUM method, with 5, 10, 15, 20, 25, 30, 40, 50, 60, 75, and 100 bins. Similar to the procedure adopted for the DM algorithm, the final ranking by PANDA was determined as the median ranking over the 11 iterations (i.e., 11 different number of bins). The instances in JM1-2445 were then rank ordered independently by both algorithms from most noisy to least noisy. The instance ordering was then presented to an expert with more than 15 years of expertise in the software engineering domain. The domain expert manually inspected the rankings produced by DM and PANDA. It should be noted that the expert only analyzes the results obtained from PANDA and DM and does not influence the obtained results.
 algorithms) are presented in Table 3 . These 250 instances are approximately 10% of the instances in the JM1-2445 data set and represent the mostly likely data noise. Based on the software engineering expert, 10% of the instances was enough to determine the noise detection ability of the two algorithms. For another data set, a different percentage of instances may represent the most likely noise. algorithms and identified four types of instances: outlier , noise , exception ,and typical . PANDA does not differentiate between these different types of instances but simply ranks them from most to least noisy. During post-analysis, the domain expert is the one who categorizes the top 250 instances into the four groups for the sole purpose of evaluating the effectiveness of PANDA and DM.
 within the tail of the general distributions of those attributes, and is a rare occur-rence in a data set. An instance is considered as noise when one or more of its attributes have incorrect or corrupted values. An exception is an instance having attributes with values that do not follow the general distributions of those attributes but can occur (albeit rarely) with a given data set. Without input from a domain expert, detecting exceptions is very difficult because they are likely to be flagged as noise. A typical or normal instance is one with attribute values that occur within the general distributions of the given data set. It should be noted that from a noise detection point of view, there is no inspection benefit in presenting typical in-stances to a domain expert for review.
 termined by the expert after manual inspection are shown in Table 3 . The results are broken down into interval ranges for each algorithm. The first group repre-sents the 25 highest ranking instances for both PANDA and DM, respectively. For this group, the expert determined that there were 13 errors, 7 exceptions, and 5 outliers detected by PANDA. The DM algorithm detected 16 errors, 7 outliers, 1 exception, and 1 typical instance among the 25 most suspicious instances. Also included are the cumulative results for observations 1 X 125 and 126 X 250 for both algorithms.
 108 errors, 10 exceptions, 7 outliers, and no typical instances, while DM detected 101 errors, 5 exceptions, 16 outliers, and 3 typical instances. For the top 5% of instances, the DM algorithm detects a few normal instances, while PANDA does not detect any normal instances. If the top 10% suspicious instances identified by both algorithms are compared, PANDA X  X  detection included 220 errors, 11 ex-ceptions, 12 outliers, and 7 typical instances, while DM X  X  detection included 173 errors, 10 exceptions, 17 outliers, and 50 typical instances. We observe that the DM algorithm misclassifies many typical instances as noisy when compared to PANDA.
 extreme by both PANDA and DM, i.e., the least suspicious instances in the data set. It would be expected that these instances are not of practical interest and would generally be labeled by the expert as typical. When using either PANDA or DM, the instances ranked as least suspicious were in all cases typical and of no interest to the expert.
 that fell into each interval range. From a software engineering point of view, a fp instance is likely to have relatively large values for its attributes, i.e., software metrics. In contrast, a nfp instance is likely to have relatively lower values for its attributes. The intuitive assumption is that the more complex a software program, the more likely it is fp .
 the second shows the distribution for the remainder of the top 250 instances. Over-all, 124 of the 125 instances that DM ranked in the top 125 most suspicious in-stances belonged to the fp group, and only 1 belonged to the nfp group. In con-trast, for PANDA only 74 of the 125 were fp , while 51 were nfp . For instances ranked by DM between 126 and 250, the vast majority (105 out of 125) of them are fp .
 far away from the main distribution of the data set. The fp group has observations which have large values for all of the software attributes, and hence are more likely to look like outliers. PANDA (while favoring the fp group for the top 5%) is much more balanced with respect to the class variable. Comparatively speaking, PANDA provides a more favorable noise detection result, since it is able to identify noisy instances in both classes without heavily weighing one over the other. The DM algorithm is more likely to detect normal fp instances.
 for different rank intervals is shown in Table 6 .Thistableshowsthedistribution for up to the top 2000 instances ranked by the two algorithms. Recall the JM1-2445 data set consists of 2210 nfp and 235 fp instances. The DM algorithm identifies all the fp instances within the top 400 instances. In contrast, among the top 400 instances ranked by PANDA, only 151 are fp . Even among the top 2000 instances ranked by PANDA, there are two fp instances still remaining. These results further demonstrate that PANDA provides a more favorable noise detection result than DM does.
 at various levels, starting from the top 25 instances to the top 1000 instances. Among the top 25 suspicious instances identified by both algorithms, 40% are in common. Similarly, among the top 125 suspicious instances identified by both algorithms, 46.4% are in common. For the top 1000 suspicious instances, repre-senting 40.9% of the data set, the overlap is still only 76%. This data confirms that there is a significant difference between the rankings produced by these two algorithms.
 we conclude that PANDA is detecting more noise and fewer outliers and typical instances than DM. Another interesting result of this analysis is that PANDA de-tects exceptions, i.e., those rare instances that lie outside the general distributions of the data set. Even if they do not contain errors, such instances are often of inter-est to the expert because they may represent potentially new and interesting cases. PANDA is also more balanced in regards to the class of the detected instances. DM was unable to detect many of the nfp instances that were noise, since the distance measure is dominated by large values, and fp instances generally have relatively larger attribute values when compared to the nfp instances. 5.2 Case Study 2: CCCS data set This case study examines the noise detection capability of both PANDA and DM on another real-world data set called CCCS. In contrast to data set JM1-2445, which was considered in Sect. 5.1 , CCCS did not undergo any data preprocessing prior to the application of PANDA. Preprocessing on JM1-2445 was done to elim-inate class noise from the data set, and JM1-2445 is a relatively cleaner data set than the one from which it was derived, JM1-8850. More specifically, JM1-2445 does not contain any instances with class noise, but it does contain instances with attribute noise. CCCS, on the other hand, is a completely unaltered and unpro-cessed real-world data set, which contains real-world noise in both the dependent and independent variables.
 rank equal to the median rank over these 7 iterations. Multiple bin sizes were used to reduce the possibility of introducing anomalies that might be caused by the partitioning process into the data set. Compared to JM1-2445, different bin sizes were used for CCCS, since the size of the data sets are drastically different (282 instances in CCCS and 2445 instances in JM1-2445). The same parameters used for DM in Case Study 1 were also used in this case study. Specifically, DM was executed 10 times with a different value for k from 1 to 10 at each iteration. The final ranking produced by DM was the median rank over these 10 iterations. instances from the CCCS data set (approximately 10% of the data set) as identi-fied by PANDA or DM. PANDA identifies many noisy instances as well as a few exceptions and outliers, while DM detects a larger number of outliers (12 vs. 5 for PANDA) but fewer noisy instances (18 vs. 21 for PANDA).
 tributed to the module during the system integration and test phases and during the first year of deployment. To obtain a data set with a binary class variable in-dicating whether each module should be considered fault prone ( fp ) or not fault prone ( nfp ), a threshold  X  can be set on the number of faults. Any module with more than  X  faults is considered fp , while the remaining modules are consid-ered nfp . When calculating the instance rankings created by both PANDA and DM, the original attribute nfaults was used. For comparing the class distribution of the two techniques in Tables 9 and 10 , we use the binary class variable with  X  = 3. Any module with more than three faults was considered fp . This particu-lar threshold was chosen based on previous empirical studies using the CCCS data set [ 12 ].
 as identified by either PANDA or DM. PANDA detects 10 fp instances and 20 nfp instances, while DM detects 26 fp instances and 4 nfp instances. Table 10 displays the cumulative distribution of fp instances for PANDA and DM. DM has ranked all 55 fp instances in the 125 most suspicious instances, while even at rank 250, PANDA has detected only 51 of the 55 fp instances. PANDA has a much more favorable class distribution related to the detected instances when compared to DM.
 data set. The overlap among the 20 most suspicious instances identified by PANDA and DM is 40%, while among the 100 most suspicious instances (which is approximately 35% of the data set) the overlap is 63%. This data confirms that PANDA and DM are detecting different instances. 6Conclusion This paper focuses on the problem of data quality in measurement data. Decisions made by data mining models are invariably affected by the quality of the under-lying data. If the quality of the data is poor, a classification model trained using that data is likely to yield incorrect estimations. While the problem of detecting instances with class noise has seen increased attention, very limited focus has been placed on detecting instances having attribute noise.
 designed to detect instances that contain attribute noise. The output of PANDA is a list of instances ordered from the most noisy to least noisy. PANDA is attractive because it can be used for noise detection in the absence of the dependent variable data, i.e., class labels. Furthermore, to our knowledge PANDA is the first algorithm to address the problem of detecting instances with attribute noise without the need of class label data.
 case study considers software measurement data from a NASA project which is preprocessed to eliminate instances that contain class noise. The second case study uses another software measurement data set which did not undergo any data preprocessing before the application of our technique. Note that many additional experiments with different data sets were conducted and similar empirical conclusions were obtained; however, their presentation is omitted due to space limitations. The proposed algorithm is automated in a software application and can be applied to data sets of different sizes and from other domains. We compare the performance of PANDA with an outlier detection algorithm. A software engineering expert inspected the instances detected by both procedures to determine whether they are noise, exceptions, or outliers. The algorithms were then evaluated for effectiveness in detecting the noisy instances.
 instances than the outlier detection procedure DM. Additionally, PANDA provides more balanced noise detection among the fp and nfp classes, while the instances identified by the DM algorithm were largely fp . It should be noted that from a software engineering point of view, the attribute values for the fp instances are generally larger than the corresponding attribute values for the nfp instances, which is why DM is more likely to detect fp instances. have often injected randomly generated noise in order to evaluate a given noise detection technique. However, the data set into which noise is injected is often not verified to be noise-free. In the first case study, we utilize a data set without class noise which does, however, contain attribute noise. The second case study uses a data set that received no special treatment before the application of PANDA. The instances in these data sets contain the types of noise that are encountered in real-world data. A software engineering expert is used to evaluate the effectiveness of our algorithm on real-world data sets that contain real-world (as opposed to injected) noise.
 tection that can be used with data sets of various domains. An investigation of PANDA by including the class attribute in its analysis can be compared with other supervised noise detection approaches. Whether the class label is treated like the other attributes or as a special (or weighted) attribute needs investigation. An-other important issue that needs further investigation is to determine the attributes within an instance that caused the instance to be classified as noisy. As with any empirical study, additional case studies will further validate the effectiveness of the proposed noise detection approach.
 References
