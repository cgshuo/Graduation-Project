 Hisashi Kashima hkashima@jp.ibm.com Yuta Tsuboi yutat@jp.ibm.com Sequence labeling is one of the important problems widely seen in the areas of natural language processing (NLP), bioinformatics and Web data analysis. Label-ing problems generalize supervised classification prob-lems, since not only the label of one hidden variable, but the labels of a set of hidden variables are pre-dicted. Labeling problems for sequences have been extensively studied for years. However, there has been almost no significant work on labeling more general structured data such as trees and graphs. In this pa-per, we consider kernel-based approaches for labeling problems with general structured data.
 Conventionally, in sequence labeling problems, gener-ative models such as Hidden Markov Models (HMMs) have been used. However, they tend to need a lot of data since they target the more difficult problems of es-timating joint probabilities of observable variables and hidden variables. Also, they can not handle overlap-ping features naturally, since observable variables must be independent of each other given the labels of the hidden variables. Recently, conditional models such as Maximum Entropy Markov Models (MEMMs) (Mc-Callum et al., 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001) are attracting consid-erable attention, since they are more suitable for the purpose of predicting the labels of hidden variables given the labels of observable variables. Recently, Collins (2002) proposed the Hidden Markov (HM) Per-ceptron, a more efficient sequence labeling algorithm based on perceptrons as an alternative to CRFs and MEMMs. SVM-based algorithms (Altun et al., 2003c) and boosting-based algorithms (Altun et al., 2003a) have also been proposed.
 In some labeling problems, combinations of local fea-tures such as bi-grams are not sufficient for modeling long-distance dependencies such as idioms in NLP, or motifs in bioinformatics. However, in all of the meth-ods proposed so far, there is a fundamental problem that they can handle only local features considering small numbers of hidden variables. Label prediction is based on the Viterbi decoding using dynamic program-ming, which requires exponential time with respect to the maximum number of hidden variables included in one feature. Although HM-Perceptrons and HM-SVMs have dual form representations, features must be explicitly considered in label prediction. Of special interest here, Kakade et al. (2002) proposed a point-wise log-loss objective function for CRFs and MEMMs that aims to maximize the number of individually cor-rect labels. The important point of their objective function is that it does not need to predict the la-bels for the entire sequence all at once. In this paper, inspired by the idea of a pointwise log-loss function, we propose marginalized labeling perceptrons that solve the label prediction problem by using the marginalized feature vectors. Marginalized labeling perceptrons re-alize point-wise label prediction, and are fully kernel-ized by using marginalized kernel functions (Tsuda et al., 2002) with long-distance dependencies among hidden variables.
 Also, in this paper, we propose several marginal-ized kernels used in the kernel marginalized labeling perceptron for labeling various structured data such as sequences, trees, and graphs. Recently, kernel-based methods for classification of structured data kernels (Lodhi et al., 2002; Leslie et al., 2002), tree kernels (Collins &amp; Duffy, 2002; Kashima &amp; Koy-anagi, 2002), and graph kernels (Kashima et al., 2003; G  X artner et al., 2003). Most of them are based on fea-ture vectors composed of the counts of the substruc-tures such as subsequences, subtrees, or subgraphs, to incorporate long-distance dependencies. As the di-mensionality of feature vectors is typically very high (possibly infinite), they adopt efficient procedures such as suffix trees, dynamic programming, or matrix com-putation to avoid explicit enumeration of the fea-tures. By extending these kernels, we propose several marginalized kernels for labeling sequences, trees, and graphs, and efficient algorithms for computing them. Finally, we show some promising results for experi-ments on sequence labeling and tree labeling for real-world tasks which require structure information. Our contributions in this paper are twofold: (i) A fully-kernelized labeling learning algorithm that solves the problem in label prediction for handling features of ar-bitrary size. (ii) Fast marginalized kernels for labeling sequences, trees and graphs. The labeling problem is defined to be the problem of learning a function that maps the observable vari-ables x = ( x 1 , x 2 , . . . , x T ) to the hidden variables y = ( y 1 , y 2 , . . . , y T ), where each x t  X   X  x and each y t For instance, in part-of-speech tagging, x t represents the t -th word, and y t represents the part-of-speech tag of the t -th word (Figure 1). The learner may exploit
Collins (2002) introduced the HM-Perceptron, a dis-criminative learning algorithm for sequence labeling as an alternative to CRFs and MEMMs. It can be trained efficiently by processing the training examples one by one. Its ability was shown to be comparable to CRFs (Altun et al., 2003b). The key idea of the HM-Perceptron is to interpret the mapping as a bi-nary classification, i.e.  X  x T  X   X  y T  X  X  +1 ,  X  1 } . Let F be a set of features for vector representation of ( x , y ). Let  X  f ( x , y ) be the number of times a feature f  X  F appears in ( x , y ), and  X ( x , y ) be their vector form. Given x , the HM-Perceptron outputs the prediction  X  y according to the current weights of the features: where w f is the weight of a feature f , and w is the vec-tor form of the weights. Under a first-order Markov assumption, two types of features, such as pairs of an observable variable and a hidden variable (Figure 1(b)), and pairs of two hidden variables (Figure 1(c)) are used.
 Starting at w = 0, the weights are updated by using Equation (1) is also written in a dual form as follows by using the weights of the examples  X  .  X  y = argmax Starting with  X  = 0 , the updating rules are rewritten as negative example since it acts like an negative exam-ple.

Now, we discuss using features that consider long-distance dependencies in the HM-Perceptron. Some-times, bi-gram features (Figure 1(b)(c)) are not suffi-cient for modeling long-distance dependencies such as idioms in NLP, or motifs in bioinformatics. What if we employ longer features such as in Figure 2? Let us assume that we allow features of lengths up to d . position t 1 to position t 2 of x , and y t 2 t cordingly. Taking into account that all features depend on at most d consecutive hidden variables,  X ( x , y ) is decomposed as  X ( x , y ) =  X ( x d 1 , y d 1 )  X   X ( x d 2 , y d 2 ) Substituting Equation (4) into Equation (1), the argmax operation in Equation (1) is performed via the following dynamic programming. max s ( y t , . . . , y t + d  X  1 ) = max However, the computation time in each step of the re-cursion grows exponentially with respect to the maxi-mum feature length d . Even the dual form (3) still has the same problem, because we have to evaluate all of the features explicitly when predicting labels. This is a serious problem when we want to employ kernels with arbitrarily long features (possibly of infinite length). Also, CRFs have the same problem since they employ dynamic programming procedures based on the same decomposition in learning and prediction. In addition, even the decomposition (4) is not applicable to kernels enumerating the occurrences of features when allowing gaps (Lodhi et al., 2002), since a feature of length of d can occur over an interval of length longer than d . Collins (2000) avoids this problem by a reranking ap-proach where a pre-trained labeler using local features generates an affordable number of candidates for y , and the candidates are reranked by using global fea-tures. However, the correct answer is not guaranteed to be contained in the candidates based on the local features. We now propose a new kernel-based labeler that solves the problems of the previous section. Usu-ally, in MEMMs and CRFs, model P s ( y | x ) is trained by maximizing the sum of the log-likelihood P label an entire sequence correctly. On the other hand, Kakade et al. (2002) noticed that it suffices to maxi-mize the number of individually correct labels in many labeling tasks. They proposed to use the marginalized P ( y | x ) over all possible assignments of labels for the hidden variables with the t -th hidden variable fixed as  X  y  X   X  y .
 The pointwise objective function y sequential log-loss function (Altun et al., 2003b). The important point is that label prediction is performed in a pointwise manner since P p depends only on y t . We combine this idea with perceptron-based labelers. We propose a marginalized labeling perceptron defined as follows. Note that the pointwise label prediction works since the argmax operation depends only on  X  y t . The main idea is that, when y t is predicted, the original out-put  X  w ,  X ( x , y )  X  is marginalized over all possible as-signments of labels for the hidden variables with the t -th hidden variable fixed as  X  y t . P ( y | x ) is some prior distribution over hidden variables that might be pre-trained probabilistic models such as MEMMs or CRFs, or might be designed manually. Since P y : y t =  X  y t P ( y | x ) X ( x , y ) can be considered as a new feature vector, the weight vector w is updated by w Next, we derive a dual form of the marginalized label-ing perceptron, and obtain a fully-kernelized labeler. Let  X  f ( x , y ; t ) be the number of times a feature f appears in ( x , y ) with including the t -th position of ( x , y ), and  X  f ( x , y ; t ) be the vector form. First, we rewrite the primal form (5) as follows by adding terms that do not depend on  X  y t . The update rule is then rewritten as w By introducing example weights  X  , w is rewritten as a linear combination, w = Finally, substituting Equation (8) into Equation (7), we obtain the kernel marginalized labeling perceptron ,  X  y = argmax where K ( x ( j ) , x ,  X , t,  X  y  X  ,  X  y t ) = (10)
X
 X  is called the marginalized kernel since the kernel is marginalized over all possible assignments of labels for hidden variables with fixed labels at  X  and t . The al-gorithm is described in Figure 3. In this section, we propose several instances of the marginalized kernel (10) for labeling sequences, or-dered trees, and directed acyclic graphs. Recently, kernels for structured data such as sequences, trees, and graphs have been studied actively (G  X artner, 2003). We extend these kernels for labeling problems. In all kernels, for the sake of simplicity, we suppose that the prior P ( y | x ) can be decomposed as However, the following discussion can be generalized to exploit more general models like CRFs. In the re-mainder of this section, we use K (  X , t,  X  y  X  ,  X  y t the kernel function given i and j . The important point in computing our kernels is that all features can be con-structed by combining smaller features. For instance, in Figure 4, suppose that the feature (b) appears with its rightmost position at t , and the feature (c) ap-pears with its leftmost position at t . Then, the fea-ture (a), the combination of the two features, appears with its second variable at t . All the marginalized ker-nels that we propose in this paper are decomposed into the upstream kernels K U (  X , t ), the downstream kernels K
D (  X , t ), and the pointwise kernels K P (  X , t,  X  y  X  ,  X  y follows.
 K (  X , t,  X  y  X  ,  X  y t ) = K U (  X , t )  X  K P (  X , t,  X  y K U (  X , t ) = K D (  X , t ) = K P (  X , t,  X  y  X  ,  X  y t )=  X P
P where y U (  X  ) is the set of the hidden variables lying upstream of  X  , y D (  X  ) is the set of the hidden variables lying downstream of  X  , and y U (  X  )  X  y D (  X  ) = { y  X  K
U (  X , t ) and K D (  X , t ). Although explicit computation of the marginalization in K U and K D is prohibitive, we can efficiently compute them by using dynamic pro-gramming in many cases. The major advantage of the decomposition is that, given the i -th example and the j -th examples, marginalized kernels for all possible val-4.1. Sequence Labeling Now we introduce kernels for sequence labeling. To consider the long-distance dependencies in sequences, we define each feature as a consecutive pair of a hidden variable and an observable variable of arbitrary length, as in Figure 2. Since the lengths of the features are not restricted, and features of various lengths are mixed, we vary the weight  X  f according to the length of the feature f by using the parameter c &gt; 0. If the length of the feature is d , and if we observe the feature N times in ( x , y ),  X  f ( x , y ) is defined as N  X  c d . K
U (  X , t ) for labeling sequences is the kernel that con-siders only the appearances of features with their right-most positions at  X  and t , and which is marginalized over all possible assignments for y U (  X  ) and y U ( t ). Similarly, K D (  X , t ) is the kernel that considers only the appearances of features with their leftmost posi-tions at  X  and t , and which is marginalized over all possible assignments for y D (  X  ) and y D ( t ). The algorithm consists of two dynamic programming loops, one for K U , and the other for K D , just like the forward-backward algorithm.
 where K D ( | T ( j ) | + 1 , t ) = 0. K P is computed as follows. = where k x and k y are functions that return 1 when the arguments are identical, and return 0 elsewhere. How-ever, we can replace them with kernels between the two variables. Apparently, the time complexity of comput-4.2. Tree Labeling The kernel for labeling ordered trees (Figure 5) is based on the labeled ordered tree kernel (Kashima &amp; Koyanagi, 2002). We allow arbitrary tree-structured features such as in Figure 5. By using the mixing parameter c as in the sequence labeling,  X  f ( x , y ) is defined to be N  X  c d where N is the number of times the feature f appears as subgraphs in ( x , y ), and d is the size of f . K U (  X , t ) for labeling trees is the kernel that considers only the appearances of features with their leaf positions at  X  and t , and that is marginal-ized over all possible assignments for y U (  X  ) and y U ( t ). Similarly, K D (  X , t ) is the kernel that considers only appearances of features with their root positions at  X  and t , and that is marginalized over all possible as-signments for y D (  X  ) and y D ( t ).
 The algorithm consists of two dynamic programming loops, one for K U , and the other for K D , just as in the inside-outside algorithm. Let ch (  X ,  X  ) be the index of the  X  -th child node of the  X  -th node, pa (  X  ) be the index of the parent node of the  X  -th node, # ch (  X  ) be the number of the child nodes of the  X  -th node, and chID (  X  ) be the index that means the  X  -th node is the chID (  X  )-th child of its parent node.
 Similar to the recursive computation of the labeled or-dered tree kernel (Kashima &amp; Koyanagi, 2002), K D is computed by dynamic programming in a post-order traversal by using K
D (  X , t ) = c 2 k ( x ( j )  X  , x where S F is also defined recursively as follows, S where S F (  X , t, 0 , u ) = S F (  X , t,  X , 0) = 1. Intuitively, S
F (  X , t,  X , u ) is the sum of the contributions of all the way of matching between the nodes indexed from ch (  X , 1) to ch (  X ,  X  ) and the nodes indexed from ch ( t, 1) to ch ( t, u ).
 At the same time, for the sake of the computation of K
U , we also define S B (  X , t,  X , u ), the sum of the contri-butions of all the way of matching between the nodes indexed from ch (  X ,  X  ) to ch (  X , # ch (  X  )) and the nodes indexed from ch ( t, u ) to ch ( t, # ch ( t )), as S 1) = 1.
 K
U is computed by dynamic programming in a pre-order traversal. K U (  X , t ) is computed by combining the upstream kernel of the parents and the down-stream kernels of the siblings (Figure 6), K
U (  X , t ) = c 2 k ( x ( j )  X  , x Note that the pointwise kernel K P is the same as in Equation (16). A similar analysis to that of Kashima and Koyanagi (2002) can show that the time complex-4.3. Graph Labeling Finally, we propose a kernel for labeling directed acyclic graphs (DAGs) (Figure 7). Unfortunately, it has been shown to be NP-hard to use arbitrary graph-structured features (G  X artner et al., 2003). There-fore, we propose a marginalized kernel using directed path features like Figure 2, based on the DAG ker-nel (Sch  X olkopf et al., 2004).
 K
U (  X , t ) for labeling DAGs is the kernel that considers only the appearances of features with their end posi-tions at  X  and t , and which is marginalized over all possible assignments for y U (  X  ) and y U ( t ). Similarly, K
D (  X , t ) is the kernel that considers only the appear-ances of features with their start positions at  X  and t , and which is marginalized over all possible assignments for y D (  X  ) and y D ( t ). As in the kernels for sequence labeling, we can write K U and K D recursively, K
U (  X , t ) = c 2 k ( x ( j )  X  , x K
D (  X , t ) = c 2 k ( x ( j )  X  , x where Pa (  X  ) and Ch (  X  ) are the set of in-dices of the parent nodes and the child nodes of the  X  -th node, respectively. Note that the pointwise kernel K P is the same as Equa-tion (16). The time complexity of computing this kernel is O max  X ,t | Ch (  X  ) || Ch ( t ) | ) Finally, we will show the result for experiments on NLP data to assess the importance of large structural features in a real-life situation. We conducted two ex-periments on Named Entity Recognition (NER) and Product Usage Information Extraction tasks. We com-pared the performance of the kernel marginalized la-beling perceptron with the HM-Perceptrons, which is limited to smaller features in nature. 5.1. Named Entity Recognition NER is a kind of information extraction task that deals with identifying proper names such as person names and organization names in sentences. We used a sub-corpus consisting of the first 300 sentences (8,541 to-kens) from the Spanish corpus provided for the Special Session of CoNLL2002 on NER. The task is to label each word with one of the nine labels, i.e. |  X  y | = 9, that represents the types and boundaries of named en-tities, including a dummy label for non-named entities. The kernel between two observable labels is designed by using words and their spelling features, which are described in Altun et al. (2003b) as the S2 features. In both algorithms, it is combined with a second de-gree polynomial kernel. A window of size 3 is also used in HM-Perceptrons according to (Altun et al., 2003c). For the proposed perceptron algorithm, we used the se-quence kernel withthe mixing parameter c = 1 which, was determined in preliminary experiments. The prior over hidden variables is modeled as a uniformed dis-tribution, i.e. P ( y t | x t ) = 1 / |  X  y | . The performances are evaluated according to the labeling accuracies (in-cluding correct answers for dummy labels), precision and recall of named entity labels, and the F1 measure which is the harmonic mean of the precision and recall. Table 1 shows the results of the NER task in a 3-fold cross-validation. The values in parentheses represent the standard deviations on each cross-validation. The proposed perceptron algorithm with the sequence ker-nel outperforms the other method. Note that the F1 measure score is a more suitable measure for evaluating NER tasks than the labeling accuracy because the ma-jority of the annotated labels are dummy labels. The good performance of the proposed sequence kernel is due to the nature of NER tasks. NER tasks are known as one of the NLP problems which require considering a wide context. For instance, the same phrase  X  X hite house X  can be a location or an organization entity in different contexts. This empirical result supports the advantages of the fully-kernelized labeler that can han-dle rich contextual information efficiently without any manual selection of an appropriate feature size. 5.2. Product Usage Information Extraction Product Usage Information Extraction is a special case of NER tasks which deals with extracting information about the usage of products from a sales log written by sales representatives. We used 184 sentences (3,570 to-kens) from an annotated sales log written in Japanese. This task aims to identify the product name (p), its vendor (v), the number of products bought (n), and the reason for buying (r) linked to a classification of the customer state that indicates whether the costomer is already using (D), wishes to use (P), or rejects (R) using the products. In total, there are |  X  y | = 12 dif-ferent labels including a dummy label (O).
 All of the Japanese sentences were previously seg-mented into words, and these words were annotated with the part-of-speech and base form. In all of the al-gorithms, we designed the kernel between two observ-able labels by using words, part-of-speech tags, base forms, and character type features. We used it by combining with the second degree polynomial kernel. In this task, we use not only the sequence kernel for the proposed perceptron, but also the tree kernel. For the tree kernel, we used (lexicalized) dependency trees that represent linguistic structures of the sentences in terms of the dependencies between words. Figure 8 shows an example of a dependency tree in English. To build the dependency trees for the data set, we used a For both kernels, the mixing parameter is c = 0 . 8. Table 2 shows the result of the Product Usage Informa-tion Extraction task in 3-fold cross-validation. The pa-rameter settings and the evaluation measures are the same as in the NER task. Again, we can see the advan-tage of the proposed perceptron algorithm against the HM-Perceptrons. Furthermore, the tree kernel utiliz-ing dependency trees shows higher performance than the sequence kernel, which elaborates the advantage of using structured information. In this paper, we introduced an efficient fully-kernelized learning algorithm for labeling structured data such as sequences, trees, and graphs. Our ap-proach can handle large features including arbitrary numbers of variables by using the pointwise label pre-diction inspired by Kakade et al. (2002), and the marginalized kernels. We also proposed several in-stances of the marginalized kernels for labeling se-quences, ordered trees and directed acyclic graphs. In the preliminary experiments on information extraction tasks using real-world data, our approach was shown to be promising.
 As a related work, Weston et al. (2003) have proposed another general framework that aims at learning the direct mapping from  X  x ( x ) to  X  y ( y ) with arbitrary kernels. However, features depending on both x and y are not used. Also, they provide no efficient way to decode  X  y ( y ) into y .
 One possible extension of this research is to incor-porate more complicated prior distributions such as MEMMs and CRFs in the marginalized kernels, al-though we employed a very simple form of prior dis-tribution in this paper. It is an interesting question whether our approach boosts such probabilistic mod-els.
 Another extension might be an SVM version of our ap-proach. Since HM-SVMs (Altun et al., 2003c) consider an entire label sequence as an example, maximization of the margin between correct label sequences and the second best label sequences is not always acceptable in problems where each sequence has some difficult la-bels. On the other hand, our approach avoids this problem by considering a label at each position as an individual example.
 Altun, Y., Hofmann, T., &amp; Johnson, M. (2003a). Dis-criminative learning for label sequences via boost-ing. Advances in Neural Information Processing Sys-tems 15 . Cambridge, MA: MIT Press.
 Altun, Y., Johnson, M., &amp; Hofmann, T. (2003b). In-vestigating loss functions and optimization methods for discriminative learning of label sequences. Pro-ceedings of the Conference on Empirical Methods in Natural Language Processing .
 Altun, Y., Tsochantaridis, I., &amp; Hofmann, T. (2003c).
Hidden Markov support vector machines. Proceed-ings of the Twentieth International Conference on Machine Learning .
 Collins, M. (2000). Discriminative reranking for natu-ral language parsing. Proceedings of the Seventeenth
International Conference on Machine Learning (pp. 175 X 182). San Francisco, CA: Morgan Kaufmann. Collins, M. (2002). Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. Proceedings of the Con-ference on Empirical Methods in Natural Language Processing .
 Collins, M., &amp; Duffy, N. (2002). Convolution kernels for natural language. Advances in Neural Informa-tion Processing Systems 14 . Cambridge, MA: MIT Press.
 G  X artner, T. (2003). A survey of kernels for structured data. SIGKDD Explorations , 5 , S268 X  X 275.
 G  X artner, T., Flach, P., &amp; Wrobel, S. (2003). On graph kernels: Hardness results and efficient alternatives. Proceedings of the Sixteenth Annual Conference on Computational Learning Theory .
 Kakade, S., Teh, Y. W., &amp; Roweis, S. (2002). An alter-native objective function for Markovian fields. Pro-ceedings of the Nineteenth International Conference on Machine Learning (pp. 275 X 282). San Francisco, CA: Morgan Kaufmann.
 Kanayama, H., Torisawa, K., Mitsuishi, Y., &amp; Tsu-jii, J. (2000). A hybrid Japanese parser with hand-crafted grammar and statistics. Proceedings of the 18th International Conference on Computational Linguistics (pp. 411 X 417).
 Kashima, H., &amp; Koyanagi, T. (2002). Kernels for semi-structured date. Proceedings of the Nineteenth In-ternational Conference on Machine Learning (pp. 291 X 298). San Francisco, CA: Morgan Kaufmann. Kashima, H., Tsuda, K., &amp; Inokuchi, A. (2003).
Marginalized kernels between labeled graphs. Pro-ceedings of the Twentieth International Conference on Machine Learning . San Francisco, CA: Morgan Kaufmann.
 Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data. Proceedings of the Eighteenth International Conference on Machine Learning (pp. 282 X 289). San Francisco, CA: Morgan Kaufmann.
 Leslie, C., Eskin, E., &amp; Noble, W. S. (2002). The spectrum kernel: A string kernel for SVM protein classification. Proceedings of the Pacific Symposium on Biocomputing (pp. 566 X 575). World Scientific. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristian-ini, N., &amp; Watkins, C. (2002). Text classification using string kernels. Journal of Machine Learning Research , 2 , 419 X 444.
 McCallum, A., D.Freitag, &amp; Pereira, F. (2000). Max-imum entropy Markov models for information ex-traction and segmentation. Proceedings of the Seventeenth International Conference on Machine
Learning (pp. 591 X 598). San Francisco, CA: Mor-gan Kaufmann.
 Sch  X olkopf, B., Tsuda, K., &amp; Vert, J.-P. (Eds.). (2004). Kernel Methods in Bioinformatics . Cambridge, MA: MIT Press, to appear.
 Tsuda, K., Kin, T., &amp; Asai, K. (2002). Marginalized kernels for biological sequences. Bioinformatics , 18 , S268 X  X 275.
 Weston, J., Chapelle, O., Elisseeff, A., Sch  X olkopf, B., &amp; Vapnik, V. (2003). Kernel dependency estimation.
Advances in Neural Information Processing Systems
