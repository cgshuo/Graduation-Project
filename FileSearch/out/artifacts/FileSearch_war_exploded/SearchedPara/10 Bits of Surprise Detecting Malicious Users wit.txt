 Malicious users are a threat to many sites and defending against them demands innovative countermeasures. When malicious users join sites, they provide limited information about themselves. With this limited information, sites can find it difficult to distinguish between a malicious user and a normal user. In this study, we develop a methodology that identifies malicious users with limited information. As infor-mation provided by malicious users can vary, the proposed methodology utilizes minimum information to identify malicious users. It is shown that as little as 10 bits of information can help greatly in this challenging task. The experiments results verify that this methodology is effective in identifying malicious users in the realistic scenario of lim-ited information availability.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Malicious User Detection; Minimum Information; Information Verification; Behavior Analysis Social media sites are inundated with malicious users. In June 2012, Facebook reported that 8.7%  X  or 83 million  X  of its user accounts are fake [35]; that is roughly the size of Egypt X  X  population and larger than the population of 230 countries in the world [37]. Facebook also reported that of that 8.7%, 1.5% are  X  X ndesirable X  accounts that are created for malicious purposes [35]. Twitter faces similar challenges. In its security filings, Twitter claims that 5% of its users are fake [10]; however, researchers estimate the percentage of its fake accounts to be as high as 10% [10]. These fake accounts are mostly sold on black market for as low as $0 . 05 and are used for malicious activities [10].
 c  X  Malicious accounts may be created for different purposes. According to Cao et al. [4], some malicious accounts are created for profitable activities, such as click fraud, iden-tity fraud, and malware distribution. Others are created for social purposes such as pranks, stalking, cyberbullying, or identity concealing. Online service providers find detecting and subsequently, suspending malicious accounts vital in or-der to protect their normal users against external threats.
Detecting malicious accounts dates back to the onset of so-cial media. Comprehensive feature-based techniques, human-in-the-loop approaches, or techniques that use social-graphs are devised (see a review in Section 2). These techniques assume that a good amount of information about malicious users has been gathered. This information includes (1) the content that malicious users generate, (2) the users they be-friend, or (3) the malicious activities they have exhibited. In short, their content , links , or activities . However, malicious users often do not have an incentive to generate content or befriend others and detecting them after they have ex-hibited malicious activities is not useful. Hence, malicious users need to be detected using their often limited content or link (i.e., friends) information. Even the limited infor-mation that malicious users provide can vary. Therefore, to detect malicious users with different levels of information, one needs to be able to detect malicious users even when there is minimum information available.

In this paper, we aim to detect malicious users with min-imum information. We make the following contributions: 1. We introduce the first methodology to detect malicious 2. We identify five general characteristics of malicious 3. We demonstrate that with as little as 10 bits of infor-4. We show via experiments that the methodology is ro-
In Section 2, we review the malicious user detection lit-erature. We formally define the malicious user detection problem with minimum information in Section 3. We detail characteristics of malicious users and how one can identify such characteristics in user content in Section 4. We de-tail our experiments in Section 5. Finally, we conclude in Section 6 with directions for future work.
While detecting malicious users with minimum informa-tion is unexplored, identifying malicious users in general is not a new topic. Often, to identify malicious users, (1) feature-based techniques, (2) human-in-the-loop techniques, or (3) techniques that use social graphs are used. We review representative techniques for each category and discuss how the current work relates to these techniques.
 I. Feature-based Techniques. In feature-based methods, different features are constructed to capture the behavior of the malicious user. These features are then used to construct a dataset that is trained by a supervised learning framework. For instance, Xie et al. [39], develop the AutoRe framework that identifies botnet campaigns. Their framework identifies traffic that is (1) bursty and (2) distributed. These features of traffic help identify botnets. The bursty and distributed nature of unwanted content is also used in detecting mali-cious posts on Facebook [12]. Wang [34] introduces a method that detects spam on Twitter using network features such as the number of followers or friends and content features such as duplicated tweets. Feature-based techniques have been discussed extensively for detecting unwanted content in so-cial tagging systems [20, 24], social networks [29], email [21], online videos [3], and microblogging sites [2, 43]. Our work differs from the existing work in two aspects. First, current techniques for identifying malicious users often employ con-tent or link information. Thus, one often needs a large col-lection of data instances to obtain guaranteed performances. Our approach employs minimum information across sites. Second, current literature is often context-dependent (e.g., site specific). Our method employs the minimum informa-tion that is universally available across sites and is robust even when information is collected from multiple sites. II. Human-in-the-loop Methods. One approach of iden-tifying malicious users is to employ human experts. Humans can naturally identify malicious users by their activities. Al-ternatively, one can combat malicious activities by technolo-gies such as CAPTCHAs [33] or photo-based authentica-tions [4] that are only solvable by humans. Although specific attacks are proposed for human-in-the-loop methods [26,41], they are in general considered effective. Unfortunately, ver-ifying accounts by humans is time consuming. For exam-ple, Tuenti, a Spain-based social networking service, hires humans to investigate reported users and block malicious ones [4]. An employee can only process 250 to 300 reports an hour from the daily 12,000 reports received. This issue makes human-in-the-loop processes infeasible for large-scale networks. Our approach in this paper is automatic and can easily scale to billions of users.
 III. Social Graph-based Techniques. In social graph-based methods, the information about the links (i.e., friend-ships) that the malicious individual has created helps detect the malicious user. For instance, Yang and colleagues [42] detect more than 100,000 fake accounts using social network features on RenRen social network. In particular, they find that invitation frequency, outgoing requests accepted, in-coming requests accepted, and network clustering coefficient can help identify fake accounts. In other works, probabilis-tic, combinatorial, or random walk models have been applied to network information to identify malicious users. Exam-ples include, Sybilguard [45], Gatekeeper [31], SybilInfer [7], SumUp [30], and Sybillimit [44]. These methods or variants can be applied on sites such as Twitter to identify malicious users [13]. Mislove et al. [32] show that most techniques in this area function by finding local communities around trusted nodes. Assuming the existence of a social graph is a strong assumption. One often requires specific privacy permission to obtain such graphs and in specific cases, this graph is not available. In cases where there is no social graph, our methodology is still easily applicable.
Who is a malicious user? The definition varies in the literature from users that harass other users to users that jeopardize the privacy of others [4]. We consider malicious users on a site, those whom normal users consider malicious. Clearly, the opinion of normal users can be subjective and has to be verified by experts. In section 5.1, we demonstrate how such human-verified data can be collected. Humans are known to be accurate in detecting malicious users on social media [15, 16, 27]. However, as discussed in our literature review, human-in-the-loop approaches are time consuming and expensive for large-scale networks. Hence, by inves-tigating how humans detect malicious users, one can not only scale detection of malicious users, but can also protect against a wide spectrum of malicious activities that humans are able to detect on social media [4, 5].
 Our goal in this paper is to identify such malicious users. Malicious users often provide little or no information. Hence, a method that can be universally employed on different sites is constrained to use the minimum information available on all sites. Usernames are the minimum information available on all social media sites [46]. Often, usernames are alphanu-meric strings or email addresses, without which users cannot join sites. Because of their unique characteristics, usernames are shown to be surprisingly effective for identifying individ-uals [46]. We formalize our problem using usernames as the minimum information available on all sites. Other content or link information such as user profile information or friends, when added to usernames, should help better identify ma-licious individuals. However, the lack of consistency in the availability of such information on all social media sites, di-rects us toward formulating our problem with usernames.
When using usernames, the goal is to detect malicious users from their usernames. Hence, one can learn a function M ( . ) that given a username u , predicts whether the user-name belongs to a malicious user or not. We denote the M function as the malicious user detection function. Formally, Definition . Malicious User Detection. Given a user-name u , a malicious user detection method attempts to learn a malicious user detection function M ( . ) such that
Malicious users have distinctive characteristics. These characteristics leave traces in the usernames of malicious users. These traces can be captured using data features. Fol-lowing the common machine learning and data mining prac-tice, the malicious user detection function can be learned using a supervised learning algorithm that utilizes these fea-tures and labeled data . In our problem, labeled data includes usernames that are known to be malicious or normal.
Supervised learning can be performed using classification or regression. Depending on the malicious user detection task at hand, one can even learn the probability that a user-name is malicious, generalizing our binary M function to a probabilistic one ( M ( u ) = p ). This probability can help select the most likely malicious username. The learning of the malicious user detection function is the most straightfor-ward. Therefore, we next elaborate on different characteris-tics of malicious users and how features can be constructed to capture traces introduced in usernames due to these char-acteristics. Note that the designed features may or may not help in the learning framework and are included as as long as they could be computed from usernames. Later on in Section 5, we will analyze the effectiveness of all features, and if it is necessary to find as many features as possible.
In summary, to detect malicious users, we (1) identify characteristics of malicious users, (2) construct features to identify traces of these characteristics in usernames, and (3) train a learning model to detect malicious users. Due to the interdependent nature of these user characteristics and feature construction, we discuss them together next.
Humans detect malicious users on social media by their characteristics. By reviewing related literature from com-puter science, security, criminology, among other fields [4, 11, 29, 32, 39, 42], we identified five general characteristics of malicious users. Malicious users can have one (or a combina-tion) of these characteristics. As researchers identify more characteristics of malicious users, our methodology can be extended with these characteristics and the corresponding features that can capture traces left by them in usernames.
Malicious users often generate (1) complex and (2) diverse information to ensure their anonymity.
 I. Complexity. To measure complexity of usernames, it is natural to borrow techniques from complexity theory. We employ Kolmogorov complexity [22] to determine the com-plexity of a username. Kolmogorov complexity of a user-name is defined as the length of the shortest program ca-pable of reproducing the username on a universal computer such as a Turing Machine. Hence, Kolmogorov complexity is the minimum quantity of information required to reproduce the username and measures its complexity.

For username u , let K ( u ) denote its Kolmogorov com-plexity. While K ( u ) defines the complexity of username u , it is well-known that its exact value cannot be com-puted [19]. Nonetheless, the following theorem helps com-pute the expected Kolmogorov complexity. Assume that usernames such as u are distributed as a random variable U , called username space , with probability distribution P .
Theorem 1. (from Li and Vit  X anyi [22]) For random vari-able U , expected Kolmogorov complexity E ( K ( U )) equals its entropy H ( U ) , plus a constant term that depends only on P .
Hence, by computing the entropy of the username space, one can approximate the expected Kolmogorov complexity in usernames. However, the theorem discusses the entropy of the username space and it is not clear how one can connect this theorem to a specific username. For connecting prop-erties of specific usernames to the entropy of the username space, we can use the concept of information surprise [6].
For username u , let p ( u ) denote the probability of observ-ing u . Information surprise for u is defined as
Hence, for a rare username u with a small observation probability p ( u ), information surprise I ( u ) is much higher than that of a common username with a higher observation probability. It is well-known that information surprise is deeply connected to entropy:
Theorem 2. (from Cover and Thomas [6]) The expected information surprise E ( I ( U )) for username space U (i.e., random variable U ) is equivalent to its entropy H ( U ) .
So, by combining Theorems 1 and 2, one can approxi-mate the expected Kolmogorov complexity of usernames by computing the expected information surprise in them. The information surprise for a username u is computed by mea-suring I ( u ) =  X  log 2 ( p ( u )), which requires the probability of observing username u . The probability of observing user-name u , denoted in characters as u = c 1 c 2 ...c n , is
We approximate this probability using an n -gram model,
Often, to denote the beginning and the end of a word special symbols are added such as ? and  X  . So, for username sara , the probability approximated using a 2-gram model is
To estimate the probability of a username using an n -gram model, one needs to compute the probability of its comprising n -grams. The probability of these n -grams can be computed using a large set of usernames. For that, we use a dataset of 158 million Facebook usernames (later discussed in Section 5.1) to train a 6-gram model. This n -gram model was employed to compute the probability of a username and in turn, its information surprise.

Being able to compute information surprise, we obtain its values for a set of 33 million usernames. The set contains both normal and malicious users. The process followed to collect these usernames is later discussed in Section 5.1. For both normal and malicious users, we estimate the empirical probability density function using Kaplan-Meier estimate. Figure 1 plots the empirical probability density function of information surprise values for normal and malicious users.
The thick solid line in Figure 1 demonstrates the distri-bution of surprise values for normal usernames and the thin solid line depicts the distribution for malicious usernames. As shown in the figure, malicious usernames are more com-plex with the expected information surprise (i.e., expected Kolmogorov complexity) value of 23.11 bits and more di-verse, ranging from 4.14 bits to 232.64 bits.

Unlike malicious usernames, normal usernames are less surprising and more concentrated around a mean value, with a mean of 12.49 bits and the information surprise value rang-ing from 3.90 to 31.93 bits. The figure shows that these Figure 1: Probability Density Function for Informa-tion Surprise Values of Malicious and Normal Users. distributions are well separated indicating that by using the information surprise of a username, one might be able to accurately classify usernames into malicious or normal.
In Figure 1, the dashed line depicts the curve for the mali-cious usernames subtracted by the curve for the the normal usernames. Hence, when this gray line is above zero, it shows that for a specific information surprise value, the username is more likely to be malicious and whenever the gray line is below zero, we observe the opposite. We notice that for values between 3.91 and 17.96 the curve is below the zero line, showing usernames are more likely to be normal. In this range, the mean value is 10.9 bits. Thus, when the in-formation surprise for a username is approximately 10 bits, the username is more likely to be normal. The title of this paper is inspired by this observation.

Hence, we include the information surprise of the user-name (i.e., its complexity) as another feature in our dataset. II. Diversity. To create diverse information, malicious users often generate usernames that include digits. There-fore, we include the number of digits in the username as a feature. We also include the proportion of digits in the username as another feature in our feature set.
In the criminology literature [11], it is well-known that crime correlates with demographic information. Thus, one expects to better detect malicious users by determining their demographics. Following the diffusion of innovations termi-nology [23], a malicious user has internal demographic at-tributes, external demographic attributes, or a combination of internal and external (i.e., mixed) attributes.

Internal attributes are endogenous attributes that the user has no control over such as his or her age. External at-tributes are attributes due to the environment that the ma-licious user lives in such as the language that the malicious user speaks. The level of knowledge that the malicious user has is an example of a user attribute that is mixed (inter-nal+external). This is because it depends on both the envi-ronment that the malicious user lives in and on the internal attributes of a user such as his interests. To concretely pro-file a malicious user, one has to consider all these attributes. We select gender from internal attributes, language from ex-ternal attributes, and knowledge (i.e., vocabulary size) from mixed demographic attributes to be predicted from user-names. Clearly, with more internal/external/mixed demo-graphic attributes, one should better profile malicious users. Figure 2: Popularity of names: Jennifer and Jacob over time. Higher values shows more popularity.
 We leave that as a future direction for this work. But, how can we detect gender, language, or other attributes of indi-viduals from their usernames?
Psychological studies [14] show that users leave traces of their personal information and attributes in the information they generate such as their usernames. For example, our analysis of popular names by birth year of US social security records 1 since 1879 shows us that the frequency of different names change over time. For instance, in Figure 2, we de-pict the popularity of first names: Jennifer and Jacob over time. For each year, the popularity of each name is shown on a scale of [0,1], 1 being the most popular name and 0 be-ing the least popular. Jennifer was the most popular female name between [1970-1984] whereas Jacob has been the most popular male name for [1991-2012]. Similar patterns can be observed for different English and non-English names given the diversity of the US population. Hence, given a name, one can estimate the most likely age. Names, interests, as well as other personal attributes are often abbreviated or used in usernames [46]. We use these information traces in user-names to predict gender, language, among other attributes. I. Malicious User Gender. To predict gender from user-names, we train a classifier. The classifier decomposes a username into character n -grams and estimates the gender likelihood based on these n -grams. This classifier is trained on the n -grams of a labeled dataset of usernames, in which the gender for each username is known. We collect our la-beled dataset from Facebook. Our labeled dataset contains a set of 4 millions usernames with their corresponding gen-der. The classifier predicts the gender of a username with up to 80% accuracy. Notice that because malicious users tend to hide their identity and gender; instead of the ac-tual prediction, we include the classifier X  X  confidence in the predicted gender as the feature.
 II. Malicious User Language. To detect the language of the username, we train an n -gram statistical language detec-tor [9] over the European Parliament Proceedings Parallel Corpus 2 , which consists of text in 21 European languages ( Bulgarian, Czech, Danish, German, Greek, English, Span-ish, Estonian, Finnish, French, Hungarian, Italian, Lithua-nian, Latvian, Dutch, Polish, Portuguese, Romanian, Slo-vak, Slovene, and Swedish ) from 1996-2006 with more than 40 million words per language. The trained model detects the username X  X  language, which is a feature in our feature http://www.ssa.gov/oact/babynames/ http://www.statmt.org/europarl/ set. The detected language feature is limited to European languages. Our language detector will not detect other lan-guages. The language detector is also challenged when deal-ing with words that may not follow the statistical patterns of a language, such as location names, etc. This issue can be tackled by including the distribution of alphabet letters in usernames as features [46]. Thus, in addition to predicted language, we include the alphabet distribution of the user-name as a feature.
 III. Malicious User Knowledge. To approximate the level of knowledge of a malicious user, we can compute his or her vocabulary size. The vocabulary size can be computed by counting the number of words in a large dictionary that are substrings of the username [46]. This approach captures different possible interpretations of the username and ap-proximates the level of knowledge of the malicious user. We include the vocabulary size as a feature. As this is a rough approximation, we will determine the efficiency of this fea-ture in our feature importance analysis in Section 5.5. Malicious activity often requires a level of anonymity [1]. Theoretically, the maximum level of anonymity is achieved when a username has the maximum entropy [40]. We com-pute the entropy of the alphabet distribution of the user-name as well as the normalized entropy of the username to measure its level of anonymity. To normalize entropy, we divide it by log n , where n is the number of unique alphabet letters used in the username. In addition, we also measure the uniqueness of letters in the username  X  that is, the num-ber of unique letters used in the username divided by the username length. We include entropy, normalized entropy, and uniqueness as features.
Malicious users tend to be similar. For instance, individ-uals marketing an illegal product Dangerous-Pill all share the name of the product Dangerous-Pill in their marketing content. This malicious content similarity can be captured in usernames by identifying specific (1) language patterns and (2) words in the usernames.
To find finer grain language patterns of users, we em-ploy character-level n -grams. Character-level n -grams have shown to be effective in detecting unwanted content [17, 18] and connecting users across social media sites [28]. We com-pute the normalized character-level bigrams of usernames and include them as features. Bigram features are normal-ized using TF-IDF. Bigrams allow for a language-agnostic solution [46] that can detect common patterns of malicious users conveniently.

For coarser grain language patterns, we investigate com-mon habits of malicious users. For instance, it is known that the use of digits is an indication of unwanted content [20]. In particular, we notice that malicious users tend to start their usernames with digits; therefore, we include the num-ber of digits at the beginning of the username as a feature. We also notice that malicious users repeat character letters more often that normal users. This strategy allows them to circumvent widely used statistical malware blockers [38]. Hence, we include the maximum number of times a letter has been repeated in the username as another feature.
A well-known approach to identify malicious users or con-tent is by finding specific keywords in the content generated by these users. Hence, we denote the existence of these spe-cific keywords in usernames as an indication of malicious ac-tivity. We utilize two dictionaries, one containing keywords related to malicious activities and the other for offensive key-words 3 For each dictionary, we count the number of words in the dictionary that appear as the substring of the user-name. We include these two counts for the aforementioned two dictionaries as features.
In contrast with complex malicious users (Section 4.1), some malicious users demand efficiency. This is because the malicious user is interested in performing the malicious ac-tivity frequently, quickly, and at large-scale. For instance, when performing click-fraud, the malicious user is interested in creating many accounts, each clicking on specific ads. This efficiency can be observed in usernames in terms of (1) the username length; and (2) the number of unique al-phabet letters in usernames. We include both as features. In addition, we can observe efficiency by determining the typing patterns of the malicious user.
 Most people use one of the two well-known DVORAK and QWERTY keyboards, or slight variants such as QWERTZ or AZERTY [36]. It has been shown that the keyboard layout significantly impacts how random usernames are se-lected [8]. For example, qwer1234 and aoeusnth are two well-known passwords commonly selected by QWERTY and DVORAK users, respectively. To model typing patterns of malicious users, for each username we construct the follow-ing 15 features for each keyboard layout (a total of 30 for both keyboard layouts), 1. (1 feature) The percentage of keys typed using the 2. (1 feature) The percentage of keys typed using the 3. (8 features) The percentage of keys typed using each 4. (4 features) The percentage of keys pressed on rows: 5. (1 feature) The approximate distance (in meters) trav-
We construct 15  X  2 = 30 features that capture the typing patterns of usernames for both keyboards and include them in our feature set.

We have detailed how characteristics of malicious users can be captured by meaningful features. These features help identify traces of malicious activities in usernames. Overall, for each username, we construct 1,413 features.

Clearly, not all characteristics of malicious users are cov-ered by our features, and with more theories on character-istics of malicious users, more features can be constructed. We will empirically study if it is necessary to use all fea-tures and the effect of using different features on learning performance of detecting malicious users.
All data available at: http://reza.zafarani.net/data/10bits
Following our approach, we compute the feature values over labeled data, and verify the effectiveness of our method-ology by learning the malicious user detection function. Next, experiments for evaluating our methodology are detailed.
We evaluate our methodology to detect malicious users in this section. First, we verify if our proposed approach can identify malicious users well. Next, we verify if different learning algorithms can influence the prediction task. Then, we determine the sensitivity of our approach to different conditions. Finally, we perform feature importance analysis and determine how features designed for each characteristic of malicious users influence the detection outcome. Before we present the experiment details, we detail how experimen-tal data was collected for this research.
Our approach to detect malicious users employs a super-vised learning framework. Hence, labeled data is required. This labeled data consists of usernames and their corre-sponding label: malicious or normal. To construct this la-beled data and for our experiments, we collect four datasets. I. Malicious Users (negative examples). We collect malicious usernames from sites such as dronebl.org, ahbl.org, among others (for a complete list see [25]). These sites gather lists of usernames that have been reported by other normal users for malicious purposes. Once reported, these accounts are manually verified by domain owners to be ma-licious. These lists are published to help sites promote their security. We collect a set of 32 million usernames that are manually reported as malicious by users on different sites. This set forms our negative examples.
 II. Normal Users (positive examples). For collecting normal users, we require users that are manually labeled as normal. For that, we refer to Twitter verified accounts, all manually verified by Twitter employees. These accounts are all followed by the Twitter handle verified 4 . By crawling all the users this account follows, we collect a set of 45,953 usernames guaranteed to be normal. These usernames form our positive examples.
 III. Facebook Users (positive+negative examples). To diversify the types of usernames we have collected, we also collect a set of 158 million usernames from Facebook, that is, 1 in 9 Facebook users in the world are included in our dataset. Note that the Facebook dataset is not completely normal as Facebook expects around 1.5% to be malicious. We employ this dataset for analyzing the sensitivity of our approach to different conditions (Section 5.4.2).
 IV. Gender Dataset. We collect a different set of 4 million Facebook usernames for which we have the gender informa-tion. This dataset was used to train our gender prediction classifier in Section 4.2 to predict gender from usernames.
After collecting these four datasets of usernames 5 , we com-pute the corresponding 1,413 features for all datasets and employ them in our experiments. http://twitter.com/verified
We ensure that the alphabet used in both sets of usernames match. To avoid site-enforced specific patterns on how user-names should be created, we filter out usernames that are not in ASCII or alphanumeric. Our experiments show that this procedure does not influence our results.
 Table 1: Malicious User Detection Performance.
 Technique AUC F1 Our Approach 0.9932 0.9644 Baseline b 1 : Keyword Detection 0.51 0.66 Baseline b 2 : Username Randomness 0.70  X  0 Baseline b 3 : Letter Repetition 0.61  X  0 Reference Point r 1 : Markines et al. [24] 0.984 0.983 Reference Point r 2 : Gao et al. [12] 0.945  X 
Reference Point r 3 : Wang [34] 0.917 0.917
Once the negative and positive examples are prepared, learning the malicious user detection function can be achieved by training a classifier. Because our collected negative ex-amples are more, we subsample the negative examples to have the same size as the positive examples. This way we create a dataset that has 50% positive examples and 50% negative ones. Using this dataset, we train a classifier. The random prediction on this dataset cannot achieve more than 50% accuracy. We train an ` 2 -Regularized Logistic Regres-sion using 10-fold cross validation and obtain an accuracy of 96.42%, an AUC of 0.9932, and an F1-measure of 0.9644.
As there are no comparable methods, we evaluate the ef-fectiveness of our approach by devising three baseline meth-ods for comparison. When individuals are asked to detect malicious users based on their usernames, they often look for specific  X  X eywords X , verify if the username looks  X  X an-dom X , or look for  X  X epetition of letters X . Hence, they form our three baselines b 1 , b 2 , and b 3 :
While the baseline performances demonstrate the diffi-culty of our problem, the proposed approach outperforms all baselines by at least 41%. The performance for our approach, and baselines are summarized in Table 1. As reference points, we also include in the table the perfor-mance of recent state-of-the-art techniques for detecting ma-licious users. These techniques have access to more infor-mation compared to our methodology and do not employ usernames; therefore, no improvement percentage will be reported. Our approach, with usernames only, outperforms these techniques. Next, we investigate if different learning algorithms can further improve the learning performance. Table 2: Malicious User Detection Performance for Different Classification Techniques.

Technique AUC Accuracy ` 2 -Regularized ` 1 -Loss SVM 0.9966 97.05% ` 2 -Regularized ` 2 -Loss SVM 0.9913 96.05% ` 2 -Regularized Logistic Regression 0.9923 96.25% ` 1 -Regularized Logistic Regression 0.9971 97.26 %
To evaluate the choice of learning algorithm, we perform the classification task using a range of learning algorithms and 10-fold cross validation. The AUCs and accuracy rates are available in Table 2. These algorithms have different learning biases, and one expects to observe different perfor-mances for the same task. While we observe a slight increase in the performance, as shown in the table, results are not significantly different across algorithms. This shows that when sufficient information is available in features, the per-formance is not sensitive to the choice of learning algorithm.
In our experiments, ` 1 -Regularized Logistic Regression is shown to be the most accurate method; therefore, we use it in the following experiments as the method of choice.
In our previous experiments, we assumed that there is no class imbalance between malicious and normal users. In re-ality this distribution is skewed. Furthermore, unlike our malicious users, all of our normal users are from one source (Twitter). Thus, we need to verify how this decision influ-ences our results. We analyze the sensitivity of our approach to class imbalance and the distribution of normal users next.
In real-world networks such as Facebook and Twitter, the percentage of malicious users in the population is approxi-mated to be at most 10% [10, 35]. In other words, for every 9 normal users there exists at most 1 malicious user. This rate could be different across networks. Thus, we perform a sensitivity analysis with respect to different ratios of mali-cious users. We construct datasets, where  X  percent of the dataset consists of malicious users and change  X  in the range 5  X   X   X  50. Values larger than 50 were not selected, be-cause then we are assuming that malicious users are more than the normal ones.

Because we collected more negative examples, we sample the negative examples many times to guarantee that each negative example is seen at least once. Thus, for each  X  , many datasets are created. For each one of these datasets, we perform classification and average the performance met-rics over all datasets created for a specific  X  .

Figure 3 depicts the average performance (accuracy, AUC, and F1-measure) of our methodology with different percent-ages of malicious users. As shown in the Figure, as the number of malicious users increase, AUC remains stable and F1-measure and accuracy slightly drop, but in all cases, all measures stay above 0.97.
To determine the sensitivity of our classifier to different normal users, we use samples of Facebook users instead Figure 3: Performance (AUC, F1, and Accuracy) of our methodology for Different Percentages of Malicious Users.
 Figure 4: Performance Measures (F1, AUC, and Accuracy) of our Methodology for Different Percentages of Malicious Users when Facebook Identities were used instead of Normal Users. of our normal users in training. Facebook users are not completely normal and Facebook approximates that around 1.5% of its users are malicious [35]. If our classifier can detect Facebook malicious users, some positive instances (Facebook users) will be classified as negative (malicious). Hence, the performance is expected to slightly decrease. In theory, for all datasets that have at most 50% negative examples (ma-licious users) and Facebook users as positive examples, one expects at most a 50%  X  1.5% = 0.0075 decrease in accuracy. Our experiments verify this expectation. Figure 4 plots the performance (accuracy, AUC, F1-measure) of the algorithm with different percentages of malicious users and using Face-book users as positive examples. We notice a slight drop in performance for all measures, but the performance remains high and is never below 0.9671. For comparison, we include the performance measures with the original normal users. Comparing the performance measures with those of normal users, we notice that the accuracy drops by at most 0.0035 (less than the expected maximum: 0.0075), AUC drops by at most 0.0012, and F1-measure drops by at most 0.0036.
In our experiments, we employ all 1,413 features to detect malicious users. Designing 1,413 features and computing their values is computationally expensive. Hence, we empir-ically determine whether all features are necessary next.
Here, we analyze how important different features are in learning the malicious user detection function. In other Table 3: Malicious User Detection Performance for Different Groups of Features.
 words, we find features that contribute the most to the clas-sification task. This can be performed by standard feature selection measures such as Information Gain,  X  2 , among oth-ers. Here, we use the  X  2 statistic to find the top features. The top 10 features in decreasing order of importance are: 1. The information surprise of the username 2. The number of digits used in the username. 3. The percentage of keys pressed on the top row of a 4. The percentage of keys pressed on the top row of a 5. The proportion of digits used in the username. 6. The approximate distance (in meters) traveled for typ-7. The percentage of keys pressed on the home row of 8. The approximate distance (in meters) traveled for typ-9. The percentage of keys pressed on the bottom row of 10. Entropy of the username.

We notice that the complexity of the username is the most important feature and that 6 of the top 10 features are fea-tures that capture typing patterns. Using only these 10 fea-tures, we trained a logistic regression model and achieved an accuracy of 92.95% and an AUC of 0.973.

We also determine groups of features that contribute most to the classification. We divide features into groups based on the characteristic of malicious users they represent. We de-note these features based on the discussion in Section 4 as (1) Complexity-based, (2) Demographic-based, (3) Anonymity-based, (4) Similarity-based, and (5) Efficiency-based. Table 3 summarizes the classification performance obtained using only these groups of features.

We observe that similarity-based features work the best and anonymity-based features are least effective. Note that similarity-based features are in general hard to construct as they require n -gram constructions. Surprisingly, efficiency-based or complexity-based features that are easier to com-pute, can classify malicious users accurately, with up to 87% accuracy. Our observations in this section allows users with limited time and resources to take informed decisions on the features and groups of features to construct.
In this research, we have introduced a methodology that can identify malicious users with minimum information. Our methodology looks into different characteristics of malicious users and systematically constructs features that can cap-ture traces of malicious behaviors. With new theories on characteristics of malicious users, new features can be intro-duced into our methodology.

We categorize characteristics of malicious users into 5 gen-eral categories. In particular, malicious users can be (1) complex and diverse, (2) demographically biased, (3) anony-mous, (4) self-similar, and (5) efficient. A malicious user can exhibit one or a combination of these characteristics. By introducing comprehensive features across these five cat-egories, we train a learning framework that can detect mali-cious users. The evaluation of this framework demonstrates the effectiveness of this systematic approach.

We notice some interesting observations. First, we no-tice that usernames that carry approximately 10 bits of in-formation surprise, are more likely owned by normal users. Second, with only minimum information, one can achieve an accuracy of 97%, an AUC of 0.9971, and robust perfor-mances with different class imbalances and irrespective of the learning algorithm. Finally, we identify that in case of limited time or resources, one can implement a limited set of features and obtain reasonable accuracy rates.

The findings in this paper have many implications. First, we note that our methodology is in general easy to imple-ment with minimum dependency on the availability of in-formation. Second, our methodology works with usernames from different sites. This is empirically shown in our exper-iments with usernames collected from a variety of sites. Fi-nally, our methodology performs with reasonable accuracy, compared to state-of-the-art techniques that have access to additional information.

Future work of this research includes integrating addi-tional information available across sites in a principled man-ner. However, this extension requires considering the het-erogeneity of data available across sites. In addition, similar to the observation we had regarding the information surprise values of usernames, we are interested in how surprise values change for other content generated by users.
 Acknowledgments. The authors would like to thank Fred Morstatter, Shamanth Kumar, and Ashwin Rajadesingan for valuable suggestions. This work was supported, in part, by the Office of Naval Research grant: N000141410095. [1] Anne Barron. Understanding spam: A macro-textual [2] Fabr X cio Benevenuto, Gabriel Magno, Tiago [3] Fabricio Benevenuto, Tiago Rodrigues, Virgilio [4] Qiang Cao, Michael Sirivianos, Xiaowei Yang, and [5] Zi Chu, Steven Gianvecchio, Haining Wang, and [6] Thomas M Cover and Joy A Thomas. Elements of [7] George Danezis and Prateek Mittal. Sybilinfer: [8] C. Doctorow. Preliminary Analysis of LinkedIn User [9] T. Dunning. Statistical Identification of Language . CR [10] J. Elder. Inside a Twitter Robot Factory. [11] Lee Ellis, Kevin M Beaver, and John Wright.
 [12] Hongyu Gao, Jun Hu, Christo Wilson, Zhichun Li, [13] Saptarshi Ghosh, Bimal Viswanath, Farshad Kooti, [14] Sam Gosling. Snoop: What your stuff says about you . [15] C Harris. Detecting deceptive opinion spam using [16] Muhammad Asim Jamshed, Wonho Kim, and [17] Ioannis Kanaris, Konstantinos Kanaris, Ioannis [18] Ioannis Kanaris, Konstantinos Kanaris, and Efstathios [19] Eamonn Keogh, Stefano Lonardi, and Chotirat Ann [20] Beate Krause, Christoph Schmitz, Andreas Hotho, and [21] Ho-Yu Lam. A learning approach to spam detection [22] Ming Li and Paul MB Vit  X anyi. An introduction to [23] Vijay Mahajan and Robert A Peterson. Models for [24] Benjamin Markines, Ciro Cattuto, and Filippo [25] MediaWiki. Combating Spam -List of Proxy and [26] Greg Mori and Jitendra Malik. Recognizing objects in [27] Terri Oda and Tony White. Increasing the accuracy of [28] Daniele Perito, Claude Castelluccia, Mohamed Ali [29] Gianluca Stringhini, Christopher Kruegel, and [30] Dinh Nguyen Tran, Bonan Min, Jinyang Li, and [31] Nguyen Tran, Jinyang Li, Lakshminarayanan [32] Bimal Viswanath, Ansley Post, Krishna P Gummadi, [33] Luis Von Ahn, Manuel Blum, Nicholas J Hopper, and [34] Alex Hai Wang. Don X  X  follow me: Spam detection in [35] T. Wasserman. 83 Million Facebook Accounts Are [36] Wikipedia. Keyboard Layouts. http://bit.ly/kXso . [37] Wikipedia. List of countries by population. [38] Gregory L Wittel and Shyhtsun Felix Wu. On [39] Yinglian Xie, Fang Yu, Kannan Achan, Rina [40] J. Yan, A. Blackwell, R. Anderson, and A. Grant. The [41] Jeff Yan and Ahmad Salah El Ahmad. A low-cost [42] Zhi Yang, Christo Wilson, Xiao Wang, Tingting Gao, [43] Sarita Yardi, Daniel Romero, Grant Schoenebeck, [44] Haifeng Yu, Phillip B Gibbons, Michael Kaminsky, [45] Haifeng Yu, Michael Kaminsky, Phillip B Gibbons, [46] Reza Zafarani and Huan Liu. Connecting users across
