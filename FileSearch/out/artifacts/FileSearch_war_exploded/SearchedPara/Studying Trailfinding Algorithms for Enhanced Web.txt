 Search engines return ranked lists of Web pages in response to queries. These pages are starting points for post-query navigation, but may be insufficient for search tasks involving multiple steps. Search trails mined from toolbar logs start with a query and con-tain pages visited by one user during post-query navigation. Im-plicit endorsements fro m many trails can enhance result ranking. Rather than using trails solely to improve ranking, it may also be worth providing trail information dire ctly to users. In this paper, we quantify the benefit that users currently obtain from trail-following and compare different methods for finding the best trail for a given query and each top-ranked result. We compare the relevance, topic coverage, topic diversity, and utility of trails se-lected using different methods, and break out findings by factors such as query type and origin re levance. Our findings demonstrate value in trails, highlight interesting differences in the performance of trailfinding algorithms, and show we can find best-trails for a query that outperform the trails most users follow. Findings have implications for enhancing Web in formation seeking using trails. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process , selection process Algorithms, Experimentation, Human Factors Search trails, trailfinding, best-trail selection Web search engines provide keyword access to Web content. In response to search queries, these e ngines return lists of Web pages ranked based on estimated releva nce. Information retrieval (IR) researchers have worked extensively on algorithms to effectively rank documents (c.f. [20]). However, research in areas such as information foraging [18], berrypicking [2], and orienteering [16], suggests that individual items may be insufficient for vague or complex information needs. In such circumstances, search results represent only the starting points of user exploration [17][21]. Logs containing the search engine interactions of many users have been mined extensively to enhanc e search-result ranking [1][13]. Richer log data from sources such as browser toolbars offers in-sight into the behavior of many users beyond search engines. Search trails comprising a query and post-query page views can be mined from these logs [28]. Although trail components X  origins (clicked search results) a nd destinations (trail end points [27]) have been used previously to support search, the typical application of trails is to be tter rank Web pages [1][3]. In As We May Think [4], Vannevar Bush envisioned using trails marked and willingly shared by trailblazing users to guide others. Joachims et al. [14] suggest that in many ca ses, only a sequence of pages and the knowledge about how they relate can satisfy a user X  X  informa-tion need. This suggests that trails should be a unit of retrieval, or at least shown to users on the s earch engine result page (SERP). Although others have investigated trail generation for site or hypertext navigation [11][25], the challenge of finding the best trails to show to users directly on the SERP is unaddressed. In this paper we present a log-based study of trailfinding for Web search. We mine trails from logs and investigate the value that full We then represent trails as graphs and create algorithms to find the best trail for each search result X  X o-called trailfinding  X  X sing graph properties such as breadth, depth, and strength. Since  X  X est X  may be task dependent, we use a va riety of metrics to evaluate the trails found. Our study answers the following questions: (i) How much benefit do users gather from following trails versus stopping after the origin page? (ii) Whic h trailfinding algorithms perform best? (iii) Can we extend our algo rithms to handle unseen queries? We conduct this study using a l og-based methodology since logs contain evidence of real user be haviors at scale and provide cov-erage of many types of information needs. Information need cov-erage is important since differences in algorithm performance may not hold for all search tasks. Our findings demonstrate value in trails, interesting differences in the performance of the algorithms, and performance tradeoffs when moving beyond logs to handle unseen queries using term matching. A search trail consists of an origin page, intermediate pages, and a destination page. Origin pages are the search results that start a search trail. Query and origin pa ges from search engine click logs can be used to improve result set relevance [13]. Agichtein et al. [1] and Bilenko and White [3] found that using trails as endorse-ments for trail pages helped search engines learn to rank search results more effectively. The goal of their research was to improve ranking rather than show trails to users on the SERP. White et al. [27] added trail destination sugge stions to the SERP. User study participants found destination suggestions useful. Our research extends that work to consider th e suggestion of full trails rather than only destinations on the SERP . Prior to adding trails to result pages, we first study a variety of trailfinding methods to find per-formant algorithms that are worth fu rther testing in user studies. Systems such as WebWatcher [14], ScentTrails [15], and Vo l a n t [17] highlight candidate pages based on models of information needs or user interests. Studies of these systems show that they can improve search speed and search success. Highlighted pages form a trail over time, but the li nk-at-a-time approach does not expose the user to much ne eded initial context [14]. Wexelblat and Maes [25] introdu ced annotations in Web browsers called footprints , which are trails through a Website assembled by the site X  X  designer. Their evalua tion found that users required sig-nificantly fewer steps to find information using their system. Freyne et al. [10] extend footprints by adding icons to links to offer users visual cues. These cues are gathered from past users and include popularity, recency, and annotations. Wang and Zhai [24] continues the footprint metaphor in a topic map that lets users navigate to related queries, and to queries of varying specificity. Simulation studies revealed pot ential benefit from topic maps. Guided tours and trails construc ted by domain experts have been proposed, mainly in the hypertext community. Hammond and Allison [12] and Trigg [22] proposed guided tours in hypertext to ease problems of user disorienta tion. Zellweger [30] introduced scripted documents which are mo re dynamic than guided tours since they have conditional and programmable paths, automated playback, and active entries. Chal mers et al. [5] propose that hu-man  X  X ecommenders X  construct and share Web navigation paths. Rather than requiring human intervention, tours and trails can also be generated automatically. Guinan and Smeaton [11] generate a tour for a given query based on te rm matching for node selection and inter-node relationships (e.g .,  X  X s_a X ,  X  X recedes X ) for node ordering. In a user study using a collection of lecture materials, they found that users followed thes e trails closely; 40% of the time, subjects di d not deviate from the pr oposed trail. Wheeldon and Levene [26] propose an algorithm for generating trails to assist in Web navigation. They de fine trails as trees and expand them from the root node using the expected information gain as the probability of expansion. This gain is based on the term fre-quency of the query in the documen t, with a pena lty for duplicate URLs. They presented trails usi ng an interface attached to the browser. User study participants found trails to be useful and noted that seeing the relationship between links helped. We extend previous work in a number of ways: (i) we recommend full trails rather than only sugges ting next steps; (ii) we focus on general Web search, where the content is less constrained than Websites or small hypertext collections, and information such as inter-node relationships is typically unavailable, and (iii) we find best-trails based on real user beha viors evident in logs, avoiding the scalability challenges associated with human intervention. In this section, we describe the log data from which trails are ex-tracted, outline trail mining, in troduce some trailfinding algo-rithms, and describe unseen quer y handling using term matching. The primary source of data for this study was the anonymized logs of URLs visited by users who consented to provide interac-tion data through a widely-distri buted browser plugin. Log entries include a unique user identifier, a timestamp for each page view, an identifier for each browser instance, and the URL of the Web page visited. Intranet and secure (https) URL visits were excluded at the source to maintain user privacy. Revisits to pages made through the browser  X  X ack X  button are also captured in the log data. To remove variability caus ed by geographic and linguistic variation in search behavior, we only include entries generated in the English speaking United States locale. The results described in this paper are based on URL visits during a nine-month period from February 2009 through December 2009 inclusive, representing billions of URL visits from millions of unique users. From these logs, we mined around a billion search trails, each trail followed by a single user. Trails start with a search engine query (which also includes the SERP) followed by a click on one of the search engine results (trail origin ). Search trails are represented as temporally-ordered URL sequences. Trails terminate once they reach 10 steps (to facilitate more controlled analysis later in the study) or a period of user inactivity of 30 or more minutes (also used in [8]), whichever condition is satisfied first. In our logs, there were 1.4 billion search trails followed by 80 million users. This comprised 314 million unique queries (  X  ), 226 million unique origins (  X  ), 542 million query-origin pairs, and 1.1 billion unique search trails (  X  ). Figure 1 illustrates three search trails expressed as Web behavior graphs. Each trail starts with the same query ( 1 X  ) and the same origin URL (  X   X  ), then proceeds to differ-ent pages. The number in brackets on each node represents its sequence order in the tr ail based on timestamps of user activity. Properties of these behavior grap hs, among other things, are used to find the best trails. We now de scribe the trailfinding algorithms. The trailfinding task is defined: given a query  X  and an observed  X  X , X , X  X  X  X  X  X  X  . The scoring function can be defined in many ways. In this study we experiment with a sample of techniques that in-clude graph properties, relevanc e, and Web domain information. Tr ai l Le ngth: The  X  X , X , X  X  X  X  X  X  X  for trail length is defined as the length of  X  in terms of the total number nodes following  X  . This algorithm prefers long trails which may be most engaging for some users in terms of browsing activity (or could signify that users are struggling to find useful information). The limitation is that long trails could be obscure, especially if frequency is low. Trails from Figure 1 ordered by length are: 3 X  (four nodes), 2 X  (three nodes), and 1 X  (one node). Trail Breadth: The  X  X  X  X  X   X   X , X , X   X  for trail breadth is defined as the number of branches in  X  from the origin  X  . In Figure 1, 2 X  has the maximum trail breadth of two and would be the best trail in the figure according to this algorithm. Broad trails let users ex-plore various sub-topics while reta ining the overall concept, e.g., users might look for specific e-cards within an e-card website. Trail Depth: The  X  X  X  X  X   X   X , X , X   X  for trail depth is defined as the maximum number of nodes on a single branch from the origin  X  . Deep trails are usually exploratory and can take users to new con-cepts or topics. 3 X  is the  X  X eepest X  trail in Figure 1 (depth = 3). Trail Frequency: The  X  X  X  X  X   X   X , X , X   X  is based on the frequency of  X  for a given query  X  and origin  X  . If we assume that in Figure 1,  X ,1 X ,1 X  X  X  X  X  X   X   X  = 3;  X ,1 X ,2 X  X  X  X  X  X   X   X  = 2 and  X ,1 X ,3 X  X  X  X  X  X  = 1, this algorithm would associ ate scores of 3, 2 and 1 to 1 X  , 2 X  and 3 X  respectively. This algorithm favors short trails. Trail Strength: Scoring trails based on their strength: (i) the en-gaging potential of the behavior gra ph in terms of size, and (ii) the ease of navigation. To estimate the strength of tree starting with query  X  and origin  X  , we first compute the total frequency of all navigations of type  X  X   X   X   X   X   X  with the user navigating to  X  from  X   X  in trails starting with query q and origin r . That is: where  X  X  X  X   X   X , X , X   X  is the frequency of  X  for query  X  and origin  X  . For 1 X  ,  X   X  in Figure 1, post-SERP naviga tions over all three trails, with frequencies as above are:  X  X   X   X   X   X   X  : 5;  X  X  2;  X  X   X   X   X   X   X  : 3;  X  X   X   X   X   X   X  : 1;  X  X   X   X   X   X   X  : 1;  X  X   X   X   X  : 1. Given this navigation model, trail strength is defined as: This helps find long trails that ar e easy to navigate. Applying this followed by 3 X  (  X  X  X  X  X  = 6) and then 1 X  (  X  X  X  X  X  = 5). Trail Diversity: The  X  X  X  X  X   X   X , X , X   X  is based on the number of pages in  X  whose Web domain (extracted automatically from the URL string for each page) differs from that of the origin  X  . In Figure 1, if we assume the domain of URLs  X   X  ,  X   X  and  X  from that of URL  X   X  , then the trail ordering would be 3 X  (three new domains), 2 X  (two new domains), and 1 X  (zero new domains). Best-trails selected using this al gorithm are diverse, offering the user new information relative to the origin page. Trail Relevance: The  X  X  X  X  X   X   X , X , X   X  for trail relevance for each  X  page is first calculated using the , X  X  X  X  X   X  X   X  X  X  X  X   X  X  X  X  X  % X  X  X  X   X  X  X  X   X  X   X  X  X  X  X   X  X  X  X  X  % then averaging these scores across all  X  contains all query words of 1 X  and title of  X   X  and  X  all query terms of 1 X  . The scores assigned to 1 X  , 2 X  and 3 X  are 50, 75 and 20 respectively. This algorithm favors trails with query-relevant titles and URLs, suggesti ng the trail itself is relevant. The trailfinding algorithms described in this section so far rely on an exact match between the user query and the query starting the trails. The algorithms can be extended to associate trails to unseen queries using term matching based on a variant of  X  X  X . X  X  . This is important because over half of que ries have never been seen by the search engine [27]. Let {  X   X   X ,  X  ,... X  be terms in  X  and for each  X  , get all trails in  X  occurring from a prior  X   X   X   X  . The following equation generates a score for each trail for query  X  and origin  X  :  X  X  X  X  X   X   X , X , X   X   X  X  X  where  X  X , X  X  X  is the frequency with which  X  appears in a query leading to result click on  X  ,  X  X  X  X  X  is the document frequency of  X  computed as the number of origins to which w is associated in logs, and  X  X , X , X  X  X  X  X  X  X  X  is based on the trailfinding algorithms above, e.g., breadth algorithm sets  X  X , X , X  X  X  X  X  X  X  X  as  X   X  X  breadth. In this section we present the research questions that drive our study, summarize the trail data preparation, present metrics used to compare the algorithms, and describe the experimental variants. Our study answers a number of research questions:  X  RQ1 : Of the trails and origins, which source: (i) provides more relevant information? (ii) provi des more coverage and diversity of the query topic? (iii) provides more useful information?  X  RQ2 : Among trailfinding algorithms: (i) how does the value of best-trails chosen differ? (ii) what are the effects of query cha-racteristics on best-trail value and selection? (iii) what is the impact of origin relevance on best-trail value and selection?  X  RQ3 : In associating trails to unseen queries: (i) how does the value of trails found through query-term matching compare to trails with exact query matches found in logs? (ii) how robust is term matching for longer queries (which may be noisy)? To help ensure experimental integrity, we did not use all search trails in  X  . Instead, we filtered the data as discussed below. In addition to the trail data, we also obtained human relevance judgments for over eighty thous and queries that were randomly sampled by frequency from the query logs of a large search en-gine. Trained judges assigned releva nce labels on a six-point scale  X  Bad , Poor , Fair , Good , Excellent , and Perfect  X  X o top-ranked pooled Web search results for each query from the Google, Ya-hoo!, and Bing search engines dur ing a separate search engine assessment activity. This led to relevance judgments for hundreds of pages for each query. These j udgments allowed us to estimate the relevance of information encount ered at different parts of the trails. We filtered original trail data so that the origins of the trails (  X  ) have human judgments for at least one query. Two of the four evaluation metrics used in our study X  X overage, and diversity X  X equired informa tion about page topicality and query interest. Firstly, we classified trail pages present in  X  into the topical hierarchy from a popular Web directory, the Open Di-rectory Project (ODP) (dmoz.org). Given the large number of pages involved, we used automati c classification. Our classifier assigned one or more labels to the pages based on the ODP using a similar approach to Shen et al . [19]. Classification begins with URLs present in the ODP and incrementally prunes non-present URLs until a match is found or miss declared. Similar to [19], we excluded Web pages labeled with the  X  X egional X  and  X  X orld X  top-level ODP categories, since they are location-based and are typically uninformative for construc ting models of user interests. The coverage of our ODP classifier with URL back-off was ap-proximately 65%. A missing or pa rtial labeling of trail was al-lowed. Next, we constructed a set of query interest models for each query having human judged data . These models served as the ground truth for our estimates of coverage and diversity. A query X  X  interest model comprises the ODP category labels assigned to the URLs in the union of the top-200 search results for that query from Google, Yahoo!, and Bing. ODP labels are grouped and their frequency values are normalized such that across all labels they sum to one. For example, the most popular labels in the interest model for the query [triathlon training] , and their normalized frequencies (  X   X  ), are shown in Figure 2. Label  X   X   X  X  X  X  X  X  X  X / X  X  X  X  X  X  X  X  X / X  X  X  X  X  X _ X  X  X  X  X / X  X  X  X  X  X / X  X  X  0.58  X  X  X  X  X  X  X  X  X / X  X  X  X  X  X / X  X  X  X  X  X  X  X / X  X  X  0.11 To improve the reliability of our evaluation metrics, the query interest models had to be based on at least 50 fully-labeled search results (i.e., were not missing a la bel and did not have a label from an ignored category) and based only on category labels with a frequency of at least three (to reduce label noise). We applied normalization and pr uning to ensure data quality:  X  All queries were normalized (involving removing punctuation, lowercasing, etc.) to facilitate comparab ility among trails and between the trails and other resources.  X  Query-origin pairs were required to contain at least five unique trails and at least one trail of length exceeding two to maintain substantial variety for trailfinding.  X  Common queries such as [facebook] , [myspace] , and [yahoo] contained thousands of short trails in the data since the ideal re-sult for such queries presents users with a number of ways to branch into social networks or directory structure. To handle this, we first bucketed each query-origin pair in  X  based on trail length. Then, for all trails of a particular length for each query-origin pair, pruned the trails for which rank based on frequency was greater than 50 and ratio of frequency to maximum fre-quency for this bucket was less than 25%. This allowed us to maintain high variability in trail data yet remove many spurious trails for some common queries. Filtering and pruning reduced  X  to 209 million trails, roughly 20% of its original size. We created two data sets from  X  : (i)  X  by queries with query interest models and human judgments, and (ii)  X   X  created by splitting  X  into terms and filtering by term-origin pairs in  X   X  .  X   X  comprises 20 thousand unique queries, 109 thousand unique origins, 139 thousand query-origin pairs and 20 million unique trails.  X   X  comprises 15 thousand unique query terms, 109 thousand unique orig ins, 265 thousand term-origin pairs and 78 million unique trails. This filtering created high-quality data sets for our log-based investigation. We used four metrics to compare the best-trails selected using our trailfinding algorithms to compare sources (origins and trails). The metrics were coverage, diversit y, relevance, and utility. These metrics were chosen to capture many important elements of in-formation seeking, as highlighted by relevant research (e.g., [6] [7]). The use of multiple metrics a llows us to compare the value of the sources in different ways and also understand how the trail-finding algorithms affect different aspects of information gain. Topic coverage is meant to reflect the value of each source in providing access to the central themes of the query topic. To esti-mate the coverage of each of source, we created a source interest model (  X   X  ) comprising ODP labels for each source assigned as described in Section 4.2.2. We then computed the fraction of the query interest model (  X   X  ) covered by  X   X  . That is: where l is an ODP label and  X   X  represents the normalized frequen-cy weight of that label in the query interest model  X   X  . Topic diversity estimates the fraction of unique query-relevant concepts surfaced by a given sour ce. Exposure to different pers-pectives and ideas may help users for complex or exploratory search tasks. To estimate the diversity of the information provided by each source we use an approach similar to our coverage esti-mation, but we only require the fr action of distinct category labels from  X   X  that appear in  X   X  (i.e., frequency is ignored). That is: where l is an ODP label and  X |  X  | is number of distinct  X  The next metric used to compare the trail sources was relevance to the query that initiated the trail. For each trail, we computed the average relevance judgment score with respect to the query. In this analysis, the missing judgments for a page were labeled Bad since the judged label data was quite exhaustive for each query and hence missing pages may signif y irrelevance to the query. We also studied the utility of each source, estimated using dwell time (i.e., the amount of time spent on a particular page by a user). Prior research has demonstrated that during search activity, a dwell time of 30 seconds or more on a Web page can be indicative of page utility [9]. We apply this threshold in our analysis to de-termine if a source contains at least one page of utility In all metrics used, a higher value is more positive. The metrics are computed for each source, micro-averaged within each query, and macro-averaged across all queries to obtain a single value for each source-metric pair. This ensures that all queries are treated equally and popular queries do not dominate aggregate metrics. More detail on the metrics is provided by White and Huang [29]. In this section so far we have described the research questions, the trail data preparation procedures, and the metrics used to evaluate the sources. Our methodology co mprised the following steps: 1. For each search trail  X  in  X   X  , assign ODP labels to all pages in  X  . Build source interest models for origin page and full trail. 
Compute metrics using methods described in Section 4.3. 2. For each query-origin pair, select the best trail using each trail-finding algorithm (  X   X _ X  X  X  X  X  ). For each trail  X  in  X   X _ X  X  X  X  X  metrics. Split findings on query length, query type (informa-tional versus navigational), and origin relevance. 3. For each query-origin pair, find the best trail by applying the term-matching approach to  X   X  , generate a trail set  X   X  X  X  X  X  X  X  compare trails in  X   X  X  X  X  X  X  X  to those in  X   X _ X  X  X  X  X  using our metrics. We report findings separately for each of our three research ques-tions. We use parametric statis tical testing where appropriate. Given the large sample sizes, all observed differences are signifi-cant at  X  &lt; 0.01 unless otherwise stated. Table 1 shows summary statistics and reports on the average per-formance of trails and origins over all trails in  X  testing involved paired  X  -tests with Bonferroni corrections. Coverage was computed using Equation 4. The average coverage scores of trails and origins are reported in the  X  X ll X  row of Table 1. Full trails show a 14% increas e in topic coverage over origins. Diversity was computed using Equation 5. The average diversity scores of trails were 15% higher than origins. Coverage and diversity increases for trails over origins reflect the extra information that users find during post-origin navigation. Although it seems that most of the value comes from origin pages, users can still derive value from trails, including benefits not cap-tured by our metrics (e.g., topic novelty). Relevance was computed using human relevance judgments on a six-point scale ranging from Bad (rating=0) to Perfect (rating=5). While the relevance of origins is on average Good , the average relevance of trails is Poor . We attribute this to mapping missing judgments for deep links in trails to the label Bad , perhaps related to dynamism in users X  information needs as they search [29]. Utility was estimated using dwell times . Findings show that just under half of origins are useful (43.7%) and over three-quarters of trails have useful pages (82.8%). This shows that the likelihood of finding a useful page via naviga tion is high, a finding supported by previous work on post-query search behavior [23]. This may also be because origins are search results, typically the starting points for a task, and hence have rapid click-though [17][21]. To determine the effect of trail length on trail performance, we segmented all search trails into three segments based on length=2, length=3-5, and length=6-10. We di d this because: (i) there were insufficient trails for a segment for each length, and (ii) so that we could maintain usable levels of tr ail variety in each segment. The findings are reported in Table 1 adjacent to  X  X rail Length. X  First, even small trails of length 2 added value over origins in terms of coverage, diversity and utility (coverage:+8%, diversity:+9%, utility:+65%). Second, trail length appears to affect trail value. For example, coverage increased from 9.2 to 10.3 (the gain over origin increased from 8% to 19%) in moving between length=2 to length=6-10. This suggests that the longer the trail, the more dif-ferent topic-related inform ation users are exposed to. The above findings show that trails can deliver value to users over origins. Even small trails of size 2 can add significant value. Al-though further study is needed, this analysis suggests that trails may be a useful addition to results on the SERP. Once we know that showing trails may help, the ne xt step is deciding which trails to show. We now report on trailfinding algorithm performance. We compare the best trails from  X   X _ X  X  X  X  X  selected by each of the seven algorithms for each query-origin pair. We used origins-only as a baseline for the algorithms. Results are shown in Table 2. Independent-measures analysis of variance (ANOVA) were used among eight sources (seven best full trails + origin) for each me-tric to measure statistical significance. Also, we carried out post-hoc Tukey tests to show if best-t rails were significantly better than origins. To select the best algorithm(s), each algorithm is first given votes equal to the number of algorithms it performs signifi-cantly better than, using post-hoc Tukey tests (  X  X  0.01). Those algorithms with the most votes performed best for each metric. Coverage: Frequency-based trails performed worst among seven algorithms with gain of only 11% over origin (9.4 vs. 8.5). This may be because frequent trails ar e typically short and may cover less of the topic space. Best-trails based on tree-size and tree-strength had average gain of 20% over origins. The trail diversity algorithm performed best with an average gain of 27% (10.7 vs. 8.5), perhaps because different domains discuss different aspects. Even though trails found by the di versity and strength algorithms were shorter than those found by the trail length algorithm, they covered more of the query topic. These and the findings for other metrics show that there are often better criteria than just length. Diversity: These findings are somewhat similar to the coverage metric. Length-based trails and stre ngth-based trails have an aver-age gain of 22% over the origin. As expected, the diversity algo-rithm performed best with on aver age a 30% gain (7.5 vs. 5.7). Relevance: Trails selected based on relevance scoring have the highest relevance of 1.4 ( Poor -Fair ). Length and depth based trails performed worst, each having average relevance of 0.5 ( Bad -Poor ). In long or deep trails, users may get sidetracked or information needs evolve during searching [2]. Utility: Best-trails based on trail length have highest utility with an average increase of 109% over origins (91.2 vs. 43.7). It seems that the longer the trail, the more likely a user finds a useful page. (  X  X 
Trail Length 
Segment Origin Relevance Query Length We also studied the effect of origin relevance on algorithm per-formance to determine whether best-trails add value to all results or only those with high or low orig in quality. We divided the data into two buckets: one with origins having the highest human-judged label for the query (note that this need not be Excellent ) and another with origins judged Poor or Bad . Best Origins: The coverage of origins increased from 8.5 to 10.0. More relevant origins appear to cover more of the topic space. Also, the coverage values of all best-trail algorithms have in-creased; for example, trails selected based on diversity increase coverage from 10.7 to 12.1 Howeve r, the percentage gain of full trails over origins has decreased to a maximum value of +21% for diverse trails (12.1 vs. 10.0) which is lower than the 27% cover-age gain for all origins. This can be explained by the fact that when origins are high quality, th e value added by trails drops. Similar results are observed for diversity. Second, trails found using relevance-based scoring performed fairly well: average relevance of Fair as compared to 1.4 ( Poor -Fair ) for all origins. This suggests that relevant origins may also link to relevant pages. Worst Origins: While the absolute coverage values of origin and full trails decreased, the percentage gain from trails increased across all trail selection algorithms. Diverse trails again performed best with an average increase of 36% compared to origin (10.2 vs. 7.5). This almost doubles the 21% increase we observed for best origins. Similar trends can be s een for diversity. Second, there was a decrease in utility for origins whereas some trail selection algo-rithms showed an increase. We studied the effect of query length and query intent on trail performance to determine whether tr ails were equally useful for all queries. We segmented query length in three ways: length=1, length=2-3, and length &gt; 3 (long queries). For query intent, we segmented the queries into navigational and informational intent based on click frequencies in sear ch engine logs separate from those used in this study. Per our definition, navigational queries led to a click on the same search result 95% of the time; informa-tional queries led to on average two or more different result clicks per query. The results from query intent were somewhat aligned with that of breakdown based on origin quality. Clear intent que-ries had trends similar to exper iments with best origins and infor-mational queries had trends similar to those of worst origins. Due to space constraints, we only discuss results on query length since those are also important for RQ3 . The experiments on query length showed no major difference among trailfinding algorithms. We observe similar behavior in term s of relative differences of full trails versus origins. On coverage and diversity metrics, the relev-ance-based trailfinding algorithm failed to obtain significant dif-ferences relative to the origin on long queries. Recall that the re-levance-based scoring finds trails based on the match between the query terms and trail titles/URLs. For longer queries, there may be more noise in the queries and th e trails found may not cover as much of the query topic space. A nother interesti ng finding was in the absolute values of utility of trails and origins. On long queries, utility increased, suggesting user s spent more time on Web pages following those queries. Interestingly, across all trails and the various segmentations there is at least one trailfinding algor ithm (and often many) that outper-forms the average over all trails followed by users (shown in the  X  X ll Trails X  rows). This suggests that trailfinding algorithms may be helpful to users, at least in cases where the benefit brought by the algorithm (e.g., a boost in dive rsity) matches the user intent. Next we report the quality of trai ls found using the term-matching based approach described in Secti on 3.4. We use this approach to find trails from  X   X  for all query-origin pairs for which we have best-trails selected from  X   X _ X  X  X  X  X  . This leads to a new set called  X   X  X  X  X  X  X  X  . Note that: (i) for comparability the same queries appeared in both sets, and (ii) creating  X   X  X  X  X  X  X  X  could result in associating new trails to query-origin pairs which were not logged. The average performance numbers for the trails in  X  reported in Table 2 alongside thos e obtained from best-trails of  X  First, the results from all best trail selections from term-based approaches have similar trends as that of best trails selections from the query based approach. This strongly suggests that our trail selection criteria can be effectively applied to unseen query-origin pairs. Second, the segment based on query length suggests robustness of this technique for longer queries, which posed a challenge because of possible noise. Thirdly, term-based trails have occasionally higher coverage and diversity. For example: for diversity-based best-trails starti ng with long queries, we have a coverage of 11.0 (i.e., 31% gain over origin) for term -based and 10.5 (i.e., 26% gain over origin) for query -based trails. Fourth, relevance dropped from 0.9 to 0.7. This suggests that despite the coverage and diversity gains, term -based trails are slightly less relevant than query-based trails, perhaps because the term-based technique finds trails that may on ly be partially query relevant. We have described a log-based st udy of various trailfinding algo-rithms to support post-query search interaction. Tra ils are selected from the search and browsing logs of many users. Our findings show that users X  trails bring them value, best-trails can be chosen that outperform users X  own trails , different trailfinding algorithms perform well under different metric s, and a term-matching variant lets algorithms effectivel y handle unseen queries. Our first research question compared the value of trails with ori-gin pages. The findings showed a significant increase in value for trails over origins across almost all metrics except relevance when we normalized for trail length. As more information is viewed by users, there is more opportunity for them to gain. Relevance de-graded because un-judge d pages were labeled Bad . If we ignore un-judged pages, trails have the same relevance as origins. Since search trails appeared to demonstrate value over origins, the next research question addressed the issue of whether we could find the trail from the available op tions that maximized coverage, diversity, relevance, and/or utility. Although there was no clear winner, the findings were roughly in line with our intuitions. The diversity algorithm that preferred trails with multiple domains performed best in terms of cove rage and diversity and the relev-ance-based algorithm preferring trails with a high query-to-title/URL match performed best in terms of relevance. Trail length algorithms had the best utility, perhaps because the longer the trail, the more likely that users would encounter a useful page. On average, trailfinding outperforms the trails that users follow them-selves. This suggests that there is typically a trail with higher re-turn than that followed by a user and, may improve a user X  X  search effectiveness if shown. It also allows us to exclude underperform-ing algorithms from further stud y (e.g., frequency, strength). As part of this analysis we studied the effects of origin quality, query type, and query length on trailfinding algorithm perfor-mance. The findings showed differences in the effectiveness of the algorithms depending on origin qualit y and query characteristics. Trails may not be appropriate for all search results and more work is needed to determine which results or queries deserve trails, to investigate trailfinding algorithms, and to explore ways to effec-tively select between these algorithms given different user needs. For example, if the user cares about topic coverage, then we should select trails based on the trail diversity algorithm. The final research question addresses whether our trailfinding approach could be adapted to handle unseen queries. Findings showed that performance was roughl y equivalent between the best trails selected from the logs and those generated based on our algorithm. The term-based approach saw an increase in the cover-age and diversity and a decrease in relevance. This could be part of a backoff strategy where we search within trails in logs and use those chosen through term matc hing if no trails are found. One limitation of this research is the assumption that there is a best trail for each query-result pair. It is conceivable that there will be multiple equivalent or complementary trails for any pairing. Ways to tiebreak between trails (e.g., showing trails that the user has not yet traveled) need to be e xplored. More work is needed to validate metrics used, in particular measures of coverage, diversi-ty, and utility currently inferred from interactions (e.g., [29]). The next step in our research is to show trails on SERPs. Trails can be presented as an alternative to result lists, as instant answers above result lists, in pop-ups sh own after hovering over a result, below each result in addition to the snippet and URL, or even on the click trail the user is following. Although we are limited by what can be inferred from log data, our approach has provided insight on what algorithms perf orm best and when. Follow-up user studies and large-scale flights are planned to compare trail presentation methods and further an alyze trailfinding techniques. In this paper we have presented a study of trailfinding techniques to support Web search. We employed a log-based methodology to afford us control over experimental variables and rapidly test mul-tiple trailfinding algorithms. We s howed that trails provided addi-tional value over trail origins, especially for longer trails that may contain more varied information. We experimented with different trailfinding algorithms and showed that they can outperform trails followed by most users; their performance was affected by the relevance of the origins and query characteristics, meaning that trails may need to be tailored to query and result properties. We also tested a term matching variant that alleviated the need for an exact term match between queries and trails, which led to cover-age and diversity gains at the cost of a slight decrease in relev-ance. In future work we will integrate best-trails into search en-gine result pages and conduct user studies on their effectiveness. [1] Agichtein, E., Brill, E. &amp; Dumais, S. (2006). Improving web [2] Bates, M.J. (1989). The design of browsing and berrypicking [3] Bilenko, M. &amp; White, R.W. (2008). Mining the search trails [4] Bush, V. (1945). As we may think. Atl. Monthly , 3(2): 37-46. [5] Chalmers, M., Rodden, K. &amp; Brodbeck, D. (1998). The order [6] Clarke, C.L.A. et al. (2008). N ovelty and diversity in infor-[7] Cole, M. et al. (2009). Usefulne ss as the criterion for evalua-[8] Downey, D., Dumais, S. &amp; Horvitz, E. (2007). Models of [9] Fox, S. et al. (2005). Evaluating implicit measures to im-[10] Freyne, J. et al. (2007). Coll ecting community wisdom: inte-[11] Guinan, C. &amp; Smeaton, A.F. (1993). Information retrieval [12] Hammond, N. &amp; Allison, L. (1988). Travels around a learn-[13] Joachims, T. (2002). Optimizi ng search engines using click-[14] Joachims, T., Freitag, D. &amp; M itchell, T. (1997). WebWatcher: [15] Olston, C. &amp; Chi, E.H. (2003) . ScentTrails: integrating [16] O X  X ay, V. &amp; Jeffries, R. (1993). Orienteering in an informa-[17] Pandit, S. &amp; Olston, C. (2007) . Navigation-aided retrieval. [18] Pirolli, P. &amp; Card, S.K. (1999). Information foraging. Psy-[19] Shen, X., Dumais, S. &amp; Horvitz, E. (2005). Analysis of topic [20] Singhal, A. (2001). Modern information retrieval: a brief [21] Teevan, J. et al. (2004). The perfect search engine is not [22] Trigg, R.H. (1988). Guided tours and tabletops: tools for [23] Vakkari, P. &amp; Taneli, M. (2009). Comparing google to ask-a-[24] Wang, X. &amp; Zhai, C. (2009). Beyond hyperlinks: organizing [25] Wexelblat, A. &amp; Maes, P. ( 1999). Footprints: history-rich [26] Wheeldon, R. &amp; Levene, M. (2003). The best trail algorithm [27] White, R.W., Bilenko, M., &amp; Cu cerzan, S. (2007). Studying [28] White, R.W. &amp; Drucker, S.M. (2007). Investigating beha-[29] White, R.W. &amp; Huang, J. (2010) . Assessing the scenic route: [30] Zellweger, P.T. (1989). Scripted documents: a hypermedia 
