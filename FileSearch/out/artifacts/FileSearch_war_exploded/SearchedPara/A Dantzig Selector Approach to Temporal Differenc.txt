 Matthieu Geist matthieu.geist@supelec.fr Sup  X elec, IMS Research Group, Metz, France Bruno Scherrer bruno.scherrer@inria.fr INRIA, MAIA Project Team, Nancy, France INRIA Lille -Team SequeL, France An important problem in reinforcement learning (RL) ( Sutton &amp; Barto , 1998 ) is to estimate the quality of a given policy through the computation of its value function ( e.g. , in the policy evaluation step of a policy iteration). Oftentimes, the state space is to large, and thus, approximation schemes must be used to repre-sent the value function. Furthermore, whenever the model (reward function and probability transitions) is unknown, the best approximation should be computed using a set of sampled transitions. Many algorithms have been designed to solve this approximation prob-lem. Among them, LSTD (Least-Squares Temporal Differences) ( Bradtke &amp; Barto , 1996 ) is the most pop-ular. Using a linear parametric representation, LSTD computes the fixed-point of the Bellman operator com-posed with the orthogonal projection.
 In many practical scenarios, the number of features of the linear approximation is much larger than the num-ber of available samples. For example, one may want to consider a very rich function space, such that the actual value function lies in it. Unfortunately, in this case, learning is prone to overfitting. A standard ap-proach to face this problem is to introduce some form of regularization. While LSTD has been often paired with ` 2 -regularization, only recently ` 1 -regularization (see Sec. 2.2 for a thorough review of the main ` 1 -regularized algorithms) has been considered to deal with high X  X imensional problems. This approach is particularly appealing since ` 1 -regularization implic-itly performs feature selection and targets sparse solu-tions. In particular, LASSO-TD ( Kolter &amp; Ng , 2009 ) can be seen as en extension of LASSO ( Tibshirani , 1996 ) to temporal difference learning, to which it re-duces when setting the discount factor to zero. How-ever, LASSO-TD is not derived from a proper con-vex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy set-ting. Although other algorithms have been proposed to overcome these drawbacks ( e.g. , ` 1 -PBR by Geist &amp; Scherrer ( 2011 )), other disadvantages may appear. This paper introduces a new algorithm, Dantzig-LSTD (D-LSTD for short, see Sec. 3 ), which extends the Dantzig Selector (DS) ( Candes &amp; Tao , 2007 ) to tempo-ral difference learning. Instead of solving a fixed-point problem as in LASSO-TD, it can simply be cast as a linear program, thus allowing to use any off-the-shelf solver. Furthermore, since the underlying optimiza-tion problem is convex, it can handle off-policy learn-ing in a principled way. Yet, when LASSO-TD is well defined, both algorithms provide similar solutions (see Prop. 2 ), as DS does w.r.t. LASSO. We show that for some oracle choice of the regularization factor, the D-LSTD solution converges quickly to the LSTD solution (at a rate depending only logarithmically on the num-ber of features), as shown in Theorem 1 . This new algorithm also opens some issues, namely how well is the true value function estimated and how to efficiently choose the regularization factor. These points are dis-cussed in Sec. 4 . Finally, we report some illustrative empirical results in Sec. 5 . A Markov reward process 1 (MRP) is a tuple { S, P, R,  X  } , where S is a finite state space, P = and  X  is a discount factor. The value function V is de-fined as the expected cumulative reward from a given fixed-point of the Bellman operator T : V  X  R +  X P V . In many practical applications, the model of the MRP ( i.e. , the reward R and transitions P ) is unknown and only a set of n transitions { ( s i , r i , s 0 i ) 1  X  i  X  n able. In general, we assume that states s 1 , . . . , s n sampled from a sampling distribution  X  (not necessar-ily the stationary distribution of the MRP) and the next states s 0 1 , . . . , s 0 n are generated according to the transition probabilities p (  X | s i ). Whenever the state space is too large, the value function cannot be com-puted exactly at each state and a function approxi-mation scheme is needed. We consider value functions  X  V  X  defined as a linear combination of p basis functions  X  ( s ), that is  X  V  X  ( s ) = P p i =1  X  i  X  i ( s ) =  X  &gt; denote by  X   X  R | S | X  p the feature matrix whose rows contain the feature vectors  X  ( s ) &gt; for any s  X  S . This defines a hypothesis space H = {  X   X  |  X   X  R p } , which contains all the value functions that can be represented by the features  X  . The objective is to find the function  X  V  X   X  that approximates V the best. 2.1. LSTD Let  X   X  denote the orthogonal projection onto H w.r.t. the sampling distribution  X  . If D  X  is the diagonal matrix with elements  X  ( s ) and M  X  =  X  &gt; D  X   X  is the Gram matrix, then the projection operator is  X   X  =  X  M  X  1  X   X  &gt; D  X  . Motivated by the fact that the value function is the fixed point of the Bellman operator T , the LSTD algorithm computes the fixed X  X oint of A  X  R p  X  p and b  X  R p as A =  X  &gt; D  X  ( I  X   X P ) X  and b =  X  &gt; D  X  R . In the following we assume that A and M  X  are invertible. It can be shown through simple algebra that  X  V  X   X  is the fixed-point of  X   X  T if and only if  X  the (unique) solution to A X   X  = b . This relationship is particularly interesting since it shows that computing the fixed point  X   X  T is equivalent to solving a linear system of equations defined by A and b .
 Since P and R are not usually known, we have to rely on sample X  X ased estimates. In particular, we de-fine  X   X  (resp.  X   X  0 )  X  R n  X  p the empirical feature ma-trices whose rows contain the feature vectors  X  ( s i ) &gt; components r i . The random matrices  X  A and  X  b are then defined as  X  A = 1 n  X   X  &gt;  X   X   X  and  X  b = 1 n  X   X 
 X  =  X   X   X   X   X   X  0 . LSTD computes the solution  X  0 of the sample X  X ased linear system  X  A X  0 =  X  b . We no-tice that both  X  A and  X  b are unbiased estimators of the model X  X ased matrices A and b ( i.e. , E [  X  A ] = A and E [  X  b ] = b ), thus suggesting that as the number of sam-ples increases, the solution of LSTD  X  0 converges to the model X  X ased solution  X   X  . Since LSTD computes the fixed point of the joint operator  X   X  T , then the sample X  X ased LSTD solution can also be formulated in an equivalent form as the solution of two nested optimization problems: where the first equation projects the image of the es-timated value function  X  V  X  under the Bellman operator T onto the hypothesis space H , and the second one solves the related fixed-point problem. 2.2. Related Work When the number of samples is close or smaller than the number of features, the matrix  X  A is ill X  X onditioned and some form of regularization should be employed to solve the LSTD problem. In this section, we review the state X  X f X  X he X  X rt regularized LSTD algorithms. The formulation of LSTD in Eq. 1 is particularly helpful in understanding the different regularization schemes that could be applied to LSTD. In particular, each of the minimizations relative to the operators  X   X  and T can be regularized, thus obtaining: (  X  With this formulation, all the regularization schemes for LSTD (except ` 1 -LSTD, which we discuss at the end of this section) can be summarized as in Tab. 1 . Ridge regression ( i.e. , ` 2 -regularization) is the most common form of regularization and it simply adds a term  X I to  X  A . This corresponds to  X  1 pen 1 (  X  ) =  X  k  X  k and  X  2 = 0 and it has been generalized by Farah-mand et al. ( 2008 ) with ` 2 , 2 -LSTD, where both penalty terms use an ` 2 -norm regularization. Although these approaches can help in dealing with ill X  X efined  X  A ma-trices, they are not specifically designed for the case of n p , where the optimal solution is sparse. In fact, it is well-known that, unlike ` 1  X  X egularization, ` 2 does not promote sparsity, and thus, it might fail when the number of samples is much smaller than the number of features.
 The ` 1 -regularization has been introduced more re-cently with LASSO-TD, where the projection is re-placed by an ` 1 -penalized projection. In this case, the nested optimization problem in Eq. 1 reduces to solving the fixed-point problem (if well defined):  X  rithm has been first introduced by Kolter &amp; Ng ( 2009 ) under the name LARS-TD, where it is solved using an ad X  X oc variation of the LARS algorithm ( Efron et al. , 2004 ). For LARS-TD to find a solution,  X  A must be a P-matrix. 2 Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the ex-periments reported in Kolter &amp; Ng 2009 ), it would be desirable to remove or relax this condition. The LARS-TD idea is further developed by Johns et al. ( 2010 ), where LASSO-TD is reframed as a linear complemen-tary problem. This allows using any off-the-shelf LCP solver (notably some of them allow warm-starts, which may be of interest in a policy iteration context), but the P-matrix condition is still required, since it is in-herent to the optimization problem and not to how it is actually solved. Finally, the theoretical properties of LASSO-TD were analyzed by Ghavamzadeh et al. ( 2011 ), who provided prediction error bounds in the on-policy fixed design setting ( i.e. , the performance is evaluated on the points in the training set). In par-ticular, they show that, similarly to LASSO in regres-sion, the prediction error depends on the sparsity of the projection of the value function ( i.e. , the ` 0 -norm of the  X  parameter of  X   X  V ), and it scales only loga-rithmically with the number of features. This implies that even if the dimensionality of H is much larger than the number of samples, the LASSO-TD accu-rately approximates the true value function in the on X  policy setting. In order to alleviate the P-matrix prob-lem, the ` 1 -PBR (Projected Bellman residual) ( Geist &amp; Scherrer , 2011 ) and the ` 2 , 1 -LSTD ( Hoffman et al. , 2011 ) algorithms have been proposed. The idea is to place the ` 1 -regularization term in the fixed-point equation instead of the projection equation. This cor-responds to adding an ` 1 -penalty term to the projected Bellman residual minimization (writing  X   X  the empiri-cal projection and  X  T the sampled Bellman operator):  X  is a convex optimization problem, there is no problem for  X  A not being a P-matrix, and off-the-shelf LASSO solvers can be used. However, this comes at the cost of a high computational cost if n p (notably in the computation the empirical projection, which could be as bad as O ( p 3 )), and there is no theoretical analysis. Finally, a novel approach has been introduced by Pires ( 2011 ). The idea is to consider the linear system for-mulation of LSTD ( i.e. , A X  = b ) and to add an ` 1 -penalty term to it:  X  1 , X  = argmin  X  k  X  A X   X   X  b k 2 2 We refer to this algorithm as ` 1 -LSTD. Being defined as a proper convex optimization problem, it does not have theoretical problems in the off-policy setting and any standard solver can be used. Notice that for  X  = 0, ` -LSTD does not reduce to a known algorithm. The Dantzig-LSTD (D-LSTD for short) algorithm that we propose in this paper returns an estimate  X  d, X  ( i.e. , a value function V  X  der the constraint that the Bellman residual (  X  R +  X   X 
 X  0  X   X   X   X   X  ), namely the correlated Bellman residual (  X 
 X  rameter  X  . Formally, D-LSTD solves:  X  d, X  = argmin This optimization problem is convex and can be easily recast as a linear program (LP): This algorithm is closely related to DS ( Candes &amp; Tao , 2007 ), to which it reduces when  X  = 0. Being a con-vex optimization problem, it does not require  X  A to be a P-matrix and it can be solved using any LP solver (no-tably the efficient primal X  X ual interior point method of Candes &amp; Tao 2007 , which makes use of the Wood-bury matrix identity when n p ). 3.1. A Finite Sample Analysis In this section we study how well the D-LSTD solution  X  d, X  compares to  X   X  , i.e. , the model X  X ased LSTD so-lution satisfying A X   X  = b . The analysis follows similar steps as in Pires ( 2011 ) for ` 1 -LSTD. In the following, we use the assumption that the samples are generated i.i.d. from an arbitrary sampling distribution  X  . We leave as future work the extension to Markov design ( i.e. , when the samples are generated from a single trajectory of the policy under evaluation).
 Theorem 1. Let B  X  , X  = max s  X  S k  X  ( s ) k  X  , the D-LSTD solution  X  d, X  (Eq. 2 ) satisfies with probability at least 1  X   X  .
 Proof. (sketch) We first need a concentration result for the `  X  -norm. Let x 1 , . . . , x n be i.i.d. random vec-tors with mean  X  x  X  R d and bounded by k x i k  X   X  B . Using Hoeffding inequality and a union bound, it is easy to show that with probability greater that 1  X   X  , one has k 1 n P n i =1 x i  X   X  x k  X   X  B q 2 n ln  X   X  sistency inequality: k A X  k  X   X k A k max k  X  k 1 . Com-bined with the triangle inequality, this gives: |k A X   X  b k  X   X  X   X  A X   X   X  b k  X  | X   X  A, max k  X  k 1 +  X  b, max . Let us choose  X  =  X  A, max k  X   X  k 1 +  X  b, max . The previous in-equality implies that k  X  A X   X   X   X  b k  X   X   X  (recall that A X   X  = b ). Combined with the fact that  X  d, X  mini-mizes Eq. 2 , we have that k  X  d, X  k 1  X k  X   X  k 1 . Combin-ing the previous results, we obtain k A X  d, X   X  b k  X   X  for k . k  X  can be used to bound  X  A, max and  X  b, max , which gives the stated result, using the fact that k  X  ( s i )(  X  ( s i )  X   X  X  ( s 0 i )) T k max  X  B 2  X  , X  Since the algorithm is specifically designed for the high X  X imensional setting ( n p ), it is critical to study the dependency of the performance on n and p . Up to constant terms, the previous bound can be written as First we notice that as the number of samples in-creases, the error of  X  d, X  tends to zero, thus implying that it matches the performance of the model X  X ased LSTD solution  X   X  . Furthermore, the dependency on the number of features p is just logarithmic, while the ` -norm of  X   X  is assumed to be small whenever the solution is sparse. This suggests that D-LSTD could work well even in the case n p whenever the problem admits a sparse LSTD solution. Finally, we also no-tice that there is no specific assumption regarding the learning setting, except that A should be invertible. This is particularly important because it means that, unlike most of the other results available for LSTD ( e.g. , see Ghavamzadeh et al. 2011 ), this result holds also in the off-policy setting. The main drawback of this analysis is that it holds for an oracle choice of  X  . We postpone a discussion about how to choose the regularizer in practice to Sec. 4 . 3.2. Comparison to Other Algorithms Similar to ` 1 -PBR and ` 2 , 1 -LSTD, D-LSTD is based on a well-defined standard convex optimization problem, which does not require  X  A to be a P-matrix (unlike LASSO-TD) and that can be solved using any off-the-shelf solvers. Nonetheless, D-LSTD has only one meta-parameter (instead of two), and in general, it has a smaller computational cost w.r.t. solving the nested optimization problems of ` 1 -PBR and ` 2 , 1 -LSTD. D-LSTD is also related to LASSO-TD: Proposition 2. The LASSO-TD solution  X  l, X  (if it exists) satisfies the D-LSTD constraints: Proof. The optimality conditions of LASSO-TD can be obtained by ensuring that 0 belongs to the sub-gradient of 1 2 k  X   X   X   X  (  X  R +  X   X   X  0  X  l, X  ) k 2 2 substituting  X  by  X  l, X  ( Kolter &amp; Ng , 2009 ). This notably implies that for all 1  X  i  X  p , we have  X   X   X  (  X  b  X   X  A X  l, X  ) i  X   X  , which is the stated result. Therefore, D-LSTD and LASSO-TD satisfy the same constraints, but k  X  l, X  k 1  X k  X  d, X  k 1 , thus suggesting a more sparse solution. This is not surprising, since D-LSTD relates to LASSO-TD in a similar way as DS does to LASSO ( Bickel et al. , 2009 ). However, thanks to its definition as a convex optimization problem, D-LSTD avoids the main drawbacks of LASSO-TD (no-tably the P-matrix requirement).
 Similar to ` 1 -LSTD, D-LSTD is built on the linear system of equations formulation of LSTD. Both ap-proaches relax the condition  X  A X  =  X  b (using an ` 2 -norm of the error for ` 1 -LSTD and an `  X  -norm for D-LSTD) while penalizing model complexity through the ` 1 -norm of the parameter vector. Both algorithms have the same advantages compared to LASSO-TD and to ` 1 -PBR/ ` 2 , 1 -LSTD. Their main difference lies in their convergence rate. A result similar to Theo-rem 1 exists for ` 1 -LSTD ( Pires , 2011 ): Although controlling the ` 2 -norm (in ` 1 -LSTD) may be harder than the `  X  -norm (as in D-LSTD), ` 1 -LSTD has a very poor dependency on p , which makes the bound not informative as n p . On the other hand, D-LSTD just has a logarithmic dependency on p . In this section, we discuss how the error || A X   X  b || relates to the value function prediction error and how to choose the regularizer  X  in practice. 4.1. From the Parameters to the Value Similar to Yu &amp; Bertsekas ( 2010 ), we can link V  X   X  to A X   X  b as in the next theorem.
 Theorem 3. For any  X  V  X  =  X   X  , we have the component X  X ise equality: Proof. Recall that V = T V (for the true value func-tion) and that  X  V  X  =  X   X   X  V  X  (for an estimate  X  V  X  =  X   X  belonging to the hypothesis space). We have that: result.
 In order to have the final prediction error, we apply the `  X  -norm to Theorem 3 . Let L  X   X  = max s k M  X  1  X   X  ( s ) k using Theorem 1 , we obtain In general, the previous expression cannot be sim-plified any further. Nonetheless, under a high X  dimensional assumption  X   X  P = P and  X   X  R = R . Therefore, the hypothesis space H is stable by the Bellman operator T and V  X  X  . In this case, we have k V  X   X   X  V k  X  = 0 and it can be shown that k ( I  X   X   X   X  P )  X  1 k  X  = 1 (valid also in the off-policy case): The main critical term in this bound is L  X   X  , which might hide a dependency on the number of features p . In fact, although the specific value of L  X   X  depends on the feature space, it is possible to find cases when it grows as potentially neutralizing the low dependency on p in Theorem 1 . It is an open question whether this de-pendency on p is intrinsic to the algorithm or is an ar-tifact of the proof. In fact, if  X  d, X  solves the linear sys-tem of equations accurately, then we expect that the corresponding function  X  V  X  as the model X  X ased solution  X  V  X   X  . The experiments of Section 5 seem to confirm this conjecture. 4.2. Cross Validation The result of Theorem 1 holds for an oracle value of  X  . In practice, the choice of  X  can only be directed by the available data. This issue is of great prac-tical importance, though not often discussed in the RL literature (especially for ` 1 -penalized LSTD vari-ations). In supervised learning, algorithms minimize a risk being defined as the (empirical) expectation of some loss function. Cross-validation consists in using an independent sample to estimate the true risk func-tion, and the meta-parameter is selected as the one minimizing the estimated true risk. However, for value function estimation, there is no such risk, and cross-validation cannot be used. A general model selection method has been derived for value function estimation by Farahmand &amp; Szepesv  X ari ( 2011 ). However, we may devise an ad X  X oc (and simple) solution for D-LSTD. Since D-LSTD is defined as a proper convex optimiza-tion problem (which reduces to a supervised learning problem when  X  tends to 0), one may be tempted to use standard cross-validation. Unfortunately, this is not directly possible. Indeed, k  X  A X   X   X  b k  X  is the loss the empirical expectation of a loss. However, we can still consider some heuristics. Assume that we want to estimate k A X   X  b k  X  for some fixed parameter vec-tor  X  . Let  X  A ,  X  b be unbiased estimates of A , b , then given an independent set of samples, we have access to an unbiased estimate of an upper X  X ound of k A X   X  b k  X  . Based on this evidence, we propose a K-fold cross-validation-based heuristic for D-LSTD. Assume that the training set is split in K folds F k . Let  X  (  X  k ) d, X  note the estimate trained without F k , and  X  A F  X  b in F k . A heuristic is to choose the  X  that minimizes However, since we are interested in the case n p and the estimate  X  A F may have a high variance. An alternative (which we empirically found to be more efficient), at the cost of adding some bias, is to choose  X  by minimizing A similar heuristic can be devised for ` 1 -LSTD. Al-though the previous heuristic worked well in our ex-periments, it does not have any theoretical guarantees. A different model selection strategy has been devised for ` 1 -LSTD by Pires ( 2011 ). It consists in choosing  X  exponential grid and  X  0 can be computed from data (no oracle choice). This does not require splitting the learning set while ensuring a bound for k A X  1 ,  X   X   X  b k We leave the adaptation of this model selection strat-egy to D-LSTD for future work. Sec. 5.1 presents an example that shows D-LSTD al-leviates the potential problem of off-policy learning. Sec. 5.2 reports a more complex corrupted chain il-lustrating the case of n p , in an on-and off-policy setting, and studies (heuristic) cross-validation. 5.1. A Pathological MDP We consider a simple two-state MDP ( e.g. , see Kolter &amp; Ng 2009 ). The transition matrix and reward vector are P = value function is therefore V =  X  1 1  X   X   X  1 &gt; with  X  the discount factor. Let us consider the one-feature ap-proximation  X  = 1 2 &gt; . We compare the (asymp-totic) regularization paths of LASSO-TD ( Kolter &amp; Ng , 2009 ), ` 1 -LSTD and D-LSTD, in the on-policy and off-policy cases (where LASSO-TD fails). On-policy Case. In the on-policy case, the sampling distribution is  X  &gt; = 0 1 . The regularization paths for each algorithm can be computed easily by solving analytically the optimality conditions (there is only one parameter) and they are reported in Fig. 1 , top panels. LASSO-TD and D-LSTD have the same regu-larization path. This was expected, as there is only one parameter, but this is not true in general (recall that LASSO-TD and D-LSTD inherit the same differences as LASSO and DS).
 Off-policy Case. Let us now consider the uniform distribution  X  &gt; = 1 2 1 2 . For  X  &gt; 5 6 , A is not a P-matrix and LASSO-TD does not have a unique solu-tion, nor a piecewise linear regularization path. Paths are shown on Fig. 1 , bottom panels. The ` 1 -LSTD X  X  path is still well-defined. LASSO-TD has more than one solution. The interesting fact here is that D-LSTD X  X  path is well-defined, there is always a unique solution, and the path is piecewise linear. Note that both in the on-and off-policy cases all the algorithms provide the LSTD solution for  X  = 0. 5.2. Corrupted Chain We consider the same chain problem as in Kolter &amp; Ng ( 2009 ) and Hoffman et al. ( 2011 ). The state s has  X  s + 1 components s i . The first one is an in-teger ( s 1  X  X  1 . . . 20 } ) that evolves according to a 20-state, 2-action MDP (states are connected by a chain, the action chooses the direction, and the prob-ability of success is 0.9). All other state compo-nents are random Gaussian noises s i +1 t  X  X  (0 , 1). The reward is +1 if s 1 t = 1 or 20. The feature vector  X  ( s )  X  R  X  s +6 consists of an intercept (con-stant function), 5 radial basis functions correspond-ing to the first state component, and  X  s identity functions corresponding to the irrelevant components:  X  ( s ) = 1 RBF 1 ( s 1 ) . . . RBF 5 ( s 1 ) s 2 . . . s compare LASSO-TD (with its LARS-like implemen-tation), ` 1 -LSTD, and D-LSTD (for which we used ` -magic ( Romberg , 2005 )). 3 We standardize the data by removing the intercept, centering the observations, centering and standardizing the features  X   X , and apply-ing the same transformation (computed from  X   X ) to  X   X  0 The intercept can be computed analytically, it is the mean Bellman error (without regularization, this al-lows recovering the LSTD solution). We also consider ` 2 ,  X  -LSTD, i.e., the standard ` 2 -penalized LSTD. On-policy Evaluation. We first study the on-policy problem. The evaluated policy is the optimal one (go-ing left if s 1  X  10, and right otherwise). We sample 400 transitions (20 trajectories of length 20 started randomly on { 1 . . . 20 } ) and vary the number  X  s of ir-relevant features between 800 and 1400. Results are presented in Fig. 2 , averaged over 20 independent runs. For LARS-TD, we computed the whole regularization path (at least until too many features are added) and trained the other algorithms for a set of regularization parameters (logarithmically spaced between 10  X  3 and 10). Each time, we report the best prediction error (on 500 test points, such that the first state compo-nent is uniformly sampled from { 1 . . . 20 } ), computed w.r.t. the true value function (therefore, this is an ora-cle choice). All ` 1 -penalized approaches perform signif-icantly better than the ` 2 -penalization ones, showing that their performance have only a very mild depen-dency on the dimensionality p (as predicted by The-orem 1 ). Among them, LASSO-TD seems to be con-sistently better, followed closely by D-LSTD and ` 1 -LSTD. For LASSO-TD and ` 2 ,  X  -LSTD, these results are consistent with those published by Hoffman et al. ( 2011 ). Notice that there was more choice of regular-ization parameters for LASSO-TD, as the whole reg-ularization path was computed. This may explain the better results of LASSO-TD compared to D-LSTD. Heuristic Cross-validation. All results of Fig. 2 require an oracle to choose the right regularization pa-rameter. This is not practical in a real setting. As explained in Sec. 4 , ` 1 -LSTD and D-LSTD can benefit from a heuristic cross-validation scheme. We tested K -fold cross-validation (with K = 5) on this problem, with the schemes J 1 (Eq. 4 ) and J 2 (Eq. 5 ) for n = 400 training samples and  X  s = 800 irrelevant features (re-sults averaged over 20 independent runs). Results are reported in Tab. 2 . The error is computed as before, but here it is not used to choose the regularization pa-rameter. The results for J 1 are quite bad, probably due to the high variance of the related estimator (still, for D-LSTD, the right regularization parameter is of-ten chosen, apart from a few outliers). The J 2 scheme is much better, comparable to the oracle scheme (see Fig. 2 ). Comparing the results of the J 2 heuristic using a Behrens-Fisher t-test, ` 1 -LSTD and LASSO-TD are different (5% risk), but not D-LSTD and LASSO-TD. Off-policy Evaluation. Here we test the off-policy evaluation problem. Let  X  opt be the optimal pol-icy (going left if s 1  X  10 and right otherwise) and  X  worst = 1  X   X  opt (going right if s 1  X  10 and left oth-erwise). We define  X   X  = (1  X   X  )  X  opt +  X  X  worst , with  X   X  [0 , 1 2 ]. Let also  X   X  be the corresponding station-ary distribution and recall V is the true value func-tion. We consider the same problem as before, with  X  s = 800. For values of  X  varying from 0 to 0 . 5, we sample n = 400 chain states according to  X   X  as well as the associated transitions according to the optimal policy. The regularization parameter is chosen to min-imize the error between the true value function and the estimated one on the training set (thus, an oracle-like selection procedure), for all algorithms. Results are averaged over 50 independent runs. Fig. 3 shows the error k  X  V  X   X  V k  X   X  as a function of  X  . The term 0 cor-responds to the zero prediction, that is k V k  X   X  . In all cases, D-LSTD seems to be slightly better than the others, and things get worse as going away from the stationary distribution (as  X  increases). In no case LASSO-TD seems to suffer from off-policy learning, suggesting that in this case the P -matrix condition is satisfied. Also, the difference between ` 2 -and ` 1 -schemes decreases as  X  increases. An ` 1 -schemes may help when there are much more features than samples, but there is little to do when the mismatch between distributions increases. Even if not reported, all ap-proaches performed equally bad when  X  tends to one, since there is no more valuable information in the data. In this paper, we introduced the Dantzig-LSTD al-gorithm with the objective of removing the drawbacks of existing ` 1 -schemes for temporal difference learning. Since D-LSTD is defined as a standard linear program, it does not require  X  A to be a P-matrix and can be com-puted using any LP solver. The D-LSTD estimate is a good approximation of the asymptotic LSTD solu-tion in the sense of Theorem 1 . It is also close to the LASSO-TD estimate (whenever well defined) in the sense of Prop. 2 . In fact, D-LSTD inherits the same difference that the Dantzig selector has w.r.t. LASSO. Also, our preliminary experiments show that D-LSTD performs at least as well as LASSO-TD.
 There are still a number of issues that need further in-vestigation. As discussed in Sec. 4 , when moving from the linear system of equations to the prediction error, an additional dependency on the number of features seems to appear. To which extent this dependency is an artifact of the proof or a characteristic of the algo-rithm is not fully clear yet. As for the choice of the regularization parameter, we plan to adapt the model selection scheme of ` 1 -LSTD ( Pires , 2011 ) to D-LSTD and test it. Finally, we plan to test D-LSTD in control schemes ( i.e. , policy iteration).
 Acknowledgments The first author thanks the R  X egion Lorraine for financial support. The third and fourth authors would like to thank French National Research Agency (ANR) under project LAMPADA n  X  ANR-09-EMER-007, European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreement n  X  231495, and PASCAL2 European Net-work of Excellence for supporting their research.
