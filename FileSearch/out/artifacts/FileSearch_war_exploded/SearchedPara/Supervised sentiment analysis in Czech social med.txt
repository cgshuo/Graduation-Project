 1. Introduction
Sentiment analysis has become a mainstream research field since the early 2000s. Its impact can be seen in many sentence-level, or aspect-based sentiment ( Hajmohammadi, Ibrahim, &amp; Othman, 2012 ).
Most of the research in automatic sentiment analysis of social media has been performed in English and Chinese, as few attempts, although the importance of sentiment analysis of social media became apparent, for example, during the  X  the leading Czech companies for social media monitoring.

Automatic sentiment analysis in the Czech environment has not yet been thoroughly targeted by the research commu-documented datasets, as known from many shared NLP tasks, may stimulate competition, which usually leads to the production of cutting-edge solutions. 3 Creative Commons BY-NC-SA licence 5 at http://liks.fav.zcu.cz/sentiment .

Section 5.3 explores the influence of feature selection methods. 2. Related work
There are two basic approaches to sentiment analysis: dictionary-based and machine learning-based. Whereas dictio-nary-based methods usually depend on a sentiment dictionary (or a polarity lexicon) and a set of handcrafted rules porate auxiliary unlabeled data ( Zhang, Si, &amp; Rego, 2012 ). 2.1. Supervised machine learning for sentiment analysis
Lee, and Vaithyanathan (2002) experimented with unigrams (presence of a certain word, frequencies of words), bigrams, further improvements in delta TFIDF weighting.
 ( Go et al., 2009; Agarwal, Xie, Vovsha, Rambow, &amp; Passonneau, 2011 ).
 must contain an emoticon.

Balahur and Tanev (2012) performed experiments with Twitter posts as part of the CLEF 2012 RepLab. captured the manner in which sentiment is expressed in social media.

Finally, we would like to direct the reader to an in-depth survey by Tsytsarau and Palpanas (2012) for actual results obtained from the above-mentioned methods. 2.2. Feature selection for improving classification performance ( Abbasi, France, Zhang, &amp; Chen, 2011 ).
 A study by Sharma and Dey (2012) compares five methods for feature selection, namely Information Gain, Chi Square,
Gain Ratio, Relief-F, and Document Frequency, together with seven different classifiers. Results are reported on the sorted by their frequency.

Abbasi, Chen, and Salem (2008) proposed an entropy-weighted genetic algorithm that combines Information Gain with a method called the Feature Relation Network. This manually constructed network of feature dependencies (e.g., subsump-tion 7 or parallel relations of various n -grams) relies on SentiWordNet in order to assign the final feature weights. One of the classical papers on feature selection for text classification by Forman (2003) proposes a metric called Tian, &amp; Qu, 2009; Aghdam, Ghasem-Aghaee, &amp; Basiri, 2009 ).
 small datasets with highly skewed classes and conclude by recommending two best-performing algorithms, especially for scenarios that require a small number of features. Another approach based on dynamic mutual information is presented from machine learning-based sentiment analysis.
 (2009) , who report Chi-square selection results in their preliminary tests without any success. 2.3. Sentiment analysis in the Czech environment be drawn.
 resulting sentiment dictionaries were merged using the overlap of the two automatic translations. A multilingual parallel news corpus annotated with opinions on entities was presented in ( Steinberger, Lenkova, saved annotation time and guaranteed comparability of opinion mining evaluation results across languages. The corpus including Czech. The research targets fundamentally different objectives from our research as it focuses on news media and aspect-based sentiment analysis.
 Recent experiments with incorporating word clusters as additional features to tackle the issue of the high flection of 3. Datasets 3.1. Social media dataset pages. The posts were then completely anonymized as we kept only their textual contents.
We also added another class called bipolar which represents both positive and negative sentiment in one post. cases, the user X  X  opinion, although positive, does not relate to the given page. in these cases, in accordance with our above-mentioned assumption.

The complete 10 k dataset was independently annotated by two annotators. The inter-annotator agreement (Cohen X  X  j ) sidered as well-defined.
 used irony, sarcasm or the context of previous posts. These issues remain open.
 reveal negative opinions towards cell phone operators ( www.facebook.com/o2cz , www.facebook.com/TmobileCz , and www.facebook.com/vodafoneCZ ) and positive opinions towards, e.g., perfume sellers ( www.facebook.com/Xparfemy.cz ) and Prague Zoo ( www.facebook.com/zoopraha ). 3.2. Movie review dataset Movie reviews as a corpus for sentiment analysis have been used in research since the pioneering research conducted by respectively. 3.3. Product review dataset dataset consists of 145,307 posts (102,977 positive, 31,943 neutral, and 10,387 negative). 4. Classification 4.1. Preprocessing remove stopwords using the stopword list from Apache Lucene project.
In many NLP applications, a very popular preprocessing technique is stemming. We tested the Czech light stemmer ( Dolamic &amp; Savoy, 2009 ) and High Precision Stemmer. imented with a lemmatizer based on Ispell . Following their work, we developed an in-house lemmatizer using rules and dictionaries from the OpenOffice suite.

Part-of-speech tagging was done with our in-house Java solution that utilizes Prague Dependency Treebank (PDT) data as informal language (see, e.g., ( Gimpel et al., 2011 ) where similar issues were tackled in English). and thus we decided to keep diacritics intact.

We were also interested in whether named entities (e.g., product names, brands, places, etc.) carry sentiment and how analysis, but Boiy and Moens (2009) , for example, remove the  X  X ntity of interest X  in their approach.  X  X ipe X  configurations. 4.2. Features 4.2.1. n-Gram features
We use presence of unigrams and bigrams as binary features. The feature space is pruned by minimum n -gram occur-rence empirically set to five. Note that this is the baseline feature in most of the related work. 4.2.2. Character n-gram features tains 3-grams to 6-grams. 4.2.3. POS-related features
Direct usage of part-of-speech n -grams that cover sentiment patterns has not shown any significant improvement in the adverbs ( Kouloumpis et al., 2011 ), and the number of negative verbs obtained from POS tags. 4.2.4. Emoticons
The feature captures the number of occurrences of each class of emoticons within the text. 4.2.5. Delta TFIDF variants for binary scenarios
Although simple binary word features (presence of a certain word) achieve a surprisingly good performance, they have (positive/negative), and thus we filtered out neutral documents from the datasets.
IDF , Delta Smoothed Prob. IDF , and Delta BM25 IDF . 4.3. Feature selection can be cut off at a certain threshold.

Let t k and t k denote the presence and absence, respectively, of a particular feature in a certain class (e.g., c
Then N denotes the total number of features in all classes, N  X  a  X  b  X  c  X  d . The joint probability p  X  t mated as and similarly for p  X  t k ; c 2  X  . The probability of a particular feature in all classes p  X  t
Furthermore, c 1 can be estimated as
The conditional probability of t k given c 1 is given by
Henceforth, let n denote the number of classes, m  X f t k ; t ods, please refer to, e.g., ( Forman, 2003; Zheng, Wu, &amp; Srihari, 2004; Uchyigit, 2012; Patoc  X  ka, 2013 ). 4.3.1. Mutual Information (MI) Mutual Information is always non-negative and symmetrical, MI  X  X ; Y  X  X  MI  X  Y ; X  X  . 4.3.2. Information Gain (IG) Also known as Kullback X  X eibler divergence or relative entropy . It is a non-negative and asymmetrical metric. 4.3.3. Chi Square (CHI) Chi Square ( v 2 ) can be derived as follows: 4.3.4. Odds Ratio (OR) 4.3.5. Relevancy Score (RS) 4.4. Classifiers
All evaluation tests were performed with two classifiers, Maximum Entropy (MaxEnt) and Support Vector Machines worse than SVM or MaxEnt. We used a pure Java framework for machine learning
SVM). 5. Results
For each combination from the preprocessing pipeline (refer to Table 1 ) we assembled various sets of features and Thelwall, 2010 ). We also involved only the MaxEnt classifier into the second scenario.
Review dataset where most of the labels are positive. 5.1. Social media icons, that were not covered by the emoticon feature. On average, the best results were obtained when HPS stemmer and preprocessing techniques for token-based features (see column FS4: Unigr + bigr + POS + emot. ). weighted form of the word feature does not improve the performance, compared with the simple binary unigram feature. gle TFIDF-weighted feature.

Furthermore, we report the effect of the dataset size on the performance. We randomly sampled 10 subsets from the most combinations of features and hence adding more data does not lead to a significant improvement. 5.1.1. Upper limits of automatic sentiment analysis fourth person. Thus even the original annotators do not achieve a 1.00 F -measure on the gold data. task the way a human would.
 5.2. Product and movie reviews
For the other two datasets, the product reviews and movie reviews, we slightly changed the configuration. First, we puting. Second, we abandoned SVM as it became computationally infeasible for such large datasets. related features do not carry any useful information in this case and also bring too much  X  X oise X  to the classifier. drop in performance. We can conclude that for larger texts, the bigram-based feature outperforms unigram features and, in some cases, a proper preprocessing may further significantly improve the results.
 brands) are very strong opinion-holders and thus their filtering significantly decreases classification performance. 5.3. Feature selection experiments
Using the two most promising preprocessing pipelines ( ShCl , ShPe ), we conducted experiments with feature selection dataset and using only MaxEnt on the other datasets (because of computational feasibility, as mentioned previously in Section 5.2 ).

In the previous experiments (Section 5 ), the feature space was pruned by a minimum occurrence which was empirically the experiments on the Facebook data. 21
Figs. 3 X 6 show dependency graphs of the macro F -measure given a feature weight threshold. Note that these figures performance even with a very small filtering threshold.

Overall, a significant improvement from 73.38% (baseline) to 73.85% was achieved for the product reviews, by means of without feature space pruning ( Tables 10, 11 , respectively) no significant improvement was achieved. speed up the classification by filtering out noisy features. 5.4. Summary of results for social media Given the results achieved on the Facebook dataset, the following strategies for sentiment analysis of social media in mance for our dataset.

It should be noted that the number of examined domains was limited because we restricted our dataset only to the top nine most popular Czech Facebook brand pages. It is thus worth investigating how the system would tackle the issues of domain-dependent features and domain portability. This remains an open question for future work. 6. Conclusion
This article presented in-depth research on supervised machine learning methods for sentiment analysis of Czech social media. We created a large Facebook dataset containing 10,000 posts, accompanied by human annotation with substantial agreement (Cohen X  X  j 0.66). The dataset is freely available for non-commercial purposes. in two other domains (movie and product reviews) with a significant improvement over the baseline. media in such a thorough manner. Not only does it use a dataset that is magnitudes larger than any in the related work to set the common ground for sentiment analysis for the Czech language but also help to extend the research beyond the mainstream languages and may be applied to sentiment analysis in other Slavic languages, such as Slovak or Polish. Acknowledgments from the University of West Bohemia, and by the European Regional Development Fund (ERDF), project  X  X  X TIS  X  New Tech-programme  X  X  X rojects of Large Infrastructure for Research, Development, and Innovations X  X  (LM2010005), is gratefully acknowledged. Access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scien-greatly appreciated. We thank Tom X  X  Brychc X n for his High-Precision Stemmer implementation, Michal Konkol for his machine learning library, and Michal Patoc  X  ka for his implementation of feature selection algorithms. References
