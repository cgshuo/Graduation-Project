 the marginals are at their mode, specifically mean (conditioned on ).
 off diagonal covariance information ( Barber et al. , 2011 ). also include the projection weights f w 1 ; w 2 ; : : : ; w M g . for each dimension.
 1 .
 This involves calculating transition and observation matrices and Q . the m -th projection will result in a GMP with matrices: notation from now on, understanding that the , and Q matrices correspond to the current projection. NLML can be written as For GMP, the terms in the sum can be efficiently calculated by running a Kalman filter on the chain. The log marginal likelihood (log Z ( )) can be written in closed form as ( to a weight components of the projection vector ( d log Z (  X  ) dw likelihood derivative can be written as where, Minka, T. Divergence measures and message passing. Technical report, Microsoft Research, 2005. Rasmussen, C.E. and Williams, C.K.I. Gaussian Processes for Machine Learning . The MIT Press, 2006. 2011.
 Algorithm 1 Gaussian Process Regression using SSMs transition function stfunc that returns and Q matrices. Hyperparameters . moments: E ( z t z  X  t ), E ( z t z  X  t  X  1 ) for t 1 . . . K do end for E ( z E ( z for t K 1 . . . 1 do
