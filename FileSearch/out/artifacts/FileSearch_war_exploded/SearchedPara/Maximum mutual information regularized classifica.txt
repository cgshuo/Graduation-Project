 fi er, we propose to maximize the mutual information between classi 1. Introduction discrete class label to a given data sample represented by its feature vector ( Cai et al., 2014; Ojala et al., 2002; Sun et al., 2014;
Li et al., 2014; Li et al. ). It has many applications in various including bioinformatics ( Alipanahi et al., 2009; Wang et al., 2013a; J.J. Wang et al., 2012; J. Wang et al., 2012; Liu et al., 2012 ), biometrics veri fi cation ( Wang, 2009; Roy et al., 2011;
Tafazzoli et al., 2010 ), computer networks ( Yang et al.; Xu et al., 2014, 2013 ), and computer vision ( Cai et al., 2013; Wang et al., 2014a, 2013b; Zhou et al., 2010 ). For example, in the face recogni-tion problem, given a face image, the target of pattern classi tion is to assign it to a person who has been registered in a database ( Jonathon Phillips et al., 2000; Zhao et al., 2003 ). This problem is usually composed of two different components  X  feature extraction ( Sun et al., 2012; Zhou et al., 2013; Wang and
Gao, 2013; Al-Shedivat et al., 2014; Wang et al., 2014b, 2013c, 2013d; J.-Y. Wang et al., 2012 ) and classi fi cation ( Zhou et al.;
Subbulakshmi and Afroze, 2013 ). Feature extraction refers to the procedure of extracting an effective and discriminant feature vector from a data sample, so that different samples of different classes could be separated easily. This procedure is usually highly domain-speci fi c. For example, for the face recognition problem, the visual feature should be extracted using some image proces-sing technologies, whereas for the problem of predicting zinc-binding sites from protein sequences,the biological features should be extracted using some biological knowledge ( Chen et al., 2013 ).
In terms of feature extraction of this paper, it is highly inspired by a hierarchical Bayesian inference algorithm proposed in Zhou et al. (2013) . This new method created in Sun et al. (2012) has advanced the ground-truth feature extraction fi eld and has provided a more optimal method for this procedure. On the other hand, different from feature extraction, classi fi cation is a much more general problem. We usually design a class label prediction function as a classi fi er for this purpose. To learn the parameter of a classi function, we usually try to minimize the classi fi cation error of the training samples in a training set and simultaneously reduce the complexity of the classi fi er. For example, the most popular classi fi er is support vector machine (SVM), which minimizes the hinge losses to reduce the classi fi cation error, and at the same time minimizes the  X  2 norm of the classi fi er parameters to reduce the complexity. In this paper, we focus on the classi fi cation aspect. the information shared between two sets of variables. It has been used as a criterion of feature extraction for pattern classi problems ( Sun and Xu, 2014 ). However, surprisingly, it has never been information has a strong relation to Kullback-Leibler divergence, and there are many works using KL-divergence for classi fi ers ( Moreno et al., 2004; Liu and Shum, 2003 ). Moreno et al. (2004) proposed a novel kernel function for support vector classi fi cation based on Kullback-Leibler divergence, while Liu and Shum (2003) proposed to learn the most discriminating feature that maximizes the Kullback-Leibler divergence for the Adaboost classi fi er. However, both these methods do not use the KL-divergence based criterion to learn parameters of linear classi fi ers. To bridge this gap, in this paper, for the try to investigate using mutual information as a criterion of classi learning. We propose to learn a classi fi er by maximizing the mutual mutual information is de fi ned as the information shared between f and y . From the viewpoint of information theory, if the two variables are not mutually independent, and one variable is known, it usually reduces the uncertainty about the other one. Then mutual informa-tion is used to measure how much uncertainty is reduced in this case. To illuminate how the mutual information can be used to measure the classi fi cation accuracy, we consider the two extreme cases:
On one hand, if the classi fi cation response variable f of a data sample is randomly given, and it is independent of its true class label y , then knowing f does not give any information about y and vice versa, and the mutual information between them could be zero, i.e., I  X  f ; y  X  X  0.

On the other hand, if f is given so that y and f are identical, knowing f can help determine the value of y exactly as well as reduce all the uncertainty about y . This is the ideal case of classi fi cation, and knowing f can reduce all the uncertainty about y . In this case, the mutual information is de fi ned as the uncertainty contained in f (or y ) alone, which is measured by the entropy of f or y , denoted by H ( f )or H ( y ) respectively, where H  X  X  is the entropy of a variable. Since f and y are identical, we can have I  X  f ; y  X  X  H  X  f  X  X  H  X  y  X  .
 Naturally, we hope that the classi fi cation response f can predict the true class label y as accurately as possible, and knowing f can reduce the uncertainty about y as much as possible. Thus, we propose to maximize the mutual information between f and y with regard to the parameters of a classi fi er. To this end, we proposed a mutual information regularization term for the learning of classi-fi er parameters. An objective function is constructed by combining the mutual information regularization term, a classi fi cation error term and a classi fi er complexity term. The classi fi er parameter is learned by optimizing the objective function with a gradient descend method in an iterative algorithm.
 The rest parts of this paper are organized as follows: in Section 2 , we introduce the proposed classi fi er learning method. The experiment results are presented in Section 3 .In Section 4 the paper is concluded. 2. Proposed method
In this section, we introduce the proposed classi fi er learning algorithm to maximize the mutual information between the classi fi cation response and the true class label. 2.1. Problem formulation
We suppose that we have a training set denoted as X  X f x i where x i A R d is the d -dimensional feature vector for the i -th training sample, and n is the number of training samples. The class label set for the training samples is denoted as Y  X f y i y
A f X  1 ; 1 g is the class label of the i -th sample. To learn a classi fi er to predict the class label of a given sample with its feature vector x , we design a linear function as a classi g  X  x ; w  X  X  sign  X  f  X  X  sign  X  w &gt; x  X  ;  X  1  X  where w is the classi fi er parameter vector, f  X  w &gt; x is the classi cation response of x given the classi fi er parameter w , and sign  X  X  is the signum function which transfers the classi fi cation response to the fi nal binary classi fi cation result. We also denote the classi tion response set of the training samples as F  X f f i g n f  X  w &gt; x i A R is the classi fi cation response of the i -th training sample. To learn the optimal classi fi cation parameter w for the classi fi cation problem, we consider the following three problems: 2.1.1. Classi fi cation loss minimization
To learn the optimal classi fi cation parameter w , we hope the classi fi cation response f of a data sample x obtained with the learned w can predict its true class label y as accurately as possible. To measure the prediction error, we use a loss function to compare a classi fi cation response against its corresponding true class label. Given the classi fi er parameter w , the loss function of the i -th training sample x i with its classi fi cation response f i loss functions which could be considered.

Squared Loss is usually used by regression problems ( X. Wang et al.,
Logistic Loss is de fi ned as follows, and it is also popular in
Exponential Obviously, to learn an optimal classi fi er, the average loss of all the training samples should be minimized with regard to w . Thus the following optimization problem is obtained by applying a loss function to all training samples, min 2.1.2. Classi fi er complexity reduction
To reduce the complexity of the classi fi er to prevent the over-fi tting problem, we also regularize the classi fi er parameter by a norm term as min 2.1.3. Mutual information maximization mutual information I  X  f ; y  X  between the classi fi cation response variables f A F and the true class label variables y A Y . The mutual information between two variables f A F and y A Y is de fi
I  X  f ; y  X  X  H  X  f  X  H  X  f j y  X  ;  X  9  X  where H ( f ) is the marginal entropy of f , which is used to measure on y , which is used as the measure of uncertainty of f when y is given. To use the mutual information as a criterion to learn the With the estimated entropy H ( f ) and the conditional entropy
H  X  f j y  X  , the mutual information between the variable f and y could be rewritten as the function of parameter w by substituting f i  X  w &gt; x i , ~
I  X  f ; y ; w  X  X  H  X  f  X  H  X  fy  X 
To learn the classi fi er parameter w , we maximize the mutual information with regard to w , max
Remark. It should be noted that similar to our method, the algorithm proposed in Liu and Shum (2003) maximizes KL-divergence between the class PDF, p  X  f j y  X  c  X  , and the total
PDF, p ( f ), therefore, Liu and Shum (2003) has relation to method in Kullback-Leibler boosting. However, different from our method, it uses KL-divergence as a criterion to select the most discriminat-ing features, whereas our method uses mutual information as a criterion to learn the classi fi er parameter. 2.1.4. Overall optimization problem (8) and (17) , the optimization problem for the proposed classi parameter learning method is obtained as min where  X  and  X  are tradeoff parameters. In the objective function, there are three terms. The fi rst one is optimized so that the prediction error is minimized, the second term is used to control the complexity of the classi fi er, and the last term is introduced so that the mutual information between the classi fi cation response and the true class label can be maximized. 2.2. Optimization closed-form solution, we try to optimize it using the gradient descend method in an iterative algorithm ( Dempster et al., 1977 ).
In each iteration, we employ the gradient descend method to update w . According to the optimization theory, if Q  X  w  X  is de and differentiable in a neighborhood of a point w old , then Q  X  w  X  decreases faster if w goes from w old in the direction of the negative gradient of Q  X  w  X  at w old ,  X  Q  X  w  X j w  X  w old . Thus the new w is obtained by where  X  is the descend step.
 calculated as  X  Q  X  w  X  X  1 ~ I  X  f ; y ; w  X  respectively. They are given analytically as follows. 2.2.1. Computation of  X  L  X  f i ; y i ; w  X  We give the analytical gradients of different de fi nitions of L  X  f i ; y i ; w  X  as follows:
Squared Loss is a smooth function, and its gradient is
Logistic Loss is also smooth with its gradient as 2.2.2. Computation of  X  ~ I  X  f ; y ; w  X 
The gradient of ~ I  X  f ; y ; w  X  is computed as  X  ~
I  X  f ; y ; w  X  X   X  n where the gradients of p  X  w &gt; x i  X  and p  X  w &gt; x i as  X   X  p  X  w &gt; x i j y  X  c  X  X  1 n  X  w 3. Experiments
In this section, we evaluate the proposed classi fi cation method on two real world pattern classi fi cation problems. 3.1. Experiment I: zinc-binding site prediction
Zinc is an important element for many biological processes of an organism, and it is closely related to many different diseases. Moreover, it is also critical for proteins to play their functional functional annotation of zinc-binding proteins is necessary to biological process control and disease treatment. To this end, predicting zinc-binding sites of proteins shows its importance in bioinformatics problems. In the fi rst experiment, we evaluate the proposed classi fi cation method on the problem of predicting zinc-binding sites. 3.1.1. Data set and protocol
For the purpose of experiment, we collected a set of amino acids of four types, which are CYS, HIS, GLU and ASP (CHED). These four types are the most common zinc-binding site types, which take up roughly 96% of the known zinc-binding sites. In the collected data set, there are 1937 zinc-binding CHEDs and 11,049 non-zinc-binding CHEDs, resulting a data set of 13,986 data samples. Given a candidate CHED, the problem of zinc-binding binding site. In this experiment, we treated a zinc-binding CHED as a positive sample, and a non-zinc-binding CHED as a negative sample. To extract features from a CHED, we computed the position speci fi c substitution matrices (PSSM) ( Kelley et al., 2000 ), the relative weight of gapless real matches to pseudocounts (RW-GRMTP) ( Menchetti et al., 2006 ), Shannon entropy ( Chen et al., 2013 ), and composition of k -spaced amino acid pairs (CKSAAP) ( Zhang et al., 2013 ), and concatenated them to form a feature vector for each data sample. Note that the value of each feature was scaled to the range between 1 and 1, so that the performance does not depend on the selection of scaling.
To conduct the experiment, we used the 10-fold cross valida-tion protocol ( Burman et al., 1989 ). The entire data set was split into ten non-overlapping folds, and each fold was used as a test set in turn, while the remaining nine folds were combined and used as a training set. The proposed algorithm was performed to the training set to learn a classi fi er from the feature vectors of the training samples, and then the learned classi fi er was used to predict the class labels of the test samples. Note that the tradeoff parameters of the proposed algorithm was tuned within the training set. The averaged value of the hyper-parameters  X  are 5.8 and 44.8. The parameter  X  was computed as  X   X   X  dist , where dist was the median value of distances between pairs of training samples, and the averaged value of  X  was 0.451. The classi fi cation performance was measured by comparing the pre-dicted labels against the true labels. The receiver operating characteristic (ROC) and recall  X  precision curves were used as performance metrics. The ROC curve was obtained by plotting true positive rates (TPRs) against false positive rates (FPRs), while recall  X  precision curve was obtained by plotting precision against recall values. TPR, FPR, recall, and precision are de fi ned as TPR  X  TP TP  X  FN ; FPR  X  FP FP  X  TN ; recall  X  TP TP  X  FN ; precision  X  TP TP  X  FP ;  X  27  X  where TP , FP , FN and TN represent the number of true positives, false positives, false negatives and true negatives, respectively. Moreover, area under ROC curve (AUC) ( Zou and Yue, 2013 )was used as a single performance measure. A good classi fi er should achieve a ROC curve close to the top left corner of the fi recall  X  precision curve close to the top right corner, and also a high AUC value. 3.1.2. Results
In this experiment, we compared the proposed mutual information without mutual information regularization, so that the improvement achieved by maximum mutual information regularization could be veri fi ed. The four different loss functions listed in Section 2 were considered, and the corresponding classi fi ers were evaluated here. The ROC and recall  X  precision curves of four loss functions based classi fi cation methods are given in Fig. 1 . The proposed maximum mutual information regularized method is denoted as  X  MaxMutInf  X  after a loss function title in the fi gure.Itturnsoutthatmaximum mutual information regularization improves all the four loss functions tions achieved different performances, all of them could be boosted by reducing the uncertainty about true class labels, which could be measured by the mutual information between class labels and Precision Precision Precision Precision classi fi cation responses. Therefore, th e results show that maximizing mutual information is highly effective in reducing uncertainty of true class labels, and hence it can signi fi cantly improve the quality of classi fi cation.
 Moreover, we also plotted AUCs of different methods in Fig. 2 . Again, we observe that maximum mutual information regulariza-tion improves different loss functions based classi fi ers. We also can see that among these four loss functions, hinge loss achieves the highest AUC values, while squared loss achieves the lowest. The AUC value of classi fi ers regularized by both hinge loss and mutual information is 0.9635, while that of squared loss and mutual information is even lower than 0.95. The performances of logistic and exponential loss functions are similar, and they are between the performances of hinge loss and squared loss.
Since the mutual information is used as a new regularization technique, we are also interested in how the proposed regulariza-tion alone works. We therefore compared the following three cases. 1. Conventional case which only uses the classi fi cation loss reg-ularization. This case is corresponding to setting  X   X  0in (18) .
In this case, we only used the hinge loss since it has been shown that this loss function obtains better accuracy than other loss functions. 2. Mutual information regularization case which is corresponding to the problem in (18) when the fi rst term is ignored. 3. Hybrid regularization case which is the proposed framework which combines the classi fi cation loss minimization and mutual information regularization.
 The comparison results are given in Fig. 3 . It can be seen that the conventional case which only uses the hinge loss function achieved better results than the method with only mutual infor-mation regularization, and the hybrid regularization achieved the best results. This means mutual information regularization cannot obtain good performance by itself and should be used with traditional loss functions. 3.2. Experiment II: HEp-2 cell image classi fi cation
Antinuclear Autoantibodies (ANA) test is a technology used to determine whether a human immune system is creating antibo-dies to fi ght against infections. ANA is usually done by a speci fl uorescence pattern of HEp-2 cell images ( Agrawal et al., 2013 ). Recently, there is a great need for computer based HEp-3 cell image classi fi cation, since manual classi fi cation is time-consuming and not accurate enough. In the second experiment, we will evaluate the performance of the proposed classi fi er on the problem of classifying HEp-2 cell images. 3.2.1. Data set and protocol
In this experiment, we used the database of HEp-2 cell images of the ICIP 2014 competition of cell classi fi cation by fl image analysis ( Peter Hobson, 2013 ). In this data set, there are 13,596 cell images, and they belong to six cell classes, which are namely Centromere, Golgi, Homogeneous, Nucleolar, Nuclear-Membrane, and Speckled. Each cell image is segmented by a mask image showing the boundary of the cell. Moreover, the entire data set is composed of two groups of different tensity types, which are Intermediate and Positive. Overall, the intermediate group out-numbers the positive group, with an exception that, for the cases of Centromere and Speckled, the latter marginally outnumbers the former. The number of images in different classes of two groups is given in Fig. 4 . To present each image for the classi fi problem, we extracted shape and texture features and concate-nated them to form a visual feature vector ( Agrawal et al., 2013 ).
Experiments were conducted in two groups respectively. We also adopted the 10-fold cross validation for the experiment. To handle the problem of multiple class problem, we used the one-against-all strategy. Each class was treated as a positive class in turn, while all remaining fi ve classes were combined to form a negative class. A classi fi er was learned for each class to discrimi-nate it from other classes. A test sample was assigned to a class with the largest classi fi cation response. The classi fi was used as a classi fi cation performance metric. 3.2.2. Results the two groups of HEp-2 cell image data set are given in Fig. 5 .
From this fi gure, it could be observed that for both groups of data sets, the proposed regularization method can improve the classi-fi cation performances signi fi cantly, despite the variety of loss functions. It can also be seen that the performances on the second group (Positive) is inferior to that of the fi rst group (Intermediate). This indicates that it is more dif fi cult to classify cell images when their contrast is low. However, the improve-ment achieved by mutual information regularization is consistent over these two groups. 4. Conclusions uncertainty about its true class label? In this paper, we proposed this question and tried to answer it by learning an optimal classi to reduce such uncertainty. Insighted by the fact that the reduced uncertainty can be measured by the mutual information between classi fi cation responses and true class labels, we proposed a new classi fi er learning algorithm, by maximizing the mutual information when learning the classi fi er. Particularly, our algorithm adds a maximum mutual information regularization term. We investigated the classi fi cation performances when maximum mutual informa-tion was used to regularize the classi fi er learning based on four different loss functions. The experimental results show that the proposed regularization can improve the classi fi cation perfor-we will study how to apply the proposed algorithm on large scale data set based on some distributed big data platforms ( Su et al., 2013; Y. Wang et al., 2012, 2013; Sun et al., 2010 ).
 Acknowledgments University of Science and Technology (KAUST), Saudi Arabia. References
