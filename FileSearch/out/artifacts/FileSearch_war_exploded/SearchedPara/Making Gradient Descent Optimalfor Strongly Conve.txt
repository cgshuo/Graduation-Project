 Alexander Rakhlin rakhlin@wharton.upenn.edu University of Pennsylvania Ohad Shamir ohadsh@microsoft.com Microsoft Research New England Karthik Sridharan skarthik@wharton.upenn.edu University of Pennsylvania Stochastic gradient descent (SGD) is one of the sim-plest and most popular first-order methods to solve convex learning problems. Given a convex loss func-tion and a training set of T examples, SGD can be used to obtain a sequence of T predictors, whose av-erage has a generalization error which converges (with T ) to the optimal one in the class of predictors we con-sider. The common framework to analyze such first-order algorithms is via stochastic optimization, where our goal is to optimize an unknown convex function F , given only unbiased estimates of F  X  X  subgradients (see Sec. 2 for a more precise definition).
 An important special case is when F is strongly con-vex (intuitively, can be lower bounded by a quadratic function). Such functions arise, for instance, in Sup-port Vector Machines and other regularized learning algorithms. For such problems, there is a well-known O (log( T ) /T ) convergence guarantee for SGD with av-eraging. This rate is obtained using the analysis of the algorithm in the harder setting of online learning (Hazan et al., 2007), combined with an online-to-batch conversion (see (Hazan &amp; Kale, 2011) for more details). Surprisingly, a recent paper by Hazan and Kale (Hazan &amp; Kale, 2011) showed that in fact, an O (log( T ) /T ) is not the best that one can achieve for strongly convex stochastic problems. In particular, an optimal O (1 /T ) rate can be obtained using a different algorithm, which is somewhat similar to SGD but is more complex (al-though with comparable computational complexity) 1 . A very similar algorithm was also presented recently by Juditsky and Nesterov (Juditsky &amp; Nesterov, 2010). These results left an important gap: Namely, whether the true convergence rate of SGD, possibly with some sort of averaging, might also be O (1 /T ), and the known O (log( T ) /T ) result is just an artifact of the analysis. Indeed, the whole motivation of (Hazan &amp; Kale, 2011) was that the standard online analysis is too loose to analyze the stochastic setting properly. Perhaps a similar looseness applies to the analysis of SGD as well? This question has immediate practical relevance: if the new algorithms enjoy a better rate than SGD, it might indicate they will work better in practice, and that practitioners should abandon SGD in favor of them.
 In this paper, we study the convergence rate of SGD for stochastic strongly convex problems, with the fol-lowing contributions:  X  First, we extend known results to show that if F  X  We then show that for non-smooth F , there are  X  However, we show that one can recover the op- X  We perform an empirical study on both artificial Moreover, our rate upper bounds are shown to hold in expectation, as well as in high probability (up to a log(log( T )) factor). While the focus here is on getting the optimal rate in terms of T , we note that our up-per bounds are also optimal in terms of other standard problem parameters, such as the strong convexity pa-rameter and the variance of the stochastic gradients. Following the paradigm of (Hazan &amp; Kale, 2011), we analyze the algorithm directly in the stochastic setting, and avoid an online analysis with an online-to-batch conversion. This also allows us to prove results which are more general. In particular, the standard online analysis of SGD requires the step size of the algorithm at round t to equal 1 / X t , where  X  is the strong con-vexity parameter of F . In contrast, our analysis copes with any step size c/ X t , as long as c is not too small. In terms of related work, we note that the performance of SGD in a stochastic setting has been extensively re-searched in stochastic approximation theory (see for instance (Kushner &amp; Yin, 2003)). However, these re-sults are usually obtained under smoothness assump-tions, and are often asymptotic, so we do not get an ex-plicit bound in terms of T which applies to our setting. We also note that a finite-sample analysis of SGD in the stochastic setting was recently presented in (Bach &amp; Moulines, 2011). However, the focus there was dif-ferent than ours, and also obtained bounds which hold only in expectation rather than in high probability. More importantly, the analysis was carried out un-der stronger smoothness assumptions than our anal-ysis, and to the best of our understanding, does not apply to general, possibly non-smooth, strongly con-vex stochastic optimization problems. For example, smoothness assumptions may not cover the applica-tion of SGD to support vector machines (as in (Shalev-Shwartz et al., 2011)), since it uses a non-smooth loss function, and thus the underlying function F we are trying to stochastically optimize may not be smooth. We use bold-face letters to denote vectors. Given some vector w , we use w i to denote its i -th coordinate. Simi-larly, given some indexed vector w t , we let w t,i denote its i -th coordinate. We let 1 A denote the indicator function for some event A .
 We consider the standard setting of convex stochas-tic optimization, using first-order methods. Our goal is to minimize a convex function F over some convex domain W (which is assumed to be a subset of some Hilbert space). However, we do not know F , and the only information available is through a stochastic gra-dient oracle, which given some w  X  W , produces a vector  X  g , whose expectation E [  X  g ] = g is a subgradient of F at w . Using a bounded number T of calls to this oracle, we wish to find a point w T such that F ( w t ) is as small as possible. In particular, we will assume that F attains a minimum at some w  X   X  X  , and our analysis provides bounds on F ( w t )  X  F ( w  X  ) either in expecta-tion or in high probability (the high probability results are stronger, but require more effort and have slightly worse dependence on some problem parameters). The application of this framework to learning is straightfor-ward (see for instance (Shalev-Shwartz et al., 2009)): given a hypothesis class W and a set of T i.i.d. exam-ples, we wish to find a predictor w whose expected loss F ( w ) is close to optimal over W . Since the examples are chosen i.i.d., the subgradient of the loss function with respect to any individual example can be shown to be an unbiased estimate of a subgradient of F . We will focus on an important special case of the prob-lem, characterized by F being a strongly convex func-tion. Formally, we say that a function F is  X  -strongly convex , if for all w , w 0  X  X  and any subgradient g of F at w , Another possible property of F we will consider is smoothness, at least with respect to the optimum w  X  . Formally, a function F is  X  -smooth with respect to w  X  if for all w  X  X  , Such functions arise, for instance, in logistic and least-squares regression, and in general for learning linear predictors where the loss function has a Lipschitz-continuous gradient.
 The algorithm we focus on is stochastic gradient de-scent (SGD). The SGD algorithm is parameterized by step sizes  X  1 ,..., X  T , and is defined as follows: 1. Initialize w 1  X  X  arbitrarily (or randomly) 2. For t = 1 ,...,T : This algorithm returns a sequence of points w 1 ,..., w T . To obtain a single point, one can use several strategies. Perhaps the simplest one is to return the last point, w T +1 . Another procedure, for which the standard online analysis of SGD applies (Hazan et al., 2007), is to return the average point For stochastic optimization of  X  -strongly functions, the standard analysis (through online learning) focuses on the step size  X  t being exactly 1 / X t (Hazan et al., 2007). Our analysis will consider more general step-sizes c/ X t , where c is a constant. We note that a step size of  X (1 /t ) is necessary for the algorithm to obtain an optimal convergence rate (see Appendix A in the full version (Rakhlin et al., 2011)).
 In general, we will assume that regardless of how w 1 is initialized, it holds that E [ k  X  g t k 2 ]  X  G 2 for some fixed constant G . Note that this is a somewhat weaker as-sumption than (Hazan &amp; Kale, 2011), which required that k  X  g t k 2  X  G 2 with probability 1, since we focus only on bounds which hold in expectation. These types of assumptions are common in the literature, and are generally implied by taking W to be a bounded do-main, or alternatively, assuming that w 1 is initialized not too far from w  X  and F satisfies certain technical conditions (see for instance the proof of Theorem 1 in (Shalev-Shwartz et al., 2011)).
 Full proofs of our results are provided in Appendix B of the full version of this paper (Rakhlin et al., 2011). We begin by considering the case where the expected function F (  X  ) is both strongly convex and smooth with respect to w  X  . Our starting point is to show a O (1 /T ) for the last point obtained by SGD. This result is well known in the literature (see for instance (Nemirovski et al., 2009)) and we include a proof for completeness. Later on, we will show how to extend it to a high-probability bound.
 Theorem 1. Suppose F is  X  -strongly convex and  X  -smooth with respect to w  X  over a convex set W , and that E [ k  X  g t k 2 ]  X  G 2 . Then if we pick  X  t = c/ X t for some constant c &gt; 1 / 2 , it holds for any T that
E [ F ( w T )  X  F ( w  X  )]  X  The theorem is an immediate corollary of the following key lemma, and the definition of  X  -smoothness with respect to w  X  .
 Lemma 1. Suppose F is  X  -strongly convex over a con- X  = c/ X t for some constant c &gt; 1 / 2 , it holds for any T that We now turn to discuss the behavior of the average point  X  w T = ( w 1 + ... + w T ) /T , and show that for smooth F , it also enjoys an optimal O (1 /T ) conver-gence rate (with even better dependence on c ). Theorem 2. Suppose F is  X  -strongly convex and  X  -smooth with respect to w  X  over a convex set W , and that E [ k  X  g t k 2 ]  X  G 2 . Then if we pick  X  t = c/ X t for A rough proof intuition is the following: Lemma 1 implies that the Euclidean distance of w t from w  X  is on the order of 1 / from w  X  is on the order of ((1 /T ) P T t =1 1 / and the rest follows from smoothness. We now turn to the discuss the more general case where the function F may not be smooth (i.e. there is no constant  X  which satisfies Eq. (2) uniformly for all w  X  X  ). In the context of learning, this may hap-pen when we try to learn a predictor with respect to a non-smooth loss function, such as the hinge loss. As discussed earlier, SGD with averaging is known to have a rate of at most O (log( T ) /T ). In the previous section, we saw that for smooth F , the rate is actu-ally O (1 /T ). Moreover, (Hazan &amp; Kale, 2011) showed that for using a different algorithm than SGD, one can obtain a rate of O (1 /T ) even in the non-smooth case. This might lead us to believe that an O (1 /T ) rate for SGD is possible in the non-smooth case, and that the O (log( T ) /T ) analysis is simply not tight. However, this intuition turns out to be wrong. Be-low, we show that there are strongly convex stochastic optimization problems in Euclidean space, in which the convergence rate of SGD with averaging is lower bounded by  X (log( T ) /T ). Thus, the logarithm in the bound is not merely a shortcoming in the standard online analysis of SGD, but is really a property of the algorithm.
 We begin with the following relatively simple example, which shows the essence of the idea. Let F be the 1-strongly convex function over the domain W = [0 , 1] d , which has a global minimum at 0 . Suppose the stochastic gradient or-acle, given a point w t , returns the gradient estimate  X  g t = w t + ( Z t , 0 ,..., 0) , where Z t is uniformly dis-tributed over [  X  1 , 3]. It is easily verified that is a subgradient of F ( w t ), and that E [ k  X  g t k 2 ]  X  d + 5 which is a bounded quantity for fixed d .
 The following theorem implies in this case, the conver-gence rate of SGD with averaging has a  X (log( T ) /T ) lower bound. The intuition for this is that the global optimum lies at a corner of W , so SGD  X  X pproaches X  it only from one direction. As a result, averaging the points returned by SGD actually hurts us.
 Theorem 3. Consider the strongly convex stochastic optimization problem presented above. If SGD is ini-tialized at any point in W , and ran with  X  t = c/t , then for any T  X  T 0 + 1 , where T 0 = max { 2 ,c/ 2 } , we have When c is considered a constant, this lower bound is  X (log( T ) /T ) .
 While the lower bound scales with c , we remind the reader that one must pick  X  t = c/t with constant c for an optimal convergence rate in general (see discussion in Sec. 2).
 This example is relatively straightforward but not fully satisfying, since it crucially relies on the fact that w  X  is on the border of W . In strongly convex problems, w  X  usually lies in the interior of W , so perhaps the  X (log( T ) /T ) lower bound does not hold in such cases. Our main result, presented below, shows that this is not the case, and that even if w  X  is well inside the inte-rior of W , an  X (log( T ) /T ) rate for SGD with averaging can be unavoidable. The intuition is that we construct a non-smooth F , which forces w t to approach the opti-mum from just one direction, creating the same effect as in the previous example.
 In particular, let F be the 1-strongly convex function over the domain W = [  X  1 , 1] d , which has a global minimum at 0 . Suppose the stochastic gradient oracle, given a point w t , returns the gradient estimate where Z t is a random variable uniformly distributed dient of F ( w t ), and that E [ k  X  g t k 2 ]  X  d + 63 which is a bounded quantity for fixed d .
 Theorem 4. Consider the strongly convex stochas-tic optimization problem presented above. If SGD is initialized at any point w 1 with w 1 , 1  X  0 , and ran with  X  t = c/t , then for any T  X  T 0 + 2 , where T 0 = max { 2 , 6 c + 1 } , we have When c is considered a constant, this lower bound is  X (log( T ) /T ) .
 We note that the requirement of w 1 , 1  X  0 is just for convenience, and the analysis also carries through, with some second-order factors, if we let w 1 , 1 &lt; 0. In the previous section, we showed that SGD with averaging may have a rate of  X (log( T ) /T ) for non-smooth F . To get the optimal O (1 /T ) rate for any F , we might turn to the algorithms of (Hazan &amp; Kale, 2011) and (Juditsky &amp; Nesterov, 2010). How-ever, these algorithms constitute a significant depar-ture from standard SGD. In this section, we show that it is actually possible to get an O (1 /T ) rate using a much simpler modification of the algorithm: given the sequence of points w 1 ,..., w T provided by SGD, in-stead of returning the average  X  w T = ( w 1 + ... + w T we average and return just a suffix, namely for some constant  X   X  (0 , 1) (assuming  X T and (1  X   X  ) T are integers). We call this procedure  X  -suffix averaging .
 Theorem 5. Consider SGD with  X  -suffix averaging as described above, and with step sizes  X  t = c/ X t where c &gt; 1 / 2 is a constant. Suppose F is  X  -strongly convex, holds that
E [ F (  X  w  X  T )  X  F ( w  X  )]  X  Note that for any constant  X   X  (0 , 1), the bound above is O ( G 2 / X T ). This applies to any relevant step size c/ X t , and matches the optimal guarantees in (Hazan &amp; Kale, 2011) up to constant factors. However, this is shown for standard SGD, as opposed to the more spe-cialized algorithm of (Hazan &amp; Kale, 2011). Finally, we note that it might be tempting to use Thm. 5 as a guide to choose the averaging window, by optimizing the bound for  X  (for instance, for c = 1, the optimum is achieved around  X   X  0 . 65). However, we note that the optimal value of  X  is dependent on the constants in the bound, which may not be the tightest or most  X  X orrect X  ones.
 Proof Sketch. The proof combines the analysis of online gradient descent (Hazan et al., 2007) and Lemma 1. In particular, starting as in the proof of Lemma 1, and extracting the inner products, we get Rearranging the r.h.s., and using the convexity of F to relate the l.h.s. to E [ F (  X  w  X  T )  X  F ( w  X  convergence upper bound of 2  X T Lemma 1 tells us that with any strongly convex F , even non-smooth, we have E [ k w t  X  w  X  k 2 ]  X  O (1 /t ). Plugging this in and performing a few more manipu-lations, the result follows.
 One potential disadvantage of suffix averaging is that if we cannot store all the iterates w t in memory, then we need to know from which iterate  X T to start com-puting the suffix average (in contrast, standard aver-aging can be computed  X  X n-the-fly X  without knowing the stopping time T in advance). However, even if T is not known, this can be easily addressed in several ways. For example, since our results are robust to the value of  X  , it is really enough to guess when we passed some  X  X onstant X  portion of all iterates. Alternatively, one can divide the rounds into exponentially increasing epochs, and maintain the average just of the current epoch. Such an average would always correspond to a constant-portion suffix of all iterates. All our previous bounds were on the expected subopti-mality E [ F ( w )  X  F ( w  X  )] of an appropriate predictor w . We now outline how these results can be strengthened to bounds on F ( w t )  X  F ( w  X  ) which hold with arbitrar-ily high probability 1  X   X  , with the bound depending logarithmically on  X  . They are slightly worse than our in-expectation bounds by having worse dependence on the step size parameter c and an additional log(log( T )) factor (interestingly, a similar factor also appears in the analysis of (Hazan &amp; Kale, 2011), and we do not know if it is necessary). The key result is the follow-ing strengthening of Lemma 1, under slightly stronger technical conditions.
 Lemma 2. Let  X   X  (0 , 1 /e ) and T  X  4 . Suppose F is  X  -strongly convex over a convex set W , and that k  X  g t k 2  X  G 2 with probability 1 . Then if we pick  X  t c/ X t for some constant c &gt; 1 / 2 , such that 2 c is a whole number, it holds with probability at least 1  X   X  that for any t  X  X  4 c 2 + 4 c,...,T  X  1 ,T } that k w t  X  w  X  k 2  X  We note that the assumptions on 2 c and t are only for simplifying the result. To obtain high probabil-ity versions of Thm. 1, Thm. 2, and Thm. 5, we sim-ply need to plug in this lemma in lieu of Lemma 1 in their proofs. This leads overall to rates of the form O (log(log( T ) / X  ) /T ) which hold with probability 1  X   X  . We now turn to empirically study how the algorithms behave, and compare it to our theoretical findings. We studied the following four algorithms: 1. Sgd-A : Performing SGD and then returning the 2. Sgd- X  : Performing SGD with  X  -suffix averaging. 3. Sgd-L : Performing SGD and returning the point 4. Epoch-Gd : The optimal algorithm of (Hazan &amp; First, as a simple sanity check, we measured the perfor-mance of these algorithms on a simple, strongly convex stochastic optimization problem, which is also smooth. We define W = [  X  1 , 1] 5 , and F ( w ) = k w k 2 . The stochastic gradient oracle, given a point w , returns the stochastic gradient w + z where z is uniformly dis-tributed in [  X  1 , 1] 5 . Clearly, this is an unbiased esti-mate of the gradient of F at w . The initial point w 1 of all 4 algorithms was chosen uniformly at random from W . The results are presented in Fig. 1, and it is clear that all 4 algorithms indeed achieve a  X (1 /T ) rate, matching our theoretical analysis (Thm. 1, Thm. 2 and Thm. 5). The results also seem to indicate that Sgd-A has a somewhat worse performance in terms of leading constants. Second, as another simple experiment, we measured the performance of the algorithms on the non-smooth, strongly convex problem described in the proof of Thm. 4. In particular, we simulated this problem with d = 5, and picked w 1 uniformly at random from W . The results are presented in Fig. 2. As our theory indicates, Sgd-A seems to have an  X (log( T ) /T ) con-vergence rate, whereas the other 3 algorithms all seem to have the optimal  X (1 /T ) convergence rate. Among these algorithms, the SGD variants Sgd-L and Sgd- X  seem to perform somewhat better than Epoch-Gd . Also, while the average performance of Sgd-L and Sgd- X  are similar, Sgd- X  has less variance. This is reasonable, considering the fact that Sgd- X  returns an average of many points, whereas Sgd-L return only the very last point.
 Finally, we performed a set of experiments on real-world data. We used the same 3 binary classification datasets ( ccat , cov1 and astro-ph ) used by (Shalev-Shwartz et al., 2011) and (Joachims, 2006), to test the performance of optimization algorithms for Support Vector Machines using linear kernels. Each of these datasets is composed of a training set and a test set. Given a training set of instance-label pairs, { x i ,y i } m we defined F to be the standard (non-smooth) objec-tive function of Support Vector Machines, namely
F ( w ) = Following (Shalev-Shwartz et al., 2011) and (Joachims, 2006), we took  X  = 10  X  4 for ccat ,  X  = 10  X  6 for cov1 , and  X  = 5  X  10  X  5 for astro-ph . The stochastic gra-dient given w t was computed by taking a single ran-domly drawn training example ( x i ,y i ), and computing the gradient with respect to that example, namely Each dataset comes with a separate test set, and we also report the objective function value with respect to that set (as in Eq. (4), this time with { x i ,y i } rep-resenting the test set examples). All algorithms were initialized at w 1 = 0, with W = R d (i.e. no projections were performed -see the discussion in Sec. 2). The results of the experiments are presented in Fig. 3,Fig. 4 and Fig. 5. In all experiments, Sgd-A performed the worst. The other 3 algorithms per-formed rather similarly, with Sgd- X  being slightly bet-ter on the Cov1 dataset, and Sgd-L being slightly better on the other 2 datasets.
 In summary, our experiments indicate the following:  X  Sgd-A , which averages over all T predictors, is  X  The Epoch-Gd algorithm does have better per- X  Sgd-L also performed rather well (with what In this paper, we analyzed the behavior of SGD for strongly convex stochastic optimization problems. We demonstrated that this simple and well-known algo-rithm performs optimally whenever the underlying function is smooth, but the standard averaging step can make it suboptimal for non-smooth problems. However, a simple modification of the averaging step suffices to recover the optimal rate, and a more sophis-ticated algorithm is not necessary. Our experiments seem to support this conclusion.
 There are several open issues remaining. In particular, the O (1 /T ) rate in the non-smooth case still requires some sort of averaging. However, in our experiments and other studies (e.g. (Shalev-Shwartz et al., 2011)), returning the last iterate w T also seems to perform quite well. Our current theory does not cover this -at best, one can use Lemma 1 and Jensen X  X  inequality to argue that the last iterate has a O (1 / the behavior in practice is clearly much better. Does SGD, without averaging, obtain an O (1 /T ) rate for general strongly convex problems? Also, a fuller em-pirical study is warranted of whether and which aver-aging scheme is best in practice.
 Acknowledgements: We thank Elad Hazan and Satyen Kale for helpful comments on an earlier ver-sion of this paper.
 Bach, F. and Moulines, E. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In NIPS , 2011.
 Hazan, E. and Kale, S. Beyond the regret minimiza-tion barrier: An optimal algorithm for stochastic strongly-convex optimization. In COLT , 2011. Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-gret algorithms for online convex optimization. Ma-chine Learning , 69(2-3):169 X 192, 2007.
 Joachims, T. Training linear SVMs in linear time. In KDD , 2006.
 Juditsky, A. and Nesterov, Y. Primal-dual subgradi-ent methods for minimizing uniformly convex func-tions. Technical Report (August 2010), available at http://hal.archives-ouvertes.fr/docs/00/50 /89/33/PDF/Strong-hal.pdf , 2010.
 Kushner, H. and Yin, G. Stochastic Approxima-tion and Recursive Algorithms and Applications . Springer, 2nd edition, 2003.
 Nemirovski, A., Juditsky, A., Lan, G., and Shapiro,
A. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim. , 19(4): 1574 X 1609, 2009.
 Rakhlin, A., Shamir, O., and Sridharan, K. Mak-ing gradient descent optimal for strongly convex stochastic optimization. ArXiv Technical Report, arXiv:1109.5647, 2011.
 Shalev-Shwartz, S., Shamir, O., Srebro, N., and Srid-haran, K. Stochastic convex optimization. In COLT , 2009.
 Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter,
A. Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming , 127(1):3 X 30,
