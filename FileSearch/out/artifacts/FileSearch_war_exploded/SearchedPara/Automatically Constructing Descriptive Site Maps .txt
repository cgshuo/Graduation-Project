 Recent research indicates that the Web is continuing to grow rapidly. However, the number of web sites did not increase much over time [8]. Thus, the growth is mostly due to the increase in the number of pages on web sites. According to the OCLC web survey [7], the average number of pages on a public web site already was 441 in 2002. Such increase in complexity of web sites inevitably makes them more and more difficult to navigate. 
It is not only the growth in the number of pages that complicates navigation on a web site. The dominance of search engines as the primary method of accessing information on the web discourages web site developers from paying enough attention to making their web sites easy to navigate. In addition, developers employ tricks to raise the rank-ing of their sites by search engines, making navigation even more difficult [9]. The traditional way to help users with navigating through a web site is a site map. and, possibly, a concise description of what each section is about. A site map is usu-ally created and maintained by the web site administrator. 
In reality, however, many web sites do not have site maps. And those that do usually do not have descriptions of sections, and are often outdated. This is not surprising, since site maps have to be created and maintained manually. Thus, there is a strong need for a technique capable of automatically generating accurate and descriptive site maps. 
The task of automatically creating a site map can be thought of as consisting of sev-eral steps. First, important sections on a web site must be identified 1 . Then, the sections must be combined into a hierarchy, taking into account user-defined constraints, such as the desired depth of the site map, maximum num ber of items in the site map, etc. Next, anchortext must be generated for every item. Finally, summaries for the sections need to be generated, which accurately describe contents of the sections. 
There are several important problems one needs to solve in order to successfully implement the above steps. There are a variety of structural and content features of web pages that determine important sections on web sites, such as content, anchortext, link patterns, URL structure, common look-and-feel. Simple heuristics that use only one or two of these features are not sufficient to identify web site sections [3]. A more complex approach is required, which would combine all these diverse features in an constraints may require merging some of the sections. Finally, generating the titles and summaries must be done in such a way that the user has enough information to decide whether he or she should navigate to the section, while keeping the summaries small to allow skimming through the overall structure of the web site. 
In this paper we propose a method for automatically constructing descriptive site maps. In contrast to the previous work, which used a heuristic-based approach [6], our work is based on a new semi-supervised clustering algorithm [3]. Given several sam-ple site maps, a learning algorithm decides on the best way to combine multiple di-verse features of web pages into a distance measure for the clustering algorithm, which is used to identify web site sections. Note that, in contrast to [6], our approach does not require specifying the number of clusters/sections in advance  X  the clustering algorithm decides on the optimal number of clusters automatically. The resulting clusters are then processed to identify l eader pages, and iteratively merged into a hierarchy that satisfies user-defined constraints. Finally, titles and summaries are generated for site map items using multi-document summarization techniques. 
An important application of our algorithm is a new paradigm for accessing informa-tion on the Web, which integrates searching and browsing. Currently, when the user clicks on a link on a search engine results page, they get to a page that is totally out of context. There is generally no way to find out the function of this page in the web site as  X  X andom X  page on the site. To solve this problem, search engines could use our algo-be presented to users when they navigate from search engine to the page. 
The rest of the paper is organized as follows. The next section presents an over-view of related work. Section 3 describes our algorithm for constructing descriptive site maps. Section 4 discusses our experiments, and section 5 concludes the paper. Research relevant to our work can be divided into two categories: recovering internal structure of a web site and automatic site map construction, and multi-document sum-marization. Below we give an overview of work in each of these areas. Recovering Web Site Structure. Eiron and McCurley [4] propose a heuristic ap-proach to identifying important sections on a web site, which they call compound documents (cDocs) . Their approach relies heavily on the path component of URL. with respect to whether it links to a page in an upper, lower, same, or unrelated direc-tory. Eiron and McCurley observe that, since cDocs are usually authored by a single person and created over a short period of time, they tend to contain the same (or very similar) set of down, inside, and across links. Another heuristic they use is based on an observation that outlinks of the members of a cDoc often have the same anchor one directory, and all other pages being in a subdirectory. These heuristics are manu-ally combined to identify cDocs on web sites. 
Li et al. [6] propose a heuristic approach to identifying logical domains  X  informa-tion units similar to cDocs  X  on web sites. Their heuristics are based on observations about what leader pages of logical domains tend to look like. They use a number of heuristics exploiting file name, URL structure, and link structure to assign a leader domains, and every other page is assigned to one of the leader pages based on the longest common substring of the URL, making adjustments to make sure that all pages of a logical domain are accessible from the leader page. 
The approach of [6] was also applied to automatic site map construction [2]. First, logical domains are identified using the approach described above. Then, for every pair of parent logical domain leader pa ge and child logical domain leader page, l pages best describing the association between the two leader pages are chosen using a PageRank style algorithm. The site map is then constructed by finding shortest paths between all selected pages. The approach does not include generating anchortexts or summaries. 
Both of the above approaches are based on a number of pre-defined heuristics, and thus fail to account for a variety of content, style, and structural conventions existing in different communities on the Web. In addition, the latter approach requires specify-ing the values for parameters k and l in advance. 
Trying to overcome the limitations of the methods mentioned above, we proposed in our previous work [3] a new approach to finding compound documents. This work uses an extension of that approach to identify important sections on web sites. We describe our approach in detail in section 3. Summarizing Multiple Documents. The task of multi-document summarization is to produce, given a set of documents, a coherent summary of a specified size describing the contents of all documents at once. The approaches to multi-document summariza-tion can be split into two classes: those that produce summaries consisting of whole sentences (or large sentence fragments) extracted from the original documents, and those that produce summaries consisting of new automatically generated sentences based on semantic information extracted from the documents. Although recently there has been some work in the latter area, the proposed methods are computationally intensive, and, to our knowledge, do not perform significantly better than the ap-proaches from the more simple former class. Thus, we only concentrate on the former class here. 
Most of the approaches of the former class consist of three main stages. On the fea-ture extraction stage, for every sentence a number of features are extracted relevant to estimating significance of the sentence; on the sentence ranking stage, every sentence is assigned a significance score based on the extracted features; on the summary gen-eration stage a summary is generated from highly ranked sentences, paying particular attention to avoiding adding redundant information to the summary. Due to space limitations, we do not discuss how each of these stages is implemented in existing systems. An interested reader is referred to the DUC web site, http://duc.nist.gov, for more information. 
Our approach to generating summaries follows the standard framework outlined above. We describe the particular methods used in section 3. Our approach to constructing descriptive site maps consists of three stages. On the compound documents identification stage, a web site is split into a set of cDocs, each cDoc representing an important section of the site; on the site map construction stage, the cDocs are combined into a hierarchy to produce a site map according to user-defined criteria; on the anchortext and summary generation stage, anchortext is gen-erated for every item in the site map, and a summary is extracted for every leaf item. We describe each of these stages in detail in the subsequent sections. Finding Compound Documents. This stage is implemented using the system for automatic identification of compound documents we developed in our previous re-search [3], extended with additional functionality. We represent a web site 2 as a graph with nodes corresponding to pages, and edges corresponding to hyperlinks. The proc-ess of finding cDocs consists of two phases. On the training phase (fig.1, left), a human labels cDocs on several web sites. Then, a vector of features 3 is extracted for every hyperlink, and a logistic regression model is trained to map vectors of feature values to weights on the corresponding repeat the process of extracting the features from the pages, and transforming them into vectors of real values. Then, we use the logistic regression model to compute weights on the edges. Finally, a graph clustering algorithm is applied to the weighted graph. The clusters are the cDocs we are looking for. 
We make a few notes about our clustering algorithm. The algorithm is an extension singleton, and then repeatedly merges the clusters, picking edges one by one in the order of decreasing weight. However, there is a notable difference from the classical case. Our algorithm usually stops before all the pages have been merged into a single cluster, since it may decide not to merge two clusters under consideration on a par-ticular step, based on the average weights of edges within the clusters, and the weight of the edge connecting them. The threshold controlling this process is learned during the training phase. 
In this work, we extended the clustering algorithm described above with the ability to satisfy the alldiff constraint, which requires, for a given set of pages, that no two of them belong to the same cluster. To satisfy it, every time we want to merge two clus-ters, we have to check that the merge is allowed. Note that this only introduces a con-stant overhead on every step of the clustering process 4 . We also note that this ap-proach can be applied to enforce alldiff cons traint in the classical agglomerative algo-equal to the number of pages in the alldiff set. 
We apply alldiff constraint to all pages with filenames index.&lt;ext&gt; or de-fault.&lt;ext&gt; . The intuition (confirmed experimentally) is that two pages having such filenames are unlikely to belong to the same cDoc. Moreover, as we discuss in the next section, such pages are very likely to be leader pages of their cDocs. Constructing Site Maps. To construct a site map we need, for every cDoc, to (1) find a leader page , i.e., the most natural entry point into the cDoc, (2) combine all cDocs on a site into a single hierarchy, and (3) a pply user-defined constraints to the hierar-chy to produce a site map satisfying the user X  X  requirements. Below we describe how each of these steps is implemented. 
Identifying leader pages in cDocs is important, because these are the pages that identify the leader page in a cDoc 5 . First, the filename of every page is examined. If a page with filename index.&lt;ext&gt; or default.&lt;ext&gt; is found, it is chosen to be alldiff constraint described earlier. Second, for cDocs that do not contain pages with the above filenames, the page with the greatest number of external inlinks (i.e., inlinks from other cDocs) is chosen to be the leader page. 
After the leader pages are identified, the cDocs are combined into a hierarchy to produce a site map. The cDoc containing the root page of the web site is taken to be examined, and all cDocs whose leader pages these outlinks point to are taken to be the descendants of the root node in the site map. New nodes are then examined in the breadth-first order, and processed according to the same procedure. 
Finally, user-defined constraints are app lied to the site map produced on the previ-ous step. Currently, the user can specify three types of constraints in our system: the minimum number of pages a cDoc must have to be present in a site map, the maxi-mum depth, or number of levels in the site map, and the maximum number of leaf items in the site map. 
These constraints allow the user balance the comprehensiveness and readability of constraints into their parent nodes. Table 1 shows the values for these constraints used in our experiments. Generating Summaries and Anchortexts. To generate summaries and anchortexts, we follow the standard multidocument summarization framework described in section 2, summary and anchortext generation. 
On the feature extraction stage, anchortexts, titles, and content are extracted for every site map item (referred to as texts in the remainder of the paper). In addition, the centroid, as well as k most frequent keywords are extracted for every text. After that, every text is processed with Automated English Sentence Segmenter 6 to identify sen-tence boundaries. Filtering is then applied to mark sentences not likely to be useful for the summary (the ones containing email addresses, phone numbers, copyright state-ments, etc.). These sentences will only be considered if a summary of the required size could not be constructed from unmarked sentences. 
For sentence ranking, we experimented with two approaches: rank the sentences sentences according to the similarity of the sentence to the k most frequent keywords of the text. 
Once the ranking is computed, we can generate anchortexts and summaries for our site map. We tried three approaches to anchortext generation: the highest ranked sen-tence from the text consisting of achortexts of the leader page of the item, the highest ranked sentence from the text consisting of titles of all pages of the item, and the title of the leader page of the item. 
For summary generation, we tried eight approaches, which differ in the type of text they used, and in the ranking method. We applied the two ranking methods mentioned above to the text of the leader page, text of all pages, and text of first sentences of all pages. In addition, we tried a hybrid approach, taking text of the leader page, and using centroid and keywords of all pages to rank the sentences. 
The summary generation algorithm used in all these cases is shown on Figure 2. It has a number of parameters. The desired number of sentences in the summary is a parameter set by the user, which lets them control the size of the generated summary. The sentence similarity threshold, t sentences that are very similar. The higher the value of this threshold, the stricter the requirement is that a new sentence included in the summary must be substantially different from the current summary. Finally, the sentence relevance threshold, t the summary, is still relevant to the centroid, or contains some of frequent keywords. Table 1 shows all parameters used by our algorithm. For the purpose of evaluating our algorithm, we used the same dataset that we used in [3] to evaluate our system for finding compound documents. This dataset consists of 50 web sites on educational topics. However, since many of the web sites in that data-set were too small to make site map construction for them interesting, we ended up using only 20 web sites. We are currently working on preparing a significantly larger dataset, which we will use for a more thorough evaluation. 
It is difficult to formally evaluate the quality of site maps, since there are too many subjective factors involved in deciding how good a site map is. Therefore, we conduct a separate evaluation of every component of the system. Figure 4 can give the reader an idea of what a typical site map generated by our system looks like. Compound Documents Identification. Since the quality of the system for identify-ing compound documents was extensively evaluated in [3], and we use the same data-set to evaluate our system, we do not repeat the details of the evaluation here. The key results of [3] were that the system could identify correctly most of the compound documents, a small number of training sites (typically 6) were enough to train a good logistic regression model, and the system was relatively insensitive to the choice of training examples. Here we used 10 sites for training, and the other 10 for testing. Leader Page Selection. As we mentioned in section 3, our heuristic approach to leader page identification produced 100% accuracy, when evaluated on all the com-pound documents from the 5 evaluation sites. Site Map Construction. We compare the site map produced from automatically identified compound documents to the one produced from gold-standard compound nodes, and for one of the sites the site map produced from the automatically identified cDocs was substantially different (and of less quality) than the one produced from the gold-standard cDocs. The difference, however, could be minimized with setting dif-ferent values for the maximum number of allowed leaf nodes and maximum depth of the site map 7 . Overall, the performance of the system on this step, though not as good as it is on the previous step, is still very good. Anchortext Generation. Contrary to our expectations, the approach using anchor-texts performed rather poorly, and the two approaches using titles showed reasonably good (and in most cases identical) results. It turned out that anchortexts within a sin-word, such as  X  X ntroduction X , or  X  X ndex X  to almost all leader pages. In many cases rather than words. This is particularly interesting because anchortexts have been found very effective in web information retrieval [1], as well as information retrieval in intranets [5], exactly because of their descriptive properties: the home page of Google does not contain the phrase  X  X earch engine X , but anchortexts often do. We attribute the difference in the results between our own and previous experiments to the difference in the nature of the datasets. Previous work studied the public web, or a large corporate intranet as a whole. To our knowledge, this work is the first to analyze anchortexts within individual web sites. Our results suggest that anchortexts within the same web site are mostly useless. This suggests that search engines might be able to improve the quality of their results by ignoring all anchortexts that come from the same site as the page itself. Summary Generation. Again, 5 sites were picked randomly, which resulted in 58 items with summaries. The algorithm described in section 3 was used to generate summaries in eight different ways. Then, we evaluated the summaries generated using each of the methods according to the following question:  X  X oes the summary give the user a good idea of the topic/function/goal (depending on the type of the item) of the site map item it describes? X  The possible answers are: (a) gives a very good idea; (b) gives a pretty good idea; however, some important information is missing; (c) gives a on Figure 4. 
The results show that the quality of the summaries produced by all methods, except the ones using first sentences, is quite high. The best methods generated 86-87% of good summaries (answer a or b ). The method using all pages of an item produced more summaries of very high quality than other methods. However, it also produced larger number of poor summaries. The reason for that is that if there are multiple sponds to a single site map item, then the method tends to generate a summary de-scribing only the most frequently mentioned topic. 
Somewhat surprising was the very good performance of the methods using text of the leader page. We think that, to large extent, this is due to the nature of our dataset. On educational web sites people provide good leader pages that give comprehensive description of the contents of their sections. We plan to conduct more experiments to see whether this will remain true for other domains. 
Poor performance of the methods using first sentences of pages is mainly due to the poor quality of the first sentences themselves. While in regular text first sentences often provide an overview of the section, on the Web they tend to contain lists of links to other sections of the web site, or header information irrelevant to the main content of the page. We conclude that such methods are not suitable for summary generation on the Web. 
Finally, we did not observe a significant difference in quality between summaries generated using text centroids and keywords 9 . 
Overall, the experiments showed that our method could generate high quality site maps. We believe that better parameter tuning can improve the results even further. We also plan to conduct a larger scale experi ment to verify that these results hold on a larger number of web sites across multiple domains. In this paper we described a system for automatic construction of descriptive site maps. The system is based on a combination of machine learning and clustering algo-rithms, which are used for automatic identification of web site sections. This frame-work provides a natural way for combing multiple structural and content features of web pages, and allows for automatic selection of the optimal number of sections. The sections are then combined to produce a site map, and multi-document summarization techniques are used to generate anchortexts and summaries for the items. Experimen-quality site maps. We believe that our work provides a good example of how Web content and structure mining, combined with natural language processing, can be used together to solve an important practical problem. 
In the future we plan to extend this work in several directions. First, we will inves-tigate the problem of automatic parameter selection for site map and summary genera-summary and overall quality of a site map. Second, we plan to evaluate our system on a larger number of web sites from multiple domains. Finally, we will explore new browsing paradigms resulting from integrating our approach with a search engine. This work is supported by the National Science Foundation under Grants No. 0227648, 0227656, and 0227888. 
