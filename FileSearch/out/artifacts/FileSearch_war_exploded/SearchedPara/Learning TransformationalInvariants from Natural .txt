 A key attribute of visual perception is the ability to extract invariances from visual input. In the realm of object recognition, the goal of invariant representation is quite clear: a successful object recognition system must be invariant to image variations resulting from different views of the same object. While spatial invariants are essential for forming a useful representation of the natural en-vironment, there is another, equally important form of visual invariance, namely transformational invariance . A transformational invariant refers to the dynamic visual structure that remains the same when the spatial structure changes. For example, the property that a soccer ball moving through the air shares with a football moving through the air is a transformational invariant; it is specific to how the ball moves but invariant to the shape or form of the object. Here we seek to learn such invariants from the statistics of natural movies.
 There have been numerous efforts to learn spatial invariants [1, 2, 3] from the statistics of natural images, especially with the goal of producing representations useful for object recognition [4, 5, 6]. However, there have been few attempts to learn transformational invariants from natural sensory data. Previous efforts have either relied on using unnatural, hand-tuned stimuli [7, 8, 9], or unrealistic unclear to what extent these models have captured the diversity of transformations in natural visual scenes or to what level of abstraction their representations produce transformational invariants. Previous work learning sparse codes of image sequences has shown that it is possible to recover local, direction-selective components (akin to translating Gabors) [11]. However, this type of model does not capture the abstract property of motion because each unit is bound to a specific orientation, spatial-frequency and location within the image X  X .e., it still suffers from the aperture problem. Here we describe a hierarchical probabilistic generative model that learns transformational invari-ants from unsupervised exposure to natural movies. A key aspect of the model is the factorization of visual information into form and motion, as compared to simply extracting these properties sep-arately. The latter approach characterizes most models of form and motion processing in the visual cortical hierarchy [6, 12], but suffers from the fact that information about these properties is not bound together X  X .e., it is not possible to reconstruct an image sequence from a representation in which form and motion have been extracted by separate and independent mechanisms. While re-construction is not the goal of vision, the ability to interact with the environment is key, and thus binding these properties together is likely to be crucial for properly interacting with the world. In the model we propose here, form and motion are factorized, meaning that extracting one property depends upon the other. It specifies not only how they are extracted, but how they are combined to provide a full description of image content.
 We show that when such a model is adapted to natural movies, the top layer units learn to extract transformational invariants. The diversity of units in both the intermediate layer and top layer pro-vides a set of testable predictions for representations that might be found in V1 and MT. The model also demonstrates how feedback from higher levels can influence representations at lower levels as a by-product of inference in a graphical model. In this section we introduce our hierarchical generative model of time-varying images. The model consists of an input layer and two hidden layers as shown in Figure 1. The input layer represents the time-varying image pixel intensities. The first hidden layer is a sparse coding model utilizing com-plex basis functions, and shares many properties with subspace-ICA [13] and the standard energy model of complex cells [14]. The second hidden layer models the dynamics of the complex basis function phase variables. 2.1 Sparse coding with complex basis functions In previous work it has been shown that many of the observed response properties of neurons in V1 may be accounted for in terms of a sparse coding model of images [15, 16]: is small compared to the image variance. The sparse coding model imposes a kurtotic, independent prior over the coefficients, and when adapted to natural image patches the A i ( x ) converge to a set of localized, oriented, multiscale functions similar to a Gabor wavelet decomposition of images. We propose here a generalization of the sparse coding model to complex variables that is primarily motivated from two observations of natural image statistics. The first observation is that although the prior is factorial, the actual joint distribution of coefficients, even after learning, exhibits strong statistical dependencies. These are most clearly seen as circularly symmetric, yet kurtotic distribu-tions among pairs of coefficients corresponding to neighboring basis functions, as first described by Zetzsche [17]. Such a circularly symmetric distribution strongly suggests that these pairs of coeffi-cients are better described in polar coordinates rather than Cartesian coordinates X  X .e., in terms of amplitude and phase . The second observation comes from considering the dynamics of coefficients through time. As pointed out by Hyvarinen [3], the temporal evolution of a coefficient in response to a movie, u i ( t ) , can be well described in terms of the product of a smooth amplitude envelope multiplied by a quickly changing variable. A similar result from Kording [1] indicates that temporal continuity in amplitude provides a strong cue for learning local invariances. These results are closely related to the trace learning rule of Foldiak [18] and slow feature analysis [19].
 With these observations in mind, we have modified the sparse coding model by utilizing a complex basis function model as follows: notation &lt;{ . } denotes taking the  X  X eal part X  of the argument.) The resulting generative model can also be written as: Thus, each pair of basis functions A R i , A I i forms a 2-dimensional subspace and is controlled by an amplitude a i and phase  X  i that determine the position within each subspace. Note that the basis functions are only functions of space. Therefore, the temporal dynamics within image sequences will be expressed in the temporal dynamics of the amplitude and phase.
 The prior over the complex coefficients, z , is designed so as to enforce circularly symmetric distri-butions and smooth amplitude dynamics as observed from time-varying natural images: The first term in the exponential imposes a sparse prior on the coefficient amplitudes. Here we is no prior over the phases, this will result in circularly symmetric kurtotic distributions over each subspace. The second term in the exponential imposes temporal stability on the time rate of change For a sequence of images the resulting negative log-posterior for the first hidden layer becomes: While this model by no means captures the full joint distribution of coefficients, it does at least capture the circular symmetric dependencies among pairs of coefficients, which allows for the ex-plicit representation of amplitude and phase. As we shall see, this representation serves as a staging ground for learning higher-order dependencies over space and time. 2.2 Phase Transformations Given the decomposition into amplitude and phase variables, we now have a non-linear represen-tation of image content that enables us to learn its structure in another linear generative model. In particular, the dynamics of objects moving in continuous trajectories through the world over short epochs will be encoded in the population activity of the phase variables  X  i . Furthermore, because we have encoded these trajectories with an angular variable, many transformations in the image do-main that would otherwise be nonlinear in the coefficients u i will now be linearized. This linear relationship allows us to model the time-rate of change of the phase variables with a simple linear generative model.
 We thus model the first-order time derivative of the phase variables as follows: variables w k influence the phase shifts  X   X  i . The additive noise term,  X  i , represents uncertainty or noise in the estimate of the phase time-rate of change. As before, we impose a sparse, independent distribution on the coefficients w k , in this case with a sparse cost function given as: The uncertainty over the phase shifts is given by a von Mises distribution: p (  X  i )  X  exp(  X  cos(  X  i )) . Thus, the log-posterior over the second layer units is given by Because the angle of a variable with 0 amplitude is undefined, we exclude angles where the corre-sponding amplitude is 0 from our cost function.
 Note that in the first layer we did not introduce any prior on the phase variables. With our second hidden layer, E 2 can be viewed as a log-prior on the time rate of change of the phase variables:  X   X  Activating the w variables moves the prior away from  X   X  i ( t ) = 0 , encouraging certain patterns of phase shifting that will in turn produce patterns of motion in the image domain.
 The structure of the complete graphical model is shown in Figure 1. 2.3 Learning and inference A variational learning algorithm is used to adapt the basis functions in both layers. First we infer the maximum a posteriori estimate of the variables a ,  X  , and w for the current values of the basis functions. Given the map estimate of these variables we then perform a gradient update on the basis functions. The two steps are iterated until convergence.
 To infer coefficients in both the first and second hidden layers we perform gradient descent with amplitudes and phases in the first layer are given by The dynamics for the second layer coefficients w k are given by Note that the two hidden layers are coupled, since the inference of w depends on  X  , and the inference of  X  in turn depends on w , in addition to I and a . Thus, the phases are computed from a combination of bottom-up ( I ), horizontal ( a ) and top-down ( w ) influences.
 The learning rule for the first layer basis functions is given by the gradient of E 1 with respect to A ( x ) , using the values of the complex coefficients inferred in eqs. 9 and 10 above: The learning rule for the second layer basis functions is given by the gradient of E 2 with respect to D , using the values of  X  and w inferred above: After each gradient update the basis functions are normalized to have unit length. 3.1 Simulation procedures The model was trained on natural image sequences obtained from Hans van Hateren X  X  repository at http://hlab.phys.rug.nl/vidlib/ . The movies were spatially lowpass filtered and whitened as described previously [15]. Note that no whitening in time was performed since the temporal structure will be learned by the hierarchical model. The movies consisted of footage of animals in grasslands along rivers and streams. They contain a variety of motions due to the movements of animals in the scene, camera motion, tracking (which introduces background motion), and motion borders due to occlusion.
 learning only the terms in E 1 are used to infer the a i and  X  i . Once the first layer reaches convergence, we begin training the second layer, using 100 bases, D i , initialized to random values. The second layer bases are initially trained on the MAP estimates of the first layer  X   X  i inferred using E 1 only. After the second layer begins to converge we infer coefficients in both the first layer and the second layer simultaneously using all terms in E 1 + E 2 (we observed that this improved convergence in the second layer). We then continued learning in both layers until convergence. The bootstrapping of the second layer was used to speed convergence and we did not observe much change in the first layer basis functions after the initial convergence. We have run the algorithm multiple times and have observed qualitatively similar results on each run. Here we describe the results of one run. 3.2 Learned complex basis functions After learning, the first layer complex basis functions converge to a set of localized, oriented, and bandpass functions with real and imaginary parts roughly in quadrature. The population of filters as a whole tile the joint spaces of orientation, position, and center spatial frequency. Not surprisingly, this result shares similarities to previous results described in [1] and [3]. Figure 2(a) shows the real part, imaginary part, amplitude, and angle of two representative basis functions as a function of space. Examining the amplitude of the basis function we see that it is localized and has a roughly Gaussian envelope. The angle as a function of space reveals a smooth ramping of the phase in the direction perpendicular to the basis functions X  orientation. movie TransInv Figure2.mov ).
 A useful way of visualizing what a generative model has learned is to generate images while varying the coefficients. Figure 2(b) displays the resulting image sequences produced by two representative basis functions as the amplitude and phase follow the indicated time courses. The amplitude has the effect of controlling the presence of the feature within the image and the phase is related to the position of the edge within the image. Importantly for our hierarchical model, the time derivative, or slope of the phase through time is directly related to the movement of the edge through time. Figure 2(c) shows how the population of complex basis functions tiles the space of position (left) and spatial-frequency (right). Each dot represents a different basis function according to its maxi-mum amplitude in the space domain, or its maximum amplitude in the frequency domain computed via the 2D Fourier transform of each complex pair (which produces a single peak in the spatial-frequency plane). The basis functions uniformly tile both domains. This visualization will be useful for understanding what the phase shifting components D in the second layer have learned. 3.3 Learned phase-shift components Figure 3 shows a random sampling of 16 of the learned phase-shift components, D i , visualized in both the space domain and frequency domain depictions of the first-layer units. The strength of connection for each component is denoted by hue (red +, blue -, gray 0). Some have a global others have influence only over a local region (e.g., row 1, column 6). Those with a linear ramp in the Fourier domain correspond to rigid translation, since the higher spatial-frequencies will spin their phases at proportionally higher rates (and negative spatial-frequencies will spin in the opposite direction). Some functions we believe arise from aliased temporal structure in the movies (row 1, column 5), and others are unknown (row 2, column 4). We are actively seeking methods to quantify these classes of learned phase-shift components.
 The phase shift components generate movements within the image that are invariant to aspects of the spatial structure such as orientation and spatial-frequency. We demonstrate this in Figure 4 by showing the generated transforms for 4 representative phase-shift components. The illustrated trans-formation components produce: (a) global translation, (b) local translation, (c) horizontal dilation and contraction, and (d) local warping. See the caption of Figure 4 for a more detailed description of the generated motions. We encourage the reader to view the accompanying videos. The computational vision community has spent considerable effort on developing motion models. Of particular relevance to our work is the Motion-Energy model [14], which signals motion via the amplitudes of quadrature pair filter outputs, similar to the responses of complex neurons in V1. Simoncelli &amp; Heeger have shown how it is possible to extract motion by pooling over a population of such units lying within a common plane in the 3D Fourier domain [12]. It has not been shown how the representations in these models could be learned from natural images. Furthermore, it is unclear how more complicated transformations, other than local translations, would be represented by such a model, or indeed how the entire joint space of position, direction and speed should be tiled to provide a complete description of time-varying images. Our model addresses each of these problems: it learns from the statistics of natural movies how to best tile the joint domain of position and motion, and it captures complex motion beyond uniform translation.
 Central to our model is the representation of phase . The use of phase information for computing motion is not new, and was used by Fleet and Jepson [20] to compute optic flow. In addition, as shown in Eero Simoncelli X  X  Thesis, one can establish a formal equivalence between phase-based methods and motion energy models. Here we argue that phase provides a convenient representation as it linearizes trajectories in coefficient space and thus allows one to capture the higher-order struc-ture via a simple linear generative model. Whether or how phase is represented in V1 is not known, Figure 4: Visualization of learned transformational invariants (best viewed as animations in movie TransInv Figure4x.mov , x=a,b,c,d). Each phase-shift component produces a pattern of motion that is invariant to the spatial structure contained within the image. Each panel displays the induced image transformations for a different basis function, D i . Induced motions are shown for four different image patches with the original static patch displayed in the center position. Induced motions are produced by turning on the respective coefficient w i positively (patches to the left of center) and negatively (patches to the right of center). The final image in each sequence shows the pixel-wise variance of the transformation (white values indicate where image pixels are changing through time, which may be difficult to discern in this static presentation). The example in (a) pro-duces global motion in the direction of 45 deg . The strongly oriented structure within the first two patches clearly moves along the axis of motion. Patches with more complicated spatial structure (4th patch) also show similar motion. The next example (b) produces local vertical motion in the lower portion of the image patch only. Note that in the first patch the strong edge in the lower portion of the patch moves while the edge in the upper portion remains fixed. Again, this component produces similar transformations irrespective of the spatial structure contained in the image. The example in (c) produces horizontal motion in the left part of the image in the opposite direction of horizontal motion in the right half (the two halves of the image either converge or diverge). Note that the oriented structure in the first two patches becomes more closely spaced in the leftmost patch and is more widely spaced in the right most image. This is seen clearly in the third image as the spacing between the vertical structure is most narrow in the leftmost image and widest in the rightmost im-age. The example in (d) produces warping in the upper part of the visual field. This example does not lend itself to a simple description, but appears to produce a local rotation of the image patch. but it may be worth looking for units that have response properties similar to those of the  X  X hase units X  in our model.
 Our model also has implications for other aspects of visual processing and cortical architecture. Under our model we may reinterpret the hypothesized split between the dorsal and ventral visual streams. Instead of independent processing streams focused on form perception and motion percep-tion, the two streams may represent complementary aspects of visual information: spatial invariants and transformational invariants . Indeed, the pattern-invariant direction tuning of neurons in MT is strikingly similar to that found in our model [21]. Importantly though, in our model information about form and motion is bound together since it is computed by a process of factorization rather than by independent mechanisms in separate streams.
 Our model also illustrates a functional role for feedback between higher visual areas and primary visual cortex, not unlike the proposed inference pathways suggested by Lee and Mumford [22]. The first layer units are responsive to visual information in a narrow spatial window and narrow spatial frequency band. However, the top layer units receive input from a diverse population of first layer units and can thus disambiguate local information by providing a bias to the time rate of change of the phase variables. Because the second layer weights D are adapted to the statistics of natural movies, these biases will be consistent with the statistical distribution of motion occurring in the natural environment. This method can thus deal with artifacts such as noise or temporal aliasing and can be used to disambiguate local motions confounded by the aperture problem.
 Our model could be extended in a number of ways. Most obviously, the graphical model in Figure 1 begs the question of what would be gained by modeling the joint distribution over the amplitudes, a , in addition to the phases. To some degree, this line of approach has already been pursued by invariants within the image. We are thus eager to combine both of these models into a unified model of higher-order form and motion in images.
 References
