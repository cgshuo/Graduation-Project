 REGULAR PAPER J. Zhang  X  D.-K. Kang  X  A. Silvescu  X  V. H o n a v a r Abstract In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT) X  X ierarchical group-ings of attribute values X  X o learn compact, comprehensible and accurate classifiers from data X  X ncluding data that are partially specified . This paper describes AVT-NBL, a natural generalization of the na  X   X ve Bayes learner (NBL), for learning clas-sifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples.
 Keywords Attribute value taxonomies  X  AV T-b a s e d n a  X   X ve Bayes learner  X  Partially specified data 1 Introduction Synthesis of accurate and compact pattern classifiers from data is one of the ma-jor applications of data mining. In a typical inductive learning scenario, instances attribute values can be grouped together to reflect assumed or actual similarities among the values in a domain of interest or in the context of a specific application. Such a hierarchical grouping of attribute values yields an attribute value taxonomy (AVT). Such AVT are quite common in biological sciences. For example, the Gene Ontology Consortium is developing hierarchical taxonomies for describing many aspects of macromolecular sequence, structure and function [ 5 ]. Undercoffer et al. have developed a hierarchical taxonomy that captures the features that are ob-servable or measurable by the target of an attack or by a system of sensors acting Semantic Web-related efforts [ 7 ] also capture hierarchical groupings of attribute values. Kohavi and Provost have noted the need to be able to incorporate back-ground knowledge in the form of hierarchies over data attributes in e-commerce applications of data mining [ 25 , 26 ]. Against this background, algorithms for learning from AVT and data are of significant practical interest for several reasons: (a) An important goal of machine learning is to discover comprehensible, yet (b) Exploiting AVT in learning classifiers can potentially perform regulariza-(c) The presence of explicitly defined AVT allows specification of data at differ-eralization of the standard algorithm for learning na  X   X ve Bayes classifiers from par-tially specified data. The rest of the paper is organized as follows: Sect. 2 formal-izes the notions on learning classifiers with AVT taxonomies; Sect. 3 presents the AVT-NBL algorithm; Sect. 4 discusses briefly on alternative approaches; Sect. 5 describes our experimental results and Sect. 6 concludes with summary and discussion.
 2 Preliminaries In what follows, we formally define AVT and its induced instance space. We in-troduce the notion of partially specified instances and formalize the problem of learning from AVT and data. 2.1 Attribute value taxonomies Let A ={ A 1 , A 2 ,..., A N } be an ordered set of nominal attributes and let dom ( A i ) denote the set of values (the domain) of attribute A i . We formally de-fine attribute value taxonomy (AVT) as follows: Definition 2.1 (Attribute Value Taxonomy) Attribute value taxonomy T i for at-tribute A i is a Tree-structured concept hierarchy in the form of a partially order ues in A i and  X  is the partial order that specifies a relationships among attribute of attribute value taxonomies associated with attributes A 1 , A 2 ,..., A N . of the tree corresponds to a relationship over attribute values in the AVT. Thus, an AVT defines an abstraction hierarchy over values of an attribute.
 scribing students in terms of their student status and work status . With regard to the AVT associated with student status , Sophomore is a primitive value while Un-dergraduate is an abstract value. Undergraduate is an abstraction of Sophomore , whereas Sophomore is a further specification of Undergraduate . We can similarly define AVT over ordered attributes as well as intervals defined over numerical attributes.
 Definition 2.2 (Cut) Acut  X  i is a subset of elements in Nodes ( T i ) satisfying the following two properties: ( 1 ) For any leaf m  X  Lea v es ( T i ) , either m  X   X  i or m is a descendant of an element n  X   X  i ; and ( 2 ) for any two nodes f , g  X   X  i ,fis neither a descendant nor an ancestor of g.
 set of values at a lower level cut and also induce a partition of all primitive values of A i . For example, in Fig. 1, the cut { Undergraduate , Graduate } defines a parti-tion over all the primitive values { Freshman , Sophomore , Junior , Senior , Master , PhD } in the student status attribute, and the cut { On-Campus , Off-Campus } de-fines a partition over its lower level cut { On-Campus , Government , Private } in the work status attribute.
 Hence, ={  X  1 , X  2 ,..., X  N } defines a global cut through T ={ T 1 , T 2 ,..., T N } , where each  X  i  X  i and  X  .
 Definition 2.3 (Cut Refinement) We say that a cut  X   X  i is a refinement of a cut  X  i if  X   X   X (v, T refinement of a global cut if at least one cut in  X  is a refinement of a cut in . Conversely, the global cut is an abstraction of the global cut  X  .
 on the AVTs shown in Fig. 1 .Thecut  X  1 ={ Undergraduate , Graduate } in the student status attribute has been refined to  X   X  1 ={ Undergraduate , Master , PhD } by replacing Graduate with its two children, Master , PhD . Therefore,  X  ={ Undergraduate , Master , PhD , On-Campus , Off-Campus } is a cut refinement of ={ Undergraduate , Graduate , On-Campus , Off-Campus } . 2.2 AVT-induced abstract-instance space A classifier is built on a set of labeled training instances. The original instance space I without AVTs is an instance space defined over the domains of all at-tributes. We can formally define AVT-induced instance space as follows: Definition 2.4 (Abstract Instance Space) Any choice of = X  i i defines an abstract instance space I . When  X  i  X  i  X  such that  X  i = Lea v es ( T i ) , the result-ing instance space is an abstraction of the original instance space I . The original instance space is given by I = I 0 ,where  X  i  X  i  X  0 ,  X  i = Values ( A i ) = Lea v es ( T i ) , that is, the primitive values of the attributes A 1 ... A N . Definition 2.5 (AVT-Induced Instance Space) AsetofAVTs,T ={ T 1 ... T N } , associated with a set of Attributes, A ={ A 1 ... A N } , induces an instance space I set of AVTs T ). 2.3 Partially specified data In order to facilitate precise definition of partially specified data, we define two operations on AVT T i associated with attribute A i .  X  depth ( T i ,v( A i )) returns the length of the path from root to an attribute value  X  lea f ( T i ,v( A i )) returns a Boolean value indicating if v( A i ) is a leaf node in Definition 2.6 (Partially Specified Data) An instance X p is represented by a tu-ple (v 1 p ,v 2 p ,...,v Np ) .X p is  X  a completely specified instance if  X  i v ip  X  Lea v es ( T i ) .
 attribute values is partially specified (or partially missing ). Relative to the AVT showninFig.1,theinstance( Senior , TA ) is a fully specified instance. Some ex-amples of partially specified instances are ( Undergraduate , RA ), ( Freshman , Gov-ernment ), ( Graduate , Off-Campus ). The conventional missing value (normally recorded as ?) is a special case of partially specified attribute value, whose attribute value corresponds to the root of its AVT and contains no descriptive information about that attribute. We call this kind of missing totally missing .
 Definition 2.7 (A Partially Specified Data Set) A partially specified data set, D
T (relative to a set T of attribute value taxonomies), is a collection of instances drawn from I T , where each instance is labeled with the appropriate class la-D 2.4 Learning classifiers from data The problem of learning classifiers from AVT and data is a natural generalization of the problem of learning classifiers from data without AVT. The original data set D is simply a collection of labeled instances of the form ( X p , c X p ) ,where X p  X  I and c X p  X  C is a class label. A classifier is a hypothesis in the form of a function h : I  X  C , whose domain is the instance space I and whose range is the set of classes C . An hypothesis space H is a set of hypotheses that can be represented in some hypothesis language or by a parameterised family of functions (e.g. decision trees, na  X   X ve Bayes classifiers, SVM, etc.). The task of learning classifiers from the original data set D entails identifying a hypothesis h  X  H that satisfies some criteria (e.g. a hypothesis that is most likely given the training data D ). lows: Definition 2.8 (Learning Classifiers from AVT and Data) Given a user-labeled instances, construct a classifier h T : I T  X  C for assigning appropriate class labels to each instance in the instance space I T .
 has structure that makes it possible to search it efficiently for a hypothesis that is both concise as well as accurate. 3AVT-basedna  X   X ve Bayes learner 3.1 Na  X   X ve Bayes learner (NBL) Na  X   X ve Bayes classifier is a simple and yet effective classifier that has competitive performance with other more sophisticated classifiers [ 18 ]. Na  X   X ve Bayes classifier operates under the assumption that each attribute is independent of others given the class. Thus, the joint probability given a class can be written as the product of individual class conditional probabilities for each attribute. The Bayesian ap-proach to classifying an instance X p = (v 1 p ,v 2 p ,...,v Np ) is to assign it the most probable class c MAP ( X p ) :  X  v probabilities, P (v i k | c j ) , from training data, D . These probabilities, which com-D , using standard probability estimation methods [ 31 ] based on relative fre-quency counts of the corresponding classes and attribute value and class label co-occurrences observed in D . We denote  X  i (v k | c j ) as the frequency count of value v k of attribute A i given class label c j and label c j in a training set D . Hence, these relative frequency counts completely summarize the information needed for constructing a na  X   X ve Bayes classifier from D , and they constitute sufficient statistics for na  X   X ve Bayes learner [ 9 , 10 ]. 3.2 AVT-NBL We now introduce AVT-NBL, an algorithm for learning na  X   X ve Bayes classifiers from AVT and data. Given an ordered set of AVTs, T ={ T 1 , T 2 ,..., T N } , corre-of labeled examples of the form ( X p , c X p ) ,where X p  X  I T is a partially or fully specified instance and c X p  X  C is the corresponding class label, the task of AVT-NBL is to construct a na  X   X ve Bayes classifier for assigning X p to its most probable class, c MAP ( X p ) . As in the case of NBL, we assume that each attribute is indepen-dent of the other attributes given the class.
 Ana  X   X ve Bayes classifier defined on the instance space I is completely specified by a set of class conditional probabilities for each value of each attribute. Suppose we denote the table of class conditional probabilities associated with values in  X  i by CPT ( X  i ) . Then the na  X   X ve Bayes classifier defined over the instance space I is specified by h () ={ CPT ( X  1 ), CPT ( X  2 ),..., CPT ( X  N ) } .
 Bayes classifier based on the attributes A 1 , A 2 ,..., A N . If each cut  X  i  X  is chosen to pass through the root of each AVT, i.e.  X  i  X  i ={ Root ( T i ) } , h () simply assigns each instance to the class that is a priori most probable.
 stract value of each attribute (the most general hypothesis in H T ) and successively between the accuracy of classification and the complexity of the resulting na  X   X ve Bayes classifier. Successive refinements of correspond to a partial ordering of na  X   X ve Bayes classifiers based on the structure of the AVTs in T .
 pothesis h (  X  ) is a refinement of h () . Relative to the two cuts, Table 1 shows the conditional probability tables that we need to compute during learning for h () and h (  X  ) , respectively (assuming C ={+ ,  X  X  as two possible class labels). From the class conditional probability table, we can count the number of class condi-tional probabilities needed to specify the corresponding na  X   X ve Bayes classifier. As showninTable 1 , the total number of class conditional probabilities for h () and h (  X  ) are 8 and 10, respectively. 3.2.1 Class conditional frequency counts Given an attribute value taxonomy T i for attribute A i , we can define a tree of class conditional frequency counts CCFC ( T i ) such that there is a one-to-one corre-spondence between the nodes of the AVT T i and the nodes of the corresponding CCFC ( T i ) . It follows that the class conditional frequency counts associated with a nonleaf node of CCFC ( T i ) should correspond to the aggregation of the corre-sponding class conditional frequency counts associated with its children. Because each cut through an AVT T i corresponds to a partition of the set of possible val-ues in Nodes ( T i ) of the attribute A i , the corresponding cut  X  i through CCFC ( T i ) specifies a valid class conditional probability table CPT ( X  i ) for the attribute A i . observed values for the attribute in the data set. Each cut through the AVT corre-sponds to a partition of the numerical attribute into a set of intervals. We calculate the class conditional probabilities for each interval. As in the case of nominal at-tributes, we define a tree of class conditional frequency counts CCFC ( T i ) for each numerical attribute A i . CCFC ( T i ) is used to calculate the conditional prob-ability table CPT ( X  i ) corresponding to a cut  X  i .
 CCFC ( T i ) for each attribute is straightforward: we simply estimate the class con-ditional frequency counts associated with each of the primitive values of A i from thedataset D and use them recursively to compute the class conditional frequency counts associated with the nonleaf nodes of CCFC ( T i ) .
 for computing CCFC ( T i ) : First we make an upward pass aggregating the class conditional frequency counts based on the specified attribute values in the data set. Then we propagate the counts associated with partially specified attribute values down through the tree, augmenting the counts at lower levels according to the distribution of values along the branches based on the subset of the data for which the corresponding values are fully specified 1 .
 c j in a training set D and p i value v of attribute A i given class label c j in a training set D .Let  X (v, T i ) be the set of all children (direct descendants) of a node with value v in T i ; (v, T i ) the list of ancestors, including the root, for v in T i . The procedure of computing CCFC ( T i ) is shown below.
 Algorithm 3.1 Calculating class conditional frequency counts.
 quency counts when some of the instances are partially specified. On the AVT for student status showninFig. 3 (A), we mark each attribute value with a count showing the total number of positively labeled ( + ) instances having that specific value. First, we aggregate the counts upward from each node to its ancestors. For example, in Fig. 3 (B), the four counts 10, 20, 5, 15 on primitive attribute values Freshman , Sophomore , Junior ,and Senior add up to 50 as the count for Under-graduate . Because we also have 15 instances that are partially specified with the value Undergraduate , the two counts (15 and 50) aggregate again toward the root. Next, we distribute the counts of a partially specified attribute value downward ac-cording to the distributions of values among their descendant nodes. For example, 15, the count of partially specified attribute value Undergraduate ,ispropagated down into fractional counts 3, 6, 1.5, 4.5 for Freshman , Sophomore , Junior and Senior (see Fig. 3 (C) for values in parentheses). Finally, we update the estimated frequency counts for all attribute values as shown in Fig. 3 (D).
 tribute value taxonomies, we can calculate the conditional probability table with regard to any global cut .Let ={  X  1 ,..., X  N } be a global cut, where  X  i stands for a cut through CCFC ( T i ) . The estimated conditional probability ta-ble CPT ( X  i ) associated with the cut  X  i can be calculated from CCFC ( T i ) using Laplace estimates [ 31 , 24 ]. Recall that the na  X   X ve Bayes classifier h () based on a chosen global cut is completely specified by the conditional probability tables associated with the cuts in : h () ={ CPT ( X  1 ),..., CPT ( X  N ) } . 3.2.2 Searching for a compact na  X   X ve Bayes classifier The scoring function that we use to evaluate a candidate AVT-guided refinement of a na  X   X ve Bayes classifier is based on a variant of the minimum description length (MDL) score [ 37 ], which provides a basis for trading off the complexity against the error of the model. MDL score captures the intuition that the goal of a learner is to compress the training data D and encode it in the form of a hypothesis or a model h so as to minimize the length of the message that encodes the model h and the data D given the model h .[ 19 ] suggested the use of a conditional MDL (CMDL) score in the case of hypotheses that are used for classification (as op-posed to modelling the joint probability distribution of a set of random variables) to capture this tradeoff. In general, computation of CMDL score is not feasible for Bayesian networks with arbitrary structure [ 19 ]. However, in the case of na  X   X ve Bayes classifiers induced by a set of AVT, as shown below, it is possible to effi-ciently calculate the CMDL score.
 Here, P h ( c X p | v 1 p ,...,v Np ) denotes the conditional probability assigned to the class c X p  X  C associated with the training sample X p = (v 1 p ,v 2 p ,...,v Np ) by the classifier h , size ( h ) is the number of parameters used by h , | D | the size of the data set, and CLL ( h | D ) is the conditional log likelihood of the hypothesis h the total number of class conditional probabilities needed to describe h . Because each attribute is assumed to be independent of the others given the class in a na  X   X ve Bayes classifier, we have where P ( c j ) is the prior probability of the class c j , which can be estimated from the observed class distribution in the data D .
 ditional likelihood CLL ( h | D ) when D contains partially specified instances: (1) When a partially specified value of attribute A i for an instance lies on the cut  X  through CCFC ( T i ) or corresponds to one of the descendants of the nodes in the cut. In this case, we can treat that instance as though it were fully specified relative to the na  X   X ve Bayes classifier based on the cut  X  of CCFC ( T i ) and use the class conditional probabilities associated with the cut  X  to calculate its contribution to CLL ( h | D ) . (2) When a partially specified value (say v )of A i is an ancestor of a subset (say  X  ) of the nodes in  X  . In this case, p (v | c j ) = u we can aggregate the class conditional probabilities of the nodes in  X  to calculate the contribution of the corresponding instance to CLL ( h | D ) .
 class, the search for the AVT-based na  X   X ve Bayes classifier (AVT-NBC) can be performed efficiently by optimizing the criterion independently for each attribute. This results in a hypothesis h that intuitively trades off the complexity of na  X   X ve Bayes classifier (in terms of the number of parameters used to describe the relevant class conditional probabilities) against accuracy of classification. The algorithm terminates when none of the candidate refinements of the classifier yield statistically significant improvement in the CMDL score. The procedure is outlined below.
 Algorithm 3.2 Searching for compact AVT-based na  X   X ve Bayes classifier. 1. Initialize each  X  i in ={  X  1 , X  2 ,..., X  N } to { Root ( T i ) } . 2. Estimate probabilities that specify the hypothesis h () . 3. For each cut  X  i in ={  X  1 , X  2 ,..., X  N } : 4. Output h () 4 Alternative approaches to learning classifiers from AVT and data Besides AVT-NBL, we can envision two alternative approaches to learning classi-fiers from AVT and data. 4.1 Approaches that treat partially specified attribute values as if they were totally missing Each partially specified (and hence partially missing) attribute value is treated as if it were totally missing, and the resulting data set with missing attribute values is handled using standard approaches for dealing with missing attribute values in learning classifiers from an otherwise fully specified data set in which some attribute values are missing in some of the instances values. A main advantage of this approach is that it requires no modification to the learning algorithm. All that is needed is a simple preprocessing step in which all partially specified attribute values are turned into missing attribute values. 4.2 AVT-based propositionalisation methods The data set is represented using a set of Boolean attributes obtained from Nodes ( T i ) of attribute A i by associating a Boolean attribute with each node (ex-cept the root) in T i . Thus, each instance in the original data set defined using N attributes is turned into a Boolean instance specified using  X  N Boolean attributes, where  X  N = N i = 1 ( | Nodes ( T i ) | X  1 ) .
 in binary features that correspond to the propositions such as (student = Under-graduate ), (student = Graduate ), (student = Freshman ), ... (student = Senior ), (student = Master ), (student = PhD ). Based on the specified value of an attribute in an instance, e.g. (student = Master ), the values of its ancestors in the AVT (e.g. student = Graduate ) are set to True because the AVT asserts that Master students are also Graduate students. But the Boolean attributes that correspond to descen-dants of the specified attribute value are treated as unknown. For example, when the value of the student status attribute is partially specified in an instance, e.g. (student = Graduate ), the corresponding Boolean attribute is set to True, but the Boolean attributes that correspond to the descendants of Graduate in this taxon-omy are treated as missing . The resulting data with some missing attribute values can be handled using standard approaches to dealing with missing attribute values. For numerical attributes, the Boolean attributes are the intervals that correspond to nodes of the respective AVTs. If a numerical value falls in a certain interval, the corresponding Boolean attribute is set to True, otherwise it is set to False. We call the resulting algorithm X  X BL applied to AVT-based propositionalized version of the data X  X rop-NBL.
 described above are not independent given the class. A Boolean attribute that cor-responds to any node in an AVT is necessarily correlated with Boolean attributes that correspond to its descendants as well as its ancestors in the tree. For example, the Boolean attribute (student = Graduate ) is correlated with (student = Master ). (Indeed, it is this correlation that enables us to exploit the information provided by AVT in learning from partially specified data). Thus, a na  X   X ve Bayes classifier that would be optimal in the maximal a posteriori sense [ 28 ] when the original attributes student status and work status are independent given class would no longer be optimal when the new set of Boolean attributes are used because of strong dependencies among the Boolean attributes derived from an AVT.
 require no modification to the learning algorithm. However, it does require pre-processing of partially specified data using the information supplied by an AVT. The number of attributes in the transformed data set is substantially larger than the number of attributes in the original data set. More important, the statistical de-pendence among the Boolean attributes in the propositionalised representation of the original data set can degrade the performance of classifiers, e.g. na  X   X ve Bayes that rely on independence of attributes given class. Against this background, we experimentally compare AVT-NBL with Prop-NBL and the standard na  X   X ve Bayes algorithm (NBL). 5 Experiments and results 5.1 Experiments Our experiments were designed to explore the performance of AVT-NBL relative to that of NBL and PROP-NBL.
 application domains, at present, there are few standard benchmark data sets of par-tially specified data and the associated AVT. We select 37 data sets from the UC Irvine Machine Learning Repository, among which 8 data sets use only nominal attributes and 29 data sets have both nominal attributes and numerical attributes. Every numerical attribute in the 29 data sets has been discretised into a maxi-mum of 10 bins. For only three of the data sets (i.e. Mushroom , Soybean ,and Nursery ), AVTs were supplied by domain experts. For the remaining data sets, no expert-generated AVTs are readily available. Hence, the AVTs on both nom-inal and numerical attributes were generated using AVT-Learner, a hierarchical agglomerative clustering algorithm to construct AVTs for learning [ 23 ]. and PROP-NBL on the original data.
 data sets with different percentages of totally missing and partially missing at-tribute values. Three data sets with a prespecified percentage (10%, 30% or 50%, excluding the missing values in the original data set) of totally or partially miss-ing attribute values were generated by assuming that the missing values are uni-formly distributed on the nominal attributes [ 45 ]. From the original data set D , a data set D p of partially (or totally) missing values was generated as follows: the root n 0 of the corresponding AVT. select one of the nodes (excluding n l ) along this path with uniform probability. Read the corresponding attribute value from the AVT and assign it as the partially specified value of the corresponding attribute. Note that the selection of the root of the AVT would result in a totally missing attribute value.
 conditional probabilities used to specify the learned classifier) were estimated us-ing 10-fold cross-validation, and we calculate 90% confidence interval on the error rate.
 classifiers generated by AVT-NBL, Prop-NBL and NBL as a function of the training-set size. We divided each data set into two disjoint parts: a training pool and a test pool. Training sets of different sizes, corresponding to 10%, 20%, ..., 100% of the training pool, were sampled and used to train na  X   X ve Bayes classifiers using AVT-NBL, Prop-NBL, and NBL. The resulting classifiers were evaluated on the entire test pool. The experiment was repeated 9 times for each training-set size. The entire process was repeated using 3 different random partitions of data into training and test pools. The accuracy of the learned classifiers on the examples in the test pool were averaged across the 9  X  3 = 27 runs. 5.2 Results 5.2.1 AVT-NBL yields lower error rates than NBL and PROP-NBL on the original fully specified data Ta b l e 2 shows the estimated error rates of the classifiers generated by the AVT-NBL, NBL and PROP-NBL on 37 UCI benchmark data sets. According to the results, the error rate of AVT-NBL is substantially smaller than that of NBL and PROP-NBL. It is worth noting that PROP-NBL (NBL applied to a transformed data set using Boolean features that correspond to nodes of the AVTs) generally produces classifiers that have higher error rates than NBL. This can be explained by the fact that the Boolean features generated from an AVT are generally not independent given the class. 5.2.2 AVT-NBL yields classifiers that are substantially more compact than those generated by PROP-NBL and NBL The shaded columns in Table 2 compare the total number of class condi-tional probabilities needed to specify the classifiers produced by AVT-NBL, NBL, and PROP-NBL on original data. The results show that AVT-NBL is effective in exploiting the information supplied by the AVT to generate accu-rate yet compact classifiers. Thus, AVT-guided learning algorithms offer an approach to compressing class conditional probability distributions that are different from the statistical independence-based factorization used in Bayesian networks. 5.2.3 AVT-NBL yields significantly lower error rates than NBL and PROP-NBL on partially specified data and data with totally missing values Ta b l e 3 compares the estimated error rates of AVT-NBL with that of NBL and PROP-NBL in the presence of varying percentages (10%, 30% and 50%) Bayes classifiers generated by AVT-NBL have substantially lower error rates than those generated by NBL and PROP-NBL, with the differences being more pronounced at higher percentages of partially (or totally) missing attribute values. 5.2.4 AVT-NBL produces more accurate classifiers than NBL and Prop-NBL for a given training set size Figure 4 shows the plot of the accuracy of the classifiers learned as a function of training set size for Audiology data. We obtained similar results on other bench-mark data sets used in this study. Thus, AVT-NBL is more efficient than NBL and Prop-NBL in its use of training data.
 6 Summary and discussion 6.1 Summary In this paper, we have described AVT-NBL 2 , an algorithm for learning classifiers from attribute value taxonomies (AVT) and data in which different instances may have attribute values specified at different levels of abstraction. AVT-NBL is a natural generalization of the standard algorithm for learning na  X   X ve Bayes classi-fiers. The standard na  X   X ve Bayes learner (NBL) can be viewed as a special case of AVT-NBL by collapsing a multilevel AVT associated with each attribute into a corresponding single-level AVT whose leaves correspond to the primitive values of the attribute.
 1. AVT-NBL is able to learn substantially compact and more accurate classifiers 2. When applied to data sets in which attribute values are partially specified or to-3. AVT-NBL is more efficient in its use of training data. AVT-NBL produces more comprehensible) accurate classifiers from data X  X ncluding data that are par-tially specified . AVT-guided learning algorithms offer a promising approach to knowledge acquisition from autonomous, semantically heterogeneous informa-tion sources, where domain-specific AVTs are often available and data are often partially specified. 6.2 Related work There is some work in the machine-learning community on the problem of learn-ing classifiers from attribute value taxonomies (sometimes called tree-structured attributes) and fully specified data in the case of decision trees and rules. [ 32 ] out-lined an approach to using ISA hierarchies in decision-tree learning to minimize misclassification costs. [ 36 ] mentions handling of tree-structured attributes as a desirable extension to C4.5 decision-tree package and suggests introducing nom-inal attributes for each level of the hierarchy and encoding examples using these new attributes. [ 1 ] proposes a technique for choosing a node in an AVT for a binary split using the information-gain criterion. [ 2 ] consider a multiple split test, where each test corresponds to a cut through AVT. Because number of cuts and hence the number of tests to be considered grows exponentially in the number of leaves of the hierarchy, this method scales poorly with the size of the hierarchy. [ 17 ], [ 39 ]and[ 22 ] describe the use of AVT in rule learning. [ 20 ] proposed a method for exploring hierarchically structured background knowledge for learning associ-ation rules at multiple levels of abstraction. [ 16 ] suggested the use of abstraction-based search (ABS) to learn Bayesian networks with compact structure. [ 45 ]de-scribe AVT-DTL, an efficient algorithm for learning decision-tree classifiers from AVT and partially specified data. There has been very little experimental inves-tigation of these algorithms in learning classifiers using data sets and AVT from real-world applications. Furthermore, with the exception of AVT-DTL, to the best of our knowledge, there are no algorithms for learning classifiers from AVT and partially specified data.
 values (corresponding to nodes in an AVT) in building classifiers. Each abstract value of an attribute corresponds to a set of primitive values of the corresponding attribute. Quinlan X  X  C4.5 [ 36 ] provides an option called subsetting, which allows C4.5 to consider splits based on subsets of attribute values (as opposed to single values) along each branch. [ 13 ] has also incorporated set-valued attributes in the RIPPER algorithm for rule learning. However, set-valued attributes are not con-strained by an AVT. An unconstrained search through candidate subsets of values of each attribute during the learning phase can result in compact classifiers if com-pactness is measured in terms of the number of nodes in a decision tree. However, this measure of compactness is misleading because, in the absence of the structure imposed over sets of attribute values used in constructing the classifier, specifying the outcome of each test (outgoing branch from a node in the decision tree) re-quires enumerating the members of the set of values corresponding to that branch, making each rule a conjunction of arbitrary disjunctions (as opposed to disjunc-tions constrained by an AVT), making the resulting classifiers difficult to interpret. Because algorithms like RIPPER and C4.5 with subsetting have to search the set of candidate value subsets for each attribute under consideration, while adding conditions to a rule or a node to trees, they are computationally more demand-ing than algorithms that incorporate the AVTs into learning directly. At present, algorithms that utilize set-valued attributes do not include the capability to learn from partially specified data. Neither do they lend themselves to exploratory data analysis wherein users need to explore data from multiple perspectives (which correspond to different choices of AVT).
 of classifiers in scenarios where class labels correspond to nodes in a predefined class hierarchy. [ 12 ] have proposed a revised entropy calculation for constructing decision trees for assigning protein sequences to hierarchically structured func-tional classes. [ 27 ] describes the use of taxonomies over class labels to improve the performance of text classifiers. But none of them address the problem of learn-ing from partially specified data (where class labels and/or attribute values are partially specified).
 ing. The use of prior knowledge or domain theories specified typically in first-order logic to guide learning from data in the ML-SMART system [ 6 ]; the FOCL system [ 33 ]; and the KBANN system, which initializes a neural network using a domain theory specified in propositional logic [ 40 ]. AVT can be viewed as a restricted class of domain theories. [ 3 ] used background knowledge to generate relational features for knowledge discovery. [ 4 ] applied breadth-first marker prop-agation to exploit background knowledge in rule learning. However, the work on exploiting domain theories in learning has not focused on the effective use of AVT to learn classifiers from partially specified data. [ 42 ] first used the taxonomies in information retrieval from large databases. [ 14 ]and[ 11 ] proposed database mod-els to handle imprecision using partial values and associated probabilities, where a partial value refers to a set of possible values for an attribute. [ 30 ] proposed aggregation operators defined over partial values. While this work suggests ways to aggregate statistics so as to minimize information loss, it does not address the problem of learning from AVT and partially specified data.
 class labels is beginning to receive attention in the machine-learning community. Examples include distributional clustering [ 35 ], extended FOCL and statistical clustering [ 43 ], information bottleneck [ 38 ], link-based clustering on relational data [ 8 ]. Such algorithms provide a source of AVT in domains where none are available. The focus of work described in this paper is on algorithms that use AVT in learning classifiers from data. 6.3 Future work Some promising directions for future work in AVT-guided learning include 1. Development AVT-based variants of other machine-learning algorithms for 2. Extensions that incorporate class taxonomies (CT). It would be interesting to 3. Extensions that incorporate richer classes of AVT. Our work has so far focused 4. Further experimental evaluation of AVT-NBL, AVT-DTL and related learning References
