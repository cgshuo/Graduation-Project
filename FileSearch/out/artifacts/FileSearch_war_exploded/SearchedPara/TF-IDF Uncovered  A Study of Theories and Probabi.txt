 Interpretations of TF-IDF are based on binary independence re-trieval, Poisson, information theory, and language modelling. This paper contributes a review of existing interpretations, and then, TF-IDF is systematically related to the probabilities and a space of disjoint terms. For independent terms, an  X  X x-treme X  query/non-query term assumption uncovers TF-IDF, and an analogy of P ( d | q ) and the probabilistic odds rors relevance feedback. For disjoint terms, a relationship between probability theory and TF-IDF is established through the integral gence from randomness and pivoted document length to be inher-ent parts of a document-query independence (DQI) measure, and interestingly, an integral of the DQI over the term occurrence prob-ability leads to TF-IDF.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Theory TF-IDF interpretations, probability theory, document-query inde-pendence, integral, derivative of logarithm
This paper contributes a systematic investigation of the inter-pretations of TF-IDF. The investigation uncovered some surprising properties and interpretations. A language-modelling-like decom-position of P ( d | q ) for independent terms yields a TF-IDF interpre-tation, and this can be related to the probabilistic odds The decomposition of P ( q | d ) ( P ( d | q ) and P ( d,q ) based on disjoint terms can be related to TF-IDF through the in-tegral R 1 Copyright 2008 ACM 978-1-60558-164-4/08/07 ... $ 5.00. divergence from randomness and pivoted document length are in-herent parts of TF-IDF, and that TF-IDF can be interpreted as an integral of a document-query independence measure.

Existing TF-IDF interpretations are based on binary indepen-dence retrieval (BIR), Poisson, information theory (Shannon), and language modelling (LM). Section 1.2, BIR, reviews the results from [12] (understanding IDF: on theoretical arguments) and [5] (BIR term weight is a linear function of IDF values). Section 1.3 outlines the usage of Poisson in IR. Section 1.4 reviews the prob-abilistic interpretation of TF-IDF proposed in [7], and an explana-tion for the event space mix is provided in section 2.1. Section 1.5 reviews information-theoretic attempts to explain TF-IDF ([1, 14]).
After the review of existing TF-IDF interpretations, this paper contributes new interpretations. Particular to some of the new inter-pretations is that they are conclusive, whereas the above mentioned interpretations based on BIR, language modelling (LM) and infor-mation theory are only partial interpretations; only the Poisson ap-proach yields what can be viewed as a conclusive interpretation ([3, 11, 2, 15, 9]). Explanations via Binomial and Poisson probabilities are perceived as complex. One of the achievements of this study is to rely merely on basic maximum-likelihood estimates and two spaces: a space of independent and a space of disjoint terms. The case of independent terms is related to language modelling ([10, 7, 8, 17]), and the case of disjoint terms is related to the framework in [16]. The predictive distribution in [17] (integral over model pa-rameters) influenced this study and the idea to form an integral over the occurrence probability.
 Section 2 investigates the decomposition of P ( q | d ) , O ( r | d,q ) for independent terms . Section 2.1, P ( q | d ) an explanation for the event space issue in language modelling. pretation of TF-IDF; this is based on the  X  X xtreme X  assumption query terms. Section 2.3 extends the study to probabilistic odds; an analogy to P ( d | q ) mirrors the equations  X  r = c and non-relevant documents correspond to the collection, and the rele-vant documents correspond to the query.
 Section 3 investigates the decomposition of P ( q | d ) (and for disjoint terms . Based on basic maximum-likelihood estimates (section 3.3), a document-query independence (DQI) measure (sec-tion 3.5) is derived. The derivation uncovers divergence from ran-domness (3.4.2) and pivoted document length (3.4.3). The DQI is ranking-equivalent to P ( q | d ) , and it leads to TF-IDF: TF-IDF is an integral of the DQI over the term occurrence probability.
For improved readability, the key equations and results are framed. This theoretical paper contains few synthetic examples, and at the end of the paper, an example demonstrates the computa-tion of term frequencies, probabilities, DQI and TF-IDF values. t occurs t occurs
The paper employs the notation presented in figure 1. With this notation, a TF-IDF-based retrieval status value ( RSV ) can be de-fined as follows: D EFINITION 1. TF-IDF RSV Variations of TF-IDF concern the tf component. The following definitions capture three main variants for tf ( t,d ) . tf total is biased towards long documents, whereas tf sum towards small documents. The BM25-like tf estimate addresses this bias. It shows a non-linear rise with saturation towards for a large within-document term occurrence n L ( t,d ) . The rise is steep for small K , and smooth for large K . K incorporates the document length. For example, K := b  X  N L ( d ) considers the total document length, and K :=(1  X  b ) + b  X  N L ( d ) document length ( b adjusts the impact of the document length). The next sections review the traditional interpretations of TF-IDF.
The starting point of the binary independence retrieval (BIR) model is the probabilistic odds O ( r | d,q ) = P ( r | d,q ) derivation steps and assumptions (see [13, 6, 15]) lead to the BIR term weight and the BIR retrieval status value ( RSV ).
D EFINITION 2. BIR RSV Variations come from the inclusion/exclusion of the negated term event  X  t , and from the assumptions regarding missing evidence ( P
D ( t | r ) unknown, n L ( t,r ) = 0 , N L ( r ) = 0 , respectively). [12] (understanding IDF: on theoretical arguments) outlines that idf can be related to the BIR model. The simplified term weight w ing relevance, set P D ( t | r )=1 and  X  r = c . This leads to: log P D ( t | r ) [4] proposes an interpretation where in the genuine term weight the factor P ( t | r ) /P (  X  t | r ) is set constant for missing relevance. [5] (relevance feedback: loss of entropy, gain for IDF) provides a fully idf -based form of BIR. For the simplified term weight, this is: through relevance information a term may experience a loss re-garding its initial discriminativeness (based on the collection non-relevant documents  X  r ).
The starting point of the Poisson model is, as for BIR, the proba-bilistic odds O ( r | d,q ) . Then, the Poisson model views in the document d to be a vector of term frequencies k t := n The Poisson probability P ( k t | q,r ) :=  X  k t within-document term frequency, leads to the RSV of the Poisson model (details of the derivation in [15]).
 D EFINITION 3. Poisson RSV
The Poisson bridge shows two possibilities to express the aver-age term frequency  X  ( t,x ) , where x is a set of documents. Here, avgdl ( x ) is the average document length, and avgtf the average occurrence of a term in the documents in which the term does occur (elite set of term t ). Inserting the Poisson bridge into RSV Poisson ([15]) yields: RSV Poisson ( d,q,r,  X  r )= X As for BIR,  X  r = c can be assumed. The Poisson model tells that this assumption justifies the following equation: The 2-Poisson model ([11]) combines the overall averages  X  ( t,r ) and  X  ( t,  X  r ) with the elite set averages avgtf ( t,r ) this motivates the non-linear estimate tf BM25 ( t,d ):= n
The starting point of LM is the probability P ( q | d ) expressed via term probabilities of independent terms: P ( t | d ) is estimated via a mixture of document model and collec-tion model. Such a mixture is a general concept of probability the-ory, i.e. not only a feature specific to LM. The next equation corre-sponds to equation 19, but in P ( t | d,c ) , the collection Now, P ( t | d,c ) can be estimated based on a mixture. This mixture is applied since P ( t | d ) and P ( t | c ) however, P ( t | d,c ) is unknown. For example, let the probability of a sunny day ( t = sunny) in London ( d = London) be 0 . 4 the probability of a sunny day at the East Coast ( c = EastCoast) be 0 . 5 . Then, the weighted average is an estimate of the probability of a sunny day in both locations.

The ranking-invariant normalisation  X  P ( q | d,c ) / ( Q P ( t | c ))  X  followed by log leads to the compact RSV :
D EFINITION 4. LM RSV Hereby, n L ( t,q )=1 is assumed for the query term frequency. [7] proposes a probabilistic interpretation of TF-IDF. The inter-pretation is based on the estimation of the probabilities P ( t | c ) . For P ( t | d ):= n L ( t,d ) N ities in RSV LM share their nature with the respective parameters in TF-IDF. However, the mix of location-based and document-based probabilities needs to be justified from a probabilistic point of view; LM. Therefore, this study explores how to solve the event space mix, and how to reach a LM formulation that is closer to TF-IDF.
Shannon X  X  quantification, H ( t ) := P the product of P ( t ) and its logarithm, and this initiates ideas for a relationship with TF-IDF ([1], information-theoretic perspective of TF-IDF). [3] views IDF as a measure of deviations from Poisson; IDF. [14] introduces  X  X  probability of being informative X  to give a probabilistic meaning to IDF. The estimate P ( t informs follows from P ( t occurs | c ) = e  X  idf ( t,c )  X   X  1  X  N := maxidf ( c ) is large. The Euler series approximates and this relates occurrence and informativeness probabilities.
Let d 33 denote a document, and view a document as a conjunc-tive sequence of terms. For example: The term event  X  X ailing X  occurs twice in the conjunction. From an intensional probabilistic semantics point of view, identical events should coincide, i.e. P ( x  X  x ) = P ( x ) . However, more precisely, the sequence reads: The L i are independent variables; each variable stands for a loca-tion in the document; each location can take exactly one term.
The traditional relationship of TF-IDF and LM has been re-viewed in section 1.4.1. The next section concerns the event space mix.
The purely location-based LM term weight is captured in the following definition: The prefix  X  LL  X  indicates the respective event spaces. For both, reports a mix of event spaces, and the prefix  X  LD  X  indicates this in the next definition: The Poisson bridge (equation 16) relates the purely location-based estimate in equation 24 and the location-document-based mix in equation 25. Inserting P L ( t | c )= avgtf ( t,c ) injects P D ( t | c ) into the purely location-based estimate: Analogously, inserting P D ( t | c ) = avgdl ( c ) tion 25 injects P L ( t | c ) into the document-location mix:
The above explanation regarding the event space mix has an im-pact on the understanding and validity of parameter estimation. The location-document mix requires from a probabilistic seman-tics point of view a justification; the  X  X ure X  probability theoretician asks for a uniform event space. Since a location-based formulation can be achieved, this problem is solved. For the location-document mix, the setting of the mixture parameter  X  can be viewed as the  X  X ix X  that transfers the location-document mix into the location-based formulation. This explanation supports that LM could be viewed as a probabilistic interpretation of TF-IDF. Since this in-terpretation is still distant to the genuine TF-IDF, the next section explores the LM-like decomposition of P ( d | q ) .
The following equation is the starting point for P ( d | q,c ) Equation 28 corresponds to equation 20 with roles of d and changed. Which transformations and assumptions lead to TF-IDF?
Our research explored numerous ways to estimate P ( t | q,c ) the most conclusive transformations and assumptions are reported next. The first transformation splits the product over document terms into two products: a product over document and query terms, and a product over document-only terms.
 P ( d | q,c )= To estimate the unknown term probability P ( t | q,c ) , the following query/non-query term assumption is applied.
 A SSUMPTION 1. P ( t | q,c ) assumption: For query terms: This assumption can be viewed as an  X  X xtreme X  mixture, since in equation 21, this assumption corresponds for query terms to and for non-query terms to  X  =0 . The next equation builds on this assumption; P ( t | q,c ) is replaced by P ( t | q ) and tively.
 P ( d | q,c ) = Equation 30 is multiplied with 1 . 0 = Q Through this, the product over document-only terms (right prod-uct) becomes a product over all document terms.
 P ( d | q,c )= The right product corresponds to P ( d | c ) . We move this document normalisation factor to the left side of the equation; this improves the readability of the equations to follow, and it prepares for estab-lishing an analogy to the probabilistic odds (section 2.3). P ( d | q,c ) equation for  X  X xtreme X  mixture.

The next two sections show the document-based and location-based estimation of P ( d | c ) , P ( t | q ) , and P ( t | c ) based estimation could be viewed as  X  X ncorrect X  since the start-ing point is independent terms; however, the tradition in IR to mix document-based and location-based estimates is a significant ratio-nale to investigate document-based estimates. The document-based estimation applied to equation 32 yields: Then, the logarithmic form is as follows: log P D ( d | q,c ) This leads to the following interpretation of TF-IDF:
For independent terms, P ( d | q,c ) , and document-based probabil-ities, TF-IDF assumes idf ( t,q )=0 , i.e. P D ( t | q )=1
This is an interesting interpretation of TF-IDF. The discrimina-tiveness expressed by idf ( t,c ) is combined with the query-specific covers TF-IDF. The interpretation of the query term probability P
D ( t | q ) is to view the query as a structured document; frequent terms occur in every part of the query, i.e. idf ( t,q ) = 0 non-frequent terms, idf ( t,q ) &gt; 0 . The query-independent docu-ment prior P D ( d | c )= 1 Inserting the location-based probabilities into equation 32 yields: Equation 36 corresponds to equation 33. To approach TF-IDF, the based probability P D ( t | c ) . This leads to the next equation: The logarithmic transformation yields: log( P L ( d | q,c ))  X  log( P L ( d | c )) = (38) Equation 38 corresponds to equation 34, and the final step applies idf ( t,c ) =  X  log P D ( t | c ) to uncover TF-IDF. log( P L ( d | q,c ))  X  X
For independent terms, P ( d | q,c ) , and location-based probabili-ties, TF-IDF assumes avgtf ( t,c ) = P L ( t | q )  X  avgdl
The location-based equation 39 corresponds to the document-based equation 35.

The component log( P L ( d | c )) = P query-independent TF-IDF value of the document. P L ( d | c ) for documents that contain discriminative terms. For the document-based case, P D ( t | q ) = 1 is assumed by TF-IDF. This corresponds to BM25, where in n L ( t,q ) / ( n K =0 is a common setting, i.e. the within-query term frequency is ignored.

For the location-based case, P L ( t | q ) = avgtf ( t,c ) by TF-IDF. This is reasonable since it reflects the probability of a term in a random document. For a random document N
L ( q )= avgdl ( c ) is the expected document length, and avgtf ( t,c ) is the expected within-document term frequency.
For independent terms, the following sequence of equations de-composes the probabilistic odds: The ranking equivalence in equation 40 follows from Bayes X  the-document-independent, and P ( d,q ) cancels out.

Equation 41 views document d as a sequence of conditionally independent term events; thereby, the frequency (multiple occur-rence) of a term is captured by the exponent n L ( t,d ) .
The next transformation reflects the non-query-term assumption: query terms occur in relevant documents as they occur in non-relevant documents. Through this, the product over all document terms reduces to the product over document and query terms. A softer approach is to assume P ( t | q,r ) /P ( t | q,  X  r ) for the non-query terms, but due to the space in this paper, this is not pursued here. For convenience, we omit q in the conditional from now. This is consistent, since r implies q .
 Equation 42, O ( r | d,q ) , shows a strong analogy to equation 32, This is reasonable since the relevant documents can be viewed as the query, and viewing the collection as an approximation of non-relevant documents is common.

The next sections discuss the document-based and location-mediate transformations, and discuss directly the interpretations.
The log transformation of equation 42 followed by inserting the document-based probabilities and idf definition leads to: log O D ( r | d,q ) rank = X
For independent terms, O ( r | d,q ) , and document-based probabil-ities, TF-IDF assumes idf ( t,r ) = 0 , i.e. P D ( t | r ) = 1
For  X  r = c and r = q , the odds-based interpretation in equation 43 is equivalent to the interpretation based on P ( d | q,c ) This reflects that the collection corresponds to non-relevant docu-ments, and that the query corresponds to relevant documents.
For the location-based probabilities, equation 42 and the Poisson bridge (equation 16) lead to: log O L ( r | d,q ) rank = (44)
For independent terms, O ( r | d,q ) , and location-based probabili-ties, TF-IDF assumes avgtf ( t,  X  r ) = P L ( t | r )  X  avgdl
For  X  r = c and r = q , the odds-based interpretation in equation 44 is equivalent to the interpretation based on P ( d | q,c )
P D ( t | r )=1 means that for missing relevance, TF-IDF assumes that the term occurs in each of the relevant documents. This corre-sponds to assuming the optimistic case, where P D ( t | r )=0 . 5 neutral case, and P D ( t | r )=0 is the pessimistic case.
For a term in non-relevant documents, P L ( t |  X  r ) = avgtf expected probability, because the expected number of t -location in  X  r locations in  X  r is N L (  X  r )= n D ( t,  X  r )  X  avgdl the assumption P L ( t | r )= P L ( t |  X  r ) .
The theorem of the total probability allows to decompose e ,...,e n (i.e. P ( e i  X  e j ) = 0 and 1 . 0 = P i P ( e i For the event (hypothesis) h being a document or query, and for events (evidence) e i being terms, the decomposition of P ( q | d ) P ( d | q ) follows. For P ( q | d ) , the decomposition via disjoint terms yields: This decomposition is  X  X s usual X ; P ( q | t ) = P ( q | t,d ) cause of the linked-independence assumption. In this decomposi-tion, the collection c is not explicit. Therefore, the next decompo-sition is given.
 The equations 50 and 51 reflect the application of Bayes X  theo-rem for P ( t | d,c ) and P ( q | t,c ) , respectively. Inserting TF-IDF from equation 51 (section 3.6).
The decomposition of P ( d | q,c ) is analogous to equations 49, 50 and 51, just the roles of d and q change.

The next section looks at the estimation of the term probabilities.
The maximum-likelihood parameter estimation proposes the fol-lowing, systematic estimation: P P For example, let term t occur in n L ( t,c ) = 100 locations of col-lection c , and let the term occur in n L ( t,d ) = 2 locations of docu-ment d . Then, P L ( d | t,c )= n L ( t,d ) a t -location that is in document d . Accordingly, P L ( t | d,c ) probability to draw a location from d that shows term t . the probability to draw a location from c that is in document query probabilities are analogous. Regarding the collection, for N to draw a t -location. Equation 50 contains the joint probability P ( d,q | c ) : This is the starting point for consequently inserting the maximum-document-query independence (DQI) measure, which can be re-lated to TF-IDF through the integral of the DQI. The transforma-tion steps uncover divergence from randomness and pivoted docu-ments length to be inherent parts of the DQI.
The next equation is obtained by inserting the maximum likeli-hood estimates into equation 52.
 P ( d,q | c ) = 1 P ( d,q | c ) equation: location frequencies only
Equation 53 contains location frequencies only. Therefore, the next section injects P D ( t | c ) to approach TF-IDF. The equation n L ( t,c ) = n L ( t,c ) following equation for the joint probability P ( d,q | c )
P ( d,q | c ) =
Equation 54 shows an interesting component, namely: The rationale of this divergence component is: for average , and less than 1 . 0 for poor terms. The next transformation forms the conditional probability ability. Since the average document length is avgdl ( c ) := we obtain from equation 54:
P ( q | d,c ) =
Equation 55 uncovers another essential component, namely: The rationale of this normalisation component is: The component n L ( t,d ) / N L ( c ) equal to n L ( t,d ) for average , and greater than n L ( t,d ) documents. The next transformation divides P ( q | d,c ) (equation 55) by a symmetric document-query independence measure: Document-Query Independence (DQI)
This measure reflects the overlap of document and query: Note the special case for an average term t avg in an average docu-ment: the average term occurs with n L ( t avg ,d ) = avgtf document d , and the document length is N L ( d )= avgdl ( c ) the DQI of an average term in an average document becomes:
The middle graph in figure 2 shows DQI curves for an average term t avg , a good term t good , n L ( t good ) = 2  X  avgtf with five terms, i.e. P L ( t | q )= 1
The middle graph illustrates that for a good term, the DQI is greater than 1 . 0 if the term occurs with P D ( t | c ) &lt; 0 . 4 P term has a strong impact (DQI &gt; 1 ) even if the term is relatively frequent ( P D ( t | c ) relatively large), whereas a poor term needs to be rarer ( P D ( t | c ) relatively small) for achieving an impact. The remaining question is: how does the DQI relate to TF-IDF? From equation 56 and from the general formulation of TF-IDF in definition 7, the following formulation of TF-IDF is born: This formulation is referred to as the DQI-TF-IDF since the formulation is motivated by the derivation of the DQI. The inverse viewing TF-IDF as an integral of the DQI.
TF-IDF can be interpreted as an area under the DQI curve. This is because of the following integral: the inverse document frequency idf ( t,c ) =  X  log P D ( t | c ) achieved by the definite integral ranging from P D ( t | c )
Figure 2 illustrates this interpretation of TF-IDF. The left plot shows TF-IDF to be the area under the DQI curve. The y-axis shows the DQI values for an average term ( n L ( t,d ) = avgtf in an average document ( N L ( d )= avgdl ( c ) ), and the x-axis corre-sponds to the probability P D ( t | c ) . The example shows a term with n ( t,d )=2 , in a document with N L ( d )=100 . The query has five terms, i.e. P L ( t | q )=1 / 5 .

The middle plot shows DQI curves for three terms: an aver-age term, a good term ( n L ( t good ,d ) = 2  X  avgtf ( t poor term ( n L ( t poor ,d ) = 0 . 5  X  avgtf ( t poor ,c ) curve for t avg follows from avgdl ( c ) / avgtf ( t,c )  X  P Then, P L ( t avg | q )  X  1 /x is the DQI of t avg , where ity P D ( t | c ) that term t occurs in a document of collection example, if P D ( t avg | c )=0 . 2 , then DQI ( t avg , 0 . 2)=1 The right plot shows the TF-IDF values of the three terms, i.e. the TF-IDF values correspond to the area under the respective DQI curve in the middle plot. In the right plot, the TF-IDF values for a good term at P D ( t | c ) = 0 . 4 , an average term at and a poor term at P D ( t | c )=0 . 1 are marked, since at these points, the gradient of the TF-IDF curve is equal to 1 . 0 probability P D ( t | c ) where the TF-IDF slope changes from large to small.

For facilitating the mathematical expressions to follow, the in-verse average term frequency iatf of a term is defined as follows: inverse average term frequency: iatf ( t,c ) := avgdl ( c ) For example, in a collection with avgdl ( c )=1 , 000 , if a term occurs in average 10 times in its elite set ( avgtf ( t,c )=10 ), then iatf 1 , 000 / 10=100 .

The interpretation of TF-IDF as the integral of DQI over the probability x = P D ( t | c ) is expressed in the following equations:
RSV DQI-TF-IDF ( t ) = iatf ( t,c )  X  P L ( t | d )  X  P L
The values of the occurrence probability x = P D ( t | c ) DQI ( t,x ) = 1 holds, deem to be of particular interest. There-fore, the DQI and TF-IDF plots show the respective points. These are where TF-IDF changes from fast to slow fall; these are where RSV DQI-TF-IDF ( t ) = P D ( t | c )  X  X  log P D ( t | c ) value corresponds to the Shannon quantification H ( t ) . Therefore, the DQI threshold DQI ( t,P D ( t | c )) = 1 seems to open new op-portunities for judging the power of terms to discriminate between relevant and non-relevant documents.

Initial experiments with DQI-TF-IDF (on INEX and TREC-3) showed that the DQI-TF-IDF achieves about 80% of the MAP and P@10 quality delivered by best-performing functions such as TF-IDF, BM25, and LM retrieval functions. In TF-IDF and BM25, the saturating TF component n L ( t,d ) / ( n L ( t,d )+ K ) pressing the pivoted document length, is responsible for the supe-rior quality; for LM, the mixture parameter was set to  X  :=0 . 8
For illustrating the math of this paper, this section shows numer-ical values for the term  X  X erl X  in two documents (it/2000/f2020 and co/2000/rz004) of the INEX collection.
 N L ( c ) 32 . 4m number of locations N
D ( c ) 12 , 107 number of documents avgdl ( c ) 2 , 676 avg document length n
L ( t,c ) 993 number of t -locations n
D ( t,c ) 279 number of t -documents avgtf ( t,c ) 3 . 56 average term frequency: 993 / 279 P
D ( t | c ) 0 . 023 term probability: 279 / 12 , 107 iatf ( t,c ) 752 inverse avg term frequency N
L ( d 1 ) 2 , 560 document with approx avg length n L ( t,d 1 ) 49 term is good in d 1 : 49 &gt; 3 . 56 DQI ( t,d 1 ,q,c ) 124 . 91 iatf ( t,c )  X  P L ( t | d )  X  P DQI-TF-IDF ( t ) 10 . 85 iatf ( t,c )  X  P L ( t | d )  X  P N
L ( d 2 ) 483 document shorter than avg document n L ( t,d 2 ) 4 term is avg in d 2 : 4  X  3 . 56 DQI ( t,d 2 ,q,c ) 54 . 04 DQI-TF-IDF ( t ) 4 . 70 The term occurs with n L ( t,d 1 )=49 and n L ( t,d 2 )=4 ear maximum-likelihood-based DQI, this makes d 1 about 12 times better than d 2 . Since d 1 is about 5.5 times longer than and TF-IDF values for d 1 are about twice of the values for
This paper contributes a study of theories and probabilities to un-cover meanings of TF-IDF. Research on a probabilistic relational framework has led to the results in this paper, and the conclusive and probabilistic interpretations of TF-IDF were reported. The in-terpretations were explored systematically for two spaces: indepen-dent and disjoint terms.

For independent terms, the LM-like decomposition of P ( d | q,c ) tation of TF-IDF. Central to this interpretation is an  X  X xtreme X  query/non-query assumption: P ( t | q,c ) = P ( t | q ) for query terms, and P ( t | q,c ) = P ( t | c ) for non-query terms. With regard to tra-ditional LM, this corresponds to setting the LM mixture param-eter to  X  = 1 for document terms, and  X  = 0 for non-document terms. Another result comes from the decomposition of probabilis-tic odds; it leads to a TF-IDF interpretation showing an analogy term probabilities expressed in the equations  X  r = c and
For disjoint terms, the decomposition of P ( d,q | c ) yields man-ifold results. Divergence from randomness and pivoted document length are shown to be inherent parts of a document-query indepen-dence (DQI) measure, where the DQI follows from the consequent application of maximum-likelihood estimates. An integral of the DQI was shown to lead from probability theory to TF-IDF.
The LM-based interpretation of TF-IDF based on P ( d | q,c ) an  X  X xtreme X  mixture, the symmetric DQI measure embedding di-vergence from randomness and pivoted document length, and the interpretation of TF-IDF as integral of the DQI contribute theoret-ical results that uncover novel meanings and properties of TF-IDF in particular and retrieval models in general.

Outlook: The DQI was elaborated in this paper for disjoint terms. The LM (starting point P ( q | d,c ) ) and the LM-like inter-pretation of TF-IDF (starting point P ( d | q,c ) ) both lead to a DQI for independent terms, and this could be explored in future work.
Integrals are central in physics; this triggered a study of what connects physics and TF-IDF. The recommendations for publica-tion asked to replace this study and details about experiments by more explanations of the theory and assumptions.

Acknowledgements: We would like to thank the reviewers who worked their way through the theory and provided helpful feed-back. We are grateful for a travel fund from a collaboration with Microsoft Research Cambridge. Various explanations and details in this paper stem from an exchange on probabilistic retrieval mod-els with Stephen Robertson. [1] Akiko Aizawa. An information-theoretic perspective of tf-idf [2] Gianni Amati and C. J. van Rijsbergen. Probabilistic models [3] K. Church and W Gale. Inverse document frequency (idf): A [4] W.B. Croft and D.J. Harper. Using probabilistic models of [5] Arjen de Vries and Thomas Roelleke. Relevance [6] David A. Grossman and Ophir Frieder. Information [7] Djoerd Hiemstra. A probabilistic justification for using tf.idf [8] John Lafferty and ChengXiang Zhai. Probabilistic Relevance [9] Qiaozhu Mei, Hui Fang, and ChengXiang Zhai. A study of [10] J.M. Ponte and W.B. Croft. A language modeling approach [11] S. E. Robertson and S. Walker. Some simple effective [12] S.E. Robertson. Understanding inverse document frequency: [13] S.E. Robertson and K. Sparck Jones. Relevance weighting of [14] Thomas Roelleke. A frequency-based and a Poisson-based [15] Thomas Roelleke and Jun Wang. A parallel derivation of [16] S.K.M. Wong and Y.Y. Yao. On modeling information [17] Hugo Zaragoza, Djoerd Hiemstra, and Michael E. Tipping.
