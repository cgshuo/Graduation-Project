 Decentralized and partially observable stochastic decision and planning problems are very common, comprising anything from strategic games of chance to robotic space exploration. In such domains, multiple agents act under uncertainty about both their environment and the plans and actions of others. These problems can be represented as decentralized partially observable Markov decision processes (Dec-POMDPs), or the equivalent, partially observable stochastic games (POSGs), al-lowing for precise formulation of solution concepts and success criteria.
 Alas, such problems are highly complex. As shown by Bernstein et al. [1, 2], the full, cooperative problem X  X here all players share the same payoff, and strategies can depend upon entire observed histories X  X s NEXP-complete. More recently, Goldsmith and Mundhenk [3] showed that the com-petitive case can be worse: when teamwork is allowed among agents, complexity rises to NEXP NP (problems solvable by a NEXP machine employing an NP set as an oracle). Much attention has thus been paid to restricted cases, particularly those where some parts of the system dynamics be-have independently. The complexity of finite-horizon Dec-POMDPs goes down X  X rom NEXP to NP X  X hen agents interact only via a joint reward structure, and are otherwise independent. Un-fortunately, our new results show that further reduction, based on other combinations of fully or partially independent system dynamics are unlikely, if not impossible.
 We show that if the situation were reversed, so that rewards alone are independent, the problem re-mains NEXP-complete. Further, we consider two other Dec-POMDP sub-classes from the literature: (a) domains where local agent sub-problems are independent except for a (relatively small) number of event-based interactions , and (b) those where agents only interact influencing the set of currently available actions . As it turns out, both types of problem are NEXP-complete as well X  X acts previ-ously unknown. (In the latter case, this is a substantial increase in the known upper bound.) These results provide further impetus to devise new tools for the analysis and classification of problem difficulty in decentralized problem solving. The cooperative, decentralized partially observable Markov decision process (Dec-POMDP) is a highly general and powerful framework, capable of representing a wide range of real-world problem domains. It extends the basic POMDP to multiple agents, operating in conjunction based on locally observed information about the world, and collecting a single source of reward.
 Definition 1 (Dec-POMDP) . A (Dec-POMDP), D , is specified by a tuple: with individual components as follows: The most important sub-instance of the Dec-POMDP model is the decentralized MDP (Dec-MDP), where the joint observation tells us everything we need to know about the system state. Definition 2 (Dec-MDP) . A decentralized Markov decision process (Dec-MDP) is a Dec-POMDP that is jointly fully observable . That is, there exists a functional mapping, J :  X  1  X  X  X  X  X   X  n  X  S , such that O ( a 1 ,...,a n , s 0 , o 1 ,...,o n ) 6 = 0 if and only if J ( o 1 ,...,o n ) = s 0 . In a Dec-MDP, then, the sum total of the individual agent observations provides a complete pic-ture of the state of the environment. It is important to note, however, that this does not mean that any individual agent actually possesses this information. Dec-MDPs are still fully decentralized in general, and individual agents cannot count on access to the global state when choosing actions. Definition 3 (Policies) . A local policy for an agent  X  i is a mapping from sequences of that agent X  X  collection of local policies, one per agent,  X  =  X   X  1 ,...,  X  n  X  .
 A solution method for a decentralized problem seeks to find some joint policy that maximizes ex-pected value given the starting state (or distribution over states) of the problem. For complexity purposes, the decision version of the Dec-(PO)MDP problem is to determine whether there exists some joint policy with value greater at least k . Before establishing our new claims, we briefly review the NEXP-completeness result for finite-horizon Dec-MDPs, as given by Bernstein et al. [1, 2]. First, we note that the upper bound, namely that finite-horizon Dec-POMDPs are in NEXP, will immediately establish the same upper bound for all the problems that we will consider. (While we do not discuss the proof here, full details can be found in the original, or the supplemental materials to this paper,  X  1.) Theorem 1 (Upper Bound) . The finite-horizon, n -agent decision problem Dec-POMDP  X  NEXP. More challenging (and interesting) is establishing lower bounds on these problems, which is per-formed via our reduction from the known NEXP-complete TILING problem [4, 5]. A TILING tiles H, V  X  L  X  L . A tiling is a mapping of board locations to tile-types, t : { 0 ,...,n  X  1 } X  { 0 ,...,n  X  1 }  X  L ; such a tiling is consistent just in case (i) the origin location of the board receives tile-type 0 ( t (0 , 0) = tile 0 ); and (ii) all adjoint tile assignments are compatible: The TILING problem is thus to decide, for a given instance, whether such a consistent tiling exists. Figure 1 shows an example instance and consistent solution. The reduction transforms a given instance of TILING into a 2-agent Dec-MDP, where each agent is queried about some location in the grid, and must answer with a tile to be placed there. By careful design of the query and response mechanism, it is ensured that a policy with non-negative value exists only if the agents already have a consistent tiling, thus showing the Dec-MDP to be as hard as TILING. Together with Theorem 1, and the fact that the finite-horizon, 2-agent Dec-MDP is a special case of the general finite-horizon Dec-POMDP, the reduction establishes Bernstein X  X  main complexity result ( again, details are in the supplemental materials,  X  1 ): Theorem 2 (NEXP-Completeness) . The finite-horizon Dec-POMDP problem is NEXP-complete. In general, the state transitions, observations, and rewards in a Dec-POMDP can involve probabilis-tic dependencies between agents. An obvious restricted subcase is thus one in which these factors are somehow independent. Becker et al. [6, 7] have thus studied problems in which the global state-space consists of the product of local states, so that each agent has its own individual state-space. A Dec-POMDP can then be transition independent , observation independent , or reward independent , as each the local effects given by each corresponding function are independent of one another. Definition 4 (Factored Dec-POMDP) . A factored, n -agent Dec-POMDP is a Dec-POMDP such that the system state can be factored into n + 1 distinct components, so that S = S 0  X  S 1  X  X  X  X  X  S n , and no state-variable appears in any S i , S j , i 6 = j .
 As with the local (agent-specific) actions, a i , and observations, o i , in the general Dec-POMDP definition, we now refer to the local state ,  X  s  X  S i  X  S 0 , namely that portion of the overall state-notation s  X  i for the sequence of all state-components except that for agent  X  i : (and similarly for action-or observation-sequences, a  X  i and o  X  i ).
 Definition 5 (Transition Independence) . A factored, n -agent DEC-POMDP is transition inde-pendent iff the state-transition function can be separated into n + 1 distinct transition functions P , ..., P n , where, for any next state s 0 i  X  S i , In other words, the next local state of each agent is independent of the local states of all others, given its previous local state and local action, and the external system features ( S 0 ). Definition 6 (Observation Independence) . A factored, n -agent Dec-POMDP is observation inde-pendent iff the joint observation function can be separated into n separate probability functions O 1 , ..., O n , where, for any local observation o i  X   X  i , In such cases, the probability of an agent X  X  individual observations is a function of their own local states and actions alone, independent of the states of others, and of what those others do or observe. Definition 7 (Reward Independence) . A factored, n -agent Dec-POMDP is reward independent iff the joint reward function can be represented by local reward functions R 1 ,...,R n , such that: and That is, joint reward is a function of local reward, constrained so that we maximize global reward if and only if we maximize local rewards. A typical example is the additive sum: It is important to note that each definition applies equally to Dec-MDPs; in such cases, joint full observability of the overall state is often accompanied by full observability at the local level . Definition 8 (Local Full Observability) . A factored, n -agent Dec-MDP is locally fully observable Local full observability is not equivalent to independence of observations. In particular, a problem may be locally fully observable without being observation independent (since agents may simply observe outcomes of non-independent joint actions). On the other hand, it is easy to show that an observation-independent Dec-MDP must be locally fully observable ( supplementary,  X  2 ). 4.1 Shared rewards alone lead to reduced complexity It is easy to see that if a Dec-MDP (or Dec-POMDP) has all three forms of independence given by Definitions 5 X 7, it can be decomposed into n separate problems, where each agent  X  i works solely within the local sub-environment S i  X  S 0 . Such single-agent problems are known to be P-complete, and can generally be solved efficiently to high degrees of optimality. More interesting results follow when only some forms of independence hold. In particular, it has been shown that Dec-MDPs with both transition-and observation-independence, but not reward-independence, are NP-complete [8, 7]. ( This result is discussed in detail in our supplementary material,  X  3. ) Theorem 3. A transition-and observation-independent Dec-MDP with joint reward is NP-complete. As our new results will now show, there is a limit to this sort of complexity reduction: other relatively obvious combinations of independence relationships do not bear the same fruit. That is, we show the NP-completeness result to be specific to fully transition-and observation-independent problems. When these properties are not fully present, worst-case complexity is once again NEXP. 5.1 Reward-independent-only models are NEXP-complete We begin with a result that is rather simple, but has not, to the best of our knowledge, been estab-lished before. We consider the inverse of the NP-complete problem of Theorem 3: a Dec-MDP with reward-independence (Df. 7), but without transition-or observation-independence (Dfs. 5, 6). Theorem 4. Factored, reward-independent Dec-MDPs with n agents are NEXP-complete.
 Proof Sketch. For the upper bound, we simply cite Theorem 1, immediately establishing that such problems are in NEXP. For the lower bound, we simply modify the TILING Dec-MDP from Bern-stein X  X  reduction proof so as to ensure that the reward-function factors appropriately into strictly local rewards. ( Full details are found in [9], and the supplementary materials,  X  4.1. ) Thus we see that in some respects, transition and observation independence are fundamental to the reduction of worst-case complexity from NEXP to NP. When only the rewards depend upon the actions of both agents, the problems become easier; however, when the situation is reversed, the general problem remains NEXP-hard. This is not entirely surprising: much of the complexity of planning in decentralized domains stems from the necessity to take account of how one X  X  action-outcomes are affected by the actions of others, and from the complications that ensue when observed information about the system is tied to those actions as well. The structure of rewards, while ob-viously key to the nature of the optimal (or otherwise) solution, is not as vital X  X ven if agents can separate their individual reward-functions, making them entirely independent, other dependencies can still make the problem extremely complex.
 We therefore turn to two other interesting special-case Dec-MDP frameworks, in which independent reward functions are accompanied by restricted degrees of transition-and observation-based interac-tion. While some empirical evidence has suggested that these problems may be easier on average to solve, nothing has previously been shown about their worst-case complexity. We fill in these gaps, showing that even under such restricted dynamics, the problems remain NEXP-hard. 5.2 Event-driven-interaction models are NEXP-complete The first model we consider is one of Becker et al. [10], which generalizes the notion of a fully transition-independent Dec-MDP. In this model, a set of primitive events , consisting of state-action that agent takes the given action to generate the associated state transition. Dependencies are then introduced in the form of relationships between one agent X  X  possible actions in given states and another agent X  X  primitive events.
 While no precise worst-case complexity results have been previously proven, the authors do point out that the class of problems has an upper-bound deterministic complexity that is exponential in the size of the state space, | S | , and doubly exponential in the number of defined interactions. This potentially bad news is mitigated by noting that if the number of interactions is small, then reasonably-sized problems are NEXP-hard (indeed, NEXP-complete); however, when the number of dependencies is a log-factor of the size of the problem state-space, worst-case NP-hardness is achieved. We begin with the formal framework of the model. Again, we give all definitions in terms of Dec-POMDPs; they apply immediately to Dec-MDPs in particular.
 Definition 9 (History) . A history for an agent  X  i in a factored, n -agent Dec-POMDP D is a sequence When a problem has a finite time-horizon T , all possible complete histories will be of the form  X  i = [ X  s representing a transition between two local states, given some action a i  X  A i . An event E = { e 1 ,e 2 ,...,e h } is a set of primitive events. A primitive event e occurs in the history  X  i , written  X  i e , if and only if the triple e is a sub-sequence of the sequence  X  i . An event E occurs in the history  X  i , written  X  i E , if and only if some component occurs in that history:  X  e  X  E :  X  i e . Events can therefore be thought of disjunctively. That is, they specify a set of possible state-action transitions from a Dec-POMDP, local to one of its agents. If the historical sequence of state-action transitions that the agent encounters contains any one of those particular transitions, then the history satisfies the overall event. Events can thus be used, for example, to represent such things as taking a particular action in any one of a number of states over time, or taking one of several actions at some particular state. For technical reasons, namely the use of a specialized solution algorithm, these events are usually restricted in structure, as follows.
 Definition 11 (Proper Events) . A primitive event e is proper if it occurs at most once in any given history. That is, for any history  X  i if  X  i =  X  1 i e  X  2 i then neither sub-history contains e :  X  ( X  1 i e )  X  X  ( X  2 i e ) . An event E is proper if it consists of proper primitive events that are mutually exclusive, in that no two of them both occur in any history: Proper primitive events can be used, for instance, to represent actions that take place at particular times (building the time into the local state  X  s i  X  e ). Since any given point in time can only occur once in any history, the events involving such time-steps will be proper by default. A proper event E can then be formed by collecting all the primitive events involving some single time-step, or by taking all possible primitive events involving an unrepeatable action.
 Our new model is then a Dec-MDP with: Interactions between agents are given in terms of a set of dependencies between certain state-action transitions for one agent, and events featuring transitions involving the other agent. Thus, if a history contains one of the primitive events from the latter set, this can have some direct effect upon the transition-model for the first agent, introducing probabilistic transition-dependencies. such that each pair occurs in at most one dependency: Such a dependency is thus a collection of possible actions that agent  X  j can take in one of its local state, each of which depends upon whether the other agent  X  i has made one of the state-transitions in its own set of primitive events. Such structures can be used to model, for instance, cases where one agent cannot successfully complete some task until the other agent has completed an enabling sub-task, or where the precise outcome depends upon the groundwork laid by the other agent. Definition 13 (Satisfying Dependencies) . A dependency d k ij =  X  E k i ,D k j  X  is satisfied when the current history for enabling agent  X  i contains the relevant event:  X  i E k i . For any state-action pair  X   X  s j ,a j  X  , we define a Boolean indicator variable b  X  s j a j , which is true if and only if some dependency that contains the pair is satisfied: The existence of dependencies allows us to factor the overall state-transition function into two parts, each of which depends only on an agent X  X  local state, action, and relevant indicator variable. Definition 14 (Local Transition Function) . The transition function for our Dec-MDP is factored into two functions, P 1 and P 2 , each defining the distribution over next possible local states: P When agents take some action in a state for which dependencies exist, they observe whether or not the related events have occurred; that is, after taking any action a j in state s j , they can observe the state of indicator variable b  X  s j a j .
 With these definitions in place, we can now show that the worst-case complexity of the event-based problems is the same as the general Dec-POMDP class.
 Theorem 5. Factored, finite-horizon, n -agent Dec-MDPs with local full observability, independent rewards, and event-driven interactions are NEXP-complete.
 Proof Sketch. Again, the upper bound is immediate from Theorem 1, since the event-based structure is just a specific case of general reward-dependence, and such models can always be converted into Dec-MDPs without any events. For the lower bound, we again provide a reduction from TILING, constrained to our special case. Local reward independence, which was not present in the original problem, is ensured by using event dependencies to affect future rewards of the other agent. Thus, local immediate rewards remain dependent only upon the actions of the individual agent, but the state in which that agent finds itself (and so the options available to its reward function) can depend upon events involving the other agent. ( See [9] and supplemental materials,  X  4.2. ) 5.2.1 A special, NP-hard case The prior result requires allowing the number of dependencies in the problem to grow as a factor of log n , for a TILING grid of size ( n  X  n ) . Since the size of the state-space S in the reduced Dec-MDP is also O (log n ) , the number of dependencies is O ( | S | ) . Thus, the NEXP-completeness result holds for any event-based Dec-MDP where the number of dependencies is linear in the state-space. When we are able to restrict the number of dependencies further, however, we can do better. Theorem 6. A factored, finite-horizon, n -agent Dec-MDP with local full observability, independent rewards, and event-driven interactions are solvable in nondeterministic polynomial time (NP) if the number of dependencies is O (log | S | ) , where S is the state-set of the problem.
 Proof Sketch. As shown by Becker [10], we can use the Coverage Set algorithm to generate an optimal policy for a problem of this type, in time that is exponential in the number of dependencies. Clearly, if this number is logarithmic in the size of the state-set, then solution time is polynomial in the problem size. ( See [9] and supplemental materials,  X  4.2.1. ) 5.2.2 Discussion of the results These results are interesting for two reasons. First, NEXP-completeness of the event-based case, even with independent rewards and local full observability (Theorem 5), means that many interest-ing problems are potentially intractable. Becker et al. [10] show how to use event-dependencies to represent common structures in the TAEMS task modeling language, used in many real-world domains [11, 12, 13]; our complexity analysis thus extends to such practical problems. Second, isolating where complexity is lower can help determine what task structures and agent interrelation-ships lead to intractability. In domains where the dependency structure can be kept relatively simple, it may be possible to derive optimal solutions feasibly. Both subjects are worth further study. 5.3 State-dependent-action models are NEXP-complete Guo and Lesser [14, 15, 16] consider another specialized Dec-MDP subclass, with apparently even more restricted types of interaction. Agent state-spaces are again separate, and all action-transitions and rewards are independent. Such problems are not wholly decoupled, however, as the actions available to each agent at any point depend upon the global system state. Thus, agents interact by making choices that restrict or broaden the range of actions available to others.
 Definition 15 (Dec-MDP with State-Dependent Actions) . An n -agent Dec-MDP with state-dependent actions is a tuple D =  X  S 0 , { S i } , { A i } , { B i } , { P i } , { R i } ,T  X  , where: Note that there need be no observations in such a problem; given local full observability, each agent available actions in any state; a local policy is thus a mapping from local states to available actions. For such cases, Guo presents a planning algorithm based on heuristic action-set pruning, along with a learning algorithm. While empirical results show that these methods are capable of solving potentially large instances, we again know very little about the analytical worst-case difficulty of problems with state-dependent actions. An NP-hardness lower bound is given [14] for the overall class, by reducing a normal-form game to the state-dependent model, but this is potentially quite weak, since no upper bound has been established, and even the operative algorithmic complexity of the given solution method is not well understood. We address this situation, showing that the problem is also just as hard as the general case. Theorem 7. Factored, finite-horizon, n -agent Dec-MDPs with local full observability, independent rewards, and state-dependent action-sets are NEXP-complete.
 Proof Sketch. Once more, we rely upon the general upper bound on the complexity of Dec-POMDPs (Theorem 1). The lower bound is by another TILING reduction. Again, we  X  X ecord X  actions of each agent in the state-space of the other, ensuring purely local rewards and local full observability. This time, however, we use the fact that action-sets depend upon the global state (rather than events) to enforce the desired dynamics. That is, we add special state-dependent actions that, based on their availability (or lack thereof), affect each agent X  X  local reward. ( See [9], and supplemental  X  4.3. ) 5.3.1 Discussion of the result Guo and Lesser [16, 14] were able to show that deciding whether a decentralized problem with state-based actions had an equilibrium solution with value greater than k was NP-hard. It was not ascertained whether or not this lower bound was tight, however; this remained a significant open question. Our results show that this bound was indeed too low. Since an optimal joint policy will be an equilibrium for the special case of additive rewards, the general problem can be no easier. This is interesting, for reasons beyond the formal. Such decentralized problems indeed appear to be quite simple in structure, requiring wholly independent rewards and action-transitions, so that agents can only interact with one another via choices that affect which actions are available. (A typical example involves two persons acting completely regardless of one another, except for the existence of a single rowboat, used for crossing a stream; if either agent uses the rowboat to get to the other side, then that action is no longer available to the other.) Such problems are intuitive, and common, and not all of them are hard to solve, obviously. At the same time, however, our results show that the same structures can be intractable in the worst case, establishing that even seemingly simple interactions between agents can lead to prohibitively high complexity in decentralized problems. This work addresses a number of existing models for decentralized problem-solving. In each case, general Dec-POMDP problem. It has been known for some time that systems where agents act entirely independently, but share rewards, have reduced worst-case complexity. We have shown that this does not apply to other variants, where we relax the independence requirements even only a little. In all of the cases addressed, the new problem variants are as hard as the general case. This fact, combined with results showing many other decentralized problem models to be equivalent to the general Dec-POMDP model, or strictly harder [17], reveals the essential difficulty of optimal planning in decentralized settings. Together, these results begin to suggest that optimal solutions to many common multiagent problems must remain out of reach; in turn, this indicates that we must look to approximate or heuristic methods, since such problems are so prevalent in practice. At the same time, it must be stressed that the NEXP-complexity demonstrated here is a worst-case measure. Not all decentralized domains are going to be intractable, and indeed the event-based and action-set models have been shown to yield to specialized solution methods in many cases, enabling us to solve interesting instances in reasonable amounts of time. When the number of action-dependencies is small, or there are few ways that agents can affect available action-sets, it may well be possible to provide optimal solutions effectively. That is, the high worst-case complexity is no guarantee that average-case difficulty is likewise high. This remains a vital open problem in the field. While establishing the average case is often difficult, if not impossible X  X iven that the notion of an  X  X verage X  planning or decision problem is often ill-defined X  X t is still worth serious consideration. Acknowledgments This material is based upon work supported by the the Air Force Office of Scientific Research under Award No. FA9550-05-1-0254. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of AFOSR. The first author also acknowledges the support of the Andrew W. Mellon Foundation CTW Computer Science Consortium Fellowship. [1] Daniel S. Bernstein, Shlomo Zilberstein, and Neil Immerman. The complexity of decentral-[2] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity [3] Judy Goldsmith and Martin Mundhenk. Competition adds complexity. In J.C. Platt, D. Koller, [4] Harry R. Lewis. Complexity of solvable cases of the decision problem for predicate calculus. [5] Christos H. Papadimitriou. Computational Complexity . Addison-Wesley, Reading, Mas-[6] Raphen Becker, Shlomo Zilberstein, Victor Lesser, and Claudia V. Goldman. Transition-[7] Raphen Becker, Shlomo Zilberstein, Victor Lesser, and Claudia V. Goldman. Solving transition [8] Claudia V. Goldman and Shlomo Zilberstein. Decentralized control of cooperative systems: [9] Martin Allen. Agent Interactions in Decentralized Environments . PhD thesis, University of [10] Raphen Becker, Victor Lesser, and Shlomo Zilberstein. Decentralized Markov decision pro-[11] Keith S. Decker and Victor R. Lesser. Quantitative modeling of complex environments. Inter-[12] V. Lesser, K. Decker, T.Wagner, N. Carver, A. Garvey, B. Horling, D. Neiman, R. Podor-[13] Tom Wagner, Valerie Guralnik, and John Phelps. TAEMS agents: Enabling dynamic dis-[14] AnYuan Guo. Planning and Learning for Weakly-Coupled Distributed Agents . PhD thesis, [15] AnYuan Guo and Victor Lesser. Planning for weakly-coupled partially observable stochastic [16] AnYuan Guo and Lesser Victor. Stochastic planning for weakly-coupled distributed agents. [17] Sven Seuken and Shlomo Zilberstein. Formal models and algorithms for decentralized deci-
