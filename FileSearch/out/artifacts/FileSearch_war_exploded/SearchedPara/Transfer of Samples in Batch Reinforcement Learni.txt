 Alessandro Lazaric lazaric@elet.polimi.it Marcello Restelli restelli@elet.polimi.it Andrea Bonarini bonarini@elet.polimi.it The main objective of transfer in Reinforcement Learning (RL) is to reduce the learning time. In fact, the solution of a set of source tasks can provide useful information about how to solve a related target task, thus reducing the amount of experience needed to solve it. In order to design an effective transfer algorithm, two aspects must be taken into account: what to trans-fer, that is the knowledge retained from the source tasks, and when to transfer, that is the identification of tasks from which transfer is likely to be effective. There exists a much empirical evidence about the ef-fectiveness of techniques such as task decomposition , options , shaping rewards , exploration strategies , in im-proving the learning speed of RL algorithms in single-task problems. Many studies focus on extending such techniques to the transfer scenario. In particular, hier-archical solutions are often used (S  X im  X sek et al., 2005; Mehta et al., 2005) to augment the action space with policies suitable for the solution of a wide range of tasks sharing the same dynamics, but with different goals. In (Konidaris &amp; Barto, 2007), a set of options is learned in an agent space defined by a set of fea-tures shared across the tasks, thus making the options reusable even in tasks with different state spaces. The improvement of learning speed can also be obtained through direct transfer of solutions from source to tar-get task. In this scenario, the main issue is to map the solution learned in a source task to the state-action space of the target task, thus initializing the learning algorithm to a convenient solution. Different aspects of a learning algorithm can be initialized, such as value functions, policies, and approximator structure (Tay-lor et al., 2007, and references therein).
 Although these approaches study how the transfer of different elements from source to target tasks can im-pact on the performance of an RL algorithm, they of-ten rely on the assumption that the tasks are strictly related and they do not address the problem of nega-tive transfer (Rosenstein et al., 2005). In fact, transfer may bias the learning process towards solutions that are completely different from the optimal one, thus worsening the learning performance. Some works fo-cus on the definition of measures of relatedness be-tween tasks that can be used to select from which source tasks transfer is actually convenient. An ex-perimental analysis of measures that estimate the ex-pected speed-up on the basis of information such as policy overlapping, Q -values, and reward structure is reported in (Carroll &amp; Seppi, 2005). Unfortunately, it is often difficult to compute these measures before actually solving the target task, and, thus, they can be used only to analyze the effectiveness of a transfer method. In (Ferns et al., 2004), different metrics for the distance between tasks are proposed and theoreti-cal bounds on the difference between the correspond-ing optimal value functions are derived.
 In this paper, we focus on a perspective that received little attention so far, the transfer of samples. We pro-pose a mechanism that selectively transfers samples from source to target tasks on the basis of the simi-larity of source tasks with the samples collected in the target task. We introduce a criterion to select from which sources transfer should occur, and, within each task, which samples are more likely to speed-up the learning process. As a result, through selective trans-fer of samples, it is possible to reduce the number of samples needed to solve the target task.
 The paper is organized as follows. In Section 2 we introduce notation and we briefly review batch RL. In Section 3 we propose a novel mechanism for trans-fer of samples in batch RL algorithms. In Section 4 we report the experimental results of sample transfer. In Section 5 we relate our work with other transfer-learning approaches. Finally, in Section 6 we draw con-clusions, and we propose directions for future works. In RL, the interaction between the agent and the environment is modeled as a discrete-time Markov Decision Process (MDP). An MDP is a tuple  X  X  , A , P , R ,  X   X  , where S is the state space, A is the ac-tion space, P : S  X  A  X   X ( S ) is the transition model that assigns to each state-action pair a probability dis-tribution over S , R : S  X A  X   X ( R ) is the reward func-tion that assigns to each state-action pair a probability distribution over R ,  X   X  [0 , 1) is the discount factor. At each time step, the agent chooses an action accord-ing to its current policy  X  : S  X   X ( A ), which maps each state to a probability distribution over actions. The goal of an RL agent is to maximize the expected sum of discounted rewards, that is to learn an opti-mal policy  X   X  that leads to the maximization of the value function in each state. The optimal action-value function Q  X  ( s, a ) is defined by the Bellman equations
Q  X  ( s, a ) = where R ( s, a ) = E [ R ( s, a )] is the expected reward. One of the main drawbacks of online RL algorithms (e.g., Q -learning) when applied to real-world problems is the large amount of experience needed to solve a task. In order to overcome this drawback, batch ap-proaches have been proposed. The main idea is to distinguish between the exploration strategy that col-lects samples of the form  X  s, a, s 0 , r  X  ( sampling phase), and the offline learning algorithm that, on the basis of the samples, computes the approximation of the action-value function ( learning phase). In this paper, we focus on fitted algorithms, although the proposed transfer mechanism can be applied to any batch RL algorithm. The idea underlying fitted solutions (Ernst et al., 2005, and references therein) is to reformulate the learning of the value function as a sequence of re-gression problems. Given a set of samples, Fitted Q -Iteration (FQI)(Ernst et al., 2005) estimates the opti-mal action-value function by iteratively extending the optimization horizon. At the first iteration, the algo-rithm defines a regression problem for a 1-step prob-lem, in which the action-value function is equal to the reward function. An approximation is computed run-ning a chosen regression algorithm on the available samples. Thereafter, at each iteration k , correspond-ing to a k -step horizon, a new regression problem is stated, in which the training samples are computed exploiting the approximation of the action-value func-tion at the previous iteration. We formulate the transfer problem as the problem of solving a target task given a set of source tasks drawn according to a given probability distribution defined on a set of tasks which differ in either the transition model or the reward function, or both, but share the same state-action space.
 Definition 1 A task T is an MDP defined by the tu-ple  X  X  , A , P T , R T ,  X   X  , in which the transition model P defines the dynamics , and the reward function R T de-fines the goal .
 Definition 2 An environment E is defined by the tu-ple  X  X  ,  X   X  , where T is the task space and  X  is the task distribution that provides the probability of a task T  X  T to occur.
 In batch RL algorithms, the element that mainly af-fects the learning performance is the set of samples used to feed the learning algorithm, the more infor-mative the samples the better the approximation. We focus on the way this set of samples can be augmented by the inclusion of samples drawn from a set of source tasks. The basic intuition underlying this idea is that, since tasks are related through the task distribution  X , some of the source tasks are likely to contain samples similar to those in the target task. Therefore, we ex-pect the transfer of samples to improve performance of batch RL algorithms even when a very limited number of samples have been actually collected from the tar-get task. This improvement is particularly important in domains where sampling is slow and expensive (e.g., robotic applications).
 More formally, we consider the scenario in which a set of n source tasks { S k } , with S k  X  T and k  X  N n , drawn from  X  are available. From each source task m sam-ples have been collected, while only t m samples are available from the target task T . Let { b S k } and b T be the sample sets for the source and target tasks respec-tively. The transfer algorithm selects a set of samples from the source tasks that are used to augment b T , thus building a new set of samples e T . Finally, samples in e T are used as input for the learning algorithm. 3.1. Task Compliance The main problem of transferring samples across tasks is to avoid negative transfer, that is the transfer of samples from source tasks that are significantly differ-ent from the target task. Therefore, we need to iden-tify which source tasks are more likely to have samples similar to those in the target task. Alternatively, this problem can be stated as a model identification prob-lem . Let us consider the following scenario: The task space T contains n tasks, and m samples have been already collected from each task. Let T be a new task drawn according to  X  and b T the set of samples col-lected from it, with | b T | = t m . Since the transfer of samples from all the tasks in T may worsen the performance in T , we need to identify which of the previously solved tasks is actually T according to the available samples. Starting from a uniform prior over the tasks in T , we compute the posterior distribution as the probability of a task to be the model from which samples in b T are drawn. As the number of samples t increases, the posterior distribution is updated accord-ingly until the total probability mass concentrates on the task equal to T . Then, the m samples previously collected in the task equal to T can be added to b T and used to feed the batch RL algorithm, thus improving its learning performance.
 In the general case in which T is infinite or contains many tasks, the probability to have one source task identical to the target task is negligible. Thus, instead of the probability of a source task to generate all the samples collected in the target task, we compute its compliance with T as the average probability of gen-erating the samples in b T . Then, we transfer samples from source tasks proportionally to their compliance with the target task.
 Let us consider a source task S and the set of target samples b T . Given a state-action pair  X  s, a  X  , the prob-ability of S to be the model from which the target samples in  X  s, a  X  are extracted, that is the likelihood of the model in  X  s, a  X  , can be simply computed by ap-plying the Bayes theorem as 1 prior on the source task S , and P ( S | b T  X  s,a  X  ) is the pos-terior distribution over the source tasks in  X  s, a  X  . Unfortunately, the posterior probability cannot be im-mediately computed without the exact model of S . On the other hand, we have a set of m samples b S previ-ously collected in S , from which an approximation of the continuous model can be computed. In the follow-ing, with an abuse of notation, with b T and b S we denote both the sets of samples and the model approximations b T , the probability of this sample to be generated by S given the set of source samples b S is where P b S and R b S are the approximated transition and reward models respectively. Since in continuous spaces the probability to have samples in the same state-action pair is negligible, it is necessary to use an ap-proximation that generalizes over all the samples close to  X  s i , a i  X  . In particular, we follow the kernel-based approximation proposed in (Jong &amp; Stone, 2007). Let  X  (  X  ) be a kernel function (e.g., a Gaussian ker-nel  X  ( x ) = exp(  X  x 2 / X  ) with bandwidth  X  ) applied to a given distance metric d (e.g., Euclidean or Maha-lanobis distance). First of all, we define the similarity ( compliance in the following) between the experience tuple  X  i and the experience tuples  X  j  X  b S in terms of dynamics and reward. We define the compliance of  X  i with respect to  X  j for the transition model as where While the first term ( w ij ) of  X  P ij is a weight that takes into consideration the relative closeness of the two samples in the state-action space, the second term measures the similarity of the outcome. In particu-lar, under the assumption that the transition model is continuous in the state-action space, it measures the distance between s 0 i and the state obtained by applying the state transition ( s 0 j  X  s j ) of  X  j to state s i (see Jong &amp; Stone, 2007). Therefore, the dynamics of  X  i is highly compliant with that of  X  j when they are close and their state transitions are similar.
 Similarly, the compliance of the reward in  X  i with re-spect to that of  X  j is defined as The approximated transition and reward models are the average of the compliance between  X  i and all the samples in b S where Z P and Z R are normalization terms. Finally, we define the compliance of  X  i to S approximated using samples in b S as  X  i = P (  X  i | b S ) = Recalling Equation 1, given the compliance of samples in  X  s, a  X  , the probability of the model in  X  s, a  X  becomes Starting from the probability in each state-action pair, we compute a global measure of the probability for the task to contain samples similar to target samples. We define the compliance of a task S as the average likeli-hood computed over each state-action pair experienced in the target task.
 Definition 3 Given the target samples b T and the source samples b S , the task compliance of S is where b U contains all the distinct state-action pairs in the samples of b T .
 Since the probability to have two samples in the very same state-action pair is negligible, it follows that | b U | = | b T | = t and the previous definition reduces to where P ( S ) is a prior on the source task. When n source tasks with m samples each are available, and t samples are collected from T , the computation of the task compliance has a time complexity of  X ( nmt ). 3.2. Sample Relevance Although the measure of compliance is effective in identifying which sources, in average, are more con-venient to transfer samples from, it does not provide any suggestion about which samples in b S are actually better to transfer. In the following, we introduce the concept of relevance of each sample  X  j  X  b S . The idea is to use the compliance of  X  j with the target task. Unfortunately, in this case, the measure of compliance is often unreliable because of a poor approximation of the target task. In fact, while each source task contains m samples, only t m samples are available for the target task. As a result, it may happen that the com-pliance of  X  j is computed according to samples  X  i that are significantly far in the state-action space. There-fore, we need a formulation of relevance strictly related to the compliance whenever the number of samples in b T close to  X  j is sufficient, while tending to a default value when the compliance is not reliable. Given the definition of compliance  X  P ji and  X  R ji of  X  j with a sam-ple  X  i , the compliance of  X  j with the approximated model of the target task b T is  X  j = P (  X  j | b T ) = Let the samples  X  i be sorted in ascending order accord-ing to w ji . We compute the average distance between  X  j and the samples  X  i  X  b T as where h j is such that determines the fraction of the total number of samples considered in the computation of the average distance. Definition 4 Given the compliance  X  j and the aver-age distance d j , the relevance of  X  j is defined as where  X  j is the compliance normalized over all the samples in b S .
 The relevance function is shown in Figure 1 for differ-ent values of distance d j . As it can be noticed, sample  X  j may have high relevance in two distinct cases: (i) where there is a number of close samples  X  i which it is compliant with, (ii) where there are no close samples and, independently from the compliance, we assume a high relevance value. The assumption underlying this definition is that, whenever there is no evidence against the transfer of a sample, it is convenient to transfer it. In fact, in transfer problems the learner often needs to infer knowledge about unexplored re-gions of the target task. In these regions, the algo-rithm selects samples from the most compliant source tasks. The assumption is that samples far from target samples, but drawn from highly compliant tasks, are worth transferring, since they provide information in regions that have not been actually experienced. 3.3. Transfer of Samples The actual transfer process is based on the compliance of the source tasks with the target samples and on the relevance of samples within each source task. For sake of simplicity, we bound the number of samples used by the learning algorithm to m . Since | b T | = t samples are already available, m  X  t samples need to be extracted and transferred from the source tasks. For each source task S k , the number of samples transferred to the sam-ple set e T of the new target task is proportional to its normalized compliance  X  k =  X  k P n source task, samples are drawn according to their rele-vance, thus avoiding to transfer samples that are quite dissimilar from those in the target task. The whole sample-transfer process is summarized in Algorithm 1. In order to evaluate the performance of the sample-transfer algorithm we consider a variant of the boat Algorithm 1 The sample transfer algorithm problem proposed in (Lazaric et al., 2007). The prob-lem is to learn a controller to drive a boat from the left bank to the right-bank quay of a river, in pres-ence of a non-linear current. The boat X  X  bow coor-dinates, x and y , are defined in the range [0 , 200] and the controller sets the desired direction a  X  turbed by a uniform noise in the range [  X  5  X  ; 5  X  ]. The control frequency is set to 1Hz. For the lack of space, we refer the reader to (Lazaric et al., 2007) for the equations of the dynamics. In addition, we introduce sandbanks, i.e., regions of the river in which the speed is reduced by 20%. The reward function is defined as: where D is a function that gives a reward decreasing linearly from 10 to -10 relative to the distance from the quay, Z s is the quay zone, Z v is the viability zone around the quay, and Z f is the failure zone in all the other bank points. The dynamics and learning param-eters are summarized in Tab. 1. In the following ex-periments, we use Gaussian kernels and Mahalanobis distance (see Section 3.1). The results are obtained by averaging 100 runs. In FQI, we use extra-randomized trees (Ernst et al., 2005) with 50 trees, 2 random splits, and 2 minimum sample size for each node, trained on 25 iterations. Samples are obtained through random sampling run on independent episodes of maximum 50 steps each. Each episode restarts the boat at the left bank in a random position. Testing is performed on 1,000 episodes with the initial position drawn at ran-dom from 20 evenly spaced positions at the left bank. The first experiment is meant to illustrate the effec-tiveness of the relevance in identifying which samples are worth transferring. We consider a transfer prob-lem with three tasks in which S 1 and S 2 are the source tasks and T is the target task. In T the quay is G 1 and there are two sandbanks as illustrated in Figure 2-( left ). In task S 1 there are two quays G 1 and G 2 , and there is only one sandbank corresponding to the re-gion labeled as sandbank1 in Figure 2-( left ). Task S 2 has the quay G 2 and the sandbanks illustrated in Fig-ure 2-( left ). While T and S 1 have the same current force ( f c = 0 . 5), the current in S 2 is in the opposite direction ( f c =  X  0 . 5). The source task S 2 has a com-pletely different dynamics and reward function from those in T because of different sandbanks and current. Therefore, samples transferred from S 2 are likely to induce negative effects on the learning performance of T . Furthermore, as it can be noticed from the trajec-tories shown in Figure 2-( left ), the optimal policy  X   X  of S 2 obtains very poor performance when tested on T . On the other hand, S 1 has the same dynamics as T in large regions of the state-action space and shares one goal with T . Although its optimal policy  X   X  1 is significantly different from  X   X  , it is possible to choose samples from b S 1 to improve the performance in T . Figure 3-( left ) shows the performance obtained by FQI with four different configurations: No Transfer , Ran-dom , Compliance , and Relevance Transfer . In the first configuration FQI is run with samples directly col-lected from T . The other three configurations are run on the sample set e T obtained by transferring samples chosen at random, according to the compliance, and according to the relevance respectively. Furthermore, we also report the performance obtained by transfer-ring policies  X   X  1 and  X   X  2 as baselines. The augmentation of b
T with samples drawn from S 1 and S 2 at random does not lead to any significant improvement of the performance with respect to learning directly on sam-ples in b T . In fact, the only advantage achieved with the transferred samples is that the agent avoids to go out-side of the boundaries, but she learns neither to avoid sandbanks nor to achieve the goal. The main reason for this poor performance is that samples drawn from S 2 do not provide any information about the actual dynamics and rewards of T and, thus, may lead to learning very bad policies. On the other hand, the compliance-based transfer successfully excludes sam-ples of S 2 from the transfer process (the normalized compliance of S 1 for t = 200 is  X  1 = 0 . 93  X  0 . 09), thus augmenting b T with samples mainly coming from S . Since S 1 shares with T the dynamics and the rewards in all the state space but at sandbank2 and in the quay G 2 , the transfer is positive and leads to a significant improvement in the performance of the learning process. Nonetheless, there are still many trajectories leading to the quay G 2 and crossing the sandbank because of the negative effect of transferring samples from regions with dynamics and reward differ-ent from T . In Figure 3-( right ) we report the relevance of the samples in b S 1 (averaged on all the actions). As it can be noticed, the relevance identifies the regions where samples are actually similar in source and tar-get tasks, excluding samples coming from sandbank2 and the lower quay G 2 . As a result, the performance of the relevance-based transfer is further improved. In order to evaluate the relative improvement of trans-fer, we compute the area ratio (Taylor et al., 2007) of the three transfer configurations, defined as the differ-ence between the accumulated reward with and with-out transfer divided by the reward accumulated with-out transfer. Figure 3-( center ) shows the area ratio for the three transfer configurations. As it can be no-ticed, random transfer does not lead to any significant improvement, while relevance-based transfer improves the performance by 75 . 3%  X  13 . 2. All the differences are statistically significant ( p &lt; 0 . 01). In the previous experiment, source and target tasks have been designed to show how the algorithm works. Now, we consider the general case in which tasks are drawn from an infinite task space T . For sake of sim-plicity, we consider the same target task of the previ-ous experiment, while source tasks have current f c =0.5 and one sandbank. Source tasks are drawn from a distribution  X  such that the coordinates of the cen-ter, height, and width of the sandbank are uniformly drawn from the space [20 . 0; 180 . 0]  X  [20 . 0; 180 . 0]  X  [40 . 0; 100 . 0]  X  [40 . 0; 100 . 0], while the quay position is drawn uniformly in [20 . 0; 180 . 0]. In Figure 4, we report the results of relevance-based transfer obtained by av-eraging the result with 10 different sets of five source tasks. Although the source tasks are different from the target in large regions, the transfer algorithm is able to identify which samples are worth transferring from the source tasks and it successfully improves the learning performance with an area ratio of 59 . 5%  X  15 . 4. Since the algorithms of transfer in RL proposed so far rely on temporal-difference or model-based learning algorithms, an empirical comparison with the perfor-mance of sample transfer would not be fair. Nonethe-less, in the next section, we discuss its similarities and differences with other transfer approaches. In (Sunmola &amp; Wyatt, 2006) a Bayesian approach is used for transfer of MDPs, where source task models are pre-posteriors for the distributions of the parame-ters of the target model and model-based RL is used to compute the solution. Although we similarly adopt a Bayesian argument in the compliance, we directly transfer samples and we use a model-free learning algo-rithm. Furthermore, instead of a parametric approx-imation of the model of the source tasks, we follow a non-parametric solution.
 The task compliance can be interpreted as a sort of distance metric between tasks. In (Ferns et al., 2004), distance metrics for MDP similarity are introduced in the context of bisimulation to aggregate states with similar dynamics and reward. Under a transfer per-spective, these metrics can be used to measure the dif-ference between states in distinct tasks and to bound the performance loss of using the optimal policy of a source task in the target task. Unfortunately, this technique cannot be directly applied to our scenario for different reasons. The computation of the Kantorovich distance between different states is very expensive, be-cause it requires the solution of a complex optimiza-tion problem. Furthermore, the proposed algorithm needs either the exact models of tasks or accurate ap-proximations. On the other hand, we adopt a solution with low computational complexity, linearly depend-ing on the number of samples of the source tasks. Fi-nally, empirical analysis (Phillips, 2006) showed that the theoretical bounds on the performance loss are too loose and they do not provide useful directions about the actual performance of the transferred policy. The transfer of samples is also related to works about transfer of solutions in the RL context (Taylor et al., 2007). Although the transfer of samples or solutions (e.g., policies) from only one source task obtains simi-lar results, there are situations in which sample trans-fer can obtain better results than solution transfer. Even when the difference between source and target tasks is limited to few state-action pairs, the optimal policies of the two tasks can be significantly different and the transfer may achieve very poor performance. On the other hand, the transfer of samples can still be effective. In fact, since most of the samples in the two tasks are identical, the learning algorithm can bene-fit from samples coming from the source task inde-pendently from the actual difference of their optimal policies. Furthermore, the transfer of samples does not require to actually solve the source tasks, and it can be used even when the samples are not enough to solve source tasks. In (Mehta et al., 2005) a solution in which the model-based hierarchical task decomposi-tion allows for transfer at multiple levels of the hierar-chy is proposed. This approach relies on the assump-tion that rewards are a linear combination of basis re-ward functions and it can be applied only to prob-lems of goal transfer, with a fixed transition model. On the other hand, sample transfer can be applied to any transfer scenario. Finally, a method for mapping samples from a source to a target task with different state-action space is proposed in (Taylor et al., 2008). In this paper, we introduced a mechanism for the transfer of samples with the aim of improving the learning performance. The main advantages of the proposed solution are: (i) it is independent from the similarity of the policies and action-value functions of the tasks at hand and, thus, can be applied to a wide range of problems, (ii) it is independent from the batch RL algorithm, (iii) it can be applied to any transfer problem in which either reward or transition or both models change. Experimental results show the effec-tiveness of the method in improving the learning per-formance and in avoiding negative transfer when the source tasks are significantly different from the target. Some aspects of the algorithm can be improved in fu-ture works. In case of tasks that either share exactly the same transition or reward model, it is possible to transfer only the part of the samples common to all the tasks. For instance, if two tasks share the same transition model, but have different goals, it is possi-ble to transfer the  X  s, a, s 0  X  part of the samples and to  X  X omplete X  the sample using an approximation of the reward function of the target task (e.g., using the first iteration of FQI). Furthermore, the sample-transfer al-gorithm could be integrated with the model proposed in (Taylor et al., 2008) in order to deal with problems with tasks defined on different state-action spaces. Carroll, J. L., &amp; Seppi, K. (2005). Task similarity measures for transfer in reinforcement learning task libraries. Proceddings of IJCNN (pp. 803 X 808). S  X im  X sek, O., Wolfe, A. P., &amp; Barto, A. G. (2005). Iden-tifying useful subgoals in reinforcement learning by local graph partitioning. Proceedings of ICML (pp. 816 X 823).
 Ernst, D., Geurts, P., &amp; Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. Journal of Machine Learning Research , 6 , 503 X 556.
 Ferns, N., Panangaden, P., &amp; Precup, D. (2004). Met-rics for finite markov decision processes. Proceedings of UAI (pp. 162 X 169).
 Jong, N. K., &amp; Stone, P. (2007). Model-based function approximation for reinforcement learning. Proceed-ings of AAMAS (pp. 1 X 8).
 Konidaris, G., &amp; Barto, A. G. (2007). Building portable options: Skill transfer in reinforcement learning. Proceedings of IJCAI (pp. 895 X 900). Lazaric, A., Restelli, M., &amp; Bonarini, A. (2007). Re-inforcement learning in continuous action spaces through sequential monte carlo methods. Advances in Neural Information Processing Systems .
 Mehta, N., Natarajan, S., Tadepalli, P., &amp; Fern, A. (2005). Transfer in variable-reward hierarchical re-inforcement learning. NIPS Workshop on Inductive Transfer .
 Phillips, C. (2006). Knowledge transfer in markov decision processes (Technical Re-port). McGill School of Computer Science. (http://www.cs.mcgill.ca/  X martin/usrs/phillips.pdf). Rosenstein, M. T., Marx, Z., Kaelbling, L. P., &amp; Diet-terich, T. G. (2005). To transfer or not to transfer. NIPS Workshop on Inductive Transfer .
 Sunmola, F. T., &amp; Wyatt, J. L. (2006). Model transfer for markov decision tasks via parameter matching.
Workshop of the UK Planning and Scheduling Spe-cial Interest Group .
 Taylor, M. E., Jong, N. K., &amp; Stone, P. (2008).
Transferring instances for model-based reinforce-ment learning. AAMAS 2008 Workshop on Adaptive Learning Agents and Multi-Agent Systems .
 Taylor, M. E., Stone, P., &amp; Liu, Y. (2007). Transfer learning via inter-task mappings for temporal dif-ference learning. Journal of Machine Learning Re-
