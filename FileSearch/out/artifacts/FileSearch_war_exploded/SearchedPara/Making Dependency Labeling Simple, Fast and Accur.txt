 Traditionally in dependency parsing, the tasks of finding the tree structure and labeling the de-pendency arcs are coupled in a joint achitecture. While it has potential to eliminate errors propogated through a separated procedure, joint decoding intro-duces other sources of issues that can also lead to non-optimal labeling assignments. One of the issues arises from inexact algorithms adopted in order to solve the hard joint search problem. For instance, many parsers (Nivre et al., 2007; Titov and Hender-son, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct la-beling hypothesis in an early decoding stage. An-other issue is caused by the absence of rich label features. Adding dependency labels to the combina-torial space significantly slows down the search pro-cedure. As a trade-off, many parsers such as MST-Parser, TurboParser and RBGParser (McDonald et al., 2005; Martins et al., 2010; Zhang et al., 2014) incorporate only single-arc label features to reduce the processing time. This restriction greatly limits the labeling accuracy.

In this work, we explore an alternative approach where the dependency labeling is applied as a sep-arate procedure, alleviating the issues described above. The potential of this approach has been ex-plored in early work. For instance, McDonald et al. (2006) applied a separate labeling step on top of the first-order MSTParser. The benefit of such approach is two-fold. First, finding the optimal labeling as-signment (once the tree structure is produced) can be solved via an exact dynamic programming algo-rithm. Second, it becomes relatively cheap to add rich label features given a fixed tree, and the ex-act algorithm still applies when high-order label fea-tures are included. However, due to performance is-sues, such approach has not been adopted by the top performing parsers. In this work, we show that the labeling procedure, when optimized with recent ad-vanced techniques in parsing, can achieve very high speed and accuracy.

Specifically, our approach employs the recent distributional representation learning technique for parsing. We apply and extend the low-rank ten-sor factorization method (Lei et al., 2014) to the second-order case to learn a joint scoring function over grand-head, head, modifier and their labels. Unlike the prior work which additionally requires traditional sparse features to achieve state-of-the-art performance, our extention alone delivers the same level of accuracy, while being substantially faster. As a consequence, the labeling model can be applied either as a refinement (re-labeling) step on top of ex-isting parsers with negligible cost of computation, or as a part of a decoupled procedure to simplify and speed up the dependency parsing decoding.

We evaluate on all datasets in the CoNLL-2009 shared task as well as the English Penn Treebank dataset, applying our labeling model on top of state-of-the-art dependency parsers. Our labeling model processes over 1,700 English sentences per sec-ond, which is 30 times faster than the sparse-feature method. As a refinement (re-labeling) model, it achieves the best LAS on 5 out of 7 datasets. 2.1 Task Formulation Given an unlabeled dependency parsing tree y of sentence x , where y can be obtained using exist-ing (non-labeling) parsers, we classify each head-modifier dependency arc h  X  m  X  y with a partic-ular label l h  X  m . Let l = is to find the assignment with the highest score: For simplicity, we omit x , y in the following dis-cussion, which remain the same during the labeling process. We assume that the score S( l ) decomposes into a sum of local scores of single arcs or pairs of arcs (in the form of grand-head X  X ead X  X odifier), i.e.
Parameterizing the scoring function s 1 ( h q  X  m ) and s 2 ( g p  X  h q  X  m ) is a key challenge. We follow Lei et al. (2014) to learn dense representations of features, which have been shown to better generalize the scoring function. 2.2 Scoring The representation-based approach requires little feature engineering. Concretely, let  X  g , X  h , X  m  X  head, head and modifier word respectively, and  X  of the two dependency arcs respectively. It is easy to define and compute these vectors. For instance,  X  g (as well as  X  h and  X  m ) can incorperate binary features which indicate the word and POS tag of the current token (and its local context), while  X  g  X  h,p (and  X  h  X  m,q ) can indicate the label, direction and length of the arc between the two words.

The scores of the arcs are computed by (1) projecting the atomic vectors into low-dimensional spaces; and (2) summing up the element-wise prod-ucts of the resulting dense vectors: where r 1 is a hyper-parameter denoting the dimen-sion after projection, and U 1 ,V 1  X  R r 1  X  n , W 1  X 
The above formulation can be shown equivalent to factorizing a huge score table T 1 (  X  ,  X  ,  X  ) into the product of three matrices U 1 ,V 1 and W 1 , where T 1 is a 3-way array (tensor) storing feature weights of all possible features involving three components X  the head, modifier and the arc between the two. Ac-cordingly, the formula to calculate s 1 (  X  ) is equivalent to summing up all feature weights (from T 1 ) over the structure h q  X  m . 2
We depart from the prior work in the following aspects. First, we naturally extend the factorization approach to score second-order structures of grand-head, head and modifier,
Here r 2 is a hyper-parameter denoting the dimen-additional parameter matrices to be learned. Sec-ond, in order to achieve state-of-the-art parsing ac-curacy, prior work combines the single-arc score s ( h q  X  m ) with an extensive set of sparse features which go beyond single-arc structures. However, we find this combination is a huge impediment to de-coding speed. Since our extention already captures high-order structures, it readily delivers state-of-the-art accuracy without the combination. This change results in a speed-up of an order of magnitude (see section 2.4 for a further discussion). 2.3 Viterbi Labeling We use a dynamic programming algorithm to find the labeling assignment with the highest score. Sup-pose h is any node apart from the root, and g is h  X  X  parent. Let f( h,p ) denote the highest score of sub-tree h with l g  X  h fixed to be p . Then we can compute f(  X  ,  X  ) using a bottom-up method, from leaves to the root, by transition function
And the highest score of the whole tree is
Once we get f(  X  ,  X  ) , we can determine the labels backward, in a top-down manner. The time com-plexity of our algorithm is O( NL 2  X  T ) , where N is the number of words in a sentence, L is the num-ber of total labels, and T is the time of computing features and scores. 2.4 Speed-up In this section, we discuss two simple but effective strategies to speed up the labeling procedure. Pruning We prune unlikely labels by simply ex-ploiting the part-of-speech (POS) tags of the head and the modifier. Specifically, let 1 ( pos h , pos m ,l ) denote whether there is an arc h l  X  m in the train-ing data such that h  X  X  POS tag is pos h and m  X  X  POS tag is pos m . In the labeling process, we only con-sider the possible labels that occur with the corre-sponding POS tags. Let K be the average number of possible labels per arc, then the time complexity is dropped to O( NK 2  X  T ) approximately. In prac-tice, K  X  L/ 4 . Hence this pruning step makes our labeler 16 times faster.
 Using Representation-based Scoring Only The time to compute scores, i.e. T , consists of building the features and fetching the corresponding feature weights. For traditional methods, this requires enu-merating feature templates, constructing feature ID and searching the feature weight in a look-up table. For representation-based scoring, the dense word representations (e.g. U 1  X  h ) can be pre-computed, and the scores are obtained by simple inner products of small vectors. We choose to use representation-based scoring only, therefore reducing the time to O( NK 2  X  ( r 1 + r 2 ) + NT 0 ) . In practice, we find the labeling process becomes about 30 times faster. 2.5 Learning Let D = { ( x i , y i , l i ) } M i =1 be the collection of M training samples. Our goal is to learn the values of the set of parameters  X  = { U 1 ,V 1 ,W 1 ,U 2 ,V 2 , W 2 ,X 2 ,Y 2 } based on D . Following standard prac-tice, we optimize the parameter values in an online maximum soft-margin framework, minimizing the structural hinge loss: where k l i  X   X  l k 1 is the number of different labels be-tween l i and  X  l . We adjust parameters  X  by  X  X  via passive-aggressive update: is a regularization hyper-parameter controlling the maximum step size of each update.

To counteract over-fitting, we follow the common practice of averaging parameters over all iterations. Experimental Setup We test our model on the CoNLL-2009 shared task benchmark with 7 differ-ent languages as well as the English Penn Treebank dataset. Whenever available, we use the predicted POS tags, word lemmas and morphological informa-tion provided in the datasets as atomic features. Fol-lowing standard practice, we use unlabeled attach-ment scores (UAS) and labeled attachment scores with previous reported numbers, we exclude punctu-ations for PTB in the evaluation, and include punc-tuations for CoNLL-2009 for consistency.
 based parser for predicting dependency trees, and then apply our labeling model to obtain the depen-dency label assignments. To demonstrate the effec-tiveness of our model on other systems, we also ap-ply it on two additional parsers  X  Stanford Neural experiments, we use the default suggested settings to run these parsers. The hyper-parameters of our labeling model are set as follows: r 1 = 50 , r 2 = 30 , C = 0 . 01 .
 Labeling Performance To test the performance of our labeling method, we first train our model us-ing the gold unlabeled dependency trees and eval-uate the labeling accuray on CoNLL-2009. Table 3 presents the results. For comparison, we implement a combined system which adds a rich set of tradi-tional, sparse features into the scoring function and jointly train the feature weights. As shown in the ta-ble, using our representation-based method alone is super fast, being 30 times faster than the implemen-tation with traditional feature computation and able to process over 1,700 English sentences per second. It does not affect the LAS accuracy except for Chi-nese.
 PTB Results Table 4 shows the performance on the English PTB dataset. We use RBGParser to pre-dict both labeled and unlabeled trees, and there is no significant difference between their UAS. This finding lays the foundation for a separate procedure, as the tree structure does not vary much comparing to the joint procedure, and we can exploit rich la-bel features and sophisticated algorithms to improve the LAS. Our re-labeling model improves over the predictions generated by the three different parsers, ranging from 0.2% to 0.4% LAS gain. Moreover, the labeling procedure runs in only 1.5 seconds on the test set. If we use the existing parsers to only predict unlabeled trees, we also obtain speed improvement, even for the highly speed-optimzed Stanford Neural Parser.
 CoNLL-2009 Results In Table 2, we compare our shared task, Bohnet (2010), Zhang and McDonald (2014) as well as the most recent neural network parser (Alberti et al., 2015). Despite the simplic-ity of the decoupled parsing procedure, our labeling model achieves LAS performance on par with the state-of-the-art neural network parser. Specifically, our model obtains the best LAS on 5 out of 7 lan-guages, while the neural parser outperforms ours on Catalan and Chinese. The most common method for dependency parsing couples the structure search and label search. We demonstrate that decoupling these two steps yields both computational gains and improvement in label-ing accuracy. Specifically, we demonstrate that our labeling model can be used as a post-processing step to improve the accuracy of state-of-the-art parsers. Moreover, by employing dense feature representa-tions and a simple pruning strategy, we can signif-icantly speed up the labeling procedure and reduce the total decoding time of dependency parsing. We thank Yuan Zhang for his help on the ex-periments, and Danqi Chen for answering ques-tions about their parser. We also thank the MIT NLP group and the reviewers for their com-ments. This work was supported in part by the National Basic Research Program of China Grant 2011CBA00300, 2011CBA00301, the Na-tional Natural Science Foundation of China Grant 61361136003.

