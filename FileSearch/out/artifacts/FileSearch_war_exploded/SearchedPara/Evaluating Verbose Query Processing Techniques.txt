 Verbose or long queries are a small but significant part of the query stream in web search, and are common in other ap-plications such as collaborative question answering (CQA) . Current search engines perform well with keyword queries but are not, in general, effective for verbose queries. In this paper, we examine query processing techniques which can be applied to verbose queries prior to submission to a search engine in order to improve the search engine X  X  result s. We focus on verbose queries that have sentence-like struc-ture, but are not simple  X  X h- X  questions, and assume the search engine is a  X  X lack box. X  We evaluated the output of two search engines using queries from a CQA service and our results show that, among a broad range of techniques, the most effective approach is to simply reduce the length of the query. This can be achieved effectively by removing  X  X top structure X  instead of only stop words. We show that the process of learning and removing stop structure from a query can be effectively automated.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithms, Experimentation, Perfomance Verbose Queries, Query Reformulation, Black Box
It has been observed that most queries submitted to search engines are short. For example, the average length of the queries in the MSN search log, a sample of about 15 million queries collected over one month, is 2.4 words [3]. A surpris -ing number of queries, however, are longer. For example, in the MSN log, about 10% of the queries are five words or longer (not including extremely long run-on queries, such a s automatically generated queries). The reason this is surpr is-ing is that current search engines do not perform particular ly well with long queries. Long queries are important in other search applications, such as Collaborative Question Answe r-ing (CQA) services and certain vertical search domains, suc h as patent searches.

Long queries can potentially express more complex infor-mation needs and they can provide much more context for ranking algorithms than the one-word queries used as exam-ples in many papers (e.g.,  X  X ava X  and  X  X aguar X ). Given their potential, there has been some recent research on technique s for improving search effectiveness with long queries. Bende r-sky and Croft [2], for example, show that key concepts can be extracted from long queries and used to improve rank-ing. Lease et al [14] described a technique for improving ranking by weighting terms in long queries. Kumaran and Carvalho [12] use learning techniques to rank subsets of the original long query based on quality prediction measures. These and some other similar studies have used TREC col-lections and open-source search engines to study the effec-tiveness of query processing techniques for long queries.
In this paper, we focus on identifying query processing techniques that can be used to improve the effectiveness of long queries where the search engine is a  X  X lack box X . As such, we have limited ourselves to techniques that generate queries that a standard search engine application program-ming interface (API; in our case, the Yahoo! API and the Bing API) is capable of processing. This means that some techniques, such as term weighting, cannot be used because they are not supported by the API. There is some risk in this decision as it is possible that the search engine will attemp t to learn from the submitted queries or that our results will reflect the query processing done by the search engine. To reduce these risks, we used two different search engines for our experiments. We also attempted to limit the learning capability of each search engine by ensuring that the links returned were never  X  X licked. X  In order to reduce the chance that the search engine is modified during our search experi-ments, we ensure that all queries produced by applying these pre-processing techniques are submitted to the search APIs within a very short time span. We note that  X  X lack box X  experiments are quite common in the literature and are be-coming more important in the search industry. One of our goals is to make the assumptions and limitations underlying these experiments more explicit.

In addition to limiting our query processing techniques to account for the black box search environment, we also restrict our study to long queries that form a clause or sen-tence containing a verb. Bendersky and Croft [3] described a classification of long queries and found that queries contai n-ing verbs made up about 15% of the long queries in the MSN query log. Our interest in these type of queries stems from the fact that their structure allows for more linguistic pro -cessing and that they are representative of the queries that people use in less constrained search environments, such as a CQA service. Verbose queries are long queries in which people use many words to say what could have been ex-pressed in a few keywords. An example of a verbose query is  X  X ould the meteor shower that hits this weekend be bet-ter to watch tonight or tomorrow night? X , taken from Yahoo! to retrieve relevant documents, but the query as a whole con-veys considerably more about the information needed or the user X  X  intent than a query such as  X  X eteor shower X . Verbose queries may also include simple  X  X h- X  (who, what, when, where) or  X  X actoid X  queries. However, because this subset o f verbose queries has been studied extensively in the TREC QA (Question Answering) track and have had specific tech-niques developed for them, we have omitted them from this work.

The query transformation techniques we consider include stopword removal, phrase detection, key concept identifica -tion, and stop structure removal. Each of these techniques is applied individually and in combination to the verbose queries in our test sets. We measure the performance of the original queries and of the queries produced by these tech-niques and compare the results to evaluate the techniques X  effectiveness. The most effective single technique found in this first round of experiments was stop structure removal.
A stop phrase has been defined as a phrase that does not provide any information on the user X  X  information need [6]. We define a stop structure as a stop phrase that begins at the first word in a query. Though it may be possible to effectively remove many stop structures from search engine queries using static lists, similar to those used for stopwo rds, doing so may inadvertently remove relevant words from the query. To address this problem we describe a sequential clas -sifier that enables us to automatically identify the stop str uc-ture in each query. We show that the queries produced by this classifier perform similarly to those produced by manu-ally identified stop structure removal.

In the next section we discuss related work. We describe how the test sets of queries were created in section 3. In section 4, we describe the query processing techniques used . We describe the experimental setup and the results from the retrieval experiments in section 5. The automatic stop structure classifier is described and analyzed in section 6. In section 7 we conclude and discuss possible future work.
There has been a large amount of recent work on query transformation techniques. Much of this work focuses on the accurate, efficient extraction of linguistic features fr om queries. Bergsma and Wang [4] present a method of learn-ing noun phrase segmentations. Tan and Peng [16] present a generative model based method of query segmentation. Guo et al [8] present a Weakly Supervised Latent Dirichlet Allocation (WS-LDA) based method of identifying named entities in queries. Each of these studies evaluate their pe r-formance over samples of commercial query logs. It should be noted that these studies do not show how the extracted noun phrases or query segments might be used in a retrieval system, either internally or externally as a pre-processin g stage.

There is some recent work on query processing that eval-uates results using a  X  X lack box X  approach. For example, Gu et al [9] describe a CRF-based framework for applying several types of transformations to a query. These trans-formations are tested on queries sampled from a commercial search engine log. Both the original and transformed querie s are submitted to a commercial search engine, and the top retrieved documents are assessed for relevance. Although there are some similarities to our study, we focus on verbose queries and processing techniques that are appropriate for longer queries, including automatic stop structure remova l.
In order to carry out experiments with query processing techniques, we needed a test collection of verbose queries. Rather than use the TREC description queries that have been studied in other recent papers (e.g., [2],[14]), we de-cided to use queries from the Yahoo! Answers CQA service. We believed these queries would be less artificial than TREC queries and less constrained than queries from a web search query log, where users tend to think in terms of keywords because that is what works well with current search engines. We would expect, therefore, that many of the CQA queries will not work well without query processing.

In order to construct a sample of verbose queries, we ex-tracted questions from a crawl of Yahoo! Answers and fil-tered the results to identify queries which match the desire d type. As mentioned above, the desired queries consist of clauses or sentences containing verbs, but are not standard  X  X actoid X  or QA questions. In Bendersky and Croft X  X  study [3], these were called non-composite-verb queries, where the  X  X on-composite X  X ndicates that the query is not just a simpl e combination of shorter web-style queries.

Our purpose in extracting queries is to construct two non-overlapping sample sets of 50 queries, such that one set is composed of more  X  X ifficult X  queries, and the other set is composed of a representative sample of this type of query. In order to construct the X  X ifficult X  X uery set, candidate queri es were submitted to a web search engine (Yahoo! Search). If the search yielded a relevant document in the top position, then the query was rejected, otherwise we added the query to our test set.
 In this paper we refer to the set of  X  X ifficult X  queries as Set 1 . The representative set of verbose queries is referred to as Set 2 .

Set 1 is used to study the query processing techniques that could have the most impact when current search en-gines fail to perform well. Set 2 is used to test that the same query processing techniques are beneficial over a more representative sample query set.

By inspecting the query log we found that capitalization and punctuation appeared to be optional for this type of query. Accordingly, we removed all punctuation from the queries in order to ensure that the performance of our tech-niques would not be dependent on correct grammar or cap-tialization. We view spelling correction as a necessary ste p for any verbose query processing system. Therefore we mod-ified the queries so that all words were correctly spelled.
Removing stopwords has long been a standard query pro-cessing step [7]. We used three different stopwords lists in this study: the standard INQUERY stopword list [1], as well as two query-stopword lists derived from the Yahoo! An-swers query log. We chose to construct new stopword lists because the language that is used in some of the test queries is more informal than the sample of English text from which the INQUERY stopword list is derived.

Lo, He and Ounis [17] present several methods of auto-matically constructing a collection dependent stopword li st. Their methods generally involve ranking collection terms by some weight, then choosing some rank threshold above which terms are considered stopwords. Their weighting meth -ods all produce similar retrieval performances, but an inve rse document frequency (IDF) based weighting scheme is the most effective.

In accordance with their findings, we applied an IDF based weighting scheme to the set of verbose queries in the Yahoo! Answers query log. As a result we were able to construct two stopword lists which might be more appropriate for these queries. The two stopword lists were extracted by taking the top 100 and 200 ranked words from this ordering. It should be noted that words are rarely repeated in a single query. Thus, in this particular case, sorting by term frequency is almost rank equivalent to sorting by inverse document fre-quency.

This process identified words such as  X  X elp X ,  X  X ind X , and  X  X now X  which do not occur in the INQUERY stopword list, but are not necessarily useful search words.

The key problem in trying to construct a query-stopword list is that common query phrases such as  X  X igh blood pres-sure X  occur as frequently as some stop phrases. These fre-quent phrases mean that words such as  X  X lood X  and  X  X res-sure X  occur in the query log as frequently as the words  X  X id X  or  X  X m X .

We chose to apply these stopword lists by removing all words in the query which occur on the stopword list. For example, if using the INQUERY stopword list, and given the query: this technique produces the query:
It is worth noting that this technique can significantly change the meaning of the query. For example when apply-ing the INQUERY stopword list to the query: the resulting query is:
By definition, a verbose query is composed by the user as a sentence or phrase. This allows us to apply a variety of linguistic tagging and chunking techniques to the query. Additionally we know from previous work that extracting noun phrases from the query can help identify the key con-cepts within the query [2, 18, 10]. We used the MontyLingua toolkit [15] to automatically identify noun phrases.
Given that we are using a search engine as a black box, we are limited in how we can use the identified noun phrases contained within the query. We are unable to assign weights to terms or phrases according to confidence or importance. Using the black box interface, we are only able to identify important phrases using quotation marks.

There are several methods by which the identified noun phrases can be communicated to the search engine within this limited interface. We show results for two such methods . The first method wraps each of the detected noun phrases in the query in quotation marks; no words are removed from the query. The second method removes all words which are not part of some detected noun phrase and quotation marks are not used.

For example, 3 noun phrases:  X  X e X  ,  X  X  name X  and  X  X y next horror script X  , are detected in the query: Using the first method we would produce the query: Using the second method we would produce the query:
The term key concept in this context is a short set of sequential words that express an important idea contained within the query. As presented by Bendersky and Croft [2], the identification of key concepts in long queries can pro-duce performance improvements in retrieval engines. Their research demonstrated that key concepts in TREC queries can be automatically identified with around 80% accuracy.
For this study, we use the same type of classifier as Bender-sky and Croft. The classifier is based upon the AdaBoost.M1 meta-classifier using C4.5 Decision Trees. The features use d in this classifier included: GOV2 collection term frequency , inverse document frequency, residual inverse document fre -quency, weighted information gain, google n-grams term fre -quency, and query frequency. These features are detailed in their paper, [2]. The GOV2 query set was used to train this classifier.

Key concepts are structurally similar to noun phrases and thus present similar problems for utilizing them. In order t o effectively communicate key concepts to the search engine, therefore, we again employ the same two methods. The first method wraps the detected key concepts in quotation marks. The second method removes all words that are not part of a given key concept.

Given the query: i read that ions cant have net dipole moments why not The key concepts  X  X  X ,  X  X ons X  and  X  X et dipole moments X  were identified by the classifier. The first method outlined above would produce the query:  X  X  X  read that  X  X ons X  cant have  X  X et dipole moments X  why not and the second method would produce:
Removal of stop phrases was originally presented by Callan and Croft as a query processing technique [6]. They define a stop phrase as a phrase which provides no information about the information need. One example they give is  X  X ind a document X . Within their study they created a static list of stop phrases by inspecting 50 TIPSTER queries. This list was reused in later studies using TIPSTER [5].

The majority of our test queries contained one relatively large stop phrase at the start of the query. We call these stop phrases stop structure . That is, a stop structure is a stop phrase which begins at the first word in the query. For this study other stop phrases in the query are considered semantically meaningful; we did not remove them.

Importantly, stop structure often contains words which are not stopwords, but which nonetheless do not add any meaning to the query. For example  X  X y husband would like to know more about cancer X  This information need could be equivalently served by the query  X  X ancer X . As in the case of stopwords, stop structure removal carries some inherent risk. In the above query, one could say that there is implied information contained within the term  X  X usband X . That is, the underlying information need may correspond to specific types of cancers that are common among males.

However, within the framework of this study we have ac-cess to additional information about the user X  X  informatio n need. We used the user-selected answer corresponding to their query to guide the manual identification of stop struc-ture. In this way, we eliminated the risk of losing meaningfu l terms through the removal of stop structure. Later, we will show that automatic, non-answer guided stop structure re-moval can closely approximate the retrieval performance of answer-guided manual stop structure removal.

Similar to stopwords, stop structure is removed from querie s prior to submission to a search engine. For example given the query: The stop structure  X  X f i am having a X  is removed, leaving: Note that this query has another stop phrase,  X  X an i X , which is not removed.
The aim of these experiments is to see which of the above techniques, if any, has the greatest effect on retrieval per-formance. As mentioned above we choose to use two com-mercial search engines as black box search engines, as this matches the limitations of a meta-search engine designer. For this study we chose to use Yahoo! Search, and Bing as  X  X lack boxes X . Yahoo! BOSS API and Bing API 2.0 were used to submit queries and retrieve documents. 10 results were collected for each query from each search engine.
There are two problems which may arise through the use of commercial search engines for a scientific study. First th e search engine may learn how to respond to our particular set of test queries. Second the search engine may be modified at any time. In order to limit the amount of learning that each search engine would be able to do over our set of test queries, we ensures that no returned links were clicked for any of the queries.

Since the search engines we are using are commercial, we won X  X  necessarily know if the underlying system changes at any time. In order to minimize this risk we chose to submit all queries generated by all of the above techniques to both APIs within a single short search session. This search sessi on occurred on September 29th, 2009.

After all queries were submitted, all returned pages were manually judged for relevance. These judgements were guide d by the answers provided for the queries in the CQA system. A three-valued judgement system was used. The three val-ues were  X  X ot relevant X ,  X  X artially relevant X , and  X  X eleva nt X , denoted by the numerical values { 0,1,2 } respectively.
It should be noted that both search engines occasionally returned the page in Yahoo! Answers from which the orig-inal query was gathered. These pages were removed from the search results, and the remaining pages re-ranked. This adjustment was made in order to avoid skewed results in favor of the original queries.

We show the evaluation metrics normalized discounted cu-mulative gain (nDCG) at ranks 5 and 10 for each processing technique. The normalization constant was computed from the sum of all annotations for each query. Thus the values are comparable across query processing techniques, the two search engines, and the subsequent retrieval experiments d e-scribed below.
Table 1 shows the retrieval results for all of the query processing techniques when applied to query set 1 using the Yahoo! and Bing search engines. The results from the two search engines are very similar in terms of relative effectiv e-ness, confirming that inconsistencies are unlikely to have been caused by internal search engine processing. The use of quotations in formulating queries is clearly not effectiv e. Both noun phrase and key concept identification produce significant improvements, when combined with stopword re-moval. The most effective technique, however, is the re-moval of stop structure. Manual removal of these words resulted in consistent and very significant improvements fo r both NDCG measures and in both search engines.

Manual removal of stop structure was guided by the user X  X  selected answer, so we view this as an oracle result under the best possible circumstances.

Interestingly, the removal of any remaining stopwords from the queries after removing stop structure, degrades the re-trieval performance of the queries Within almost all of the queries produced through the removal of manual stop struc-ture semantically meaningless terms are present. The per-formance degradation stems from the removal of semanti-cally meaningful terms. For example given the query: Removal of manual stop structure produces the query: Clearly  X  X rom X  and X  X y X  X ould be removed without changing the meaning of the query. However, removing the INQUERY
Yahoo! API Bing API  X  0.0735  X  0.0947  X  0.0763  X   X  0.1071  X  0.1384 0.1262  X   X  0.1110  X  0.1261  X  0.1201  X 
Yahoo! API Bing API inal), results which show significant improvements, (p-val ue stopwords produces the query: Both search engines returned websites which advertised ex-ercise regimes for this query. Similarly the performance of the stopword based techniques was hindered by this type of inadvertent removal of semantically meaningful terms.
Recall that query test set 1 was sampled such that the queries are more difficult for commercial search engines. We repeated the experiments with stopword and stop structure removal using the second set of queries. These results are shown in Table 2. This second set of experiments are de-signed to test the performance of the removal of stop struc-ture over a more representative sample of verbose queries.
These results show that stop structure removal is indeed very effective. Given these, very significant effectiveness i m-provements from manual stop structure removal, in the re-mainder of this paper, we focus on describing and evaluat-ing an automatic method for classifying and removing query stop structure.
We have defined stop structure to be a stop phrase be-ginning at the first word in the query. It it important to note that a stop structure can have many variations with-out significantly changing its meaning. Indeed, any phrase that does not provide some insight into the underlying in-formation need may be considered stop structure for some particular query. Additionally, it is possible that the sto p structure from one query may not necessarily be stop struc-ture in another query. For example X  X lease tell me why X  X ay be identified as stop structure in the query: But it would not be considered stop structure in the query: So we can see that using a single static stop structure list is not an appropriate approach to stop structure removal.
Therefore, we have developed a method of automatically classifying stop structure within some input query. The pur -pose of our classifier is to identify each of the words in each of the queries as a  X  X top structure term X  or a  X  X uery term X . It is natural to formulate this type of problem as a sequentia l binary class tagging problem.
In order to obtain the best possible classifier a range of fea-tures were extracted for term in each query. Broadly these features fall into two categories; multinomial and numeric al. The features extracted for each term in a query were then used as input to the classifiers which are discussed in the next section.

First I will outline the multinomial features generated for each term in the input query. Using the MontyLingua toolkit [15], we extract the part of speech tag for each term (NN, VB, etc) The next feature generated was the position of the term (1,2,3,4,5,...) in the query. Two binary features were also extracted: whether or not the term is present in the INQUERY stopword list, and whether or not there is any non-stopwords in the current query which precede the current word.

The numerical features were extracted from three TREC collections, and from three query logs. We used the WT10G, GOV, and GOV2 TREC collections. The query logs we used are: the Yahoo! Answers CQA log, Wondir CQA log, and the MSN query log. We found that each of these collec-tions and query logs conveyed some unique information to the classifier. We believe that this is due to the vocabu-lary mismatch between the test queries and any one of the collections or query logs.

All three TREC collections are composed of web docu-ments, the GOV and GOV2 collections were limited to the  X .gov X  domain, while the WT10G was not limited to any par-ticular domain. Further details can be found on the TREC
The Yahoo! Answers CQA log consists of 216,563 user submitted question and answer sets. The log was constructed Wondir CQA log consists of 401,560 user submitted ques-tion and answer sets from the Wondir community question answer service. The MSN query log consists of 15 million queries submitted to the MSN search engine during a period of one month.

We define subsets of each of the query logs were created based on the length of the queries; the first subset con-tains very short queries (  X  2 words), the second contains short queries (  X  4 words) and the final subset contains long queries ( &gt; 4 words). Our hope is that features extracted from these subsets will enable a classifier to distinguish be -tween commonly searched phrases and common stop struc-tures. A common search phrase should have a higher fre-quency amongst the short or very short queries, while a com-mon stop structure should have a lower frequency in these subsets. Within the long subsets, common query phrases and stop structures may have similar, relatively high fre-quencies.

Three types of numeric features were extracted for each term in a query; term frequency, bi-term frequency, and in-verse document frequency (IDF). Term frequency refers to the number of times the term occurs in the collection; f ( t Bi-term frequency referse to the number of times the term and it X  X  predecessor occurs in the corpus; f ( t i | t i  X  1 refers to the inverse document frequency; idf ( t i ) = log Where the document frequency, d f , is the number of doc-uments which contains the term, and | D | is the number of documents in the collection.
 We extracted all three numeric features from each of the TREC collections. Term frequencies and bi-term frequencie s were extracted from each of the three query logs and each of the nine query log subsets.

We also experimented with features based on phrase chunk tags and n-word frequencies. However these features were found to be detrimental to the classifier X  X  performance. Thi s may have stemmed from the inaccuracy of the phrase chunks, and corpus sparsity problems for the frequency of longer phrases.
As has been mentioned above, the purpose of this classifier is to mark each of the words in the queries as a  X  X top struc-ture term X  X r a X  X uery term X . We experimented with two im-plementations of sequential taggers; CRF++ and YamCha. CRF++ is an open source implementation of Conditional Random Fields for sequential tagging, based on work done be Lafferty, McCallum and Pereira [13]. YamCha (Yet An-other Multipurpose CHunk Annotator) is a sequential tagger based on support vector machines [11].

The training set used for each of these classifiers was com-posed of 100 new verbose queries identified from the Yahoo! Answers query log. These newly identified queries have sim-ilar properties to the two sets of queries already identified . The stop structure in these queries was manually identified in a similar manner to the test sets of queries. The test sets for this classifier are the two sets of queries which we have already identified for this study.

The CRF++ classifier is constructed such that all fea-tures must be multinomial. This means that numerical fea-tures are unsuitable for this classifier. In order to convert the frequency-based features detailed above into multino-mial features, we applied binning after taking the log of each numerical value. After taking the log of the frequency values the numerical features ranged from 0 and 20. The most effective binning method we were able to find seg-mented the number plane at multiples of 5, with an extra bin for values 0 &lt; x &lt; 1. This method produced 6 bins; (0,  X  1,  X  5,  X  10,  X  15,  X  15).

The best performance from the YamCha classifier that we were able to obtain used a context window of 3 terms; the previous term, the current term and the next term. The pre-viously assigned tag was also used as a feature. Using larger window sizes degraded performance. Similarly the best per-formance for the CRF++ that we were able to obtain used a context window of 3 terms.

We used two types of evaluation metrics to measure the performance of each classifier. First we used precision, re-call, and accuracy of the tags applied to each term in the queries. A true positive, with respect to precision and re-call, refers to a semantically meaningful term being correc tly classified as a  X  X uery term X .

Recall that stop structure begins with the first word in the query, and extends to the first semantically meaningful term , or  X  X uery term X . The second measure is the mean squared error distance (MSE) of the position of the first  X  X uery term X  . This metric is intended to measure the error distance of the boundary between stop structure terms and query terms. Yahoo API Bing API improvements, (p-value &lt; 0.05) are marked + .
 Yahoo API Bing API improvements, (p-value &lt; 0.05) are marked + .

Results for classification evaluation are shown in Table 3. We are most interested in maximizing recall within this classifier. A high recall value means that all semantically meaningful terms are retained within the processed query. That is not to say that precision is not valuable; higher precision should dramatically improve the performance of the query, as semantically meaningless words are removed from the processed query.

Due to the effectiveness of the YamCha classifier, the out-put of this classifier was used as the automatically classifie d stop structure in the following retrieval experiments.
We performed these retrieval experiments in a similar way to the previous retrieval experiments in section 5. The aim of these experiments is to evaluate the performance of the queries produced by removing automatically identified stop structure.

Because the commercial search engines we are using may have changed in the months between this search session and the previous search session, we chose to re-submit queries produced by previously analyzed pre-processing technique s. In particular, the original query, and techniques involvin g the removal of INQUERY stopwords, and the removal of manually identified stop structure were re-submitted as par t of these experiments. Queries produced by these techniques and queries produced through the automatic removal of stop structure were submitted to Yahoo! API and Bing API dur-ing a search session on January 9th, 2010.

The retrieval performance for each of the processed queries as submitted in the second search session is shown in Tables 4 and 5. We show nDCG@5 and nDCG@10 for each pro-cessing technique. We use the same normalizing factor used in the above retrieval experiments, thus these results are directly comparable to the results in the previous retrieva l experiments.

Comparing these results to the results of the previous ses-sion, we can see that the performance of these queries and pre-processing techniques have improved somewhat. in the months since the last search session. However, we can see that the relative performances of the processing technique s has not significantly changed. Queries produced by remov-ing stop structure still significantly outperform the origi nal queries and the queries with stopwords removed.

We can infer from these results that stop structure can be effectively automatically classified. When compared with the manually identified stop structure, similar performanc e increases over the baseline are achieved. The differences between the manually identified stop structure and the clas-sified stop structure generally stemmed from misclassified semantically meaningful query words. That, is some mean-ingful terms are incorrectly classified as part of the stop structure. For example, in the query  X  X efine turf toe as in football X  , the automatic stop structure classifier identified  X  X efine turf X  as stop structure, thereby missing documents on the medical condition  X  X urf toe X  .
We have shown that pre-processing techniques are a viable option to improve the performance of verbose queries for commercial web search engines. In particular, we found that the removal of stop structure, a stop phrase beginning at the first word in the query, can dramatically improve the performance of the query.

We also investigated methods of automatically identifying stop structure within a given query. Using a YamCha based classifier, we were able to classify stop structure with high degree of accuracy. It was also shown that the retrieval per-formance of automatically identified stop structure closel y matches the performance of manually identified stop struc-ture.

Future work may include investigating the automatic re-moval of stop phrases that may occur at any position within a verbose query. This is based on the observation that it is possible to formulate queries which contain stop phrases which do not occur at the beginning of the query.
It would also be interesting to investigate methods of com-bining the results of several partial query searches. It is p os-sible that some rank fusion methods might produce better results by applying several pre-processing techniques to t he query then combining the results from several searches. This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] J. Allan, W. Croft, D. Fisher, M. Connell, F. Feng, [2] M. Bendersky and W. B. Croft. Discovering key [3] M. Bendersky and W. B. Croft. Analysis of long [4] S. Bergsma and Q. Wang. Learning Noun Phrase [5] J. Callan, W. Croft, and J. Broglio. TREC and [6] J. P. Callan and W. B. Croft. An evaluation of query [7] B. Croft, D. Metzler, and T. Strohman. Search [8] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity [9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [10] A. Hulth. Improved automatic keyword extraction [11] T. Kudoh and Y. Matsumoto. Use of support vector [12] G. Kumaran and V. R. Carvalho. Reducing long [13] J. Lafferty, A. McCallum, and F. Pereira. Conditional [14] M. Lease, J. Allan, and W. B. Croft. Regression Rank: [15] H. Liu. MontyLingua: An end-to-end natural language [16] B. Tan and F. Peng. Unsupervised query segmentation [17] R. Tsz-Wai Lo, B. He, and I. Ounis. Automatically [18] J. Xu and W. Croft. Query expansion using local and
