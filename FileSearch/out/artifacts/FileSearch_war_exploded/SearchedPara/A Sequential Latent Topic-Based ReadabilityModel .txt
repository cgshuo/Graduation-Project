 Conventional search engines aim to return relevant documents based on  X  X imi-larity X  (between a document and a query) and  X  X opularity X  (with respect to the hyperlink structure). A recently emerging relevance criteria is document read-ability. With the diversification of web resources and users, it is increasingly difficult for a search engine to provide different users, especially lay users, with documents in a specific domain that are not only relevant but also readable [ 10 ]. The readability plays an important role in assessing documents X  relevance measures do not necessarily reflect the readability of the returned documents [ 10 ]. In a typical human reading process, the readability of documents can be inter-preted at different levels [ 16 , 17 ]. It is argued in [ 13 ] that humans X  minds appear to go far beyond the data available, which means there will be a complicated process of abstraction in humans X  understanding. Thus, we propose to measure documents readability from two levels. The first is the surface level readabil-ity that relates to the surface content. It can be assessed by a series of classical readability features. Beyond the surface content, a higher level, namely the topic level readability, reflects whether it is easy for a user to comprehend the hidden topics in documents. Thus, we propose a topic-based readability method, which can be used to enhance domain-specific IR by considering both the surface and topic level readability of documents. There have been various general-purpose readability measures in the literature, such as the Flesh-Kincaid Grade Level and SMOG Index [ 3 , 11 ].Basedonthe surface-level features of a document, e.g., word length, sentence length, etc., these classical measures usually generate a numeric score that maps onto an educa-tional grade level. To further improve the accuracy of readability computation, various statistical, semantic and syntactic features of documents have been used [ 9 , 15 ]. However, they are designed for traditional general-purpose texts, thus insufficient to deal with domain-specific documents. Most of the existing mea-sures do not consider the documents X  readability at a topic level, which is indeed important for domain-specific documents which often contain a large amount of domain related topics and concepts.
 A concept-based approach has been proposed by Yan et al. [ 16 , 17 ], which takes into account the coverage (Scope) and relatedness (Cohesion) of domain topics (concepts) within a document, with reference to a domain taxonomy. In the tax-onomy, the topics are at different abstraction levels and their relationships are organized into a hierarchical tree structure [ 12 ]. Topic taxonomy encodes high-quality domain knowledge and can be used to improve a user X  X  understanding of the content of the text [ 2 ]. The general hypothesis is that the more abstract a topic is, the more general and easier to understand the topic tends to be. A lim-itation is that explicit domain taxonomy may not be always available. Recently, the hierarchical Latent Dirichlet Allocation (hLDA) has been widely used to discover the latent topics from large scale data [ 2 , 20 ]. Thus in this paper we propose to automatically build latent topic structures to represent the domain. Moreover, Yan X  X  model does not take into account the sequential dependency between adjacent topics which is important in understanding documents content easily and logically. Different from Yan X  X  work, in [ 6 , 7 , 14 ] a readability measure based on the term embedding and sequential discourse cohesion is proposed. However, it does not refer to a domain taxonomy. Nonetheless, their thought about sequential discourse cohesion gives us an inspiration for incorporating sequential dependency information within a latent topic based approach. In this paper, we propose a novel readability enhanced domain-specific infor-mation retrieval model. Specifically, two latent topical indicators, i.e., Topic Scope and Topic Trace , are proposed. They capture the sequential dependency of topics at different granularities, through mapping a document onto an auto-matically constructed topic taxonomy. Topic Scope , as originally proposed by Yan et al. [ 16 ], reflects the overall coverage of domain topics in a document. Topic Trace tracks how the sequence of topics occurring in a document traverses on the topic taxonomy. Additionally, we use the ratio of complex words as an indicator of the document X  X  surface level readability. The individual indicators and their combinations can be used to measure, from different perspectives, the readability of a document. Based on the documents X  readability scores, we can then rerank the initial list of results generated by a convention search engine. 3.1 Topic Taxonomy Extraction and Topic Identification A topic taxonomy can be extracted from a collection of documents. As a tree structure, it consists of topics (nodes) that are at different abstraction levels and connected by the subsumption relationships (edges). In this paper, we use a nonparametric generative procedure, namely the hierarchical Latent Dirich-let Allocation (hLDA), to generate a tree structure of topics by means of a nested Chinese Restaurant Process (nCRP) and Bayesian nonparametric infer-ence. Each topic can be represented as a probability distribution over words in the vocabulary. In the extracted topic taxonomy, the deeper a topic is, the more specific it tends to be. Thus, the root has the broadest meaning, while the leaves are the most specific ones. Figure 1 shows a fragment of topic taxonomy extracted from the CLEF eHealth 2013 medical collection [ 5 ].
 through a topic identification process. In this paper, we identify topics contained in a document based on the occurrence of top 10 probability words from the underlying distributions of topics. Therefore, a document can be represented as a sequence of identified topics, i.e., d =( t 1 ,t 2 , ..., t 3.2 Topical Readability Indicators After the topic identification, we propose two topical readability indicators. Topic Trace , tracks the identified topics sequentially on the taxonomy. Another indi-cator, Topic Scope reflects the coverage of the identified topics in a document. Topic Trace (TT). This indicator is based on the hypothesis that the topical line to compose a document is like the planning of travels among a number of scenery spots. A good traveling plan can help tourists visit as many scenery spots as possible with as little expense as possible and as small bumpy leap as possible. Similarly, a well-organized (thus more readable) document should introduce the related topics sequentially with little Topical Expense and small Topical Leap , which can reflect the coherence among sequential topics defined as Topic Trace here. Thus, the Topic Trace for a document d i in Eq. ( 1 ), where Expense ( d i )and Leap ( d i ) refer to the Topical Expense and Topic Leap , respectively, and  X  is a parameter to control the influence of the Leap on the trace score (  X  =0 . 001, the optimal values by experiments). A high trace score means the high readability of the document.
 Topical Leap means the bumpiness when the identified topics sequentially traverse on the topical taxonomy, as defined in Eq. ( 2 ). H of topic t j in the taxonomy.
 Topical Expense reflects the difficulty to parse the identified topics sequen-tially. Hypothesizing that the topical expense of a document is inversely related to the overall coherence among the topics within the document, we measure it as follows: where MC is the size of the set of identified topics and ConCoh ( t the contextual coherence, simplified as cc t j ,of t j in term of its average topical similarity with its surrounding topics (i.e., context).
 Specifically, to compute ConCoh ( t j ), we use a sliding window [ 4 ] with fixed size M (an odd number, M = 5 is the optimal value by experiments in this paper) which takes the center topic as the current topic, while the other surrounding topics within the window as contextual topics, as illustrated in Fig. 2 . The con-textual coherence of the current topic can be computed as in Eq. ( 4 ): topic t j and a context term t j +1 within the sliding window. e dependency between ti and ti+m gets stronger when they are closer in the sliding window. m the relative distance between the two topics in the window. Thus, we can get a global topic trace vector, i.e., tv ( d )=( cc document.
 Eq. ( 5 )[ 8 ]. L means the shortest path, and H is the depth of the most specific subsumer. The constants  X  and  X  are set to 0.2 and 0.6, respectively (the optimal values by experiments).
 document trace, i.e., Trace ( d i ). The score of Trace ( d (0,1). Figure 3 gives an example, where the Trace ( d i )=0 . 42 For d j ,the Trace ( d j )=0 . 17  X  e  X  0 . 005 =0 . 169. It turns out that d readable. Furthermore, from the structure perspective, d i concise and logical than d j .
 Topic Scope (TS). Based on a general hypothesis that the overall lower tax-onomy depths of identified topics in the taxonomy would indicate a better doc-ument readability, we also employ the average tree depth of the identified topics to calculate the topic scope. Compared with Yan X  X  work [ 16 ], we measure the document scope on topic level rather than conceptual level. As shown in Eq. ( 6 ), n t is the number of identified topics, while depth ( t i ) represents the depth of the identified topic t i on the topic taxonomy. Falling in (0,1), the higher the scope score is, the more readable the document tends to be.
 3.3 Document Reranking Based on Readability We combine the two levels of readability to calculate the overall readability score of d as follows: where Surface ( d i ) measures the surface level readability of the document. Specif-ically, x , y and z are explored to control the weight of three readability indicators, respectively. Both limited to (0,1), x + y =1,and z is 0 or 1. Thus, ReadScore can be normalized into (0,1). The larger the ReadScore is, the more readable the document will be.
 AsshowninEq.( 8 ), we employ the ratio of complex words that are not in the Dale-Chall word list [ 3 ] to calculate the surface level readability, where ComplexW ords is the number of complex words and TotalWords is the number of total words in the document.
 After we get the readability score, in the same way as in [ 16 ], we use Eq. ( 9 )to compute the total score for reranking, where RelScore ( d returned by a conventional search engine. m controls the weight of relevance score in documents reranking, while n controls the weight of readability score. In order to evaluate our proposed model, both user-oriented and system-oriented evaluations have been carried out. The former aims to find out how well our model X  X  prediction is correlated with human judgment on document readability, while the latter aims to evaluate how effectively our model can improve document ranking in medical information retrieval.
 eHealth 2013 dataset [ 5 ] contains 50 test queries and one million English doc-uments covering a broad set of medical topics. The initial search results were returned by the TF-IDF model in Lemur. All documents have been stemmed by Porter stemmer and filtered by SMART 571 stop word list. As a comparison, MeSH (Medical Subject Headings), an existing medical taxonomy, had been used to calculate Yan X  X  model ( Scope , the most effective indicator). Since it is expen-sive to construct taxonomy by hLDA on all documents, we employed the top 20 returned documents for all queries as the same method used in [ 18 ]. Specifically, we limited the vocabulary to be the 29795 words that appeared in more than 5 documents and a number of meaningless symbols were removed, such as  X  X  X ,  X - X ,  X &amp; X ,  X $ X  etc. As a result, 634 topics have been nested in a topic taxonomy with a depth of 8, of which a fragment has been shown in Fig. 1 .
 User-oriented Evaluation. In this evaluation, users were instructed to answer a series of questions related to the readability of the passages selected from CLEF eHealth 2013. We only selected 6 simple queries (Query1-Crohn X  X  dis-ease; Query2-Scar; Query3-Lightheaded; Query4-Liver transplantation; Query5-C.diff; Query6-Cardiac arrest) to avoid exhausting users. For each query, two user tasks, corresponding to topic scope and topic trace respectively, were per-formed independently with different sets of users to avoid the learning effect. In the first task, one pair of medical passages with different topic scopes (pre-selected from the top returned documents for the query in initial search results and labeled as passage  X  X  X  and  X  X  X , each of which are limited to 80-90 words, as shown in Fig. 4 ) are presented to a set of users. Through actual reading, the users were asked to answer the following questions: (1) Filtering question; (2) Scope related question; (3) Readability score for A; (4) Readability score for B. Detailed information has been shown in Fig. 5 .

SimilarRate ( TS )= In the second task, another pair of passages, also manually selected from the top returned documents, with different topic occurrence sequences (i.e., different topic traces), are used for another set of users to answer the same question in the first task, except that the question (2) is replaced by  X  X hich passage describes the topic more logically and smoothly? X ( X  X ore logically and smoothly X  refers to better trace). In question (3) and (4) of both tasks, the readability score  X 5 X  means the simplest to read, while  X 1 X  means the hardest to read.
 The evaluation was conducted through Amazon Mechanical Turk which tar-gets at  X  X rowdsourcing X  of Human Intelligence Tasks (HIT) in large scale. Only the high-qualification turkers are used (i.e., HIT Approval Rate (%) filtered the data of turkers who did not answer the filtering question ( i.e., ques-tion (1)) correctly, whose dwell time was less than 40 s or whose individual HIT is uncompleted. As a result, we collected the high-quality data of 20 Mechanical Turk users for each pair. For every pair of passages, we computed the aver-age readability score for each passage (with average standard deviation 0.89), and we calculated the consistency of users X  judgements on topic scope (topic trace) with average readability score in terms of Similar Rate . Through refer-ring to Table 1 , we derived SimilarRate for topic scope in Eq. ( 10 ), where n n bs , n at , n bt means the number of users who picked the corresponding choice. Read ( A )and Read ( B ) are the average readability scores assigned by all users. In addition, we calculate it for topic trace in the same way.
 shows a good average similar rate with 0.84 among users. It means that users tend to assign higher readability score to the passage with better topic trace. age assigned readability scores and that computed by our model and Yan X  X , which have been shown in Table 3 with best tuned combing parameters (i.e., x , y and z in Eq. ( 7 )).  X  X T+SI X  (combination of Topic Trace and Surface Indicator) has the highest coefficient among all combinations, and  X  X S+TT+SI X  (combination of all indicators) also correlates more closely with average assigned score than Yan X  X  model,which also implies the potential of our proposed model. System-oriented Evaluation. We also conducted system experiment to exam-ine the proposed indicators and combinations of them to rerank the top 20 doc-uments for all 50 test queries in CLEF eHealth 2013. To explore the relative effect of readability and relevance, we tuned the weights of m and n . Parts of the tuning results have been shown in Figs. 6 and 7 , through which we can infer that by integrating a certain weight of readability, i.e., n (for instance in Fig. 6 , when n is around 1), we can get consistent improvement by increasing weight of relevance, i.e., m .
 Specifically, we compared the reranking MAP of each indicator and some combinations of them, and picked up their best performance to do the signifi-cance test. Detailed results have been shown in Table 4 , in which  X  X T X  ( Concept Trace that implements the idea of Trace by referring to MeSH), gains the highest improvement of 5.92 % that is better than Yan X  X  model. Meanwhile,  X  X T X  ( Topic Trace ) and  X  X T+SI X  (combination of Topic Trace and Surface Indicator ) also improve the reranking performance significantly. Compared with  X  X T X ,  X  X T X  ( Topic Trace ) is competitive by constructing taxonomy automatically, which indicates the good potential of the idea of Trace .
 In this paper, we proposed a sequential latent topic-based document readability model for domain-specific information retrieval. In our model, two topical read-ability indicators, namely Topic Scope and Topic Trace have been developed, which can capture the overall coverage and sequential trace of the latent topics in the document, respectively. Compared with Yan X  X  work [ 16 ], on one hand, our model does not require referencing to an existing domain taxonomy. Instead, we automatically construct a latent topic taxonomy from the data. Therefore, our approach is more general and applicable to any domains that may not have an existing taxonomy. On the other hand, we take advantage of the sequential information between adjacent latent topics. Through user-oriented evaluation, our proposed readability indicators and the re-ranking model demonstrate a good correlation with human judgments. Furthermore, our model outperforms a state of the art concept-based model.
 rating n-grams. Meanwhile, refined algorithms and more suitable combinations of readability indicators will be tested.

