 Regular expressions are the dominant technique to extract business relevant entities (e.g., invoice numbers or product names) from text data (e.g., invoices), since these entity types often follow a strict underlying syntactical pattern. However, the manual construction of regular expressions that guarantee a high recall and precision is a tedious man-ual task and requires expert knowledge.

In this paper, we propose an approach that automatically infers regular expressions from a set of (positive) sample en-tities, which in turn can be derived either from enterprise databases (e.g., a product catalog) or annotated documents (e.g., historical invoices). The main innovation of our ap-proach is that it learns effective regular expressions that can be easily interpreted and modified by a user. The effective-ness is obtained by a novel method that weights dependent entity features of different granularity (i.e. on character and token level) against each other and selects the most suitable ones to form a regular expression.
 H.3 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic Processing Algorithms Information Extraction, Machine Learning, Regular Expres-sions, Minimum Description Length
Information extraction from textual data has various ap-plications, such as semantic search [13] (e.g., to provide semantic facets for information retrieval systems) or pre-processing of documents to enable structured queries [9] (e.g., to process invoices automatically by assigning text fragments like an invoice number to the corresponding database column). Many entity types  X  especially in en-terprise settings  X  can be extracted from texts using regular expressions that describe their syntactic patterns as a kind of formation rule. These entity types (codes, identifiers, etc.) occur most frequently in enterprise settings, because there is a strong need to identify entities like customers, invoices or products in a unique fashion.

Because the inherited patterns of identifier-like entities are often very distinctive, the exploitation of context infor-mation (e.g., words that occur frequently around entities within a corpus of annotated documents) is often not neces-sary. Thus, it is in many cases possible to construct effective regular expressions even though only a set of positive sample entities is available (e.g., derived from a database).
Examples of entity types, which may be extracted using regular expressions, some sample instances and likely effec-tive regular expressions to extract unknown entities of the same type are shown in Table 1. Let us for instance consider the examples for software names depicted in Table 1.(a): Fireworks 4.0, Fireworks MX, Word 2010, Frontpage 2007 and Antivirus 2007. Possible regular expressions that cap-ture their common features (i.e. syntactical patterns) are:
The most general pattern is that they share two tokens, such as shown in (R1) . In order to achieve a higher precision, we suggest describing the tokens in terms of token classes (see (R2) and (R3) ), where the expression ( \{ p }| [0-9]) scribes tokens that consist of punctuations and numbers. For some sub-sequences of the samples a more fine grained representation with character classes will lead to a higher precision without any impact on the recall (for instance [0-9] \{ p } [0-9] in (R4) as opposed to ( \{ p }| [0-9]) it is not obvious how to decide which parts should be repre-sented by token classes and which as character classes.
In order to achieve a high precision, dedicated sub-sequences of the samples should be kept in the final regular expression as they are. These sub-sequences may be shared across all instances of an entity type and increase the preci-sion tremendously. For instance in (R5) the part 20[0-9] { expresses that multiple version qualifiers refer to the year of the software release, while (R6) shows that it might also be possible to consider fixed tokens. The examples (R7) and (R8) illustrate another difficulty. Usually, a formation rule that underlies a set of entities fol-lows either a prefix-or suffix-like pattern. Thus, the most distinguishing features (e.g., fixed tokens or characters) oc-cur either at the beginning of the names or at the end. For our example, even though software vendors will use a prefix-like pattern (software name followed by different version qualifiers), a more generic suffix-like pattern, is used across vendor boundaries to represent version qualifiers. Thus, hav-ing a set of sample entities from one vendor (indicating that only software entities from one vendor shall be extracted), we may end up with a regular expression such as (R7) -asoft-ware name with an arbitrary version qualifier. If we would consider software names of different vendors (indicating that we like to extract all software names), we will observe var-ious overlapping version qualifiers and a rule such as (R8) will lead to a better extraction quality.

To tackle the challenges described above and automati-cally infer regular expressions that promise a high recall and precision, we present in this paper an approach that learns regular expressions by analyzing the given instances using the following processing steps: 1. the creation of pre-and suffix automatons that repre-2. the identification of fixed patterns, i.e., tokens or char-3. the selection of the best fitting feature representation 4. the selection of the best fitting features from a pre-The remainder of this paper is structured as follows: In the next section we discuss related work. We give an overview on statistical learning approaches, discuss the lim-itations of current rule learning techniques to features of one abstraction level and elaborate on related learning tasks in other research areas. Our method for inferring regular ex-pressions is presented in Section 3 and 4. While in Section 3 we focus on abstracting from the sample entities by generat-ing prefix and suffix automatons that capture all potential features, in Section 4 we elaborate on the methods for fea-ture selection and regular expression generation. Finally, we demonstrate the applicability of our approach in Section 5. To show that our approach provides a similar performance as the best performing state-of-art machine learning tech-niques for extracting identifier-like entities, we compare the extraction quality of our approach to the one of the ma-chine learning technique Conditional Random Fields. Fur-thermore, we show examples for learned regular expressions to demonstrate their readability.
In general one can distinguish learning algorithms in the area of information extraction into stochastic-statistical ap-proaches and rule learning approaches. In the following, we give an overview on widely adopted stochastic models and elaborate on the limitations of current rule learning ap-proaches to learn formation patterns for given sample enti-ties. In the end of this section we discuss related learning approaches in other research areas such as data cleansing or DTD-schema inference.

Prominent stochastic-statistical models for information extraction are Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) or Conditional Random Fields (CRF) [16], which interpret a text as conditional probabilistic feature sequence (e.g., features like words, word shape, part-of-speech, character-n-grams, etc.). The gener-ative nature of HMM restrict their application to feature representations of one abstraction level (e.g., word features OR character-n-grams) [10]. MEMM and CRF follow a dis-criminative approach and thus allow the combination of de-pendent feature types of different granularity into one model (e.g., word features AND character-n-grams). Recently [12] studied the bootstrapping of CRF-models by incorporating entity samples from pre-existing databases into the training process (still requiring a document corpus).

In general stochastic sequence models have two major drawbacks: First, there is no obvious way to train the above mentioned models in the absence of any document corpus, which is in principle possible with our approach if sample entities are available. Second, compared to rule based ap-proaches, it is rather difficult to manually debug, modify and tune probabilistic models [16], which is often required in real world settings. Thus, rule based approaches are still the preferred choice in enterprise settings.

Many approaches have been developed in the past that tackled the problem of learning deterministic extraction rules from annotated document corpora (see [19, 16] for an overview). Traditional approaches can be classified accord-ing to the adapted learning principle: propositional learning (e.g., CRYSTAL [17] or TIMES [3]) or relational learning (e.g., SRV [6] or (LP) 2 [4]). The latter ones have the ad-vantage that they can infer not only characteristic token features, but also relations among them. Both propositional learning and relational learning approaches encode the most discriminative token features of entities, features in their context and the relations among those features in a kind of logical expression.
 More recent approaches such as proposed by WHISK [17], SPIES [20] or AliBaba [13] induce a kind of regular expres-sions that describe the sequential occurrence of token fea-tures in a text (usually in the scope of one sentence). Note that SPIES, similar to our approach, adapted the minimum description length principle (MDL). However, MDL is lever-aged to determine the optimal combination of token features for extracting relationships among recognized entities, and not to select from features of different granularity.
Similar as for HMM, the feature space that can be lever-aged by current rule learning approaches is limited to one token granularity (usually word features). This is usually adequate for the extraction of proper names (such as per-sons, organizations, genes, proteins), which requires the ex-ploitation of context words, but not for the extraction of identifier-like entities. On the one hand, our approach com-plements previous rule learning techniques with a method to exploit dependent features of different granularity. On the other hand, it broadens the application of rule learn-ing approaches to scenarios where only sample entities are given.

In the research area of information extraction, the closest related approach is ReLIE [11]. It optimizes regular expres-sions that are fed to the system by a user, by exploiting an annotated training corpus. Therefore, it transforms reg-ular expressions, e.g., by adding negative lists (terms that should not be matched) until a (local) maximum with re-spect to recall and precision (i.e. F -measure) is reached. In contrast, our approach does not require such initial regular expressions. Therefore, our approach and [11] complement each other in situations where a training corpus is available. In summary, even though some previous approaches stud-ied the application of prefix-and suffix patterns to extract unknown entities [18, 2], no approach has been proposed so far that weights dependent features of different granularity for a given set of sample entities to induce concise regular expressions for information extraction.

There are some other research areas that adapted MDL for the inference of regular expressions. Notable among those are approaches to infer a DTD-schema from a given set of XML-files [5, 7, 1]. However, MDL is used to determine cardinalities of elements within a schema. Thus their focus is the inference of sequentially re-occurring patterns rather than distinguishing the optimal abstraction level.
The most similar approach to ours was developed in the context of the data cleansing system  X  X otters Wheel X  [15]. The basis of  X  X otters Wheel X  are user defined do-mains, such as  X  number  X ,  X  time  X  X r X  word  X , which are used to infer exactly one pattern (i.e. a sequential concatenation of domains) for a database column, such as  X  number word time  X  for instances like  X 19 January 06:45 X . Due to the different application domains, there are fundamental differences in the adaption of MDL. While [15] is meant to infer one general rule to detect outliers, our ap-proach infers an arbitrary number of rules to provide robust extraction mechanisms that lead to a high extraction qual-ity on unseen data. Thus the coverage of a rule with respect to the given instances is an important factor in the MDL-definition of [15], while our approach always covers all given instances. More importantly, abstraction levels or fixed in-stance sub-sequences are not taken into account by [15].
The problem of inducing regular expressions from a given set of entities can be generalized as follows: Given a set of positive instances I , we assume that there is an underly-ing grammar G that was used to generate those instances (formally: I X  X  ( G ) ). The ultimate goal of our algorithm, which we denote as infer , is to approximate G by a regular grammar G  X  such that L ( G  X  )  X  X  ( G ) holds. Thereby, the approximation  X  can be measured by the extraction qual-ity, i.e. recall and precision. To summarize, we formalize the problem of inferring regular expressions (represented by )asfollows:
A regular grammar can be represented by an automaton, which can be easily constructed and modified by computer programs. Thus automatons are frequently used in gram-mar inference problems [14]. We follow this approach and introduce in this section the construction of prefix and suffix automatons that capture all possible feature sequences of the given sample entities on different abstraction levels. Before, we introduce the feature types that should be considered by our approach or more precisely the alphabet (denoted as  X  ) of the target grammar G  X  .
The algorithm described in this paper is initialized by con-figuration of character classes. Let K B be the set of ba-sic characters (e.g., as defined in UTF-8 1 ) that can be used in regular expressions of an information extraction system. Through defining disjunctive subsets of the characters, we can specify a set of character classes K C . We denote a char-acter class by angle brackets (e.g.,  X  ) and refer in the re-maining paper to the following standard configuration 2 for Definition 3.1 (character classes).

Therein, LC represents lower case characters, UC up-per case characters, NB numbers, SC all special char-acters and WT arbitrary white spaces (space, tabulator, etc.). OT is a special character class ( X  X thers X ) that cap-tures all not explicitly defined characters. The language formed by a character class, which is used for classify-ing the input character sequences later on, is defined as
On top of the character classes K C , we define a set of token classes, which we denote as K T . In the following we consider each character sequence that is bounded by a white space symbol ( WT ) or a special character ( SC )astoken. Each white space symbol and special character sequence is treated as a token on its own. Note, that it is possible to configure other tokenization methods for special use cases. To allow for arbitrary combinations of character classes in a token class (e.g., to describe something like [A-Za-z] + define them as follows:
Definition 3.2 (token classes). The set of token classes K T is defined by the power set of K C except the empty set: K T := 2 K C \ X  .Asetin 2 K C \ X  characterizes one token class.
ISO/IEC 10646-3:2003
For simplification only ASCII characters are shown.
In the remainder of the paper we will denote a token class by the alternation of its underlying character classes and a Kleene plus (e.g. UC | NB + ), wherein the order of character classes is arbitrary.

To classify token sequences with token classes, we have to define the corresponding language L (  X  T ) , X  T  X  X  T .Forto-ken classes with only one element, we define its language by the Kleene closure of the character classes X  language: complex token classes, which are built from several charac-ter classes, is not straightforward because a character class language that could be expressed by [A-Za-z0-9] + inherits the language of [A-Za-z] + . Inordertoavoidsuchinher-itances, we define the languages of complex token classes to be disjunctive by excluding the languages of otherwise inherited token class languages. For instance we will get classify input tokens such as  X  AA  X   X  L ( UC + ) and  X  A0A  X  L ( UC | NB + ) , but  X  AA  X  /  X  L ( UC | NB + ) .Inpracticesuch a definition of token class languages leads to the natural behavior that tokens that contain many different character classes will be treated as more specialized ones (since they occur less often) than tokens which are built of fewer char-acter classes.

Having discussed feature types that are considered in the final regular expressions, we can define the alphabet  X  of the target grammar G  X  as  X = K B  X  X  C  X  X  T .
In order to come up with the target grammar G  X  that is built upon the alphabet  X  , we have to extract all potential features from the sample instances and weight them against each other to determine the best fitting pattern description. The weighting is described in Section 4. In this sub-section, we describe how to generate automatons that capture all possible features of the sample instances. Therefore, we gen-erate a prefix-and suffix automaton that describes the se-quential occurrences of token classes as transition sequences and keep the respective tokens for each transition. In a next step we add for each token set that is underlying to a tran-sition a parallel structure with respect to character classes.
The initial prefix and suffix automatons are generated us-ing a sequence classification algorithm (see Algorithm 1). To generate the suffix automaton, we invert the character se-quences of the given instances before running Algorithm 1. After tokenizing each input instance i  X  I in Algorithm 1, each token is classified with respect to the defined to-ken classes K T (based on the token class language defini-tion). The corresponding token class is added to the respec-tive token class sequence (see: c i  X  classify ( token , K Algorithm 1). Finally, we get a set of pairs (denoted as C ), which consists of token class sequences and the correspond-ing token sequence.
 Algorithm: 1 token class feature extraction 1: C  X  X  X  2: for all i  X  I do 4: for all token  X  tokenize ( i ) do 6: end for 8: end for
The automatons are generated through ordering the re-sults of Algorithm 1 by classification sequences and itera-tively building up a prefix (or suffix) tree automaton. In Figure 1 we illustrate an example prefix automaton (blue marked transitions) for the sample instances of notebook names shown in Table 1.(b). The transition labels for token classes are shown in Figure 1(a) (blue edges), while the un-derlying token sets are depicted in Figure 1(b). Note that we derive only one token class sequence for this simple ex-ample, while the examples from Table 1.(c) and Table 1.(d) will lead to more complex structures.

Next, we generate for each token class transition a parallel structure that describes character features. Therefore, we analyze the instance sub-sequences underlying a transition t , which we will denote in the following as I t . Each token i is classified with respect to the character class configuration
C similar as explained for Algorithm 1. However, before encoding the character class sequences into the automaton, we apply an additional processing step to identify blocks of identical character classes with similar cardinality, e.g., to distinguish number sequences with three elements from such that have for instance two elements. The reason is that we would like to avoid treating sequences with different cardinalities as overlapping features in later processing steps (see Section 4.1).

We illustrate the blocking of character class sequences for the first tokens of the sample entities from Table 1.(b) in Mapping 2. The left matrix illustrates the tokens I t .We sort the resulting character class sequences and store them in a matrix as shown in the right part of Mapping 2. To derive a blocking as shown in Mapping 2, we mark identical character class sequences per row, e.g., the three occurrences of NB in the first row, and group in a second run identical character class sequences across row boundaries.  X   X   X   X   X 
Next, we add states and edges by iterating the charac-ter block sequences as shown in Mapping 2 columnwise. Therefore, for each block, we encode a sequence of charac-ter class transitions into the previously generated automa-tons (see orange transitions in Figure 1(a)) and maintain in parallel the underlying character sets (orange transitions in Figure 1(b)).
As result of the previous processing steps, we extracted features of different granularity from the given instances and encoded their sequences into a prefix-and suffix automa-ton. In this section, we discuss the selection of features that most likely represent the underlying formation rule of the given instances and shall be encoded into the final regular expressions. Therefore, we first describe in Section 4.1 how to compare features on instance level to those on class level (cf. Figure 1(a) vs. Figure 1(b)) by considering the instance sub-sequence distribution in an automaton.

In Section 4.2, we compare features on token level to those on character level (cf. blue vs. orange transitions in Figure 1) by adapting the minimum description length prin-ciple. As a result, we get a deterministic prefix and suffix automaton with feature sequences that can easily be trans-e start state is labeled with b and possible end states with e 1 and e 2 . Transitions describe the occurrences of to character level features (cf. Section 4.2). formed into regular expressions. In order to support hetero-geneous instances, where some follow a prefix like pattern and others follow a suffix like formation rule (seeSection 1), we select per instance the most suitable feature sequence either from the prefix or suffix automaton, transform them into a regular expression and add them to the final rule set (if not yet present) as explained in Section 4.3.
As mentioned before, to achieve a high precision during extraction, we would like to encode character or token sets into the regular expressions instead of their class representa-tions. In order to judge whether a feature on instance level is very common, we compare the size of the instance set | attached to a transition t to the total number of instances that led to this transition (denoted as A ( I t ) ). For instance for the first character level transition in Figure 1, we would get: | I t | = |{ z,d }| =2 and A ( I t )=5 (cf. Table 1.(b)).
Thus, if we derive a rather small ratio | I t | /A ( I t ) compared to other transitions, it is more likely that the set of instance sub-sequences underlying such transitions will also occur in unknown instances. For instance if we have a prefix-like formation pattern such as shown in Figure 1, we will usually derive low values for | I t | /A ( I t ) in the transitions that are close to the start state of the prefix automaton. However, since the ratios in different transition sequences may differ a lot, e.g., because the underlying instances follow different formation rules, we have to compare the derived values for a transition to those in their neighborhood.

We define the neighborhood in terms of preceding and suc-ceeding transitions as follows: Let P be the set of transitions on the same abstraction level (token or character) that are predecessors of a transition t when traversing the automaton into the direction of the start state ( b in Figure 1). Further, let S be all transitions that are reachable from a transition t when traversing the automaton into the direction of all pos-sible end states (e.g., e 1 and e 2 for t  X { z,d } in Figure 1(b)). That is, we use the mean ratio in the neighboring transitions to decide whether we are facing a low or high value for the ratio | I t | /A ( I t ) . The mean ratio, E ( | I t | /A ( I
E
We define the selection criteria, which incorporates a pa-rameter  X   X  (  X  1 , 1) to allow engineers to influence the fea-ture selection, as follows:
In case Inequation 4 holds for a transition t , we select the token or character set I t as transition label. Otherwise we select the class representation as transition label (denoted as  X  t  X  X  C  X  X  T ). A high value of the parameter  X  will lead to fewer instance sets in the transition labels and thus to a higher recall, while a low value will lead to a higher precision. We consider two standard configurations:  X  =0 . 1(highre-call) and  X  =  X  0 . 1 (high precision) and evaluate their per-formance in Section 5. As a result, we merged, figuratively speaking, the features shown in Figure 1(a) and Figure 1(b) into one single feature representation. Still, we maintain features on character and token level. For our running ex-ample in Figure 1, we would select  X  assuming a  X  =0 . 1 X  the underlined transition labels for further processing.
So far, we derived a prefix and suffix automaton that con-sist of a mix of instance and class labeled transitions. Still, we maintain transitions on token and character level (i.e. orange and blue marked transitions in Figure 1). In the fol-lowing we discuss our adaption of the minimal description length principle (MDL) [8] to select the most promising ab-straction layer to represent parts of the underlying sample instances, e.g., whether the last tokens of our samples in Figure 1 shall be represented by UC + or its alternative on character level { A } UC UC .
The fundamental idea behind MDL is to compare different models, which describe a certain data structure, by means of their ability to compress the data. The underlying as-sumption is that the better a model describes the patterns within the data, the higher is the (approximated) compres-sion rate. By incorporating the complexity of the model and its fitness to encode the actual data, MDL avoids over-and under-generation in a natural way. The measure to compare different models is the description length , which is commonly defined as follows:
Definition 4.1 (description length). Let D be a dataset that can be described by a finite alphabet and M be a model describing the data. The description length L ( D,M ) to store D with M is defined as L ( D,M )= L ( M )+ L ( D where L ( M ) is the description length to store the model M itself and L ( D | M ) is the description length that is required to encode the data D given M .

In our case, we have to generate and compare all valid combinations of transitions (i.e. our alternative models), which we denote as T . We start with the variant T  X  X  that encodes all instances with transitions on token level (e.g., T  X  LC | NB + ( ) UC + for Figure 1) and pro-ceed with generating all variants by replacing some to-ken transitions by alternatives on character level (e.g., for the example shown in Figure 1 seven possible variants. For further explanation in the course of this sub-section, we list in Table 2 three of those variants, starting with a representation for transitions on token level in line 1, the combination that will turn out to be the most suitable one in line 2 and a representation on character level in line 3. Table 2: Example description length for different abstraction level combinations for Figure 1
The problem reduces, applying the MDL principle, to computing a description length L ( D,M )= L ( I,T ) for each set of transitions T  X  X  , given the sample entities I .The description length definition shall implement the following heuristic: A high abstraction layer (i.e. many transitions on token level) shall lead to a low model description length ( T ) , but a rather high data description length L ( I | a lower abstraction level (i.e. many transitions on character level) shall lead to a high model description length L ( T ) , but a rather low data description length L ( I | T ) .Withrespect to instance feature sets, which more likely occur on a lower abstraction level, the description length definition shall lead to a higher model description length L ( T ) , but a lower data description length L ( I | T ) .

Model description length: In order to compute a de-scription length for a model T  X  X  , we can imagine a dictio-nary based compression. Thus, we, visually speaking, create a dictionary entry for each symbol in the transition labels and generate a code word that replaces a certain symbol in an transition label during an imagined compression step. To approximate the size of such a dictionary (i.e. the model description length), we have to consider two factors: the storage space for the actual symbol which is required for de-coding a compressed transition sequence later on, and the storage space for the corresponding code word (i.e. a bit sequence that replaces a symbol). By the term  X  X ymbol X  we refer in the following either to a classification label, if we selected in the previous processing step a representation on class level, or a token or character in a transition label, if we selected a representation on instance level. We denote the set of symbols that are derived from a set of transitions T in the remaining paper as S ( T ) .

For the symbols themselves, we assume that each char-acter of an instance feature i t  X  I t or likewise a class label requires 2 3 bits (e.g, L ( UC + )=8bits ). The storage space for one code word, that will be used to compress symbols s  X  S ( T ) later on, is computed by assuming an optimal en-tropy coding as: H ( s )=  X  log 2 p ( s ) ,s  X  S ( T ) ,where p ( s )is the probability that s occurs in T . Thus, in summary we get an approximated description length L ( T ) of:
The second column of Table 2 illustrates some example values for L ( T ) , given the running example from Figure 1. Data description length: The description length ( I | T ) that results from compressing the data I with the model T  X  X  should behave as follows: If there is in-deed a prefix or suffix like pattern on character level with many overlapping features (e.g., as shown in the first sub-automaton on character level in Figure 1), the resulting de-scription length L ( I | T ) should be rather low, compared to those on token level. Further, as mentioned above, the pres-ence of instance features (which usually appear, with in-creasing  X  , first on character level) shall lead to a lower ( I | T ) compared to a transition with a class label.
We start with a description length definition for one tran-sition t and its underlying instance sub-sequences I t ,which we denote as L ( I t | t ) . We have to consider two cases: First, if we have selected instance features as transition labels ( t  X  I t ), we replace, visually speaking, the symbols by the previously derived code words. To reflect how many in-stances are covered by a certain code word, we assume a lossless compression, which means that we should be able to re-construct each individual instance sub-sequence. There-fore, we multiply the storage space of the corresponding code word H ( i t ) ,i t  X  I t with the total number (denoted as A ( i t ) ) of the occurrences of a symbol within the instance sub-sequences. This leads to the following description length definition for one transition:
As a second case, we consider transitions that are labeled feature label does not provide any exploitable information for lossless compression, but can be derived from the in-stance sub-sequences themselves. Thus, we encode only the instance set I t that led to t  X   X  t without any compression and again consider one byte ( 2 3 bits ) to store each character of each instance i t  X  I t inthecodelength L ( i t ) . Considering in addition the number of occurrences A ( i t ) , the description length L ( I t | t ) for feature class labeled transitions is com-puted by the equation: To incorporate the structure of an automaton into L ( I | we consider a factor to encode the transition sequences. Therefore, we assume 2 3 bits to store each state (denoted as N T ) and define the description length L ( I | T ) as:
Again, we show some example values for L ( I | T ) ,given the running example from Figure 1, in the third column of Table 2. The differences between the values for L ( I | T ) are rather low, because we considered only five instances in this minimal example.

Feature selection: Having computed the description length L ( I,T )= L ( T )+ L ( I | T ) for each T  X  X  ,wegetthe minimum description length by:
For further processing, we keep only the combination of transitions T  X  X  that led to L min ( I, T ) (i.e., the combi-nation in the second line of Table 2) from both the prefix and suffix automaton. Thus, we selected from the potential pattern descriptions as shown in Figure 1, the most suit-able ones with respect to their complexity and fitness (i.e. the thick marked transitions in Figure 1) and get as result deterministic prefix and suffix automatons.
In order to allow for heterogeneous instances, where some follow a prefix like pattern and others follow a suffix like formation rule, we merge the results from the prefix-and suffix automaton. Therefore, we maintained for each end state in the automaton a pointer to the corresponding sam-ple instances. In addition, during processing of the minimum description length (see Equation 9), we kept the computed description lengths for each path from the start state to each end state. To come up with the final set of regular expres-sions, we iterate over the input instances, choose the corre-sponding end states (e.g., e 1 for  X  X 800 X  or e 2 for  X  X 800 AAB X  in Figure 1) in the prefix and suffix automaton and select the end state which led to the smallest description length. Thus, we get for each input instance a sub-automaton either from the prefix or suffix automaton, which is best suited to describe its pattern with respect to all other instances.
For our running example, we finally get two pattern descriptions from the prefix automaton (i.e., for e 1 and e 2 in Figure 1). We apply some simplification rules such as introducing cardinalities (denoted as curly brackets) if possible or merging of overlapping regular expressions. More complex sub-automatons on character level are en-coded by the alternation of alternative transition sequences. Thus, we get only one expression for our running ex-can be expressed for instance in standard Java reg-ular expression range syntax for easier readability as z)([0-9]0 { 2 }| [0-9]0[a-z]) ( [A-Z] + )? (see also Table 1.(b)).
The generated regular expressions are ready-to-use to ex-tract unknown entities that follow the same patterns as the given sample entities. The configuration of the algorithm can be adapted to any regular-expressions-based informa-tion extraction system. Furthermore, the generated regular expressions can be easily understood by users and tuned by an expert if required.
Our learning approach is intended for identifier-like enti-ties that can be described by regular expressions. As dis-cussed in Section 2, we are not aware of any other learning approach for rule based information extraction that supports the inference of regular expressions from given example enti-ties and solves the problem of incorporating dependent fea-tures of different granularity. In our evaluation, we show that our approach generates easily understandable regular expressions and provides a comparable extraction quality to statistical approaches that leverage dependent features of different granularity. Therefore, we compare the extraction quality of our approach to results of the popular machine learning technique Conditional Random Fields (CRF) [10] and show examples of generated regular expressions. How-ever, in contrast to our approach, CRF requires annotated documents for training and context information to deter-mine entity boundaries. For comparison, we discuss the F -measure (harmonic mean of precision and recall). We assume that results are comparable if the F 1 -measure differ-ence is less than 0 . 15.
 Table 3: Example rules (1)  X  see Figure 2 for ex-traction quality comparison to CRF and DICT Phone numbers (Invoices) -3 of 28 rules
PAT-.1
PAT+.1 SWIFT-Codes (Invoices) -2 rules
PAT+.1 UC Invoice numbers (Invoices) -3 of 7 rules
PAT-.1 PAT+.1 Notebooks (Reviews) -3 of 32 rules
PAT-.1
PAT+.1
As CRF-implementation, we used the Stanford NLP toolkit 3 with a comparable feature combination. It consid-ers: tokens in a context window of size 2, (overlapping) n-grams with a maximal length of 4 and a word shape feature (comparable to our configuration for token classes). For our approach, we tested the two standard configurations with  X  =0 . 1 (high recall) and  X  =  X  0 . 1 (high precision).
We applied experiments on four (manually annotated) corpora from different domains and evaluated the perfor-mance for eight types of entities. Our method is trained with a randomly chosen subset of the annotated entities. We show the average results from multiple runs. To eval-uate the performance with different training data sizes, we conducted experiments for 1%, 2%, 4%, ..., 64% and 100% of the available entities for training. For the CRF, we always http://nlp.stanford.edu/software/crf-faq.shtml used the whole document corpus in the training phase, but removed the text segments (entities and context) for those entities which were not selected as training samples.
Since we choose random sample entities from the docu-ments for training (and not randomly selected example doc-uments), it was difficult to implement a folding for cross-validation (especially with respect to the CRF). Instead, we always run the tests on the whole corpus (which contained the training data as well) and conducted experiments for a dictionary-based approach for comparison. The difference in the results of the evaluated method and the dictionary approach indicates how many additional, unknown entities were identified by an approach and is lowered by the number of false positive that were included in the results.
In the following we briefly describe the used data sets and the entity types that were considered.

Invoices: To test the performance of our system in enter-prise settings, we acquired a corpus of 103 real world paper invoices that were digitalized by an OCR-software. The cor-pus consists of invoices from different domains, such as from car dealers, hotels or IT-services. We evaluated the extrac-tion of Swift-codes, invoice numbers and phone numbers. Note, that Swift-codes and invoice numbers usually occur in texts and databases in the same form. Thus our approach could train on a database as well. Phone numbers occur in texts usually with a reader friendly formatting.

Notebook reviews: To test the performance for prod-uct names, we used a corpus of notebook reviews 4 .The corpus contains 90 documents, each having a list of about 20 notebook names that should be extracted by the algo-rithms. The notebook names might occur in some product catalog in the same form.

SAP Forums: As another use case, we conducted ex-periments on user generated developer forum contents 5 . http://www.notebookreview.com http://www.sdn.sap.com The goal was to extract mentioned Java exceptions (e.g., java.lang.NullPointerException) from 1,500 forum entries. The problem is that Java exceptions generally follow the syntax of class names in Java (which are also mentioned fre-quently in the forums), but have some distinguishing tokens.
ReLie corpus: The last corpus was provided by the au-thors of [11]. It contains about 80,000 text fragments that were extracted from 50,000 intranet sites, 50,000 web sites of the University of Michigan and 10,000 emails from the Enron data set 6 . The text fragments were selected using general regular expressions for phone numbers, software names and university course names. Finally, the corpus was annotated manually to distinguish false positives (with respect to the initial regular expressions) and correct matches.
We mentioned already in Section 2 that our approach and the one of [11] are complementary, because [11] optimizes a regular expression using a training corpus through transfor-mations (e.g., by adding terms that shall not be matched). Thus, a direct comparison is not possible. Rather, a com-bination of our approach, rule learning approaches that in-corporate context and [11] is a promising future integration problem to fully leverage annotated training corpora, if they are available.
Figure 2 and 3 illustrate the results of our approach (  X  = 0 . 1as PAT+.1 and  X  =  X  0 . 1as PAT-.1 ), of the CRF ap-proach (see CRF ) and the baseline method ( DICT ). Exam-ples for generated regular expressions are shown in Table 3, 4 and 5. We use the same syntax as we used throughout this paper, shortened some alternations of instance features by  X  ...  X  and cropped very long feature sequences by intro-ducing  X  X  ... ] X . In this sub-section, will mainly investigate on the extraction quality and leave it to the experienced reader to decide on the simplicity of the generated rule sets.
The CRF-approach provides a high precision. Therefore, http://www.cs.cmu.edu/ enron/ we start comparing the results of CRF and the configuration  X  =  X  0 . 1. In the two use cases for extracting phone num-bers (see Figure 2(a) and Figure 3(b)) as well as in the use case for university courses, PAT-.1 outperforms CRF by far, given only a small training data set. In Figure 2(a) the dif-ference is 0 . 24 and in Figure 3(b) 0 . 34 using 4% of the train-ing data. With an increasing amount of training data, CRF adapts to the corpus and produces comparable results with respect to PAT-.1 . Note that CRF produces poor results for Figure 3(c). In an advanced analysis of the results, it turned out that the classifier predicted in many cases a likelihood for being an entitiy that was slightly below the likelihood for not being an entity. However, a recombination and a different configuration of the used features did not lead to an improvement. We assume that a domain specific feature definition could improve the results significantly. In this use case PAT-.1 performed very well and already leads to very good results using a training data size of 2% .
 In the use case of extracting invoice numbers (see Figure 2(c)) and the one for notebook names (see Figure 2(d)), we can observe a very good, comparable per-formance of PAT-.1 and CRF . In two other cases, namely the extraction of swift codes (see Figure 2(b)) and software names (see Figure 3(d)), we can also observe comparable re-sults of PAT-.1 and CRF , but a unsatisfactory performance compared to DICT . Again, we see in Figure 3(c) that perfor-mance of CRF is not optimal since the F 1 -measure for 100% of the training data is only 0 . 81.
 For the experiments concerning Java exceptions (see Figure 3(a)), we see that PAT-.1 led to poor results com-pared to CRF (0 . 34 difference for 4% of the training data) because CRF implicitly encodes the re-occuring sequential patterns that occur in exception names (tokens separated by dots). In this case PAT+.1 performs as well as CRF even though it is not aware of re-occuring sequential struc-tures. Furthermore, PAT+.1 turns out to perform best for the phone number entities (see Figure 2(a) and Figure 3(b)), because the syntactic structure is distinctive even though fewer instance features are considered. A similar behaviour can be seen for software names (see Figure 3(d)).
However, the configuration PAT+.1 over-generates in other cases, e.g., for university course names (see Figure 3(c)) and especially for Swift-codes (Figure 2(b)), and leads to too abstract regular expressions (e.g., a expres-sion like UC + for Swift-codes). Therefore, the recall is with few training data rather high, but the precision does not im-prove significantly when supplying more training data.
In summary, we could show that in most of the cases our approach performs as well as the prominent CRF-approach (configured with comparable features) for the tar-geted classes of entity types. Furthermore, our approach leads in many cases (either with  X  =  X  0 . 1or  X  =0 . 1) Phone number (ReLIE) -3 of 7 rules
PAT-.1
PAT+.1 University courses (ReLIE) -3 rules PAT-.1 PAT+.1 Software names (ReLIE) -3 of 11 rules PAT-.1 PAT+.1 to better results than CRF, even though it does not rely on context information (and can be trained in absence of a doc-ument corpus). In addition, users can understand and tune our regular expressions easily, which is extremely compli-cated for statistical machine learning approaches. Clearly, an integration of our approach with statistical approaches is straightforward since they support regular expression fea-tures. However, in order to keep the advantage of ease in traceability and manual tuning, an integration with other rule learning and optimization approaches appears more promising for extracting identifier-like entities in the pres-ence of annotated training documents.
In this paper, we addressed the problem of inferring regu-lar expressions, which can be easily understood by an user, from sample entities, e.g., derived from a database or an annotated document corpus, to extract unknown entities of the same type from text data. In principle, it does not re-quire any (annotated) document corpus and extends there-fore the applications of information extraction especially in enterprise settings, where usually a lot of structured data is available, as opposed to (annotated) training documents. Compared to previous rule learning approaches in informa-tion extraction, which are more suited to learning context in-formation from training documents, our approach supports dependent features of different granularity, as opposed to a limitation to token features of one granularity.

An experimental study showed the applicability of our approach in different enterprise use cases and that it has a similar performance as the prominent approach of Condi-tional Random Fields (CRF) for use cases where annotated documents are available. Furthermore, it turned out that our approach leads often to a better performance than CRF for targeted class of identifier-like entities.

In future we would like to study  X  besides an integration with other learning approaches  X  the incorporation of higher abstraction levels and adapt our approach to the domain of web wrapper induction.
 This work is supported by the FP7 EU Project  X  X OBUST  X  Risk and Opportunity management of huge-scale BUSiness communiTy cooperation X  7 (contract no. ICT-257859).
