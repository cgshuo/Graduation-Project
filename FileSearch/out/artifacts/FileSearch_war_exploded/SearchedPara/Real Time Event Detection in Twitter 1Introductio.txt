 Events usually refer to something abnormal, that is, something that rarely happens in normal situation. Event detection aims to find such abnormal phenomenon from col-outbreaks, criticism and so on [1, 2]. 
When it comes to Twitter, events are topics that suddenly draw public attention, for lar according to the trendy topics provided by twitter.com officially 1 . They are talked talk about the coming of weekends while tweets on Monday usually complain about the beginning of a week of work. Such topics should not be regarded as events. 
Several approaches have been proposed for event detection from tweets, such as wave analysis[3], topic model approach based on LDA[4], HDP[15] or text classifica-detection or inefficiency in involving topics models. ther to decide whether a bursty word repr esents a new topic, we borrow the ideas of evolutionary clustering which focuses on detecting the dynamics of a given topic such as appearance, disappearance and evolution for new topics. We develop a novel time dependent HDP(td-HDP) model based on HDP model[6] for new event detection. Our and each day X  X  topic distribution is affected by the topic distribution of previous time. disappearance of new topics which can be det ected in td-HDP. Finally, the location of ponents of an event in Twitter: topic, location, the time it becomes bursty. Our model can grasp the three points promptly and accurately at the same time. 
The rest of paper is organized as follows. Section 2 describes related works in event model which aims at detecting new topics from busty words is presented in Section 4. sented in Section 6 and we conclude the paper in Section 7. 2.1 Event Detection Existing event detection methods usually treat words as signals. Some researches deal detect events, with events represented by the transitions of state. Fung et al.[8] devel-bursty words are detected with a threshold-based heuristic. There are some other me-Fourier Transformation (DFT) is applied to convert the signals from the time domain into the frequency domain. Words are converted into signals and corres ponding signal auto-correlations are used The problem of such kind of work is that words are treated as signals and it is hard to problem is, in LDA, the number of topics should be fixed in advance. So it X  X  not suit-able for real time event detection where the amount of data gradually grows, especial-ly when the data is huge. 2.2 Evolutional Clustering and HDP Evolutionary clustering is a relatively new research for topic detection, which aims to preserve the smoothness of clustering results over time, while fitting the data of each evolutionary setting: (1) k-means clustering , and (2) agglomerative hierarchical clus-tering. The problem of evolutionary clustering is that the number of clusters stays the same over time. This assumption is obviously violated in many real applications. 
Recently, HDP has been widely used in evolutionary clustering due to its capability of learning number of clusters automatically and sharing mixture components across (DP) mixture model, and the infinite set of mixture clusters is shared among all corpo-ra. Sethuraman [11] gave a stick-breaking constructive definition of DP for arbitrarily measurable base space and Blackwell and MacQueen[12] explained DP using the Polya urn scheme. The Polya urn scheme is closely related to the Chinese Restaurant Process (CRP) metaphor, which is applied on HDP demonstrating the  X clustering property X  as the  X distribution on partition X . Based on HDP, some algorithms of evo-lutionary clustering are proposed by incorporating time dependencies, such as DP-Chain, HDP-EVO and dynamic HDP et al [13], [14], [15]. As stated, we firstly try to find bursty words which serve as candidates for new event detection in tweets. Bursty words are thos e whose frequencies severely increase in a tion. According to the central limit theorem, the frequencies of a word in each day can be approximately modeled by a Gaussian distribution. The parameters of distribution can be estimated from historical data. We use the records of data in tweets from Jan 2011 to Dec 2011 as the historical data. The frequency of each word is recorded in a 1*365 dimensional vector, denoting the number of appearance of certain word in each day of the year. Let , bursty word, we assume that the distribution of , words such as  X  X t is getting colder and co lder X  in tweets. And these topics should not word, we assume that: where t w denotes whether time t is Monday, Tuesday,...Sunday, and t m denotes in Eq(1) concerns about the overall effect within a year for word i x . The second term largely cuts off running time. || T is 365 . whole year. Similarly,  X  and 2 lihood estimation. Experiments show that this approximation works well. Next we get down to the bursty word selection. At time t, word with a daily frequency would be selected as a bursty word. At each time t, we selected all words whose fre-event detection in Section 4. process, and then introduce our time dependent HDP model (tdHDP). 4.1 DP and HDP  X  is a positive concentration parameter and G 0 is called a base measure. Sethuraman [11] showed that a measure G drawn from a DP is discrete by the stick-breaking con-struction: where  X  ~ GEM(  X  ) if  X  is a random probability measure defined by Eq. (5). 
A HDP[4] defines a distribution over a set of DPs. In HDP, a global measure G 0 is process is described as: plete the definition of a HDP mixture model, whose graphical representation is shown in Figure 2(a). According to Eq. (5), the stick-breaking c onstruction of HDP can be represented as: The corresponding graphical model is shown in Figure 2(b). 4.2 td-HDP We model the multiple correlated corpus with Dirichlet Processes with Markovian time dependency. Specially, at each time t, we use a DP to model the data from dif-ferent time and then put the time dependency between different epochs based on the measure G, and G is drawn from ( , ) DP H  X  where H is the base measure. According to Markovian assumption, t G is not only affected by the overall base measure G, but also affected by 1 t G  X  . The generation process of td-HDP is as follows: 1. Draw an overall base measure 0 ~(,) GDP G  X  , which denotes the base distribution for all time data. ence of topic distribution from previous time. c is the time controlling factor. be represented with the following form: =  X  ,~ () v GEM  X   X  is sampled from a vague gamma prior which is set to be Ga (10.0, 1.0). 4.3 Inference We begin with the metaphor following Chinese Restaurant fanchise for inference. At xD  X  corresponds to a customer. Each table t in the restaurant would have a dish serves dish tp k . MCMC sampling is used for td-HDP sampling. 
We also need a notation for counts. tp n  X  represents the number of customers in res-and k m  X  denotes the total number of tables serving dish k in all restaurants. Sampling p . Due to the space limit, we would just show the sampling formula with-fx w then we can sample the topic of new table as follows new Sampling k . Because the process of sampling t actually changes the component mem-ber of tables, we continue to sample tp k for each table in the restaurant as follow: E denotes the number of replicates ti x at current table. 4.4 New Events Detection In the two following situations, the event is regarded as new: 1. event . 2. new event. Note each topic is represented by the t op five words with largest probability. model[16]. The training data contains more than 1M tweets in Singapore, with several lected and tagged using simple rules. For ex ample, a car accident in Rochor Road is an event. Then  X  X ochor Road X  in tweets of the same day such as  X  X here X  X  a car acci-location of event. The feature template is unigram, current word w and four adjacent words, which are w, w+/-1 and w+/-2. Bigram is also used as an important feature. We use tweets of Singapore in one year as history data to decide the normal situa-tion of words and tweets in May 2012 as test data. All these tweets are from more Jan. 2011 to Dec.2011 is used as history data for training of parameters in Gaussian mixtures described in Section 3 for bursty words extraction and data from Jan. 2011 to Mar.2011 for parameter tuning in 6.2. Data from Apr. 2012 to Jun. 2012 is used as test data. 6.1 Pre-processing Words which contain too many replicates such as  X  X ahahahha X ,  X  X mmmmm X  or words are also deleted. Tweets that contai n less than two words are also ignored. 6.2 Parameter Tuning Firstly the time controlling factor in time dependant HDP needs to be tuned. We use collection by manually selecting new events detected by different models. We asked period of time), LDA model, tf-idf model, tdHDP(C=0), tdHDP(c=1) and tdDHP(c=5). We collect 154 events in even t collection. Then we experiment the td-HDP algorithm by setting c in the range from 0 to 5 with interval of 1. We compare the results with different c value with true event collection which was built previously according to manual evaluation and calculate accuracy for each experiment. We find in the range from 0 to 2.0 with interval of 0.2. Fig. 4 show the different value of accu-their peak at around 1.2 and drops afterwards. 6.3 Experimental Results Due to the limitation of space, only events detected in May 2012 are shown in Table according to manual evaluation. The wrongly detected events are marked in grey. As check the related tweets of the 9 th one which is not a valid event. There are two differ-share words such as  X  X ics X . 6.4 Evaluations The evaluation of our model is conducted in three aspects. The first one is Timeliness, evaluate the precision and recall of our model, which respectively denote the model X  X  ability in detecting new events correctly and completely. 6.4.1 Evaluation of Timeliness use the top word in the new topic to represent the event. The timeliness of an event is events detected in May 2012 are shown in Fig. 4. Only 4 out of 32 events are detected after the frequency of bursty words arrives at its peak. 6.4.2 Evaluation of Precision and Recall time dependent relations. LDA model and tf-idf model are also used as baselines. (a) (b) 
As in 6.2, we build the event collection and manually judge the results of these me-thods. Then we compare the results of each model to the collection and get the Preci-achieves better results than LDA because HDP can model data more properly and topic number can increase and decrease na turally with the appearance and disappear-ance of topics. But in LDA, topic number has to be fixed in advance. Td-HDP achieves best result and this further illustrates the necessity of adding Markovian time relation in topic modeling. Our approach enjoys the precision of 0.968 and a Recall of 0.931, demonstrating the effectiveness of our model. We propose a novel event detection model for Twitter. A Gaussian Mixture model is dependent HDP model is developed for new event detection from word candidates. Our model can deal with large amount of data effectively and detect events efficient-ly. Experiments show the good performance of our model. Acknowledgements. This work is supported by NSFC programs (No: 61273278), 2011BAH10B04-03), National High Technology R&amp;D Program of China (No. 2012AA011101) and Singapore National Research Foundation under its IRC @ SG FI and administered by the IDM Programme Office. 
