 Understanding intents from search queries can improve a user X  X  search experience and boost a site X  X  advertising pro f-its. Query tagging via statistical sequential labeling mod els has been shown to perform well, but annotating the train-ing set for supervised learning requires substantial human effort. Domain-specific knowledge, such as semantic class lexicons, reduces the amount of needed manual annotations, but much human effort is still required to maintain these as search topics evolve over time.

This paper investigates semi-supervised learning algorit hms that leverage structured data (HTML lists) from the Web to automatically generate semantic-class lexicons, which are used to improve query tagging performance  X  even with far less training data. We focus our study on understanding the correct objectives for the semi-supervised lexicon lea rn-ing algorithms that are crucial for the success of query tag-ging. Prior work on lexicon acquisition has largely focused on the precision of the lexicons, but we show that precision is not important if the lexicons are used for query tagging. A more adequate criterion should emphasize a trade-off be-tween maximizing the recall of semantic class instances in the data, and minimizing the confusability . This ensures that the similar levels of precision and recall are observed on both training and test set, hence prevents over-fitting the lexicon features. Experimental results on retail produ ct queries show that enhancing a query tagger with lexicons learned with this objective reduces word level tagging erro rs by up to 25% compared to the baseline tagger that does not use any lexicon features. In contrast, lexicons obtaine d through a precision-centric learning algorithm even degra de the performance of a tagger compared to the baseline. Fur-thermore, the proposed method outperforms one in which semantic class lexicons have been extracted from a database . H.3.3 [ Information Search and Retrieval ]: Information search and retrieval X  Search process ; I.2.6 [ Artificial Intel-ligence ]: Learning X  Knowledge Acquisition ; I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms, Theory, Experimentation, Performance Lexicon acquisition/set expansion, query understanding/ semantic tagging, HTML lists, conditional random fields, semi-super vised learning, information extraction, model over-fitting
One can greatly improve a user X  X  search experience and boost a site X  X  advertising profits by better understanding the user X  X  intent. Query tagging  X  assigning a pre-defined semantic label to each query term  X  is an attempt towards this goal. For example, a product query may be tagged as follows:
By understanding what users are looking for, one can provide the exact information they need. Indeed, much of this information may be found in the contents of relational databases, which are now indexed by most search engines. By automatically tagging queries with field information, on e can use the index to directly retrieve the precise informati on a user needs. Furthermore, an improved understanding of query intent enables better decisions in the selection of co n-textual ads. For example, relevant local ads may be selected when query tagging identifies a city in the following query:
Query tagging [10] is a specialized form of information extraction (IE) or named entity recognition (NER), which is often performed by training statistical sequential labe l-ing models, such as Conditional Random Fields (CRFs) [8]. CRFs generally outperform rule based systems by large mar-gins, and usually rules can be incorporated as features into CRFs, so in practice CRFs reach at least similar perfor-mance as a rule-based system. Additionally, CRFs are bet-ter at ambiguity resolution by modeling context dependen-cies with a principled data-driven approach. Ambiguity is one of the most difficult problems in query tagging, as illus-trated by the example above  X   X  X hite swan X  could denote an animal, arts and crafts or a city;  X  X ashington X  could re-fer to a person, city, county or state. However, while su-pervised learning generates statistical models which disa m-biguate effectively, substantial human effort is required to create an annotated training set for learning. For product query tagging, this problem is exacerbated by the fact that queries change over time due to the emergence of new kinds of products, new brands, new models, new merchants, or new product features. Domain specific knowledge, such as phrase lists for concepts like Brand or Model (henceforth  X  X emantic class lexicons X ), has been shown to improve the generalization capability of statistical models, reducin g the need for annotated data[21]. However, such knowledge is often not available and usually requires substantial human effort to compile and maintain, which may be too costly as information changes over time.

In this work, we compare two semi-supervised learning al-gorithms that leverage structured data (HTML lists) from the Web to generate semantic-class lexicons. The algorithm s are based on the assumption that many HTML lists contain instances of a single concept. For example, if  X  X owershot sd850 X  is labeled as Model in the training data and it is observed to co-occur with  X  X ebel xsi X  in many HTML lists, then  X  X ebel xsi X  is likely to be a Model as well. In our case, instance phrases of semantic classes ( Brand , Model , etc.) are first extracted from annotated queries; the instances ar e then used as seeds in a graph learning algorithm to obtain additional instances of the same semantic classes. These sets of instances, or lexicons, enable statistical models t o rely on less labeled training data  X  in fact we will show later that with only a few hundred of labeled queries plus the lexicons that are automatically obtained from the proposed algorithm, a query tagger achieves better performance than a model trained with tens of thousands of labeled examples without any lexicons. Because the lists on the ever-changin g Web reflect the evolving nature of the contents of semantic classes, and the context patterns are relatively fixed over time  X  e.g., it is always more likely that a model follows a brand ( X  X anon rebel X ) than a brand follows a model ( X  X ebel canon X ), incorporating lexicons learned from the Web into a statistical model also makes this technique adaptive to the query content shift as it occurs over time.

The algorithms discussed in this paper are not specific to queries; they can also be applied to other information ex-traction (IE) tasks (in fact, we have applied them to extract from natural language text). One requirement, however, is that the entities of interest are well represented on the Web . For product query understanding, the domain the present work focuses on, this is generally the case. The tag set con-tains nine labels illustrated in Table 1. We attempt to ac-quire lexicons for the first five classes that occur frequentl y in queries and Web lists.

We adapt two graph-learning algorithms to the task of lexicon learning and observe opposite impacts on query tag-ging accuracy. To understand the difference, we carry out a series of experiments, and find that traditional precision -centric lexicon learning is inadequate for query tagging  X  i t causes a mismatch of lexicon coverage between the training and test data, and consequently results in a severe over-fitting of the lexicon features in the statistical sequentia l labeling model. We show that a more adequate criterion Table 1: Semantic classes (CRF labels) in product query tagging task should emphasize the coverage of the acquired lexicons while maintaining a low level of confusability . Here we prefer to have more entries acquired by the algorithm to cover the in-stances in the query tagging data (i.e., broad coverage). In the mean time, low level of confusability requires that an un -ambiguous phrase e has a high posterior probability P ( c | e ) for the corresponding semantic class c , while the probability is lower if e is ambiguously labeled with different semantic tags in the query tagging data. If e does not occur often in the domain of interest, the value of the probability has little impact on the final query tagging accuracy  X  hence precision should not be the primary criterion of lexicon ac-quisition for query tagging. By increasing coverage while maintaining low level of confusability, similar precision and recall levels can be observed on both training and test data, which effectively avoids model over-fitting.

Although we study these effects in the context of query tagging, we believe that our discoveries generalize beyond our task and provide a guideline for future research on semi-supervised knowledge acquisition for information extract ion and named entity recognition.
Semantic class and relation acquisition is a well-studied topic. Much research leverages linguistic patterns to extr act semantic classes and relations from free text [6, 2, 11, 14, 7 ]. In [4], an algorithm is introduced to learn semantic classes and named entity extraction patterns simultaneously. Re-cently there has been increasing interest in leveraging str uc-tured data from the Web to learn semantic classes and re-lations. In [20], a Web page wrapper induction algorithm is presented that learns language-independent patterns fo r semantic class lexicon expansion. In [5], both linguistic p at-terns and wrappers for lists are used to extract semantic class members. In [3], a Web-scale relational database is built by filtering HTML tables with statistical classifiers. The work described in [19] is closely related to ours. Like us, the authors use graph learning to acquire open-domain semantic classes by leveraging structured Web data, in thei r case, the HTML tables reported in [3]. Another closely re-lated work is described in [18], where a context pattern in-duction algorithm is used to obtain lexicons, which in turn are used by a named entity recognition model.

In contrast to this existing work, which primarily focuses for the sake of semantic class acquisition, hence precision is ambivalent about the precision. In our case, the semantic classes are only used as intermediate information to improv e the query tagging accuracy and are hidden from the end users. Therefore, we consider over-generalization accept able as long as it does not degrade the performance of query tagging. As shown later in the paper, this philosophical difference leads to different graph learning algorithms  X  wit h dramatically different results.

Finally, we note that there exist large bodies of work on Conditional Random Fields and graph-based learning. CRFs have been broadly applied to many natural language related tasks, including part-of-speech tagging [8], name d entity recognition [12], information extraction [16], and pars-ing [17]. Graph-based learning [23, 24] has been used for various natural language processing tasks, as evidenced by many publications in the TextGraphs Workshops [1].
CRFs [8] are conditional models with the following log-linear form defined with respect to a set of features f k The features are functions of the label sequence y and asso-ciated observation x : Here  X  = {  X  k } is a set of parameters. The value of  X  k determines the impact of the feature f k ( y , x ) on the condi-tional probability. Z ( x ;  X ) = P y exp P k  X  k f k ( y , x ) is a partition function that normalizes the distribution. Give n a set of m labeled training examples ( x 1 , y 1 ) . . . ( x can be learned by using stochastic gradient decent, L-BFGS or other numeric optimization algorithms to maximize the following objective function: The second term in Eq. (2) regularizes the parameters to keep them from taking extreme values, thus preventing model over-fitting. Note that the objective function is a convex function, so a single global optimum exists.
The CRF in Eq. (1) is unconstrained in the sense that the feature functions are defined on the entire label sequence y . Because the number of all possible label sequences is com-binatorial, the model training and inference of an uncon-strained CRF is very inefficient. To improve efficiency, it is common to restrict attention to linear-chain CRFs [8, 15], imposing a Markov constraint on the model topology, and restricting the feature functions to depend only on the labe ls assigned to the current and immediately previous terms, in the form f ( y t  X  1 , y t , x , t ). These allow efficient dynamic pro-gramming algorithms for inference and model training  X  yet still support potentially interdependent features on obse rva-tions via discriminative training of the conditional model .
Our technique for semi-supervised lexicon acquisition and query tagging proceeds in the following steps: the most important metric.

Seed Brand Model Merc. Type Attr. Neg. camera 0.003 0.017 0 0.963 0 0.017 powershot 0 0.230 0 0.770 0 0 office depot 0.289 0 0.711 0 0 0 1. Extracting seed instances with an initial distribution 2. Constructing a sub-graph from a phrase-list bipartite 3. Applying graph learning algorithms on the sub-graph 4. Constructing stratified lexicons according to the pos-5. Training the CRFs with features built on the stratified Step 1, 2, and 4 will be described in this section. Step 3 will be devoted a separate section afterwards. Step 5 will be explained in Section 5.1.
The training set for CRF query tagging contains queries manually tagged with labels like the examples in Section 1. Seed instances for each semantic class of interest are ex-tracted from it. A sequence of terms with the same semantic class labels is combined as an instance phrase. For the exam-ple in Section 1, seed instance  X  X anon X  can be extracted for Brand ,  X  X owershot sd850 X  for Model , and  X  X igital camera X  for Type . Phrases can often ambiguously belong to multiple semantic classes. This may be due to the fact that they are intrinsically ambiguous ( X  X pple X  can be a Brand or a Mer-chant as the Apple Store) or due to annotation mistakes  X  the majority of occurrences of X  X owershot X  X as mislabeled a s Type instead of the correct label Model . Instead of assigning a single semantic class to a phrase, each phrase is therefore associated with a distribution over the set of classes  X  for t he product query tagging task, the distribution is over the five classes at the top of Table 1, plus an additional  X  X egative X  class representing the union of the remaining four semantic classes, for which we do not learn the separate lexicons to use as features in the CRF. Table 2 shows some examples of the extracted seeds with their distributions.
A set of HTML lists (contents of &lt; ol &gt; or &lt; ul &gt; tags) are crawled from the Web and heuristically cleaned to remove the SPAM lists or lists used to format Web page layout. The resulting set includes about 55 million lists, which contai n about 61 million unique phrases. From the lists a bipartite graph can be built, as illustrated by Figure 1, where an edge connects a phrase node with a list node if the phrase is an item in the list. The edges have the uniform weight of 1 . 0.
Given a set of seeds with their class distributions, a sub-graph is constructed. First, all the list nodes are marked with the number of items that exactly match a seed phrase. Figure 1: Bipartite list graph. On the left are the unique phrases, on the right are the HTML lists The list nodes are pruned if their seed count is under a number of the surviving lists they belong to. The same threshold is used to prune out the phrase nodes that do not appear in a sufficient number of lists.
The graph learning algorithms described in the next sec-tion produce P ( c | v ), a distribution over semantic classes c for a phrase node v in the graph. While the probabilities can be directly used as continuous features in CRFs, they are not very reliable due to the noisy nature of the lists. In-stead, phrases are binned to multiple stratified lexicons fo r a semantic class c according to their distributions  X  phrase p (represented by v p in the graph) is included in the lexicon c for class c if 1  X  P ( c | v p )  X  [( k  X  1) / 10 , k/ 10). For exam-ple, if P ( Brand |  X  X ony ericsson X ) = 0 . 85, the phrase  X  X ony ericsson X  will be placed in the lexicon Brand 2 . This binning strategy produces lexicons c 1 , . . . , c 10 for a semantic class c .
Seed phrases may not exist in the list graph. In this case, they can be either included in the lexicons according to thei r initial distribution, or not used at all. We will address thi s in Section 5.
We present two graph learning algorithms here. The first was proposed in [19] and was used for semi-supervised se-mantic class learning. The second is based on the algorithm for semi-supervised learning of query classification propo sed in [9]. While the former can operate on arbitrary weighted graphs, the latter was specifically designed for weighted bi -partite graphs. None of the two algorithms was originally used to obtain lexicon features for a sequential tagger. Bot h algorithms have the same computational complexity  X  each iteration is temporally and spatially linear to the number o f edges in the graph.
In [19], a high precision learning algorithm was applied to acquire semantic classes from free text, based on dis-tributional similarity and template patterns. The classes were subsequently expanded based on a bipartite graph con-structed from HTML tables. To improve the precision of the semantic class, the algorithm introduces an additional pseudo-class  X  , which represents  X  X ack of information for se-mantic class assignment. X  All nodes in the graph that are ter results than without pruning. with L  X  (  X  ) = 1 .

The algorithm can be interpreted as as combination of distribution propagation and random graph walks. Here W is a square matrix, W ( u, v ) stands for the weight on the edge ( u, v )  X  E . C v is the influence on the distribution of the node v from its adjacent nodes in the graph, which is normalized by the sum of all incoming edges X  weights to make it a proper probabilistic distribution. The parameter s p represent the probabilities of three different actions that can be taken at v during the random walk  X  continue the walk to an adjacent node; stay at current node v ; or stop the walk. They can be set with heuristics based on a node X  X  fan-out entropy [19].
The second graph learning algorithm stems from [23], and was applied to semi-supervised learning of query classifica -tion by leveraging query-click graph[9]. Instead of perfor m-ing learning on generic graphs, this algorithm focuses on bipartite graphs: The nodes of the graph can be partitioned to two sets, I and L , such that there is no edge between any two nodes in the same set. The weight matrix W is | I | X | L | instead of the square matrix in Algorithm I. The algorithm listed below first  X  X ormalizes X  W to B = D  X  1 / 2 W . Here D is a diagonal matrix in which d i,i equals the sum of all ele-ments in the i th row (or column) of W W T . Intuitively, d is the  X  X olume X  of all length-of-two paths starting at node v . Since the graph is bipartite, this ensures that the prop-agation of distributions from phrases to lists then back to phrases will not introduce additional probability mass.
The algorithm first propagates the distribution of the nodes u  X  I (list entries) to the nodes v  X  L (HTML lists) to obtain H v , a semantic class distribution for v , then propagates back from the distribution of nodes in L to F i v , the distribution of v  X  I . The parameter  X  regularizes the graph-learning. Unlike the F i v  X  X  in Algorithm I, neither H i v nor F i abilistic distribution right after the distribution propa gation (line 4 and 6), so they need to be normalized inside the re-peat loop.

Algorithm I and II are similar in the sense that both are learning multiple competing semantic classes simultaneou sly via distribution propagation. Algorithm II differs from Al-gorithm in the following three aspects: 1. No inclusion of  X  as a dummy competing semantic 2. Weight normalization . The matrix in Algorithm I is 3. Fewer parameters to set for graph-based learning . Al-In practice, we do not learn with Algorithm II till full con-vergence. We noticed that too many (around 100) iterations make the tagging results significantly worse  X  this is relate d to the over-fitting problem that will be discussed in Section 5.3. In the experiments reported in the next section, we stopped at iteration 5 and used 0 for  X  . Because of the early stop of graph learning, the effect of the value of  X  is not significant as long as it is not too far away from the small value (0 . 01) suggested in [22].
This section compares the effectiveness of the two semi-supervised lexicon learning algorithms for the purpose of query tagging. A series of experiments are then conducted to explain the performance difference between the two al-gorithms. The experiments lead to the discovery of a more adequate lexicon learning objective that is crucial for effe c-tive sequential labeling.
We have conducted experiments with a data set of prod-uct search queries logged by a commercial search engine, which was manually labeled by annotators. Since much of the data was labeled using Mechanical Turk, it is very noisy and contains many inconsistent labelings. The test sets hav e been examined and corrected by an independent expert an-notator, while the training sets have not been altered.
We compared tagging accuracy on the test set for three different conditions: using CRFs without lexicon features, using CRFs with lexicon features obtained by Algorithm I, and using CRFs with lexicon features obtained by Algorithm II. For some product categories including Computing and Electronics (C&amp;E) and Clothing and Shoes (C&amp;S), struc-tured databases are available, from which one can directly Table 3: The size of the training and test data Overall 27410 239362 4420 23476 C&amp;E 14843 134362 669 3572
C&amp;S 4122 36774 898 4804 extract lexicons for each semantic class. In addition to the above conditions, we also compared the performance of a tagger with the semi-automatically acquired lexicons to a tagger with lexicons extracted from these databases on the datasets of these two categories. Table 3 shows summary statistics of these datasets.

The baseline tagging model is a linear chain CRF. It uses the state transition features f T R i,j to capture the influence of context on the tag assigned to a given word: where i, j are states (labels) of the model. It is reported in [10] that the transition features play a very important role in improving the tagging accuracy for the task.

In addition, the baseline tagger uses unigram f UG w,j and bigram f BG w,w 0 ,j features to capture the dependency on the identity of the current (and previous) words: f where w, w 0 are words and j is a model state (label).
In conditions other than the baseline, lexicon features ( a.k.a. word cluster features) are also included: where L is a lexicon, L  X  [ x t ] means that an entry in L is a substring of x that covers x t . j is a label.

The performance of CRF tagging is measured by word level labeling accuracy  X  the percentage of words that are assigned the correct labels by the model. This measure was chosen since we found that it correlated well with assess-ments of end-to-end search quality.
Unless mentioned otherwise, the experiment results in this section were obtained by excluding the seed phrases that do not exist in the bipartite graph from the learned lexicons.
In the first experiment, we examine the contributions of lexicons at different strata to the tagging accuracy. Figure 2 shows the test set word level query tagging accuracy as dif-ferent numbers of lexicon strata are included in the model for each semantic class. When only the top stratum (which contains the highest posterior instances) is included, the re is little difference between Algorithm I and Algorithm II. As more strata of lexicons are used, the CRF with stratified lex-icons learned by Algorithm II achieves improved word level tagging accuracy. The accuracy plateaus after the seven top strata of lexicons are included. In contrast, the performan ce of the CRF with lexicons learned by Algorithm I does not change much as more strata of lexicons are included. While the number of lexicon strata has little impact on tagging accuracy after the seventh stratum, it does make a prac-tical difference with respect to running time and memory Figure 2: Word level tagging accuracy in relation to the number of strata of lexicons included in the CRF model for each learned semantic class Figure 3: Word level tagging accuracy of the CRFs trained without lexicon features; with lexicon fea-tures learned by Algorithm I or Algorithm II usage. For example, about 1/3 of entries are at the lowest stratum in the lexicons learned by Algorithm II  X  getting rid of them can significantly reduce the memory footprint. Also, note that the lowest stratum only contains phrases that have been assigned an extremely small posterior of &lt; 0.1, and collectively the ten lexicon strata contain all phr ases in the pruned bipartite graph. For these reasons, we drop the lowest stratum from all experiments, and only use the top nine lexicon strata.

Figure 3 compares CRF word level tagging accuracy us-ing no lexicon features, using lexicons learned by Algorith m I, and using lexicons learned by Algorithm II as different numbers of labeled samples are used for obtaining seeds and training the CRF. Lexicons learned by Algorithm II improve the accuracy by 4% to 9% absolutely over the model us-ing no lexicon features (up to 25% relative error reduction) ; whereas the lexicons learned by Algorithm I degrade the tag-ging accuracy. Another important observation can be made in Figure 3: trained with only 5% of the training data, the CRF using the lexicon features obtained by Algorithm II has already outperformed the CRF trained with all training data but without using the lexicon features. Algorithm II can therefore greatly reduce the workload of data annota-tion.

In the next experiment, we compare the performance of the CRFs using lexicons extracted from the structured datab ase and lexicons learned with each of the two graph learning al-gorithms on the C&amp;E and C&amp;S test data. Figure 4 plots the word level tagging accuracy in relation to the number of training samples used. For the C&amp;E category, the lexicons extracted from the database perform at a similar level as the lexicons learned by Algorithm II. As the training data increases, the model using no lexicon features achieves a si m-ilar level of accuracy as the models that use the lexicon fea-tures. This is mainly due to the fact that there are enough training samples and the diversity of interests in different products in C&amp;E is not as significant as in other categories. The lexicons learned by Algorithm I, however, degrade per-formance. For the C&amp;S category, both the CRFs using lex-icon features with the lexicons acquired by Algorithm II or extracted from the database achieve significantly improved tagging accuracy over the baseline model. The lexicons learned by Algorithm I, again, hurt accuracy. The models using lexicons from Algorithm II have additional gains over the models using the database lexicons  X  about 2% absolute in most cases. Figure 4: Word level tagging accuracy of the CRFs without lexicon features; with lexicon features, where lexicons were either obtained from structured database, learned by Algorithm I or II. Top chart: C&amp;E. Bottom chart: C&amp;S category
Finally, it is also interesting to note that the machine learning based approaches have superior performance to a rule based system: Using only the entries from the database as rules and a disambiguation strategy based on the prior probability of semantic classes, the word level tagging acc u-racy reaches 54.8% for the C&amp;E test set and 48.4% for the C&amp;S test set  X  far less than the 90% and 69% obtained by the CRFs. Table 4: Number of lexical phrases for different se-mantic classes for the C&amp;E and C&amp;S categories
What are the key factors that lead to the different behav-ior of Algorithm I and II? It was a surprise that Algorithm I degraded the accuracy of the baseline CRFs using no lexicon features. In this subsection, as an aftereffect, we compare the lexicons acquired by the two algorithms with the seeds extracted from the entire training sets. The first difference we noticed is the collective size of the top nine strata of lex -icons learned by the two algorithms. Table 4 shows the size of the lexicons for different semantic classes for the C&amp;E and C&amp;S categories. Compared to the lexicons extracted from the database or learned by Algorithm I, the lexicons learned by Algorithm II are at least an order of magnitude larger (the only exception is C&amp;E Model for which the database is extremely comprehensive). The semantic lexicons learne d with the overall data have similar patterns as with the C&amp;E and C&amp;S categories.

The lexicon size of a semantic class has direct impact on the classifier X  X  performance on the semantic class. Table 5 shows the precision, recall and F1 score on each semantic class, of the model trained with all training data. When the lexicon size is too small (e.g., Merchant), the perfor-mance (especially recall) suffers. Table 6 shows the number of semantic class instance occurrences and types (multiple occurrences of the same instances are counted once) in the test set, as well as the coverage of different lexicons for the se instances. While coverage explains the superior performan ce of Algorithm II, it is clearly not the only factor since Algo-rithm I learned lexicons have similar (or better) coverage as the lexicons extracted from the database for C&amp;E and C&amp;E, yet their performance is much worse than the DB lex-icons. Furthermore, the learned lexicons are quite differen t from the DB lexicons even though they have similar perfor-mance as in the case of the DB and Algorithm II learned lexicons for the C&amp;E category  X  they do not overlap much as illustrated by the last two rows in Table 6. However, the Algorithm II learned lexicons cover almost all the test set instances covered by the DB lexicons, which suggests that Algorithm II is good at acquiring the popular semantic class instances that occurs frequently in the test set.
What are the other factors that affect the tagging perfor-mance? Naturally question may be raised about the qual-ity of the learned lexicons, especially the huge one learned by Algorithm II. We believe that a more fundamental ques-tion is what metrics the quality should be measured against. The traditional precision-centric metrics may be irreleva nt: Since the lexicons are hidden from users, over-generalizin g a lexicon may not affect the final tagging accuracy  X  as long Table 5: The performance of each semantic class Table 6: Test set semantic class instance coverage Lexicon Coverage Coverage DB 100973 229 570 5614 209 754 Algo I 9347 392 833 2739 205 761 Algo II 539591 606 1028 250424 683 1248 DB  X  I 1295 159 498 489 123 624
DB  X  II 11241 210 551 2991 203 746 as it does not cause confusion in discriminating among the semantic classes of interest to the current task (here this includes the agglomerated  X  X egative X  class).

If only the lexicons are used for query tagging without other features, they can keep growing as long as their growth brings more benefit than harm to the query tagger. Here, the benefit is the number of semantic class instances in the test set that are correctly covered by the lexicon (true posi -tives, TP). The harm is the mislabeling of a word sequence as a semantic class due to overgeneralization (false positi ves, FP). Figure 5 shows the FP/TP ratio in relation to the num-ber of stratified lexicons of each semantic class included  X  the statistics are collected by simply using the lexicons as matching rules in a rule-based system. No CRFs are used in this and the next experiment for precision/recall. We also include the lexicons obtained from the structured database ; since these are not stratified, their FP/TP ratio is a hori-zontal line. The curve for Algorithm I is much flatter than that for Algorithm II. That implies that the precision of the lexicons from Algorithm I is much higher than that from Al-gorithm II as more strata of lexicons are included. However, the ratio is still below 1 (benefit is greater than harm) for Al -gorithm II after the 8th stratified lexicon is included (whic h covered 61% of the total acquired semantic class instances for C&amp;E and 58% for C&amp;S.) This shows that Algorithm II expands the lexicon more aggressively (thus results in high er coverage for instances in the test queries). In the mean time it also maintains a low level of confusion. The statistics he re directly correlate with the CRF X  X  behavior in Figure 2  X  ac-curacy keeps improving until around the seventh stratum of lexicons learned by Algorithm II is included in the model.
The capability of maintaining a lower confusion level dur-ing lexicon growth can be largely attributed to the fact that the learning algorithms propagate entire class distributi ons instead of only a single class X  probability, as is common in other approaches such as PageRank [13]. Because competing semantic classes in a given task are learned simultaneously , a phrase that occurs in multiple lists containing seed phras es from different semantic classes will not be able to get a high posterior probability for a specific class. Consequently, i n-trinsically ambiguous phrases are less likely to end up in Figure 5: FP/TP ratio on the test set semantic classes for the lexicons extracted from database or learned with different algorithms. Top chart: C&amp;E. Bottom chart: C&amp;S the top strata of lexicons. This has the advantage that in cases of ambiguity a CRF can put more emphasis on other contextual features, such as state transitions.

By varying the number of stratified lexicons included, we plot the training and test set semantic class instance re-call/precision curves in Figure 6. The lexicon extracted from the database is represented by a single point in each of the charts for C&amp;E and C&amp;S, since it is not stratified. At the same precision level, the lexicons learned by Algorithm II have much higher recall of the test set semantic class instances than the lexicons extracted from the databases. With more strata included, the precision of the lexicons learned by Algorithm I is much higher. In fact, both preci-sion and recall on test data do not change very much. This explains why tagging accuracy does not change much as more lexicons learned by Algorithm I are included in the CRF, as illustrated in Figure 2. In contrast, Algorithm II has a better recall on test data when more stratified lexicons are included. On the other hand, because the seed data is taken from the training sample, the learned lexicons have a high recall of semantic class instances on the training set. That high recall may not be available on the test set if the learning algorithm is conservative for better precision. T his is exactly what we observe in Figure 6: the training data curve of Algorithm I has much larger recall than the cor-responding test data curve, whereas the difference is much smaller with the curves for Algorithm II. In fact, in the case of the C&amp;E data, the test data recall is often larger than that of the training data for Algorithm II. In summary, the precision/recall curves of the training data and test data a re much more similar for the lexicons learned by Algorithm II than those learned by Algorithm I.

Note that the FP/TR ratios in Figure 5 and the precisions in Figure 6 do not reflect the ability of ambiguity resolution of the lexicons learned by Algorithm II  X  every match of a Figure 6: Recall/Precision curves of the lexicons. Top chart: C&amp;E. Middle chart: C&amp;S category. Bot-tom chart: overall data test set instance by a lexicon of a wrong class was counted as an error without considering the competition from the lexicons of the correct semantic class. Table 7 compares the stratum of the correct semantic class lexicon covering a tes t set instance phrase with the highest stratum among the lex-icons of competing semantic classes. It reports the number (and percentage) of times that the correct semantic class le x-icon has higher, equal or lower rank (hence higher, similar o r lower posterior probability) than the lexicons of competin g semantic classes. In most cases, the correct semantic class has higher posterior probability. Therefore, the potentia l benefit of over-generalizing the lexicon is even larger than it seems to be in Figure 5.

The degradation of tagging accuracy with the lexicons learned by Algorithm I can be attributed to model over-fitting. Because of the large discrepancy of recall between the training and test set in the case of Algorithm I, as il-lustrated by Figure 6, the learned models rely too much on the lexicon features and do not generalize well to the test data. The over-fitting of the lexicon features may depress the contribution of other important features to the final tag -ging decision, such as state transitions features. Table 8 Table 7: Stratum comparison of the ambiguous lex-icons that cover a test set instance phrase. The lexicons of the correct semantic classes have higher ranks for 70% or more of test set instance occurrence than those of competing semantic classes Overall 4881 (70.5%) 621 (9.0%) 1424 (20.6%) CE 925 (80.7%) 65 (5.7%) 156 (13.6%)
CS 1025 (69.3%) 157 (10.6%) 297 (20.1%) compares the average absolute values of the weights for the lexicon features and those for other features learned by the CRF models with the DB lexicons, the lexicons learned by Algorithm I and the lexicons learned by Algorithm II. It is clear that the models based on the lexicons learned by Al-gorithm I have the highest ratios of lexicon feature weights and other feature weights, while the models based on the lexicons learned by Algorithm II have the lowest such ra-tios. The ratios correlate well with the tagging accuracy. It clearly indicates that over-fitting the lexicon features is the major problem with the models based on the lexicons learned by Algorithm I.
 Table 8: Comparison of the average absolute val-ues of the weights between the lexicon features and other features
We found that more restrictive regularization helped but was not able to bring the accuracy on par with the CRFs using no lexicon features  X  in fact the word level tagging ac-curacy we reported earlier was obtained with tuned regular-ization parameters. Surprisingly, the high precision make s the problem even worse! Since the higher precision is also observed in the training set, the learned model is more confi-dent on relying on the lexicon features. In the extreme case, if all training set instances of semantic classes are covere d by lexicons (100% recall) and the lexicon for each seman-tic class does not cover any training set instances of other semantic classes (100% precision), the learner will treat t he lexicon features (match of lexicon entries) as the necessar y and sufficient condition for a substring to be labeled as a semantic class  X  though the condition may not hold at all on the test set. This explains the observation that the CRFs using lexicon features based on the lower precision databas e lexicons outperform the higher precision lexicons learned by Algorithm I, when the two sets of lexicons have similar level of recall on test data.

Precision-centric learning is particularly harmful when t he knowledge is acquired from HTML lists, due to the high noise level of the lists  X  focusing on precision with noisy inputs implies an even more conservative learning strategy Figure 7: The effect of adding seed phrases not in the bipartite graph into the lexicons, for lexicons learned with Algorithm I and II hence lower coverage of the test set instances. On the other hand, since the training set instances are used as seeds, the y have a high chance to be covered by the learned lexicons. Therefore the mismatch between the training and test data becomes more severe.

The final experiment confirms that over-fitting is the cul-prit. In the experiment results reported so far, we have ex-cluded the seed phrases that do not exist in the bipartite lis t graph from the lexicons. If we include those in the lexicons according to their initial distribution, we get 100% covera ge of the training data semantic classes. This, on one hand, increases the coverage of the test set semantic classes, on the other hand, makes the over-fitting problem even worse. Figure 7 shows that adding these seed indeed degrades the performance for both algorithms on the entire test set. The degradation is more severe for Algorithm I because the preci -sion of the lexicons is so high that the model almost learned to make decisions solely based on lexicon features.
N-fold cross-learning of lexicons and CRFs may alleviate the over-fitting problem of Algorithm I. However, prelimi-nary results show little improvement for Algorithm II with 10-fold cross learning  X  over-fitting is not a big issue there .
We applied two different semi-supervised graph learning algorithms to acquire semantic class lexicons from Web list s, and used the lexicons as features in CRFs for query tagging. One algorithm resulted in significant improvements in query tagging accuracy, and substantially reduced the human ef-fort needed to manually label training data.

By comparing the behavior of two algorithms, we found that the precision-centric learning algorithms are not sui t-able for use in sequential labeling tasks, due to the problem of over-fitting. Instead, it is better to over-generalize th e learned lexicons to result in a similar recall on the train-ing and the test set, while maintaining a low level of con-fusion among the semantic classes of interest. This can be achieved by simultaneously learning lexicons of multi-ple competing classes via distribution propagation. We not e that each of the two algorithms discussed was not designed for the purpose of lexicon acquisition for query tagging. Each can be enhanced to increase the recall of semantic class instances. While the present work compares existing algo-rithms adapted to our task of query tagging, we are plan-ning to develop novel algorithms based on our insights in the future. For example, we can quantify  X  X onfusability X  and include it in an objective function (together with lex-icon coverage), such that new learning algorithms can be designed to directly optimize the objective function.
The major contribution of the present work lies in reveal-ing the key factors in semi-supervised lexicon acquisition that make it successful in a sequential labeling task. While we studied these factors in the context of query tagging, we are confident that the conclusions extend beyond that. In fact, the lessons learned in this work provide a general guid e-line for future research on semi-supervised knowledge acqu i-sition for sequential labeling tasks. Importantly, this ap plies to our finding that knowledge acquisition should adopt an objective that ensures that the knowledge acquired through partial-supervision from the training set has similar prop er-ties on the training set as well as an independent develop-ment set. Only then the sequential labeling model will not over-fit the knowledge obtained in a semi-supervised fashio n.
The authors would like to thank Alex Acero, Paul Viola and Dan Weld for their helps, comments and feedback on this work. [1] Textgraphs: Graph-based algorithms for natural [2] E. Agichtein and L. Gravano. Snowball: extracting [3] M. J. Cafarella, A. Halevy, Z. D. Wang, E. Wu, and [4] E. Eiloff and R. Jones. Learning dictionaries for [5] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, [6] M. A. Hearst. Automatic acquisition of hyponyms [7] M. Komachi and H. Suzuki. Minimally supervised [8] J. Lafferty, A. McCallum, and F. Pereira. Conditional [9] X. Li, Y.-Y. Wang, and A. Acero. Learning query [10] X. Li, Y.-Y. Wang, and A. Acero. Extracting [11] D. Lin and P. Pantel. Concept discovery from text. In [12] A. McCallum and W. Li. Early results for named [13] L. Page, S. Brin, R. Motwani, and T. Winograd. The [14] P. Pantel and M. Pennacchiotti. Espresso: Leveraging [15] F. Peng and A. McCallum. Accurate information [16] S. Sarawagi and W. W. Cohen. Semi-markov [17] F. Sha and F. Pereira. Shallow parsing with [18] P. P. Talukdar, T. Brants, M. Liberman, and [19] P. P. Talukdar, J. Reisinger, M. Pas  X ca, [20] R. C. Wang, N. Schlaefer, W. Cohen, and E. Nyberg. [21] Y.-Y. Wang, A. Acero, C. Chelba, B. Frey, and [22] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [23] D. Zhou, B. Sch  X  olkopf, and T. Hofmann.
 [24] X. Zhu. Semi-Supervised Learning with Graphs . PhD
