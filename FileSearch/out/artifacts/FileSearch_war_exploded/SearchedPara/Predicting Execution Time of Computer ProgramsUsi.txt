 Computing systems today are ubiquitous, and range from the v ery small (e.g., iPods, cellphones, are management components that decide how to schedule the ex ecution of different programs over resource allocation), and how to weather anomalies (e.g., fl ash crowds or attacks [6, 17, 24]). These management components typically must make guesses ab out how a program will perform consider a simple scenario in a data center with two computer s, fast computer A and slow computer B the program at A much faster. If the scheduler can predict accurately how lon g the program would take to execute on input f at computer A or B , he/she can make an optimal decision, returning results faster, possibly minimizing energy use, etc.
 Despite all these opportunities and demands, uses of predic tion have been at best unsophisticated in modern computer systems. Existing approaches either cre ate analytical models for the programs of such methods is highly dependent on human experts who are a ble to select important predictors come by, because programs can get complex quickly beyond the capabilities of a single expert, or dependent not on externally visible features such as comman d-line parameters and input files, but on the internal semantics of the program (e.g., what lines of code are executed). To address this problem (lack of expert and inherent semanti cs), we recently developed a new sys-can be built from those features without the need for a human expert.
 each other in the way they build the nonlinear terms into the m odel  X  SPORE-LASSO first selects than a given degree; while SPORE-FoBa chooses adaptively a subset of the full expanded terms and hence allows possibly a higher order of polynomials. Our algorithms are in fact new general methods motivated by the computer performance prediction p roblem. They can learn a relationship features, and select a few from hundreds of features to const ruct an explicit polynomial form to predict the response. The compact and explicit polynomial f orm reveals important insights in the program semantics (e.g., the internal program loop that aff ects program execution time the most). Our approach is general, flexible and automated, and can adap t the prediction models to specific programs, computer platforms, and even inputs.
 We evaluate our algorithms experimentally on three popular computer programs from web search and image processing. We show that our SPORE algorithms can a chieve accurate predictions with of interpretability and accuracy.
 to predict time and resource consumption for database queri es using statistics on query texts and execution plans. To measure the empirical computational co mplexity of a program, Trendprof [12] constructs linear or power-law models that predict program execution counts. The drawbacks of such their requirement for simple input-size to execution time c orrelations.
 such as worst-case execution time, for embedded systems. Th ese properties depend heavily on the target hardware environment in which the program is execute d. Modeling the environment manually property of interest while the adversary sets environment s tates and parameters. gram codes. Then machine learning techniques can be used to s elect the most important features sion models such as LASSO have been widely studied in the past decade. Feature selection with notable are the SpAM work with theoretical and simulation re sults [20] and additive and general-ized forward regression [18]. Empirical studies with data o f these non-linear sparse methods are very few [21]. The drawback of applying the SpAM method in our execution time prediction prob-lem is that SpAM outputs an additive model and cannot use the i nteraction information between tion time [12]. One non-parametric modification of SpAM to re place the additive model has been not desirable for our execution time prediction problem. In stead, we propose the SPORE method-ology and propose efficient algorithms to train a SPORE model . Our work provides a promising example of interpretable non-linear sparse regression mod els in solving real data problems. review the problem within which we apply these techniques to provide context [7]. Our goal is to files and command-line parameters). The system consists of f our steps.
 to extract values of program features such as loop counts (how many times a particular loop has executed), branch counts (how many times each branch of a conditional has executed), a nd variable values (the k first values assigned to a numerical variable, for some small k such as 5 ). for all created program features and the program X  X  executio n times. The time impact of the data collection is minimal.
 therefore possibly valuable in a predictive model.
 costs computed during slicing to build a predictive model on a small subset of generated features. computed using the corresponding slices, and the model is us ed to predict execution time from the feature values.
 The above description is minimal by necessity due to space co nstraints, and omits details on the Though important, those details have no bearing in the resul ts shown in this paper. At present our system targets a fixed, overprovisioned compu tation environment without CPU job contention or network bandwidth fluctuations. We therefore assume that execution times observed during training will be consistent with system behavior on-line. Our approach can adapt to modest and software environment (e.g., OS, cache policy, etc) for p redictive model construction. plain the execution time well. In other words, we seek a compa ct model X  X n explicit form function of a small number of features X  X hat accurately estimates the execution time of the program. some (combination of) features. Second, a polynomial model up to certain degree can approximate well many nonlinear models (due to Taylor expansion). Final ly, a compact polynomial model can providing program developers with intuitive feedback and a solid basis for analysis. as tuples of { y modeling the relationship between Y = [ y proceed to our SPORE methodology. 3.1 Sparse Regression and Alternatives y by minimizing the sum of the squares of the residuals [14]. Re gression with subset selection squares. However, it is a combinatorial optimization and is known to be NP-hard [14]. In recent years a number of efficient alternatives based on model regul arization have been proposed. Among them, LASSO [25] finds the selected features with coefficient s  X   X  given a tuning parameter  X  as follows: LASSO effectively enforces many  X  non-zero  X  complexity of the model: as  X  grows larger, fewer features are selected.
 Being a convex optimization problem is an important advanta ge of the LASSO method since several Furthermore, LASSO has convenient theoretical and empiric al properties. Under suitable assump-defined below in Equation (2) can overcome this problem where w where I is the identity matrix.
 Technically LASSO can be easily extended to create nonlinea r models (e.g., using polynomial basis tionally expensive. We give two alternatives to fit the spars e polynomial regression model next. 3.2 SPORE Methodology and Two Algorithms Our methodology captures non-linear effects of features X  X  s well as non-linear interactions among nomial basis functions subsequently). We expand the featur e set x = { x all the terms in the expansion of the degree-d polynomial (1 + x as the mapping from the original data matrix X to a new matrix with the polynomial expansion terms up to degree d as the columns. For example, using a degree-2 polynomial with feature set x basis functions to construct the following function for reg ression: Complete expansion on all p features is not necessary, because many of them have little c ontri-bution to the execution time. Motivated by this execution ti me application, we propose a general methodology called SPORE which is a sparse polynomial regre ssion technique. Next, we develop two algorithms to fit our SPORE methodology. 3.2.1 SPORE-LASSO: A Two-Step Method For a sparse polynomial model with only a few features, if we c an preselect a small number of features, applying the LASSO on the polynomial expansion of those preselected features will still be efficient, because we do not have too many polynomial terms . Here is the idea: Step 1: Use the linear LASSO algorithm to select a small number of fea tures and filter out (often many) features that hardly have contributions to the execut ion time.
 Step 2: Use the adaptive-LASSO method on the expanded polynomial te rms of the selected features (from Step 1) to construct the sparse polynomial model.
 Adaptive-LASSO is used in Step 2 because of the collinearity of the expanded polynomial features. Step 2 can be computed efficiently if we only choose a small num ber of features in Step 1. We present the resulting SPORE-LASSO algorithm in Algorithm 1 below.
 Algorithm 1 SPORE-LASSO Input: response Y , feature data X , maximum degree d ,  X  Output: Feature index S , term index S 1:  X   X  = arg min  X  1 2: S = { j :  X   X  j 6 = 0 } 3: X 4: w = ( X T 5:  X   X  = arg min  X  1 6: S X ( S ) in Step 3 of Algorithm 1 is a sub-matrix of X containing only columns from X indexed by S . For a new observation with feature vector X = [ x vector X ( S ) , then obtain the polynomial terms X the prediction:  X  Y = X maximum degree d . In this paper, we fix d = 3 .  X  2 s , where  X  Y is the fitted Y and s is the number of polynomial terms selected in the model. To be  X  a solution path with varied  X   X  , we calculate the AIC, and choose the (  X  One may wonder whether Step 1 incorrectly discards features required for building a good model n, p and s can also be obtained.
 Under the following conditions, we show that Step 1 of SPORE-LASSO, the linear LASSO, selects the relevant features even if the response Y depends on predictors X ( S ) nonlinearly: where  X  and the inequalities are defined element-wise.
 Theorem 3.1. Under the conditions above, with probability  X  1 as n  X   X  , there exists some  X  , such that  X   X  = (  X   X   X   X  j 6 = 0 , Remark. The first two conditions are trivial: Condition 1 can be obtai ned by rescaling while Con-Condition 3 is a reasonable condition which means that the li near projection of the expected re-it says that the irrelevant predictors ( X its projection onto X consistency [26, 28]. The proof of the theorem is included in the supplementary material. 3.2.2 Adaptive Forward-Backward: SPORE-FoBa the SPORE-FoBa algorithm, a more flexible algorithm using ad aptive forward-backward searching new feature X by S ) and candidate feature X Algorithm 2 below is a short description of the SPORE-FoBa, w hich uses linear FoBa [27] at step 5and 6. The main idea of SPORE-FoBa is that a term from the cand idate set is added into the model currently selected terms by the SPORE-FoBa algorithm. When considering deleting one term from is small enough, we delete that term from our current active s et.
 Algorithm 2 SPORE-FoBa Input: response Y , feature columns X Output: polynomial terms and the weights 1: Let T =  X  2: while true do 3: for j = 1 , . . . , p do 4: Let C be the candidate set that contains non-linear and interacti on terms from Equation (3) 5: Use Linear FoBa to select terms from C to form the new active set T . 6: Use Linear FoBa to delete terms from T to form a new active set T . 7: if no terms can be added or deleted then 8: break We now experimentally demonstrate that our algorithms are p ractical, give highly accurate predic-the-art sparse-regression algorithms, and produce interp retable, intuitive models. To evaluate our algorithms, we use as case studies three prog rams: the Lucene Search Engine [4], and two image processing algorithms, one for finding maxima a nd one for segmenting an image (both of which are implemented within the ImageJ image proce ssing framework [3]). We chose all three programs according to two criteria. First and most importantly, we sought programs with that implement reasonably complex functionality, for whic h an inexperienced observer would not be able to trivially identify the important features.
 two corpora: the works of Shakespeare and the King James Bibl e. We collected a data set with the Find Maxima program within the ImageJ framework, we coll ected n = 3045 samples (from an 0.24. Finally, from the Segmentation program within the sam e ImageJ framework on the same image d = 3 for polynomial expansion, and normalized each column of fea ture data into range [0 , 1] . Prediction Error. We first show that our algorithms predict accurately, even wh en training on a small number of samples, in both absolute and relative terms . The accuracy measure we use is the and y we repeat the  X  X plitting, training and testing X  procedure 1 0 times and show the mean and standard both of our algorithms can achieve less than 7% prediction error on both Lucene and Find Maxima datasets; on the segmentation dataset, SPORE-FoBa achieve s less than 8% prediction error, and SPORE-LASSO achieves around 10% prediction error on average.
 Comparisons to State-of-the-Art. We compare our algorithms to several existing sparse regres sion in the model), and show our algorithms can clearly outperfor m LASSO, FoBa and recently proposed non-parametric greedy methods [18] (Figure 2). As a non-par ametric greedy algorithm, we use Ad-than Generalized Forward Regression (GFR) algorithms. We u se the Glmnet Matlab implementa-tion of LASSO and to obtain the LASSO solution path [10]. Sinc e FoBa and SPORE-FoBa naturally the solution path for SPORE-LASSO, we first use Glmnet to gene rate a solution path for linear LASSO; then at each sparsity level k , we perform full polynomial expansion with d = 3 on the selected k features, obtain a solution path on the expanded data, and ch oose the model with the figure, we see that our SPORE algorithms have comparable perf ormance, and both of them clearly achieve better prediction accuracy than LASSO, FoBa, and AF R. None of the existing methods can computer program often depends on non-linear combinations of different features, which is usually our algorithms can select 2-3 high-quality features and bui ld models with non-linear combinations of them to predict execution time with high accuracy. Model Interpretability. To gain better understanding, we investigate the details of the model con-structed by SPORE-FoBa for Find Maxima. Our conclusions are similar for the other case studies, sparsity configurations, SPORE-FoBa can always select two h igh-quality features from hundreds of automatically generated ones. By consulting with experts o f the Find Maxima program, we find that image, which may in practice differ from the actual image wid th and height. Those are indeed the 10% training set fraction and  X  = 0 . 01 , SPORE-FoBa obtained 5.5% prediction error). Especially when Find Maxima is used as a component of a more complex image processing pipeline, this model would not be the most o bvious choice even an expert would methods handle well such nonlinear terms, and result in infe rior prediction performance. A more detailed comparison across different methods is the subjec t of our on-going work. In this paper, we proposed the SPORE (Sparse POlynomial REgr ession) methodology to build the relationship between execution time of computer programs a nd features of the programs. We in-troduced two algorithms to learn a SPORE model, and showed th at both algorithms can predict other sparse modeling techniques in the literature when app lied to this problem. Hence our work problems. Moreover, the SPORE methodology is a general meth odology that can be used to model computer program performance metrics other than execution time and solve problems from other areas of science and engineering.
