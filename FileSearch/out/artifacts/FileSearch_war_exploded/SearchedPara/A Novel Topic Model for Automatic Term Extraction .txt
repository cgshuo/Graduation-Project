 Automatic term extraction (ATE) aims at extracting domain-specific terms from a corpus of a certain domain. Termhood is one essential measure for judging whethe r a phrase is a term. Previous researches on termhood mainly depend on the word frequency information. In this pape r, we propose to compute termhood based on semantic representation of words. A novel topic model, namely i-SWB, is developed to map the domain corpus into a latent semantic space, which is composed of some general topics, a background topic and a documents -specific topic. Experiments on four domains demonstrate that our approach outperforms the state-of-the-art ATE approaches. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  linguistic processing, Thesauruses. Algorithms, Experimentation. Term Extraction, Topic Model, Termhood. So far, most researches on automatic term extraction have been guided by two essential measures defined by [6], namely unithood and termhood . Unithood examines syntactic formation of terms or the degree (or significance) of the association among the term constituents. Termhood , on the other hand, aims to capture the semantic relatedness of a term to a domain concept. However, there is no uniform definition of what is semantic relatedness, and how to compute termhood is still an open problem. Previous researches have attempted to measure termhood by applying several statistical meas ures within a domain or across domains, such as TF-IDF, C-value/NC-value [5], co-occurrence [4] and inter-domain entropy [2]. These statistical measures often ignore the informative words with very high frequency or very low frequency and do not take into account the semantics carried by terms. Taking the term  X  NRZ electrical input  X  in the electric engineering domain for example,  X  NRZ  X  only occurs in a few documents while  X  electrical  X  occurs in many documents frequently. Using TF-IDF to measure termhood , low scores will be assigned to both  X  NRZ  X  and  X  electrical  X , which in turn causes the term  X  NRZ electrical input  X  to have a low termhood . It is obvious that frequency-based measures will keep many real terms out of the door. In fact, a domain is described semantically from various aspects. Again, let X  X  take the electric engineering domain for example. The words like  X  input  X  emphasize some specific topic in a domain while the words like  X  electrical  X  provide the background of that domain. There also exist a cluster of words like  X  NRZ  X  which occur in the corpus infrequently, but tend to occur in a few documents frequently. Such words can reflect some special characteristics of the domain. Based on these observations, we argue that three semantic aspects can be used in the representation of words: Domain background words (e.g. electrical ) describe the domain in general. Domain topic words (e.g. input ) represent a certain topic in a given domain. Domain documents-specific words (e.g. NRZ ) are specific to a small number of documents and exhib it the characteristics of the domain. We assume that a term can be recognized by identifying whether its constituent words belong to some of the three semantic aspects. As for semantic representation of words, unsupervised topic models have shown their advantag es [1] [3]. Latent Dirichlet Allocation (LDA) is a well-known example of such models. It posits that each document can be seen as a mixture of latent topics and each topic as the distribution over a given vocabulary. To trade-off generality and specificity of words, Chemudugunta et al. [3] further defined the special words with background (SWB) model that allowed words to be modeled as originating from general topics, or document-speci fic topics, or a corpus-wide background topic. The existing work proves that topic models are competent for the semantic representation of words. However, to our knowledge, no prior work has introduced such kind of semantic representation to term extraction. Inspired by Chemudugunta X  X  idea of generality and specificity [3], in this paper we propose a novel topic model, namely i-SWB to model the three suggested seman tic aspects. In i-SWB, three kinds of topics, namely bac kground topic , general topics , and documents-specific topic are correspondingly constructed to generate the words in a domain corpus. Compared with Chemudugunta X  X  SWB model, there are two main improvements in i-SWB to tailor to term extraction. First, specificity in i-SWB is modeled at the corpus level and one documents-specific topic is set to identify a cluster of idiosyncratic words from the whole corpus. Thus, i-SWB avoids th e computationally intensive problem in SWB where the numbe r of document-specific topics grows linearly with the number of documents. Second, i-SWB makes use of both document frequency (DF) and topic information to control the generation of words, while SWB only uses a simple multinomial variable to control which topic a word is generated from. This impr ovement comes from the following findings that have been verified in the experiments: the words occurring in many documents and distributing over many general topics with higher probability in LDA are likely to present background information, while the words occurring in a few documents only and distributing over a certain topic in LDA with medium or low probabilities are usually idiosyncratic and provide some special information of the domain. Next, with the semantic representation of words in i-SWB, we implement an ATE system which outperforms the existing ATE approaches. The hierarchical Bayesian LDA models the probability of a corpus on hidden topics as in Fig. 1(a). The topic distribution of each document  X  d is drawn from a prior Dirichlet distribution Dir (  X  ), and each document word w d,n is sampled from a topic-word distribution z  X  specified by a drawn from the topic-document distribution  X  d . The topic assignments z for each word in the corpus can be efficiently sampled via Gibbs sampling, and the predictive distributions for  X  and  X  can be computed by averaging over multiple samples. To formulate background and special information, Chemudugunta et al. (2006) proposed the SWB model as illustrated in Fig. 1(b). In SWB, the variable u d , n acts as a switch: if u word is generated by the general topics as in LDA, whereas if u multinomial or a document-specific multinomial (with symmetric Dirichlet priors  X  1 and  X  2 ) respectively. u d , n document-specific multinomial  X  d , and it has a symmetric Dirichlet prior  X  . Applying a Gibbs sampler on SWB, we can get the sampling equations for each word w d,n in document d :  X  X  X   X  X  X  where  X ( d , n ) indicates that the current word w the count, V denotes the vocabulary size, N d is the number of the words in document d and N d 0 , N d 1 , and N d 2 are the number of the words in document d assigned to the latent topics, background words that are assigned topic t in document d . WT wt are the number of the words that w d,n is assigned to topic t , to the background topic and to the documents -specific topic respectively. In i-SWB, the documents-specific topic is defined at the corpus including background topic, documen ts-specific topic and general topics. To control the generation of these topics, a simple way is to set a variable (e.g.  X  d in SWB) which is drawn from a symmetric Dirichlet prior. As a rule of thumb, document frequency (DF) information can be us ed as a determining factor to examine the specificity or generality of words related to a domain. In addition, we use the word di stribution in general topics as another factor to determine the specificity and generality of words. Fig. 2 and 3 show the graphic m odel and the generation process of i-SWB. Instead of  X  d , a three-dimensional vector control topic generation and its value is determined by an experience function document frequency of the word w d , n and probability vector that the word w d , n distributes over all the general topics. To formally present the i-SWB model, let V be the vocabulary size and D be the number of documents. There are T general topics (1 ) t tT  X   X   X  , one background topic B  X  documents-specific topic D  X  which have symmetric Dirichlet priors of  X  0 ,  X  1 and  X  2 respectively. Each topic is characterized by a distribution over the V words.  X  is the fixed parameter of symmetric Dirichlet prior for the D document-topic multinomials represented by a D  X  T matrix  X  . Let w d , n be the observed variable representing the n th word in document d , u d , n denoting which kind of topic w d , n is assigned to, and z hidden variable indicating that the general topic w assigned to. We use a Gibbs sampler to perform model inference. Due to space limitation, we give the result of Gibbs Sampling directly. Eqs. (4), (5) and (6) are similar to Eqs. (1), (2) and (3), except the first terms in each of them. Then, we determine value depends on where that w d , n is assigned to each general topic. where the general topic t . In Eq. (8), the numerator computes the entropy that the word w d , n is assigned to the general topics, and the denominator denotes the maximum entropy value that w evenly assigned to each general topic. The range of (0,1]. The experience Equations (7) and (8) reflect that a background word is more likely to uniformly distribute on general topics and a documents-specific wo rd is more likely to have a larger DF value. With one Gibbs samp ling, we can also make the following estimation: In our experiments, we set T =20,  X  =50/ T ,  X  0 =0.1,  X  =0.01. We initialize the corpus by sampling each word from the general topics and run 1000 iterati ons to stabilize the distribution of z and u . This section will introduce the proposed i-SWB based term extraction. As stated in Secti on 1, the ATE process is usually guided by two measures: unithood for acquiring term candidates and termhood for further identifying terms from the candidates. Following the work of Frantzi [5], the unithood technique adopted in our work is mainly based on a linguistic filter as the following three steps: Step 1 : Part-of-speech (POS) tagging: We use a Maximum-entropy POS tagger 1 implemented by Standford NLP group to assign a grammatical tag (e.g. noun, verb, adjective etc.) to each word in the corpus. Step 2 : Linguistic filtering: Since most terms are noun phrases which consist of nouns, adjectives and some variants of verbs and end with a noun word, here we present our method with the filter: unknown words. Step 3 : Frequency recording: for each term candidate, we record its frequency occurring in the w hole corpus and exclude those occurring only once from the candidate list. Now we get a list of term candidates with their frequency { c Suppose each candidate c i consists of L i words compute termhood , each candidate is scored according to tf the results of i-SWB model. According to the values of , and D  X  in Eqs. (9), we extract the top H (e.g. 200) highest distributed words for each topic, namely V t , V B and V be seen as the typical words of the corresponding topic. An intuitive idea is that, a good candi date should be composed of typical words which are representative of a certain topic. Thus, the termhood of c i is computed as: where assigned. To compute c i , we only consider those constituent words that are typical of a certain topic. Then the candidates with the highest termhood values are taken as terms. We evaluate the i-SWB based term extraction method on four domain specific patent corpora, including molecular biology (C12N) , metallurgy(C22C) , electric engineering(G01C) and mechanical engineering(H03M) . Statistics of the documents in each domain are summarized in Table 1. Corpus Domain No. of words No. of documents Corpus C12 N molecular biology 1,880,739 11,496 Corpus G01 C mechanical engine. 1,741,820 9,462 Corpus H 03M electric engine. 935,059 5,492 necessary for evaluation. Inspired by the pooling technique used in Information Retrieval (IR), we semi-automatically construct a lexicon for each domain. For each domain, every baseline system and our system submits one term list. The five baseline systems used will be introduced in the ne xt subsections. We take the top 500 terms from every system to fo rm the pool for that domain. Then from the pool, four graduate students majoring in the corresponding domain manually pick out the correct ones to form a pseudo-lexicon for each domain. The sizes of the pseudo-lexicons are 1473, 1380, 1509 and 1370 for molecular biology, metallurgy, mechanical engineering and electric engineering. Then, we compare system perfo rmances with the popular IR evaluation measures -precision at 6 different cut-off values P@ n , where n =50, 100, 200, 300, 400, 500. The i-SWB model is the key of our ATE system and we compare it with two commonly used topic models, i.e. SWB ( Baseline 1 ) and LDA ( Baseline 2 ). Except the construction of topic model, the whole process of term extracti on is the same as introduced in Section 3. Fig. 4 below illustrate s the P@n values of 3 different topic models on four domains. Th e blue bars represent the P@n values of our system, the red ba rs represent the SWB model, and the green bars represent the LDA model. From the figure, we can see that our model significantly outperforms the other two models. Taking the P@50 measure for exam ple, the relative improvement of i-SWB over SWB in the domains of molecular biology , metallurgy , mechanical engineering and electric engineering reaches respectively 12.2%, 11.9%, 9.8% and 11.4% respectively. When further analyzing the terms that are wrongly identified by each system, we find that if SWB is applied, the word  X  signal  X  reflecting some general topic is assigned to some document-specific topics due to its high frequency in some documents and then the phrases such as  X  X rigi nal signal X  and  X  X esulting signal X  are wrongly identified as terms. On the other hand, if LDA is used, some general words such as  X  purpose  X  and  X  problem  X  which occur frequently in the corpus are assigned to specific topics and the phrases of  X  signal purpose  X  and  X  problem solving  X  are wrongly identified as terms. Th ese kinds of problems can be overcome in i-SWB. However, the terms identified by i-SWB that are formed by sequences of several typical words are not always the real terms. For example,  X  signal  X  and  X  component  X  are both correctly assigned to general topics with higher probability but  X  signal component  X  is not a real term. To solve this problem, semantic representation of terms is worth exploring. We also compare our system with three state-of-the-art ATE systems. We use two online free systems -TerMine TermoStat 3 , as Baseline 3 and Baseline 4 . TerMine uses the C-value/ NC-value to compute termhood , while TermoStat is based domain corpus being analyzed. In addition, we implement a baseline system ( Baseline 5 ) which adopts TF-IDF in termhood computation. Fig. 5 compares our system with the three baseline systems over the P@n values and shows that our system obviously performs better than the other three. This verifies that the semantic analysis of word s are helpful to the ATE task. TerMine performs slightly better than TermoStat. But the TF-IDF technique performs unstably: better than TerMine in the metallurgy domain, worse than TerMine and TermoStat in the domain of mechanical engineeri ng. This may suggest that the performance of TF-IDF heavily depends on the corpus quality. In this paper, we argue that the termhood measure should take consideration of semantic informa tion. To cater to this idea, we design a novel topic model i-SWB to map the domain corpus into the latent semantic space, which includes general topics, background topic and documents-sp ecific topic. Based on i-SWB, we implement our ATE system a nd evaluate it on four domains (i.e. molecular biology , metallurgy , mechanical engineering , and electric engineering ). The experimental results show that our approach outperforms the state-of-the-art ATE approaches. This research has been supported by NSFC grants (No. 61273278 and 61272291), National Key Technology R&amp;D Program (No:2011BAH1B0403), National 863 Program (No. 2012AA011101) and National Social Science Foundation (No: 12&amp;ZD227),. We also thank the anonymous reviewers for their helpful comments. Corresponding au thors: Sujian Li and Baobao Chang. [1] Blei, D. M., Ng, A. Y., Jordan, M.I. 2003. Latent Dirichlet [2] Chang J.-S. 2005. Domain specific word extraction from [3] Chemudugunta, C., Smyth, P. and Steyvers, M. 2006. [4] Hisamitsu, T., Niwa, Y., and Tsujii, J. 2000. A method of [5] Frantzi, K., Ananiadou, S. and Mima, H. 2000. Automatic [6] Kageura, K. and Umino, B. 1996. Methods of automatic term 
