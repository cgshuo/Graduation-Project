 Correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality. Several algorithms for this task have been proposed recently. However, all algorithms only compute the partitioning of the data into clusters. This is only a first step in the pipeline of advanced data analysis and system modelling. The sec-ond (post-clustering) step of deriving a quantitative model for each correlation cluster has not been addressed so far. In this paper, we describe an original approach to handle this second step. We introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering. Our concepts are indepen-dent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm. Furthermore, we show how these quantitative models can be used to predict the probability distribution that an object is created by these models. Our broad experimental evalu-ation demonstrates the beneficial impact of our method on several applications of significant practical importance. I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Algorithms Data mining, clustering, correlation clustering, cluster de-scription, cluster model Partly supported by the German Ministry for Education, Science, Research and Technology (BMBF) under grant no. 031U212F within the BFAM project.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
The detection of correlations between different features in a given data set is a very important data mining task. High correlation of features may result in a high degree of collinearity or even a perfect one, corresponding to approx-imate linear dependencies between two or more attributes. These dependencies can be arbitrarily compex, one or more features might depend on a combination of several other fea-tures. In the data space, dependencies of features are man-ifested as lines, planes, or, generally speaking, hyperplanes exhibiting a relatively high density of data points compared to the surrounding space. Knowing of correlations is tradi-tionally used to reduce the dimensionality of the data set by eliminating redundant features. However, detection of cor-related features may also help to reveal hidden causalities that are of great interest to the domain expert.

Recently, correlation clustering [6] has been introduced as a novel concept of knowledge discovery in databases to detect dependencies among features and to cluster those points that share a common pattern of dependencies. It corresponds to the marriage of two widespread ideas: First, correlation analysis performed e.g. by principle component analysis (PCA) and, second, clustering which aims at identi-fying local subgroups of data objects sharing high similarity. Correlation clustering groups the data set into subsets called correlation clusters such that the objects in the same cor-relation cluster are all associated to a common hyperplane of arbitrary dimensionality. In addition, many algorithms for correlation cluster analysis also require the objects of a cluster to exhibit a certain density, i.e. feature similarity.
Correlation clustering has been successfully applied to several application domains (see e.g. [3, 24, 6]). For exam-ple, costumer recommendation systems are important tools for target marketing. For the purpose of data analysis for recommendation systems, it is important to find homoge-neous groups of users with similar ratings in subsets of the attributes. In addition, it is interesting to find groups of users with correlated affinities. This knowledge can help companies to predict customer behavior and thus develop future marketing plans. In molecular biology, correlation clustering is an important method for the analysis of several types of data. In metabolic screening, e.g., the collected data usually contain the concentrations of certain metabo-lites in the blood of thousands of patients. In such data sets, it is important to find homogeneous groups of patients with correlated metabolite concentrations indicating a common metabolic disease. Thus, several metabolites can be linearly dependent on several other metabolites. Uncovering these patterns and extracting the dependencies of these clusters is a key step towards understanding metabolic or genetic disorders and designing individual drugs. A second exam-ple where correlation clustering is a sound methodology for data analysis in molecular biology is DNA microarray data analysis. Microarray data comprise the expression levels of thousands of genes in different samples such as experi-mental conditions, cells or organisms. Roughly speaking, the expression level of a gene indicates how active this gene is. The recovering of dependencies among different genes in certain conditions is an important step towards a more com-prehensive understanding of the functionality of organisms which is a prominent aspect of systems biology. When the samples represent some patients, it is important to detect homogeneous groups of persons exhibiting a common linear dependency among a subset of genes in order to determine potential pathological subtypes of diseases and to develop individual treatments.

In all these cases, however, knowing merely of the ex-istence of correlations among some features is just a first step. It is far more important to reveal quantitatively and as exactly as possible which features contribute to which de-pendencies as a second step. Having performed this second step, modelling a system becomes possible, that describes the respective underlying data quantitatively as well as qual-itatively. Thus, in order to gain the full practical potentials from correlation cluster analysis, this second step is urgently needed. All existing approaches to correlation clustering usually focus only on the first step of detecting the clus-ters. To the best of our knowledge, there is no method for the second step of extracting quantitative correlation cluster information.

In this paper, we describe an approach to handle this sec-ond step of data analysis. We introduce general concepts for extracting quantitative information on the linear dependen-cies within a correlation cluster such that domain experts are able to understand the correlations and dependencies in their data. In fact, our method can be applied to any correlation clusters, regardles s of what correlation cluster-ing algorithm produced the results. As output, we obtain a set of linear equations that are displayed to the user. These equations can be used to understand the dependencies hid-den in the analyzed data set and to create complex real-life models. As an example, how this information can be used for further analysis, we additionally introduce a framework to predict the probability that a new object is generated by a specific model of the derived ones.
 The remainder of this paper is organized as follows. In Section 2 we review related wor k on correlation clustering and existing approaches for deriving descriptions of quanti-tative dependencies among several attributes. Our concepts to derive quantitative models of correlation clusters are pro-posed in Section 3. Section 4 presents a broad experimental evaluation where we demonstrate the practical importance of our approach. The paper concludes in Section 5.
The expectation maximization (EM) [8] is one of the first clustering algorithms that can detect correlation clusters. The EM algorithm tries to model the data distribution of a data set using a mixture of non-axis parallel Gaussian distributions. Let us note that the EM algorithm cannot distinguish between correlation clusters and full-dimensional clusters without any correlation.

ORCLUS [3] is a k -means style correlation clustering al-gorithm and, thus, can be seen as a specialization of the EM algorithm that detects only correlation clusters. The correlation clusters are allowed to exist in arbitrarily ori-ented subspaces represented by a set of Eigenvectors. The number of clusters k and the average dimensionality l of the correlation clusters are the input parameters of ORCLUS.
In [6] the algorithm 4C, a combination of DBSCAN [9] and PCA, is presented to find correlation clusters. The user must specify several parameters, including:  X  and  X  , defin-ing minimum density of a cluster, a threshold  X  to decide which principal axes of a cluster are relevant for the correla-tion, and the dimensionality  X  of the computed correlation clusters.

Let us note that none of the proposed approaches to cor-relation clustering provides a cluster model including an ex-plicit description of the correlations within the cluster.
CURLER [20]aims at detecting arbitrary, non-linear cor-relations. It uses the concept of micro-clusters that are gen-erated using an EM variant and then are merged to uncover correlation clusters. The correlations underlying the found clusters are not necessarily linear. Furthermore, CURLER assumes each data object to belong to all clusters simul-taneously, but with different probabilities for each cluster assigned. By merging several clusters according to their co-sharing level, the algorithm on the one hand becomes less sensitive to the predefined number of clusters k .Onthe other hand, the user becomes disabled to directly derive a model describing the correlations, since the original k models are no longer persistent in the resulting clustering. However, we focus on linear correlations between features. Thus, the non-linear correlations uncovered by CURLER are orthogo-nal to our approach.

Recently, several subspace clustering algorithms [4, 2, 14, 17, 5] were proposed to find clusters in axis-parallel projec-tions of the data space. These algorithms are not able to capture local data correlations and find clusters of corre-lated objects since the principal axes of correlated data are arbitrarily oriented.

Pattern-based clustering methods [24, 21, 16, 15] aim at finding groups of objects that exhibit a similar trend in a subset of attributes. This problem is also known as co-clustering or biclustering [12, 7]. In contrast to correlation clustering, pattern-based clustering limits itself to a very special form of correlation where all attributes are positively correlated. It does not include negative correlations or cor-relations where one attribute is determined by two or more other attributes. Thus, bi-clustering or pattern-based clus-tering could be regarded as a special case of correlation clus-tering, as more extensively discussed in [6].
An interesting approach to derive descriptive models of quantitative relationships among subsets of attributes is known as quantitative association rule mining. Some earlier ap-proaches to this task loose information requiring discretiza-tion of attributes (e.g. [19]) or representation of numerical values in a rule X  X  right-hand side by some statistical charac-terizations, e.g. the mean or sum of the values (cf. [22]). Dis-cretization of attributes, moreover, does not overcome the restriction to axis parallel dependencies. Recently, R  X  uckert et al. [18] proposed to base quantitative association rules on half-spaces, thus allowing the discovery of non-axis-parallel rules and possibly accounting for cumulative effects of sev-eral variables. The rules derived by this approach are of the form  X  X f the weighted sum of some variables is greater than a threshold, then a different weighted sum of variables is with high probability greater than a second threshold X . This ap-proach has been shown to be useful in detecting some rules of gene-expression data sets [10]. However, these association rules do not yet uncover continuous linear dependencies, but stick to certain thresholds, reflecting the boundaries of half-spaces.
A task very similar to the one tackled in this paper is linear and multiple regression analysis (e.g. cf. [11] for an overview). The general purpose of linear regression is to learn a linear relationship between a  X  X redictor X  variable and a  X  X esponse X  variable. Multiple regression extends this task by allowing multiple  X  X redictor X  variables. Other non-linear regression models can be used to learn non-linear re-lationships among the predictor and the response variables. However, the main difference between regression analysis and our approach is that in regression analysis, the predictor variables are assumed to be i ndependent. Since correlation clusters are defined to consist of points that exhibit a linear dependency among a set of attributes, we want to identify these dependencies when deriving a quantitative model for each cluster. Obviously, we cannot define any independent variable(s), i.e. we cannot derive a set of predictor variables. Thus, regression analysis cannot be applied to derive quan-titative models for correlation clusters as envisioned in this paper.
In the following we assume D to be a database of n feature vectors in a d -dimensional real-valued feature space, i.e. . A cluster is a subset of those feature vectors exhibiting certain properties, e.g. th e members of a cluster may be close to each other in the feature space compared to non-members, or  X  in case of correlation clustering  X  they may be close to a common regression line, while other points are not. Generally, clustering algorithms as those reviewed above can provide (implicitely or explicitely) a description of the found clusters by means of a covariance matrix per cluster.

Formally, let C be a cluster, i.e. C X  X  ,and  X  x C denote the centroid (mean) of all points x  X  X  .The covariance matrix  X  C of C is defined as:
In general, the covariance matrix describes a distribution of attributes. EM-like algorithms utilize such a description of a distribution of attributes to derive a Gaussian model that may have created the observed data. In case of cor-relation clusters, however, a far more adequate description may be possible. Indeed, the fact, that correlations between features have been found, even disqualifies the covariance matrix as an adequate model of a correlation cluster, since it is sort of a probabilistic model of scatter around a certain mean value. Strong correlation s as in correlation clusters, on the other hand, do suggest not only probabilistic scat-ter, but linear dependencies, and (by a higher perspective of interpretation) perhaps even functional or causal relations. Thus, we will now consider the intrinsic properties of cor-relation clusters, and how to make use of them in order to derive a more appropriate model covering dependencies quantitatively.
Consider a correlation cluster C that is derived using any algorithm capable of finding correlation clusters. Since the covariance matrix  X  C of C is a square matrix, it can be decomposed into the Eigenvalue matrix E C of  X  C and the Eigenvector matrix V C of  X  C such that
The Eigenvalue matrix E C is a diagonal matrix holding the Eigenvalues of  X  C in decreasing order in its diagonal elements. The Eigenvector matrix V C is an orthonormal matrix with the corresponding Eigenvectors of  X  C .
Now we define the correlation dimensionality of C as the number of dimensions of the (arbitrarily oriented) subspace which is spanned by the major axes in V C . Let us note, that the correlation dimensionality is closely related to the intrinsic dimensionality of the data distribution. If, for in-stance, the points in C are located near a common line, the correlation dimensionality of these points will be 1. This means that we have to determine the principal components (Eigenvectors) of the points in C . The Eigenvector asso-ciated with the largest Eigenvalue has the same direction as the first principal component, the Eigenvector associated with the second largest Eigenvalue determines the direction of the second principal component and so on. The sum of the Eigenvalues equals the trace of the square matrix  X  C which is the total variance of the points in C .Thus,the obtained Eigenvalues are equal to the variance explained by each of the principal components, in decreasing order of importance. The correlation dimensionality of a set of points C is now defined as the smallest number of Eigenvec-tors explaining a portion of at least  X  of the total variance of
C . These ideas are illustrated in Figure 1. Figure 1(a) shows a correlation cluster of correlation dimensionality 1 corresponding to a (perfect) line. Only one Eigenvector ( e explains the total variance of C . Figure 1(b) shows a correla-tion cluster of correlation dimensionality 2 that corresponds to a (perfect) plane. Here, two Eigenvectors explain the total variance of C . Let us note that in the displayed exam-ples, the correlations are perfect, i.e. there is no deviation from the hyperplane but all points within the set perfectly fit to the hyperplane. However, in real-world data sets, this is a quite unrealistic scenario. A threshold  X  may account for that fuzziness to define an adequate dimensionality of the correlation hyperplane. We call the dimensionality of a hyperplane neglecting a certain amount of deviation in or-thogonal direction correlation dimensionality . The correla-tion dimensionality is defined more formally in the following. Definition 1 (correlation dimensionality).
 Let  X   X  ]0 , 1[ . Then the correlation dimensionality  X  C set of points C is the smallest number r of Eigenvalues e the d  X  d Eigenvalue matrix E C explaining a portion of at least  X  of the total variance: Typically, values for  X  are choosen between 0 . 8and0 . 9. For example,  X  =0 . 85 denotes that the obtained principal components explain 85% of the total variance. In the fol-lowing, we denote the  X  C -dimensional affine space which is spanned by the major axes of C , i.e. by the  X  C first Eigen-vectors of C and translated by, e.g. the mean vector  X  x correlation hyperplane of C .

Thus, the correlation dimensionality  X  C is the dimension-ality of the affine space containing all points of the set allowing a small deviation corresponding to the remaining portion of variance of 1  X   X  . The remaining, neglected vari-ance scatters along the Eigenvectors e  X  C +1 ,...,e d .
We therefore distinguish between two disjoint sets of Eigen-vectors: Definition 2 (strong and weak Eigenvectors).
 We call the first  X  C Eigenvectors of V C strong Eigenvectors . The strong Eigenvectors of V C are denoted by  X  V C .The remaining Eigenvectors are called weak Eigenvectors .We denote the weak Eigenvectors by  X  V C .

For an illustration see again Figure 1: In the correlation cluster of correlation dimensionality 1 (Figure 1(a)) e 1 a strong Eigenvector whereas e 2 and e 3 are weak Eigenvec-tors . In the correlation cluster of correlation dimensionality 2 (Figure 1(b)) e 1 and e 2 are strong Eigenvectors whereas e 3 is a weak Eigenvector . The Eigenvectors are overexem-plified in this example. Suppose they were scaled by their corresponding Eigenvalues. If no variance remains along an Eigenvector, as it may e.g. appear for e 2 and e 3 in Figure 1(a), this Eigenvector will disappear since the corresponding Eigenvalue becomes zero.
 While the correlation hyperplane is spanned by the strong Eigenvectors, it is equally well defined by the weak Eigen-vectors that are orthogonal to this hyperplane in R d .Fur-thermore, describing the correlation cluster by means of the weak Eigenvectors (instead of the strong Eigenvectors) di-rectly yields an equality system that defines not only the corresponding hyperplane, but also allows to directly inspect the underlying dependencies among attributes numerically, as we will show in more detail subsequently. Let C be a  X  -dimensional correlation cluster in D ( C X  X  ). Thus, there are  X  strong Eigenvectors and d  X   X  weak Eigen-vectors in the describing matrix of Eigenvectors derived by PCA on the points of cluster C .A  X  -dimensional hyperplane defining the correlation cluster C is therefore completely de-fined by the mean point (centroid)  X  x C =(  X  x 1  X  X  X   X  x points belonging to cluster C and the set of weak Eigenvec-tors,  X  V C , that are normal vectors to the hyperplane. Then we can derive the following equation system to describe the hyperplane, consisting of d  X   X  equations: where v i,j is the value at column i ,row j in the Eigenvector matrix V C of C . As we have pointed out, only the weak Eigenvectors are relevant. Thus we can equivalently denote this equation system by The defect of  X  V T C gives the number of free attributes, the other attributes may actually be involved in linear depen-dencies. Basically, these dependencies are revealed by trans-forming the equation system using Gauss-Jordan elimina-tion. The thus derived reduced row echelon form of the ma-trix is known to be unique [25]. The unique form does, of course, not provide new information, but it is easily compa-rable to alternative solutions and conveniently interpretable by inspecting experts. To enhance numerical stability, we suppose to use total pivoting for the Gauss-Jordan elimina-tion.

By construction, the equation system is  X  at least approx-imately  X  fulfilled for all points x  X  X  . But, furthermore, it suggests a quantitative model for the cluster. This model could be evaluated using retained data points. Besides, as we will see below, it may also serve as a predictive model to classify new data points.

In summary, we propose the fo llowing general method to derive quantitative models of clusters in a dataset of feature vectors D X  R d : 1. Run a clustering algorithm on D that is able to find 2. For each correlation cluster C i  X  X  found in the pre-
Suppose by applying this method we obtain the following solution describing a cluster in a 5-dimensional feature space : This would provide a quantitative model describing a corre-lation cluster of correlation dimensionality 2 (corresponding to the number of free attributes, or, equivalently, the number of strong Eigenvectors) where we have linear dependencies among by given factors c 1 , e 1 , c 2 , e 2 ,and e 3 .

Note that we must not draw any conclusions concerning causalities between attributes. But relations between cer-tain attributes are quantitatively and uniquely defined. To resolve these relations to any formula that suggests a causal-ity we have to rely on the domain knowledge of experts. However, we believe that uncovered quantitative relation-ships will lead to refined experiments and help to finally explore supposable causalities. Thus, we could choose ex-perimental settings involving either and changing the quantities in relation to each other. The dependencies revealed in the original experiment could have been interpreted such as fall or rise of an arbitrary subset of S  X  X  x 1 ,x 3 ,x 5 } caused fall or rise of the remaining sub-set { x 1 ,x 3 ,x 5 }\ S . Further experiments could refine the model by excluding certain combinations of causal models. Of course, the three variables, x 1 , x 3 ,and x 5 , may also sim-ply be connected by a fourth variable, that has not been monitored so far. Thus, trivially, a quantitative connection will never guarantee a direct causal relationship. Further-more, in many domains, one-way causal relationships pro-vide only one part of the whole picture, since systems often are regulated by negative-feedback-loops, that make causal-ities circular. Nevertheless, modelling parts of a complex system remains useful even under restrictive constraints (as shown e.g. for genetic regulatory interaction networks, cf. [13]).
Having derived a descriptive model, it can be refined by determining an average distance of the cluster members from the correlation hyperplane. Such deviations are typically to be expected in natural systems. At least, one has to account for errors in measurement. The distance of a point to a hyperplane is thereby naturally defined as the Euclidean distance to its perpendicular projection onto the hyperplane, i.e.: where C denotes the idealized hyperplane of a correlation cluster. By definition, the hyperplane C is an affine space, that is a subspace translated by  X  x C , the mean vector of all points of the cluster corresponding to C .proj S : R n  X  R n denotes the perpendicular projection of a vector to an ar-bitrary subspace S of R n .If S is given by an orthonormal basis, e.g. the set of strong Eigenvectors derived for the cor-responding correlation cluster, { s 1 ,  X  X  X  ,s  X  S } ,then
Assuming the deviations fit to a Gaussian distribution with  X  = 0, the standard deviation  X  of the distances of all cluster members suffices to define a Gaussian model of devi-ations from the common correlation hyperplane. For each of the derived models, the probability is given for a new data object to be generated by this specific Gaussian distribution. A set of models for a set of correlation clusters can there-fore provide a convenient instrument for classification in the perspective of different linear dependencies among the data. The probability that an object x was generated by the j th of n Gaussian distributions, C j ,isgivenby
Compared to many traditional classification algorithms, like SVM or kNN, our predictive models do not only provide a separating boundary between classes (cf. Figure 2(a)), but (a) Linear decision boundaries (c) Density functions (d) Deviations from hy-also give a meaningful definition of the class. So do other classifiers, like decision trees or rule based learners, but their descriptions usually are limited to (at least in sections) axis parallel decision boundaries (cf. Figure 2(b)). The models provided by the EM algorithm or other Bayesian learners differ from our models in that they simply define a scatter-ing around a mean point, using a quadratic form distance function or a density function for a certain probability dis-tribution (cf. Figure 2(c)). For underlying linear dependen-cies, a quadratic distance function will resemble our models only if the dependencies are perfectly expressed in the data without any aberrations. Accounting for some variance per-pendicular to a hyperplane, while the hyperplane represents a linear dependency among several attributes, is a novel ap-proach among the family of classification algorithms (cf. Fig-ure 2(d)).
In our experiments we use the correlation clustering algo-rithm COPAC [1] to generate t he correlation clusters in a preprocessing step to our method. We choose this algorithm due to its efficiency, effectivity, and robustness. In each case, parameters for clustering wer echosenaccordingto[1]. Let us again note that any other (correlation) clustering algo-rithm is applicable for preprocessing.
For our experiments we used several synthetic data sets containing correlation clusters in the unit cube of R d that have been generated by a generic data generator. The gen-erated correlation clusters form a  X  -dimensional hyperplane whichisspecifiedbyanequationsystemof d  X   X  equations. The distances of the points to the hyperplane are normally distributed with a specified standard deviation and a mean of zero.

The first data set DS1 consists of five correlation clusters, each forming a line of 1,000 points in R 3 (cf. Figure 4). In each cluster, the distances of the points to the correlation lines are normally distributed with a standard deviation of about 1.5% of the maximum distance in the unit cube. The purpose of this data set is to demonstrate the capability of our proposed method to obtain a quantitative model for the correlation clusters. As it can be seen in Table 1 we derived a good approximation of the equation systems that define the models for the correlation clusters despite the obviously strong jitter in the data set.

In the second experiment we evaluated our method on data sets with varying standard deviation. We generated six data sets (DS2 0 , ..., DS2 5 ) forming a 2-dimensional hyper-plane in R 3 with different values for the standard deviation of the distances. The values for the standard deviation were set to  X  0 =0%upto  X  5 = 5% of the maximum distance in the unit cube (cf. Figure 4). The results are shown in Table 2. As expected, with increasing standard deviation of the distances, the detected correlation models suffer from a slight blurring, i.e. the coefficients of the models slightly deviate from the exact coefficients. However, the general correlations are still detected and also the hidden quanti-tative relationships are still uncovered rather clear even if the points stronger deviate from the optimal hyperplane. In general, our proposed method has proven to be rather robust w.r.t. small jitter.

In addition to the reported experiments on 3-dimensional data, we performed several experiments on higher dimen-sional data. In all experiments, we achieved results of sim-ilar high quality, i.e. all linear dependencies hidden in the data were correctly uncovered. Due to space limitations and clearness reasons, we omit a detailed presentation of these results. =0 . 0173) (c) DS2 2 (  X  2 =0 . 0346) =0 . 0693) (f) DS2 5 (  X  5 =0 . 0866) Wages data. The Wages data set 1 consists of 534 11-dimensional observations from the 1985 Current Population Survey. Since most of the attributes are not numeric, we used only 4 dimensions ( A =age, YE =years of education, YW =years of work experience, and W =wage) for correla-tion analysis.

COPAC detected three correlation clusters in this data set. The resulting dependencies of these clusters are sum-marized in Table 3. The first cluster consists only of people having 12 years of education, whereas the second cluster consists only of people having 16 years of education. Fur-http://lib.stat.cmu.edu/datasets/CPS_85_Wages thermore, in both of these clusters the difference between age and work experience is a specific constant, namely years of education plus 6, which makes perfectly sense. Addition-ally, for the first cluster, we found a dependency between wage and age: the wage equals a constant plus a small fac-tor times the age of an employee, i.e., the older an employee, the more he earns. This relationship is independent from the attribute work experience. Note that years of education is a constant where this condition holds. In the third cluster only those employees are grouped which started school in the age of 6 years and after graduation immediately began working. Thus, the sum of years of education and work experience equals the age minus 6. Gene expression data. This data set was derived from an experimental study of apoptosis in human tumor cells 2 Apoptosis is a genetically controlled pathway of cell death. The data set contains the expression level of 4610 genes at five different time slots (5, 10, 15, 30, 60 minutes) after initiating the apoptosis pathway.
 We analyzed two correlation clusters detected by COPAC. The derived dependencies of these clusters are depicted in Table 4. The attributes are abbreviated by Mi ,where i de-notes the time slot of this attribute, e.g. M 5 denotes time slot  X 5 minutes X . The first cluster contains several genes that are located at the mitochondrial membrane. The first four time slots exhibit a negative linear relationship with M 60. Similar observations can be made for the second cluster that contains several genes that are related to the tumor necrosis factor (RNF). The uncovered dependencies suggest that the activity of the corresponding genes decrease with proceeding cell death. The strong negative correlations among genes related to mitochondria (cluster 1) indicates that the volume of the energy metabolism (which is located in mitochondria) is decreasing over time. In addition, the correlation among the genes related to RNF makes sense since the dying cells are tumor cells.
 Breast cancer data. We also applied our method to four correlation clusters found in the Wisconsin Breast Cancer data derived from UCI ML Archive 3 . This data set mea-sures nine biomedical parameters characterizing breast can-cer type in 683 humans (humans with missing values were removed from the data set). The parameters include Clump
The data are donated by our project partners. http://www.ics.uci.edu/~mlearn/MLSummary.html Thickness (attribute  X  X 1 X ), Uniformity of Cell Size ( X  X 2 X ), Uniformity of Cell Shape ( X  X 3 X ), Marginal Adhesion ( X  X 4 X ), Single Epithelial Cell Size ( X  X 5 X ), Bare Nuclei ( X  X 6 X ), Bland Chromatin ( X  X 7 X ), Normal Nucleoli ( X  X 8 X ), and Mitoses ( X  X 9 X ).

The derived dependencies of the four clusters are depicted in Table 5. Let us note that each cluster only contains hu-mans suffering from a benign tumor type. The patients suf-fering from a malignant tumor type were classified as noise. The dependencies in the first cluster are quite clean and in-dicate a constant behaviour of seven attributes. In addition, A 5 is related to A 7. The models of the remaining clusters are quite complex. Mostly, the first attributes which mea-sure an aggregated information about the shape and the size of the tumor cells exhibit a relationship to more specific measurements on single parts of the tumor. In general, since the clusters only contain benign tumors, our results indicate that this mostly harmless tumor type can still be explained and modelled by linear relationships among the measure-ments, whereas the more dangerous tumor type cannot be explained or modelled through any linear relations among the measurements.
Last but not least, we briefly discuss a further potential application of our method that utilizes the derived models for subsequent data analysis. As sketched above, the quan-titative models generated by our method can e.g. be used to predict the class of a new object. To evaluate this potential, we used three 2-dimensional synthetic data sets each with 5 classes. The first data set ( X  X S3 0  X ) contains 50 points per class, the second and the third data sets ( X  X S3 1  X  X nd  X  X S3 2  X ) each contain 100 points per class. Each class is gen-erated according to a certain linear dependency. The class distributions in DS3 0 and DS3 1 exhibit a jitter of 0.5% of the maximum distance in the unit cube, whereas the jitter of the classes in DS3 2 is 0.75%. The third data set is depicted in Figure 5. Note that these data sets are rather artificial and are only applied for a proof of principle.

We compared the classification accuracy of our sketched classifier to several other standard learning approaches. For this comparison we used the WEKA framework [23] with standard parameter settings, in particular, k NN (IBk) with k = 1 (best results reported), SVM (SMO), rule-based learner (PART), Naive Bayes, decision tree (J48), and multinomial  X  A 8=  X  20 . 9  X  0 . 3  X  A 8=0 . 8  X 
A 8=0 . 3  X  A 9=  X  21 . 1  X  A 9=  X  6 . 5  X  A 9=  X  8 . 5  X  A 9=6 . 5 logistic regression (Logistic). The results are depicted in Ta-ble 6. As it can be seen, our approach significantly outper-forms most of the other approaches, except k NN, in terms of accuracy.

Let us note that standard classifiers will most likely pro-duce comparative or even better results if the classes are Table 6: Comparison of different classifiers in terms of accuracy (in %).
 DS3 0 95 91 62 82 65 82 67 DS3 1 94 94 54 85 64 83 60
DS3 2 91 91 58 81 60 83 57 generated through models that cannot be captured by our concepts of linear dependencies. However, our small exam-ple may show that if the classes are generated by a model of linear dependencies as captured by our proposed concepts, our method obviously yields a better prediction accuracy than standard supervised learners.
Several correlation clustering algorithms have been pro-posed recently. However, none of these algorithms derive a quantitative model for each correlation cluster which is ur-gently needed in order to gain the full practical potentials from correlation cluster analysis. In this paper, we describe an original approach to derive quantitative information on the linear dependencies within correlation clusters. Our con-cepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clus-tering algorithm. Furthermore, as a sample application of our approach, we sketched how these quantitative models can be used to predict the probability distribution that an object is created by these models. Our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance. We exemplified how our method can be used in conjunction with a suitable clustering algorithm to gain valuable and important knowledge about complex relationships in real-world data. [1] E. Achtert, C. B  X  ohm, H.-P. Kriegel, P. Kr  X  oger, and [2] C. C. Aggarwal, C. M. Procopiuc, J. L. Wolf, P. S. [3] C. C. Aggarwal and P. S. Yu. Finding generalized [4] R. Agrawal, J. Gehrke, D. Gunopulos, and [5] C. B  X  ohm, K. Kailing, H.-P. Kriegel, and P. Kr  X  oger. [6] C. B  X  ohm, K. Kailing, P. Kr  X  oger, and A. Zimek. [7] Y. Cheng and G. M. Church. Biclustering of [8] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [9] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [10] E. Georgii, L. Richter, U. R  X  uckert, and S. Kramer. [11] J. Han and M. Kamber. Data Mining: Concepts and [12] J. A. Hartigan. Direct clustering of a data matrix. [13] D. Husmeier. Sensitivity and specificity of inferring [14] K. Kailing, H.-P. Kriegel, and P. Kr  X  oger. [15] J. Liu and W. Wang. OP-Cluster: Clustering by [16] J. Pei, X. Zhang, M. Cho, H. Wang, and P. S. Yu. [17] C. M. Procopiuc, M. Jones, P. K. Agarwal, and T. M. [18] U. R  X  uckert, L. Richter, and S. Kramer. Quantitative [19] R. Srikant and R. Agrawal. Mining quantitative [20] A. K. H. Tung, X. Xu, and C. B. Ooi. CURLER: [21] H. Wang, W. Wang, J. Yang, and P. S. Yu. Clustering [22] G. I. Webb. Discovering associations with numeric [23] I. H. Witten and E. Frank. Data Mining: Practical [24] J. Yang, W. Wang, H. Wang, and P. S. Yu.
 [25] T. Yuster. The reduced row echelon form of a matrix
