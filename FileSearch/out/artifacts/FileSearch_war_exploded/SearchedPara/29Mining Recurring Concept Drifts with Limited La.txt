 Data streams are generated from real-world events and their associations, such as ubiquitous computing, e-commerce, and sensor networks. These streaming data present new characteristics as being continuous, high-volume, and open-ended. It is hence a challenge to classify data streams using traditional models and algorithms [Gehrke et al. 1998, 1999; Shafer et al. 1996]. This is because they usually work on static data, and require multiple scans of the training data in order to build a model. In the classification on data streams, the input is a dataset of training instances (also called a training data stream), and each instance has several attributes. Attributes whose domain is numerical are called numerical attributes, whereas attributes whose domain is not numerical are called discrete attributes. There is one distinguished attribute called the class label, namely the last attribute. Assume that an input data stream X contains d -dimensional instances which are denoted by X = { X 1 ,  X  X  X  , X N ,  X  X  X  } . Associated with each instance is a class which is drawn from the class label set Y = { y The goal of classification on data streams is to build an online learning system of the distribution of the class labels in terms of the predictor attributes using the training data stream. The resulting system is used to assign class labels to future testing in-stances where the values of the predictor attributes are known but the value of the class label is unknown, namely to output a class prediction H ( X t ), for the given input, X class label y t  X  Y . Each instance ( x t , y t ) is independently drawn from the distribution of the target concept, Pr( x , y ). The task of an online learning system is to minimize cumulative prediction errors during online learning.

Meanwhile, there are serious problems with online learning, especially in environ-ments where the statistical properties of the target variable may change over time, namely Pr t +1 ( x , y ) may differ from Pr t ( x , y ). This change is known as concept drift [Widmer and Kubat 1996] and includes both gradual changes and sudden and signif-icant changes. One example of concept drift is the spam filtering problem. Spam is characterized by skewed and changing class distributions, changing target concepts, and intelligent and adaptive adversaries. An effective online learning system should be able to respond to concept drifts. To handle concept drifting data streams, a natural approach is incremental learning algorithms with an error-rate-based concept drifting detection mechanism, such as the incremental tree induction algorithm [Li et al. 2010; Hulten et al. 2001; Zhu et al. 2010]. However, in the real world, recurring events will cause recurring concepts in data streams, such as weather changes, buyer habits, etc. Thus, as a subtype of concept drift, recurring concept drifting is also challenging for these existing classification streaming models/algorithms, which still has not yet met proper attention from the research community. This is because existing classification models only store the current concept and have to relearn every time when a new concept occurs. Relearning significantly affects the performance of these classification models. Therefore, an ideal classification model for streaming data mining should be capable of learning in one pass, be able to do any-time classification, track the drift in the data over time, and remember historically learned concepts. Most existing clas-sification algorithms on data streams, such as Hulten et al. [2001], Zhu et al. [2010], and Li et al. [2010], assume that all arrived streaming data are completely labeled and these labels could be used at hand. Unfortunately, this assumption is violated in many practical applications, especially in the fields of fraud identification, intru-sion detection, and Web user profiling. That is, the input training data stream D can be divided into small data chunks D = { D 1 ,  X  X  X  , D i ,  X  X  X } and each data chunk contains |
D X instance set X u =( x 1 ,  X  X  X  , x u )( | D i | = l+u ), the labels of which are not known. Under these data stream scenarios, if we only wait for the future labels passively, it is likely that much potentially useful information is lost. If we want to label a full stream manually, it is too costly and time-consuming, if not impossible. Thus, it is significant and necessary to learn actively and immediately. That is, an online learning system that can handle data streams with concept drifts and unlabeled data should be able to (i) label unlabeled data using the information of labeled data to reduce the concept drifting rate; (ii) detect the occurrence of concept drifts and recognize the recurring concepts effectively; (iii) present higher classification accuracy even if in the data stream environment with a certain noise.

Motivated by the preceding analysis, our work in this article is to focus on track-ing recurring concept drifts in the environment of data streams with unlabeled data. A semi-supervised algorithm of REDLLA for mining REcurring concept Drifts from Limited Labeled streaming data is hence proposed. REDLLA provides several contri-butions. (i) To reduce the impact from unlabeled data on concept drifting rates, we first label those unlabeled data in X u using the information from labeled dataset X l . That is, we label the unlabeled data with a clustering approach ( k -means) in the growing of a decision tree and reuse the unlabeled data combined with the labeled data for split-tests of the current tree. This is different from the semi-supervised algorithms of clustering-training [Wu et al. 2006] and SmSCluster (Semi-supervised Stream Clus-tering) [Masud et al. 2008]. The clustering-training algorithm uses clustering to select confidently unlabeled samples, and uses them to retrain the classifier incrementally while the SmSCluster algorithm is based on an ensemble classification model, which is built as microclusters (based on k -means) and uses the k -nearest-neighbor algorithm to classify. Meanwhile, both algorithms are not suitable for data streams with recurring concept drifts. (ii) We use the deviations between clusters to identify new concepts (i.e., concept drifts) and recurring concepts at leaves, that is, we detect whether the from that in the t th data chunk (i.e., Pr t ( x , y )). Meanwhile, we integrate the recurring concepts and maintain all concepts for next concept drift detection. To the best of our knowledge, this is a new method to detect recurring concept drifts from data streams. (iii) We systematically study the performance of our algorithm in different ratios of unlabeled data with recurring concept drifts. Experiments conducted on both syn-thetic and real-world recurring concept drifting data show that REDLLA could track recurring concept drifts well in data streams with unlabeled data, and that it is compa-rable to the state-of-the-art supervised concept drifting algorithms of CVFDT [Hulten et al. 2001], DWCDS [Zhu et al. 2010], and CDRDT [Li et al. 2010], and several on-line semi-supervised algorithms involved in Wu et al. [2006] on the predictive accuracy even on unlabeled data streams.

The rest of this article is organized as follows. We start with an overview of re-lated work in Section 2 before we present our semi-supervised classification algorithm of REDLLA in Section 3. Section 4 provides our experimental study and Section 5 summarizes our results and future work. In this section we analyze related work in two dimensions. One is related to the al-gorithms of CVFDT, CDRDT, and DWCDS that will be used in our comparative study, and the other refers to the strategies and algorithms for unlabeled data and recurring concept drifts. CVFDT (Concept-adapting Very Fast Decision Tree learner) [Hulten et al. 2001] is one of the best-known algorithms that can efficiently classify continuously changing data streams. During training, an input is a data stream and a decision tree is learned in CVFDT by recursively replacing leaves with decision nodes using this data stream. Each leaf stores sufficient statistics about attribute values. If there is enough statis-tical support in favor of one test, the split-test is installed in the heuristic evaluation function of Information Gain and the inequality of Hoeffding bounds [Hoeffding 1963] at this node. To handle the concept drift, CVFDT periodically scans the whole tree to search for internal nodes whose sufficient statistics indicate that concept drifts occur. An alternative subtree is started at every such node. Old subtrees are replaced if the alternative ones become more accurate on new data. However, in cases where his-tory repeats itself, CVFDT does not take advantage of previous experience and hence converges to new target concepts slowly when concept drifts occur. During testing, each testing instance first traverses from the root of the current decision tree to an available leaf, and its class label is predicted in the majority-class method by the label distribution of training data accumulated at the leaf.

The streaming data algorithm for Concept Drifts in Random Decision Tree ensem-bling called CDRDT [Li et al. 2010] is an efficient algorithm for handling different types of concept drifts in the noisy data streams. During training, an input is also a data stream, which is divided into small data chunks. CDRDT first generates an en-semble classifier based on random decision trees with various sequential data chunks, maintains history data selectively at this ensemble classifier, and adopts a double-threshold-based mechanism to discern concept drifts in noise. Because CDRDT main-tains old classifiers in an ensemble, hence, it could handle recurrent drifts. This is also validated in the experimental study on the HyperPlane database with recurring gradual concept drifts. Further, during testing, each testing instance first traverses from the roots of all decision trees to the corresponding leaf. Second, the strategy of majority voting is utilized to give the label for each test instance. It is implemented af-ter the individual decision from each tree in the na  X   X ve Bayes method, namely choosing the majority class label as the classification result.

DWCDS is a new classification algorithm based on a double-window mechanism for handling various concept drifting data streams [Zhu et al. 2010]. In contrast to CDRDT, DWCDS is based on an ensemble classifier using random decision-tree forest, namely, DWCDS consists of n classifiers ( n = 10 in this algorithm) and each classifier consists of k random decision trees. During training, an input is also a data stream, which is divided into small data chunks. With the incoming of the streaming data chunk, it utilizes a double-window-based mechanism to periodically detect concept drifts (i.e., every data chunk). To adapt to the concept drifts, it will incrementally cre-ate the classifier using the new data chunk with concept drift. If the upper limit of the number of classifiers is met, it will remove the worst classifier in terms of classification error. Because DWCDS maintains many old classifiers in the model, it is also suitable for recurrent concept drifting datasets, such as HyperPlane (see Section 3 in Zhu et al. [2010]). This is similar to CDRDT. During testing, each testing instance first traverses from the roots of all decision trees to the corresponding leaf. Second, the strategy of majority voting is utilized to give a label for each test instance. It is implemented after the individual decision from each tree in the na  X   X ve Bayes method or the majority-class method, namely choosing the majority class label as the classification result.
In sum, the aforementioned algorithms enable handling concept drifting data streams with complete class labels effectively and efficiently, but they all ignore the case with unlabeled data in real applications. There are mainly three machine learning paradigms, including semi-supervised learn-ing [Belkin and Niyogi 2004; Blum and Mitchell 1998; Sindhwani et al. 2007; Zhou and Li 2005; Zhou and Li 2007], transductive learning [Joachims 1999], and active learning [Abe and Mamitsuka 1998] for the handling of unlabeled data. Regarding the differ-ences among these three approaches, details are given by Zhou in Zhou and Li [2010] as follows. Semi-supervised learning deals with methods for automatically exploiting unlabeled data in addition to labeled data to improve learning performance, where no human intervention is assumed. Transductive learning is a cousin of semi-supervised learning, which also tries to exploit unlabeled data automatically. The main difference between them lies in the different assumptions on the test data. The latter takes an assumption that the test dataset is known in advance and the goal of learning is to optimize the generalization ability on this test dataset, while the unlabeled examples are exactly the test examples. The former takes the assumption that the test dataset is unknown and the unlabeled examples are not necessarily test examples. However, in active learning, it requires an oracle to exploit unlabeled data, such as a human expert, from which the ground-truth labels of instances can be queried. The goal of active learning is to minimize the number of queries for building a strong learner. In the analysis of these three approaches, active learning needs the interaction with an expert in the corresponding field while semi-supervised learning and transductive learning depend on the classifier itself to implement the labeling. In addition, many researchers consider that transductive learning is also semi-supervised. In this arti-cle, our proposed algorithm also belongs to the semi-supervised algorithm, thus, in the following subsection, we will give the details of the aforementioned semi-supervised algorithms to clarify the difference.

More specifically, Blum et al. presented a cotraining algorithm to classify Web pages [Blum and Mitchell 1998]. The cotraining model is as follows. Suppose an in-put dataset consists of two parts, one is the labeled dataset L with | L | instances and the other is the unlabeled dataset U with | U | instances. Assume an instance space X = X 1  X  X 2 , where X 1 and X 2 correspond to two different  X  X iews X  of an instance. That is, each instance x is given as a pair [ x 1 , x 2 ]. On the assumption that each view in itself is sufficient for correct classification, this algorithm trains two initial classifiers separately on these two different views (namely two independent sets of attributes) using the labeled instances in L , and uses the predictions of each classifier on partially unlabeled instances from U to augment the training set of the other. After labeling all unlabeled instances in U , this algorithm uses the generated classifiers to classify the test dataset. Zhou et al. proposed a new cotraining-style semi-supervised learning algorithm named tri-training [Zhou and Li 2005], which does not assume sufficient and redundant views. In this algorithm, initial classifiers are trained from datasets generated via bootstrap sampling from the original labeled example set. These classi-fiers are then refined in the tri-training process, and the final hypothesis is produced via majority voting. In addition, the same authors also proposed a cotraining-style semi-supervised regression algorithm called COREG [Zhou and Li 2007]. In contrast to the tri-training algorithm, it uses two regressors each labeling the unlabeled data for the other regressor, where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean square error over the labeled neighborhood of that example. In addition, Belkin et al. proposed a semi-supervised algorithm to improve the classification accuracy utilizing both labeled and unlabeled data [Belkin and Niyogi 2004]. It models the manifold using the adjacency graph for the data and approximates the Laplace-Beltrami operator by the graph Laplacian. This method is suitable for practical applications of image, speech, and text classi-fication. Sindhwani et al. introduced a graph-based construction of semi-supervised Gaussian process classifiers [Sindhwani et al. 2007]. This approach is based on the techniques for incorporating the geometric properties of unlabeled data within globally defined kernel functions. The full machinery for standard supervised Gaussian process inference is brought to bear on the problem of learning from labeled and unlabeled data. It provides a natural probabilistic extension to unseen test examples. However, these aforementioned algorithms aim to tackle the dataset with labeled data and un-labeled data without the environment of data streams. That is, these algorithms are all batch algorithms and they are not suitable for huge scales of datasets.
Meanwhile, much research has been performed on classifying data streams with unlabeled data. On one hand, algorithms employ a decision tree or a classifier in general. For example, Wu et al. proposed a semi-supervised learning algorithm of clustering-training [Wu et al. 2006] in 2006. In this algorithm, an input dataset is a data stream with labeled instances and unlabeled data. The labeled instances are first used to train an initial classifier. Next, the classifier is used in a data stream envi-ronment. In each round, a number of instances are extracted from the data stream. The K -prototype clustering is applied to these instances, and their clustering labels are adjusted by the current classifier, which keeps the two labels by the two methods having the same meanings. Moreover, some of the labeled instances whose labels given by the two methods are identical are selected as confident ones to retrain the classi-fier, and others are discarded. Finally, the current classifier will be tested on a test dataset. In 2009, Li et al. introduced an OcVFDT (One-class Very Fast Decision Tree) algorithm for the problem of one-class classification of data streams [Li et al. 2009a], which is only suitable for data streams with discrete attribute values. In this algo-rithm, an input is a streaming data. Given a streaming data S = { s 1 ,  X  X  X  , s i ,  X  X  X } , where s attributes, y i  X  X  0 , 1 } represents whether y i is available or not to the leaner. Under the one-class classification scenario, only positive samples are labeled. Thus, if l i =1,it indicates y i = 1, otherwise, the true class label of X i is unknown. In terms of this in-put, OcVFDT builds a Hoeffding decision tree, which adopts the one-class information gain as the split evaluation function to choose the best splitting attribute. Last, it is classified on the test dataset to validate the performance of the time consumption, the predictive accuracy, and the value of F1. These algorithms are state-of-the-art classi-fication algorithms for data streams with unlabeled data, but they are not suitable for the concept drifting cases. On the other hand, some streaming systems employ only unsupervised methods to provide a semi-supervised approach, such as a framework for classification of dynamically evolving data streams with unlabeled data [Aggarwal et al. 2004], and an ensemble classification model [Masud et al. 2008] learned from a training set with both unlabeled and labeled data. These algorithms have the same inputs as our algorithm, and they enable handling the concept-drifting data streams with unlabeled data. However, they all ignore the recurring issue of concept drifts in data streams. To handle the recurring concept drifts, many classification algorithms or systems have been proposed as follows. (i) Widmer and Kubat designed a series of FLORA algorithms [Widmer and Kubat 1996] in 1996, where FLORA3 could adapt to the case with recurring concept drifts. In the FLORA 3 algorithm, an input is a training dataset with recurring concept drifts, such as the dataset of STAGGER [Schlimmer and Granger Jr. 1986]. It stores concept descriptions and reuses them if a previous context reappears. The output is the performance on the tracking of a recurring con-cept drifting dataset. This algorithm is suitable for datasets with discrete attributes. (ii) Harries and Sammuta proposed a splice methodology for the recurring context problem [Harries et al. 1998] in 1998. The proposed approach uses a metalearner, which works in an offline and batch way, to extract a hidden context, and to induce the local concepts. It is broadly applicable to the extraction of context reflected in time and spacial attributes. (iii) Nishida et al. introduced an online learning system based on ensemble classifiers leveraging prior knowledge in recurring contexts [Nishida et al. 2005] in 2005. This system generates ensembles of classifiers on sequential chunks of training instances and it can respond to gradual changes in large-scale data streams but they have problems responding to sudden changes and leveraging prior knowledge of recurring contexts. (iv) Wang et al. proposed a Recognizing and Treating Recurring Contexts (RTRC) system [Wang et al. 2006]. By using Markov chains and the least square method, this system enables predicting not only what the next concept is but also when the concept is to drift. (v) Ramamurthy and Bhatnagar proposed an ensemble-learning-based approach to handling data streams with multiple underlying modes [Ramamurthy and Bhatnagar 2007] in 2007. The proposed approach builds a global set of decision trees from sequential data chunks and creates new classifiers to represent the recurrent concept in the stream. The performance of this approach is validated on the test datasets. (vi) Katakis et al. presented a general framework for classifying email streaming data with recurring concept drifts [Katakis et al. 2010]. It exploits stream clustering and a transformation function that maps batches of examples into a new conceptual representation model to dynamically build and up-date an ensemble of incremental classifiers. Clustering is applied to group batches of examples into concepts and identify recurring contexts, and the ensemble is produced by creating and maintaining an incremental classifier for every concept discovered in the data stream. The proposed algorithm preliminarily explores the email streaming data with recurring concept drifts and unlabeled instances, but it does not consider the various types of concept drifts and does not systematically explore the relation between the classification performance and the different values of ulr . However, for other algorithms mentioned earlier, none of them addresses the issue of recurring concept drifts in streaming environments with unlabeled data. Meanwhile, the methods of FLORA3 and splice are not oriented to handle large amounts of streaming data.

In contrast to the aforementioned algorithms, our algorithm presents the following characteristics. (1) The classification algorithms for labeled data and unlabeled data, such as Blum and Mitchell [1998], Belkin and Niyogi [2004], Zhou and Li [2005], and Sindhwani et al. [2007], etc., also could handle an input dataset with labeled data and unlabeled data, but they are batch algorithms. They assume that all input data are at hand and they could not update the current model incrementally with new training data. However, in our algorithm, we utilize an incremental decision tree as a base model to adapt to a dynamic training data stream. To handle streaming data with unlabeled data and labeled data, we adopt a new semi-supervised mechanism. That is, with the incoming of the sequential streaming data chunk with labeled data and unla-beled data, the decision tree is built incrementally. Meanwhile, those labeled training data and relevant unlabeled training data traverse from the root to an available leaf in the growing of a decision tree, then k -means is used to cluster unlabeled data and la-beled data at leaves. Correspondingly, they are divided into small clusters at leaves. In this case, those unlabeled data in a cluster are labeled using the information of labeled data in the same cluster at leaves, and then the labeling information is reutilized after labeling. It is conducive to improve the labeling accuracy. (2) We know that concept drifts are relevant to the changes of data distribution, that is, the distribution changes of attribute values or class labels. However, the concept clusters are the information presentation of attribute values and class labels. Thus, we adopt the deviations be-tween concept clusters created by a clustering algorithm to represent these changes. This is a novel detection method for concept drifts. In our algorithm, we select the k -means clustering algorithm to create concept clusters because of its simplicity and efficiency for databases with numerical attributes.

For simplicity, in this article, we only consider the recurring concept drifting issue in streaming data with numerical attributes. We will study streaming data with categor-ical attributes or mixed attributes for handling recurring concept drifts in our future work. We use the concept drifting rate learning designed in Helmbold and Long [1994] and the theory on learning from noisy samples proposed in Angluin and Laird [1988] as the foundation for our work. A brief introduction is as follows.

First, the drift rate (e.g., ) in concept drifting learning specifies the probability that two successive target concepts disagree on a randomly drawn instance. Hence, the larger the drift rate, the more the change frequency of target concepts in the learning of a data stream. A theoretical bound on the allowable drift rate was provided in Helmbold and Long [1994], which guarantees tractability with an error of at most  X  , namely, where c &gt; 0, d is the Vapnik-Chervonenkis dimension of a concept, and both values are constants. This bound indicates that it is more difficult to track concept drift in tandem of learning with fewer labeled data per target concept (namely, a higher drift rate). Thus , according to Eq. (1), to reduce the rate of concept drift, one of the possible strategies is to fill the gap in labeled data with relevant unlabeled data for improving the performance of the current concept drifting learner [Widyantoro 2007]. This is also our consideration in this article.

Second, according to the theorem on learning from noisy samples proposed in An-gluin and Laird [1988], if a sequence  X  of m instances is drawn, where the instance size m satisfies Eq. (2). where is the worst-case classification error rate of a hypothesis,  X  is an upper bound on the noise rate, N is the number of hypotheses, and  X  is the confidence, then a hy-pothesis H i that minimizes disagreement with  X  will have the PAC property, shown in Eq. (3) as follows. We have where d (,) is the sum over the probability of elements from the symmetric difference between the two hypothesis sets H i and H  X  (the ground truth). Let c =2  X   X  ln (2 N / X  ), where  X  makes Eq. (2) hold equality. Then Eq. (2) becomes Eq. (4). According to Eq. (3),  X  is a constant, then the hypothesis H i is more close to the ground truth H  X  when decreases along with the training process. Correspondingly, Eq. (4) could be expressed into Eq. (5) as follows. In our algorithm, the value of m indicates the size of a data chunk (i.e., D i ). In addition, according to Eq. (1) and Eq. (5), we could obtain the following equation. Let a = m / c ,and b =  X   X  c  X  c / md , where  X  makes Eq. (6) hold equality. Assume 0 &lt; X &lt; 1 / 2 is met in this case, then Eq. (6) could be rewritten by Eq. (7), where to the value of  X  . In our algorithm, the noisy data consist of two parts: one refers to the original noisy data from the training streaming dataset and the other indicates the noisy data intro-duced by the labeling mechanism. Thus, on one hand, we could obtain that the value of is directly proportional to the value of noise rate  X  according to Eq. (5). On the other hand, we can obtain that the value of is in direct proportion to the value of  X  if the value of  X  is more than the constant e according to Eq. (7). That is, the lower the value of  X  , the higher the probability that the value of  X  is larger than e .Withthe preceding analysis, the following describes an approach that: (i) combines the labeled data and relevant unlabeled data to build a decision tree incrementally and installs labeling at leaves. In this labeling process, it is necessary to maintain a higher label-ing accuracy, namely to reduce the noise rate. This is also conducive to reduce the rate of concept drift. And (ii) compares the deviation between history concept clusters and new ones to track recurring concept drifts. This relies that the deviation of concept clusters indicates the change of data distribution. Our REDLLA algorithm to be presented in this section aims to handle recurring concept drifting data streams with unlabeled data. Figure 1 shows the system framework of REDLLA, whose main handling components are addressed in bold.
 More specifically, first, as the incoming streaming data arrive, a decision tree is initialized and created incrementally. In the growing of decision tree, unlabeled data are labeled at leaves in the current tree using a clustering strategy and the informa-tion of unlabeled data is reused to grow the decision tree. Second, if the thresholds specified in our algorithm are met, three handling mechanisms are installed. To be concise, (1) the recurring concept drifting detection is installed using concept clusters maintained at leaves if the detection period-DP is met. (2) A pruning mechanism is installed when reaching a certain detection period-PP . It benefits avoiding the space overflow or overfitting with the continuously growing of the decision tree. (3) To track the performance of the current classification model, prediction results are evaluated periodically (i.e., satisfying the threshold of output period-OP )in the prequential estimation. According to the aforementioned description of system framework, the corresponding pseudocode of processing flow is shown in Algorithm 1. In the following subsections, we will give the technique details for each component in REDLLA.

Growing a decision tree. Our algorithm is built on an incremental decision tree. That is, a training instance-e first traverses the decision tree-T from the root to an avail-able leaf l , evaluating the appropriate attribute at each node, and following the branch corresponding to the attribute X  X  value in e . Meanwhile, relevant statistics are stored at the current node, such as the total number of instances, the distributions of class labels and attribute values of all available features (denoted as the array variables of classArray and attrArray ) (see steps 1 X 3 in Algorithm 1). If the statistical count at this node is up to the value of n min ,the k -means clustering algorithm is installed to label unlabeled data in each cluster and the labeled information is loaded into the arrays attrArray and classArray for split-tests (steps 4 and 5). In step 6, the merit of a split-test is evaluated in a heuristic method based on Information Gain and Ho-effding bound inequality as the method in CVFDT. The inequality of Hoeffding bounds is as follows. Consider a real-valued random variable r whose range is R . Suppose we have made n independent observations of this variable, and computed their mean  X  r , which shows that, with probability 1- X  , the true mean of the variable is at least  X  r  X  refers to the minimum number of instances required in a split-test, marked as n min . Suppose H (  X  ) is the estimator function of information gain, the difference between two S indicates the training examples at the current node and the size is n min . The infor-mation gain of H ( S , a i ) could be expressed as Eq. (9). It specifies the difference between the entropy of the training examples S at the node and the weighted sum of the en-tropy of the subsets s v caused by partitioning on the values v of that attribute a i . This is similar to H ( S , a j ).
 subject to: label in s v . It should be mentioned that for numeric attributes, our algorithm takes advantage of discretization to divide the sequential attribute-values into min(10, the number of discrete attribute-values for the current numeric attribute) intervals. In this case, Eq. (9) is also suitable for numeric attributes. In terms of the aforemen-tioned estimation of information gain, for a given  X  ,if H &gt; X  or H  X   X &lt; X  (  X  is a threshold for avoiding the case of ties), the attribute a i with the highest gain will be selected as the final split feature. For more details, refer to Hulten et al. [2001]. Corre-spondingly, the current leaf-l is replaced with the decision node and children leaves are generated.

Labeling unlabeled data with relevant labeled data. To exploit unlabeled data, we adopt the clustering algorithm k -means to create concept clusters and implement labeling, be-cause k -means is a simple and efficient clustering algorithm for numerical attributes. As shown in steps 1 X 7 in Function 1, the clustering algorithm is triggered if there are new unlabeled data at the current leaf. Based on the concept clusters generated in this processing, the majority-class method is adopted to label unlabeled data in step 8 as illustrated in Figure 2. In this figure, suppose there are two clusters at a leaf. Because the number of unlabeled instances belonging to C 1 (i.e., 18) is more than that belonging to C 2 (i.e., 4), unlabeled instances-a , b , c in this cluster are hence labeled in the majority-class of C 1 , and cluster-1 is also labeled in C 1 . Similarly, unlabeled in-stances ( d  X  i ) in cluster-2 are labeled in C 2 and cluster-2 is labeled in C 2 . Lastly, the information of unlabeled instances relevant to the attribute values and class labels is stored into variables of attrArray and classArray as shown in step 9. It will be used for split-tests in the growing of this tree. Correspondingly, the statistics about labeling unlabeled data are updated, namely, the total number of unlabeled data at the current leaf and the number of unlabeled data that are labeled correctly. They will be used to draw the graphs of labeling accuracy in the experiments.
 An extreme case should be addressed that there is no labeled data at the clusters. How to label unlabeled data is a problem. In this article, we provide the following strategies. (i) Select the nearest one of history concept clusters to label unlabeled data. (ii) Otherwise, collect concept clusters with class labels from other leaves and select the nearest cluster to label unlabeled data. (iii) If no clusters in the current tree are labeled, select a class label randomly from all known class labels as the class label of the current cluster, and then readjust the class label after seeing incoming labeled data as the streaming data arrive.

In terms of the aforementioned semi-supervised mechanism, we conclude that in general, the higher the height of the decision tree, the larger the accuracy of labeling unlabeled data at leaves. An analysis is given in detail as follows. Suppose the decision tree generated in REDLLA is a complete binary-tree. The height of this tree is denoted as l . Apparently, the total number of leaves is up to 2 l  X  1 . Meanwhile, suppose there are m -instances arrived at the current tree and the probability that each training instance reaches a leaf is equal. Thus, the mean number of instances at a leaf amounts to m / 2 l  X  1 (  X  1). Therefore, the least probability that all unlabeled data are labeled correctly at a leaf (denoted as pc ) could be expressed in Eq. (12). It shows that the value of pc is generally in direct proportion to the value of l . However, an extreme case is that if there is only one leaf in the current decision tree, the value of pc will amount to 1 / | class | m . This indicates that all instances are directly handled by the k -means algorithm. Apparently, the accuracy of labeling unlabeled data in this case is the worst 1 . According to Eq. (12), the more the value of l , the larger the value of pc , correspond-ingly, the larger the accuracy of labeling unlabeled data for the current model. This is a straightforward conclusion which is validated in the experiments (see Figure 7). In fact, this conclusion is based on the consumption that the current decision tree adapts to the current data distribution, that is, with the increasing height of the decision tree, the split-nodes in each path are valid, correspondingly, the current model performs well on the current training data. However, this indicates more memory consumption. Therefore, to achieve the trade-off between the accuracy of labeling unlabeled data and the memory overhead, our algorithm specifies a threshold to limit the memory overhead, that is, the Pruning Period PP . In other words, the decision tree grows in the limit of the memory consumption, correspondingly, the height of decision tree is limited. In fact, in our algorithm, the accuracy of labeling unlabeled data could be granted if setting PP = 500k (see Figure 9).

In addition, regarding the setting of parameter-k , we initialize it with the value of | class | , because the number of class labels indicates how many concept clusters in the clustering. Thus, in step 3 of Function 1, we first divide the training data at the current leaf into different sets corresponding to the distribution of class labels, and then select a cluster center randomly from each set to generate an initial cluster center set. This is beneficial to find optimal clusters in time. However, if all class labels of the current training data are unknown, we will select the initial cluster centers randomly from training data.

Recurring concept drifting detection. In the growing of decision tree, the concept drifting detection is installed to distinguish concept drifts from noise every certain number of instances (namely, the detection period of DP ) at leaves. In this article, we consider all arrived training instances in a detection period as a streaming data chunk. That is, the value of the detection period indicates the size of a data chunk. As shown in Function 2, we first create a set of concept clusters using newly arrived instances at the current leaf, called M ne w . If it is the first detection period, these concept clusters are directly stored in the concept list-conceptList at this leaf. Otherwise, they are used to compare with the last set of concept clusters (called M last ) for drifting detection involved in step 5. To measure the deviation between cluster sets of M ne w and M last ,we define two variables, namely: (i) r to specify the radius of a concept cluster (the radius of the concept cluster in M new and M last is called r new and r last respectively), and (ii) dist to refer to the distance between concept clusters as follows.

Definition 3.1.1. The radius of a concept cluster specifies the mean Euclidean e current cluster centroid, which is composed of { m p 1 ,  X  X  X  , m p | A | } and | m p | means the number of instances in this cluster. Definition 3.1.2. The distance between two concept clusters m l and m h refers to the Euclidean distance between these two cluster centroids with respect to attribute set-A :
According to statistics theory, for a stationary distribution of the instances, the online error of na  X   X ve Bayes will decrease, while the distribution function of the instances changes, the online error of na  X   X ve Bayes at the node will increase [Duda et al. 2001]. In our algorithm, the change of online error of na  X   X ve Bayes implies the distribution change of attribute values in partial attributes or in all attributes and the data distribution of class labels at the current cluster, namely, the change of concept clusters. Correspondingly, we could obtain four cases of concept drifts as illustrated in Figure 4(a) regarding the deviation of concept clusters. That is, the following holds. (1) If a concept cluster contains another concept cluster, namely, the value of dist is (2) Otherwise, if the value of dist is less than the maximum value of these two ra-(3) If the value of dist is less than the total sum of r last and r ne w and more than the (4) Or else, a true concept drift is considered. This detection strategy is built on the
Furthermore, because the concept drifts are possibly caused by the distribution change of class labels, we further check the distribution of class labels if there is no concept drift or potential concept drift occurring. In other words, if the class label of a concept cluster in M ne w is in complete contrast to that in M last , it is also considered as a true concept drift. As illustrated in Figure 4(b), firstly, we compare the deviation of con-cept clusters between M ne w and M last (each set contains two concept clusters). Because the concept clusters of m 21 and m 22 in M ne w are the same as the concept clusters-m 11 and m 12 in M last respectively, that is, the centroids are the same and the distance-dist equals to 0. Thus, a nonconcept drift is considered. Secondly, we compare the distri-bution of class labels for the same concept clusters, due to the difference of class labels between m 11 with the label-C 1 and m 21 with the label-C 2 (and between m 12 with the label-C 2 and m 22 with the label-C 1 ). In an overall consideration, a true concept drift is taken into account. In sum, our drifting detection strategy relies on the fact that concept drifts are caused by either the distribution change of attribute values or the distribution change of class labels.

In terms of the aforementioned description of concept drifting detection, if there is no concept drift occurring at the current leaf, the current set-M ne w will be changed into an old set, called M last while all remaining concept clusters generated over the previous data chunks consist in a set of history concept clusters, called M hist (steps 6 X 7 in Function 2). Otherwise, the concept clusters in M ne w contain the new (drift-ing) concept, and we further judge whether it is a recurring concept by comparing the deviation between concept clusters in M ne w and the ones in M hist (step 9). This process-ing of recurring concept detection is similar to the concept drifting detection referred in step 5. If this is a new concept compared to all concepts in M hist , this cluster will be stored into conceptList ; or else, this cluster is integrated into the cluster in M hist (steps 10 X 13). As illustrated in Figure 3, suppose the concept list in leaf-b contains n sets of concept clusters, that is, it indicates that concept clusters are generated over at least n streaming data chunks. Each set contains different clusters indexed with different numbers. First, to detect a concept drift between the set-M ne w and M last , we obtain that these two concept clusters are new concepts. Second, we further compare the concept clusters in M ne w with all concept drifts in M hist , and find that these two new concepts are old ones for the history concept clusters. This is because they have occurred in the sets of M 1 and M 2 , respectively. We use the same number labeled in these clusters to indicate the recurring concepts. In this case, we will update the cluster centroids of concept-2 and concept-3 in M 1 and M 2 by integrating all instances in concept-2 and concept-3 in M ne w , respectively.

Pruning mechanism and Error Estimator. Considering the open-ended characteristic of a data stream, if the count of instances arrived at the tree amounts to the pruning period-PP , several subtrees from bottom to top with the roots whose classification er-ror rates are more than 50% are cut off according to the simple pruning principle. This is conducive to avoid possible overfitting or space overflow if the current decision tree grows continuously. Meanwhile, as shown in steps 11 and 12 in Algorithm 1, if the count of instances arrived at the tree is up to the output period-OP , we adopt the pre-quential evaluation [Gama et al. 2009] to evaluate the classification performance for the current model, where the prequential error is computed based on an accumulated sum of a loss function between the prediction value y  X  i and the observed value y i , namely, g = m i =1 L ( y i , y  X  i ). The mean error rate is given by g / m . The time complexity of REDLLA mainly consists of the training+testing time and the is the total number of training instances, l p is the average length of the path in the training, | attr | indicates the number of attribute dimensions, V is the maximum num-ber of attribute-values in an attribute (for numeric attributes, it refer to the number of discretized attribute values), n c means the total clustering times, n l refers to the number of instances at a leaf, k is the number of clusters, and t indicates the iteration count of k -means. With respect to the space complexity in REDLLA, the space over-+O( tr n  X | len | X  k ), where | len | refers to the average length of the concept list at leaves. To validate the efficiency and effectiveness of our REDLLA algorithm, we have per-formed extensive experiments on benchmark concept drifting databases, real-world databases, as well as representative noisy databases. Section 4.1 discusses the char-acteristics of databases mentioned before. Section 4.2 compares our experimental re-sults against concept drifting data stream algorithms of CVFDT, DWCDS, and CDRDT (learned from completely labeled data) and several semi-supervised algorithms in-volved in Wu et al. [2006]. All experiments are conducted on a P4, 3.00 GHz PC with 2G main memory, running Windows XP Professional, and all algorithms are imple-mented in C++.
SEA. SEA [Street and Kim 2001] is a well-known dataset of concept shifts. It con-sists of a three-dimensional feature space with two classes and four concepts. To sim-ulate the recurring concept drifting streaming data, these four concepts change in the cyclic order as illustrated in Figure 5 (where A , B , C ,and D each refer to a concept in this dataset, respectively) and each concept contains t instances. In our experiments, we select three different periods ( t = 5k, 10k, and 50k) respectively. Meanwhile, we introduce p % class noise in this dataset ( p  X  { 5, 10, 20, 30 } ). All data are generated using the SEA generator from MOA (an experimental tool for Massive Online Analy-sis) [Bifet et al. 2010] and the total size of this dataset is up to 1000k instances ( k = 1000).

HyperPlane. HyperPlane is a benchmark database of data streams with a gradual concept drift. A HyperPlane in a d -dimensional space ( d = 10) is denoted by equation d i =1 w i x i = w 0 . Each vector of variables ( x 1 , erated instance and is uniformly distributed in the multidimensional space [0, 1] d .If d to [  X  10, 10]. For a weight of w i , each initial value is generated at random; and it increases or decreases continuously by the value of w i until it is up or down to the boundary, then changes the direction with the probability of p w = 10%. Meanwhile, to simulate a recurring concept drifting case, we fix two dimensions and change their weights. Each dimension changes as illustrated in Figure 6. We also generate 1000k training instances in MOA with p % class noise ( p  X  { 5, 10, 20, 30 } ) and specify three drifting periods respectively, namely, t = 5k, t = 10k, and t = 50k.

Real databases. The Elec dataset is a widely used real dataset, which was collected from the Australian New South Wales Electricity Market [Harries 1999]. In this mar-ket, the prices are not fixed and are affected by demand and supply of the market. This dataset contains about 45k instances with two class labels. In our experiments, we reorganize this database and generate two datasets with the monthly period (from May to December) and the yearly period (from 1995 to 1998). Meanwhile, to simulate the recurring concept drifts with sufficient learning data, these data occur periodically as similarly as SEA.

The aforementioned three databases all contain recurring concept drifts, but the following databases to be mentioned only contain concept drifts without the recurring characteristic. They will be used to validate the performance of our algorithm against CDRDT, DWCDS, and CVFDT in the environment without recurring concept drifts.
RBF. The description of the RBF (Radial Basis Function) database is as follows. A fixed number of random centroids are generated. Each center has a random position, with a single standard deviation, class label, and weight. New instances are generated by selecting a center randomly, taking weights into consideration so that centers with higher weights are more likely to be chosen. A random direction is chosen to offset attribute values from the central point. The length of the displacement is randomly drawn from a Gaussian distribution with a standard deviation determined by the cho-sen centroid. The chosen centroid also determines the class label of the instance. Drift is introduced by moving the centroids with a constant speed, which is initialized by a drift parameter. We use the Random RBF Generator in MOA to generate training data with 50 centroids and a 0.001 drifting speed. The size of training data is up to 1000k.
Waveform. The Waveform database comes from the UCI repository [Blake et al. 1998]. There are two versions of the database with three class labels, Waveform21 con-taining 21 numerical attributes and Waveform40 containing 40 numerical attributes with an additional 19 irrelevant attributes. In our experiments, we use the Wave-form generator in MOA to generate two training datasets with 500k-sized instances of Waveform21 and Waveform40, respectively. The former dataset contains 3 dimen-sions of drifting attributes while the latter dataset contains 10 dimensions of drifting attributes and 19 dimensions of irrelevant attributes.
 In this subsection, we first analyze the parameter settings of all algorithms involved in our experiments, and then we give the experimental results between the labeling accuracy and its impact factors (such as the height of decision tree, the noise rate, and the value of ulr ). Third, we systematically introduce the performance of our al-gorithm compared against other algorithms in four dimensions, including the perfor-mance on the handling of recurring concept drifting data streams, the performance on the handling of data streams with little recurring concept drifts, and the performance on the time overhead. Last, we further compare our algorithm against several state-of-the-art semi-supervised classification algorithms (including tri-training [Zhou and Li 2005], clustering-training [Wu et al. 2006], cotraining [Blum and Mitchell 1998], and self-training [Zhu 2001]) in two estimation metrics of prediction accuracy and time consumption. 4.2.1. Parameter Estimation. Parameters of n min ,  X  and  X  are involved in Hoeffding bounds inequality. According to this inequality, the smaller the value of  X  , the more the predictive accuracy in the current model. However, it will lead to a larger value of  X  , namely a lower confidence. Hence, an optimal solution should achieve a good trade-off between  X  and  X  . Meanwhile, our study demonstrates that the greater value of n min indicates the smaller value of  X  but the more computational complexity at the numerical nodes. However, an experimental conclusion [Li et al. 2009b] reveals that the model performs sufficiently well if the value of n min is set to 200. For example, supposing the value of  X / R is 0.01, the value of  X  is only 0.0393 in this case, that is, the confidence is more than 96%. However, in this article, we specify n min = 200 and  X  =10  X  7 to ensure that the cut-point evaluated in the current model of learning from finite data is close to the case of learning from all streaming data (with the confidence up to 99%). The value of k in k -means is initialized as the number of class labels in the current database. A detailed analysis is given in subsection  X  X abeling unlabeled data with relevant labeled data X  of Section 3.1. Considering the parameter of detection period DP , the lower the value of DP , the more the false alarms; while the higher the value of DP , the more the missing concepts. To achieve a trade-off between false alarms and missing concepts in REDLLA, we empirically select DP =0.2 k . With respect to values of OP and PP , the larger the value of OP indicates the fewer experimental results, which is not beneficial to describe continuous curves of experimental results. However, the larger the value of PP indicates the more the memory consumption. In REDLLA, both values are user-specified. We select two values of OP = 0.5k (or 1k) and PP = 500k, because it is sufficient to guarantee the description of continuous curves and better performance of the current model. In addition, considering parameters in CVFDT, DWCDS, and CDRDT, they follow the default values in Hulten et al. [2001], Zhu et al. [2010], and Li et al. [2010] respectively. Three of them are conducted on the datasets with completely labeled instances ( ulr = 0%). All experimental results are averaged over 10 runs. 4.2.2. Experimental Estimations on the Labeling Accuracy. This subsection aims to validate the performance of the semi-supervised mechanism in our algorithm, namely the per-formance on labeling accuracy. In the analysis of our algorithm, we first conclude that the mean accuracy of labeling unlabeled data is in direct proportion to the height of the decision tree corresponding to Eq. (12). Second, we give experimental results of the labeling accuracy impacted from the noisy data in our algorithm. In addition, our algorithm is designed to handle labeled data and unlabeled data with various ratios. Therefore, in this subsection, we first give the experimental results about the rela-tion between the labeling accuracy and the height of a decision tree, and then present the experimental results of labeling accuracy impacted from the noisy data. In terms of experimental conclusions drawn from the aforementioned experiments, we further investigate the performance of our algorithm on the labeling accuracy varying with different values of ulr .

Relation between the labeling accuracy and the height of decision tree. Figure 7 shows the experimental results of average labeling accuracy  X  standard deviation varying with the height of decision tree from 2 to 10 (In our algorithm, the lowest height of decision tree begins from 2 to guarantee the validity of decision tree.) From this figure, we can see that with the increasing of the decision tree X  X  height, labeling accuracies increase gradually. Meanwhile, the higher value of the height of the decision tree, the larger the deviation of the labeling accuracy. It is obviously shown in Figures 7(c), 7(f), 7(h), 7(i), and 7(k). More precisely, as the height of the decision tree increases, the mean label-ing accuracy increases by 2.9% on Sea-interval-5k, by 3.7% on Sea-interval-10k, and by 5.4% on Sea-interval-50k. In this database, the largest fluctuation of standard devi-ation occurs in the database of Sea-interval-5k if the height of the decision tree is set to 2, it is more than 5%. As the height of decision tree increases, the standard deviation fluctuates weakly, and the least standard deviation is in the limit of 0.5% if the height of the decision tree is set to 10 (see Figure 7(c)). For the database of HyperPlane, the mean labeling accuracy varies from 91.8% to 94.1% on HyperPlane-interval-5k, from 91.6% to 93.8% on HyperPlane-interval-10k, and from 90.4% to 92.2% on HyperPlane-interval-50k. The standard deviation fluctuates more strongly if the height of the decision tree is lower. For example, if the height of the decision tree is no more than 4, the largest standard deviation is in the range of (1.5, 3), otherwise, it is less than 1.5. This is also similar to the Waveform database. However, for the RBF database, the labeling accuracy improves least only by 0.6%. Though the standard deviation fluctuates by 5.3% if the height of the decision tree is specified by 2, with increasing of the height of the decision tree, the fluctuation of standard deviation is quickly con-verged to 0.16%. For the database of Elec-interval-month, the mean labeling accuracy also increases gradually varying with the height of the decision tree and the standard deviation fluctuates in the limit of 1.1% after the height of the decision tree is more than 3. However, for the Elec-interval-year database, the mean labeling accuracy is improved by 5%, but the standard deviation fluctuates by a large margin. This indi-cates more concept changes exist in a year for Elec, that is, the data distribution in Elec-interval-year changes more frequently than that in Elect-interval-month. In the preceding analysis, a general conclusion is obtained that the average accuracy of label-ing unlabeled data will be improved gradually if the height of decision tree increases. In fact, this set of experimental results is obtained in the case of ulr 2 = 90% (and the noise rate in SEA and HyperPlane is set to 10%), but actually other experimental results in the different values of ulr varying from 1% to 99% also show a similar con-clusion. In addition, we know that the larger the height of a decision tree, the more the space consumption. Hence, according to the previous analysis, we specify the pruning period (i.e., PP ) by an optimal value 500k in our algorithm, because it is sufficient to guarantee the decision tree with a large height and limit the space consumption. In fact, in this case, the height of the decision tree in REDLLA does not exceed half of the number of attribute dimensions. Therefore, we conclude that if the height of a decision tree is no more than a half of the number of attribute dimensions, the larger the height of the decision tree, the higher the labeling accuracy. In the following experiments, we still specify an optimal value of PP = 500k to guarantee a high labeling accuracy.
Noise impact on the labeling accuracy. In the other dimension, we take into account the noise impact on the labeling accuracy. Figure 8 reports the experimental results of mean labeling accuracy in databases of HyperPlane and SEA impacted from 5%  X  30% noisy data (the variances of labeling accuracy are not marked in this figure because of the lower values, no more than 1%). It clearly reveals that: (1) more noisy data indicates larger reduction of labeling accuracy. More specifically, with increasing noise rate, the labeling accuracy is reduced from 99.0% to 88.5% for SEA while it is reduced from 97.5% to 89.6% for HyperPlane. (2) However, if the noise rate is fixed, labeling accuracies on databases with three different drifting periods are approximate. For example, the labeling accuracy on SEA with three different drifting periods of 5k, 10k, and 50k is 94.3%, 94.5%, and 95.0%, respectively in the case with 10% noisy data. In other cases varying with the noise rate from 5% to 30%, the largest fluctuation of labeling accuracy is in the limit of 1%. Meanwhile, for the HyperPlane database with three different drifting periods, the impact from the fixed noise rate is also limited to 1%. These data confirm that the accuracy of labeling unlabeled data is linearly impacted from the noisy data if the noise rate is limited in [0%, 30%]. To guarantee the performance of our REDLLA algorithm (including the labeling accuracy and the prequential error), we specify the noise rate p = 10% in the relevant databases and the pruning period PP = 500k in the following experiments.

Labeling accuracy varying with different values of ulr . In terms of the experimental conclusions relevant to the height of a decision tree and the noise rate mentioned earlier, a set of experiments is conducted to reveal the mean labeling accuracy in our algorithm varying with values of ulr from 1% to 99%. Due to less variance of labeling accuracy, we do not mark it in the corresponding figures. As shown in Figure 9, we can draw a conclusion that the predictive accuracy of labeling unlabeled data in our algorithm decreases gradually with increasing the value of ulr , but still maintains a higher value. More precisely, on SEA, the highest labeling accuracy is up to 96% in case of ulr = 10% (Figure 9(c)) while the lowest labeling accuracy is 94% in case of ulr = 99% (Figure 9(a)). If the value of ulr is no more than 90% or equals 99%, the standard deviation of experimental results is less than 1% while in the cases of ulr = 90% and ulr = 95%, it is around 2%. On HyperPlane, the labeling accuracy could be up to 94% in the cases of ulr  X  90% while the lowest predictive accuracy also amounts to 93% in the case of ulr = 99%. The standard deviation is no more than 1% varying from ulr =1%to ulr = 99%. On Elec, the predictive accuracy on Elec-interval-year fluctuates stronger than that on Elec-interval-month. The average predictive accuracy varies from 96% to 92% over the different values of ulr varying from 1% to 99% and the largest standard deviation of prediction accuracy is limited to 1.2%. On Waveform, the highest labeling accuracy is around 97% in the case of ulr = 1% while the lowest prediction accuracy is also more than 94% in the worst case of ulr = 99%. The standard deviation of prediction accuracy is no more than 1%. On RBF, the predictive accuracy changes from 93% in the case of ulr = 1% to 91% in the case of ulr = 99% and the standard deviation is much less than 1%. In sum, as the unlabeled ratio increases, the labeling accuracy is reduced by a small margin if the value of ulr is less than 95%. A higher predictive accuracy (more than 90%) also could be achieved even if in the case of ulr = 99%. These data confirm that if the height of a decision tree is sufficient (but no more than a half of the number of attribute dimensions), our algorithm enables achieving a better performance on the clustering accuracy even with limited unlabeled data in a certain noise rate (less than 30%). 4.2.3. Performance on Databases with Recurring Concept Drifts. In this subsection, we aim to evaluate whether our technique of REDLLA could handle scenarios where there are recurring concept drifts. In our experiments, we only present tracking results over the sequential data chunks of databases with different types of recurring concept drifts in case of ulr = 90% 3 as shown in Figures 10, 12, and 14 (partial curves are drawn in these figures for clear presentation). Correspondingly, in Figures 11, 13, and 15, we give the prediction accuracy of REDLLA in the same case. In addition, we also give an extreme case of the prediction accuracy, namely without the unlabeled data ( ulr = 0%). It is conducive to show how REDLLA performs in the same environment (namely learning from completely labeled data) compared to other algorithms, includ-ing CVFDT, DWCDS and CDRDT. In these figures, dotted lines are used to mark the drifting positions, and the symbol of  X #instances X  refers to the size of training data.

Performance on SEA with recurringly abrupt concept drifts. First, Figure 10 plots tracking curves over the sequential data chunks of SEA with three different concept drifting periods. We know that SEA is a benchmark database with abrupt concept drifts. From the observation, we can find that: (i) if a concept changes into another concept, the fluctuation of error rates in REDLLA is weaker than those in CDRDT, DWCDS, and CVFDT. It indicates the the impact from concept drifts in REDLLA is less. For exam-ple, in Figure 10(a), the largest fluctuation of error rates is around 20% from Concept-B to Concept-C or from Concept-C to Concept-D while the lowest fluctuation is around 5% from Concept-A to Concept-B if a concept drift occurs. In Figure 10(b), the error rates fluctuate by a range of (10%, 20%) in CDRDT, CVFDT, and DWCDS. However, in REDLLA, the fluctuation of error rates is limited in the range of (5%, 10%). (ii) With the alternative changes of concepts, tracking curves present a similar change trend for all of the same concepts in REDLLA and CDRDT. Meanwhile, in the same concept, error rates of classification converge quickly. It is obviously shown in the case with a 50k-concept drifting period in Figure 10(c) compared to other figures with lower con-cept drifting periods. This is because the more instances a concept contains, the more reduction the error rate of classification presents. However, for DWCDS, the fluctua-tion frequency of error rate is less than that of CVFDT, but the fluctuation range of error rate is larger than REDLLA and CDRDT (see Figure 10(c)). These data confirm that our algorithm is obviously superior to CVFDT and DWCDS in the adaptation to recurringly abrupt concept drifts even in the cases with different concept drifting pe-riods, while it performs better than CDRDT especially in the case with a large concept drifting period.

In addition, to further verify whether the current model in REDLLA adapts to these recurring concept drifts, Figure 11 presents the performance of REDLLA on the pre-dictive accuracy 4 over sequential data chunks of SEA compared to CDRDT, DWCDS, and CVFDT. In this figure, on one hand, we observe that: (1) at the beginning of learn-ing over sequential data chunks (in the first period), the starting prediction accuracy for REDLLA, CDRDT, and CVFDT is lower; this is due to the insufficiency of training data. (2) However, with the increasing size of training data, their predictive accuracies are improved. The predictive curve in REDLLA is similar to CDRDT and both of them perform best, while CVFDT performs worst and DWCDS presents a larger fluctua-tion in predictive accuracy. Specifically, in REDLLA and CDRDT, a higher accuracy is rapidly reached and steadily maintained after the 51st data chunk as shown in Figures 11(a) and 11(b). In Figure 11(c), the predictive accuracy of REDLLA and CDRDT will be maintained steadily after the 201st data chunk (namely, after see-ing all different four concepts-A , B , C ,and D once). That is, the predictive accuracy in REDLLA is not affected by the recurring concept drifts any more after having learned these concepts. However, for the CVFDT algorithm, the predictive accuracy will be steady (maintaining a certain value) until reaching the 401st data chunks. Apparently, CVFDT is impacted from the concept drifting period more than CDRDT and REDLLA. For the DWCDS algorithm, the predictive accuracy first starts from a high value, then converges to a lower value gradually. This is due to the fact that DWCDS owns a cer-tain number of base classifiers (5 classifiers in this algorithm). Each classifier contains ten random decision trees. DWCDS uses the data chunk to create the classifiers and tests the same data chunk in the current ensemble classifier. Overfitting hence occurs. After the generation of base classifiers, a new classifier will be added if the concept drift in the incoming data chunk is detected. That is, the current ensemble classifier will firstly predict the accuracy on the new data chunk, then update the current en-semble classifier or create a new classifier into this ensemble model. Correspondingly, the predictive accuracy jumps down to a lower value. However, with increasing size of the training data, the predictive accuracy will be improved and converge to a value (for example, up to 75% for Sea-interval-50k) 5 . On the other hand, we can see that the average predictive accuracy in REDLLA is improved by no less than 40% compared to CVFDT, it is higher than DWCDS by the range of (10%, 20%), while it is lower than CDRDT by about 4%. These experimental results of REDLLA are obtained in the case of ulr = 90%. Actually, the less the value of ulr , the higher the predictive accu-racy in REDLLA. Thus, REDLLA could outperform CDRDT on the predictive accuracy if the value of ulr is no more than 60%, 60%, and 80%, respectively, in the cases of three periods. In other words, the prediction accuracy in REDLLA could be improved in the different recurring periods and the best value is about 3% in the case of ulr = 0%. These data confirm that on the handling of a database with recurringly abrupt concept drifts, REDLLA outperforms CVFDT and DWCDS very much on the prediction accuracy in cases with different concept drifting periods, while our algorithm achieves better performance in the prediction accuracy compared to CDRDT only if the value of ulr is lower than a certain value (e.g., 60%).

Performance on HyperPlane with recurringly gradual concept drifts. Second, Figure 12 presents the tracking curves over the sequential data chunks of HyperPlane with re-curringly gradual concept drifts. From the experimental results, we can see that in the different drifting periods, there are some fluctuations of error rate even in the same concept for all involved algorithms, but in the whole consideration, the impact from recurring concept drifts on the fluctuation of error rates is little. This is because Hy-perPlane is a very slowly drifting dataset, and the attribute values are changed slowly even in the same concept. In addition, from this figure, we also can see that DWCDS is superior to REDLLA with ulr = 90%, because error rates in the drifting track-ing are lower. To further validate the deviation of performance in these algorithms, Figure 13 reports the predictive accuracy of REDLLA compared to CDRDT, DWCDS, and CVFDT. To be concise, at the beginning of learning over sequential data chunks (in the first period), due to the insufficiency of training data, the starting error rate of classification is higher. This is similar to the Sea database. As the training data arrives, DWCDS performs as well as REDLLA with ulr = 0% and the best prediction accuracy could amount to 80% on HyperPlane-interval-50k. It is superior to CDRDT and CVFDT by 10% and 25%, respectively. Meanwhile, as the drifting period increases, the performance deviation between CDRDT and REDLLA with ulr = 90% is less, and the performance of CDRDT is comparable to REDLLA with ulr = 90% on HyperPlane-interval-50k. This is because the larger drifting period indicates the longer period that a stable classification model is maintained. It is conducive to improve the predic-tion accuracy. These data show that our REDLLA algorithm performs a little worse than DWCDS on HyperPlane with recurringly gradual concept drifts, while it performs much better than CDRDT and CVFDT in cases with different concept drifting periods.
Performance on a real dataset with recurring concept drifts. Finally, Figure 14 reports the tracking curves over the data chunks of a real dataset Elec with two periods. In gen-eral, all algorithms perform similarly for the recurring concepts, but the fluctuation of error rates in a period changes a lot. These cases indicate that the drifting period is less than the monthly-period and the yearly-period. It is necessary to mention that in Figure 14(b), as the training data arrive, error rates of classification in our algo-rithm fluctuate moderately, even with a large margin of reduction (for example, the tracking curve in the period of 96). Meanwhile, with respect to the predictive accu-racy over these sequential data chunks as shown in Figure 15, CDRDT and CVFDT present a low predictive accuracy steadily, while REDLLA gradually increases its ac-curacy. After seeing the 51st data chunk and 101st data chunk for the monthly-period and the yearly-period respectively, a stable prediction model in REDLLA is maintained because the predictive accuracy is steady with a high value. In other words, the pre-dictive accuracy is not affected by concept drifts any more after a certain interval of instances. This is similar to the case in CDRDT. However, the predictive accuracy in REDLLA could be improved by 30% and 20%, respectively, even in the case of ulr = 90% as shown in Figures 15(a) and 15(b) while the best value in the case of ulr = 0% will in-crease by 5% compared to the values in the case of ulr = 90%. This is because there are more than 1/3 instances in Elec containing missing attribute values. It is not beneficial to the correct estimation of information gain for CVFDT and REDLLA, but in the clas-sification, CVFDT takes the simple majority-class method while REDLLA introduces the clustering algorithm at leaves (missing attribute values are considered as zero in the clustering algorithm). In the whole consideration, REDLLA is superior to CVFDT in the classification. However, for CDRDT, the height of the decision tree is set to half of the number of attribute dimensions. Elec contains 5-dimension attributes, and there are 3-dimension attributes containing missing attribute values. The maximum height of decision trees in CDRDT is hence 3. In addition, CDRDT adopts the random feature selection in the split tests. Thus, for each path of the decision tree, there is higher probability that the split-feature is the attribute with missing attribute values. This is a disadvantage to the partition of the instances. Meanwhile, CDRDT utilizes the na  X   X ve Bayes classifier at leaves to test the instances. Missing attribute values will impact the posteriori probability estimation in the na  X   X ve Bayes classifier. Therefore, the performance of classification is worse than REDLLA. In sum, on the handling of the real-world dataset Elec, our algorithm performs as well as DWCDS and presents a prominent advantage compared to CDRDT and CVFDT.

In conclusion, REDLLA always outperforms CVFDT on the handling of three databases with different concept drifts, even in the case with higher values of ulr (90%). However, as compared to CDRDT, REDLLA performs on SEA not as well as that on HyperPlane and Elec. This is because CDRDT could adapt to abrupt concept drifting databases well. Thus, it performs better on SEA than on HyperPlane. Corre-spondingly, the advantage of REDLLA on HyperPlane is more than that on SEA. With respect to Elec, CDRDT is not suitable for this database because of its characteristics with missing attributes. In addition, as compared to DWCDS, our algorithm performs as well as DWCDS on databases with recurringly gradual concept drifts and real-world datasets, while it is superior to DWCDS on databases with recurringly abrupt concept drifts. This is because DWCDS is sensitive to abrupt concept drifts. When it meets the recurringly abrupt concept drifts, it takes the simple mechanism of  X  X iscarding the worse base classifier X  to adapt to the concept drift. It will cause information loss, which is a disadvantage to maintain a high prediction accuracy. 4.2.4. Performance on Data Streams with Few Recurring Concept Drifts. Experimental re-sults shown in Figures 11, 13, and 15 present that our REDLLA algorithm performs best on recurring concept drifting databases. To achieve comprehensive comparisons, we further conduct our algorithm on streaming data without recurring concept drifts and compare against CDRDT, DWCDS, and CVFDT. Figures 16 and 17 present the predictive accuracy over sequential data chunks of Waveform and RBF. From these experimental results, we can conclude that: (i) REDLLA also has a higher prediction accuracy compared to CDRDT, DWCDS, and CVFDT on concept drifting databases without recurring concept drifts. For instance, on the databases of Waveform21 and RBF, REDLLA performs as well as DWCDS in the case without unlabeled data, and it is superior to CDRDT by around 2% even in the case with ulr = 90%. Moreover, the prediction accuracy of REDLLA in the best case with ulr = 0% could be improved by more than 10% and 5% respectively. On the database of Waveform40, the average prediction accuracy of REDLLA in the case with ulr = 90% could be comparable to that of DWCDS while it is lower than that of CDRDT by 5%. However, it will be higher than that of CDRDT if the value of ulr is no more than 80% and the best improvement is around 5% in the case with ulr = 0%. In addition, it performs much better than CVFDT. The prediction accuracy of CDRDT and REDLLA when ulr = 90% could be improved at least 5% on the database of RBF while it is improved more than 35% on the database of Waveform.

Though labeling data would cause extra errors, our algorithm can do a better job on data streams with little recurring concept drifts. The reasons are as follows. (1) With the arriving of training data, more labeled data will be accumulated at leaves and the decision tree grows continuously. Correspondingly, the labeling accuracy will be improved. Thus, the predictive accuracy will converge to a high value gradually. (2) In the incremental test, the prequential error is estimated in the majority-class method using the clustering algorithm. The classification performance is superior to that of the pure majority-class method or na  X   X ve Bayes method. 4.2.5. Runtime Overheads. In the last dimension, we compare REDLLA with CVFDT, DWCDS, and CDRDT on the consumption of runtime. A summary of experimental results in Table I shows that REDLLA and CDRDT perform better than CVFDT and DWCDS. More specifically, (i) on the databases of Hyperplane, Waveform21, Waveform40, and RBF, the time consumption in CDRDT is about 1/2  X  1/3 of that in REDLLA. This is because the deviation of runtime between REDLLA and CDRDT de-pends on the split-tests in the growing of the decision tree and the labeling of the unla-beled data. The more the dimensions of attributes, the more the time overhead. Thus, the time overhead in REDLLA is more than CDRDT in general. (ii) On the database of Sea, however, the runtime overhead in REDLLA is about 1/2 of that in CDRDT at most. The reason is that this database has only three attribute dimensions. The num-ber of instances accumulated at leaves in CDRDT is much more than REDLLA due to the random feature selection used in the training. Meanwhile, the na  X   X ve Bayes classi-fier is adopted at leaves, and the time consumption is hence demanded more heavily in CDRDT. (iii) On the database of Elec, the average time consumption in REDLLA is similar to that in CDRDT regarding the different periods. (iv) Regarding the run-time overhead in DWCDS, it demands are heavier than those in REDLLA. The reason is that the time overhead in the generation of a random decision tree for DWCDS is lighter than that of a decision tree with informed splitting tests in REDLLA, but the number of trees in DWCDS is much more than that in REDLLA, correspondingly the demand on the whole time overhead is heavier for DWCDS. However, for CVFDT, the time overheads are about 5 times more than those in REDLLA and CDRDT. This is due to the fact that a large number of alternative subtrees in CVFDT are generated, especially in the case with more concept drifts. The time overhead is hence demanded heavily. 4.2.6. Performance Comparison with Semi-Supervised Algorithms. The aforementioned ex-periments aim to compare our algorithm against other state-of-the-art classification algorithms conducted on completely labeled databases. However, because our algo-rithm is semi-supervised, in this subsection, we further compare our algorithm against several state-of-the-art semi-supervised algorithms on the performance of predictive accuracy and time overhead. That is, we first compare our REDLLA algorithm with a state-of-the-art semi-supervised algorithm, called tri-training [Zhou and Li 2005] 6 on databases of Waveform (containing 500k-sized training data and 250k-sized test data) and RBF (containing 1000k-sized training data and 500k-sized test data) in the case with ulr = 90%. Experimental results show that: (1) the predictive accu-racy of REDLLA is lower than that of tri-training by 7.0% on Waveform21 (REDLLA: 74.4% versus tri-training: 81.4%), and by 7.9% on Waveform40 (REDLLA: 72.9% ver-sus tri-training: 80.2%), but the time overhead in REDLLA is only 0.38 of that in tri-training on Waveform21 (REDLLA: 111s versus tri-training: 287s), and only 0.36 on Waveform40 (REDLLA: 169s versus tri-training: 459s). (2) On the database of RBF, the predictive accuracy in tri-training (tri-training: 53.1% versus REDLLA: 50.2%) is improved by 2.9% while the overhead of time (tri-training: 342s versus REDLLA: 85s) is demanded more than three times compared to REDLLA. These data show that if the databases contain nonconcept drifts (such as Waveform21 and Waveform40), tri-training presents more advantage on the predictive accuracy, otherwise, this ad-vantage is not obvious. However, REDLLA is more suitable for streaming data with large volumes (especially for concept drifting data streams) because of the lower time overhead. In fact, we also compare our algorithm with tri-training and other semi-supervised algorithms involved in Wu et al. [2006] on a simulated streaming data. This streaming data are generated according to the description in Domingos and Hul-ten [2000]. Experimental results show that the average predictive accuracy classified on 5k-sized test data in REDLLA amounts to 98% even if 101k-sized training data contains less than 80% unlabeled instances. It performs as well as clustering-training [Wu et al. 2006], and outperforms tri-training and other two semi-supervised algo-rithms (cotraining [Blum and Mitchell 1998] and self-training [Zhu 2001]) by a large margin (up to 30%) 7 . Thus, in a whole consideration, we also could draw a conclusion that REDLLA is comparable to other semi-supervised algorithms in the environment of data streams.

In the preceding analysis, we can obtain that REDLLA presents advantage in data streams with recurring concept drifts and unlabeled data. This is because our algo-rithm relies on the semi-supervised mechanism. (1) According to the theory analysis in Eq. (12), the larger the height of the decision tree, the higher the predictive accuracy. In REDLLA, the decision tree could grow with a sufficient height because of the larger threshold of memory overhead. (2) Meanwhile, the clustering algorithm is adopted at leaves, and it is conducive to improve the prediction accuracy compared to that using the majority-class method or the na  X   X ve Bayes method. This guarantees REDLLA per-forms robustly with varying values of ulr . (3) In addition, history concepts are stored and utilized in our algorithm. It benefits adapting to the recurring concepts. However, because the higher the decision tree, the larger the labeling accuracy, thus, to guar-antee the accuracy of labeling unlabeled data, the corresponding memory overhead is heavier. Meanwhile, with the growing of the decision tree, the time overhead is de-manded heavily in the split-test. In addition, the labeling accuracy is not only related to the ratio of labeled data at leaves, but it is impacted from the noisy data. The more noisy data at leaves will lead to worse labeling accuracy. Therefore, our algorithm is suitable for data streams with recurring concept drifts and unlabeled data in a certain noisy environment (no more than 30%). This article presented a semi-supervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data (REDLLA). In this algorithm, we adopt k -means to generate concept clusters at leaves in tandem of incrementally build-ing a Hoeffding tree. Meanwhile, we use the deviation between concept clusters to detect recurring concept drifts. Experimental evaluations reveal that in comparison to several representative concept-drifting data stream algorithms and online semi-supervised algorithms, our REDLLA algorithm is efficient and effective for mining recurring concept drifts even in cases with a large volume of unlabeled data. Mean-while, how to reduce the space consumption, how to fix the periods of recurring concept drifts accurately, how to reduce the noise impact in labeling unlabeled data, and how to apply more distance estimation methods in the drifting detection are still challenging and interesting issues for our future work.

