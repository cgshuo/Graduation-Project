 IBM Almaden Research Center , San Jose, CA 95123 USA The study of generalization abilities of learning algorithms and its dependence on sample comple xity is one of the fun-damental research efforts in learning theory . Understand-ing the inherent dif culty of learning problems allo ws one to evaluate the possibility of learning in certain situations, estimate the degree of condence in the predictions made, and is crucial in understanding, analyzing, and developing impro ved learning algorithms.
 Recent efforts in these directions (Gar g et al., 2002; Lang-ford &amp; Sha we-T aylor , 2002) were devoted to developing generalization bounds for linear classiers which mak e use of the actual observ ed mar gin distrib ution on the training data, rather than relying only on the distance of the points closest to the hyperplane (the  X mar gin X  of the classier). Similar results have been obtained earlier for a restricted case, when a con vex combination of multiple classiers is used as the classier (Schapire et al., 1997). At the heart of these results are analysis techniques that can use an ap-propriately weighted combination of all the data points, weighted according to their distance from the hyperplane. This paper sho ws that these theoretical results can be made practical, be used for model selection, and to dri ve a new learning algorithm.
 Building on (Gar g et al., 2002), which introduced the data dependent generalizations bounds, we rst develop a more realistic version of the bound, that tak es into account the classier' s bias term. An important outcome of (Gar g et al., 2002), which we slightly modify here, is a data dependent comple xity measure for learning which we call the projec-tion prole of the data. The projection prole of data sam-pled according to a distrib ution D , is the expected amount of error introduced when a classier h is randomly pro-jected, along with the data, into a k -dimensional space. Our analysis sho ws that it is captured by the follo wing quantity: a ( D ; h ) = and  X  ( x ) is the distance between x and the classifying hyperplane 1 dened by h , a linear classier for D and b is the bias of the classier . The sequence P ( D ; h ) = ( a 1 ( D ; h ) ; a 2 ( D ; h ) ; : : : ) is the projection prole of The projection prole turns out to be quite informati ve, both theoretically and in practice. (Gar g et al., 2002) pro ved its rele vance to generalization performance and used it to develop sample comple xity bounds (bounds on generalization error) that are more informati ve than exist-ing bounds for high dimensional learning problems. The main contrib ution of this paper is to sho w that the pro-jection prole of the observ ed data with respect to a learned classier can be directly optimized, yielding a new learning algorithm for linear classiers, MDO (Mar gin Distrib ution Optimization), that attempts to be optimal with respect to the mar gin distrib ution based comple xity measure. Specif-ically , we rst argue that this comple xity measure can be used for model selection. Empirically , we sho w that the mar gin distrib ution of the data with respect to a classier beha ves dif ferently than the mar gin and observ e that it is both better correlated with its accurac y and is more sta-ble in terms of measurements over the training data and expected values. With this as moti vation we develop an approximation to Eqn. 1 that is used to dri ve an algorithm that learns a linear classier which directly optimizes this measure. We use several data sets to compare the resulting classiers to those achie ved by optimizing the mar gin, an in SVM, and sho w that MDO yields better classiers. The paper is organized as follo ws. In the next section we introduce the notations and some denitions that will be used in later sections. Sec. 3 introduces our enhanced ver-sion of the data dependent generalization bound based on the mar gin distrib ution of the training data and discusses its implications. In Sec. 4 we sho w that the projection pro-le is not only meaningful for the purpose of generalization bounds but can also be used as a model selection criterion. The MDO algorithms is introduced in Sec. 5 and experi-mental results with it are presented in Sec. 6. We conclude by discussing some open issues. We study a binary classication problem f : IR n ! f X  1 ; 1 g , a mapping from a n dimensional space to class labels f X  1 ; 1 g . Let S = f ( x 1 ; y 1 ) ; : : : ; ( x a sample set of m examples. The hypothesis h 2 IR n is an n -dimensional linear classier and b is the bias term. That is, for an example x 2 IR n , the hypothesis predicts b y ( x ) = sign( h T x + b ) .
 We use n to denote the original (high) dimensionality of the data, k to denote the (smaller) dimension into which projections are made and m to denote the sample size of the data. The subscript will refer to the inde x of the example in the sample set and the superscript will refer to particular dimension that is under consideration.
 Denition 2.1 Under 0 -1 loss function, the empirical err or b E of h over a sample set S and the expected err or E are given resp. by, where I (  X  ) is the indicator function which is 1 when its argument is true and 0 otherwise. The expectation E x is over the distrib ution of data.
 We denote by jj X jj the L 2 norm of a vector . We will assume w.l.o.g that all data points come from the surf ace of unit sphere (ie. 8 x; jj x jj = 1 ), and that jj h jj = 1 (The non unity norm of classier and data will be accounted for by normalizing the bias b .) Let  X  ( x ) = h T x denote the signed distance of the sam-ple point x from the classier h . When x j refers to the j th sample from S , we denote it by  X  j =  X  ( x j ) = h T x With this notation (omitting the classier h ) the classica-tion rule reduces simply to sign(  X  ( x ) + b ) . Our analysis of mar gin distrib ution based bounds is based on results in the area of random projection of high dimen-sional data, developed in (Johnson &amp; Lindenstrauss, 1984), and further studied in several other works, e.g., (Arriag a &amp; Vempala, 1999). Briey , the method of random projec-tion sho ws that with high probability , when n dimensional data is projected down to a lower dimensional space of di-mension k , using a random k  X  n matrix, relati ve distances between points are almost preserv ed. We will use this to sho w that if the data is separable with lar ge mar gin in a high dimensional space, then it can be projected down to a low dimensional space without incurring much error . Ne xt we give the denition of random matrix: Denition 2.2 ( Random Matrix ) A random projection matrix R is a k  X  n matrix whose each entry r ij  X  N (0 ; 1 =k ) . For x 2 IR n , we denote by x 0 = Rx 2 IR the projection of x from an n to a k -dimensional space us-ing projection matrix R .
 Similarly , for a classier h 2 IR n , h 0 denotes its projec-tion to a k dimensional space via R , S 0 denotes the set of points which are the projections of the sample S , and = ( h 0 ) T x 0 j , the signed distance in the projected space. The decision of the classier h is based on the sign of  X  ( x ) + b = h T x + b . Since both h and x are normalized, j  X  ( x ) j can be thought of as the geometric distance between x and the hyperplane orthogonal to h that passes through the origin. Given a distrib ution on data points x , this in-duces a distrib ution on their distance from the hyperplane induced by h , which we refer to as the mar gin distrib ution . Note that this is dif ferent from the mar gin of the sample set S with respect to a classier h , traditionally dened in the learning community as the distance of the point which is closest to the hyperplane. In (Gar g et al., 2002), we have developed a data dependent mar gin bound for the case when the classication is done as b y = sign(  X  ( x )) . Although theoretically one can rede-ne x  X  [ x; 1] and h  X  [ h; 1] and obtain a similar result, it turns out that in practice this may not be a good idea. The bias term b is related to the distance of the hyperplane from the origin and h is the slope of the hyperplane. Pro ving the bound, as well as developing the algorithmic approach (based on Eqn. 3 belo w), require considering a projected version of the hyperplane into a lower dimensional space, k , in a way that does not signicantly effects the classi-cation performance. At times, howe ver, the absolute value of b may be much lar ger than the indi vidual components of h (even after normalization with the norm of h ). In these cases, b will contrib ute more to the noise associated with the projection and it may not be a good idea to project b Due to this observ ation, the algorithmic approach that is based on Eqn. 3 necessitates that we pro ve a slightly mod-ied version of the result given in (Gar g et al., 2002). Theor em 3.1 Let S = f ( x 1 ; y 1 ) ; : : : ; ( x 2 m ; y of n -dimensional labeled examples and h a linear classier with bias term b . Then, for all constants 0 &lt;  X  &lt; 1; 0 &lt; k with probability at least 1  X  4  X  , the expected err or of bounded by with  X  k redened as Note that by choosing b = 0 , the abo ve result reduces to the one given in (Gar g et al., 2002). Although one can pro ve the abo ve theorem follo wing the steps given in (Gar g et al., 2002), we give the sketch of the proof as it is instrumental in developing an intuition for the algorithm.
 The bound given in Eqn. 2 has two main components. The rst component,  X  k , captures the distortion incurred due to the random projection to dimension k ; the second term follo ws directly from VC theory for a classier in k dimensional space. The random projection theorem (Ar -riag a &amp; Vempala, 1999) states that relati ve distances are (almost) preserv ed when projecting to lower dimensional space. Therefore, we rst argue that, with high probability the image, under projection, of data points that are far from h in the original space, will still be far in the projected ( dimensional) space. The rst term quanties the penalty incurred due to data points whose images will not be con-sistent with the image of h . That is, this term bounds the additional error incurred due to projection to k dimensional space. Once the data lies in the lower dimensional space, we can bound the expected error of the classier on the data as a function of the dimension of the space, number of samples and the empirical error there (that is, the rst com-ponent). Decreasing the dimension of the projected space implies increasing the contrib ution of the rst term, while the VC-dimension based term decreases. To get the optimal bound, one has to balance these two quantities and choose the dimension k of the projected space so that the gener -alization error is minimized. The follo wing lemma is the key in pro ving the abo ve theorem; it quanties the penalty incurred when projecting the data down to k dimensional space. For proof, please refer to (Gar g et al., 2002). Lemma 3.2 Let h be an n -dimensional classier , x 2 IR n a sample point, suc h that jj h jj = jj x jj = 1 , and  X  = h T x Let R 2 IR k  X  n be a random projection matrix (Def . 2.2), with h 0 = Rh; x 0 = Rx . Let the classication is done as sign( h T x + b ) . Then the probability of misclassifying relative to its classication in the original space , due to the random projection, is The abo ve lemma establishes a bound on the additional classication error that is incurred when projecting the sample down from an n to a k dimensional space. Note that in this formulation, unlik e the one in (Gar g et al., 2002)) we only project h while keeping b as in the original space. It can be sho wn that in man y cases this bound is tighter than the one given in (Gar g et al., 2002).
 Once the abo ve result is established, one can pro ve the the-orem by making use of the symmetrization lemma (An-thon y &amp; Bartlett, 1999) and standard VC-dimension argu-ments (Vapnik, 1998). See (Gar g et al., 2002) for details. 3.1. Existing Lear ning Algorithms In this section, we attempt to pro vide some intuition to the signicance of considering the mar gin distrib ution when selecting a classier . We note that the discussion is gen-eral although done in the conte xt of linear classiers. It has been sho wn that any classier can be mapped to a linear classier in high dimensional space. This suggests that analysis of linear classiers is general enough and can give meaningful insights. The selection of a classier from among a set of classiers that beha ves well on the training data (with respect to some comple xity measure) is based Vapnik' s (Vapnik, 1998) structural risk minimization prin-ciple. The key question then is, what is the measure that should guide this selection.
 A cartoon example is depicted in Fig. 1. Training data with  X   X   X  corresponds to positi ve and with  X o X  corresponds to negative data. Clearly , there are a number of linear classi-ers that can classify the training data perfectly . Fig. 1(b) sho ws some of these classiers. All these classiers have zero error on the training data and as such any selection criteria based on training error only will not distinguish be-tween them.
 One of the popular measure that guides the selection of classier is the mar gin of a hyperplane with respect to its training data. Clearly , howe ver, this measure is sensi-tive since it typically depends on a small number of ex-treme points. This can be signicant not only in the case of noisy data, but also in cases of linearly separable data, since this selection may effect performance on future data. In the conte xt of lar ge mar gin classiers such as SVM, re-searchers tried to get around this problem by introducing slack variables and ignoring some of the closest points in an ad hoc fashion, but this extreme notion of mar gin still dri ves the optimization. The cartoon in Fig. 1(d) sho ws a hyperplane that was chosen by taking into account all the data points rather than those that are closest to the hyper -plane. Intuiti vely , this looks lik e a better choice than the one in Fig. 1(c) (which would be chosen by a lar ge mar gin classier) , by the virtue of being a more stable measure. In the next section, we argue that one can use projection prole as a model selection criterion and sho w that this hy-perplane is the one that would actually be selected if this model selection criteria is used. The expected probability of error for a k -dimensional im-age of a point x that is at distance  X  ( x ) from an n -dimensional hyperplane (where the expectation is with re-spect to selecting a random projection matrix) is given by the projection prole in Eqn. 1. This expression measures the contrib ution of a point to the generalization error as a function of its distance from the hyperplane (for a x ed k Figure 2(a) sho ws this term (Eqn. 1) for dif ferent values of k . All points contrib ute to the error , and the relati ve contri-bution of a point decays as a function of its distance from the hyperplane. Hypothetically , given a probability distri-bution over the instance space and a x ed classier , one can compute the mar gin distrib ution which can then be used to compute the projection prole of the data.
 The bound given in Thm. 3.1 depends only on the projec-tion prole of the data and the projection dimension and is independent of the particular classier . Of all the clas-siers with the same training error , the one with the least projection prole k value will have the best generalization performance. Therefore, the Theorem suggests a model se-lection criterion. Of all the classiers, choose the one that optimizes the projection prole and not merely the mar -gin. A similar argument, but for a slightly restricted prob-lem, was presented by (Mason et al., 2000). The y argued that when learning a con vex combination of classiers, in-stead of maximizing the mar gin one should maximize a weighted function of the distance of the points from the learned classier . This was used to develop mar gin distri-bution based generalization bound for con vex combination of classiers. The weighting function that was proposed is sho wn in Fig. 2(b). The results of their algorithm sho wed that a weighted mar gin distrib ution (for con vex combina-tion of classiers) is a better comple xity measure than the extreme notion of mar gin which has been typically used in lar ge mar gin classication algorithms lik e SVM. Although the problem addressed there is dif ferent from ours, inter -estingly , the form of their weighting function (sho wn in Fig. 2(b), discussed later) is very similar to ours. In our work, instead of working with the actual projection prole, we work with a simpler form of it given in Eqn. 3. The adv antage of using the projection prole as a model selection criteria is evident from the experiments presented in Table 1. In particular , we took four data sets from UCI ML repository . For each data set, we randomly divided it into a test and a training data set. A classier was learned on the training set and was evaluated on the test data. This experiment was repeated 200 times (each time choosing a dif ferent subset as training data). Each time we computed the actual mar gin of the SVM that was used as the initial guess and the weighted mar gin according to a cost function, given in Eqn. 5, for this classier . Table 1 gives the corre-lation between the generalization error of the classier (its performance on the test data) and these quantities. The rst column in Table 1 gives the correlation between the error on test data and the mar gin on training data. The second column gives the correlation between the error on the test data and the weighted mar gin (projection prole) computed on the training data. Note that here we are looking at the absolute value of the correlation coef cient. The correla-tion results highlights the fact that our new cost function is a better predictor of generalization error . Further evidence is presented in Table 2. It presents empirical evidence to sho w that the cost function given in Eqn. 5 is a more stable measure than the mar gin. The rst column in Table 2 gives the correlation between the mar gin (which is computed as the distance of the closest correctly classied point from the hyperplane) on the training and test data over 200 runs. The second column gives the correlation between the weighted mar gin (projection prole) computed on the training data and test data. Ag ain, a high correlation in the latter case indicates the stability of this new measure. In this section, we introduce a new learning algorithm, the Mar gin Distrib ution Optimization (MDO) algorithm. MDO is dri ven by optimizing with respect to the simpli-ed form of projection prole given in Eqn. 3. The learning algorithm attempts to nd a classier that minimizes a cost function which happens to be the projection prole of the data. The cost function in Eqn. 3 determines the contri-bution we want our selected classier to give to the data points. In order to choose a classier that optimizes with respect to this measure, though, we need to change it in or-der to tak e into account misclassied points. We do that by dening a new cost function which explicitly gives higher weights to misclassied points. Thus, the weight given to an example x i with true class label y i and learned hyper -plane h with bias b is given by
W ( x i ) = That is, for correct classication, the weight is the one given by the projection prole. Ho we ver, for misclassied points, the example is exponentially weighted. The con-stants  X ;  X  allo ws one to control the weight of these two Breast Cancer 0.2782 0.5375 terms.  X  should be thought of as the optimal projection dimension and could be optimized (although at this point our algorithms determines it heuristically). Fig. 2(c) sho ws a family of weight functions as a function of the mar gin  X  ( x ) + b for an example. In this gure, a positi ve mar -gin means correct classication and negative mar gin cor -responds to misclassication. The weight given to points which were classied with good mar gin is very low while points which are either misclassied or very close to the mar gin get higher weights. Minimizing this cost function will essentially try to maximize the weighted mar gin while reducing the misclassication error . Note the close resem-blance to the weighting function proposed by Mason et. al. for the con vex combination case, sho wn in Fig. 2(b). Given a training set S of size m , the optimization problem can be formulated as: Find ( h; b ) such that the follo wing cost function is minimized.
 Here I (  X  ) is an indicator function which is 1 when its argu-ment is true otherwise 0 .  X  is directly proportional to the concept of projection dimension and  X  is a related to the tradeof f between the misclassication error and the projec-tion prole. There are a few observ ations that needs to be made before we proceed further . In most practical cases, learning is done with non-separable training data. Standard learning algorithms lik e SVM handle this case by introduc-ing slack variables and the optimal values of the slack vari-ables are chosen by a search procedure. This mak es the performance of the algorithm dependent on the particular search method. Our formulation automatically tak es care of misclassied points. While minimizing the abo ve cost function, it gives lar ger penalty to points which are in er-ror as against the ones which are correctly classied. The amount of error that will be tolerated can be controlled by choosing  X ;  X  appropriately . One possible way to choose these parameters is by evaluating the learned classier over some hold out set. In our case, we observ ed that in most cases  X  = 1  X  is an estimate of average mar gin given by  X  = P j  X  i j =m for a data set of size m for some h .
 The main computational dif culty in our algorithm lies in the non-con vexity of the objecti ve function. In general, one would lik e to avoid solving a non-con vex problem as it is hard to obtain a globally optimal solution and there is no closed form expression. Ne vertheless we sho w that gra-dient descent methods are quiet useful here. Due to non-con vexity , there are a number of local saddle points and one may not reach a global optimal point. This mak es the choice of starting point extremely critical. In our work, we suggest to use SVM learning algorithm to choose the ini-tial starting classier . Once an initial classier is obtained, the algorithm uses gradient descent over the cost function given in Eqn 5. The algorithm stops once it has reached a local minima of the cost function. Two important points that deserv es attention are (1) the norm of the classier and (2) the non-dif ferentiability of the cost function. In general, by increasing the norm of the classier , we can decrease the value of the cost function (as the mar gin will increase). Therefore, we don' t tak e into account the norm of the classier h in the cost function and at each iteration, after obtaining the updated vector h , we re-normalize it. The second issue is more trick y. The deri vative of the cost function is not dened for those x i which have 0 mar gin i.e.  X  ( x i ) + b = 0 . At such points, we compute both the left and right deri vatives. All the gradient directions are then evaluated and those which lead to maximum decrease in the cost function is selected. The algorithm terminates if no such gradient direction are found.
 The algorithm given in Fig. 7 gives the implementation de-tails. It starts with learning a classier using SVM learning algorithm obtaining ( h; b ) . We assume 2 that jj h jj = 1  X  are then initialized (as abo ve). MAX CONST -maxi-mum number of points with zero mar gin that will be con-sidered at any given iteration of the algorithm. Since at points with zero mar gin, we are evaluating both the left and right deri vative, the number of directions considered is exponential in the number of points with zero mar gin, and as such we limit the number of such points (for which both gradient directions are considered) to MAX CONST . MDO is the main procedure. If there are no points with 0 mar gin, the algorithm simply computes the gradients as:
These deri vatives are then used by procedure Update to obtain the updated values of ( h; b ) given by ( h 0 ; b 0 ) updating, we mak e sure that cost function is decreasing at each iteration. Ho we ver, if there are points with 0 mar -gin, we randomly select of MAX CONST such points, and at those we compute all the possible gradients (computing both left and right deri vatives). The other points with zero mar gin are ignored. Once we have the deri vative of the cost function, the same update procedure, as abo ve, is called. While doing the experiments, we observ ed that the algo-rithm typically con verges in a small number of steps. In this section, we pro vide the empirical comparison of the MDO with SVM. Note that although, in our experiments, MDO' s starting point is the SVM classier , the results ob-tained by MDO could be worse than SVM. Indeed, the selection of the new classier is dri ven by our new opti-mization function and it could lead to lar ger error . Inter -estingly , when doing the experiments we observ ed that in man y cases we obtained better generalization at the cost of slightly poor performance on the training data. To under -stand this we analyzed in detail the results for the breast cancer dataset from UCI ML repository . In Fig. 3 we plot the mar gin and the value of the cost function given in Eqn. 5 over the gradient descent iterations. As expected, while the cost function of MDO impro ved (decreased in value) the mar gin deteriorated (since we started with SVM, the mar -gin at 0 th iteration is maximal). Fig. 4 is even more reveal-ing. We sho w how the training and test error changed with the iterations of the gradient descent. Interestingly , the er-ror on the training error went up from 5% at 0 th iteration to 9% . The algorithm traded the increase in training error with the decrease in projection prole. The second plot gives the test error and as evident, the test error went down with more iterations. The gap between the training error and test error also decreased which sho ws that the new classier gener -alized well and the cost function proposed in Eqn. 5 is a better measure of learning comple xity .
 In Table 3, we compare the classication performance of MDO and SVM on a number of datasets from UCI ML repository and on two real world problems related to con-text sensiti ve spelling correction. The spelling data is tak en from the pruned data set of (Golding &amp; Roth, 1999). Two particular examples of conte xt sensiti ve spelling correction, being &amp; begin and peace &amp; piece , are considered. For de-tails on the dataset see (Murph y, 1994). In all experiments, data was randomly divided into training ( 60% ), and test set ( 40% ). The experiments were repeated 100 times each time choosing a dif ferent subset (randomly chosen) of training and test data. It is evident that except for a few case (in which the impro vement is not statistically signicant), in most cases MDO outperforms SVM. We presented a new, practical, mar gin distrib ution based comple xity measure for learning classiers that we deri ved by enhancing a recently developed method for developing mar gin distrib ution based generalization bounds. We have sho wn that this theoretically justied comple xity measure can be used rob ustly as a model selection criterion and, most importantly , used it to dri ve a new learning algorithm for linear classiers, that is selected by optimizing with re-spect to this comple xity measure. The results are based on a novel use of the mar gin distrib ution of the data relati ve to the learned classier , that is dif ferent than the typical use of the notion of mar gin in machine learning. Consequently , the resulting algorithm is not sensiti ve to small number of samples in determining the optimal hyperplane.
 Although the bound presented here is tighter than existing bounds and is sometimes informati ve for real problems, it is still loose and more research is needed to match observ ed performance on real data. Algorithmically , although we have given an implementation of the new algorithm MDO, one of the main direction of future research is to study it further , as well as to investig ate other algorithmic implica-tions of the ideas presented here.
 Ackno wledgments: This research is supported by NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-9984168 and an IBM fello wship to Ashutosh Gar g.
