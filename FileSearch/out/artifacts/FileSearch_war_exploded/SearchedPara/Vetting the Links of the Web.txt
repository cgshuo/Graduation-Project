 Many web links mislead human surfers and automated crawlers be-cause they point to changed content, out-of-date informati on, or invalid URLs. It is a particular problem for large, well-kno wn di-rectories such as the dmoz Open Directory Project, which mai n-tains links to representative and authoritative external w eb pages within their various topics. Therefore, such sites involve many ed-itors to manually revisit and revise links that have become o ut-of-date. To remedy this situation, we propose the novel web mini ng task of identifying outdated links on the web. We build a gene ral classification model, primarily using local and global temp oral fea-tures extracted from historical content, topic, link and ti me-focused changes over time. We evaluate our system via five-fold cross -validation on more than fifteen thousand ODP external links se -lected from thirteen top-level categories. Our system can p redict the actions of ODP editors more than 75% of the time. Our model s and predictions could be useful for various applications th at depend on analysis of web links, including ranking and crawling. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [I nfor-mation Storage and Retrieval]: Online Information Service s X  Web based services General Terms: Algorithms, Experimentation, Measurements Keywords: Link analysis, Web decay, ODP, web archives
The Web is in constant flux. Page content and links are changed , added, and removed from the Web on a continuous basis [2, 3, 5, 6]. This presents a significant challenge to the fundamental tec hnology of the web X  X ypertext X  X s the target of a link, if it exists at a ll, is often not the same as when the link was first created. About two -thirds of web pages change their content each year. As a resul t, many links on the Web are obsolete. At best, such links point t o forgotten sites that have long been abandoned. At worst, suc h links point to sites whose original purpose has been subverted, pe rhaps in an attempt to exploit the value of links to search engine ra nking. High-profile sites must be especially vigilant (and therefo re should be carefully maintained) as links from such sites are partic-ularly valuable because of visitor traffic or reputation flow or both. Most existing work on web link maintenance involves identif ying error-generating pages by developing tools to find invalid l inks, to automatically correct links that lead to file not found error s [10] and to find irrelevant (and possibly annoying or offensive) spam pages [12, 17]. However, none consider that the link context chang es over time. That is, the two end points of a link may change in differ ent  X  X irections X  rendering the link inappropriate.

Therefore, in this paper we introduce a new task X  X hat of vett ing web links. Our goal is to better understand which factors are impor-tant for detecting out-of-date page citations such that we c an build a link vetting system, from which web content providers woul d benefit tremendously from automatically identifying wheth er links on their pages need to be re-examined. In particular, large, well-known sites like the Yahoo! directory [18] and the dmoz Open D i-rectory Project [14] involve many editors to maintain the ac curacy and representativeness of their categories by manually and periodi-cally checking the quality of the external pages to which the y link. Our system can make their maintenance work less expensive. We state the link vetting problem formally as follows.

Definition 1. A link l is defined as an outdated link iff its web provider or link maintainer removes or should remove it from the original source web page. Otherwise, we consider it as a fres h link. We assume the link was fresh when it was first created. Changes in both source page and target page causes the link to decay over time. Therefore, we believe that our definition of an outdated link covers the basic aspects of out-of-date links. However, we only des cribe a binary judgment on whether a link is outdated or not.
 Our link vetting problem is defined as follows:
Definition 2. The link vetting problem : Suppose a link l was created at time t 0 ; determine whether this link is outdated at time point t 1 , given t 0 &lt; t 1 .

This problem can have two variants by considering the curren t time point t 2 . When t 2 = t 1 &gt; t 0 , this problem is to identify the outdated links. When t 1 &gt; t 2 &gt; t 0 , the problem turns to predict whether a link will become out-of-date at a given future time point. We mainly focus on the former variant in this work; however, w e also investigate the models X  predictability in Section 3.
We consider the task of determining whether links are outdat ed as a classification problem. Our link vetting system (LVS) bu ilds up a general classification model by combining multiple grou ps of temporal features extracted from local and global historic al infor-mation, such as content, topics, etc. These features can rep resent basic information about link context change over time.
In order to map link context changes onto a time axis, we dis-cretize time into multiple intervals. We use a page snapshot at one time point to represent the page situation during the time in terval which covers that time point. While the time interval could b e mea-sured by any time units, we use Year as our basic unit to measure link context change. We base our work on the assumption that a link was fresh when it was first created. By comparing the snap -shot for each time unit with that for the first time unit and suc ces-sive time units, we can know how the page changes over the time units, and potentially know whether such changes make the li nks between pages stronger or weaker.

Table 1 defines some notation. We use the following metrics to measure the difference between two snapshots: where A and B are two sets which contain some elements, and and  X  X  X  v are two vectors with m dimensions.
Our features are extracted from the changes of title, meta in for-mation, content, link, topicality (defined in Section 2.2.2 ) and the fraction of words in our predefined list. These changes are de ter-mined by the comparison between different historical snaps hots of both source pages and target pages. Most of these features em pha-size the comparison between snapshots. The main feature list is listed in Table 2.
Local features are organized to represent the characterist ics of link context changes over time.

Features based on title, meta information and content. We use the downloaded snapshots for each target page and collec t the terms in different page fields, such as title, keywords, desc ription and content. We use JC, ERC and EAC (see Section 2.1) in each field as our features to compare the similarity between two sn ap-shots. For meta information recorded by the IA, we also check finer fields within it, including HTML content base, returned HTTP status code and so on, which reflect the state of the target pag e.
Features based on time measures. This group of features repre-sents the time information hidden in the content of historic al snap-shots. We treat  X  X ear X  as the atomic time unit, and extract al l the time information which is presented by year, such as 1999, 20 00 and so on. Combining the last-modified time in meta informati on (about 60% of the pages show last-modified time in returned HT TP information), we calculate a time-based distribution with respect to each snapshot. We use the L 1 distance between two compared snapshots as our features to represent time evolution.

Features based on global bi-gram and tri-gram lists. We man-ually build up two lists composed of bi-grams and tri-grams. One list records 45 representative phrases which show that the p age con-tent is still incomplete (e.g.,  X  X nder construction X , etc. ). The other records 45 phrases which relate to the professionality or tr ustwor-thiness of the pages/snapshots (e.g.,  X  X ll right reserved X  , etc.). The statistics for each of these records the presense (or absenc e) of that phrase in a page snapshot under consideration. This group of fea-tures is based on the comparison of the statistics between sn apshots.
Features based on category. Based on the twelve selected top-level ODP topics, we use a well-known naive Bayes classi fier  X  X ainbow X  [11] trained on the texts of 1000 randomly selecte d doc-uments per class from the ODP. We then classify each page snap -shot by the trained classifier. For each snapshot, we produce a topic distribution vector, which presents the normalized probab ility that the snapshot belongs to each topic. This group of features is calcu-lated from the L 1 distance of topic distributions between two com-pared snapshots, either between source and target page snap shots in the same time unit, or between the same target page in snapsh ots in two different time units.

Features based on links. This group of features checks the con-sistency of the outgoing links between two compared snapsho ts. We also compare the information about  X  X ailto: X , which may r e-flect the page X  X  trustworthiness to some degree. In addition , the change of frameset information gives some clue about the pro ba-bility that a page becomes a redirection or cloaking page.
Global features use the knowledge based on characteristics of members of a calculated group/cluster. They reflect the back ground (a) Feature source type per bucket, ordered by information gain. Figure 1: Distribution of feature discriminability with re spect to their category times and source year. of the whole group. The deviation of features from such a back -ground will make these extracted features more discriminat ive.
Features based on topicality inferred from pre-computed language models (LMs). Given a time point t 1 , we calculate sep-arate language models based on the content corresponding to out-dated link context and fresh link context respectively. Our hope is that we train a set of outdated topics from outdated link cont exts, and a set of fresh topics from fresh link context for each time point, and track how close the topical distribution of an unseen lin k con-text is to these two sets of topics and how these distances cha nge over time.

We first cluster the link contexts into different groups, wit h each sharing some similar characteristics. The reason is that th e outdated topics and/or the fresh topics should be different among gro ups. Hence, we group link contexts according to the category of so urce pages or the anchor text on them. For each group, we compute separate sets of latent topic-specific language models from all the snapshots of the target pages within the link contexts in tha t group, with one calculated by using those involved in outdated link con-texts and the other one by the target page snapshots in fresh l ink contexts. Define w as a word in the dictionary. Define  X  1 as unigram language models for k topics.  X  d,j is the mixture weight ( P k j =1  X  d,j = 1 ). We use the EM algorithm in pLSA [7] to esti-mate the parameters  X  d,j for each snapshot and p ( w |  X 
Next, we define the topic centroid of the outdated or fresh lin k contexts of the i th cluster/group as 1 | C s  X  X  fresh, outdated } and C i,s  X  represents the corpus composed of all the page snapshots involved in the link contexts of the i cluster. We use the topic centroid as our estimation of p (  X  a unseen link context, we estimate  X  d,j of involved page snapshot content by the pre-computed language models. Bayes X  rule in fers where p (  X   X  j | w ) unravels the contribution of w to the j subtopic. Therefore, the  X   X  d,j can be given by 1 | d | P We can use |  X   X  d,j  X  p (  X   X  j ) | to represent the distance of the j subtopic distribution within a link context from the backgr ound of its clusters. Thus, this group of features tracks how far the hidden topic distribution of a link context is from the outdated/fr esh back-ground distribution and how prominent a link context is on a p ar-ticular fresh/outdated subtopic when considering the back ground in different time units.
We explore a variety of classification algorithms with respe ct to their capability to detect outdated links. Specially, we se lect 37 classifiers implemented in the Weka toolkit [15] and evaluat e them on the proposed task in Section 3. Our selected classificatio n al-gorithms include multiple classifiers in the decision tree f amily, support vector machine, NaiveBayes, rule generators, boos ting and other meta-learning methods. We believe these classificati on algo-rithms can represent the state-of-the-art. Thus, by explor ing these classification algorithms, we find which classification algo rithms are suitable for this task and how well LVS can be generalized .
We use the ODP data set, which is based on the external pages cited by dmoz Open Directory Project and corresponding hist orical snapshots provided by the Wayback Machine service offered b y the Internet Archive [8].

By exploring the historical ODP RDF file and content file, we get some statistics about the external link removal. In orde r to set up a realistic link vetting task, we define our task as to determine whether the links to selected external pages will be removed by 2007 (decisions by ODP editors) . We first use the 2008 ODP RDF file to extract active categories. There are 756,439 categor ies in total. We randomly select 15,910 external pages among these cat-egories, which have complete historical snapshots from the year in which they are first observed in the ODP directory to the year 2 007 as our data set for training and testing the classifier. The re moved external pages (outdated external links) are labeled as posi tive ex-amples, while those remaining are negative.
We first select the most suitable feature set for this task. By basing our feature selection on Information Gain (IG), we ca n un-derstand which features are more discriminative for the task . We sample from a larger data set, excluding the 15910 examples, and perform feature selection on it. The distributions of featu re infor-mation gain with respect to their category and time units are de-picted in Figure 1. The x-axis is the buckets from the ranking of feature IG values. From Figure 1(a), the most discriminativ e fea-tures focus on content, LMs and title related features. Espe cially the top 10 features are all from the LM group covering from 200 6 to 2007. The category and meta related features gradually do minate the bucket with the decrease of IG value rankings. Interesti ngly, the features about outgoing links don X  X  show good discriminabi lity in this task. Figure 1(b) demonstrates that the feature discri minabil-ity highly correlates with the time point from which the feat ures formed. Earlier features show poorer discriminability.

We tried 27 different feature set sizes, and trained the 37 cl as-sifiers provided by Weka. We found most of these classifiers ge t their best performance when using the top 200 discriminativ e fea-tures selected by IG values. Hence, we choose to present the c las-sification performance based on these 200 features. The comp ar-ison among multiple classifiers are all based on five-fold cro ss-validation, shown in Table 3. The top four classifiers for thi s task are EnsembleSelection, Bagging, DecisionTable, and R EP-Tree, where EnsembleSelection gets the best performance on all the four metrics we used. We also list the performance of some traditional classifiers, including C4.5 decision tree. The Random-Forest classifier in decision tree family can achieve a F-mea sure of 0.749 while NB has the lowest F-measure and accuracy. We also
Table 3: Classification performance results on ODP dataset. found the performance of many classifiers is quite close to th e high-est one. The deviation of the top 4 classifiers on F-measure is only 0.0127, which demonstrates that the system can provide a gen eral classification model independent of the specific classifier.
We explore the predictability of classification models by re mov-ing all the features involved in the information from 2007 (T able 3, rightmost columns). From Table 3, all classifiers reveal an i nferior performance to those trained by all the features on both F-me asure and accuracy. In particular, Adaboost shows the best capabi lity of prediction of future outdated links, with only a 2.7% decr ease in F-measure performance. In contrast, NB shows the worst pr e-dictability performance since its F-measure score decreas es 19.4%.
Web link maintenance involves significant manual labor to de tect outdated links based on complex and diverse criteria. Exist ing re-search work on web link maintenance, using only a snapshot of the current web, typically focuses on one specific criteria for d etecting pages/links which violate it. Tools such as W3C Link Checker [16] can automatically identify error generating pages. Some re cent re-search work extends this task by automatically correcting b roken links on the web [9, 10].

Some researchers extracted useful temporal data for web inf or-mation retrieval tasks. Nunes [13] identifies temporal web e vidence by using two classes of features based on individual informa tion and global information. They also propose several sources o f tem-poral web evidence, including document-based and web-base d ev-idence, which can be utilized in improving multiple retriev al tasks. Researchers at Google filed a patent [1] on using historical i nfor-mation for scoring and spam detection. Berberich et al. [4] p ropose two temporal link analysis ranking algorithms which incorp orate pages X  temporal freshness (timestamps of most recent updat es) and activity (update rates), and improves ranking performance . Yu et al. [19] incorporate temporal factors to overcome the probl em that traditional link analysis approaches favor old pages by int roducing a temporal weight into the PageRank algorithm, which decrea ses exponentially with citation age.
Many web links reflect choices and information that, while va lid at time of link creation, are now woefully out-of-date. In th is work we have proposed a new web mining task of vetting web links. As an initial attempt to satisfy this task for links of the ODP di rectory, we have presented a classification performance comparison a mong a variety of state-of-the-art classifiers on this task, trai ned by tem-poral features extracted from the historical link context, including content, category, and link-based information from histor ical snap-shots of both source and target pages. Our proposed system is able to achieve a F-measure of 0.782 when compared to ODP editor removal actions. This evidence suggests that, with sufficie nt, co-herent archival data, it is possible to vet automatically ma ny links of the web, and points the way to new tools to help maintain the accuracy of the Web, benefitting various applications that d epend on analysis of web links beyond web site maintenance, includ ing crawling, ranking, and classification.
 This work was supported in part by a grant from the National Sc i-ence Foundation under award IIS-0803605 and an equipment gr ant from Sun Microsystems. We also thank Dennis Fetterly, Liang jie Hong, Gordon Mohr and Xiaoguang Qi for helpful feedback.
