 1. Introduction: the semantic annotation challenge
The Information Revolution has brought about wonderful new opportunities for humans and organizations alike. At the same time, it has created an unprecedented and growing information overload. The Semantic Web initiative [1] aims to address this problem by rendering web data accessible to both humans and software agents. This is to be achieved by enriching web data with semantic annotations . These are similar to XML annotations in that they structure text into (usually hierarchically organized) text fragments, as in &lt;/person&gt;
Unlike XML annotations, however, semantic annotations come with definitions of the annotations used, e.g.,  X  X  X erson X ,  X  X  X esidence X . These definitions may be given in terms of an ontology that employs a rich modeling language such as OWL, or in terms of a conceptual schema using UML class diagrams or the Entity-Relationship model. In the rest of the discussion, we call the collection of definitions of the annotations used a semantic model .

Annotations assign a meaning to web text fragments, thereby facilitating their processing, for example, populating a data-base by extracting data from text. The main thrust of the Semantic Web is exactly this point: web data should be bundled along with their semantics, provided by a semantic model (ontology or conceptual schema). Unfortunately annotating web data to render them  X  X  X emantic X  is an expensive, laborious and error-prone process and only accounts today for a small frac-tion of web data. Accordingly, much research effort is being invested in developing annotation tools that automatically, or semi-automatically (with a human in the loop) annotate web data with respect to a semantic model.

The main objective of this work is to explore the use of light-weight tools and techniques for the semi-automatic anno-tation of web data. Our framework is light-weight in two ways. Firstly, the semantic model used is defined in terms of UML class diagrams. As such, it is less expressive than ontologies defined in terms of description logics, such as OWL. Secondly, our framework analyzes text to determine where to introduce annotation by exploiting software source code analysis tools and techniques from Software Engineering (more precisely, Reverse Engineering). Conceptual modeling techniques are less expressive than their AI cousins, but have a huge advantage over their cousins: they have been used in research and practice for decades and have passed the test of usability and scalability. The same applies for code analysis techniques over Natural
Language Processing (NLP) techniques. The latter involve large (syntactic) rule sets and special mechanisms for dealing with anaphora and other natural language phenomena. Code analysis techniques, on the other hand, use small sets of context-dependent rules and rely on elaborate, experimentally-determined processes to deliver good results even when their input includes millions of lines of code.

But of course, light-weight techniques have an advantage over their counterparts only when they deliver good enough results. Accordingly, along with the development of the framework, called Cerno, cations to several case studies with a range of experimental results. Two of these are reported here.

The Cerno framework consists of (i) a systematic process for defining keyword and grammar-based rules for identifying instances of concepts in a text, and (ii) an architecture based on software design recovery for applying the rules to mark up and extract identified instances in a document set. The case studies we report on involve accommodation advertisements in derived from a set of user queries. In the second experiment, the annotation process is based on a more elaborate model derived from expert requirements.

Our conclusions from these experiments are positive and suggest that light-weight techniques have a place in the tran-sition from the Web to the Semantic Web, especially so in the annotation of bulk data.

There are several requirements that had to be addressed in designing Cerno:  X  Adaptability to a diversity of document formats and domains. In order to make the framework less dependent on  X  Portability of the framework across domains. External data resources, such as manually annotated training corpora, are  X  Accuracy and efficiency. Our framework is intended for large textual documents that need to be analyzed quickly, but  X  Scalability. Large scale with respect to any dimension  X  e.g., grammar coverage, vocabulary, domain modeling, adap- X  Evaluation. Performance of a semantic annotation tool needs to be assessed on a standard dataset. However, such
As indicated earlier, this paper presents two case studies involving different domains. During these studies, the domain-dependent components were adjusted to reduce human work required to tune the tool for a different domain. Moreover, we domain models needed by the Cerno framework.

The two fields of semantic text annotation and code analysis seem worlds apart, but actually share common goals and approaches. Similarly to the domain of web data, legacy software source code analysis became prominent a few years ago, thanks to the infamous  X  X  X ear 2000 X  problem [2]. More specifically, the main goal of the software design recovery process is understanding the structure of a software system by interpreting source code fragments in formal terms described by a grammar and a domain model [7]. Accordingly, the two tasks have the following similarities:  X  The need to interpret the source text according to a semantic model . In fact, in both areas the goal is to classify textual
Our evaluation of both experimental studies uses a three-stage evaluation framework which takes into account:
Elements of this research have been presented in a series of conference papers. For instance, an early version of the frame-work was presented in [19]. Some excerpts of the results of the first case study have been presented in [46], as well as the second case study [20]. This paper extends and integrates these earlier works with further experiments, analyses and dis-cussion to offer a complete account of the Cerno framework.

The rest of the paper is structured as follows: Section 2 describes the challenges of semantic annotation task and provides an overview of work in the area. The semantic annotation method we propose is introduced in Section 3. Sections 4 and 5 discuss the experimental setup and evaluation method, while the evaluation results are presented in Section 6. Finally, con-clusions and directions for future work are presented in Section 7. 2. Overview of semantic annotation tools
The semantic annotation problem comprises tasks that have been traditionally researched in diverse areas, such as Infor-mation Extraction, Data Mining, and Document Categorization. For this reason, existing semantic annotation approaches dif-terms of different semantic model adopted, expected document format, and domain considered  X  make comparison of semantic annotation tools very difficult. The present survey highlights such differences and focuses only on tools that have a substantial degree of automation, ignoring authoring environments for manually generating semantic annotations.
Several methods use structural analysis to annotate web documents. Among these is SemTag [8], which performs auto-mated semantic tagging of large corpora. SemTag annotates text with terms from the TAP ontology, using corpus statistics to improve the quality of annotations. The TAP ontology contains lexical and taxonomic information about a wide range of named entities, as for instance, locations, movies, authors, musicians, autos, and others. SemTag detects the occurrence of these entities in web pages and disambiguates them using a Taxonomy Based Disambiguation (TBD) algorithm. A large-scale evaluation was fulfilled on a set of 264 million web pages; the total time required for annotation was 32 h. The performance was estimated based on 200 manually evaluated text fragments. Approximately 79% of the annotations were judged to be correct with accuracy of about 82%. Human intervention was required at the disambiguation stage to tune the algorithm.
Another semantic annotation environment framework is CREAM (CREAtion of Metadata) [16]. Annotations are generated either manually  X  by typing or in a drag-and-drop manner, associating instances with the concepts appearing in the ontology browser,  X  or semi-automatically  X  by using wrappers and information extraction components. OntoAnnotate and OntoMat
Annotizer are different implementations of the CREAM framework. In particular, OntoMat Annotizer is a user-friendly tool that exploits the structure of HTML documents to infer annotations and helps the analysts to collect knowledge from doc-uments and Web pages and to enrich an ontology with metadata. OntoMat uses regularity in a collection of documents to quickly induce extraction rules from few human annotations. Therefore, the input collection must be very consistent in its structure. OntoAnnotate is the commercial version of OntoMat. CREAM is well suited for highly structured web documents, while for annotating HTML pages of a less structured nature, SemTag is more appropriate.

Wrapper induction methods such as Stalker [26] and BWI [13] try to infer patterns for marking the start and end points of fields to extract. Wrappers mine the information using delimiter-based extraction rules. When the learning stage is over, complete phrases related to the target concepts are identified. The biggest advantage of wrappers is that they need a small amount of training data. Alternatively, extraction rules for wrappers can be specified manually, as it is done in the World
Wide Web Wrapper Factory [34]. However, whether they are generated manually or semi-automatically, wrappers strongly rely on document structure and work best for collections of rigidly structured Web pages.

The semantic annotation task tackles the same class of text analysis problems that Artificial Intelligence-(AI)-based NLP annotations. To achieve good quality results, such tools employ NLP methods that require large computational and/or mem-ory resources. The KIM (Knowledge and Information Management) platform [17] is an application for automatic ontology-based annotation of named entities. Similar to SemTag, KIM focuses on assigning to the entities in the text links to their semantic descriptions, provided by an ontology. The analysis is based on GATE (the General Architecture for Text Engineer-ing) [15]. KIM recognizes occurrences of named entities from the KIMO ontology that, apart from containing named entity classes and their properties, is pre-populated with a large number of instances. The generated annotations are linked to the entity type and to the exact individual in the knowledge base. Evaluation of KIM was performed over a 1 Mb text corpus modular architecture allowing users to combine tools for each text analysis task they may want to address. The ontology-based gazetteer processing module of GATE supports editing of the gazetteer lists, which reminds creation of an annotation schema in Cerno. However, the nature of Cerno X  X  modules is different, as it does not employ any linguistic analysis, such as part of speech recognition or chunk parsing, instead relying on the TXL engine.

Along similar lines, Unstructured Information Management Architecture (UIMA) [11] uses the same kind of linguistic analysis as in GATE to associate metadata with text fragments. OpenCalais [28] goes beyond using natural language process-ing and machine learning tools to recognize named entities in textual content, and also provides links between related knowledge pieces.

Armadillo is a system for unsupervised automatic domain-specific annotation on large repositories [4]. This tool employs an adaptive information extraction algorithm, learning extraction pattern from a seed set of examples provided, and gener-alization over the examples. Learning is seeded by extracting information from structured sources, such as databases and automatic annotation of examples for the other structured sources or free text. Retrieved information is then used to par-tially annotate new set of documents. Then, the annotated documents are used to bootstrap learning. The user can repeat the bootstrapping process until obtaining the annotations of the required quality. Armadillo has been used in a number of real-life applications: mining web sites of Computer Science Departments, information extraction about artworks by fa-mous artists, and the discovery of geographical information.
 Pankow (Pattern-based Annotation through Knowledge on the Web) is an unsupervised learning method for annotation
Web documents based on counting Google hits of instantiated linguistic patterns [3]. The approach uses linguistic patterns to identify ontological relations and the Web as corpus of training data to bootstrap the algorithm and thus overcoming the problem of data sparseness. Similarly, the method of S X nchez and Moreno [35] identifies non-taxonomic ontological rela-tionships using the Web as learning repository. Web content is exploited to extract domain-related linguistic patterns for such relationships.

Apart from structure-and NLP-based approaches, there is another group of tools that uses pattern-based extraction rules for semantic annotation. KnowItAll is a system for large-scale unsupervised named entities recognition, which uses domain-independent rules to locate instances of various classes in text [10]. The tool aims at extraction and accumulation of basic facts, as person X  X  names, from the large collections of Web documents in an unsupervised, domain-independent, and scalable manner. The tool does not need a set of seeds; instead it relies on automatically generated domain-specific extraction rules. The rules are based on a set of keywords, for example, a rule  X  X  X ities such as X  is used to automatically extraction rules.

The ontology-based method of Wessman et al., Ontos, is aimed at processing preferably semi-structured Web pages with multiple records and relies primarily on regular expressions [40]. The evaluation of Ontos was performed for the set of obit-uaries from two newspapers containing a total of 25 individual obituaries annotating both named entity and general con-cepts. As a result, dispersed values for recall and precision for different concepts were obtained; both overall recall and precision varied from 0% to 100%.

In contrast to NLP-based approaches, our approach does not utilize any linguistic patterns, but combines keyword-and structure-based annotation rules. In this sense, our technique is light-weight. Moreover, unlike most of the methods dis-cussed in this section whose the success is largely determined by their focus on identifying and classifying various named the semantic model may differ depending on the task or type of document. Therefore, it seems appropriate to develop a to Wessman X  X  approach that relies primarily on regular expressions to identify instances in structured web sites. Our ap-proach combines context-free robust parsing and simple word search to annotate relevant fragments of unstructured text.
The method applies a set of rules constructed beforehand to guide the annotation process. Some of these rules are actually generic and therefore reusable. Wrapper induction methods also relate well to our work. When these methods are applied, result in a fundamentally different way  X  by predicting start and end points using phrase parsing in advance, rather than phrase induction afterwards.

There are tools that use reverse engineering technique to identify transaction service elements in existing electronic ser-engineering techniques to deal with unstructured textual content. Artifacts extracted by SmartGov are largely determined by webpage design, thus the system deals with traditional reverse engineering tasks. Cerno, on the other hand, aims at identi-fying and classifying text fragments that could be previously dealt with only by heavyweight linguistic analysis.
Concluding our summary of related work, we can say that, in contrast to all listed approaches, our method uses context-independent parsing and does not rely on any specific input format. 3. Method
Over several decades, the software source code analysis area has accumulated a wealth of effective techniques for addressing some of the problems that semantic annotation faces now. In order to cope with the Year 2000 problem, some techniques for automating solutions utilized design recovery , the analysis and markup of source code according to a semantic design theory [2]. Formal processes for software design recovery utilize a range of tools and techniques designed and proven efficient to address these challenges for many billions of lines of software source code [5]. One of these is the generalized parsing and structural transformation system TXL [6], the basis of the automated Year 2000 system LS/2000 [7]. Given the need for cost-effective tools supporting the semantic annotation process, we propose applying a novel method based on software code analysis techniques for the task of semantic annotation of natural language texts. In this section, we de-scribe the process and the architecture for the processing of documents so that they can be annotated with respect to a semantic model. 3.1. TXL as an instrument for semantic annotation
TXL [6] is a programming language for expressing structural source transformations from input to output text. The struc-ture to be imposed on input text is specified by an ambiguous context free grammar. Transformation rules are then applied, and transformed results are represented as text. TXL uses full backtracking with ordered alternatives and heuristic resolu-
The transformation phase can be considered as term rewriting, but under functional control. Functional programming control provides abstraction, parameterization and scoping. TXL allows grammar overrides to extend, replace and modify existing specifications. Grammar overrides can be used to express robust parsing , a technique to allow errors or exceptions in the input not explained by grammar. Overrides can also express island grammars . Island parsing recognizes interesting of the parse to each individual transformation task.

Originally, TXL was designed for experimenting with programming language dialects, but soon it was realized it could be preprocessors, theorem provers, source markup, XML, language translation, web migration, and so on. TXL has also been suc-cessfully used for applications other than computer programs, for example handwritten math recognition [44], document table structure recognition [45], business card understanding [27], and more. 3.2. The Cerno architecture
The architecture of Cerno is based partly on the software design recovery process of the LS/2000 system [7], although in that case the documents to be analyzed were computer programs written in formal programming languages, and the markup process was aimed specifically at identifying and transforming instances of Year 2000-sensitive data fields in the programs.
The Cerno adaptation and generalization of this process to arbitrary text documents includes four steps: (1) document parse, (2) recognition of basic facts, (3) their interpretation with respect to a domain semantic model, and (4) mapping of the iden-tified information to an external database. Each of the processing steps has been re-implemented according to the peculiar-ities of the text annotation task. Thus, the Cerno architecture includes three new blocks: Parse , Markup and Mapping that semantically correspond to steps (1) X (2), (3) and (4) of the design recovery process, respectively. 3.2.1. Parse
To begin, the system breaks down raw input text into its constituents, by producing a parse tree. In contrast to code de-sign recovery techniques, the parse tree produced by Cerno is composed of natural language document fragments such as document, paragraph, phrase, word, rather than program, function, expression, etc. Any of these fragments may be chosen by the user as an annotation unit, depending on the purpose of annotation. In order to properly recognize sentence bound-aries, this step also identifies basic word-equivalent objects such as e-mail and web addresses, phone numbers, enumeration indices, and so on. All input structures are described in an ambiguous context-free TXL grammar using a BNF-like notation (see Fig. 1 ). Note that the standard non-terminal  X  X  X rogram X  stands for the root element of any input to the TXL engine. The
TXL engine automatically tokenizes and parses input according to the specified grammar, resulting in a parse tree repre-sented internally, as in the example shown in Fig. 2 . If necessary, the recognition of document structure may also involve a different visualization of document elements, for instance, indentation of paragraphs, normalization of monetary amounts, and other similar operations. TXLs parsing is essentially different from the parsing that NLP tools normally do, because TXL recognizes structures based on identifiers, numbers, and symbols. Linguistic parsing instead recognizes linguistic constructs such as nouns, verbs, adjectives, etc. and therefore needs a dictionary of valid word forms and a disambiguation step. An example of the output obtained using the document grammar recognition of the first stage is shown in Fig. 3 .
In the example, the input is parsed into a sequence of ads, whereby each  X  X  X d X  consists of sentences, each  X  X  X entence X  is a sequence of words. A  X  X  X ord X  can be an identifier (non-terminal  X  X  X d X ), number or object (non-terminal  X  X  X hone X ), a shortform ( X  X  X .g. X ,  X  X  X el. X ,  X  X  X .m. X ) or any other input symbol.
 3.2.2. Markup
This stage recognizes instances of concepts, i.e., annotates text units that contain relevant information according to an annotation schema . This schema is a structure that includes a list of concept names and the domain-dependent vocabulary, that is, syntactic indicators related to each concept. Cerno assumes that the annotation schema is constructed beforehand, either automatically using learning methods or manually in collaboration with domain experts. Indicators can be single lit-concept. The systematic process for annotation schema construction is discussed in Section 3.3. If one of the indicators from the list is present in a text fragment, the fragment is marked up with a corresponding tag name. If necessary, the decision-making criteria can be refined to specify more complex criteria. For instance, at least N words, or P percent of the wordlist must appear in a single text unit in order to consider it pertinent to a given concept, or other triggering criteria. The text processing in this stage exploits the structural pattern matching and source transformation capabilities of the TXL engine similarly to the way it is used for software markup to yield an annotated text in XML form. 3.2.3. Mapping
In the last stage, which is optional, annotated text units selected from all annotations according to a predefined database schema template are extracted to be stored in an external database. The schema template represents a target structure to accommodate automatic annotations. An example of a DTD for database schema templates is shown in Fig. 4 . The template is manually derived from the domain-dependent semantic model, as discussed later for case studies in Section 5, and rep-field. In this way we do not prejudice one interpretation as being preferred. To recognize and copy the annotated fragments according to the given template, this step uses a schema grammar, which is domain-independent. The final outputs are both the XML marked-up text ( Fig. 5 ) and the populated relational database (a fragment of a filled XML-document corresponding to the above database schema template is shown in Fig. 6 ).

Low-level objects such as email and phone numbers , while recognized and marked-up internally, are intentionally not part of the result since they are not in the target schema (see Fig. 4 ). Phrases that contain information related to more than one concept are marked-up once for each concept; notice for example type and location tags in the first sentence.
Fig. 7 illustrates the annotation process (along the center axis) specifying domain-independent and domain-dependent components of the Cerno architecture (on the left and right hand sides, respectively).

We have designed the method to be general by making the core components of Cerno applicable to different applications and semantic domains. The architecture explicitly factors out reusable domain-independent aspects such as the structure of basic word-equivalent objects, i.e., e-mail and web addresses, monetary formats, date and time formats, etc., and language structures, i.e., document, paragraph, sentence and phrase structure, shown on the left hand side. Components which vary for different domains are represented on the right hand side. They comprise the annotation schema and the corresponding vocabulary containing syntactic indicators. 3.3. Systematic process for annotation schema construction
Applying Cerno to a specific annotation task requires construction of an annotation schema which effectively represents the requirements of an annotation task. Therefore, to develop such schemas for our case studies, we have been using the systematic process similar to the common requirements engineering approaches:  X  Identification of the most relevant concepts by means of discussions with the end users or domain experts.  X  Enhancement of the obtained list of main concepts with background knowledge, related concepts, and synonyms using  X  Structuring the collected information in a semantic model.  X  Selection of the first-class concepts that must be identified and related keyword-based annotation rules from the enti-
We also envisage a refinement step in which the expert provides feedback on the quality of the obtained annotation sche-ma to improve its quality. The reason is that the use of complementary knowledge sources may lead to enhancing the sche-ma with too much information and eventual reduction of the annotation accuracy.

Ultimately, the improvement of the annotation schema is realized by tuning the result quality on a small training set of text. Fig. 8 shows a fragment of a Cerno annotation schema for term concept at two different time points of the refinement phase.

Here  X  X  X ate X  is a word-equivalent object whose structure is manually specified using a TXL grammar pattern that recog-nizes both precise dates, e.g.,  X  X 10.12.2001 X , as well as more general date expressions like  X  X  X ovember 1999 X . 3.4. Implementation
The Cerno framework is designed as a multi-stage pipeline process that can be executed from the command line given that the TXL engine is installed on a PC. For instance, the following command executes the markup phrase of Cerno and the mapping phase right after completing the run of markup. The second phase reads the output of the previous phase from txl Input n example.txt markup.txl
Alternatively, the multi-stage processing can be performed in the standard way by consecutive execution of single commands: txl  X  X  Input n example.txt  X  markup.txl &gt; X  X  Temp n example.tmp txl  X  X  Temp n example.tmp  X  mapping.txl &gt; X  Output n example.out
Here intermediate output files are stored in a temporal folder Temp . The second phrase reads its input files from Temp folder. In this way, the process of TXL transformations can be automated via shell scripts. Each processing module accepts as input a file and produces an output file. Thus, the entire process can be monitored and corrected if necessary by control-ling intermediate output files and changing processing settings. This approach can be applied during the tuning phase, i.e., when adapting Cerno to a specific type of documents and a semantic model. By looking at intermediate outputs, the user may get some hints on how to improve the grammar and obtain better results.

Domain-independent grammars used at different phases are called from TXL files using a special directive include and employed at the compile time, for example: include  X  X  Grammars/category.grm  X  include  X  X  Grammars/schema.grm  X 
Instead, domain-dependent components are read from files on-the-fly. The following line of TXL code creates a new list of categories by reading them from ads.cat file: construct MyCategories [repeat category] _ [read  X  X  Categories/ads.cat  X  ]
In order to recognize word-equivalent objects during the Parse phase, we exploit the TXL parse tree of input and infer object annotation based on the type of an object in the tree. This grammatical recognition of some objects of interest is very rapid in TXL. The two rules shown in Fig. 9 embody the first processing step.

Similar rules are used during the Markup phase for annotation of larger text fragments. Recognition of relevant concepts is realized by combining keyword search with grammatical patterns. For example, the annotation schema may incorporate complex patterns, such as: call (object _ phone j object _ person j us ) where  X  X  X bject_phone X  and  X  X  X bject_person X  are word-equivalent objects provided by the parse phase, i.e., any phone number matching phrases like  X  X  X all Alice X ,  X  X  X all mr. Johnson X ,  X  X  X all 000-1111111 X ,  X  X  X all us X . Such combined patterns make Cerno X  X  semantic annotation very powerful. 3.5. Discussion
The proposed process has a number of advantages. Generally speaking, it is domain-independent and does not rely on document structure. For instance, in contrast to wrapper induction approaches, our framework uses context-independent parsing and does not require any strict input format.

Compared to machine learning systems, the cost of human attention required is lower. The main reason for this is that tuned to a particular domain by providing the system some hints, i.e., semantic clues, about concepts. The semantic process-ing itself is light-weight: wordlist-and pattern-based. Another important benefit of Cerno is that the human engaged to modify or replace domain dependent knowledge, need not be an expert in the TXL language or possess specific programming skills. He or she can quickly realize the necessary modifications and test Cerno to obtain new results.

Given that Cerno does not employ full linguistic parsing, this approach renders Cerno tolerant to ungrammatical, errone-ous input. At the same time, the shallow processing involved in each step of the process enhances scalability, an issue of particular concern in the Semantic Web.

In addition, Cerno supports the reuse of domain-independent components, factored out by the Cerno architecture, as with some other annotation tools, such as GATE. This feature means that domain-independent knowledge and the core of Cerno remains unchanged when applying the tool to a new domain. 4. Case studies
Our empirical studies include two experiments: (1) proof-of-concept experiment : validation of the feasibility of the new method, performed on the relatively restricted domain of on-line accommodation advertisements; (2) test-of-generality experiment: verification of the scalability of our method to larger documents and more complex semantic models; for this experiment we analyzed the contents of tourist board web sites.

Both case studies belong to the tourism sector, a sector that is a broad in the concepts it covers and rich in the data that can be found on the web [21]. The complexity of the tourism sector is reflected in the concept of tourism destination [12] that has to be described as a composition of services belonging to different domains. Besides travel operators, hotels and restaurants, contributing services comprise: sports (covering activities, competitions, courses, facilities), transportation (destinations, transportation means, timetables, terminals), culture and history (history, cultural heritage, places to visit, cultural events, local traditions, holidays, customs), and medicine (medical services and treatments of the resort). Being such a broad sector both in terms of content and challenges, tourism constitutes a rich evaluation testbed for semantic annotation tools. 4.1. Evaluation challenges
Assessment of output quality poses a great challenge for semantic annotation tools. The output of these tools is often as-calculating standard quality metrics such as recall and precision. Thus, evaluation benchmarks for some of the information extraction tasks were created in the framework of the Message Understanding Conferences (MUC) [25]. To our knowledge, such standard benchmark evaluation tasks and competitions for semantic annotation systems have yet to be established. annotation. While for more complicated annotation schemas, we cannot rely on this assumption, because human opinions on the correct markup vary widely. Ideally, we should compare the automated results against a range of high quality human opin-ions. However, in practice the cost of the human work involved is prohibitive for all but the largest companies and projects.
Manual construction of the reference annotation is complicated because of many issues: (i) annotators must be familiar with the domain of the documents of interest, the language the documents are written in and the annotation model; (ii) there is no guarantee that they are consistent throughout the entire corpus, because different parts can be processed by dif-ferent people or because an annotator has changed her opinion about some concepts during the process; (iii) the process is costly, time-consuming, and error-prone due to human limitations (tiredness, lack of attention, incapacity to memorize a large number of concepts, etc.). Nevertheless, even annotations accurately crafted by experts in a given domain, native-speakers, well acquainted with the annotation process, may differ because the annotation process includes a healthy ele-ment of human discretion.

Another problem is choosing the right evaluation criteria given that an annotation tool can be evaluated in terms of qual-ity of the output, processing speed, and other performance aspects. From an engineering perspective, the most important criteria are the effectiveness of the tool in terms of quality of results and the productivity of human annotators who use the too. The motivation underlying this assumption is that results of poor quality would require a human expert to manually revise the annotated documents and potentially repeat the entire annotation process. As for productivity, this aspect is used to evaluate the tool in terms of the effort saved by using the tool.

The quality of results provided by a semantic annotation system can be measured by several metrics adopted from Infor-mation Retrieval [43]. In most cases, and in the present work, it is assumed that any given text fragment in a collection is either pertinent or non-pertinent to a particular concept or topic. If the annotated fragment differs from the correct answer in any way, its selection is counted as an error. Thus, there is no scale of relative relevance that is considered by some ap-trieve a relevant fragment is considered as a miss. Finally, returning an irrelevant fragment constitutes a false hit.
Accordingly, in our case studies we were interested in assessing the quality of answers returned by the system using the following six metrics. Recall is a measure of how well the tool performs in finding relevant information; Precision measures how well the tool performs in not returning irrelevant information; Fallout is a measure of how quickly precision drops as recall is increased, it characterizes the degree to which a system X  X  performance is affected by the availability of a large measure is an aggregate characteristic of performance, expressed in the weighted harmonic mean of precision and recall. In our evaluation, we did not assume priority of recall over precision or vice versa and used the traditional balanced F-measure with equal weights for recall and precision. 4.2. Evaluation framework
To assess the results of both experimental studies, we developed a new evaluation framework. In each step of the eval-uation process, we were concerned with measuring the quantitative performance measures outlined above for the tool X  X  automated markup compared to manually-generated annotations.

Our evaluation framework consists of three main steps: (1) The first step compares system output directly with manual annotations. We assume that quality of manual annota-(2) The second step verifies if the use of an automatic tool increases the productivity of human annotators. We measure (3) Finally, the third step compares system results against the final human markup made by correcting automatically gen-
The motivation for our calibration technique is that the performance of any semantic annotation tool cannot be consid-ered in isolation from corresponding human performance. In fact, a study conducted as part of MUC-5 in a domain involving technical microelectronic documents has estimated that human markers demonstrate only about 82% precision and 79% re-call, while the best system achieved about 57% precision and 53% recall on the same information extraction task [41]. There-fore, in order to adequately evaluate the performance of any tool, we propose to calibrate the tool X  X  performance against human performance. In this case, as a reference annotation for the automatic results we used the annotation provided by multiple human markers.
 The proposed evaluation schema allows for comprehensive performance assessment of any semantic annotation tool.
Each of the proposed stages is important because we are interested to fully assess the capabilities of the semantic annotation tool, and in particular, to what extent the tool can match human performance. 5. Experiments 5.1. Accommodation ads
The first experiment was aimed at validating feasibility of the new method for natural language documents in a limited semantic domain [19]. For this purpose, we worked with accommodation advertisements for tourist cities drawn from online newspapers (see for example Fig. 10 ). These advertisements offer accommodation-related information provided by individ-ual users in a free, unstructured form.

From a linguistic viewpoint, this application poses a number of problems beyond those normally present in typical nat-ural language documents, for instance:  X  partial and malformed sentences, such as  X  X 30 square meter studio apt. in Rome center near FAO. X ;  X  abbreviations and shortforms ( X  X  X urn./equip. X );  X  location-dependent vocabulary: names of geographical objects, both proper nouns ( X  X  X olosseum X ) and common nouns  X  presence of mixed foreign language terms ( X  X  X ia X ,  X  X  X trasse X ,  X  X  X oliclinico X );  X  monetary units ( X  X  X uro 73.00/83.00/93.00 p.n. X ,  X  X   X  2000 X );  X  varied date and time conventions ( X  X  X rom the 15/20th of July 2006 X ,  X  X  X rom next March X ).
 From a functional viewpoint, such advertisements are present in various kinds of web sites publishing classified ads.
Given that these kinds of advertisements are unstructured, searching them is very often a real test of patience for the user who has to look for useful information in long lists of ads. In order to make a realistic test of generality of the method, we restricted our problem. In particular, we avoided using any proper names and location-dependent words; we did not pre-process the text of accommodation descriptions by normalizing their format or correcting errors, and we did not use any formatting or structural clues to detect semantic categories.

To annotate these ads, we designed a conceptual model that represents the information needs of a tourist looking for accommodation, as shown in Fig. 11 .

The annotation schema derived from the semantic model consisted of the concepts Type , Contact , Facility , Term , Location , and Price . This schema contains information typical of the tourism sector in general and is rather standard, given the nature of the documents.

The annotation schema was manually translated into an XML database schema as input for Cerno X  X  Mapping stage (see the template demonstrated in Fig. 4 ). The desired result was a database with one instance of the schema for each advertise-ment in the input, and the marked-up original advertisements.

To adapt the semantic annotation method to this experiment, domain-related wordlists were constructed by hand on the basis of the semantic model using names of sub-classes of the relevant concepts and from a set of examples. As we described was allowed to be tuned to do well on this first set by hand. The total number of annotation patterns in the vocabulary was 152.
 5.2. Tourist board web sites
The second case study pursued two main goals: to demonstrate the generality of the method over different domains, and to verify its scalability on a richer semantic model and larger documents.
 For this purpose, we considered a sub-set of the tourist destination web sites of local Tourist Boards in the province of
Trentino, Italy, that is the 13 out of 15 web sites of the Aziende di Promozione Turistica di ambito [38] that have an English version (web sites version considered is of 2007). This application presents a number of problematic issues for semantic annotation:
Under these conditions, we decided that HTML structure was more hindrance rather than help in inferring semantic annotations. Therefore, we extracted plain text from the web sites and conducted our experiments on this text. This experiment was run in collaboration with the marketing experts of the eTourism group of University of Trento [9].
The high-level goal of the study was to assess the communicative efficacy of the web sites. In the area of marketing, the com-particular, to the degree to which the web site covers relevant information according to the strategic goals of the Tourist Board. A full description of the assessment of the communicative efficacy is beyond the scope of this paper.
Semantic annotation of the web pages is a necessary step of the project to gather input data for evaluating communicative end, we asked the experts of the eTourism group to provide a list of semantic categories and their descriptions. Then we identified concepts related to these categories and pruned them according to the domain knowledge related to the local tour-ism strategies, as shown in Fig. 12 . The final list of semantic categories is shown in Fig. 13 .
 To adapt the annotation framework to this specific task, we needed replace the domain-dependent components of Cerno.
For this purpose, the initial domain-specific knowledge provided by the tourism experts was transformed into a rich seman-tic model. In this process we took advantage of existing two knowledge bases  X  WordNet [42] and an on-line Thesaurus [37]  X  to expand relevant domain knowledge. These resources also helped us to derive additional linguistic indicators for Cerno domain modules. The semantic model was constructed using the Prot X g X  3.0 ontology editor [30] and stored in RDF. The model consisted of about 130 concepts connected by different semantic relationships. A slice of the tourist destination semantic model, visualized using the RDFGravity tool [32], is depicted in Fig. 14 . The figure shows semantic information, the annotation process, but ultimately stored along with the model.
 The final annotation schema was essentially a set of concepts at the general level provided by the experts: Geography, plate for mapping system annotations into an external database, shown in Fig. 15 , was derived straightforwardly from the annotation schema.
 The domain dependent vocabulary for the annotation schema was generated both semi-automatically and manually.
Parts of it were mined from a set of sample documents and using sub-terms in the hierarchy of the domain conceptual mod-el, as it was done in the first case study. Additional relevant synonyms were obtained from the definitions provided by Word-
Net [42] and the on-line Thesaurus [37]. The total number of keywords collected was more than 500. Moreover, four object patterns were reused from the previous application to grammatically recognize monetary amounts, phone numbers, e-mails and web addresses, as these structures frequently occur in the content of web sites.

In this experiment, we downloaded the 13 local Tourist Board web sites which had English versions from a total set of 15 web sites. A set of 11,742 text fragments extracted from these web sites was given to each of the two human annotators and to the tool for annotation. The required result was a database with one instance of the schema for each Tourist Board web site in the input, and the marked-up original text, see Fig. 16 . Once the automatic annotation was completed, we proceeded to the evaluation stage. The next section analyzes Cerno X  X  performance for different semantic categories. 6. Results 6.1. Experiment 1: Accommodation ads
According to the evaluation framework described in Section 4.3, in the first stage the tool and each of two human anno-tators marked up a sample set of ten advertisements different from the training set used to tune the tool for the domain. The tool was then compared against each of the human markers for this set separately  X  see Table 1  X  and then calibrated assum-ing each of the two human annotations as definitive, see Tables 2 and 3. By comparison with these two human annotators, the system exhibited a high level of recall (about 92% compared to either human, higher than either human compared to the other), but a lower level of precision (about 75% compared to either human, whereas they each exhibit about 89% compared to the other). However, the system was able to show a 92% accuracy rating compared to either human, extremely high for such a light-weight system.

If we compare the performance of the tool and one of the annotators, considering as the  X  X  X round truth X  the markup of the other annotator, we see that the tool retrieves more information than the humans but with lower precision. Indeed, the tool demonstrated a recall of about 92% compared to 91% and 88% for the two humans, while precision was 74 X 76% compared to 88% and 91% for the humans.

Considering the quality rates for different concepts, we can see that most accurate results were demonstrated for Contact address, and phone number in Contact and monetary amounts in Price . Such constructs easily allow identifying related phrases. Moreover, meaning of this particular concept is unambiguous for human annotators, this way reducing the proba-bility of disagreement to a minimum. This type of construct is now reliably recognized by many information extraction tools, as for instance GATE and UIMA.

The fragments related to Facility concept were identified with a 94% average recall, which is quite good for a rather com-(2) human markers tended to miss information related to this concept, consequently, raising the number of false positive replies for automatic annotation. Taking into account evaluation of system performance against assisted human opinions tations the tool showed good performance for the Facility concept. Therefore, the second explanation appears more likely in this case. The problem with this concept is that even for humans it is difficult to choose which amenities listed in the ads were in fact accommodation facilities. For example, this problem arises for such phrases as:  X  X  X loors, high ceilings, sleeping area on upper level, tasteful decor and fittings. X ,  X  X  X levator. X ,  X  X  X ecure and quiet building with doorman. X 
Identification of Location information shows high precision, but lower recall for both reference annotations, i.e., annota-tors 1 as well as 2. This can be explained by the fact that the tool did not use any location-dependent vocabulary, containing proper names related to Rome. In short and concise text, as with an accommodation ad, it is often difficult to infer the pres-ence of Location information from context. Consider, for example,  X  X  X revi Fountain, charming miniapartm. X  Therefore, for eventual commercial application, a location-dependent vocabulary can be employed in the tool to achieve a higher recall.
Identification of Term -related information turned out to be the most difficult task. The problem is that availability infor-mation is not always expressed by exact date, but intended implicitly, therefore is more difficult to predict. For example, sible period for rent, but on the other hand may be ignored as not being specific enough. Recognition of temporal information in general is one of the most complex problems in NLP, especially so in the area of question answering [31].
The Type concept was identified with high recall and lower precision. The reason for the low precision rate is due to incon-sistent assumptions underlying the automated and manual annotations. Human markers actually skipped many items re-lated to this concept. For instance, the word  X  X  X partment X  related to the type of accommodation often appears in the text Instead, the tool correctly annotated each Type instance across the entire text.

In the second stage evaluation, we were interested in measuring the effect of the initial automated annotation of the tool on human productivity. The time taken by an unassisted human marker to semantically annotate a new sample of 100 adver-tisements was measured, and compared to the time taken by the same human marker when asked to correct the automated markup created by the tool. In this first evaluation the human annotator was able to annotate 4.5 times faster with the aid of the tool (i.e., used 78% less time to mark up text with assistance than without), a significant saving. Because the system was shown in the first evaluation to be more aggressive than humans in markup, the majority of the correction work was remov-ing markup inserted by the tool. With an appropriate interface for doing this easily, the time savings could have been even greater. Ideally, one should also add to the productivity estimation the time for programming and training Cerno. However, this rate cannot be accurately approximated from these two first case studies, in the context of which Cerno was actually designed.

In the third stage, we gave the human annotators the advantage of correcting automatically marked up text from the tool to create their markups, and compared the final human markup to the original output of the tool. For this experiment, three sets of documents were used in addition to the original training set, one new set of 10 advertisements from the same online newspaper, another set of 100 from Rome (the same city as the original set), and a new set of 10 from Venice. The summary of results is shown in Table 4 . Accuracy for all of the Roman sets is about 98%, and in the new set from Venice, a completely different location, the accuracy was measured as 96% with similar precision. A drop in recall to 86% is indicative of locality effects from the original training set.

This experiment is limited because of the small semantic model. However, it is important to note that with limited do-main knowledge and a very small vocabulary, we were able to demonstrate accuracy comparable to the best methods in the literature. Computational performance of our  X  as yet untuned  X  experimental tool is also already excellent, handling for example 100 advertisements in about 1 s on a 1 GHz PC. The tool has been evaluated on sets ranging from 38 to 7600 adver-tisements (about 2500 to 500,000 words), and found to process text at a rate of about 53 kb/s on a 1 GHz PC. Thus, the tool scales well to large document datasets. 6.2. Experiment 2: tourist board web pages
In the second study, for all the categories in the annotation schema we performed a simple metrics-based evaluation, shown in Tables 5 and 6 over the entire set of 11,742 paragraphs. These tables show estimated rates for each of the concepts defined in the semantic model. Table 7 summarizes the two tables by means of the average rate on all concepts and the total quality rate calculated for all the annotations independently from their semantic category.

As we can observe from these results, for the given annotation schema the task turned out to be difficult both for the sys-tem and for the humans. One reason for this is the absence of structural patterns for all of the semantic categories. Another reason is that ambiguities occur frequently in such tourism documents, as they contain arbitrary information from a broad spectrum of topics, which are not always independent. For example, text about local food may be associated with either or both of Local Products category and Food and Refreshment category, depending on the context. Unfortunately, such overlaps in the semantic model cannot or even should not be resolved due to the nature of ontological modeling. Consequently, a text fragment may relate to more than one entity in the semantic model. A human marker, however, tends to pick only one, most relevant in his or her opinion, annotation tag for such multidimensional instances, whereas the tool will normally choose both.
 Table 8 summarizes the comparison of our tool against each of the two human annotators as definitive.

By comparison with these two human annotators, the system exhibits a 65 X 77% level of recall, whereas the human coun-terparts each exhibit a recall of about 55 X 76% compared to each other. These rates indicate a high degree of disagreement between the annotators, for instance, a higher recall of annotator 1 compared to annotator 2 than the inverse ratio shows that annotator 1 provided more annotations than 2. This result can be explained by (a) difference in opinions about the con-cepts; (b) human factors, such as tiredness when annotating large documents. The first problem relates to the evaluation difficulties in general and was already discussed in Section 4.2, whereas the second reason underlines the need for auto-mated support of the semantic annotation task. The system showed a 55 X 90% level of precision compared to the humans, as good as or better than either human compared to the other.

In the second stage of evaluation, the human annotators were observed to use 75% less time to correct automatically annotated text than they spent on their original unassisted annotations.

In the third stage, where the human annotators corrected automatically marked up documents, the results of comparison to the final human markup are given in Tables 9 and 10 , while Table 11 summarizes both tables by estimating the average metrics on all concepts and the overall quality metrics calculated for all the retrieved annotations. Calibration to human as-sisted performance is evaluated in Table 12 .

Observing the calculations vs. human assisted opinions, we note a high performance of the tool with respect to both pre-cision and recall. Fallout is very low for all items, thus demonstrating that the tool was not misled by the large quantity of irrelevant information; the error rate is also fairly low; and accuracy reaches almost 100% rate.

For both human annotators, comparison of the tool X  X  and the human X  X  results shows that the tool outperforms humans for all quality metrics. We conclude that such large-scale applications as Tourist Board web sites can greatly benefit from employing Cerno for semantic annotation of documents. Moreover, even human annotators are not able to accurately per-the annotator works with the output provided by Cerno, rather than conducting the annotation task manually from scratch. The time required to handle the documents containing from 6143 to 24,810 words ranged from 1.19 to 5.14 s on a 1 GHz
PC with 512 Mb of memory running Windows XP. Thus, the tool has demonstrated scalability to the large document sizes and given the bigger semantic model of the domain.

As a result of this experiment, we can say that the semantic annotation framework can demonstrate reasonable results on more general documents and richer domain while maintaining high performance. Taking into account that the experiment involved analysis of large textual documents where human expertise is particularly expensive and difficult to obtain, we can say that Cerno had also allowed to minimize the costs of the assessment of the communicative efficacy of the web sites. The tool therefore can be especially useful in such applications where input needs to be analyzed quickly, but not necessarily very accurately. 6.3. Comparative assessment of the results
Although it is not really feasible to directly compare the quality of different tools because of unavailability of implemen-tations, semantic models and, most importantly, comparable data sets, a possible solution is to approximate this comparison.
For this purpose we use the author-reported performance previously summarized in [33] (Table 13 ). We also provide the apart from the two case studies presented in this paper, take into consideration performance reported for other experiments where Cerno has been applied [47,18] . In general, Cerno was able to provide essential aid in generating semantic annotations that were in some cases equal to perfect recall and precision rates.

From our experiences in applying Cerno to different domains, we can say that the effort of adapting the tool to a new task is relatively small and does not require any specific linguistic or programming expertise apart from general computer skills.
In particular, we found that each new application required human effort ranging from one person-day to a couple of weeks to tune Cerno.

In terms of factors that influence Cerno X  X  performance, our experiences suggest that domain homogeneity and document structure are most important. The first means that the results are better for the annotation schema that contains a limited and unambiguous set of concepts, as in the case of accommodation ads. The broader the domain is, the more difficult it be-comes to draw a discriminative set of annotation rules for each concept. We also observed that the quality of the results do not improve with a richer semantic model. For many applications, ER-models are sufficient for representing the semantic knowledge that will then be used by the annotation schema. Instead, domain expertise is very crucial for building a good quality annotation schema.

The presence and regularity of structure in input documents may facilitate the annotation process by providing some for-mat-based annotation hints. For instance, in [47] we used information about the document structure to recognize title and abstract in academic papers. Alternatively, one can consider giving different weights to indicators identified in different structural elements. For instance, in HTML web pages annotations found in title and heading tags can be assigned higher ranks compared to those found in paragraphs or lists. 7. Conclusions and lessons learned This paper addresses the problem of semantic annotation of textual documents using light-weight analysis techniques. We emphasized the need for robust and scalable tools to help automate this process in the context of web data processing. To address this need, we presented and evaluated the Cerno framework for semantic annotation of web documents.
The results of two experimental studies of the use of Cerno in two different semantic domains lead us to the following observations:  X  As concerns the impact of the tool on human productivity, observing the time gains obtained in both experiments, we  X  In terms of required resources, Cerno does not necessarily need gazetteers, linguistic knowledge bases, proper name  X  Cerno has demonstrated high processing speed and scalability to large documents.  X  Because Cerno requires only limited computational resources, it can be easily adapted to light-weight interfaces to  X  Another important feature of the tool is that it is not limited to a certain type of entities to be annotated. Two appli- X  Based on our experience in adapting Cerno to several different semantic domains, we can claim that the required effort  X  In addition, the results of the experiments represent useful data for the designer of a new semantic annotation appli-
In summary, this research provides concrete evidence that a light-weight semantic annotation approach derived from software code analysis methods can be effective in semantic text document annotation. Our experiments with Cerno suggest that such an approach can provide acceptable performance and scalability, while yielding good quality results.
Apart from the experiments presented in this work, the feasibility of Cerno has been demonstrated in several additional studies. One of these is Biblio, an application developed for information mining from large collections of published research papers [47].In [18], Cerno was used to support requirements extraction from system descriptions in natural language. The
Gaius T. tool  X  based on Cerno  X  identifies legal requirements in legislative documents [22]. In that application, apart from the annotation of legal concept instances such as rights and obligations, we also identified relationships between concept instances and associated constraints. Cerno is also currently being evaluated in the context of the European Project Papyrus [29], where the proposed semantic annotation process is employed for the annotation of textual news content. Acknowledgements
The authors are grateful to the anonymous reviewers whose comments led to many improvements to the results and the presentation of this work.

This research was conducted within the context of the STAMPS project, funded by the Province of Trento (Italy). It was also partly funded by the European project Papyrus, and the Natural Sciences and Engineering Research Council of Canada (NSERC).
References
