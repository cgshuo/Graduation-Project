 We present a new approach to large-scale graph mining based on so-called backbone refinement classes. The method efficiently mines tree-shaped subgraph descriptors under mi n-imum frequency and significance constraints, using classes of fragments to reduce feature set size and running times. The classes are defined in terms of fragments sharing a com-mon backbone. The method is able to optimize structural inter-feature entropy as opposed to occurrences, which is characteristic for open or closed fragment mining. In the experiments, the proposed method reduces feature set sizes by &gt; 90 % and &gt; 30 % compared to complete tree mining and open tree mining, respectively. Evaluation using crossval i-dation runs shows that their classification accuracy is simi -lar to the complete set of trees but significantly better than that of open trees. Compared to open or closed fragment mining, a large part of the search space can be pruned due to an improved statistical constraint (dynamic upper bound adjustment), which is also confirmed in the experiments in lower running times compared to ordinary (static) upper bound pruning. Further analysis using large-scale dataset s yields insight into important properties of the proposed de -scriptors, such as the dataset coverage and the class size represented by each descriptor. A final cross-validation ru n confirms that the novel descriptors render large training se ts feasible which previously might have been intractable. A C++ implementation is available at http://www.maunz. de/libfminer-doc/ .
 G.2.2 [ Graph Theory ]: Graph Algorithms, Trees; H.2.8 [ Database Applications ]: Data Mining, Statistical Databases Theory, Experimentation, Performance
Current methods for subgraph mining still suffer from scalability problems and, quite related, problems with ex-cessively large solution sets. Most of the predominant ap-proaches employ minimum frequency and possibly statistica l correlation criteria such as  X  2 values [6, 8, 13, 2, 5]. An in-creasing minimum frequency tends to favor a higher entropy of a pattern occurrence. Statistical constraints retrieve sub-graphs that are correlated with the target classes. However , the thresholds used usually lead to an explosion in the num-ber of frequent patterns, which is a significant drawback of these approaches. As a result, the size of the resulting pattern set is often multiple times the size of the original database, which makes it unusable for subgraph-based clas-sification models, at least for very large datasets.
In order to build a more sparse representation with a significantly reduced number of solution patterns, the pat-tern set is often constrained to open 1 or closed features. This might however prune important structural information , since only the support is considered. In this study, we pro-pose a method to increase inter-pattern entropy and reduce pattern set size by increasing structural dissimilarity. I n order to reduce the space of frequent and significant pat-terns, we use a natural property of tree-shaped subgraphs (the backbone) to represent classes, which renders the set u s-able for computational models even for large scale datasets . We employ the chemical domain as our application area, i.e., we seek fragments of chemical compounds that are associ-ated with a classification endpoint, e.g., carcinogenicity or genotoxicity.

Various types of subgraph patterns, such as paths, trees, and general subgraphs, have been investigated for database s of molecular graphs so far [2]. For the databases used in this study, however, pattern sets contain about 5-10% paths, 80-85% real trees and 10% real cycle-closing graphs, which may be quite representative for many applications. Although there exists an efficient method for mining subgraphs shaped as outerplanar graphs (a strict generalization of trees whi ch
We decided to use the term  X  X pen X  instead of  X  X ree X  (which is more common in the data mining literature), because  X  X ree tree X  is used as a synonym for  X  X nrooted tree X  in graph theory . may contain cycles [4]), we restrict ourselves to the larges t fragment class, because there is some evidence that cycle-closing graphs may not generally improve performance in SAR models [2].
We assume a graph database R = ( r ,  X  , a ), where r is a set of graphs,  X  6 =  X  is a totally ordered set of labels and a : r  X  X  0 , 1 } is a function that assigns a truth value to every graph in the database (binary classification). Graphs with the same classification are collectively referred to as target classes . Every graph r  X  r is a labelled, undirected graph , i.e. a tuple r = ( V, E,  X  , l ), where V 6 =  X  is a finite set of nodes and E  X  V = {{ v 1 , v 2 } X  X  V  X  V } , v 1 6 = v is a set of edges and l : V  X  E  X   X  is a label function. The set of all labelled, undirected graphs is referred to as G . A set of nodes { v 1 , . . . , v m } is a path between v v , if { v i , v i +1 } X  E, i  X  X  1 , . . . , m  X  1 } . We only consider connected graphs here, i.e., there is a path between each two nodes in the graph. The graph r = ( V, E,  X  , l ) is double connected , if there exist two distinct paths between a pair of nodes { v i , v j } X  E . A node v is adjacent to an edge e = { e 1 , e 2 } , if v = e 1 or v = e 2 . The number of distinct edges that a node is adjacent to is called its degree . A subgraph r of r is a subset of vertices of r and some edges of r with both endpoints in r  X  , s.t. r  X  is connected. We denote the subgraph relation by  X  . If r  X   X  r , then r  X  is said to cover r . This induces a partial order on graphs, the more-general-than relation : for any graphs r, r  X  , s , The subset of r that a graph r covers is referred to as the occurrences of r , its size as support of r in r , denoted by supp ( r, r ). It is called open ( closed ), if for all s with supp ( r, r ) = supp ( s, r ) it holds that r  X  s ( r  X  s ). Undirected, labelled graphs are partially ordered. Let P X  G be the set of not double connected graphs with degree at most 2 (paths) and let T X  X  be the set of not double connected graphs (trees). It holds that
Let p = { v 1 , . . . , v m } X  X  be a path, then its sequence is defined as the string l ( v 1 ) l (( v 1 , v 2)) . . . l (( v obtained by concatenating node and edge labels along the path. Every tree t  X  X  has a backbone b ( t ), which is defined as the longest path p  X  t with the lowest sequence according to a lexicographic ordering (described by Nijssen and Kok [8]). A (immediate) tree refinement of t  X  X  is an addition of an edge and a node to t s.t. the result t  X  is still not double connected, i.e. t  X   X  X  . A backbone refinement is a tree refinement that is backbone preserving, i.e. b ( t  X 
We are considering the Backbone Refinement Classes of b  X  X  , denoted by BBRC b = {BBRC b 1 , . . . , BBRC b n } , where each BBRC b i is the set of trees that are backbone refine-ments of each other with respect to b , i.e. for all r, r BBRC b 1. b ( r ) = b ( r  X  ) and 2. r r  X  or r  X  r . Figure 1: Three example trees with the same back-bone  X  C1C2C1C2C1O1C  X  (edges are labelled by bond or-der) in bold. It holds that q 1 b q 2 and q 3 b q 2 , but neither q 1 b q 3 nor q 3 b q 1 . Therefore, q 1 and q 3 are not in the same Backbone Refinement Class.
 We denote the backbone refinement class relation by b . Note that the classes are not disjoint for the same backbone (but they are across different ones). For example, in Figure 1, q 1 and q 3 are in different classes, but q 2 is in the respective classes of both q 1 and q 3 . The set of all backbone refinement classes for a graph database R is called BBRC R . The objec-tive of Backbone Refinement Class Representative Mining is to find the most significant representative for each backbone refinement class:
Given a graph database R = ( r ,  X  , a ), a user-defined min-imum support m and user-defined minimum  X  2 value p , for all B  X  X BRC R , find the most significant t  X  B that is sig-
The complexity of BBRC mining is upper-bounded by the complexity of regular tree mining [8]. We will however show that our approach decreases running times significantly for practical applications.
In the following, we review the literature for reducing so-lution sets in graph mining. For the unsupervised setting, i.e., when no target class is available, methods can only tak e into account the support of features. For instance, the solu -tion can be restricted to open (closed) subgraphs of various types (see, e.g., the work of Yan and Han [14]). By defini-tion, these techniques represent refinements with identica l occurrences by the most general (the most specific) pattern. They can be easily integrated into graph mining with mini-mum support.

R  X  uckert and Kramer [10] presented a solution based on occurrence lists, which aims for extensionally diverse set s of structural features. Stochastic local search (SLS) is used to optimize the dissimilarity of features, more specifically, to minimize the dot product between occurrence vectors. The main drawback of the method is the excessively long running time of SLS to find small sets of diverse features.
The subset of closed subgraphs for which no frequent su-pergraphs exist is called maximal (also known from version space theory as the positive border ). Hasan et al. [1] em-ploy sparse representations of maximal frequent subgraphs obtained by sampling. The approach aims for structural di-versity of the mined features while enforcing a certain leve l of representativeness at the same time.

The common strategy in most of the approaches is to rep-resent a large part of frequent patterns by representatives that form a summary of their occurrences. However, ignor-ing the wealth of structural information may be a drawback for three main reasons: First, this type of representation i s not directly related to structures but solely to their occur -rence, which might prune important data. Second, it is not necessary to distinguish between different subgraph types (e.g., paths are refined to trees arbitrarily without chang-ing the mining strategy). Third, occurrence summarization methods are forced to mine a representative for any frequent support level, which could impact performance. Hence, the implementation of an explicitly structural pruning criter ion (defined intensionally) should yield a higher compression r a-tio with less information loss and better running time.
To obtain diverse patterns homogeneously distributed in structural space, we combine principles from correlated su b-graph mining [2, 7] with a novel approach to ensure diversity in structure rather than in subgraph occurrences. We com-pare the method to an occurrence summarization approach (in this case to open fragments) in terms of feature count, information load, and representativeness.
Pruning by minimum frequency can be solved with depth first search methods. N.B.: If each subgraph occurred in half of the graphs, a set of k subgraphs could in principle distinguish 2 k bins of graphs. Since a higher global minimum frequency raises frequencies in at least one target class as well, it leverages a subgraph X  X  discriminative potential f or classification tasks.

Subgraphs that are correlated significantly with the target class can be filtered with statistical measures. Due to the absence of both monotonicity and antimonotonicity, signif -icance values cannot be used for antimonotonic (nor mono-tonic) pruning directly, however, the convexity of the  X  2 function allows to derive a related measure for antimono-tonic pruning.

In our work, this is done as initially suggested by Mor-ishita and Sese [7], however, we use the  X  2 distribution test (checking the adherence of a variable to a given distributio n) instead of the independence test (checking the statistical in-dependence of variables). In the following, we briefly outli ne our use of the  X  2 distribution test and its upper bound for pruning.
For a given subgraph q , a 2  X  2 contingency table counts the occurrences, depending on whether the covered com-pounds are active or inactive (see Table 1). More specif-ically, y is defined as |{ r  X  r | covers ( q, r )  X  a ( r ) }| , the count for active compounds containing q , x is defined as |{ r  X  r | covers ( q, r ) }| , the count for all compounds con-taining q . During calculation of  X  2 significance values, a weight x n is assigned to every candidate pattern q , corre-sponding to its support. Subgraphs with low support get a low weight, while subgraphs with high support are unlikely to deviate much from the sample mean. The regions of in-teresting patterns therefore lie  X  X omewhere in the middle X  .
It holds that 0  X  y  X  n and 0  X  x  X  y  X  n . We are now able to check whether the distribution of q differs sig-nificantly from the distribution of all subgraphs. The  X  function for distribution testing is defined as (see the work of Morishita and Sese [7]). Following their notation, we use x ( q ) and y ( q ) to emphasize the dependency of x and y on q . They showed that, for any subgraph q , x ( q ) and y ( q ) allow to calculate an upper bound for the  X  2 of q  X  , for all refinements q  X  with q q  X  :  X  ( x ( q  X  ) , y ( q  X  ))  X  max {  X  2 d ( y ( q ) , y ( q )) ,  X  Therefore, the upper bound measure is antimonotonic and may be used for pruning the search space with fragment re-finement methods. Bringmann et al. [2] demonstrated the application to graph mining while comparing the expressive -ness of different subgraph types (paths, trees, and graphs). In the following, this pruning technique with a static, user -defined upper bound threshold is referred to as static upper bound pruning .
The partial order P X  X  X  X  of graphs allows for par-titioning the subgraph mining process into three different phases or levels according to the type of subgraph mined. The approach proposed here is concerned only with the set T . Thus, no refinement yields a double connected graph and we only consider path and tree refinements.

We modified the graph miner Gaston [8] to support BBRC mining 2 . Two specific properties allow for an efficient BBRC mining implementation on top of Gaston:
We used version 1.1 (with embedding lists), see http:// www.liacs.nl/~snijssen/gaston/ . Algorithm 1. expand() function Input : A tree q max with significance  X  2 ( q max , r ) &gt; u above the global user defined significance threshold u , a global user defined minimum frequency m , a global variable updated = true Output : The most significant backbone refinement of q 1 function expand(Tree q max , Float  X  2 ( q max , r ) ) { 2 If this.ImmediateT reeRefinements ==  X  { 3 print q max 4 updated = false 5 } 6 foreach Tree q  X   X  this.ImmediateT reeRefinements { 7 cmax = max(  X  2 ( q  X  , r ) ,  X  2 ( q max , r ) ) 8 if  X  2 u ( q  X  , r )  X  cmax { 9 if  X  2 ( q max , r ) &lt;  X  2 ( q  X  , r )  X  q  X  .frequency &gt; m { 11  X  2 ( q max , r ) =  X  2 ( q  X  , r ) 12 updated = true 13 } 14 q  X  .expand( q max ,  X  2 ( q max , r ) ) 15 } 16 else { 17 if updated { 18 print q max 19 updated = false 20 } 21 } 22 } Figure 2: Implementation of backbone refinement class mining via dynamic upper bound pruning.

Let u be the user defined minimum  X  2 value. For any frequent subtree q  X  r  X  r , let  X  2 ( q, r ) and  X  2 u ( q, r ) de-note the  X  2 value and upper bound value for q , respectively, both found by the  X  2 distribution test for q . Let u max max {  X  2 ( p, r ) | p b q } , the highest  X  2 value seen so far in this backbone refinement class. Then, if u max ( q ) &gt; u , u may be increased to u max ( q ), since we only search for the maximum class element ( dynamic upper bound adjustment ).
Every path p satisfying the user defined minimum fre-quency and significance constraint serves as backbone. The search starts with the longest  X  X .e., non-refinable  X  X aths and subsequently backtracks to the shorter ones. The algo-rithm for mining the representatives of p  X  X  refinement classes using dynamic upper bound pruning is shown in Figure 2. The procedure is invoked as p.expand ( p,  X  2 ( p, r )).
Lines 2-5 output the maximum element, if no further re-finements are available. Otherwise, for every refinement on the same level, a new class is instantiated for every refine-ment q  X  . In line 7, cmax , the maximum  X  2 value seen so far (including q  X  ) is calculated. Line 8 then implements dy-namic upper bound pruning by checking q  X   X  X  upper bound value against cmax . If it is lower, the search is truncated and the maximum at that point is output (lines 16-20). Oth-erwise, it is updated and the iteration continues.
Figure 3 visualizes the refinement process for tree q 2 from Figure 3: Backbone Refinement Classes for paths a , b , and c .
 Figure 1(b). It shows how the three different non-refinable paths in q 2 are used as backbones while the structure is ex-plored. The paths are marked bold and all carbon node labels have been made explicit. The borders of the back-bone refinement classes are outlined, their sizes are given i n brackets. Specifically, The class members are enumerated by subjecting a , b , and c to the algorithm in Figure 2. After backtracking, the same concept is applied to the other (shorter) paths in q 2 .
In this section we will investigate BBRCs in terms of running time, feature count and expressiveness. First, we test on smaller structure-activity relationship (SAR) dat a. Then, we proceed with large-scale testing on the largest la-belled dataset of structures tested so far. All calculation s were carried out on an 8 processor 2.4 GHz Intel Xeon sys-tem with 16G of RAM running Linux 2.6.24.
We evaluated four types of fragment descriptors according to feature count, running time, predictive accuracy as well as to sensitivity and specificity for the detection of active compounds. The four types are: 1. All Linear Fragments 2. Significant Trees: all trees that are frequent and have Table 2: Comparison of feature set sizes for All Lin-ear Fragments, Significant Trees and BBRC repre-sentatives. 3. Open Trees: the most general representatives of all 4. BBRC Representatives: the most significant represen-It should be pointed out that 3. and 4. form a summariza-tion of the features in 2. For the tree-shaped descriptors (2.-4.), we employed a minimum frequency of 6 to avoid ex-cessive numbers of subgraphs, which is well below 1 % of the data set size in all cases. For the linear subgraphs (1.), we applied no minimum frequency and no significance thresh-old. However, refinement was stopped at frequency 1, i.e., a fragment with single occurrence was included in the set of fragments but not further refined.

We used four chemical datasets obtained from the Car-cinogenic Potency Database (CPDB) 3 , version 08/04/29: Salmonella Mutagenicity (SM, 388 active / 810 compounds), Rat Carcinogenicity (RC, 459 active / 1145 compounds), Mouse Carcinogenicity (MoC, 428 active / 927 compounds) and Multicell Call (MuC, 553 active / 1067 compounds).
Fragment types 1., 2., and 4. were calculated with the proposed approach. For the open trees (3.), we used the method by Bringmann et al. [2]. 4
The mined subgraphs were evaluated in a leave-one-out cross-validation. For each fold, significance value calcul a-tion and feature selection were done from scratch to avoid information flow between folds. Prediction was performed with a nearest-neighbor technique, using Tanimoto similar -ity, where each substructural feature was quantified by its  X  2 significance value [3]. A training compound was selected as neighbor, if its similarity to the query structure exceed ed 0.3 (this cut-off value was used as default throughout our experiments), and its contribution to the prediction was weighted by its similarity. If no neighbor could be iden-tified, no prediction was made. Otherwise, a minimum of five neighbors was used. Besides the actual classification, a confidence value based on mean neighbor similarity was calculated for every single prediction. Aromatic rings in the structures were represented in Kekul  X e notation with al -ternating single and double bonds (aromatic perception is discussed in section 3.2).
Table 2 compares the resulting fragment set sizes for the different fragment types and Table 3 indicates the mining http://potency.berkeley.edu/cpdb.html
The authors kindly provided us with a binary of their al-gorithm sfgm , pointing out that it may not be optimized for speed and uses a breadth-first search technique known to be memory demanding. times we obtained for BBRC representatives with differ-ent statistical metric pruning techniques. More specifical ly, those times correspond to BBRC mining using no statisti-cal pruning, static upper bound pruning and dynamic upper bound pruning, respectively. Mining times for open trees us -ing sfgm were as follows: 1,899s (SM), 28,537s (RC), 1,744s (MoC), and 2,594s (MuC).

Figure 4 summarizes the information from Tables 2 and 3 by mean values of relative feature count and running time reduction across the different datasets.
 Figure 4: Mean values of Tables 2 and 3, taken over the datasets SM, RC, MoC, and MuC.

These empirical findings suggest that fragment set sizes may be reduced by 94 %, 91 % and 31 % through the use of BBRC representatives compared to linear subgraphs, signif -icant trees and open fragments, respectively. Furthermore , the application of dynamic upper bound adjustment is as-sociated with a reduction in running time by 63.34 % and 60.92 % compared to using no statistical pruning and static upper bound pruning, respectively. Considering that open trees cannot use dynamic upper bound adjustment, static upper pruning applies to them (see also section 3.2.2 and the work of Bringmann et al. [2]). The  X  X o pruning X  setting corresponds to ordinary fragment search with only minimum frequency as antimonotonic constraint, i.e., to the origin al Gaston implementation. Performance comparisons in this class-blind setting, e.g., to gSpan, can be found in the lite r-ature [9, 12].

To assess the amount of overhead incurred by our method, running time analysis was performed by comparing its pro-file to that of the original Gaston algorithm. The results indicate that our algorithm is about 6% of the time con-cerned with  X  2 and upper bound calculations, and about 3% with additional control overhead due to the more sophisti-cated expand () routine. Since running time gains of over 60% were obtained, it may be concluded that the additional effort is justified.
This section compares accuracy, sensitivity and specificit y of descriptors 1.-4. in a binary classification task. Sensi-tivity (specificity) measures the proportion of target clas s positives (target class negatives) which are correctly ide n-tified as such. Accuracy is the overall fraction of correct predictions. The figures reported in this section are derive d as follows: all includes all unweighted predictions, AD (or Applicability Domain predictions ) considers the top 80 % unweighted predictions as ranked by confidence [3], whereas wt. includes again all predictions, but this time the con-tribution of every prediction is weighted by its associated confidence value. The rationale for this measure is that er-rors of high-confidence predictions should be penalized mor e heavily than errors of low-confidence predictions. Therefo re, wt. combines and aggregates both types of information into one measure.

Table 4 compares the accuracy values of BBRC represen-tatives to all linear fragments, significant trees, and open significant trees. Table 4 shows that tree-shaped subgraphs always perform better than linear subgraphs, whether for all or AD predictions or for weighted accuracy. BBRC rep-resentatives outperform open trees in 10 out of 12 cases. The mean accuracy difference between BBRC representa-tives and significant trees is -0.27  X  1.47, whereas it is 1.1  X  1.44 compared to open trees and 2.77  X  1.66 compared to linear fragments, respectively. A paired t -test on the accu-racy values revealed that BBRC representatives perform sig -nificantly better than open trees ( t = 2 . 65, df = 11, p -value = 0.02267), while no significant difference between BBRC representatives and the complete set of trees ( t = 0 . 65, df = 11, p -value = 0.5302) was observed. Figure 5 compares the four different types of descriptors in ROC space, showing the differences in sensitivity and specificity for the predic -tion of active compounds. There is a trend for better val-ues when tree-shaped fragments are used, clearly signallin g the higher information load present in those descriptors. BBRC representatives seem to exhibit a lower false alarm rate compared to open trees. Indeed, a paired t -test on the false positive ratios confirmed that BBRC representatives significantly improved on specificity ( t =  X  4 . 60, df = 11, p -value = 0.00077). A significant difference in sensitivity could not be detected. All four test results were confirmed with Wilcoxon signed rank tests [11].

We also evaluated backbone refinement class representa-tives on the three datasets from the study of R  X  uckert and Kramer [10]. To render the results comparable, we also used 6 % of the dataset size as minimum frequency threshold. Af-ter performing 10-fold cross-validation, we obtained the a c-Table 5: Accuracy table for datasets used in the study of R  X  uckert and Kramer [9] curacies shown in Table 5. Apart from the Yoshida dataset, the results seem to be competitive to the respective figures from the original publication. In particular within the app li-cability domain, results are very similar to those of the SLS method. The table also gives the mean number of features and the running time for feature calculation per fold. In terms of running times, the method proposed here is much faster, as the construction of the trie for the SLS method typically takes a few minutes, whereas the SLS run itself may take hours [10].

In summary, BBRC representatives exhibit a significantly higher specificity compared to open trees, while overall acc u-racy seems to be competitive to significant trees. They are more sparse than open trees (reducing feature count), and dynamic upper bound adjustment is multiple times more ef-fective than the static method used for open tree mining in terms of running times.
We employed publicly available large-scale databases to assess several advanced aspects of BBRCs beyond running time, feature count and predictivity. More precisely, we pe r-formed experiments on parts of the NCI Yeast Anticancer Drug Screen datasets 5 (April 2002 release). This dataset reports growth inhibition of yeast strains when exposed in-dividually to a large number of compounds as compared to solvent only. To the best knowledge of the authors, AC-One (stage 0) and AC-All (stage 0) are the largest labelled datasets that have been considered in correlated graph mining. The analy-sis investigates different minimum frequency thresholds an d the impact of aromatic perception, i.e., whether the miner http://dtp.nci.nih.gov/yacds/download.html figures indicate the best results. uses special labels for carbons and edges that are part of an aromatic ring, or whether Kekul  X e notation employing al-ternating single and double bonds is used. All calculations were carried out on the same computer as in section 3. Section 3.2.1 gives an overview of dataset coverage and BBRC class sizes, and in section 3.2.2 we report on a cross-validation run using those large-scale data.
Given that BBRCs are partially ordered, it is likely that their representatives occur more frequently than at mini-mum frequency, since, in general, they are not the most specific fragment of their respective classes. Ideally, thi s should give a good coverage of the dataset with few descrip-tors of a relatively high minimum frequency. Such a reduced set would then allow for fast, memory saving computational models as well as for interpretable representations of the (de)activating substructures of a training set. In particu lar, for use in a predictive model, it is necessary to assess the coverage and representativeness of descriptors.

First, we examined the highest minimum frequency of 200 for the AC-One (stage 0) and AC-All (stage 0) datasets to see how well they would cover the data set, i.e., assessed numbers of descriptors per compound, and compared them to the respective figures for a minimum frequency of 100.
It clearly can be observed that the mean coverage does not change much. For instance, the AC-One (stage 0) dataset with aromatic perception has a mean log value of 1.650 for a minimum frequency of 200, and a mean log value of 1.673 for a minimum frequency of 100. A comparison of the median values (1.623 vs. 1.633) showed an even smaller difference indicating a skew to the right for a minimum frequency of 100.

In contrast, the effects of missing aromatic perception were much greater. The AC-One (stage 0) dataset with-out aromatic perception has a mean log value of 1.878 for a minimum frequency of 200 and a mean log value of 1.896 for a minimum frequency of 100. Similar results were obtained for the AC-All (stage 0) data (details omitted).
The conclusion here is that a lower minimum frequency does not increase coverage for the majority of compounds, as it might be expected from the feature count differences presented in the previous section. In contrary, the normal-ity of the corresponding density curves (not shown) indi-cate clearly that coverage is handled more appropriately by the higher threshold. In this setting, for AC-One (stage 0) [AC-All (stage 0)], the mean number of descriptors per com-pound is about 10 1 . 63  X  42 . 6 [10 1 . 61  X  40 . 7] with aromatic ring perception and about 10 1 . 91  X  81 . 28 [10 1 . 82  X  66 . 07] without.

The higher coverage for the non-aromatic setting raises the question whether this is an expression of low repre-sentativeness, i.e., whether backbone refinement classes a re smaller for the non-aromatic setting. We assessed back-bone refinement class sizes via static upper bound pruning, thus putting any significant and frequent tree into exactly one class and counting them. The frequency distribution of classes with different sizes in Figure 6 shows clearly that the non-aromatic setting produces far more classes of the same size, and moreover, the mean class size is significantly lower than for the aromatic setting (a skew to the right for the non-aromatic setting). Therefore, class size is indeed smaller, and this may be attributed to a lower expressive-ness.

In summary, the method proved robust in terms of de-scriptor coverage for the higher threshold, allowing for fa st mining with higher frequency thresholds. Moreover, the re-sults provide additional evidence for the coverage capacit y of tree-shaped descriptors. Finally, the aromatic setting se ems to produce a more sparse and representative collection of patterns.
To conclude our experiments, we report on two large-scale cross-validation runs, using aromatic perception. Since a balanced dataset is vital for a nearest-neighbor approach, and in view of the results from earlier sections, we extracte d a subset of AC-one (stage 0), composed of all actives and an equal number of inactives sampled randomly from the dataset (2*11,700 = 23,400 compounds). The second run, on AC-All (stage 1), used all actives and inactives (in total , 5,248+5,300 = 10,548 compounds).
 Table 6: Feature counts for AC-One (stage 0) and AC-All (stage 1)
We compared the number of BBRC representatives to the feature counts of other summarization methods. This time we investigated additionally the set sizes of maximum pat-terns (the positive border as implied by the minimum fre-quency and 95 % significance constraints; see also the work by Hasan et al. [1]).

For AC-All (stage 1), the extraction of open trees with sfgm took &gt; 10h, BBRC representatives took 1m13s. For AC-one (stage 0), open trees did not finish, while BBRC representatives took 4m52s.

Table 6 shows that BBRC representatives had a very con-densed representation of below  X  5%; maximal and open patterns achieved a reduction up to only  X  50%; thus, BBRC representatives reduce feature counts much more drastical ly for these large-scale datasets than for the smaller validat ion datasets (see Section 3).

Indeed, BBRC representatives turned out to be the only practically useful feature type for validation on the (quit e powerful) computer we used. A prediction included the derivation of the training set similar to the query instance based on features occurring in the compounds and the calcu-lation of the prediction in the same fashion as in Section 3. With open trees, we obtained impractical prediction times of &gt; 60 s , whereas BBRC representatives gave a mean of 4 . 7 s and 11 . 1 s , respectively. Also, RAM usage was unacceptable with open trees. Table 7 shows the validation results we obtained. Table 7: Validation results for AC-One (stage 0) and AC-All (stage 1), using BBRCs
In summary, we consider the class of BBRC representa-tives a promising candidate for large-scale structural dat asets, rendering them computationally feasible. Judging from our results in section 3, we can also be quite confident about the quality of predictions.
In the paper, we introduced backbone refinement classes (BBRCs), a particularly useful class of subgraphs for min-ing databases of chemical compounds. Due to their formal properties, BBRCs can be mined efficiently and searched by existing graph mining systems like Gaston with only minor modifications. The overall method proves to be highly effi-cient compared to mining significant and open trees, dramat-ically reducing running time and number of features mined.
Moreover, the experimental results revealed that the ex-pressiveness of backbone refinement class representatives is significantly higher than that of open trees, because a lower number of features is associated with better accuracy, main ly due to higher specificity, reducing false alarms in classific a-tion tasks. The mined tree structures form a sparse and structurally diverse pattern collection. Their inter-ent ropy seems to be beneficial for SAR systems using tree-shaped features in that a large fraction of structurally similar fe a-tures are left out, which cannot be guaranteed by occurrence summarization methods, such as open or closed subgraphs.
In our experiments on the largest labelled set of chemi-cal compounds used so far in class-correlated graph mining, we showed that BBRCs can be computed within reasonable time and used in simple predictive learning schemes. In view of the dramatic decrease in numbers, they may be also suit-able for identifying so-called structural alerts, i.e., ch emical substructures that are thought to be particularly toxic.
The authors thank Bj  X  orn Bringmann for providing a bi-nary and friendly cooperation in dataset testing, and Ulric h R  X  uckert for providing datasets. The research was (partially ) supported by the EU seventh framework programme under contract no Health-F5-2008-200787 (OpenTox). [1] M. Al Hasan, V. Chaoji, S. Salem, J. Besson, and [2] B. Bringmann, A. Zimmermann, L. de Raedt, and [3] C. Helma. Lazy Structure-Activity Relationships [4] T. Horv  X ath, J. Ramon, and S. Wrobel. Frequent [5] K. Jahn and S. Kramer. Optimizing gSpan for [6] S. Kramer, L. De Raedt, and C. Helma. Molecular [7] S. Morishita and J. Sese. Traversing Itemset Lattices [8] S. Nijssen and J. N. Kok. A Quickstart in Frequent [9] S. Nijssen and J. N. Kok. Frequent Subgraph Miners: [10] U. R  X  uckert and S. Kramer. Optimizing Feature Sets [11] F. Wilcoxon. Individual comparisons by ranking [12] M. W  X  orlein, T. Meinl, I. Fischer, and M. Philippsen. [13] X. Yan and J. Han. gSpan: Graph-Based Substructure [14] X. Yan and J. Han. CloseGraph: Mining Closed
