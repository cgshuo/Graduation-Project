
Many real applications can be modeled using bipartite graphs, such as users vs. files in a P2P system, traders vs. stocks in a financial trading system, conferences vs. au-thors in a scientific publication network, and so on. We introduce two operations on bipartite graphs: 1) identify-ing similar nodes (Neighborhood formation), and 2) find-ing abnormal nodes (Anomaly detection). And we propose algorithms to compute the neighborhood for each node us-ing random walk with restarts and graph partitioning; we also propose algorithms to identify abnormal nodes, us-ing neighborhood information. We evaluate the quality of neighborhoods based on semantics of the datasets, and we also measure the performance of the anomaly detection al-gorithm with manually injected anomalies. Both effective-ness and efficiency of the methods are confirmed by experi-ments on several real datasets.
A bipartite graph is a graph where nodes can be divided into two groups V 1 and V 2 such that no edge connects the vertices in the same group. More formally, a bipartite graph G is defined as G = V 1  X  V 2 , E ,where V 1 = { a i | 1  X  i  X  k } and V 2 = { t i | 1  X  i  X  n } , E  X  V 1  X  V 2 as shown in Figure 1 .
Many applications can be modeled as bipartite graphs, for example: presentation purposes, we will only focus on unweighted graphs; our algorithms can be easily generalized to other graph types.

Under this setting, our work addresses two primary prob-lems: 1. Neighborhood formation(NF): Given a query node 2. Anomaly detection(AD): Given a query node a in V 1 ,
Nodes that belong to the same group ( V 1 or V 2 )havethe same type; it is the connections between the two types of ob-jects that hold the key to mining the bipartite graph. Given the natural inter-group connections (between V 1 and V 2 ), our objective is to discover the intra-group relationships, such as the clusters and outlier within the group. For example, in the research publication bipartite graph, we have two nat-ural groups of entities: conferences and authors. The rela-tionship between these two groups is reflected by the edges. Based on these edges, we want to find the similar confer-ences and unusual authors that publish in different commu-nities. An effective mining algorithm should thus be able to utilize these links across the two natural groups.
Our algorithm for NF is based on the idea of random walks with restarts [ 8 ]. The method is simple, fast and scal-able. In addition, we approximate the NF computation by graph partitioning to further boost the performance.
The algorithm for AD uses the relevance scores from NF to calculate the normality scores. Intuitively, a node (in V 2 ) is an anomaly if it links to two nodes (in V 1 ) that do not belong to the same neighborhood/community. For example, an author becomes an anomaly if he/she publishes papers in conferences from two different fields. In the sequel, we will use neighborhood and community interchangeably.

Note also that a natural symmetry exists in the roles of neighborhoods and anomalies. In particular, we can swap V and V 2 and apply the same algorithms in order to obtain the relevance score in V 2 and the normality score in V 1 . In summary, the contributions of the paper are that:
The nodes in V 1 ( V 2 ) are called row(column) nodes. Note that a column node links to a row node if the corresponding matrix element is not zero. Moreover, row node a connects to another row node b if there is a column node c linking to both a and b . We call that path a connection between a and b through c . Nodes a and b can have multiple connections via different column nodes. For example in the matrix above, rows 3 and 5 links through column 1, 2, 4 and n .
We can construct the adjacency matrix M A of G using M easily: In particular, M A ( a , t ) denotes the element at a -th row and t -th column in M A .

Suppose we want to traverse the graph starting from the row node a . The probability of taking a particular edge &lt; a , t &gt; is proportional to the edge weight over all the outgoing edges from a . More formally, P A ( a , t )= M
A ( a , t ) /  X  k matrix P A of G is constructed as: P A = col norm ( M A ) , where col norm ( M A ) normalizes M A such that every col-umnsumupto1.

The main reasons to have M instead of working directly on M A and P A are the computational and storage savings. Next, we define the two problems addressed in the paper: Neighborhood Formation (NF): Given a node a  X  V 1 , which nodes in V 1 are most related to a ? There are two ways to represent the neighborhoods: 1) select a set of nodes as the neighbors and the other nodes are not the neighbors (Hard Neighborhood); 2) assign a relevance score to ev-ery node where  X  X loser X  nodes have high scores, and no hard boundary exists (Soft Neighborhood). In this paper, we adopt the soft neighborhood, because the score can help identify neighborhood but also differentiate the neighbors. In particular, we want to compute a relevance score to a for every node b  X  V 1 . The higher the score is, the more re-lated that node is to a . More specifically, the node with the highest score to a is a itself; the nodes that are closer to a probably have higher scores than the other nodes that are further away from a .
 Anomaly Detection (AD): What are the anomalies in V 2 to a query node a in V 1 ? Again we adopt the notion of soft anomalies by computing the normality scores for nodes in V that link to a . Hence, the nodes with lowest normality score are the anomalies to a .
In this section we discuss the algorithms that solve the two problems presented above. We first define relevance score and describe how to compute the relevance scores for the row nodes (neighborhood formation) in section 3.1 . Algorithms: We propose three methods for computing relevance scores: (1) Exact NF implements the basic idea but can have slow convergence rates, (2) Parallel NF im-plements the same algorithm in parallel, and (3) Approxi-mate NF performs graph partitioning first which calculate results approximately but much more efficiently. Exact NF: First, we transform the input row node a into a ( k + n )  X  1 query vector q a with1inthe a -th row and 0 otherwise. Second, we need to compute the ( k + n )  X  1 steady-state probability vector u a over all the nodes in G . Last we extract the probabilities of the row nodes as the score vectors. Note that u a can be computed by an iterated method from the following lemma.
 Lemma 3.1. Let c be the probability of restarting random-walk from the row node a. Then the steady-state probability vector u a satisfies where P A is already the column normalized.
 Proof. See [ 16 ] Algorithm NF E (Exact NF) Input: node a , bipartite matrix M , restarting probability c , tolerant threshold  X  0. initialize q a = 0 except the a -th element is 1 ( q a ( a )= 1) 1. construct M A (see Equation 1 )and P A = col norm ( M A ) 2. while ( |  X  u a | 3 &gt;  X  ) u a =( 1  X  c ) P A u a + c q a 3. return u a ( 1: k )
The algorithm simply applies Equation 2 repeatedly until it converges. The actual computation of the algorithm can utilize the bipartite structure to have more saving. More specifically, we do not materialize M A and P A and modify Equation 2 as follows: u a =( 1  X  c ) and last n elements of u a , respectively. The relevance score rs ( a ) is u a ( 1: k ) . If we compute the relevance scores for all the nodes, we have a similarity matrix S .

The saving is significant when the number of rows k and the number of columns n differ a lot. Therefore, Equa-tion 3 is always recommended in practice, while Equation 2 is only for demonstrating the concept.
Essentially, given an input t  X  V 2 , we first compute the relevance score vectors for every adjacent row node S t to t (using any of the NF methods described in section 3.1 ). Then we obtain the similarity matrix RS t and apply the score function on RS t . A computational trade-off is whether or not to pre-compute the relevance score vectors of all the row nodes. It usually depends on the number of row nodes involved. For example, if the dataset has a large number of rows and the input queries are skewed, pre-computation is not recommended, because it incurs huge cost and most of them is wasted due to the skewed distribution of the queries. Algorithm AD(Anomaly Detection) Input: input node t , bipartite transition matrix P E . 1. compute all the relevance score vectors R of a  X  S t 2. construct the similarity matrix RS t from R over S t 3. apply the score function over RS t to obtain the final nor-mality score ns ( t ) 4. return ns ( t ) Examples of anomalies: Figure 4 shows the typical ex-ample of an anomaly t , which links to two row nodes a and b that communicate to different sets of nodes. Without t , a and b belong to different neighborhoods. Note that one requirement of constructing the neighborhoods is that a and b need to have enough connections to establish their identi-ties. For example, a and b in Figure 4 still have a number of connections without t , while in Figure 5 , b has no other con-nections apart from t .Thisimpliesin Figure 5 the neighbor-hood of b is unknown (or we do not have enough confidence to say whether b belongs to the same neighborhood as a or not). Therefore, t in Figure 5 will not be identified as an anomaly, while t in Figure 4 will.

On the other hand, the example in Figure 5 is easy to be found by simply counting the degree of the row nodes and picking the ones with only one connection. Potentially, the number of such nodes can be huge. The point is that our method aims at a non-trivi al case of the anomaly, which tries to identify the connections across multiple neighbor-hoods. For example, author A and B write many papers with different groups of authors. If there is a paper between A and B, it will be an anomaly, because we know A and B belong to different neighborhoods. However, if B only has one paper and A is the co-author, we cannot decide whether the paper is an anomaly, because we do not know the neigh-borhood of B other than the sole paper with A.
In this section we evaluate the exact and approximate methods on neighborhood formation and anomaly detec-tion. We focus on answering the following questions:
Author-Paper(AP) dataset: Every row represents an author; every column represents a paper. The elements in the bipartite matrix M are either 0 or 1. In particular, M ( i , j )= 1 indicates that the i -th author is an author for the j -th paper. On average, every author has 3 papers, every pa-per has 2 authors. The distribution is very skewed as most of authors have only one paper.

IMDB dataset: Every row is an actor/actress; every col-umn is a movie. The elements in the bipartite matrix M are either 0 or 1. In particular, M ( i , j )= 1 indicates that the i -th actor/actress is in the j -th movie. On average, every actor/actress plays in 4 movies, and every movies has 11 actors/actresses. 4.2 (Q1) Evaluation of Exact NF Exact NF: We want to check whether the nodes with high relevance scores are closely related to the query node. The goal is to ensure the result makes sense in the context of the applications. More specifically, we select some rows from the three datasets as the query nodes and verify the NF scores through user study. Due to the page limit, we just show one example from each dataset.

CA dataset: Figure 6 (a) shows the top 10 neigh-bors of ICDM conference. As expected, the most re-lated conferences are the other data mining conferences: KDD, PAKDD, PKDD. After that, the database conference (ICDE, SIGMOD, VLDB) and the machine learning con-ference (ICML) also have high relevance scores 5 . AP dataset: Figure 6 (b) plots the top 10 neighbors of Prof. Jiawei Han. They are indeed the close collabora-tors to Prof. Han, who either have many joint papers with Prof. Han or have several exclusive joint papers.
IMDB dataset: For IMDB dataset, we perform the same set of experiment as above. Unlike the previous two datasets, the people in this d ataset are not well-clustered, meaning that if a and b play in the same movie, it does not increase the likelihood that they will play together again in the future. Of course, they are exceptions in the sequels of successful movies.

We choose Robert De Niro as an example here. The persons with the highest relevance scores, as shown in Fig-ure 6 (c), are Billy Crystal and Lisa Kudrow because they all perform in the same 2 movies ( X  X nalyze this X  and the se-quel  X  X nalyze that X ). Furthermore, they are the only main actors/actress in the movies. This is again due to the result of the combination of 2 scenarios in section 3.1 . 4.3 (Q2) Evaluation of Approximate NF
We partition each dataset into k partitions with equal size using METIS [ 11 ]. The NF computation for a row node a ing to k row nodes, where k equals the average degree of column nodes. The row nodes are randomly selected among the column nodes with large degree (greater than 10 times the average) 6 . Note that the difference between using exact and approximate NF is marginal. And we use approximate NF in the AD algorithm to reduce computational cost.
Figure 9 plots the average normality scores of genuine and injected nodes over three different datasets. We ob-serve a big gap of the normality scores between genuine and injected ones. Hence, we can easily identify the anoma-lies by looking at the ones with the lower scores within the same dataset. Note that only the relative score matters in detecting anomaly not the absolute score. And it is hard to compare the scores across datasets because of the different graph structure.

Figure 9. Normality scores between genuine and injected nodes across 3 datasets 4.5 (Q4) Evaluation of the computational cost
All the computation of different methods boils down to the NF computation. The only difference is how large the matrix is. Intuitively, the computational cost is large if we work with the entire dataset. It is usually beneficial to parti-tion the dataset. The partition incurs a one time cost which can be amortized over the future queries (involving NF and AD computation). Figure 10 shows the computation cost on neighborhood formation vs. the number of partitions. formed by the graph nodes themselves, so the vector space and the points in it are related.
 Random-walk on Graphs: Page-Rank [ 2 ] learns the ranks of web pages using the iterated power method on web graph M (adjacency matrix of the entire graph). The ranks of all webpages are cast as an N-dimensional vector, and then the fixed point is found for the following equation: r =( 1  X   X  ) M  X  r +  X  p ,wherethe  X  is the damping fac-tor and p =[ 1 N ] N  X  1. Thus, there is an uniform prior on all the web pages. In order to deal with personalized query, Topic-Sensitive PageRank [ 8 ] increases the importance of certain web pages by putting non-uniform weights for p . Similar random-walk approaches have been used into other domains; for example, Mixed Media Graph(MMG) [ 14 ]ap-plies random walk with restart on image captioning applica-tion. We plan to further explore the random walk algorithm on bipartite graph and use it to identify anomaly nodes. Similar idea also appear in SimRank [ 9 ] which is a similar-ity measure between nodes in a graph with the intuition that two nodes are similar if they are related by similar nodes. Collaborative Filtering Collaborative filtering is a well-studied method of making automatic filtering about the user interests based on the historical information from many users (collaborating) [ 15 ]. The goal is to develop a rec-ommendation system not to find anomalies.
A variety of datasets can be modeled as bipartite graphs, such as P2P networks, stock trades, author-paper relation-ships, and so on. This paper addresses two problems on such bipartite graphs: 1) neighborhood formation; 2) anomaly detection. The main properties of the methods are:  X  Fast convergence  X  Scalability to large graphs  X  Simplicity of implementation  X  Results that are easily interpreted The main idea is to use random-walk with restarts and graph partitioning. We evaluate the methods on several real datasets. Our experiments confirm the efficiency as well as the effectiveness of the proposed methods.
 [1] C. Aggarwal and P. Yu. Outlier detection for high-[2] Sergey Brin and Lawrence Page. The anatomy of
