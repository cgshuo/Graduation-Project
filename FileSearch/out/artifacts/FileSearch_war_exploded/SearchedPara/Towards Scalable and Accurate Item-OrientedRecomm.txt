 Most recommenders research aims at personalized systems, which suggest items based on user profiles. However, in real-ity many systems deal with item-oriented recommendations. In such setups, given a single item of interest, the system needs to provide other related items, following patterns like  X  X eople who liked this also liked... X .

While item-oriented systems are central in their impor-tance, they have been approached so far using very basic tools. We identify several hurdles faced by standard ap-proaches to the item-oriented task. First, the sparseness of observed activities prevents establishing reliable similarity relations for many item pairs. Second, we address a scalabil-ity challenge at the retrieval stage present in many real-world systems: Given an item inventory, which may encompass millions of items, it is desired to identify the most related item pairs in a sub-quadratic time. This work addresses these two challenges, thereby improving both accuracy and scalability of item-oriented recommenders. Additionally, we propose an empirical evaluation scheme for comparing the quality of different solutions with encouraging results. H.2.8 [ Database Management ]: Database Applications X  Data Mining collaborative filtering, item-based recommendation
Most research on recommender systems is focused on mod-eling relations between users and items. New items are rec-ommended to users based on their past purchases or activi-ties. However, in reality user profiles are often not available (e.g., when a user is new or not logged-in) or irrelevant (e.g., when the current, short-term interest is unrelated to longer term inclinations). Therefore, many industrial systems are based on item-oriented recommendations, where item sug-gestions are solely based on their relatedness to a small set of currently considered items. Such recommendations aim at highlighting alternative items or items that are often bought together. A well known example of such a recommender is Amazon X  X  shopping cart recommender. Amazon X  X  system capitalizes on customers impulse buying patterns [10]  X  i.e., when a customer is currently considering a specific book, the system suggests to her more books that are often sold together. This real-world popular recommendation setup is characterized by inferring the recommendations based on the user X  X  current interest rather than on her long term activity history. In general, these item-oriented systems employ sig-nals like  X  X sers who liked this product also liked... X , which are directly mined from usage logs.

While evidentially used in practice, we are not aware of many published scientific works addressing collaborative-filtering item-oriented recommendations. This is not very surprising as the problem seems rather simplistic compared to personalized recommendations, which indeed may use item-item relations internally. After all, one could argue that simply counting co-usage patterns among items can almost directly deliver the required similarity scores in the form of item-item conditional purchase probabilities, or other pair-wise similarity metrics (e.g., cosine and Jaccard similari-ties, etc.). However, we wish to highlight two shortcomings of such straightforward retrieval techniques. First, point-wise counting of item-item co-occurrences requires an ad-equate support for both items. Hence, similarities related to items subject to less user activity cannot be estimated reliably. Second, a na  X   X ve retrieval of most recommended items requires evaluating all possible items for each target item. Therefore, computing all related item pairs requires a quadratic time which is hard to scale as item inventories grow in size. The scalability issue could be alleviated by dif-ferent pruning rules or by caching results, yet we suggest a more systematic solution which accounts for both scalability challenges as well accuracy challenges.

We present Euclidean Item Recommender (EIR) which is a new method for identifying related item-pairs while ad-dressing the two aforementioned shortcomings. Instead of determining item-item relations by the limited number of co-occurrences, we represent item-item conditional probabil-ities through latent factor vectors which are learned using a  X  X lobal  X  learning algorithm. Our method is utilizing the en-tire training data rather than just the pairwise information. Hence, we achieve a smooth estimation of item-to-item re-lations that facilitates more reliable recommendations even fo r rarely co-purchased item pairs. Furthermore, the latent factor vectors are embedded in a Euclidean space, thereby allowing a fast retrieval of most similar items using metric trees , without requiring the consideration of all possible item pairs. Finally, another contribution of this work is the es-tablishment of a modeling and experimentation scheme for item-oriented recommendation, which employs existing user preference datasets.
Item-item neighborhood methods are well studied in the academic literature [10, 13]. We share with these works the need to derive pairwise relations between items. However, our research is item-oriented rather than user-oriented. Un-like previous work, we do not model the users in our dataset. We focus on modeling the item-item relations directly.
Some prior works [9, 12] used latent factors for represent-ing item-item relations. This work also follows this path with several distinctions. First, we aim at item-oriented rec-ommendation dictating a different cost function and training method. In addition, we emphasize fast retrieval which is facilitated by embedding the factor vectors in a Euclidean space, rather than the inner-product space used by others.
Our usage of Euclidean embedding shares some similari-ties with a recent work on Euclidean matrix factorization [7]. However we model item-item relations and not user-item re-lations. Beyond this, a fundamental distinction between the works is that we address the implicit feedback case and not explicit ratings, which is in fact the common case in most systems where explicit rating data is not present. Implicit feedback requires a different formulation based on maximiz-ing likelihood rather than squared error minimization [6]. In addition, we propose a novel way of incorporating biases into the model without hindering the effectiveness of the fast retrieval.
We represent item-item relations through their conditional probabilities. That is, given items i and j we will estimate P ( j | i ), the conditional probability that a user consuming item i will consume item j as well. The conditional prob-abilities will be learned by embedding all items in a low-dimensional Euclidean space. An item k will be represented by a d -dimensional vector y k  X  R d , and a scalar bias b latent item vectors are designed to capture item similarities and the biases capture popularity patterns independently of other co-consumed items. To this end, we define the condi-tional probability P ( j | i ) by the multinomial distribution Note that we assure set D , containing item pairs of co-consumed (or co-liked) items, we seek to learn model parameters that maximize the log-likelihood of the training set:
Learning proceeds by stochastic gradient ascent. Given a training pair ( i; j ) we update each parameter (a latent vector component or a bias ) by  X  = @P ( j where is the learning rate. However, such a training scheme would be too slow in practice as each update rule requires summing over all items. We thus resort to sampling the weighted sum in (3) based on the importance sampling idea proposed by Bengio and Sen  X ecal [2]; see also Aizenberg et al. [1] for a related usage of the technique.

With importance sampling we draw items according to a proposal distribution . In our case we assign each item a probability proportional to its empirical frequency in the training set (fraction of train pairs containing the item), and denote this proposal distribution by P ( i |D ). Items are sampled with replacement from P ( i |D ) into a list L . Using L , we approximate P ( k | i ) for each k  X  X  with the weighting scheme
Consequently, the approximated gradient ascent step given a training pair ( i; j ) will be As mentioned in [2], it is desirable that the size of the set grows as the training process proceeds because at later train-ing phases more delicate parameter adjustments are needed. Hence, we employ a simple rule for controlling the sample size ( |L| ) based on the fitness of the current estimate. Given a training pair ( i; j ), we keep sampling items into L until the following condition is satisfied: The adaptive sampling automatically lets the sample size grow when parameters are nearing final values and the cor-rect paired item is getting a relatively high probability. In our implementation we used = 3. We impose a minimal size of 5 on the sample size. For efficiency we also limit the maximal sample size to 500.
Retrieval time of recommendations is a key factor when designing real-world large-scale systems that need to address tens of thousands of queries per second [8]. A major design goal of EIR is enabling fast pairing of items. In this setting, the task of efficiently retrieving recommendations requires finding items that the user is most likely to purchase given the item she is currently considering. Namely, we wish to find an item j that maximizes: T able 1: The number of items, users, training-set examples and test-set examples in each dataset.
 It is easy to see that by ignoring biases we are left only with the squared Euclidean distance between the two items vectors, and the retrieval is reduced to a simple nearest neighbor task. Nevertheless, biases are a key contributor to recommendations accuracy, and should not be dismissed. We therefore propose a simple transformation to reduce the problem in (7) to that of a simple Euclidean search. For each item vector y j , we define a concatenated item vector  X  y j as follows: where M b is the maximum bias ( M b = max j b j ). We also define a concatenated query vector as follows: It can be easily shown that (7) is equivalent to: Therefore, this transformation facilitates a variety of Eu-clidean nearest neighbor algorithms e.g. Metric Trees, or Locality Sensitive Hashing (LSH). We evaluate our algorithm using four different datasets: Netflix [3], the Million Song Dataset (MSD) [4], Ziegler X  X  books X  reviews 1 (Books) [16], and Yahoo! Music (YMusic) [5]. Our work is focused on implicit ratings, however of the four aforementioned datasets only the MSD dataset is implicit. Therefore, we simulated imlicit data from the ex-plicit datasets as follows: In Netflix, we first filtered only the ratings with a value  X  4. We then produced a dataset of  X  X o-liked X  movies  X  namely, movies that were liked by the same users. For the YMusic dataset, we repeated the same process for ratings  X  80. In the books dataset we simply used all the data-entries in order to create a dataset of co-consumed books that were read and reviewed by the same users (regardless of the review). Finally, in all datasets we generated item pairs as follows: For each user we created a random cyclic order of the items consumed by her. Then, the final dataset is comprised from all the pairs of consecu-tive items.

We split our datasets into train and test subsets by ran-domly choosing a subset of users and placing all their item pairs in the test-set. Table 1 summarize the final datasets statistics.
We compared performance against traditional item-item similarity measurements: ww w.informatik.uni-freiburg.de/~cziegler/BX/ No te that ranking based on these similarity measures differ in the way of normalizing the co-counts ( n i;j ).
We measure performance in terms of Mean Percentile Rank (MPR). This metric was earlier used in studies of implicit feedback datasets within the context of personalized recom-mendations [6, 14]. In our item-oriented context, it is de-fined as follows: For each test pair of related items ( i; j ) we sample N additional random items. We rank every item k (the N random ones and j ) with respect to P ( k | i ) based on our model. Then, we compute the percentile rank of item j within this ranking. Ranks are averaged over all test pairs. Accordingly, percentile ranks closer to zero indicate better rankings. We used N = 200 in our experiments, though re-sults are insensitive to changes in N given the large number of test pairs.

We trained the EIR model with d = 50 dimensions. Ta-ble 2 presents the MPR results of EIR against the baselines. We were encouraged by the fact that our algorithm outper-formed the baselines on the Netflix, MSD and the Books datasets. On the YMusic dataset, our algorithm was second to ECP with a very small difference between the two.
Figure 1 depicts the mean percentile rank of the different algorithms vs. the support (popularity) of the conditioned item i in P ( j | i ). While all the algorithms perform well on popular items, EIR has a clear advantage in the long tail. This is explained by the fact the EIR employs global op-timization that utilize the entire training data rather than just the pairwise information. Among the different baseline algorithms, ECP is most similar to EIR. This is to be ex-pected, as ECP is merely an empirical estimate of P ( j | the objective of EIR.
We evaluate the fast retrieval capabilities of EIR using metric trees [11, 15]. Metric trees are binary space-partitioning trees widely used for the task of indexing Euclidean datasets. The tree construction is very fast and space efficient, and the search employs the depth-first branch-and-bound algorithm similar to that of [15]. We quantify the improvement in retrieval time for the task described in (7) when using the dataset (lower is better).
 Ne t ix 5.8 25% 6.57% 9.728% 8.376% MSD 6.6 02% 13.663% 35.287% 49.18% YMusic 2. 837% 2.536% 9.795% 9.545% Books 2 7.342% 35.902% 53.896% 61.011% T able 2: Comparing MPR of EIR against common baselines (lower is better).
 T able 3: Speedup values for EIR with 50 dimensions metric tree compared to a na  X   X ve search as follows: The speedup values for each dataset are presented in Table 3.
We tackle item-oriented recommendations, where the goal is to find items related to the current item. Our approach is centered around a method embedding items and their bi-ases within a Euclidean space in a way that preserves co-consumption patterns. A unique feature of our embedding is its being Euclidean thereby facilitating fast item retrieval with readily available data structures suitable to the Eu-clidean space. We demonstrated the efficacy of the method in terms of both accuracy and indexing time. [1] N. Aizenberg, Y. Koren, and O. Somekh. Build your [2] Y. Bengio and J.-S. Sen  X ecal. Quick training of [3] J. Bennett and S. Lanning. The netflix prize. In Proc. [4] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and [5] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [6] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [7] M. Khoshneshin and W. N. Street. Collaborative [8] N. Koenigstein, P. Ram, and Y. Shavitt. Efficient [9] Y. Koren. Factor in the neighbors: Scalable and [10] G. Linden, B. Smith, and J. York. Industry report: [11] F. P. Preparata and M. I. Shamos. Computational [12] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. [13] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [14] H. Steck. Training and testing of recommender [15] J. K. Uhlmann. Satisfying general [16] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
