 Recently there has been much interest in the problem of similarity search (query-by-content) in multimedia databases (i.e. time series, spatial data, images etc.). Similarity search is useful in its own right as a tool for exploratory data analysis, and it is also an important subroutine of many data mining applications such as clustering [8], classification [17] and mining of association rules [7]. The similarity between two objects can usually be calculated very efficiently, so the similarity search process is heavily I/O bound. The volume of data encountered exasperates the problem. Multi-gigabyte datasets are increasing prevalent. As typical example, consider the MACHCO project. This database contains more than a terahyte of data and is updated at the rate of several gigabytes a day [23]. The most promising similarity search methods are techniques that perform dimensionality reduction on the data, then use a multidimensional index structure to index the data in the transformed space. The technique was introduced in [1] and extended in [19, 6, 21, 10]. The original work by Agrawal et. al. utilizes the Discrete Fourier Transform (DFT) to perform the dimensionality reduction, but other techniques have been suggested, including Singular Value Decomposition (SVD) [18, 15], the Discrete Wavelet Transform (DWT) [5, 24] and Pieeewise Polynomial Approximations [ 15, 26]. In general, the efficiency of indexing depends only on the fidelity of the approximation in the reduced dimensionality space. Clearly no single dimensionality reduction technique can be optimal on all datasets, therefore, most researchers advocate empirical comparisons of several approaches before implementing a single fixed technique [24, 25, 15]. 
While this approach seems reasonable, it does risk the following problem. The chosen representation may tightly approximate most of the data, but provide a very poor approximation of a small section of the data. Any query for an item whose eventual best match is one of the poorly approximated items is likely to result in a long query time, because large amounts of the multidimensional index structure will have to be inspected. As an example, consider 
Figure 1. On a global level, this particular financial time series is much better approximated by the DFT transform than the DWT transform. However there are some small areas, for example beginning at about 850, where the DWT transform is superior. So, in general, we would be better off indexing this time series with 
DFT, but for queries that will have an eventual best match different sections of the dataset favor different dimensionally reduction techniques. Rather than committing to a single dimensionally reduction technique, E-index creates several indices, one for each representation. When the data is indexed, E-index places each object in the index that can represent it with the greatest fidelity. At query time, the query is transformed into all the different representations in the ensemble, and the fidelity in each representation is measured. This information is then used to determine the order in which the indices are searched. E-index has a surprising and very unintuitive property. One might expect that its performance for any given query would be bound by the performance of the best possible single-representation index. In fact, as we will theoretically and empirically demonstrate, the performance of E-index can be better than the best possible single-representation index. In other words, an ensemble of K representations can outperform the best of those K representations used by itself, not only on average, but on each individual query. 
Although the ideas presented in this paper apply to all types of multimedia data, for concreteness and brevity we will confine our discussion and experimental evaluation to time series data. Time series are an ubiquitous form of data, accounting for a large proportion of the data stored in financial, medical and scientific databases. 
The rest of the paper is organized as follows. In Section 2 we provide background on indexing time series data for similarity search. In Section 3 we formally introduce E-index. Section 4 contains experimental evaluation. In Section 5 we consider some related work and finally in Section 6 we offer conclusions and directions for future work. Given two time series Q = {ql...q,} and C = {Cl...Cn}, their Euclidean distance is defined as: Figure 3 shows the intuition behind the Euclidean distance. Figure 3. The intuition behind the Euclidean distance. The 
Euclidean distance can be visualized as the square root of the sum of the squared lengths of the gray lines. 
There are essentially two ways the data might be organized [10]:  X  Whole Matching. Here it assumed that all sequences to be reduction method to allow indexing. The technique was originally introduced for time series, but has been successfully extended to other data types [ 18]. An important result in [10] is that the authors proved that in order to guarantee no false dismissals, the distance measure in the index space must satisfy the following condition: This theorem is known as the lower bounding lemma or the contractive property. Given the lower bounding lernrna, and the ready availability of off-the-shelf multidimensional index structures, GEMINI requires just the following three steps.  X  Establish a distance metric from a domain expert (in this case  X  Produce a dimensionality reduction technique that reduces  X  Produce a distance measure defined on the N dimensional Table 1 contains an outline of the GEMINI indexing algorithm. All sequences in the dataset C are transformed by some dimensionality reduction technique and then indexed by the spatial access method of choice. The indexing tree represents the transformed sequences as points in N dimensional space. Each point contains a pointer to the corresponding original sequence on disk. Table 1. Outline of the GEMINI indexing building algorithm 
Algorithm Buildlndex(C,n); # C is the dataseq n is the window size for all objects in database 
Ci &lt;--Ci-Mean(Ci); //Optional: remove the mean of Ci ~,. &lt;---SomeTransformation(Ci); end; Note that each sequence has its mean subtracted before indexing. This has the effect of shifting the sequence in the y-axis such that its mean is zero, removing information about its offset. This step is included because for most applications the offset is irrelevant when computing similarity. The nearest neighbor algorithm outlined in Table 2. For generality, instead of initializing best-so-far to infinity inside the algorithm, we can initialize it outside the algorithm, then pass it in. This generalization is just to make the outline of our algorithm simpler (el. section 3). R. More importantly, if there is significant VIP in the set of dimensionality reduction techniques in R, then the average performance of our new approach will be significantly better than the best technique in R. Unfortunately, it is not possible to create a single dimensionality reduction technique that is as good as the best of a set of completing techniques on every query, however, we can do something almost equivalent. Instead of indexing the data with a single fixed representation, we can create an ensemble of indices. The intuition is that it is better to have many indices that specialize in representing certain kinds of data objects, than one general index that may represent some objects well but others poorly. When a query arrives, we convert it into each possible representation, and note the fidelity of each representation on that particular query. This information is then used to determine the order in which we search the indices. It may not be obvious to the reader why this results in faster query response. After all, the total number of objects in the ensemble of indices that must be either examined or pruned, is the same as the number of objects that must be examined or pruned in the single representation approach. We will provide a worked example later to help develop the readers intuition. Until then, we offer this simple explanation. The number of objects that may be pruned depends upon the quality of the eventual best-match. So it is always to our advantage to find the best-match sooner rather than later. If the single representation approach poorly represents some objects, they may appear to be closer to the query than the eventual best-match, and thus we will be forced to retrieve them (i.e these items are false hits). In contrast, in the ensemble index, we do not have to deal with items in the index space that are poorly represented (they are in one of the other indices), so we have fewer false hits. This allows us to find a good match more quickly. Of course, we still have to deal with those objects we have avoided, but we can deal with them later, in a representation in which they are more tightly approximated. In addition, when we deal with them, we will already have found a good match, thus it is more likely that we will be able to prune them. The algorithm used to build the E-index is described in Table 3. The inputs are R, the set of allowable representations (with [R[ = K) and C, the dataset containing m objects. The output is a set of K indices which taken together, index all m objects. Note the indices are not necessary all the same size (although we would prefer this). The distribution of sizes is determined by the data itself. Note we have no storage redundancy [12]. The algorithm used to search the E-index is described in Table 4. The input is the set of K indices and the query itself, and the output is the nearest neighbor match. The algorithm begins by transforming the query into each representation in the ensemble and recording its reconstruction error. This information is used to determine the order in which the indices are search. The indices are searched using the classic NearestNeighbor algorithm shown in Table 2, however there is one minor difference in how each search is initialized. The first index to be searched has its best-so-far argument initialized to infinity as usual. However, each subsequent invocation of NearestNeighbor inherits the best-so-far value from the previous invocation. There are some notable omissions. We could include the Discrete Cosine Transform (DCT), however, because both DCT and DFT are spectral techniques we would expect very little VIP between them, and therefore there is very little point in including both. Singular Value Decomposition has been successfully implemented for time series [ 15] and other types of multimedia data [14] however it requires a global view of the data. It is possible that this list could be greatly expanded. The present authors have tested several novel representations in recent years. We were disappointed to find that while these representations work very well for certain types of data, they could not compete with existing approaches overall. This, of course, is exactly the property we desire for representations in the ensemble. The E-index has a very surprising and desirable property. Suppose we compare E-Index to standalone versions of indices using just one of the various representations in the ensemble. For example, we could compare E-Index with R = {DWT, DFT}, to both the classic DWT-Index and DFT-index. Given a query, E-Index can potentially return the best match faster than the best of the standalone approaches! Using the example above, it is possible that for a particular query, DWT-Index might require 100 disk accesses, DFT-Index might do better with only 90 disk accesses and E-Index might require only 50 disk accesses. Paradoxically, E-index can be better than the sum of its parts (more accurately, will provide a simple worked example to help the reader gain intuition for it. Imagine we need to index a database with 7 objects {A, B, C, D, E, F, G}. We will trace the behavior of three approaches, DFT-Index, DWT-Index and E-Index with R = {DWT, DFT}, for a particular query Q. The estimated distances in the DFT space and DWT space, together with the true distances are shown in Table 6. Remember that the estimated distances can be obtained cheaply, by examining the index, but the true distances can only be obtained by expensive disk accesses, which we are trying to minimize. Table 6. Column 1: The true distance between the query Q, and the seven candidate objects in the database. Columns 2 and 3: The estimated distances in the reduced dimensionality space for DFT and DWT. This information is graphically depicted in Figure 5. A B C D E F G Dt~c(Q,A) is 5, which means that objects E, F and G can all be pruned, because the lower bound on their distance to Q exceeds 5. One by one objects B, C and D are retrieved from disk and found to be no better than the current best-so-far. In summary, the DFT-Index must make 4 disk accesses (to objects A, B, C and D). The E-Index builds two indices, a DWT-lndex containing A, B, C, D and a DFT-lndex containing E, F, G. The object A is retrieved from the disk, the true distance Dt~c(Q,A) is found to be 5. All the other items in the DWT-lndex can be pruned because their distance to Q exceeds 5. The DFT-Index is visited next, all items in this index have a minimum distance that exceeds the best-so-far, so all objects can be pruned. In summary, the ensemble approach must make just 1 disk access (to object A). This example is of course highly contrived, it is possible to produce an example in which E-Index does no better than its Note the value of P for any indexing approach depends only on the efficiency of the indexing algorithm and the data itself. It is completely independent of any implementation choices, including spatial access method, page size, computer language or hardware. A similar idea for evaluating indexing schemes appears in [ ! 8]. Table 7. The fraction P, of dataset that must be retrieved from disk using the five indexing approaches, for various dataset / query length combinations (averaged over 1,000 queries). The dimensionally of the index was 16 in every case. The results for the random walk dataset are not spectacular, but the ensembles do outperform the three competing approaches. In fact, we anticipated this result because there is very little VIP in this domain between the 3 representations under consideration (See Appendix A). We hoped for better results with the Space Shuttle data because there is high VIP between the 3 representations for this data. Our optimism was justified, the Ensemble approach significantly outperforms the single representation approaches. We further note that the empirical results validate the prediction that the more representations added to ensemble the greater the increase in performance. To the best of our knowledge, this paper is the first to suggest indexing data with ensembles of representations in order to improve the query response time. Others have suggested combining several representations in multimedia databases but their motivation was to provide more accurate matches in domains where the best match is more subjective [20]. The idea of using multiple representations to improve accuracy is also well understood in the text retrieval community [2]. Although they are using multiple representations to improve effectiveness and we are using multiple representations to improve efficiency, the underlying reason for the improvement in both cases is the same. Representations with globally similar performance may differ greatly on a local level. We exploit this fact by first querying the data in the representation to which it is best suited. Others exploit this fact by querying the data in all representations, and combining the results. There has been much research in exact similarly search (i.e. search 10, 15, 16, 19 21, 24, 26], and there is an even greater body of research in approximate search (i.e. search that may allow false dismissals) [22, 17]. For brevity we will not discuss this work here, instead we refer the reader to [15] or [16] which contains detailed discussions and extensive bibliographies. [9] Egecioglu, O., &amp; Ferhatosmanoglu, H. (2000). [10]Faloutsos, C., Ranganathan, M., &amp; Manolopoulos, Y. [11]Guttman, A. (1984). R-trees: A dynamic index structure for [12]Hellerstein, J. M., Papadimitriou, C. H., &amp; Koutsoupias, E. [ 13] Kahveci, T. &amp; Singh, A (2001). Variable length queries for [14]Kanth, K.V., Agrawal, D., &amp; Singh, A. (1998). [15] Keogh, E,. Chakrabarti, K,. Pazzani, M. &amp; Mehrotra (2001) [16] Keogh, E,. Chakrabarti, K,. Pazzani, M. &amp; Mehrotra (2001) [17]Keogh, E., &amp; Pazzani, M. (1998). An enhanced [18]Korn, F., Jagadish, H &amp; Faloutsos. C. (1997). Efficiently [19]Loh, W., Kim, S &amp; Whang, K. (2000). Index interpolation: [20]Minka, T &amp; Picard, R (1996) Interactive learning using a [21 ] Refiei, D. (1999). On similarity-based queries for time series [22]Wang, C. &amp; Wang, S. (2000). Supporting content-based [23] Welch. D. &amp; Quinn. P (1999). 124 [26]Yi, B,K., &amp; Faloutsos, C.(2000). Fast time sequence The results, shown in Figure AI, seem to confirm the hypotheses. The three representations show very little local variability in their ability to compress the Random Walk dataset, and this result is echoed the relatively small speedup achieved by E-index. In contrast the three representations show great local variability in their ability to compress the Space Shuttle dataset, and in this dataset we saw the greatest speedup achieved by E-index. In future work we hope to exploit this relationship to formalize the notion of VIP. ~ .... 1~ DFT : , , , "H;~AIR, _1~ ............... ' X  APCA 
