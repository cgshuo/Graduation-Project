
Automatic document classification (DC) is essential for the management of information and knowledge. This paper explores two practical issues in DC: (1) each document has its context of discussion, and (2) both the content and vocabulary of the document database is intrinsically evolving. The issues call for adaptive document classification (ADC) that adapts a DC system to the evolving contextual requirement of each document category, so that input documents may be classified based on their contexts of discussion. We present an incremental context mining technique to tackle the challenges of ADC. Theoretical analyses and empirical results show that, given a text hierarchy, the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category. Based on the contextual requirements mined by the system, higher-precision DC may be achieved with better efficiency. 
H.3.1 [Information Storage and Retrieval]: Content analysis and indexing -Indexing methods. Algorithms 
Context text mining, incremental classification 
Automatic document classification (DC) aims to map documents to suitable categories. It facilitates the storing, dissemination, elicitation, and sharing of information and knowledge, which are represented in document form. As the spaces of information and knowledge are often ever changing in the real world [13], there could be lots of documents to be classified and stored into the document database at any time. Automatic DC is thus a must for cost-effective management of the documents. 
However, as the document database evolves, both its content and vocabulary evolve as well. This phenomenon calls for adaptive document classification (ADC), which aims to promote the efficiency and precision of DC by incremental and efficient adaptation to the evolution. ADC brings several challenges to text mining. The first challenge is context mining. Previous studies have identified two essential forms of contexts: neighbor terms surrounding the keywords [8] and content-beating terms indicating the context of usage/discussion of the document [ 12]. Previous DC techniques mainly focused on the first form (e.g. word strings [2] and linguistic phrases [ 14]). The mining of the content-beating and usage-indicative terms for each category deserves exploration. 
These terms may serve as the contextual requirement (CR) of the category. Unlike neighbor terms, which are extracted based on their locality in a single document, CR terms should be mined by analyzing multiple documents from multiple categories. 
The second challenge is incremental mining, due to the fact that both the content and the vocabulary of the database may evolve when new documents are added. As the database evolves, CR of each category evolves as well. Re-triggering the whole mining process for each new document is obviously computationally impractical. It requires lots of I/O and computations. Typical previous DC techniques included symbolic rule induction [1], regression [19], the Rocchio's linear classifiers [15], the k-Nearest 
Neighbor (kNN) method [6], the Bayesian independence classifier [7], the support vector machine method [3], and the Pereeptron-based method [11]. There were also studies relying on a given text hierarchy to cluster documents [4, 16] and classify documents [3, 5, 9, 10]. They often preset a feature set (vocabulary) on which their classifiers were built (a feature often corresponded to a term or a phrase). Obviously, since the vocabulary may evolve in ADC, no feature set may be presumed. Even the feature set may "evolve" by covering all features currently seen in the documents (e.g. [2]), inappropriate features may introduce the problems of inefficiency [18] and errors (over-fitting) [10] in DC. Better performance (in terms of efficiency and precision of DC) is often achieved by semi-automatic and/or trail-and-error feature selection [11]. The number of features selected was thus often treated as an experimental issue (e.g. [9, 10, 18]). The construction of an "optimum" feature set (if any) thus consists of a series of tuning processes, which may be re-triggered by the addition of a new document. 
The third challenge is efficient DC, which should be supported by the result of mining. Obviously, classification is often triggered more frequently than mining. Its efficiency is thus essential to the management of information and knowledge. That is, by incremental and efficient mining of CR for each category, precision of DC should be promoted with good efficiency. Simultaneously tackling the challenges is essential to text mining. 
It also provides practical contributions to document classification in the ever-changing world. 
In this paper, an incremental mining technique ACclassifier (Adaptive _Context-based Classifier) is developed to tackle the challenges of ADC. To empirically evaluate ACclassifier, experiments on a real-world document database were conducted (ref. section 3). Empirical results show that ACclassifier may achieve efficient and incremental mining of contextual requirement for each document category, which may serve as the basis for supporting efficient and high-precision DC. 
ACclassifier consists of two components: an incremental context miner and a document classifier. Both components work on a given text hierarchy in which a node corresponds to a document category. 
Given a training document together with its category label (a leaf node in the tree), the incremental context miner updates the CR of each related category. 
Input: (I) A text hierarchy T, and 
Effect: Update the contextual requirements (CR) of 
Begin 
End. The algorithm of the miner is defined in Table 1. CR of a category c is a set, which is initially empty. Each element in CR of a the strength of w serving as a context word for the documents under c (i.e. leaf categories that are descendants of c). More specially, a word w may have a higher strength in c if it may be used to significantly distinguish c from sibling categories of c. For example, suppose category "computer-based information system" (CBIS) has two children categories: "decision support systems" (DSS) and "management information systems" (MIS). The word "computer" should have a lower strength in categories MIS and 
DSS (since it occurs frequently in both categories), but a higher strength in category CBIS (if sibling categories of CBIS are not about computer systems). That is, "computer" may be a good CR term for the documents under CBIS. The pair of "computer" and its strength should thus be included in CR of CBIS. 
To determine whether a word may be a CR term, the miner employs a threshold 8 (Step I). A word with its associated strength is qualified to be included in a CR only if it occurs at least 8 (e.g. five) times in the training document. Thus 8 actually acts as a simple threshold to filter out irrelevant words. Similar thresholds were often employed in previous studies as well [5, 9, 10, I I]. 
To estimate the strength s of a word w in distinguishing category c from siblings of c (Step 2.2.1 and 2.2.2.1), the miner employs a modified TFIDF (term frequency x inverse document frequency) technique: Strength(w, c) = P(w[c) * (Be / ZiP(w[ci)), the number of siblings of c plus one (i.e. including c). P(wlc) is estimated by computing the probability of w occurring in the documents under c (i.e. descendant leaf categories under c). The summation ofP(wlci ) is conducted over c and its siblings. Thus, for example, w may get a higher strength if it occurs frequently in c (i.e. P(wlc) is high), but infrequently (on average) in siblings of c (i.e. Bc / ZiP(wlcj) is high). The maximum strength is B~. Given a training document, the update of CR is conducted on related categories only (in a bottom-up manner). The related categories include the category c, its sibling categories, its antecedent categories, and siblings of its antecedent categories (ref. steps 2.2.1, 2.2.2, and 2.3). To update the strength of w in the related categories (ref. steps 2.2.1 and 2.2.2.1), the context miner only needs to keep track of the related data for computing P(wlci). Time complexity of the mining process may be measured by estimating the normal range of the number of the features updated. When a document is added into a category c, P(wlc) for each feature w in c changes. Therefore, the strengths of all features in CR of c, including the existing ones and the new ones, need to be updated. The update of P(wlc) changes the IDF component (i.e. B~ / ZiP(wlci)) of the strength of w. Therefore, the strengths of these features in each sibling of c need to be updated as well. Similarly, when the update process proceeds to the father of c (ref. Step 2.3), the strengths of all features in the father category (and the father's siblings) need to be updated. That is, only a small subset of CRs is updated for every training document. No training documents are reprocessed, no feature set is predefined, and no time-consuming trail-and-error feature set tuning is conducted. Moreover, the miner achieves hierarchical context annotation and weighting for each category in the text hierarchy, which are essential for high-precision DC. For a non-context-indicative term (feature), its strength is low in lower-level categories (i.e. it's rare or common and even a stop word). On the other hand, for a context-indicative term, its strength is high in higher-level tolerance by consolidating the DOA values from different nodes without invoking an exponential number of complicated classification tasks. 
To further evaluate ACclassifier, we conduct experiments on a real-world document database. 
Experimental data was extracted from the "science" category, the "computers and Intemet" category, and the "society and culture" category of Yahoo! (http://www.yahoo.com). There were totally 83 categories extracted, among which there were 25 non-leaf categories (and hence 58 leaf categories). There were 1997 documents in the collection. The average length of the documents was about 6K. Among the 1997 documents, 1838 documents were extracted as training documents and 159 documents as testing documents. The testing documents were extracted from each category. The number of testing documents extracted from a category was proportional to the total number of documents in that category. The testing documents could thus comprehensively represent the contents of the information space. Their distribution could also reflect the common distribution of the documents entered for classification. 
To simulate common evolution of most document databases, we separated the training documents into two sets: 1100 documents for initial training and 738 documents for evolutionary training. 
The documents for initial training were comprehensively sampled from each category as well. They were used to train all systems (including ACclassifier and the baseline systems to be described later). After initial training, the documents for evolutionary training were entered one by one so that the contributions of adaptation could be evaluated. We measured both the efficiency and the precision of all the systems. 
As noted above, ACclassifier has two parameters: 8 (for filtering out non-important words in training) and minSupport out non-representative features in testing). In the experiment, 8 was set to 5. As to minSupport, since there were often thousands of words in a document and 8 was 5, minSupport was set to 0.001 (5/5000). As noted above, the setting does not need be changed as the document database evolves (unlike the feature set size, which is dependent to the current collection of documents). 
As to the baseline systems for the experiment, we did not find any previous DC techniques dedicated to all the challenges of ADC. 
To facilitate performance comparison between ACclassifier and most previous techniques, we set up two baselines using the k-
Nearest Neighbor technique (kNN) and the Naive Bayes technique (NB), which are popular techniques in DC. Both of them have been employed and evaluated with respect to various techniques, including non-hierarchical DC [6, 7, 17] and hierarchical DC [5, 9]. 
They treated all leaf categories as candidate categories. Under a public document database (i.e. the database from Yahno!), the performance comparisons between ACclassifier'and the baselines may facilitate cross-evaluation for measuring the contributions of incremental context mining. 
Given a document d to be classified, kNN estimated the similarity between d and each training document. The most similar k training documents (i.e. neighbors) were allowed to use their degrees of similarity to "vote". The final output category was simply the one that got the highest accumulated degree of similarity. In the experiment, the similarity estimation was based on the well-known vector space model (VSM) in which each document was represented as a vector (using the predefined features as the dimensions for the vector space), and the similarity between two documents was measured by the cosine of the angle between them (i.e. the cosine similarity). We tested two versions of kNN by setting k (i.e. the number of neighbors) to be 5 (kNN-5) and 10 (kNN-10), respectively. On the other hand, NB pre-estimated the conditional probability P(wilCj) for every selected feature wi and category Cj (with standard Laplace smoothing to avoid probabilities of zero). The "similarity" between a category and an input document d was based on the product of the conditional probabilities of the features (in the category) that occurred in d. 
The NB classifier simply output the most similar category (for more details, the reader is referred to [6, 7, 9, 17]). 
Therefore, another important reason of setting kNN and NB as the baselines was that they could represent two typical branches of the techniques: kNN can "adapt," while NB cannot, kNN is basically a memory-based reasoning method. It classifies the input document by finding the most similar training documents (neighbors). 
Therefore, as more training documents were entered, the capability of kNN evolved. On the other hand, NB employs a fixed training set to build a table of conditional probabilities. Its capability could not evolve as new documents were entered. 
Both kNN and NB required a fixed feature set, which was built from the 1100 documents for initial training. The selection of the features was based the strength of each feature, which was estimated by the g 2 weighting technique. The technique has been shown to be more promising than others [18]. As noted above, there is no perfect way to determine the size of the feature set. 
Therefore, for each baseline, we tried three different sizes: 3500 features (kNN-3500 and NB-3fi00), 5000 features (kNN-5000 and 
NB-5000), and all features (kNN-AII and NB-AII). When all features were selected, there were 6715 features. The reason of setting such feature sets was that they have a similar size difference (1500 = 5000-3500 ~ 6715-5000). We aimed to measure the performances of the baseline systems under different feature set sizes. The result facilitated the investigation of the contributions of context mining in reducing the errors incurred by inappropriate features. 
Figure 1 shows the performances of all baselines under different feature set sizes. NB outperformed kNN (with k=5) in classification precision, although kNN could improve its performance through evolution. Moreover, when considering the final performances of the baselines, kNN-5000 outperformed kNN-3500 (63 vs. 57), while NB-5000 could not outperform NB-3500 (97 vs. 97). Actually, NB-AII did not outperform NB-5000 and NB-3500 either. This confirmed that, using more features did not necessarily lead to a better performance. The setting of the feature set was actually a trial-and-error process, which depended on the technique employed and the documents collected. 
Based on the result, the baseline systems were thus allowed to use 5000 features in their feature set. For kNN, we tested different settings for the parameter k (i.e. kNN-5 and kNN-10). The results were shown in Figure 2. ACclassifier achieved the best classification precision with adaptation capability. When training documents were entered. This was because the number of new features learned grew slower after 1400 training documents were entered, reducing the growth of the load of ACclassifier in updating the strengths of features. It is also interesting to compare the systems' efficiency in classifying documents (i.e. excluding training time). Figure 5 shows the cumulative classification time spent by ACclassifier and NB, which was much more efficient than kNN. The result showed ACclassifier was 3.3 times faster than NB (i.e. 160 seconds vs. 528 seconds) in DC. It confirms the time-complexity analysis in section 2.2.2. As classification is often invoked more frequently than training, the high efficiency contributed by ACclassifier is of particular significance to DC in practice. Together with the above results on precision, the results show that incremental context mining may be efficient enough to support adaptive and higher-precision DC in better efficiency. This paper presents an efficient incremental context mining technique ACclassifier, and demonstrates its contributions to adaptive DC. Its contributions lie on (1) efficient mining of the contextual requirements for high-precision DC, (2) incremental mining without reprocessing previous documents, (3) evolutionary maintenance of the feature set, and (4) efficient and fanlt-tolerant hierarchical DC. The contributions are essential for those applications that base their operation on efficient classification of information and knowledge documents whose content and vocabulary may evolve over time (e.g. management of inforrnation and knowledge in the ever-changing world). We conjecture that misclassifieations caused by ACclassifier could mainly be attributed to the impreciseness of the contexts of the documents, although there is no perfect way to judge whether the context of a document has been precisely specified. Some of the errors may even be due to incorrect classifications of training documents (this is also treated as the "noise" of the real-world training data [9]). We are thus further analyzing the errors and seeking the possibility of refining the technique. Typical refinements include the active sampling of training documents, the detection of errors in training documents, the tolerance of the errors, and the incremental recovery from the errors. This research was supported in part by the National Science Council of the Republic of China under the grants NSC 88-2213-E-216-003 and NSC 89-2218-E-216-008. [1] C. Apte, F. Damerau, and S. M. Weiss (1994), Automated Learning of Decision Rules for Text Categorization, ACM 
Transactions on Information Systems, Vol. 12, No. 3. [2] W. W. Cohen and Y. Singer (1996), Context-Sensitive Learning Methods for Text Categorization, Prec. of ACM 
SIGIR'96. [3] S. Dumais and H. Chen (2000), Hierarchical Classification of 
Web Content, Prec. of ACM SIGIR 2000. [4] M. Iwayama and T. Tokunaga (1995), Cluster-Based Text 
Categorization: A Comparison of Category Search Strategies, Prec. of ACM SIGIR'95. [5] D. Keller and M. Sahami (1997), Hierarchically Classifying 
Documents Using Very Few Words, Prec. oflCML'97. [6] W. Lain and C. Y. He (1998), Using A Generalized Instance 
Set for Automatic Text Categorization, Prec. of ACMSIGIR'98. [7] L. S. Larkey and W. B. Croft (1996), Combining Classifiers in 
Text Categorization, Prec. of A CM SIGIR'96. [8] S. Lawrence and C. L. Giles (1998), Context and page Analysis for Improved Web Search, IEEE Internet Computing, 
Vol. 2, No. 4, pp. 38-46. [9] A. MeCallum, R. Rosenfeld, T. Mitchell, A. Y. Ng (1998), Improving Text Classification by Shrinkage in a Hierarchy of 
Classes, Prec. of lCML '98. [10] D. Mladenic and M. Grobelnik (1998), Feature Selection for 
Classification based on Text Hierarchy, Prec. of the Conference on Automated Learning and Discovery. [11] H. T. Ng, W. B. Gob, and K. L. Low (1997), Feature Selection, Perceptron Learning, and a Usability Case Study for 
Text Categorization, Prec. of SIGIR. [12] H. M. Nicholas and P. J. Clarkson (2000), Web-Based Knowledge Management for Distributed Design, IEEE Intelligent 
Systems, pp. 40-47. [13] I. Nonaka (1994), A Dynamic Theory of Organizational 
Knowledge Creation, Organization Science, Vol.5, No. 1. [ 14] E. Riloff and W. Lehnert (1994), Information Extraction as a 
Basis for High-Precision Text Classification, ACM Transactions on Information Systems, Vol. 12, No. 3. [15] R. E. Shcapire, Y. Singer, and A. Singhal (1998), Boosting and Rocchio Applied to Text Filtering, Prec. of ACMSIGIR '98. [16] P. Willett (1988), Recent Trends in Hierarchical Document Clustering: A Critical Review, Information Processing &amp; 
Management, Vol. 24, No. 5. [17] Y. Yang and X. Lin (1999), A Re-examination of Text 
Categorization Methods, Prec. of ACM SIGIR'99. [18] Y. Yang and J. O. Pedersen (1997), A Comparative Study on 
Feature Selection in Text Categorization, Proc. oflCML "97. [19] Y. Yang and C. G. Chute (1994), An Example-Based Mapping Method for Text Categorization and Retrieval, A CM 
Transactions on Information Systems, Vol. 12, No. 3. 
