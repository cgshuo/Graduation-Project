 An adaptive semi-supervised ensemble method, ASSEM-
BLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning "pseudo-classes" to the unla-beled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudo-labeled data. Mathematically, this intuitive algorithm cor-responds to maximizing the classification margin in hypoth-esis space as measured on both the labeled and unlabeled data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base clas-sifter. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method. Boosting, Semi-Supervised Learning, Ensemble Learning, 
Classification For many practical classification applications in areas such as image analysis, drug discovery, and web pages analy-sis, labeled data (with known class labels) can be in short supply but unlabeled data (with unknown class labels) is more readily available. Semi-supervised learning deals with methods for exploiting the unlabeled data in addition to the labeled data to improve performance on the classifica-tion task. Semi-supervised learning has been the topic of four different Neural Information Processing Workshops [5, http://www.rpl.edu/,-~bennek permission and/or a fee. SIGKDD '02 Edmonton, Alberta CA 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. boosted using labeled and unlabeled data. based ensemble algorithms for both two-class and multi-class problems. 
Here, Ft represents the ensemble classifier after the tth com-ponent classifier has been added. T is the maximum num-ber of classifiers we propose to include in our ensemble. 
Z:(Ft,-VC(Ft)) is a weak learning function applied to our existing ensemble classifier that produces a new possible ner product of the new base classifier with the gradient of the cost function. If this inner product is negative, adding ft+l cannot decrease the cost function, and the algorithm terminates without adding the new component classifier. To add the classifier to the ensemble, a weighting factor Wt+l is selected via an appropriate linesearch that guarantees de-crease in the cost function. But adding in unlabeled points our cost function now be-
Note that this function is not differentiable since the abso-lute value function is not differentiable. By introduction of a pseudo-class yi for the the unlabeled data, the subgradient 1 of C is 
VC(F)(z) = i = g+ 1... X + u where g and u are the number of labeled and unlabeled data class for unlabeled), and Me(z) is the derivative of the mar-gin cost function with respect to z. Then we get 
We can safely assume the margin cost function is monoton-ically decreasing, thus -Me(z) is always positive. Maximiz-
Yi=f(zl) yiCf(=i) where for j e unlabeled, UJ = s~gn(F(x:)). 
Equivalently we can define a vector D of misclassification by constructing the minimum cost classifier. Let and define 
The base learner then can construct the base classifier f by minimizing (or approximately minimizing) the weighted error ~ i : f(xl) ~ yiD(i) . Note that the pseudo-classes are used for the unlabeled data, thus any cost sensitive base learning algorithm can be used. 
The resulting Adaptive Semi-Supervised 'ensEMBLE al-gorithm is the following: 
ALGORITHM 2.2. ASSEMBLE 1To simplify presentation, we use V to also represent the subgradient. Strictly speaking we are now using a sub-gradient method instead of a gradient descent method. 1. Select Do(i) 2. yi := [1,0,-1] for i E unlabeled. 3. Let Fo(x) := 0 4. for t := O toT do 5. Let ft+l :=I~(S,Y, Dt) 6. if ~ Dt(i)ylft+l(X,) &lt; 0 then 7. return Ft 8. end if 9. Choose Wt+l 10. Let Ft+l := Ft + Wt+lft+l 11. Let y~ = sign(Ft+l(X~)) if i E unlabeled 13. end for 14. return FT+i 
L(S, Y, Dr) is our base learning algorithm that is applied to the distribution of data points S with current labels Y where the data points are weighted according to the current distribution Dt. We assume that this base learning algo-rithm Z: will optimize the misclassification costs, ignoring gorithm that results depends on the choice of cost function. For example if assume we choose the cost function used in 
AdaBoost, M(z) = e -z, then step 12 in the algorithm be-comes 
Let Dt+l(i) :=  X tie-~Ft+l(=~) ~j o~je_~Ft+l(=D for all i (12) 
More problematic is how to determine the step-size wt+l ~iElabeled aiM(y,(Ft(x,) + wt+lft(x,))) (13) Unfortunately, no closed form solution exists for Wt+l. 
An intuitive approach would be to just use the step--sizes It is easy to show that mathematically, this leads to a de-crease whenever a decrease is possible. For any unlabeled is correctly predicted by the new base classifier in the ensem-ble (i.e., sign(ft(x,)) = yj), the pseudo-class cost function is accurate since 
U(lFt(xi) + wt++lft(x,)l) = i(yi(Et(x,) + wt+xft(x,)) ) If the unlabeled point is incorrectly classified (i.e., if y, Since M is a monotonically decreasing function, 
M(y,(Ft(x,) + Wt+lft(x,))) _&gt; M(l(Ft(x,)  X  wt+~A(x,)l) So the the pseudo-class cost function provides an upper bound on the true cost function. So if we choose wt+l to strictly decrease the pseudo-cost function then it must strictly decrease the true cost function. Furthermore it is always possible to strictly decrease the pseudo-cost function if the problem is not optimal. If o~, is a constant for every data point, then we can just use the step-sizes developed for supervised ensemble methods for various costs (see for example [16]). If c~i is variable, then minor modifications may be required. It is possible to construct many variations of the basic ASSEMBLE algorithm. For example, to employ ASSEM-BLE for multi-class problems we can extend the mechanism by assigning pseudo-classes for unlabeled points based on the weighted vote of the previous classifiers in the ensemble. This approach works well in our letter-recognition results (see Section 4.2). 
As another example, we could employ other loss functions and associated step-sizes defined for gradient based classi-fication algorithms such as exponential loss and logistic re-gression. All one has to do is add a step to estimate the pseudo-costs in each iteration. In the next sections we ex-amine how AdaBoost has been adapted to semi-supervised learning. 
The following variation of the ASSEMBLE algorithm was used for the semi-supervised method competition at the NIPS'2001 workshop, Competition: Unlabeled Data for Su-pervised Learning, organized by Stefan C. Kremer and Deb-orah A. Stacey. ASSEMBLE (previously known as Semi-Supervised Boosting) was the best among 34 algorithms and over 100 participants utilizing unlabeled data [14]. 
ASSEMBLE was used to assimilate unlabeled data into a multiclass version of AdaBoost. AdaBoost was adopted to mnlticlass using a similar approach to [12]. Specifically, -1 otherwise. This makes AdaBoost increase the weight of a misclassified point and decrease it otherwise. The pre-dicted class is the one which achieves a majority in a vote weighted by the ensemble weights. As in the two-class case, the pseudo-classes of the unlabeled data are their predicted Here is the Contest version of ASSEMBLE: 
ALGORITHM 3.1. ASSEMBLE.AdaBoost(L, U, T,a,/3) 1. Let ~ := ILl and u := [U I 2. LetDl(i):= (1-~)/u if lEe 3. Let yi := c where c is the class of the nearest neighbor point in L for i E U. 4. Let f~ :=  X (L + U, Y, D1) 5. for t :=l to T do 6. Let ~i := ft(xl), i= 1...~e+u 7. e=E, Dt[yi X .~,], i=l... X +u 8. If e &gt; 0.5 then Stop 9. wt = 0.5 * log(~) 10. Let Ft ::= Ft-1 + wtft 11. Let yi = Ft(xi) if i E U 12. Let Dt+~ as in AdaBoost (equation (12) with c~) 13. S = Sample(L + U, l, Dr+l) 14. ft+a =  X (S,Y, Dt+i) 15. end for 16. return FT+i 
The ASSEMBLE.AdaBoost algorithm practically inherits 
For the contest, ASSEMBLE.AdaBoost was implemented (U) and test data were provided in this competition. Ac-curacy results on the test data and the improvements com-pared to AdaBoost are reported in Table 1. We used the same number of iterations for both AdaBoost and ASSEM-
BLE.AdaBoost in general for the competition. The number of iterations (T) were determined based on the training error convergence in the supervised AdaBoost runs. The parame-algorithm produced double-digit improvements relative to 
AdaBoost. lined above, we performed additional experiments on a num-ber of datasets drawn from the literature. To test the effec-tiveness of ASSEMBLE across different classifier method-ologies, we performed experiments using decision trees and neural networks, methods that have proven very effective in previous experiments with ensembles [1, 18]. on three benchmark datasets obtained from boosting liter-ature [19]. The benchmark datasets were used without any data transformation. Each dataset consists of 100 random instances of training and test dataset pairs. The detailed information about datasets can be found in [19]. To con-form with previous approaches in empirical studies of semi-training data as unlabeled data and then evaluated the al-gorithm on test data. Training data were sampled 10 times at three different levels to form labeled and unlabeled data. 
Specifically, the size of the combined labeled and unlabeled data was held constant, but the proportion of data treated as unlabeled was varied from 20 to 60 percent. Thus 10 predictions are made for each test point. was identical to the one used in the contest described in 
Section 3. The code was implemented in SPLUS to allow the RPART [21] decision tree algorithm to be used as the base learner. We set the maximum depth of the decision trees generated by RPART at 4. Information gain was used as the splitting criterion. RPART can be used for regression problems but we examined only the classification case in this paper. imum of 25. Again we set the parameters fl to 0.9 and c~i to 1 in our experiments. The results from experiments are reported in Table 2. We report the mean error rates of AS-
SEMBLE.Adaboost and Adaboost on 1000 different runs for each benchmark dataset. Unlabeled data were sampled as 60%, 40% and 20% of the total data used for training. boosting is better or comparable to AdaBoost despite the fact that in each trial ASSEMBLE.Adaboost received much less labeled data. mance of ASSEMBLE on neural networks. The ASSEM-
BLE variant for the neural network experiments was slightly different than that used for the NIPS Contest. The algo-rithm differs in that step 13 from ASSEMBLE.AdaBoost is are not incorporated into the first classifier). These changes were used largely because of the significant cost for nearest neighbor labeling for the larger datasets we employed in our multiple tests. We also employ a parameter a~ that is dif-ferent for the two sets of data, using a term that weights the margins of unlabeled points by less (fractions from 0.4 to 1.0 were employed) to prevent the networks from overly focusing on the unlabeled points. layer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learn-ing rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are breast-cancer-wisconsin, pima-indians diabetes, and letter-recognition drawn from the UCI Ma-chine Learning repository [3]. The number of units in the hidden layer for the datasets was 5 for the breast-cancer and diabetes datasets and 40 in the letter-recognition dataset. 
The number of training epochs was set to 20 for breast-in previous ensemble experiments (see [18]). We explored a number of values ranging from 0.4 to 1.0 for the parameter used to weight the unlabeled data points, though there was very little difference among the values. The results we show use a value of 0.4 for this parameter. idation experiments for each result. Unlabeled data was obtained by randomly marking a percentage of the data (in this case, 10, 25, and 50 percent) of the data as unlabeled points. Each semi-supervised and regular AdaBoost exper-iment was run on the same set of points (though the unla-beled points from each dataset are left out when applying 
AdaBoost). Each method was allowed to produce up to 30 members of the ensemble, although the breast-cancer and 5.09 (1.17) 4.91 (0.62) 4.46 (0.41) 25.95 (1.44) 25.81 (1.25) 25.45 (1.29) 9.63 (0.30) 6.87 (0.21) diabetes converged quickly and therefore stopped at many fewer classifiers. 
Table 3 shows the results using the neural networks de-scribed above as component classifiers for this variation of the ASSEMBLE algorithm. Note that in every case, AS-SEMBLE produced a small but measurable gain in the over-all performance. 
As a further test of the effectiveness of our algorithm we performed several experiments on the largest of these datasets (letter-recognition) to assess how useful unlabeled data would be in overcoming limitations imposed by the bias hidden layer (we performed experiments using, 5, 10, and 20 hidden units). Since this problem has 26 output classes, the resulting neural networks not only have to perform general-ization, but also an encoding/decoding problem. Thus, we would expect that additional data might help to overcome these problems. 
Figure 1 shows the resulting error rate graphed as a func-tion of the number of classifiers. Note that not only does ASSEMBLE outperform standard AdaBoost but that the resulting gains generally happen earlier, and it is only with a large number of classifiers in the ensemble that AdaBoost begins to catch up. 
In this paper we introduce a novel semi-supervised learn-ing technique ASSEMBLE (for Adaptive Semi-Supervised enSEMBLE) that is able to solve semi-supervised learning problems (i.e., problems where some of the data is not la-beled with a class). ASSEMBLE can be used to make any cost sensitive classification method semi-supervised by in-corporating it into a semi-supervised boosting algorithm. The key to making ASSEMBLE work is the introduction of pseudo-classes for the unlabeled data. We demonstrate that an appropriate margin can be derived by attaching pseudo-class yi to an unlabeled data point that simply reflects the majority class picked for that point by the current ensem-boosting algorithms that can be applied in semi-supervised learning situations. By incorporating pseudo-classes, exist-ing ensemble algorithms can be readily adapted to semi-supervised learning. In this work, we focus on ASSEM-BLE.AdaBoost, a semi-supervised version of the popular AdaBoost algorithm based on exponential margin cost func-tions. 
ASSEMBLE performs extremely well in empirical tests on both two-class and multi-class problems. Using decision gorithms in the NIPS 2001 competition on semi-supervised datasets. In further empirical tests using neural networks and decision trees on datasets where some of the data points were artificially marked as unlabeled, ASSEMBLE consis-tently performs as well as or better than AdaBoost. Overall, the ASSEMBLE algorithm appears to be a robust method for combining unlabeled data with labeled data. 
We plan to apply ASSEMBLE to larger datasets in or-der to determixm how well the algorithm scales for larger problems. ASSEMBLE should be readily adaptable to scal-able boosting algorithms such as in [20]. An interesting open problem is how to exploit unlabeled data in regression problems. For classification ASSEMBLE favors ensembles that vote consistently on the unlabeled data. The analogous strategy for regression would be to favor ensembles that ex-hibit low variance on the unlabeled data. But we leave these issues to future work. and 0.5) for AdaBoost (superv) and ASSEMBLE (semi_sup). 
