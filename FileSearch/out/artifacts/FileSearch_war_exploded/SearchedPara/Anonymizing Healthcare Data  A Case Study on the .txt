 Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients X  privacy. In this paper, we study the privacy concerns of the blood transfusion information-sharing system between the Hong Kong Red Cross Blood Transfusion Service (BTS) and public hospitals, and identify the major challenges that make traditional data anonymization methods not applica-ble. Furthermore, we propose a new privacy model called LKC -privacy , together with an anonymization algorithm, to meet the privacy and information requirements in this BTS case. Experiments on the real-life data demonstrate that our anonymization algorithm can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.
 H.2.7 [ Database Administration ]: [Security, integrity, and protection]; H.2.8 [ Database Applications ]: [Data mining] Algorithms, Performance, Security Privacy, anonymity, classification, healthcare
Gaining access to high-quality health data is a vital re-quirement to informed decision making for medical practi-tioners and pharmaceutical researchers. Driven by mutual benefits and regulations, there is a demand for healthcare institutes to share patient data with various parties for re-search purposes. However, health data in its raw form often Figure 1: Data flow in Hong Kong Red Cross Blood Transfusion Service (BTS) contains sensitive information about individuals, and pub-lishing such data will violate their privacy. The current prac-tice in data sharing primarily relies on policies and guidelines on the types of data that can be shared and agreements on the use of shared data. This approach alone may lead to excessive data distortion or insufficient protection. In this paper, we study the challenges in a real-life information-sharing scenario in the Hong Kong Red Cross Blood Trans-fusion Service (BTS) and propose a new privacy model, to-gether with a data anonymization algorithm, to effectively preserve individuals X  privacy and meet the information re-quirements specified by the BTS.

Figure 1 illustrates the data flow in the BTS. After col-lecting and examining the blood collected from donors, the BTS distributes the blood to different public hospitals. The hospitals collect and maintain the health records of their patients and transfuse the blood to the patients if neces-sary. The blood transfusion information, such as the pa-tient data, type of surgery, names of medical practitioners in charge, and reason for transfusion, is clearly documented and is stored in the database owned by each individual hos-pital. Periodically, the public hospitals are required to sub-mit the blood usage data, together with the patient-specific surgery data, to the BTS for the purpose of data analysis. This BTS case illustrates a typical dilemma in information sharing and privacy protection faced by many health insti-tutes. For example, licensed hospitals in California are also required to submit specific demographic data on every dis-charged patient [5]. Our proposed solution, designed for the BTS case, will also benefit other health institutes that face similar challenges in information sharing. We summarize the concerns and challenges of the BTS case as follows.
Privacy concern: Giving the BTS access to blood trans-fusion data for data analysis is clearly legitimate. However, it raises some concerns on patients X  privacy. The patients are willing to submit their data to a hospital because they consider the hospital to be a trustworthy entity. Yet, the trust in the hospital may not necessarily be transitive to a third party. Many agencies and institutes consider that the released data is privacy-preserved if explicit identifying information, such as name, social security number, address, and telephone number, is removed. However, substantial re-search has shown that simply removing explicit identifying information is insufficient for privacy protection. Sweeney [20] showed that an individual can be re-identified by simply matching other attributes, called quasi-identifiers ( QID ), such as gender, date of birth, and postal code. Below, we illustrate the privacy threats by a simplified BTS example.
Example 1. Consider the raw patient data in Table 1, where each record represents a surgery case with the patient-specific information. Job , Sex , and Age are quasi-identifying attributes. The hospital wants to release Table 1 to the BTS for the purpose of classification analysis on the class attribute, Transfuse , which has two values, Y and N , indi-cating whether or not the patient has received blood transfu-sion. Without a loss of generality, we assume that the only sensitive value in Surgery is Transgender . The hospital expresses concern on two types of privacy threats:
Identity linkage : If a record in the table is so specific that not many patients match it, releasing the data may lead to linking the patient X  X  record and, therefore, her received surgery. Suppose that the adversary knows that the target patient is a Mover and his age is 34. Hence, record #3, together with his sensitive value ( Transgender in this case), can be uniquely identified since he is the only Mover who is 34 years old in the raw data.

Attribute linkage : If a sensitive value occurs frequently together with some QID attributes, then the sensitive in-formation can be inferred from such attributes even though the exact record of the patient cannot be identified. Suppose the adversary knows that the patient is a male of age 34. In such case, even though there exist two such records (#1 and #3), the adversary can infer that the patient has received a Transgender surgery with 100% confidence since both the records contain Transgender .
 High-dimensionality: Many privacy models, such as K -anonymity [18][20] and its extensions [14][23], have been proposed to thwart privacy threats caused by identity and attribute linkages in the context of relational databases. The usual approach is to generalize the records into equivalence groups so that each group contains at least K records with respect to some QID attributes, and the sensitive values in each QID group are diversified enough to disorient confident inferences. However, [1] has shown that when the number of QID attributes is large, that is, when the dimensional-ity of data is high, most of the data have to be suppressed in order to achieve K -anonymity. Our experiments confirm this curse of high-dimensionality on K -anonymity [1]. Ap-plying K -anonymity on the high-dimensional patient data would significantly degrade the data quality. In order to overcome this bottleneck, we exploit one of the limitations of the adversary: in real-life privacy attacks, it is very dif-ficult for an adversary to acquire all the information of a target patient because it requires non-trivial effort to gather each piece of prior knowledge from so many possible values. Thus, it is reasonable to assume that the adversary X  X  prior knowledge is bounded by at most L values of the QID at-tributes of the patient. Based on this assumption, we define a new privacy model called LKC -privacy for anonymizing high-dimensional data.

The general intuition of LKC -privacy is to ensure that every combination of values in QID j  X  QID with maxi-mum length L in the data table T is shared by at least K records, and the confidence of inferring any sensitive values in S is not greater than C , where L , K , C are thresholds and S is a set of sensitive values specified by the data holder (the hospital). LKC -privacy bounds the probability of a successful identity linkage to be  X  1 /K and the probability of a successful attribute linkage to be  X  C , provided that the adversary X  X  prior knowledge does not exceed L . Ta-ble 2 shows an example of an anonymous table that satisfies (2 , 2 , 50%) -privacy by generalizing all the values from Ta-ble 1 according to the taxonomies in Figure 2. (Ignore the dashed curve for now.) Every possible value of QID j with maximum length 2 in Table 2 (namely, QID 1 , QID 2 , and QID 3 in Figure 2) is shared by at least 2 records, and the confidence of inferring the sensitive value Transgender is not greater than 50%. In contrast, enforcing traditional 2-anonymity will require further generalization. For example, in order to make  X  Professional, M, [30  X  60)  X  to satisfy tra-ditional 2-anonymity, we may further generalize [1  X  30) and [30  X  60) to [1  X  60), resulting in much higher utility loss.
Information needs: The BTS wants to perform two types of data analysis on the blood transfusion data col-lected from the hospitals. First, it wants to obtain some general count statistics. Second, it wants to employ the surgery information as training data for building a classifi-cation model on blood transfusion. One frequently raised question is: To avoid the privacy concern, why doesn X  X  the hospital simply release the statistical data or a classifier to the BTS? The BTS wants to have access to the blood trans-fusion data, not statistics, from the hospitals for several rea-sons. First, the practitioners in hospitals have no expertise and interest in doing the data mining. They simply want to share the patient data with the BTS, who needs the health data for legitimate reasons. Second, having access to the data, the BTS has much better flexibility to perform the required data analysis. It is impractical to continuously re-quest practitioners in a hospital to produce different types of statistical information and fine-tune the data mining results for research purposes.

Contributions: The contributions of this paper are sum-marized as follows. First, we use the BTS as a real-life example to present the challenges of privacy-aware informa-tion sharing for data analysis. Second, to thwart the privacy threats caused by identity and attribute linkage, we propose a new privacy model called LKC -privacy that overcomes the challenge of anonymizing high-dimensional relational data without significantly compromising the data quality (Sec-tion 3). Third, we present an efficient anonymization algo-rithm for achieving LKC -privacy with two different adap-tations. The first adaptation maximizes the information preserved for classification analysis; the second one mini-mizes the distortion on the anonymous data for general data analysis. Minimizing distortion is useful when the particu-lar information requirement is unknown during information sharing or the shared data is used for various kinds of data mining tasks (Section 4). Fourth, experiments demonstrate that our developed algorithm is flexible and scalable enough to handle large volumes of blood transfusion data that in-clude both categorical and numerical attributes. In 2008, the BTS received 150,000 records from the public hospitals (Section 5).
There is a large body of work on anonymizing relational data. Traditional K -anonymity [18][20], ` -diversity [14], and confidence bounding [23] are based on a predefined set of QID attributes. (  X , k )-anonymity [24] further requires ev-ery QID group to satisfy both K -anonymity and confidence bounding. As discussed earlier, these single QID -based ap-proaches suffer from the curse of high dimensionality [1] and render the high-dimensional data useless for data mining. In this paper, we solve the problem of dimensionality by as-suming that the adversary knows at most L values of QID attributes of any target patient. [6] proposes a new pri-vacy model called differential privacy , which ensures that the removal or addition of a single database record does not significantly affect the overall privacy of the database. Yet, the randomization approach is not applicable to the BTS case because they require data truthfulness at the record level. [8] presents a top-down refinement (TDR) method to flexibly K -anonymize various types of attributes; how-ever, their method does not take attribute linkage and high-dimensionality into consideration.

There are some recent works on anonymizing high di-mensional transaction data [10][21][26][27]. [10] divides the transaction data into public and private items; the pub-lic items are grouped together based on similarity. Each group is then associated with a set of private items so that the probability of linking private items from public items is bounded. The idea is similar to the privacy model of Anatomy [25]. The methods presented in [21][26][27] model the adversary X  X  power by a maximum number of known items as prior knowledge. This assumption is similar to ours, but our problem has major differences. First, a trans-action is a set of items, whereas our health data is relational. Second, we have different privacy and utility measures. The privacy model of [21] is based on only K -anonymity and does not consider attribute linkages. [26] and [27] aim at mini-mizing data distortion and preserving frequent item sets, re-spectively, while we aim at preserving classification quality. Finally, [26] and [27] use suppression, while we use general-ization and discretization for anonymizing various types of attributes.

Many techniques have been previously proposed to pre-serve privacy, but only a few have considered the goal for classification. [12] show that some simple statistical infor-mation, like means and correlations, can be preserved by adding noise and swapping values. This technique is stud-ied in data mining for classification [3]. In these works, pri-vacy was measured by how closely the original values of a masked attribute can be estimated, which is very different from the notion of anonymity that quantifies how uniquely an individual can be linked with sensitive information. [28] propose a privacy-preserving approach for building cox re-gression model. However, unlike this paper, they only target to build an analysis model and fall in the category of privacy preserving data mining (PPDM) research.

Iyengar [11] presented the anonymity problem for classi-fication and proposed a genetic algorithmic solution. The idea is to encode each state of generalization as a  X  X hromo-some X  and data distortion into the fitness function. Then Table 2: Anonymous data ( L = 2 , K = 2 , C = 0 . 5 ) they employ the genetic evolution to converge to the fittest chromosome. Similarly, Bayardo and Agrawal [4] also ad-dressed the classification problem using the same classifi-cation metric (CM) of [11]. Recently, LeFevre et al. [13] proposed another anonymization technique for classification using multidimensional recoding. Unlike the random genetic evolution and the bottom-up generalization, our approach produces a progressive generalization process that users can step through to determine a desired trade-off of privacy and accuracy. We also handle both categorical and numerical at-tributes. Moreover, all the proposed models for classification analysis do not address the problem of high-dimensionality, which is a primary contribution of this paper.
We first describe the privacy and information require-ments, followed by a problem statement.
Suppose a data holder (e.g., a hospital) wants to publish a health data table T ( ID, D 1 , . . . , D m , Class, Sens ) (e.g., Table 1) to some recipient (e.g., BTS) for data analysis. ID is an explicit identifier, such as SSN , and it should be removed before publication. Each D i is either a categorical or a numerical attribute. Sens is a sensitive attribute. A record has the form  X  v 1 , . . . , v m , cls, s  X  , where v value of D i , cls is a class value of Class , and s is a sensitive value of Sens . The data holder wants to protect against linking an individual to a record or some sensitive value in T through some subset of attributes called a quasi-identifier or QID , where QID  X  X  D 1 , . . . , D m } .

One recipient, who is an adversary, seeks to identify the record or sensitive values of some target victim patient V in T . As explained in Section 1, we assume that the adversary knows at most L values of QID attributes of the victim pa-tient. We use qid to denote such prior known values, where | qid |  X  L . Based on the prior knowledge qid , the adver-sary could identify a group of records, denoted by T [ qid ], that contains qid . | T [ qid ] | denotes the number of records in T [ qid ]. For example, T [  X  Janitor, M  X  ] = { ID #1 , 6 } and | T [ qid ] | = 2 . Then, the adversary could launch two types of privacy attacks:  X  Identity linkage : Given prior knowledge qid , T [ qid ] is  X  Attribute linkage : Given prior knowledge qid , the ad-
To thwart the identity and attribute linkages on any pa-tient in the table T , we require every qid with a maximum length L in the anonymous table to be shared by at least a certain number of records, and the ratio of sensitive value(s) in every group cannot be too high. Our privacy model, LKC -privacy , reflects this intuition.

Definition 3.1 ( LKC -privacy). Let L be the maxi-mum number of values of the prior knowledge. Let S  X  Sens be a set of sensitive values. A data table T satisfies LKC -privacy if and only if for any qid with | qid | X  L , 1. | T [ qid ] |  X  K , where K &gt; 0 is an integer anonymity 2. P ( s | qid )  X  C for any s  X  S , where 0 &lt; C  X  1 is a real The data holder specifies the thresholds L , K , and C . The maximum length L reflects the assumption of the adver-sary X  X  power. LKC -privacy guarantees that the probability of a successful identity linkage to be  X  1 /K and the prob-ability of a successful attribute linkage to be  X  C . LKC -privacy has several nice properties that make it suitable for anonymizing high-dimensional data. First, it only requires a subset of QID attributes to be shared by at least K records. This is a major relaxation from traditional K -anonymity, based on a very reasonable assumption that the adversary has limited power. Second, LKC -privacy generalizes several traditional privacy models. K -anonymity [18][20] is a spe-cial case of LKC -privacy with L = | QID | and C = 100% , where | QID | is the number of QID attributes in the data ta-ble. Confidence bounding [23] is also a special case of LKC -privacy with L = | QID | and K = 1 . (  X , k )-anonymity [24] is also a special case of LKC -privacy with L = | QID | , K = k , and C =  X  . Thus, the data holder can still achieve the traditional models, if needed.
The measure of data utility varies depending on the data analysis task to be performed on the published data. Based on the information requirements specified by the BTS, we define two utility measures. First, we aim at preserving the maximal information for classification analysis. Second, we aim at minimizing the overall data distortion when the data analysis task is unknown.

In this BTS project, we propose a top-down specialization algorithm to achieve LKC -privacy. The general idea is to anonymize a table by a sequence of specializations starting from the topmost general state in which each attribute has the topmost value of its taxonomy tree [8]. We assume that a taxonomy tree is specified for each categorical attribute in QID . A leaf node represents a domain value and a parent node represents a less specific value. For a numerical at-tribute in QID , a taxonomy tree can be grown at runtime, where each node represents an interval, and each non-leaf node has two child nodes representing some optimal binary split of the parent interval. Figure 2 shows a dynamically grown taxonomy tree for Age .

A specialization , written v  X  child ( v ), where child ( v ) de-notes the set of child values of v , replaces the parent value v with the child value that generalizes the domain value in a record. A specialization is valid if the specialization re-sults in a table satisfying the anonymity requirement after the specialization. A specialization is performed only if it is valid. The specialization process can be viewed as pushing the  X  X ut X  of each taxonomy tree downwards. A cut of the taxonomy tree for an attribute D i , denoted by Cut i , con-tains exactly one value on each root-to-leaf path. Figure 2 shows a solution cut indicated by the dashed curve repre-senting the anonymous Table 2. Our specialization starts from the topmost cut and pushes down the cut iteratively by specializing some value in the current cut until violating the anonymity requirement. In other words, the specialization process pushes the cut downwards until no valid specializa-tion is possible. Each specialization tends to increase data utility and decrease privacy because records are more distin-guishable by specific values. We define two utility measures depending on the information requirement to evaluate the  X  X oodness X  of a specialization.
For the requirement of classification analysis, we use infor-mation gain, denoted by InfoGain ( v ), to measure the good-ness of a specialization. Our selection criterion, Score ( v ), is to favor the specialization v  X  child ( v ) that has the maxi-mum InfoGain ( v ): InfoGain(v) : Let T [ x ] denote the set of records in T gener-alized to the value x . Let freq ( T [ x ] , cls ) denote the number of records in T [ x ] having the class cls . Note that | T [ v ] | = P c | T [ c ] | , where c  X  child ( v ). We have where E ( T [ x ]) is the entropy of T [ x ] [17]: E ( T [ x ]) =  X  Intuitively, I ( T [ x ]) measures the mix of classes for the records in T [ x ], and InfoGain ( v ) is the reduction of the mix by spe-cializing v into c  X  child ( v ).

For a numerical attribute, the specialization of an interval refers to the optimal binary split that maximizes information gain on the Class attribute. See [17] for details. Sometimes, the data is shared without a specific task. In this case of general data analysis, we use discernibility cost [19] to measure the data distortion in the anonymous data table. The discernibility cost charges a penalty to each record for being indistinguishable from other records. For each record in an equivalence group qid , the penalty is | T [ qid ] | . Thus, the penalty on a group is | T [ qid ] | imize the discernibility cost, we choose the specialization v  X  child ( v ) that maximizes the value of over all qid v containing v . Example 3 shows the computation of Score ( v ).
Our goal is to transform a given data set T into an anony-mous version T 0 that satisfies a given LKC -privacy require-ment and preserves as much information as possible for the intended data analysis task. Based on the information re-quirements specified by the BTS, we define the problems as follows.
 Definition 3.2 (Anonymization for data analysis). Given a data table T , a LKC -privacy requirement, and a taxonomy tree for each categorical attribute contained in QID , the anonymization problem for classification analysis is to generalize T on the attributes QID to satisfy the LKC -privacy requirement while preserving as much information as possible for the classification analysis. The anonymization problem for general analysis is to generalize T on the at-tributes QID to satisfy the LKC -privacy requirement while minimizing the overall discernibility cost.
 Computing the optimal LKC -privacy solution is NP-hard. Given a QID , there are QID j with maximum size L . For any value of K and C , each Algorithm 1 Privacy-Aware Information Sharing (PAIS) 1: Initialize every value in T to the topmost value; 2: Initialize Cut i to include the topmost value; 3: while some x  X  X  X  Cut i is valid do 4: Find the Best specialization from  X  Cut i ; 5: Perform Best on T and update  X  Cut i ; 6: Update Score ( x ) and validity for x  X  X  X  Cut i ; 7: end while ; 8: Output T and  X  Cut i .; combination of QID j in LKC -privacy is an instance of the (  X , k )-anonymity problem with  X  = C and k = K . [24] has proven that computing the optimal (  X , k )-anonymous solu-tion is NP-hard; therefore, computing optimal LKC -privacy is also NP-hard. Below, we provide a greedy approach to ef-ficiently identify a sub-optimal solution.
Algorithm 1 provides an overview of our algorithm privacy-aware information sharing (PAIS) for achieving LKC -privacy. Initially, all values in QID are generalized to the topmost value in their taxonomy trees, and Cut i contains the top-most value for each attribute D i . At each iteration, PAIS performs the Best specialization, which has the highest Score among the candidates that are valid specializations in  X  Cut (Line 4). Then, apply Best to T and update  X  Cut i (Line 5). Finally, update the Score of the affected candidates due to the specialization (Line 6). The algorithm terminates when there are no more valid candidates in  X  Cut i . In other words, the algorithm terminates if any further specialization would lead to a violation of the LKC -privacy requirement. An important property of PAIS is that the LKC -privacy is anti-monotone with respect to a specialization: if a gen-eralized table violates LKC -privacy before a specialization, it remains violated after the specialization because a spe-cialization never increases the | T [ qid ] | and never decreases the maximum P ( s | qid ). This anti-monotonic property guar-antees that the final solution cut is a sub-optimal solution. PAIS is modified from TDR [8], which is originally designed for achieving only K -anonymity, not LKC -privacy. One ma-jor difference is the validity check in Line 6, which will be discussed in detail in Section 4.3.

Example 2. Consider Table 1 with L = 2 , K = 2 , C = 50%, and QID = { Job, Sex, Age } . Initially, all data records are generalized to  X  ANY Job, ANY Sex, [1-99)  X  , and  X  Cut = { ANY Job, ANY Sex, [1-99) } . To find the Best special-ization among the candidates in  X  Cut i , we compute Score ( ANY Job ), Score ( ANY Sex ), and Score ( [1-99) ).
A simple yet inefficient implementation of Lines 4-6 is to scan all data records and recompute Score ( x ) for all candi-dates in  X  Cut i . The key to the efficiency of our algorithm is having direct access to the data records to be specialized, and updating Score ( x ) based on some statistics maintained for candidates in  X  Cut i , instead of scanning all data records. In the rest of this section, we explain our scalable implemen-tation and data structures in detail.
Initially, we compute Score for all candidates x in  X  Cut For each subsequent iteration, information needed to calcu-late Score comes from the update of the previous iteration (Line 7). Finding the best specialization Best involves at most | X  Cut i | computations of Score without accessing data records. The procedure for updating Score will be discussed in Section 4.3.

Example 3. Continue from Example 2. We show the computation of Score ( ANY Job ) for the specialization For general data analysis, Score ( ANY Job ) = 6 2 +5 2 = 61 . For classification analysis, E ( T [ ANY Job ]) =  X  6 11  X  log 2 6 11  X  5 11  X  log 2 5 E ( T [ Blue-collar ]) =  X  1 6  X  log 2 1 6  X  5 6  X  log 2 5 E ( T [ White-collar ]) =  X  5 5  X  log 2 5 5  X  0 5  X  log 2 InfoGain ( ANY Job ) = E ( T [ ANY Job ])  X  ( 6 11  X  E ( T [ Blue-collar ]) + 5 11  X  E ( T [ White-collar ])) = 0 . 6396 Score ( ANY Job ) = InfoGain ( ANY Job ) = 0 . 6396 .
Consider a specialization Best  X  child ( Best ), where Best  X  D i and D i  X  QID . First, we replace Best with child ( Best ) in  X  Cut i . Then, we need to retrieve T [ Best ], the set of data records generalized to Best , to tell the child value in child ( Best ) for individual data records. We employ a data structure, called Taxonomy Indexed PartitionS (TIPS) [8], to facilitate this operation. This data structure is also cru-cial for updating Score ( x ) for candidates x . The general idea is to group data records according to their generalized records on QID .

Definition 4.1 (TIPS). TIPS is a tree structure with each root-to-leaf path represents a generalized record over QID . Each leaf node stores the set of data records having the same generalized record for all the QID attributes along the path. Each path is called a path partition . For each x in  X  Cut i , P x denotes a path partition whose generalized record contains x , and Link x denotes the link of all P x , with the head of Link x stored with x .

At any time, the generalized data is represented by the path partitions of TIPS, but the original data records re-main unchanged. Link x provides a direct access to T [ x ], the set of data records generalized to the value x . Initially, TIPS has only one path partition containing all data records, generalized to the topmost value on every attribute in QID . In each iteration, we perform the best specialization Best by refining the path partitions on Link Best .
 Updating TIPS: We refine each path partition P Best found on Link Best as follows. For each value c in child ( Best ), a new partition P c is created from P Best , and data records in P Best are split among the new partitions: P c contains a data record in P Best if c generalizes the corresponding do-main value in the record. An empty P c is removed. Link c is created to link up all P c  X  X  for the same c . Also, link P c to every Link x to which P Best was previously linked, except for Link Best . We emphasize that this is the only operation in the whole algorithm that requires accessing data records. The overhead of maintaining Link x is small. For each attribute in  X  QID j and each path partition on Link Best , there are at most | child ( Best ) |  X  X elinkings X , or at most | X  QID j | X | Link Best | X | child ( Best ) |  X  X elinkings X  in total for applying Best .

Example 4. Initially, TIPS has only one path partition containing all data records and representing the generalized record  X  ANY Job, ANY Sex, [1-99)  X  . Let the best special-ization be ANY Job  X  { White-collar , Blue-collar } on Job . We create two new partitions under the root partition as in Figure 3, and split data records between them. Both the path partitions are on Link ANY Sex and Link [1-99) .  X  Cut is updated into { White-collar , Blue-collar , ANY Sex , [1-99) } . Suppose that the next best specialization is [1-99)  X  { [1-60),[60-99) } , which specializes the two path partitions on Link [1-99) , resulting in the TIPS in Figure 3.
A scalable feature of our algorithm is maintaining some statistical information for each candidate x in  X  Cut i for up-dating Score ( x ) without accessing data records. For each new value c in child ( Best ) added to  X  Cut i in the current iteration, we collect the following count statistics of c while scanning data records in P Best for updating TIPS: | T [ c ] | , and cls is a class label. These information will be used in Section 4.3.

TIPS has several useful properties. First, all data records in the same path partition have the same generalized record although they may have different raw values. Second, every data record appears in exactly one path partition. Third, each path partition P x has exactly one generalized qid on QID and contributes the count | P x | towards | T [ qid ] | . Later, we use the last property to extract | T [ qid ] | from TIPS.
This step updates Score ( x ) and validity for candidates x in  X  Cut i to reflect the impact of the Best specializa-tion. The key to the scalability of our algorithm is updating Score ( x ) using the count statistics maintained in Section 4.2 without accessing raw records again.
The procedure for updating Score is different depending on the information requirement.
 Case 1 classification analysis: An observation is that InfoGain ( x ) is not affected by Best  X  child ( Best ), except that we need to compute InfoGain ( c ) for each newly added value c in child ( Best ). InfoGain ( c ) can be computed from the count statistics for c collected in Section 4.2.
Case 2 general data analysis: Each path partition P c keeps the count | T [ qid c ] | . By following Link c from TIPS, we can compute
A specialization Best  X  child ( Best ) may change the va-lidity status of other candidates x  X   X  Cut i if Best and x are contained in the same qid with size not greater than L . Thus, in order to check the validity, we need to keep track of the count of every qid with | qid | = L . Note, we can ignore qid with size less than L because if a table satisfies LKC -privacy, then it must satisfy L 0 KC -privacy where L 0 &lt; L .
We present an efficient method for checking the valid-ity of a candidate. First, given a QID in T , we identify all QID j  X  QID with size L . Then, for each QID j , we use a data structure, called QIDTree j , to index all qid QID j . QIDTree j is a tree, where each level represents one attribute in QID j . Each root-to-leaf path represents an ex-isting qid j on QID j in the generalized data, with | T [ qid and | T [ qid j  X  s ] | for every s  X  S stored at the leaf node. A candidate x  X  X  X  Cut i is valid if, for every c  X  child ( x ), every qid j containing c has | T [ qid j ] |  X  K and P ( s | qid any s  X  S . If x is invalid, remove it from  X  Cut i .
In this section, our objectives are to study the impact of enforcing various LKC -privacy requirements on the data quality in terms of classification error and discernibility cost, and to evaluate the efficiency and scalability of our proposed anonymization method by varying the thresholds of maxi-mum adversary X  X  knowledge L , minimum anonymity K , and maximum confidence C .

We employ two real-life datasets, Blood and Adult . Blood is a real-life blood transfusion dataset owned by an anony-mous health institute. Blood has 62 attributes after remov-ing explicit identifiers; 41 of them are QID attributes. Blood Group represents the Class attribute with 8 possible val-ues. Diagnosis Codes , which has 15 possible values repre-senting 15 categories of diagnosis, is considered to be the sensitive attribute. The remaining attributes are neither quasi-identifiers nor sensitive. Blood contains 10,000 blood transfusion records in 2008. Each record represents one in-cident of blood transfusion. The publicly available Adult dataset [16] is a de facto benchmark for testing anonymiza-tion algorithms [4][8][11][14][15][22][23]. Adult has 45,222 census records on 6 numerical attributes, 8 categorical at-tributes, and a binary Class column representing two in-come levels,  X  50K or &gt; 50K. See [8] for the description of attributes. We consider Divorced and Separated in the at-tribute Marital-status as sensitive, and the remaining 13 at-tributes as QID . All experiments were conducted on an Intel Core2 Quad Q6600 2.4GHz PC with 2GB RAM. To evaluate the impact on classification quality (Case 1 in Section 3.2.1), we use all records for generalization, build a classifier on 2/3 of the generalized records as the training set, and measure the classification error ( CE ) on 1/3 of the gen-eralized records as the testing set. For classification models, we use the well-known C4.5 classifier [17]. To better visu-alize the cost and benefit of our approach, we measure ad-ditional errors: Baseline Error ( BE ) is the error measured on the raw data without generalization. BE  X  CE repre-sents the cost in terms of classification quality for achieving a given LKC -privacy requirement. A naive method to avoid identity and attributes linkages is to simply remove all QID attributes. Thus, we also measure upper bound error ( UE ), which is the error on the raw data with all QID attributes removed. UE  X  CE represents the benefit of our method over the naive approach.

To evaluate the impact on general analysis quality (Case 2 in Section 3.2.2), we use all records for generalization and measure the discernibility ratio ( DR ) on the final anony-mous data. DR = cernibility cost, with 0  X  DR  X  1. Lower DR means higher data quality.
Figure 4a depicts the classification error CE with ad-versary X  X  knowledge L = 2 , 4 , 6, anonymity threshold 20  X  K  X  100, and confidence threshold C = 20% on the Blood dataset. This setting allows us to measure the performance of the algorithm against identity linkages for a fixed C . CE generally increases as K or L increases. However, the increase is not monotonic. For example, the error drops slightly when K increases from 20 to 40 for L = 4 . This is due to the fact that generalization has removed some noise from the data, resulting in a better classification structure in a more general state. For the same reason, some test cases on L = 2 and L = 4 have CE &lt; BE , implying that generaliza-tion not only achieves the given LKC -privacy requirement but sometimes may also improve the classification quality. BE = 22 . 1% and UE = 44 . 1%. For L = 2 and L = 4 , CE  X  BE spans from -2.9% to 5.2% and UE  X  CE spans from 16.8% to 24.9%, suggesting that the cost for achieving LKC -privacy is small, but the benefit is large when L is not large. However, as L increases to 6, CE quickly increases to about 40%, the cost increases to about 17%, and the benefit decreases to 5%. For a greater value of L , the difference be-tween LKC -privacy and K -anonymity is very small in terms of classification error since more generalized data does not necessarily worse classification error. This result confirms that the assumption of an adversary X  X  prior knowledge has a significant impact on the classification quality. It also in-directly confirms the curse of high dimensionality [1].
Figure 4b depicts the discernibility ratio DR with adver-sary X  X  knowledge L = 2 , 4 , 6, anonymity threshold 20  X  K  X  100, and a fixed confidence threshold C = 20% . DR gen-erally increases as K increases, so it exhibits some trade-off between data privacy and data utility. As L increases, DR increases quickly because more generalization is required to ensure each equivalence group has at least K records. To illustrate the benefit of our proposed LKC -privacy model over the traditional K -anonymity model, we measure the discernibility ratio, denoted DR TradK , on traditional K -anonymous solutions produced by the TDR method in [8]. DR TradK  X  DR , representing the benefit of our model, spans from 0.1 to 0.45. This indicates a significant improvement on data quality by making a reasonable assumption on limiting the adversary X  X  knowledge within L known values. Note, the solutions produced by TDR do not prevent attribute linkages although they have higher discernibility ratio.
Figure 5a depicts the classification error CE with ad-versary X  X  knowledge L = 2 , 4 , 6, anonymity threshold 20  X  K  X  100, and confidence threshold C = 20% on the Adult dataset. BE = 14 . 7% and UE = 24 . 5%. For L = 2 , CE  X  BE is less than 1% and UE  X  CE spans from 8.9% to 9.5%. For L = 4 and L = 6 , CE  X  BE spans from 1.1% to 4.1%, and UE  X  CE spans from 5.8% to 8.8%. These results suggest that the cost for achieving LKC -privacy is small, while the benefit of our method over the naive method is large.
 Figure 5b depicts the CE with adversary X  X  knowledge L = 2 , 4 , 6, confidence threshold 5%  X  C  X  30%, and anonymity threshold K = 100 . This setting allows us to measure the performance of the algorithm against attribute linkages for a fixed K . The result suggests that CE is insen-sitive to the change of confidence threshold C . CE slightly increases as the adversary X  X  knowledge L increases.
Figure 5c depicts the discernibility ratio DR with ad-versary X  X  knowledge L = 2 , 4 , 6, anonymity threshold 20  X  K  X  100, and confidence threshold C = 20% . DR some-times has a drop when K increases. This is due to the fact that our greedy algorithm identifies only the sub-optimal solution. DR is insensitive to the increase of K and stays close to 0 for L = 2 . As L increases to 4, DR increases sig-nificantly and finally equals traditional K -anonymity when L = 6 because the number of attributes in Adult is rela-tively smaller than in Blood . Yet, K -anonymity does not prevent attribute linkages, while our LKC -privacy provides this additional privacy guarantee.
 Figure 5d depicts the DR with adversary X  X  knowledge L = 2 , 4 , 6, confidence threshold 5%  X  C  X  30%, and anonymity threshold K = 100 . In general, DR increases as L increases due to a more restrictive privacy requirement. Similar to Figure 5b, the DR is insensitive to the change of confidence threshold C . It implies that the primary driving forces for generalization are L and K , not C .
One major contribution of our work is the development of an efficient and scalable algorithm for achieving LKC -privacy on high-dimensional healthcare data. Every previ-ous test case can finish the entire anonymization process within 30 seconds. We further evaluate the scalability of PAIS with respect to data volume by blowing up the size of the Adult data set. First, we combined the training and testing sets, giving 45,222 records. For each original record r in the combined set, we created  X   X  1  X  X ariations X  of r , where  X  &gt; 1 is the blowup scale. Together with all original records, the enlarged data set has  X   X  45 , 222 records.
Figure 6 depicts the runtime from 200,000 to 1 million records for L = 4 , K = 20 , C = 100% . The total runtime for anonymizing 1 million records is 107s, where 50s are spent on reading raw data, 33s are spent on anonymizing, and 24s are spent on writing the anonymous data. Our algorithm is scalable due to the fact that we use the count statistics to update the Score , and thus it only takes one scan of data per iteration to anonymize the data. As the number of records increases, the total runtime increases linearly.
The experimental results on the two real-life datasets can be summarized as follows. (1) Our anonymization method PAIS can effectively preserve both privacy and data utility in the anonymous data for a wide range of LKC -privacy re-quirements. There is a trade-off between data privacy and data utility with respect to K and L , but the trend is less obvious on C . (2) Our proposed LKC -privacy model retains more information than the traditional K -anonymity model and provides the flexibility to adjust privacy requirements according to the assumption of adversary X  X  background knowl-edge. (3) PAIS is highly scalable for large data sets. These characteristics make PAIS a promising component for anony-mizing healthcare data.
We have proposed a privacy-aware information sharing method for healthcare institutes with the objective of sup-porting data mining. Motivated by the BTS X  privacy and information requirements, we formulated the LKC -privacy model for high-dimensional relational data. Moreover, our developed algorithm can accommodate two different infor-mation requirements according to the BTS X  information need. Our proposed solution is different from privacy-preserving data mining (PPDM) due to the fact that we allow data sharing instead of data mining result sharing . This is an es-sential requirement for the BTS since they require the flex-ibility to perform various data analysis tasks. We believe that our proposed solution could serve as a model for data sharing in the healthcare sector.

Finally, we would like to share our collaborative expe-rience with the healthcare sector. Health data are com-plex, often a combination of relational data, transaction data, and textual data. So far, our project focuses only on the relational data, but we notice that some recent works, e.g., [9][10][21][27], are applicable to solve the privacy prob-lem on transaction and textual data in the BTS case. Be-sides the technical issue, it is equally important to edu-cate health institute management and medical practition-ers about the latest privacy-preserving technology. When management encounters the problem of privacy-aware in-formation sharing as presented in this paper, their initial response is often to set up a traditional role-based secure ac-cess model. In fact, alternative techniques, such as privacy-preserving data mining and data publishing [2][7], are avail-able to them provided that the data mining quality does not significantly degrade.
The research is supported in part by Discovery Grants (356065-2008) and Canada Graduate Scholarship from the Natural Sciences and Engineering Research Council of Canada. [1] C. C. Aggarwal. On k -anonymity and the curse of [2] C. C. Aggarwal and P. S. Yu. Privacy Preserving Data [3] R. Agrawal and R. Srikant. Privacy preserving data [4] R. J. Bayardo and R. Agrawal. Data privacy through [5] D. M. Carlisle, M. L. Rodrian, and C. L. Diamond. [6] C. Dwork. Differential privacy: A survey of results. [7] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu. [8] B. C. M. Fung, K. Wang, and P. S. Yu. Anonymizing [9] J. Gardner and L. Xiong. An integrated framework for [10] G. Ghinita, Y. Tao, and P. Kalnis. On the [11] V. S. Iyengar. Transforming data to satisfy privacy [12] J. Kim and W. Winkler. Masking microdata files. In [13] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. [14] A. Machanavajjhala, D. Kifer, J. Gehrke, and [15] N. Mohammed, B. C. M. Fung, K. Wang, and P. C. K. [16] D. J. Newman, S. Hettich, C. L. Blake, and C. J. [17] J. R. Quinlan. C4.5: Programs for Machine Learning . [18] P. Samarati. Protecting respondents X  identities in [19] A. Skowron and C. Rauszer. Intelligent Decision [20] L. Sweeney. k -anonymity: A model for protecting [21] M. Terrovitis, N. Mamoulis, and P. Kalnis.
 [22] K. Wang and B. C. M. Fung. Anonymizing sequential [23] K. Wang, B. C. M. Fung, and P. S. Yu. Handicapping [24] R. C. W. Wong, J. Li., A. W. C. Fu, and K. Wang. [25] X. Xiao and Y. Tao. Anatomy: Simple and effective [26] Y. Xu, B. C. M. Fung, K. Wang, A. W. C. Fu, and [27] Y. Xu, K. Wang, A. W. C. Fu, and P. S. Yu.
 [28] S. Yu, G. Fung, R. Rosales, S. Krishnan, R. B. Rao,
