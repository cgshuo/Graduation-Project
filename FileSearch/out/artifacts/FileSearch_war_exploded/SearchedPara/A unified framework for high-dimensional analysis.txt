 problems in which the number of predictors p is comparable to or even larger than the number of dependencies among the variables might have Markov structure specified by a graphical model. In such settings, a common approach to estimating model parameters is is through the use of a is regularized by a function appropriate to the assumed structure. Such estimators may also be interpreted from a Bayesian perspective as maximum a posteriori estimates, with the regularizer reflecting prior information. In this paper, we study such regularized M -estimation procedures, and attempt to provide a unifying framework that both recovers some existing results and provides new results on consistency and convergence rates under high-dimensional scaling. We illustrate some applications of this general framework via three running examples of constrained parametric models which involve an explicit constraint on the number on non-zero model parameters, and also collaborative filtering, and other types of matrix completion problems.
 overview of the broad range of past and on-going work on high-dimensional inference. For the case alized linear models (GLMs) and exponential family models, estimators based on ! 1 -regularized maximum likelihood have also been studied, including results on risk consistency [18] and model models, including convergence rates in Frobenius and operator norm [14], and results on operator norm and model selection consistency [12]. Motivated by inference problems involving block-sparse high-dimensional consistency results have been obtained for model selection and parameter consis-tency [4, 8].
 In this paper, we derive a single main theorem, and show how we are able to rederive a wide range of known results on high-dimensional consistency, as well as some novel ones, including estima-paper. In this section, we begin with a precise formulation of the problem, and then develop some key of restricted strong convexity that the loss function must satisfy. 2.1 Problem set-up denote n observations drawn in an i.i.d. manner from P , and suppose  X   X   X  R p is some parameter so, we consider the following class of regularized M -estimators. Let L : R p  X  Z n % X  R be some r : R p % X  R denote a regularization function. We then consider the regularized M -estimator given by convex and differentiable, and that the regularizer r is a norm.
 L that underlie our analysis. 2.2 Decomposability Our first condition requires that the regularization function r be decomposable, in a sense to be our regularizer. For the bulk of the paper, we assume that H = R p and use the standard Euclidean inner product (which should be assumed unless otherwise specified).
  X  is a very large subspace. As a second example, consider the class of s -sparse parameter vectors  X   X   X  R p , meaning that  X   X  any given subset S and its complement S c , let us define the subspaces subspaces A ( S ) are relatively small as long as | S | = s + p .
 (
A, B ) if In our subsequent analysis, we impose the following condition on the regularizer: Definition 1. The regularizer r is decomposable with respect to a given subspace collection V , meaning that it is decomposable for each subspace pair ( A, B )  X  V .
 Note that any regularizer is decomposable with respect to the trivial subspace collection T relatively large. Let us illustrate with some examples. required.  X  Group-structured sparse matrices and ! 1 ,q matrix norms . Various statistical problems involve matrix-valued parameters  X   X  R k  X  m ; examples include multivariate regression problems or (inverse) covariance matrix estimation. We can define an inner product on such matrices via  X  X 
 X  ,  X   X  X  = trace(  X  T  X  ) and the induced (Frobenius) norm $ k the subspace pair For some fixed s  X  k , we then consider the collection which is a group-structured analog of the s -sparse set S for vectors. For any q  X  [1 ,  X  ] , now suppose that the regularizer is the ! 1 / ! q matrix norm, given by r (  X  ) =  X  Low-rank matrices and nuclear norm . The estimation of low-rank matrices arises in vari-ous contexts, including principal component analysis, spectral clustering, collaborative filter-ing, and matrix completion. In particular, consider the class of matrices  X   X  R k  X  m that have row space and column space respectively. For a given pair of r -dimensional subspaces U  X  R k and V  X  R m , we define a pair of subspaces A ( U, V ) and B ( U, V ) of R k  X  m as follows: norm is decomposable with respect to V . Indeed, since any pair of matrices M  X  A ( U, V ) and (e.g., see the paper [13]).
 Thus, we have demonstrated various models and regularizers in which decomposability is satisfied with interesting subspace collections V . We now show that decomposability has important con-dual norm can be obtained via some easy calculations. For instance, given a vector  X   X  R p and r ( singular value).
 Lemma 1. Suppose !  X  is an optimal solution of the regularized M -estimation procedure (1) , with with  X  n  X  2 r  X  (  X  L (  X   X  )) . Then for any ( A, B )  X  V analysis. 2.3 Restricted Strong Convexity where the number of parameters p may be much larger than the sample size, the strong convexity assumption need not be satisfied. As a simple example, consider the usual linear regression model y = X  X   X  + w , where y  X  R n is the response vector,  X   X   X  R p is the unknown parameter vector, precisely, we have: Definition 2. Given some subset C  X  R p and error norm d (  X  ) , we say that the loss function L C ( A, B, &amp; ) that are indexed by a subspace pair ( A, B ) and a tolerance &amp;  X  0 as follows: equivalent to a restricted eigenvalue condition introduced by Bickel et al. [1]. We are now ready to state a general result that provides bounds and hence convergence rates for of low-rank matrices.
 any set A  X  R p , we define so that r ( u )  X   X  ( A ) d ( u ) for u  X  A .
 Theorem 1 (Bounds for general models). For a given subspace collection V , suppose that the reg-fies restricted strong convexity over C ( A, B, &amp; ) , we have The proof is motivated by arguments used in past work on high-dimensional estimation (e.g., [9, of the results except for the weak-sparse models treated in Section 3.1.2. 3.1 Bounds for linear regression Consider the standard linear regression model y = X  X   X  + w , where  X   X   X  R p is the regression vector, X  X  R n  X  p is the design matrix, and w  X  R n is a noise vector. Given the observations ( &amp; !  X   X   X   X  &amp; 2 = O (  X   X   X  is an s -sparse vector. 3.1.1 Lasso estimates of hard sparse models program !  X   X  arg min  X   X  R p strong convexity for the least-squares loss is equivalent to ensuring the following bound on the design matrix: with high probability for various random ensembles of Gaussian matrices with non-i.i.d. elements. In addition to the RSC condition, we assume that X has bounded column norms (specifically, &amp; X i &amp; 2  X  2 ments with zero-mean and sub-Gaussian tails (i.e., there exists some constant  X  &gt; 0 such that P [ Theorem 1 the following known result [1, 7].
 probability at least 1  X  c 1 exp(  X  c 2 n  X  2 n ) , the solution satisfies and moreover that  X  L (  X   X  )= X T w/n . Under the column normalization condition on the design matrix X and the sub-Gaussian nature of the noise, it follows that &amp; X T w/n &amp;  X   X  high probability. The bound in Theorem 1 is thus applicable, and it remains to compute the form have  X  ( A S )= that &amp; !  X   X   X   X  &amp; 2  X  2  X  ( L ) 3.1.2 Lasso estimates of weak sparse models We now consider models that satisfy a weak sparsity assumption. More concretely, suppose that  X   X  lies in the ! q - X  X all X  of radius R q  X  X amely, the set B q ( R q ) := {  X   X  R p |  X  that the matrix X satisfies the condition C .

A ( S ) ,B ( S ) , &amp; n ) , where &amp; n = which we obtain by applying Theorem 1 in this setting, is new to the best of our knowledge: solution satisfies We note that both of the rates X  X or hard-sparsity in Corollary 1 and weak-sparsity in Corollary 2 X  are known to be optimal 1 in a minimax sense [10]. 3.2 Bounds for generalized linear models of response y  X  Y based on a predictor x  X  R p is given by p ( y | x ;  X   X  ) = exp( y  X   X   X  ,x  X  X  X  a ( likelihood !  X   X  arg min  X   X  R p case of our M -estimator (1) with L (  X  )=  X  X   X  , version of the condition (8).
 Corollary 3. Suppose that the true vector  X   X   X  R p is exactly s -sparse with support S , and the model ( a, X ) satisfies an RSC condition. Suppose that we compute the ! 1 -regularized MLE with  X  We defer the proof to the full-length version due to space constraints. 3.3 Bounds for sparse matrices In this section, we consider some extensions of our results to estimation of regression matrices. than the ! 1 norm (e.g., [17, 20, 23, 5]). Such regularizers allow one to impose various types of block-sparsity constraints, in which groups of parameters are assumed to be active (or inactive) simultaneously. We assume that the observation model takes on the form Y = X  X   X  + W , where  X   X   X  R k  X  m is the unknown fixed set of parameters, X  X  R n  X  k is the design matrix, and W  X  R n  X  m is the noise matrix. As a loss function, we use the Frobenius norm 1 $ parameter as follows: Corollary 4. Suppose that the true parameter matix  X   X  has non-zero rows only for indices i  X  S  X  { 1 probability at least 1  X  c 1 exp(  X  c 2 n  X  2 n ) , the q -block Lasso solution satisfies result. A simple argument shows that  X  ( S )= For q =1 , solving the group Lasso is identical solving a Lasso problem with sparsity sm and ambient dimension km , and the resulting upper bound 8  X   X  ( L ) to Corollary 1). For the case q =2 , Corollary 4 yields the upper bound 8  X   X  ( L ) parameters in the matrix (once the non-zero rows have been determined). We note that recent work by Lounici et al. [4] established the bound O (  X   X  ( L ) from a term 3.4 Bounds for estimating low rank matrices authors (see the paper [13] and references therein). To illustrate our main theorem in this con-text, let us consider the following instance of low-rank matrix learning. Given a low-rank matrix tem identification settings in control theory [13]. The following regularized M -estimator can be considered in order to estimate the desired low-rank matrix  X   X  : the rank-r collection V defined for low-rank matrices in Section 2.2. Let  X   X  = U  X  W T be the  X  hold it can be shown that the design matrices X i must satisfy and satisfy the appropriate analog of the column-normalization condition. As with analogous con-random matrices. 2 { X i } satisfy condition (15) . If we solve the regularized M -estimator (14) with  X  n = 16 then with probability at least 1  X  c 1 exp(  X  c 2 ( k + m )) , we have Proof. Note that if rank(  X   X  )= r , then |||  X   X  ||| 1  X   X  Therefore, 1 n unit vectors u and v is bounded above by 8 verifying that  X  n  X  2 r  X  (  X  L (  X   X  )) with high probability. [1] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. [2] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than [3] S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. [4] K. Lounici, M. Pontil, A. B. Tsybakov, and S. van de Geer. Taking advantage of sparsity in [6] N. Meinshausen and P. B  X  uhlmann. High-dimensional graphs and variable selection with the [7] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional [8] G. Obozinski, M. J. Wainwright, and M. I. Jordan. Union support recovery in high-dimensional [9] S. Portnoy. Asymptotic behavior of M-estimators of p regression parameters when p 2 /n is [10] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional [12] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estima-[13] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix [14] A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance [16] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. [17] B. Turlach, W.N. Venables, and S.J. Wright. Simultaneous variable selection. Technometrics , [20] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. [22] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning [23] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite
