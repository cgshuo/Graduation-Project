 In business process management graphically depicted process models serve as com-munication vehicles about the working procedures of organizations. They are the basis for a shared understanding and process improvements. Moreover, process models are often used as requirements engi neering artifacts for software imple-mentation projects. Supporting processes with software offers great potential to save time, enhance reliability and deliver standardized output [9]. At the same time, misunderstandings in early stages lead to expensive change requests at later stages of the software project. Thus, the q uality of communication between stake-holders is crucial to translate process requirements into software implementation.
In current practice, process models are created by trained method experts, typically external consultants. They gat her the required information in inter-views or workshops with the stakeholders of the process [1]. Afterwards, the method expert creates a business proces s model using notations such as EPC or BPMN. Creation of process models i s done with dedicated software.

Domain experts provide information upfront but are typically passive while their knowledge is translated into a pro cess model by the modeling expert. This translation step undertaken by the modeling expert de-couples the domain expert from the model. When asked for feedback, additional effort is needed to explain the models meaning and to resolve misunderstandings. This paper addresses this problem by introducing an approach to couple domain experts with process models using tangible objects.
 We have developed the tangible business process modeling (t.BPM) toolkit. It is a transcribable set of plastic tiles that can be used to model processes on a table. It reflects the iconography of the Business Process Model and Notation (BPMN), see Fig. 1. It consists of shapes for activities, gateways, events and data objects. Control flow and roles are drawn on the table. In our opinion, it enables domain experts to actively shape their processes and allows the method expert to act as a facilitator rather than a translator. For the scope of our work, we consider domain experts to be the stak eholders of the project, i.e. clerks or managers. The method expert is either an external process consultant or an internal process expert who is trained in methods and notations to frame knowledge in process-oriented projects.

This paper reports on a controlled experiment in which we analyze one-to-one interview situations with respect of the e ffectiveness of process elicitation with or without t.BPM. It is a condensed versio n of a extensive technical report [15] on this experiment. Two hypotheses we re cut out and discussed in detail in a separate publication [8]. Three more hypotheses were dropped for this paper as they did not hold and don X  X  add value to the discussion here.

We review related research on process modeling in the next section. After-wards, we explain the hypotheses, the experiment setup, the variables and the analysis procedures used in Section 3. T he experiment execution is discussed in Section 4 and the data analysis is reported in Section 5. The results from the analysis are interpreted in Section 6 and the paper is concluded in Section 7. Empirical research on process modeling is typically focussed on the models that are produced with software tools and can be automatically analyzed, e.g. [5]. Only some research is turned towards the modelers in front of the screen and the process of model creation.

As examples, Sedera et al. [21] used ca se study research and survey methods to derive qualitatively a framework of factors that influence the success of process modeling efforts. Amongst others, they found tooling and participation to be key drivers. Participative model building was investigated by Persson [16]. She found that it leads to enhanced model quality, more stakeholder consensus and more commitment to results. The workshops are set up with a dedicated software tool operator to channel participant knowledge and create a common picture at the projector [23]. Rittgen developed software and guidance for modeling workshops in which the participants themselves use the software to create the model together [18]. Our approach uses intuitive tooling to remove software as a barrier for individuals to participate.

For individuals, Recker found that modeler performance is influenced by the complexity of the grammar [17], modeling experience and modeling background. Controlled experiments with individuals have been conducted e.g. by Weber et al. [24] to investigate the effect of events on process planing performance or by Holschke [10] to investigate the influence of model granularity on reusability of artifacts. To our best knowledge there is n o controlled experiment that investi-gates the presence of an intuitive mapping tool for business process modeling.
The setup and execution of our controlled experiment was guided by Creswell [3] and Wohlin et al. [25]. We use literature from experimental software engineer-ing [12] and statistics [6] to inform the structure of the paper and the level of reporting. We outline all planning activities in this section. We start by deriving our hy-potheses, talk about setup, the actual measurement of the hypotheses and the analysis procedures. 3.1 Goal and Hypotheses The goal of this paper is to examine the effect of t.BPM on process elicitation with individuals. Therefore we compare t.BPM to structured interviews which are seen as the most effective requirement s elicitation technique [4]. By  X  X ffective X  we mean that it produces a  X  X esired or intended result X  [22]. In requirements en-gineering, more information is typically indicating more effective elicitation. But it was already shown that the presence of visual representations does not neces-sarily elicit more information [4]. We argue that effective process elicitation has more aspects such as user engagement and validated results. Fig. 2 visualizes how we refine our model towards hypotheses based on the following considerations:
User engagement is widely recognized as a key factor for success of IT projects [21]. Our approach uses tangible media which is seen as a key factor for task engagement, e.g. in HCI research [11]. In those cases, engagement is typically measured as the time spent on a problem, e.g. by Xie et al. [26]. Since tangible modeling consumes time to handle the tool itself (e.g, writing on tiles), we split up the time into more fine granular observations. We hypothesize that people will spent more time talking ( H 1 ) about the process but also spent more time to think ( H 2 ) about what they do.

Schaufeli segments engagement into the dimensions activation and identifica-tion [19]. While activation is already measured with the time spent on the task, we additionally hypothesize that people have more fun ( H 3 ) as a further aspect of activation. The dimension of identification inspires us to hypothesize that people modeling with t.BPM will have more motivation ( H 4 ) to accomplish the task and will be more committed to the solution( H 5 ) that they shaped. The second key aspect that we see for effective elicitation is a validated result. Schneider [20] points out that validation cycles are a time consuming aspect of requirements elicitation projects. He proposes to crea te a model during the elicitation to trigger instant feedback and speedup validation. Validation cycles are characterized by reviews and adjust ments to the model. We hypothesize that people will do more reviews ( H 6 ) when using t.BPM and apply more corrections ( H 7 ) to their process model.

Finally, validation in model building depends on the competencies of the par-ticipants. Frederiks [7] proposes that users validate models by deciding on the significance of information. We propose that this depends on a clear understand-ing of the modeling goal. We hypothesize that t.BPM provides a clearer goal ( H 8 ) for the elicitation session. Frederiks also proposes that modeling experts guide the validation by grammatical analysis, in other words their modeling knowledge. We hypothesize that tangible modeling can create insights into process thinking ( H 9 ) for the user and thereby support the validation process.

We note that the hypotheses are not a forced consequence of the identified aspects and their building involved int erpretation. We come back to this decom-position when we assess the measurement validity in Section 5.3. 3.2 Experiment Setup and Sampling Strategy We design the following experimental setup as illustrated in Fig. 3. Subjects get first conditioned to a certain level of BPM understanding. Afterwards they are randomly assigned to do either interviews or model with t.BPM. The topic is randomly chosen between buying expensive equipment and running a call for tender. Two persons operate the experiment. One guides the subjects in the role of an interviewer, the other one observes the situation and ensures a stable treatment throughout the experiment. They randomly swap roles.

During the experimental task data is co llected using video recording. After-wards, a questionnaire is to be filled in by the subjects. In every step of the experiment, the time is tracked but time constrains are not imposed on subjects. After the first run, subjects rerun the experimental task using the other method and the other process to report on.

In other words, we use a randomized balanced single factor design with re-peated measurements [25] also known as a within-subjects design. All subjects get both treatments assigned in different order. All subjects do interviews and process modeling. Finally, all subjects are rewarded for their participation with a chocolate bar and a cinema voucher. 3.3 Experimental Material We briefly outline and explain the printed material used in this experiment. The original documents are appended to the technical report [15]. Like the experi-ment, the experimental material is in German.  X  BPM introduction: Two pages explaining the terms Business Process Man- X  Sample model: One page depicting the process of  X  X aking Pasta X  and four  X  2x task sheet: One paragraph explaining the process to report on. It explicitly  X  Interview guide (for experimenter): Experimenters read out the same six  X  Questionnaire: Items to be rated on a 5-point Likert scale. Details are ex-3.4 Participant Selection The sample population used in research studies should be representatives of the population to which the researchers wish to generalize [2]. Thus, we want potential users of t.BPM to participate in the study. We got the opportunity to run an on-site experiment at a trade school in Potsdam (Germany) with graduate office and industrial clerk students. They all work in companies and study part time at the trade school. Industrial clerks do planing, execution and controlling of business activities. Office clerks do supp orting activities in a department, e.g. as office managers. Both groups might be questioned in process-oriented projects by external consultants. Thus, they represent the target population that we like to address with t.BPM. 3.5 Operationalized Hypotheses We operationalize the hypoth eses presented in Section 3.1 by means of a question-naire and video analysis. We define each hypothesis as H x and its null hypothesis Questionnaire-Based Hypotheses ( H 3 , H 4 , H 5 , H 8 , H 9 ): Hypotheses which rely on perceived measures are tested us ing a questionnaire. On a five-point Lik-ert scale, subjects rate their agreement t o, in summary, fifteen statements. Three statements together represent one hypoth esis. Two statements are formulated to-wards the hypotheses, one is negatively formulated. The level of agreement is mapped to the values [1..5] where 1 is n o agreement and 5 is a strong agreement. The values are aggregated (negative statement is turned around by calculating 6  X  value ) to retrieve the actual value to work with. The hypothesis holds if there is a significant difference accord ing to the method immediately used be-fore, t.BPM or interviews. We t est the following hypotheses:  X  H  X  H  X  H  X  H  X  H Video Hypotheses ( H 1 , H 2 , H 6 , H 7 ): We operationalize hypotheses related to time and actions taken during the experimental task using video coding analysis. We define the following coding schemes: Time Slicing( H 1 , H 2 ): The duration of the experimental task is sliced exclu-sively to belong to one of five categories. The use of t.BPM ( Use tBP M )suchas labeling and positioning the shapes without talking, Talk tBP M/int is the time people talk about the process, UseTalk tBP M is talking and using t.BPM (to avoid overlap between Use tBP M and Talk tBP M ). We define a code for the time spent silent ( Silence tBP M/int ) when people do not talk and do not handle t.BPM. Finally, Rest tBP M/int captures remaining time such as interactions with the in-terviewer. The same coding scheme is used for both experimental tasks. However, Use and UseTalk do not apply for interviews as there is no t.BPM to use. Corrections and Reviews( H 6 , H 7 ): Both are coded as distinct events. We code Corrections tBP M/int if the context of an already explained process part is explicitly changed. In t.BPM sessions this involves re-labeling or repositioning that impacts the process model meaning. In interviews explicit revisions of pre-viously stated information is considered a co rrection. The Reviews tBP M/int are coded if subjects decide to recapitulate their process. This must involve talking about the process as we cannot account possibly silent reviews. This scheme is the same for both experimental tasks.

Using this coding scheme we operationalize the video hypotheses in the following way:  X  H  X  H  X  H  X  H 3.6 Variables The independent variable in this experiment setup is the method used for process elicitation. Subjects do either a struct ured interview or the same structured interview in the presence of t.BPM, the tangible modeling toolkit. The dependent variables are formed from the data co llected during and immediately after a session. We use a notational convention for the data sets collected: intention V/Qx . As an example, talking V 1 describes the set of talking times as measured with the video analysis for hypothesis 1. Likewise, fun Q 3 is the set of all ratings collected with the fun related questionnaire items for hypothesis 3. 3.7 Analysis Procedures For hypothesis testing, we use a one-way repeated-measures ANOVA (analysis of variances). It aims to determine the variation within subjects that is caused by the method. Additionally, we carry out a dependent t-test with acceptance confounding factors that might have influenced the performance of the subjects.
To assess reliability of the questionnaire, we use Cronbach X  X  alpha. It deter-mines the internal consistency of the th ree questionnaire items measuring one hypothesis. The video data is analyzed by two independent reviewers. They com-pare their results and (if needed) resol ve conflicts by negotiation. Cohen X  X  Kappa is used to determine the inter-rater agreement before negotiation to assess the quality of our coding guidelines. The experiment design was executed in December 2009 at a trade school in Potsdam. Slots were offered to the students by short teasers given in the classes. All subjects were at the age of nineteen to twenty-one. Students could choose to swap one lecture unit for experiment participation (about 1h). We expected to test industrial clerks only, but only ten volunteered. Thus, we opened up the experiment to office clerks as well. We ended up testing 7 office clerks and 10 industrial clerks within the week.

Each experiment run started with a short informal warm-up chat and after-wards followed the design as outlined in Section 3.2. One experimenter ran the experiment, the other one operated the cameras and observed the situation to ensure a stable treatment. Fig. 4 depicts the two experimental tasks as taped by the cameras. One video taping went wrong, leading to a sample size of sixteen for the video coding hypotheses. We explain the analysis techniques used and the results found in this section. We reason about the results in Section 6.
 5.1 Descriptive Statistics From seventeen students in two runs, we c ollected 34 questionnaires with 510 statements in total. The video analysis is based on 6,74 hours of video material. One t.BPM session taping went wrong. That results in N=16 for all hypotheses that rely on video analysis. Videos taken during t.BPM sessions took twenty minutes (19.52) on average ranging from ten (10.25) to almost forty minutes (38.98). On the other hand, interviews took about five minutes (5.42) on average ranging from three and a half (3.53) to ten minutes (9.68) at most. 5.2 Data Set Preparation The data was tested with the Kolmogorov-Smirnov and Shapiro-Wilk test and is normally distributed. The original experiment evaluation involved two more video codings and three more questionnaire items. The related hypotheses did not hold and the data was therefore dropped for discussion in this paper due to the limited space. Apart f rom that, no collected data was excluded from the set. 5.3 Measurement Reliability and Validity According to Kirk and Miller the reliability is the extent to which  X  X  measure-ment procedure yields the same answer ho wever and whenever carried out X  ([13], p.19) while validity is the  X  X xtent to which it gives the correct answer X .
We assess two aspects of measurement reliability. First we check the inter-rater agreement for the video codings u sing Cohen X  X  kappa coeficient (  X  ). It compares both video codings before the negotiation process. The inter-rater agreement over all videos and all coding schemes is  X  = . 463 where 0 . 41 &lt; X &lt; 0 . 60 is a moderate agreement level [14]. Thus, we interpret our coding instructions as reasonably reliable and the results as moderately reproducible.

Furthermore, the reliability of the questionnaire is measured using Cronbach X  X  alpha (  X  ). It determines the degree to which the items related to one hypothesis coincide. In other words, whether they actually measure the same underlying concept, e.g. fun. In the literature [6]  X &gt; .8 is suggested to be a good value for questionnaires, while  X &gt; 0.7 is still acceptable. All our variables had  X &gt;. 8, except for  X  ( motivation Q 4 )= . 702 and  X  ( clarity Q 8 )= . 687. We keep those exceptions in mind but overall a high degree of reliability is indicated for the questionnaire.

Validating whether our variables correctly describe  X  X ffective elicitation X  is not directly possible. We use effective elicitation as an umbrella term for the aspects of engagement and result validation. From there we derive variables to measure these aspects. In [15] we conducted a principal component analysis for validation. It is a technique to determine sets of strongly correlating variables which are approximated with one factor, the principal component [6]. Ideally, the variables would form two factors. Those that reflect the measures for engagement and those measuring result validation.

Using orthogonal (varimax) rotation, our nine dependent variables split up to three factors that do not match our hypothesis decomposition. Interestingly, all questionnaire based variables aggregate to one large principal component. These measures rely on self-perception of the s ubjects and therefore describe one side of the coin. Moreover, the time for talking V 1 and silence V 2 strongly correlate with the amount of reviews V 6 done. It indicates the degree to which people were involved with the task. Finally, corrections V 7 builds a single factor. Overall, the measurement validation calls for a more thought-out hypothesis decomposition and clever selection of mea surement instruments. 5.4 Hypothesis Testing We use the repeated-measures ANOVA to determine the effect of our indepen-dent variable (method) within each individual per dependent variable. In other words, to what extend did the method influence the performance of each individ-ual? Fig. 5 illustrates how our data is partitioned. From the overall variability ( SS T ), we identify the performance difference within participants ( SS W )and canfurtherdistinguishthevariationcausedbythetreatment( SS M )andthe variation not explained by our treatment( SS R ).

The ratio of explained to unexplained variability in our dataset is described by F = SS M df M / SS R df R .Where df are the degrees of freedom calculated from the number of different methods ( df M =2-1=1) and the participant number ( df R =17-sorted the variables a ccording to descending F . 05 ratios. We also report SS B , SS M , SS R and  X  2 (eta squared). The value of  X  2 = SS M SS W describes the ratio of variation within the subjects that can be explained by the treatment method. It is an effect size measure.

Furthermore we conduct a dependent t-test to create a different view on the data, see Table 2. It compares the groups doing t.BPM and interviews by their mean scores ( V =in minutes, Q =Likert scale [1..5]), the statistical significance of this difference (one-tailed with acceptance level p &lt; .05) and the confidence interval. The upper and lower boundaries indicate that the real mean difference between the groups is in that range with 95 percent probability. It should not include zero to be sure about the effect between the groups.

From both tables we see, that all parameters for corrections V 7 , silence V 2 and insights Q 9 meet scientific standards. For reviews V 6 , talking V 1 and fun Q 3 we see that they just missed acceptable standa rds in both tests. E.g. the difference between the groups is significant in Table 2 but the confidence intervals do not allow acceptance by rigor scie ntific standards. Finally, commitment Q 5 , clarity Q 8 and motivation Q 4 did not hold. 5.5 Testing Potentially Influential Factors We use a two-tailed dependent t-test to compare groups were two different influ-ences were applied. For example, we had two processes to report on, two exper-imenters, and two different educational groups. Furthermore, each subject goes through the experimental task twice. Repetition effects might have influenced the performance of the subjects.

While the experimenters had no significant influence on the dependent vari-ables, we found that the second experimental task led to significantly more clarity Q 8 about the goal (1st=3.1, 2nd=3.77, p=.001) and more commitment Q 5 to the solution (1st=3.2, 2nd=3.63, p=.004). Participants X  education had significant influence on clarity Q 8 and insights Q 9 . In particular, office clerks reported to have a clearer goal understanding (o-clerks=3.98, i-clerks=3.05, p=.031) and get more new insights into process think-ing (o-clerks=4.05, i-clerks=3.30, p=. 022). In all cases, the confidence intervals left no doubt about the effect. We can identify three types of variables. Those that support their hypothesis, those that do not support their hypothesis, and those that just missed rigor scientific standards. We consider the latter ones as conditionally supportive and argue that a slightly larger sample set would have made the difference.
This claim is based on the t-test in Table 2. It indicates a significant dif-ference for talking V 1 , fun Q 3 ,and reviews V 6 due to method. The confidence intervals do not allow acceptance with s cientific rigor. That means, we can-For example, talking V 1 time is significantly higher (p=.044) in t.BPM sessions (t.BPM=4.65min, int=3.49min) but the confidence interval includes zero (lb=-0.19min, ub=2.52min). In this case we mi ss rigor acceptable levels by twelve seconds. The rest of the discussion is st ructured according to the hypothesis decomposition in Fig. 2.

The engagement variables to measure activation indicate a positive effect through method. Participants in t.BPM sessions did spend more time talking ( F 3 . 24,  X  2 =0 . 17) in t.BPM sessions. We reject H 02 and argue that H 01 and H 03 might be rejected with a bigger sample size.

The engagement variables measuring the dimension of identification did not hold. People did not report significantly more motivation or commitment to their solution due to the method used. We assume a ceiling effect for motivation Q 4 . Participants got off from class, plus a chocolate bar and a cinema voucher for com-pensation. On a five point Likert scale we could not find a statistically relevant difference in motivation Q 4 due to the method applied (t.BPM=4.45, int=4.37). For commitment Q 5 we found in Section 5.5 that it significantly raises with rep-etition (p=.004). Thus, we assume that commitment (as operationalized by us) indicates self-confidence that raises with due to the learning effect. We do neither reject H 04 nor H 05 .

The variables that operationalize the aspect of validated results show a mixed more corrections ( F 0 . 5 (1 , 15) = 46 . 3,  X  2 =0 . 76) due to the method. We reject H 07 and argue that H 06 might be accepted with a sigh tly larger sample size. We conclude that t.BPM provokes more feedback in process elicitation sessions. The competencies required for result validation rely on perceived measures. We see that people report significantly more insights into process thinking ( F 0 . 5 (1 , 15) = 5 . 36,  X  2 =0 . 25) in t.BPM sessions but the goal clarity does not clarity significantly raises with repetition (p=.001). We conclude that, similar to commitment, the goal clarity is determined by learning rather than the method. We reject H 09 but not H 08 .

In summary, we interpret the t.BPM method to be engaging through activa-tion of subjects. We can not reason on the concept of identification which was determined by other effects in this exp eriment. The t.BPM method also leads to validated results through more fee dback on the model. The competencies for result validation raise partially with the method and partially with learning through repetition. 6.1 Validity Threats The internal validity was addressed by the experiment design. In particular, we use two processes and two experimen ters assigned in random order. In Sec-tion 5.5 we assess potentially confounding variables for their influence. While experimenters and processes did not harm the results, we found learning effects due to the repeated mea surements design on clarity Q 8 and commitment Q 5 . We found education to be influential on the reported clarity Q 8 and insights Q 9 . In short, office clerks tend to report better s cores while scoring worse in objective tasks [8]. While group heterogeneity is a threat to the internal validity, it also increases the external validity as both groups represent the population that we address with our tool. This is as important as choosing domain processes rather than artificial graphs to test with. We chose the domain processes in coordination with the school to ensure all students are equally familiar with them. However, wedidnotassesstowhichextendindivi duals are exposed to these processes in their companies. The measurement instruments were tested in one pre-study with ten computer science students. Small adjustments were made afterwards. To ensure quality standards for data analysis, we used two independent coders for the video analysis and we have split each questionnaire variable into three items, one poled negatively. Finally, we provide a longer version of this paper including more data and the experimental material in [15]. 6.2 Generalizability of Findings We think the findings about t.BPM can be generalized from the sample group to the general population. Besides their age (19-21years) the students represent exactly the group we address with the t.BPM tool.

We also think that the findings will hold for other tangible modeling ap-proaches when compared to pure talking. Some aspects have also been reported for visual mappings of requirements such as instant feedback [20]. However, a different tests would be needed to dete rmine exactly the aspects that lead to activation of participants. 6.3 Lessons Learned If we had to start over again, we would put more effort into the reliability of our questionnaire items, in particular clarity Q 8 . But we also learned that people may report a glorified self-image. Thus, we su ggest to mix measurement instruments for each measured concept. In other words , complement perceived measures with external measures such as video codings. But we also had to learn that rigor video analysis is the most time-consuming evaluation task.

Besides all that, we think that the compact on-site experiment was a good idea. Instead of spreading it out over various weeks with changing conditions, we could collect the data in a compact w eek with a stable setup. Moreover, the two experimenters to review each others work did ensure a stable setup. This paper reports on a controlled experiment which was conducted with 17 student clerks at a trade school. We investigate the process elicitation method as an independent variable. Subjects did structured interviews and t.BPM in a repeated measurement design. We cla im that t.BPM enables more efficient process elicitation. We argue that efficient elicitation is not about the amount of information but about user engagement and validated results. We decompose these aspects into nine operationalized hypotheses. Three hypotheses did hold. Three more might hold with a larger sample set.

The results show strong support for user engagement through activation of participants and validated results through more feedback from participants. We think that these findings are reproducible with other tangible system modeling approaches when compared with interviews.

Our findings are limited by the measurement instruments and the small sam-ple size (N=17). A future experiment with a larger group and better tested instruments might re-enforce our findings and also support H 1 , H 3 and H 6 .In other words, it would extend our rigor findings to more talking, more fun and more reviews with tangible media. For now we only showed significantly more thinking time ( H 2 ), more corrections ( H 7 ) and more insights into modeling ( H 9 ) when using tangible media instead of interviews.
 We are grateful to the students that helped setting up, running, and evaluating this experiment, namely Karin Telschow, Markus G  X  untert and Carlotta Mayolo. We X  X  also like to thank the reviewers for their valuable feedback. It led to a substantial revision of Sections 3.1 and 6.

