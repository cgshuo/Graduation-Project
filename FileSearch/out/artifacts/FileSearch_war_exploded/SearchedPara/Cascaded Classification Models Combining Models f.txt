 The problem of  X  X olistic scene understanding X  encompasses a number of notoriously difficult com-puter vision tasks. Presented with an image, scene understa nding involves processing the image to answer a number of questions, including: (i) What type of scen e is it (e.g., urban, rural, indoor)? (ii) What meaningful regions compose the image? (iii) What objects are in the image? (iv) What is the 3d structure of the scene? (See Figure 1). Many of these quest ions are coupled X  X .g., a car present in the image indicates that the scene is likely to be urban, wh ich in turn makes it more likely to find road or building regions. Indeed, this idea of communicatin g information between tasks is not new and dates back to some of the earliest work in computer vision (e.g., [1]). In this paper, we present a framework that exploits such dependencies to answer quest ions about novel images.
 While our focus will be on image understanding, the goal of com bining related classifiers is relevant to many other machine learning domains where several relate d tasks operate on the same (or related) raw data and provide correlated outputs. In the area of natur al language processing, for instance, we might want to process a single document and predict the par t of speech of all words, correspond the named entities, and label the semantic roles of verbs. In the area of audio signal processing, we might want to simultaneously do speech recognition, source separation, and speaker recognition. In the problem of scene understanding (as in many others), st ate-of-the-art models already exist for many of the tasks of interest. However, these carefully engi neered models are often tricky to modify, or even simply to re-implement from available descriptions . As a result, it is sometimes desirable to treat these models as  X  X lack boxes, X  where we have we have acc ess only to a very simple input/output interface. in short, we require only the ability to train on d ata and produce classifications for each data instance; specifics are given in Section 3 below.
 In this paper, we present the framework of Cascaded Classific ation Models ( CCM s), where state-of-the-art  X  X lack box X  classifiers for a set of related tasks are combined to improve performance on some or all tasks. Specifically, the CCM framework creates multiple instantiations of each classifi er, and organizes them into tiers where models in the first tier le arn in isolation, processing the data to produce the best classifications given only the raw instance features. Lower tiers accept as input both the features from the data instance, as well as features comp uted from the output classifications of the models at the previous tier. While only demonstrated in th e computer vision domain, we expect the CCM framework have broad applicability to many applications in machine learning.
 We apply our model to the scene understanding task by combini ng scene categorization, object detection, multi-class segmentation, and 3d reconstructi on. We show how  X  X lack-box X  classifiers can be easily integrated into our framework. Importantly, i n extensive experiments on large image databases, we show that our combined model yields superior r esults on all tasks considered. A number of works in various fields aim to combine classifiers t o improve final output accuracy. These works can be divided into two broad groups. The first is t he combination of classifiers that predict the same set of random variables. Here the aim is to improved classific ations by combining the outputs of the individual models. Boosting [6], in which many weak learners are combined into a highly accurate classifier, is one of the most common and powe rful such scemes. In computer vision, this idea has been very successfully applied to the task of fa ce detection using the so-called Cascade of Boosted Ensembles (CoBE) [18, 2] framework. While similar to our work in constructing a cascade of classifiers, their motivation was computational efficiency, rather than a consideration of contextual benefits. Tu [17] learns context cues by cascad ing models for pixel-level labeling. However, the context is, again, limited to interactions bet ween labels of the same type. The other broad group of works that combine classifiers is aim ed at using the classifiers as compo-nents in large intelligent systems. Kumar and Hebert [9], fo r example, develop a large MRF-based probabilistic model linking multiclass segmentation and o bject detection. Such approaches have also been used in the natural language processing literature. Fo r example, the work of Sutton and McCal-lum [15] combines a parsing model with a semantic role labeli ng model into a unified probabilistic framework that solves both simultaneously. While technical ly-correct probabilistic representations are appealing, it is often painful to fit existing methods int o a large, complex, highly interdepen-dent network. By leveraging the idea of cascades, our method provides a simplified approach that requires minimal tuning of the components.
 The goal of holistic scene understanding dates back to the ea rly days of computer vision, and is highlighted in the  X  X ntrinsic images X  system proposed by Ba rrow and Tenenbaum [1], where maps of various image properties (depth, reflectance, color) are computed using information present in other maps. Over the last few decades, however, researchers have instead targeted isolated computer vision tasks, with considerable success in improving the st ate-of-the-art. For example, in our work, we build on the prior work in scene categorization of Li and Pe rona [10], object detection of Dalal and Triggs [4], multi-class image segmentation of Gould et a l. [7], and 3d reconstruction of Saxena et al. [13]. Recently, however, researchers have returned t o the question of how one can benefit from exploiting the dependencies between different classifiers .
 Torralba et al. [16] use context to significantly boost objec t detection performance, and Sudderth et al. [14] use object recognition for 3d structure estimati on. In independent contemporary work, Hoiem et al. [8] propose an innovative system for integratin g the tasks of object recognition, surface orientation estimation, and occlusion boundary detection . Like ours, their system is modular and leverages state-of-the-art components. However, their wo rk has a strong leaning towards 3d scene reconstruction rather than understanding, and their algor ithms contain many steps that have been specialized for this purpose. Their training also requires intimate knowledge of the implementation of each module, while ours is more flexible allowing integrat ion of many related vision tasks regard-less of their implementation details. Furthermore, we cons ider a broader class of images and object types, and label regions with specific classes, rather than g eneric properties. Our goal is to classify various characteristics of our data u sing state-of-the-art methods in a way that allows the each model to benefit from the others X  experti se. We are interested in using proven  X  X ff-the-shelf X  classifiers for each subtask. As such these classifiers will be treated as  X  X lack boxes, X  each with its own (specialized) data structures, feature se ts, and inference and training algorithms. To fit into our framework, we only require that each classifier provides a mechanism for including additional (auxiliary) features from other modules. Many s tate-of-the-art models lend themselves to the easy addition of new features. In the case of  X  X ntrinsi c images X  [1], the output of each com-ponent is converted into an image-sized feature map (e.g., e ach  X  X ixel X  contains the probability that it belongs to a car). These maps can easily be fed into the othe r components as additional image channels. In cases where this cannot be done, it is trivial to convert the original classifier X  X  output to a log-odds ratio and use it along with features from their oth er classifiers in a simple logistic model. A standard setup has, say, two models that predict the variab les Y same input instance I . For example, I might be an image, and Y in the image, while Y by processing I to produce a set of features, and then learn a function that ma ps these features into a predicted label (and in some cases also a confidence estimat e). Cascaded Classification Models ( CCM s) is a joint classification model that shares information be tween tasks by linking component classifiers in order to leverage their relatedness. Formall y: Definition 3.1: An L -tier Cascaded Classification Model ( L -CCM ) is a cascade of classifiers of the target labels Y = { Y f indexed by  X  , indicating the  X  X ier X  of the model, where y other than y A CCM uses L copies of each component model, stacked into tiers, as depic ted in Figure 1(d). One copy of each model lies in the first tier, and learns with only t he image features,  X  Subsequent tiers of models accepts a feature vector,  X  features and additional features computed from the outputs of models in the preceeding tier. Given a novel test instance, classification is performed by predic ting the most likely (MAP) assignment to each of the variables in the final tier.
 We learn our CCM in a feed-forward manner. That is, we begin from the top level , training the independent ( f ing data. Because we assume a learning interface into each mo del, we simply supply the subset of data that has ground labels for that model to its learning fun ction. For learning each component k in each subsequent level  X  of the CCM , we first perform classification using the (  X   X  1) -tier CCM that has already been trained. From these output assignments, ea ch classifier can compute a new set of features and perform learning using the algorithm of choice for that classifier.
 For learning a CCM , we assume that we have a dataset of fully or partially annota ted instances. It is not necessary for every instance to have groundtruth labe ls for every component, and our method volunteer-annotated datasets (e.g., the LabelMe dataset [ 12] in vision or the Penn Treebank [11] in language processing), is likely to provide large amounts of heterogeneously labeled data. Our scene understanding model uses a CCM to combine various subsets of four computer vision tasks: scene categorization, multi-class image segmentat ion, object detection, and 3d reconstruction. We first introduce the notation for the target labels and then briefly describe the specifics of each component. Consider an image I . Our scene categorization classifier produces a scene label C from one of a small number of classes. Our multi-class segmentati on model produces a class label S for each of a predefined set of regions j in the image. The base object detectors produce a set of scored windows ( W window, that indicates whether or not the window contains th e object. Our last component module is monocular 3d reconstruction, which produces a depth Z Scene Categorization Our scene categorization module is a simple multi-class log istic model that classifies the entire scene into one of a small number of class es. The base model uses a 13 dimen-sional feature vector  X  ( I ) with elements based on mean and variance of RGB and YCrCb colo r channels over the entire image, plus a bias term. In the condi tional model, we include features that indicate the relative proportions of each region label (a hi stogram of S counts of the number of objects of each type detected, produc ing a final feature vector of length 26. Multiclass Image Segmentation The segmentation module aims to assign a label to each pixel. We base our model on the work of Gould et al. [7] who make use of rel ative location X  X he preference for classes to be arranged in a consistent configuration with res pect to one another (e.g., cars are often found above roads). Each image is pre-partitioned into a set { S and the pixels are labeled by assigning a class to each region S conditional Markov random field (CRF) constructed over the s uperpixels with node potentials based on appearance features and edge potentials encoding a prefe rence for smoothness.
 In our work we wish to model the relative location between det ected objects and region labels. This has the advantage of being able to encode scale, which was not possible in [7]. The right side of Figure 2 shows the relative location maps learned by our mode l. These maps model the spatial location of all classes given the location and scale of detec ted objects. Because the detection model provides probabilities for each detection, we actually use the relative location maps multiplied by the probability that each detection is a true detection. Pre liminary results showed an improvement in using these soft detections over hard (thresholded) dete ctions.
 Object Detectors Our detection module builds on the HOG detector of Dalal and T riggs [4]. For each class, the HOG detector is trained on a set of images disj oint from our datasets below. This detector is then applied to all images in our dataset with a lo w threshold that produces an overde-tection. For each image I , and each object class c , we typically find 10-100 candidate detection windows W  X  Our conditional classifier seeks to improve the accuracy of t he HOG detector by using image seg-mentation (denoted by S each region, and a categorization of the scene as a whole ( C ), to improve the results of the HOG detector. Thus, the output from other modules and the image a re combined into a feature vector  X  ( I , C, S , Z ) . A sampling of some features used are shown in Figure 2. This a ugmented feature vector is used in a logistic model as in the independent case. Both the independent and context aware logistics are regularized with a small ridge term to prevent overfitting.
 Reconstruction Module Our reconstruction module is based on the work of Saxena et al . [13]. Our Markov Random Field (MRF) approach models the 3d reconstruc tion (i.e., depths Z at each point in the image) as a function of the image features and also mode ls the relations between depths at various points in the image. For example, unless there is occ lusion, it is more likely that two nearby regions in the image would have similar depths.
 More formally, our variables are continuous, i.e., at a poin t i , the depth Z consists of two types of terms. The first terms model the depth at each point as a linear function of the local image features, and the second type models relat ionships between neighboring points, encouraging smoothness. Our conditional model includes an additional set of terms that models the depth at each point as a function of the features computed fro m an image segmentation S in the neighborhood of a point. By including this third term, our mo del benefits from the segmentation outputs in various ways. For example, a classification of gra ss implies a horizontal surface, and a classification of sky correlates with distant image points. While detection outputs might also help reconstruction, we found that most of the signal was present in the segmentation maps, and therefore dropped the detection features for simplicity. We perform experiments on two subsets of images. The first sub set DS1 contains 422 fully-labeled images of urban and rural outdoor scenes. Each image is assig ned a category ( urban , rural , water , other ). We hand label each pixel as belonging to one of: tree , road , grass , water , sky , building and foreground . The foreground class captures detectable objects, and a void class (not used during training or evaluation) allows for the small number of regio ns not fitting into one of these classes (e.g., mountain) to be ignored. This is standard practice fo r the pixel-labeling task (e.g., see [3]). We and cow ) by drawing a tight bounding box around each object. We use th is dataset to demonstrate the combining of three vision tasks: object detection, multi-c lass segmentation, and scene categorization using the models described above.
 Our much larger second dataset DS2 was assembled by combining 362 images from the DS1 dataset (including either the segmentation or detection labels, bu t not both), 296 images from the Microsoft Research Segmentation dataset [3] (labeled with segments) , 557 images from the PASCAL VOC 2005 and 2006 challenges [5] (labeled with objects), and 534 images with ground truth depth in-formation. This results in 1749 images with disjoint labeli ngs (no image contains groundtruth la-bels for more than one task). Combining these datasets resul ts in 534 reconstruction images with groundtruth depths obtained by laser range-finder (split in to 400 training and 134 test), 596 images with groundtruth detections (same 6 classes as above, split into 297 train and 299 test), and 615 with groundtruth segmentations (300 train and 315 test). This da taset demonstrates the typical situation in learning related tasks whereby it is difficult to obtain la rge fully-labeled datasets. We use this dataset to demonstrate the power of our method in leveraging the data from these three tasks to improve performance. 5.1 DS1 Dataset Experiments with the DS1 dataset were performed using 5-fold cross validation, and w e report the mean performance results across folds. We compare five tr aining/testing regimes (see Table 1). Independent learns parameters on a 0 -Tier (independent) CCM , where no information is exchanged between tasks. We compare two levels of complexity for our me thod, a 2-CCM and a 5-CCM involve using groundtruth information at every stage for tr aining and for both training and testing, respectively. Groundtruth trains a 5 -CCM using groundtruth inputs for the feature construction (i.e., as if each tier received perfect inputs from above), b ut is evaluated with real inputs. The Ideal Input experiment uses the Groundtruth model and also uses the groundtruth input to each tier at testing time . We could do this since, for this dataset, we had access to ful ly labeled groundtruth. Obviously this is not a legitimate operating mode, but does p rovide an interesting upper bound on what we might hope to achieve.
 To quantitatively evaluate our method, we consider metrics appropriate to the tasks in question. For scene categorization, we report an overall accuracy for assigning the correct scene label to an image. For segmentation, we compute a per-segment accuracy , where each segment is assigned the groundtruth label that occurs for the majority of pixels in t he region. For detection, we consider a particular detection correct if the overlap score is larger than 0.2 (overlap score equals the area of intersection divided by the area of union between the detect ed bounding box and the groundtruth). We plot precision-recall (PR) curves for detections, and re port the average precision of these curves. AP is a more stable version of the area under the PR curve.
 Our numerical results are shown in Table 1, and the correspon ding graphs are given in Figure 3. The PR curves compare the HOG detector results to our Independent results and to our 2-CCM results. It is interesting to note that a large gain was achieved by add ing the independent features to the object detectors. While the HOG score looks at only the pixels inside the target window, the other features take into account the size and location of the windo w, allowing our model to capture the fact that foreground object tend to occur in the middle of the image and at a relatively small range of scales. On top of this, we were able to gain an additional be nefit through the use of context in the CCM framework. For the categorization task, we gained 7% using t he CCM framework, and for segmentation, CCM afforded a 3% improvement in accuracy. Furthermore, for thi s task, running an additional three tiers, for a 5-CCM , produced an additional 1% improvement.
 Interestingly, the Groundtruth method performs little better than Independent for these three tasks. This shows that it is better to train the models using input fe atures that are closer to the features it will see at test time. In this way, the downstream tiers can le arn to ignore signals that the upstream tiers are bad at capturing, or even take advantage of consist ent upstream bias. Also, the Ideal Input results show that CCM s have made significant progress towards the best we can hope f or from these models. 5.2 DS2 Dataset For this dataset we combine the three subtasks of reconstruc tion, segmentation, and object detec-tion. Furthermore, as described above, the labels for our tr aining data are disjoint. We trained an Independent model and a 2-CCM on this data. Quantitatively, 2-CCM outperformed Independent on segmentation by 2% (75% vs. 73% accuracy), on detection by 0.02 (0.33 vs. 0.31 mean average precision), and on depth reconstruction by 1.3 meters (15.4 vs. 16.7 root mean squared error). Figure 4 shows example outputs from each component. The first three (top two rows) show images where all components improved over the independent model. I n the top left our detectors removed some false boat detections which were out of context and dete rmined that the watery appearance of the bottom of the car was actually foreground. Also by prov iding a sky segment, our method allowed the 3d reconstruction model to infer that those pixe ls must be very distant (red). The next two examples show similar improvement for detections of boa ts and water.
 The remaining examples show how separate tasks improve by us ing information from the others. In each example we show results from the independent model for t he task in question, the independent contextual task and the 2-CCM output. The first four examples show that our method was able to make correct detections whereas the independent model co uld not. The last examples show improvements in multi-class image segmentation. In this paper, we have presented the Cascaded Classification Models ( CCM ) method for combining a collection of state-of-the-art classifiers toward improv ing the results of each. We demonstrated our method on the task of holistic scene understanding by com bining scene categorization, object detection, multi-class segmentation and depth reconstruc tion, and improving on all. Our results are consistent with other contemporary research, including th e work of Hoiem et al. [8], which uses different components and a smaller number of object classes .
 Importantly, our framework is very general and can be applie d to a number of machine learning domains. This result provides hope that we can improve by com bining our complex models in classifiers have been used extensively within a particular t ask, and our results suggest that this should generalize to work between tasks. In addition, we showed tha t CCM s can benefit from the cascade even with disjoint training data, e.g., no images containin g labels for more than one subtask. In our experiments, we passed relatively few features betwe en the tasks. Due to the homogeneity of our data, many of the features carried the same signal (e.g., a high probability of an ocean scene is a surrogate for a large portion of the image containing water r egions). For larger, more heterogeneous datasets, including more features may improve performance . In addition, larger datasets will help prevent the overfitting that we experienced when trying to in clude a large number of features. It is an open question how deep a CCM is appropriate in a given scenario. Overfitting is anticipat ed for very deep cascades. Furthermore, because of limits in th e context signal, we cannot expect to get unlimited improvements. Further exploration of cases w here this combination is appropriate is an important future direction. Another exciting avenue is t he idea of feeding back information from focus its effort on fixing certain error modes, or allow the ea rlier classifiers to ignore mistakes that do not hurt  X  X ownstream. X  This also should allow components with little training data to optimize their results to be most beneficial to other modules, while wo rrying less about their own task. Acknowledgements This work was supported by the DARPA Transfer Learning progr am under con-tract number FA8750-05-2-0249 and the Multidisciplinary U niversity Research Initiative (MURI), contract number N000140710747, managed by the Office of Nava l Research.

