 The need to analyze structured data for various business intelli-gence applications such as customer churn analysis, social net-work analysis, etc. is well known. However, the potential size to which such data will scale in future will make solutions that re-volve around data warehouses hard to scale. We begin by present-ing a business case that prompted us to look at building a distributed analytics platform that is leveraging the MapReduce framework pi-oneered by Google. We present the results of the study and high-light issues with the current structured data access techniques for MapReduce platforms. Finally, we present a distributed and scal-able data platform that leverages Apache Hadoop to enable busi-ness analysts to seamlessly query archived data along with data stored in the warehouse.
 H.2.4 [ Database Management ]: Systems X  query processing, re-lational databases ; H.3.3 [ Information Search and Retrieval ]: Query formulation, search process Algorithms, Design, Performance Massive Data Analytics, Data Archiving, Hadoop
Telecommunication companies generate large amounts of data such as call detail records (CDRs) , which describes the calls that traverse the telecommunication networks, network data , which de-scribes the state of the hardware and software components in the network, and customer data that describes the telecommunication customers. Traditionally, data warehouses have been used to man-age the data generated at the telcos. However, the warehouses and solutions built around them will be unable to provide reasonable re-sponse times in handling expanding data volumes mostly brought on by the increase in customer base and proportional increase in call traffic generated over the telecom network. Existing data man-agement solutions cannot manage such volumes and hence most of the data is simply archived and stored in tapes without being used in any analysis solutions. There is a clear need for data man-agement solutions that go beyond th e traditional warehousing sys-tem to support advanced analytics for generating models for churn management, better customer segmentation for personalized cam-paign creation, fraud detection, network usage analysis etc. In this paper, we present an analytics platform designed to help telecom customers derive useful and actionable insights over the soon-to-be Petabyte scale data that will become available in their data centers.
We motivate the need for an large scale data management and analytics solution for telecom in the context of a leading Indian telephone service provider. The said service provider adds nearly 2 million customers each m onth. Clearly, they a re looking at massive amounts of data coming in the form of CDRs, transaction data aris-ing from service requests and also the huge amounts of customer demographic data. Specifically, nearly a billion CDRs per day i.e. about a terabyte of data is generated in their call network. The com-pany already has a data warehousing solution powered by shared-nothing parallel IBM DB2[1] implementation. It was primarily used for reporting and billing. The business intelligence deriva-tion was done by allowing analysts to fire adhoc cubing queries for browsing along interesting dimensions. However, no advanced analytics applications were taping into the warehouse even though many potential use cases for such analytics existed. Below we high-light one such case which becomes challenging when we start con-sidering the impending larger volumes of data.
 Network Infrastructure Usage Dashboard: The telecom oper-ator was interested in building a dashboard that would allow the analysts and architects to understand the traffic flowing through the network along various dimensions of interest. As previously men-tioned, the traffic is captured using CDRs whose volume runs into a terabyte per day. The key dimension of interest to the customer was the usage of a cell site. A cell site is a term used for denoting a site where antennas and electronic communications equipment are placed on a radio mast or tower to create a cell in a network. Mon-itoring of traffic flowing through the sites would help the customer decide regions where there is high network congestion. Further-more, the telecom regulatory authority also imposes fines for ser-vice providers whose networks are highly congested. Adding new cell sites is the obvious solution for congestion reduction. However, each new site costs approximately 20000 USD to setup. Therefore, determining the right spot for setting up the cell site and measuring the potential traffic flowing through the site will allow the company to measure the return on investment. Other uses of the dashboard include determining the most used cell site for each customer, iden-tifying whether users are mostly making within cell site calls and for cell sites in rural areas identifying the source of traffic i.e. local versus routed calls.
 Challenges: Given the huge and ever growing customer base and large call volumes, the above mentioned task cannot be handled by solutions designed to exploit the warehouse. A typical CDR gen-erated by switches on the operators network contains 81 attributes out of which about 22 attributes give useful information about the subscriber, the network, billing details and call details. The dash-board had to create various aggregates around combinations of the 22 attributes for helping the analysts. Furthermore, it had to be projected into future based on trends observed in the past. For busi-ness purposes, company only needed 3 months of CDRs, and hence adding additional capacity on the warehouse was not economical. A cheaper alternative hence becomes essential.

An important constraint of the solution was the need that analysts often wanted to run ad-hoc queries to check KPIs of interest that might not be part of the periodic reports. This was being facilitated by allowing the analysts to create and execute custom SQL queries over the warehouse. Hence, this warranted that any solution we create must have support for SQL or SQL-like high level query language.

Based on the above, a solution that scales to ever increasing data volumes but is seamless to the existing users was needed. This became our key design motivator -leverage the existing data ware-house as much as possible and ensure least amount of learning .We therefore chose to build our solution using the open-source mas-sive data management solution, Apache Hadoop [2], which runs on commodity hardware, is known to scale easily to Petabyte size and has several SQL-like query language add-ons.
Our endeavour was to develop a non-disruptive system that can extend the capab ilities of the existing data warehouse by allowing the business to make the archived data available for use on com-modity hardware. Moreover, we wanted to enable the data to be used for building models that could be used for customer segmen-tation for better marketing, attrition modeling for preventing churn, fraud detection, etc apart for the above mentioned case of network usage monitoring.

Since our eventual goal was to enable the telecom client to deal with very large data sizes that are easily dealt by MapReduce based systems, we decided to build our proposed solution by leveraging the MapReduce framework. MapReduce framework developed by Google has been identified as a fit platform for performing analyz-ing data in the Petabyte scale. The map reduce framework provides a simple model to write distributed programs to be run over a large number of (cheap) machines [3]. At heart, MapReduce is a very simple dataflow programming model that passes data items through simple user-written code fragments. MapReduce programs start with a large datafile that is broken into contiguous pieces called  X  X plits X . Each split is converted via user-defined parsing code into key-value pairs that are sent to a Map module, which invokes a user supplied Map function on each pair, producing a new key and list of output values. Each (key, output_list) pair is passed to a Reduce module (possibly on another machine) that gathers them together, assembles them into groups by key, and then calls a user-supplied Reduce function. Below we briefly explain the various components that of our massive data storage and retrieval solution.
 Hadoop: Apache Hadoop [2] is an open source implementation of Google X  X  MapReduce framework. It supports a highly fault tol-erant massive file storage and management system called Hadoop Distributed File System (HDFS). Each file in HDFS is split into a number of blocks. Fault tolerance is achieved by replicating these data blocks over a number of node s. Scalability is achieved by the master-slave architecture where a master node keeps track of slave nodes which can number in thousands and distributes the storage and execution tasks as needed.
 Jaql: Jaql [6] is query language for processing structured or semi-structured data based on Java Script Object Notification (JSON) [7] data model. In JSON data model, data is represented as an array of objects. Objects contain a series of name:value pairs where the value can be atomic or nested type. Jaql is compiled to a series of map-reduce tasks which are executed over Hadoop. Jaql has some of the best features of SQL and XQuery making it an easy to use yet powerful language to query JSON data. Jaql provides SQL like grouping and aggregation functions frequently used for analytical queries.
 Hive: Hive [5] is a data-warehousing tool developed at Facebook. It uses database schema to convert SQL like queries into corre-sponding map-reduce jobs which are executed on the underlying Hadoop system.
To decide on the solution architecture to be used for massive ana-lytics, we designed queries that reflected the reporting needs of the business analysts and were likely to be issued over the data stored and/or the models (customer profiles, loyalty, churn likelihood, seg-mentation etc) mined from them. The analytics platform was setup over a 4 node Hadoop[2] cluster built using Blade Servers with two 3Ghz Xeon processors having 4GB memory and 200 GB SATA drives. The machines ran on Red Hat Linux 5.2. The software stack comprised of Hadoop 0.18.3 with HDFS, Jaql 0.4 and Hive. The system also contains a staging server containing 50 million syntheic CDRs created from an initial masked set of 20000 CDRs. The dashboard itself was built using IBM COGNOS dashboards residing on another machine that also ran IBM DB2.

Table 1 shows 3 sample queries, one for each category. As we can see from Table 1 that query Q1 is a simple selection query which is likely to output lots of call records. Q2 and Q3 are aggre-gation queries. The query Q2 has simple aggregation over call_duration whereas Q3 has union of two sub-queries. We used the queries to compare performances of four structured data access mechanisms. Below, we describe the different data querying mechanisms we setup to query CDR data. They are:
Figure 1 shows performance of various queries Q1 over four sys-tems with varying number of CDRs from 2 million to 10 million. In this case, relational database works very well. This can be ex-plained with the existence of corresponding indices on the database. In Q1, map output was large number of CDR records (e.g., 6 mil-lion out of 10 million) satisfying the select condition. Thus each map has to write large amount of data to the disk. With Figure 1, we can conclude that raw MapReduce is not optimal when large amount of data is required as output. Hive performed better job of optimizing performance using suitable numbers of map and re-duce jobs. For MapReduce the number of map and reduce jobs were 9 and 1 (default setting) whereas in Hive it was 35 and 0. Thus, setting the optimal number of map and reduce jobs is very important. We obtained a lower response time of 130seconds for our raw MapReduce implementation for Q1 by forcibly setting the number of reducers to zero. This analysis makes it clear that excut-ing queries over Hadoop will be difficult for lay users even in the presence of high level query languages.
Figure 2 shows performance of Q2 for the four alternative sys-tems. In this case Hive and MapReduce systems perform better than SQL and Jaql. This shows where aggregation is required over attributes (in this case over caller_number), distributed processing helps in getting better performance.
Performance results for Q3, shown in Figure 3, are similar to that for Q2 except that the raw MapReduce program performs better than Hive. This query involves union of two sub-query results, and Hive is known to have poor join implementation. For all three queries Jaql performs worst. This is due to the fact that Jaql is designed to query semi-structured data in XML like format. So the intermediate query results have to be written into JSON format before it can be used by the final query thereby increasing the time to answer the query.
The business requirements analysis done by us has convinced us about the value of building our massive telecom data analytics so-lution over Hadoop. Evaluations done by us have shown that no single structured data management approach can solve the various needs of the potential users of our system. Furthermore, we want to make the new combined system as seamless as possible for busi-ness users who are used to existing data management tools provided over the warehouse.

The above needs led to our idea of building a solution that will allow the telecom client to move data into the Hadoop cluster for archival instead of to the tape system as is done currently. Further-more, we plan to enable the users to seamlessly query the entire col-lection of data -that residing inside the warehouse and archived on Hadoop using a single query interface. This requires us to replicate the telecom data model, known as the Shared Information/Data Model (SID) on Hadoop as well as intellig ently splitting t he incom-ing query into two parts, one that would run on the warehouse and the other that runs on Hadoop. Apart from data retrieval support, the system will also provide libraries that can be invoked to mine and build segmentation, loyalty models etc. We will use the models to answer analytic queries that can also be fired from the common interface by user. The proposed architecture of our solution and the flow of information through the system is illustrated in Figure 4. The left hand side of the figure shows how the archived data will be integrated with an existing data warehouse. A key component to achieve this is the SQL-to-MapReduce Adapter . On the right hand side we give a functional view of the adapter X  X  functions. Specif-ically, the adapter takes an input SQL query, checks whether data stored on the Hadoop based archive is needed and accordingly cre-ates a new SQL query for executing on the warehouse and a Hive query to run on HDFS. Currently we are working on automatic query partition adaptor and also techniques for improving the re-sponse times of various query answering systems over Hadoop.
A comparison study of Hadoop with various distributed database is done in [4]. Authors show that Hadoop outperforms parallel databases in data loading and in queries involving aggregation. Our work is complementary to that effort. Efficiently building aggre-gates on stored data sets has been widely studied [8, 9]. Pre-computation of such aggregates is only effective when a repeating set of queries is run at regular intervals. But when a workload can-not be anticipated in advance, it is difficult to decide what to pre-compute. Our solution does not suffer from this problem as we do not keep any views but the entire dataset which is effectively stored and retrieved from HDFS. Including two different DBMS designs in a single system has been studied before in data mirrors [10]. However, the goal of data mirrors was to achieve better query per-formance than could be achieved by either of the two underlying systems alone in a warehouse environment. In contrast, our goal is to achieve good performance while answering a query over two widely differing data storage mechanisms.
We present an analytics platform for extracting actionable in-sights from the massive amounts of data arriving at one of the largest telecom service provider s in India. The platform is based on the open-source version of MapReduce called Hadoop and thereby enables easy scalability in terms of hardware resources needed in future. Our solution is complementary to the existing data ware-house implementation existing at the client. We believe that is the best model for any such MapReduce based solution irrespective of the industry vertical where the solution is deployed.
