 1. Introduction
This paper examines the relationship between two extensions to Salton X  X  vector space model (VSM) of 1971 ) and latent semantic indexing (LSI) ( Deerwester, Dumais, Landauer, Furnas, &amp; Harshman, 1990 ). define a projection of documents onto a low-dimensional subspace of the corpus. Rocchio X  X  model seeks uments onto the dimensions that capture the maximal amount of the original variance. This paper explicates retrieval.

We ask: subject to admittedly narrow assumptions, under which conditions can LSI X  X  variance-based projec-between these projection methods is complex. But a clear pattern emerges from the research presented here: the conditions under which one projection outperforms the other depends on how we understand the retrieval relevant documents. But if we imagine retrieval as a regression problem, where our goal is to estimate the degree of relevance of a document, discovering a low-dimensional orthogonal subspace is desirable. optimality as it pertains to the problem of data projection for IR in a theoretical sense. 2. Projection methods in IR
Under the vector space model, documents and queries are represented as vectors in the vector space the cosine of the angle between these vectors: simply the inner product between q and d . Representing our corpus as the n p document-term matrix X with documents on the rows and terms on the columns, we have the basic vector space model of relevance for the query q represented by query vector q : each document to a query by projecting the document from the p -dimensional term space to a single dimen-sion. The projection in the classic VSM is defined by the query vector q .

Since its inception, the VSM has seen criticism. Documents and queries, a common complaint runs, are underdetermined in a way that the VSM cannot account for ( Wong, Ziarko, &amp; Wong, 1985 ). Although a query may lack a particular term, that does not imply that the term is not associated with the information need that generated the query. Thus queries about cars might be satisfied by documents about automobiles , despite the VSMs failure to recognize these terms X  latent similarity.

Numerous extensions to the VSM have been proposed to remedy this problem. While the details of the to compute similarity relationships. Two of the most widely studied VSM extensions are Rocchio relevance
Buckley, 1997 ). Secondly, Rocchio X  X  model and LSI both spawned further elaborations on Salton X  X  original model. Thus by studying these models rigorously, we may gain insight into a wide body of research.
Relevance feedback involves query expansion. That is, Rocchio X  X  model estimates an optimal query in basic vector space model.
 discriminant, while LSI derives much of its motivation and methodology from principal component analysis. how does this differ from LSI X  X  optimality? 3. Least-squares projections for IR
This section reviews Rocchio relevance feedback and latent semantic indexing. The discussion concerns each model X  X  basis in least-squares optimization. The method of least-squares is an optimization technique for estimating a function y  X  f  X  x  X  ( Plackett, 1972 ). Given a sample of data x y
P standing precisely how each approach to IR  X  relevance feedback and LSI  X  relates to the method of least-squares will lend us a strong theoretical basis for understanding their interrelationship. 3.1. Rocchio X  X  model of relevance feedback In his work on relevance feedback ( Rocchio, 1971 ), Rocchio posited a framework for improving queries.
To guide his research, Rocchio proposed the notion of the  X  X  X ptimal X  X  query. That is, given a corpus D and some query q , for which a subset D r of the corpus is relevant and the remainder D defines the optimal query q opt : where l r is the mean vector of D r and l n is the mean vector of D
Eq. (3) is equivalent to the Fisher linear discriminant under the assumption that the covariance matrix of space that maximizes the squared distance between the projections of the class means l while minimizing the variance within classes.
 discriminant.

To show the least-squares optimality of the linear discriminant, our discussion follows Anderson (2003, p. maximizes
Expanding the numerator we have
To maximize the numerator, holding the denominator constant, let k be a Lagrange multiplier. We thus need to maximize
Setting the derivatives with respect to w equal to zero, we obtain where  X  l r l n  X  w is some scalar, say s . We then have Eq. (3) is equal to the linear discriminant under the condition of identity covariance.
Under Rocchio X  X  formulation, then, retrieval is optimized when we replace q with q sense) the means of relevant and non-relevant documents. If we assume that documents are generated by mul-documents along the vector that minimizes the probability of misclassifying a relevant document as non-we understand retrieval as a two-class classification problem. 3.2. Latent semantic indexing
Whereas Rocchio relevance feedback is concerned with a projection that is optimal with respect to mean the vector space model.

The latent semantic indexing model derives a projection matrix for documents by the singular value decom-position (SVD) of the document-term matrix X : where U and V are orthonormal matrices containing the eigenvectors of XX are given in Golub and Van Loan (1996) .

Assuming that those eigenvectors corresponding to small eigenvalues constitute random error, LSI operates by defining the projection matrix P LSI : where V k contains the first k columns of V and the diagonal R values. Prior to retrieval, a reduced rank approximation of the original matrix is obtained by and each query is projected into the reduced space by and query-document similarity is simply the inner product between the projected documents and queries:
The appeal of LSI lies in the notion that our observed data (term occurrences in documents) are subject to
PCA will frame our discussion of LSI X  X  basis in least-squares optimization. 3.2.1. Principal components and least-squares
Principal component analysis can be described as a solution to several optimization problems. In the con-
X that minimizes which is simply the squared error zero-dimensional representation of the data. To advance to one dimension we write x mean where a i gives the distance of the point from the mean. We find an optimal set of coefficients a
Setting k e k X  1, we partially differentiate with respect to a our projection: we desire. Letting k be a Lagrange multiplier subject to k e k X  1 we differentiate with respect to e and set the derivative to zero, obtaining principal component of X .
 ing X onto the first k eigenvectors of its covariance matrix.
 projection matrix in terms of the first k eigenvectors of X In other words, they are uncorrelated. Second, the LSI projection matrix P approximation of X T X , the term-term co-occurence matrix.

From a practical standpoint, LSI provides our model with an approximation of the term relationships. If it the best approximation in this context. 4. Comparing projection methods
We have shown that Rocchio relevance feedback and LSI alter the standard vector space model in a way of optimality with respect to retrieval?
To begin our comparison, assume that we have two multivariate normal distributions: D relevant documents is p  X  d r  X  N  X  l r ; R r  X  and the density for non-relevant documents is p  X  d evant or non-relevant documents are equal.

Consider Fig. 1 , which shows 95% confidence regions for different arrangements of two bivariate normal non-relevant documents are centered around the origin with R  X  I covariance between features for relevant documents is 0.5 and 0.9, respectively.
Altering the orientation of the distributions for relevant and non-relevant documents affects the relative relevant documents will be easy to recognize. Conversely, if the distributions are similar, discrimination covariance. Further, we assume that for any orientation of relevant and non-relevant distributions, we may of relevant and non-relevant documents. We examine the effect on each projection method introduced above as the orientation of relevant documents shifts. 4.1. IR as classification
The upper left panel of Fig. 2 shows 95% confidence regions where relevant documents have l T and the term covariance is 0.9. In the upper right panel, two vectors are superimposed on the confidence dimension. In contrast the dotted line points in the direction of the Rocchio optimal query.
Thus the Rocchio optimal query is the vector that passes through the mean vectors of each population. Nor-malizing this to unit length we have u T opt  X  X  10  X  , which defines the projection of each mean vector: distributed N  X  m i ; s i  X  where m i is defined in Eq. (20) and s normally distributed with m non  X  l T rel u opt  X  4 and s 2  X  1.

On the other hand, the projection defined by LSI is based on the pooled covariance matrix of the documents:
To find the direction in term space that captures maximal variance among the relevant documents (as is the relevant documents. We find the eigenvalues and eigenvectors of R by
We find the characteristic polynomial of R r by
Solving the characteristic equation gives the principal eigenvalue k v  X  X  0 : 707 0 : 707  X  .

Recalling the projection matrix used by LSI (Eq. (11) ), we find the mean vector for relevant documents in the space spanned by the first eigenvector: bution (relevant and non-relevant) by relevant documents, we have variance of 0.476. How can we characterize the projections entailed by Rocchio X  X  model and local LSI X  X ?
Because the two states of nature (relevance versus non-relevance) are for our purposes mutually exclusive, the probability of predicting non-relevant when a document is relevant. More formally we have error when we project onto the Rocchio optimal query and the probability of error when we project onto documents. Thus at the extreme right of the plot, non-relevant documents are centered at the origin, while projection and the probability of error under LSI X  X  projection.

When the mean vectors of both document populations are equal, neither projection is successful at sepa-tions yield probability of error near 0.5 (equivalent to a random guess).

However, as the mean vector of relevant documents tends towards a higher score on Feature 1, discrimi-nating between relevant and non-relevant documents becomes more successful. As the means become more distant along Feature 1, both projection methods X  probability of error decreases. However, the probability the principal eigenvector.

A more complex picture arises if we alter the mean of relevant documents with respect to both features
Thus the top panels deal only with mean vectors that have positive coordinates (as is most common in IR), error in the context of this model.

We can understand the inverse relationship between LSI X  X  accuracy and Rocchio X  X  by consulting Fig. 5 . The upper panels show the orientation of relevant and non-relevant populations in the cases that correspond to mal document arrangement, we see, is the one in which the Rocchio optimal query and the principal eigen-vector point in the same direction. On the other hand, LSI does worst when the dominant eigenvector and the Rocchio optimal query are orthogonal. Under this arrangement, we can see, both distributions X  X elevant and non-relevant X  X re collapsed onto each other when we perform the LSI projection. Meanwhile, under this them to have very low variance on its projection (and concomitantly high accuracy).
It is worth noting parenthetically that the minima of Rocchio X  X  error probability in Fig. 4 correspond to those arrangements in which the optimal query is identical to the true Fisher discriminant, as opposed to the simplified one used in the common Rocchio method. 4.2. IR as regression
If the Rocchio optimal query always provides a lower risk of misclassification, what use is the projection able insofar as it reduces the mean squared error (MSE) of the estimated regression model.
The standard linear regression model describes a real-valued response variable Y as a linear function of p predictor variables X j for j  X  1 ; ... ; p : that each predictor variable x j makes to the response Y . ^ b for these unknown parameters. The standard approach uses the method of least-squares to find the estima-tor ^ b that minimizes the squared error of the model: where ^ y i  X  x T i ^ b . It is easily shown that we minimize Eq. (26) by finding form: optimal query. Altering the notation of Eq. (2) we have
Thus the parameter b is unknown and query-specific.
LSI X  X  contribution to retrieval X  X rojecting documents onto k maximally expressive dimensions  X  can be predictors. 4.3. Colinearity in linear regression
Though independence among the predictor variables x j is not requisite in the standard regression model, problems in estimation arise if the predictors show marked covariance. The general term for this mated ^ b .
 has overfitted the training data. That is, a highly variable a low sum of squared error. But faced with new data, the same model is likely to perform poorly.
A clear way to see the relation between estimator variance and overfitting arises in the context of model selection by analysis of the mean squared error (MSE). Given a parameter h and an estimator for it MSE:
In other words, the mean squared error of a regression model is simply the expected squared difference be-tween y i and ^ y i .
 two parts: the bias and the variance of the estimator as in Eq. (30) variance: given a slightly different training set, it X  X  fitted
Colinear predictors lead to high model variance in regression, with concomitantly high MSE. This fact is evident from the definition of the covariance matrix for ^ To derive Eq. (31) recall the definition of ^ b from Eq. (27) . Further, let A be the matrix
Substituting A we have so-called standardized regression model). If we assume for now that p  X  2 then we have approaches infinity.
 of the predictors. Using the singular value decomposition we have where V is the p p matrix containing the eigenvectors of X sion parameters depends directly on the magnitude of the corresponding eigenvalues of the predictor variables: of the eigenvalues, high colinearity leads to high model variance, and by extension, high MSE.
The standard vector space model uses the complete term-document matrix to inform similarity measure-using the classic vector model we pay a price for assuming term independence in the form of increased MSE. 4.4. Biased regression
Due to inter-term correlations, the standard vector space model incurs the high variance that accompanies minimizing predictor variable correlation.
 member of a family of regression models that attempt to reduce model MSE by introducing a small amount of ponents. If we truncate the model, using only k components where k &lt; p , our estimated biased, but is likely to have far lower variance than a model fitted from the untransformed data. 4.4.1. The standardized regression model It is convenient to describe PCR in the context of the standardized regression model referred to above. Under the standardized model we transform both the predictors and the response variable
Here r y and r k are the standard deviations of Y and X k the original data by the multiplication r y r 4.4.2. Principal component regression
The following explication draws from Jolliffe, 2002, Section 8.1 .If X is the n p matrix of standardized predictors, we find the values of the principal components for each observation: where V contains the right singular vectors of X (i.e. the eigenvectors of X define the regression model by
Because V is orthonormal we can rewrite X b as XVV T b  X  Z c where
Since Z is column orthogonal where R is the diagonal matrix of singular values.

The advantage of using Eq. (33) to estimate our model parameters lies in the ability to produce a model of reduced dimensionality. Above we noted that small eigenvalues contribute large amounts of variance to the model: of X .

The decrease in variance comes at the expense of introducing bias into our model. This is the case because where v k is the k th column of V . Since the expected value of onto the k uncorrelated components with maximum variance.

While Rocchio X  X  optimal query defines the one-dimensional projection in term space that minimizes the probability of document misclassification, LSI offers a k -dimensional basis that lowers the MSE of the IR uncorrelated (and due to dimensionality reduction, statistically stable) basis for estimation. 4.5. An example nario where strong colinearity is present in a regression model, next showing how dimensionality reduction can improve a model, as this is at the heart of latent semantic indexing.

Let us assume that we have data generated by a multivariate normal distribution with three-dimensional mean vector l  X  0 and 3 3 covariance matrix R containing 1 on the main diagonal, with 0.9 on all off-diag-onals. Finally, we assume that there is a response variable Y that has the form where N  X  0 ; 1  X  . Thus we know in advance that our true regression parameters are and the error factor  X  r 2  X  1.
 lem with colinearity in regression).
 If we draw N , say 100, documents from this distribution, along with 100 relevance scores Y we can use Eq. (27) to calculate ^ b , the least-squares estimate of b .
 From a statistical standpoint we would like to know, how good our estimate of the regression function is.
Given a new document x i , how close to the true relevance score y give us to the tools to answer this question and to address the limitations of our model.
To inform this discussion, I generated a 100 3 matrix X from the distribution described above. I also gen-erated 100 corresponding y  X  X . Using these data, I fit a regression model, M generated a second model, M 2 , by using only the first two principal components to perform PCR.
The model coefficients  X  the true parameters and our estimates  X  appear in Table 1 . Neither model found and response vector and applied each model to the unseen data. The sum of squared error on unseen data for the bias introduced by discarding the weakest principal component was small, only 0.234. Thus we MSE  X  PCR  X  X  0 : 234 2  X  55 : 346  X  55 : 404.

To gain a more thorough understanding of the behavior of each model  X  the full regression and the reduced rank PCR  X  I repeated the process described above, generating 1000 training matrices (each 100 3) and 1000 response vector) to estimate accuracy on new data.
 while the reduced rank model X  X  SSE was 1939.767. This is not an overwhelming reduction in error. But a of the reduced model is lower than that of the full-rank model.

Fig. 6 can help us understand why a truncated model can outperform a model trained with more informa-tion. The figure shows the MSEs under each model. MSEs for the reduced rank PCR model appear in gray, with the full model in white. The PCR models tend to have lower MSE than the standard models (a t -test on the equality of means for the MSE under each model yielded p 0 : 01). While the full model is unbiased, it model admits a small amount of bias. But it does so with a large reduction in undesirable variance.
Admittedly, this example operates at a high level of abstraction. What does it show us about LSI in par-that occur in documents. When we use the classic vector space model we assume that these terms are uncor-cation can impede prediction.

Mathematically this implies that the variance of our predictions will drop if we use LSI X  X  low-dimensional approximation. More heuristically, it means that LSI assuages some of the brittleness of keyword matching. high variance in the sense that numerous words can have many meanings and concepts can be expressed by iance of the vector space model by discarding what is hopefully redundant information. 5. Discussion tion onto the first LSI dimension. But LSI X  X  dimensionality reduction was able to reduce the mean squared
While the difference between LSI and Rocchio appears clearly when we bring them to bear on classification real implementations of relevance feedback are at best semi-supervised.

The Rocchio optimal query is determined completely by the mean vectors of the relevant and non-relevant documents. On the other hand, LSI derives dimensions that reflect the covariance structure of the corpus.
The Rocchio optimal query separates relevant and non-relevant mean vectors as widely as possible. The matrix V k in LSI gives an estimate of the term co-occurrence matrix X than the overdetermined sample.
 ory and motivations. Both methods assume that artifacts of human language (e.g. queries and documents) are redundant and ambiguous. To remedy this, both methods attempt to learn from the corpus of documents in and LSI, this learning is framed as a least-squares optimization problem. 6. Conclusion
This study has pursued the notions of optimality that inform latent semantic indexing and Rocchio rele-vance feedback. Both techniques are extensions of the classic vector space IR model, and both are based not simply one of query expansion versus document expansion. We may understand Rocchio relevance feed-here). On the other hand, LSI provides a k -dimensional uncorrelated basis that represents documents with minimal error. Thus, in the presence of high inter-term covariance, LSI X  X  dimensionality reduction allows us to construct a linear model such as a regression with lower variance than we can in the observed space.
The notions of optimality that inform LSI and Rocchio X  X  model are well studied. But they are not without ers. How do projection methods such as probabilistic LSA ( Hofmann, 1999 ) or independent component anal-research.

Additionally, in future work I hope to complement the theoretical approach taken in this paper with exper-and principal components of the data. The advantages that Rocchio X  X  method saw with respect to classifica-ronment? On the other hand, it will be of interest to learn whether term-term covariances are problematic be sure, will complicate the understanding of projection optimality outlined here. Acknowledgement
The author wishes to thank Robert M. Losee and Jonathan Elsas, who provided helpful comments on an revision, for which I was very grateful.
 References
