 The representation of a knowledge graph ( KG ) in a latent space re-cently has attracted more and more attention. To this end, some proposed models (e.g., TransE) embed entities and relations of a KG into a  X  X oint X  vector space by optimizing a global loss func-tion which ensures the scores of positive triplets are higher than negative ones. We notice that these models always regard all enti-ties and relations in a same manner and ignore their (un)certainties. In fact, different entities and relations may contain different cer-tainties, which makes identical certainty insufficient for model-ing. Therefore, this paper switches to density-based embedding and propose KG2E for explicitly modeling the certainty of entities and relations, which learn the representations of KGs in the space of multi-dimensional Gaussian distributions. Each entity/relation is represented by a Gaussian distribution, where the mean denotes its position and the covariance (currently with diagonal covariance) can properly represent its certainty. In addition, compared with the symmetric measures used in point-based methods, we employ the KL-divergence for scoring triplets, which is a natural asymme-try function for effectively modeling multiple types of relations. We have conducted extensive experiments on link prediction and triplet classification with multiple benchmark datasets (WordNet and Freebase). Our experimental results demonstrate that our method can effectively model the (un)certainties of entities and relations in a KG, and it significantly outperforms state-of-the-art methods (in-cluding TransH and TransR).
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods X  Knowledge Graph Representation Distributed Representation, Gaussian Embedding, Knowledge Graph
Knowledge representation and reasoning ( KR&amp;R ) is a funda-mental issue for artificial intelligence ( AI ) and knowledge man-c  X  agement ( KM ) [6]. To fulfill this aim, recent researchers devote to knowledge graph ( KG ) which provides an effective represent mechanism for knowledge and has become useful resources to sup-port many intelligent applications, such as expert system [10], web search [24][27] and question answering [31][2][11]. Commonly, a KG, such as Freebase 1 [1], NELL 2 [7] or WordNet 3 [18], describes knowledge as many relational data and represent them as inter-linked subject-property-object ( SPO ) triplet facts. Usually, a triplet fact ( head entity, relation, tail entity ) (denoted as ( h, r, t )) consists of two entities and a relation between them.

With the expansion of domains and the increase of data size, representation of KGs is required to support generalization, robust inference and other desirable functionalities [23]. However, tradi-tional representations of KGs are based on a ( hard ) symbolic logic representation framework, which heavily rely on the learned logic inference rules for knowledge reasoning [15][29]. Thus, they lack certain ability for supporting numerical computation in continuous spaces, and cannot be effectively extended to large-scale KGs, such as Freebase. To address this problem, a new approach based on rep-resentation learning was recently proposed by attempting to embed a KG into a low-dimensional continuous vector space that preserves certain properties of the original graph [19][13][5]. The represen-tations in ( soft ) latent space could act as a supplement for the sym-bolic representations, which are learned by optimizing a global loss function that involves all entities and relations in the entire graph. As a result, each entity and relation encodes global KG informa-tion through mutual effects and restrictions with the others. These embedding representations can be used in many applications, for example, verifying the correctness of one triplet fact, predicting the relations between two entities and reasoning about the implications among relations.

The promising methods (introduced in Section  X  X elated Work X ) usually represent an entity as an n -dimensional vector h (or t ) and regard it as a  X  X oint X  in low-dimensional spaces. A relation in KGs is represented as an operation between two  X  X oints X  (e.g., trans-lation as a vector [4], linear transformation as a matrix [20], and mixed operation [25]). In this way, a relation-dependent scoring function f r ( h,t ) , such as  X  X | h + r  X  t || ` 1 / 2 , is defined to measure the correctness of the fact ( h,r,t ) in the embedding space. The embedding of a KG is learned to ensure that the score of a positive triplet (e.g., ( h,r,t )) is higher (or lower) than that of a correspond-ing (mostly corrupted) negative triplet (e.g., ( h 0 ,r,t )). Based on this paradigm, multiple models are proposed, such as TransE [4], TransH [30], and TransR [16]. Although these mod-els are proved to be effective in many scenarios, we notice that www.freebase.com/ www.rtw.ml.cmu.edu/rtw/ www.wordnet.princeton.edu/ different entities and relations often share the same margin when separating a positive triplet and its corresponding negative triplet, and the (un)certainties of entities and relations in KGs are totally neglected. In fact, different entities and relations may contain dif-ferent certainties, which makes identical certainty insufficient for modeling. We consider that the (un)certainty of one entity/relation represents the confidence for indicating its semantic when scoring a triplet as context. For example, the certainty of relation spouse is obviously larger than nationality when inferring a person (e.g., for predicting Hillary Clinton , we may have more confidence to know who is she when knowing her husband ( spouse ) is Bill Clinton than knowing she was born on ( nationality ) USA .). Thus, we argue that if we set a larger margin for separating ( spouse ) related positive and negative triplets in the embedding model, we could obtain bet-ter performances.

In this paper, we argue that the (un)certainties in KGs could be influenced by multiple factors, including imbalance between the re-lation X  X  head and tail, different number of the linked triplets for dif-ferent relations and entities, and the ambiguous of the relations and so on. For example, we hypothesize that an entity containing fewer triplets has more uncertainty, and a relation linking more triplets with more complex contexts has more uncertainty as well. We es-pecially perform statistics on the entities and relations in Freebase and Figure 1 shows the statistics of the person type in the people domain. In Figure 1, the upper left indicates the diversity numbers of triplets contained in 6 randomly sampled entities; the upper right expresses the distribution of the ratios of entity numbers in the head and tail parts with some typical relations; and the bottom shows the distribution of triplet numbers with multiple relations. This fig-ure illustrates that popular entities (e.g., the politician Hillary Clin-ton ) contain more relations and facts than unpopular ones (e.g., the writer Murray Silverstein ). Furthermore, different parts of relations can contain very different numbers of entities, (such as the head part and tail part in gender or nationality ), and, high-frequency re-lations (e.g., nationality ) link more entity pairs than low-frequency ones (e.g., religion ). It indicates that the variations of uncertainty with different entities and relations in a KG is vary enormously, and it is desirable to consider this problem when learning the represen-tations of a KG in a latent space.
To address the aforementioned problem, this paper proposes a new density-based embedding method, KG2E 5 , to model KGs and
We use the dump data released on 2014-06-29. This name has two meanings. The first indicates mapping K nowledge G raph to E mbedding and the second indicates the rep-resentation of a KG with G aussian E mbedding. learn the representations in the space of multi-dimensional Gaus-sian distributions. Inspired by [28], our approach models each entity and relation with a multi-dimensional Gaussian distribution N (  X  ,  X  ) (currently with diagonal covariance for computing effi-ciency). The mean vector of such multi-dimensional Gaussian dis-tribution indicates its position, and the covariance matrix indicates the corresponding (un)certainty which impacts on others, as shown in Figure 2. Similar to previous methods, we also design a scor-ing function f r ( h,t ) to measure the correctness of the triplet fact ( h,r,t ) in the embedding space. In our scoring function, the dis-tributions between H X  X  and R are similar when ( h,r,t ) holds, where H , R and T indicate the Gaussian distributions of h , r and t , respectively. Different from previous methods, which adopts point-based scoring functions (dot products, cosine distance, ` distances, etc.), we employ the KL-divergence between two prob-ability distribution (the entity-pair distribution and the relation dis-tribution) as the scoring function for all triples in KGs, which is a naturally asymmetric and effective way for incorporating covari-ance (denotes (un)certainties of entities and relations in KGs) into the model. Moreover, we will employ another scoring function based on the expected likelihood [12] to inspect the different per-formances of asymmetric and symmetric measures.
 Figure 2: Illustrated of the means and (diagonal) variances of entities and relations in a Gaussian Embedding. The label indicates its position. Relations attach with underlined labels. Circles with the same color indicate a fact for Hillary Clinton . In the representations, we might infer that Hillary Clinton was born in ( place_of_birth ) Chicago , and is an ( nationality ) Ameri-can .

We have conducted extensive experiments on link prediction and triplet classification with multiple benchmark datasets such as Word-Net and Freebase. The experimental results demonstrate the effec-tiveness of our method. In particular, the proposed model can ef-fectively address one-to-many, many-to-one and reflexive relations, and significantly outperforms state-of-the-art methods (including TransH [30] and TransR [16]) by as much as  X  30% when predict-ing head (tail) entities in many-to-one (one-to-many) relations.
In summary, our main contributions are as follows:
Currently, the proposed methods mainly represent KGs in a low-dimensional latent space. We briefly summarize the most relevant work in Table 1 6 . These methods embed entities into a vector space and define a (mainly relation-dependent) scoring function to mea-sure the compatibility of ( h,r,t ). The differences in these models are the defined scoring functions f r ( h,t ) .
 We first highlight TransE [4] and its variants (TransH [30] and TransR [16]) because they are simple and effective and achieve the state-of-the-art performance in the majority of related tasks, especially in KGs with thousands of relations. Inspired by the word2vec [17], which finds the learning word vectors with a neural network own linear relations, such as, vec( X  X aris X ) -vec( X  X rance X )  X  vec( X  X ome X ) -vec( X  X taly X ) , that is, the difference in word vectors is similar when they are attached to the same relation (e.g., capi-tal_of , corresponding to the above example), TransE [4] represents a relation as a vector r indicating the semantic translation from the head entity h to the tail entity t , aiming to satisfy the equation t -h  X  r when triplet ( h,r,t ) holds. TransE effectively handles one-to-one relations but has issues in handling one-to-many, many-to-one and many-to-many relations. For example, consider a one-to-many relation r with multiple tail entities t i satisfying h + r  X  t  X  i  X  X  1 ,...,m } , ( h,r,t i )  X  KG , and it outputs invalid representa-tions ( t 1 =...= t m ) for distinguishing entities.

To address the aforementioned issues in TransE, TransH [30] and TransR [16] are proposed to enable an entity to have distinct representations when involved in different relations. For a triplet ( h,r,t ), TransH first projects a head/tail entity vector ( h / t ) into a relation-dependent hyper-plane by the following formulas: h h ( I  X  w T r w r ) and t  X  = t ( I  X  w T r w r ) , where w that spans the hyper-plane. It then measures the score using the function || h  X  + r  X  t  X  || ` 1 / 2 in the hyper-plane of the relation r . TransR is slightly different from TransH: it transforms a head/tail entity vector into a relation-dependent sub-space with h r and t r = tM r , where M r represents the transform matrix from entity space to the sub-space of relation r .

However, the TransH and TransR methods only partly address the issues encountered by TransE. For example, consider TransR in a one-to-many relation. It tends to learning t i M r = t  X  i,j  X  { 1 ,...,m } , ( h,r,t i ) , and ( h,r,t j )  X  KG , and as a re-sult, the different part between t i and t j only depends on the num-ber of eigenvalues equal to zero 7 . In addition, the existing meth-ods have difficulty learning valid representations for reflexive re-lations because they use the same operation for head and tail en-tities. For example, when triplets ( h,r,t ) and ( t,r,h ) both hold For convenient comparison, we use a different expression for TransH.
Decompose matrix M r with singular value decomposition (SVD): ( t i  X  t j ) M r = ( t i  X  t j ) S  X  0 0 0 that (most) parts of t i and t j are equal because the eigenvalues of the upper part of matrix ( S X D ) cannot contain zero. for a reflexive relation r , TransE, TransH and even TransR tend to make ( h  X  t ) and ( r  X  0 ). Whereas h equals t may be a good character for a reflexive relation, the same vector ( 0 ) for represents all reflexive relations is not helpful for related tasks. Issues often exist in the  X  X oint X  based embedding models, in which  X  X oint X  vectors are typically compared by dot products, cosine-distance or ` 1 / 2 norm, all of which provide for symmetric comparison between instances. The proposed  X  X ensity X  based methods represent enti-ties and relations by Gaussian distributions that explicitly modeling the uncertainty of KGs and the asymmetry function scores ( h,r,t ) and ( t,r,h ) using different parameters associated with not only the head and tail entities but also its order.

To our knowledge, Linear Relational Embedding (LRE) [20] is the pioneer work in learning representations of multi-relational data that represent concepts as vectors and binary relations as transform matrices. For concepts and their relations ( i,r,j ), LRE learns to maximize the generatation probability from concept i and relation r to concept j in proportion to exp (  X  X | R r v i  X  v j || 2 many approaches have followed this line. The unstructured model (UM) [4] was proposed as the simplified version of TransE by as-signing all translation vectors r = 0. However, it cannot distinguish different relations. Structured embedding (SE) [5] adopts two dif-ferent relation-specific matrices for head and tail entities but can-not capture precious semantics of relations because the two matri-ces are separated in optimization. The latent factor model (LFM) [13] considers the second-order correlations between entities em-bedding with a quadratic form. The single layer model (SLM) and neural tensor network (NTN) were proposed by Socher [25]. The SLM is a naive baseline of the NTN and scores triplets using relation-specific weights u r with a non-linear operation (tanh) for triplet representation W rh h + W rt t + b r . To date, the NTN is the most expressive model based on multi-layer neural networks. As shown in Table 1, the NTN extends the SLM by considering the second-order correlations between entity embedding (similar to LFM), feeding into a non-linear hidden layer, and then combining with a linear output parameterized by the relation. However, the NTN is not sufficiently simple to handle the large-scale KGs with numerous relations.

In addition to these methods based on the ranking loss frame-work, there is another line of related work that focuses on learning the latent representations for KGs by tensor (matrix) decomposi-tion and completion ; it was inspired by the wide usage of decom-position in recommended system [14] and relation extraction [32]. The collective matrix factorization model RESCAL [19] was pro-posed to model KGs, which regarding a KG as a 3-model tensor and learning the latent representations (entity as a vector and rela-tion as a matrix) by reconstructing the original graph. RESCAL can be used to cluster related entities and relations. Clustering concepts have been widely used in modeling multi-relational data, such as tensor factorization based on Bayesian clustering [26] and jointly spectral clustering [8]. We compare our method with RESCAL in our experiments. Vilnis and McCallum firstly proposed the Gaus-sian Embedding models learning the representations of words [28], which inspired this work. However, they mainly focus on the word representations based on the contexts of text, this work focus on the entity/relation representations in KGs based on the inter-linked relationships between them.
A Gaussian distribution is capable of representing (un)certainty explicitly. We represent KGs with Gaussian embedding. In this ( h,t ) # Parameters ,W
R , r  X  R k r k e ( n e + n r )  X  R TransH (2014) [30] || h ( I  X  w T r w r ) + r  X  t ( I  X  w w
R KG2E_KL (this paper) 1 2 { tr (  X   X  1 r (  X  h +  X  t )) +  X 
KG2E_EL (this paper) 1 2 {  X  T  X   X  1  X  + log det  X  } ,  X  =  X  and their complexities (the numbers of parameters). n e and n k section, we firstly introduce the framework for learning KG embed-ding. We then present the proposed KG2E method and the learning strategy.

First, we describe some common notations: h , r and t denote The mathematical symbols H , R and T denote the corresponding Gaussian distributions: H X  X  (  X  h ,  X  h ) (similarly for R and T ). The mean vector  X  and covariance matrix  X  indicate the corre-sponding embedding representations for the Gaussian distribution, and E and R are the sets of entities and relations in KGs, respec-tively.
We follow the energy-based framework to learn the representa-tions of a KG, as commonly used in learning word embedding [17] and knowledge graph embedding [4] in a large scale corpus. The core of this framework is an energy function E  X  ( x ) that scores the input x , parameterized by  X  . The goal of energy-based learning is to learn the parameters of the energy function to ensure that the score of an observed positive example is higher (or lower, depend-ing on the definition) than those of negative (mainly constructed) examples. This approach is often associated with a loss function L that provides gradients of the parameters given the predictions of the energy function according to some specific supervision. The framework is also called ranking loss learning because the defined loss function is based on the ranks of positive and negative samples.
In energy-based KG embedding models, the parameters  X  cor-respond to our learned representations, and the input x correspond to observed true triplet facts in a KG. That is, the defined energy function renders the score of a true fact higher (or lower) than that of a false fact, parameterized with the representations  X  .
We will describe the energy functions used in our proposed method that measure the score of a triplet ( h,r,t ). Borrowing concepts from translation-based methods [4][30][16], we consider the trans-formation result from the head entity to the tail entity to be akin to the relation in the positive triplet. We use the following simple for-mula to express this transformation: H X  X  , which corresponds to the probability distribution P e  X  X  (  X  h  X   X  t ,  X  h +  X  pothesize that the head entity and tail entity are independent with regard to some specific relation). As a result, combined with the probability distribution of relation P r  X  X  (  X  r ,  X  r ) , the most im-portant step is to measure the similarity between P e and P
KL-divergence is a straightforward method of measuring the sim-ilarity of two probability distributions and is naturally asymmetric. Moreover, we use another similarity method based on the expected likelihood or probability product kernel [28][12] to inspect the dif-ference in performance between asymmetric and symmetric mea-sures. We illustrate the two similarity measures in detail below.
We optimize the following energy function based on the KL di-vergence between the entity-transformed distribution and relation distribution and denote it as KL.
 In the upper formula, tr (  X  ) and  X   X  1 indicate the trace and inverse of the covariance matrix, respectively. Considering the simplified diagonal covariance, we can compute the trace and inverse of the matrix simply and effectively.

The gradient of the log determinant is  X  log detA  X  X  = A  X  1  X  ( A  X  1 Y X T A  X  1 ) T [21]. We can compute the gradients of this energy function with respect to the mean vectors and covariance matrix (currently acting as a vector) as follows:
We can define a symmetric similarity measure based on KL di-vergence as follows: However, this measure lacks any gains in performance in terms of link prediction and triplet classification, likely because the discrim-inative ability of this formula is not distinct from the previous func-tion for positive and negative triplets.
The dot product between the entity mean and relation mean is not a suitable measure of similarity because it does not integrate the covariance and cannot consider the diversity of uncertainty among different entities/relations. Therefore, we take the inner product between two distributions themselves to measure the similarity be-tween P e and P r .
 For better computation and comparison, we use the logarithm of the upper formula as the final energy function and denote it as EL.
E ( h,r,t ) = log E ( P e , P r ) = log N (0;  X  e  X   X  r ,  X  As in the previous case, we can compute the gradients for this en-ergy function in a closed form.
We define the following margin-based ranking loss for effective discrimination between observed (positive) triplets and incorrect (negative) triplets:
L = X where [ x ] + , max(0 ,x ) aims to obtain the maximums between 0 and x ,  X  is the margin separating positive and negative triplets, E ( h,r,t ) indicates the energy function formula 1 or 6 for scoring triplets,  X  is the set of positive triplets observed in the KG, and  X  ( h,r,t ) denotes the set of negative triples corresponding to ( h,r,t ) , which will be introduced below.

Under the open world assumption (OWA), existing KGs contain only correct triplets. The routine method for constructing a nega-tive triplet ( h 0 ,r 0 ,t 0 ) is to replace the head or tail entity randomly, such as sampling h 0 for h and obtaining ( h 0 ,r,t ) . To obtain practi-cal corrupted triplets, we follow [30] and assign different probabil-ities for head/tail entity replacement. The main idea is to provide a greater likelihood of replacing the side that will reduce the possi-bility of generating false-negative instances. For example, with re-gard to the relation gender , replacing tail is more likely to generate true-negative triplets. Following the notation in previous methods [30][16], we will denote the traditional sampling method as  X  X nif X  and the new method [30] as  X  X ern X . We also generate a negative triplet by corrupting the relation and ensure that it is not a false-negative triplet.

To avoid overfitting, we add some regularization while learn-ing the Gaussian embedding. Considering the different geomet-ric characteristics, we use different regularization strategies for the mean and covariance. The following hard constraints are consid-ered when we minimize the loss L : where the constraint 10 ensures that the means remain sufficiently small and the constraint 11 guarantees that the covariance matrices are positive definite and of appropriate size. We can use  X  max ( c min ,min ( c max ,  X  ii )) to achieve these goals for diagonal covariance.
 Algorithm 1: T HE L EARNING A LGORITHM OF KG2E Input : An energy function E ( h,r,t ) , training set
Output : All the Gaussian embeddings (mean vector and 1 foreach `  X  X   X  X  do 2 ` .mean  X  Uniform(  X  6  X  3 ` .cov  X  Uniform( c min , c max ) 4 regularize ` .mean and ` .cov with constraints 10 and 11 5 i  X  0 6 while i + +  X  n do 7  X  batch  X  sample(  X  , b ) //sample a minibatch of size B 8 T batch  X  X  X  //pairs of triplets for learning 9 foreach ( h,r,t )  X   X  batch do 10 ( h 0 ,r,t 0 )  X  negSample( ( h,r,t ) ) //sampling negative 11 T batch  X  T batch  X  (( h,r,t ) , ( h 0 ,r,t 0 )) 12 ( h,r 0 ,t )  X  negSample( ( h,r,t ) ) //sampling negative 13 T batch  X  T batch  X  (( h,r,t ) , ( h,r 0 ,t )) 14 Update Gaussian embeddings based on Equations 2, 3 and 15 regularize the means and covariances for each entity and
We use Stochastic Gradient Descent (SGD) 8 in small mini-batches to iteratively update the Gaussian embeddings of entities and rela-tions. In our model, we must first choose an energy function (EL or KL) ( hereafter, we denote the corresponding models as KG2E_EL
We also use AdaGrad [9] to optimize the parameters but found no improvement. and KG2E_KL, respectively.). The detailed learning procedure is described in Algorithm 1. All Gaussian embeddings for entities and relations are first initialized randomly following a uniform dis-tribution. At each main iteration of the algorithm, we first sample a batch of observed triplets, and construct corresponding negative triplets based on the aforementioned sampling methods ( X  X nif X  or  X  X ern X ). The parameters of Gaussian embedding are then updated by taking a gradient step (using formulas 2, 3 and 4 or 7 and 8, depending on the choice of energy function E ( h,r,t ) ), with a con-stant learning rate. For each step (including the initial embedding), we ensure that all embeddings satisfy with the constraints 10 and 11.
In this work, we empirically study and evaluate related methods for two tasks: link prediction [4] and triplet classification [25]. We use datasets commonly used in previous methods, which are built from two typical KGs: WordNet [18] and Freebase [1]. Word-Net is a lexical database of the English language. In WordNet, each entity represents a synset consisting of several words, and a word can also belong to different synsets. Relationships between synsets include hypernym , hyponym , meronym , holonym , troponym and other lexical relations. We adopt two datasets from WordNet, WN18, used in [4] for link prediction, and WN11, used in [25] for triplet classification. Among them, WN18 contains 18 relations and WN11 contains 11. Freebase is a large collaborative knowl-edge graph of general world facts. For example, the triplet ( Bill Gates, place_of_birth, Seattle ) indicates that the person with en-tity Bill Gates was born in ( place_of_birth ) the location with entity Seattle . We adopt two datasets from Freebase, FB13, used in [25] for triplet classification, and FB15k, used in [4] for link predication and triplet classification. Among them, FB13 contains 13 relations and FB15k contains approximately 15,000 entities. The statistics of these datasets are listed in Table 2.

Before evaluation in each specific task and comparison with other methods, we first examine the effectiveness and ability of our pro-posed method to represent the uncertainty in a KG with a quali-tative analysis. The following surveys and observations are based on the representations of embeddings learned by KG2E and using KL-divergence as a similarity measure in FB15k.

First, we want to know the effect of covariance in modeling the uncertainty in a KG. Based on our ideas, an entity/relation with a higher level of uncertainty has a larger covariance (correspond-ing with determinant or trace). Considering the uncertainty of an entity, we focus on the relationship between the (log) determinant of covariance matrix and its density. The entity/relation density is indicated by the number of corresponding triplets, and the entity density is measured at different positions: head part, tail part or entire set. As shown in Figure 3, there is a clear tendency for the larger determinant of entity covariance to have fewer correspond-ing triplets, regardless of position. Considering the uncertainty of a relation, we measure the (log) determinant and trace of covari-ance for 13 relations with /people/person as domain , as shown in Table 3 (each row includes the following information with a rela-tion: label, number of triplets,number of head entities, number of tail entities, type 9 , (log) determinant and trace of covariance ma-trix). We can draw the following conclusions: 1) the covariance of Gaussian embedding can effectively model the (un)certainty of a relation; 2) relations with complex semantic (e.g., many_to_one (m-1) and many_to_many (m-n) relations) have larger uncertainty, and 3) the more unbalanced the head and tail entities, the larger the uncertainty. For example, the nationality relation has the largest uncertainty, and the parents relation has the smallest uncertainty among these 13 relations. Figure 3: The relationships between the density (linked triplet number) of entity and the determinant of its corresponding co-variance.
 Table 3: The relationships between some relations and the determinants and traces of their corresponding covariances, sorted by the descending order of trace.

Next, we want to know the ability of Gaussian embedding to learn valid entity/relation representations. Tables 4 and 5 give the top 5 similarity entities/relations with regard to some sampling ex-amples. The tables illustrate that the proposed method can learn a valid representation for modeling KGs.
Following the usage in [5][4], link prediction aims to predict the missing h or t for a relation fact triplet( h,r,t ). Instead of ob-taining one best answer, this task puts more emphasis on ranking a set of candidate entities from the KG. Similar to the setting in [5][4][30][16], we conduct initial experiments using the datasets WN18 and FB15k.

Evaluation protocol. We follow the same protocol as in TransE [4] and its variants [30][16]: In the testing phrase, for each test
We follow the definition in [4] and measure used in [30]. Table 4: The top-5 similarity entities with regards to some ex-amples. Table 5: The top-5 similarity relations with regards to some ex-amples, using a wildcard  X  to reduce space occupation without ambiguous expression.
 KG and rank these entities in descending order of similarity scores, measured by the energy function E ( h,r,e ) . A similar process is performed for the head entity measure by E ( e,r,t ) . Based on these entity ranking lists, we use two evaluation metrics by aggregation over all the testing triplets: 1) the average rank of correct entities (denoted as Mean Rank ) and 2) the proportion of correct entities in the top 10 ranked entities (denoted as Hits@10 ). A good method should obtain lower Mean Rank or higher Hits@10 . Considering the fact that a corrupted triplet for ( h,r,t ) also exists in a KG, such a prediction should also be deemed correct. However, the above evaluations do not consider the issue and may underestimate the metrics. To eliminate this factor, we remove those corrupted triplets that already appeared in training, valid or testing sets before obtain-ing the rank entity list of each testing triplet. We term the former evaluation setting as  X  X aw X  and the latter setting as  X  X ilter X .
Implementation. Because the testing datasets are the same, we directly compare our models with several baselines reported in [4][30][16]. In learning KG2E, we select the learning rate  X  for SGD among {0.001, 0.01, 0.05}, the margin  X  among {1, 2, 4}, the dimensions of entity and relation sharing embedding k among {20, 50, 100}, the batch size B among {20, 120, 1440, 2480}, and the pair of restriction values c min and c max for covariance among {(0.01, 1), (0.03, 3), (0.05, 5)}. The optimal configuration is de-termined by the Hits@10 in the validation set. As the strategy of constructing negative labels can greatly influence the evaluations, we use different parameters for  X  X nif X  and  X  X ern X . We also use dif-ferent parameters for KG2E_KL and KG2E_EL. The default con-figuration for all experiments is as follows:  X  = 0 . 001 ,  X  = 1 , k = 50 , B = 120 , and ( c min , c max )= (0.05, 5). Below, we list only the non-default parameters. For KG2E_KL, under the  X  X nif X  setting, the optimal configuration is as follows:  X  = 0 . 01 , and  X  = 4 on WN18 and B = 1440 on FB15k. Under the  X  X ern X  setting, the optimal configuration is as follows:  X  = 0 . 01 ,  X  = 4 , B = 20 , and ( c min , c max )= (0.03,3) on WN18 and B = 2480 on FB15k. For KG2E_EL,under the  X  X nif X  setting, the optimal config-uration is as follows:  X  = 0 . 01 ,  X  = 4 , and B = 20 on WN18 and  X  = 2 on FB15k. Under the  X  X ern X  setting, the optimal configura-tion is as follows:  X  = 0 . 01 ,  X  = 4 , B = 20 , and ( c (0.03,3) on WN18 and B = 2480 on FB15k. For both datasets, we traverse all the training triplets for 500 rounds.
 Results. The results are reported in Table 6. On WN18, TransE, TransH, TransR, KG2E and even the naive baseline unstructured models outperform other approaches in terms of the Mean Rank metric, but the majority of models are poor in terms of the Hits@10 metric. KG2E with the KL energy function outperforms other base-line models, including TransE, TransH and TransR, in terms of the Hits@10 metric but achieves a worse Mean Rank . One reason may be that WN18 simply contains a small number of relations, and thus, simple methods can judge the correct triplet but cannot rank it in the top position. Another reason is that the Mean Rank is easily reduced by an obstinate triplet with a low rank. On FB15k, KG2E_KL consistently outperforms the other baseline models in both Mean Rank and Hits@10 . As the density diversity in FB15k is greater than that in WN18, we hypothesize that the improvements are because the uncertainty diversities in FB15k are greater than those in WN18, and thus, the density-based embedding methods can handle it better. From the observations, we can draw the follow-ing conclusions: 1) Gaussian embedding can learn valid representa-tions of KGs for link prediction. 2) KG2E is superior to other base-line methods. 3) KG2E_KL performs better than KG2E_EL, which indicates that the asymmetric energy function is more suitable for learning the representation of KGs with Gaussian embedding. 4) The  X  X ern X  sampling strategy works well for most approaches, es-pecially on FB15k, which has many more relation types.

Table 7 shows the evaluation results with separated types of re-lation properties. Following [4], we divide relations into four types: one-to-one, one-to-many, many-to-one and many-to-many, for which the proportions in FB15k (1345 relations in total) are 24%, 23%, 29% and 24%, respectively, based on the measure used in [30]. KG2E_KL and KG2E_EL significantly outperform TransE, TransH, TransR and other baseline methods in one-to-one, one-to-many, and many-to-one relations. For the difficult tasks of predicting tails in one-to-many relations and predicting heads in many-to-one re-lations, KG2E_KL obtains 29.3% and 29.9% improvements, re-spectively. However, the proposed method presents only a slight advantage for many-to-many relations, possibly because there are various fine-grained types within a many-to-many relation that can-not be effectively expressed by one Gaussian embedding. We be-lieve CTransR, proposed by [16], is an effective way to handle this issue by adopting a clustering strategy to divide entity pairs into different sub-types. To better review the modeling improvement of uncertainty of KG2E over TransE and its variants, Table 8 shows the Hits@10 results on some typical one-to-many, many-to-one, many-to-many and reflexive relations. We directly copy the ex-perimental results of TransH from [30] for a fair comparison. We can observe that the asymmetric similarity measure can effectively handle reflexive relations.
This task seeks to judge whether a given triplet ( h,r,t ) is cor-rect or not. That is, it is a binary classification task for fact triplets, which was first explored in [25] and then widely used to evalu-ate KGs embedding [30] [16]. In this task, we use three datasets: WN11, FB13 and FB15k.

Evaluation protocol. We follow the same protocol as in NTN [25]. The evaluation of binary classification requires negative triplets. Table 8: Hits@10 (Filter) of KG2E_KL and TransH on some examples of one-to-many  X  , many-to-one  X  , many-to-many  X  reflexive relations  X  .
 WN11 and FB13 released by NTN [25] already contain negative triplets, which are built by corrupting the corresponding positive (observed) triplets. As FB15k has not released negative triplets in previous works, we construct negative triplets following the same procedure used in [25] for FB13. The setting for triplet classifi-cation is very simple: for each triplet ( h,r,t ), if the dissimilarity score obtained by the energy function E ( h,r,t ) is below a relation-specific threshold  X  r , then the triplet will be classified as positive. Otherwise, it will be classified as negative. The relation-specific threshold  X  r is optimized by maximizing the classification accu-racy on the validation set.

Implementation. Considering that the same datasets (negative triplets) are used in WN11 and FB13, we directly compare our models with the baseline methods reported in [16]. For evaluation on FB15k, we use the code released by Lin 10 [16] (running TransE, TransH and TransR) and Socher 11 [25] (running NTN). For TransE, TransH and TransR, we select the learning rate  X  for SGD among {0.001, 0.01, 0.05}, the margin  X  among {0.5, 1, 2}, the dimen-sions of entity and relation sharing embedding k among {20, 50, 100}, and the batch size B among {20, 120, 1440, 2480}. Other parameters follow the default configuration in the shared codes. For the NTN, we did not change the settings: dimension k = 100 , and the number of slices equals 3. The optimal configurations are as follows:  X  = 0 . 001 ,  X  = 1 , and B = 4800 for TransE (bern);  X  = 0 . 001 ,  X  = 2 , and B = 120 for TransE (unif);  X  = 0 . 001 ,  X  = 0 . 5 , and B = 4800 for TransH (bern)  X  = 0 . 01 ,  X  = 0 . 5 , and B = 4800 for TransH (unif);  X  = 0 . 001 ,  X  = 1 , and B = 4800 for TransR (bern); and  X  = 0 . 001 ,  X  = 1 , and B = 120 for TransR (unif). The dimension k = 100 for all the above configurations.
In learning KG2E, we select the learning rate  X  for SGD among {0.001, 0.01, 0.05}, the margin  X  among {1, 1.5, 2}, the dimen-https://github.com/mrlyk423/relation_extraction www.socher.org sions of entity and relation sharing embedding k among {20, 50, 100}, the bach size B among {20, 120, 1440, 2480}, and the pair of restriction values c min and c max for covariance among {(0.01, 1), (0.03, 3), (0.05, 5)}. The optimal configuration is determined by the classification accuracy in the validation set. For all three datasets, we traverse all the training triplets for 1000 rounds. We also use different parameters for KG2E_KL and KG2E_EL. The default configuration for all experiments are as follows:  X  = 0 . 001 ,  X  = 1 , k = 50 , B = 120 , and ( c min , c max )= (0.05, 5). Below, we list only the non-default parameters. For KG2E_KL, under the  X  X nif X  setting, the optimal configuration is as follows: k = 20 ,  X  = 2 , and B = 120 on WN11; k = 100 , and B = 1440 on FB13. Under the  X  X ern X  setting, the optimal configuration is as fol-lows: k = 20 , and  X  = 2 on WN11 and k = 100 , and B = 1440 on FB13. For KG2E_EL, under the  X  X nif X  setting, the optimal con-figuration is as follows: k = 20 ,  X  = 2 , and B = 120 on WN11; k = 100 , and B = 120 on FB13 and  X  = 1 . 5 , on FB15k; Under the  X  X ern X  setting, the optimal configuration is as follows: k = 20 ,  X  = 2 , and B = 120 on WN11; k = 100 on FB13 and B = 2480 on FB15k.

Results. The accuracy of triplet classification on the three datasets is shown in Table 9. On WN11, KG2E_KL and TransR outper-form all the other models. The NTN, the powerful model with the most parameters, outperforms the other approaches on FB13, but it performs poorly on FB15k, which contains many more re-lations. In contrast, on a more practical KG with large-scale re-lations (such as Freebase), the proposed method KG2E_KL per-forms much better than the other baseline models, and even the KG2E_EL is also a competitive model. We can draw the following conclusions from the observations: 1) KG2E_KL achieves supe-rior performance compared to other baseline methods for a multi-relational KG, which indicates that Gaussian embedding can ef-fectively model the enormous diversity of uncertainty in a KG. 2) KG2E_KL performs better than KG2E_EL, which is consis-tent with the results of link prediction. 3) The  X  X ern X  sampling strategy outperforms the majority of the approaches ( including TransE, TransH, TransR and our proposed models KG2E_EL and KG2E_KG) on all three datasets.

We also compare with CTransR, another model proposed by Lin [16] to handle many-to-many relations with entity-pairs clustering. However, in the NTN [25], another set of results combined with word embedding [17] is reported. There are different ways to im-prove KG embedding between the aforementioned method and our method, and for fairness, we have not compared their results.
As shown in [30] and [16], the training times of TransE, TransH and TransR are approximately 5 minutes, 30 minutes and 3 hours, respectively. The computational complexities of our proposed meth-ods are lower than that of TransR but higher than those of both TransE and TransH: KG2E_KL and KG2E_EL take approximately 80 and 75 minutes for training, respectively.
In this paper we propose KG2E, a new method for learning rep-resentations of entities and relations in KGs with Gaussian embed-ding. Each entity and relation is represented by a Gaussian dis-tribution with a mean vector and a covariance matrix (currently with diagonal covariance for computational efficiency), which aims to model the uncertainty of entities and relations in a KG. The (un)certainties vary considerably for different entities and relations: for example, popular entities with fewer uncertainty, which contain more relations and facts than unpopular ones, high-frequency rela-tions with more uncertainty, which link more entity pairs than low frequency ones, and, different parts of relations can contain a very SME (bilinear) (Bordes et al.2012) 70.0 63.7 -TransE (unif) (Bordes et al. 2013) 75.9 70.9 79.2 TransE (bern) (Bordes et al. 2013) 75.9 81.5 81.4 TransH (unif) (Wang et al. 2014) 77.7 76.5 85.4 TransH (bern) (Wang et al. 2014) 78.8 83.3 85.8 CTransR (bern) (Lin et al. 2015) 85.7 -87.4
Table 9: Experimental results of Triplet Classification (%). different numbers of entities that sharpen the uncertainties of rela-tions. We use two energy functions (symmetric and asymmetric) to compute the score of a triplet fact. Extensive experiments on link prediction and triplet classification with multiple benchmark datasets (including WordNet and Freebase) demonstrate that the proposed method significantly outperforms state-of-the-art meth-ods.

In the future, we plan to address the following limitations that still exist in the methods: The authors are grateful to anonymous reviewers for their construc-tive comments. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015405) and the National Natural Science Foundation of China (No. 61272332 and No. 61202329). [1] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. [2] A. Bordes, S. Chopra, and J. Weston. Question answering [3] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic [4] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and [5] A. Bordes, J. Weston, R. Collobert, Y. Bengio, et al. Learning [6] R. Brachman and H. Levesque. Knowledge representation [7] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr., [8] X. Dong, P. Frossard, P. Vandergheynst, and N. Nefedov. [9] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient [10] F. Hayes-Roth, D. Waterman, and D. Lenat. Building expert [11] S. He, K. Liu, Y. Zhang, L. Xu, and J. Zhao. Question [12] T. Jebara, R. Kondor, and A. Howard. Probability product [13] R. Jenatton, N. L. Roux, A. Bordes, and G. R. Obozinski. A [14] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [15] N. Lao, T. Mitchell, and W. W. Cohen. Random walk [16] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity [17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [18] G. A. Miller. Wordnet: a lexical database for english. [19] M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way model [20] A. Paccanaro and G. E. Hinton. Learning distributed [21] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook , [22] T. Rockt X schel, S. Singh, and S. Riedel. Injecting logical [23] H. Schuetze and C. Scheible. Two svds produce more focal [24] W. Shen, J. Wang, P. Luo, and M. Wang. Linking named [25] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning [26] I. Sutskever, J. B. Tenenbaum, and R. R. Salakhutdinov. [27] S. Szumlanski and F. Gomez. Automatically acquiring a [28] L. Vilnis and A. McCallum. Word representations via [29] W. Y. Wang, K. Mazaitis, N. Lao, and W. W. Cohen.
 [30] Z. Wang, J. Zhang, J. Feng, and Z. Chen. Knowledge graph [31] M. Yahya, K. Berberich, S. Elbassuoni, and G. Weikum. [32] L. Yao, S. Riedel, and A. McCallum. Probabilistic databases
