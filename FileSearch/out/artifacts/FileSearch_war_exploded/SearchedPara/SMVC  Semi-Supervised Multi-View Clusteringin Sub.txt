 Since data is often multi-faceted in its very nature, it might not adequately be summarized by just a single clustering. To better capture the data X  X  complexity, methods aiming at the detection of multiple, alternative clusterings have been pro-posed. Independent of this research area, semi-supervised clustering techniques have shown to substantially improve clustering results for single-view clustering by integrating prior knowledge. In this paper, we join both research areas and present a solution for integrating prior knowledge in the process of detecting multiple clusterings.

We propose a Bayesian framework modeling multiple clus-terings of the data by multiple mixture distributions, each responsible for an individual set of relevant dimensions. In addition, our model is able to handle prior knowledge in the form of instance-level constraints indicating which objects should or should not be grouped together. Since a priori the assignment of constraints to specific views is not necessarily known, our technique automatically determines their mem-bership. For efficient learning, we propose the algorithm SMVC using variational Bayesian methods. With experi-ments on various real-world data, we demonstrate SMVC X  X  potential to detect multiple clustering views and its capabil-ity to improve the result by exploiting prior knowledge. H.2.8 [ Database Management ]: Database Applications  X  Data mining ; I.2.6 [ Artificial Intelligence ]: Learning semi-supervised learning; subspace clustering; constraints
Clustering aims at grouping data instances based on their similarity. For complex data, however, the similarity often depends on the point of view. In a customer database, for example, users might be grouped according to their demo-graphic profile or according to their buying patterns. In a document database, different groups might reflect the doc-uments X  subjects or their writing style. Thus, depending on the application and the user X  X  preferences, a single grouping does not capture all aspects but multiple, alternative clus-tering solutions are required. The emerging research field of multi-view or alternative clustering [27] addresses this chal-lenge by finding multiple high quality clusterings.
On the other hand, semi-supervised clustering techniques [8] try to incorporate the user X  X  preferences by exploiting prior knowledge during the clustering process. For tradi-tional single-view clustering, these techniques have shown to substantially increase the clustering results. Motivated by the success of both research areas, we propose a semi-supervised multi-view clustering technique. Our goal is to exploit user provided prior knowledge to enhance the results of multiple, alternative clusterings.

For semi-supervised clustering, it is crucial that the user can provide supervision in an easy and understandable way. While cluster level constraints, such as the clusters X  sizes, po-sitions, or distributions, usually require an abstract under-standing of the desired clustering structure, instance level constraints which, e.g., indicate partial information about cluster memberships, are much more intuitive. A popular way of modeling such prior information is via equivalence constraints, which indicate for pairs of instances whether they should belong to the same cluster (must-link constraint) or to different clusters (cannot-link). Even though lacking a full understanding of the clustering structure, this allows the user to partly specify her intuition by indicating for selected object pairs their pairwise cluster relation. Since in many cases these user constraints express a belief rather than cer-tainty, we use the concept of soft constraints, where mistakes are possible and a complete fulfillment is not enforced.
The transfer of the semi-supervised clustering principle to the multi-view case poses a severe challenge, particularly regarding the multi-faceted nature of the data. One user might for example see the similarity of two movies based on their cast, while another user might foreground their dissim-ilarity based on differing genres. It, therefore, might remain unclear to which view specific constraints refer to. In par-ticular, when naively assigning all constraints to a single view, a large proportion of the constraints might be con-flicting such that even a relaxation to soft constraints will not be sufficient anymore. Therefore, the challenge in semi-supervised multi-view clustering is not only to optimize the clustering such that constraints are optimally fulfilled but also to learn the affiliation of constraints to views .
It has to be highlighted that some of the sequentially work-ing multi-view clustering approaches (which iteratively find find multiple different clusterings, these weights can be dif-ferent for each view. We use the random variable where  X  m,k is the weight of component k in clustering m . Due to conjugate properties, we use a Dirichlet distribution as its prior. Again, in our study, we use a non-informative prior by selecting  X   X  = 1 since a priori no knowledge about the cluster sizes is given.

Integrating Subspaces. To detect the data X  X  multiple views, we refer to the principle of subspace clustering. Our goal is to assign each mixture model to a specific subspace projection, which it describes well. Since the relevant di-mensions of the mixtures are a priori not known, we learn them with our method. Therefore, we introduce the random variable to indicate which of the M clusterings is responsible for a specific dimension d . The vector ~r d  X  [0 ... 1] P r k,m = 1) can be used to give some prior knowledge which dimension belongs to which view. Again, we use a constant non-informative prior, i.e. r k,m = 1 /M .

Knowing about the subspaces as well as the mixture mod-els X  parameters, we are now able to generate observations which show multiple clustering structures: We denote with z m,i the random variable indicating to which cluster an ob-ject i belongs to in clustering m , i.e. Note that for each view m , the object might follow a different cluster, i.e. z m,i 6 = z m 0 ,i is possible. Thus, in each view the object might be grouped together with different objects. This idea is illustrated in Figure 1: the grouping on the left differs from the one on the right. Given z m,i , the attribute value of object i in dimension d is drawn according to That is, we use the clustering m which is responsible for dimensions d and the corresponding component k the object belongs to in this view.

Integrating User Constraints. So far, our model cor-responds to a completely unsupervised technique for finding multiple clusterings. As a major advancement, we now inte-grate user provided prior-knowledge. As discussed, we aim to support the concept of instance level constraints. More precisely, we support the idea of soft constraints between pairs of objects that indicate whether the objects should or should not be grouped together. We selected this type of semi-supervision since it reflects an intuitive understanding of clustering and is easy to specify for the user.

The user can provide a constraint between the objects i and j via a weight w i,j . If the weight is positive, the user indicates that there should exist a clustering where the objects are grouped together. If the weight is negative, the user indicates that there should exist a clustering where i and j are not grouped together. Different magnitudes of the weights can be used to indicate the different importance or relevance of the constraints.

At this point it is crucial to keep in mind that we are in-terested in finding multiple, alternative clusterings: A con-straint between i and j means that there exists a view where the constraint is fulfilled. We do not require that i and j are grouped together in all views , which actually would contradict the fundamental assumption for multi-view sce-narios that clusterings of different views differ and contain alternative knowledge. Forcing constraints to be valid for all views would be too restrictive. Furthermore, we argue that the user is generally not aware of the details of all possible groupings. Thus, the user should not define constraints re-stricting views that he does not understand. Accordingly, for each constraint, we are interested in finding (at least) one clustering fulfilling this constraint.

Resulting from this principle, another challenge of our method becomes apparent: we have to determine the clus-tering which is responsible for a specific constraint. In the following, we show how to model all these aspects.
 As mentioned, the constraints are modeled via weights. In our model, we represent them via a symmetric matrix W of size N  X  N , where entries with weight zero indicate no prior knowledge about the corresponding pairs of objects. In practice, we can use a sparse representation of the ma-trix which only encodes the given constraints and allows for an efficient processing. Interesting to note is that the (ob-served) matrix W appears in our grapical model as one of the root nodes (cf. Figure 2), and not as a leaf like X . As shown, the weights influence the grouping Z of the objects.
Additionally, we introduce the categorical random vari-ables c i,j (due to the symmetry of the weights, we only need to consider i &lt; j ). These variables indicate which view is responsible for a specific constraint. That is, we have use ~ h ( i,j ) to express some further prior knowledge about the constraint between object i and j . If the user, for example, knows that a set of constraints should most likely belong to one view, the h vectors can be selected accordingly. Per default, we assume that no knowledge about the assignment of constraints to views is known, i.e. we use h ( i,j ) m
Given W and C , how can we use their values to influence the clustering structure of the data? Our idea is to add a bias to the probability distribution of the z m,j . The proba-bility of generating a clustering that matches the constraints should be higher than the probability of a clustering which violates the constraints. Particularly, this results in a de-pendency between the variables z m,  X  which is guided by the constraints. We define p ( z m,  X  | ~ X  m ,W,C ) Here,  X  ( z m,i ,z m,j ) denotes the Kronecker delta, which eval-uates to 1 if both objects are located in the same cluster (in view m ), and 0 otherwise. Please note that Equation 7 is the joint distribution for all z m,  X  .

The first part of the equation corresponds to the mixture weights as used in standard mixture models. If all w i,j Equation 4 and 7 are equivalent. The second part models the bias to specific groupings: As one can see, if w positive and the objects are located in the same cluster, the probability of selecting this grouping increases. Accordingly, if w i,j is negative, one would decrease the probability of clusterings where i and j are grouped together. A similar principle was used in [25, 7] for single-view clustering.
Important to mention is that the second part of the equa-tion incorporates the automatic assignment of constraints to views. The constraint between i and j adds a bias to the clustering structure in view c i,j = m only. In accordance to our discussion above, the other views are not affected.
Given the new definition for the distribution of Z , the actual observations are, as before, generated according to Equation 5. Overall, our model combines the principle of multiple clusterings in subspace projections with the para-digm of semi-supervised clustering and automatically as-signs constraints to their responsible views.
While the previous section has focused on the model X  X  generative process, we now present our learning technique. That is, given a set of observations X and a set of constraints W , we infer the model X  X  parameters. Our method is called SMVC (S emi-Supervised M ulti-V iew C lustering).
The general inference problem we have to solve is to deter-mine the distribution p ( Y | X,W ), where Y = { V,Z,C,~ X , X , X  } is the set of all latent variables. Based on this distribution, we can, e.g., pick the realizations of the latent variables lead-ing to the highest likelihood given the data. Since comput-ing p ( Y | X,W ) is intractable, we compute an approximation based on the principle of variational inference [10]: we ap-proximate p ( Y | X,W ) by a tractable family of parametrized distributions q ( Y |  X ). The parameters  X  are the free vari-ational parameters. These parameters are optimized such that the best approximation between q and p is obtained. Technically, one minimizes the Kullback-Leibler divergence between q and p by optimizing  X . Using Jensen X  X  inequality, minimizing the KL divergence is equivalent to maximizing the following lower bound on the log marginal likelihood [10]: where E q [ . ] denotes the expectation w.r.t. the q distribution.
Following primarily the idea of mean field approximation, we assume the function q to factorize in p ( Y | X,W )  X  q ( Y |  X ) := Y  X 
Y
As we will later see, assuming the above factorization, the optimal variational distributions have the form q ters to be optimized. Note that each distribution has its own variational parameters [10]. Thus, e.g. the functions q 1 and q 1 ( v d 0 ), are not necessarily identical. This extra degree of freedom allows to find a good approximation between q and p . As discussed in Section 2, for c i,j , i.e. the function q , we only need to consider pairs i,j with w i,j 6 = 0.
General Processing Scheme. We use an iterative co-ordinate ascent method to maximize Equation 8 w.r.t. the parameters  X  (the update equations follow in Section 3.2). The processing scheme is as follows:
Note that due to the properties of variational inference [10], it is guaranteed that the method converges. In prac-tice, we assume convergence if the change in the lower bound on the marginal likelihood is below than 0 . 01. Additionally, to avoid the problem of local minima, we enhance the pro-cessing scheme by gradually increasing the importance of the constraints. That is, starting with low weights, we linearly increase the values w i,j until they reach the user specified scores. For initializing our method, we exploit the same principle as described in [21]. The random variable C/q 3 initialized randomly based on its prior distribution.
We briefly present the update equations required for the coordinate ascent method. We primarily follow the princi-ple of [10]: The optimal distribution for q x ( B ) can be deter-mined by Here, the constant C absorbs all terms which are indepen-dent of B and, thus, do not affected the optimal distribution of q x . E q \ B [ . ] denotes the expectation w.r.t. the distribution q taken over all variables Y except of B . To avoid cluttering the notation, we simply write E q in the following (it is clear from the context which variable is excluded).

Updating the constraint responsibility. Let [[ . ]] denote the Iverson bracket. We can rewrite Equation 7 as follows
Y This formulation makes it easier to derive the following re-sults. Accordingly, we can rewrite the remaining equations.
The optimal distribution for q 3 ( c a,b ) (with a &lt; b ) can be obtained via Eq. 9. Removing all terms which are indepen-dent of c a,b and using the above reformulation, we get = E q [log ( P ( c i,j ) P ( Z |  X ,C,W ))] + C = E q [log 1 Since c a,b has a finite domain, the distribution q 3 is a cat-egorical distribution. Renaming the variables, the optimal hyperparameters of the distribution q 3 ( c i,j ) are given by where P m  X  i,j,m = 1. The occurring expectations can be replaced by the known expectations of the variational dis-tributions (cf. appendix). Intuitively, the parameter  X  shows the probability of assigning the constraint between i and j to the view m .

Updating the views. Computing Eq. 9 for q 1 ( v d ) and removing all terms which are independent of v d leads to = E q [log ( P ( x  X  ,d | v d ,Z, X , X  ) P ( v d ))] + C = E q [log = E q [log = Here, we used the definition
Thus, q 1 is a categorical distribution and the optimal hy-perparameters for q 1 ( v d ) are given by where P m  X  d,m = 1.

Updating the cluster indicator. The same principle can be applied for the cluster indicator variable. We obtain:
Here, we exploit the symmetry of w i,j and the definition of f as given above. Note again, that we do not actually need to sum over all j 6 = a when using a sparse encoding of the matrix W . It is sufficient to iterate over those j for which a constraint with a is given. Similar as before, the optimal hyperparameters for q 2 ( z m,i ) are given by + E q [log  X  m,k ] +
Updating the mixing weights. The mixing weights are continuous. Since we selected a conjugate prior in our model, it follows:
As seen, the optimal distribution for q 4 is a Dirichlet dis-tribution, where the hyperparameters are given by Updating the mixture components. Updating the mean and precision of each mixture component follows the stan-dard principle of variational inference in a conjugate setting. Let u m,k = P N i =1 E q [[ z m,i = k ]] be the unnormalized weight of a cluster and  X  x m,k,d = 1 u weighted mean in dimension d (when considering the expec-tation w.r.t. q ). Using conjugacy, it follows that the optimal hyperparameters of the distribution q 5 are given by
Inspecting the individual update equations, it becomes apparent that each iteration of our algorithm runs in time O ( M  X  N  X  K  X  ( D + W )), where W denotes the number of constraints. Thus, we obtain a linear complexity in all im-portant parameters.

Overall, our method efficiently computes an approxima-tion of the posterior distribution p ( Y | X,W ) which shows us the multiple clustering structures, their relevant subspaces, and the assignment of constraints to views.
Our approach is related to four main paradigms in the field of cluster analysis, as we will discuss in the following. Table 1 shows on overview of the related works and their corresponding properties.

Subspace clustering. For traditional full-space cluster-ing, a large proportion of irrelevant attributes can cause an obfuscation of the clustering structure. The underlying as-sumption of subspace clustering (co-clustering/bi-clustering) [24, 2, 26] is that the set of irrelevant attributes might differ for each cluster. These locally irrelevant attributes hinder a meaningful global dimensionality reduction [29] and make traditional, full-space approaches futile. The consideration of attribute subsets is highly related to our multi-view sce-nario, since different views of the data are most likely at-tributable to different characteristics. However, subspace clustering does not realize a grouping of clusters to repre-sent alternative views as required for multi-view clustering.
Multi-view clustering. The paradigm of multi-view or alternative clustering can be categorized in three types [27]: Approaches of the first category, e.g. [23, 4, 13, 14], operate in the full-space and, therefore, suffer from similar problems as traditional clustering. Furthermore, they usually aim at finding just two alternative clusterings. The second cate-gory X  X  representatives iteratively determine an alternative clustering based on the previous one via space transforma-tions such as PCA [11, 15] or distortion of the distance func-tion [30]. They do not globally/simultaneously optimize the whole set of all clusterings. Since previous clustering solu-tions serve as guidance for the discovery of new clustering structures, these approaches can partially be categorized as semi-supervised. However, the constraints affect only the solution of the single, next clustering and, thus, already de-tected solutions cannot benefit from them. Additionally, dis-tortions of the original space usually hinder an intuitive in-terpretation of the clustering result. Contrarily, axis-parallel projections of the data as used in our approach allow an easy interpretation. The third category, which is mostly related to our approach, represents methods that simultaneously re-veal all clusterings by analyzing axis-parallel subspace pro-jections [28, 22, 21]. These approaches do not incorporate any user knowledge. With our SMVC approach, we want to examine the usefulness of instance level constraints for the process of simultaneous multi-view clustering.

Model-based clustering. This general paradigm assumes the considered data to be sampled from a statistical model. Several approaches for estimating the parameters of the un-derlying probability distributions, e.g., to maximize the log-likelihood of the data, were proposed including the EM or variational inference [10]. Model-based clustering is very flexible as the modeled distributions can be arbitrarily com-plex. Traditionally, such approaches use a single mixture distribution (which spans across all dimensions of the data space). Even though each observation might be associated with a membership degree (e.g. the likelihood of belonging to a cluster), this principle does not capture the idea of gen-erating objects through multiple components as required for the multi-view scenario. To overcome this issue, a few mod-els [17, 5, 19] try to represent such multi-component mem-bership (i.e. overlapping clusters). Although, these models lead to results where an object might take multiple roles within a single view, they do not account for the principle of multiple views. So far, MVGen [21] is the only approach assuming a statistical model where each data point is drawn from multiple components each within a different view. It has proven to successfully detect the multi-view clustering structure on various data sets.

Semi-supervised clustering. As already argued, the de-tection of multiple clustering solutions strongly depends on the user X  X  preferences. Semi-supervised clustering [8] pro-vides a possibility to accommodate these preferences as ad-ditional information or domain knowledge into the cluster-ing process. For traditional single view, full-space cluster-ing (e.g. k-Means) a popular solution is to use instance level constraints: the objective function is extended by penalizing violated constraints [6] or one learns a distance metric that best represents the constraints [9]. For model-based cluster-ing few extensions for equivalence constraints exist. [31] in-troduces a closed form EM based on the transitive closure of must-link constraints and proposes a Markov network for handling cannot-link constraints. Since it neither can incor-porate both constraint types simultaneously nor cope with conflicting constraints, [25, 7] propose to integrate negative and positive pairwise constraints as priors into Gaussian mixture models, which allows for modeling soft as well as hard constraints. These approaches have shown to substan-tially improve the clustering result in the single view case. Since in the multi-view case we are uncertain which con-straints refer to which view, these existing solutions cannot easily be transferred.

Methods such as [1] use supervision (e.g. human interac-tion) to enhance the clustering in a single given subspace. In contrast, we exploit supervision to enhance the cluster-ing result across all views simultaneously. Works such as [18] combine subspace clustering with graph clustering. The un-derlying graph might be regarded as a certain type of super-vision. These methods do not focus on finding alternative groupings in the attribute space.

Overall, none of the existing approaches is able to incorpo-rate prior information for a multi-view clustering solution, where constraints may refer to different clustering views. Our new statistical model handles different clustering views in different attribute subspaces and learns responsibilities of views for the provided equivalence constraints.
Setup. We compare SMVC with representatives from all three paradigms: multi-view clustering, subspace clustering, and semi-supervised clustering. For multi-view clustering we choose the four approaches Multi-View 1 and Multi-View 2 proposed in [11], the Alternative Clustering method pro-posed in [30], and our MVGen [21] approach. These ap-proaches best reflect the demands for multi-view clustering
Fig. 3: Quality for a varying number of constraints
Fig. 4: Quality for a varying number of views as discussed in Sec. 4. As subspace clustering approaches we choose the partitioning approach Proclus and StatPC , which allows for overlapping clusters. Furthermore, we compare against the two semi-supervised approaches PCKMeans [6] and MPCKMeans [9], both using instance level constraints. For case studies on real world data we use the CMU-Faces, Iris, and Wine data (all from the UCI repository [3]), and drawn stick figures. Synthetic data containing multiple views is generated based on our generative model. The de-fault data set contains 2 disjoint views, each with 4 clusters, 20 dimensions, and 5000 objects.

Each method is provided with the number m max of views and the number k max of clusters per view. If the algorithm does not allow for setting these parameters, we choose the default parameter setting.

Runtime is measured on 4GHz AMD FX-8350 CPU with 16 GB main memory. Quality is assessed based on the E 4 SC measure [20], which is a symmetric and subspace aware vari-ant of the popular F1 measure. Since most of the competing approaches do not determine axis parallel subspaces, we re-frain from evaluating the subspaces and just concentrate on the object groupings (for clarity we rename the measure to  X  X 4FC X ). For all quality experiments, we average the results over ten executions.
Varying number of constraints. We start our eval-uation by examining the influence of a varying number of constraints in Figure 3. Here, we tested three different vari-ants of the semi-supervised clustering approaches: We either used only must-link constraints (SMVC-ML), only cannot-link constraints (SMVC-CL), or a combination of 50% from both (SMVC-Comb). Note that in this experiment, we ran-domly generated constraints based on the ground truth clus-ters known for synthetic data. These constraints might not help to improve the clustering and, thus, represent only very weak supervision. In practice, the user might provide better constraints, e.g. via the principles of active learning [6].
Figure 3 shows the results for an increasing number of constraints: Here, we generated a challenging dataset with a large variance to study the benefit of semi-supervision. Most approaches fail to identify a meaningful clustering structure for this difficult clustering scenario. SMVC is not only the approach showing the best clustering results without the help of prior knowledge, it is also the only approach able to improve its clustering based on additional constraints. For the two other semi-supervised approaches PCKMeans and MPCKMeans, we even observe a decreasing clustering qual-ity with increasing amount of prior knowledge! This indi-cates, that they cannot deal with the potentially disagreeing constraints of the two views.

We furthermore can see the varying influence of the differ-ent constraints (100% must-link constraints, 100% cannot-link constraints, or 50%must-link + 50% cannot-link). The higher the proportion of must-link constraints, the higher is the influence. The reason is that cannot-link constraints a priori have a higher possibility to be fulfilled than must-link constraints (for m views, each with k clusters, the probabil-ity to fulfill a cannot-link constraint is m  X  k 2 , whereas for must-link constraints it is m  X  k ). Therefore, we will focus on must-link constraints in the following experiments.
Another interesting observation, also stated in [16], is that more constraints do not necessarily result in a better quality. They can even decrease the clustering quality. In Figure 3 we can observe this slightly for cannot-link constraints (SMVC-CL); other experiments showed similar effects for must-link constraints. We kindly refer to [16] for a discussion about these effects. Unfortunately the principles discussed in [16] for wisely choosing the set of constraints are not easily transferable to our scenario.

Varying number of views. In the next experiment, we study the potential of using SMVC as an unsupervised tech-nique in a multi-view setting. In Figure 4, we vary the number of hidden views in the data. The dimensionality of each view is 5, i.e. with increasing number of views, the data X  X  overall dimensionality increases as well. As depicted, SMVC and MVGen are the only approaches able to detect the clustering structure in the case of a large number of views. Their clustering quality is very high and proves to be robust against a varying number of views. The compet-ing methods behave differently: while for single-view data the quality is relatively high, their quality heavily decreases with an increasing number of views.

Scalability. Even though the focus for SMVC lies on its clustering quality, we briefly analyze its efficiency. As al-ready discussed in Section 3, SMVC scales linearly in the number of objects (Figure 5), linearly in the number of dimensions (Figure 6), and linearly in the number of con-straints (Figure 7). Please note the logarithmic scaling of both axes in all three plots. For a varying database size
Fig. 5: Runtime vs. database size (Figure 5), all algorithms show an increasing runtime. The approaches that represent adaptations of the simple and efficient KMeans algorithm (which also includes Proclus) clearly show the lowest runtimes. The runtime of SMVC is comparable to the other algorithms analyzing subspace pro-jections (MVGen, StatPC) and even manages to outperform them thanks to the efficient variational inference techniques.
The benefit of SMVC becomes apparent for a high data dimensionality (Figure 6). Due to the exponential num-ber of subspaces, most subspace clustering algorithms (e.g. StatPC) suffer from a tremendously increasing runtime for an increasing number of dimensions. Also MVGen cannot compete with our SMVC due to the complex model selec-tion process. Contrarily, for SMVC, we observe a moderate increase in runtime. This enables us to apply SMVC also on high-dimensional data, as we will see in the experiments on real world data.

Figure 7 shows the runtime results of the semi-supervised methods for a varying number of constraints. Here, it is hard to verify the linear runtime of SMVC because constraints support the clustering procedure and, thus, help decreas-ing the number of iterations. For a small number of con-straints, the two KMeans-based approaches can maintain a low runtime. For an increasing number of constraints, how-ever, their runtime eventually even meets the one of SMVC. Of course, such a high number of constraints might not be realistic for most applications.
For evaluation on real world data we use different evalua-tion principles, all focusing on the multi-view aspect.
Case study A. In Figures 8 and 9, we extend the data sets Iris and Wine to data containing multiple views: for this, we randomly concatenate the attribute values of differ-ent objects up to five times to a higher dimensional space. The original data sets have dimensionalities of 4 and 13, re-spectively, while the extension to multi-view data leads to dimensionalities up to 5  X  4 = 20 (Iris) and 5  X  13 = 65 (Wine).
For just one view, the quality of some competing ap-proaches is similar to the one of SMVC. However, for an in-creasing number of views the clustering quality for almost all competing approaches decreases. Only MVGen and SMVC are nearly not affected by an increasing number of views but detect the different object groupings even for multiple views.
To study the effects of semi-supervision, we additionally provided for both datasets 100 and 500 constraints. For just a single view SMVC is able to improve the cluster quality. On iris, for example, the quality increases from 0.94 over 0.97 to 1.0. The full potential of our approach, however, can bee seen in the case of multiple views: While it is still able to benefit from prior knowledge, the clustering quality of the competing approaches dramatically decreases.
It is noticeable, that with increasing number of views, the constraints seem to have less positive effect on the result of SMVC. This phenomenon can, however, easily be explained by the fact that the constraints have to be distributed among the views, i.e. the proportion of prior knowledge decreases with increasing number of views.

Summarizing, the results for real world data are consistent with the observations made for the synthetic data.

Case study B. For our next study, we created a data set consisting of 900 20x20 images of  X  X ancing stick fig-ures X . This dataset allows an easy visual interpretation of the clustering results. We drew 9 basic stick figures (Fig-ure 10(a)) and built 900 samples by randomly introducing noise. Since the subspace clustering and single-view clus-tering approaches have proven to be not applicable for the multi-view scenario, we applied only the multi-view cluster-ing approaches in this experiment. We provide this data set on our website 1
Although this data does not seem to be very complex, all approaches are challenged in identifying two meaningful views as shown by their clustering results (cf. Figure 10(d)). Even the initial result of our SMVC approach is not convinc-ing as it produces the clustering depicted in Figure 10(b), which is very similar to those of the other approaches. The illustrated images correspond to the means of each detected cluster. In contrast, if we provide SMVC with 100 must-link constraints, it is able to perfectly identify the two clustering views as depicted in Figure 10(c). These two views differ-entiate between the stick figures X  top position (view 1) and their leg position (view 2). Please note that we only choose 100 random constraints out of the 269,100 (= 2  X  (3  X  300 possible constraints. By exploiting this small amount of prior knowledge, our SMVC approach clearly outperforms all competing methods.

Case study C. To show that the findings of the stick fig-ures data also apply to more complex scenarios, we next ana-lyze the clustering result of all multi-view approaches on the CMUFace data. This data is interesting for multi-view clus-tering since it consists of images taken from persons showing varying characteristics such as their facial expressions (neu-tral, happy, sad, angry), head positions (left, right, straight, up), and eye states (open, sunglasses). As also done in [12], we randomly select 3 persons with all their images and ap-plied PCA retaining at least 90% of the data X  X  variance as a pre-processing. h ttp://www.dme.rwth-aachen.de/SMVC
The result of SMVC without prior knowledge for two views each with three clusters is illustrated in Figure 11(a). The images correspond again to the clusters X  means. By visual inspection, we can easily identify that the first view partitions the images based on the 3 different persons. The second view, in contrast, cannot be explained easily.
If we provide 100 constraints in order to find one view for partitioning w.r.t. the persons and another view to partition w.r.t. the head position (in total 2,592 (= 3  X  32 2 + 4  X  possible constraints), SMVC gets the result depicted in Fig-ure 11(b). Here we can easily identify the different head positions straight, side (left and right), and up (note that we have four head positions but only search for 3 clusters). Using the original labels provided by the dataset as ground truth, i.e. the groupings based on the different persons and the grouping based on different head positions, we obtain the clustering results of Figure 11(c). We can see, that the un-supervised multi-view approaches all yield similar clustering qualities. They were only able to identify the first view. For SMVC, we can observe a noticeable quality improvement if we integrate prior knowledge into the clustering process.
Overall, our experiments show that SMVC is able to de-tect the multi-view clustering structure on a variety of data sets. It successfully solves the challenge to learn the as-signment of user constraints to views such that it is able to improve its clustering results based on this prior knowledge. We have presented the semi-supervised clustering method SMVC that detects multiple clustering solutions in subspace projections and that exploits prior knowledge by incorpo-rating instance level constraints. Our method is based on a sound Bayesian framework which models the data via multi-ple mixture distributions. The model uses the instance level constraints to guide the clustering of objects, and it auto-matically determines which views are responsible for which constraints. For learning the clustering, we use the prin-ciple of variational inference. Our experimental study has shown the high potential of SMVC to detect multiple clus-tering views and its capability to use the prior knowledge for improving the clustering results.
 Acknowledgments. This work has been partly funded by the SteerSCiVA DFG-664/11 project (part of SPP 1335). Stephan G  X  unnemann has been supported by a fellowship within the postdoc-program of the German Academic Ex-change Service (DAAD). [1] C. C. Aggarwal. A human-computer interactive [2] C. C. Aggarwal, J. L. Wolf, P. S. Yu, C. Procopiuc, [3] A. Asuncion and D. Newman. UCI machine learning [4] E. Bae and J. Bailey. Coala: A novel approach for the [5] A. Banerjee, C. Krumpelman, J. Ghosh, S. Basu, and [6] S. Basu, A. Banerjee, and R. J. Mooney. Active [7] S. Basu, M. Bilenko, and R. J. Mooney. A [8] S. Basu, I. Davidson, and K. Wagstaff. Constrained [9] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [10] C. Bishop. Pattern recognition and machine learning . [11] Y. Cui, X. Z. Fern, and J. G. Dy. Non-redundant [12] X. H. Dang and J. Bailey. Generation of alternative [13] X. H. Dang and J. Bailey. A hierarchical information [14] X. H. Dang and J. Bailey. A framework to uncover [15] X. H. Dang and J. Bailey. Generating multiple [16] I. Davidson. Two approaches to understanding when [17] Q. Fu and A. Banerjee. Multiplicative mixture models [18] S. G  X  unnemann, B. Boden, and T. Seidl. Finding [19] S. G  X  unnemann and C. Faloutsos. Mixed membership [20] S. G  X  unnemann, I. F  X  arber, E. M  X  uller, I. Assent, and [21] S. G  X  unnemann, I. F  X  arber, and T. Seidl. Multi-view [22] S. G  X  unnemann, E. M  X  uller, I. F  X  arber, and T. Seidl. [23] P. Jain, R. Meka, and I. S. Dhillon. Simultaneous [24] H.-P. Kriegel, P. Kr  X  oger, and A. Zimek. Clustering [25] Z. Lu and T. K. Leen. Semi-supervised learning with [26] G. Moise and J. Sander. Finding non-redundant, [27] E. M  X  uller, S. G  X  unnemann, I. F  X  arber, and T. Seidl. [28] D. Niu, J. G. Dy, and M. I. Jordan. Multiple [29] L. K. M. Poon, N. L. Zhang, T. Chen, and Y. Wang. [30] Z. Qi and I. Davidson. A principled and flexible [31] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. For the variational distributions, the following holds:
