 We study the problem of finding the k most frequent items in a stream of items for the recently proposed max-frequency measure. Based on the properties of an item, the max-frequency of an item is counted over a sliding window of which the length changes dynamically. Besides being pa-rameterless, this way of measuring the support of items was shown to have the advantage of a faster detection of bursts in a stream, especially if the set of items is heterogeneous. The algorithm that was proposed for maintaining all frequent items, however, scales poorly when the number of items be-comes large. Therefore, in this paper we propose, instead of reporting all frequent items, to only mine the top-k most frequent ones. First we prove that in order to solve this problem exactly, we still need a prohibitive amount of mem-ory (at least linear in the number of items). Yet, under some reasonable conditions, we show both theoretically and empirically that a memory-efficient algorithm exists. A pro-totype of this algorithm is implemented and we present its performance w.r.t. memory-efficiency on real-life data and in controlled experiments with synthetic data.
 H.2.8 [ Database applications ]: Miscellaneous Algorithms, Performance Data stream mining, top-k frequent items
In this paper we study the problem of mining frequent items in a stream under the assumption that the number of different items can become so large that the stream sum-mary cannot fit the system memory. This occurs, e.g., when monitoring popular search terms, identifying IP addresses that are the source of heavy traffic in a network, finding fre-quent pairs of callers in a telecommunication network, etc. In stream mining it is usually assumed that the data arrives at such a high rate that it is impossible to first store the data and subsequently mine patterns from the data. Typi-cally there is also a need for the mining results to be available in real-time; at every moment an up-to-date version of the mining results must be available.

Most proposals for mining frequent items in streams can be divided into two large groups: on the one hand those that count the frequency of the items over the complete stream [3] and on the other hand, those based on the most recent part of the stream only [6, 5]. We are here concerned with the latter type. The current frequency of an item is in this context usually defined as either the weighted average over all transaction where the weight of a transaction decreases exponentially with the age of the transaction or by only considering the items that arrived within a window of fixed length (measured either in time or in number of transac-tions) from the current time. As argued by Calders et al. [1, 2], however, setting these parameters, the decay factor and the window size, is far from trivial. As often the case in data mining, the presence of free parameters rather represents ad-ditional burden on the user than it provides more freedom. Even worse, in many cases no single optimal choice for the parameters exists, as the most logical interval to measure frequency over may be item-dependent.

For these reasons, the max-frequency was introduced. The max-frequency of an item is parameterless and is defined as the maximum of the item frequency over all window lengths. As such, an item is given the  X  X enefit of the doubt X ; for every item its most optimal window is selected. The window that maximizes the frequency can grow and shrink again as the stream grows. Experiments have shown that  X  this new stream measure turns out to be very suitable to early detect sudden bursts of occurrences of itemsets, while still taking into account the history of the itemset. This behavior might be particularly useful in applications where hot topics, or popular combinations of topics need to be tracked  X  [1]
An algorithm was introduced in [1], based on the obser-vation that even though the maximal window can shrink and grow again as the stream passes, only a few points in the stream are actually candidate for becoming the start-ing point of a maximal window; many time points can be discarded. Only for these candidates, called the borders , statistics are maintained in a summary. However, the algo-rithm scales linearly in the number of distinct items in the stream. Therefore, this algorithm is clearly not suitable for systems with limited memory, such as for instance, a dedi-cated system monitoring thousands of streams from a sensor network. Under such circumstances, a very limited amount of memory is allocated for each stream summary. In this pa-per we extend the work on max-frequency by studying how this problem can be solved efficiently. Our contributions are as follows: The organization of the paper is as follows. Section 2 re-visits some background knowledge of the recently proposed Max-Frequency measure. Section 3 presents the theoretical aspects of the top-k frequent items mining problem in a data stream with flexible windows. An approximate algorithm is presented in this section with theoretical analysis to solve the top-k mining problem effectively. In Section 4, indepen-dently from data distribution, a practical approximate al-gorithm is presented. Experimental results conducted with real-life datasets in Section 5 show that the memory require-ments for the approximate algorithm are extremely small compared to the straightforward approach. Nevertheless, the accuracy of the top-k results is preserved with high prob-ability. Section 6 concludes the paper and presents possible extensions of the work.
In this paper we use the following simple yet expressive stream model. We assume the existence of a countable set of items I . A stream S over I is a, possibly infinite, sequence of items from I . We will adopt the notations given in Table 1.
Definition 1 (Max-frequency). Let S be a stream, s be an item, and n a positive integer. The max-frequency of s in S at time n , denoted MaxFreq ( s, S n ) is defined as: The smallest k that maximizes the fraction is called the max-imal border and will be denoted MB ( s, S n ) : Figure 1: Border points of a in S correspond to the highlight positions. Summary ( a, S ) and MaxFreq ( a, S ) are also shown in this figure.

In other words, the max-frequency is defined as the max-imal relative frequency of s in a suffix of S n . For example, in Figure 1 we can see a stream of items S n ( n = 9 ). The first ever occurrence of a is associated with a relative fre-currence of a is associated with a relative frequency equal to frequency is associated. In this case MaxFreq ( a, S 9 ) = 3 and the maximal border is MB ( a, S 9 ) = 6 (the third a from the beginning of the item stream). It is worth noting that the more traditional sliding window approaches consider only one prefix of a user-defined fixed length.

Obviously, for most data streams it is impossible to keep the whole stream into memory and to check, at every times-tamp n , all suffixes of S n in order to find the suffix which gives the maximal frequency. Luckily, however, not every point can become a maximal border. Points that can still potentially become maximal borders are called the border points :
Definition 2 (Borders). An integer 1  X  j  X  n is called a border for s in S n if there exists a continuation of S n in which j is the maximal border. The set of all borders for s in S n will be denoted B ( s, S n ) :
B ( s, S n ) := { j | 1  X  j  X  n |  X  T s.t. S n is a prefix of T In Calders et. al. [1] the following theorem exactly charac-terizing the set of border points was proven:
Theorem 1 (Calders et. al. [1]). Let S be a stream, s be an item, and n a positive integer. j is in B ( s, S n only if for every suffix B of S j  X  1 and every prefix A of S
Intuitively, the theorem states that a point j is a border for item a if and only if for any pair of blocks where the first block lies B efore and the second A fter point j , the frequency of a in the first block should be lower than in the second one. For example, we can take a look at the data stream in Figure 1 again. Let us consider the fourth occurrence of a corresponding to the seventh position in the stream S 9 If we choose B = S 6 , 6 and A = S 7 , 9 we have freq ( a, B ) = 1 &gt; freq ( a, A ) = 2 3 which does not satisfy the condition in Theorem 1 and hence this occurrence of a is not a border point. According to Theorem 1, of the 5 instances of a in S 9 only the two highlighted positions could ever become maximal borders. As was shown in [1], this property is a powerful pruning criteria effectively reducing the number of border points that need to be checked.

Clearly, if j is not a border point in S n , then neither it is in S m for any m &gt; n . Therefore, in [1], the following summary of S n is maintained and updated whenever a new item arrives: let j 1 &lt; . . . &lt; j b be the borders of s in S border set summary is depicted in Table 4. This summary can be maintained easily, is typically very small, and the max frequency can be derived immediately from it. For example, in Figure 1 we see the border set summary of item a in S 9 .

The following theorem allows for reducing the border set summary even further, based on a minimal support thresh-old:
Theorem 2 (Calders et. al. [1]). Let S be a stream, s be an item, n a positive integer, and j in B ( s, S n ) . Let T be such that S n is a prefix of T , and j is the maximal border of s in T . Then, MaxFreq ( s, T )  X  freq ( s, A ) for any prefix Hence, if there exists a block A starting on position j such that the frequency of s in A does not exceed a minimal threshold minfreq , j can be pruned from the list of borders because whenever it may become the maximal border, its max-frequency won X  X  exceed the minimal frequency thresh-old anyway.
In this section, we are going to investigate some theoretical aspects of the mining top-k frequent items with flexible win-dows problem . First, we will prove that the number of bor-der points in a data stream with multiple items can increase along with the data stream size and therefore this number will eventually reach the memory limit of any computing system. As a result, it is emerging to design a memory-efficient algorithm that maintains just partial information about the border point set while it is still able to answer the top-k queries with high accuracy.

Besides, we will also show that any deterministic algo-rithm solving our interested problem exactly requires a mem-ory space with size being linear in the number distinct items of the stream. Thus, under the context that the system memory is limited, we need to design either a randomized exact algorithm or an approximate deterministic algorithm.
We propose a deterministic algorithm to approximate the problem X  X  solutions. More importantly, we will show that the proposed algorithm theoretically uses less memory than the straightforward approach storing the complete set of bor-der points while it is able to answer the top-k queries with surprisingly high accuracy.  X  X igh accuracy X  here refers to that the algorithm reports the top-k answers at any time point with a bounded low probability of having false nega-tives, without any false positives and the order between the items in the reported top-k list is always correct.
We first start by showing that there exists an item stream such that the number of border points increases along with the data stream size regardless how many distinct items in the data stream are. Hence, when the item stream evolves the number of border points will eventually reach the mem-ory limit of any computing system.

Assume that we have a set of m + 1 distinct items I = { a 1 , a 2 ,  X  X  X  , a m , b } and let us denote w l as the sequence  X  ( a 1 )  X  l ( a 2 )  X  X  X   X  l ( a m ) b . Consider the following data stream: W k = w 0 w 1 w 2  X  X  X  w k for k  X  1. The data stream size | W k | is equal to m k ( k +1) 2 + k + 1 . The following lemma gives the number of border points of the data stream W k :
Lemma 1. Data stream W k has exactly mk +1 border points for any k  X  1
Proof. First, regarding the item b there is a unique bor-der point corresponding to the position of the first occur-rence of b in W k . Moreover, we will prove that for every item a i there are exactly k border points corresponding to These k occurrences of the item a i divide W k into k +1 por-tions.

For instance, let us consider a simple case with W k = b a k = 3 and m = 3 . The three first occurrences of a 1 in every sequence  X  j ( a 1 ) divide S k into 4 portions (separated by | ): b | a
Let P i 0 , P i 1 ,  X  X  X  , P i ( k +1) be the portions. The number of instances of the item a i inside the portion P ij is j and the size of P ij is equal to m  X  j + i . Hence, for every a i have a partition of S k corresponding to a strictly increasing sequence of fractions: By the result of the paper [1], we can imply that all the considered positions correspond to the border points of the items a i in W k . Thus, for every item a i we have exactly k border points and thus in total W k has exactly m  X  k + 1 border points.

A direct consequence of this lemma is that given a fixed number of distinct items m when the data stream evolves, the number of border points also evolves along with k . It is important to note that the upper bound on the number of border points presented in the paper [1] is only for a single item, from this result it is not easy to derive a similar upper bound for the multiple-item case. The result in this section is not as strong as the result presented in [1] for the single-item case. However, for the multiple-item case, it is enough to support the fact that there is no modern computing system being able to store the complete set of border points inside its limited memory when k becomes extremely large.
In this section, we will derive a lower bound on the mem-ory usage that every deterministic algorithm will need in or-der to solve the top-k frequent items mining problem exactly. In particular, if we assume that the data stream has at least m distinct items we can show that there is no deterministic algorithm solving the top-k problem exactly with memory less than m . By the terminology deterministic algorithm we mean the model in which whenever a new item arrives in the data stream the algorithm has to decide whether it needs to evict an existing border point in the memory or just ignore it deterministically.

The main theoretical result of this section is shown in the following lemma:
Lemma 2. Let m be the number of distinct items in the data stream. If the system memory limit is m  X  1 , there is no deterministic algorithm being able to answer the top-k ( k &gt; 1 ) frequent items queries exactly all the time even for the special case k = 2 .

Proof. First of all, at a time point t , if the system has information about an item we call it a recorded item and otherwise it is called a missing item . Given a data stream, the most recent item in the data stream is always the highest MaxFreq item, so in order to answer the top-2 query exactly this item must be recorded as long as it arrives in the stream.
Therefore, let us consider the following data stream with m distinct items: S =  X  m ( a m )  X  m  X  1 ( a m  X  1 )  X  X  X  X  this data stream, it is clear that at the moment a 1 must be a recorded item. Since we have assumed that the system has limited memory which is less than m there is always at least one missing item in S . Without loss of generality we assume that a n ( n 6 = 1 ) is the missing item.
We extend the data stream S with l  X  1 instances of the item a 1 , s.t. S =  X  m ( a m )  X  m  X  1 ( a m  X  1 )  X  X  X  X  2 In doing so, a deterministic algorithm does not have any information about a n so far, hence, the item a n remains missing for any value of l .

Moreover, every item a i for i = 2 , 3 ,  X  X  X  , m has a unique border point. This point corresponds to the maximum point prove that there exists l such that a n can become the second highest MaxFreq. In fact, in order to prove this, we have to show that the following inequalities are true for some value of l for any i 6 = n and n  X  2: We rewrite the above inequality as follows: exists such l ) then the inequality (2) is true for all i 6 = n and n  X  2. Hence, with such value of l the item a n becomes the second highest MaxFreq. On the other hand, a n so far is not a recorded item so it will be missing in the top-2 list reported by every deterministic algorithm.

Lemma 2 allows to derive a lower bound as large as m , which is the number of distinct items in the stream, on the memory usage of any deterministic algorithm for solving the top-k problem exactly. When m is greater than the memory limit, solving the problem exactly with a deterministic exact algorithm is no longer possible. Therefore, in the following sections we will focus on approximate approaches which are memory-efficient.
In this subsection we will propose an approximate algo-rithm for the top-k frequent items mining problem. We show that the proposed algorithm can answer top-k queries with high accuracy and consumes less memory than the straightforward approach of storing the complete set of bor-der points. First, we assume that the item distribution in the data stream is know in advance and does not change over time. We make this assumption to simplify the analy-sis of the proposed algorithm. For the cases with unknown data distribution we propose another algorithm in the next section.

Prior to the description of the approximate algorithm we revisit a useful property of the border points stated in The-orem 2, Section 2. According to this theorem, if the tight lower bound on the MaxFreq of the top-k frequent items is known in advance, then we can safely prune any border point p of an item a which has frequency being strictly less than this bound without effect on the accuracy of the top-k results. We call the value that we use to do pruning the pruning threshold . Obviously, zero is a feasible lower bound. However, it is not a meaningful pruning threshold, because there is no border point with relative frequency being less than this threshold.

Indeed, let us revisit the data stream S shown in the proof of Lemma 2. If we extend the data stream S with l instances of a 1 and let l go to infinite we will have a data stream in which the only feasible non-negative lower bound on the MaxFreq of the top-k items is zero, in other words, there is no meaningful pruning threshold for this data stream. Thus, it is impossible to devise a meaningful pruning threshold value for every data stream such that there is no accuracy loss in the top-k results. However, if the item distribution is known in advance we can estimate a good pruning threshold such that the accuracy loss is negligible. We first start with the following lemma:
Lemma 3. Given a time point n and a parameter k , we assume that Y n is the size of the smallest suffix of the item stream S n that contains exactly k distinct items. The top-k frequent items of the data stream S n have MaxFreq at least
Proof. Since the window with size Y n contains k distinct On the other hand, the top  X  k frequent items are always at least as frequent as the least frequent item in this window so the Lemma 3 holds.
 The consequence of Lemma 3 is that at every time point the reciprocal of the smallest suffix of S n containing exactly k distinct items is the lower bound on the MaxFreq of the top-k frequent items. This lower bound is not a fixed value but it may change when the data stream evolves. Let the size of the smallest suffix containing exact k distinct items be de-noted by the random variable X k . Estimating the expected value of X k is well-known in the literature as the classical Algorithm 1 MeanSummary ( k, l ) 1: B  X  X  X  X  2: while New item a arrives do 3: if previous occurred item is not a then 4: create a new border point for a and include it into 5: end if 6: Delete all the elements in B that are no longer border 7: Update relative frequency of all elements in B 8: Delete all the border points in B that have relative 9: end while Coupon Collector Problem (CCP) [7]. We will revisit this problem later in the analysis part. At this point, we will start with the description of the MeanSummary algorithm summarizing the data stream as in Algorithm 1.

Recall that X k is the random variable standing for the size of the smallest window containing exact k distinct items. Algorithm 1 assumes that E ( X k ) is known in advance for l are two user-defined parameters. Having a stream sum-mary the system just needs to take the k highest relative frequency items present in this summary and report this list as the answer to the top-k query. To understand how precise the top-k list produced by MeanSummary is we make some analyses in the subsequent part.
Lemma 4. Given two positive integers k and l , indepen-dently of the item distribution in the data stream, the proba-bility that a top-k item is missing in the answer list reported on the MaxFreq of the top-k frequent items, the event that the least frequent item in the top-k frequent items has its On the other hands, according to the Markov X  X  inequality Pr ( X k  X  lE ( X k ))  X  1 l which proves the lemma.
By the result of Lemma 4, the probability of having false negatives is less than 1 l , independently of the item distribu-tion. In the next section we will derive a better bound for this type of error when the data set follows the uniform dis-tribution in which the expectation and the variance of X k are known.

Finally, we consider another interesting property of the proposed stream summary: MeanSummary is able to answer top-k queries without false positives, that is, the reported top-k list is always a subset of the right answer and the item order in the reported top-k list is preserved.

Lemma 5. Using MeanSummary we are able to answer the top-k queries without false positive, moreover, the item order in the reported top  X  k list always correct.
Proof. Given a time point, assume that a is the k  X  th most frequent item of the item stream S . If a is not present in MeanSummary, that is, when MaxFreq ( a, S ) &lt;  X  , the other items less frequent than a must also be absent in the summary. In this case, the top-k list produced by taking only items present in the summary will not contain any other items than the real top-k items.

On the other hand, if a is present in the summary, that is when MaxFreq ( a, S ) &gt;  X  and the MaxFreq of other top-k frequent items must also larger than  X  , therefore, the border points corresponding to the maximum points of these items must be also present in the summary. In other words, the summary will report the right MaxFreq of them, and hence these items remain in the top-k of the summary and are all present in the answer list.

Since the MaxFreqs of the top-k items we report based on the data stream summary are always exact, the items in the answer list will always be in the correct order.
Algorithm 1 uses prior knowledge about the item distri-bution to define the pruning threshold. In particular, it requires the expected value of X k for any k &gt; 1. Estimating E ( X k ) is well-known as the classical Coupon Collector Prob-lem [7]. Unfortunately, to the best of our knowledge, there is no post work being able to present the value of E ( X k a closed form for all types of distributions [4, 8]. Indeed, in [4], the authors have tried to present E ( X k ) in form of integral formulae which can be approximately estimated by classical numerical methods. These methods however are computationally demanding, especially when k is large [4]. Estimation of this expected value for any type of item dis-tribution is out of the scope of this paper.

Fortunately, for the uniform distribution a simple presen-tation of E ( X k ) is well-known [7, 4]. In addition to that the variance of X k , denoted by  X  2 k , has a closed formula. Having knowledge of the expected value and the variance of X k we can derive a tighter bound on the false negative error for this particular distribution as follows:
Lemma 6. Pr ( X k  X  lE ( X k ))  X  1 ( l  X  1) 2 +1 for any l &gt; 1 Proof. First, we have the well-known formulae [7, 4] of E ( X k ) and  X  2 k :
Recall that m is the number of distinct items in the data stream and that it is much larger than k . Since k &lt; m any i &lt; k we have i m  X  i &lt; 1 and  X  k &lt; By the one-sided version of Chebyshev X  X  inequality we get:
Lemma 6 gives a tighter bound on the false negative than the bound in Lemma 4: the false negative probability is less
In order to demonstrate the effectiveness of the proposed algorithm we have carried out an experiment on a synthetic data set which follows the uniform distribution. We simulate Figure 2: The comparison of memory usage in terms of the number of border points each algorithm has to store in memory. The plot shows how this number evolves when the data stream evolves. MeanSum-mary memory usage is shown for different values of l . It is clear from the figure that MeanSummary uses significantly less memory than the complete set of border points (No Pruning).
 Table 3: Recall (%) of the top-1 000 answer pro-duced by MeanSummary an item stream with 10 000 uniformly distributed distinct items until the stream size reaches the number 100 000 . We have measured the performance of MeanSummary in terms of memory consumption and the quality of the top-1 000 answer. The results are reported in Figure 2 and Table 3.
According to Lemma 5, MeanSummary does not cause any false positives so it always produces the top-k answer with maximum Precision Value , i.e. 100% . On the other hand, MeanSummary may produce false negatives which are usu-ally measured by the Recall Value , i.e. the fraction of the number of real top-k items reported by MeanSummary. In Table 3 we show the Average Recall of the top-1 000 answers produced by MeanSummary over different pruning thresh-olds when the data stream evolves. It is clear from the con-text that when the value of l is higher, MeanSummary is able to answer the top-1 000 queries more accurately on av-erage. The average recall reaches its maximum value i.e. 100% when l is above 1. In order to show the stability of the obtained results we also report the Maximum and the Minimum Recall values of the top-1 000 answers in this ta-ble. It is clear from the context that the obtained results are also quite stable as the differences between the mini-mum, the maximum recall and the average recall are not large.

It is important to note that the higher the value of l , the lower the pruning threshold becomes, resulting in less border points being pruned. In other words, when l is high Mean-Figure 3: The MaxFreq of the k-th most frequent item of the Kosarak data stream over time.
 Summary may use more memory but it will produce more accurate answers. This is a tradeoff between result quality and memory usage. In order to show the effectiveness of MeanSummary in terms of memory usage we plot the num-ber of border points that MeanSummary has to store over different values of the pruning threshold in Figure 2. It is clear that when l is higher, more memory space is required. Fortunately, the number of border points that MeanSum-mary has to store seems to be bounded regardless of the stream size. Concretely, at the end of the algorithm execu-tion the number of border points that Meansummary has to store when l = 2 . 0 is always less than 3 000 which is almost ten times smaller than the total number of border points of the data stream ( 28 793 ). It is important to notice that when l = 2 . 0, Meansummary produces no errors at all for the top-1 000 queries (see Table 3). This fact emphasizes the extreme significance of MeanSummary. We have seen the effectiveness of using MeanSummary in Algorithm 1 to answer the top-k queries with the assumption that the expected value of X k could be computed in advance. However, in practice, this effort may not be possible due to non-trivial expected value computation. Therefore, using lE ( X k ) as a pruning threshold is impractical when the data distribution is unknown in advance or the data distribution changes over time. Even in the case that the data distribu-tion is supposed to be known in advance, for instance, with the Zipfian distribution, there is no closed representation of E ( X k ) that is easy to compute [4].

For the situation that the expectation of X k is unknown, we now propose another summary algorithm which still has similar properties as MeanSummary. Before continuing with a detailed description of the proposed summary in Algorithm 2, we explain the crucial idea behind our proposal in Figure 3. In this figure, we plot the MaxFreq (multiplied by 1 000 ) of the k-th most frequent item of the Kosarak data stream (see section 5.1 for information about this dataset). Four lines correspond to different values of k from which we can observe that they are quite well-separated from each other. Intuitively, if we consider the upper bound on the top-100 Algorithm 2 MinSummary ( k, l ) 1: B  X  X  X  2:  X   X  0 3: while New item a arrives do 4: if previous occurred item is not a then 5: create a new border point for a and include it into 6: end if 7: Delete all the elements in B that are no longer border 8: Update frequency of all elements in B 9: Delete all the elements in B that have frequencies 10:  X   X  MaxFreq of the least frequent item in B 11: |B| X  the number of distinct items in B 12: if |B| X  l AND  X  &gt;  X  then 13:  X   X   X  14: end if 15: end while Figure 4: The Kosarak and Sligro data-sets fol-low Zipfian distribution but with different levels of skewed. line as a pruning threshold we can prune a lot of border points while the top-10 items are warranted to be present in the summary with high probability.

The aforementioned observation from the Kosarak data set is intuitive, allowing us to propose a summary algorithm which is then shown to be effective in the experiments with real-life datasets. The approach is briefly described in Al-gorithm 2. MinSummary is similar to MeanSummary, the only difference is that it uses a dynamic pruning threshold  X  instead of a static value. This threshold is updated in each step such that its value is monotonically increasing, thus, more border points are pruned in the further steps. The algorithm admits two user-defined parameters k and l . The bigger the value of l , the bigger the summary will become, but the more precise the top-k queries will be answered.
In order to prevent  X  from increasing too fast we only up-date the value of  X  when the summary B contains at least l different items. By doing so, we keep  X  increasing but always less than the upper bound of the l-th most frequent item. As a result,  X  is less than the frequency of k-th most frequent item with high probability as explained in the aforemen-tioned intuitive example. The following lemma shows that MinSummary has similar properties as MeanSummary:
Lemma 7. Using MinSummary we are able to answer top-k queries without false positives and the item order in the reported top-k items is preserved.

It is important to note that  X  in MinSummary is kept increasing over time to let MinSummary have similar prop-erty like MeanSummary. In doing so, the proof of Lemma 7 proceeds in a very similar way as the proof of Lemma 5.
We use two real world data sets to conduct our experi-ments. The characteristics of them are summarized in Table 4. The Kosarak is a publicly available dataset 1 containing click-stream data of a Hungarian online news portal while the Sligro data set is released under restricted conditions containing information about products purchased by Sligro company X  X  customers in a specific city in the Netherlands from August 2006 to October 2008.

Both data sets follows a Zipfian distribution as we can see in Figure 4 in which we plot the item frequency (vertical axis) from the most frequent to the least frequent items (hor-izontal axis). Generally, the Sligro data set follows a Zipfian distribution but the occurrence of every specific item varies in different periods of the year. Some items may be suddenly frequent in a short period of time and may completely disap-pear in another period, e.g. seasonal products. We conduct the experiment on the Sligro dataset to see the behavior of MinSummary in the context of rapidly changing frequencies. It is worth noting that by the results of Lemma 7, Min-Summary always produces the top-k list with the maximum precision value, i.e. 100%. So in order to measure the ac-curacy of MinSummary we just need to measure the recall values of the top-k lists.

Concretely, each time when a new item arrives in the data stream we do querying from MinSummary for the top-k fre-quent items. The obtained top-k list from MinSummary will then be compared with the true top-k set to estimate the re-call value. We average the recall value over time and show the results in Table 5. We also present the maximum and the minimum recall values in this table to see the deviation of the recall from its average value.
 We present results for different values of the parameters. The value of k is set to 100 and 1 000 respectively while l is set to k and higher. Recall that l is a parameter that controls the memory usage in MinSummary. The higher value of l the more memory is used to maintain MinSummary.

In Table 5 we can observe that whenever l is increased the obtained top-k list is more accurate. In particular, we are http://fimi.cs.helsinki.fi/data/ able to reach the maximum recall value with proper choice of l . The obtained results with higher values of l are also very stable since the deviation of maximum and minimum recall from its average value is negligible. In summary, using MinSummary with a proper choice of its parameters we are able to answer the top-k query with very high accuracy for these particular real world datasets.
In order to illustrate the relation between the pruning threshold and the performance of MinSummary, we plot the value of  X  with different setups of parameter l in Figure 6. We also plot the MaxFreq of the k  X  th highest frequent item in the stream corresponding to k = 100 and k = 1 000 .
It is clear from the context that  X  starts with a very low value and increases every time a new item arrives in the stream. With a proper choice of l we can let  X  increase up to a tight lower bound on the MaxFreq of the top-k frequent items. In doing so, as the pruning threshold grows, we prune more border points while preserving the accuracy of the top-k answers. Concretely, assume that we intend to answer the top-1 000 frequent item query, l = 2 000 or l = 3 000 are the proper choices because in these cases  X  (lines  X  X =2 000 X  and  X  X =3 000 X ) increases up to the lower bound on the MaxFreq of the top-1 000 frequent items (line X  X op-1 000 X ). Obviously, when l = 3 000 we have a more accurate answer because there is no overlap between the line  X  X =3 000 X  and the line  X  X op-1 000 X , but MinSummary will consumes more memory as compared to the case l = 2 000 . For top-100 queries it is clear that l = 1 000 is a very good choice.
Following section 5.2 about the accuracy of the obtained top-k lists, we plot the memory usage of MinSummary in Figure 5. In each plot we compare the size of MinSummary in terms of the number of border points that it has to store with the complete set of border points of the data stream (No Pruning line). We can observe that the size of Min-Figure 6: Evolution of the pruning threshold  X  over time.  X  starts with low value and increases up to the tight lower bound of the top-k MaxFreq Summary increases with increasing l . Yet, if we compare these value with the complete set of border points we see that MinSummary consumes significantly less memory and the memory consumption seems to be independent from the stream size.

For instance, consider the case in Figure 5.B in which we plot the memory usage of the MinSummary with the Kosarak data stream and k = 1 000 . When the program finishes its execution, MinSummary stores less than 4 000 border points for all values of l while the complete set of border points still keeps evolving and reaches its highest value 128 249 . That means MinSummary is 300 times more memory-efficient than the straightforward approach. More-over, we have seen in Table 5 that when l = 2 000 , MinSum-mary produces top-k answers with negligible errors. This fact confirms the significance of MinSummary in compari-son with the straightforward approach.

The results with the Sligro dataset in Figure 5.C and 5.D confirm again the significant performance of MinSummary. Another important point we can observe is that given a value of l , the size of MinSummary seems to be bounded regardless of the stream size. This means when the stream evolves the set of the border points will eventually reach any limit but MinSummary size will not.
In this paper we have shown that an exact deterministic algorithm for computing the top-k MaxFreq items inher-ently has to use an amount of memory at least as large as the number of distinct items in the data stream. Thus, an exact deterministic algorithm is not feasible for applications with limited memory such as, e.g. mobile devices, sensor networks. However, fortunately memory-efficient approxi-mate algorithm exists. We have proposed such an algorithm using significantly less memory as compared to the straight-forward approach while preserving the high accuracy of the top-k results. The experiment conducted with real-life and syntactic datasets confirms the significance of the proposed algorithm. For future work we will consider extending the approach to the top-k frequent itemsets mining problem in the transaction stream with sliding windows. Another pos-sible extension is considering the problem with a minimum windows constraint [2].
 We would like to thank Frank Takes and the anonymous reviewers for their useful comments to improve our work. [1] T. Calders, N. Dexters, and B. Goethals. Mining [2] T. Calders, N. Dexters, and B. Goethals. Mining [3] E. D. Demaine, A. Lopez-Ortiz, and J. I. Munro. [4] P. Flajolet, D. Gardy, and L. Thimonier. Birthday [5] R. E. Giannella C., Han J. and L. Chao Mining [6] L. K. Lee and H. F. Ting. A simpler and more efficient [7] R. Motwani and P. Raghavan. Randomized algorithms. [8] A. N. Myers and H. S. Wilf. Some new aspects of the
