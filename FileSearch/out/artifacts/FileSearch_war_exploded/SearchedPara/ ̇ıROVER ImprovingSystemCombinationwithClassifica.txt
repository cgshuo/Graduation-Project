 State-of-the-art automatic speech recognition (ASR) sys-tems today usually include multiple contrasting systems, which are ultimately combined to produce the final hy-pothesis. There is consensus that impro vements from combination are usually best when systems are suf fi-ciently dif ferent, but there is uncertainty about which sys-tem combination method performs the best. In addition, the success of commonly used combination techniques varies depending on the number of systems that are com-bined (Hof fmeister et al., 2007). In this work, we develop a system combination method that outperforms all pre vi-ously kno wn techniques and is also rob ust to the number of component systems. The relati ve impro vements over ROVER are particularly lar ge for combination when only using two systems.

The aim of system combination for ASR is to mini-mize the expected word error rate (WER) given multiple system outputs, which are ideally annotated with word confidence information. The most widely used system combination approach to date is ROVER (Fiscus, 1997). It is a simple voting mechanism over just the top hy-pothesis from each component system. Two alternati ves that incorporate information about multiple hypotheses and leverage word posterior probabilities are confusion netw ork (CN) combination (Mangu et al., 2000; Ev er-mann and Woodland, 2000) and minimum Time Frame Word Error (min-fWER) decoding (Hof fmeister et al., 2006), discussed further in the next section. Pre vious work found that among ROVER, CN combination, and min-fWER combination, no one method was consistently superior across varying numbers and types of systems (Hof fmeister et al., 2007).

The main contrib ution of this work is to develop an approach that always outperforms other possible system combination methods. We train a classifier to learn which system should be selected for each output word, using features that describe the characteristics of the compo-nent systems. ROVER alignments on the 1-best hypothe-ses are used for decoding, but man y of the features are deri ved from the system lattices. The classifier learns a selection strate gy (i.e. a decision function) from a devel-opment set and then is able to mak e better selections on the evaluation data then the current 1-best or lattice-based system combination approaches.

Ne xt, Section 2 describes pre vious work in system combination techniques. Section 3 describes our ap-proach, and Section 4 pro vides experiments and results. Finally , we summarize the approach and findings in Sec-tion 5. Pre vious work in speech recognition system combination has produced significant impro vements over the results possible with just a single system. The most popular , and often best performing method is ROVER (Fiscus, 1997), which selects the word that the most systems agree on at a particular location (majority voting). An extended version of ROVER also weights system votes by the word confidence produced by the system (confidence voting).
Further impro vements have been achie ved by includ-ing multiple system alternati ves, with methods such as Confusion Netw ork Combination (CNC) (Ev ermann and Woodland, 2000), or N-Best ROVER (Stolck e et al., 2000), which is a special case of CNC. Alternati vely , the combination can be performed at the frame level (min-fWER) (Hof fmeister et al., 2006). Recent work found that the best system combination method depended on the number of systems being combined (Hof fmeister et al., 2007). When only two systems are available, approaches considering multiple alternati ves per system were bet-ter , but as the number of systems increased the standard ROVER with confidence scores was more rob ust and sometimes even better than CNC or min-fWER combi-nation.

Another approach (Zhang and Rudnick y, 2006) used two stages of neural netw orks to select a system at each word, with features that capture word frequenc y, posteri-ors at the frame, word, and utterance level, LM back-of f mode, and system accurac y. The y obtained consistent but small impro vements over ROVER: between 0.7 and 1.7% relati ve gains for systems with about 30% WER. We develop a system that uses the ROVER alignment but learns to consistently mak e better decisions than those of standard ROVER. We call the new system  X   X  ROVER, where the  X   X  stands for impro ved results, and/or intelligent decisions. The follo wing sections discuss the compo-nents of our approach. First, we emulate the approach of ROVER in our lattice preprocessing and system align-ment. We then introduce new methods to extract hypoth-esis features and train a classifier that selects the best system at each slot in the alignment. 3.1 Lattice Pr eparation Our experiments use lattice sets from four dif ferent sites. Naturally , these lattice sets dif fer in their vocab ulary , segmentation, and density . A compatible vocab ulary is essential for good combination performance. The main problems are related to contractions, e.g.  X  X ou X  ve X  and  X  X ou have X , and the alternati ves in writing foreign names, e.g.  X  X chr  X  oder X  and  X  X chroder X . In ASR this problem is well-kno wn and is addressed in scoring by using map-pings that allo w alternati ve forms of the same word.
Such a mapping is pro vided within the TC-S TAR Ev al-uation Campaign and we used it to normalize the lat-tices. In case of multiple alternati ve forms we used only the most frequent one. Allo wing multiple parallel alter -nati ves would have distorted the posterior probabilities deri ved from the lattice. Furthermore, we allo wed only one-to-one or one-to-man y mappings. In the latter case we distrib uted the time of the lattice arc according to the character lengths of the tar get words.

In order to create comparable posterior probabilities over the lattice sets we pruned them to equal average density . The least dense lattice set defined the tar get density: around 25 for the development and around 30 for the evaluation set.

Finally , we unified the segmentation by concatenat-ing the lattices recording-wise. The concatenation was complicated by segmentations with overlapping regions, but our final concatenated lattices scored equally to the original lattice sets. The unified segmentation is needed for lattice-based system combination methods lik e frame-based combination. 3.2 System Alignments In this work we decided to use the ROVER alignment as the basis for our system combination approach. At first glance the search space used by ROVER is very limited because only the first-best hypothesis from each compo-nent system is used. But the oracle error rate is often very low, normally less than half of the best system X  s error rate.
The ROVER alignment can be interpreted as a confu-sion netw ork with an equal number of arcs in each slot. The number of arcs per slot equals the number of compo-nent systems and thus mak es the training and application of a classifier straightforw ard.

For the production of the alignments we use a stan-dard, dynamic programming-based matching algorithm that minimizes the global cost between two hypothesis. The local cost function is based on the time overlap of two words and is identical to the one used by the ROVER tool. We also did experiments with alternati ve local cost functions based on word equalities, but could not outper -form the simple, time overlap-based distance function. 3.3 Hypothesis Featur es We generate a cohort of features for each slot in the alignment, which is then used as input to train the classi-fier . The features incorporate kno wledge about the scores from the original systems, as well as comparisons among each of the systems. The follo wing paragraphs enumerate the six classes of feature types used in our experiments (with their names rendered in italics).

The primary , and most important feature class covers the basic set of features which indicate string matches among the top hypotheses from each system. In addition, we include the systems X  frame-based word confidence. These features are all the information available to the standard ROVER with confidences voting.

An additional class of features pro vides extended con-fidence information about each system X  s hypothesis. This feature class includes the confusion netw ork (CN) word confidence, CN slot entrop y, and the number of alter -nati ves in the CN slot. The raw language model and acoustic scores are also available. In addition, it in-cludes a frame-based confidence that is computed from only the acoustic model, and a frame-based confidence that is computed from only the language model score. Frame-based confidences are calculated from the lattices according to (W essel et al., 1998); the CN-algorithm is an extension of (Xue and Zhao, 2005).

The next class of features describes dur ational aspects of the top hypothesis for each system, including: charac-ter length, frame duration, frames per character , and if the word is the empty or null word. A feature that normalizes the frames per character by the average over a windo w of ten words is also generated. Here we use characters as a proxy for phones, because phone information is not available from all component systems.

We also identify the system dependent top err or words for the development set, as well as the words that occur to the left and right of the system errors. We encode this information by indicating if a system word is on the list of top ten errors or the top one hundred list, and lik ewise if the left or right system conte xt word is found in their corresponding lists.

In order to pro vide comparisons across systems, we compute the character distance (the cost of aligning the words at the character level) between the system words and pro vide that as a feature. In addition, we include the confidence of a system word as computed by the frame-wise posteriors of each of the other systems. This allo ws each of the other systems to  X  X core X  the hypothesis of a system in question. These cross-system confidences could also act as an indicator for when one system X  s hy-pothesis is an OO V-w ord for another system. We also compute the standard, confidence-based ROVER hypoth-esis at each slot, and indicate whether or not a system agrees with ROVER X  s decision.

The last set of features is computed relati ve to the combined min-fWER decoding. A confidence for each system word is calculated from the combined frame-wise posteriors of all component systems. The final feature indicates whether each system word agrees with the com-bined systems X  min-fWER hypothesis. 3.4 Classifier After producing a set of features to characterize the sys-tems, we train a classifier with these features that will decide which system will propose the final hypothesis at each slot in the multiple alignment. The tar get classes include one for each system and a null class (which is selected when none of the system outputs are chosen, i.e. a system insertion).

The training data begins with the multiple alignment of the hypothesis systems, which is then aligned to the reference words. The learning tar get for each slot is the set of systems which match the reference word, or the null class if no systems match the reference word. Only slots where there is disagreement between the systems X  1-best hypotheses are included in training and testing. The classifier for our work is Booste xter (Schapire and Singer , 2000) using real Adaboost.MH with logistic loss (which outperformed exponential loss in preliminary ex-periments). Booste xter trains a series of weak classifiers (tree stumps), while also updating the weights of each training sample such that examples that are harder to classify recei ve more weight. The weak classifiers are then combined with the weights learned in training to predict the most lik ely class in testing. The main dimen-sions for model tuning are feature selection and number of iterations, which are selected on the development set as described in the next section. We first perform experiments using cross-v alidation on the development set to determine the impact of dif ferent feature classes, and to select the optimal number of iter -ations for Booste xter training. We then apply the models to the evaluation set. 4.1 Experimental setup In our experiments we combine lattice sets for the English task of the TC-S TAR 2006 Ev aluation Campaign from four sites. The TC-S TAR project partners kindly pro-vided RWTH their development and evaluation lattices. Systems and lattice sets are described in (Hof fmeister et al., 2007).

Table 1 summarizes the best results achie ved on the single lattice sets. The latter columns sho w the results of CN and min-fWER based posterior decoding (Mangu et al., 2000; Wessel et al., 2001). 4.2 Featur e analysis on development data We evaluate the various feature classes from Section 3.3 on the development set with a cross validation testing strate gy. The results in Tables 2 and 3 are generated with ten-fold cross validation, which maintains a clean separation of training and testing data. The total number of training samples (alignment slots where there is system disagreement) is about 3,700 for the 2 system case, 5,500 for the 3 system case, and 6,800 for the 4 system case.
The WER results for dif ferent feature conditions on the development set are presented in Table 2. The typical ROVER with word confidences is pro vided in the first row for comparison, and the remainder of the rows con-tain the results for various configurations of features that are made available to the classifier .

The basic features are just those that encode the same information as ROVER, but the classifier is still able to learn better decisions than ROVER with only these fea-tures. Each of the follo wing rows pro vides the results for adding a single feature class to the basic features, so that the impact of each type can be evaluated.

The last two rows contain combinations of feature classes. First, the best three classes are added, and then all features. Using just the best three classes achie ves almost the best results, but a small impro vement is gained when all features are added. The number of iterations in training is also optimized on the development set by se-lecting the number with the lowest average classification error across the ten splits of the training data. Features 2 System 3 System 4 System
ROVER 10.2% 8.8% 9.0% basic 9.4% 8.6% 8.5% +confidences 9.3% 8.7% 8.4% +dur ational 9.2% 8.6% 8.4% +top err or 9.0% 8.5% 8.4% +comparisons 8.9% 8.6% 8.4% +min-fWER 8.5% 8.5% 8.4% +top+cmp+fWER 8.3% 8.3% 8.2% all featur es 8.3% 8.2% 8.2% Table 2: WER results for development data with dif ferent feature classes. ROVER (maj.) 10.8% 9.1% 9.1%
ROVER (conf.) 10.1% 8.8% 9.0% min-fWER 9.6% 9.2 % 8.9 %  X   X 
ROVER 8.3% 8.2% 8.2% oracle 6.5% 5.4% 4.7% Table 3: WER[%] results for development data with manual segmentation, and using cross-validation for  X   X 
ROVER. 4.3 Results on evaluation data After analyzing the features and selecting the optimal number of training iterations on the development data, we train a final model on the full development set and then apply it to the evaluation set. In all cases our clas-sifier achie ves a lower WER than ROVER (statistically significant by NIST matched pairs test). Table 3 and Ta-ble 4 present a comparison of the ROVER with majority voting, confidence voting, frame-based combination, and our impro ved ROVER (  X   X  ROVER). In summary , we develop  X   X  ROVER, a method for sys-tem combination that outperforms ROVER consistently across varying numbers of component systems. The rela-tive impro vement compared to ROVER is especially lar ge for the case of combining two systems (14.5% on the evaluation set). The relati ve impro vements are lar ger than any we kno w of to date, and the four system case achie ves the best published result on the TC-S TAR English evalu-ation set. The classifier requires relati vely little training data and utilizes features easily available from system lattices.

Future work will investigate additional classifiers, clas-sifier combination, and expanded training data. We are also interested in applying a language model to decode an alignment netw ork that has been scored with our clas-sifier .
 ROVER(maj.) 9.0% 7.2% 7.3%
ROVER(conf.) 8.2% 7.1% 7.0% min-fWER 7.6 % 7.4 % 7.2 %  X   X  ROVER 7.1% 6.9% 6.7% oracle 5.2% 4.1% 3.6%
