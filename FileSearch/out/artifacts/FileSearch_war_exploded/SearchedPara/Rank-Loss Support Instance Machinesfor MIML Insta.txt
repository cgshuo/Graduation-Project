 Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classi-fied are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predict-ing instance labels while learning from data labeled only at the bag level. We propose Rank-Loss Support Instance Machines, which optimize a regularized rank-loss objective and can be instantiated with different aggregation models connecting instance-level predictions with bag-level predic-tions. The aggregation models that we consider are equiv-alent to defining a  X  X upport instance X  for each bag, which allows efficient optimization of the rank-loss objective using primal sub-gradient descent. Experiments on artificial and real-world datasets show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label clas-sification.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval; I.5.2 [ Design Methodology ]: Classifier Design and Evaluation Algorithms, Performance, Experimentation instance annotation, image annotation, multi-instance, multi-label, support vector machine, sub-gradient, bioacoustics
Many problems in supervised classification have a certain structure, where the objects of interest (e.g., images or text documents) can naturally be decomposed into a collection of parts or formally, a bag-of-instances representation. For example, in image classification, an image is typically a bag, and the pixels or segments in it are instances. This struc-ture motivates multiple-instance learning (MIL) [7]. The original formulation of MIL concerns problems where bags are associated with a single binary label. Zhou and Zhang [30] proposed multi-instance multi-label learning (MIML), where bags are instead associated with a set of labels. For example, an image might be associated with a list of the objects it contains.

MIML arises in situations where the cost of labeling indi-vidual instances becomes prohibitive and consequently mul-tiple instances are grouped and associated with a set of la-bels. For example, labeling individual pixels in an image takes minutes, but assigning a few words to an image can be accomplished in seconds. In MIML, the training dataset consists of a collection of bags of instances, where each bag is associated with multiple labels. The goal is to learn a classifier that predicts the label set for a previously unseen bag. Numerous algorithms for MIML have been proposed and applied to image and text domains [30, 31, 27, 14, 20].
A related problem which has received little attention [29] is learning to predict instance labels from MIML training data. For example, one might train a classifier on a collection of images paired with lists of object names in each image, then make predictions about the label for each region in an image. This problem is called the instance annotation problem for MIML. The key issue in instance annotation is how to learn an instance-level classifier from a MIML dataset (which presents only bag-level labels).

A common strategy in designing MIML algorithms is to learn an instance-level model by minimizing a loss function defined at the bag level. For example, several MIML al-gorithms minimize bag-level Hamming loss, which captures the disagreement between a ground-truth label set, and a predicted label set. For instance annotation, typically the instance-level classifier outputs a score for each class, and the instance label is predicted as the highest scoring class. Therefore the predicted label depends on the ranking of class scores. Hamming loss is not appropriate in this context, de-spite its success at bag-level predictions, because it lacks a mechanism to calibrate the scores between different classes. This observation motivates us to introduce a rank-loss ob-jective for instance annotation, which directly optimizes the ranking of classes.

In order to learn an instance-level classifier using a bag-level loss function, it is necessary to define an aggregation model that connects instance-level predictions with bag-level predictions. In this paper, we examine two different aggre-gation models, which are equivalent to defining a  X  X upport instance X  for each bag. Therefore, we name our methods Rank-Loss Support Instance Machines (SIM). In this paper, we make the following contributions:
In this section we formalize the instance annotation prob-lem and contrast it with several related problems.

We are given a training set of n bags ( X 1 ,Y 1 ) ,..., ( X Each X i is a bag of n i instances, i.e., X i = { x i, 1 ,  X  X  X  , x with x i,q  X  X , where X = R d is a d-dimensional feature space. Each bag X i is associated with a label set Y i  X  Y where Y = { 1 ,  X  X  X  ,c } and c is the total number of classes. The goal of instance annotation in MIML is to learn an in-stance level classifier f IA : X  X  X  that maps an element of the input space X to its corresponding class label.
The instance annotation problem is different from the traditional MIML learning problem studied by Zhou and Zhang [30] and many others, where the goal is to learn a bag-level classifier F MIML : 2 X  X  2 Y . However, the de-sign principles of many traditional MIML algorithms can be used to learn instance-level classification models, which is the approach we take in this paper.

Instance annotation is closely related to the classic super-vised classification problem, where the goal is to learn an in-stance classifier, but using training data that does not have a bag structure and has each instance labeled individually. In contrast to MIML, this classic setting is often referred to as single-instance single-label (SISL) learning.

Ambiguous label classification (ALC) [13, 6] is another related framework. In ALC, there are no bags; instead in-stances are paired with a set of possible labels, only one of which is correct. An ALC dataset is ( x 1 ,Y 1 ) ,..., ( x where x i  X  X  and Y i  X  X  . In ALC the goal is to learn a clas-sifier that predicts instance labels, hence an ALC classifier is a function f ALC : X  X  Y . A MIML instance annotation problem can be transformed into a ALC problem by cre-ating one ALC instance for each instance in a MIML bag, paired with all of the labels from the bag. Hence ALC algo-rithms can be applied to MIML instance annotation prob-lems. However, this reduction may discard useful bag-level structure in the MIML data.
Here we discuss some design patterns in traditional MIML algorithms (aimed at bag-level predictions) that contribute to our proposed methods (see Sec. 6 for different approaches to instance annotation, e.g., graphical models).

One common approach in MIML algorithms is to make bag-level predictions based on the outputs of instance-level models. Many algorithms leverage an assumption that the bag label set is equal to the union of the instance labels (i.e. there are no missing or spurious labels). This assump-tion is used in several MIML algorithms including M 3 MIML [28], and D-MimlSvm [31]. The following formulation is fre-quently used to capture this assumption. Let f j ( x ) : X  X  be a function which takes an instance and returns a real-valued score for class j . The output at the bag-level for class j is defined to be F j ( X ) = max x  X  X f j ( x ). A bag-level classifier can be obtained by applying a threshold (e.g., 0) to that if an instance x  X  within bag X is predicted to belong will necessarily contain j because F j ( X ) = max x  X  X f f ( x  X  ) &gt; 0. Hence, connecting instance and bag-level scores via the max function is equivalent to defining the bag label set as the union of the instance labels.

In contrast with SISL or instance annotation which are evaluated based on instance-level accuracy, MIML algorithms are evaluated based on their label set predictions. Two com-mon performance measures are Hamming loss, and rank loss [31]. Hamming Loss is the number of false positives and false negatives, averaged over all classes and bags, 1 nc Rank loss captures the number of label pairs that are incor-rectly ordered by the scores of the MIML classifier. Classes in the true label set should receive higher scores than classes that are not. Let  X  Y denote the complement of Y . Rank loss is defined as
These objectives are difficult to optimize directly because they are not continuous. Several algorithms for MIML can be viewed as optimizing a surrogate for Hamming loss. For example, D-MimlSvm [31] and M 3 MIML [28] optimize vari-ations of the following loss function (with different regular-ization terms) where Y j i = +1 if j  X  Y i and  X  1 if j /  X  Y i .
The hinged Hamming-loss objective (2) can be decom-posed into an independent MIL problem for each class, so it does not calibrate the scores between classes, which could make predicting an instance label based on the highest scor-ing class unreliable. To overcome this limitation, we consider rank loss instead. We are not aware of any MIML algorithms that learn an instance-level model by minimizing rank loss (i.e. rank loss has only been used as a performance measure, not as an objective). Rank loss has been used as an objective for (single-instance) multi-label SVMs [9].
We consider classifiers for instance annotation that use one instance-level model for each class f j ( x ) = w j  X  x , and predict a specific label via f ( x ) = arg max j f j ( x ). The goal is to learn the weights W = [ w 1 ,..., w c ] (note x  X  R R , and W  X  R cd ). The predicted instance label depends on the ranking of scores for each class, so we propose to directly optimize this ranking.
Because we are learning from a MIML dataset, we can only use a loss function that evaluates predictions at the bag level, i.e. the loss function measures the agreement between a bag label set and the bag-level scores F j ( X i ). We propose a regularized surrogate for the rank loss (1): This objective is designed to encourage a correct ranking of the bag-level scores for each class. For a bag X i with cor-responding label set Y i , if j  X  Y i and k  X   X  Y i , then the loss of at least 1 promotes a large-margin solution). The objec-tive is also designed to facilitate primal sub-gradient descent optimization methods, which we discuss in the next section.
This objective can be instantiated with various aggrega-tion models that compute bag-level scores from instance-level scores. For example, the max model, which has been used in prior work, with a linear instance classifier is It is equivalent to write F j ( X i ) = w j  X   X  x i,j , where We refer to  X  x i,j as the  X  X upport instance X  for bag X i j , because the bag-level output for X i depends only on the support instance for each class (analogous to a support vec-tor). Therefore we name our proposed method Rank-Loss Support Instance Machines (SIM).

The max model represents each bag with the most char-acteristic instance of each class. This approach can ignore other instances that are also useful for learning, and may not be appropriate when the assumption that the bag label set is equal to the union of instance labels does not hold. We propose an alternative softmax model, which can also be expressed in terms of support instances, but has the ad-vantage of basing the support on more than one instance per class for each bag. The softmax model represents each bag as a weighted average of the instances, with weights specific to each class:
F j ( X i ) = X The weights are defined according to a softmax rule, We can also write the softmax model as F j ( X i ) = w j  X   X  x with the support defined as
We can rewrite the rank-loss objective in terms of support instances as  X  g rank ( W ) =  X 
If  X  x i,j is constant, this objective is convex. However, using the max and softmax models,  X  x i,j is a function of w j , and the objective is non-convex. We can still use convex optimiza-tion techniques by alternating between updating the support and keeping the support constant while optimizing the ob-jective. The MI-SVM algorithm for MIL (single-label) also uses the max function to connect instance and bag labels, and alternates between computing the support and optimiz-ing the resulting objective [1].
To optimize  X  g rank , we use a sub-gradient descent method similar to Pegasos, an algorithm for training linear two-class SVMs [19]. The Pegasos algorithm is based on a general framework for optimizing regularized convex objectives [18]. This framework can be applied to convex optimization prob-lems in the form: For such problems, the following algorithm can be applied: The feasible space is S ; P [ W ] = arg min a projection back into the feasible space. Note that when S = { W : || W ||  X  r } , the projection simplifies to P [ W ] = min { 1 , r || W || } W (this will be the case for our objective).
This algorithm can be considered an example of the pro-jected sub-gradient method [2] with learning rate 1 W  X  be the optimal solution i.e. W  X  = arg min Shwartz and Singer [18] showed the following convergence rate for this algorithm, where L is a constant bounding the magnitude of the sub-number of iterations of sub-gradient descent T is small enough to treat log T as constant, hence to obtain a solution that is within of optimal, it suffices to run T  X  O ( L  X  ) iterations of the above algorithm [18].

Treating the support instances as constant, we can apply the Pegasos framework for sub-gradient descent to minimize the rank-loss objective  X  g rank . We denote the component of the sub-gradient corresponding to w q as v rank q , where q = 1 ,...,c . The full sub-gradient is V rank = [ v rank 1 ,..., v v q =  X  w q +
In order to prove sub-gradient descent converges, we must establish L , the bound on the sub-gradient. We begin by showing the following Lemma:
Lemma 1: Consider any objective of the form g ( W ) = || W || 2 + loss ( W ), such that loss ( W )  X  0 and loss ( 0 ) = 1. Let the optimal solution be W  X  = arg min W g ( W ). Then Proof: The optimal solution must be at least as good as W = 0 , therefore g ( W  X  )  X  1. Furthermore, loss ( W  X  (assuming the contrary implies g ( W  X  ) &gt; 1, which is a con-tradiction). Because the loss is non-negative, 0  X  loss ( W 1. We will use this property to finish the proof: g ( W  X  ) =  X 
The  X  g rank objective satisfies the criteria of Lemma 1, so we can replace unconstrained minimization of  X  g rank minimization restricted to the set S = { W : || W || 2  X  2 without changing the solution. It is also necessary to bound the magnitude of an instance feature vector, || x || X  R . Note that W  X  S implies ||  X  w q ||  X   X  q 2  X  = 1). Also, 1 | Y is not true, the objective is undefined). Using the above, we can derive the bound L : Therefore L = c 2
To compute V rank , one could compute v rank 1 ,..., v rank from (5), but this will take O ( nc 3 ) time; where n is the number of bags and c is the number of classes. We give an O ( nc 2 ) algorithm below:
Running T = O ( L  X  ) iterations with a runtime of O ( nc per iteration of sub-gradient descent gives an upper bound of in the number of bags and 1 . In practice, T = 100 iterations is sufficient for many datasets (Fig. 1).
For the max and softmax models, we run K phases, where each phase consists of updating the support, then running T iterations of projected sub-gradient descent. For the first phase, we start with W = 0 and compute the support as the average of the instances in each bag (to compute the sup-port for the max and softmax models, we require some prior non-zero W ). Going from one phase to the next, we use the final weights from the earlier run of sub-gradient descent as the initial weights for the next run. Note that convergence analysis only applies to the sub-gradient descent part of each phase (it does not describe changes in the support over mul-tiple phases). We summarize the proposed algorithm below.
Computing the support takes O ( m ) time where m is the total number of instances, and this must be done K times.
We conduct experiments on artificial and real-world MIML datasets. The goals of our experiments are to (1) compare the proposed rank-loss objective with alternative Hamming loss and ambiguous loss; and (2) compare the max and soft-max aggregation models. Figure 1: Convergence of sub-gradient descent for rank loss, average model (transductive).
 Table 1: MIML datasets used in our experiments.
 Dataset Classes Dimension Bags Instances HJA Birdsong 13 38 548 10,232 MSRC v2 23 48 591 1,758 Letter-Carroll 26 16 166 717
Letter-Frost 26 16 144 565
We describe the setup of our experiments below.
We consider instance annotation in two different settings: transductive and inductive. In the transductive setting, the goal is to predict the instance labels for bags with known label sets. In this setting, the instance-level classifiers can only predict labels that appear in the bag label set. For-mally, the instance classifier in the transductive setting is goal is to predict instance labels in previously unseen bags (with unknown label sets). There is no restriction on which label an instance may be given in this case.

For both modes, we compute classifier accuracy as the fraction of instances correctly classified. For the inductive mode, we run 10-fold cross-validation and report average accuracy  X  standard deviation in accuracy between folds.
Table 1 summarizes the properties of each dataset used in our experiments. All of these datasets are available online. HJA Bird Song. Our collaborators have collected audio recordings of bird song at the H. J. Andrews (HJA) Exper-imental Forest, using unattended microphones. Our goal is to automatically identify the species of bird responsible for each utterance in these recordings, thereby generating an au-tomatic acoustic survey of bird populations. This problem is a natural fit for the MIML instance annotation framework. We treat a 10-second audio recording as a bag with labels http://web.engr.oregonstate.edu/~briggsf/ kdd2012datasets Figure 2: An example spectrogram from the HJA Birdsong dataset. This spectrogram corresponds to one bag. Each outlined region is an instance. corresponding to the set of species present in the recording. The instances are segments in a spectrogram. A spectro-gram is a graph of the spectrum of a signal as a function of time (computed by applying the Fast Fourier Transform to successive overlapping frames of the audio signal). Fig-ure 2 shows an example spectrogram for a 10-second audio recording containing several species of birds.

Starting with a 10-second audio recording, we first convert it to a spectrogram. A series of preprocessing steps are then applied to the spectrogram to reduce noise, and to identify bird song segments in the audio [15]. Each segment is consid-ered an instance and described by a 38-dimensional feature vector characterizing the shape of the segment, its time and frequency profile statistics, and a histogram of gradients.
This dataset contains 548 10-second recordings (bags), and a total of 10,232 segments (instances), of which 4,998 are labeled, and the rest are unlabeled. The available instance labels were provided by a human expert. Some instances were left unlabeled because they correspond to segmenta-tion errors (i.e. noise rather than bird song), because they are too difficult to identify in the presence of other sounds, or because it is extremely time-consuming to produce these labels. The bag-level label sets are formed by taking the union of the instance labels (not including any unlabeled instances).

The presence of the unlabeled instances which are not ac-counted for by the bag label set presents an additional chal-lenge for instance annotation. To evaluate how well various algorithms handle this problem, we consider two variants of this dataset:  X  X iltered X  and  X  X nfiltered X . For the filtered vari-ant, all of the unlabeled instances are removed, and in the unfiltered variant they are left in during the training pro-cess. In both variants, the accuracy is measured only on the labeled instances.
 Image Dataset: MSRC v2. A subset of the Microsoft Research Cambridge (MSRC) image dataset 2 [23] named  X  X 2 X  contains 591 images and 23 classes. The MSRC v2 dataset is useful for the instance annotation problem, be-cause pixel-level labels are included (Fig. 3). Several authors used MSRC v2 in MIML experiments [27, 24, 22].
 We construct a MIML dataset from MSRC v2 as follows: We treat each image as a bag. The bag label set is the list of all classes present in the ground-truth segmentation (i.e. the http://research.microsoft.com/en-us/projects/ objectclassrecognition/default.htm Figure 3: An image from MSRC v2 and the cor-responding pixel-level labeling. The classes in this image are  X  X ky X ,  X  X rees,  X  X rass X ,  X  X ody X , and  X  X ar X . The black regions are  X  X oid X ; we discard void regions. union of the instance labels). The instances correspond to each contiguous region in the ground-truth segmentation (to simplify the experiment, we use the ground-truth segmenta-tion rather than automatic segmentation). Each instance is described by a 16-dimensional histogram of gradients, and a 32-dimensional histogram of colors.
 Synthetic MIML Datasets. Limited availability of MIML datasets with instance labels has been a barrier to studying instance annotation (because instance labels are needed to evaluate accuracy). Using the Letter Recognition dataset [11] from the UCI Machine Learning repository, we con-struct two synthetic MIML datasets. The Letter Recogni-tion dataset consists of 20,000 instances with 16-dimensional features, and 26 classes. Note that randomly forming the bags will not be realistic because real-world MIML problems often have correlations between labels. Instead, we gener-ate datasets derived from the words in two poems,  X  X abber-wocky X  [3], and  X  X he Road Not Taken X  [12]. We call these datasets Letter-Carroll and Letter-Frost. For each word in these poems, we create a bag, with instances corresponding to the letters in the word. For each instance, we sample (without replacement), an example from the Letter Recog-nition dataset with the corresponding letter. The bag-level labels are the union of the instance labels. For example, the word  X  X iverged X  is transformed into a bag with 8 instances, and the label set { d, i, v, e, r, g } .
 Preprocessing. For all datasets, we apply the following preprocessing to the instance features. First, we transform each feature to the range [0 , 1]. Next, we apply the same feature rescaling process used in the Convex Learning from Partial Labels Toolbox (for ALC [6]), which centers the data and scales each feature by 1  X 
We compare our proposed Rank-Loss SIM methods with a Hamming-Loss SIM, and the Ambiguous Label Classifica-tion algorithm proposed by Cour et al. [6]. As a reference, we also consider a SISL classifier, which will have the unfair advantage of learning directly from instance labels. Hamming-Loss SIM. To compare Hamming loss to rank loss, we use the following Hamming-loss objective, with both max and softmax aggregation models: g ham ( W ) =  X  or in terms of support instances,  X  g ham ( W ) =  X  We use a similar projected sub-gradient descent algorithm to optimize this objective (and update the support in the same way as for the rank-loss objective over multiple phases). The conditions for Lemma 1 are also met by this objective. We compute the following sub-gradient In this case, the bound on the magnitude of the sub-gradient is L = c 2 Ambiguous Label Classification (ALC). Cour et al. [5, 6] proposed an SVM formulation for the ALC problem. We compare our proposed method to Cour X  X  ALC algorithm be-cause they both learn one linear model per class f w j  X  x and predict the instance label as arg max j f j ( x ), and both use an L 2 regularized loss function. The primary dif-ference in Cour X  X  ALC method is that the loss function is designed for use with ALC data (instead of the bag-level loss functions we use for MIML data). The ALC loss function is a convex upper bound to the 0/1 loss with respect to the true (unknown) instance labels, L ( f, x ,Y ) = max { 0 , 1  X  1 | Y | X
Minimizing regularized ambiguous loss can be converted into an equivalent SISL SVM problem with squared hinge-loss, and solved using an off-the-shelf linear SVM [10]. Comparison to SISL. We also run a SISL SVM for the inductive setting, whose performance can be interpreted as an empirical upper bound for inductive instance annotation because it is trained using unambiguously labeled instances. For this experiment, we use LIBSVM [4] with a linear kernel. Note that LIBSVM uses one linear model for each pair of classes rather than one for each class. For the HJA Birdsong dataset, we only run the SISL SVM on the filtered variant, because we cannot use the unlabeled instances.
The Hamming and rank-loss objectives have a regulariza-tion parameter  X  . Similarly, Cour X  X  ALC method has a reg-ularization parameter C , which achieves roughly the same effect when C = 1  X  . To obtain a fair comparison of different methods regardless of the parameter settings, we repeat all experiments for each value of  X   X  { 10  X  1 , 10  X  2 ,..., 10 and corresponding C values, and report the maximum ac-curacy achieved by each method.

In practice one could use cross-validation (on the bag-level labels) to select the regularization parameter. However, our results suggest that it maybe sufficient to use a default parameter for many datasets. For example, setting  X  to 10  X  7 or 10  X  8 tends to work well for all datasets that we considered (Fig. 4). This approach is consistent with prior work using Cour X  X  ALC method where a fixed value of C is used in all experiments. 3
For the SISL SVM, the regularization parameter C is op-timized by nested 10-fold cross-validation (within each fold Cour et al. used C = 10 3 for all experiments in prior work. Our results support this choice. (c) HJA Birdsong (filtered) of 10-fold cross validation, we run 10-fold cross validation in the training set to select the parameter). We search over the range C  X  X  10 1 , 10 2 ,..., 10 7 } .
 For the SIM algorithms, we use K = 10 phases, with T = 100 iterations of sub-gradient descent in each iteration. Figure 1 shows the rank-loss objective vs. number of iter-ations of sub-gradient descent with a fixed set of support instances on each dataset. These results show that most of the improvement in the objective occurs within 100 itera-tions. Note the convergence analysis does not guarantee the objective will decrease monotonically (only that an upper bound decreases monotonically).
Table 2 lists results in the transductive setting, and Table 3 lists results in the inductive setting.
 Comparing Different Loss Functions. First, we fo-cus on the transductive setting. Rank-loss methods gen-erally outperform Hamming loss and ambiguous-loss meth-ods. In particular, when used with the same aggregation model, rank loss consistently outperforms Hamming loss on all datasets. The performance differences of these two meth-ods for the softmax aggregation model range from 4% for HJA filtered to 15% for Letter-Carroll, and are even more pronounced for the max model. These results support our claim that rank loss is more appropriate than Hamming loss for instance annotation. Rank loss with either aggregation model also consistently outperforms ambiguous loss (ALC) in the transductive setting.

In the inductive setting, the performance generally is lower than the transductive setting, which is expected because in the transductive setting predictions are made with ex-tra bag-level label information. However, rank-loss meth-ods still achieve the best overall performance (excluding SISL SVM, which learns from instance labels). The per-formance of the Hamming-loss based methods is degraded most severely (possibly because Hamming loss does not cal-ibrate scores between classes, so its predictions become less reliable without the restriction on which classes may be se-lected imposed by the bag label sets).
 Comparing Different Aggregation Models. The soft-max model generally outperforms max for both rank loss and Hamming-loss objectives. In fact, rank loss with soft-max achieves the best performance for all five datasets in both settings (except for the inductive HJA Birdsong filtered dataset, where the difference in accuracy between rank loss with softmax and ALC is within the margin of uncertainty). The difference between the two aggregation models is more pronounced for the Hamming-loss objective.
 The Effect of Unlabeled Instances. Comparing the fil-tered and unfiltered variants of the HJA dataset, all methods suffer some accuracy degradation when we include the un-labeled instances. This is not surprising as these unlabeled instances introduce noise both at the instance level (because they may not correspond to any of the defined classes) and at the bag level (because the label set may be incomplete). The rank loss / softmax method suffers less than other al-gorithms in the presence of the unlabeled instances. For ex-ample, in the transductive setting the difference in accuracy is 0.4% for rank loss / softmax and 6.4% for Cour X  X  ALC. Note that ALC assumes that every instance is associated with one of the labels of the bag. This assumption is vio-lated by the unlabeled instances that are not accounted for by the bag label set, making ALC more sensitive to the pres-ence of such instances. The softmax model suffers less than the max model, possibly due to the fact that max can mis-takenly select the unlabeled instances as support instances.
To summarize, we observe that the rank loss / softmax method achieves the best overall performance. This result is verified in both transductive and inductive settings. The rank loss / softmax method is also the most robust in the presence of instance and label noise introduced by the unla-beled instances in the HJA Birdsong dataset.
MIML algorithms are developed under multiple frame-works, some of which naturally lend themselves to instance annotation. One such framework is graphical models, which have been previously used to perform bag and instance-level classification. Such models often treat instance labels as latent variables. Inference over such models allows the clas-sification of instances. While a variety of algorithms ex-ists, we highlight some representative examples of recent work. Dirichlet-Bernoulli Alignment [26] and the Exponen-tial Multinomial Mixture model [25] are topic models for MIML datasets and use variational inference to perform in-stance labeling. Zha et al. [27] proposed the MLMIL al-gorithm, a conditional random field model for MIML image annotation that uses Gibbs sampling to infer instance labels. Du et al. [8] propose another application of graphical models to simultaneous image annotation and segmentation.
While graphical models offer intuitive probabilistic inter-pretation, the computational complexity of inference in such models is one of the standing challenges. In this paper, we focus on another class of MIML approaches based on regular-ized loss minimization. Hamming-Loss SIM can be viewed as alternative approach for optimizing a similar objective to the M 3 MIML algorithm [28] (which learns an instance-level model, but was not designed for instance annotation). Also note that Cour X  X  ALC method follows the same framework of regularized loss minimization (but using a loss function designed for ALC).
 There are a small number of other works that address MIML instance annotation [21, 22]. Vijayanarasimhan and Grauman [22] developed a MIML SVM that learns a bag-level model with a set kernel (it does not learn a model of the instance feature space). Their algorithm makes predictions at either the bag or instance level by treating an instance as a bag of one instance. Vezhnevets and Buhmann [21] pro-posed an algorithm for instance annotation in MIML data where images are represented as a bag of pixels. Their al-gorithm alternates between sampling instance labels from an estimated distribution, and training an ensemble of de-cision trees on the sampled labels. Similarly, Nguyen [16] proposed a MIML SVM algorithm which alternates between assigning instance labels and maximizing margin given the assigned labels (although they did not conduct experiments on instance annotation).
We introduce Rank-Loss Support Instance Machines for instance annotation of MIML datasets. The key to our approach is a loss function that is measured at the bag-level and encourages the correct ranking of classes such that classes present in the label set of a bag score higher than classes that are not present. This general objective can be instantiated with different aggregation models includ-ing the previously used max model or a newly proposed softmax model. Both models can be viewed as represent-ing each bag with a support instance for each class. We alternate between computing this support, and optimizing a convex rank-loss objective using an efficient primal sub-gradient descent method. Experiments demonstrate that Rank-Loss SIM achieves higher accuracy than similar Ham-ming or ambiguous-loss based methods, and the softmax aggregation model achieves higher accuracy than the max model.

Our empirical evaluation focuses on methods based on regularized loss minimization. Comparisons to other types of methods including graphical models will be a topic of fu-ture work. We only consider linear models in this work, but our proposed methods could be extended to use kernels for non-linear classification. One possibility is to use the stan-dard kernel trick with a dual optimization method. Shalev-Shwartz et al. [19] proposed a way of kernelizing Pegasos that could be applied to our algorithms. A third possibility proposed by Rahimi and Recht [17] is to map the original features into a feature space such that inner products in the new space are approximately equal to a kernel (e.g., a radial basis function kernel). Further work is needed to determine which of these methods works well for instance annotation, and in what circumstances.
This work is partially funded by the Ecosystems Informat-ics IGERT program via NSF grant DGE 0333257, NSF grant 1055113 to X. Z. F., and the College of Engineering, Ore-gon State University. We would also like to thank Matthew Betts, Sarah Frey, Adam Hadley, and Jay Sexsmith for their help in collecting HJA data, Iris Koski for labeling the data, Katie Wolf for her work on noise reduction, and Lawrence Neal for his work on segmentation. [1] S. Andrews, I. Tsochantaridis, and T. Hofmann. [2] S. Boyd and L. Vandenberghe. Convex optimization . [3] L. Carroll. Through the looking-glass : and what Alice [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [5] T. Cour, B. Sapp, C. Jordan, and B. Taskar. Learning [6] T. Cour, B. Sapp, and B. Taskar. Learning from [7] T. Dietterich, R. Lathrop, and T. Lozano-P  X erez. [8] L. Du, L. Ren, D. Dunson, and L. Carin. A bayesian [9] A. Elisseeff and J. Weston. A kernel method for [10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [11] P. W. Frey and D. J. Slate. Letter recognition using [12] R. Frost. Mountain Interval . 1916. [13] E. H  X  ullermeier and J. Beringer. Learning from [14] Y. Li, S. Ji, S. Kumar, J. Ye, and Z. Zhou. Drosophila [15] L. Neal, F. Briggs, R. Raich, and X. Fern.
 [16] N. Nguyen. A new svm approach to multi-instance [17] A. Rahimi and B. Recht. Random features for [18] S. Shalev-Shwartz and Y. Singer. Logarithmic regret [19] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [20] C. Shen, J. Jiao, B. Wang, and Y. Yang.
 [21] A. Vezhnevets, J. Buhmann, and E. Zurich. Towards [22] S. Vijayanarasimhan and K. Grauman. What X  X  it [23] J. Winn, A. Criminisi, and T. Minka. Object [24] O. Yakhnenko. Learning from Text and Images: [25] S. Yang, J. Bian, and H. Zha. Hybrid [26] S. Yang, H. Zha, and B. Hu. Dirichlet-Bernoulli [27] Z. Zha, X. Hua, T. Mei, J. Wang, G. Qi, and Z. Wang. [28] M. Zhang and Z. Zhou. M3MIML: A maximum [29] Z. Zhou. Multi-instance learning: A survey. Technical [30] Z. Zhou and M. Zhang. Multi-instance multi-label [31] Z. Zhou, M. Zhang, S. Huang, and Y. Li.

