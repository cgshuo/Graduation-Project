 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguistic processing .; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval X  Retrieval models .; I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis .
 Algorithms, Human Factors, Theory.
 Athena Lecturer Award, Information Layer.
 This talk is in response to two Awards: the Association for Com-puting Machinery X  X  Athena Award, given by the ACM X  X  Committee on Women, on the nomination of the ACM Special Interest Group on Information Retrieval; and the British Computer Society X  X  Lovelace Medal. It is a very great honour to have been given these awards. I would like to say how much I appreciate this recognition. Thank you, ACM and BCS. I would particularly like to say, and I hope the ACM will not take this amiss, how I appreciate being the first woman to be awarded the BCS Lovelace Medal. The awards carry the opportunity to give a lecture with them. I deeply regret not be-ing able to do this live and in a way to suit each specifically. But I hope the single video based on this talk will go a little way as a substitute for two proper lectures.
 My talk has three parts: on the first phase of natural language pro-cessing research and its lessons; on subsequent developments up to the present and their lessons; and on where we are now and what I think are the wider implications for the future.
 When I began research in computing nearly fifty years ago, peo-ple were very excited about what could be done with computers, challenged by how to do it, and pushing applications that offered new opportunities in dealing with information. One of these areas was NLP  X  natural language processing  X  or NLIP  X  processing information conveyed in natural language. At that time, most re-searchers thought primarily of the efficiency gains that could be made by emulating or supporting people, e.g. in translation or doc-ument retrieval, though earlier visionaries like Warren Weaver had seen how computers could open up quite novel opportunities, and a little later Doug Englebart would illustrate some of these.
My own research focussed on automatic classification (nowa-days called unsupervised machine learning). It was clear that hu-mans rely in NLP on the use of general conceptual classifications of words  X  like a thesaurus  X  to resolve ambiguity in language (in research practice, text). Individual words are ambiguous, and so are the structures represented by word strings. These ambiguities are resolved by the fact that discourse is coherent, and effective be-cause concepts and topics are repeated, and repeated enough to get them across. Repeated individual concepts and standard concep-tual patterns enable us to select the right meanings for words and structural relationships for word sequences. This very general idea fairly obviously applies to translation, but it also applies to docu-ment retrieval: different words in a query and a document may still stand for the same concept and thus be allowed to match.

The question is, where to get the lexical classification and stock of text patterns from. The obvious answer is, from text. If words tend to co-occur in many texts this similar behaviour suggests they are conceptually related. Thus in principle one should be able to build a thesaurus automatically from a vast text corpus, and analo-gously to extract repeating conceptual patterns.

I was interested in building a thesaurus for translation, but of course had no corpus. I managed to finesse this by exploiting some limited dictionary (not thesaurus) data, for a pilot demonstration. For retrieval the situation is easier. The collection of documents or texts from which you want to retrieve supplies the corpus to build the classification. This is the line we followed, with some success in retrieval performance, particularly when relative word frequency was adequately factored in and eventually captured by word weighting. Documents score not just by the number of words matching, but by the sum of their weights.

All of this was essentially statistical in character. Facts about word occurrences and co-occurrences were used to capture mean-ing in a way that could be manipulated without needing to know what that meaning was. An unusually frequent word in a document is a good indicator of an important topic in the document, so if the query uses the word, the document is likely to be relevant to your information need.

One very simple word weighting formula captures this effec-tively. If a word occurs in many documents most of these  X  other things being equal  X  are unlikely to be relevant to the user X  X  infor-mation need. Weighting by inverse document frequency, idf , rela-tively favours less common and more discriminating terms. If you also take account of within-document frequency  X  so-called term frequency, tf  X  and also modulate by document length as appro-priate especially for full text, you get a robust, easily imp lemented mechanism  X  tf  X  idf -type weighting  X  that has consistently per-formed well.

If, in addition, you can find out that some documents are rele-vant to the user X  X  need, as you might in an interactive or rele vance feedback environment, you can distinguish between query te rm oc-currences in relevant and non-relevant documents and do a mu ch more discriminating, and hence effective, retrieval job, s till very simply.

All of this was established by the research community, throu gh exhaustive experiments supported by theory development, b y the mid seventies. But quite apart from being ignored by convent ional library and abstract services, it wasn X  X  thought of as havin g any-thing to do with computing proper, which was all about fundam en-tal, generic things like programming languages and compile rs, and operating systems, and which developed further, again fund amen-tally and generically, with motivating and validating theo ry  X  say with Scott-Strachey semantics  X  on the one hand, and with dis -tributed systems and security on the other.

But the retrieval research contained an important lesson. T his was not a novel lesson in itself, but one that is important tho ugh too often forgotten, when you try to get computers to do thing s that humans do.

Thus particularly in the early days of automatic classificat ion work, nobody really examined the assumptions on which it was based, i.e. what a classification should be like and what it wa s for. It is easy to believe there are natural classes, and because t here are natural classifications, these will be appropriate for what ever you want to use them for. The only challenge in automation is how t o find them.

But this isn X  X  true even in biology any more than it is true in retrieval, as retrieval researchers found. There are lots o f equally plausible ways, to group things (even given the same feature de-scriptions), and the issue is to analyse the purpose for whic h a clas-sification is intended and to develop one to suit that purpose . In re-trieval you want to achieve precision (only relevant) and re call (all relevant), which are in fact conflicting and it turned out, co ntrary to expectation, that precision-promoting tight classes, not hospitable recall-promoting ones, were what was really required. The e ffec-tiveness of weighting was geared to the same truth: idf is not an obvious notion, but words are sloppy, and if you don X  X  constr ain word matching, things get too sloppy. Learning all of this  X  w hat you want to achieve by automation not what you think you want to achieve by automating humans X  beliefs and actions  X  took m uch trial and error, and demonstrated the need for fresh thinkin g about goals, thorough experiment, and a sound evaluation methodo logy.
So the lesson from the first phase of research was that retriev al isn X  X  what you thought, but when you have figured it out, stati stics are a good thing.
 Indexing and retrieval, thus statistically characterised , are towards one end of the spectrum of NLIP tasks. Translation, or questi on-answering, or summarising, appear to require symbolic proc essing, i.e. syntactic parsing and semantic analysis in discourse i nterpreta-tion, and manipulation of an explicit meaning representati on (and complementarily for generation). By symbolic I mean displa ying the meaning of  X  X he girl ate chocolates X  as a young female as a gent taking into herself some sweet consumables as objects, with ev-ery component of the representation actually a formal, well -defined symbol for something in some world.

Thus correctly answering such a question as  X  X id John marry t he girl he loved? X  will not necessarily be achieved by just matc hing on the words  X  X ohn X ,  X  X arry X ,  X  X irl X , and  X  X oved X . Similarly i t is hard to see how summarising the mini text  X  X ary went into the garde n and cut some roses and lilies. She arranged them all in a tall, dark brown jug X , can be done by some statistical word-based opera tion: that wouldn X  X  deal with referring expressions like  X  X hem X  o r the greater importance of  X  X ug X  than  X  X rown X .
 While research on retrieval was slowly building up, people i n the NLP community, from the 1970s onwards through the eighties a nd into the nineties, were developing the tools for symbolic pr ocessing  X  parsers, lexicons, representation formalisms and infere nce mech-anisms  X  that encouraged them to tackle challenging NLIP tas ks that seemed to require symbolic processing in order to build and use deep sentence and text meaning representations, and to r elate these to world models. For example, given  X  X ill fell off her h orse X , to answer the question  X  X id it run away? X 
This turned out to be hard, harder than expected, even for el-ementary tasks like simple natural language queries to data bases. There was also a growing problem about evaluation. In princi ple one can build a system, deploy it for real, and see whether it w orks satisfactorily. But as this is very expensive you would like some means of earlier evaluation. Unfortunately it is extremely difficult to say exactly what a system ought to deliver, in the fine grain , especially when you are trying to do something new and not jus t replicate an old, manually obtained output (and looking at s ystem internal products like meaning representations is of very l imited value).

Thus with information or fact extraction from text, what fac ts should one be trying to obtain from  X  X eter patted the dog and s troked the big black cat X ? Just or or or There is no limit to what you can inferentially extract if you put your mind to it, but do you want it all, how do you express it, an d how do you assess it?
Retrieval research avoided this problem by taking users X  re le-vance assessments for their queries as a gold standard. But r ele-vance is a broad notion, and the retrieved documents are only ve-hicles for the user X  X  discovery process. With fact extracti on for a large information base, or summarisation, the situation i s more complex. Facts are specific nuggets, and summaries are new, b ut information-losing, discourses in their own right, so it is impossi-ble to say in advance whether a user X  X  interest  X  e.g.  X  X ell me about Peter X   X  will only, or best, be satisfied by one system output.
The underlying problem is that tasks like information extra ction, or summarising, or translation, are not unitary tasks. Retr ieval isn X  X  either, but it has a modest aim: offer the user something they may be able to make something of in pretty well any context. Summari sing for example, on the other hand, is a very complex task with mor e potential variation, and therefore more need to be designed for dif-ferent types of situation (Google snippet summaries are use ful for some purposes, useless for others).

What NLIP research, especially since the early 1990s, has sh own in attempting to build and evaluate systems for such challen ging tasks as summarising, is that these tasks come in many shapes and there are consequently many different appropriate strateg ies for do-ing them to suit circumstances. More importantly, it became evi-dent that these tasks in their simpler forms could be done wit h sta-tistical techniques, or hybrid statistical-symbolic appr oaches of a relatively shallow kind. You can do translation, or fact ext raction, or summarising statistically quite well enough to meet some needs, and even apply these techniques cross-linguistically. For example you can learn word selection and ordering from example sourc es and their summaries for telegraphic one-line summaries  X  an d we have plenty of training data for such things. Statistical da ta is also a vital support for symbolic processing when this is needed, for example about normal parsing preferences.

So the lesson learnt from the second phase of NLIP research wa s that you can do a lot more with statistics in NLIP than you thou ght. Statistics can be useful in more ways than might have seemed p os-sible in the early days of NLIP, when statistics even for a hum drum task like retrieval was revolutionary. Now tf , idf and so forth are embedded in Web search engines (though they took decades to g et there), machine learning is the default base for speech tran scrip-tion, and largely statistical extractive techniques can pr ovide useful automatic summaries, even if to meet high quality needs all t he full symbolic apparatus of meaning representations, world mode ls, and inference is essential.
 What, then, do the key features of the present state of NLIP su ggest for the future, and what in particular do they suggest for com puting in general, not just NLIP itself. I assume now that NLIP will g o from strength to strength along its present lines. I want to f ocus on its implications for computing in general.

The key feature of the present state is the way that NLIP is sea m-less. It is seamless both because different tasks, or compon ent func-tions like answering a single question, merge into one anoth er, and because basic processes, especially statistical ones like estimating relative word significance, apply across the field. This seam less-ness is not surprising given that we are dealing with natural lan-guage, and that natural language, despite its huge range of s pecific capabilities and varied uses, is the common vehicle for our c ommu-nication, and in this communication we always have to deal wi th discourse X  X  constituent ambiguity. The seamlessness in th e utility of statistics, like applying Bayes X  X  Theorem, seems more su rpris-ing, but is not so given we have to cope with ambiguity quickly in real time, and so need robust and simple methods for doing thi s. Relying on frequency  X  of course in reality by approximation , not actual calculation  X  is such a robust and simple method.

Thus even if you can X  X  rely only on such methods in NLIP, the important point about the experience of the last fifty years, first in retrieval and then other NLIP tasks, is that you get a lot of mi leage in, as it were, finding your feet, by exploiting rough and simp le pro-cesses either on their own or to leverage more sophisticated ones, for example to suggest likely preferred paths to follow in fu ll text interpretation.

I believe the consequences of this for computing in general, not just NLIP, are profound. They have to do with the notion of the information layer .

This is not a well-defined notion, but that does not make it any less real or important. The notion has also been around for so me time. What I want to suggest is that with the experience of NLI P we can give it more substance. What follows is only sketchy an d indicative, about the future, but I will stick my neck out and say it.
Early on, and putting it very simplistically, the model of co m-puting was that it consisted of a hardware layer at the bottom , then an operating system layer, then maybe a utilities layer, wit h a final applications layer on top. By application here I mean someth ing like an accounting package.

This simple model was further developed to accommodate dis-tributed systems and communications, introducing a commun ica-tions layer between hardware and operating system. It then a p-peared, as systems grew more powerful, that it could be appro priate to think of an information layer  X  not just a bits layer or a dat a layer in some conventional sense (i.e. of well-defined strings or t uples)  X  as part of the computing system proper and not just, as in the old library days, as an application. The natural place for the in forma-tion layer was on top of the operating system, or at least util ities layer. It could indeed perhaps absorb the utilities layer in a closer relationship with the operating system. The crucial point a bout the information layer was that it would have widely useful subst antive content, something rather different from the familiar but f ormal no-tion of data as something systems ship about.
 Now you may say that this is what we have been getting with Web browsers and, especially, all-pervading Web engines. B ut this is only half right if one thinks only of browser or search func tion-ality. What really matters is what browsers and engines work over  X  the stuff, incipient information. My claim is that what is c entral to the idea of an information layer is the stuff itself.

This seems to fly in the face of computing as essentially synta ctic manipulation of opaque coded objects. So my argument is as fo l-lows. People don X  X  talk in numbers. They talk in language. Wh er-ever end-users appear, which is everywhere, they need a fami liar communication vehicle for information, and natural langua ge is what they use. Unfortunately they still have to interpret la nguage to decide what the information is and how to use it. They know how to do that, but can be materially assisted by automatic proce sses like, in the simple case, text retrieval, but also by other fa cilities like translation or summarising.

One might argue that this isn X  X  any different from conventio nal computing. But I claim that the code and what X  X  coded are far nearer and more accessible to humans. Words are there for the m-selves, as their familiar selves, and are not replaced by oth er codes.
We have at the same time to recognise, and allow for, the fact that every language use in a context is different, so there wi ll be an early limit on how resolved and explicit the language proc essing and what it manipulates in the information layer are.

The idea of a Semantic Web as a universal characterisation of knowledge strikes me as misconceived. There can be no ontolo gy that will work for everything and everyone. There may be many specific ontological horses for particular courses. But the univer-sal means of getting from one to another cannot but be a sort of lightweight rope-way. That is, something that, modest and p artial though it may be, does establish links and make it possible to move around. This is what natural language tools like statistica l asso-ciations provide. People can get a start, and move, from simp le words or phrases and connective relations between them. Nat ural language is general but leads to particulars.

But it X  X  not quite clear why such an information layer, incor po-rating the vast language stuff that exists electronically, but with an eye on the user, should be a middle layer rather than an outer l ayer in computing systems. My point is that there is no reason to su p-pose that what are actually semantic operations  X  if only sha llow ones  X  in the layer are only for the immediate benefit of the use r. There is no reason to suppose that as computing systems become more powerful and pervasive, they will not find it useful, even nec-essary, to do the same sort of things themselves.

For example, while system security ought to be as tight as a drum with formal coding, in reality it wont be watertight and it may be desirable for the system to see how it can be helped by whatever or can dig out of the information layer. For example it may be possible  X  though I can only be sketchy and indicative here too as I am not a security expert  X  to learn more and hence plug gaps in some supposedly secure situation by trawling the information layer for correlates of the words that figure somewhere in the security setup, whether these words have to do with the agents involved, the wrapping of the package, or the contents of the package.

I recognise this is very vague. This is not surprising given that I am talking about a future possibility. But I think that people are getting overexcited about images and forgetting the crucial role that language has. You can look at pictures, but you have to use language to talk about them. So I believe we have a wonderfully rich resource in all the words, the stuff of information, that are be-coming available, and we should think imaginatively about what it could be like for computing systems not only to make such stuff available to users, but to exploit it for themselves. It X  X  an exciting opportunity.

Finally, I would like not only to thank you again for the awards and the opportunity to give this lecture, but also my husband, the late Roger Needham, who always encouraged my work.

