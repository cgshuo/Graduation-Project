 A sketch of a matrix A is another matrix B which is signifi-cantly smaller than A , but still approximates it well. Find-ing such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challeng-ing in the streaming model, where each row of the input matrix can be processed only once and storage is severely limited.

In this paper, we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives n rows of a large matrix A  X  R n  X  m one after the other, in a streaming fashion. It maintains a sketch B  X  R  X   X  m containing only  X   X  n rows but still guarantees that A T A  X  B T B . More accurately, Or This algorithm X  X  error decays proportionally to 1 / X  using O ( m X  ) space. In comparison, random-projection, hashing or sampling based algorithms produce convergence bounds proportional to 1 / amortized O ( m X  ) operations and the algorithm is perfectly parallelizable. Our experiments corroborate the algorithm X  X  scalability and improved convergence rate. The presented algorithm also stands out in that it is deterministic, simple to implement, and elementary to prove.
 G.1.2 [ Numerical Analysis ]: Approximation Streaming, matrix sketching, frequent items
Modern large data sets are often viewed as large matri-ces. For example, textual data in the bag-of-words model is represented by a matrix whose rows correspond to docu-ments. In large scale image analysis, each row in the matrix corresponds to one image and contains either pixel values or other derived feature values. Other large scale machine learning systems generate such matrices by converting each example into a list of numeric features. Low rank approxi-mations for such matrices are used in common data mining tasks such as Principal Component Analysis (PCA), Latent Semantic Indexing (LSI), and k -means clustering. Regard-less of the data source, the optimal low rank approximation for any matrix is obtained by its truncated Singular Value Decompositions (SVD).

Data matrices, as above, are often extremely large and distributed across many machines. This renders standard SVD algorithms infeasible. Given a very large matrix A , a common approach is to compute a sketch matrix B that is significantly smaller than the original. A good sketch matrix B is such that computations can be performed on B rather than on A without much loss in precision.

Matrix sketching methods are, therefore, designed to be pass-efficient, i.e., the data is read at most a constant num-ber of times. If only one pass is required, the computa-tional model is also referred to as the streaming model. The streaming model is especially attractive since a sketch can be obtained while the data is collected. In that case, storing the original matrix becomes superfluous, see [17] for more motivations and examples of streaming algorithms for data mining applications.

There are three main matrix sketching approaches, pre-sented here in an arbitrary order. The first generates a sparser version of the matrix. Sparser matrices are stored more efficiently and can be multiplied faster by other matri-ces [4][2][15]. The second approach is to randomly combine matrix rows [25][28][27][21]. The proofs for these rely on subspace embedding techniques and strong concentration of measure phenomena. The above methods will be collectively referred to as random-projection in the experimental section. A recent result along these lines [8], gives simple and efficient subspace embeddings that can be applied in time O ( nnz ( A )) for any matrix A . We will refer to this result as hashing in the experimental section. While our algorithm requires more computation than hashing , it will produce more ac-curate sketches given a fixed sketch size. The third sketch-ing approach is to find a small subset of matrix rows (or columns) that approximate the entire matrix. This problem is known as the Column Subset Selection Problem and has been thoroughly investigated, [16][12][6][11][14][5]. Re cent results offer algorithms with almost matching lower bounds, [11][5][7]. Alas, it is not immediately clear how to compare some of these methods X  results to ours since their objectives are different. They aim to recover a low rank matrix whose column space contains most of the space spanned by the matrix top k singular vectors. Moreover, most of the above algorithms are quite intricate and require several passes over the input matrix.

A simple streaming solution to the Column Subset Selec-tion problem is obtained by sampling rows from the input matrix. The rows are sampled with probability proportional to their squared  X  2 norm. Despite this algorithm X  X  appar-ent simplicity, providing tight bounds for its performance required over a decade of research [16][3][12][26][29][24][14]. We will refer to this algorithm as sampling . Algorithms such as CUR utilize the leverage scores of the rows [13] and not their squared  X  2 norms. The discussion on matrix leverage scores goes beyond the scope of this paper, see [22] for more information and references.

This manuscript proposes a fourth approach. It draws on the similarity between the matrix sketching problem and the item frequency estimation problem. In the following, we shortly describe the item frequency approximation problem, as well as a well known algorithm for it.
In the item frequency approximation problem there is a universe of m items a 1 , . . . , a m and a stream A 1 , . . . , A item appearances. The frequency f i of item a i stands for the number of times a i appears in the stream. It is trivial to pro-duce all item frequencies using O ( m ) space simply by keep-ing a counter for each item. Our goal is to use O (  X  ) space and produce approximate frequencies g j such that | f j  X  g j for all j simultaneously.

This problem received an incredibly simple and elegant solution in [23]. It was later independently rediscovered by [10] and [19], who also improved its update complexity. The algorithm simulates a process of  X  X eleting X  from the stream  X  appearances of different items. This is performed repeatedly for as long as possible, namely, until there are less than  X  unique items left. This trimmed stream is stored concisely in O (  X  ) space. The claim is that, if item a j appears in the final trimmed stream g j times, then g j is a good approximation for its true frequency f j (even if g j = 0). This is because f  X  g j  X  t , where t is the number of times items were deleted. Each item type is deleted at most once in each deletion batch. Moreover, we delete  X  items in every batch and at most n items can be deleted altogether. Thus, t X   X  n or t  X  n/ X  , which completes the proof. The reader is referred to [19] for an efficient streaming implementation. From this point on, we refer to this algorithm as Frequent-items .
The following is a description of the item frequency prob-lem as a matrix sketching problem. Let A be a matrix that is given to the algorithm as a stream of its rows. For now, let us constrain the rows of A to be indicator vec-tors. In other words, we have A i  X  { e 1 , . . . , e m } , where e is the j  X  X h standard basis vector. Note that such a matrix can encode a stream of items (as above). If the i  X  X h ele-ment in the stream is a j , then the i  X  X h row of the matrix is set to A i = e j . The frequency f j can be expressed as f = k Ae j k 2 . Moreover, a good sketch B would be one such that g j = k Be j k 2 is a good approximation to f j . Replacing n = k A k 2 f , we get that the condition | f j  X  g j | X  n/ X  is equiv-clear that for  X  X tem indicator X  matrices, a sketch B  X  R  X  can be obtained by the Frequent-items algorithm.
In this paper we describe Frequent-directions , an extension of Frequent-items to general matrices. Given any matrix A  X  R n  X  m , the algorithm processes the rows of A one by one and produces a sketch matrix B  X  R  X   X  m , such that The intuition behind Frequent-directions is surprisingly simi-lar to that of Frequent-items : In the same way that Frequent-items periodically deletes  X  different elements, Frequent-dire-ctions periodically  X  X hrinks X   X  orthogonal vectors by roughly the same amount. This means that during shrinking steps, the squared Frobenius norm of the sketch reduces  X  times faster than its squared projection on any single direction. Since the Frobenius norm of the final sketch is non nega-tive, we are guaranteed that no direction in space is reduced by  X  X oo much X . This intuition is made exact below. As a remark, when presented with an item indicator matrix, Frequent-directions exactly mimics a variant of Frequent-items .

As its name suggests, the Frequent-items algorithm is of-ten used to uncover frequent items in an item stream. Namely, if one sets  X  &gt; 1 / X  , then any item that appears more than  X n times in the stream must appear in the final sketch. Sim-ilarly, Frequent-directions can be used to uncover any unit vector (direction) in space x for which k Ax k 2  X   X  k A k taking  X  &gt; 2 r/ X  . 1
This property makes Frequent-directions very useful in practice. In data-mining, it is common to represent data matrices by low rank matrices. Typically, one computes the SVD of A and approximates it using the first k singular vec-tors and values. The value k is such that the k  X  X h singular value is larger than some threshold value t . In other words, we only  X  X are about X  unit vectors such that k Ax k  X  t . Us-ing Frequent-directions we can invert this process. We can prescribe t in advance and find the space of all vectors x such that k Ax k  X  t directly while circumventing the SVD computation altogether.
Here we point out another similarity between item fre-quency estimation and matrix sketching. It is simple to see that all item frequencies can be approximated from a uniform sample of the stream. Using Chernoff X  X  bound (and then applying the union bound carefully) one concludes that O ( r log( r ) / X  2 ) samples suffice to ensure that | f i  X f max . In this context we define r = n/f max . Similarly, ma-trix sketching by row sampling [24][14] requires O ( r log( r ) / X  row samples where r = k A k 2 f / k A k 2 2 to ensure that k A B
T B k  X   X  k A T A k . From the above discussion, it is evi-dent that the matrix sampling result implies the item sam-pling algorithm. This is because running the matrix sam-pling algorithm on an item indicator matrix (as before) pro-duces uniform random samples. Moreover, for such matri-
H ere r = k A k 2 f / k A k 2 2 denotes the numeric rank of A . For any matrix A  X  R n  X  m the numeric rank r = k A k 2 f / k A k a smooth relaxation of the algebraic rank Rank ( A ). ces, r = k A k 2 f / k A k 2 2 = f m ax /n and f max = k A argue that Frequent-directions improves on matrix sampling in the same way that Frequent-items improves on item sam-pling.
Low rank approximation of matrices is a well studied prob-lem. The goal is to obtain a small matrix B , containing  X  rows, that contains in its row space a projection matrix  X  of rank k such that k A  X  A  X  A B, X  k  X   X  (1+  X  ) k A  X  A A k is the best rank k approximation of A and  X  is either 2 (spectral norm) or f (Frobenius norm). It is difficult to compare our algorithm to this line of work since the types of bounds obtained are qualitatively different. We note, how-ever, that it is possible to use Frequent-directions to produce a low rank approximation result.
 Lemma 4 from a version of [12] (modified) . Let P B k denote the projection matrix on the right k singular vectors of B corresponding to its largest singular values. Then the following holds: k A  X  AP B k k 2  X   X  2 k +1 + 2 k A T A  X  B where  X  k +1 is the ( k + 1) X  X h singular value of A .
Let r = k A k 2 f / k A k 2 2 denote the numeric rank of A and let  X   X  4 r X  2 1 / X  X  2 k +1 . Using Frequent-directions and letting the sketch B maintain  X  columns, we get that 2 k A T A  X  B
T B k  X   X  X  2 k +1 and therefore k A  X  AP B k k  X   X  k +1 (1 +  X  ), which is a 1+  X  approximation to the optimal solution. Since r X  1 / X  2 k +1  X   X ( k ), this is asymptotically inferior to the space directions is also optimal due to [7].
The algorithm keeps an  X   X  m sketch matrix B that is updated every time a new row from the input matrix A is added. Rows from A simply replace all-zero valued rows of the sketch B . The algorithm maintains the invariant that all-zero valued rows always exist. Otherwise, half the rows in the sketch are nullified by a two-stage process. First, the sketch is rotated (from the left) using its SVD such that its rows are orthogonal and in descending magnitude order. Then, the sketch rows norms are  X  X hrunk X  so that at least half of them are set to zero. In the algorithm, we denote by [ U,  X  , V ]  X  SVD( B ) the Singular Value Decomposition of B . We use the convention that U  X  V T = B , U T U = V
T V = V V T = I  X  , where I  X  stands for the  X   X   X  identity matrix. Moreover,  X  is a non-negative diagonal matrix such  X  = diag([  X  1 , . . . ,  X   X  ]),  X  1  X  . . .  X   X   X   X  0. We also assume that  X / 2 is an integer.

Claim 1. If B is the result of applying Algorithm 1 to matrix A , then:
Proof. First, 0 B T B because B T B is positive semidef-inite for any matrix B . Second, B T B A T A is a conse-quence of the fact that  X  x k Ax k 2  X  X  Bx k 2  X  0. Let B C i denote the values of B and C after the main loop in the algorithm has been executed i times. For example, B 0 is an all zeros matrix and B n = B is the returned sketch. k Ax k 2  X  X  Bx k 2 = Algorithm 1 F requent-directions Input:  X  , A  X  R n  X  m
B  X  all zeros matrix  X  R  X   X  m for i  X  [ n ] do end for Return: B The statement that h A i , x i 2 + k B i  X  1 x k 2 = k C i true because A i is inserted in a zero valued row of B i  X  Also, note that C i is an isometric left rotation of a matrix containing the rows of B i  X  1 and the new row A i . Finally, k C i x k 2  X  X  B i x k 2  X  0 because C i T C i B i T B i by the defi-nition of the shrinking step.

Claim 2 . If B is the result of applying Algorithm 1 to matrix A with prescribed sketch size  X  , then:
Proof. Let  X  i denote the value of  X  at time step i . If the algorithm does not enter the  X  X f X  section in the i  X  X h step, then  X  i = 0. Similarly, let B i , C i , and V i be the values of B , C and V after the main loop in the algorithm is executed i times.

We start by bounding the value of k A T A  X  B T B k as a function of  X  i . In what follows, x is the eigenvector of A B
T B corresponding to its largest eigenvalue. k A T A  X  B T B k = k Ax k 2  X  X  Bx k 2 The second transition is correct because h A i , x i 2 + k B k C i x k 2 which is explained in the proof of Claim 1. The last step is obtained by replacing C i and B i by their definitions.
We now turn to bounding the value of P n i = 1  X  i by com-puting the Frobenius norm of the resulting sketch B : C i is, up to a unitary left rotation, a matrix that contains both B i  X  1 and A i . The last transition is correct because the matrix (( X  i ) 2  X  (  X   X  i ) 2 ) contains  X  non-negative elements on its diagonal at least half of which are equal to  X  i . We with our earlier observation that k A T A  X  B T B k X  P n i =1 we obtain that k A T A  X  B T B k  X  2( k A k 2 f  X  X  B k 2 f fact will be used in Section 2.2. By the simple fact that k B k 2 f  X  0, we obtain k A T A  X  B T B k  X  2 k A k 2 f / X  . This completes the proof of the claim.
L et T SVD (  X , m ) stand for the number of operations re-quired to obtain the Singular Value Decomposition of an  X  by m matrix. The worst case update time of Frequent-directions is therefore O ( T SVD (  X , m )), which is also O ( m X  That said, the SVD of the sketch is computed only once ev-ery  X / 2 iterations. This is because the shrinking step in the algorithm nullifies at least  X / 2 rows in B . When the SVD is not computed, the addition running time is O ( m ). The total running time is therefore bounded by O ( nm X  ). This gives an amortized update time of O ( m X  ) per row in A .
A convenient property of this sketching technique is that it allows for combining sketches. Let A = [ A 1 ; A 2 ] such that A consists of the rows of A 1 and A 2 stacked on top of one another. Also, let B 1 and B 2 be the sketches computed by the above technique for A 1 and A 2 respectively. Now let the final sketch, C , be the sketch of a matrix B = [ B 1 ; B contains the two sketches B 1 and B 2 vertically stacked. We show below that k A T A  X  C T C k  X  2 k A k 2 f / X  . This means that sketching each half of A separately and then sketching the resulting sketches is as good as sketching A directly. To see this, we compute k Cx k 2 for a test vector k x k = 1: Here we use the fact that k B 1 x k 2  X  k A 1 x k 2  X   X  ( k A k B 1 k 2 f ) for k x k = 1, which is shown in the proof of Claim 2. This property trivially generalizes to any number of parti-tions of A . It is especially useful when the matrix (or data) is distributed across many machines. In this setting, each machine can independently compute a local sketch. These sketches can then be combined in an arbitrary order using Frequent-directions . We compare Frequent-directions to five different algorithms. The first two constitute brute force and na  X   X ve baselines. The other three are common algorithms that are used in prac-tice: sampling , hashing , and random-projection . References can be found in the introduction. All tested methods re-ceive the rows of an n  X  m matrix A one by one. They are all limited in storage to an  X   X  m sketch matrix B and additional o (  X  X  ) space for any auxiliary variables. This is with the exception of the brute force algorithm that requires  X ( m 2 ) space. For a given input matrix A we compare the computational efficiency of the different methods and their resulting sketch accuracy. The computational efficiency is taken as the time required to produce B from the stream of A  X  X  rows. The accuracy of a sketch matrix B is measured by k A T A  X  B T B k . Since some of the algorithms below are randomized, each algorithm was executed 5 times for each input parameter setting. The reported results are median values of these independent executions.

The experiments were conducted on a FreeBSD machine with 48GB of RAM and a 12MB cache using a single Intel(R) Xeon(R) X5650 CPU. All experimental results, from which the plots below are obtained, are available for download as json formatted records at [20]. Brute Force: The brute force approach produces the opti-mal rank  X  approximation of A . It explicitly computes the matrix A T A = P n i A T i A i by aggregating the outer prod-ucts of the rows of A . The final  X  X ketch X  consists of the top  X  right singular vectors and values (square rooted) of A T which are obtained by computing its SVD. The update time of Brute Force is  X ( m 2 ) and its space requirement is  X ( m Na  X   X ve: Upon receiving a row in A the na  X   X ve method does nothing. The sketch it returns is an all zeros  X  by m matrix. This baseline is important for two reasons: First, it can ac-tually be more accurate than random methods due to under sampling scaling issues. Second, although it does not per-form any computation, it does incur computation overheads s uch as I/O exactly like the other methods.
 Sampling: Each row in the sketch matrix B samp is chosen i.i.d. from A and rescaled. More accurately, each row B samp takes the value A i / The space it requires is O ( m X  ) in the worst case but it can be much lower if the chosen rows are sparse. Since the value of k A k f is not a priori known, the streaming algorithm is implemented by  X  independent reservoir samplers, each sam-pling a single row according to the distribution. The update running time is therefore O ( m ) per row in A . For a the-oretical analysis of this algorithm the reader is referred to [12][26][24].
 Hashing: The matrix B hash is generated by adding or sub-tracting the rows of A from random rows of B hash . More accurately, B hash is initialized to be an  X   X  m all zeros matrix. Then, when processing A i we perform B hash h ( i ) B perfect hash functions. There is no harm in assuming such functions exist since complete randomness is na  X   X vely possi-ble without dominating either space or running time. This method is often used in practice by the machine learning community and is referred to as  X  X eature hashing X  or  X  X ash-ing trick X  [31]. For a surprising new analysis of this method see [8].
 Random-projection: The matrix B proj is equivalent to the matrix RA where R is an  X   X  n matrix such that R i,j  X  { X  1 / matrix [1], B proj contains the m columns of A randomly projected from dimension n to dimension  X  . This is easily computed in a streaming fashion, while requiring at most O ( m X  ) space and O ( m X  ) operation per row updated. For proofs of correctness and usage see [25][28][27][21]. Sparser constructions of random projection matrices are known to exist [9][18]. These, however, were not implemented since the running time of applying random projection matrices is not the focus of this experiment.
Each row of the generated input matrices, A , consists of a d dimensional signal and m dimensional noise ( d  X  m ). More accurately, A = SDU + N/ X  . The signal coefficients matrix S  X  R n  X  d is such that S i,j  X  X  (0 , 1) i.i.d. The diag-onal matrix D is D i,i = 1  X  ( i  X  1) /d , which gives linearly diminishing signal singular values. The signal row space matrix U  X  R d  X  m contains a random d dimensional sub-space in R m , for clarity, UU T = I d . The matrix SDU is exactly rank d and constitutes the signal we wish to recover. The matrix N  X  R n  X  m contributes additive Gaussian noise N i,j  X  X  (0 , 1). Due to [30], the spectral norms of SDU and N are expected to be the same up to some universal constant c . Experimentally, c 1  X  1. Therefore, when  X   X  c 1 we can-not expect to recover the signal because the noise spectrally dominates it. On the other hand, when  X   X  c 1 the spectral norm is dominated by the signal which is therefore recover-able. Note that the Frobenius norm of A is dominated by the noise for any  X   X  c 2 p m/d , for another constant close to 1, c . Therefore, in the typical case where c 1  X   X   X  c 2 p m/d , t he signal is recoverable by spectral methods even though the vast majority of the energy in each row is due to noise. Figure 1: Accuracy vs. sketch size. The y -axis in-dicates the accuracy of the sketches. If a method returns a sketch matrix B , the accuracy is measured by k A T A  X  B T B k . The size of the sketch is fixed for all methods and is B  X  R  X   X  m . The value of  X  is indicated on the x -axis. The form of the input matrix is explained in Section 3.2. Here the sig-nal dimensions are d = 10 , 20 , 50 ordered from top to bottom. The signal to noise ratio is kept constant at  X  = 10 . Each plot line corresponds to one of the sketching techniques explained in Section 3.1. The only plot line that does not correspond to an algo-rithm is denoted by Frequent-directions bound. This is the theoretical worst case performance guaranteed by Frequent-directions .
T he performance of Frequent-directions was measured both in terms of accuracy and running time compared to the above algorithms. In the first experiment, a moderately sized matrix (10 , 000  X  1 , 000) was approximated by each algorithm. The moderate input matrix size is needed to accommodate the brute force algorithm and to enable the exact error measure. The results are shown in Figure 1 and give rise to a few interesting observations. First, all three random techniques actually perform worse than na  X   X ve for small sketch sizes. This is a side effect of under-sampling which causes overcorrection. This is not the case with Frequent-directions . Second, the three random techniques perform equally well. This might be a result of the chosen input. Nevertheless, practitioners should consider these as poten-tially comparable alternatives. Third, the curve indicated by  X  X requent Direction Bound X  plots the accuracy guaran-teed by Frequent-directions . Note that  X  X requent Direction Bound X  X s consistently lower than the random methods. This means that the worst case performance guarantee is lower than the actual performance of the competing algorithms. Finally, Frequent-directions produces significantly more ac-curate sketches than predicted by its worst case analysis.
The running time of Frequent-directions , however, is not better than its competitors. This is clearly predicted by their asymptotic running times. In Figure 2, the running times (in seconds) of the sketching algorithms are plotted as a function of their sketch sizes. Clearly, the larger the sketch, the higher the running time. Note that hashing is extremely fast. In fact, it is almost as fast as na  X   X ve , which does nothing! Sampling is also faster than Frequent-directions but only by a factor of roughly 2. This is surprising because it should be faster by a factor of  X  . Frequent-directions , however, executes faster than random-projection although they share the same asymptotic running time ( O ( nd X  )). It is important to stress that the implementations above are not very well optimized. Different implementations might lead to different results.
Nevertheless, we will claim that Frequent-directions scales well. Its running time is O ( nm X  ), which is linear in each of the three terms. In Figure 3, we fix the sketch size to be  X  = 100 and increase n and m . Note that the running time is indeed linear in both n and m as predicted. More-over, sketching an input matrix of size 10 5  X  10 4 requires roughly 3 minutes. Assuming 4 byte floating point num-bers, this matrix occupies roughly 4 Gb of disk space. More importantly though, Frequent-directions is a streaming algo-rithm. Thus, its memory footprint is fixed and its running time is exactly linear in n . For example, sketching a 40 Gb matrix of size 10 6  X  10 4 terminates in half an hour. The fact that Frequent-directions is also perfectly parallelizable (Section 2.2) makes Frequent-directions applicable to truly massive and distributed matrices.
Note that Frequent-directions does not take advantage of any possible sparsity of the input matrix. Designing a better version of this algorithm that utilizes the input matrix spar-sity should be possible. One possible direction is to replace the SVD step with a light-weight orhogonalization step. An-other improvement might enable a Frequent-directions -like algorithm that can process the entries of the matrix in an arbitrary order and not only row by row. This is impor-Figure 2: Running time in seconds vs. sketch size. E ach method produces a sketch matrix B of size  X   X  m for a dense n  X  m matrix. Here, n = 10 , 000 , m = 1 , 000 and the value of  X  is indicated on the x -axis. The total amount of computation time re-quired to produce the sketch is indicated on the y -axis in seconds. The brute force method computes the complete SVD of A , and therefore its running time is independent of  X  . Note that hashing is al-most as fast as the na  X   X ve method and independent of  X  which is expected. The rest of the methods ex-hibit a linear dependence on  X  which is also expected. Surprisingly though, Frequent-directions is more com-putationally efficient than random-projection although both asymptotically require O ( nm X  ) operations. Figure 3: Running time in seconds vs. input ma-t rix size. Here, we measure only the running time of Frequent-directions . The sketch size is kept fixed at  X  = 100 . The size of the input matrix is n  X  m . The value of n is indicated on the x -axis. Differ-ent plot lines correspond to different values of m (indicated in the legend box). The running time is measured in seconds and is indicated on the y -axis. It is clear from this plot that the running time of Frequent-directions is linear in both n and m . Note also that sketching a 10 5  X  10 4 dense matrix termi-nates in roughly 3 minutes. tant for recommendation systems. In this setting, user ac-t ions correspond to single non-zeros in the matrix and are presented to the algorithm one by one in an arbitrary or-der. New concentration results show that sampling entries (correctly) yields good matrix sketches, but no deterministic streaming algorithm is known.
 Acknowledgments: The author truly thanks Petros Drineas, Jelani Nelson, Nir Ailon, Zohar Karnin, Yoel Shkolnisky and Amit Singer for very helpful discussions and pointers. [1] Dimitris Achlioptas. Database-friendly random [2] Dimitris Achlioptas and Frank Mcsherry. Fast [3] Rudolf Ahlswede and Andreas Winter. Strong [4] Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast [5] Christos Boutsidis, Petros Drineas, and Malik [6] Christos Boutsidis, Michael W. Mahoney, and Petros [7] Kenneth L. Clarkson and David P. Woodruff.
 [8] Kenneth L. Clarkson and David P. Woodruff. Low [9] Anirban Dasgupta, Ravi Kumar, and Tam  X as Sarl  X os. A [10] Erik D. Demaine, Alejandro L  X opez-Ortiz, and J. Ian [11] Amit Deshpande and Santosh Vempala. Adaptive [12] Petros Drineas and Ravi Kannan. Pass efficient [13] Petros Drineas, Michael W. Mahoney, and [14] Petros Drineas, Michael W. Mahoney, [15] Petros Drineas and Anastasios Zouzias. A note on [16] Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast [17] Phillip B. Gibbons and Yossi Matias. External [18] Daniel M. Kane and Jelani Nelson. Sparser [19] Richard M. Karp, Scott Shenker, and Christos H. [20] Edo Liberty. [21] Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, [22] Michael W. Mahoney, Petros Drineas, Malik [23] Jayadev Misra and David Gries. Finding repeated [24] Roberto Imbuzeiro Oliveira. Sums of random [25] Christos H. Papadimitriou, Hisao Tamaki, Prabhakar [26] Mark Rudelson and Roman Vershynin. Sampling from [27] Tamas Sarlos. Improved approximation algorithms for [28] S. S. Vempala. The Random Projection Method . [29] Roman Vershynin. A note on sums of independent [30] Roman Vershynin. Spectral norm of products of [31] Kilian Weinberger, Anirban Dasgupta, John Langford,
