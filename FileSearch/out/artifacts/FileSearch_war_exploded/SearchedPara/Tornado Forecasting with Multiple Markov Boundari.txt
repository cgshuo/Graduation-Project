 Reliable tornado forecasting with a long-lead time can greatly support emergency response and is of vital impor-tance for the economy and society. The large number of meteorological variables in spatiotemporal domains and the complex relationships among variables remain the top diffi-culties for a long-lead tornado forecasting.

Standard data mining approaches to tackle high dimen-sionality are usually designed to discover a single set of fea-tures without alternating options for domain scientists to select more reliable and physical interpretable variables.
In this work, we provide a new solution to use the concept of multiple Markov boundaries in local causal discovery to identify multiple sets of the precursors for tornado forecast-ing. Specifically, our algorithm first confines the extremely large feature spaces to a small core feature space, then it mines multiple sets of the precursors from the core feature space that may equally contribute to tornado forecasting. With the multiple sets of the precursors, we are able to re-port to domain scientists the predictive but practical set of precursors.

An extensive empirical study is conducted on eight bench-mark data sets and the historical tornado data near Okla-homa City, OK in the United States. Experimental results show that the tornado precursors we identified can help to improve the reliability of long-lead time catastrophic tor-nado forecasting.
 I.5.4 [ Pattern Recognition ]: Applications-Weather Fore-casting
K. Yu and D. Wang contributed equally to this work Corresponding Author c  X  alternative options. In our study, the tornado precursors identified are meteorological predictor variables with certain spatial and temporal information, but due to the complex interactions within the variables, the problems of only iden-tifying a single choice precursors are as follows. (1) Key interpretable precursors may be missed. Due to spatial and temporal adjacency, many precursors contain equivalent information for prediction, because these precur-sors are not physically independent of one another. For example, when there is a dropping in sea level pressure (anomalously low sea level pressure is an important charac-teristic of atmospheric regimes, which may lead to extreme precipitations), at the same location there must also be con-vergence of the winds near the surface and divergence of the winds at the top of the atmosphere. Also, the field of pres-sure vertical velocity may be strongly negative. In this case, the low level winds, high level winds, and vertical motions in the same location are considered equivalent precursors. A typical feature selection method will only identify one of the three equivalent precursors, the others will not be reported to the domain scientists. (2) The precursors identified may be considered imprac-tical. It is useful to explore alternative cost-effective but equally effective solutions in the cases where different precur-sors may have different costs/utilities associated with their acquisition/quality in a forecasting model at the next step. For instance, although the three precursors mentioned in the above example are considered as equivalent precursors, com-pared to high level and low level winds, vertical motion is a noisy and unreliable precursor. Using vertical motion as a precursor in a prediction model introduces extra uncertainty. Accordingly, if we know all of the three precursors contain-ing equivalent information for forecasting, we can select the low level winds or high level winds for reliable prediction.
It is an important yet new research topic to identify mul-tiple Markov boundaries from data using Bayesian inference in the domain of causal discovery [17]. A Bayesian network is presented by a directed acyclic graph G and a joint prob-ability distribution P over a set of features F [11]. If every conditional independence entailed by G is present in P , G is said to be faithful to P [11]. If a data distribution and an underlying Bayesian network which models that domain are faithful to each other, the Markov boundary of a node is unique and contains its parents, children, and the parents of the children (spouses) [11, 1]. In the past decade, peo-ple focused on identification of a single Markov boundary under the faithfulness assumption for causal discovery [1, 18]. However, tornado and other many real-world data dis-tributions often violate the faithfulness condition due to var-ious factors, such as (but not limited to) small sample size, noise in data, and hidden variables. Thus the Bayesian net-works built from such data often contains multiple Markov boundaries [12, 17]. Developing methods of discovery of multiple Markov boundaries would improve the discovery of the underlying mechanisms to avoid overlooking key causal variables, and thus this can be useful to explore alterna-tive cost-effective but equally effective solutions in prediction in applications where different variables may have different costs/utilities associated with their acquisition/quality. of features [5]. Local causal discovery aims to learn a local causal structure closely related to a target feature of inter-est, i.e., a Markov boundary of a target feature of interest. If a joint probability distribution satisfies the faithful con-dition, it is guaranteed to have a unique Markov boundary for every node in a Bayesian network [1]. Accordingly, in the past decade, most of the existing algorithms of local causal discovery focused on identification of a single Markov boundary under the faithfulness assumption [1, 12, 18].
However, many real-world data distributions often vio-late the faithfulness condition due to small sample sizes, noise in data, and hidden variables, and thus contain multi-ple Markov boundaries [12, 17, 22]. Algorithms of multiple Markov boundaries attempts to discover all Markov bound-aries of a target feature containing in data without missing causative variables [17]. Among the most notable advances in the field of discovery of multiple Markov boundaries are the KIAMB algorithm and the TIE* algorithm [17]. Pe  X  n a et al. [12] proposed a stochastic Markov boundary algorithm, called KIAMB by employing a stochastic search heuristic that repeatedly disrupts the order in which features are se-lected for inclusion into a Markov boundary with the proba-bility p at each round, thereby introducing a chance of identi-fying alternative Markov boundaries of a target feature. The limitation is that we do not know how many iterations the KIAMB algorithm needs to run because the exact number of Markov boundaries of a target feature is unknown and varies in different data. To solve this problem, Statnikov et al. [17] recently proposed the TIE* (Target Information Equivalence) algorithm and proved that TIE* can discover all Markov boundaries under the non-faithful condition.
Integrating the drawbacks of the existing work for long-lead tornado forecasting and multiple Markov boundaries, it provides a natural choice to find multiple precursors for long-lead tornado prediction using multiple Markov boundaries. However, both the KIAMB and TIE* algorithms focus on mining multiple Markov boundaries from data on a single feature set. With the real-world tornado data, the KIAMB and TIE* algorithms face the challenges of both very high dimensionality and multiple feature sets. This motivates us to further investigate new algorithms of multiple Markov boundaries for long-lead tornado prediction.
The tornado data set contains eight explanatory variables and tornado information near Oklahoma City, OK, one of the most tornado-prone areas in the United States. All the explanatory variables are sampled at the spatial domain of 90  X  E to 357 . 5  X  E and 0  X  N to 90  X  N with a horizontal res-olution of 2 . 5  X   X  2 . 5  X  (totally 2 , 700 locations) and a daily temporal resolution for the months of March, April and May (MAM, when the highest frequency of violent tornadoes oc-curs in the studied area) for the years 1979-2013. The tor-nado study area near Oklahoma City is located in the spatial eight variables at different levels (Table 1) are selected from the NCEP-NCAR Reanalysis data set [9] by the domain scientists (co-authors of the paper) for the study. In partic-ular, the Relative Humidity data only goes from 1000hPa to 300hPa because the amount of water in the upper tropo-sphere was thought to be negligible when the dataset was designed. Two variables only have one single level (for Sea Level Pressure, the values represent the surface level; for
H ( X | Y ) =  X  X From Equations (1) to (3), the conditional mutual informa-tion is computed by
I ( X ; Y | Z ) = H ( X | Z )  X  H ( X | Y Z ) =  X  X
The lower cases x i , y i , and z i in the above equations de-note possible values that the variables X , Y and Z take.
Definition 1 (Faithfulness). [11] Give a Bayesian network  X  F,G,P  X  , G is faithful to P over F if and only if every independence present in P is entailed by G and the Markov condition. P is faithful if and only if there exists a directed acyclic graph G such that G is faithful to P . Definition 2 (Markov boundary). [11] If a Bayesian network satisfies the faithfulness, the Markov boundary of any node T in this Bayesian network is unique with the set of parents, children and spouses (the parents of the children of T ) of T .

However, if a Bayesian network does not satisfy the faith-fulness, the Markov boundary of any node may not be unique [17]. We use Theorem 1 to explicitly construct and verify multiple Markov boundaries when the distribution P violates the faithful condition.

Theorem 1. [17] If MB 1 is a Markov boundary of T that contains a feature set S 1 , and there exists a subset S 2 such that MB 2  X  ( MB 1  X  S 1 )  X  S 2 , if I ( T ; MB 1 | MB 2 ) = 0 , then MB 2 is also a Markov boundary of T .

In Theorem 1, both MB 1 and MB 2 are Markov bound-aries of T , since S 1  X  MB 1 and S 2  X  MB 2 contain the equivalent information about T .
According to the design of the MB-DEA algorithm, the key to the algorithm is how to discover the core feature space from distributed feature data. The core feature space we are looking for is defined as a feature space that contains all possible Markov boundaries of a target feature. By theorem 1, we get Corollary 1 below.
 Corollary 1. Assuming MB 1 is a Markov boundary of T and MB 2  X  { MB 1  X  X  F i } X  X  F j }} , if I ( C ; F j | F i ) = 0 , MB 2 is also a Markov boundary of T . Figure 2: Discovery of the core feature space of C from distributed feature data I ( C ; F j ). By Corollary 2, the corollary is proven.
Corollary 4. If both MB 1 and MB 2  X  ( MB 1  X  X  F i } )  X  { F j } are Markov boundaries of C and I ( F j ; C | F i ) = 0 holds, then I ( F i ; F j )  X  max( I ( F i ; C ) ,I ( F j ; C )) . Proof.
 Since I ( F j ; C | F i ) = 0 holds and F i and F j are correlated, we can get I ( F i ; F j )  X  I ( F j ; C ). And at the same time, the following equation also holds.
 By Corollary 1, we can also get I ( F i ; C | F j ) = 0. By Eq.(6), we have I ( F i ; F j )  X  I ( F i ; C ). With the equations (5) and (6), Corollary 4 is proved.
 By Corollaries 3 and 4, we can get the following.

Observation 1. If I ( F i ; C ) &gt; I ( F j ; C ) and I ( F i ; F j )  X  max ( I ( F i ; C ) ,I ( F j ; C )) , then F i is in a Markov boundary of C ( MB ( C ) ), and F j is included by &lt; ( F i ) .
Figure 2 gives the new framework to efficiently find the core feature space of C from distributed feature data. In Figure 2, each feature block B i is processed independently at a time, and as a feature block B i arrives, features in B i are processed one-by-one in a sequential scan.

As illustrated in Figure 2, we discuss the pseudocode of the MB-DEA algorithm in Algorithm 1. To achieve a relatively low computational complexity to deal with high-dimensional yet distributed tornado data, according to Ob-servation 1, Corollaries 3 and 4, the MB-DEA algorithm uses online pairwise comparisons as the selection criterion for adding features into the core feature space.

In Algorithm 1, &lt; ( F i ) keeps the set of features correlated to F i that satisfies Definition 3; &lt; dynamically keeps the 4; and CFS ( C ) denotes the core feature space of C . Two key Table 2: Summary of the benchmark data sets 1 arcene 10,000 100 100 2 dexter 20,000 300 300 3 dorothea 100,000 800 300 4 colon 2,000 42 20 5 leukemia 7,129 48 24 6 lung-cancer 12,533 121 60 7 ovarian-cancer 2,190 144 72 8 thrombin 139,351 2,000 543
Steps 27 to 35. We integrate the TIE* algorithm into our MB-DEA algorithm to mine Markov boundaries from the discovered core feature space. The main idea of TIE* [17] is to first identify a Markov boundary of a target feature T in the original data distribution and then itera-tively run a single Markov boundary induction algorithm from the embedded distributions that are obtained by re-moving subsets of features from the original Markov bound-ary in order to identify new Markov boundaries in the origi-nal distribution. From Steps 27 to 35, with the core feature space, the TIE* algorithm can search for multiple Markov boundaries in a smaller feature space.

Step 27 uses a single Markov boundary induction algo-rithm HITION PC [1] to learn a Markov boundary, called MB 1 , from the data set D defined on the feature set CFS ( C ) (i.e., in the original distribution). Step 30 gener-ates a data set D e (the embedded distribution) that removes a subset of features from CFS (Regarding how to generate an embedded distribution, please see the IGS algorithm in [17], Page18, Figure 9). The motivation is that D e may lead to identification of a new Markov boundary of T that was previously  X  X nvisible X  to a single Markov boundary induc-tion algorithm, because it is shielded by another subset of features within the discovered Markov boundaries. Step 32 uses prediction accuracy as a criterion to verify whether a discovered feature set from the embedded distribution is a new Markov boundary or not. If the prediction accuracy of MB new in Step 32 is not less than that of MB 1 , MB new is also considered as a Markov boundary of C . Steps 30-34 are repeated until all data sets D e generated have been considered.
In this empirical study, since the state-of-the-art multi-ple Markov boundary discovery algorithm, the TIE* algo-rithm [17], cannot deal with the high-dimensional yet dis-tributed tornado data set, we will first validate the effec-tiveness and efficiency of the MB-DEA algorithm against the TIE* algorithm using the benchmark data sets, and then use the MB-DEA algorithm for tornado prediction.
We have chosen eight benchmark data sets as described in Table 2, which cover a wide range of real-world appli-cation domains. In Table 2, for the first four NIPS 2003 challenge data sets and the hiva data set from the WCCI 2006 performance prediction challenge, we use the originally provided training and validation sets; for the other five data sets we adopt the first 2/3 instances for training and the last 1/3 instances for testing. As for discrete data sets, we Table 4: Efficiency of MB-DEA vs.TIE*(seconds) Figure 4: Running time of the discovery of the core feature space. The labels of the x-axis in both figures from 1 to 8 denote the data sets:1.arcene, 2.dex-ter, 3.dorothea, 4.colon, 5.leukemia, 6.lung-cancer, 7.thrombin, 8.ovarian-cancer.

Meanwhile, Figure 3 gives prediction (classification) accu-racies of MB-DEA against TIE* using the K Nearest Neigh-bor classifier. We select the highest prediction accuracy among all of the Markov boundaries discovered by MB-DEA and TIE*, respectively. From Figure 3, we can see that with the core feature space, MB-DEA gets the same prediction accuracy as TIE*. Although MB-DEA does not find all of Markov boundaries on the colon data set, it still gets the Markov boundaries with the highest prediction accuracies.
Figure 4 gives the running time of the discovery of the core feature space from Steps 2 to 25 in Algorithm 1. From Figure 4, we can see that the computational cost of the dis-covery of the core feature space is very low due to identifying the &lt; feature set (see Definition 3) without additional time costs. Table 4 gives the running time of MB-DEA against TIE*. From Table 4, we can see that in term of efficiency, MB-DEA is faster than TIE* on all of ten data sets. From the dorothea , dexter , and thrombin data sets in Table 4, with the core feature space, MB-DEA can efficiently deal with data with very high-dimensionality while TIE* is very expensive, or even impossible due to its exhaustive search over the entire feature space.

In summary, with the above results, we can conclude that the core feature space that the MB-DEA algorithm has dis-covered is only a small fraction of the entire feature space, but MB-DEA gets a very promising prediction accuracy with a reasonable running time. Therefore, the MB-DEA algo-rithm is a scalable and accurate method to deal with dis-tributed feature data.

Without Undersampling 0.1429 238  X   X  0.9218 0.2778 result (accuracy=0 . 94, F1=0 . 35) is achieved with P/N ratio of 1:25, in which we have successfully predicted 7 tornado events one day ahead out of 17 during the testing period (MAM 2009-2013, 435 days), with 16 false positives. To the best of our knowledge, our result among the most promising tornado forecasting results at daily level compared to the state-of-the-art algorithms [3].
 Figure 6 uses our empirical results to explain how MB-DEA works. We plot the single Markov Boundary ( MB ( C ) in Algorithm 1), the core feature set CFS ( C ), and the tor-nado precursors (the best Markov boundary according to the criteria of prediction powers) with respect to the ex-periment having best prediction results (F1=0.35) in Figure 6. Firstly, the single Markov boundary MB ( C ) is learned from the tornado data set (Figure 6a), then the core feature set CFS ( C ) is built based on the MB ( C ) (Figure 6b-6e); finally, the best Markov boundary according to prediction powers, is reported (Figure 6f). Different variables in the core feature set CFS ( C ) fall into clusters in the spatial do-mains (blue circles in Figure 6b-6e). The arrows from Figure 6a to Figure 6d show that how a feature from the field of Pressure Vertical Velocity helps to generate a cluster of re-lated features (mostly from the field of Pressure Vertical Ve-locity and Relative Humidity), and later contributes to the identification of precursor 12 (Figure 6f). The variables from the same field closed to each other in the spatial domains will tend to have similar values due to the spatial autocor-relation effect [23]. These spatial clusters can be considered as real-world examples of the CFS ( C ) illustrated in Figure 2. Our algorithm picks individual precursors out of each cluster and successfully finds the best combination (Markov Boundary) according to the forecasting task.

As an algorithm of discovering multiple Markov bound-aries, MB-DEA is not limited to finding the feature set with the best prediction result. MB-DEA is able to report differ-ent precursor sets with similar prediction powers according to other domain interests. For example, compared to other variables in the tornado data set, the Pressure Vertical Ve-locity is considered as a noisy and unreliable variable. In the feature set with the best prediction result (Figure 6f, b-set for short), we have two features (No.7 and No.12) from the field of Pressure Vertical Velocity. From all the MB-DEA outputs generated from the same input data set, we are able to find a feature set whose features are all from fields other than Pressure Vertical Velocity (Figure 7, we call it the  X  X ost reliable X  set, or m-set for short.) and have similar prediction power (F1=0.3265). From Figure 6f and Figure 7, we find that more than half of the features in the two sets (No.1,2,3,4,8,10,and 11) are exactly the same, and others (except No.6) are from the same spatial clusters in the core feature set. In the m-set instead of Pressure Vertical Ve-locity, variables from Relative Humidity are selected (No.5 area for tornado labeling. map is the target area for tornado labeling. [5] C. P. De Campos and Q. Ji. Efficient structure [6] C. A. Doswell, H. E. Brooks, and N. Dotzek. On the [7] J. H. Faghmous and V. Kumar. A big data guide to [8] I. M. Held, R. T. Pierrehumbert, S. T. Garner, and [9] E. Kalnay, M. Kanamitsu, R. Kistler, W. Collins, [10] H. Liu and L. Yu. Toward integrating feature selection [11] J. Pearl. Probabilistic reasoning in intelligent systems: [12] J. M. Pe  X na, R. Nilsson, J. Bj  X  orkegren, and J. Tegn  X er. [13] C. M. Shafer, A. E. Mercer, L. M. Leslie, M. B. [14] C. E. Shannon. A mathematical theory of [15] K. Simmons and D. Sutter. Economic and societal
