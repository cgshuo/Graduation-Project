 Phrase-based statistical machine translation has im-proved significantly over the last decade. The avail-ability of large amounts of parallel data and access to open-source software allow for easy setup of trans-lation systems with acceptable performance. Pub-lic evaluations such as the NIST MT Eval or the WMT Shared Task help to measure overall progress within the community. Most of the groups use a phrase-based decoder (e.g. Pharaoh or the more re-cent Moses) based on a log-linear fusion of models that enable the avid researcher to quickly incorpo-rate additional features and investigate the effect of additional knowledge sources to guide the search for better translation hypotheses.

In this paper, we deal with an extended lexicon model and its incorporation into a state-of-the-art decoder. We compare the results of the integration to a similar setup used within a rescoring frame-work and show the benefits of integrating additional models directly into the search process. As will be shown, although a rescoring framework is suit-able for obtaining quick trends of incorporating ad-ditional models into a system, an alternative that in-cludes the model in search should be preferred. The integration does not only yield better performance, we will also show the benefit of combining both ap-proaches in order to boost translation quality even more. The extended lexicon model which we apply is motivated by a trigger-based approach (Hasan et al., 2008). A standard lexicon modeling dependen-cies of target and source words, i.e. p ( e | f ) , is ex-tended with a second trigger f 0 on the source side, resulting in p ( e | f, f 0 ) . This model allows for a more fine-grained lexical choice of the target word de-pending on the additional source word f 0 . Since the second trigger can move over the whole sentence, we capture global (sentence-level) context that is not modeled in local n -grams of the language model or in bilingual phrase pairs that cover only a limited amount of consecutive words.
 Related work A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collo-cations of words are used to refine phrase pair selec-tion dynamically by incorporating scores from the WSD classifier (Chan et al., 2007). A maximum-entropy based approach with different features of surrounding words that are locally bound to a con-text of three positions to the left and right is re-ported in (Garc  X   X a-Varea et al., 2001). A logistic regression-based word translation model is investi-gated by Vickrey et al. (2005) but has not been eval-uated on a machine translation task. Another WSD approach incorporating context-dependent phrasal translation lexicons is presented by Carpuat and Wu (2007) and has been evaluated on several translation tasks. The triplet lexicon model presented in this work can also be interpreted as an extension of the standard IBM model 1 (Brown et al., 1993) with an additional trigger. The main focus of this work investigates an extended lexicon model in search and rescoring. The model that we consider here and its integration in the de-coder and setup for rescoring are presented in the following sections. 2.1 Extended lexicon model The triplets of the extended lexicon model p ( e | f, f 0 ) are composed of two words in the source language triggering one target word. In order to limit the over-all number of triplets, we apply a training constraint that reuses the word alignment information obtained in the GIZA ++ step. For source words f , we only consider the ones that are aligned to a target word e given the GIZA ++ word alignment. The second trig-ger f 0 is allowed to move over the whole source sen-tence, thus capturing long-distance effects that can be observed in the training data: where { a ij } denotes the alignment matrix of the sen-tence pair f J 1 and e I 1 and the first sum goes over all f j that are aligned to the current e i (expressed as j  X  { a i } ). The factor Z i = J  X |{ a i }| normalizes the double summation accordingly. Eq. 1 is used in the iterative EM training on all sentence pairs of the training data. Empty words are allowed on the trig-gering part and low probability triplets are trimmed. 2.2 Decoding Regarding the search, we can apply this model di-rectly when scoring bilingual phrase pairs. Given a trained model for p ( e | f, f 0 ) , we compute the feature score h t of a phrase pair (  X  e,  X  f ) as h t (  X  e,  X  f, {  X  a ij } , f J 1 ) = (2) where i moves over all target words in the phrase  X  e , the sum over j selects the aligned source words  X  f j given {  X  a ij } , the alignment matrix within the phrase pair, and j 0 incorporates the whole source sentence 1 . Analogous to Eq. 1, Z i = J  X  |{  X  a i }| denotes the number of overall source words times the num-ber of aligned source words to each  X  e i . In Eq. 2, we take negative log-probabilities and normalize to obtain the final score (representing costs) for the given phrase pair. Note that in search, we can only use this direction, p ( e | f, f 0 ) , since the whole source sentence is available for triggering effects whereas not all target words have been generated so far, as it would be necessary for the reverse direction, p ( f | e, e 0 ) . Due to data sparseness, we smooth the events during decoding. Furthermore, an implicit backoff to IBM1 exists if the second trigger is the empty word, i.e. for events of the form p ( e | f,  X  ) . 2.3 Rescoring In rescoring, we constrain the scoring of our hy-potheses to a limited set of n -best translations that are extracted from the word graph, a pruned com-pact representation of the search space. The advan-tage of n -best list rescoring is the full availability of both source text and target translation, thus allow-ing for the application of additional (possibly more complex) models that are hard to implement directly in search, such as e.g. syntactic models based on parsers or huge LMs that would not fit in memory during decoding. Since we are limiting ourselves to a small extract of translation hypotheses, rescoring models cannot outperform the same models if ap-plied directly in search. One advantage though is that we can apply the introduced trigger model also in the other direction, i.e. using p ( f | e, e 0 ) , where two target words trigger one source word. Generally, the combination of two directions of a model yields fur-ther improvements, so we investigated how this ad-ditional direction helps in rescoring (cf. Section 3.1).
In our experiments, we use 10 000-best lists ex-tracted from the word graphs. An initial setting uses the baseline system, whereas a comparative setup in-corporates the ( e | f, f 0 ) direction of the trigger lexi-con model in search and adds the reversed direction in rescoring. Additionally, we use n -gram posteri-ors, a sentence length model and two large language Sent. pairs 9.1M 480 490 Run. words 259M/300M 14.8K 12.3K Vocabulary 357K/627K 3.6K 3.2K models, a 5-gram count LM trained on 2.5G running words and the Google Web 1T 5-grams. The feature weights of the log-linear mix are tuned on a separate development set using the Downhill Simplex algo-rithm. The experiments are carried out with a GALE sys-tem using the official development and test sets of the GALE 2008 evaluation. The corpus statistics are shown in Table 1. The triplet lexicon model was trained on a subset of the overall data. We used 1.4M sentence pairs with 32.3M running words on the En-glish side. The vocabulary sizes were 76.5K for the source and 241.7K for the target language. The final lexicon contains roughly 62 million triplets.
The baseline system incorporates the standard model setup used in phrase-based SMT which com-bines phrase translation and word lexicon models in both directions, a 5-gram language model, word and phrase penalties, and two models for reorder-ing (a standard distortion model and a discriminative phrase orientation model). For a fair comparison, we also added the related IBM model 1 p ( e | f ) to the baseline since it can be computed on the sentence-level for this direction, target given source. This step achieves +0.5% BLEU on the development set for newswire but has no effect on test. As will be pre-sented in the next section, the extension to another trigger results in improvements over this baseline, indicating that the extended triplet model is superior to the standard IBM model 1. The feature weights were optimized on separate development sets for both newswire and web text.

We perform the following pipeline of experi-ments: A first run generates word graphs using the baseline models. From this word graph, we ex-tract 10k-best lists and compare the performance to a reranked version including the additional models. In a second step, we add one of the trigger lexi-Chinese-English newswire web text
GALE test08 BLEU TER BLEU TER baseline 32.5 59.4 25.8 64.0 triplets in search ef 33.1 58.8 26.0 63.5 con models to the search process, regenerate word graphs, extract updated n -best lists and add the re-maining models again in a reranking step. 3.1 Results Table 2 presents results that were obtained on the test sets. All results are based on lowercase eval-uations since the system is trained on lowercased data in order to keep computational resources fea-sible. For the newswire setting, the baseline is 32.5% BLEU and 59.4% TER. Rescoring with addi-tional models not including triplets gives only slight improvements. By adding the path-aligned triplet model in both directions, we observe an improve-ment of +0.7% BLEU and -0.8% TER. Using the triplet model in source to target direction ( e, f, f 0 ) during the search process, we arrive at a similar BLEU improvement of +0.6% without any rerank-ing models. We add the other direction of the triplets ( f, e, e 0 ) (the one that can not be used directly in search) and obtain 33.7% BLEU on the newswire set. The overall cumulative improvements of triplets in search and reranking are +0.9% BLEU and -0.9% TER when compared to the rescored baseline not in-corporating triplet models and +1.2%/-1.3% on the decoder baseline, respectively.

For the web text setting, the baseline is consid-erably lower at 25.8% BLEU and 64.0% TER (cf. right part of Table 2). We observe an improvement for the baseline reranking models, a large part of which is due to the Google Web LM. Adding triplets to search does not help significantly (+0.2%/-0.5% BLEU/TER). This might be due to training the triplet lexicon mainly on newswire data. Rerank-ing without triplets performs similar to the baseline experiment. Mixing in the ( f, e, e 0 ) direction helps again: The final score comes out at 27.2% BLEU and 62.0% TER, the latter being significantly better than the reranked baseline (-1.5% in TER). 3.2 Discussion The results indicate that it is worth moving models from rescoring to the search process. This is not surprising (and probably well known in the com-munity). Interestingly, the triplet model can im-prove translation quality in addition to its related IBM model 1 which was already part of the base-line. It seems that the extension by a second trigger helps to capture some language specific properties for Chinese-English which go beyond local lexical (word-to-word) dependencies. In Table 3, we show an example of improved translation quality where a triggering effect can be observed. Due to the topic of the sentence, the phrase local employment was cho-sen over own jobs . One of the top triplets in this con-text is p (employment |  X   X  ,  X   X  ) , where  X   X  is  X  X mployment X  due to the path-aligned constraint and  X   X  means  X  X alent X . Note that the distance be-tween these two triggers is five tokens. We presented the integration of an extended lexicon model into the search process and compared it to a variant which was used in reranking n -best lists. In order to keep the overall number of triplets feasi-ble, and thus memory footprints and training times low, we chose a path-constrained triplet model that restricts the first source trigger to the aligned target word, whereas the second trigger can move along the whole source sentence. The motivation was to allow for a more fine-grained lexical choice of tar-get words by looking at sentence-level context. The overall improvements that can be accounted to the triplets are up to +0.9% BLEU and -1.5% TER.

In the future, we plan to investigate more triplet model variants and work on additional language pairs such as French-English or German-English. The reverse direction, p ( f | e, e 0 ) , is hard to imple-ment outside of a reranking framework where the full target hypotheses are already fully generated. It might be worth looking at cross-lingual trigger mod-els such as p ( f | e, f 0 ) or constrained variants like p ( f | e, e 0 ) with e 0 &lt; e , i.e. the second trigger com-ing from the left context within a sentence which has already been generated.
 Acknowledgments
