 Click models are developed to interpret clicks by making assumptions on how users browse the search result page. Most existing click models implicitly assume that all users are homogeneous and act in the same way when browsing the search results. However, a number of researches have shown that users have diverse behavioral patterns, which is also observed in this paper by eye-tracking experiments and click-through log analysis. As a uniform click model for all users can hardly capture the diverse click behavior, in this paper we incorporate user preferences into both a variety of existing click models and a novel click model. The experimental results on a large-scale click-through data set show consistent and significant performance improvement of the click models with user preferences integrated. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Human Factors Web search, click model, user preference
Query logs of a search engine, in particular, the click-through data, contains rich information of user preference and satisfaction with the search results and thus is a valu-able source in relevance inference. A number of studies [1, 8, 13, 14, 16] have been carried out to extract useful informa-tion from click-through data and to better understand user interactions to help improve and evaluate the search quality.
A fundamental problem in modeling user clicks is the po-sition bias. That is, the probability of a document being clicked depends not only on its relevance, but also on its position in the search result page. Previous study [14] has shown that the examination probability decays as the rank-ing position increases. Such bias has been taken into account in the previous click models, such as dependent click model [11], click chain model [10], user browsing model [9], dynam-ic Bayesian network click model [4] and etc. Some studies even introduced more complex variables in the examination assumption, such as users X  revisit behavior [23] and the in-fluence of vertical search results [19]. These previous models showed success in fitting the real-world data and predicting future clicks. However, what we observe is that in all these models, the users are assumed to act in the same way, i.e. to have the same examination and click behavior. This is obviously not true in reality, and it has been documented in many studies. For example, White et al. showed dramatic differences between Web search users in some key aspects of the interaction, such as issued query, clicked result, and post-query browsing [20, 21]. It is also reported that domain experts search differently from users with little or no domain knowledge with respect to session length, site selection and search effectiveness [2, 24, 7, 22]. All these studies indi-cate that users differ with respect to their search behaviors. Therefore, click models based on a uniform user behavior assumption can hardly account for personal preferences of users.

The personal preferences of a Web search user can be re-flected in many aspects, such as domain interest, search in-tent and behavioral habit. Recent researches of click model have incorporated user personality with respect to domain interest [18] and search intent [12], which showed promising results. In our study, we observe that one X  X  behavioral habits in browsing and clicking have impact on all her/his queries, and should be taken into account in click models. To the best of our knowledge, no previous study has considered user personality from this aspect in a click model.

In this paper, we aim to develop click models that in-corporate user search habits and preferences. Such models are motivated by the strong differences in search behavior of users observed in our analysis of click logs of a search engine as well as the eye-tracking experiments with human subjects. We observe that some users tend to examine more documents than the others; and some users tend to click on more documents. Based on the observations, we intro-duce examination preference and click preference to describe the general behavioral preferences of a user in Web search environment. We then build a variety of user-specific click models by incorporating these preferences. Our experimen-ta l results show consistent and significant performance im-provement for click models with user preferences integrated. The main contributions of this work are:
The rest of this paper is organized as follows: In section 2, we review the related work on click model. In section 3, we investigate the diversity of user behavior by log analysis and eye-tracking study. In section 4, we build user-specific click models by incorporating user preferences. The experiment results on a real-world click-through data are reported in section 5. We then discuss the results in Section 6 and give the conclusion in Section 7.
The cascade model [6] is a classic click model which makes the following assumption: users examine the result docu-ments in the ranking order from top to bottom, and the session will be terminated once a document is clicked. It actually makes a very strong assumption that all users are equally patient in finding relevant documents, and it can not model the sessions with more than one click. The de-pendent click model [11] extends the one-click assumption of the cascade model by allowing users to continue exam-ining the next document with a fixed probability  X  after a document is clicked. The click chain model [10] further im-proves the dependent click model. Instead of letting  X  being a fixed value, the probability is set to be related to the rele-vance of the clicked document in the click chain model. The user browsing model [8] is another popular click model for its clear representation, easy computation and good perfor-mance. It assumes that the probability of a document being examined depends both on its ranking position and the dis-tance to the last clicked position, which is different from the previous models. The user browsing model introduces a large number of examination parameters, 55 in total.
Each of the abovementioned models imposes strong hy-pothesis on users X  examination and click behavior. Although the results showed that such models can capture user clicks to certain extent, we observe that all these models have the same examination/click parameters for different users. In practice, it has been observed that users behave very dif-ferently in previous studies [20, 21]. For example, it is sug-gested by White et al. [21] that search users have dramatic differences in the interaction with the search engine. Two extreme user classes are reported in their paper. The  X  X av-igators X  have consistent interaction patterns. They appear to tackle problems sequentially and are more likely to revisit the domains. The  X  X xplorers X  have variable interaction pat-terns. They tend to branch frequently in search tasks and visit many new domains. According to their findings, the behavioral level differences of users can greatly affect the probability of a user examining or clicking a document.
Some recent studies tried to incorporate some user fac-tors into click models. Hu et al. [12] considered different search intents of users, which has an influence on clicks. It assumes a bias between the users X  search intent and the re-turned results in each search session. Search intent is taken into account as a hidden variable in their model, and the click probability of a document then depends on both the document relevance and the search intent of the user. The experiments showed that the model can do better in inter-preting clicks. However, search intent is a factor that is still difficult to recognize and capture in advance. As a conse-quence, the proposed model has a limited ability to make click prediction for an incoming search session. Shen et al. [18] noted the influence of the interested topics of individual search users and proposed a framework for personalized click models from the view of collaborative filtering. They argue that the global query-document relevance is not sufficient to reflect the interest of an individual user to a document. That is, one will be interested in a document when the underlying topics of the document match her/his own interested topics. In their paper, matrix factorization was used to characterize the latent factors of queries, documents and users. The la-tent factors of a user indicate the potential aspects of her/his interest. Thus for a query-document pair, their method will generate personalized probabilities of user being interested. Their experiment results showed that using the latent factors of queries and documents alone can significantly improve the accuracy of the baseline click model (user browsing model), and incorporating user latent factors can further boost the improvement. Besides Web search, click models are also ap-plied in sponsored search to optimize the CTR of ads [17]. Cheng et al. [5] investigated the role of user personality in sponsored search and proposed two groups of user specific features. One group contains demographic features, such as gender, age, marriage status, interest and job title; the other group contains user click features, such as the CTRs at different levels. After adding these features to a baseline non-personalized click model, an improved accuracy of click prediction is obtained. This work shows the effectiveness of the personalized click model in sponsored search.
All the above approaches incorporated some user-specific features to improve click models. However, these models do not capture the user personality at a general behavioral level, i.e. a user has consistent behavioral habits across all her/his searches, which is content independent. In this paper, we will focus on the differences of users at the general behavioral level and build user-specific click models with this type of user preferences incorporated.
From a general behavioral perspective, search users may have different behavioral habits due to their personalities. For example, when searching with a query, some users are willing to click many documents, whereas some others are reluctant to click the documents beyond top-ranking posi-tions. Some users browse the search result page from top to bottom and make careful click selections, while some oth-ers simply trust the search engine to provide the most rele-we observe the diversity in users X  click behavior. vant documents in the top-ranking positions and will easily lose patience after having examined several irrelevant doc-uments. These personal preferences indicate clearly that different behavioral assumptions are required for different types of user. To verify the diversity of user behavior, in this section we study users X  examining behavior by carrying out an eye-tracking experiment on 21 human subjects, and study users X  clicking behavior by analyzing a click log of a real-world search engine.
We first investigate how users click the documents in search result pages by analyzing the click log of a search engine. For each user, we compute: (i) the number of clicks per session, (ii) the average click position and (iii) the average maximum click position. In the click log, a user is identified by cookie ID, which lasts a long enough time unless the user clears the cookie manually in the browser. A session is defined as the sum of the activities of a user searching one query in a continuous period of time, with a timeout of 30 minutes. To avoid noise, we only count the users who have no fewer than ten sessions. The result is presented in Figure 1.
The blue curve in Figure 1(a) shows the user distribu-tion in terms of average click position. The majority of users (43.0%) have this value between 2 and 3. However, 27% of users are below 2 and the remaining 30% are above 3. The red curve in Figure 1(a) shows the user distribu-tion in terms of average maximum click position, which is the last clicked position averaged across sessions. We ob-serve that over a half (54%) of the users have this value over 3. These dispersed distributions show user diversity in the clicking behavior. It may also imply the diversity of users in examination depth, which we will further verify in the following eye-tracking study. Figure 1(b) shows the user distribution over average number of clicks per session. Al-though the majority of the users have fewer than two clicks per session, there are still quite a lot of users (29%) who conducted more than two clicks per session. Besides the reason that the users may have examined different number of documents, another explanation for this diversity is that some users have stronger intent to click the returned doc-uments than the others, which can be interpreted as click preference.
Click logs cannot record the full information of users X  ex-amination. Although some simple rules can be used to infer whether a document is examined or not (e.g. the clicked doc-Fi gure 3: The deepest examination positions of the 21 participants on one of the queries. Each triangle represents a participant. The size of a triangle indi-cates the fixation time. The fixation time threshold is 300 milliseconds for a position to be judged as  X  X xamined X  ument must be examined and the documents before a clicked document are supposed to be examined in most cases), we still can not decide whether the documents after a clicked document are examined. Thus it is difficult to determine the exact depth that a user has examined in a search session us-ing click logs alone. Therefore, we carry out an eye-tracking experiment to obtain the explicit examination data of search users in a laboratory environment. We recruited 21 partici-pants, who are college students from different disciplines. In the experiment, ten queries are randomly selected from the search engine query log as search tasks. Each participant is required to search with all the ten queries using a real Web search engine on a computer with eye-tracking device installed.

The eye-tracking device detects the pupil of the subject and the software computes the corresponding fixation point on the computer screen. When a subject searches a query, the trail of her/his eye movements is then recorded. With the scan trail of each session (i.e. a subject searching a query), the examination depth can be obtained. Here ex-amination depth is defined as the position of lowest-ranked document whose snippet is examined by the subject. To de-termine whether a document is examined or not, we test if there is any point with a fixation time larger than 300 mil-liseconds in the corresponding area of the document. Figure 2(a) shows how the sessions are distributed on examination depth. The distribution is quite dispersed, showing the di-versity of examination depth across all sessions. We notice that position 10 attracted the most user attention across sessions, which may not be the case in real search scenarios. One of the reasons is that the queries we use here are ran-domly sampled from the search engine query log, so most of them are not very frequent queries for which relevance doc-uments can be easily found at early positions. For most our queries, the subjects will have to explore more documents to meet the information need. Although the participants are given the instruction that they should search the queries as normal, the laboratory environment may still be an factor that makes the participants behave differently from the real-world scenarios. However, as all the participants are in the same settings, it is still feasible to investigate the differences among them, which is the main purpose of this experiment.
To investigate the difference among the participants, we calculate the average examination depth of each participant over the ten queries. As demonstrated in Figure 2(b), the participants are diversely distributed over average examina-tion depth. Participant P04 has the lowest average exam-ination depth 5.6 while the number for P08 is 9.4, which indicates that the participant almost examined all the ten documents for each query. This fact is a strong indicator of the user diversity in examining behavior. More concretely, Figure 3 shows the examination depth of the participants on one of the queries, in which each triangle represents a participant and the position corresponds to the deepest ex-amined position. This image gives us a direct impression on how different the search users are when examining a search result page.
According to the results of click log analysis and eye-tracking study, the examining behavior and the clicking be-havior exhibit considerable diversity across users. As sug-gested by the analysis above, the behavioral level user per-sonality is one of the causes for this diversity. Therefore, we propose two user preference factors to describe the behav-ioral level personality of a user in Web search scenario:
In the following of this paper, we will focus on studying the influence of these two preference factors to click models. In the next Section, we build user-specific click models by incorporating them into several click models.
As we have found in the previous section that the personal preferences indeed exist in users X  search behaviors, better performance can be expected for click models by taking into account the user preference factors. In this section, we build a variety of user-specific click models by incorporating the examination preference and the click preference into some existing click models and a novel click model we propose in this paper.
The user browsing model [9] is an effective and efficient model proposed by Dupret et al. Unlike the click models which assume that the examination probability of a docu-ment depends on its ranking position only, the user brows-ing model takes into account additional information when modeling the examination probability: the distance of the current document to the last clicked document in the search result page. This assumption increases the number of ex-amination parameters from the typical 10 to 55 (we only consider the top ten returned documents in the first search result page), which brings in more flexibility to the mod-el. Following the examination hypothesis, a document gets clicked only when it is both examined and relevant. Two bi-nary variables E and A are associated with the examination and the perceived relevance respectively in this model, and they both follow Bernoulli distribution: in which q is the query; u is the result document; r is the ranking position of u and d is the distance to the last clicked document.  X  is the probability of the document being rele-vant and  X  is the probability of the document to be exam-ined. Let C be the variable indicating whether the document is clicked. The joint probability of the variables is written as:
P ( C, A, E j u, q, r, d ) = P ( C j A, E ) P ( A j u, q ) P ( E where probability P ( C j A, E ) is deterministic according to the examination hypothesis. A and E are independent from each other so their joint probability can be written as the product of two separate parts. In this calculation, user re-lated factors are not considered. Therefore, it will derive the same click probability for different users. As indicated by our analysis above, both E and A should be user spe-cific in our setting of user preferences. A straightforward way to introduce the user preference factors is to make E and A Bernoulli variables with new parameters. That makes A denotes the specific user.  X  p is the parameter for examina-tion preference of user p and  X  p is the parameter for click preference. This transformed formulation seems to mod-el well what we intend to do; however, it does not have a closed form solution for optimization with the expectation-maximization (EM) algorithm, which is used for parameter estimation in the original user browsing model. Although gradient descent algorithms can be used instead, we still want to use the EM algorithm for its efficiency and elegant form, and this also makes the following comparison between the two models fair. Therefore, instead of making new pa-rameters for E and A , we directly add two new variables into the model, that are H and I . H indicates whether the user X  X  examination preference is met and I indicates whether the click preference is met. We assume they follow Bernoulli distribution as well: and the joint probability becomes:
Given that a click only happens when H, I, A, E all take the value 1, the click probability becomes:
The form of this joint probability makes EM algorithm possible for optimization. We note that it might be more flexible to let H and I be dependent on specific ranking position and other factors. However, as we have stated in the previous section, we simply let H and I be consistent for a user within all her/his search sessions to avoid introducing too many parameters that will probably cause serious over-fitting problems.
In the position model [6], the examination probability of a document is assumed to be related only to its ranking po-sition, regardless of the previous clicks. Compared to the user browsing model, it has only ten examination parame-ters which actually implies a stronger examination assump-tion. In this model, the click probability of a document is calculated as: and we incorporate the user preferences in the same way as we did in the user browsing model. Variables H and I are introduced and the click probability becomes:
With only P ( E ) being different from the user browsing model, the EM algorithm can also be used for efficient pa-rameter estimation in the position model.
The logistic model is proposed along with the user brows-ing model in [9]. It also follows the examination hypothe-sis that the click probability is the product of two separate probabilities, except that each of the two probabilities is cal-culated by a logistic function. In the logistic model, the click probability is rewritten as: where  X  is the logistic function:
This model is attractive because the logistic function al-ways outputs a value ranging between 0 and 1, which has a perfect probability meaning. In such case the linear con-straints for 0  X  r;d 1 and 0  X  u;q 1 in optimization are removed. Without the constraints, algorithms such as gradient descent can be easily adopted to estimate the pa-rameters for this model. On the other hand, the logistic function also has a great advantage to plug in new features in a much easier way compared to the models in probabilistic frameworks, such as the user browsing model and the posi-tion model. To incorporate user preferences into the logistic model, we can simply rewrite the logistic functions with the new parameters:
Now  X  p and  X  p will have an influence on the probability along with  X  r;d and  X  u;q . The parameters can be estimat-ed by maximizing the likelihood of the data set using the gradient descent algorithm.

Note that the examination parameter we use here is  X  r;d , which follows the examination assumption of the user brows-ing model. Similarly, we can also use the examination as-sumption of the position model in this logistic model, for which the click probability becomes:
Equations 10 and 11 lead to two logistic models with d-ifferent examination assumptions. We implement both of them in our experiments.
The cascade model [6] assumes that users examine the documents one by one from top to bottom in the search result page until a document is clicked. It has shown success in explaining clicks at early ranking positions. In this model, the probability of the i th document to be clicked is: in which r i is the probability of the i th document being relevant. By maximizing the likelihood of all session obser-vations of a query, r i can be efficiently estimated in a closed form calculation where n i is the number of times the document is clicked and m i represents the number of times the document is skipped before a click. The solution is quite straightforward and easy to compute. Note that as the assumption that the users will examine all documents before a click is solidly embedded in this cascade model, the examination preference is not valid here. Therefore, we only incorporate the click preference into the cascade model and the click probability becomes:
Now the probability of the i th document to be clicked after being examined by user p is dependent on both its relevance and the click preference of p . After this transformation, the EM algorithm is still usable for optimization.
So far we have been using the  X  X lobal + personal X  way to incorporate user preference factors into a variety of existing click models. On the other hand, instead of using this pat-tern, we can make all parameters user-specific as well, i.e. without global parameters. This approach is more straight-forward but often suffers from over-fitting problems if too many personalized parameters are introduced. For example, in the user browsing model there are 55 examination param-eters in total. If we make all these parameters user-specific to reflect personal examination preferences, we will have 54 times more examination parameters to estimate in the new model. In such case, it is clear that heavy over-fitting prob-lems will occur. However, if the number of parameters in the original model is small enough, it may be possible to assign each user a set of personal parameters. To compare with the previous method of incorporating user preferences, we propose a new click model which has only three parameters for the examination assumption.

Generally, more parameters bring more flexibility to the model, leading to a stronger fitting capacity. However, if there are some patterns embedded in the parameters (i.e. the parameters are correlated and thus redundant some-how), we may use fewer parameters with approximately the same representation ability. It has been found in the es-timated result of the user browsing model that the exam-ination probability decays when: 1) the ranking position increases; 2) the distance to the last click increases. Thus in this new model, we set two damping factors for the two cases respectively. Furthermore, to capture the additional information on how many documents have been clicked be-fore, which is not considered in the user browsing model, we add the third damping factor: the examination probability decays as the number of documents clicked before increases (i.e with more documents clicked before, the user X  X  informa-tion need is more likely to have been met). We call this model the dilution model.

Given that k documents are clicked before the r th ranking position and the distance from r to the last clicked position is d , the examination probability of the r th document in this dilution model is calculated as: in which  X  p is the damping factor of p for the ranking posi-tion;  X  p is the damping factor for the clicks before; and  X  is the damping factor for the distance to the last click. Each damping factor is a real valued parameter ranging from 0 to 1, which needs to be estimated. In the model, we assume that the first document in a search result page always has an examination probability of 1, and d is set to 0 if there are no clicks before.

Compared to the  X  X lobal + personal X  way of incorporating user preferences in the previous click models, this dilution model allows each user to have personal examination pa-rameters that are not shared by the others. It makes the model more descriptive and reasonable for a specific user. By knowing the damping factors of a user, we can have a direct sense of how likely she/he is going to examine the doc-uments. As to the click preference, we use the same method as in the previous models because there are too many query-document pairs and we cannot make all the relevance pa-rameters user-specific. After these modifications, the click probability turns out to be: in which  X  p is the click preference of user p and the param-eter set f  X  p ,  X  p ,  X  p ,  X  p g describes the preferences of p . All the parameters are valued between 0 and 1.
We use the click logs of a real-world Chinese search engine during a period of one month (November 2011) for exper-iments. The data is sampled by users to control the total size, i.e we select a random subset of users and use all of their click data in that month for the following experiments. To limit the noise in the data set, all the queries with fewer than 10 sessions are removed because there are not enough click data for the estimation of document relevance for these queries. Here a session is defined as a unique user-query pair in a continuous time period (with 30 minutes timeout). Query reformulations are treated as different sessions. In our data, a user is identified by cookie ID. For the protec-tion of users X  privacy, all sensitive attributes, such as query string and document URL, are replaced by numbers. As the users may clean the cookie from time to time, there are a big portion of users who have few search sessions in the month. To guarantee that there is enough data for each user when building the user-specific click models, we also remove the users who issued fewer than 10 distinct queries during the period of time. After the filtering, the final data set has 10,012 unique users, 53,048 unique queries and 668,105 sessions. We then split the data set into training part and test part in the following way: for each user, the first 80% of her/his sessions, ordered by timestamp, are used for train-ing and the remaining 20% are used for testing. For each session, only the top ten returned documents (in the first search result page) are used for modeling.

In our experiments, we train the following models: us-er browsing model (UBM), position model (POS), logistic model (LOG-r and LOG-rd, using different examination as-sumptions), cascade model (CAS), dilution model (DIL) and their user-specific versions with user preferences incorporat-ed (with postfix  X -user X  in the name). A comparative study is carried out using multiple evaluation measures.
After learning the parameters of each model, we first eval-uate their performances using perplexity as a metric. Per-plexity is widely used in the evaluation of click models. It measures how well a trained model fits the real data, which is calculated as follows: where N is the number of observations and p i is the probabil-ity of the observation i being correctly predicted. Perplexity has a perfect value of 1 when the model is able to predict each single observation correctly. The lower the perplexity value, the better the model. When evaluating click models with perplexity, an event of click or skip is considered as an observation. For a click observation, p i is the predicted probability of the document to be clicked. For a skip obser-vation, p i is the predicted probability of the document not to be clicked. We calculate the perplexity for click observa-tions, skip observations and all observations separately and the results are shown in Table 1. To avoid zero values in equation 17, we let the predicted probability of an observa-tion have a minimum value of 0.001 and a maximum value of 0.999. Also, the query-document pairs that appeared in few-er than five observations are excluded from the evaluation to limit the noise in the result.

It is observed that after incorporating user preferences, perplexity on the test set is consistently improved for all click models. Some of them gain notable improvements. LOG-rd-user gains as much as 13.7% 1 improvement over LOG-rd, followed by DIL-user (11.3%) and CAS-user (7.1%). Among the probabilistic models, only the cascade model gains signif-icant improvement. The improvements for the user browsing model and the position model are however limited. The rea-son is that they both fail in improving perplexity for the skip observations. On the other hand, the logistic function based models benefit more from the integration of user preferences. This fact indicates that the current method we use to inte-grate user preferences is generally helpful for click models but is still not optimized.

It is expected that the models with distance information (distance to the last click) considered are likely to perform better than the models without the information. The former have richer information and more examination parameters, so they are supposed to be more powerful and flexible. In our experimental results, UBM is 12.5% better than POS; LOG-rd is 9.5% better than LOG-r. After incorporating us-er preferences, UBM-user and LOG-rd-user still beat POS-user and LOG-r-user respectively as expected. We also ob-serve that the models with distance information considered benefit more from the integration of user preferences than the models without distance information. It is reasonable because a model with larger flexibility has larger room for improvement as well.

Among all the models presented in Table 1, the best per-forming model is LOG-rd-user. Given the fact that LOG-rd is not the best performing one among the models without user preferences, the significance of taking into account user preferences in a click model is demonstrated.
Perplexity is an appropriate measure for the fitting abil-ity of a click model. In this section we evaluate the click models by predicting real clicks. The aim is to investigate how the clicks predicted by different models match the real data. The click models are used to predict the first clicked position and the last clicked position [11]. The gap between the predicted position and the real position is able to reflect the effectiveness of a model to certain extent.

Given a click model and the trained parameters, we sim-ulate user clicks for each session presented in the test set. Then the first click and the last click are identified from the simulated clicks. The simulated click positions are compared to the ground truth to compute the mean absolute error (MAE), which is the average difference between the predict-ed and true positions of clicks. For the models without dis-tance information in examination assumption, such as POS and LOG-r, clicks are simulated for each ranking position independently because the click probability of a document is independent from the previous clicks. But for the click models with distance information in the assumption, such a s the ideal value for perplexity is 1, the improvement of perplexity p 1 over p 0 is calculated as: improvement=( p p ) / ( p 0 1) 100% a s UBM and LOG-rd, the click probability of a document is dependent on the previous clicks, so we have to simulate the clicks in order from top to bottom. In such case, if a click is wrongly predicted at a certain position, the following pre-dictions will be affected, which will cause more uncertainty in the prediction result. During the simulation, there are chances that a session may have no simulated clicks. We only keep the sessions with at least one simulated click for evaluation.

Figure 4 shows the prediction errors of the first clicked position and the last clicked position. The cascade model only predicts one click each session so it is evaluated by the first click prediction only. According to the results, predict-ing the last clicked position turns out to be a more difficult task than predicting the first clicked position, indicated by the larger error bars in (b) than that in (a). In both (a) and (b), we also observe that the errors are smaller for the mod-els with user preferences, and this result is consistent for all the models. The trend that all click models get improved is consistent with the perplexity result as well. Besides, the performance gain ratio of the models is also similar to that in the perplexity result, i.e. LOG-RD has the largest perfor-mance gain, followed by DIL and LOG-r. And the variance of error among different models is also reduced. For the first clicked position, the variance of error is reduced from 0.019 to 0.006; and for the last clicked position, the vari-ance of error is reduced from 0.015 to 0.001. In other words, the performance difference of the click models in this click prediction task is smoothed after incorporating user prefer-ences.

On the other side, the performance ranking of the click models in this task is different from the ranking in perplexity. POS and LOG-r are among the worst by perplexity but have relatively low errors in click prediction. This inconsistency between the two evaluation methods simply indicates that different measures may lead to different evaluation results. However, it does not change the fact that all the reported models benefit from incorporating user preferences on both evaluation measures.

With the simulated clicks, we also draw the distribution of the first clicked position and the last clicked position in Figure 5. We use the top two models that have the largest performance gain for demonstration, along with the empiri-cal ground truth. We observe that neither LOG-rd nor DIL matches the empirical distribution very well. But after in-corporating user preferences, LOG-rd-user and DIL-user are significantly better matching the real distribution. For the click models that are not shown in Figure 5, small but not very significant improvements can be observed as well. This result further verifies the effectiveness of user preferences in improving a click model.
The idea of user preference was initially motivated by our observations in the eye-tracking study and click log analy-sis. Our experiment results showed that the user preferences were consistently helpful to improve the click model perfor-mance on multiple evaluation metrics. A good explanation is that the model assumptions with user preferences can better reflect the real situation.

The two evaluation measures used in this paper, i.e. the perplexity and the error of the predicted click position, pro-duced different performance ranking of the models. For ex-ample, LOG-rd-user was the best performing model in per-plexity but failed to lead in the click prediction task. There-fore, it seems tricky which evaluation measure one chooses when comparing two click models. However, this problem is not the main focus of this paper because our purpose is not to build a model that is better than all the other models on all evaluation metrics. Instead, we aim to provide a gen-eral approach that can be applied to most click models in order to consistently improve their performance on different metrics. For a variety of click models, we have shown consis-tent improvement by incorporating user preferences, using two different evaluation methods. This result supports our claim that the user preferences proposed in this paper are helpful in improving the click models. while LOG-rd and DIL have relatively large gaps with the ground truth.
We also note that the significance of the improvement d-iffers across model. One possible reason might be the way how user preferences are integrated. The logistic function based models gained significant improvement from the inte-gration of user preferences, while the user browsing model and the position model gained less improvements. It may indicate that the integration method we used for the proba-bilistic models needs to be further optimized. Since we have verified the effectiveness of incorporating user preferences, optimizing a specific click model would be an interesting direction for future work.

It is interesting to find that the dilution model (DIL), which has only three examination parameters in total, ob-tained comparable performance with the other models. With respect to perplexity, the dilution model is the second best (next to the user browsing model) among the models with-out user preferences. It beats the models that have more examination parameters (POS and LOG-r both have ten examination parameters) and tied with LOG-rd which has 55 examination parameters. With user preferences incorpo-rated, it maintains the good performance. Having the most user-specific parameters in total, the dilution model did not suffer from over-fitting problems in our experiment. The reason is that the total number of user parameters are still much less than the number of relevance parameters. We also filtered out users with few query sessions. In a real-world application scenario, we usually cannot expect to have suf-ficient data for all users. In such case, the model can be further improved by using a hybrid mechanism: use global parameters for the users without sufficient data and learn personal parameters when enough data is collected.
Search users have considerable differences in search be-havior. However, existing click models do not take this di-versity into consideration and they usually assume that all users have the same examination and click preferences. In-tuitively, this does not seem to be the case. In this paper, we carried out analyses on real click-through data, which confirmed that users have different click preferences. With an eye-tracking experiment with 21 human subjects, we also observed that the users X  examination behavior differs a lot. Motivated by these observations, we proposed two user pref-erence factors, namely the examination preference and the click preference, and incorporated them into multiple click models. Our proposed factors can be easily adopted to a variety of click models. In the experiments, we showed that b y incorporating the proposed user preferences, the perfor-mance of the existing models and our proposed model are consistently improved on multiple evaluation measures. The series of experiments confirmed that the two additional user-dependent elements can capture some common factors un-derlying the examination and click behavior of search users, which is thus a step further toward a better understanding of user behavior facing a search result. In this paper, we on-ly took into account two user preference factors. There may be more such user-dependent factors that affect the click and examination behavior of users. We will consider more factors in our future work.
This work was supported by Natural Science Foundation (61073071), National High Technology Research and Devel-opment (863) Program (2011AA01A205) of China. Part of the work has been done at the Tsinghua-NUS NExT Search Centre, which is supported by the Singapore National Re-search Foundation &amp; Interactive Digital Media R&amp;D Pro-gram Office, MDA under research grant (WBS:R-252-300-001-490). [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] S. Bhavnani. Domain-specific search strategies for the [3] S. Bochkanov and V. Bystritsky. Alglib [4] O. Chapelle and Y. Zhang. A dynamic bayesian [5] H. Cheng and E. Cant  X u-Paz. Personalized click [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] G. Duggan and S. Payne. Knowledge in the head and [8] G. Dupret, V. Murdock, and B. Piwowarski. Web [9] G. Dupret and B. Piwowarski. A user browsing model [10] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, [11] F. Guo, C. Liu, and Y. Wang. Efficient multiple-click [12] B. Hu, Y. Zhang, W. Chen, G. Wang, and Q. Yang. [13] T. Joachims. Optimizing search engines using [14] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [15] S. Kullback and R. Leibler. On information and [16] F. Radlinski and T. Joachims. Active exploration for [17] M. Richardson, E. Dominowska, and R. Ragno.
 [18] S. Shen, B. Hu, W. Chen, and Q. Yang. Personalized [19] C. Wang, Y. Liu, M. Zhang, S. Ma, M. Zheng, [20] R. White and D. Morris. Investigating the querying [21] R. W. White and S. M. Drucker. Investigating [22] R. W. White, S. T. Dumais, and J. Teevan.
 [23] D. Xu, Y. Liu, M. Zhang, S. Ma. Incorporating [24] X. Zhang, H. Anghelescu, and X. Yuan. Domain
