 Caching is an important optimization in search engine architec-tures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random ac-cess latency is comparable to its sequential access latency. There-fore, the use of SSDs to replace HDDs in a search engine infrastruc-ture may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The re-sults give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.
 H.3.3 [ Information Search and Retrieval ]: Search Process Search Engine, Solid State Drive, Cache, Query Processing
Caching is an important optimization in search engine architec-tures. Over the years, many caching techniques have been devel-oped and used in search engines [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]. The primary goal of caching is to improve query latency (reduce the query response time). To that end, search engines commonly dedicate portions of servers X  memory to cache certain query results Figure 1: Read latency on two HDDs and two SSDs (for com-mercial reasons, we do not disclose their brands). Each read fetches 4KB. The OS buffer is by-passed. [1, 5, 7], posting lists [2, 3, 6], documents [4] and snippets [12] in order to avoid excessive disk access and computation.

Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium. SSD offers a number of benefits over magnetic hard disk drive (HDD). For example, random reads in SSD are one to two orders of magnitude faster than in HDD [13, 14, 15, 16]. In addition, SSD is much more energy efficient than HDD (around 2.5% of HDD energy consumption [17, 18]). Now, SSD is getting cheaper and cheaper (e.g., it dropped from $40/GB in 2007 to $1/GB in 2012 [19]). Therefore, SSD has been em-ployed in many industrial settings including MySpace [20], Face-book [21], and Microsoft Azure [22]. Baidu, the largest search engine in China, has used SSD to completely replace HDD as its main storage since 2011 [23].

The growing trend of using SSD to replace HDD has raised an interesting research question specific to our community:  X  what is the impact of SSD on the cache management of a search engine ? X  Figure 1 shows the average cost (latency) of a random read and a sequential read on two brands of SSD and two brands of HDD. It shows that the cost of a random read is 100 to 130 times of a se-quential read in HDD. Due to the wide speed gap between random read and sequential read in HDD, the benefit of a cache hit, in tra-ditional, has been largely attributed to the saving of the expensive random read operations. In other words, although a cache hit of a data item could save the random read of seeking the item and the subsequent sequential reads of the data when the item spans more than one block, the saving of those sequential reads has been tra-ditionally regarded as less noticeable  X  because the random seek operation usually dominates the data retrieval time.

Since a random read is much more expensive than a sequential read in HDD, most traditional caching techniques have been de-signed to minimize random reads. The technology landscape, how-ever, has changed if SSD is used to replace HDD. Specifically, Fig-ure 1 shows that the cost of a random read is only about two times of a sequential read in SSD. As such, in an SSD-based search en-gine infrastructure, the benefit of a cache hit should now attribute to both the saving of the random read and the saving of the subse-quent sequential reads for data items that are larger than one block. Furthermore, since both random reads and sequential reads are fast on SSD while query processing in modern search engines involves several CPU-intensive steps (e.g., query result ranking [24, 25] and snippet generations [26, 27]), we expect that SSD would yield the following impacts on the cache management of search engines: 1. Caching techniques should now target to minimize both ran-2. The size of the data item and the CPU cost of the other com-
In this paper, we carry out a large-scale experimental study to evaluate the impact of SSD on the effectiveness of various caching policies, on all types of cache found in a typical search engine ar-chitecture. We note that the traditional metric cache hit ratio for evaluating caching effectiveness is inadequate in this study  X  in the past the effectiveness of a caching policy can be measured by the cache hit ratio because it is a reliable reflection of the actual query latency: a cache hit can save the retrieval of a data item from disk, and the latency improvement is roughly the same for a large data item and a small data item because both require one ran-dom read, which dominates the time of retrieving the whole data item from disk. With SSD replacing HDD, the cache hit ratio is no longer a reliable reflection of the actual query latency because a larger data item being found in the cache yields a higher query latency improvement over a smaller data item (a cache hit for a larger item can save a number of time-significant sequential reads). In our study, therefore, one caching policy may be more effective than another even though they achieve the same cache hit ratio if one generally caches some larger data items. To complement the inadequacy of cache hit ratio as the metric, our study is based on real replays of a million of queries on an SSD-enabled search en-gine architecture and our findings are reported based on actual query latency .

To the best of our knowledge, this is the first study to evaluate the impact of SSD on search engine cache management. The ex-perimental results here can bring the message  X  it is time to rethink about your caching management  X  to practitioners who have used or are planning to use SSD to replace HDD in their infrastructures. Furthermore, the results can give the research community insights into the redesign of caching policies for SSD-based search engine infrastructures.

The rest of this paper is organized as follows. Section 2 provides an overview of contemporary search engine architecture and the various types of cache involved. Section 3 presents the set of eval-uation experiments carried out and the evaluation results. Section 4 discusses some related studies of this paper. Section 5 summarizes the main findings of this study.
In this section, we first give an overview of the architecture of the state-of-the-art Web search engines and the cache types involved (Section 2.1). Then we give a review of the various types of caching policies used in Web search engine cache management (Section 2.2).
The architecture of a typical large-scale search engine [28, 29, 30] is shown in Figure 2. A search engine is typically composed by three sets of servers: Web Servers (WS), Index Servers (IS), and Document Servers (DS).
 Web Servers The web servers are the front-end for interacting with end users and coordinating the whole process of query evaluation. Upon receiving a user X  X  query q with n terms t 1 ,t 2 , ..., t 1 ], the web server that is in charge of q checks whether q is in its in-memory Query Result Cache (QRC) [1, 11, 31, 3, 5, 7, 9, 10]. The QRC maintains query results of some past queries. If the results of q are found in the QRC (i.e., a cache hit), the server returns the cached results of q to the user directly. Generally, query results are roughly the same size and a query result consists of (i) the title, (ii) the URL, and (iii) the snippet [26, 27] (i.e., an extract of the document with terms in q being highlighted) of the top-k ranked results related to q (where k is a system parameter, e.g., 10 [31, 32, 12, 10]). If the result of q is not found in the QRC (i.e., a cache miss), the query is forwarded to an index server [ STEP Index Servers The index servers are responsible for the compu-tation of the top-k ranked result related to a query q . An index server works by: [ STEP IS 1] retrieving the corresponding posting list PL i =[ d 1 ,d 2 , ... ] of each term t i in q ,[ STEP IS all the retrieved posting lists PL 1 ,PL 2 , ..., PL n to obtain the list of document identifiers (ids) that contain all terms in q , and [ 3] ranking the documents for q according to a ranking model. After that, the index server sends the ordered list of document ids d ,d 2 , ..., d k of the top-k most relevant documents of query q back to the web server [ STEP 3 ]. In an index server, an in-memory Post-ing List Cache (PLC) [11, 3, 33] is employed to cache the posting lists of some terms. Upon receiving a query q ( t 1 ,t 2 , ..., t web server, an index server skips STEP IS 1 if a posting list is found in the PLC.
 Document Servers Upon receiving the ordered list of document ids d 1 ,d 2 , ..., d k from the index server, the web server forwards the list and the query q to a document server for further processing [
STEP 4 ]. The document server is responsible for the generating the final result. The final result is a Web page that includes the title, URL, and a snippet for each of the top-k documents. The snippet s i of a document d i is query-specific  X  it is the portion of a document which can best match the terms in q . The generation process is as follows: [ STEP DS 1] First, the original documents that the list of document ids referred to are retrieved. [ STEP DS the snippet of each document for query q is generated. There are two levels of caches in the document servers: Snippet Cache (SC) [12] and Document Cache (DC) [4]. The in-memory Snippet Cache (SC) stores some snippets that have been previously generated for some query-document pairs. If a particular query-document pair is found in the SC, STEP DS 1 and STEP DS 2 for that pair can be skipped. The in-memory Document Cache (DC) stores some docu-ments that have been previously retrieved. If a particular requested document is in the DC, STEP DS 1 can be skipped. As the output, the document server returns the final result (in the form of a Web page with a ranked list of snippets of the k most relevant docu-ments of query q ) to the web server [ STEP 5 ] and the web server may cache the result in the QRC and then pass the result back to the end user [ STEP 6 ].
Caching is a widely-used optimization technique to improve sys-tem performance [1, 34, 35, 36]. Web search engines use caches in different types of servers to reduce the query response time [11]. There are two types of caching policies being used in search en-gines: (1) Dynamic Caching and (2) Static Caching . Dynamic caching is the classic. If the cache memory is full, dynamic caching follows a replacement policy (a.k.a. eviction policy )to evict some items in order to admit the new items [37, 38, 39]. Static caching is less common but does exist in search engines [1, 5, 3, 6, 8, 9, 10]. Initially when the cache is empty, a static cache follows an admis-sion policy to select data items to fill the cache. Once the cache has been filled, it does not admit any new items at run-time. The same cache content continuously serves the requests and its entire con-tent is refreshed in a periodic manner [1]. Static caching can avoid the situations of having long-lasting popular items being evicted by the admission of many momentarily popular items as in dynamic caching. Query Result Cache (QRC) is the cache used in the web servers. It caches the query results such that the whole stack of query pro-cessing can be entirely skipped if the result of query q is found in the QRC. Both dynamic and static query result caches exist in the literature.
 Dynamic Query Result Cache Markatos [1] was the first to dis-cuss about the use of dynamic query result cache (D-QRC) in search engines. By analyzing the query logs of Excite search engine, the authors observed a significant temporal locality in the queries. Therefore, they proposed the use of query result cache. Two classic replacement policies were used there: least-recently-used (LRU) and the least-frequently-used (LFU). In this paper, we refer them as D-QRC-LRU and D-QRC-LFU, respectively.

Recently, Gan and Suel [7] have developed a feature-based re-placement policy for D-QRC. In this paper, we refer this policy as D-QRC-FB. That policy applies machine learning techniques of-fline to learn the values of a set of query features from the query log. The cache replacement is then tuned using those values. Static Query Result Cache Markatos [1] also discussed about the potential use of static caching in search engines. In that work, a static query result cache with an admission policy that analyzes the query logs and fills the cache with the most frequently posed queries was proposed. In this paper, we refer this policy as S-QRC-Freq.
 Ozcan et al. [5] reported that some query results cached by S-QRC-Freq policy are not useful because there is a significant number of frequent queries quickly lose their popularities but still reside in the static cache before the next periodic cache refresh. Consequently, they proposed another admission policy that selects queries with high frequency stability . Queries with high frequency stability are those frequent queries and the logs show that they re-main frequent over a time period. In this paper, we refer to this as S-QRC-FreqStab. Later on, Ismail et al. [8, 9, 10] developed a cost-aware replacement policy for S-QRC. In addition to traditional factors such as the popularity of terms, it also takes into account the latency of a query, as different queries may have different process-ing time. Nonetheless, its policy has not considered the cost of other expensive steps (e.g., snippet generation) in the whole query processing stack. In this paper, we refer that as S-QRC-CA. Remark: According to [7], D-QRC-FB has the highest effective-ness among all D-QRC policies. Nonetheless, it requires a lot of empirical tuning on the various parameters used by the machine learning techniques. According to [8, 9, 10], S-QRC-CA is the most stable and effective policy in static query result cache. D-QRC and S-QRC can co-exist and share the main memory of a web server. In [31], the empirically suggested ratios for D-QRC to S-QRC are 6:4, 2:8, and 7:3 for Tiscali, AltaVista, and Excite data, respectively. Posting List Cache (PLC) is the cache used in the index servers. It caches some posting lists in the memory so that the disk read of a posting list PL of a term t in query q can possibly be skipped. Both dynamic and static posting list caches exist in the literature. Dynamic Posting List Cache Saraiva et al. [11] were the first to discuss the effectiveness of dynamic posting list caching in in-dex servers. In that work, the simple LRU policy was used. In this paper, we refer this as D-PLC-LRU. In [3, 6], the authors also evaluated another common used policy, LFU, in dynamic PLC, in which we refer this as D-PLC-LFU in this paper. In addition, the authors observed that a popular term tends to have a longer post-ing list. Therefore, to balance the tradeoff between term popularity and the effective use of the cache (to cache more items), the au-thors developed a replacement policy that favors terms with a high frequency to posting list length ratio. In this paper, we refer this policy as D-PLC-FreqSize.
 Static Posting List Cache Static posting list caching was first studied in [2] and the admission policy was based on selecting post-ing lists with high access frequency. In this paper, we refer to this as S-PLC-Freq. In [3, 6], the static cache version of D-PLC-FreqSize was also discussed. We refer that as S-PLC-FreqSize here. Remark: According to [3, 6], S-PLC-FreqSize is the winner over all static and dynamic posting list caching policies.
Two types of caches could be used in the document servers: Doc-ument Cache (DC) and Snippet Cache (SC). These caches store some documents and snippets in the memory of the document servers. So far, only dynamic document cache and dynamic snippet cache have been discussed and there is no corresponding static caching technique yet.
 Document Cache In [4], the authors observed that the time of reading a document from disk dominates the snippet generation process. So, they proposed to cache some documents in the docu-ment servers X  X  memory so as to reduce the snippet generation time. In that work, the simple LRU replacement policy was used and here we refer it as DC-LRU. According to [4], document caching is able to significantly reduce the time for generating snippets. Snippet Cache In [12], the authors pointed out that the snippet generation process is very expensive in terms of both CPU compu-tation and disk access in the document servers. Therefore, the au-thors proposed to cache some generated snippets in the document server X  X  main memory. In this paper, we refer this as SC-LRU be-cause it uses the LRU policy as replacement policy.
In this study, we use the typical search engine architecture (Fig-ure 2) to evaluate the impact of SSD on the cache management of the index servers (Section 3.1), document servers (Section 3.2), and the web servers (Section 3.3). We focus on a pure SSD-based ar-chitecture like Baidu [23]. Many studies predict that SSD will soon completely replace HDD in all layers [40, 41, 42]. The study of us-ing SSD as a layer between main memory and HDD in a search engine architecture before SSD completely replaces HDD has been studied elsewhere in [43].
 Experimental Setting We deploy a Lucene 1 based search engine implementation on a sandbox infrastructure consisting of one in-http://lucene.apache.org dex server, one document server, and one web server. All servers are Intel commodity machines (2.5GHz CPU, 8GB RAM), with Windows 7 installed. We have carried out our experiments on two SSDs and two HDDs (evaluated in Figure 1). The experimental results are largely similar and thus we only present the results of using SSD1 and HDD1. In the experiments, the OS buffer is dis-abled.

We use a real collection of 5.3 million Web pages 2 crawled in the middle of 2008, by Sogou ( sogou.com ), a famous commer-cial search engine in China. The entire data set takes over 5TB. To accommodate our experimental setup, we draw a random sample of 100GB data. It is a reasonable setting because large-scale Web search engines shard their indexes and data across clusters of ma-chines [23]. As a reference, the sampled data sets in some recent works are 15GB [6], 37GB in [10], and 2.78 million Web pages in [3]. Table 1 shows the characteristics of our sampled data.
As mentioned in the introduction, the cache hit ratio alone is in-adequate to evaluate the effectiveness of caching techniques. So, the experiments are done by a replay of real queries found in So-gou search engine in June 2008 3 , and the average end-to-end query latency is reported in tandem with cache hit ratio. The log contains 51.5 million queries. To ensure the replay can be done in a manage-able time, we draw a random sample of 1 million queries for our experiments (as a reference, 0.7 million queries were used in [10]). The query frequency and term frequency of our sampled query set follow power-law distribution with skew-factor  X  =0 . 85 and respectively (see Figure 3). As a reference, the query frequencies in some related work like [11], [7], and [6] follow power-law dis-tribution with  X  equals 0.59, 0.82, and 0.83, respectively; and the term frequencies in some recent work like [6] follow power-law distribution with  X  =1 . 06 . (a) Query frequency distribution (b) Term frequency distribution
In addition, our sampled query set also exhibits the property that the temporal locality of terms is higher than that of queries [11, 3, 6] by observing that the skew-factor  X  of the query frequency is lower than that of the term frequency. Table 2 shows the characteristics of our query set.

We divide the real query log into two parts: 50% of queries are used for warming the cache and the other 50% are for the replay, following the usual settings [1, 8, 10, 32, 9]. http://www.sogou.com/labs/dl/t-e.html http://www.sogou.com/labs/dl/q-e.html
In the experiments, we follow some standard settings found in recent work. Specifically, we follow [31, 32, 12, 10] to retrieve the top-10 most relevant documents. We follow [11] to configure the snippet generator to generate snippets with at most 250 char-acters. Posting list compression is enabled. To improve the exper-imental repeatability, we use the standard variable-byte compres-sion method [44] in the experiments. The page (block) size in the system is 4KB [45, 46, 47]. When evaluating the caching policy on one type of cache, we disable all other caches in the sandbox. For example, when evaluating the query result cache on the web server, the posting list cache, snippet cache, and document cache are all disabled.
We first evaluate the impact of SSD on the posting list cache management in an index server. As mentioned, on SSD, a long posting list being found in the cache should have a larger query latency improvement than a short posting list being found because (i) a cache hit can save more sequential read accesses if the list is a long one and (ii) the cost of a sequential read is now comparable to the cost of a random read (see Figure 1).

To verify our claim, Figure 4 shows the access latency of fetching from disk the posting lists of terms found in our query log. We see that the latency of reading a list from HDD increases mildly with the list length because the random seek operation dominates the access time. In contrast, we see the latency of reading a list from SSD increases with the list length at a faster rate. Figure 4: Read access latency of posting lists of varying lengths
Based on that observation, we believe that the effectiveness of some existing posting list caching policies would change when they are applied to an SSD-based search engine infrastructure. For ex-ample, according to [3, 6], S-PLC-FreqSize has the best caching effectiveness on HDD-based search engines because it favors pop-ular terms with short posting lists (i.e., a high frequency to length ratio) for the purpose of caching more popular terms. However, on SSD, a short list being in the cache has a smaller query latency improvement than a long list. As such, we believe that design principle is void in an SSD-based search engine infrastructure.
To verify our claim, we carried out experiments to re-evaluate the static and dynamic posting list caching policies on our SSD sandbox. In the evaluation, we focus on the effectiveness of indi-vidual caching policy type. The empirical evaluation of the optimal ratio between static cache and dynamic cache on SSD-based search engine architecture is beyond the scope of this paper. Figure 5: [Index Server] Effectiveness of static posting list caching policies Reevaluation of static posting list caching policy on SSD We begin with presenting the evaluation results of the two existing static query result caching policies, (1) S-PLC-Freq and (2) S-PLC-FreqSize, mentioned in Section 2.2.2. Figure 5 shows the cache hit ratio and the average query latency of S-PLC-Freq and S-PLC-FreqSize under different cache memory sizes.

Echoing the result in [3, 6], Figure 5(a) shows that S-PLC-FreqSize, which tends to cache popular terms with short posting lists, has a higher cache hit ratio than S-PLC-Freq. The two policies have the same 98% cache hit ratio when the cache size is 4GB because the cache is large enough to accommodate all the posting lists of the query terms (3.89GB; Table 2) in the cache warming phase. The 2% cache miss is attributed to the difference between the terms found in the training queries and the terms found in the replay queries.

Although having a higher cache hit ratio, Figure 5(b) shows that the average query latency of S-PLC-FreqSize is actually longer than S-PLC-Freq in an SSD-based search engine architecture. As the caching policy S-PLC-FreqSize tends to cache terms with short posting lists, the benefit brought by the higher cache hit ratio is wa-tered down by the fewer sequential read savings caused by short posting lists. This explains why S-PLC-FreqSize becomes poor in terms of query latency. Apart from the above, the experimental re-sults above are real examples that illustrate cache hit ratio is not a reliable metric for caching management in SSD-based search en-gine architectures.

Figure 5(c) shows the average query latency on HDD. We see a surprising result: even on HDD cache hit ratio is not always re-liable! Specifically, we see that S-PLC-FreqSize X  X  average query latency is slightly worse than S-PLC-Freq at 512MB cache mem-ory even though the former has a much higher cache hit ratio than the latter. To explain, Figure 6 shows the average size of the post-ing lists participated in all cache hits (i.e., whenever there is a hit in the PLC, we record its size and report the average). Figure 6: Average list size (in blocks) of all hits in static posting list cache We see that when the cache memory is small (512MB), S-PLC-Freq keeps the longest (most frequent) posting lists in the cache. In contrast, S-PLC-FreqSize consistently keeps short lists in the cache. At 512MB cache memory, a cache hit under S-PLC-Freq policy can save 470  X  250 = 220 more sequential reads than S-PLC-FreqSize. Even though sequential reads are cheap on HDD, a total of 220 sequential reads are actually as expensive as two ran-dom seeks (Figure 1). In other words, although Figure 5(a) shows that the cache hit ratio of S-PLC-Freq is 20% lower than S-PLC-FreqSize at 512MB cache, that is outweighed by the two extra ran-dom seeks saving between the two policies. That explains why S-PLC-Freq slightly outperforms S-PLC-FreqSize at 512MB cache. Of course, when the cache memory increases, S-PLC-Freq starts to admit more short lists into the cache memory, which reduces its benefit per cache hit, and that causes S-PLC-FreqSize to outper-form S-PLC-Freq again through the better cache hit ratio. Reevaluation of dynamic posting list caching policy on SSD We next present the evaluation results of the three existing dynamic query result caching policies, (1) D-PLC-LRU, (2) D-PLC-LFU, and (3) D-PLC-FreqSize, mentioned in Section 2.2.2. Figure 7 shows the cache hit ratio and the average query latency of the three policies under different cache memory sizes. Figure 7: [Index Server] Effectiveness of dynamic posting list cache policies
First, we also see that while D-PLC-FreqSize has a better cache hit ratio than D-PLC-LFU (Figure 7(a)), its query latency is actu-ally longer than D-PLC-LFU (Figure 7(b)) in SSD-based architec-ture. This further supports that the claim of favoring terms with high frequency over length ratio no longer sustains in SSD-based search engine architecture. Also, this gives yet another example of the fact that cache hit ratio is not a reliable measure in SSD caching management.

Second, comparing D-PLC-LRU and D-PLC-LFU, we see that while D-PLC-LFU has a poorer cache hit ratio than D-PLC-LRU, their query latencies are quite close in SSD-based architecture. As is shown in [3, 6], the posting list length generally increases with the term frequency, therefore D-PLC-LFU, which favors terms with high frequency, also favors terms with long posting lists. As men-tioned, on SSD, the benefit of finding a term with a longer list in cache is higher than that of finding a term with shorter list. This ex-plains why D-PLC-LFU has a query latency close to D-PLC-LRU, which has a higher cache hit ratio.

Figure 7(c) shows the average query latency on HDD. First, we once again see that cache hit ratio is not reliable even on HDD-based architecture. For example, while D-PLC-FreqSize has a higher cache hit ratio than D-PLC-LFU (except when the cache is large enough to hold all posting lists), their query latencies are quite close to each other in an HDD-based architecture. That is due to the same reason that we explained in static caching  X  Figure 8 shows that D-PLC-LFU can save hundreds of sequential reads more than D-PLC-FreqSize per cache hit when the cache memory is less than 1GB. That outweighs the cache hit ratio difference between the two. In contrast, while D-PLC-LRU has a better cache hit ratio than D-PLC-LFU, they follow the tradition that the former outper-forms the latter in latency because Figure 8 shows the size differ-ence of the posting lists that participated in the cache hits (i.e., the benefit gap in terms of query latency) between D-PLC-LRU and D-PLC-LFU is not as significant as the time of one random seek operation. Therefore, D-PLC-LRU yields a shorter query latency than D-PLC-LFU based on its higher cache hit ratio. Figure 8: Average list size (in blocks) of all hits in dynamic posting list cache
We next evaluate the impact of SSD on the cache management of document servers. A document server is responsible for stor-ing part of the whole document collection and generating the final query result. It receives a query q and an ordered list of docu-ment ids { d 1 ,d 2 ,...,d k } of the k most relevant documents of q from a web server, retrieves the corresponding documents from the disk, and generates the query-specific snippet for each document and consolidates them as a result page. In the process, k query-specific snippets have to be generated. If a query-specific snippet q,d i is found in the snippet cache, the retrieval of d i disk and the generation of that snippet are skipped. If a query-specific snippet is not found in the snippet cache, but the document d is in found the document cache, the retrieval of d i from the disk is skipped. In our Web data (Table 1), a document is about 8KB on average. With a 4KB page size, retrieving a document from the disk thus requires one random seek (read) and a few more se-quential reads. In traditional search engine architecture using HDD in the document servers, the latency from receiving the query and document list from the web server to the return of the query result is dominated by the k random read operations that seek the k doc-uments from the HDD (see Figure 9(a)). These motivated the use of document cache to improve the latency. As the random reads are much faster on SSD, we now believe that the time bottleneck in the document servers will shift from document retrieval (disk Figure 9: Document retrieval time vs. snippet generation in a document server access) to snippet generation (CPU computation). More specif-ically, the snippet generation process that finds every occurrence of q in d i and identifies the best text synopsis [26, 27] according to a specific ranking function [27] is indeed CPU-intensive. Fig-ure 9(b) shows that the CPU time spent on snippet generation be-comes two times the document retrieval time if SSD is used in a document server. Therefore, contrary to the significant time reduc-tion brought by document cache in traditional HDD-based search engine architectures [26], we believe the importance of document cache in SSD-based search engine architectures is largely dimin-ished. In contrast, we believe the snippet cache is far more power-ful in an SSD-based search engine architecture for two reasons: (1) a cache hit in a snippet cache can reduce both the time of snippet generation and document retrieval; and (2) the memory footprint of a snippet (e.g., 250 characters) is much smaller than the memory footprint of a document (e.g., an average 8KB in our Web data). That implies a double-benefit of the use of snippet cache over doc-ument cache: the same amount of cache memory in a web server can cache many more snippets than documents, and the benefit of a snippet cache hit is much more significant than the benefit of a document cache hit.

To verify our claims, we carried out experiments to vary the memory ratio between the snippet cache and the document cache in our SSD-based sandbox infrastructure. The caching policies in document cache and snippet cache are DC-LRU [4] and SC-LRU [12], respectively. Figure 10 shows the query latency and the hit ratios of the snippet cache and document cache when varying the memory allocation ratio between snippet cache and document cache of 2GB cache memory. Figure 10(a) shows the cache hit ratio of snippet cache increases when we allocate more memory to the snippet cache, but when we continue to allocate more than 2GB  X  40% = 800MB of memory to the snippet cache, the snippet cache hit ratio stays flat because the memory footprint of a snip-pet is so small that all snippets generated in the warming phase can be resided in the cache. When we go on allocating more memory to the snippet cache even after all snippets could be cached in the memory, it only gives less memory to the document cache. There-fore, the document cache hit ratio in Figure 10(a) continues to drop even when the snippet cache hit ratio has plateaued out. Figure 10(b) shows that when we allocate more memory to the snippet cache (i.e., the document cache has less memory allocated), the query latency drops monotonically before all snippets are in the cache. Furthermore, after all snippets are in the cache and we allo-cate less memory to the document cache, the query latency almost stays flat. The results indicate that snippet caching is far more ef-Figure 10: [Document Server] Effectiveness of snippet cache and document cache on SSD fective than document caching. The observations above persist for cache memory of size 512MB, 1GB, and 4GB (figures are omitted for space reasons).
We believe the use of SSD has no impact on the cache manage-ment in the web servers because the storage media does not play a major role in web servers (see Figure 2). Therefore, we believe that the use of SSD has no impact on the effectiveness of the caching policies either. Figure 11 shows the cache hit ratio and query la-tency of four selected query result caching policies: (1) D-QRC-LRU, (2) D-QRC-LFU, and (3) S-QRC-Freq, (4) S-QRC-CA. We see that the tradition holds here: when one policy has a higher cache hit ratio than the others, its query latency is also shorter than the others. 4
SSD is expected to gradually replace HDD as the primary per-manent storage media in both consumer computing and enterprise computing [41, 42, 14, 47, 40] because of its outstanding perfor-mance, small energy footprint, and increasing capacity. This has led to a number of studies that aim to better understand the impacts of SSD on different computer systems. Figure 11: [Web Server] Effectiveness of query result caching policies on SSD and HDD
The discussion of the impacts of SSD on computer systems had started as early as in 1995 [48], in which an SSD-aware file system was proposed. Later on, more SSD-aware file systems have been designed, e.g., JFFS [49], YAFFS [50]. After that, the discussion has extended to other components in computer systems, e.g., virtual machine [51, 52], buffer manager [53, 54], I/O scheduler [55], and RAID [56, 57].

The discussion of the impacts of SSD on database systems, our sister field, had started as early as in 2007 [58]. Since then, SSD has become an active topic in database research. The discussions cover the impacts of SSD on database architecture [58, 14, 59], query processing [47, 60], index structures [61, 62, 63] and algorithms [45], data placement and migration [64, 65], transaction manage-ment [14, 66, 67] and buffer management [68, 69]. Most concern about the asymmetric performance between slow random write and fast sequential write on SSD [70, 61, 62, 63, 58, 14, 59, 45, 64, 67], some concern about the asymmetric fast read and slow write [68, 14, 69, 45, 66], and some concern about the comparable perfor-mance between fast random reads and fast sequential reads [47, 60, 45] like what we do in this paper. The recent trend is to exploit the energy efficiency of SSD [71, 72] and its parallelism [73, 74] to fully leverage the power of SSD in large-scale data processing systems.

Baidu first announced their SSD-based search engine infrastruc-ture in 2010 [23], but they did not investigate the impact of SSD on their cache management. The issue of allocating indexes on SSD was recently studied in 2011 [75]. Last year, the issue of main-taining inverted index resided on SSD was also discussed [76]. In [43], the use of SSD as a layer between the RAM and HDD was discussed. Our work focuses on the possibly the next generation of search engine architecture in which SSD completely replaces HDD.
In this paper, we present the results of a large-scale experimental study that evaluates the impact of SSD on the effectiveness of vari-ous caching policies, on all types of cache found in a typical search engine architecture. This study contributes the following messages to our community: 1. Traditional cache hit ratio is no longer a reliable measure of 2. The previous known best caching policy in index servers, 3. While previous work claims that document caching is very 4. While SSD can improve the disk access latency of all servers
As future work, we will next focus on the new bottleneck of query processing in SSD-based web search engine architecture and consider the potential of other cache types (e.g., intersection cache [77]) in the study.
This work is partially supported by the Research Grants Council of Hong Kong (GRF PolyU 525009, 521012, 5302/12E), NSFC of China (60903028, 61070014), Key Projects in the Tianjin Science &amp; Technology Pillar Program (11ZCKFGX01100).
