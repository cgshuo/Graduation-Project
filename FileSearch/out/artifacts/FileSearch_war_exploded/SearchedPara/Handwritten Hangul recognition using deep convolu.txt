 ORIGINAL PAPER In-Jung Kim  X  Xiaohui Xie Abstract In spite of the advances in recognition technol-ogy, handwritten Hangul recognition (HHR) remains largely unsolved due to the presence of many confusing characters and excessive cursiveness in Hangul handwritings. Even the best existing recognizers do not lead to satisfactory perfor-mance for practical applications and have much lower per-formance than those developed for Chinese or alphanumeric characters. To improve the performance of HHR, here we developed a new type of recognizers based on deep neural networks (DNNs). DNN has recently shown excellent per-formance in many pattern recognition and machine learning problems, but have not been attempted for HHR. We built our Hangul recognizers based on deep convolutional neural net-works and proposed several novel techniques to improve the performance and training speed of the networks. We system-atically evaluated the performance of our recognizers on two public Hangul image databases, SERI95a and PE92. Using our framework, we achieved a recognition rate of 95.96% on SERI95a and 92.92% on PE92. Compared with the previous best records of 93.71% on SERI95a and 87.70% on PE92, our results yielded improvements of 2.25 and 5.22%, respec-tively. These improvements lead to error reduction rates of 35.71% on SERI95a and 42.44% on PE92, relative to the previous lowest error rates. Such improvement fills a signif-icant portion of the large gap between practical requirement and the actual performance of Hangul recognizers. Keywords Handwritten Hangul recognition  X  Character recognition  X  Deep convolutional neural network  X  Deep learning  X  Gradient-based learning 1 Introduction Character recognition technology has been developed for decades. For alphanumeric and Chinese characters, recog-nition technologies are mature enough to achieve high accu-racy. However, in handwritten Hangul recognition (HHR), none of existing recognizers are accurate enough for practical applications. The difficulty of handwritten Hangul recogni-tion is mainly caused by a multitude of confusing characters and excessive cursiveness in Hangul handwritings. Figure 1 shows some examples of Hangul characters that are very sim-ilar and are often confused by recognizers. Hangul contains a lot of such confusing characters. Moreover, shape variation in cursive handwritings makes it even harder to distinguish them. On the SERI95a and PE92 databases, two most pop-ular public Hangul image databases, the state-of-the-art per-formances in terms of recognition rates are merely 93.71 and 87.70%, respectively [ 1 , 2 ]. Such poor performances have discouraged the utilization of HHR in practical systems.
On the other hand, in recent years, deep neural networks (DNNs) have been highlighted in machine learning and pat-tern recognition fields. Composed of many layers, DNNs can model much more complicated functions than shallow net-works [ 3 ]. The availability of large-scale training data and advances in computing technologies have made the train-ing of such deep networks possible, leading to a widespread adoption of DNNs in many problem domains. For example, deep convolutional neural networks (DCNNs) have shown outstanding performances in many image recognition fields, beating benchmark performances by large margins [ 4  X  7 ]. However, although DNN has been employed for many pat-tern recognition systems, it has not been attempted for HHR. We reason that DNN could be especially beneficial for HHR for a number of reasons. First, DNN integrates feature extrac-tion and classification within a unified framework. Such an integrated structure can be advantageous in learning discrim-inative features necessary to distinguish confusing characters in Hangul handwritings, because features are systematically learned together with the classifier within the framework of error minimization, and such features are important in error minimization. Second, DNN is very good at extracting high-level features. In particular, the convolution and max-pooling layers used by DCNN are very effective in handling shape variations, which will likely be key in handling the exces-sivecursivenessinHangulwritings.Therefore,DCNNseems to be well posed to overcome the two main difficulties in HHR.

In this research, we built handwritten Hangul recognizers using DCNNs. Then, we improved the performance and the training speed of the recognizers by applying a few improve-ment techniques. Using the system we built, we achieved a recognition rate of 95.26% on SERI95a and 92.92% on PE92.Comparedwiththepreviousbestrecordsof93.71%on SERI95a and 87.70% on PE92, our results yielded improve-ments of 2.25 and 5.22%, respectively. These improvements lead to error reduction rates of 35.71% on SERI95a and 42.44% on PE92, relative to the previous lowest error rates. Such improvement fills a significant portion of the large gap between practical requirement and the actual performance of Hangul recognizers.

The rest of this paper is organized as follows: in Sect. 2 , we briefly review previous works on HHR and deep learn-ing. In Sects. 3 and 4 , we describe the DCNN-based Hangul recognizer and the training algorithm. In Sect. 5 ,wepro-pose several techniques to further improve performance and training speed. In Sect. 6 , we present experimental results on two popular HHR data sets. Conclusions are provided in Sect. 7 . 2 Related works 2.1 Handwritten Hangul recognition Character recognition methods can be generally grouped into two categories: structural and statistical. The structural method describes the input character as strokes or contour segments and identifies the class by matching with the struc-tural models of candidate classes. In contrast, the statistical method represents the character image as a feature vector and classifies the feature vector using statistical methodolo-gies. Between the two, the statistical method is more widely used in practice because it is easy to build and is effective in recognizing many character sets, including Chinese charac-ters. However, unlike handwritten Chinese character recog-nition (HCCR), structural methods outperformed statistical methods in HHR for a long time. We believe the reason is that the statistical methods used in early days were not sophisti-cated and therefore insufficient to deal with the multitude of confusing characters and the excessive cursiveness in Hangul handwritings.

Kim and Kim proposed a structural method based on the hierarchical random graph representation [ 8 ]. Given a char-acter image, they extracted strokes and represented them onto an attributed graph, which is matched with character models using a bottom-up matching algorithm. Kang and Kim improved [ 8 ] by modeling between stroke relation-ships [ 2 ]. They extended the hierarchical random graph in [ 8 ] by adding another type of nodes to represent relation-ships between strokes. Then, they matched those nodes with relationships among input strokes. Jang proposed a post-processing method for the structural recognizers in [ 8 ] and [ 2 ] to improve discrimination ability [ 9 ]. The post-processor consists of a set of pair-wise discriminators, each of which is specialized for a pair of graphemes with similar shapes. To build each pair-wise discriminator, they extracted parts that separate the character pair and then applied statisti-cal methods focusing on those parts. These systems were evaluated on two public handwritten Hangul image data-bases: SERI95a 1 and PE92 [ 10 , 11 ]. The best performances on SERI95a and PE92 achieved by the structural methods were 93.4% reported in [ 9 ] and 87.7% reported in [ 2 ], respectively.

In the early days of HHR, researchers attempted to use statistical methods to recognize handwritten Hangul [ 12  X  14 ]. However, the performances of those statistical methods were either much poorer than those of the structural methods, or could not be directly compared, because they were measured on small-size private data sets. As a result, statistical methods were not frequently used in HHR for a while.

On the other hand, statistical methods have become the mainstream approach in HCCR and have been improved sig-nificantly. Recently, Park et al. [ 1 ] applied state-of-the-art statistical methods to HHR and evaluated their performance. Combining nonlinear shape normalization, the gradient fea-ture extraction, and the modified quadratic discriminant func-tion (MQDF) classifier, they achieved much better results than the early statistical recognizers. Their best performances were 93.71% on SERI95a and 85.99% on PE92, which are comparable to the performances of the structural recogniz-ers. Especially, the recognition rate 93.71% on SERI95a is even higher than the best result of the structural method, 93.4%. Moreover, it is possible that the performance of sta-tistical methods can be further improved when more train-ing data become available [ 15 ]. However, at present, neither the structural method nor the statistical method can provide a performance level high enough for practical applications. Consequently, handwritten Hangul recognition remains an unsolved problem. 2.2 Deep neural networks For the past few years, DNN has produced outstanding results inmachinelearningandpatternrecognitionfields.Composed of many layers, DNN is much more efficient at representing highly varying nonlinear functions than shallow neural net-works [ 3 ]. In addition, DNNs enable integrated training of feature extractors and classifiers. Unlike conventional clas-sifiers, most DNNs accept raw images as input and do not require separate feature extraction or preprocessing, except for size normalization. The low-and middle-level DNN lay-ers extract and abstract features from the input image, while high-levellayersperformclassification.Assuch,aDNNinte-gratesfeatureextractionandclassificationwithinasinglenet-work, which can be systematically optimized with respect to a single objective function. Such integrated training can often lead to better performance than those based on the indepen-dent training of each module.

Despite their appealing properties in extracting and repre-senting features, training DNNs are, however, computation-ally challenging. Back propagation is the dominant algorithm used in training neural networks. In the back-propagation training, the error signals in the output layer of the net-work are propagated backward layer by layer from the out-put layer to the input layer to guide the update of connection weights. The back-propagation algorithm performs poorly when the number of hidden layers is large due to the so-called diminishing gradient problem X  X s the error signals propagate backward, they become smaller and smaller and eventually become too small to guide the update of weights in the lowest few layers. The diminishing gradient problem is a major obstacle in training of DNNs.

However, in 2006, Hinton et al. [ 16 ] proposed a greedy layer-wise training algorithm to train the deep belief network (DBN). They first pre-trained the weights through an unsu-pervised training algorithm starting from the bottommost layer. Then, they fine-tuned the weights to minimize clas-sification error using a supervised training algorithm [ 17 ]. Their work made a breakthrough that vitalized deep learning research. Later on, the idea of the unsupervised pre-training was applied to other neural networks such as the stacked auto-encoders [ 18 ].

Exceptionally, DCNN can be trained with gradient-based learning algorithm even without pre-training. The network structure was proposed by Fukushima in 1980 [ 19 ]. How-ever, it has not been widely used because the training algo-rithm was not easy to use. In 1990s, LeCun et al. [ 20 ] applied a gradient-based learning algorithm to DCNN and obtained successful results. After that, researchers further improved DCNN and reported good results in image recognition [ 21 ]. Recently, Cire  X  san et al. applied multi-column DCNNs to recognize digits, alphanumerals, Chinese characters, traf-fic signs, and object images [ 5 , 6 ]. They reported excellent results and surpassed conventional best records on many pub-lic databases, including MNIST digit image database, NIST SD19 alphanumeric character image database, and CASIA Chinese character image database.

In addition to the common advantages of deep neural net-works,DCNNhassomeextraniceproperties:Itwasdesigned to imitate human visual processing, and therefore, it has a highly optimized structure to process 2D images. Further, DCNN can effectively learn the extraction and abstraction of 2D features. Particularly, the max-pooling layer of DCNN is very effective in absorbing shape variations. Moreover, com-posed of sparse connection with tied weights, DCNN has significantly fewer parameters than a fully connected net-work of similar size. Most of all, DCNN is trainable with the gradient-based learning algorithm and suffers less from the diminishing gradient problem. Given that the gradient-based algorithmtrainsthewholenetworktominimizeanerrorcrite-rion directly, DCNN can produce highly optimized weights.
However, before now, DCNN has not been applied for recognizing handwritten Hangul characters. Several difficul-ties discourage the swift application of DCNN in practi-cal situations. Because of its complexity, implementing and debugging DCNN is difficult and time-consuming. More-over, training a large DCNN requires heavy computation. For example, Cire  X  san et al. [ 5 ] estimated that training a DCNN to recognize 3,755 Chinese characters on a single CPU would take more than 1year. Fortunately, the problem of the heavy computation can be partially alleviated by training with the GPU-based massive parallel processing [ 5 , 21 ]. 3 The DCNN-based Hangul recognizer 3.1 Overall structure Figure 2 shows the overall structure of the DCNN. Each layer receivestheoutputofthepreviouslayerasitsinputandpasses the output to the next layer. We built the DCNN by combining three types of layers: convolution, max-pooling, and classi-fication. The low-and middle-level layers are composed of convolution and max-pooling layers alternately. The odd numbered layers, including the bottom layer, are composed of convolution layers, and the even numbered layers are com-posed of max-pooling layers. The nodes on convolution and max-pooling layers are grouped into 2D planes, also called feature maps. Each plane is connected to one or more planes of the previous layer. Each node on a plane is connected to a small region on the connected input planes. The node of the convolution layer extracts features from the input image (or input 2D feature maps) through the convolution operation on the input nodes, whereas the node of the max-pooling layer abstracts the features by propagating the maximum value among the input nodes.

As the features are propagated to higher-level layers, they areabstractedandcombinedtoproducehigher-level features. Meanwhile, the size of the feature map is reduced. In other words, the higher the level, the smaller the size of the feature map.Whentheresolutionofthefeaturemapbecomes1x1ata high-levellayer,thefeaturesarepassedontotheclassification layers. The classification layers are placed at the top of the DCNN. They decide the classification result by analyzing the features extracted and abstracted by the preceding layers. For the classification, we applied the fully connected network, because it is popular and has provided good performances in several recent works [ 5 , 21 ]. Each node of the top-level layer computes the score of a class. When the propagation finishes, the recognizer outputs the class with the highest score as the classification result.
 3.2 Convolution layers The convolution layer extracts features through the convo-lution operation on the input image or the feature maps of the previous layer. Each output plane is connected to one or more input planes. Each output node is connected to the input nodes in a small window. The horizontal and vertical distance between two adjacent windows is called stride. Denoting the stride by S , a node at ( i , j ) is connected to the input nodes in an M  X  M window whose upper left corner is at ( iS , jS ). Each node has a set of weights to connect itself to the input nodes. All nodes on a plane share the same weights.
Let X n ( p , i , j ) denote the activation of a node at coordinate ( i , j )onthe p th plane of the n th layer and C n set of input planes connected to plane p of layer n .Asall nodes on a plane share the same set of weights, the weight of the connection from X n  X  1 ( q , iS denoted by w n ( q , p , u ,v) , where 0  X  u ,v  X  M n  X  1, with M being the width and height of the convolution mask connect-ing layers n  X  1 and n . The output of each node is computed as in Eq. ( 1 ), where  X  n p is a bias, and f is an activation function. X
The first term in the argument of the activation function is a convolution operation with a mask composed of the shared weights. From this point of view, the nodes on a plane compute the same feature extracted from different locations. When the stride is set to one, the convolution layer extracts features from all possible coordinates. In this case, the con-volution layer does not miss important features even if the positions of the features are shifted.

It is worth noting that Eq. ( 1 ) convolutes multiple input feature maps, thereby enabling the convolution layers to extract higher-level features from multiple lower-level fea-tures. The size of the output feature map after convolution is derived based on the size of the input feature map as shown in Eq. ( 2 ): width n = ( width n  X  1  X  M n + 1 )/ S n height n = ( height n  X  1  X  M n + 1 )/ S n 3.3 Max-pooling layers The max-pooling layer abstracts the input feature into a lower dimensional feature. The output feature maps have one-to-one correspondence with the input feature maps. A node at ( i , j ) is connected to the input nodes in an M  X  M window whose upper left corner is at ( iS , jS ) . Each node selects the maximum value among the input nodes as indicated by Eq. ( 3 ). Note that Eq. ( 3 ) does not require any weight. X
The average-pooling, frequently used for down-sampling, is an alternative to the max-pooling. However, a previous study reported that max-pooling produces better results than average-pooling [ 22 ]. Similar to the case of the convolution layer, the resolution of the output feature map is decided by Eq. ( 2 ). The stride of the max-pooling layer is often set to two. In this case, the max-pooling layer reduces the size of the feature map approximately by a quarter. The max-pooling layer plays an important role: It absorbs shape variation or distortion. In handwritten characters, the positions of salient features often shift. The max-pooling node propagates only the maximum value in a window, ignoring the offset. There-fore, the max-pooling node catches the feature, but ignores small displacements within the window. Given that a DCNN has a collection of max-pooling layers, each of which absorbs small positional shifts, the DCNN does not require a separate shape normalization step to regulate shape variation. More-over, the max-pooling layers in a DCNN absorb shape vari-ation in phases, which is desirable to minimize information loss. 3.4 Classification layers Classification layers are placed at the top of the DCNN. They compute the score of each class from the features extracted and abstracted by the preceding layers. The size of the fea-ture map is reduced to 1  X  1 at the last feature extraction or abstraction layer. Then, the feature maps are treated as scalar values and passed to the first fully connected layer. We used the fully connected feed-forward network for the classifi-cation. The output of each node is computed as shown by Eq. ( 4 ), where the 2D coordinate ( i , j ) on each feature map is omitted because each feature map is composed of only one node in the final classification layers.
 X 3.5 Activation functions Various types of activation functions are used in neural net-works. In this research, we implemented sigmoid [ 23 ], hyper-identity functions. In our preliminary experiments, the com-bination listed in Table 1 produced best results. Therefore, we used this particular combination in our experiments. 4 Training DCNN 4.1 Gradient-based learning The gradient-based learning algorithm in [ 20 ] is a general-ization of the back-propagation algorithm, which iterates to adjust the weights to minimize an error function E . Starting from an initial weight vector W , it updates the weights as indicated by Eq. ( 5 ) at each iteration, where  X  is a learning rate.
 W  X  W  X   X 
Denoting the output and the weight vectors of the n th layer as X n and W n , respectively, and applying the chain rule, the gradient at the n th layer  X  E  X  W n is expanded as demonstrated by Eq. ( 6 ).  X 
E  X 
It is noteworthy that the product of the first two factors tional back-propagation algorithm. In Eq. ( 6 ),  X  X n  X  NET n derivative of the activation function, and  X  NET n  X  W n is obtained from the input vector as listed in Table 2 .

The factor  X  E  X  X n at the top layer is computed through the derivative of the error with respect to the output, as presented in the next section. However, those of other layers should be back-propagated from their upper layers. Therefore, each layer should compute  X  E  X  vide to the lower layer.  X  E  X 
Note that X n  X  1 is the output of the ( n  X  1 ) th layer as well as the input of the n th layer. In Eq. ( 7 ),  X  NET n  X  from the structure of the layer and the weight vector W n listed in Table 3 .

Similar to the conventional back-propagation algorithm, the gradient-based learning algorithm trains from the top layer to the bottom layer. At each layer, it computes equation ( 6 ) to update the weights as Eq. ( 5 ); then, it computes using Eq. ( 7 ) to back-propagate to the lower layer.
The gradient-based learning algorithm is applicable to neural networks composed of any types of layers for which ( 7 ), regardless of whether the layers are homogeneous or heterogeneous. Although the entire network function is not differentiable because of the max-pooling layer, it is still piece-wise differentiable, and therefore, the gradient-based learning is still applicable.

There are several modes to train a neural network based on Eq. ( 5 ). The online mode training updates the weights with the gradient computed from each training sample. In contrast, the batch mode training first accumulates the gradi-ents computed from all training samples and then updates the weights with the accumulated gradient. The former decreases the error more quickly than the latter, but is less stable. On the other hand, the latter requires very long time to train a large DCNN with a large set of training samples. An intermediate, the mini-batch training, has a good balance between speed and stability. It partitions the training samples into groups and updates the weights with the accumulated gradient obtained from each group. 4.2 Error criteria Different error criteria lead to different objective functions for guiding the training process. Among them, the mean square error (MSE) is a popular choice. With a desired out-put D = ( d 1 , d 2 ,..., d C ) for the training sample, MSE is defined as in Eq. ( 8 ), where X N c is the output of the top-level layer for the c th class, and C is the number of classes. E
The desired output is represented as follows: d c is one, if c is the true class, and otherwise, d c is zero, for the unipolar activation function, or  X  1, for the bipolar activation function. The gradient of MSE with respect to the output is derived as in Eq. ( 9 ).  X   X 
An alternative to MSE is the cross-entropy (CE) error function. When used in conjunction with the softmax activa-tion function, the CE has the form shown in Eq. ( 10 ). E
While MSE minimizes the absolute error at each output node, CE maximizes the relative size of the true class output with respect to the outputs of other class nodes. CE is usually combinedwiththesoftmaxactivationfunction.Giventhatthe softmax is a unipolar function, the desired output D consists of a single one for the true class and zeroes for all other classes. The gradient of CE with respect to the output is computed as shown in Eq. ( 11 ).  X   X  4.3 Weight normalization In order to avoid overfitting and to improve the generalization ability, we normalized the weights after each update. Weight normalization scales the incoming weights of each node into a unit vector [ 28 ]. Weight normalization of the convolution layer and the classification layer are as shown by Eqs. ( 12 ) and ( 13 ), respectively. Given that the max-pooling layer does nothaveanyweight,itdoesnotrequireweightnormalization. There is an additional reason to normalize the weights. Unlike the sigmoid and the hyperbolic tangent functions, the outputs of the rectified linear and the identity activation functions are not bounded. Given that the network outputs affect the weight update, extreme output values can result in extreme weights. For this reason, the training of a DCNN with the rectified linear or the identity activation function can be unstable. Weight normalization keeps the weights from diverging to extreme values. 5 Further improvement techniques In order to further improve the performance and the training speed of DCNN, we applied a few additional techniques. Two of them are proposed in this research for the first time, whereas other two have been introduced in the literature. 5.1 Modified MSE criteria A neural network-based recognizer with a large number of output classes is not easy to train with MSE. We attempted to train the DCNN recognizer with 520 output nodes, but our attempt was unsuccessful. Table 4 shows the improvement in MSE and recognition rate during the first 12 training epochs. Although we trained in the mini-batch mode, which is much faster than the batch mode, the decrease of MSE as well as the growth of the recognition rate was extremely slow. After 12 epochs, the recognition rate was 0.22%, which is only slightly better than random classification rate 1 / 520 0 . 19%.

The reason for the slow improvement can be found from the definition of MSE represented in Eq. ( 8 ). The desired output of the top-level layer is composed of many  X  1s but only a single one. The nodes of the hidden layers receive the signals back-propagated from all output nodes. The true class node sends a positive signal to cause the hidden nodes to encourage the activation of itself. However, the other C-1 output nodes send negative signals to cause the hidden nodes to discourage the activations of the other output nodes. The positive signal from the only true class node is not sufficiently strong to guide the training compared with the negative sig-nals from the other 519 nodes. This problem is especially serious at the beginning of the training when the weights are not mature enough to compensate the unbalance in the strength of signals.

To overcome this problem, we slightly modified the MSE criterion as shown by Eq. ( 14 ).
 E Equation ( 14 ) is a generalization of Eq. ( 8 ) that assigns coef-ficient a c to each class.  X  is an amplifying factor multiplied to the signal from the true class node. We can compensate the unbalance between the positive and the negative signals by setting  X  greater than one. Figure 3 shows the growth of the recognition rates when the DCNN was trained with various amplifying factors. The horizontal axis represents training epochs, and the vertical axis represents the recognition rate on the training samples. With  X  = 1, which makes equation ( 14 ) equivalent to Eq. ( 8 ), the increase of the recognition rate for 20 epochs was almost negligible. Training with large amplifying factors increased the recognition rate much faster. Large amplifying factors were especially helpful in the early stages of the training.

However, the modified MSE with a large amplifying fac-tor changes the objective function presented in Eq. ( 8 ). It can guide the training inappropriately. Fortunately, the sig-nal unbalance problem becomes less serious as the weights mature.Therefore,weassignedalargenumberto  X  thatcould sufficiently compensate the signal unbalance at the beginning of the training and then decreased it gradually as the train-ing proceeds. When the training ends,  X  was reduced to one, which makes equation ( 14 ) no different from Eq. ( 8 ). 5.2 Initializing convolution masks by edge operators In a deep neural network, the bottom layer is the most dif-ficult to train with the top-down gradient-based algorithm because of the diminishing gradient problem described in Sect. 2.2 . Although the gradient-based learning algorithm on DCNN is less susceptible to the diminishing gradient prob-lem than other DNNs, training bottom layer from random weights is not always the best way. As explained in Sect. 3 , the bottom-level convolution layer extracts features from the input image. The gradient-based algorithm can train good feature extractors even from random initial weights. How-ever, starting with good initial masks can help to find better feature extractors.

In the classical statistical recognition, researchers have achieved good performances by combining the contour direc-tional feature extraction and QDF-based classification algo-rithms [ 1 ]. The set of eight directional gradient features is known as one of the best contour directional feature sets and can be extracted by edge operators [ 29 ]. Inspired by the gradi-ent feature extraction algorithm, we initialized the first eight convolution masks of the bottom layer with the 8-directional edge operators shown in Fig. 4 . As shown in the Sect. 6 , these initial masks were effective in improving the overall performance. 5.3 Elastic distortion Many previous works reported that expanding training data set with artificially synthesized samples improved the perfor-mance [ 5 , 6 , 21 ]. The elastic distortion is an effective way to produce artificial samples from the training samples [ 21 , 30 ]. The distortion algorithm distorts the image by shifting each pixel X  X  coordinate according to a distortion map. We applied the algorithm in [ 21 ] to build the distortion map. First, it gen-erates pairs of random numbers between  X  1 and 1 for the horizontal and vertical displacements of all pixels. Then, it convolves the displacement field with a Gaussian of standard deviation  X  to avoid drastic deformation and normalizes the displacement field to a norm of one. Finally, it multiplies the displacement field by a scaling factor s. In the experiments, we set  X  by four and s by one. For details, see [ 21 ].
In the training, we generated a distortion map whenever a new mini-batch group began and applied it to all the samples in the group. In preliminary experiments, this method showed better results than generating a new distortion map for each sample. We believe the reason is that generating one distor-tion map for each sample causes the DCNN to confuse the shape variation with salient information for the recognition. 5.4 GPU-based parallel processing Accelerating training speed by GPU-based parallel process-ing is essential to train a DCNN-based recognizer for a large character set [ 5 ]. The full set of Hangul contains 11,172 char-acters, and 2,350 characters among them are used daily. The PE92 database contains 2,350 classes, and the SERI95a data-base contains 520 classes. It takes days to train the DCNN-based Hangul recognizer for a single epoch on a CPU. Train-ing for hundreds epochs on a CPU would take years.
Recent high-end GPUs contain thousands of computing units. Composed of many nodes, neural networks are appro-priate to exploit the benefits of massive parallel processing. We applied NVDIA CUDA SDK to run the training algo-rithm on GPU. The improvement of the training speed heav-ily depends on the parallelism of the network structure. On a narrow network composed of a small number of hidden nodes, the GPU-based parallel processing demonstrates lit-tle improvement. However, on a broad network containing a large number of hidden nodes, it accelerates training sig-nificantly. In Hangul recognition, training an epoch takes about 1.25h on GTX Titan, which is about 20 times faster than the serial implementation written in highly optimized C++ codes. With GPU-based parallel processing, training a Hangul recognizer for 500 epochs consumes 625h, which is about 26 days. 6 Experiments 6.1 Experimental environment We evaluated the DCNN-based recognizers on the PE92 and the SERI95a databases. PE92 contains 2,350 character classes each of which has about 100 samples. SERI95a has 520 most frequently used character classes, and each class contains about 1,000 samples. Some examples are presented in Fig. 5 . For fair evaluation, we chose the training and the test sets in the same way as [ 1 ]. We used every 10th sample of each class for the test and all other samples for the training. Thus, the training and the test sets contain 90 and 10% of total samples, respectively.

We described two training criteria and several improve-ment techniques in the previous sections. These training cri-teria and improvement techniques can be combined in vari-ous ways. Considering the amount of time required to train a Hangul recognizer, testing all possible cases on the Hangul databases is significantly time-consuming even on a GPU. Therefore, we first tested all combinations on the MNIST handwritten digit database [ 31 ], which is much less time-consuming. Then, we tested only meaningful combinations on the two Hangul databases.

We experimented on six computers with CPUs that are varied from Q6600 2.4 GHz to ZEON E3-1230V3 3.3 GHz. The GPUs are also varied from GTX 660Ti (1,344 CUDA cores, 2GB RAM) to GTX Titan (2,688 CUDA cores, 6GB RAM). For the GPU-based implementation, we used CUDA SDK v.5.5. In all experiments, we trained in the mini-batch mode. 6.2 Numeral digit recognition (MNIST) The input of the digit recognizer is a 32  X  32 image that con-tains a 28  X  28 digit image at the center and four padding rows and columns at the boundary. The resolutions of the feature maps were decided by Eq. ( 2 ). The digit recognizer is com-posed of seven layers. The feature maps of each convolution layer are fully connected to all feature maps of the previous layer. The detail of the network structures is presented in Table 5 . The DCNN has 299,882 parameters, totally.
We tested the two error criteria as well as the improve-ment techniques described in Sects. 4.2 , 5.2 and 5.3 .Inthis experiment, we evaluated all possible eight cases. For each case, we trained for 1,000 epochs, which took several days on a GPU. The experiment results are presented in Table 6 . Regarding the error criteria, MSE was slightly better than CE when we trained without distortion. However, using elastic distortion, CE showed better results. Overall, CE was slightly better than MSE in the best performance values. The elas-tic distortion significantly reduced error rates in all cases. Setting initial convolution masks by the edge operators fur-ther improved the recognition performance. The best perfor-mance we achieved was 99.67%, obtained by training with the CE criterion and elastic distortion starting from the con-volution masks initialized by the edge operators.
After we obtained the results listed in Table 6 , we con-tinued to train the best combination. The recognition rate increased even after the 1,000th epoch. However, the incre-ment was very slow. After training for 3,000 epochs, we achieved 99.71% of recognition rate (0.29% of error rate). The homepage of the MNIST database lists the best per-formances on their database achieved by various methods [ 31 ]. The lowest error rate in the list is 0.23%, not far from our result of 0.29%. Only two systems in the list reported better results than ours. The best two results were achieved by committees of many DCNNs, not by single classifiers. 6.3 Hangul recognition (SERI95a and PE92) The input of the Hangul recognizers is a 64  X  64 image that consists of a 60  X  60 Hangul image and four padding rows/columns. The Hangul recognizers are composed of ten layers. Similar to the digit recognizer, the feature maps of each convolution layer are fully connected to all feature maps of the previous layer. The DCNNs for the two Hangul data-bases are different in the number of nodes on the highest two layers. The details of the network structures are described in Tables 7 and 8 . The DCNNs have a total of 1,006,728, and 1,106,184 parameters, respectively.

Given that training Hangul recognizers requires a sig-nificant amount of time, and we know that the methods described in Sects. 5.2 and 5.3 are helpful in improving per-formance, we did not test all possible combinations of exper-iment options. Instead, we tested the improvement methods incrementally. For each case, we trained for 500 epochs. We applied the method introduced in Sect. 5.1 to train DCNNs with the MSE criterion. The amplifying factor  X  was set to 300 when the training started and linearly decreased to one.

Table 9 presents the results. Unlike the digit recognition results, the results of the MSE criterion are significantly infe-rior when compared to those of the CE criterion. We believe the reason is that minimizing the absolute error of each out-put node is inefficient in training a recognizer for hundreds or thousands of classes. Similar to the previous experiment, ini-tializing convolution masks by the edge operators and apply-ing elastic distortion to training samples improved the per-formance. The effect of the elastic distortion on the PE92 database was more remarkable than that on the SERI95a database. This is because PE92 contains more classes but less samples per class; therefore, the recognizer suffers more from a lack of training samples. In order to know whether the improvements are statistically significant, we carried out McNemar X  X  test on the results on SERI95a database [ 32 ]. The p values of the improvements by cross-entropy crite-rion (A-&gt; B), edge operator (B-&gt; C), and elastic distortion (C-&gt; D) were 0 . 00 , 2 . 45  X  10  X  3 , and 2 . 42  X  10 tively. These p values show that the improvements are statis-tically significant.
 Table 10 compares our results with previous best works on SERI95a and PE92. The elastic distortion was not used listed in the previous works in Table 10 . As underlined in Table 10 , the conventional best performances on SERI95a and PE92 databases were 93.71 and 87.70%, respectively. Most of the results in Table 9 are better than the conventional best per-formances.Particularly,ourbestperformancesarenoticeably higher than the two conventional best results. Compared with the previous best records, our results yielded improvements of 2.25% on SERI95a and 5.22% on PE92, respectively. These improvements lead to error reduction rates of 35.71% on SERI95a and 42.44% on PE92, relative to the previous lowest error rates. 7 Conclusion In spite of the advances in recognition technology, hand-written Hangul recognition (HHR) has remained largely unsolved due to the presence of many confusing charac-ters and excessive cursiveness in Hangul handwritings. On the other hand, the DCNN has provided outstanding perfor-mances in many recognition fields. However, before now, the DCNN has not been applied to recognize handwritten Hangul. In this research, we built handwritten Hangul recog-nizers using DCNNs and evaluated their performances on the SERI95a and the PE92 databases. Then, we improved the training speed and the recognition performance through GPU-based parallel processing and elastic distortion.
We also proposed two new improvement techniques. The modified MSE error criterion significantly improved the training efficiency of the Hangul recognizer by compen-sating the unbalance between positive and negative signals from the output nodes. Additionally, we achieved further improvement by initializing bottom-level convolution masks by edge operators. Training convolution masks starting from good initial weights was helpful in obtaining good feature extractors.

In the experiments, we achieved recognition rates 95.96% on SERI95a and 92.92% on PE92, which are significantly higher than conventional best records. In handwritten digit recognition, we achieved 99.71% recognition rate on the MNIST database.
 References
