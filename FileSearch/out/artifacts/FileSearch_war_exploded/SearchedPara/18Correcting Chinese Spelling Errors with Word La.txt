 MING-HONG BAI, SHU-LING HUANG, and KEH-JIANN CHEN , Academia Sinica A spell checker is a writing assistance tool which provides users with better word suggestions by automatically detecting spelling errors in documents. For high-quality publications, one important criterion for judging the quality of the document is a low number of typographical errors. Because manual proofreading is costly and time con-suming, computer-assisted tools have been used since the early 1960s [Damerau 1964]. However, the true breakthrough was not until the 1990s, when the noisy channel model started to be used to improve spelling detection and correction [Kernighan et al. 1990; Mays et al. 1991]. When using the noisy channel model to check spelling, based on the principle of information theory [Shannon 1948], spelling correction is seen as the decoding of a noisy channel. The spell checker is defined as follows: given a word w we seek the correct word c that maximizes the probability of c given w : Using Bayes X  theorem, the given formula is equivalent to Since P ( w ) is the same for each possible c , the formula is further simplified to where P ( c ) ,the language model , represents the possibility of c appearing in a sentence, and P ( w | c ) ,the error model , represents the possibility of c being miswritten as w .In their research, Kernighan and Mays demonstrated that combining the language and error models yields better results than either model alone. Because the noisy channel model works so well and is easy to implement, most spell checkers are based on it, with only minor variations.

For Chinese, most spell checkers of the last twenty years have also been based on this noisy channel framework. However, Chinese spell checkers are much more diffi-cult to develop because of certain Chinese language characteristics. (1) No word boundaries . There are no word boundaries in Chinese text, so we must (2) Large character set . Unlike alphabetical languages such as English and French
Difficulties in Chinese spelling correction are caused by these two characteristics, and they have delayed progress in Chinese spell checking research. We will discuss these issues in detail. In English, spelling errors can be essentially classified into two types: nonword er-rors , referring to misspelled words which are not valid words, and real-word errors , referring to misspelled words which are valid words but ungrammatical or logically improper. Nonword errors in English can be determined by consulting a dictionary. In Chinese, spelling errors can likewise be classified as nonword and real-word errors, but we cannot simply consult dictionaries to differentiate between the two errors, because there are no blanks to mark word boundaries in Chinese.

For example, when the Chinese word  X  X  X  ( shu-gui  X  X ookshelf X ) is miswritten as  X   X  ( shu gui  X  X ook expensive X ), although it is not a word, we still cannot guarantee that  X  X  X  ( shu gui ) is necessarily a spelling error because in Chinese a single character can be either a word or a morpheme. That is,  X  X  X  ( shu gui ) could be a misspelling of  X  X  X  ( shu-gui  X  X ookshelf X ), or it could be read as the two words  X  ( shu  X  X ook X ) and  X  ( gui  X  X xpensive X ) in a sentence like  X  X  X  X  X  X  X  X  X  ( du-shu gui you xin de  X  X he precious part of reading is gaining refreshing experience X ). Hence in Chinese nonword errors cannot be discovered by simply consulting dictionaries; instead, we must rely on context to detect and discriminate errors.

One may argue that Chinese words could be automatically identified by a Chinese word segmentation system. However, word segmentation works well only on sentences that contain no spelling errors, as it treats unrecognized character sequences as mono-syllabic words; this makes it difficult for word-based language models to spot and cor-rect errors. For instance, In the given sentence, because  X  ( gui  X  X abinet X ) was miswritten as  X  ( gui  X  X xpen-sive X ), it results in the further error of identifying  X  X  X  ( jia-shu  X  X amily letter X ) as a word, resulting in misleading context for the word n-gram language model, which then miscalculates the conditional probabilities of  X  ( gui  X  X abinet X ) as well as  X  ( gui  X  X xpensive X ). Even worse, multiple spelling errors in a given sentence can lead to cascaded segmentation errors, which are difficult to correct. Some researchers thus use character-based language models [Huang et al. 2007; Hsieh et al. 2013] to avoid these problems. However, word-based language models are much more robust than character-based language models [Gu et al. 1991; Yang et al. 1998; Gao et al. 2002].
In order to design a reliable Chinese spelling error detection system, some re-searchers have attempted to detect unknown words [Chen and Bai 1998; Chen and Ma 2002] to aid in detecting spelling errors [Huang et al. 2007; Hsieh et al. 2013; Wu et al. 2013; Chiu et al. 2013]. They find that after word segmentation procedures, a Chinese sentence with spelling errors usually contains ungrammatical single char-acters, such as  X  ( cuo )and ( X ) ( zhe ) in the sentence  X  X  X   X  zhe di fen-dou ). They combine every character in the confusion set of each ungram-matical single word with the preceding and following words, and if one of the resultant candidate words is recognized as a new word, then they regard this new compound as a candidate for replaceable words. This method can detect nonword errors and limits the error candidates, but is largely ineffective for real-word errors due to the difficulty of detecting ungrammatical words. For example, in  X  X  X  X  X  X  X  X  X  X  ( qian yi-fa er dong quan-shen  X  X ull one hair and the whole body moves X ), when  X  X  X  ( quan-shen  X  X hole body X ) is miswritten as  X  X  X  ( he-shen  X  X it the body X ), it results in a sentence that is not logical but still grammatical. In past studies, English spelling errors have been roughly summarized into four types of editing operations [Damerau 1964]: Insertion  X  X pelling error with redundant letters, for example, cress is miswritten as acress ; Deletion  X  X pelling error with lost letters, for example, actress is miswritten as acress ; Substitution  X  X pelling error with wrong letters, for example, access is miswritten as Transposition  X  X pelling error with reverse letters, for example, caress is miswritten According to Peterson [1986], edit distance 1 covers about 90% of spelling errors, and edit distance 2 accounts for almost all of the remaining spelling errors. Thus the error tance between c and w . For example, p ( access | acress ) can be simplified to p ( r leaves us with the challenge of establishing within the probability model a confusion matrix of letters by which we may evaluate the probability of each letter representing one of the four types of spelling errors. Since English letters compose a small closed set, a small amount of error pairs are sufficient to yield a good result for the confu-sion matrix probabilities. However, the Chinese character set is large, consisting of more than ten thousand characters [Unicode Consortium 2014]; the number of com-monly used characters is estimated at 5,000 [MOE 1994]. To establish a Chinese 5000-word confusion matrix even considering only the substitution error type, we would need to calculate substitution probabilities for 5,000  X  5,000 words. This is infeasi-ble in practice. Furthermore, in Chinese, insertion, deletion, and transposition errors are considered grammatical errors, since all Chinese characters may also be words; hence inserted, deleted, or transposed characters are merely grammatical errors. Thus Chinese spelling checkers consider character substitution errors only.

To handle substitution errors, some researchers have found that most Chinese spelling errors are caused by characters with similar pronunciations, shapes, or mean-ings [Chang 1995; Liu et al. 2011; Chen et al. 2011]. Therefore a confusion character set is created using similar characters for each character, based on the previous sim-ilarity features. For example, the confusion set of  X  ( nai ) may include the similarly-shaped characters {  X  ( na ),  X  ( nai ),  X  ( jin ),... } and similarly-pronounced characters {  X  ( nai ),  X  ( nai ),  X  ( lai ),... } . Such a confusion set suggests a limited number of can-didates for correction and greatly reduces the parameters that must be estimated by the spelling checker X  X  language model for use in decoding. Therefore the first design issue for Chinese spelling check is how to assemble a good confusion set for each char-acter which is not only small in size but also boasts a high inclusion rate for correct spelling candidates. The second design issue is how to develop a model for spelling check which best estimates the probability of the correct output character sequence for an input sentence; this however calls for a large training corpus of spelling errors. In other words, to estimate the confusion set parameters for the error model, the large character set of Chinese necessitates a bigger training dataset of spelling errors than is needed for English. To avoid the difficulties of constructing Chinese error models, many researchers adopt only a language model when estimating the correctness of word substitution [Chang 1995; Lin et al. 2002; Huang et al. 2007; Hsieh et al. 2013; Jia et al. 2013], resulting in more false alarms. Consider the following examples. Because the probabilities of  X   X  ( zhi-ji  X  X osom friend X ) and  X   X  ( yi-ban  X  X alf X ) are lower than that of the corresponding  X   X  ( zi-ji  X  X yself X ) and  X   X  ( yi-ban  X  X enerally X ), the former are substituted by the latter. Thus if a conventional language model is using in decoding, it always chooses the word sequence with the highest probability. However, the objective of the spelling checker is to find the word sequence that most likely expresses the intended meaning given the input expression, such as Because the language model probability of p (  X  X  X  X  X   X  X  )( ta shi ge nu-hai-zi  X  X e is a girl X ) is much greater than p (  X  X  X  X   X  X  )( ta shi ge hao-hai-zi  X  X e is a good child X ), the system favors  X  X  X  X  ( nu-hai-zi  X  X irl X ) over  X  X  X  X  ( hao-hai-zi  X  X ood child X ). However the  X  ( yu )and  X  ( hao ) characters are much more similar than  X  ( yu )and  X  ( nu ). Therefore better spelling error decoding can be achieved by combining the conventional language model with an error model.

In this article we propose a word lattice decoding model for Chinese spell checking that solves these problems. The model corrects nonword errors as well as real-word errors. In addition, for better probability estimation of error models, we also propose a methodology for extracting spelling error samples automatically from the Google web 1T corpus. Due to the large quantity of data in the Google web 1T corpus, a sufficient number of spelling error samples can be extracted that reflect real-world spelling error distributions. Finally, in order to make the spelling checker more practical, we intro-duce a methodology for providing n-best suggestions for the user to choose from.
This article is divided into seven sections. In Section 2 related works are reviewed and addressed, and in Section 3 we introduce and discuss the proposed system. The extraction of spelling error samples is described in Section 4, and the results are pre-sented in Section 5. In Section 6, we discuss extending the model to handle other types of errors, and in Section 7 we conclude and describe directions for future work. In the early 1990s, Kernighan et al. [1990] and Mays et al. [1991] first applied the noisy channel model to spell checkers. Due to the framework X  X  simplicity and effec-tiveness, most later studies also adopted and improved upon this model. The Chinese spell checking system is basically also based on the noisy channel model. However, due to the special characteristics of the Chinese language, as mentioned in Section 1, a Chinese spell checker is more difficult to design than an English one. We will address these challenges and discuss the proposed solutions.
 Regarding Chinese error detection, researchers [Ren et al. 2001; Huang et al. 2007; Chen et al. 2011; Hsieh et al. 2013] found that similar to unknown words, Chinese spelling errors usually result from ungrammatical single characters after word seg-mentation. They thus likewise detected unknown words [Chen and Bai 1998; Chen and Ma 2002] to detect spelling errors. That is, they used ungrammatical single char-acters as clues to look for spelling errors. They found this approach to be simple and effective, although among the two types of spelling errors, most real-word errors can-not be detected in this way. Because ungrammatical words are difficult to detect, an ungrammatical character can be detected by boundedness of the character and by us-ing simple grammatical patterns. For example, when  X  X  X  ( quan-shen  X  X hole body X ) is miswritten as  X  X  X  ( he-shen  X  X it the body X ), because the latter is also a recognizable word, the unknown-word detection method does not detect spelling errors in such a real-word example.

Many researchers have used error template rules to detecting spelling errors [Huang et al. 2008; Chen et al. 2009, 2011]. An error template rule usually is formed by tuple and the second column X 2 denotes the string that has been corrected, for example, &lt;  X  X  X  X   X  ( bu-tong-fan-xiang ),  X  X  X  X  X  X  ( bu-tong-fan-xiang  X  X xtremely good X ) &gt; .If X appears in a sentence, the spell checker then marks it as a possible error. Since error template rules are usually derived from a spelling error corpus with many contexts, the precision of error detection when using error template rules is relatively high. On the other hand, as the recall rate of error template rules is very low, this approach is more suitable during pre-or post-processing to detect common spelling errors.
As proposed by Chang et al. [1995], another way to improve error detection is to treat every character as a potential typo and provide candidate characters for each error. Through noisy channel model calculations, the original word competes with its candidate words; if the score of the original word is lower than its candidate words, the original word is regarded as a typo and is replaced by the candidate with the highest score. This method, which considers every word as a potential spelling error, is similar to our approach. However, they use only a language model, resulting in excessive false alarms, since language models always select the words or characters with the highest probability.

As mentioned in Section 1, in error model estimation, the Chinese character set is large. There are about 5,000 commonly used characters, so it is unlikely that each character can be treated as a potential candidate, as is done in English. For this reason, most Chinese spell-check systems rely on a reasonably-sized confusion set [Zhang et al. 2000; Lin et al. 2002; Liu et al. 2008, 2009a, 2009b, 2011]. Since the confusion set coverage strongly affects the recall rate of a correction system, in order to establish a confusion set with sufficient coverage, researchers have summarized the three major causes of spelling errors (similar pronunciation, shape, or meaning) as the criteria upon which to build a confusion set. However, among them, generally only similar-shape errors are addressed. For instance, Zhang et al. [2000] generated a confusion set by calculating the encoding similarity of the five stroke input method; and Lin et al. [2002] generated a confusion set by calculating the Cangjie code similarity. Later, Liu et al. [2008] found that using this approach is flawed, because Cangjie is specifically designed for input and sets a five-code limit for each word; hence more complicated character shapes are simplified to meet the code limits  X  but such simplification is not conducive to character shape comparison. They thus proposed a methodology to extend Cangjie code to achieve more precise shape comparisons.

Similar-shape studies have contributed to Chinese confusion set generation, but quantifying the similarity of character shapes for error modeling is another issue. Some researchers simply ignore the error model for lack of reliable similarity mea-sures, and instead rely on the language model alone to select the correct word [Chang et al. 1995; Ren et al. 2001; Huang 2002, 2007; Jia et al. 2013; Hsieh et al. 2013]. How-ever, as mentioned earlier, if the similarity of the error model is ignored, the language model favors more probable word sequences over more reasonable words, leading to in-correct spelling. Chen et al. [2011] attempted training the error model probabilities on a corpus of student writing with spelling correction. Although this approach leveraged real spelling errors, such manually-edited data is difficult to acquire. Liu et al. [2009a, 2009b, 2011] presented three criteria based on the assessment of the difference in the Cangjie codes. Chiu et al. [2013] also proposed a simple frequency ratio between the error word and the correction as the score of the error model.

Regarding language models, character-based language models and word-based lan-guage models are generally adopted. Using a character-based language model is at-tractive because there is no need for word segmentation and because it requires less memory. Although preliminary tests indicated that the word-based language model is much more powerful than the character-based language model [Gu et al. 1991; Yang et al. 1998; Gao et al. 2002], word-based language models not only suffer from seg-mentation errors, but also require a corpus of sufficient size to avoid data sparseness. To reduce data sparseness in the language model, Chang [1995] applied the inter-word character bigram, proposed by Lee et al. [1993b], into his language model. The language model is a transformation of a word-class-based language model which rep-resented the class of a word by its suffix and prefix words. The bigram statistics are built based on the co-occurrences of the word suffix of a word and the word prefix of the following word.

Unlike the approach of detecting spelling errors after word segmentation, Jia et al. [2013] propose a graph model; they simultaneously segment sentences and detect spelling errors. This is very similar to our approach of word lattice decoding, except that while scoring the decoding paths, they employ only the language model and ignore the error model.

In addition to research on Chinese spell checking, in automatic speech recognition the noisy channel model has also been adopted to find the best character sequence for a sequence of acoustical signals. Lee et al. [1993a, 1993b] propose a Chinese speech recognition model that is very similar to our word lattice decoding model. However, the estimation of the acoustic model in automatic speech recognition is quite differ-ent from that for the error model of a spell checker. This is because the error model estimates the probabilities of a miswritten character to other characters with similar pronounciations, shapes, or meaning, as mentioned in Section 1. Currently, most Chinese spell checkers first perform word segmentation, and then carry out detection and correction based on the results of segmentation. The disad-vantage of this approach is that spelling errors often affect the word segmentation results, which then leads to additional errors during the detection and correction pro-cesses. Therefore with our spell checker we adopt a word lattice decoding approach in which we simultaneously include all confusing characters for each character of an in-put sentence, from which we create the word lattice. This results in word lattice nodes that carry not only the words from original sentence but also the potential substitu-tion words contained in the confusion sets. Consider the example of  X  X  X  X ( X ) X  ( bu-pa cuo zhe di ), as shown in Figure 1; substituting the confusion word  X  ( cuo )for  X  ( cuo ) yields a recognizable compound  X ( X ) ( cuo-zhe  X  X rustration X ), so we add a node for it in the word lattice. In other words, by finding the best path in the resultant word lattice we find not only the best word segmentation result but also the best substitute words. For a more realistic example, see Section 3.1.

Given a sentence which may contain spelling errors S = s 1 procedure can be divided into two major steps. (2) According to the noisy channel decoding model, decode the word lattice to find the After word lattice decoding we have a best path W , which is still an n -character sen-tence. However, unlike the original sentence S , for each character c character, and c i = s i if s i has been identified as an incorrect character, the suggested replacement for which is c i .

We describe in Section 3.1 how to construct the word lattice from sentence S by step (1), and in Section 3.2 we address step (2) in detail. Finally, in Section 3.3, we demonstrate how to obtain the corrected result from the n-best suggestions. In the first step, we consider each character in the sentence S potential typo, and select possible substitute characters for each s set. Thus, for each character s i in S , we have a set of candidates { c | c dao le ) as an example: in Figure 2, listed below each character is its list of confusion characters.

We then consult a dictionary D to find potential words in the neighboring candidate list, as shown in Figure 3. If we find in the neighboring candidate list from a continuous string  X  c j i = c i ,c i + 1 , ... ,c j ,c then we generate a word lattice node w i , j =  X  c j i . For example, in Figure 3 the words  X  X  X  ( wo-men  X  X e X ),  X  X  X  ( wo-lia  X  X he two of us X ),  X  X  X  ( jia-shu  X  X amily letter X ),  X  X  X  ( shu-gui  X  X ookshelf X ), and  X  X  X  ( gui-dao  X  X neel down X ) are added to the word lattice as nodes.

In addition to multisyllabic words, monosyllabic words are also be added to the word lattice, that is, every word s i in S is added to the word lattice as a new node. However, in order to reduce decoding complexity, only high-frequency monosyllabic words in the confusion set are added into the word lattice.
 Then according to each node X  X  position in the sentence, we link the nodes in series. That is, for any node w i , j , if there is another node w sition, then we link w i , j to w j + 1, k . Finally, the starting node w w + 1, n + 1 are added to the word lattice as well, and are linked to w vidually. Word lattice G is defined as G = (V,E), V ={ w i , j D , c k  X  C k } X  X  w 0,0 , w n + 1, n + 1 } ,E ={ &lt; w i , j is shown in Figure 4. Because there are so many nodes with single character words, we omit most of these. The purpose of word lattice decoding is to find the best path through the word lat-tice. It is supposed that this path will include both the best segmentation result and the best substitution words. Expressed mathematically, we seek a path W in the word lattice: where L ( S ) is the set of all possible paths through S  X  X  word lattice. P ( W ) is the prior probability of path W as estimated by the n-gram language model. Thus the language model is a quantitative measure of the likelihood that W is a Chinese sentence. P ( S is the error model, which measures the likelihood that the author typed S by mistake when W was intended.  X  is a weight parameter between the error and language models; the optimal value may be tuned on the development set.

Figure 5 shows a word lattice and the best path W =  X  X  X  ,  X  jia shu-gui dao le  X  X ur bookshelf fell over X ) which includes both the segmentation and substituted words, such  X  ( gui ) replacing  X  ( gui ).

For the language model, we calculate the sentence probability using a word-based n-gram language model. For the error model P ( S | W ) , we calculate the sentence prob-ability as the product of the conditional probabilities of words w defined as acter c k being written as s k .
 The probability of each character c k being miswritten as s where p err is a control parameter ranging from 0 to 1 which indicates the probability of any character being miswritten. With smaller values of p err and the model returns fewer typos. The value of freq ( c k learned from a spelling error corpus. In our experiments, as described in Section 4, we extracted the spelling error samples from Google Web 1T automatically.

For the word lattice decoding, we adopt dynamic programming; that is, for any node w i , j , we define the forward decoding algorithm recursively as Function Q w i , j denotes the forward decoding probability of node w is the combined score of the error and language models. We define the initial value Q ( w 0,0 ) = 0. To facilitate calculations, we use log probabilities to calculate the forward decoding probability. When calculating the forward decoding algorithm, the forward decoding probability from w 0,0 to each node w i , j is calculated along with the best path. Hence when the algorithm has processed the complete word lattice, the best path as well as the log probability are computed. In the previous section, we used forward decoding to calculate the best path, obtaining the best error-corrected suggestion for the sentence. We provided for each typo one corrected suggestion; however, in real-world applications, to increase the likelihood of getting the correct answer, users may desire more than one such suggested correction. Taking Figure 6 as an example, forward decoding suggests only the best alternative character  X  ( ju )for  X  ( ju ), but sometimes more suggestions, such as  X  ( ju ), would be helpful. According to the definition, the forward decoding algorithm computes Q ( w the best path from the start &lt; S &gt; to any node, say  X  ( ju ). But forward decoding does not compute the best path from  X  ( ju )to &lt; /S &gt; . To provide n-best suggestions for a given character, we must calculate the best path from the alternative nodes to &lt; /S &gt; .
To calculate the best path from any node w ij to &lt; /S &gt; , we just reverse the decoding direction from &lt; /S &gt; to &lt; S &gt; . We define the backward decoding algorithm as The purpose of backward decoding is to search for the best partial path from node w i , j to node w n + 1, n + 1 . Thus, as long as we have performed both forward and backward decoding, for any node w i , j in the word lattice, as shown in Figure 7, we have the best partial path from node w 0,0 to node w i , j as well as the best partial path from w w
Then, for any node w i , j we obtain the best path from w by connecting the partial paths from the two algorithms. The decoding probability of score of any character c k is estimated as the maximum probability of all nodes w contain the character:
Finally, we sort the possible candidate characters in each position of the sentence by their scores and then return the n-best suggestions. Web texts are more casual and contain more errors than published books or profes-sional articles because the authors spend less time proofreading the material. Al-though for much research these errors are seen as noise or as obstacles to be removed, for research on spell checkers, these spelling errors are valuable information. By ex-tracting spelling errors from a large web corpus automatically, we gain not only a sufficient number of spelling error samples but also the actual statistical distribution of spelling errors. In this section, we propose a method of automatically extracting spelling error data from the Google web 1T corpus. The approach can be divided into the following steps. (1) Error candidate collection . Find each string t that contains a spelling error among (4) Character similarity score computation . Use the &lt; t , t &gt; pairs to calculate the The Chinese Google web 1T corpus contains mainly Chinese word n-grams and their frequency counts. These n-grams ranges from unigrams (single tokens) to 5-grams (five-token sequences). Our extraction strategy is to identify each n-gram containing a potential incorrectly spelled word, and then to determine the corrected n-gram. For any term t among the n-grams, if t is not included in the dictionary D ,then t may be either a pure n-gram or a term with miswritten characters. We assume that t con-tains one spelling error: we test each character in t one-by-one by substituting each with the characters in its corresponding confusion set. If we find a new string t we then add the pair &lt; t , t &gt; into the error candidate table. Note that, when a term contains miswritten characters, the Chinese segmentation system segments the term into separated characters. For example, when  X  X  X  X  X  X  ( chu-mu-jing-xin  X  X readful X ) is miswritten as  X  X  X  X  X  X  ( chu-mu-jing-xin ), it is instead segmented into the three sepa-rate tokens  X   X  X  X   X  ,  X   X   X  ,and  X   X   X  , and counted as a 3-gram. This shows why we collect error candidates from n-grams.

Take  X  X  X  X  X  X  ( bu-you-zi-zhu ) as an example (Figure 8); when  X  ( zhu ) is replaced by  X  ( zhu ),  X  X  X  X  X  X  ( bu-you-zi-zhu  X  X nvoluntarily X ) turns out to be a word in the dic-tionary, and we add &lt;  X  X  X  X  X  X  ( bu-you-zi-zhu ),  X  X  X  X  X  X  ( bu-you-zi-zhu) &gt; to the error candidate table. Figure 9 shows sample error candidates; note the words included that are not spelling errors, such as  X  X  X  ( ren-ge ),  X  X  X  ( shang-fa ), and which call for further evaluation. In the previous section, we ended up with a large set of error candidate pairs, but many of them were not really spelling errors, and thus require further validation. Our assessment approach is based on the assumption that words with similar meanings are often used in similar contexts [Sch  X  utze 1999]. We calculate the context similarity of t and t in texts; a high context similarity suggests that t and t are synonymous, and thus that t is likely a misspelled word of t .

Specifically, for each error candidate pair &lt; t , t &gt; , we create context vectors v v t for t and t by collecting the adjacent words preceding and following t and t in the Google web 1T corpus, respectively. Figure 10 demonstrates this using  X  X  X  X  X  X  ( bu-you-zi-zhu )and  X  X  X  X  X  X  ( bu-you-zi-zhu  X  X nvoluntarily X ) as examples.
 Next, we use cosine similarity to calculate the similarity of the context vector pairs. Cosine similarity is defined as
Table I shows the example error candidate pairs and their cosine similarities. This shows that the cosine similarity for false-misspelled word pairs are usually very low, suggesting the use of cosine similarity to filter out false drops. Thus filtering the list yields a reliable list of error pairs which can be used for two purposes: for training data in calculating character similarity, and for error template rules [Huang et al. 2008; Chen et al. 2009, 2011] for detecting spelling errors. In the following section, we explain how to calculate character similarity using the error pair list. In the previous section, we extracted a reliable error pair list; we now use the data to calculate character similarity. We estimate the probability of a character c being miswritten as e as where freq ( t ) denotes the frequency of the n-gram t , which contain spelling errors, appearing in the corpus.

Table II shows actual confusion sets sorted by character similarity. Note that the characters most likely to be miswritten are those that are similar in both shape and pronunciation, for instance,  X  ( wu )  X   X  ( wu ),  X  ( bo ) ( yao ); followed by characters with similar shapes, for instance,  X  ( she )  X  ( pei )  X   X  ( ding ). Similarly-pronounced characters are more difficult to predict; we found that some were never confused, such as  X  ( yao )  X   X  ( yao ), and  X  ( she ) ( she ). Others, however, are easily miswritten, such as  X  ( pei )  X   X  ( chong ). This could be because the latter characters in the pairs are semantically related to the paired characters. Therefore, reliable similarity measures for similarly-pronounced characters can only be estimated by a spelling error corpus. In order to evaluate the similarities between characters and their confusion set, we proposed a method to extract error candidate pairs. We used a cosine measure of the context vector to evaluate the similarity between the terms with error and correct terms. Error candidate pairs with their cosine similarities can also be used as error template rules in detecting spelling errors. We select highly similar pairs as error template rules. An error candidate rule is defined as where X 1 denotes a term with a spelling error and X 2 denotes the correction of the term. The application of the rules are very intuitive: we add the terms with spelling er-rors into the dictionary of a segmentation system, and employ the word segmentation system to identify error terms. Taking rule &lt;  X  X  X  X   X  ( bu-tong-fan-xiang ),  X  X  X  X  X  X  ( bu-tong-fan-xiang  X  X xtremely good X ) &gt; as an example, we add mentation dictionary, and when the system identifies  X  X  X  X   X  in a sentence, the sys-tem knows that is has detected an spelling error and suggests replacing  X  ( xiang ) with  X  ( xiang ).

Since error template rules are derived from a corpus which includes more contexts, the precision of error detection when using error template rules is relatively high. On the other hand, the recall rate is very low, so such rules are more suitable for use in pre-or post-processing to detect common spelling errors. We used three datasets to build our word lattice decoding model. The first one was the Google web 1T corpus; from this we used n-grams from unigrams to 5-grams to extract the miswritten samples, which were further used to train the word similarity list (Sections 4.1 X 4.3). The samples were also used to extract error template rules (Section 4.4). We used n-grams from unigrams to trigrams to build the language model. At runtime, we used unigrams as a dictionary to create the word lattices for the input sentences. Our second dataset was the CKIP dictionary, which contains about 90,000 Chinese words and is used when extracting the error candidate pairs automatically (Section 4.1). The third dataset was the character confusion set proposed by Liu et al. [2011], which contains 5,401 Chinese common characters and their confusion sets.
For system tuning and testing, we used 700 training sentences from the Bakeoff 2013 CSC Data set [Wu et al. 2013] as our development set, and as our test data we used the test set of subtask 2, which contains 1000 sentences with spelling errors. In order to identify the advantages and disadvantages between different systems, we used the recall, precision, and F-score metrics to estimate the performance of the var-ious spell checkers. These metrics are defined as We used the following models in our evaluations.
 WLD. This is the proposed word lattice decoding model.
 WLD-no-err-model. In order to demonstrate the importance of the error model for the WLD + ETR. In order to determine whether or not the error template rule and the SinicaCKIP. This is the Bakeoff 2013 CSC SinicaCKIP-Run2 system [Hsieh et al.
SinicaCKIP + ETR. This is the Bakeoff 2013 CSC SinicaCKIP-Run3 system [Hsieh
From Table III, we observe that the precision rate dropped 25% when using the lan-guage model alone without the error model, but the recall rate increased only slightly. This shows that the spell checker performance suffers without the help of the error model. Secondly, the F-score of SinicaCKIP was about 9% lower than that of WLD. WLD X  X  recall and precision were both better than that of SinicaCKIP, in particular for recall. This is consistent with what we mentioned in the first section: unknown word detection-based approaches miss real-word errors. Finally, focusing on error template rules, we find that WLD + ETR outperformed WLD by a mere 1% in terms of the F-score, but outperformed SinicaCKIP by 10%. Thus we conclude that most of the errors detected by the error template rule are also detected by WLD. On the other hand, in terms of precision, the error template rule performance is poorer than that of WLD because fewer errors are detected by the former.

In addition, in Section 3, we proposed a methodology for n-best suggestions; we use the coverage rate to estimate the performance of n-best suggestions. Coverage is de-fined as
From Table IV, we see that the coverage difference between best-1 and best-2 is 10 points, and that between best-2 and best-3 is 3.5 points. This means that the system has difficulty discriminating between the suggestion quality of the top three words. Thus it is important for the spell checker to provide the user with the top three sug-gestions as references.
 When comparing our approach with the SIGHAN-2013 Bake-off Chinese Spelling Check Task systems, we use the same metrics [Wu et al. 2013] to evaluate our sys-tem. The metrics for the error detection subtask are defined as follows.  X  False-alarm rate (FAR): # of sentences with false positive errors / # of testing sen- X  Detection accuracy (DA): # of sentences with correctly detected results / # of all  X  Detection precision (DP): # of sentences with correctly detected errors / # of sen- X  Detection recall (DR): # of sentences with correctly detected errors / # of testing  X  Detection F1 (DF1): 2*DP*DR / (DP + DR)  X  Error location accuracy (ELA): # of sentences with correct location detection / # of  X  Error location precision (ELP): # of sentences with correct error locations / # of  X  Error location recall (ELR): # of sentences with correct error locations / # of testing  X  Error location F1 (ELF1): 2* ELP*ELR / (ELP + ELR) The metrics for the error correction subtask are defined as follows.  X  Location accuracy (LA): # of sentences with correct location detection / # of all test- X  Correction accuracy (CA): # of sentences with correctly corrected errors / # of all  X  Correction precision (CP): # of sentences with correctly corrected errors / # of sen-
In the spelling error detection task (subtask1), as shown in Table V, Sinica-CKIP had the best performance; our WLD + ETR system further raised the error detection score to 0.79.

For error location detection, as shown in Table VI, our Sinica-CKIP system perfor-mance fell to third place, because the error location detection metrics demand that all errors are correctly marked in a sentence when calculating the score; this puts high-recall systems at a disadvantage. However, the proposed WLD-ETR system still achieves the best detection performance.

For the error correction task, as shown in Table VII, Sinica-CKIP made the second place in correction accuracy and third place in correction precision. Our proposed ap-proach made great progress in correction precision.
 As mentioned in Section 1, the difficulty in correcting Chinese spelling errors derives mainly from two Chinese language features: the lack of word boundaries, and the large character set, which contains more than ten thousand characters. These features not only affect substitution errors, but also present great difficulties for insertion, dele-tion, and transposition errors. In the previous sections, we proposed a noisy channel model to improve the performance of correcting substitution errors by addressing the mentioned difficulties. Recall though that in Chinese, insertions, deletions, and trans-positions are considered grammatical errors, since all Chinese characters may also be considered words. However grammatical error correction is out of the scope of this article. We do believe, however, that the proposed model can be extended to correct the other three error types. The crucial considerations when extending the model are (1) How to take different error types into account while generating the word lattice, so that the correct result is one of the alternatives; and (2) In the decoding model, how to adjust the probabilities of the different error types for the model to avoid promoting shorter sentences. For example, the model might consider a modifier of a noun phrase to be an insertion error, since after removing the modifier it yields a valid shorter sen-tence, resulting in shorter sentences tending to be assigned higher probabilities than the original longer sentences. To avoid this favoring of shorter sentences by the lan-guage model, the principle is to multiply the same number of n-gram probability terms for each candidate sentence; that is, in the case of insertion errors, the probability of the candidate sentence should include an additional deletion probability factor, or we should maximize the perplexity of each candidate sentence instead of the generation probability. We present some examples that explain the extension of word lattices and the adjustment of error models.

For insertion errors, take  X  X  X  X  X  X  X  X  where  X  error, we construct a word lattice that includes the empty string  X  as a candidate for  X 
In this example, we view every character as a potentially redundant word, where  X  nodes indicate the empty string. When the decoder selects an  X  node, it is judging the corresponding character to be a redundant word.

For deletion errors, take  X  X  X  X  X  X  X  X  X   X  X  X  Figure 12, to resolve this deletion error, we construct a candidate list that includes  X  ( huan ) when building the word lattice.

In this example, we did not consider the possibility deletion errors occurring for ev-ery character, because the large size of the Chinese character set would produce long candidate lists for each character. The most practical method is to take into consider-ation only bound morphemes when addressing deletion errors. For example,  X  rarely used as a single word: this is a clue that helps to detect the deletion error.
Finally, for transposition errors, we take  X  X  X  X  X  X  X  X  X  X  as an example, where  X  X  X  Figure 13 shows, we construct a word lattice that includes the correctly-ordered as a candidate.
 In this example,  X  X  X  ( yi-zheng-ci-yan  X  X ternly X ) are combined into new compounds after correcting the char-acter order, and yield new nodes.

When the word lattice contains every possible candidate for each type of error, the next step is to calculate the error probabilities for the decoding model. For substitu-tion errors, we define the probability of c k being miswritten as s model. To extend the model, we redefine the misspelling probability for each of these three error types respectively. For insertion errors, we redefine the misspelling proba-For deletion errors, we redefine the probability as p(  X  | the character c k being mistakenly deleted. For transposition errors, we redefine it as p ( s k + 1 s k | s k s k + 1 ) , denoting the probability of the two characters s in reverse order. These probabilities can be estimated from a spelling error corpus. In this article, we proposed using word lattice decoding to correct Chinese spelling errors. By including all the confusion characters in the word lattice, we prevent the word segmentation errors from propagating misspelling-related errors. As to the im-plementation of the error model, we proposed an innovative method to extract spelling errors from a large web corpus. The extracted spelling error pairs are used both as error template rules and also to estimate the probability of characters in the confusion set. In this study we focused on character substitution errors. We believe that other errors caused by insertions, deletions, and transpositions can also be corrected using this model. In addition, we modified the word lattice decoding of the spell checker to provide n-best suggestions, thus increasing the coverage of correct candidates and al-lowing users to select the right corrections in real-world systems.

We also found that samples of spelling errors can be extracted automatically from a large web corpus. However there are still many confusion characters that do not occur in the extracted spelling error pairs. Therefore, their similarity measures cannot be de-rived from spelling error pairs. We thus consider that in the research on error models, Chinese words are broken down into smaller parts, such as by using the Cangjie code; by using a large spelling error corpus we can calculate the error probability for each part of the word, yielding a more accurate error model. For the language model, we here used only the simplest word-based language model. If part-of-speech information were included in the language model, we believe this would improve the selection of the substitute corrections. There are other possible improvements for the current model. Since derivative words like numerals and named entities result in data sparseness in the language model, word-class or interword language models could prove helpful in future work.
 Finally, we must address the unknown word problem. According to statistics, Chinese unknown words account for about 5% of text. Since these words are not in-cluded in dictionaries and do not appear in corpora, they are easily identified as either unknown or incorrect words. How to prevent them from being regarded as spelling errors is also a challenging research issue.

