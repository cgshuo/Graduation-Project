 We present a generative probabilistic model for learning concept graphs from text. We define a concept graph as a rooted, directed graph where the nodes represent thematic units (called concepts) and the edges represent relationships between concepts. Concept graphs are useful for summarizing document collections and providing a visualization of the thematic content and structure of large document sets -a task that is difficult to accomplish using only keyword search. An example of a concept graph is Wikipedia X  X  category graph 1 . Figure 1 shows a small portion of the Wikipedia category graph rooted at the category M ACHINE LEARNING 2 . From the graph we can quickly in-fer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary algorithms and Markov models with less emphasis on other aspects of machine learning such as Bayesian networks and kernel methods.
 documents where (optionally) we may have concept labels for the documents and an initial graph structure. In the latter scenario, the task is to identify additional concepts in the corpus that are Figure 1: A portion of the Wikipedia category supergraph for the node M ACHINE LEARNING
Figure 2: A portion of the Wikipedia category subgraph rooted at the node M ACHINE LEARNING not reflected in the graph or additional relationships between concepts in the corpus (via the co-occurrence of concepts in documents) that are not reflected in the graph. This is particularly suited for document collections like Wikipedia where the set of articles is changing at such a fast rate that an automatic method for updating the concept graph may be preferable to manual editing or re-learning the hierarchy from scratch. The foundation of our approach is latent Dirichlet allocation (LDA) [1]. LDA is a probabilistic model for automatically identifying topics within a document collection where a topic is a probability distribution over words. The standard LDA model does not include any notion of relationships, or dependence, between topics. In contrast, methods such as the hierarchical topic model (hLDA) [2] learn a set of topics in the form of a tree structure. The restriction to tree structures however is not well suited for large document collections like Wikipedia. Figure 1 gives an example of the highly non-tree like nature of the Wikipedia category graph. The hierarchical Pachinko allocation model (hPAM) [3] is able to learn a set of topics arranged in a fixed-sized graph with a nonparametric version introduced in [4]. The model we propose in this paper is a simpler alternative to hPAM and nonparametric hPAM that can achieve the same flexibility (i.e. learning arbitrary directed acyclic graphs over a possibly infinite number of nodes) within a simpler probabilistic framework. In addition, our model provides a formal mechanism for utilizing labeled data and existing concept graph structures. Other methods for creating concept graphs include the use of techniques such as hierarchical clustering, pattern mining and formal concept analysis to construct ontologies from document collections [5, 6, 7]. Our approach differs in that we utilize a probabilistic framework which enables us (for example) to make inferences about concepts and documents. Our primary novel contribution is the introduction of a flexible probabilistic framework for learning general graph structures from text that is capable of utilizing both unlabeled documents as well as labeled documents and prior knowledge in the form of existing graph structures. In the next section we introduce the stick-breaking distribution and show how it can be used as a prior for graph structures. We then introduce our generative model and explain how it can be adapted for the case where we have an initial graph structure. We derive collapsed Gibbs X  sampling equations for our model and present a series of experiments on simulated and real text data. We compare our performance against hLDA and hPAM as baselines. We conclude with a discussion of the merits and limitations of our approach. Stick-breaking distributions P (  X  ) are discrete probability distributions of the form: from a base distribution H (where H is assumed to be continuous). The stick-breaking weights  X  j have the form where the v j are independent Beta (  X  j , X  j ) random variables. Stick-breaking distributions derive their name from the analogy of repeatedly breaking the remainder of a unit-length stick at a randomly chosen breakpoint. See [8] for more details.
 Unlike the Chinese restaurant process, the stick-breaking process lacks exchangeability. The prob-ability of sampling a particular cluster from P (  X  ) given the sequences { x j } and { v j } is not equal to the probability of sampling the same cluster given a permutation of the sequences { x  X  ( j ) } and { v  X  ( j ) } . This can be seen in Equation 2 where the probability of sampling x j depends upon the value of the j  X  1 proceeding Beta random variables { v 1 ,v 2 ,...,v j  X  1 } . If we fix x j and permute every other atom, then the probability of sampling x j changes: it is now determined by the Beta The stick-breaking distribution can be utilized as a prior distribution on graph structures. We con-governs the probability of transitioning from node t to another node in the graph. There is some freedom in choosing P t ; however we have two constraints. First, making a new transition must have non-zero probability. In Figure 1 it is clear that from M ACHINE L EARNING we should be able to transition to any of its children. However we may discover evidence for passing directly to a leaf node such as S TATISTICAL N ATURAL L ANGUAGE P ROCESSING (e.g. if we observe new articles related to statistical natural language processing that do not use Markov models). Second, making a transition to a new node must have non-zero probability. For example, we may observe new ar-ticles related to the topic of Bioinformatics. In this case, we want to add a new node to the graph (B IOINFORMATICS ) and assign some probability of transitioning to it from other nodes.
 With these two requirements we can now provide a formal definition for P t . We begin with an initial graph structure G 0 with t = 1 ...T nodes. For each node t we define a feasible set F t as the collection of nodes to which t can transition. The feasible set may contain the children of node t or possible child nodes of node t (as discussed above). In general, F t is some subset of the nodes in G 0 . We add a special node called the  X  X xit node X  to F t . If we sample the exit node then we exit from the graph instead of transitioning forward. We define P t as a stick-breaking distribution over the finite set of nodes F t where the remaining probability mass is assigned to an infinite set of new nodes (nodes that exist but have not yet been observed). The exact form of P t is shown below. The first |F t | atoms of the stick-breaking distribution are the feasible nodes f tj  X  X  t . The remaining atoms are unidentifiable nodes that have yet to be observed (denoted as x tj for simplicity). This is not yet a working definition unless we explicitly state which nodes are in the set F t . Our model does not in general assume any specific form for F t . Instead, the user is free to define it as they like. In our experiments, we first assign each node to a unique depth and then define F t as any node at the next lower depth. The choice of F t determines the type of graph structures that can be learned. For the choice of F t used in this paper, edges that traverse multiple depths are not allowed and edges between nodes at the same depth are not allowed. This prevents cycles from forming and allows inference to be performed in a timely manner. More generally, one could extend the definition of F t to include any node at a lower depth. Due to a lack of exchangeability, we must specify the stick-breaking order of the elements in F t . Note that despite the order, the elements of F t always occur before the infinite set of new nodes in the stick-breaking permutation. We use a Metropolis-Hastings sampler proposed by [10] to learn the permutation of feasible nodes with the highest likelihood given the data. Figure 3 shows the generative process for our proposed model, which we refer to as GraphLDA. We observe a collection of documents d = 1 ...D where document d has N d words. As discussed earlier, each node t is associated with a stick-breaking prior P t . In addition, we associate with each node a multinomial distribution  X  t over words in the fashion of topic models.
 A two-stage process is used to generate document d . First, a path through the graph is sampled is sampled from P p di (  X  ) which is the stick-breaking distribution at the i th node in the path. This process continues until an exit node is sampled. Then for each word x i a level in the path, l di , is sampled from a truncated discrete distribution. The word x i is generated by the topic at level l di of the path p d which we denote as p d [ l di ] . In the case where we observe labeled documents and an initial graph structure the paths for document d is restricted to end at the concept label of document d .
 One possible option for the length distribution is a multinomial distribution over levels. We take a different approach and instead use a parametric smooth form. The motivation is to constrain the length distribution to have the same general functional form across documents (in contrast to the rel-atively unconstrained multinomial), but to allow the parameters of the distribution to be document-specific. We considered two simple options: Geometric and Poisson (both truncated to the number of possible levels). In initial experiments the Geometric performed better than the Poisson, so the Geometric was used in all experiments reported in this paper. If word x di has level l di = 0 then the word is generated by the topic at the last node on the path and successive levels correspond to earlier nodes in the path. In the case of labeled documents, this matches our belief that a majority of words in the document should be assigned to the concept label itself. collapsed Gibbs sampler [9] to infer the path assignment p d for each document, the level distribution parameter  X  d for each document, and the level assignment l di for each word. Of the five hyper-parameters in the model, inference is sensitive to the value of  X  and  X  so we place an Exponential prior on both and use a Metropolis-Hastings sampler to learn the best setting. 4.1 Sampling Paths For each document, we must sample a path p d conditioned on all other paths p  X  d , the level variables, and the word tokens. We only consider paths whose length is greater than or equal to the maximum level of the words in the document. The first term in Equation 1 is the probability of all words in the document given the path p d . We compute this probability by marginalizing over the topic distributions  X  t : We use  X  d to denote the length of path p d . The notation N p word type v has been assigned to node p d [ l ] . The superscript  X  d means we first decrement the count N The second term is the conditional probability of the path p d given all other paths p  X  d . We present the sampling equation under the assumption that there is a maximum number of nodes M allowed at each level. We first consider the probability of sampling a single edge in the path from a node x to one of its feasible nodes { y 1 ,y 2 ,...,y M } where the node y 1 has the first position in the stick-breaking permutation, y 2 has the second position, y 3 the third and so on.
 We denote the number of paths that have gone from x to y i as N ( x,y paths that have gone from x to a node with a strictly higher position in the stick-breaking distribution If y m is the last node with a nonzero count N ( x,y probability of transitioning to y i , for i  X  m , and the probability of transitioning to any node higher than y m . The probability of transitioning to a node higher than y m is given by sampling a node higher than y m when M is equal to infinity. Now that we have computed the probability of a single edge, we can compute the probability of an entire path p d : 4.2 Sampling Levels For the i th word in the d th document we must sample a level l di conditioned on all other levels l  X  di , the document paths, the level parameters  X  , and the word tokens. the probability of the level l di given the level parameter  X  d . 4.3 Sampling  X  Variables Finally, we must sample the level distribution  X  d conditioned on the rest of the level parameters  X   X  d , the level variables, and the word tokens. (c) Learned Graph (250 labeled documents) Due to the normalization constant (1  X  (1  X   X  d )  X  d +1 ) , Equation 2 is not a recognizable probability distribution and we must use rejection sampling. Since the first term in Equation 2 is always less than or equal to 1 , the sampling distribution is dominated by a Beta ( a,b ) distribution. According to the rejection sampling algorithm, we sample a candidate value for  X  d from Beta( a , b ) and either 4.4 Metropolis Hastings for Stick-Breaking Permutations In addition to the Gibbs sampling, we employ a Metropolis Hastings sampler presented in [10] to mix over stick-breaking permutations. Consider a node x with feasible nodes { y 1 ,y 2 ,...,y M } . We sample two feasible nodes y i and y j from a uniform distribution 3 . Assume y i comes before y j in the stick-breaking distribution. Then the probability of swapping the position of nodes y i and y j is given by ment, we propose one swap for each node in the graph. In this section, we present experiments performed on both simulated and real text data. We compare the performance of GraphLDA against hPAM and hLDA. 5.1 Simulated Text Data In this section, we illustrate how the performance of GraphLDA improves as the fraction of labeled data increases. Figure 4(a) shows a simulated concept graph with 10 nodes drawn according to the stick-breaking generative process with parameter values  X  = . 025 ,  X  = 10 ,  X  = 10 , a = 2 and b = 5 . The vocabulary size is 1 , 000 words and we generate 4 , 000 documents with 250 words each. Each edge in the graph is labeled with the number of paths that traverse it.
 Figures 4(b)-(d) show the learned graph structures as the fraction of labeled data increases from 0 labeled and 4 , 000 unlabeled documents to all 4 , 000 documents being labeled. In addition to labeling the edges, we label each node based upon the similarity of the learned topic at the node to the topics of the original graph structure. The Gibbs sampler is initialized to a root node when there is no labeled data. With labeled data, the Gibbs sampler is initialized with the correct placement of nodes to levels. The sampler does not observe the edge structure of the graph nor the correct number of nodes at each level (i.e. the sampler may add additional nodes). With no labeled data, the sampler is unable to recover the relationship between concepts 8 and 10 (due to the relatively small number of documents that contain words from both concepts). With 250 labeled documents, the sampler is able to learn the correct placement of both nodes 8 and 10 (although the topics contain some noise). 5.2 Wikipedia Articles In this section, we compare the performance of GraphLDA to hPAM and hLDA on a set of 518 machine-learning articles taken from Wikipedia. The input to each model is only the article text. All models are restricted to learning a three-level hierarchical structure. For both GraphLDA and hPAM, the number of nodes at each level was set to 25 . For GraphLDA, the parameters were fixed at  X  = 1 , a = 1 and b = 1 . The parameters  X  and  X  were initialized to 1 and . 001 respectively and optimized using a Metropolis Hastings sampler. We used the MALLET toolkit implementation of hPAM 4 and hLDA [11]. For hPAM, we used different settings for the topic hyperparameter  X  = ( . 001 ,. 01 ,. 1) . For hLDA we set  X  = . 1 and considered  X  = ( . 1 , 1 , 10) where  X  is the smoothing parameter for the Chinese restaurant process and  X  = ( . 1 , 1 , 10) where  X  is the smoothing over levels in the graph. All models were run for 9 , 000 iterations to ensure burn-in and samples were taken every 100 it-erations thereafter, for a total of 10 , 000 iterations. The performance of each model was evaluated on a hold-out set consisting of 20% of the articles using both empirical likelihood and the left-to-right evaluation algorithm (see Sections 4 . 1 and 4 . 5 of [12]) which are measures of generalization to unseen data. For both GraphLDA and hLDA we use the distribution over paths that was learned during training to compute the per-word log likelihood. For hPAM we compute the MLE estimate of the Dirichlet hyperparameters for both the distribution over super-topics and the distributions over sub-topics from the training documents. Table 5.2 shows the per-word log-likelihood for each model averaged over the ten samples. GraphLDA is competitive when computing the empirical log likeli-hood. We speculate that GraphLDA X  X  lower performance in terms of left-to-right log-likelihood is due to our choice of the geometric distribution over levels (and our choice to position the geomet-ric distribution at the last node of the path) and that a more flexible approach could result in better performance.
 Figure 5: Wikipedia graph structure with additional machine learning abstracts. The edge widths correspond to the probability of the edge in the graph 5.3 Wikipedia Articles with a Graph Structure In our final experiment we illustrate how GraphLDA can be used to update an existing category graph. We use the aforementioned 518 machine-learning Wikipedia articles, along with their cat-egory labels, to learn topic distributions for each node in Figure 1. The sampler is initialized with the correct placement of nodes and each document is initialized to a random path from the root to its category label. After 2 , 000 iterations, we fix the path assignments for the Wikipedia articles and introduce a new set of documents. We use a collection of 400 machine learning abstracts from the International Conference on Machine Learning (ICML). We sample paths for the new collec-tion of documents keeping the paths from the Wikipedia articles fixed. The sampler was allowed to add new nodes to each level to explain any new concepts that occurred in the ICML text set. Figure 5 illustrates a portion of the final graph structure. The nodes in bold are the original nodes from the Wikipedia category graph. The results show that the model is capable of augmenting an existing concept graph with new concepts (e.g. clustering, support vector machines (SVMs), etc.) and learning meaningful relationships (e.g. boosting/ensembles are on the same path as the concepts for SVMs and neural networks). Motivated by the increasing availability of large-scale structured collections of documents such as Wikipedia, we have presented a flexible non-parametric Bayesian framework for learning concept graphs from text. The proposed approach can combine unlabeled data with prior knowledge in the form of labeled documents and existing graph structures. Extensions such as allowing the model to handle multiple paths per document are likely to be worth pursuing. In this paper we did not discuss scalability to large graphs which is likely to be an important issue in practice. Computing the probability of every path during sampling, where the number of graphs is a product over the number of nodes at each level, is a computational bottleneck in the current inference algorithm and will not scale. Approximate inference methods that can address this issue should be quite useful in this context. This material is based upon work supported in part by the National Science Foundation under Award Number IIS-0083489, by a Microsoft Scholarship (AC), and by a Google Faculty Research award (PS). The authors would also like to thank Ian Porteous and Alex Ihler for useful discussions. [1] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of Machine [2] David M. Blei, Thomas L. Griffiths, and Michael I. Jordan. The nested chinese restaurant [3] David Mimno, Wei Li, and Andrew McCallum. mixtures of hierarchical topics with pachinko [4] Wei Li, David Blei, and Andrew McCallum. Nonparametric bayes pachinko allocation. In [5] Blaz Fortuna, Marko Grobelnki, and Dunja Mladenic. Ontogen: Semi-automatic ontology [6] S. Bloehdorn, P. Cimiano, and A. Hotho. Learning ontologies to improve text clustering and [7] P. Cimiano, A. Hotho, and S. Staab. Learning concept hierarchies from text using formal [8] Hemant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors. [9] Tom Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the Natl. Academy [10] Ian Porteous, Alex Ihler, Padhraic Smyth, and Max Welling. Gibbs sampling for coupled [11] Andrew Kachites McCallum. Mallet: A machine learning for language toolkit. [12] Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation meth-
