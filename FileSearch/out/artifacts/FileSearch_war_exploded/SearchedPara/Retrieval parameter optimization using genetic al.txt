 Sumio Fujita * 1. Introduction
TextRetrievalConference(TREC)( Harman,1995 )orNACSIS-NIITestCollectionforInformationRetrievalSystems(NTCIR)(Kan-thetop1000documentsretrievedbythesystem.Forasetofabout50topics,themeanofeachaverageprecision(MAP)isutilized poses. Typically, the retrieval procedure against a search topic in such experiments comprises the following three stages. (1) Query construction from the topic descriptions provided
Recently, automatic query construction has usually been adopted, in which terms are extracted from natural language texts following morphosyntactic analysis. For Japanese, in particular, word-based indexing and n -gram-based indexing achieve comparable effectiveness, but word-based indexing is usually adopted because word-indexed terms make sense for both the machine and human users. This stage is not critical even in Japanese because simple short unit words in Japa-nese achieve the best effectiveness in many cases (Fujita, 1999 ). (2) Relevance computation by a scoring function
A scoring function computes a relevance score between the query representation and the indexed document representa-ther elaboration of the scoring function is unlikely, given that performance improvement seems to have plateaued in ad hoc search tasks against static text document collections. A new scoring paradigm, namely the language model-based approach recently proposed by Lafferty and Zhai (2001) , does not greatly outperform more orthodox approaches. Even in Japanese re-abilistic models when properly calibrated for the task (Fujita, 2005b, 2007 ). (3) (Pseudo-)relevance feedback
For automatic search tasks, pseudo-relevance feedback is a frequently used technique, achieving as much as a 20% tasks in recent TRECs and NTCIRs.

In our previous NTCIR-4 and NTCIR-5 cross-language information retrieval: Japanese to Japanese (CLIR-J-J) experiments, choosing the BM25TF IDF retrieval method for official submissions was found to be successful, in comparison with other participating groups. Decisions are taken based on presubmission experiments using the test collections of the previous
NTCIR. The scoring functions have coefficient parameters that are determined during the presubmission experiments. Failing
Therefore, calibration of parameters becomes a major element of presubmission experiments for the NTCIR tasks. These coef-ficient parameters make the scoring function adaptable to diverse environments, which compensates for the effort of recal-ibration. When using a pseudo-relevance feedback strategy, effectiveness is more sensitive to feedback-specific parameters to find a theoretically sound approach to estimating the best feedback parameters. The sensitivity of feedback parameters and feedback effectiveness to test collections was studied in Fujita (2005a) .

Given the four available Japanese test collections in the NTCIR-3 to NTCIR-6 CLIR tasks, we aim to investigate whether automatic calibration can achieve the same effectiveness as human calibration with a limited number of training examples.
We consider the calibration process as a general optimization problem and adopt a problem-independent approach, using genetic algorithms (GAs) to optimize the parameters for the given test collections. The following difficulties may arise:  X  The optimization process may terminate at local maximum points and fail to find the global maximum.  X  The optimized parameters might be overfitted to the training collection and therefore might not perform well for other collections.

In adopting GAs, we face mainly the second of these issues. In this paper, we present our experiments on a Japanese monolingual retrieval task, focusing on the possibility of automatic calibration of search parameters.
The remainder of the paper is organized as follows. Section 2 introduces prior research on related issues. Section 3 de-scribes our experimental environment and retrieval system, with Section 4 describing the NTCIR test collections. Section 5 briefly explains the GA. Section 6 reports our baseline experiments, and Section 7 presents the GA optimization in runs of CLIR-J-J tasks for NTCIR-3 to NTCIR-6. Section 8 concludes the paper. 2. Related work
Optimization of scoring functions has been studied as part of several regression approaches to information retrieval (IR) probability of the document relevance given term attributes such as document term frequency or inverse document fre-quency. Optimum coefficients of each term attribute are estimated by a least-squares method or a maximum likelihood esti-mate from training collections. While the regression models approximate the probability of the relevance of a document using GAs.

The application of GAs to IR has a long history: Raghavan and Birchard (1979) applied GAs to the clustering problem in IR, and Gordon (1988) applied them to document index modification. Yang and Korfhage (1994) optimized query weights in relevance feedback by using GAs. Vrajitoru (1998) adopted Gordon X  X  model and introduced improved crossover operations.
Our approach is close to that of Fan et al. (Fan, Gordon, and Pathak (2004); Fan, Luo, Wang, Xi, and Fox (2004)), but we optimize the feedback parameters at the same time, aiming to achieve a MAP comparable with the best official runs in NTC-
IRs. Some researchers have applied discriminative models to learn ranking functions, but the reported effectiveness is much poorer than the TREC best official runs (Nallapati, 2004 ). More recently, de Almeida, Goncalves, Cristo, and Calado (2007) reported on genetic programming (GP)-based ranking function discovery experiments and presented optimization results using the TREC-8 collection, but the results are much worse than the best official ad hoc runs of TREC-8 ( Kwok, Grunfeld, algorithm trains a ranking function from query X  X ocument pairs provided with graded relevance judgments. 3. System description
Our evaluation environment for retrieval experiments is the YLMS system, based on the Lemur toolkit 4.0 for indexing systems ( Ogilvie &amp; Callan, 2002 ), which is being developed by the Lemur project. 3.1. Indexing language
The Chasen version 2.2.9 Japanese morphological analyzer with IPADIC dictionary version 2.5.1 was utilized for Japanese text segmentation, and single output words were used as indexing units. Stop word lists for newspaper documentation were prepared. 3.2. BM25TF IDF
The retrieval status value (RSV) between a document d and a query q is calculated as the dot product between the doc-ument term vector and the query term vector, where each term is weighted by the term frequency-inverse document fre-&amp; Gatford, 1995 ) is used.
 3.3. Feedback strategies
The strategy of  X  X  X eedback from the top k documents from pilot search results X  is applied. The Rocchio feedback for TF IDF is adopted as the term extraction method, where the term precision measure to select salient terms is calculated as an ele-ment of the centroid vector of pseudo-relevant documents (Rocchio, 1971 ). Finally, an updated query vector Q is computed from the original query vector Q and a set of (pseudo-)relevant document vectors R .

Instead of using a linear mean to average the feedback document vectors, we use an alpha average, which was introduced by the information geometry society to mixture probability distributions (Amari, 2005 ). Alpha averaging is also used by an feature vectors. The model is parameterized by a real number a as follows: Setting a to 1 makes this approach equivalent to a linear mean.

An alpha average function ranges from the minimum value (alpha is infinity) among given values to the maximum value (alpha is minus infinity) according to the alpha parameter. The minimum value is the most pessimistic expectation against the set whereas the maximum value is the most optimistic expectation. Alpha averaging is understood as a type of fuzzy averaging operation that considers subjective human attitudes such as pessimism and optimism. 3.4. Parameter calibration
In our previous experiences in NTCIR-4 and NTCIR-5 (Fujita, 2004, 2005a ), where we compared BM25TF IDF with the KL-divergence approach proposed by Lafferty and Zhai (2001) , we found that the BM25TF IDF scoring method was usually a better function for achieving effectiveness against diverse test collections. However, some of the coefficients used must tively. This makes the query TF function approximate a simple count of in-text term occurrences. Feedback effectiveness is also subject to the feedback parameters, namely the number of feedback documents (fbDocCnt), the number of feedback terms (fbTermCnt), and the feedback term coefficient (fbPosCoeff), as shown in Fujita (2005a) . 4. NTCIR test collections
The NTCIR (Kando, 2004 ) project group has organized a series of workshops featuring  X  X  X istributed experiments with cen-tralized evaluation X  every 18 months. They provide reusable test collections to workshop participants (and to other researchers) for empirical research experiments on diverse information access technologies. These test collections comprise several types of document, including scientific paper abstracts, newspapers, and Web and patent documents, mainly in Jap-anese, but with some in English, Chinese, and Korean. Here, we report on studies using the NTCIR-3 to NTCIR-6 CLIR-J-J test collections, which are Japanese newspaper collections intended for use in Japanese monolingual search subtasks as well as for cross-lingual retrieval tasks. In this paper, we consider only Japanese monolingual usage of these test collections. 4.1. NTCIR CLIR-J-J test collections
The NTCIR-3 CLIR-J-J test collection comprises 220,078 documents from the Mainichi newspaper for 1998 X 1999, and 42 from the Yomiuri newspaper were added to the NTCIR-3 CLIR-J-J collection, and 55 search topics with assessed document lists were provided.

Each topic has four fields: TITLE, DESC, NARR and CONC (Kishida et al., 2004 ). The TITLE field comprises a few words, and ing the conditions for documents to be relevant, and the CONC field contains some words or phrases describing the principal concepts of the information needs.

The NTCIR-5 CLIR-J-J test collection uses Mainichi newspaper articles from 2000 to 2001 (199,681 documents) and Yomi-3 and NTCIR-4 ( Kishida et al., 2007 ). Fig. 1 shows a typical topic description.
  X  X  X onrelevant (C) X . Using these relevance levels, two types of relevance judgment files are provided. S or A documents are marked as relevant in  X  X  X igid X  relevance judgment files, whereas S, A, or B documents are marked as relevant in  X  X  X elaxed X  relevance judgment files. In this paper,  X  X  X igid X  relevance judgment files are used unless otherwise noted. 5. GAs for IR
GAs ( Goldberg, 1989;Holland,1992 ) are applied to IR systems to optimizedocumentrepresentation, as proposed in Gordon feedback, as has been noted by Lopez-Pujalte, Guerrero-Bote, and de Moya-Anegon (2003) . On the other hand, Fan, Gordon, et al. (2004) have proposed learning ranking functions directly by applying GP, which is an extension of the GA approach.
Our approach is close to that of Fan, Gordon et al., but we decided to use a well-known scoring function (BM25TF IDF) as the basic scoring function and to optimize parameters rather than learn the scoring function from scratch. Unlike Fan, Gor-don et al., we optimized feedback parameters as well, because these parameters are difficult to estimate in a theoretically
However, from intensive feedback experiments (Fujita, 2005a, 2007 ), we have concluded that the tuning of feedback param-eters has more impact on final effectiveness than prefeedback tuning. We will revisit this problem in Section 6. Another problem with the GP approach is that it generates extremely complicated formulae when applied to real-scale problems
On the other hand, the outputs of our approach, i.e., optimized parameters for well-known functions, can be understood and modified by human experts. 5.1. GAs at a glance
Using the metaphor of organic reproductive systems, GAs have been applied to optimization problems in diverse do-mains, by considering each trial point in the search space as an individual. Individuals to be examined are generated by applying genetic operations to each chromosome, representative of an individual, in which parameters to generate a partic-ular individual are encoded. Given a population of individuals, genetic operations are applied iteratively to produce a new terminates when a targeted fitness value is achieved or a predefined number of generations is processed. In addition to the traditional genetic operations of selection, crossover, and mutation, the distributed GA (Tanese, 1989 ) adopts a migration ation moving a certain number of individuals to another island, as described in Hiroyasu, Yoshida, Sano, Fukunaga, and Kataura (2002) . We used their implementation, with Fig. 2 showing an approximate sketch of our GA procedure. In Fig. 2 , P ( t ) denotes the population of the t th generation.
 5.2. GA process and operations
Unlike many applications of GAs to IR in the literature, we do not encode term vectors directly as chromosomes. Instead, real numbers are encoded into a 45-bit binary string using the ranges shown in Table 1 . We carried out GA optimization on an eight-node cluster of Xeon 3.00 GHz Dual CPU machines running the Free BSD operating system. The process was divided into eight islands with each island containing 10 individuals. Initial populations were randomly generated, and the opera-tions described below were applied sequentially in each island. These operations were executed independently for each is-land except for the migration operation, which synchronously moves migrant individuals using a ring topology of islands.
We stopped the learning process at the 20th generation because further improvement is not observed after approxi-mately this number of iterations. Furthermore, it takes 20 X 50 h of computation for 20 generations of learning, depending on the training set size. This threshold seems a reasonable limit for the computation costs. 5.2.1. Migration
The migration operation makes a random cycle through the islands at each operation, as shown in Fig. 3 . It moves ran-the migration rate against the subpopulation. The migration operation takes place synchronously at each generation through all islands.
 5.2.2. Crossover
We used a two-point crossover operator with a crossover ratio of 1.0. This takes pairs of individuals, chooses two posi-tions randomly and exchanges the selected part between the members as follows: Individual a: 0101|1100|1000 ) 010101101000.
 Individual b: 0111|0110|1100 ) 011111001100.
 randomly selected. 5.2.3. Mutation
The mutation operation consists of reversing randomly chosen bits of all individuals according to the mutation rate. A mutation rate of 1/chromosome length is used, and one bit for each individual is reversed.

Individual a: 0101|1|1001000 ) 0101|0|1001000. 5.2.4. Evaluation by fitness function
Each individual is evaluated according to the fitness function, namely the MAP that the retrieval system using the de-coded parameters achieved against the training collection. As we are trying to maximize the MAP against test collections, it is natural to adopt this fitness function. However, there may be other strategies, such as combining several measures to avoid overfitting to the training collection. 5.2.5. Elitism
For the specified number of elites, the elite group of the previous generation and the same number of the best-fitted indi-makes the current population temporarily larger than the population size. 5.2.6. Selection
As recommended by Hiroyasu et al. (2002) , we used tournament selection with a tournament size of four. This process new tournament and repeats until the indicated size of population has been put into the next generation. 6. Baseline experiments
The details of the NTCIR-3 to NTCIR-6 CLIR tasks are described in Chen et al. (2002), Kishida et al. (2004, 2005, 2007), respectively. The J-J subtask is a simple Japanese monolingual retrieval using Japanese topic descriptions and newspaper document collections. In this paper, we focus on the runs of automatic query construction from the title fields of topic descriptions, which are the so-called title-only runs in the J-J subtask. 6.1. NTCIR CLIR-J-J baseline experiments
The NTCIR-3 to NTCIR-6 CLIR-J-J test collections are used in each NTCIR workshop. In the NTCIR-6 workshop, CLIR task lections. These submissions are intended to evaluate current system performance in comparison with old systems. Table 2 shows the effectiveness of the title-only runs against NTCIR-3 to NTCIR-5, i.e., the old test collections, and the
All runs use the TF IDF method with BM25 TF and Rocchio feedback with a top-k -documents strategy. The parameters for the models shown in Table 3 were calibrated by using the NTCIR-5 CLIR-J-J test collection at the time of the NTCIR-6 submission.

Table 4 shows the MAP for our previous official runs in NTCIR-4 to NTCIR-5, in comparison with the best-performing runs runs, i.e., NTCIR-3 as training for NTCIR-4 submission and NTCIR-4 as training for NTCIR-5 submission, whereas the NTCIR-5 collection is used for runs in Table 2 . We did not participate in the NTCIR-3 workshop, and only the best-performing MAP-relax is published in the Proceedings.

MAP
We adopted the same indexing language and the same scoring function for all the runs shown in Tables 2 and 4 , and therefore the performance differences are caused by the selected parameters. In the following subsections, we give some examples of the sensitivity of the effectiveness to these parameters.
In the following experiments, we used the runs in Table 4 as the baseline. We devoted our best efforts to tuning these runs ipating runs in the workshops. Therefore, they make very strong baselines for our experiments with GA optimization. 6.2. Sensitivity of effectiveness to k1 and b
Fig. 6 shows the MAP distributions for different k 1 and b parameter values against four test collections when no feedback total, we examined 176 runs by changing only these two parameters. The MAP is consistently good when b is set to 0.2, which adjusts the impact of the document length normalization. Because all test collections are from newspapers, low b val-no local maximum point was found, although the effectiveness was greatly affected by parameter tuning. We fixed k 4 to 1.0 here, as a neutral value.

Fig. 7 shows the same MAP distributions when the pseudo-feedback strategy is applied. The same search space as in Fig. 6 is examined. Here, we can see that the BM25 parameters are not only document collection dependent but also query depen-dent. Consequently, with expanded long queries, the best parameter settings are completely different. The MAP peaks when b is between 0.4 and 0.5 and k 1 is between 0.4 and 1.4. The 3D surface is much more irregular, and there seem to be some local maximum points. As Fan, Luo, et al. (2004) claim, when the strategy of tuning before feedback is adopted, the optimal parameters are completely different after feedback. Furthermore, the MAP 3D surface is much peakier than before the feed-
Fig. 6 . For the feedback parameters, the following settings are used: fbDocCnt = 9, fbTermCnt = 70, fbPosCoeff = 0.5, and fbAlpha = 1. 6.3. Sensitivity of feedback effectiveness to test collection characteristics
Fig. 8 shows the MAP distribution of the runs where the two feedback parameters are varied, namely fbDocCnt: 10 X 100 at interval 10, fbTermCnt: 20 X 200 at interval 20. Here, 110 runs are examined. We noticed that the pseudo-relevance feedback very high for the larger region, whereas in other test collections, there are some peaky areas. Consequently, we can expect that it would be difficult to optimize these feedback parameters manually with test collections other than NTCIR-5. Other parameters are as follows: k 1 = 1.1, b = 0.4, k 4 = 1.5, fbPosCoeff = 0.5, and fbAlpha = 1.

Fig. 9 illustrates a situation in which higher fbPosCoeff parameter values give better results. Because the runs in Table 2 were calibrated to the NTCIR-5 CLIR-J-J, the fbPosCoeff was adjusted to 0.8, a comparatively high value. In other runs, the
MAP value peaks when the fbPosCoeff is 0.1 X 0.6. The situation is quite different from that for NTCIR-5 CLIR-J-J. As we can see from the MAP distributions shown, the most difficult aspects of retrieval parameter optimization are those for feedback parameters, in addition to the basic scoring parameters.

From Fig. 10 , we can see that any value other than 1.0 for fbAlpha causes degradation in NTCIR-6 CLIR-J-J. For the fol-lowing experiments, we fixed the fbAlpha parameter to 1.0. 6.4. Computational problems of grid search strategies
In previous subsections, we tried simple grid searches in the space of two parameters. When optimizing six parameters at the same time, using a six-dimensional grid search, two grid searches are combined. Moreover, other pairs of parameters with 10 different values for each may be included. The size of the search space accounts for 1,936,000 combinations even for such a coarse grid search. It is not realistic to do an exhaustive grid search given the times necessary to evaluate a to convergence to local maxima. They may not be applicable to feedback parameter optimization because of the existence of local maxima as well as plateaus.

Among other reasons, we chose GAs because they are able to achieve near-globally optimal solutions, although they are computationally more expensive than local search algorithms.
 7. Parameter optimization by GAs
To evaluate parameter optimization by GAs, we carried out two experiments using the same test collections. First, we 7.1. Parameter optimization for NTCIR-3 to NTCIR-6 runs
We tried to reoptimize the parameters of the title runs of NTCIR CLIR-J-J by applying a distributed GA using the eight-node cluster servers described above. The NTCIR-3 to NTCIR-6 CLIR-J-J test collections (N3, N4, N5, N6) were used for cross-validation. Each of these test collections and a combination of N3, N4, and N5 were used for training. Table 6 shows the MAPs for runs where coefficient parameters were optimized by the training collection. Table 7 shows the optimized parameter set for each training collection. In our GA-optimized run with N5, the fbPosCoeff parameter was adjusted to 1.6953125, whereas it was 0.7890625 in the GA-optimized runs for N3, N4, and N5. This is consistent with our observation difficult to interpret, or GP approaches that generate extremely complicated formulae, optimized parameters can be inter-preted by referring to our experiences in previous empirical experiments (see Table 8 ).

In the column for N5, when we use N5 itself for training, a MAP as high as 0.4525 is achieved, whereas the use of N4 for training gives a MAP of 0.3616, which is much worse than our best official title run in NTCIR-5, where a MAP of 0.4193 was achieved. In effect, this NTCIR-5 official run was manually calibrated with N4 (Fujita, 2005a ).

GA optimization processes seem to achieve close to maximum fitness against the training collection. The problem is rather overfitting to the training collection. Human calibrators do much more than simply seeking the best fitness setting in the search space; they carefully avoid overfitting problems. For example, we avoid giving extreme values to sensitive parameters. We prefer to choose the median of the current best parameter and the previous best-performing parameters both Tables 6 and 9.

Consequently, the final fitness values correspond to the self-training runs of Table 6 (see Table 10 ). 7.2. GA optimization without feedback such as the number of terms/documents to use for feedback and the feedback coefficient, which are presumably dependent on the document collection or query. 7.3. Optimization costs
As can be seen from Table 7 , the GA-optimized N6 runs are very close to our official title-only run, which was tuned man-ually. The GA optimization using N5 took 36 h for 20 generations on the eight-node cluster described above. One possible way to execute such optimizations in real operating environments is to use servers at night, when they are less busy. This ation because no further improvement was observed, even at the 100th generation. 7.4. Validation sets
In the next step, we tried another learning strategy using a validation set. We used N3 as the training set, N4 as the val-
MAP against the validation set. In operating environments, the trained parameter set that achieves the best MAP against the validation set is naturally used.

With feedback, the overfitting problem is alleviated so well that the best validation setting achieves the best MAP against the N5 run in Table 2 (MAP = 0.4464) is better than this validation run (MAP = 0.4126), because the former run is optimized manually to the N5 set itself.

The test run with N6 (MAP = 0.3221) performs better than the baseline run (MAP = 0.3182), although the difference is also were used as training and validation sets, although the target document collections and relevance judgments of N6 are com-training sets.

Table 12 shows the same experimental trials but without pseudo-feedback. Without feedback, the best validation run achieves the best MAP against the N6 test set but not against N5. The differences between learned parameter sets are rather small in this setting. The best validation fitness is 0.3189, whereas the worst is 0.3184 (only a 0.1% improvement). With no-feedback optimization, the validation set is not required, whereas it is effective for feedback optimization.
Tables 13 and 14 show similar experiments to those in Tables 11 and 12 except for the training collection and one of the test collections being interchanged.

With feedback, the best training setting achieves the best MAP against the N4, N3, and N6 test sets. The best validation
N6). Table 14 shows that, consistent with Table 12 , validation is not necessary for no-feedback runs because the MAP improvement of the best run over the worst is less than 1.0%.
 Fig. 12 (left) shows the means of MAPs over 10 runs of N3 with training as in Tables 11 and 12 , as well as feedback gain. data, it achieves not only a better MAP but also a larger feedback gain. This is a consequence of feedback parameters being more collection sensitive than BM25 parameters, as has been noted in the previous section. Fig. 13 shows that the standard function of the learning. It is much larger when feedback is applied than without feedback because with feedback, the num-ber of parameters is doubled and the optimization task becomes more complex. When the test collection has different char-the test set and vice versa.
 Table 15 shows the mean values and standard deviations of parameters over 10 trials of GA training in Tables 11 and 13 .
The two sample means are different at a statistically significant level ( p = 0.01) only for feedback parameters such as fbDocCnt, fbTermCnt and fbPosCoeff.
 This clearly illustrates that the feedback parameters are more sensitive to the choice of the training collection than the
BM25 parameters. It is reasonable to choose document collections of the same genre as a training collection, such as N3 and N5 being similar newspaper document collections.

Despite such similarity, feedback parameters and effectiveness depend on each test collection. Remember that using a the training set.
 8. Conclusions We proposed a retrieval parameter optimization approach using GAs and reported evaluation results using the NTCIR val model. GA-optimized runs achieved almost the same effectiveness as human-calibrated runs, although the GA runs were somewhat overfitted to the training collections when feedback parameters were optimized. Automatic calibration also illus-trates the different characteristics of the test collections, especially feedback effectiveness. These optimized parameters could be utilized to help experts in system calibration. We pointed out also that the retrieval parameter optimization be-comes much more difficult when feedback is applied, and overfitting is problematic only when feedback is applied. The over-applied to the fully automatic calibration of retrieval parameters.

In future work, we intend to compare GA optimization performance with other parameter-optimization and scoring-function learning approaches such as the discriminative model-based approach. Less time-consuming local search strategies for optimization, such as the steepest-descent approach with parallelism extensions, will also be examined. On the other rather than using complete-relevance judgments.
 Acknowledgements We thank NTCIR management, secretariat members, and CLIR task organizers for providing the data to us. References
