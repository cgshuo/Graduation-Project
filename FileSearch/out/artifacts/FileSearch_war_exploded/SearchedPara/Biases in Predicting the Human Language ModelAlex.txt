 Computational linguists build statistical language models for aiding in natural language processing (NLP) tasks. Computational psycholinguists build such models to aid in their study of human lan-guage processing. Errors in NLP are measured with tools like precision and recall, while errors in psycholinguistics are defined as failures to model a target phenomenon.

In the current study, we exploit errors of the lat-ter variety X  X ailure of a language model to predict human performance X  X o investigate bias across several frequently used corpora in computational linguistics. The human data is revealing because it trades on the fact that human language process-ing is probability-sensitive : language processing reflects implicit knowledge of probabilities com-puted over linguistic units (e.g., words). For ex-ample, the amount of time required to read a word varies as a function of how predictable that word is (McDonald and Shillcock, 2003). Thus, failure of a language model to predict human performance reveals a mismatch between the language model and the human language model, i.e., bias.

Psycholinguists have known for some time that the ability of a corpus to explain behavior depends on properties of the corpus and the subjects (cf. Balota et al. (2004)). We extend that line of work by directly analyzing and quantifying this bias, and by linking the results to methodological con-cerns in both NLP and psycholinguistics.

Specifically, we predict human data from three widely used psycholinguistic experimental paradigms X  X exical decision, word naming, and picture naming X  X sing unigram frequency esti-mates from Google n-grams (Brants and Franz, 2006), Switchboard (Godfrey et al., 1992), spoken and written English portions of CELEX (Baayen et al., 1995), and spoken and written portions of the British National Corpus (BNC Consor-tium, 2007). While we find comparable overall fits of the behavioral data from all corpora un-der consideration, our analyses also reveal spe-cific domain biases. For example, Google n-grams overestimates the ease with which humans will process words related to the web ( tech , code , search , site ), while the Switchboard corpus X  X  collection of informal telephone conversations be-tween strangers X  X verestimates how quickly hu-mans will react to colloquialisms ( heck, darn ) and backchannels ( wow, right ). 7 2.1 Data Pairwise Pearson correlation coefficients for log frequency were computed for all corpora under consideration. Significant correlations were found between log frequency estimates for all pairs (Fig-ure 1). Intuitive biases are apparent in the corre-lations, e.g.: BNCw correlates heavily with BNCs (0.91), but less with SWBD (0.79), while BNCs 2.2 Approach We ask whether domain biases manifest as sys-tematic errors in predicting human behavior. Log unigram frequency estimates were derived from each corpus and used to predict reaction times (RTs) from three experiments employing lexical decision (time required by subjects to correctly identify a string of letters as a word of English (Balota et al., 1999)); word naming (time required to read aloud a visually presented word (Spieler and Balota, 1997); (Balota and Spieler, 1998)); and picture naming (time required to say a pic-ture X  X  name (Bates et al., 2003)). Previous work has shown that more frequent words lead to faster RTs. These three measures provide a strong test for the biases present in these corpora, as they span written and spoken lexical comprehension and production.

To compare the predictive strength of log fre-quency estimates from each corpus, we fit mixed effects regression models to the data from each experiment. As controls, all models included (1) mean log bigram frequency for each word, (2) word category (noun, verb, etc.), (3) log mor-phological family size (number of inflectional and derivational morphological family members), (4) number of synonyms, and (5) the first principal component of a host of orthographic and phono-logical features capturing neighborhood effects (type and token counts of orthographic and phono-logical neighbors as well as forward and backward inconsistent words; (Baayen et al., 2006)). Mod-els of lexical decision and word naming included random intercepts of participant age to adjust for differences in mean RTs between old (mean age = 72) vs. young (mean age = 23) subjects, given differences between younger vs. older adults X  pro-cessing speed (cf. (Ramscar et al., 2014)). (All participants in the picture naming study were col-lege students.) 2.3 Results For each of the six panels corresponding to fre-quency estimates from a corpus A , Figure 2 gives the  X  2 value resulting from the log-likelihood ra-tio of (1) a model containing A and an estimate from one of the five remaining corpora (given on the x axis) and (2) a model containing just the cor-pus indicated on the x axis. Thus, for each panel, each bar in Figure 2 shows the explanatory power of estimates from the corpus given at the top of the panel after controlling for estimates from each of the other corpora.

Model fits reveal intuitive, previously undocu-mented biases in the ability of each corpus to pre-dict human data. For example, corpora of British English tend to explain relatively little after con-8 trolling for other British corpora in modeling lexi-cal decision RTs (yellow). Similarly, Switchboard provides relatively little explanatory power over the other corpora in predicting picture naming RTs (blue bars), possibly because highly image-able nouns and verbs frequent in everyday interac-tions are underrepresented in telephone conversa-tions between people with no common visual ex-perience. In other words, idiosyncratic facts about the topics, dialects, etc. represented in each cor-pus lead to systematic patterns in how well each corpus can predict human data relative to the oth-ers. In some cases, the predictive value of one corpus after controlling for another X  X pparently for reasons related to genre, dialect X  X an be quite large (cf. the  X  2 difference between a model with both Google and Switchboard frequency estimates compared to one with only Switchboard [top right yellow bar]).

In addition to comparing the overall predictive power of the corpora, we examined the words for which behavioral predictions derived from the corpora deviated most from the observed behav-ior (word frequencies strongly over-or under-estimated by each corpora). First, in Table 2 we give the ten words with the greatest relative differ-ence in frequency for each corpus pair. For exam-ple, fife is deemed more frequent according to the
These results suggest that particular corpora may be genre-biased in systematic ways. For in-stance, Google appears to be biased towards termi-nology dealing with adult material and technology. Similarly, BNCw is biased, relative to Google, to-wards Britishisms. For these words in the BNC and Google, we examined errors in predicted lexi-cal decision times. Figure 3 plots errors in the lin-ear model X  X  prediction of RTs for older (top) and younger (bottom) subjects.

The figure shows a positive correlation between how large the difference is between the lexical de-cision RT predicted by the model and the actu-ally observed RT, and how over-estimated the log frequency of that word is in the BNC relative to Google (left panel) or in Google relative to the BNC (right panel). The left panel shows that BNC produces a much greater estimate of the log fre-quency of the word lee relative to Google, which leads the model to predict a lower RT for this word than is observed (i.e., the error is positive; though note that the error is less severe for older relative to younger subjects). By contrast, the asymmetry be-tween the two corpora in the estimated frequency of sir is less severe, so the observed RT deviates less from the predicted RT. In the right panel, we see that Google assigns a much greater estimate of log frequency to the word tech than the BNC, which leads a model predicting RTs from Google-derived frequency estimates to predict a far lower RT for this word than observed. Researchers in computational linguistics often as-sume that more data is always better than less data (Banko and Brill, 2001). This is true in-sofar as larger corpora allow computational lin-guists to generate less noisy estimates of the av-erage language experience of the users of compu-tational linguistics applications. However, corpus size does not necessarily eliminate certain types of biases in estimates of human linguistic experience, as demonstrated in Figure 3.

Our analyses reveal that 6 commonly used cor-pora fail to reflect the human language model in various ways related to dialect, modality, and other properties of each corpus. Our results point to a type of bias in commonly used language mod-els that has been previously overlooked. This bias may limit the effectiveness of NLP algorithms in-tended to generalize to a linguistic domains whose statistical properties are generated by humans.
For psycholinguists these results support an im-portant methodological point: while each corpus presents systematic biases in how well it predicts human behavior, all six corpora are, on the whole, of comparable predictive value and, specifically, the results suggest that the web performs as well as traditional instruments in predicting behavior. This has two implications for psycholinguistic re-search. First, as argued by researchers such as Lew (2009), given the size of the Web compared to other corpora, research focusing on low-frequency linguistic events X  X r requiring knowledge of the distributional characteristics of varied contexts X  is now more tractable. Second, the viability of the web in predicting behavior opens up possibil-ities for computational psycholinguistic research in languages for which no corpora exist (i.e., most 9 languages). This furthers the arguments of the  X  X he web as corpus X  community (Kilgarriff and Grefen-stette, 2003) with respect to psycholinguistics.
Finally, combining multiple sources of fre-quency estimates is one way researchers may be able to reduce the prediction bias from any sin-gle corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum  X  e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collec-tions for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the tar-get in the construction of such a corpus? Con-cretely, can we build corpora by optimizing an ob-jective measure that minimizes error in predicting human reaction times? Prior work in building bal-anced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Fran-cis, 1967)), or simply sampled text evenly across genres (e.g., COCA: the Corpus of Contemporary American English (Davies, 2009)). Just as lan-guage models have been used to predict reading grade-level of documents (Collins-Thompson and Callan, 2004), human language models could be used to predict the appropriateness of a document for inclusion in an  X  X utomatically balanced X  cor-pus. We have shown intuitive, domain-specific biases in the prediction of human behavioral measures via corpora of various genres. While some psy-cholinguists have previously acknowledged that different corpora carry different predictive power, this is the first work to our knowledge to system-atically document these biases across a range of corpora, and to relate these predictive errors to do-main bias, a pressing issue in the NLP community. With these results in hand, future work may now consider the automatic construction of a  X  X rop-erly X  balanced text collection, such as originally desired by the creators of the Brown corpus. The authors wish to thank three anonymous ACL reviewers for helpful feedback. This research was supported by a DARPA award (FA8750-13-2-0017) and NSF grant IIS-0916599 to BVD, NSF IIS-1150028 CAREER Award and Alfred P. Sloan Fellowship to TFJ, and an NSF Graduate Research Fellowship to ABF.
