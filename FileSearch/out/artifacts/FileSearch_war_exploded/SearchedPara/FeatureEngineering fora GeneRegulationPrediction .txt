 SIGKDD Exploration s . Volume 4 , Issue 2  X  page 106 This paper describes an app roach that won hono rable mention for the gene regulation p rediction task of the 2002 KDD Cup competiti on [1]. Our meth odo logy used extensive c ross -validation to d irect t he search for an app rop riate prob lem representation and the selection o f an  X  X ff -the -shelf X  indu ction algorithm. A prominent trait of the dataset is the presence of three hierarchical attribu tes, for eac h of which we generated a no vel predictive feature: t he percentage of po sitives hierarchically aggregated at the nod e specified b y the instance. Machine learning, hierarchical attribu tes, bioinformatics . In con trast with most of the ma chine learning benchmark datasets, much o f the c hallenge of this year X  X  KDD Cup prediction con test was in d etermining ho w best to represent the available data  X  as is often the ca se with company -internal prediction tasks we face in the Data Mining g roup at H P Labs. A companion p aper in this issue [1] describes the c ompetiti on , the a vailable data, and the c ompetitors X  r esults. Due to space limitation s, we must assume the reader is familiar with the task. The pu rpo se of this paper is to explain on e of the a pp ro aches that achieved hono rable mention . The take -away messages beyond the c ompetiti on itself includ e the methodo logy employed and an app roach for r epresenting no minal attribu tes having a hierarchical relation ship amon g the values.
 Section 2 d escribes the me thodo logy and ph ilosoph y guiding the work. Section 3 presents ou r feature e ngineering. Section 4 lists the final cho ices we made. Section 5 summ arizes. Since the c on test score is the sum of the performance on the narrow and b road prediction tasks, we op timized for eac h ind epend ently.
 In o rder to facilit ate a n efficient search for the best prediction mod el, we leveraged a Perl software framework we had previou sly develop ed. It enables us to focus ou r effort on qu ickly prototyping a variety of f eature e ngineering op tion s. It provides an automated p rocess to p erform feature selection and indu ction p erformance testing using stratified cross -validation . Rather than measuring acc uracy, we e xtend ed it t o evaluate the same performance meas ure used to jud ge the c on test: area und er the ROC curve. Using this framework and fou r 600 -800 MHz CPUs, we were a ble to qu ickly measure the performance for many con figuration s. Indu ction algorithms that we trialed includ ed the WEKA op en -sou rce implementat ion s of Na X ve Bayes, linear kernel Suppo rt Vector Machines (SVM), and AdaBoo sted d ecision stumps [5] . The feature selection method s trialed were Information Gain, Bi -Normal Separation [2] and variants. Beca use the framework provides for feature sele ction (within eac h cross -validation fold), we need no t t roub le ou rselves with manu ally eliminating useless features, bu t on ly with d esigning features that may be predictive, as discussed in the next section .
 To con sider the c ross -produ ct of the e ntire desi gn space is no t feasible, ho wever, automated testing of po rtion s of the space gives much more visibilit y of the search terrain to the person guiding the e xploration . Cross -validation h elps mitigate, bu t canno t eliminate, the po ssibilit y of overfitti ng the data. Likewise, althou gh SVMs are popu larly tou ted for their theoretical guarantees against overfitti ng [4], their wide margin on ly exists in a feature space that is arbitrarily malleable when reformulating the prob lem representation [6]. Non etheless, we attempted to op timize a verage performance on stratified cross -validation samples of the a vailable training data.
 The guidance drawn from cross -validation testing is con strained somewhat by large variance. For example, in the narrow task with just 38 po sit ive e xamples, a stratified 10 -fold split yields just 3.8 po sitives on average in the testing fold, leading to wide variance in the performance e stimation . To compensate, we perform a large nu mber of rando mized trials (e.g. 20 -100 as in boo tstrapp ing), rath er than 10 as in tradition al cross -validation . We simplified management of the voluminou s and d istribu ted performance data by app end ing all results to a single database table, columns capturing all parameters of the test cond ition s. We c ou ld easily determ ine leading con figuration s with ou r Perl too ls or with interactive pivot charts in Excel. Next we discuss ou r feature e ngineering from the three hierarchical attribu tes, the interaction graph , the textual abstracts, and the gene names themselves.
 Hierarchical Attribu tes: Given a no minal attribu te who se values can b e hiera rchically aggregated with a kno wn tree, a natural representation is to generate a binary feature for eac h nod e in the tree  X  set to  X 1 X  on ly on the path to the c urrent value. (We treat the many missing v alues as a separate top -level nod e.) We generated 494 s uch features, and many were predictive, bu t we were dissatisfied with ho w dispersed the information is in the large hierarchies. Only a few nod es con tained po sitives at all. To remedy this, we e ngineered add ition al hierarchy prevalenc e features for eac h h i erarchical attribu te. Option al step 1 : We prun ed away nod es for which there a re no instances in the con test X  X  testing set, which we refer to as tran sdu ctive prun ing . It is in the spirit of tran sdu ctive learning [4], which focuses the mod eling task on the s pecific e xamples to b e labeled and do es no t use information from the a nswer key. This eliminated 200 + nod es. Step 2 : We generated two p ercentage -valued features: given the a ttribu te value, we return the percentage prevalence of po sitives (vs po sitives &amp; ne gatives) found in the lowest and highest nod e in the path o f the prun ed tree. For example, a gene with fun ction attribu te= X  X RNA synthesis X  wou ld h ave the value 1/41 for the low attribu te, and 2 /80 for the high attribu te, since there a re just two po sitives ou t of 80 genes at or und er the top -level nod e  X  X ranscription  X . A value of 0% ind icates that no SIGKDD Exploration s . Volume 4 , Issue 2  X  page 107 po sitives ever had the c urrent attribu te value. Withou t t his feature, it wou ld b e much h arder ( more data) for an indu ction algorithm to learn to  X  or X  together the 230 mutually exclusive  X  X un ction  X  nod es that con tain no po sitive e xamples.
 Interaction Graph : The und irected interaction graph lists associated genes. We generated a simple integer feature that ind icates for a given instance, the nu mber of genes it i nt eracts with. This was a stron g predictor for the broad task.
 A natural feature e ngineering app roach for this s ort of interaction information is to dup licate the feature set, cop ying the features for the gene(s) it i nteracts with. Since the motivation bo ils do wn to a hun ch that t he prediction o f the a ssociation helps predict the instance (as in relation al learning ), and since in this con test the set of ultimate test instances is s mall and fixed, it seemed more straightforward to generate a single add ition al feature that mod eled this assumption d irectly. So, we generated just t wo add ition al i nteger features ind icating the nu mber of narrow and b road po sitives the gene interacts with (determined on ly from tho se kno wn po sitive in the c urrent training s plit). This features proved less valuable, yet somewhat predictive.
 Textual Abstracts: For eac h gene, we c on catenated all pertinent abstracts as determined b y gene -abstracts.txt, and generated a binary feature for eac h un iqu e (lowercased, alph anu meric) wor d. Having to load/process the 18 MB of abstracts increased the run time from 7 second s to 50 second s, and , un fortun ately, tend ed to d egrade prediction acc uracy overall.
 Gene Names: We suppo sed that the naming of the genes, e.g. YMR228 W, was generated b y a non -rando m process that may have some bearing on the prediction tasks at hand . We generated a nu meric feature for the nu mber, and b inary features for variou s sub -sequ ences of characters. As it t urns ou t, for the narrow task, the second and third characters together are (negatively) predictive:  X  X l X  is amon g the stron gest predictors (app earing in 0 po sitives and 158 n egatives), followed b y  X  X r X  and  X  X l X . These were no t very stron g predictors overall, so we do no t believe they leaked information from the a ns wer key illegitimately. In the e nd , ou r best average predictor for the narrow task was Na X ve Bayes on just t welve features, includ ing the percent po sitives at t he top and bo ttom of the three unp run ed hierarchies, the three interaction feature s (made binary), and gene name sub strings  X  X l X  and  X  X  X . Its estimated p erformance when training on 90 % of the a vailable training data was 0.67 ROC area, and when trained on 100 % of the data, 0.6731 on the con test test set.
 Our best average predictor for th e broad task was Na X ve Bayes on 48 features, includ ing the top and bo ttom hierarchical prevalence features for the three tran sdu ctively prun ed hierarchies, the three interaction features (bu t no t made binary) plus binary ind icators for several of the hiera rchy nod es, e.g. localization =transpo rt vesicles / golgi ER transpo rt vesicles, fun ction =classification no t yet clear cut, fun ction =ce ll rescue defense a nd virulence, and p rotein class=protein pho sph atases / catalytic subun its / PP 2C family. The latter is a po sitive predictor with two po sitives and two n egatives, bu t most features were negative predictors. Its estimated p erformance on 90 % of the training data was 0.59 , and 0 .6295 in the c on test. While we e xpect some variance, it i s difficult t o estimate ho w much, given that we ca n generate on ly highly correlated samples. By plotting the learning curve a s we vary the percentage of training data, and extrapo lating the performance for % of the training data, we e stimated an add ition al ~+.007 ROC area for bo th tasks in the final con test. After seeing the low ROC scores, we wished to validate the c ompetiti on following [3]. A rando mized d istribu tion analysis of all t he c on testants X  scores validated that they are significantly better than a rando m collection o f simple c lassifiers. No amoun t of clever indu ction o r feature selection can make up for a lack of predictive features in the inpu t. Hence, feature engineering is a key step for difficult prediction tasks. We estimate via a c ross -va lidation lesion stud y that creative feature engineering was respon sible for add ing +11 % to the performance (+0.08 and +0.06 ROC area for narrow and b road tasks).
 Likewise, extensive use of automated cross -validation to guide the search for an effective mod el add ed imm easurable benefit over selecting a single mod el  X  X lind , X  which is the on ly method that can safely be said to avoid o verfitti ng. Even structural risk minimization techn iqu es s uch as SVM [4] canno t safeguard against  X  X verfitti ng X  the features to the dataset [6]. Non etheless, experience sho ws that feature e ngineering is generally a worthwhile risk, and cross -validation h elps mitigate the risk. Perl again p roved an excellent language for qu ick prototyping  X  minimizing hu man p rogramm ing effort rathe r than CPU time. Althou gh C is often cho sen for being faster, the overall runn ing time was very acce ptable a t und er a minu te per data po int on an 800 MHz HP Kayak XU runn ing Linu x  X  despite the inefficiencies on eac h run o f re -parsing 19 + MB of inpu t data, &amp; laun ching a separate Java process runn ing the WEKA machine learning algorithms, comm un icating v ia generating/parsing files. We wish to thank Bin Zhang, Jaap Suermond t, and WEKA. [1] Craven, Mark. The Geno mics of a Signaling Pathway : A [2] Forman, G. An Extensive Empirical Stud y of Feature [3] Forman, G. A Method for Discovering the Insignifi cance [4] Vapn ik, V. The Nature of Statistical Learning Theory, [5] Weka machine learning project, www .cs.waikato.ac.nz/ml [6] Zhang, B. Is the Maximal Margin Hyperplane Special in a George Forman is a research scientist at HP Labs in the Data Mining Group . He rece ived h is CS Ph.D. from the University of 
