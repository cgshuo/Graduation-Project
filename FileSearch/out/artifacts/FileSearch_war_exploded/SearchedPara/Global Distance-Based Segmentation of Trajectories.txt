 This work introduces distance-based criteria for segmenta-tion of object trajectories. Segmentation leads to simplifica-tion of the original objects into smaller, less complex prim-itives that are better suited for storage and retrieval pur-poses. Previous work on trajectory segmentation attacked the problem locally, segmenting separately each trajectory of the database. Therefore, they did not directly optimize the inter-object separability, which is necessary for mining op-erations such as searching, clustering, and classification on large databases. In this paper we analyze the trajectory seg-mentation problem from a global perspective, utilizing data aware distance-based optimization techniques, which opti-mize pairwise distance estimates hence leading to more effi-cient object pruning. We first derive exact solutions of the distance-based formulation. Due to the intractable complex-ity of the exact solution, we present an approximate, greedy solution that exploits forward searching of locally optimal so-lutions. Since the greedy solution also imposes a prohibitive computational cost, we also put forward more lightweight variance-based segmentation techniques, which intelligently  X  X elax X  the pairwise distance only in the areas that affect the least the mining operations.

Despite the dramatic increase in processing power, com-puter systems and networks are still being challenged by the ongoing information avalanche, which necessitates the design of efficient data storage and retrieval mechanisms. Redundant data need to be discarded and potentially useful data can be compressed (simplified), in order to facilitate their efficient visualization, retrieval and processing. In this work, we are attacking the problem of trajectory segmen-tation, that is, the simplification of a multi-dimensional se-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. quence of values into smaller and simpler primitives, which require a significantly reduced memory footprint than the original object.

Most data-mining tasks operate on a compressed data di-mension to speed up operations such as clustering, classifi-cation or Nearest-Neighbor search. The data simplification is performed in a disciplined way, so as to provide certain quality guarantees on the data-mining results (such as ab-sence of misclassification). Those guarantees are typically provided by exploiting lower [2] or upper bounds [15] on the distance between the simplified representations. Ortho-normal dimensionality reductions techniques (such as SVD, Fourier or wavelets) can provide such guarantees.
Therefore, in this work we provide techniques for  X  X impli-fication X  of trajectories, but in a principled manner. We use Minimum Bounding Rectangles (MBRs) as the simplified primitive structures for approximating trajectories, because of their tight integration with existing multidimensional in-dexes in commercial DBMS systems (such as R-trees). Addi-tionally, such a representation has been successfully used in a variety of applications, ranging from signature compres-sion of handwritten image data [15], to storage of motion capture data [4, 11] and even for online anomaly detection in time-series data [5]. The notions and techniques presented in this paper are generic enough, and can be adapted into other approximating functions (such as piecewise linear or polynomial functions) with minimal or no changes whatso-ever.

Figure 1 illustrates the concept of segmentation on a 2-dimensional trajectory, which is approximated with ten Bound-ing Rectangles. Figure 1: Segmentation of a 2D trajectory into bounding rectangles.
Trajectory segmentation is important in numerous fields: 1. In spatio-temporal databases the position of moving 2. In video tracking, motion segmentation [12] is a com-3. Sensor network devices have gained momentum lately 4. In many visualization applications the original data 5. Finally, in video games the approximation of complex Previous approaches that approximate trajectories with MBRs, manipulate each trajectory separately from the re-maining sequences [8], with the objective of minimizing the overlap between MRBs through volume minimization. How-ever, most mining operations are based on distance. Con-sider, for example, the problem of clustering or outlier de-tection. With this observation in mind, we try to design segmentation schemes that will approximate as tightly as possible the inter-trajectory distances. More accurate dis-tance estimates on the data approximations are bound to provide higher quality mining results.

Therefore, instead of examining separately each trajec-tory (like previous techniques do), here we present segmen-tation techniques that collectively utilize information from the whole dataset and attempt to satisfy global, distance-based criteria. The contributions and merits of our work are summarized below: 1. We instigate a study of the global, distance-based seg-2. We present approximate solutions at various compu-3. Finally, we demonstrate with an application the use-
Previous work on trajectory segmentation looked at the problem mainly from a database organization perspective and not from a data-mining point of view. For example [13] examines cost models for evaluating splitting strategies, so as to answer effectively range queries (i.e.,  X  X hich trajecto-ries pass within area X between times t 1 and t 2  X ). However, in this work we are interested in optimizing search opera-tions, that is, given a query Q identify as quickly as pos-sible the k most similar trajectories to Q , which reside in the database. The operations that we consider are heavily distance based, which explains our rationale for maximiz-ing the approximated pairwise distances between trajectory segmentations.

Our work also exhibits similarities with the work of [8], which considers local criteria such as volume minimization, for mitigating the effect of MBR overlap in the index or-ganization. To our best knowledge, this is the first work that looks at the problem of trajectory segmentation using global, distance-based criteria. Figure 2: Illustration of distance-based segmenta-tion.

We consider scenarios where the majority of operations in-volve comparison of trajectories (which is the basis of many data-mining operations, such as clustering, aggregation, and so on), therefore, maintaining tight distance approximations is bound to lead to more accurate results. As already men-tioned, previous approaches incorporate different optimiza-tion criteria and are not suitable for distance preservation. In order to make this observation more lucid, we depict a brief visual comparison between a volume based segmenta-tion technique and a distance-based one. Figure 2 depicts an example where two trajectories are approximated with 5 MBRs. The top figure illustrates the segmentation achieved by a volume-based optimization criterion, which results in a total volume consumption of 76 . 29 and pairwise Euclid-ean distance between the resulting MBRs of 103 . 11. The bottom schematic illustrates the distance-based segmenta-tion, which (while consuming more space) achieves a tighter distance estimate of 125.73 (i.e., it is closer to the original distance between the raw trajectories).

In the sections that follow we introduce global approxima-tion criteria, based on the total pairwise distances between trajectories. Due to the high computational overhead of the exact approach, in Section 4.2 we present a greedy-based approximation. Section 5 presents an even more lightweight distance-based segmentation technique that utilizes variance for identifying fast candidate areas for  X  X elaxing X  the pair-wise distance.
We assume that we are dealing with 2-dimensional tra-jectories of objects that move on the plane. Extensions to higher-order trajectories are straightforward. We proceed by describing the model and giving some formal definitions. The input consists of a collection of trajectories {
T 1 ,...,T n } . Each trajectory T i is a mapping T i : { 0 , 1 ,...,m 1 }  X  X  , from the set of integers { 0 , 1 ,...,m  X  1 } (which correspond to discrete time steps) to the range R .Inour setting, as we mentioned, usually T i corresponds to a 2-dimensional trajectory, so R = R 2 ,thatis, T i ( t )=( x Our results hold for any number of dimensions, while we also consider a discrete range set, single dimensional ( R { 1 , 2 ,...,R } ) or multidimensional.

A given trajectory T i can be approximated by a set of  X  X inimum Bounding Rectangles X  (MBRs) that completely contain the original multi-dimensional sequence on all di-mensions (see Figure 1).

For R = R 2 ,anMBR B j for a trajectory T i is repre-sented as a parallelepiped (rectangle) B j =[ t j b ,t j f [ ,h j y ], and we require that j x  X  x i ( t )  X  h j x and y ( t )  X  h j y for all T i ( t )=( x i ( t ) ,y i ( t )) such that t
Then a segmentation S ( T i ) for a trajectory T i is defined as a set of MBRs, S ( T i )= { B 1 ,B 2 ,...,B r } , and we require that t 1 b =0, t j b  X  t j f for j =1 ,...,r , t j +1 b = t j =1 ,...,r  X  1, and t r f = m  X  1.

For a segmentation S ( T i ) of a trajectory we define P ( S ( T as the projection of the MBR B j  X  S ( T i ), for which t t  X  t j where t j b  X  t  X  t j f . The area of the projection at time t is The volume of a segmentation is
The distance between two points x i , x j  X  R 2 is their Euclid-ean distance: but one can consider any other metric. For example, when we consider trajectories that take values in the discrete range R = { 1 , 2 ,...,R } we use as distance the function d ( x i | x i  X  x j | . We also define the distance between two segmen-tations at time t as the distance between the rectangles at time t . Formally: Finally, the distance between two segmentations is the sum of the distances between them at every time instant: ThedistancebetweenthetrajectoryMBRsisalowerbound (see Figure 3) of the original distance between the raw data, which is an essential property for guaranteeing correctness of results for most mining tasks (such as kNN search). Figure 3: Top : Distance between trajectories. Bottom : Distance between their respective MBRs (shown in 1D for clarity).

One can define a variety of segmentation problems de-pending on the function to be optimized. Given a function f that takes as input a segmentation for each trajectory and returns the  X  X ost X  of the segmentation (for a minimization problem) the corresponding segmentation problem is posed as follows. Find a segmentation S ( T i ) for each trajectory T that minimizes A maximization problem is defined similarly.

Usually the objective is to find segmentations for all the trajectories such that the cost is minimized, given a lim-ited storage capacity (i.e., the total number of rectangles is bounded by some number K ).

Typically, the computation of the global minimum (or maximum) of function f is quite costly, and exact approaches might need time and space that increases exponentially to the input size; in Section 4.1 we see some examples. There-fore, one can resort to heuristic approaches that utilize a forward search of locally optimal solutions whose approxi-mation quality in practice is very close to the optimal solu-tion. The skeleton of such an algorithm is given in Figure 4.
The various greedy segmentation algorithms differentiate themselves at step 2 (optimization function), step 5 (MBR merging) and step 6 (update of statistics). In the next Sec-tion we present one such greedy approach where the opti-mization function is the total distance minimization, while in Section 5 we present another approach where the opti-mization function is based on variance minimization.
The distance-based segmentation criterion attempts to create MBRs in such a way that the original pairwise dis-tances between all trajectories are preserved as well as pos-sible. Therefore, the objective function is reduced to the following maximization problem:
The intuition behind maximizing the pairwise distances is that this would be beneficial for operations such as cluster-ing, or kNN search, since it will provide better inter-object separability.
Exhaustive search on the aforementioned problem is com-putationally prohibitive, as we now show. For simplicity, assume that there are only two trajectories; the extension to multiple trajectories is straightforward. The total number of possible segmentations equals To see why the expression in the left hand side gives the number of segmentations, notice that if the first trajectory contains K 1 rectangles (and each trajectory must have at least one bounding rectangle, so K 1 ranges from 1 to K  X  1), there are m K 1  X  1 ways to select the starting point for all but the first rectangle, and similarly m K  X  K 1  X  1 for the second one. The first equality follows by properties of the binomials. This means that the number of choices that have to be considered grows exponentially with the number of available rectangles K .

A more efficient exact solution is based on dynamic pro-gramming. For conciseness we present our main theorem considering only two trajectories, with a range of values taken from a discrete set R = { 1 , 2 ,...,R } .Theresultscan be extended for multiple trajectories and higher-dimensional spaces, while in the case that the range is continuous we can achieve an approximation by discretizing the range set.
Theorem 1. Let T 1 and T 2 be trajectories of length m , taking values in the range { 1 , 2 ,...,R } . There exists a dy-namic programming algorithm that assigns K MBRs to two trajectories so that the distance between the resulting seg-mentations is maximized. The algorithm computes the opti-mal bounding-rectangle assignment in time O ( KmR 8 ) ,using space O ( KmR 4 ) .
 Proof. Given in the appendix.

This algorithm, although polynomial to the range R and the time m , is very impractical even for two trajectories. Even worse, the straightforward extension for more trajec-tories is exponential in the number of trajectories. Hence, we study approximate solutions to the problem.
We now begin a study of heuristic approaches to the distance-based segmentation problem, which will allow for a more computationally efficient algorithm, at the expense of the quality of the results. The first algorithm that we present is a greedy forward-search technique, which constitutes a vari-ation on the generic forward-search approach as presented in Figure 4.

For concreteness, assume that we try to solve the prob-lem of maximizing the sum of the distances between the segmentations, as presented in Equation (1). A detailed de-scription of the algorithm appears in Figure 5. For ease of exposition we present a simple variation of the algorithm. In practice, this algorithm can be substantially improved in terms of computational cost, as will be discussed shortly.
The algorithm starts by assigning each point of all tra-jectories (for a total of nm points) in its own MBR (lines 2 X 7). Subsequently, as long as the total number of MBRs is larger than K the algorithm merges two consecutive MBRs for any trajectory T i , such that the merge will result in the least change of the total pairwise distance (lines 11 X 26). Af-ter each merge, the pairwise distances on the affected time instances are updated and the process is repeated (line 10).
Theorem 2. Assume that we execute the Forward Search algorithm with n trajectories, m points per trajectory, and K MBRs, where n  X  K  X  nm . Then the time required to execute the algorithm is O ( n 3 m 2 ) .

Proof. First notice that the while loop (lines 10-27) is executed nm  X  K times. Let us now analyze the running time of each iteration. In every iteration, the algorithm ex-amines all the n trajectories ( for loop, line 11). Assume that the algorithm examines trajectory T i and that the cor-responding segmentation is S i = { B 1 i ,B 2 i ,...,B r i 13 X 22, the algorithm attempts to merge each MBR with the one next to it. If r = 1, then there is nothing to do and the algorithm proceeds to the next trajectory. Other-wise it creates the merged MBR (line 14), merging B j i with B i , and calculates the cost difference between the new and the current segmentation (lines 15 X 16). Line 15 can be executed in constant time, while line 16 can be executed in time ( n  X  1)( X  j + X  j +1 ), where  X  j = t j f  X  t j b +1 is the temporal length of the j th MBR. Since every MBR B j i is considered twice X  X nce when it is merged with the previous one and once when it is merged with the next one, except for B 1 i and B r i which are considered only once X  the total time to calculate the cost difference for a given segmentation is ( n  X  1)(2 m  X   X  0  X   X  r &lt; 2 nm ). Therefore each iteration of the for loop of line 11 can be executed in time 2 n 2 m , so the total running time of the algorithm is bounded by ( nm  X  K )2 n 2 m = O ( n 3 m 2 ). Figure 5: Greedy distance-based segmentation algo-rithm.

Notice that the analysis above considers a straightfor-ward, albeit inefficient implementation; our implementation is more efficient. For example, many quantities are being cal-culated multiple times. A more efficient implementation can maintain a priority queue with the cost difference of merg-ing two consecutive MBRs belonging to the same trajectory. When the two MBRs at the top of the queue are merged, one can update only the cost differences of the other MBRs that are affected (the ones that intersect with the merged MBRs in time). Nevertheless, even this simplified analy-sis demonstrates that while the greedy approach achieves a significant computational leap from the exact solution, for practical purposes the usefulness of the greedy algorithm can be quite limited.

Another greedy approach is the backward search: the al-gorithm initially approximates each trajectory in a single MBR and then starts splitting, until it eventually reaches K MBRs, always performing the split that leads to the least distance deterioration. Nevertheless, even though this approach superficially seems less expensive, in practice the bookkeeping costs per iteration render it even more expen-sive than the forward search approach.
Even though the greedy approach is much more efficient than the exact solution, it is still prohibitively expensive when we are dealing with very large datasets and lengthy trajectories. Indicatively, some simulations presented in Sec-tion 6 required several days to run to completion. Therefore, we attempt to develop even faster algorithms. Figure 6: The mean and variance of three one-dimensional trajectories at every time point. Notice that in dense areas the variance is low.

The main reason for the slow performance of the greedy distance-based algorithm is that whenever two consecutive MBRs are merged, a total recomputation of the pairwise dis-tances between all trajectories needs to be performed. One approach for speeding up this algorithm is based on the idea that at heavily clustered locations (i.e., time points where a large number of trajectories concentrate around the same area) one can afford to have coarser trajectory approxima-tions since the total pairwise distance in these areas is very small in practice in the first place. This idea is illustrated in Figure 6.

To identify the heavily clustered time points efficiently one can compute for every time step an empirical mean and variance of the trajectories. Consecutive time instants with similar means and small variances intuitively contain clus-ters of trajectories. After such sequences of time instants have been found, the algorithm can start approximating the trajectories contained therein without substantially affect-ing the overall pairwise distance. In practice, the algorithm identifies the most clustered sequence of time instants, per-forms one merge operation, recomputes the mean and vari-ance of the affected time points after the merge, and con-tinues iteratively with the next candidate. Figure 7 shows a high-level description of the algorithm.

There are several issues that need to be discussed. First, notice that the value of a trajectory at a given time point is a two-dimensional vector. We define the mean and the variance of n elements x i as where x T is the transpose vector of x . Notice that the
Figure 7: Variance-based segmentation algorithm. mean is a two-dimensional vector, while the variance is a real value.

The mean and variance can easily be computed if each tra-jectory is represented by a 1D point for a given time instant. Nevertheless, as trajectories are continually approximated with MBRs, for some time instants a number of trajecto-ries will be represented by line segments (the projection of the MBR on that time instant; see Figure 8). While com-puting the mean n i =1 T i ( t ) /n when there are no rectangles is straightforward, computing the mean and the variance when some of the trajectories contain bounding rectangles at time t must involve both ends of the rectangle. Figure 8: The definition of the mean and the vari-ance have to be generalized to take into account the existence of the MBRs.

So, for the general case, we define the mean estimate to be the two-dimensional quantity and our variance estimate as the real value
After we have computed the means and variances of all the time points, we select a heavily clustered time point (line 7). Our estimator returns a time point t  X  where the means  X  ( t and  X  ( t  X  +1) are close and the variances V ( t  X  )and V ( t are small. In detail, here is how we compute t  X  .Foreach time point, define  X   X  ( t )=  X  ( t )  X   X  ( t +1) . For some constant k (in our experiments we used k  X  X  1 ,..., 50 } ) we compute the set C  X  = { t  X  1 ,...,t  X  k } of candidates, which are the k time points with smallest corresponding  X   X  ( t ). Having computed the set C  X  ,weset t  X  equal to where we denote with ( V ( t  X  i ) ,V ( t  X  i +1)) the two-dimensional vector with elements V ( t  X  i )and V ( t  X  i +1)(seeFigure9). Figure 9: We select the time point t  X  such that both V ( t  X  ) and V ( t  X  +1) are small.

Having selected the time point t  X  , we select at line 7 the trajectory whose MBRs we combine. Ideally we would like to merge the MBRs that will lead to the smallest decrease in the total pairwise distance. This approach, however, suffers from similar efficiency problems with the greedy approach: in the worst case we need to compute O ( mn ) pairwise dis-tances per MBR. A good compromise is to employ a local optimization criterion, namely we select to merge the two MBRs that will lead to a minimal total increase of volume.
As we show in the next theorem, the running time of the variance-based algorithm is much lower than the greedy algorithm.

Theorem 3. Assume that we execute the variance-based algorithm with n trajectories, m points per trajectory, and K MBRs, where n  X  K  X  nm . Then the time required to execute the algorithm is in the worst case O ( m 2 n log n ) .
Proof. The initial computation of the mean and vari-ances takes O ( mn ) steps, while creating a heap with the pairwise differences of the means of consecutive time in-stants, which is useful in determining the denser time points, takes O ( m ) steps. We perform the main loop of the algo-rithm nm  X  K times. The dense area estimation can be com-puted in k log k steps. Choosing which trajectory to merge after the time point has been selected and updating the necessary data structure can be accomplished in O ( m log n ) steps (we keep a sorted list of all the trajectories for each time instance, but in every merge up to m lists might be up-dated resulting in time O ( m log n )), and updating the mean and variance of the affected time points in constant time for the means (we maintain the differences  X   X  ( t )ofthemeans, and only three of those values change), and O ( m )timefor the variances. Finally, updating the list of mean differences requires O (log m ) time. By accumulating all those steps, we conclude that in the worst case the time complexity of the algorithm is O ( m 2 n log n ).
We conduct a performance evaluation of the proposed algorithms using a number of real and synthetic datasets, with the objective of demonstrating the accuracy and per-formance of these techniques in practice. We compare all flavors of distance-based segmentation; the optimal dynamic programming algorithm, the greedy distance-based algorithm, and the variance-based segmentation techniques. We also include in our comparisons the local volume-based MBR segmentation [8, 15]. All algorithms were implemented in C++ and executed on a 3GHZ Pentium 4 with 1GB RAM.
The first experiment evaluates the qualitative affinity of all techniques to the optimal distance-based dynamic pro-gramming algorithm. We capture this by recording the sum of pairwise distance between the simplified trajectories (dis-tance between the final MBRs).

For this experiment we utilize a large dataset pool, since the performance of each technique is highly dependent on the data characteristics. We use a number of real datasets from the UCR time-series data mining archive [9], which span a wide variety of areas, such as computer networks, medicine, environmental measurements, and more. Each dataset consists of 50 sequences with length of 512 points. We note that the datasets used in this experiment are 1D and not 2D. This was necessary in order to keep the running requirements of the optimal dynamic programming segmen-tation algorithm within the limits of the computational and storage capabilities of our computer testbed.

The performance of all strategies against the optimal ap-proach is shown at the top of Figure 10. This figure depicts the results for each dataset separately. The y axis shows the sum of pairwise distances between all the objects, nor-malized by the total pairwise distance of the optimal algo-rithm. Numbers closer to 1 indicate better distance preser-vation. Each pair of sequences was simplified using 100 MBRs ( K = 100). From the plots we can observe that in general the the greedy distance-based preserves closely the sum of pairwise distances achieved by the optimal, while the variance-based preserves a smaller amount of the pair-wise distance sum. This is expected since the variance-based technique introduces many distance simplifications, in order to expedite the running performance. The distance affinity of the local volume-based algorithm stands between the two (non-optimal) distance-based algorithms.

Notice, that the previous experiment captures how close absolute distances are to the optimal algorithm. An even more meaningful experiment for data-mining operations, is how well relative distances are preserved. For example, if for the optimal algorithm the distances between objects A, BandCare d(A,B) = 5 and d(B,C) = 10 this will lead to a distance ratio of 1 / 2. In this setting, any algorithm X pro-viding distance approximations of d(A,B) = 1 and d(B,C) =2 might be weighted more favorably against another al-gorithm Y with respective distances of 3 and 4. Intuitively, the output of algorithm X can provide more similar behavior to the original data for clustering/classification operations, since relative relative object spacing will be affected the least (similar objectives are achieved by data embeddings [3]).
The results for the relative distances are reported at the bottom of Figure 10 and in this experiment numbers ap-proaching zero are closer to the optimal. One can observe that the variance based algorithm depicts better relative dis-tance preservation than the volume-based for 25% of the Figure 10: Affinity of heuristic algorithms to the op-timal dynamic programming solution. Top: Close-ness of absolute distances to optimal (numbers closer to 1 are better). Bottom: Closeness of rel-ative distances to optimal (numbers closer to 0 are better). datasets. These are primarily the datasets that contain mul-tiple data bursts (e.g., earthquake, eeg, packet).
In general, the optimal and greedy distance-based algo-rithms present the best distance preservation. However, the optimal algorithm for any practical purpose is completely infeasible; our experiments for it took several days. The greedy-based runs in the order of minutes or hours and the variance-and volume-based require only seconds, for the data instances of this experiment. Each simplification typ-ically detracts at least an order of magnitude in running time, which was also depicted with the complexity analy-sis of each algorithm. The most efficient algorithms are the local volume-based and the global variance-based. The first one exhibits better approximations for smooth datasets, while the second one is better for  X  X usier X  datasets, where the global dataset view can provide a much better indication of which areas need to be simplified.
Here we focus more closely on the performance of the the variance-based segmentation algorithm which is the only computationally feasible approach, and hence practical for real-world applications. We measure its performance on a k-Nearest-Neighbor (k-NN) retrieval experiment using a larger dataset instance, with synthetic datasets that simulate a road network of moving objects. We create multiple dataset instances with various object cardinalities (500, 2000, 4000 trajectories), where each object has length of 256 points.
The above dataset is utilized for evaluating the efficiency of the algorithm under k-NN search using the Euclidean distance metric. The segmented versions of the trajectories can speed up the search as follows. The trajectory segmen-tations represent a compressed version of the dataset, which essentially prioritize the retrieval of the raw trajectories from disk, by guiding the k-NN search.

Assume that the original trajectories are kept on disk but their simplifications (MBRs) are small enough to be stored in memory. The distance between the MBRs of two trajecto-ries represent a lower bound oftheoriginaldistancebetween the raw trajectories [15]. To identify the nearest neighbor one can compute a lower bound of the distances between the sequence approximations (given by each algorithm) and the query. The raw trajectories are then retrieved from disk in the order suggested by the lower bound (i.e., we examine the most likely candidates first). The best-so-far distance is potentially updated with every raw trajectory retrieved from disk. One can guarantee that the best match to the query is found when the currently examined compressed tra-jectory has a lower bound distance larger the distance of the best-so-far match. The number of retrieved matches from disk is an implementation invariant way of computing the efficiency of each segmentation method.

Note that the above search technique does not introduce any false dismissals, that is, the returned answer set will be exactly the same, as the k-NN search on the raw data. This is guaranteed by the lower bounding properties of the MBR approximation and is in accordance with the GEMINI indexing framework [2].

We perform the above k-NN search experiment using 100 different queries (not already found in the dataset) and in Figure 11 we plot the percentage of raw trajectories that are retrieved when searching for 20-NN, using different tra-jectory approximations (5, 10 and 20 MBRs per sequence). The above measure is an implementation independent way of measuring the efficiency of the representation. The ex-periment suggests that we consistently examine only a very small portion of the dataset, which reduces gracefully with finer trajectory approximations (i.e., use of more MBRs). While k-NN search on the raw data requires access to all trajectories on the disk, using the above simple technique we can prune from examination most of the trajectories. The good k-NN search performance using distance-driven MBRs approximations is attributed to the fact that this type of segmentation essentially relaxes the distance in the already dense areas (small variance), and hence in practice it does not penalize the search performance, which is primar-ily impacted by the areas of large variance [7]. Additionally, we can observe that the preprocessing time required for seg-menting the trajectories is kept in very realistic levels. The MBR generation time is shown in Figure 12 as a function of the dataset size.

We conducted the data-pruning experiment using also the volume-based segmentation technique (which shares the closest computational complexity with the variance based algorithm). The results are almost identical data pruning efficiency. Additional experiments are still needed for ascer-taining whether these results can be generalized for other classes of datasets. Additionally it would be interesting to examine under what circumstances local techniques can be a viable alternative to global ones (see for example compar-ison of APCA vs SVD [10]).

As concluding remarks, with this empirical evaluation we have highlighted a lightweight version of global distance-based segmentation, which comes in the form of the variance-based segmentation. This method achieves significantly lower preprocessing times, while at the same time accomplishing distance relaxation only where needed, therefore not penaliz-ing performance. This flavor of distance-based segmentation depicts excellent pruning power and therefore is suitable for any algorithm that utilizes k-NN search operations. Figure 11: Evaluation of k -nearest neighbor queries. Percentage of retrieved disk resident trajectories for 20-NN search. Figure 12: Segmentation time for K=20 MBRs with respect to dataset size.
We conclude our experimental section with an interesting application and technique, which clearly highlights the im-portance of segmentation for manipulation and visualization of complex data. Specifically, we show how trajectory seg-mentation techniques can be used for visualizing and com-paring the affinity of DNA strings.

Visual comparison of DNA symbol strings can be particu-larly troublesome to perform, because typical DNA datasets contains thousands of symbols. Humans cannot easily com-pare or visually represent bulk of text; our brains are much more efficient at comparing lines or shapes. Therefore, first we provide a technique for converting a DNA string into a two-dimensional trajectory. Given a string of length n drawn from the alphabet A,T,C,G , which we will denote as speciesDNA , we wish to convert it to a two-dimensional vec-tor of length n +1, which we will denote as speciesTrajectory . We can use the following rule to build the trajectory vector: speciesT rajectory ( i )= speciesT rajectory ( i  X  1) + B , where B is a basis vector constructed as follows: Example: Suppose that speciesTrajectory(1) = [0 0]. Thus, for the DNA string AATCG ,wegetthetrajectoryvector { [0 0],[0 1],[0 2],[1 2],[1 1],[0 1] } . tion into 20 MBRs.

The resulting trajectories can be quite long, therefore it is more meaningful to be represented with fewer MBRs, which significantly aids their manipulation or on-screen depiction. Additionally, since the variance-based segmentation can ac-curately capture relative object distances, tasks such as clas-sification, taxonomy categorization and dendrogram visual-ization, are expected to perform very effectively even on the simplified data. We demonstrate this later with an exam-ple. For simplicity we consider only mitochondrial DNA (mtDNA). mtDNA is passed on only from the mother dur-ing sexual reproduction, meaning that the mitochondria are clones. This means that there is little change in the mtDNA from generation to generation (i.e rare mutations), unlike nuclear DNA which changes by 50% each generation. This gives mtDNA a long memory .

Utilizing the mtDNA of 11 species (human, chimp, ele-phant, etc.), we create their respective trajectories using the technique described above and finally we segment each re-sulting trajectory into 20 MBRs using the variance-based al-gorithm. For this datasets the length of each mtDNA string consists of approximately 16000 symbols (with mtDNA of humans being 16,569 symbols long, and all other mammals mtDNA are within plus or minus 1% of this). The final MBR representation of every mtDNA, essentially represents a coarse signature of every symbol string. Notice that not only have we managed to represent the DNA string into a format that is amenable to visualization, but we have effectively compressed our dataset, since we have reduced 16000 points down to 20 MBRs. Every MBR can be repre-sented by 4 numbers (lower and upper 2D points), there-fore we have effectively compressed the original data by (16000 / (20  X  4)) = 200 times.
 After computing the pairwise distance matrix between the MBR representation of all species, we create the (average linkage) dendrogram depicted on Figure 13. The first thing that one can notice is that even though the representation is highly compressed, the dendrogram correctly captures the taxonomy between the different species. A second observa-tion on the figure is that, at first glance, the grouping of the hippo with whales might seem like a mistake. Intu-itively the hippo should depict a greater affinity with the elephants. Interestingly, this is not the case; the hippos are more closely related to whales than to any other mammals! Whales and hippos diverged a mere 54 million years ago, whereas the whale/hippo group parted from the elephants about 105 million years ago. The group that includes hippo and whales/dolphins, but excludes all other mammals above is called Cetartiodactyla [14].

While this figure serves merely as a demonstration of the effectiveness of our MBR approach to capture structure in trajectories, we feel that such a representational transforma-tion, combined with our MBR and other indexing tools and techniques could have great utility for mining large sequence collections. Recall that the full DNA sequence of a human is approximately 3 billion symbols long. Matching substrings, either within or between species is a computationally de-manding task. While we are not suggesting this method to replace sophisticated string alignment methods, it could be used as a initial filtering step for finding promising candidate substrings.
In this work we motivated the need for global distance ori-ented segmentation techniques. We present different flavors of distance-based segmentation algorithms that operate at various scales of computational granularities. We introduce an optimal and a greedy version, and we show analytically and empirically that they are computationally impractical. However, we conclude the paper by presenting a variance-based hybrid variation that can provide an excellent com-promise between running time and approximation quality. Here we describe in detail the dynamic programming algo-rithm of Section 4 and we prove Theorem 1.

The algorithm maintains a matrix A ,where A [ t, k, 1 ,h ] contains the value of the optimal way for assigning exactly k bounding rectangles to the two trajectories for time 0 up to and including time t (for t  X  X  0 , 1 ,...,m  X  1 } , k { 0 , 1 ,...,K } ,and 1 ,h 1 , 2 ,h 2  X  X  1 , 2 ,...,R } ), with the rightmost rectangle of trajectory i being on the range [ i This value is  X  X  X  if there is no valid assignment of bounding rectangles (e.g., when T 1 ( t ) &gt;h 1 ).

We define the distance of two ranges to be:
For the time step 0 we set the entries of A to:  X  A [0 ,k, 1 ,h 1 , 2 ,h 2 ]=  X  X  X  , for k =2;  X  A [0 , 2 , 1 ,h 1 , 2 ,h 2 ]=  X  X  X  , for T 1 (0)  X  [ 1 ,h  X  A [0 , 2 , 1 ,h 1 , 2 ,h 2 ]= d ([ 1 ,h 1 ] , [ 2 ,h 2 ]), for T We now complete the table for increasing values of t .As-sume that we know all the values A [ t  X  1 ,  X  ,  X  ,  X  ,  X  to compute all the values A [ t,  X  ,  X  ,  X  ,  X  ,  X  ]. For T we set A [ t, k, 1 ,h 1 , 2 ,h 2 ]=  X  X  X  , for all k, 2 ,h for T 2 ( t )  X  [ 2 ,h 2 ]weset A [ t, k, 1 ,h 1 , 2 ,h 2 k, 2 ,h 2 .
 Finally, for T 1 ( t )  X  [ 1 ,h 1 ]and T 2 ( t )  X  [ 2 ,h every k, 1 ,h 1 , 2 2 ,h 2 :
A [ t, k, 1 ,h 1 , 2 ,h 2 ]= d ([ 1 ,h 1 ] , [ 2 ,h 2 ])+
The value of the optimal assignment of bounding rectan-gles is given by:
In order to compute the optimal assignment we backtrack on array A , as is typical in dynamic programming.
First we show by induction that the algorithm computes an optimal assignment. Assume that for all t  X  t  X  1, all the ments of bounding rectangles. We show that our algorithm gives the value of the optimal assignment for the entries A [ t,  X  ,  X  ,  X  ,  X  ,  X  ].

Assume that this is not the case and that for some value of k , there is another optimal assignment that gives a higher value. Let V be the value of the assignment produced by our algorithm, and V  X  be that of the optimal assignment. Assume that the coordinates of the rightmost rectangles (at time t ) in the assignment given by our algorithm (the maxi-mum of the assignments of A [ t,  X  ,  X  ,  X  ,  X  ,  X  ]) are first trajectory, and 2 and h 2 for the second, and those cor-responding to the optimal assignment are  X  1 , h  X  1 ,  X  2
Let us focus on the optimal assignment. We look at the assignment of rectangles up to time t  X  1 in that one and assume that it uses k rectangles (since we consider the case where the optimal uses k rectangles we have that k  X  2  X  k  X  k ). By the induction hypothesis, an assignment at least as good as the optimal is the one corresponding to the entry
Since our algorithm examines all those cases it would have found it, so we end with a contradiction. Therefore, even-tually the algorithm finds the optimal assignment. For the space complexity, notice that the size of A is m  X  K  X  R  X  R  X  R  X  R , while for the time complexity, notice that in order to compute each entry of A we need to perform at most O ( R 4 )operations.
