 In the field of population coding, a recurring question is the impact on coding efficiency of so-called noise correlations , i.e. , trial-to-trial covariation of different neurons X  activities due to shared connec-tivity. Noise correlations have been proposed to be either detrimental or beneficial to the quantity of information conveyed by a population [1, 2, 3]. Also, some proposed neural coding schemes, such as those based on synchronous spike waves, fundamentally rely on second-and higher-order correlations in the population spikes [4].
 The problem of noise correlations is made particularly difficult by its high dimensionality along two distinct physical magnitudes: time, and number of neurons. Ideally, one should describe the the population; which is clearly impossible experimentally. As a result, when recording from a population of neurons with a finite number of trials, one only has access to very partial correlation data. First, studies based on experimental data are most often limited to second order (pairwise) correlations. Second, the temporal correlation structure is generally simplified ( e.g. , by assuming stationarity) or forgotten altogether (by studying only correlation in overall spike counts). Third and most importantly, even with modern multi-electrode arrays, one is limited in the number of neurons which can be recorded simultaneously during an experiment. Thus, when data are pooled over experiments involving different neurons, most pairwise noise correlation indices remain unknown. extracted experimentally from a population.
 In contrast to noise-correlation data, the first-order probabilistic data are easily extracted from a pop-ulation: They simply consist in the trial-averaged firing rates of the neurons, generally referred to as their  X  X ignal X . In particular, one can easily measure so-called signal correlations which measure how different neurons X  trial-averaged firing rates covary with changes in the stimulus.
 In this paper, we propose a method to  X  X ill the gaps X  in noise correlation data, based on signal corre-lation data. This approach can be summarized by the notion that  X  X imilar tuning reveals shared in-puts X . Indeed, noise correlations reveal a proximity of connection between neurons (through shared inputs and/or reciprocal connections) which, in turn, will generally result in some covariation of the neurons X  first-order response to stimuli. When browsing through neural pairs in the population, one should thus expect to find a statistical link between their signal-and noise-correlations; and this has indeed been reported several times [5, 6]. If this statistical structure is well described, it can serve as basis to randomly generate noise correlation structures, compatible with the measured signal cor-relation. Furthermore, to assess the impact of this randomness, one can perform repeated picks of potential noise correlation structures, each time observing the resulting impact on the coding capac-ity in the population. Then, this method will provide reliable estimates (average + error bar) of the impact of noise correlations on population coding, given partial noise correlation data. We present this general approach in a simplified setting in Section 2. The input stimulus is a single parameter which can take a finite number of values. The population X  X  response is summarized by a single number for each neuron (its mean firing rate during the trial), so that in turn a correlation structure is simply given by a symmetric, positive, N x N matrix. In Section 3, we detail the method used to generate random noise correlation matrices compatible with the population X  X  signal corre-lation, which we believe to be novel. In Section 4, we apply this procedure to assess the amount of information about the stimulus in the somatosensory cortex of macaques responding to tactile stimulation. Population activity R. We consider a population of N neurons tested over a discrete set of pos-sible stimuli f  X  { f 1 ,...,f K } , lasting for a period of time T . The spike train of neuron i can the number of emitted spikes n i and the spike times t ( i ) k are random variables, whose distribution depends (amongst other things) on the value of stimulus f .
 At each trial, information about f can be extracted from the spike trains S i ( t ) using several possible readout mechanisms. In this article, we limit ourselves to the simplest type of readout: The popu-lation activity is summarized by the N -dimensional vector R = { R i } i =1 ...N , where R i = n i /T is the mean firing rate of neuron i on this trial. A more plausible readout, based on sliding-window estimates of the instantaneous firing rate, has been presented elsewhere [7].
 First-moment measurements. Given a particular stimulus f , we note  X  i ( t,f ) the probability of observing a spike from neuron i at time t regardless of other neurons X  spikes ( i.e. , the first moment is measured fairly easily, as the trial-averaged firing rate of neuron i in stimulus condition f . Since R i = 1 /T P T t =0 S i ( t ) , its expectancy is given by This function of f is generally called the tuning curve of neuron i .
 The trial-averaged firing rates  X  i ( t,f ) can also be used to define the signal correlation matrix  X  = {  X  where b  X  i = 1 / ( KT ) P f,t  X  i ( t,f ) is the overall average firing rate of neuron i across trials and stimuli. The Pearson correlation  X  ij measures how much the first-order responses of neurons i and j  X  X ook alike X , both in their temporal course and across stimuli. Being a correlation matrix,  X  is positive definite, with 1 s on its diagonal, and off-diagonal elements between  X  1 and 1 . As opposed to most studies which define signal correlation only based on tuning curves, it is important for our purpose to also include the time course of response in the measure of signal similarity. Indeed, similar temporal courses are more likely to reveal shared input, and thus possible noise correlation. A model for noise correlations. While first-moment ( X  X ignal X ) statistics can be measured exper-imentally with good precision, second-moment statistics (noise correlations) can never be totally measured in a large population. For this reason a parametric model must be introduced, that will allow us to infer the correlation parameters that could not be measured.
 We introduce a simple model in which the noise correlation matrix  X  is independent of stimulus f : For a given stimulus f , the population activity R is supposed to follow the multivariate Gaussian N (  X  ( f ) , Q ( f )) , with Let us make a few remarks about this model. The first line is imposed by eq. (1). The second line implies that var( R i | f ) = Q ii ( f ) =  X  i ( f ) , meaning that all neurons in this model are supposed to have a Fano factor of one. This model is the simplest possible for our purpose, as its only free parameter is the chosen noise correlation matrix  X  , and it has often been used in the literature [8]. Naturally, the assumption of Gaussianity is a simplifying approximation, as the values for R really come from a discretized spike count. 3.1 Statistical link between signal and noise correlation We propose that, across all pairs ( i,j ) of distinct cells in the population, the noise correlation index is linked to the signal correlation index by the following statistical relationship: where function F (  X  ij ) provides the expected value for  X  ij if  X  ij is known, and c measures the statistical variations of  X  ij across pairs of cells sharing the same signal correlation  X  ij . By extension, we note F (  X  ) the matrix with 1 s on its diagonal, and non-diagonal elements F (  X  ij ) . The choice of F and c is dictated by the experimental data under study. In our case, these are neural recordings in the primary somatosensory cortex (S1) of monkeys responding to a frequency discrimination task (see Section 4). For all pairs ( i,j ) of simultaneously recorded neurons (total of several hundred pairs), we computed the two correlation coefficients (  X  ij , X  ij ) . This allowed us to compute an experimental estimate for the distribution of  X  ij given  X  ij (Figure 1). We find that Figure 1: Statistical link between signal and noise correlations. A : Experimental distribution of (  X  ij ,  X  ij ) across simultaneously recorded neural pairs in population data from cortical area S1 ( dark gray : noise correlation coefficients significantly different from 0). B : Same data transformed into a conditional distribution for  X  ij given  X  ij . Plain ligns: experimental mean ( green ) and error bars ( white ). Dotted ligns: model mean F (  X  ij ) ( red ) and standard deviation c ( yellow ). provides a good fit, with a ' 0 . 6 ,  X  ' 2 . 5 and b ' 0 . 05 . For the standard deviation in eq. (4), we choose c = 0 . 1 . This value is slightly reduced compared to experimental data (Figure 1, white vs. yellow confidence intervals), because part of the variability of  X  ij observed experimentally is due to finite-sample errors in its measurement. We also note that the value found here for a is higher than values generally reported for noise correlations in the literature [2], possibly due to experimental limitations ; however, this has no influence on the method proposed here, only on its quantitative results.
 Once that function F is fitted on the subset of simultaneously recorded neural pairs, we can use the statistical relation (4)-(5) to randomly generate noise correlation matrices  X  for the full neural population, on the basis of its signal correlation matrix  X  . However, such a random generation is not trivial, as one must insure at the same time that individual coefficients  X  ij follow relation (4), and that  X  remains a (positive definite) correlation matrix.
 As a first step towards this generation, note that the  X  X verage X  noise correlation matrix predicted by the model, that is F (  X  ) , is itself a correlation matrix. First, by construction, it has 1 s on the diagonal and all its elements belong to [  X  1 , 1] . Second, F (  X  ) can be written as a Taylor expansion on element-wise powers of  X  (plus diagonal term (1  X  a  X  b )Id ), with only positive coefficients (due to the exponential in eq. (5)). Since the element-wise (or Hadamard ) product of two symmetric semi-definite positive matrices is itself semi-positive definite ( X  X chur X  X  product theorem X  [9]), all matrices in the expansion are semi-definite positive, and so is F (  X  ) . This property is fundamental to apply the method of random matrix generation that we propose now. 3.2 Generating random correlation matrices Wishart and anti-Wishart distributions. The Wishart distribution is probably the most straight-forward way of generating a random symmetric, positive definite matrix with an imposed expectancy matrix. Let  X  be an N x N symmetric definite positive matrix, k an integer giving the number of de-grees of freedom, and introduce the sample covariance matrix of k i.i.d Gaussian samples X i drawn according to N ( 0 ,  X ) :  X  = 1 /k P k i =1 X i X T i . When k  X  N , the matrix  X  has almost surely full-rank. In that case, its pdf has a relatively simple expression, and the distribution for  X  is called the Wishart distribution [10]. When k &lt; N , the matrix  X  is almost surely of rank k , so it is not invertible anymore. In that case, its pdf has a much more intricate expression. This distribution has sometimes been referred to as anti-Wishart distribution [11]. In both cases, the resulting distribution for random matrix  X  , which we note W ( X  ,k ) , can be proven to have the following characteristic function [11]: (where T is a real symmetric matrix). This result can be used to find the two first moments of  X  : with a variance naturally scaling as 1 /k .
 Then, a second step consists in renormalizing  X  by its diagonal elements, to produce a correlation matrix  X  . The resulting distribution for  X  , which we note W ( X  ,k ) , has been studied by Fisher and others [12, 10], and is quite intricate to describe analytically. If one takes the generating matrix  X  = F (  X  ) to be itself a correlation matrix, then E(  X  ) ' F (  X  ) still holds approximately, albeit with a small bias, and the variance of  X  still scales with 1 /k .
 Distribution W ( F (  X  ) ,k ) could be a good candidate to generate a random correlation matrix  X  that would approximately verify E (  X  ) = F (  X  ) . Unfortunately, this method presents a problem in our case. To fit the statistical relation eq. (4), we need the variance of an element  X  ij to be on the order of c 2 ' 0 . 01 . But this implies (through eq. 7) that k must be small (typically, around 20), so that noise correlation matrices  X  generated in this way necessarily have a very low rank (anti-Wishart distribution, Figure 2, blue traces). This creates an artificial feature of the noise correlation structure which is not at all desirable.
 Iterated Wishart. We propose here an alternative method for generating random correlation ma-trices, based on iterative applications of the Wishart distribution. This method allows to create random correlation matrices with a higher variance than a Wishart distribution, while retaining a much wider eigenvalue spectrum than the more simple anti-Wishart distribution.
 The distribution has two positive integer parameters k and m (plus generative matrix F (  X  ) ). It is based on the following recursive procedure: Since E(  X  n ) ' E(  X  n  X  1 ) , one expects approximately E(  X  ) ' F (  X  ) . Furthermore, by taking a large k , one can produce full-rank matrices, circumventing the  X  X ow-rank problem X  of the anti-Wishart distribution. Because k is large, the variance added at each step is small (proportional to 1 /k ), which is compensated by iterating the procedure a large number m of times.
 Simulations allowed us to study the resulting distribution for  X  (Figure 2, red traces) and compare it to the more standard  X  X nti-Wishart-based X  distribution for  X  (Figure 2, blue traces). We used the signal correlation data  X  observed in a 100 -neuron recorded sample from area S1, and the average noise correlation F (  X  ) given by our experimental fit of F in that same area (Figure 1). As a simple investigation into the expectancy and variance of these distributions, we computed the empirical distribution for  X  ij conditionned on  X  ij , for both distributions (Panel A ). On this aspect the two distributions lead to very similar results, with a mean sticking closely to F (  X  ij ) , except for low values of  X  ij where the slight bias, previously mentionned, is observed in both cases. In contrast, the two distributions lead to very different results in term of their spectra (Panel B ). The iterative Wishart, used with a large value of k , preserves a non-null spectrum across all its dimensions. It should be noted, though, that the spectrum is markedly more concentrated on the first eigenvalues than the spectrum of F (  X  ) (dotted line). However, this tendency towards dimensional reduction is much milder than in the anti-Wishart case ! As long as m is sensibly smaller than k , the variances added at each step (of order 1 /k ) simply sum up, so that m/k is the main factor defining the variance of the distribution. For example, in Figure 2: Random generation of noise correlation matrices. N = 100 neurons from our recorded sample (area S1). A : Empirical distribution of noise correlation  X  ij conditioned on signal correlation  X  ij (mean  X  std). B : Empirical distribution of eigenvalue spectrum (mean  X  std in log domain). Figure 2, k/m equals 20 , precisely the number of degrees of freedom in the equivalent anti-Wishart distribution. Also, the eigenvalue spectrum of  X  appears to follow a quasi-perfect exponential decay (even on a trial-by-trial basis), a result for which we have yet no explanation. The theoretical study of the  X  X terated Wishart X  distribution, especially when k and m tend to infinity in a fixed ratio, might yield an interesting new type of distribution for positive symmetric matrices. To illustrate the interest of random noise correlation matrix generation, we come back to our exper-imental data. They consist of neural recordings in the somatosensory cortex of macaques during a two-frequency discrimination task. Two tactile vibrations are successively applied on the fingertips of a monkey. The monkey must then decide which vibration had the higher frequency (the detailed experimental protocol has been described elsewhere). Here, we analyze neural responses to the first presented frequency, in primary somatosensory cortex (S1). Most neurons there have a positive tun-ing (  X  i ( f ) grows with f ) and positive noise correlations ; however, negative tunings (resulting in the appearance of negative signal correlations) and significant negative noise-correlations can also be found (Figure 1-A ).
 In the notations of Section 2, stimulus f is the vibration frequency, which can take K = 5 possible values (14, 18, 22, 26 and 30 Hz). The neural activities R i consist of each neuron X  X  mean firing rate over the duration of the stimulation, with T = 250 ms. Our goal is to estimate the amount of information about stimulus f which can be extracted from a linear readout of neural activities, depending on the number of neurons N in the population. This implies to estimate the impact of noise correlations. We thus generate a random noise correlation structure  X  following the above procedure, and assume the resulting distribution for neural activity R to follow eq. (2)-(3). This being given, one can estimate the sensitivity  X  f of a linear readout of f from R , as we now present. 4.1 Linear stimulus discriminability in a neural population Linear readout from the population. To predict the value of f given R , we resort to a simple one-dimensional linear readout, based on a prediction variable  X  f = P N i =1 a i R i . The set of neural weights a = { a i } i =1 ...N must be chosen in order to maximize the readout performance. We find it through 1-dimensional Linear Discriminant Analysis (LDA), as the direction which maximizes ( a
T Ma ) / ( a T Qa ) , where M is the inter-class covariance matrix of class centroids {  X  ( f ) } f = f and Q = 1 /K P f Q ( f ) is the average intra-class covariance matrix. Then, the norm of a is chosen so as for variable  X  f to be the best possible predictor of stimulus value f , in terms of mean square error. Readout discriminability. The previous procedure produces a prediction variable  X  f which is nor-compute analytically the neurometric curve giving the probability that two successive stimuli be correctly compared by the prediction variable: Finally, a sigmoid can be fit to this curve and provide a single neurometric index  X  f , as half its 25%  X  75% interval.  X  f measures what we call the linear discriminability of stimulus f in this neural population. It provides an estimate of the amount of information about the stimulus linearly present in the population activity R . 4.2 Discriminability curves Discriminability versus population size. The previous paragraphs have described a means to estimate the linear discriminability  X  f of a given neural population, with a given noise correlation structure. We apply this method to estimate  X  f ( N ) in growing populations of size N = 1 , 2 ,... , up to the full recorded neural sample (approx. 100 neurons in S1, Brodmann area 1). For each N ,  X  f ( N ) is computed to approximate the linear discriminability of the best N -tuple population available from our recorded sample. As it is not tractable to test all possible N -tuples, we resort to the following recursive scheme: Search for neuron i 1 with best discriminability, then search for the discriminability curve for the population. Note that this curve is not necessarily decreasing, as the last neurons to be included in the population can actually deteriorate the overall readout, by their influence on the LDA axis a .
 Each draw of a sample noise correlation structure gives rise to a different discriminability curve. To better assess the possible impact of noise correlations, we performed 20 random draws of possible noise correlation structures, each time computing the discriminability curve. This produces an av-erage discrimination curve flanked by a confidence interval modelling our ignorance of the exact full correlation structure in the population (Figure 3, red lines). The confidence interval is found to be rather small. This means that, if our statistical model for the link between signal and noise correlation (4)-(5) is correct, it is possible to assess with good precision the content of information present in a neural population, even with very partial knowledge of its correlation structure. Since the resulting confidence interval on  X  f ( N ) is small, one could assume that the impact of noise correlations is only driven by the  X  X tatistical average X  matrix F (  X  ) . In this particular application, however, this is not the case. When the noise correlation matrix  X  is (deterministically) set equal to F (  X  ) , the resulting linear discriminability is underestimated (blue curve in Figure 3). Indeed, the statistical fluctuations in  X  ij around F (  X  ij ) , of magnitude c ' 0 . 1 , induce an overcorrelation of certain neural pairs, and a decorrelation of other pairs (including a significant minority of negative correlation indices  X  as observed in our data, Figure 1). The net effect of the decorrelated pairs is stronger and improves the overall discriminability in the population as compared to the  X  X tatistical average X .
 In our particular case, the predicted discriminability curve is actually closer to what it would be in a totally decorrelated population (  X  = 0 , green curve). This result is not generic (it depends on the parameter values in this particular example), but it illustrates how noise correlations are not necessarily detrimental to coding efficiency [2], in neural populations with balanced tuning and/or balanced noise correlations (as is the case here, for a minority of cells).
 Comparison with monkey behavior. The measure of discriminability through G ( X ) (eq. 8) mim-ics the two-stimulus comparison which is actually performed by the monkey. And indeed, one can build in the same fashion a psychometric curve for the monkey, describing its behavioral accuracy in comparing correctly f 1 and f 2 across trials, depending on  X  = f 2  X  f 1 . The resulting psychometric index  X  f monkey can then directly be compared with  X  f , to assess the behavioral relevance of the proposed linear readout (Figure 3, black dotted line). In our model, the neurometric discriminability curve crosses the monkey X  X  psychometric index at around N ' 8 . If neurons are assumed to be decorrelated, the crossing occurs at N ' 5 . Using the  X  X tatistical average X  of the noise correlation structure, the monkey X  X  psychometric index is approached around N ' 20 . Figure 3: Discriminability curves for various correlation structures. Neural data: Mean firing rates over T = 250 msec, for N = 100 neurons from our recorded sample (area S1). Green : No noise correlations. Red : Random noise correlation structure (mean+std). Blue : Statistical average of the noise correlation structure. Black : Psychometric index for the monkey.
 These results illustrate a number of important qualitative points. First, a known fact: the chosen noise correlation structure in a model can have a strong impact on the neural readout. Maybe not so known is the fact that considering a simplified,  X  X tatistical average X  of noise correlations may lead to dramatically different results in the estimation of certain quantities such as discriminability. Thus, inferring a noise correlation structure must be done with as much care as possible in sticking to the available structure in the data. We think the method of extrapolation of noise correlation matrices proposed here offers a means to stick closer to the statistical structure (partially) observed in the data, than more simplistic methods.
 Second, a comment must be made on the typical number of neurons required to attain the monkey X  X  behavioral level of performance ( N  X  10 using our extrapolation method for noise correlations). No matter the exact computation and sensory modality, it is a known fact that a few sensory neurons are sufficient to convey as much information about the stimulus as the monkey seems to be using, when their spikes are counted over long periods of time (typically, several hundreds of ms) [13, 14]. This is paradoxical when considering the number of neurons involved, even in such a simple task as that studied here. The simplest explanation to this paradox is that this spike count over several hundreds of milliseconds is not accessible behaviorally to the animal. Most likely, the animal X  X  percept relies on much more instantaneous integrations of its sensory areas X  activities, so that the contributions of many more neurons are required to achieve the animal X  X  level of accuracy. In this optic, we have started to study an alternative type of linear readout from a neural population, based on its instantaneous spiking activity, which we term  X  X nline readout X  [7]. We believe that such an approach, combined with the method proposed here to account for noise correlations with more accuracy, will lead to better approximations of the number of neurons and typical integration times used by the monkey in solving this type of task. We have proposed a new method to account for the noise correlation structure in a neural population, on the basis of partial correlation data. The method is based on the statistical link between signal and noise correlation, which is a reflection of the underlying neural connectivity, and can be estimated through pairwise simultaneous recordings. Noise correlation matrices generated in accordance with this statistical link display robust properties across possible configurations, and thus provide reliable estimates for the impact of noise correlation  X  if, naturally, the statistical model linking signal and noise correlation is accurate enough. We applied this method to estimate the linear discriminability in N -tuples of neurons from area S1 when their spikes are counted over 200 msec. We found that less than 10 neurons can account for the monkey X  X  behavioral accuracy, suggesting that percepts based on full neural populations are likely based on much shorter integration times.
