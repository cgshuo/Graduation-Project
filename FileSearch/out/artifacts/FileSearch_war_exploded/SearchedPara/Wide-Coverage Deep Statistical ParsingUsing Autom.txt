 Dublin City University Dublin City University IBM Center for Advanced Studies Dublin City University Palo Alto Research Center Dublin City University IBM Center for Advanced Studies Dublin City University IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing  X  X eep X  hand-crafted wide-coverage with  X  X hallow X  treebank-and machine-learning-based parsers at the level of dependencies, using simple and automatic methods to convert tree output generated by the sophisticated automatic LFG f-structure annotation methodologies with surprising results. We compare various PCFG and history-based parsers to find a baseline parsing system that fits best into our automatic dependency structure annotation technique. This combined system of syntactic parser and dependency structure annotation is compared to two hand-crafted, deep constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
Our experiments show that machine-learning-based shallow grammars augmented with so-phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant 3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing system. 1. Introduction Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVAL metrics (Black et al. 1991) of labeled and unlabeled bracketing precision, recall and f-score measures, number of crossing brackets, complete matches, and so forth. Although tree-based parser evalua-tion provides valuable insights into the performance of grammars and parsing systems, it is subject to a number of (related) drawbacks: 1. Bracketed trees do not always provide NL Papplications with enough 2. A number of alternative, but equally valid tree representations can 3. Because a tree-based gold standard for parser evaluation must adopt a based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-proximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.
 addition to defining a language (as a set of strings), deep grammars relate strings to in-formation/meaning, often in the form of predicate X  X rgument structure, dependency re-lations, 2 or logical forms. By contrast, a shallow grammar simply defines a language and may associate syntactic (e.g., CFG tree) representations with strings. Natural languages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate X  X rgument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism.
 Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Lin-guistic Environment [Butt et al. 2002], the RAS Pdependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous  X  X nowledge acquisition bottleneck X  familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input.
 mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition in-curs relatively low development cost. With few notable exceptions, treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. constraint-based grammar development, a growing body of research has emerged to au-tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a first approximation, these approaches can be classified as  X  X onversion X -or  X  X nnotation X -based. TAG-based approaches convert treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSG-and LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources.
 the research strands just sketched: They use dependency-based parser evaluation to compare wide-coverage parsing systems using hand-crafted, deep, constraint-based grammars with systems based on a simple version of treebank-based deep grammar acquisition technology in the conversion paradigm. In the experiments, tree output generated by Collins X  X  Model 1, 2, and 3 (1999) and Charniak X  X  (2000) parsers, for example, are automatically translated into dependency structures and evaluated against gold-standard dependency banks.
 described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing systems (Briscoe and Carroll 1993; Collins X  X  1997 models 1 and 2; and Charniak 2000) using a simple version of the conversion-based deep grammar acquisition process (i.e., reading off grammatical relations from CFG parse trees produced by the treebank-based shallow parsers). The article also reports on a task-based evaluation experiment to rank the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations. 4 Kaplan et al. (2004) compare their deep, hand-crafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins X  X  (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at the price of a small decrease in parsing speed. versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al., this time using the sophisticated and fine-grained treebank-and annotation-based, deep, probabilis-tic LFG grammar acquisition methodology developed in Cahill et al. (2002b), Cahill et al. (2004), O X  X onovan et al. (2004), and Burke (2006) with a number of surprising results: 1. Evaluating against the PARC 700 Dependency Bank (King et al. 2003) 84 2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500 700 Dependency Bank were recently published in Clark and Curran (2007), reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll (2006) point out, these evaluations are not directly comparable with the Kaplan et al. (2004) style evaluation against the original PARC 700 Dependency Bank, because the annotation schemes are different.
 f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al. (2002b), Cahill et al. (2004), and Burke (2006). In Section 3, we present our experiment design. In Section 4, using the DCU 105 Dependency Bank as our development set, we evaluate a number of treebank-induced LFG parsing systems against the automatically generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best parsing system for the evaluations against the wide-coverage, hand-crafted RAS Pand
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al. (2004) using the CBS 500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and issues raised by our methodology, outline related and future research and conclude in
Section 7. 2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG-and history-based parsers, which allows us to compare these parsers at the level of dependency structures, rather than just trees. 2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) is a constraint-based theory of grammar. It (minimally) posits two levels of representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-resented by context-free phrase-structure trees, and captures surface grammatical configurations such as word order. The nodes in the trees are annotated with functional equations (attribute-value structure constraints, for example ( resolved (in the case of well-formed strings) to produce an f-structure. F-structures are recursive attribute-value matrices, representing abstract syntactic functions, which approximate to basic predicate-argument-adjunct structures or dependency relations.
Figure 1 shows the c-and f-structures for the string U.N. signs treaty . Each node in the c-structure is annotated with f-structure equations, for example ( uparrows (  X  ) point to the f-structure associated with the mother node, downarrows (  X  ) to that of the local node. In a complete parse tree, these instantiated to unique tree node identifiers and a set of constraints (a set of terms in an equality logic) is generated which (if satisfiable) generates an f-structure. 2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes enough information to support the derivation of deep grammatical information, such as predicate X  X rgument structures, deep dependency relations, or logical forms. Many second generation treebanks such as Penn-II provide information to support the compi-lation of meaning representations, for example in the form of traces relating displaced linguistic material to where it should be interpreted semantically. The f-structure anno-tation algorithm exploits configurational and categorial information, as well as traces and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II CFG trees with LFG f-structure information.
 the tree and deterministically add f-structure equations to the phrasal and leaf nodes of the tree, resulting in an f-structure annotated version of the tree. The annotations are then collected and passed on to a constraint solver which generates an f-structure (if the constraints are satisfiable). We use a simple graph-unification-based constraint solver constraints. Given parser output without Penn-II style annotations and traces, the same algorithm is used to assign annotations to each node in the tree, whereas a separate module is applied at the level of f-structure to resolve any long-distance dependencies (see Section 2.3). 86
McCarthy (2003), Cahill et al. (2004), and Burke (2006). In brief, the algorithm is modular with four components (Figure 3), taking Penn-II trees as input and automatically adding LFG f-structure equations to each node in the tree.

Lexical Information. Lexical information is generated automatically by macros for each of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with the equations (  X  PRED )= word ,(  X  NUM )=pland(  X  PERS ) = 3rd, where word is the lemmatized word .

Left X  X ight Context Annotation. The Left X  X ight context annotation component identifies the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a left context (left sisters), and a right context (right sisters). The contexts together with information about the local mother and daughter categories and (if present) Penn-II 88 functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations about how to annotate immediate daughters dominated by the mother category relative to their location in relation to the local head. To give a (much simplified) example, theheadfindingrulesforNPsstatethattherightmostnominal(NN,NNS,NNP,...) not preceded by a comma or  X - X  6 is likely to be the local head. The Annotation ma-trix for NPs states (inter alia) that heads are annotated to the left of the head are annotated (  X  SPEC DET )=  X  , NPs to the right of the head as  X  X  X  matrix. Figure 4 provides an example of the application of the N Pand P PAnnotation matrices to a simple tree.
 most frequent Penn-II rule types expanding the category such that the token occurrences of these rule types cover more than 85% of all occurrences of expansions of that category in Penn-II. For NP rules, for example, this means that we analyze the most frequent 102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
N Prule types, in order to populate the N PAnnotation matrix. Annotation matrices generalize to unseen rule types as, in the case of NPs, these may also feature DTs to the left of the local head and NPs to the right and similarly for rule types expanding other categories.

Coordination. In order to support the modularity, maintainability, and extendability of the annotation algorithm, the Left X  X ight Annotation matrices apply only to local trees of depth one, which do not feature coordination. This keeps the statement of Annotation matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-tionally) flat. The annotation algorithm has modules for like-and unlike-constituent coordination. Coordinated constituents are elements of a COORD set and annotated ( trices to annotate any remaining nodes in a local subtree containing a coordinating conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-tions resulting from the application of the Left X  X ight context Annotation matrices. The
Left X  X ight Annotation matrices are allowed a certain amount of overgeneralization as this facilitates the perspicuous statement of generalizations and a separate statement of exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right of the local V Phead as adjuncts:  X  X  X  (  X  ADJUNCT ). The Catch-All and Clean-Up module uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely related to local head), to replace the original adjunct analysis by an oblique argument analysis: (  X  OBL )=  X  . An example of this is provided by the PP-CLR in the left VP-conjunct in Figure 5. In other cases, argument X  X djunct distinctions are encoded configurationally in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-tation matrix indiscriminately associates SBARs to the right of the local head with (  X  RELMOD )=  X  . However, some of these SBARs are actually arguments of the local
N Phead and, unlike SBAR relative clauses which are Chomsky-adjoined to N P(i.e., relative clauses are daughters of an N Pmother and sisters of a phrasal N Phead), SBAR arguments are sisters of non-phrasal N Pheads. 7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement argument analysis (  X  COMP )=  X  .Figure6showsthe COMP f-structure analyses for an example N Pcontaining an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate  X  X isplaced X  material to where it 90 should be interpreted semantically. The f-structure annotation algorithm covers wh -and wh -less relative clause constructions, interrogatives, control and raising constructions, right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an example that shows the interplay between coordination, right-node-raising traces and the corresponding automatically generated reentrancies at f-structure. 2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al. (2004) and Cahill (2004) for parsing raw text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based lexicalized parsers are extracted from the unannotated treebank and used to parse raw text into trees. The resulting parse trees are then passed to the automatic f-structure annotation algorithm to generate f-structures. 8 parsers is impoverished: Parsers do not normally output Penn-II functional tag an-notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded in terms of a fine-grained system of empty productions (traces) and coindexation in the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on traces and coindexation to capture LDDs in terms of corresponding reentrancies at f-structure.
 between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as arguments iff they are labeled -CLR, -PUT, -DTV or -BNF , for example. Conversely, functional labels (e.g., -TMP ) are also used to analyze certain NPs as adjuncts, and -LGS labels help to identify logical subjects in passive constructions. In the absence of functional labels, the annotation algorithm will default to decisions based on simple structural, configurational, and CFG-category information (and, for example, conserva-tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
PCFGs and a version of Bikel X  X  history-based, lexicalized generative parser) trained to output CFG categories with Penn-II functional tags. We achieve this through a simple 92 masking and un-masking operation where functional tags are joined with their local CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR ) for training and parsing (for Bikel, the parser head-finding rules are also adjusted to the expanded set of categories). After parsing, the Penn-II functional tags are unmasked and available to the f-structure annotation algorithm.

LDDs represented in terms of traces and coindexation in the original Penn-II treebank trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-notation algorithm does not apply. Initially, the f-structures produced for parser output trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete (or proto) f-structures, where displaced material (e.g., the values of FOCUS , TOPIC ,and
TOPICREL attributes [ wh -and wh -less relative clauses, topicalization, and interrogative constructions] at f-structure) is not yet linked to the appropriate argument grammati-cal functions (or elements of adjunct sets) for the governing local PRED . A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-tion in parse trees.
 the LDD between the WHN Pin the relative clause and the embedded direct object position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation with the antecedent WHNP-3. Note further that the control relation between the subject of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm is able to derive a fully resolved f-structure where the LDD and the control relation are captured in terms of corresponding reentrancies (Figure 9). output tree (Figure 10) for the same string: The parser output does not explicitly record the control relation nor the LDD.
 architecture (Figure 7), the f-structure annotation algorithm would initially construct the partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-tion is unresolved (i.e., the value of TOPICREL is not coindexed with the OBJ grammatical function of the embedded verb reward ). The control relation (shared subject between the two verbs in the relative clause) is in fact captured by the annotation algorithm in terms of a default annotation (  X  SUBJ )=(  X  SUBJ ) on sole argument VPs to the right of head verbs (as often, even in the full Penn-II treebank trees, control relations are not consistently captured through explicit argument traces).
 certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen 1989] relating f-structure components in different parts of an f-structure), obviating traces and coindexation in c-structure trees. For the example in Figure 10, a functional uncertainty equation of the form (  X  TOPICREL )=(  X  [ COMP be associated with the WHN Pdaughter node of the SBAR relative clause. The equation states that the value of the TOPICREL attribute is token-identical (re-entrant) with the value of a SUBJ or OBJ function, reached through a path along any number (including 94 zero) of COMP or XCOMP attributes. This equation, together with subcategorization frames (LFG semantic forms) for the local PRED s and the usual LFG completeness and coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
LFG LDD resolution using automatically induced finite approximations of functional-uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O X  X onovan et al. 2004) in an LDD resolution component. From the fully LDD-resolved f-structures from the Penn-II training section treebank trees we learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice) (Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37 reference language definde by the full functional uncertainty equation ( (  X  the parser output for the relative clause example in Figure 11), admissible LDD res-olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL ) and a grammatical function (or adjunct set element) of an embedded local predicate, subject to the conditions that (i) the local predicate can be reached from the LDD trigger using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the grammatical function is not already present (at the relevant level of embedding in the local f-structure); and (vi) the local predicate subcategorizes for the grammatical function in question. 9 Solutions satisfying (i) X (iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution (possibly involving multiple interacting LDDs for a single f-structure) is returned by the algorithm (for details and comparison against alternative LDD resolution methods, see Cahill et al. 2004). 10 (  X  96 (together with the subject control equation described previously) turns the parser-output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in (Figure 9).

Traces component for LDD resolved Penn-II treebank trees) component (and the LDD path and subcategorization frame extraction) is given in Figure 7.
 or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers. 3. Experiment Design
In our experiments we compare four history-based parsers for integration into the pipeline parsing architecture described in Section 2.3: ger (Ratnaparkhi 1996). Charniak X  X  parser provides its own POS tagger. The combined system of best history-based parser and automatic f-structure annotation is compared to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-based, deep grammars: associate strings with dependency relations (in the form of grammatical relations or LFG f-structures).

We use the DCU 105 Dependency Bank (Cahill et al. 2002a) as our development set for the treebank-based LFG parsers. We use the f-structure annotation algorithm to automatically generate a gold-standard test set from the original Section 22 treebank trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al. (2004), we use the Penn-II Section 23-based PARC 700 Dependency Bank (King et al. 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler et al. (2002) and Kaplan et al. Following Preiss (2003), we use the SUSANNE Based CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-induced LFG resources against the hand-crafted RAS Pgrammar and parsing system (Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al. 2002). cally tagged input 14 sentences with the treebank-and machine-learning-based parsers trained on WSJ Sections 02 X 21 in the pipeline architecture, pass the resulting parse trees to our automatic f-structure annotation algorithm, collect the f-structure equations, pass them to a constraint-solver which generates an f-structure, resolve long-distance dependencies at f-structure following Cahill et al. (2004) and convert the resulting LDD-resolved f-structures into dependency representations using the formats and software of Crouch et al. (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations) and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS 500 evaluation). In the experiments we did not use any additional annotations such as -A (for argument) that can be generated by some of the history-based parsers (Collins 1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do not contain such annotations). We also did not use the limited LDD resolution for wh -relative clauses provided by Collins X  X  Model 3 as better results are achieved by LDD 98 resolution on f-structure (Cahill et al. 2004). A complete set of parameter settings for the parsers is provided in the Appendix.
 and the CBS 500 dependency banks, a certain amount of automatic mapping is re-quired to account for systematic differences in linguistic analysis, feature geometry, and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and 5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the statistical significance of the results. 4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-isons with the hand-crafted XLE and RAS Presources in Section 5. We use the DCU 105 Dependency Bank as our development set and carry out comparative evaluation and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel X  X  (2002) parser retrained to retain Penn-II functional tags (Table 1) achieves overall best results. 4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and tested on Section 23. The published results 15 on these experiments for the history-based parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs are induced following standard treebank preprocessing steps, including elimination of empty nodes, but following Cahill et al. (2004), they do include Penn-II functional tags (Table 1), as these tags contain valuable information for the automatic f-structure anno-tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation. than the more basic PCFGs (with and without parent transformations). Charniak X  X  (2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage points. The hand-crafted XLE and RAS Pgrammars achieve around 80% coverage (measured in terms of complete spanning parse) on Section 23 and use a variety of (longest) fragments combining techniques to generate dependency representations for the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and history-based parsers all achieve coverage of over 99.9%. Given that the history-based parsers score considerably better than PCFGs on trees, we would also expect them to produce dependency structures of substantially higher quality. 4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al. 2002a) is a hand-crafted gold-standard dependency bank for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank. relatively small gold standard, initially developed to evaluate the automatic f-structure annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with each of the treebank-induced parsers in the pipeline parsing and f-structure annotation architecture. The f-structures of the gold standard and the f-structures returned by the parsing systems are converted into dependency triples following Crouch et al. (2002) and Riezler et al. (2002) and we also use their software for evaluation. The following dependency triples are produced by the f-structure in Figure 1: value: the predicate-argument-adjunct structure skeleton) and all grammatical func-tions (GFs) including number, tense, person, and so on. The results are given in Table 6. the better the trees produced by the parsers, the better the f-structures automatically generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-notation algorithm will exploit Penn-II functional tag information if present to generate configurational and categorial information if Penn-II tags are not present in the trees. based parser output will improve LFG f-structure-based dependency results, we use
Bikel X  X  (2002) training software, 17 and retrain the parser on a version of the Penn-II treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated in such a way that the resulting history-based parser will retain them (Section 2.3). The retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels and these are used by the f-structure annotation algorithm. We evaluate the f-structure dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only 100 and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system based on the retrained parser is now much better able to identify oblique arguments and overall preds-only accuracy has improved by 3.53% over the original Bikel experiment and 3.31% over Charniak X  X  parser, even though Charniak X  X  parser performs more than 2% better on the tree-based scores in Table 5 and even though the retrained parser drops 0.79% against the original Bikel parser on the tree-based scores. preds-only evaluation against the DCU 105 shows that just over one third of all depen-dency triples in the gold standard are adjuncts. SUBJ (ects) and OBJ (ects) together make up a further 30%. unable to identify APP (osition). This is due to Collins X  X  treatment of punctuation and the fact that punctuation is often required to reliably identify apposition. the original history-based parsers produced trees which enabled the annotation algo-rithm to identify second oblique dependencies ( OBL 2), and they generally performed considerably worse than Parent-PCFG when identifying OBL (ique) dependencies. This is because the automatic f-structure annotation algorithm is cautious to the point of undergeneralization when identifying oblique arguments. In many cases, the algorithm relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly for oblique agents ( OBL AG , agentive by -phrases in passive constructions), whereas the history-based parsers are able to identify these with considerable accuracy. This is be-cause Parent-PCFG often erroneously finds oblique agents, even when the preposition is not by , as it never has enough context in which to distinguish by prepositional phrases from other PPs. The history-based parsers produce trees from which the automatic f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-cies than Parent-PCFG. This, in turn, leads to improved long distance dependency resolution which improves overall accuracy.
 testing of the performance ranking of the six treebank-based LFG parsing systems. In order to carry out significance testing to select the best treebank-based LFG parsing system for comparative evaluation against the hand-crafted deep XLE and RAS Pre-sources, we move to a larger dependency-based evaluation data set: the gold-standard dependency bank automatically generated from WSJ Section 22. 4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002), evaluate each parser against a large automatically generated gold standard. The gold-standard dependency bank is automatically generated by annotating the original 1,700 treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-notation algorithm. We then evaluate the f-structures generated from the tree output of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22 strings against the automatically produced f-structures for the original Section 22 Penn-II treebank trees. The results are given in Table 8.
 so for the history-based parsers. This trend is possibly due to the fact that the WSJ 102
Section 22 gold standard is generated automatically from the original  X  X erfect X  Penn-II treebank trees using the automatic f-structure annotation algorithm, whereas the DCU 105 has been created manually without regard as to whether or not the f-structure annotation algorithm could ever generate the f-structures, even given the  X  X erfect X  trees.
 83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92% preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-only evaluation.
 not able to identify any cases of apposition from the output of Collins X  X  Model 3 parser. Apart from Bikel X  X  retrained parser, none of the history-based parsers are able to identify trees from which it is easier to identify obliques ( OBL ), because of the Penn-II functional -CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although the history-based parsers score reasonably well for this function. Whereas Charniak X  X  parser is able to identify some dependencies better than Bikel X  X  retrained parser, overall the system based on Bikel X  X  retrained parser performs better when evaluating against the dependencies in WSJ Section 22.
 proximate Randomization Test (Noreen 1989). 21 This test is an example of a computer-intensive statistical hypothesis test. Such tests are designed to assess result differences is unknown. Comparative evaluations of outputs of parsing systems according to test are computed by accumulating certain count variables over the sentences in the test set. In the case of f-score, variable tuples consisting of the number of dependency-relations in the parse for the system translation, the number of dependency-relations in the parse for the reference translation, and the number of matching dependency-relations between system and reference parse, are accumulated over the test set. able tuple produced by one of the systems could just as likely have been produced by the other system. So shuffling the variable tuples between the two systems with equal probability, and recomputing the test statistic, creates an approximate distribution of the test statistic under the null hypothesis. For a test set of S sentences there are 2 different ways to shuffle the variable tuples between the two systems. Approximate randomization produces shuffles by random assignments instead of evaluating all 2 possible assignments. Significance levels are computed as the percentage of trials where the pseudo statistic, that is the test statistic computed on the shuffled data, is greater sketch of an algorithm for approximate randomization testing is given in Figure 12. 104 be rejected) for comparing each parser against all of the other parsers. We test for sig-nificance at the 95% level. Because we are doing a pairwise comparison of six systems, giving 15 comparisons, the p -value needs to be below .0034 for there to be a significant difference at the 95% level. 22 For each parser, the values in the row corresponding to that parser represent the p -values for those parsers that achieve a lower f-score than that parser. This shows that the system based on Bikel X  X  retrained parser is significantly better than those based on the other parsers with a statistical significance of &gt; 95%. For the XLE and RAS Pcomparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system. 5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using the retrained version of Bikel X  X  parser (which retains Penn-II functional tag labels) to compare against parsing systems using deep, hand-crafted, constraint-based grammars at the level of dependencies. We report on two experiments. In the first experiment (Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-ing system (Riezler et al. 2002; Kaplan et al. 2004) on the PARC 700 Dependency Bank (King et al. 2003). In the second experiment (Section 5.2), we evaluate against the hand-crafted, wide-coverage unification grammar and RAS Pparsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998). 5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al. 2003) provides dependency relations (including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup of Kaplan et al. (2004) with a split of 560 dependency structures for the test set and 140 for the development set. The set of features (Table 12, later in this article) evaluated in the experiment form a proper superset of preds-only, but a proper subset of all grammatical functions (preds-only  X  PARC  X  all GFs). This feature set was selected in
Kaplan et al. because the features carry important semantic information. There are sys-tematic differences between the PARC 700 dependencies and the f-structures generated in our approach as regards feature geometry, feature nomenclature, and the treatment of named entities. In order to evaluate against the PARC 700 test set, we automatically map the f-structures produced by our parsers to a format similar to that of the PARC 700 Dependency Bank. This is done with conversion software in a post-processing stage on the f-structure annotated trees (Figure 13).

PARC 700, except for the Multi-Word Expressions section. Following the experimental setup of Kaplan et al. (2004), we mark up multi-word expression predicates based on the gold-standard PARC 700 Dependency Bank.

Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-106
Feature Nomenclature There are a number of systematic differences between feature
Additional Features A number of features in the PARC 700 are not produced by the au-
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and For full details of the mapping, see Burke et al. (2004).
 crafted, wide-coverage, deep LFG resources and XLE parsing system with improved results over those reported in Kaplan et al. (2004): This latest version achieves 80.55% f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing system combines a large-scale, hand-crafted LFG for English and a statistical disam-biguation component to choose the most likely analysis among those returned by the symbolic parser. The statistical component is a log-linear model trained on 10,000 partially labeled structures from the WSJ. The results of the parsing experiments are presented in Table 11. We also include a figure for the upper bound of each system.
Using Bikel X  X  retrained parser, the treebank-based LFG system achieves an f-score of 82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score of 80.55%. The approximate randomization test produced a p -value of .0054 for this pairwise comparison, showing that this result difference is statistically significant at the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC 700 Dependency Bank were recently published in Clark and Curran (2007), reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
Carroll point out, these evaluations are not directly comparable with the Kaplan et al. (2004) style evaluation against the original PARC 700 Dependency Bank, because the annotation schemes are different. Kaplan et al. and our experiments use a fine-grained feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17 features.
 108 hand-crafted parsing system can better identify FOCUS , OBL (ique) arguments, PRECO -5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP parsing system of Carroll and Briscoe (2002) to our treebank-and retrained Bikel parser-based LFG system. The RAS Pparsing system is a domain-independent, robust statistical parsing system for English, based on a hand-written, feature-based unification grammar. A probabilistic parse selection model conditioned on the structural parse context, degree of support for a subanalysis in the parse forest, and lexical informa-tion (when available) chooses the most likely parses. For this experiment, we evaluate against the CBS 500, 25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to evaluate a precursor of the RAS Pparsing resources. The CBS 500 contains dependency structures (including some long distance dependencies 26 ) for 500 sentences chosen at random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. As with the PARC 700, there are systematic differences between the f-structures produced by our methodology and the dependency structures of the CBS 500. In order to be able to evaluate against the CBS 500, we automatically map our f-structures into a format similar to theirs. We did not split the data into a heldout and a test set when developing the mapping, so that a comparison could be made with other systems that report evaluations against the CBS 500. The following CBS 500-style grammatical relations are produced from the f-structure in Figure 1: structure annotated trees, and the remaining mapping is carried out on the f-structures (Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
Treatment of topicalized sentences The predicate of the topicalized sentence became
Multi-word expressions Multi-word expressions (such as according to ) were not
Treatment of the verbs be and become Our automatic annotation algorithm does not 110 The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
Objects of Prepositional Phrases No dependency was generated for these objects, as
Nomenclature Differences There were some trivial mappings to account for differ-
Encoding of wh -less relative clauses These are encoded by means of reentrancies in the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998) 27 to evaluate the grammatical relations produced by each parser. The results are given in Table 13.
 whereas the hand-crafted RAS Pgrammar and parser achieves an f-score of 76.57%.
Crouch et al. (2002) report that their XLE system achieves an f-score of 76.1% for the same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel X  X  retrained parser is able to better identify MOD (ifier) de-which is syntactically realized as a modifier, for example by -phrases), IOBJ (indirect object) and AUX iliary relations. RAS Pis able to better identify XSUBJ (clausal subjects controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement) relations. Again we use the Approximate Randomization Test to test the parsing results for statistical significance. The p -value for the test comparing our system using Bikel X  X  retrained parser against RAS Pis &lt; .0001. The treebank-based LFG system using Bikel X  X  retrained parser is significantly better than the hand-crafted, deep, unification grammar-based RAS Pparsing system with a statistical significance of &gt; 95%. 6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources outperform the hand-crafted XLE and RAS Pgrammars.
 erably wider coverage ( &gt; 99.9% measured in terms of complete spanning parse) than the hand-crafted grammars (  X  80% for XLE and RAS Pgrammars on unseen treebank text). Both XLE and RAS Puse a number of (largest) fragment-combining techniques to achieve full coverage. If coverage is a significant component in the performance difference observed between the hand-crafted and treebank-induced resources, then it is reasonable to expect that the performance difference is more pronounced with increasing sentence length (with shorter sentences being simpler and more likely to be within the coverage of the hand-crafted grammars). In other words, we expect the hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to be within their coverage), whereas the treebank-induced grammars should show better performance on longer strings (less likely to be within the coverage of the hand-crafted grammars).
 tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are approximately normally distributed, with the CBS 500 distribution possibly showing the effects of being chosen subject to the constraint that the strings are parsable by the parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean,  X  ,and two standard deviations, 2  X  , to the left and right of the mean to exclude sentence lengths not supported by sufficient observations: For PARC 700,  X  = 23 . 27,  X 
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700,  X  is actually outside the observed data range, whereas for CB 500,  X  112 with the left border. It is therefore useful to further constrain the sentence count threshold of  X  5. 28 This results in a sentence length range of 4 X 41 for PARC 700 and 4 X 32 for CBS 500.
 plotted the percentage of fragment parses over sentence length for the XLE parses of the 560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment parses tends to increase with sentence length.
 treebank-induced resources against sentence lengths. Figure 20 shows the results for PARC 700, Figure 21 for CBS 500.
 show that the treebank-induced resources outperform the hand-crafted resources within (most of) the 4 X 41 and 4 X 32 sentence length bounds, with the results for the very short and the very long strings outside those bounds not being supported by sufficient data points.
  X  40. Below we give those results and statistical significance testing for the PARC 700 and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained X  X ased LFG system achieves a higher dependency f-score on sentences of length sentences, whereas the XLE system achieves a slightly lower score on sentences of length  X  40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically significant improvement of 2.67 percentage points over the XLE system on sentences of length  X  40. Against the CBS 500, Bikel X  X  retrained system achieves a weighted f-score of 82.58%, a statistically significant improvement of 3.87 percentage points over the RASP system which achieves a weighted f-score of 78.81% on sentences of length 114 fect on the results for the PARC 700 experiments. Recall that following Kaplan et al. (2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical functions that is a superset of preds-only and a subset of all-GFs. A preds-only based of predicate X  X rgument/adjunct misattachments in the resulting dependency represen-tations (while local functions such as NUM (ber), for example, can score properly even if the local predicate is misattached). Table 17 29 below gives the results for preds-only evaluation 30 on the PARC 700 for all sentence lengths. The results show that the Bikel-retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage points lower than the score for all the PARC dependencies. The XLE system achieves an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained X  based LFG system suffers less than the XLE system when preds-only dependencies are evaluated.
 based LFG parsing architectures, we do not claim to provide fully adequate statistical models. It is well known (Abney 1997) that PCFG-or history-based parser approxima-tions to general constraint-based grammars can yield inconsistent probability models 116 due to loss of probability mass: The parser successfully returns the highest ranked parse tree but the constraint solver cannot resolve the f-structure equations and the probability mass associated with that tree is lost. Research on adequate probability models for constraint-based grammars is important (Bouma, van Noord, and Malouf 2000; Miyao and Tsujii 2002; Riezler et al. 2002; Clark and Curran 2004). In this context, it is interesting to compare parser performance against upper bounds. For the PARC 700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83% for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05% (f-score 80.55%) of its upper bound using a discriminative disambiguation method, whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper bound.
 results, the figures are actually very difficult to compare. In the case of the XLE, the upper bound is established by unpacking all parses for a string and scoring the best match against the gold standard (rather than letting the probability model select a parse). By contrast, in the case of the treebank-based LFG resources, we use the original  X  X erfect X  Penn-II treebank trees (rather than the trees produced by the parser), auto-matically annotate those trees with the f-structure annotation algorithm, and score the results against the PARC 700 (it is not feasible to generate all parses for a string, there are simply too many for treebank-induced resources). The upper bound computed in this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main reason is that the automatic mapping required to relate the f-structures generated by the treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated by comparing the upper bound for the treebank-based LFG resources for the PARC 700 against the upper bound for the DCU 105 gold standard, where little or no mapping (apart from the feature-structure to dependency-triple conversion) is required: Scoring the f-structure annotations for the original treebank trees results in 86.83% against PARC 700 versus 96.80% against DCU 105.
 their disambiguation models. Ultimately what is required is an evaluation strategy that separates out and clearly distinguishes between the grammar, parsing algorithm, and disambiguation model and is capable of assessing different combinations of these core components. Of course, this will not always be possible and moving towards it is part of a much more extended research agenda, well beyond the scope of the research reported in the present article. Our approach, and previous approaches, evaluate systems at the highest level of granularity, that of the complete package: the combined grammar-parser-disambiguation model. The results show that machine-learning-based resources can outperform deep, state-of-the-art hand-crafted resources with respect to the quality of dependencies generated.
 has become an important research topic: Starting with the seminal TAG-based work of Xia (1999), there are now also HPSG-, CCG-and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-search on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-based models including LDD resolution. It would be interesting to conduct a com-parative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RAS Pparsing systems. (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are some-what partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual infor-mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks. 31 7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-uated four machine-learning-based shallow parsers and two hand-crafted, wide-coverage deep probabilistic parsers involving four gold-standard dependency banks, using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-cance. We used a sophisticated method for automatically producing deep dependency relations from Penn-II-style CFG-trees (Cahill et al. 2002b, 2004) to compare shallow parser output at the level of dependency relation and revisit experiments carried out by Preiss (2003) and Kaplan et al. (2004).

This result is surprising for two reasons. First, it is established against two externally-provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE (Riezler et al. 2002) and RAS P(Carroll and Briscoe 2002) parsing systems to evaluate those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an instance of domain variation for the Penn-II-trained LFG resources, likely to adversely affect scores. Second, the treebank-and machine-learning-based LFG resources require automatic mapping to relate f-structure output of the treebank-based parsing systems to the representation format in the PARC 700 and CBS 500 Dependency Banks. These mappings are partial and lossy: That is, they do not cover all of the systematic dif-ferences between f-structure and dependency bank representations and introduce a certain amount of error in what they are designed to capture, that is they both over-and undergeneralize, again adversely affecting scores. Improvements of the mappings should lead to a further improvement in the dependency scores. 118
Earlier we criticized tree-based parser evaluation on the grounds that equally valid different tree-typologies can be associated with strings, and identified this as a major obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation, differences between the PARC 700 dependency and the CBS 500 dependency represen-tations; there are significant systematic differences between the LFG f-structures gener-ated by the hand-crafted, wide-coverage grammars of Riezler et al. (2002) and Kaplan et al. (2004) and those of the treebank-induced and f-structure annotation algorithm based resources of Cahill et al. (2004). These differences require careful implementation of mappings if parsers are not to be unduly penalized for systematic and motivated differences at the level of dependency representation. By and large, these differences are, however, less pronounced than differences on CFG tree representations, making dependency-based parser evaluation a worthwhile and rewarding exercise.
 treebank-and f-structure annotation algorithm-based LFG system using Bikel X  X  parser retrained to retain Penn-II functional tag labels performs best, achieving f-scores of 82.92% preds-only and 88.3% all grammatical functions. Against the automatically generated WSJ Section 22 Dependency Bank, the system using Bikel X  X  retrained parser achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate against the PARC 700 and CBS 500 gold standards, we automatically map the dependen-cies produced by our treebank-based LFG system into a format compatible with the gold standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system using Bikel X  X  retrained parser achieves an f-score of 82.73%, a statistically significant improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel X  X  retrained parser achieved the highest f-score of 80.23%, a statistically significant improvement of 3.66 percentage points on the highest previously published results for the same experiment with the hand-crafted RAS Presources in Carroll and Briscoe (2002).
 Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the parsers described in this article.
 Acknowledgments References 120 122
