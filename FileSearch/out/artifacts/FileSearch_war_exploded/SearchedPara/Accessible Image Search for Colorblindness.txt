 MENG WANG Microsoft Research Asia BO LIU
University of Science and Technology of China and XIAN-SHENG HUA Microsoft Research Asia 1. INTRODUCTION
The last decade has witnessed great advances in image search technology [Lew et al. 2006, 2000; Smeulders et al. 2000; Zhou and Dai 2007]. However, existing commercial image search engines, such as Google, Yahoo, and Microsoft image search, all serve normal viewers and focus on returning more relevant images and illustrating them first in the ranking lists. Special human groups, such as colorblind users, have not been taken into consideration. In fact, worldwide around 8% of men and 0.8% of women have certain kinds of colorblindness, that is, they have difficulties in discriminating certain color differences. Good search results for a normal viewer may not be acceptable for colorblind users since many images will not be readily perceived by them. As an example, Figure 1(a) illustrates the top results of query  X  X lower X  returned by the Microsoft image search engine, and we can see that these results are all relevant and with high visual quality. Then, in Figure 1(b) we mimic the view of a colorblind user (i.e., what the colorblind user will perceive) using the simulation algorithm proposed in Brettel et al. [1997], and we can see that the qualities of many of them have significantly degraded due to the loss of color information, that is, they cannot be well perceived by colorblind users.

As noted by Berners-Lee,  X  X he power of the Web is in its universality, and access by everyone regardless of disability is an essential part X  until now there is no service that accommodates colorblind users in image search, and an associated fact is that many colorblind users cannot efficiently find and enjoy images with the existing search engines. Therefore, an image search scheme that can serve the numerous colorblind users is highly desired.
It is also worth mentioning that there are already several techniques that can support special users in web search. For example, Google provides an accessible search service, which aims at facilitating visually impaired users (mainly blindness) 2 . It is developed based on several heuristic criteria, such as the page X  X  simplicity, how much visual imagery it carries, and whether it is immediately viable with keyboard navigation, and then analyze whether the page can be easily read by speech synthesis software accordingly. Baidu, the most popular Chinese search engine, provides a service to accommodate elder users 3 . However, accommodating colorblind users in image search will be much more challenging since it will involve not only the psychological and cognitive study of colorblind users but also image analysis technology.
By communicating with a large number of colorblind users, it was discovered that we can facilitate their Web experience in the following three aspects. (1) Identify and prioritize those images that can be better perceived by color-(2) Provide a method to modify the colors of the images so that they can be (3) Indicate the colors of points-of-interest for colorblind users in an
In this work, we introduce an Accessible Image Search (AIS) system that can meet these three requirements. The system contains three components, that is, image colorblind accessibility assessment, accessibility improvement, and color indication. The colorblind accessibility of an image can be understood as how well an image can be perceived by colorblind users, and we will further explain this in detail later. Based on these three components, we can facilitate colorblind users in different ways, such as reranking search results accord-ing to accessibility measurements or recoloring the images, as illustrated in
Figure 2. Comprehensive study with a batch of anonymous colorblind users has demonstrated the usefulness of the proposed system. The novelty of the proposed system can be summarized as follows.
 (1) To the best of our knowledge, this is the first system to facilitate colorblind (2) It adopts an image colorblind accessibility assessment approach, based on (3) It adopts an efficient image recoloring method which is able to improve (4) It contains a color indication component to indicate the color names of
The article also proposes a performance evaluation measure for accessible image search that takes the accessibilities of search results into account. In ad-dition, we have conducted a comprehensive empirical study. The experiments involve more than 60,000 images and 20 real colorblind users, and this com-pares favorably with previous efforts on accommodating colorblind users (such as image recoloring [Huang et al. 2007; Huang et al. 2008]) that adopt only a small set of images and colorblind simulation algorithms.

The organization of the rest of this article is as follows. In Section 2, we provide a short review on the related work. In Section 3, we introduce the AIS system, including accessibility assessment, accessibility improvement, color indication and the performance evaluation measure. Experimental results are presented in Section 4. Finally, we conclude the article in Section 5. 2. RELATED WORK 2.1 Accessibility in Search
Accessibility is a general term used to describe the degree to which a product is accessible by as many people as possible, and it is often used to focus on people with disabilities and their right of access to entities often through the use of assistive technology 4 . With the popularity and wide use of search en-gines, the accessibility aspect is receiving increasing attention in order to help people with disabilities more efficiently find and enjoy web content. Besides the previously introduced Google Accessible Search and Baidu Elderly Search, several research efforts has been dedicated to developing search systems that can accommodate different groups. Azzopardi and Ruthven [2009] proposed a
PuppyIR system that aims to serve children in web search. Arrue and Vigo [2007] proposed a scheme that estimates the accessibility of search content and then generates ranking lists accordingly. Fajardo et al. [2006] investigated approaches to improve deaf users X  accessibility in hypertext search with the help of graphical interfaces. Andronico et al. [2006] proposed a method to im-prove search engine interfaces to help blind users. However, it lacks a study on accessibility in multimedia search, which motivates our work. 2.2 What X  X  Colorblindness
Colors are perceived by humans with their cones absorbing photons and send-ing electrical signal to the brains [Wandell 1995]. According to their peak sen-sitivity, the cones can be categorized into Long ( L ), Middle ( M ) and Short ( S ), which absorb long wavelengths, medium wavelengths, and short wavelengths, respectively. Consequently, light is perceived as three members: ( l l , m ,and s represent the amount of photons absorbed by L -, M -, and S -cones, respectively. More formally, color stimulus ( S i ) for a light can be computed as the integration over the wavelengths  X  : where  X  stands for the power spectral density of the light, and l indicate L -, M -, and S -cones.

Colorblindness, formally known as color vision deficiency, is caused by the deficiency or lack of a certain type of cone. Dichromats are referred to as those who have only two types of cones, and they consist of protanopes, deuteranopes, and tritanopes which indicate the lack of L -cones, M -cones, and S -cones, re-spectively. Protanopes and deuteranopes have difficulty in discriminating red from green, whereas tritanopes have difficulty in discriminating blue from yel-low. In this work, we focus on protanopia and deuteranopia as most dichromats belong to these two types, but our methods can also be easily extended to deal with tritanopia.

Significant research work has been dedicated to simulating colorblindness [Evans 1948]. Brettel et al. [1997] proposed a method that transforms colors from RGB space to LMS (long, medium, short) color space based on cone re-sponse and then modifies the response of the deficient cones. This algorithm is widely adopted by colorblindness simulation systems such as VisCheck
IBM aDesigner X  X  low vision mode 6 . Obviously, to facilitate colorblind users, we first need to reveal what they are observing. Therefore, the colorblind-simulating algorithms form the basis of our work. Figure 3 illustrates several images and their corresponding simulated protanopic and deuteranopic views using the algorithm in Brettel et al. [1997]. 2.3 Efforts for Colorblindness
Several efforts have been dedicated to helping colorblind users better per-ceive and enjoy visual documents, such as web pages and images. In 2005 and 2008, a workshop named Computer Vision Applications for the Visually Im-paired (CVAVI) was organized with two top conferences in the computer vision research community 7 , 8 . However, most of the research focuses on recoloring images or web pages that they can be better perceived by colorblind viewers, whereas how to help them find more useful and accessible images or web pages, which is also important for these users, receives much less attention. A related work is Kovalev X  X  study on image retrieval for colorblind users [Kovalev 2004].
But this work only investigates several effects of colorblindness in image re-trieval and does not provide any solution or service for colorblind users.
About image/web page recoloring, Dougherty et al. proposed an image recol-oring process named Daltonize (http://www.vischeck.com), which first increases the red/green contrast in the image, and then uses the red/green contrast in-formation to adjust brightness and blue/yellow contrast. Yang et al. [2008a, 2008b] proposed an approach that is able to quantify colorblindness and a color compensation scheme that can enhance the perception of colorblind viewers.
Iaccarino et al. [2006] have proposed a simple recoloring method to improve the accessibility of web pages. Yang and Ro [2003] proposed a method that changes a monochromatic hue into another hue with less saturation for dichromats.
Rasche et al. [2005] formulate the recoloring task as a dimensionality reduc-tion problem, that is, how to map the colors in a 3-dimensional space into a 2-dimensional space that can be recognized by colorblind viewers. Huang et al. [2007] proposed an image recoloring algorithm that keeps both the discrimina-tive abilities of colors and the naturalness of the image. Jefferson and Harvey [2006] proposed a document recoloring algorithm. It first selects a representa-tive set of colors from the source document, and then changes these colors while preserving their differences, and finally it performs in interpolation operation for other colors. Jefferson and Harvey [2007] then further provided an inter-face to support the interactive recoloring implementation for colorblind viewers.
Huang et al. [2008] proposed a generalized histogram-equalization method to remap the hue components of images in HSV color space. Wakita and Shima-mura [2005] proposed an optimization approach which simultaneously takes into account the contrast, consistency, distinguishability, and naturalness of the mapped colors.

Several encouraging results have been reported in these works. However, most of them implement an optimization process to accomplish the color map-ping and thus incur large computational costs so they can hardly be applied in practical large-scale application. In our proposed recoloring method, we simply perform several color rotation operations, and it is much more efficient. Empir-ical results will show that it even outperforms the existing optimization-based methods.

It is worth noting that recoloring can only improve the colorblind accessibili-ties of images to a certain degree, that is, the images will not be as good as those that are perceived by normal viewers even after recoloring as 1D color infor-mation is lost in the colorblind view [Rasche et al. 2005]. Therefore, in AIS we simultaneously provide the accessibility-based reranking, recoloring-based ac-cessibility improvement and color indication techniques, and their combination can provide a series of services for colorblind users. 3. ACCESSIBLE IMAGE SEARCH
In this section, we introduce our AIS system. First, we provide a definition of the colorblind accessibility of an image, and then we investigate different accessibility assessment methods. After that, we introduce the accessibility improvement method, that is, the image-recoloring algorithm, and the color indication component. We will show what services we can provide in the AIS system. Finally, we introduce a performance evaluation measure for AIS.
It is worth mentioning that in fact AIS should simultaneously take into account relevance and accessibility of search results. But there are already many research efforts focusing on relevance [Smeulders et al. 2000; Jing and
Baluja 2008a] (as well as several other related criteria, such as diversity [Jing and Baluja 2008b] and typicality [Hua et al. 2007]), and thus in this work we mainly focus on accessibility. Our scheme can also be easily integrated with the relevance-improvement methods [Smeulders et al. 2000; Jing and Baluja 2008a; Jing and Baluja 2008b; Hua et al. 2007] or extended to achieve a trade-off between relevance and accessibility. 3.1 What is Image Colorblind Accessibility
As previously mentioned, accessibility is often used to describe the degree to which a product is accessible by people, especially those with certain disabil-ities. A typical example is web accessibility, for which the W3C has provided a guideline in order to makes web pages accessible to everyone. However, cur-rently most standards about the accessibility of multimedia focus on their access on the web while ignoring the content of the media data [Moreno et al. 2008].

Analogous to the definition of web accessibility, we straightforwardly define an image X  X  colorblind accessibility as the degree to which the image can be per-ceived by colorblind users. Given the context of helping colorblind users in this work, in the rest of our article we will replace colorblind accessibility by acces-sibility for short. It is worth noting that accessibility is a subjective measure, but common judgment still exists. For example, Figure 4 illustrates several web images with different colorblind accessibilities and their deuteranopic views.
We can see that most users will agree that in Figure 3 the colorblind views in the first row have better accessibility than those in the last row. We can also see that several high-quality images will have low accessibility due to the loss of color information.
 3.2 Accessibility Assessment
Obviously a crucial component in the AIS system is the automatic assessment of image accessibility. Based on the evaluated accessibilities, we can adopt different reranking techniques to prioritize the highly accessible images, as illustrated in Figure 2.

According to the definition of images X  accessibility, we can see that it can be regarded as a kind of quality estimation. Therefore, a straightforward approach is to adopt image-quality assessment algorithms, which have been extensively studied in the past [Wang and Bovik 2006]. However, directly investigating the existing quality assessment methods may not achieve satisfying results because it does not take into account the specific knowledge of our task, that is, the quality loss of images is introduced by the confusion of several colors.
Therefore, here we investigate three methods for accessibility assessment, as showninFigure5. (1) Quality-based method. We regard accessibility assessment as a full-(2) Analysis-based method. It is based on the prior knowledge that the col-(3) Learning-based method. It learns the accessibility of images through a
In experiments, we will compare these three methods in terms of both per-formance and computational efficiency. 3.2.1 Quality-Based Accessibility Assessment. Image quality assessment methods can be divided into two categories, namely, full-reference and no-reference approaches, according to whether there is a reference image [Wang et al. 2004; Pappas and Safranek 2000; Winkler 1999]. Here we adopt the full-reference approach. We assume that the quality of an image is degraded when it is observed by a colorblind viewer. Therefore, we regard the original image as reference and then estimate the quality of its colorblind view. We adopt the method proposed in Wang et al. [2004]. But this method is designed for gray images, and here we implement it on R, G and B channels individually and then fuse the results. For a channel C (it can be R, G or B), the quality score is estimated as where  X  x and  X  x are the mean and standard deviation of the color channel of the original image and its colorblind view in the color channel. It actually can be viewed as a measurement of the difference between the reference image and the image to be assessed. We can see that, if the two images are identical, then the Q c measurement will be 1.
 The accessibility score is then defined as 3.2.2 Analysis-Based Accessibility Assessment. We assume that the acces-sibility is decreasing with respect to the color information loss between the original image and its colorblind view, which is defined as where ( c i , c j ) indicates the difference between colors c resents the colorblind view of c i . It measures whether the difference between color pairs has been preserved in the colorblind view. Thus we can see that we need a color difference estimator here. The most straightforward approach is to use the Euclidean distance of colors in the RGB space. However, existing studies have revealed that this method is not consistent with human percep-tion. Here we adopt the CIE94 color difference [CIE 1995], which is a weighted Euclidean distance in the LCH (luminance, chroma, hue) color space. For the sake of computational efficiency, we equally quantize the original
RGB color space into Q bins, and denote by n i the number of samples that belong to the i th bin. Thus Eq. (5) becomes
Based on the information-loss measurement, we define the colorblind acces-sibility of an image as 3.2.3 Learning-Based Accessibility Assessment. In learning-based accessi-bility assessment, each image is represented by a d -dimensional feature vector, that is, it is assumed that the accessibility score of an image can be predicted with these features. Then we collect a set of training data y )...,( x l , y l ) } , where y i indicates the ground-truth accessibility score of x shown in Figure 5(c), a model is learned from these training samples, and the accessibility scores of the other images can be directly predicted by this model.
We adopt the Support Vector Regression (SVR) model [Vapnik 1995], for which the fitting function takes the form where ( . ) is a mapping from R d to a Hilbert Space H ,and dot product in H . Then the soft-margin SVR is formulated as follows.
With the help of Lagrange multipliers, the dual of the preceding problem can be obtained as where  X  is a vector with components  X  i that are the Lagrange multipliers. Since the mapping ( . ) only appears in the dot product, we need not know its explicit form. Instead, we can define a kernel K ( ., . )with K ( x accomplish the mapping from the training data space to the Hilbert Space
In this work, we adopt the blockwise color moment as the low-level feature for its capability to simultaneously capture the statistical and spatial distribution of colors in images. We split each image into 5  X  5 blocks and then extract 9D
LAB color moments from each block (the mean, variance, and skewness of l , a , and b components, respectively), as shown in Figure 5(c). For the SVR model, we adopt the RBF kernel. In experiments we will show that the accessibility scores predicted in this way correlate well with subjective perception. 3.3 Accessibility Improvement
We propose an efficient image-recoloring algorithm to improve the accessibil-ities of images. Different from the traditional recoloring algorithms that opti-mize the color mapping functions, the proposed method simply performs several color rotation operations in the CIELAB domain to accomplish the recoloring.
The method consists of two steps, that is, local color rotation and global color rotation, as illustrated in Figure 6.
 For local color rotation, we adopt a method similar to the one proposed by
Huang et al. [2007], for which the basic idea is to map the information of a into the b  X  axis since the color information in a  X  gets lost significantly for protanopia and deuteranopia. We rotate the color which has the included angle  X  with respect to the a  X  axis by an angle  X  (  X  ), that is,
Such a rotation has three advantages: (1) the image after color rotation has the same luminance as the original image; (2) colors with the same hue in the original image still maintain the same hue after color rotation; (3) the saturation of the original colors is not altered after color rotation. Huang et al. [2007] define  X  (  X  )as
They use different parameters,  X  max and  X  , for left and right planes (left plane means a &gt; 0 and right plane means a &lt; 0), and there are six param-eters involved in all. They use Fletcher-Reeves conjugate-gradient method to find the optimal values of the parameters and the optimization process needs intensive computation. Here we scale up the method by reducing the num-ber of parameters and simplifying the parameter decision process. Intuitively, the parameter  X  max should be more sensitive than the parameter controls the range scope of rotated colors, and, in practical experiments, we have also empirically validated this fact. Thus we only keep the parameter  X  max by simply setting the parameter in left and right planes, which can help balance the blue and yellow hues in the recolored images. Therefore, there is only one parameter function  X  (  X  )
Then, we select the parameter  X  max by grid search from a predefined candi-date set, for which the criterion is to maximize the diversity of the colors on b axis, that is, It is equivalent to maximizing the variance of the b components of the colors.
After local color rotation, we then further adopt a global color rotation to refine the result. We regard each color in the image as a sample in a perform 2D Principle Component Analysis (PCA) to extract the major compo-nent. We then rotate the colors such that the major component is consistent with the discriminative orientation of colorblind users. The discriminative ori-entation stands for the orientation of the 1D surface on which the colors can be best distinguished. It can be calculated that the normals of the approximating criminative orientation and the major component with respect to the a then we can derive that the rotation angle  X  r is
The global rotation can be formulated as where a and b are the mean values of the a and b components, respectively. In fact, if we suppose that the colorblind simulation  X  is equivalent to projecting colors into the discriminative orientation, we can prove that the preceding equation is an optimal global rotation which maximizes that is, optimizing the discriminative abilities of the colors perceived by color-blind users on a  X   X  b  X  plane.

Although the two steps both perform color rotation, they have different im-pacts. In the local color rotation step, there is a stretch effect on the color space such that the space near b* will be condensed, whereas the global color rotation just rolls the colors while keeping the relative position of each color.
However, there is a problem in that the scope of available gamut in the LAB color space is not a sphere, and thus the colors after rotation may not belong to the available gamut anymore (i.e., they do not correspond to meaningful
RGB values). Therefore, we adopt a simple method to shrink the results such that each color value falls into the available gamut. More specifically, for a color [ L , a , b ] that does not belong to the available gamut, we keep performing [ L , a , b ] =  X   X  [ L , a , b ](  X &lt; 1) until it falls into the gamut. In our system, we empirically set the parameter  X  to be 0.98.

Therefore, the recoloring process can be summarized as follows. (1) Transform colors in the image from RGB space to LAB space; (2) Perform local color rotation according to Eq. (10); (3) Perform global color rotation according to Eq. (15); (4) Shrink colors for those that do not belong to the available gamut; (5) Transform the images from LAB space to RGB space. 3.4 Color Indication
Color indication attempts to tell colorblind viewers what color a point is in an image. For example, when users put a mouse on an apple in an image, our tool may indicate  X  X ed X  or  X  X reen X . Figure 7 demonstrates a schematic illustration of color indication. It is accomplished by a look-up table which is built from a color name list and a color difference metric. We establish a list which contains the color names that can be indicated, and then a color difference metric can be used to quantize every possible RGB combination.

The color name list is established based on the X11 color names [Hess 1996] that are standardized by SVG 1.0. It contains 140 colors in all and each color has a corresponding RGB value. However, letting our color name list contain all these 140 colors is not a good choice since many names are actually rarely used and average users are not familiar with them (such as  X  X ightGoldenrodYel-low X ). Therefore, we only select 38 colors, which are illustrated in Table I with their corresponding RGB values. These colors are manually selected communi-cating with practical users and by considering multiple factors, such as their coverage, diversity, and usage frequencies.

In the RGB color space, there are 256  X  256  X  256 colors in all. Now the problem is how to map all these colors to the names in Table I. We adopt a nearest-neighbor approach, that is, for each color in the space (a RGB combina-tion), we compute its difference with the colors in the Table I, and the name of the closest color is selected. This method thus needs an estimator of color dif-ference, and here we adopt the CIE94 color difference metric [CIE 1995]. Based on the color name list and the color difference estimator, we can construct a look-up table that maps every possible RGB combination to a color name. The table contains 256  X  256  X  256 entries in all and costs a storage space of about 16M bytes. 3.5 Services in Accessible Image Search
The accessibility assessment and improvement are offline, and they are imple-mented on the backend. With these two components, we can provide a series of services for colorblind users. We illustrate several typical applications as follows. (1) Rerank top K search results based on accessibility scores. (2) Rerank top K search results based on the combination of original ranking (3) Improve the accessibilities of the originally searched images (i.e., recoloring (4) Rerank search results after improving images X  accessibilities. (5) Improve the accessibility of an individual image. Users can accomplish it (6) Perform color indication on a returned image.
 These choices can be customized and selected according to the users X  preference.
It is also worth mentioning that there are people that have colorblindness but not so severe (an analysis of colorblindness level can be found in Yang et al. [2008a, 2008b]). These people can choose whether to use the services provided by us: for example, if a user only has a very slight level of colorblindness, then he/she can choose not to use our services, and likewise if a user has a high level of colorblindness, he/she can use the services. Actually the features are optional and this makes the system flexible for different degrees of colorblindness (e.g., users can choose whether to rerank/recolor images or not). 3.6 Evaluation of Accessible Image Search
Obviously the existing performance evaluation measures for information re-trieval, such as Average Precision (AP) and NDCG [Jarvelin and Kekalainen 2002], all focus only on relevance. So, we have to provide a complementary per-formance evaluation measure for AIS to estimate its effectiveness in searching accessible images. Here we modify the existing AP evaluation to obtain a new measure named Accessibility Average Precision (AAP), but it is worth noting that we can also extend other measures to AIS, such as NDCG, which take into account the different levels of relevance in comparison of AP. The AAP measurement at M ( AAP @ M , i.e., the AAP measure of the top M results) is defined as where y i is the binary relevance label of i th sample (i.e., y is relevant and otherwise y i = 0), g r is the ground-truth of the r th sample X  X  accessibility score, and G max is the maximum value of the accessibility score which is used to normalize the AAP measurement into [0 , 1]. If we only consider two levels for accessibility (e.g., easily accessible and hardly accessible) and set their scores to 1 and 0, respectively, the formulation of AAP will degenerate to the traditional AP measurement.

Based on the AAP measure, we can easily obtain a Mean Accessibility Aver-age Precision (MAAP) measurement by averaging the AAP measurements of multiple concepts as an overall evaluation. It is the coordinate of the existing
Mean Average Precision (MAP) measurement. Thereby, the performance of AIS can be evaluated with two measures, for example, MAP and MAAP. 4. EXPERIMENTS
We conduct our experiments using 65,443 images collected from a popular com-mercial image search engine. We first select 68 queries from the query log of the image search engine and then collect the top images for each query. The queries are: Party, Cat, Panda, Earth, Dogs, Snakes, Cartoon, Backgrounds, Ronald-inho, Horses, Women, Dragons, Spider, Car, Fish, Boy, Ghosts, Live, Youtube, Birds, Animals, Flowers, Angel, Turtles, People, Heart, Frogs, Chocolates, Cake, Starts, Baby, Beach, Wolves, Weather, Batman, Email, Hairstyles, Trees, Lion, Children, Hawaii, Food, Tiger, Waterparks, Indians, School, Sports, Military, Bees, Medical, Plants, Pigs, Cow, Disney, Flags, Rose, Baseball, Football, Games,
Police, Fruit, Nokia, War, Jesus, Golf, Maps, Cowboys ,and Hotels . It is worth noting that, instead of selecting the queries specifically to fit our algorithms, we have chosen a set of diverse and representative queries as a picture of real-world image search. For simplicity, we use 1 to 68 to denote the IDs of these queries, respectively.

There are 20 anonymous protanopic/deuteranopic users in total (18 male and 2 female) participating in the labeling and user study tasks. These participants come from the two largest cities of China and they have diverse occupations, such as students, teachers, and editors. 4.1 Evaluation of Accessibility Assessment
First, we evaluate the consistency of our accessibility measurement with a subjective test. Three colorblind viewers are involved in the subjective labeling. Every image is manually given a subjective accessibility score 0, 0.5 or and 1 by each volunteer. These three scores indicate low, medium, and high accessibility, respectively. Several typical examples with different accessibility scores are shown in Figure 4.

We compare the three different accessibility assessment methods proposed in Section 3.2. In quality-based accessibility assessment, the parameter C and C 2 are empirically set to 0. For analysis-based accessibility assessment, the parameter Q is empirically set to 4096. For learning-based accessibility assessment, we regard the images collected from the queries 1 to 20 as training data. The images of the other 48 queries are used for testing. Since the training and testing images are collected from different queries, the overfitting effect can be avoided. The parameter  X  in SVR model is empirically set to 0.01, and the trade-off parameter C and the radius parameter are tuned to 5-fold cross-validation. We average the scores provided by the three volunteers and then compute the Pearson correlation of the averaged subjective accessibility scores and our accessibility measurements.

Figure 8 illustrates the results. From the results we can clearly see that the learning-based method performs much better than the quality-based and analysis-based methods. This can be attributed to the fact that the SVR model built on blockwise color moment features can implicitly capture the high-order relationships between accessibility scores and the statistical and spatial dis-tribution of colors through kernel mapping. For example, it can be learned from training data that the central blocks should be more important than the boundary blocks. The mean correlation coefficients obtained by quality-based, analysis-based, and learning-based methods are 0.075, 0.169, and 0.471, re-spectively. Considering these are only linear correlation coefficients, we can conclude that the learning-based accessibility assessment method can achieve rather consistent results with the perception of colorblind viewers (Figure 9 illustrates the measured accessibility scores of the images in Figure 4, and we can see they are reasonable). We also test the sensitivity of the parameter in the learning-based method. Table II illustrates the mean correlation coef-fcients when is set to different values. We can see that the performance is stable when varies in a large range and this demonstrates the robustness of the approach. 4.2 Evaluation of Reranking Strategy for Accessible Search
To evaluate the accessibility-based reranking approach, we adopt the AAP mea-sure introduced in Section 2.2. In addition, we also illustrate the AP measure-ments before and after reranking to investigate the impact of the reranking on search relevance. Based on our conclusion in the last section that the learning-based accessibility assessment is superior to the analysis-based method, we adopt the accessibility scores obtained by the first approach. We rerank the top 300 images for each query with the accessibility scores in descending order, and then we compute the AAP measures before and after reranking. Figure 10 illustrates the top images for the queries tree , flowers and spider before and after reranking. Here we have mimicked the perception of protanopic viewers using the simulation algorithm in Brettel et al. [1997]. The detailed results for different queries are illustrated in Figure 11, from which we can clearly see the improvement of AAP after reranking. The MAAP increases from 0.59 to 0.65 af-ter reranking. This indicates that the AIS system can successfully identify and prioritize the images with better accessibilities. The figure also illustrates the
AP@300 measures before and after reranking. We can see that the reranking has only slightly degraded the search performance in terms of relevance. The
MAP measure degrades from 0.74 to 0.72 after reranking. In fact, as previously mentioned, we can also choose to combine the original ranking scores and the accessibility scores to achieve a trade-off between relevance and accessibility if the relevance after reranking is a concern. The existing relevance improvement methods can also be further integrated [Smeulders et al. 2000; Jing and Baluja 2008a].
 4.3 Evaluation of Accessibility Improvement Algorithm
We now evaluate the accessibility improvement approach introduced in Sec-tion 3.3. To reduce the workload of colorblind labelers, we randomly select only 90 images from each of the 68 queries and obtain a set of 6120 images in this way. We empirically set the parameter candidate set and computational cost. Figure 12 illustrates several examples, including orig-inal images, their protanopic views, and the recolored results (to better demon-strate the effectiveness of the recoloring approach, we have further added two Isharaha plates that are out of the web image dataset).

We compare our recoloring algorithm with two existing methods: (1) Optimization-Based Color Rotation (OBCR) [Huang et al. 2007], (2) Generalized Histogram Equalization (GHE) [Huang et al. 2008].

We choose these two methods because the existing studies have shown their superiority over many other recoloring methods. Each recolored result is given a subjective accessibility score using the method introduced in Section 4.1 with three colorblind labelers. In the labeling process, the results produced by all algorithms are shuffled and blended to generate a fair comparison. Figure 13 illustrates the average accessibility scores of the original images and the recol-ored results using the three algorithms. From the results we can see that the three recoloring methods all achieve better accessibility scores than the orig-inal images. This demonstrates the effectiveness of the recoloring approach.
Among the three recoloring methods, the proposed algorithm performs the best. Our method also has the advantage in computational cost. In our experi-ments, the OBCR, GHE, and our method cost about 10 minutes, 6 seconds, and 1.5 seconds, respectively in processing one image on average. Therefore, the superiority of the proposed method is evident considering both performance and computational efficiency. 4.4 Evaluation of Color Indication
To evaluate the color indication component, we invited ten normal viewers fa-miliar with colors to estimate its indication accuracy. Each participant freely chooses points in searched images and then judge whether the indicated color names are correct in his perception. There are two options:  X  X orrect X  and  X  X ncor-rect X . The  X  X orrect X  option means that the indicated name exactly describes the color of the point, and  X  X ncorrect X  means that there is a better choice than the indicated color. Each participant was asked to implement 100 such judgments, and in this way we collected 1000 results in all. The statistical result is shown in
Table II. We can see that our color indication component can achieve fairly high accuracy. Among the incorrect results, many indicated color names are not un-reasonable. For example, the indicated color name is  X  X irebrick X  but the partici-pant considered  X  X rown X  better, but in fact the two colors are perceptually close. 4.5 User Study
Finally, we conducted a user study with all of the 20 colorblind viewers to evaluate the usability of AIS.

In the accessible search scheme, we recolored all of the images and then reranked the top 300 images for each query based on the evaluated accessibility scores. First, we regarded the original search and the accessible search as two schemes, and then asked the users to freely choose queries and observe the results. The users were asked to rank the two schemes using  X   X  =  X , which mean  X  X etter X ,  X  X uch better X  and  X  X omparable X , respectively. To quantify the results, we converted the ranking information into ratings, and we assigned score 1 to the worse scheme. The other scheme was assigned score 2, 3, and 1 if it is better, much better, and comparable than this one, respectively. The average rating scores and the variances are illustrated in Figure 14. We can clearly see the preference of users towards the accessible search scheme.
We also performed an ANOVA test. The F-statistic of the scheme factor is 64.08 and it can be derived that p &lt; 0 . 000001. This indicates that the difference of the two schemes is significant. The F-statistic of the user factor is 1.0 and it can be derived that p &gt; 0 . 5, and this indicates that the difference among users is statistically insignificant.

Then we conducted a more detailed study. For each query, the users were provided with the original search results and the accessible search results.
The two ranking lists were shuffled and blended, and each user then selected the preferred list from the pair. The percentages of different preferences are illustrated in Figure 15. From the figure, we can see that most users prefered the accessible search results. A small group of users chose the original results.
This can be attributed to two facts: one is that for several queries the top re-sults in the original ranking list already have high accessibilities (we can see in Figure 10 that the AAP measures of several queries can hardly be improved after reranking), and the other is that the relevance of several queries slightly degrades after reranking and some users feel uncomfortable with this. How-ever, considering the diversity of the queries and the anonymous users, these results are enough sufficient to demonstrate the usefulness of the AIS scheme.
The results will also be better if we provide the reranking and recoloring as the additional options that can be chosen by the users. 5. CONCLUSION
This article describes an AIS system which serves colorblind users. Different from the conventional image search that aims to return more relevant im-ages to users, the AIS system takes into account the accessibility of images. It contains image accessibility assessment, accessibility improvement, and color indication components, and it can provide a series of services to accommodate colorblind users in different aspects, such as prioritizing the images with high accessibility in the search results and recoloring images to improve their ac-cessibility. Experiments and user studies have demonstrated the effectiveness of the system. In our future work, we will investigate different features for accessibility assessment and we will also conduct a study with more users and queries on the system.

