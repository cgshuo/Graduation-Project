
Chinese Academy of sciences a ranked list of the objects.
 ample, measures such as Normalized Discounted Cumulative G ain (NDCG) [8] and Mean Average of the loss functions can really lead to the optimization of t he ranking measures . 2 the relationship between ranking measures and the pairwise /listwise losses. and the ranking measures.
 minimum value of the weighted sum over all the permutations i n the set. functions can effectively maximize NDCG and MAP.
 measures. 2.1 Loss functions in learning to rank Let x = { x as multi-level ratings L = { l (1) , ..., l ( n ) } , where l ( i )  X  X  r as Several approaches have been proposed to learn the optimal r anking function. in subset regression [5], the loss function is as follows, all have the following form, where the  X  functions are hinge function (  X  ( z ) = (1  X  z ) and logistic function (  X  ( z ) = log(1 + e  X  z ) ) respectively, for the three algorithms. in ListMLE [16], the following loss function is used, for any two objects x represents the index of the object ranked at the i -th position in y . 2.2 Ranking measures ranking function. Here we introduce two of them, NDCG [8] and MAP[1], which are popularly used in information retrieval.
 NDCG is defined with respect to K -level ratings L , where  X  z  X  C , and D ( z ) = 0 if z &gt; C ( C is a fixed integer).
 MAP is defined with respect to 2-level ratings as follows, where I k objects as having label 1 [11].
 call them measure-based ranking errors . 2.3 Previous bounds The regression based pointwise loss is an upper bound of (1  X  NDCG), The classification based pointwise loss is also an upper boun d of (1  X  NDCG), where  X  l ( i ) is the label of object x For the pairwise approach, the following result has been obt ained [9], values are even not sufficient for the zero value of (1-MAP).
 two approaches. necessary condition for the zero values of (1  X  NDCG) and (1  X  MAP). 3.1 Essential loss: ranking as a sequence of classifications In this subsection, we describe the essential loss for ranking.
 of the object that is ranked at the i -th position in y . We denote Y Second, given each permutation y  X  Y quential steps. For each step s , we distinguish x denote x ( definition of T s classification errors of all individual tasks, and then define the minimum value of the weighted sum over all t he permutations in Y essential loss for ranking .
 Denote the ranked list produced by f as  X  loss is a sufficient and necessary condition for the zero valu es of (1-NDCG) and (1-MAP). 3.2 Essential loss: upper bound of measure-based ranking er rors when specific weights  X  ( s ) are used.
 Theorem 1. Given K -level rating data ( x , L ) with n then  X  f , the following inequalities hold, permutation set Y position r in  X   X  f y ( s ) Second, we consider the essential loss case by case. Note tha t Then  X  y  X  Y  X  f ( y ( i )) 2-level rating data ( x , L ) , it can be proved (see Lemma 1 in [4]) that L where i n first object with label 0 is ranked after position n before the objects with label 0 . Thus n If 0 . Thus n 2-level ratings.
 is easy to verify Y
Using the result for 2-level ratings, we obtain 3.3 Essential loss: lower bound of loss functions in ListMLE are all upper bounds of the essential loss, i.e., losses using permutation set Y where y is an arbitrary permutation in Y loss. Thus, the value of the pairwise loss is equal  X  y  X  Y Second, we consider the value of a T if increasing, and  X  (0) = 1 , we have,
If a T a T f ( x ( s ) ) , y ( s ) . Therefore, Third, it can be proved (see Lemma 2 in [4]) that the following inequality holds, Considering inequality (8) and noticing that the pairwise l osses are equal  X  y  X  Y case. Consider the loss of ListMLE in Eq.(3).  X  y and  X  s  X  X  1 , 2 , ..., n  X  1 } , if I (i.e.,  X  i By further relaxing the inequality, we obtain the following result, 3.4 Summary and (1  X  MAP). (2) The listwise loss in ListMLE is an upper bound of (1  X  NDCG) and (1  X  MAP). idea is to introduce weights related to  X  of (1  X  NDCG).
 (1 We took RankNet and ListMLE as example algorithms. The metho ds that minimize the weighted W-RankNet and W-ListMLE significantly outperform RankNet a nd ListMLE. (2) W-RankNet and W-ListMLE also outperform other baselines on LETOR such as R egression, Ranking SVM, Rank-optimizing tighter bounds of the ranking measures can lead t o better ranking performances. theoretical analysis. As future work, we plan to investigat e the following issues. essential loss is an upper bound of other measure-based rank ing errors. (3) We have taken the loss functions in Ranking SVM, RankBoos t, RankNet and ListMLE as ex-methods, such as RankCosine [13], ListNet [3], FRank [15] an d QBRank [19]. sential loss and the measure-based ranking errors.
