 Primary focus of Information retrieval (IR) systems has been to optimize for Relevance . Existing approaches used to rank documents or evaluate IR systems do not account for  X  X ser effort X . At present, relevance captures topical overlap be-tween document and user query. This mechanism does not take into consideration either time or effort of end user to satisfy information need. While a judge may spend time assessing a document, an end user may not thoroughly ex-amine a document. We identified factors that are associated with effort for a single document and gathered judgments for same. We also investigated the role of several features in predicting effort on webpage. In future, we shall investigate role of effort on mobile and investigate effort based evalua-tion methodology that also takes into account user X  X  search task.
Search engines measure the estimate document relevance on basis of Topicality . A document is considered relevant to user X  X  query if its content topically overlaps with user X  X  information need. Information Retrieval (IR) systems are designed to optimize for relevance. It is assumed that rele-vant documents shall answer user X  X  information need, which in turn will yield higher user satisfaction. Evaluation met-rics are also designed to compare systems on basis of how many relevant documents are retrieved and where are they shown on SERPs.

At present, researchers rely on pre-designed small-scale test collections to evaluate effectiveness of IR systems known as batch evaluation . These test collections consist of some documents manually judged for relevance for pre-defined queries. Live users are also used to evaluate system X  X  search effectiveness. User based evaluation relies on observing and measuring user X  X  interaction with document to determine its relevance. One would expect that batch evaluation would agree with user-based evaluation of systems. However it has been shown in the past [1] that these two forms of evalua-tion do not agree with each other. Recently, Yilmaz et al . [4] proposed that this mismatch is due to the disagreement between what judges and users consider relevant. Trained judges are asked to identify document relevance regardless of how much time and effort it may take to consume it. While a judge can take significant time to evaluate a document, an end user may not be willing to spend as much time consum-ing it, even if it is relevant. They report that the primary reason of mismatch between explicit and implicit document judgments is a result of effort needed to find and consume required information from a given document.

Thus, we need to design systems and evaluation method-ologies that consider both user effort and relevance to rank documents or compare systems. Effort would becomes cru-cial with change in device of information access. In mobile, where interaction is limited by screen size and restricted text-input, users are more likely to give up finding informa-tion via search engines. Thus, we need to characterize effort across devices. Finally, effort based evaluation of search en-gines would have to be designed around user X  X  search tasks. For instance, systems that rank documents high effort doc-uments on the top for easy tasks should be penalized more than those that rank low effort documents (on top) for rel-atively difficult task.

With the above mentioned motivations we wish to ex-plore ways to accommodate user effort in IR evaluation. We shall focus on three research questions regarding effort. RQ1: What constitutes user effort in IR. What parame-ters are most representative of user effort. Given a docu-ment and user query, what features will be useful to predict effort based judgments. We identified three factors [3] to be associated with effort, gathered judgments (both explicit and preference based) and investigated their correlation with user satisfaction. Our findings suggest that ease of finding information dominates all other factors. RQ2: How can ex-isting learning to rank framework be modified to optimize for both relevance and effort. RQ3: How does effort vary with user device. What factors characterize effort on mobile and desktop. We conducted a user study [2] to understand variation in relevance with device. We shall use this data to control for relevance and investigate role of effort on mo-biles. RQ4: Finally, how can we design effort based evalua-tion methodology which takes into account user X  search task while rewarding or penalizing systems. [1] M. Sanderson, M. L. Paramita, P. Clough, and [2] M. Verma and E. Yilmaz. Characterizing relevance on [3] M. Verma, E. Yilmaz, and N. Craswell. On obtaining [4] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and
