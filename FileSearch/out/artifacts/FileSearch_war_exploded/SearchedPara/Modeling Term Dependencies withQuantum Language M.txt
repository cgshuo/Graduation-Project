 Traditional information retrieval (IR) models use bag-of-words as the basic representation and assume that some form of independence holds between terms. Representing term dependencies and defining a scoring function capable of in-tegrating such additional evidence is theoretically and prac-tically challenging. Recently, Quantum Theory (QT) has been proposed as a possible, more general framework for IR. However, only a limited number of investigations have been made and the potential of QT has not been fully explored and tested. We develop a new, generalized Language Model-ing approach for IR by adopting the probabilistic framework of QT. In particular, quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies. This naturally allows us to avoid the weight-normalization prob-lem, which arises in the current practice by mixing scores from matching compound terms and from matching single terms. Our model is the first practical application of quan-tum probability to show significant improvements over a ro-bust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Density Matrices; Language Modeling; Retrieval Models
The quest for the effective modeling of term dependen-cies has been of central interest in the information retrieval (IR) community since the inception of first retrieval models. However, the gradual shift towards non bag-of-words mod-els is strewn with modeling difficulties. One of the central problems is to find an effective way of representing and scor-ing documents based on such dependencies. As pointed out by Gao et al. [9], dependencies can be handled in two ways.
The first approach is to extend the dimensionality of the representation space. In early geometrical retrieval models such as the Vector Space Model (VSM), dependencies arising from phrases (compound terms) are represented by defining additional dimensions in the space, i.e. both the phrase and its component single terms are regarded as representation features [8, 21, 28]. For example, computer architecture is considered as disjoint from computer and architecture , which is a strong modeling assumption, and does not take advan-tage of the semantic relation that generally exists between a compound phrase and its component terms.

The second approach is more principled in such that sim-ple terms are kept as representational units and term de-pendencies are modeled statistically as joint probabilities, i.e. p ( computer , architecture ). Proposed dependence mod-els such as n -gram Language Model (LM) for IR [30], biterm LM [31] or the dependence LM [9] adopt such a represen-tation. However, the gain from integrating dependencies was smaller than hoped [35] and it came with higher com-putational costs due to dependency parsing or n -gram mod-els [13, 30], or unsupervised iterative methods for estimating the joint probability [9].

Recently, non bag-of-words models such Markov random field (MRF) [19], quasi-synchronous dependence model [24] and the query hypergraph model [2] have been proposed. Most of these retrieval models take a log-linear form, which offers a very flexible way of taking into account term depen-dencies by integrating different sources of evidence, such as proximity heuristics and exact matching. However, the LM is used as a black box to estimate single-term and compound-term influences separately and then the model combines them to compute the final score. We believe that, from a representational point of view, these models have implicitly made a turn back to the first VSM approach in the sense that the dependencies are assumed to represent additional concepts, i.e. atomic units for the purpose of document and query representation, thus disjoint from the component terms [2, 3]. This choice indeed allows for flexible scoring functions. However, the retrieval model boils down to a com-bination of scores obtained separately from matching single terms and from matching compound dependencies. This is the main cause of the weight-normalization problem [9, 11] which is that a dependency may be counted twice, as a compound and as component terms. In the context of phrases, Sparck Jones et al. note that  X  X he weight of the phrase should reflect not the increased odds of relevance im-plied by its presence as compared to its absence, as a whole unit, but the increased odds compared to the presence of its components words X  X 11]. When integrating the evidence, the weights for the combination are usually estimated by opti-mizing a retrieval measure such as mean average precision (MAP). In this sense, a principled probabilistic interpreta-tion of these models is difficult.

The pioneering work by Van Rijsbergen [33] officially for-malized the idea that Quantum Theory (QT) could be seen as a  X  X ormal language that can be used to describe the ob-jects and processes in information retrieval X . The idea of QT as a framework for manipulating vector spaces and probabil-ity is appealing. However, the methods that stem from this initial intuition provided only limited evidence about the usefulness and effectiveness of the framework for IR tasks. For example, Piwowarski et al. [25] test if acceptable perfor-mance for ad-hoc tasks could be achieved with a quantum approach to IR. The authors represent documents as sub-spaces and queries as density operators. However, both doc-uments and queries representations are estimated through passage-retrieval like heuristics, i.e. a document is divided into passages and is associated to a subspace spanned by the vectors corresponding to document passages [25]. Different representations for the query density matrix are tested but none of them led to good retrieval performance. Succes-sively, a number of works took inspiration from quantum phenomena in order to relax some common assumption in IR [37, 38]. Zuccon and Azzopardi [38] introduce interfer-ence effects into the Probability Ranking Principle (PRP) in order to rank interdependent documents. Although this method achieves good results, it does not make principled use of the quantum probability space and cannot be con-sidered as evidence towards the usefulness of the enlarged probabilistic space. In general, these methods made heuris-tic use of the concepts of the theory and no clear probabilis-tic interpretation can be given.

The intrinsic heuristic flavor in preceding approaches mo-tivated some authors to provide evidence to the hypothesis that there exists an IR situation in which classical proba-bilistic IR fails, or it is severely limited, and it is thus neces-sary to switch to a more general probabilistic theory [16, 17, 34]. Although these works are theoretically grounded and heavily influenced our general vision of the theory, no clue is given on how to operationalize such results in real-world applications.

In this paper, we propose a novel retrieval framework for modeling term dependencies based on the probabilistic cal-culus offered by QT. In our model, both single terms and compound dependencies are mathematically modeled as pro-jectors in a vector space, i.e. elementary events in an en-larged probabilistic space. In particular, a compound de-pendency is represented as a superposition event which is a special kind of projector that is neither disjoint from its component terms, nor a joint event. Documents and queries are represented as a sequence of projectors associated to a quantum language model (QLM), encapsulated in a partic-ular matrix. The scoring function is a divergence between query and document QLMs. We will show that our model is a generalization of classical unigram LMs. To our knowl-edge, this work can be seen as the first work to use the quan-tum probabilistic calculus in order to achieve improvements over state-of-the-art models.

Our contributions are as follows: 1. We propose a novel application of quantum probability 2. Using this approach, we show significant improvements 3. We propose a new way of representing dependencies 4. We show how the new representation of the depen-5. In our model, the dependency information is not in-
In quantum probability, the probabilistic space is natu-rally encapsulated in a vector space, specifically a Hilbert space, noted H n , but for the sake of simplicity, in this pa-per we limit ourselves to finite real spaces, noted R n . We will be using Dirac X  X  notation restricted to the real field, for which a unit vector ~u  X  R n , k ~u k 2 = 1 and its transpose ~u are respectively written as a ket | u i and a bra h u | . Using this notation, the projector onto the direction u writes as | u ih u | . The inner product between two vectors writes as h u | v i . Moreover, we note by | e i i the elements of the stan-dard basis in R n , i.e. | e i i = (  X  1 i , . . . ,  X  ni i = j .

Events are no more defined as subsets but as subspaces, more specifically as projectors onto subspaces [23, 34]. Given a 1-dimensional subspace spanned by a ket | u i , the projector onto the unit norm vector | u i , | u ih u | , is an elementary event of the quantum probability space, also called a dyad . A dyad is always a projector onto a 1-dimensional space. Given the bijection between subspaces and projectors, it is correct to state that | u i is itself an elementary event. For example, if n = 2, the quantum elementary events | e 1 i = (1 , 0) | f i = ( 1  X  Generally, any ket | v i = of the {| u i i} where {| u 1 i , . . . , | u n i} form an orthonormal basis. In order to see the generalization that is taking place, one has to consider that in R n there is an infinite number of vectors even if the dimension n is finite. Hence, contrary to the classical case, an infinite number of elementary events can be defined.
A quantum probability measure is the generalization of a classical probability measure such that (i) for every dyad ability measure for any orthonormal basis {| u 1 i , . . . , | u i.e.
 for any real vector space with dimension greater than 2, there is a one-to-one correspondence between quantum prob-ability measures and density matrices  X  . The form of this correspondence is given by: A real density matrix is symmetric,  X  =  X   X  , positive semidef-inite,  X   X  0, and of trace 1, tr  X  = 1 1 . From now on, the set of n  X  n real density matrices would be noted S n .
By Gleason X  X  theorem, a density matrix can be seen as the proper quantum generalization of a classical probability distribution. It assigns a quantum probability to each one of the infinite dyads. For example, the density matrix: assigns probabilities tr(  X  | e 1 ih e 1 | ) = 0 . 5 and tr(  X  | f 1. Hence, the event | f 1 ih f 1 | is certain and still there is non-classical uncertainty on | e 1 ih e 1 | . Only if {| u form an orthonormal system of R n can the dyads | u i ih u understood as disjoints events of a classical sample space, i.e. their probabilities sum to one. The relation that ties | e and | f 1 ih f 1 | is purely geometrical and cannot be expressed using set theoretic operations.

Any classical discrete probability distribution can be seen as a mixture over n elementary points, i.e. a parameter ~ (  X  1 , . . . ,  X  n ), where  X  i  X  0 and trix is the straightforward generalization of this idea by con-sidering a mixture over orthogonal dyads  X  = where  X  i  X  0 and one can find the components dyads by taking its eigende-composition and building a dyad for each eigenvector. We note such decomposition by  X  = R  X  R  X  = where | r i i are the eigenvectors and  X  i their corresponding eigenvalues. This decomposition always exists for density matrices [23].

Conventional probability distributions can be represented by diagonal density matrices. The sample space corresponds to the standard basis E = {| e i ih e i |} n i =1 . Hence, the density matrix corresponding to the parameter ~  X  above can be repre-sented as a mixture over E , i.e.  X   X  = diag( ~  X  ) = Consider a vocabulary of two terms V = { a, b } . A unigram language model ~  X  = (0 . 75 , 0 . 25) defined on V is represented by: Hence, term projectors are orthogonal, i.e. terms correspond to disjoint events. For example, the probability of the term a is computed by tr(  X   X  | e a ih e a | ) = 0 . 75. As conventional probability distributions are restricted to the identity eigen-system, they differ in their eigenvalues, which correspond to diagonal entries. On the contrary, general density matrices can differ also in the eigensystem. For example, the density matrix  X  of Eq. 3 has eigenvector | f 1 i = ( 1  X  eigenvalue 1 and the eigenvector | f 2 i = ( 1  X  eigenvalue 0. Hence, it can be represented as a one-element mixture containing the projector  X  = | f 1 ih f 1 | . When the mixture weights are concentrated into a single projector, the corresponding density matrix is called pure state . Otherwise, it is called mixed state .

When defined over R n , density matrices can be seen as el-lipsoids, i.e. deformations of the unit sphere (Figure 1) [34]. Classical probability distributions, i.e. diagonal density ma-trices, are ellipsoids stretched along the identity eigensys-tem. As quantum probability has access to an infinite num-ber of eigensystems, the ellipsoid can be  X  X otated X , i.e. de-fined on a different eigensystem. In this work, we will use this additional feature in order to build a more reliable rep-resentation of documents and queries taking into account more complex information than single terms.
The approach Quantum Language Modeling (QLM) re-tains the classical Language Modeling for IR as a special case. Hereafter, we will present in details the quantum counterpart of unigram language models. Although it is not explicitly developed in this paper, we argue that arbitrary n -gram models could be modeled as well.
In classical bag-of-words language models, a document d is represented by a sequence of i.i.d. term events, i.e. W d = { w i : i = 1 , . . . , N } , where N is the document length. Each w i belongs to a sample space V , corresponding to the vocabulary, of size n . It is assumed that such sequences cor-respond to a sample from an unknown distribution ~  X  over the vocabulary V , for which we want to gain insight.
A quantum language model assigns quantum probabilities to arbitrary subsets of the vocabulary. It is parametrized by an n  X  n density matrix  X  ,  X   X  S n , where n is the size of the vocabulary V . In QLM, a document d is considered as a sequence of M quantum events associated with a density matrix  X  : where each  X  i is a general dyad | u ih u | and represents a sub-set of the vocabulary. Note that the number of dyads M can be different from N , the total number of terms in the doc-ument. The sequence P d is constructed from the observed terms W d : we have to define how to map subsets of terms to projectors. Separating the observed text from the observed projectors constitutes the main flexibility of our model. In what follows, we define a way of mapping single terms and arbitrary dependencies to quantum elementary events. For-mally, we seek to define a mapping m : P ( V )  X  L ( R n ), where P ( V ) is the powerset of the vocabulary and L ( R the set of dyads on R n . As an initial assumption, we set m (  X  ) = O , where O is the projector onto the zero vector.
In Section 2.2, we showed that unigram sample spaces can be represented as the set of projectors on the standard basis E = {| e i ih e i |} n i =1 and unigram language models can be represented as mixtures over E , i.e. diagonal matrices. Therefore, a straightforward mapping from single terms to quantum events is: where w  X  V . This choice associates the occurrence of each term to a dyad | e w ih e w | , and these dyads form an or-thonormal basis. Hence, occurrences of single terms are still represented as disjoint events. Consider n = 3 and V = { computer , architecture , games } . If W d = { computer , archi-tecture } and one applies m to each of the terms, the sequence where E w = | e w ih e w | :
E Note that if we decide to observe only single terms, P d turns out to be the quantum counterpart of classical observed terms W d , i.e. M = N .
In this paper, by dependency, we mean a relationship linking two or more terms and we represent such an en-tity abstractly by a subset of the vocabulary, i.e.  X  = { w 1 , . . . , w K } . We define the following mapping for an arbi-trary dependency  X  : m (  X  ) = m ( { w 1 , . . . , w K } ) = |  X  ih  X  | , |  X  i = where the coefficients  X  i  X  R must be chosen such that P Figure 2: The dependency  X  ca is modeled as a pro-jector onto |  X  ca i , i.e. as a superposition event. |  X  i . The well-defined dyad |  X  ih  X  | is a superposition event. As we showed in Section 2.2, superposition events are justifi-able only in the quantum probabilistic space. They are nei-ther disjoint from their constituents | e w i ih e w i | nor do they solely constitute joint events in the sense of n-grams: here, the compound dependency is not considered as an additional entity, as done in previous models [2, 3, 19, 21]. The pro-posed mapping allows for the representation of relationships within a group of terms by creating a new quantum event in the same n -dimensional space.

In addition, superposition events come with a flexible way in quantifying how much evidence the observation of depen-dency  X  brings to its component terms. This is achieved by changing the distribution of the  X  i : if one wants to attempt a classical interpretation, the  X  i can be viewed as relative pseudo-counts, i.e. observing |  X  ih  X  | adds fractional occur-rence to the events of its component terms | e w i ih e w our knowledge, until now this feature has been only mod-eled heuristically, or not modeled at all. In our framework, it fits nicely in the quantum probabilistic space by specify-ing how a compound dependency event and its constituent single terms events are related.

As an example, one could model the compound depen-dency between computer and architecture ,  X  ca = { computer , architecture } , by the dyad K ca = |  X  ca ih  X  ca | , where |  X  p 2 / 3 | e c i + ple taken above, the event is represented by the matrix: The superposition coefficients entail that observing K ca more evidence to | e c ih e c | than to | e a ih e a | .
Once we have defined the mapping m , one must ask three questions: 1. Which compound dependencies to consider? 2. When does such a compound dependency hold in a 3. When the compound dependency is detected, should
Regarding the first question, one may (a) use a dictio-nary of phrases or frequent n -grams, or (b) assume that any Figure 3: Two possible quantum sequences P i d of an excerpt W computer architecture is associated to a superposition projector K projectors. For P 2 d we observed only the compound while in P subset of terms that appear in short queries are candidate compound dependencies to capture. In this paper, we want to make the approach as independent as possible of any lin-guistic resource. So the second approach (b) is used. This will also allow us to make a fair comparison with the previ-ous approaches using the same strategy (such as the MRF model [19]).

The second question regards whether such selected com-pound dependencies hold in a given document. In other words, one has to decide when to add the selected depen-dency projector into a document sequence P d . This can be done for example by assuming that the components terms in the dependency appear as a bigram in a document, as biterm or in a unordered window of L terms. Convergent evidence from different works [1, 12, 14, 18, 31, 36] confirms that proximity is a strong indicator of dependence. There-fore, in this work we choose to detect a dependency if its component terms appear in a fixed-window of length L .
The third question regards how to apply the mapping m and can be more easily understood by a practical example. Consider a document W d = { computer , architecture } and a query W q = { computer, architecture } . Once the depen-dency  X  ca = { computer, architecture } has been detected in the document, i.e. the component terms appear next to each other, one can further decide: 1. to map only the dependency, i.e. P d = {K ca } , 2. to map both the dependency and the component terms, These two choices are illustrated in Figure 3. The first choice is a highly non-classical one because it completely steals the occurrence of its component terms. Nevertheless, it becomes a valid choice in our framework. Differently from classical approaches, the fact that we only consider a count for the compound computer architecture does not mean that we as-sume that the terms computer and architecture do not occur. The dependency event is not disjoint from the single term events, and its occurrence partially entails the occurrence of its component terms. However, this choice is more dan-gerous because it over-penalizes the component terms: we should know very precisely when such a strong dependency is observed and which coefficients to assign to it.
The second choice is implicitly done in current dependency models and is at the basis of the weight-normalization prob-lem. From this point of view, the sequence P d could be seen as composed by concepts as recently formalized by Bender-sky et al. [2, 3]. However, there are crucial differences from that work: (1) we give a clear probabilistic status to such concepts and (2) we do not assume that concepts are atomic units of information, completely unrelated from each other. In classical dependence models, single terms and compound dependencies are scored separately and then the scores are combined together [2, 19, 35]. A critical aspect of such mod-els is that the occurrence of the phrase computer architec-ture will be counted twice -as single terms and as a com-pound. That is why the score on compound dependencies must be reweighed before integrating it with the indepen-dence score [9, 11, 19]. Contrary to classical models, our model does not suffer from such a problem because the ev-idences brought by the compound dependency as a whole and by its component terms are integrated in the estima-tion phase. Even if not reported explicitly in the experi-ments section, conducted experiments show that including projectors for both the dependency and its subsets is much more effective for the ad-hoc task evaluated here and thus this strategy will be preferred throughout this paper. In addition, an algorithm building the sequence of projectors from the document sequence will be presented in Section 4.3.1.
Given that a document is represented by a set of observed projectors, one has to find ways to learn a quantum lan-guage model  X  to associate with a document. In QT, a number of objective functions have been proposed to esti-mate an unknown density matrix from a set of projectors: Linear Inversion [23] and Hedged ML [4] are notorious exam-ples. In this work, we use the Maximum Likelihood (ML) formulation proposed in [15], because (1) it can easily be seen as a quantum generalization of a classical likelihood function (2) contrary to linear inversion, ML generates a well-defined density matrix, i.e.  X   X  S n , and (3) proposed estimation methods remain computationally affordable in high-dimensional spaces.

Given the observed projectors P d = {  X  1 , . . . ,  X  M } for document d , we define as training criterion for the quan-tum language model  X  the maximization of the following product proposed in [15] and corresponding in the unigram case to a proper likelihood: The estimate b  X  can be obtained by approximately solving the following maximization problem: This maximization is difficult and must be approximated by using iterative methods. In [15], the following iterative scheme is proposed, also called the  X  R X R algorithm X . One introduces the operator: and updates an initial density matrix b  X  (0) by applying repet-itive iterations: in order to ensure that b  X  ( k +1) respects the constraint of uni-tary trace [15]. Despite the R X R algorithm being a quantum generalization of the well-behaving Expectation Maximiza-tion (EM) algorithm, the likelihood is not guaranteed to increase at each step because the nonlinear iteration may overshoot, similarly to a gradient descent algorithm with a too big step size. Characterizing such situations still remains an open problem [27]. In this work, in order to ensure con-vergence, if the likelihood is decreased at k + 1, we use the following damped update: where  X   X  [0 , 1) controls the amount of damping and is op-timized by linear search in order to ensure the maximum in-crease of the training objective 2 . As S n is convex [23], e  X  is a proper candidate density matrix. The process stops if the change in the likelihood is below a certain threshold or if a maximum number of iterations is attained.

From an IR point of view, the metric divergence prob-lem [22] tells us that the maximization of the likelihood does not mean that the evaluation metric under consideration, such as mean average precision, is also maximized. In the experiments section, we address the two following questions from a perspective closer to IR concerns: 1. Which initial matrix b  X  (0) to choose? 2. When to stop the update process? As the estimation of a quantum document model requires an iterative process, one may believe that the complexity will make the process intractable. In Section 4.5, we provide an analysis of the complexity of the proposed computation, which will show that the process is quite tractable.
The ML estimation presented above suffers from a gen-eralization of the usual zero-probability problem of classi-cal ML, i.e. the estimator assigns zero probability to un-seen data [35]. This is also called the zero eigenvalue prob-lem [4]. Bayesian smoothing for density matrices has not yet been proposed. This may be because Bayesian inference in the quantum setting has just started to be the subject of intensive research [5, 34]. In this work, we propose to smooth density matrices by linear interpolation [35]. If b  X  a document quantum language model obtained by ML, its smoothed version is obtained by interpolation with the ML collection quantum language model b  X  c : where  X  d  X  [0 , 1] controls the amount of smoothing. As the set of density matrices S n is convex, the resulting  X  is a proper density matrix. In this work, we assume that  X  d =  X  (  X  + M ) , which is the well-known form of the parameter for Dirichlet smoothing [35].
The flexibility of the Kullback Liebler (KL) divergence approach in keeping distinct query and document represen-tations makes it attractive for a candidate scoring function in our new framework. The direct generalization of classi-cal KL divergence was introduced by Umegaki in [32] and is called quantum relative entropy or Von-Neumann (VN) di-vergence . Given two quantum language models  X  q and  X  d for the query and a document respectively, our scoring function is the negative query-to-document VN divergence: where log applied to a matrix denotes the matrix logarithm, i.e. the classical logarithm applied to the matrix eigenvalues. Rank equivalence is obtained by noting that tr(  X  q log  X  does not depend on the particular document. Denote by  X  = sitions of the density matrices  X  q and  X  d respectively. By substituting into the above equation, the scoring function rewrites as:
Compared to a classical KL divergence, the additional term h q i | d j i 2 quantifies the difference in the eigenvectors between the two models. Following the representation in-troduced in Section 2.2, the VN divergence compares two ellipsoids not only by differences in the  X  X hape X  but also by differences in the  X  X otation X .

If a VSM-like interpretation is attempted, one can think about {| q i i} , {| d j i} as semantic concepts for the query and the document respectively, whereas the vectors of eigenval-ues ~  X  q , ~  X  d denote the importance of the corresponding se-mantic concepts in the two models. The VN divergence offers a way of matching query concepts by analyzing how much such concepts are related to documents concepts, i.e.  X  i, j , h q i | d j i 2 . Particularly, can be interpreted as the quantum probability associated with the pure state | q i ih q i | for the elementary event | d i.e. q i ( | d j ih d j | ) = tr( | q i ih q i | d j ih d could rewrite Eq. 16 as: Therefore, the VN divergence scores a document based on the expectation of how important concept | q i i is in document d even if it does not appear in it explicitly. Figure 4: A synthetic example of QLM with a vocab-ulary of n = 3 terms. The orthogonal rays are the eigenvectors of the ellipsoids.  X  q is not smoothed thus degenerates onto a ray.  X  d 1 rotates towards the direction of observed query dependencies and is thus ranked higher.
The estimation and scoring process of quantum language models retains classical unigram LMs and KL divergence as special cases. The classical unigram LM is recovered by restricting the maximization in Eq. 10 to diagonal density matrices and including into the sequence of projectors P d only an orthonormal basis, such as the elements of E . Clas-sical KL divergence is recovered by noting that if  X  q and  X  are diagonal density matrices, they share the same eigensys-tem. Hence, | q i i = | d i i and  X  qi =  X  qi ,  X  di =  X  di ~  X  are the parameters of classical unigram LMs for the query and the document respectively. In this setting, h q i | d for i 6 = j and the VN divergence reduces to classical KL, i.e.
In Figure 4, we report a synthetic example of the appli-cation of the model. We plot the density matrices obtained by the MLE (Section 3.2.1) on the sequence of projectors reported in the table. As usual in ad-hoc tasks, we smooth only the QLMs of the documents. The model corresponding to the query is a projector, i.e. it has two zero eigenvalues, because we did not apply smoothing. If the dependencies are included in the sequence P o , the MLE rotates the cor-responding QLM towards the direction spanned by the ob-served projector (i.e. K ca ). This entails that the model  X  is considered more similar to the query than the model  X  d which corresponds to a classical language model.
All the experiments reported in this work were conducted using the open source Indri search engine (version 5.3) 3 test collections used are reported in Table 1. We choose the collections in order to vary (1) the collection size and (2) collection type. This will produce a comprehensive test set in order to verify the properties of our approach. All the Table 1: Summary of the TREC collections used to support the experimental evaluation. collections have been stemmed with the Krovetz stemmer. Both documents and queries have been stopped using the standard INQUERY stopword list. For all the methods, the Dirichlet smoothing parameter is set to the default Indri value ( = 2500). The optimization of all the other free pa-rameters for the proposed model and the baselines is done using five-fold cross validation using coordinate ascent [18] with mean average precision (MAP) as the target metric. The performance is measured on the top 1000 ranked doc-uments. In addition to MAP, for newswire collections we report the early precision metric @10 (precision at 10) and for web collections with graded relevance judgements we re-port the recent ERR@10, which correlates better with click metrics than other editorial metrics [6]. The statistical sig-nificance of differences in the performance of tested meth-ods is determined using a two-sided Fisher X  X  randomization test [29] with 25,000 permutations evaluated at  X  &lt; 0 . 05.
Our experimental methodology goes as follows. In a first step, we compare our QLM approach to a unigram Language Modeling baseline (denoted LM ) based on Dirichlet smooth-ing [35], which is a strong bag-of-words baseline. This com-parison is done by assigning uniform superposition weights to each dependency  X  , i.e.  X  i = 1 / cardinality of  X  (denoted QLM-UNI ). This step has two main objectives: (1) to test if quantum probability can bring bet-ter performance than a standard bag-of-words model and (2) to test if uniform superposition weights are a reasonable baseline setting.

As a second step, we test the proposed model against the strong non bag-of-words MRF model, which has shown to be highly effective especially for large scale web collections [19, 20]. We test the full dependence version of the model (de-noted MRF-FD ) which captures dependencies between all the query terms and thus is the most natural choice for a com-parison with our model. However, MRF-FD exploits both proximity (#uw) and exact matching (#1). As our model only exploits proximity as an indicator of dependence, we also propose to test the variant MRF-FD-U , which is a MRF using only the proximity feature. This could provide inter-esting insights on how the models score based upon the same evidence.

Finally, we propose a slightly more elaborate version of our model (denoted QLM-IDF ) in which the superposition weights are no more assumed to be uniform. Instead, we assign to each  X  i the normalized idf weight of the corre-sponding term w i . The objective is to test if a more reason-able parametrization of superposition weights can improve the retrieval effectiveness. matrix for SJMN, TREC7-8 and WT10g (left, center and right).
All the results exposed in this paper have been obtained by reranking. We rerank a pool of 20000 documents retrieved using LM in order to make a fair comparison between our method and the baselines.
Very similarly to MRF-FD , given a query Q = { q 1 , . . . , q we assume that the interesting dependencies to consider cor-respond to the power set P ( Q ) 4 . In order to build the set of projectors for the given document we apply Algorithm 1. Algorithm 1 Builds the sequence P d given W d , Q Require: W d , Q 2: for  X   X  X  ( Q ) do 3: for #(  X , W d ) do 5: end for 6: end for
For each dependency  X  in P ( Q ), the algorithm scans the document sequence W d . For each occurrence of  X  , it adds a projector m (  X  ) to the sequence P d . The function #(  X , W returns how many times the dependency  X  is observed in W d Therefore, the algorithm adds as many projectors as the number of detected compound dependencies. Note that by looping on P ( Q ), we are actually implementing the strategy exposed in Section 3.1.3, i.e. adding both the dependence and all of its subsets. Following Section 3.1.3, we choose to parametrize # as the unordered window operator in Indri (#uw L ). Therefore, a given dependency  X  will be detected if the component terms appear in any order in a fixed-window of length L = l |  X  | . This kind of adaptive parametrization of the window length is state-of-the-art for dependence models such as MRF-FD [2, 19]. For all the dependence models, the coordinate ascent for l spans { 1 , 2 , 4 , 8 , 16 , 32 } , which is a robust pool covering different window lengths, including the standard value ( l = 4) for MRF-FD .
Before doing any comparisons, we answer the questions related to the construction of a quantum language model, i.e. (1) how to initialize b  X  (0) ? (2) when to stop the update process? In order to help the maximum likelihood process to converge faster, we initialize the matrix b  X  (0) to the density matrix corresponding to the classical maximum likelihood language model ~  X  ML of the document or query under con-sideration. This is a diagonal matrix b  X  (0) = diag( ~  X  also tested with the uniform density matrix, as suggested in [15], but we found that the MAP was severely harmed.
In order to address the second question, we analyze the variation of MAP with respect to the maximum number of iterations n it  X  [1 , 50]. The damping factor  X  is optimized over the set of values  X  = { 0 , 0 . 1 , ..., 0 . 9 } . The iterative process stops before n it if the change in the likelihood is below 10  X  4 . In order to check for possible variations due to the collection type, we plot the iteration-MAP curve for two similar collections, i.e. SJMN and TREC7-8, and a web collection, WT10g. We also plot the training objective in Eq. 10 over the set of topics: 1 |R| R is the multiset of retrieved documents. The trend is shown in Figure 5. Generally, at any number of iterations, the MAP stays significantly above the baseline. It seems that there is a good correlation between likelihood maximization and MAP, although one can note some overfitting at high number of iterations. Capping by 10  X  n it  X  20 seems a good trade-off between likelihood maximization and MAP. However, to provide a fair comparison with the baselines, we choose to include n it as a free parameter to train by coordinate ascent.
The results discussed in this section are compactly re-ported in Table 2.
From the comparisons with the LM baseline, one can see that QLM-UNI outperforms LM significantly, with relative im-provements in MAP going up to 12.1% in the case of WT10g collection and 19.2% for the ClueWeb-B collection. This seems to be in line with the hypothesis formulated in [19], for which dependence models may yield larger improvements for large collections.

The weight-normalization problem seems to be addressed automatically: our model does not need for any combina-LM .3064 .1995 .4230 .2120 .1068 .1975 .0718 .1003 MRF-FD-U .3138 .2071 .4350 .2228 .1136 .2097 .0828 .1103 MRF-FD .3074 .2061 .4460 .2243 .1147 .2146 .0881 .1137
QLM-UNI .3181 .2077 .4480 .2240 . 1162 . 2215  X  X  . 1015  X  X 
QLM-IDF .3170 .2093 .4450 .2254 . 1176 . 2264  X  X  . 0997  X  X  significance over MRF-FD-U , MRF-FD respectively. tion weights. Moreover, it is robust across the folds. From an analysis of the optimal values of the parameters obtained across the different folds, we found that optimal window sizes were l  X  { 1 , 2 } . This can be explained by considering that in the current version of QLM, it is possible to decide if the dependency is detected or not, but the model cannot discriminate its  X  X mportance X . If one decides to increase l , more inaccurate dependencies will be detected and the per-formance will be deteriorated. However, even with a larger window size, statistical significance over LM is maintained. From these considerations, we suggest l = 2 as a default setting for our model. Finally, the results endorse that our QLM does not need an engineered estimation of superposi-tion weights to perform well. As a second test, we report the results obtained for the MRF-FD and MRF-FD-U baselines. These have proved to be very robust non bag-of-words baselines [2, 19, 20]. Contrary to our model, MRF does not handle dependency informa-tion in the estimation phase. One has to specify the coeffi-cients (  X  T ,  X  O ,  X  U ) for the combination of dependence and independence scores. To limit per-fold overfitting, for the dependence models, we first train combination parameters (  X  f  X  X  0 , 0 . 01 , ..., 1 } ) then l for each fold. For MRF-FD-U , we set  X  O = 0.
 Results show that for SJMN and TREC7-8, QLM-UNI , MRF-FD and MRF-FD-U are essentially equivalent. However, for the two Web collections, our model significantly outperforms both MRF variants. On ClueWeb-B, statistical significance is attained for the two reported measures. As conjectured in [19], noisy web collections could be a more discriminative testbed for dependence models. Optimal l values for MRF-FD were very small for SJMN ( l  X  X  1 , 2 } ) in contrast to the optimal setting for ClueWeb-B ( l  X  { 16 , 32 } ). In [19], the authors suggest that for homogenous newswire collections a small window is enough to capture useful dependencies, while for large, noisy web collections, a larger span must be set. However, the performances obtained by our model seem to suggest that it can greatly benefit from term dependen-cies, on a variety of collections, even when a small window size is used. This elucidates the fact that even short range information can be extremely useful if integrated in the es-timation phase. In order to get a more comprehensive view on such issues, we trained on the entire set of ClueWeb-B topics three versions of MRF-FD-U , each obtained by clamp-ing a different value of l  X  { 1 , 2 , 4 } . The best performing model obtained a MAP of 10 . 91. It seems that our model can exploit this short range information in a better way than MRF models.
Our last test aimed at verifying if a more reasonable set-ting of the superposition weights could further improve re-trieval performance. For a dependency { w 1 , . . . , w K set  X  i = a larger count to the more  X  X mportant X  term in the depen-dency. QLM-IDF generally increases MAP. However, this is not the case for ClueWeb-B. From a query-by-query analysis, we noticed that QLM-IDF increases the performance for noisy queries by promoting the most  X  X mportant X  terms in unnec-essary subsets. For multiword expressions such as ClueWeb-B topics continental plates and rock art , weighting by idf may be misleading by assigning more weight to one of the terms. In this cases, a uniform parametrization is far more effective. This demonstrates that there is still room for im-provement by a clever tuning of superposition parameters, for example by leveraging feature functions [2, 3].
Complexity issues can be tackled by noting that it is not necessary to manipulate n  X  n matrices. We associate a dimension for each query term and an additional dimen-sion for a  X  X on X  X  care X  term that will store the probability mass for the other terms in the vocabulary. Therefore, a multinomial over n points is reduced to a multinomial over |Q| + 1 points, where |Q| is the number of unique terms in the query and the additional dimension is simply a re-labeling of the other term events. In this way, the QLM to manipulate is k  X  k , where k = |Q| + 1. The eigende-composition generally requires O ( k 3 ). The iterative process requires at most |P ( Q ) | = 2 |Q| matrix multiplications for the expectation step, where 2 |Q| is the maximum number of unique projectors in P d and 2 matrix multiplications for the maximization step. In the case the likelihood is decreased, |  X  | more iterations are done giving a worst-case complexity of O ( n it |  X  | 2 k + k 3 ), i.e. if each iteration needs damping. We showed that 10  X  n it  X  20 is enough; we use |  X  | = 10 and k is very small for title queries, which make the process computationally tractable. In practice, we observed that the damping process is very effective and dramatically improves convergence speed. As an example, the mean number of iter-ations for ClueWeb-B when n it = 15 is 7 . 02 which is orders of magnitude less than n it |  X  | = 150. Finally, we conjecture that such process could be executed at indexing time, thus eliminating any additional on-line costs.
In this paper, we presented a principled application of quantum probability for IR. We showed how the flexibility of vector spaces joined with the powerful tools of probabilis-tic calculus can be mixed together for a flexible, yet prin-cipled account of term dependencies for IR. In our model, dependencies are neither represented as additional dimen-sions, nor stochastically as joint probabilities. They assume a new status as superposition events. The relationship of such an event to the traditional term events are encoded by the off-diagonal values in the corresponding projection ma-trix. Both documents and queries are associated to density matrices estimated through the maximization of a product, which in the classical case reduces to a likelihood. As our model integrates the dependencies in the estimation phase, it has no need for combination parameters. Experiments showed that it performs equivalently to the existing depen-dence models on newswire test collections and outperforms the latter on web data.

To our knowledge, this work provides the first experimen-tal result showing the usefulness of this kind of probabilistic calculus for IR. The marriage between vector spaces and probability can be endlessly improved in the future. One straightforward direction is to relax the assumption that sin-gle terms represent orthogonal projectors. This could lead to a new way of integrating latent directions as estimated by purely geometric methods such as Latent Semantic Indexing (LSI) [7] into a probabilistic model. In this work, we did not exploit the full machinery of complex vector spaces. We do not have a practical justification for the use of the complex field for IR tasks. However, we speculate that this could bring improved representational power and thus remains an interesting direction to explore. At last, we believe that our model could be potentially applied to other fields of natural language processing only by means of a principled Bayesian calculus capable of manipulating density matrices. We hope that this work will foster future research in this direction.
We would like to thank the anonymous reviewers for their valuable comments and suggestions.
