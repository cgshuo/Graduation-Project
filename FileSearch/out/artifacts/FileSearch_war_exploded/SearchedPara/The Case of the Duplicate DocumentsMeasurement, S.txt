 Research in areas such as the web and informatio n retrieval often involves identification of new problems and proposals of novel solutions to these problems. Our investigation of methods for discovery of duplicate documents was a case of this kind of research. We had noticed that sets of answers to queries on text collections developed by TREC often contained duplicates, and thus we investigated the problem of duplicate removal. We developed a new algorithm for combing for duplicates in a document collection such as a web crawl, and found that our method identified many instances of apparent duplication. While none of these documents were bytewise identical, they were often nearly so; in other cases, the differences were greater, but it was clear that the documents were in some sense copies.

However, this research outcome potentially involved circular reasoning. The exis-tence of the problem is demonstrated by the s olution, because, in large collections, manual discovery of duplicates is infeasible ; and the success of the solution is indicated by the extent of the problem. That is, our algorithm succeeded on its own terms, but there was no evidence connecting this su ccess to any external view of what a dupli-cate might be. We are, potentially, being misled by the use of the word  X  X uplicate X , which seems to have a simple natural interpretation. But this belies the complexity of the problem. Duplicates arise in many ways  X  mirroring, revision, plagiarism, and many others  X  and a pair of documents can be duplicates in one context but not in others.
This issue is perhaps more easily understood in an abstract case. Suppose a re-searcher develops an algorithm f or locating documents that are grue (where grue is a new property of documents that the resear cher has decided to investigate) and docu-ments are defined as being grue if they are located by the algorithm. Or suppose the re-searcher develops an algorithm that, on some test data, scores highly for grueness when compared to some alternative algorithms . We can say that these algorithms succeed, but, without an argument connecting grueness to some useful property in the external world, they are of little interest.

Such problems are an instance of a widespr ead issue in computer science research: the paradox of measurement. We measure systems to objectively assess them, but the choice of measure  X  even for simple cases such as evaluating the efficiency of an al-gorithm  X  is a subjective decision. For example, information retrieval systems are clas-sically measured by recall and precision, bu t this choice is purely a custom. In much research there is no explicit consideratio n of choice of measure, and measures are some-times chosen so poorly that a reader cannot determine whether the methods are of value.
Thus appropriate use of measures is an essential element of research. An algorithm that is convincingly demonstrated to be efficient or effective against interesting criteria may well be adopted by other people; an algorithm that is argued for on the basis that it has high grueness will almost certainly be ignored. Problems in measurement are a common reason that research fails to have impact.

Researchers need, therefore, to find a suitable yardstick for measurement of the suc-cess of their solution. Yardsticks rely on assumptions that have no formal justification, so we need to identify criteria by which the value of a yardstick might be weighed. In this paper, we explore these issues in the context of our research into duplicates. We pose criteria for yardsticks and how they might be applied to duplicate detection.
Our investigation illustrates that strong, sound research not only requires new prob-lems and novel solutions, but also requires an appropriate approach to measurement. As we noted elsewhere,  X  X any research papers fail to earn any citations. A key reason, we believe, is that the evidence does not meet ba sic standards of rigor or persuasiveness X  (Moffat and Zobel, 2004). Consideration of these issues  X  which concern the question of what distinguishes applied science fro m activities such as software development  X  can help scientists avoid some of the pitfalls encountered in research and lead to work of greater impact. In 1993, not long after the TREC newswire collections were first distributed, we dis-covered passages of text that were copied between documents. This posed questions such as: how much plagiarism was there in the collections? How could it be found? The cost of searching for copies of a document is O ( n ) , but na  X   X vely the cost of discovery of copies, with no prior knowledge of which documents are copied, is O ( n 2 ) .
We developed a sifting method for discovery of duplicates, based on lossless identi-fication of repeated phrases of length p . In this method, the data is processed p times, with non-duplicated phrases of increasing le ngth progressively eliminated in each pass: a phrase of, say, four words cannot occur twice if one of its component phrases of length three only occurs once. In our recent refineme nt of this method (Bernstein and Zobel, 2004), a hash table of say one b illion 2-bit slots is used to identify phrase frequency, allowing false positives but no false negatives. When all p -word repeating phrases have been identified, these are processed to iden tify pairs of documents that share at least a specified amount of text.

However, in our experiments we observed a vast number of cases of reuse of text, due to factors such as publication of the same article in different regions on different days. Cases of plagiarism  X  if there were any  X  were hidden by the great volume of other material. Moreover, the method did not scale well. In 2003 we returned to work on this problem, inspired by issues in manage ment of large corporate document repos-itories, where it is common for documents such as policies and manuals to be present many times in multiple official versions, and for authors to have their own inconsistent versions. These documents represent corporate memory, yet management of them in practice may be highly chaotic; duplicate detection is a plausible method of helping to bring order to such collections. We refined our original sifting method and proposed metrics for measuring the degree of duplication between two documents.

Using the TREC .gov crawls, we found, disturbingly, that our metric for measur-ing duplication led to a smooth, undifferentiated range of scores: there was no obvious threshold that separated duplicates from non-duplicates. We had na  X   X velyassumedthat pairs would either be largely copied, with say 70% of their material in common, or largely different, with say no more than 20% in common. This assumption was en-tirely wrong. And again, we failed to find the kinds of duplicates we were seeking. Amongst the millions of documents there wer e millions of pairs (a collection of a mil-lion documents contains half a trillion pot ential pairs) with a reasonable amount of text in common. The diversity of kinds of duplication, rather than algorithmic issues, was the main obstacle to success. For web data , potential sources of duplicates include:  X  Mirrors.  X  Crawl artifacts, such as the same text with a different date or a different advertise- X  Versions created for different delivery mechanisms, such as HTML and PDF.  X  Annotated and unannotated copies of the same document.  X  Policies and procedures for the same purpose in different legislatures.  X  Syndicated news articles delivered in different venues.  X   X  X oilerplate X  text such as licen ce agreements or disclaimers.  X  Shared context such as summaries of other material or lists of links.  X  Revisions and versions.  X  Reuse and republication of text (legitimate and otherwise).

At the same time as our original work, fingerprinting methods for duplicate detection were being developed by other groups (Manber, 1994, Brin et al., 1995, Heintze, 1996, Broder, 1997, Chowdhury et al., 2002, Fetterly et al., 2003). Several groups developed methods that broadly have the same behaviour. Some phrases are heuristically selected from each document and are hashed separately o r combined to give representative keys. Two documents that share a key (or a small number of keys) are deemed to be dupli-cates. As most phrases are neglected, the process is lossy, but it is relatively easy to scale and is sufficient to detect pairs of documents that share most of their text.
Our sifting method can be seen as lossless but costly fingerprinting, and it is an easy step to regard the work as comparable. But closer inspection of the past work reveals that the groups were all working on different problems.  X  Manber (1994) used fingerprints to find similar files on a filesystem. Datasets used  X  Brin et al. (1995) investigated fingerprinting in the context of copyright protection  X  Heintze (1996) investigated the characteristics of different fingerprinting schemes.  X  Broder (1997) used fingerprinting to find documents that are  X  X oughly the same X ,  X  Chowdhury et al. (2002) identify documents that are identical after removal of  X  Fetterly et al. (2003) used a variant of fingerprinting known as super-shingling to  X  Our work (Bernstein and Zobel, 2004) concerned detection of co-derivative doc-
There are good reasons to want to identify duplicates. They may represent redundant information; intuitively, there seems no reason to store the same information multiple times, and it is rarely helpful to have multiple copies of a document in an answer list. Elimination of duplicates may have benefits for efficiency at search time. In a web col-lection, the presence of duplicates can indicate a crawler failure. Knowledge of duplica-tion can be used for version management or fil e system management, and can plausibly be used to help identify where an item of information originated (Metzler et al., 2005). And copies of information may be illegitimate.

However, in much of the prior work in the area, the different kinds of duplication, and the different ways in which knowledge of duplication might be used, were jumbled together. There was no consideration of whether the information about duplicates could be used to solve a practical problem and, fundamentally, in none of these papers was there a qualitative definition of what a dupli cate was. Without such a definition, it is not clear how the performance of these systems might be measured, or how we could evaluate whether they were doing useful work. Over the next few sections we explore the difficulties of measurement in the context of research, then return to the question of duplicate detection. Successful research leads to change in the practice or beliefs of others. We persuade people to use a new algorithm, or show that an existing belief is wrong, or show how new results might be achieved, or demonstrate that a particular approach is effective in practice. That is, research is va luable if the results have impact and predictive power. Research is typically pursued for subjective or context-dependent reasons  X  for exam-ple, we find the topic interesting or look in to it because we have funding for investiga-tion of a certain problem.

However, research outcomes are expected to be objective, that is, free from the biases and opinions of the researcher doing the work. If a hypothesis is objectively shown to be false, then it is false, no matter how widely it is believed or how true it had seemed to be; and, if there is solid evidence to support a hypothesis, then probably it should be believed, even if it seems to contradict intuition. That is, we say the hypothesis is confirmed , meaning that the strength of belief in the hypothesis is increased.
For research to be robust and to have high impact, three key elements must be present. First, the hypothesis being investigated must be interesting  X  that is, if it is confirmed, then it will alter the practice and research of others. Second, there must be a convincing way of measuring the outcomes of the research investigation. Third, ac-cording to this measure the hypothesis should be confirmed. In this paper, we call the thing being measured a system and the measure a yardstick . Examples of systems in-clude a search engine, a sorting algorithm, and a web crawler; these are bodies of code that have identifiable inputs and are expected to produce output meeting certain crite-ria. Examples of yardsticks include computation time on some task, number of relevant documents retrieved, and time for a human to complete a task using a system.
Without measurement, there are no research outcomes. Nothing is learnt until a mea-surement is taken. The onus is on the researcher to use solid evidence to persuade a skeptical reader that the results are sound; how convincing the results are will partly depend on how they are measured.  X  X  major difference between a  X  X ell-developed X  sci-ence such as physics and some of the less  X  X ell-developed X  sciences such as psychology or sociology is the degree to which things are measured X  (Roberts, 1979, page 1).
How a system is measured is a choice made by the researcher. It is a subjective choice, dictated by the expected task for w hich the system will be used or the expected context of the system. For example, will the system be run on a supercomputer or a palmtop? Will the user be a child or an expert? Will the data to be searched be web pages or textbooks? There is no authority that determines what the yardstick for any system should be. For measurement of a research outcome such as an interface, this observation is obvious; what may be less obvious is that the observation also applies to algorithmic research.

Consider empirical measurement of the efficiency of some algorithm whose prop-erties are well understood, such as a method for sorting integers. The efficiency of an algorithm is an absolutely fundamental computer science question, but there are many different ways to measure it. We have to choose test data and specify its properties. We then have to make assumptions about the environment, such as the volume of data in relation to cache and memory and the relative costs of disk, network, processor, and memory type. There is no absolute referenc e that determines what is a reasonable  X  X yp-ical X  amount of buffer memory for a disk-based algorithm should be, or whether an algorithm that uses two megabytes of me mory to access a gigabyte of disk is in any meaningful way superior to one that is faster but uses three megabytes of memory.
Complexity, or asymptotic co st, is widely accepted as a m easurement of algorithmic behaviour. Complexity can provide a clear reason to choose one algorithm over another, but it has significant limitations as a yardstick. To begin with,  X  X heoretical results can-not tell the full story about real-world algorithmic performance X  (Johnson, 2002). For example, the notional cost of search of a B-tree of n items is O (log n ) , but in practice the cost is dominated by the effort of retrieval of a single leaf node from disk. A partic-ular concern from the perspective of measurement is that complexity analysis is based on subjective decisions, because it relies on a ssumptions about machine behaviour and data. Worst cases may be absurd in practice; there may be assumptions such as that all memory accesses are of equal cost; and average cases are often based on simplis-tic models of data distributions. Such issues arise in even elementary algorithms. In an in-memory chained hash table, for example, implemented on a 2005 desktop computer, increasing the number of slots decreases th e per-slot load factor  X  but can increase the per-key access time for practical data volumes (Askitis and Zobel, 2005).

While a complexity analysis can provide insight into behaviour, such as in compar-ison of radixsort to primitive methods such as bubblesort, it does not follow that such analysis is always sufficient. First,  X  X nly experiments test theories X  (Tichy, 1998). Sec-ond, analysis is based on assumptions as subjective as those of an experiment; it may provide no useful estimate of cost in practice; and it is not the answer to the problem of the subjectivity of measurement.

Philosophical issues such as paradoxes of measurement are not merely academic concerns, but are significant practical pr oblems in design of research projects. We need to find a basis for justification of our claims about research outcomes, to guide our work and to yield results that are supported by plausible, robust evidence. Identification of what to measure is a key step in development of an idea into a concrete research project. In applied science, the u ltimate aim is to demonstrate that a proposal has utility . The two key questions are, thus, what aspect of utility to measure and how to measure it. We propose that principles for choice of a process of measurement  X  that is, choice of yardstick  X  be based on the concept of a warrant . Booth et al. (1995) define a warrant as an assumption that allows a particular kind of evidence to be used to support a particular class of hypothesis. An example from Booth et al. is: Hypothesis. It rained last night.
 Evidence. The streets are wet this morning.
 This argument may seem self-supporting and self-evident. However, the argument re-lies on an implied warrant: that the most likely cause of wet streets is rain. Without the warrant, there is nothing to link the evidence to the hypothesis. Crucially, there is nothing within either the hypothesis or the evidence that is able to justify the choice of warrant; the warrant is an assertion that i s external to the system under examination.
The fact that the warrants under which an experiment are conducted are axiomatic can lead to a kind of scientific pessimism, i n which results have no authority because they are built on arbitrary foundations. With no criteria for choosing amongst warrants, we are in the position of the philosopher who concludes that all truths are equally likely, and thus that nothing can be learnt. However, clearly this is unhelpful: some warrants do have more merit than others. The issue then becomes identification of the properties a good set of warrants should have.

The answer to the question  X  X hat should we measure? X  we refer to as the qualitative warrant, and the answer to the question  X  X ow should we measure it? X  we refer to as the quantitative warrant, that is, the yardstick. These assertions are what links the measure-ment to the goal of demonstrating utility. We p ropose a set (not necessarily exhaustive) of four properties that are satisfied by a good qualitative warrant, and of three properties that are satisfied by a good yardstick:  X  Applicability. A qualitative warrant should reflect the task or problem the system  X  X ower. The power of a qualitative warrant is the degree to which it makes a mean- X  Specificity. Evaluation of a system cannot be meaningful if we are not specific  X  Richness. The utility of many systems depends on more than just one dimension The quantitative warrant is effectively dictated by the choice of yardstick used to mea-sure the system. A good yardstick should have the following properties:  X  Independence. A yardstick needs to be independent of the solution; it should not  X  Fidelity. Because the yardstick is used to qua ntify the utility of the system under  X  Repeatability. We expect research results to be predictive, and in particular that Using these criteria, it can be argued that some qualitative warrants are indeed superior to others, and that, given a particular qualitative warrant, some yardsticks are superior to others. Note that measures often conflict, and that this is to be expected  X  consider yardsticks such as speed versus space, or speed versus complexity of implementation, or speed in practice versus expected asymptotic cost. We should not expect yardsticks to be consistent, and indeed this is why choice of yardstick can be far from straightforward.
For algorithmic work, we may choose a qualitative warrant such as  X  X n algorithm is useful if it is computationally efficient X . This satisfies the criteria: it is applicable, pow-erful, reasonably specific, and rich. Given this warrant, we can consider the yardstick  X  X educed elapsed computation time X . It is independent (we don X  X  even need to know what the algorithm is), repeatable, and in general is a faithful measure of utility as de-fined by the qualitative warrant. The yards tick  X  X educed instruction count X  is indepen-dent and repeatable, but in some cases lacks fidelity: for many algorithms, other costs, such as memory or disk accesses, are much mor e significant. The yardstick  X  X akes use of a wider range of instructions X  is independent and repeatable, but entirely lacks fi-delity: measures by this yardstick will b ear little correspondence to utility as defined by our qualitative warrant.
Some potential criteria that could be used to justify a yardstick are fallacies or ir-relevancies that do not stand scrutiny. For example, the fact that a property is easy to measure does not make the measure a good choice. A yardstick that has been used for another task may well be applicable, but the fact that it has been used for another task carries little weight by itself; the rationale that led to it being used for that task may be relevant, however. Even the fact that a yardstick has previously been used for the same task may carry little weight  X  we need to be persuaded that the yardstick was well chosen in the first place.

An underlying issue is that typically yardsticks are abstractions of semantic prop-erties that are inherently not available by symbolic reasoning. When a survey is used to measure human behaviour, for example, a complex range of real-world properties is reduced to numerical scores. Confusion over w hether processes are  X  X emantic X  is a fail-ing of a range of research activities. Symbolic reasoning processes cannot be semantic; only abstract representations of real-world properties  X  not the properties themselves, in which the meaning resides  X  are available to computers.

Note too that, as computer scientists, we do not write code merely to produce soft-ware, but to create a system that can be measured, and that can be shown to possess a level of utility according to some criterion. If the principal concern is efficiency, then the code needs to be written with great care, in an appropriate language; if the concern is whether the task is feasible, a rapid prototype may be a better choice; if only one component of a system is to be measured, the others may not need to be implemented at all. Choice of a yardstick determines whi ch aspects of the system are of interest and thus need to be implemented. In algorithmic research, the qualitative warrants are fairly straightforward, typically concerning concrete properties such as speed, throughput, and correctness. Such war-rants can be justified  X  although usually the justification is implicit  X  by reference to goals such as reducing costs. Yardsticks for such criteria are usually straightforward, as the qualitative warrants are inherently quantifiable properties.

In IR, the qualitative warrant concerns the quality of the user search experience, of-ten in terms of the cost to the user of resolving an information need. Yardsticks are typically based on the abstractions precision and recall . The qualitative warrant sat-isfies the criteria of applicability, power, and richness. Furthermore, the IR yardsticks typically demonstrate independence and repeatability.

However, the qualitative warrant is not sufficiently specific. It is difficult to model user behaviour when it has not been specified what sort of user is being modelled, and what sort of task they are supposed to be performing. For example, a casual web searcher does not search in the same way as a legal researcher hoping to find relevant precedents for an important case. Even if the qualitative warrant were made more spe-cific, for example by restricting the domain to ad-hoc web search, the fidelity of many of the current yardsticks can be brought into question. Search is a complex cognitive pro-cess, and many factors influence the degree of satisfaction a user has with their search experience; many of these factors are simplified or ignored in order to yield a yard-stick that can be tractably evaluated. It is not necessarily the case that the user will be most satisfied with a search that simply presents them with the greatest concentration of relevant documents.

To the credit of the IR research community , measurement of effectiveness has been the subject of ongoing debate; in some other research areas, the issue of measurement is never considered. In particular, user studies have found some degree of correla-tion between these measures and the ease w ith which users can complete an IR task (Allan et al., 2005), thus demonstrating that  X  despite the concerns raised above  X  the yardsticks have at least limited fidelity and research outcomes are not entirely irrelevant. Yardsticks drive the direction of resear ch; for example, the aim of a great deal of IR research is to improve recall and precision. To the extent that a yardstick represents community agreement on what outcome is desirable, letting research follow a yardstick is not necessarily a bad thing. However, if the divergence between yardsticks and the fundamental aim of the research is too great, then research can be driven in a direction that is not sensible. We need to be confident that our yardsticks are meaningful in the world external to the research. In some of the work on duplicate discovery discussed earlier, the qualitative warrant is defined as (we paraphrase)  X  X ystem A is useful if it is able to efficiently identify duplicates or near-duplicates X . However, anything that is found by the algorithms is deemed to be a duplicate. Such a yardstick c learly fails the criteria set out earlier. It is not independent, powerful, or rich. It pro vides no guidance for future work, or any information as to whether the methods are valuable in practice.

The question of whether these methods ar e successful depends on the definition of  X  X uplicate X . When the same page is crawled twice, identical but for a date, there are still contexts in which the two versions are not duplicates  X  sometimes, for example, the dates over which a document existed are o f interest. Indeed, almost any aspect of a document is a reasonable target of a user X  X  interest. It is arguable whether two docu-ments are duplicates if the name of the author has changed, or if the URL is different. Are a pair  X  X he same X  if one byte is changed? Two bytes? That is, there is no one obvious criterion for determining duplication. Again, we argue that the warrant is not specific enough. A pair of documents that are duplicates in the context of, say, topic search may not be duplicates in the context of, say, establishing which version is most up-to-date.
As in IR, there need to be clear criteria a gainst which the assessment is made, in which the concept of task or u tility is implicitly or explicitly present. For duplication, one context in which task can be defined is that of search. Consider some of the ways in which a document might address an information need:  X  As a source of new information.  X  As confirmation of existing knowledge.  X  As a means of identifying the author, date, or context.  X  As a demonstration of whether the information is from a reputable provider. That is, a pair of documents can only be judged as duplicates in the context of the use that is being made of them.
To establish whether our SPEX method for duplicate discovery was effective, we explored several search-oriented varieties of duplication, using human experiments to calibrate SPEX scores against human judgements (Bernstein and Zobel, 2005).
The first kind of duplication was retrieval equivalence : a pair of documents is re-trieval equivalent if they appear identical to a typical search engine. This definition can be validated mechanically, by parsing the documents according to typical search engine rules to yield a canonical form. A pair is deemed to be retrieval equivalent if their canonical forms are bytewise identical. However, even retrieval equivalent docu-ments may not be duplicates in the context of some search tasks. Two mirrors may hold identical documents, but we may trust one mirror more than another; removal of either document from an index would be a mistake. Knowledge of duplication can affect how such answers are presented, but does not mean that they can be eliminated.

The second kind of duplication we considered was content equivalence .Inanini-tial experiment, we identified document pairs where SPEX had returned a high match score, and asked test subjects to assess the pai rs against criteria such as  X  X ffectively du-plicated X . However, our subjects differed widely in their interpretation of this criterion. For some, a minor element such as date was held to indicate a substantive difference; for others it was irrelevant. We therefore refined these criteria, to stat ements such as  X  X if-ferences between the documents are trivial and do not differentiate them with respect to any reasonable query X  and  X  X ith respect to any query for which both documents may be returned by a plausible search, the documents are equivalent; any query for which the documents are not equivalent would only return one or the other X . We called this new criterion conditional equivalence .
 We could define our warrants for this task as follows: Qualitative warrant. The SPEX system is useful if it accurately identifies pairs of doc-Quantitative warrant. The extent to which pairs of documents identified by a system Superficially, retrieval and c ontent equivalence, and the sub-classes of content equiva-lence, may seem similar to each other, but in a good fraction of cases documents that were duplicates under one criterion were not duplicates under another. An immediate lesson is that investigation of duplicate discovery that is not based on a clear definition of task is meaningless. A more positive lesson is that these definitions provide a good yardstick; they meet all of the criteria listed earlier.

Using these yardsticks, we observed that there was a clear correlation between SPEX scores and whether a user would judge the documents to be duplicated. This meant that we could use SPEX to measure the level of duplication  X  from the perspective of search!  X  in test collections. Our experiments used the GOV1 and GOV2 collections, two crawls of the .gov domain created for TREC. GOV1 is a partial crawl of .gov from 2002, with 1,247,753 documents occupying 18 gigabytes. GOV2 is a much more complete crawl of .gov from 2004, with 25,205,179 documents occupying 426 gigabytes.
On the GOV1 collection, we found that 99,227 documents were in 22,870 retrieval-equivalent clusters. We found a further 116,087 documents that participated in content-equivalence relationships, and that the change in definition from content-equivalence to conditional equivalence led to large variations in the numbers of detected dupli-cates. On the GOV2 collection, we found a total of 6,943,000 documents in 865,362 retrieval-equivalent clusters  X  more than 25% of the entire collection. (Note that, prior to distribution of the data, 2,950,950 documents were removed after being identified as duplicates by MD5.) Though we were unable to scan the entire GOV2 collection for content-equivalence, we believe that a similar proportion again is content-equivalent, as was the case for the GOV1 collection.

These results indicate that there are man y pairs of documents within these collec-tions that are mutually redundant from a user perspective: if a user were to see one document in relation to a particular query, there may be several other documents that would no longer be of interest to them. This observation provides empirical support to the questioning of the notion of independent relevance. The results suggest that the volume of retrieval-and content-equivalent documents in the collection may be so great that the assumption of independent relevance is significantly affecting the fidelity of the IR yardsticks.

To investigate this further, we experimented with the runs submitted for the TREC 2004 terabyte track, consisting of result sets for 50 queries on the GOV2 collection. In our first experiment, we modified the query relevance assessments so that a document returned for a particular query on a particular system would be marked as not relevant if a document with which it was content-equivalent appeared earlier in the result list. This partially models the notion of relevance as dependent on previously seen documents. The result was significant: under this assumption, the MAP of the runs in the top three quartiles of submission dropped by a relative 20.2% from 0.201 to 0.161. Interestingly, the drop in MAP was greater for the more successful runs than for the less successful runs. While ordering between runs was generally preserved, it seems that the highest-scoring runs were magnifying their success by retrieving multiple copies of the same relevant document, an operation that we argue does nothing to improve the user search experience in most cases.

These experiments allowed us to observe the power that measurement and yardsticks have in influencing the direction of research. Consider two examples.

The first example is that, in defining an appropriate measure of the success of our system, we were forced to re-evaluate and ultimately redefine our task. We had origi-nally intended to simply measure the occurrence in collections of documents that were content-equivalent with a view to removing them from the collection. Our user exper-iments quickly showed us that this appro ach was unrealistic: even minor differences between documents had the potential to be significant in certain circumstances. The concept of conditional equivalence, in which documents were equivalent with respect to a query, proved to be far more successful. This meant that it was unsuitable to sim-ply remove documents from the collection; ra ther, duplicate removal was much better performed as a postprocessing step on result lists. This lesson, learnt in the process of defining a yardstick, has practical effects on the way in which duplication should be managed in search engines.

The second example concerns the fidelity of measures based on the assumption of independence of relevance. We have shown that, based on user experiments, our soft-ware can reliably identify pairs of documents that are conditionally equivalent, and that lifting the general assumption of independent relevance can have a significant impact on the reported effectiveness of real search sy stems. Furthermore, postprocessing result lists in order to remove such equivalent documents, while significantly increasing MAP from the lower figure, failed to restore the MAP of runs to its original level. The conse-quence of this is that the current TREC assessment regime discourages the removal of duplicate documents from result lists. This demonstrates the power of yardsticks, and the dangers if they are poorly chosen. Becau se yardsticks are the measured outcomes of research, it is natural for research communities to have as their goal improvement in performance according to comm only accepted yardsticks. Given an insufficiently faith-ful yardstick it is likely, or perhaps inev itable, that the research activity may diverge from the practical goals that the research co mmunity had originally intended to service. Careful consideration of how outcomes are to be measured is a critical component of high-quality research. No researcher, one presumes, would pursue a project with the expectation that it will have little impact, y et much research is unpersuasive and for that reason is likely to be ignored. Each paper needs a robust argument to demonstrate that the claims are confirmed. Such argum ent rests on evidence, and, in the case of experimental research, the evidence depends on a system of measurement.

We have proposed seven criteria that should be considered when deciding how re-search outcomes should be measured. These cr iteria  X  applicability, power, specificity, richness, independence, fidelity, and repeatability  X  can be used to examine yardsticks used for measurement. As we have argued in the case of IR research, even widely ac-cepted yardsticks can be unsatisfactory. In the case of the duplicate documents, our examination of the problems of measuremen t reveals one plausible reason why some prior work has had little impact: the yardstic ks are poor or absent, and consequently the work is not well founded.

We applied the criteria to evaluation of our new yardsticks for duplicate detection, and found that the concept of  X  X uplicate X  is surprisingly hard to define, and in the absence of a task is not meaningful. Almost every paper on duplication concerns a dif-ferent variant and our user studies show that slightly different definitions of  X  X uplicate X  lead to very different results. Duplicates can be found, but there is no obvious way to find specific kinds of duplicates  X  previous work was typically motivated by one kind of duplication but measured on all kinds of duplication. Our examination of yardsticks not only suggests future directions for research on duplicate detection, but more broadly suggests processes that researchers should follow in design of research projects. Acknowledgements. This work was supported by the Australian Research Council.
