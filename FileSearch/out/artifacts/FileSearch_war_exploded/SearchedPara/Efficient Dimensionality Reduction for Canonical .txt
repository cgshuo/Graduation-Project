 Haim Avron haimav@us.ibm.com Christos Boutsidis cboutsi@us.ibm.com IBM T.J. Watson Research Center Sivan Toledo stoledo@tau.ac.il Tel-Aviv University Anastasios Zouzias zouzias@cs.toronto.edu University of Toronto Canonical Correlation Analysis (CCA), originally due to Hotelling (1936), is an important technique in statistics, data analysis, and data mining. CCA has been successfully applied in many machine learning ap-plications, e.g. dimensionality reduction (Sun et al., 2010), clustering (Chaudhuri et al., 2009), learning of word embeddings (Dhillon et al., 2011), sentiment classification (Dhillon et al., 2012), discriminant learn-ing (Su et al., 2012), and object recognition (Kim et al., 2007). In many ways CCA is analogous to Principal Component Analysis (PCA), but instead of analyzing a single dataset (in matrix form), the goal of CCA is to analyze the relation between a pair of datasets (each in matrix form). From a statistical point of view, PCA extracts the maximum covari-ance directions between elements in a single matrix, whereas CCA finds the direction of maximal correla-tion between a pair of matrices. From a linear alge-braic point of view, CCA measures the similarities be-tween two subspaces (those spanned by the columns of each of the two matrices analyzed). From a geo-metric point of view, CCA computes the cosine of the principle angles between the two subspaces.
 There are different ways to define the canonical cor-relations of a pair of matrices, and all these ways are equivalent (Golub &amp; Zha, 1995). The linear algebraic formulation of Golub &amp; Zha (1995), which we present shortly, serves our algorithmic point of view best. Definition 1. Let A  X  R m  X  n and B  X  R m  X  ` , and as-sume that p = rank( A )  X  rank( B ) = q . The canonical correlations  X  1 ( A , B )  X   X  2 ( A , B )  X   X  X  X   X   X  q ( A , B ) of the matrix pair ( A , B ) are defined recursively by the following formula ( i = 1 ,...,q ):  X  ( A , B ) = max where  X   X  ( u , v ) = | u T v | / ( k u k 2 k v k 2 ) ,  X  A i = { x : Ax 6 = 0 , Ax  X { Ax 1 ,..., Ax i  X  1 }} ,  X  B i = { y : By 6 = 0 , By  X { By 1 ,..., By i  X  1 }} . The unit vectors Ax 1 / k Ax 1 k 2 ,..., Ax q / k Ax By 1 / k By 1 k 2 ,..., By q / k By q k 2 are called the canonical or principal vectors .
 The vectors x 1 / k Ax 1 k 2 ,..., x q / k Ax q k 2 , y / k By 1 k 2 ,..., y q / k By q k 2 are called canonical weights (or projection vectors ). Note that the canoni-cal weights and the canonical vectors are not uniquely defined.
 1.1. Main Result The main contribution of this paper (see Theorem 12) is a fast algorithm to compute an approximate CCA. The algorithm computes an additive-error approxima-tion to all the canonical correlations. It also com-putes a set of approximate canonical weights with provable guarantees. We show that the proposed algo-rithm is asymptotically faster compared to the stan-dard method of Bj  X orck &amp; Golub (1973). To the best of our knowledge, this is the first sub-cubic time algo-rithm for approximate CCA that has provable guaran-tees.
 The proposed algorithm is based on dimensionality re-duction : given a pair of matrices ( A , B ), we transform the pair to a new pair (  X  A ,  X  B ) that has much fewer rows, and then compute the canonical correlations of the new pair exactly, alongside a set of canonical weights, e.g. using the Bj  X orck and Golub algorithm. We prove that with high probability the canonical correlations of (  X 
A ,  X  B ) are close to the canonical correlations of ( A , B ), and that any set of canonical weights of (  X  A ,  X  be used to construct a set of approximately orthogo-nal canonical vectors of (  X  A ,  X  B ). The transformation of the Randomized Walsh-Hadamard Transform (RHT) to both A and B . This is a unitary transformation, so the canonical correlations are preserved exactly. On the other hand, we show that with high probability, the transformed matrices have their  X  X nformation X  equally spread among all the input rows, so now the trans-formed matrices are amenable to uniform sampling. In the second step, we uniformly sample (without re-placement) a sufficiently large set of rows and rescale them to form (  X  A ,  X  B ). The combination of RHT and uniform sampling is often called Subsampled Random-ized Walsh-Hadamard Transform (SRHT) in the lit-erature (Tropp, 2011). Note that other variants of dimensionality reduction (Sarl  X os, 2006) might be ap-propriate as well, but for concreteness we focus on the SRHT.
 Our dimensionality reduction scheme is particularly effective when the matrices are tall-and-thin, that is they have much more rows than columns. Targeting such matrices is natural: in typical CCA applications, columns typically correspond to features or labels and rows correspond to samples or training data. By com-puting the CCA on as many instances as possible (as much training data as possible), we get the most reli-able estimates of application-relevant quantities. How-ever in current algorithms adding instances (rows) is expensive, e.g. in Bj  X orck and Golub algorithm we pay O ( n 2 + ` 2 ) for each row. Our algorithm allows prac-titioners to run CCA on huge data sets because we reduce the cost of an extra row, making it not much more expensive than O ( n + ` ). 1.2. Related Work Dimensionality reduction has been the driving force behind many recent algorithms for accelerating key machine learning and linear algebraic tasks. A rep-resentative example is linear regression, i.e., solve the least squares problem min x k Ax  X  b k 2 , where A  X  R m  X  n and b  X  R m . If m n , then one can use the SRHT to reduce the dimensions of A and b , to form  X  A and  X  b , and then solve the small problem min x k  X  Ax  X   X  b k 2 . This process will return an approx-imate solution to the original problem (Sarl  X os, 2006; Boutsidis &amp; Drineas, 2009; Drineas et al., 2011). Al-ternatively, one can observe that A T A and  X  A T  X  A are spectrally close, so  X  A is an effective preconditioner for A (Rokhlin &amp; Tygert, 2008; Avron et al., 2010). Other problems that can be accelerated using dimen-sionality reduction include: (i) approximate PCA (via low-rank matrix approximation) (Halko et al., 2011); (ii) matrix multiplication (Sarl  X os, 2006); (iii) K-means clustering (Boutsidis et al., 2010); (iv) approximation of matrix coherence and statistical leverage (Drineas et al., 2012); to name only a few.
 Our approach uses similar techniques as the algorithms mentioned above. For example, Lemma 4 in our article plays a central role in these algorithms as well. How-ever, our analysis requires the use of advanced ideas from matrix perturbation theory and it leads to two new technical lemmas that might be of independent interest: Lemmas 7 and 8 provide bounds for the sin-gular values of the product of two different sampled orthonormal matrices. Previous work only provides bounds for products of the same matrix (Lemma 4; see also Sarl  X os (2006, Corollary 11)).
 Dimensionality reduction techniques for accelerating CCA have been suggested or used in the past. One common technique is to simply use less samples by uni-formly sampling the rows. While this technique might work reasonably well in many instances, it may fail for others unless all rows are sampled. In fact, Theorem 10 analyzes uniform sampling, and establishes bounds on the required sample size.
 Sun et al. (2010) suggest a two-stage approach which involves first solving a least-squares problem, and then using the solution to reduce the problem size. How-ever, their technique involves explicitly factoring one of the two matrices, which takes cubic time. Therefore, their method is especially effective when one of the two matrices has significantly less columns than the other. When the two matrices have about the same number of columns, there is no asymptotic performance gain. In contrast, our method is sub-cubic in any case. Finally, it is worth noting that CCA itself has been used for dimensionality reduction (Sun et al., 2008; Chaudhuri et al., 2009; Sun et al., 2010). This is not the focus of this paper; we suggest a dimensionality reduction technique to accelerate the computation of CCA. We use i : j to denote the set { i,...,j } , and [ n ] = 1 : n . We use A , B ,... to denote matrices and a , b ,... to denote column vectors. I n is the n  X  n identity matrix; 0 m  X  n is the m  X  n matrix of zeros. We denote by R (  X  ) the column space of its argument matrix. We denote by [ A ; B ] the matrix obtained by concatenating the columns of B next to the columns of A . Given a subset of indices T  X  [ m ], the corresponding sampling matrix S is the | T | X  m matrix obtained by discarding from I m the rows whose index is not in T . Note that SA is the matrix obtained by keeping only the rows in A whose index appears in T . A symmetric matrix A is positive semi-definite (PSD), denoted by 0 A , if x T Ax  X  0 for every vector x . For any two symmetric matrices X and Y of the same size, X Y denotes that Y  X  X is a PSD matrix.
 We denote the compact (or thin ) SVD of a matrix A  X  R m  X  n of rank p by A = U A  X  A V T  X  pseudo-inverse of A is A + = V A  X   X  1 A U T A  X  R n  X  m . We denote the singular values of A by  X  1 ( A )  X   X  2 ( A )  X   X  X  X  X   X  p ( A ). 2.1. The Bj  X orck and Golub Algorithm There are quite a few algorithms to compute the canonical correlations (Golub &amp; Zha, 1995). One of the most popular methods is due to Bj  X orck &amp; Golub (1973). It is based on the following observation. Theorem 2 (Bj  X orck &amp; Golub (1973)) . Assume that the columns of Q  X  R m  X  p ( m  X  p ) and W  X  R m  X  q ( m  X  q ) form an orthonormal basis for the range of A and B (respectively). Let Q T W = U X V T be its com-pact SVD. The diagonal elements of  X  are the canon-ical correlations of ( A , B ) . The canonical vectors are given by the first q columns of QU (for A ) and WV (for B ).
 Theorem 2 implies that once we have a pair of matri-ces Q and W with orthonormal columns whose column space spans the same column space of A and B , respec-tively, then all we need is to compute the singular value decomposition of Q T W . Bj  X orck and Golub suggest the use of QR decompositions, but U A and U B will serve as well. Both options require O m n 2 + ` 2 time. Corollary 3. Frame Definition 1. Let U T A U B = U X V T be its compact SVD. Then, for i  X  [ q ] :  X  ( A , B ) =  X  ii . The canonical weights are given by the columns of V A  X   X  1 A U (for A ) and V B  X   X  1 B V (for B ). 2.2. Matrix Coherence and Sampling from an Matrix coherence is a fundamental concept in the anal-ysis of matrix sampling algorithms (e.g. Talwalkar &amp; Rostamizadeh (2010)). There a quite a few similar but different ways to define the coherence, however in this paper we use the following definition. Given a matrix A with m rows, the coherence of A is defined standard basis (column) vector of R m . Note that the coherence of A is a property of the column space of A , and does not depend on the actual choice of A . Therefore, if R ( A ) = R ( B ) then  X  ( A ) =  X  ( B ). Fur-thermore, it is easy to verify that if R ( A )  X  R ( B ) then  X  ( A )  X   X  ( B ). Finally, we mention that for every matrix A with m rows: We focus on tall-and-thin matrices, i.e. matrices with (much) more rows than columns. We are interested in dimensionality reduction techniques that (approx-imately) preserve the singular values of the original matrix. The simplest idea to do dimensionality re-duction in tall-and-thin matrices is uniform sampling of the rows of the matrix. Coherence measures how susceptible the matrix is to uniform sampling; the fol-lowing lemma shows that not too many samples are required when the coherence is small. The bound is almost tight (Tropp, 2011, Section 3.3).
 Lemma 4 (Sampling from Orthonormal Matrix, Tropp (2011) Corollary to Lemma 3.4) . Let Q  X  R m  X  d have orthonormal columns. Let 0 &lt; &lt; 1 and 0 &lt;  X  &lt; 1 . Let r be an integer such that Let T be a random subset of [ m ] of cardinality r , drawn from a uniform distribution over such subsets, and let S be the | T | X  m sampling matrix corresponding to T rescaled by p m/r . Then, with probability of at least 1  X   X  , for i  X  [ d ] : Proof. Apply Lemma 3.4 from Tropp (2011) with the following choice of parameters: ` =  X M log( k/ X  ) ,  X  = 6 / 2 , and  X  tropp =  X  = . Here, ` ,  X  , M , k ,  X  are the parameters of Lemma 3.4 from Tropp (2011); also  X  tropp plays the role of  X  , an error parameter, of Lemma 3.4 from Tropp (2011). and  X  are from our Lemma. In the above lemma, T is obtained by sampling coor-dinates from [ m ] without replacement. Similar results can be shown for sampling with replacement, or using Bernoulli variables (Ipsen &amp; Wentworth, 2012). 2.3. Randomized Walsh-Hadamard Transform Matrices with high coherence pose a problem for al-gorithms based on uniform row sampling. One way to circumvent this problem is to use a coherence-reducing transformation. One popular coherence-reducing transformation is the Randomized Walsh-Hadamard Transform (RHT) matrix. We start with the definition of the deterministic Walsh-Hadamard Transform matrix.
 Fix an integer m = 2 h , for h = 1 , 2 , 3 ,... . The (non-normalized) m  X  m matrix of the Walsh-Hadamard Transform (WHT) is defined recursively as, H The m  X  m normalized matrix of the Walsh-Hadamard transform is H = m  X  1 2 H m .
 The recursive nature of the WHT allows us to compute HX for an m  X  n matrix X in time O ( mn log( m )). However, in our case we are interested in SHX where S is a r -row sampling matrix. To compute SHX only O ( mn log( r )) operations suffice (Ailon &amp; Liberty, 2008, Theorem 2.1).
 Definition 5 (Randomized Walsh-Hadamard Trans-form (RHT)) . Let m = 2 h for some positive integer h . A Randomized Walsh-Hadamard Transform (RHT) is an m  X  m matrix of the form where D is a random diagonal matrix of size m whose entries are independent random signs, and H is a nor-malized Walsh-Hadamard matrix of size m .
 Lemma 6 (RHT bounds Coherence, Tropp (2011) Lemma 3.3) . Let A be an m  X  n ( m  X  n , m = 2 h for some positive integer h ) matrix, and let  X  be an RHT. Then, with probability of at least 1  X   X  , This section states three new technical lemmas which analyze the perturbation of the singular values of the product of a pair of matrices after dimensionality re-duction. The proofs appear in the full version of the present article (Avron et al., 2012).
 Lemma 7. Let A  X  R m  X  n ( m  X  n ) and B  X  R m  X  ` ( m  X  ` ). Define C := [ A ; B ]  X  R m  X  ( n + ` ) , and suppose C has rank  X  , so U C  X  R m  X   X  . Let S  X  R r  X  m be any matrix such that  X  ( SU C )  X  i = 1 ,..., min( n,` ) , |  X  i A T B  X   X  i A T S T SB | X   X k A k 2  X k B k 2 . Lemma 8. Let A  X  R m  X  n ( m  X  n ) and B  X  R m  X  ` ( m  X  ` ). Let S  X  R r  X  m be any matrix such that rank( SA ) = rank( A ) and rank ( SB ) = rank( B ) , and all singular values of SU A and SU B are inside [  X  1  X  , i = 1 ,..., min( n,` ) , Lemma 9. Repeat the conditions of Lemma 7. Then, for all w  X  R n and y  X  R ` , we have w T A T By  X  w T A T S T SBy  X   X k Aw k 2  X k By k 2 . Given A and B , one straightforward way to accelerate CCA is to sample rows uniformly from both matri-ces, and to compute the CCA of the smaller matri-ces. In this section we show that if we sample enough rows, then the canonical correlations of the sampled pair are close to the canonical correlations of the orig-inal pair. Furthermore, the canonical weights of the sampled pair can be used to find approximate canoni-cal vectors. Not surprisingly, the sample size depends on the coherence. More specifically, it depends on the coherence of [ A ; B ].
 Theorem 10. Suppose A  X  R m  X  n ( m  X  n ) has rank p and B  X  R m  X  ` ( m  X  ` ) has rank q  X  p . Let 0 &lt; &lt; 1 / 2 be an accuracy parameter and 0 &lt;  X  &lt; 1 be a failure probability parameter. Let  X  = rank([ A ; B ])  X  p + q . Let r be an integer such that Let T be a random subset of [ m ] of cardinality r , drawn from a uniform distribution over such subsets, and let S  X  R r  X  m be the sampling matrix corresponding to T rescaled by p m/r . Denote  X  A = SA and  X  B = SB . Let  X   X  1 ,...,  X   X  q be the exact canonical correlations of (  X 
A ,  X  B ) , and let and be the exact canonical weights of (  X  A ,  X  B ) . With proba-bility of at least 1  X   X  all the following hold simultane-ously: (a) (Approximation of Canonical Correlations) For (b) (Approximate Orthonormal Bases) The vectors (c) (Approximate Correlation) For every i = Proof. Let C := [ U A ; U B ]. Lemma 4 implies that each of the following three assertions hold with prob-ability of at least 1  X   X / 3, hence all three hold simul-taneously with probability of at least 1  X   X  :  X  For every r  X  [ p ]:  X  For every k  X  [ q ]:  X  For every h  X  [  X  ]: We now show that if indeed all three hold, then (a)-(c) hold as well.
 Proof of (a). Corollary 3 implies that  X  i ( A , B ) =  X  ( U T A U B ) and  X  i ( SA , SB ) =  X  i ( U T SA U SB ). We now use the triangle inequality to get, |  X  i ( A , B )  X   X  i ( SA , SB ) | To conclude the proof, use Lemma 7 and Lemma 8 to bound these two terms, respectively.
 Proof of (b). For any c  X  [ q ], since k  X  Aw c k 2 = 1. Now Lemma 9 implies the first inequality.
 For any i 6 = j In the above, we used the triangle inequality, the fact that the w i  X  X  are the canonical weights of  X  A , and Lemma 9.
 Proof of (c). We only prove the upper bound. The lower bound is similar, and we omit it. In the above, the first equality follows by the definition of  X  (  X  ,  X  ), the first inequality by using 1 = k  X  Aw i (1 + ) k Aw i k 2 2 (same holds for Bp i ), the second in-equality from Lemma 9, the third inequality by using (1  X  ) k Aw i k 2 2  X  k  X  Aw i k 2 2 = 1 (same holds for Bp and the last inequality by (a).
 First, we define what we mean by approximate CCA. Definition 11 (Approximate CCA) . For 0  X   X   X  1 , an  X  -approximate CCA of ( A , B ) , is a set of posi-w 1 ,..., w q for A and a set of vectors p 1 ,..., p q for B , such that (a) For every i  X  [ q ] , (b) For every i  X  [ q ] , (c) For every i  X  [ q ] , We are now ready to present our fast algorithm for approximate CCA of a pair of tall-and-thin matrices. Algorithm 1 gives the pseudo-code description of our algorithm.
 The analysis in the previous section (Theorem 10) shows that if we sample enough rows, the canonical correlations and weights of the sampled matrices are an O ( )-approximate CCA of ( A , B ). However, to turn this observation into a concrete algorithm we need an upper bound on the coherence of [ A ; B ]. It is con-ceivable that in certain scenarios such an upper bound might be known in advance, or that it can be com-puted quickly (Drineas et al., 2012). However, even if we know the coherence, it might be as large as one, which will imply that sampling the entire matrix is needed.
 To circumvent this problem, our algorithm uses the RHT to reduce the coherence of the matrix pair before sampling rows from it. That is, instead of sampling rows from ( A , B ) we sample rows from (  X A ,  X B ), where  X  is a RHT matrix (Definition 5). This unitary transformation bounds the coherence with high proba-bility, so we can use Theorem 10 to compute the num-ber of rows required for an O ( )-approximate CCA. We now sample the transformed pair (  X A ,  X B ) to obtain (  X 
A ,  X  B ). Now the canonical correlations and weights of (  X  A ,  X  B ) are computed and returned.
 Algorithm 1 Fast Approximate CCA 1: Input: A  X  R m  X  n of rank p , B  X  R m  X  ` of rank 3: Let S be the sampling matrix of a random subset 4: Draw a random diagonal matrix D of size m with 5:  X  A  X  X  X  SH  X  ( DA ) using fast subsampled WHT (see 6:  X  B  X  X  X  SH  X  ( DB ) using fast subsampled WHT (see 7: Compute and return the canonical correlations Theorem 12. With probability of at least 1  X   X  , Algo-rithm 1 returns an O ( ) -approximate CCA of ( A , B ) . Assuming Bj  X orck and Golub X  X  algorithm is used in line 7, Algorithm 1 runs in time O mn log m +  X  2 h Proof. Lemma 6 ensures that with probability of at least 1  X   X / 2,  X  ([  X A ;  X B ])  X  Assuming that the last inequality holds, Theorem 10 ensures that with probability of at least 1  X   X / 2, the canonical correlations and weights of (  X  A ,  X  B ) form an O ( )-approximate CCA of (  X A ,  X B ). By the union bound, both events hold together with probability of at least 1  X   X  . The RHT transforms applied to A and B are unitary, so for every  X  , an  X  -approximate CCA of (  X A ,  X B ) is also an  X  -approximate CCA of ( A , B ) (and vice versa).
 Running time analysis. Step 2 takes O (1) opera-tions. Step 3 requires O ( r ) operations. Step 4 requires O ( m ) operations. Step 5 involves the multiplication of A with SHD from the left. Computing DA requires O ( mn ) time. Multiplying SH by DA using fast sub-sampled WHT requires O ( mn log r ) time, as explained in Section 2.3. Similarly, step 6 requires O ( m` log r ) operations. Finally, step 7 takes O ( rn` + r ( n 2 + ` 2 time. Assuming that n  X  ` , the total running time is O ( rn 2 + mn log( r )). Plugging the value for r , and us-ing the fact that r  X  m , established our running time bound.
 From a practical point of view, our algorithm is use-ful for measuring the size of the correlated subspace, and obtaining the principal vectors of it. A reasonable value for is 0 . 1, or perhaps 0 . 01. So for reasonably high correlations, say above 0 . 2, we get some useful information. However, for lower correlations we get no information at all. Furthermore, it is too expen-sive to compute all the principal vectors, but once we know the size of the correlated subspace we can use the approximate weights to compute the vectors for that subspace. Now, we demonstrate that, unless r  X  m , it is not pos-sible to replace the additive error guarantees of Theo-rem 12 with relative error guarantees.
 Lemma 13. Assume that given any matrix pair ( A , B ) and any constant 0 &lt; &lt; 1 , Algorithm 1 computes a pair (  X  A ,  X  B ) by setting a sufficient large value for r in Step 2 so that the canonical correlations are relatively preserved with constant probability, i.e., with constant probability ( i = 1 ,...,q ): (1  X  )  X  i ( A , B )  X   X  i (  X  A ,  X  B )  X  (1 + )  X  i ( A , B ) Then, it follows that r =  X ( m/ log( m )) .
 The proof of this lemma appears in the full version of the present article (Avron et al., 2012). We now report the results of a few small-scale experi-ments. Our experiments are not meant to be exhaus-tive; however, they do show that our algorithm can be modified slightly to achieve very good performance in practice while still producing acceptable errors. Our implementation of Algorithm 1 differs from the pseudo-code description in two ways. First, we use r  X  X  X  min(  X  2 h for setting the sample size, i.e. we keep the same asymptotic behavior, but drop the constants. The con-stants in Algorithm 1 are rather large, so they preclude the possibility of beating Bj  X orck and Golub X  X  algorithm for reasonable matrix sizes. Our implementation also differs in the choice of underlying mixing matrix. Algo-rithm 1, and the analysis, uses the WHT. However, it is possible to show that other Fourier-type transforms will work as well (the bounds remain unchanged), and that some of these alternative transforms have certain advantages that make them better suited for an actual implementation (Avron et al., 2010). Specifically, we use the implementation of randomized Discrete Hart-ley Transform in the Blendenpik library 1 .
 We report the results of three experiments. In each experiment we run our code five times on a fixed pair of pair of matrices (datasets) A and B , and compared the different outputs to the true canonical correlations. The first two experiments involved synthetic data-sets, for which we set = 0 . 25 and  X  = 0 . 05. The last experiment was conducted on a real-life dataset, and we used = 0 . 5 and  X  = 0 . 2. All experiments were conducted in a 64-bit version of MATLAB 7 . 8. We used a two quad-core Intel E5410 computer running at 2.33 GHz, with 32GB DDR2 800 MHz RAM, running Linux 2.6, but we use a single core only.
 Synthetic Experiment 1. In this experiment we first draw five random matrices: three matrices G , W , Z  X  R m  X  n with independent entries from the normal distribution, and two matrices X , Y  X  R n  X  n with independent entries from the uniform distribu-tion on [0 , 1]. We now set A = GX + 0 . 1  X  W and B = GY + 0 . 1  X  Z . We use the sizes m = 120 , 000 and n = 60. Conceptually, we first take a random ba-sis (the columns of G ), and linearly transform it in two different ways (by multiplying by X and Y ). The transformation does not change the space spanned by the bases. We now add to each base some random noise (0 . 1  X  W and 0 . 1  X  Z ). Since both A and B essen-tially span the same column space, only polluted by different noise, we expect ( A , B ) to have mostly large canonical correlations (close to 1), but also a few small ones. Indeed, Figure 1(a), which plots the canonical correlations of this pair, shows that this is the case. Figure 2(a) shows the (signed) error in approximating the correlations, in five different runs. The actual error is always an order of magnitude smaller than the input ; the maximum absolute error is only 0 . 011. For large canonical correlations the error is much smaller, and the approximated value is very accurate. For smaller correlations, the error starts to get larger, but it is still an order of magnitude smaller than the actual value for the smallest correlation. As for the running time, the proposed algorithm takes about 40% less time than Bj  X orck and Golub X  X  algorithm (3 sec vs. 5 sec). Synthetic Experiment 2. In this experiment we first draw three random matrices. The first matrix, X  X  R m  X  n has independent entries from the normal distribution. The second matrix Y  X  R m  X  k has inde-pendent entries which take value  X  1 with equal proba-bility, and the third matrix Z  X  R k  X  n has independent entries from the uniform distribution on [0 , 1]. We now set A = X + 0 . 1  X  Y  X  ( 1 k  X  n + Z ) and B = Y , where 1 k  X  n is the k  X  n all-ones matrix. We use the sizes m = 80 , 000, n = 80 and k = 60. Here we basically have noise ( B ) and a matrix polluted with that noise ( A ). So there is some correlation, but really the two subspaces are different; there is one large correlation (almost 1) and all the rest are small (Figure 1(b)). Figure 2(b) shows the (signed) error in approximating the correlations, in five different runs. The actual error is an order of magnitude smaller than the target ; the maximum absolute error is only 0 . 02. Again, for the largest canonical correlation (which is close to 1) the result is very accurate, with tiny errors. For the other correlations it is larger. For tiny correlations the error is about the same magnitude as the actual value. Interestingly, we observe a bias towards over-estimating the correlations. As for the running time, the proposed algorithm takes about 30% less time than Bj  X orck and Golub X  X  algorithm (3 . 1 sec vs. 4 . 5 sec). Real-life dataset: Mediamill. We also tested the proposed algorithm on the annotated video dataset from the Mediamill Challenge (Snoek et al., 2006) 2 Combining the training set and the challenge set, 43907 images are provided, each image is a represen-tative keyframe image of a video shot. The dataset provides 120 features for each image, and the set is annotated with 101 labels. Figure 1(c) shows the ex-act canonical correlations. We see there is a few high correlations, with very strong decay afterwards. Figure 2(c) shows the (signed) error in approximating the correlations, in five different runs. The maximum absolute error is rather small (only 0.055). For the large correlations, which are the more interesting ones in this context, the error is much smaller, so we have a relatively high accuracy approximation. Again, there is an interesting bias towards over-estimating the cor-relations. As for the running time, the proposed algo-rithm is considerably faster than Bj  X orck and Golub X  X  algorithm (2 . 05 sec vs. 5 . 84 sec).
 Summary. The experiments are not exhaustive, but they do suggest the following. First, it appears that the sampling size bounds are rather loose. The al-gorithm achieves much better approximation errors. Second, there seems to be a connection between the canonical correlation value and the error: for larger correlations the error is smaller. Our bounds fail to capture these phenomena. Finally, the experiments show that the proposed is faster than Bj  X orck and Golub X  X  algorithm in practice on both synthetic and real-life datasets, even if they are fairly small. Haim Avron and Christos Boutsidis acknowledge the support from XDATA program of the Defense Ad-vanced Research Projects Agency (DARPA), admin-istered through Air Force Research Laboratory con-tract FA8750-12-C-0323. Sivan Toledo was supported by grant 1045/09 from the Israel Science Foundation (founded by the Israel Academy of Sciences and Hu-manities) and by grant 2010231 from the US-Israel Bi-national Science Foundation.
 Ailon, N. and Liberty, E. Fast dimension reduction using Rademacher series on dual BCH codes. In SODA , 2008.
 Avron, H., Maymounkov, P., and Toledo, S. Blenden-pik: Supercharging LAPACK X  X  least-squares solver.
SIAM Journal on Scientific Computing , 32(3):1217 X  1236, 2010.
 Avron, Haim, Boutsidis, Christos, Toledo, Sivan, and
Zouzias, Anastasios. Efficient dimensionality re-duction for canonical correlation analysis. CoRR , abs/1209.2185, 2012.
 Bj  X orck, A. and Golub, G.H. Numerical methods for computing angles between linear subspaces. Mathe-matics of computation , 27(123):579 X 594, 1973. Boutsidis, C. and Drineas, P. Random projections for the nonnegative least-squares problem. Linear Al-gebra and its Applications , 431(5-7):760 X 771, 2009. Boutsidis, C., Zouzias, A., and Drineas, P. Random projections for k -means clustering. In NIPS , 2010. Chaudhuri, K., Kakade, S. M., Livescu, K., and Srid-haran, K. Multi-view clustering via canonical corre-lation analysis. In ICML , pp. 129 X 136, 2009. Dhillon, P., Rodu, J., Foster, D., and Ungar, L. Using
CCA to improve CCA: A new spectral method for estimating vector models of words. In ICML , 2012. Dhillon, P. S., Foster, D., and Ungar, L. Multi-view learning of word embeddings via CCA. In NIPS , 2011.
 Drineas, P., Mahoney, M.W., Muthukrishnan, S., and
Sarl  X os, T. Faster least squares approximation. Nu-merische Mathematik , 117(2):217 X 249, 2011.
 Drineas, P., Magdon-Ismail, M., Mahoney, M. W., and
Woodruff, D. P. Fast approximation of matrix co-herence and statistical leverage. In ICML , 2012. Golub, G.H. and Zha, H. The canonical correlations of matrix pairs and their numerical computation.
IMA Volumes in Mathematics and its Applications , 69:27 X 27, 1995.
 Halko, N., Martinsson, P.G., and Tropp, J.A. Find-ing structure with randomness: Probabilistic algo-rithms for constructing approximate matrix decom-positions. SIAM Review , 53(2):217 X 288, 2011.
 Hotelling, H. Relations between two sets of variates. Biometrika , 28(3/4):321 X 377, 1936.
 Ipsen, I. and Wentworth, T.. The effect of coherence on sampling from matrices with orthonormal columns, and preconditioned least squares problems. Arxiv preprint arXiv:1203.4809 , 2012.
 Kim, T.-K., Kittler, J., and Cipolla, R. Discriminative learning and recognition of image set classes using canonical correlations. IEEE Trans. Pattern Anal. Mach. Intell. , 29(6):1005 X 1018, 2007.
 Rokhlin, V. and Tygert, M. A fast randomized al-gorithm for overdetermined linear least-squares re-gression. Proceedings of the National Academy of Sciences , 105(36):13212, 2008.
 Sarl  X os, T. Improved approximation algorithms for large matrices via random projections. In FOCS , 2006.
 Snoek, C. G. M., Worring, M., van Gemert, J. C.,
Geusebroek, J. M., and Smeulders, A. W. M. The challenge problem for automated detection of 101 semantic concepts in multimedia. In Proceedings of the ACM international conference on Multimedia , pp. 421 X 430, 2006.
 Su, Y., Fu, Y., Gao, X., and Tian, Q. Discriminant learning through multiple principal angles for visual recognition. Image Processing, IEEE Transactions on , 21(3):1381  X 1390, March 2012.
 Sun, L., Ji, S., and Ye, J. A least squares formula-tion for canonical correlation analysis. In ICML , pp. 1024 X 1031, 2008.
 Sun, L., Ceran, B., and Ye, J. A scalable two-stage ap-proach for a class of dimensionality reduction tech-niques. In KDD , pp. 313 X 322, 2010.
 Talwalkar, Ameet and Rostamizadeh, Afshin. Matrix coherence and the nystrom method. In UAI , pp. 572 X 579, 2010.
 Tropp, J. Improved analysis of the subsampled ran-domized Hadamard transform. Adv. Adapt. Data
Anal., special issue,  X  X parse Representation of Data
