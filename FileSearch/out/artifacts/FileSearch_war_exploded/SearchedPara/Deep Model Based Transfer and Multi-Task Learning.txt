 A central theme in learning from image data is to develop appropriate image representations for the specific task at hand. Traditional methods used handcrafted local features combined with high-level image representations to generate image-level representations. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila melanogaster , texture features based on wavelets were particularly effective for determining the developmen-tal stages from in situ hybridization (ISH) images. Such image representation is however not suitable for controlled vocabulary (CV) term annotation because each CV term is often associated with only a part of an image. Here, we developed problem-independent feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks (CNNs) that can act on image pixels directly. To make the extracted features generic, the models were trained us-ing a natural image set with millions of labeled examples. These models were transferred to the ISH image domain and used directly as feature extractors to compute image rep-resentations. Furthermore, we employed multi-task learn-ing method to fine-tune the pre-trained models with labeled ISH images, and also extracted features from the fine-tuned models. Experimental results showed that feature repre-sentations computed by deep models based on transfer and multi-task learning significantly outperformed other meth-ods for annotating gene expression patterns at different stage ranges. We also demonstrated that the intermediate layers of deep models produced the best gene expression pattern representations.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Deep learning; transfer learning; multi-task learning; image analysis; bioinformatics
A general consensus in image-related research is that dif-ferent recognition and learning tasks may require different image representations. Thus, a central challenge in learn-ing from image data is to develop appropriate representa-tions for the specific task at hand. Traditionally, a common practice is to hand-tune features for specific tasks, which is time-consuming and requires substantial domain knowledge. For example, in the study of gene expression patterns in Drosophila melanogaster , texture features based on wavelets, such as Gabor filters, were particularly effective for deter-mining the developmental stages from in situ hybridization (ISH) images [24]. Such image representation, often referred to as  X  X lobal visual features X , is not suitable for controlled vocabulary (CV) term annotation because each CV term is often associated with only a part of an image, thereby re-quiring an image representation of local visual features [8, 27]. Current state-of-the-art systems for CV term annota-tion first extracted local patches of an image and computed local features which are invariant to certain geometric trans-formations (e.g., scaling and translation). Each image was then represented as a bag of  X  X isual words X , known as the  X  X ag-of-words X  representation [7], or a set of  X  X parse codes X , known as the  X  X parse coding X  representation [9, 19, 25].
In addition to being problem-dependent, a common prop-erty of traditional feature extraction methods is that they are  X  X hallow X , because only one or two levels of feature ex-traction was applied, and the parameters for computing fea-tures are usually not trained using supervised algorithms. Given the complexity of patterns captured by biological im-ag es, these shallow models of feature extraction may not be sufficient. Therefore, it is desirable to develop a multi-layer feature extractor, alleviating the tedious process of manual feature engineering and enhancing the representation power.
In this work, we proposed to employ the deep learning methods to generate representations of ISH images. Deep learning models are a class of multi-level systems that can act on the raw input images directly to compute increas-ingly high-level representations. One particular type of deep learning models that have achieved practical success is the deep convolutional neural networks (CNNs) [13]. These models stack many layers of trainable convolutional filters and pooling operations on top of each other, thereby com-puting increasingly abstract representations of the inputs. Deep CNNs trained with millions of labeled natural images using supervised learning algorithms have led to dramatic performance improvement in natural image recognition and detection tasks [6, 10, 18].

However, learning a deep CNN is usually associated with the estimation of millions of parameters, and this requires a large number of labeled image samples. This bottleneck currently prevents the application of CNNs to many biolog-ical problems due to the limited amount of labeled training data. To overcome this difficulty, we proposed to develop generic and problem-independent feature extraction meth-ods , which involves applying previously obtained knowledge to solve different but related problems. This is made pos-sible by the initial success of transferring features among different natural image data sets [4, 17, 26]. These studies trained the models on the ImageNet data set that contains millions of labeled natural images with thousands of cate-gories. The learned models were then applied to other im-age data sets for feature extraction, since layers of the deep models are expected to capture the intrinsic characteristics of visual objects.

In this article, we explored whether the transfer learning property of CNNs can be generalized to compute features for biological images. We proposed to transfer knowledge from natural images by training CNNs on the ImageNet data set. To take this idea one step further, we proposed to fine-tune the trained model with labeled ISH images, and resumed training from already learned weights using multi-task learn-ing schemes. The two models were then both used as a fea-ture extractors to compute image features from Drosophila gene expression pattern images. The resulting features were subsequently used to train and validate our machine learn-ing method for annotating gene expression patterns. The overall pipeline of this work is given in Figure 1. Experimental results show that our approach of using CNNs outperformed the sparse coding methods [19] for an-notating gene expression patterns at different stage ranges. In addition, our results indicated that the transfer and fine-tuning of knowledge by CNNs from natural images is very beneficial for producing high-level representations of biolog-ical images. Furthermore, we showed that the intermediate layers of CNNs produced the best gene expression pattern representations. This is because the early layers encode very primitive image features that are not enough to capture gene expression patterns. Meanwhile, the later layers captured features that are specific to the training natural image set, and these features may not be relevant to gene expression pattern images.
Deep learning models are a class of methods that are ca-pable of learning hierarchy of features from raw input im-ages. Convolutional neural networks (CNNs) are a class of deep learning models that were designed to simulate the vi-sual signal processing in central nervous systems [1, 10, 13]. These models usually consist of alternating combination of convolutional layers with trainable filters and local neigh-borhood pooling layers, resulting in a complex hierarchical representations of the inputs. CNNs are intrinsically capable of capturing highly nonlinear mappings between inputs and outputs. When trained with millions of labeled images, they have achieved superior performance on many image-related tasks [10, 13, 18].

A key challenge in applying CNNs to biological problems is that the available labeled training samples are very lim-ited. To overcome this difficulty and develop a universal representation for biological image informatics, we proposed to employ transfer learning to transfer knowledge from la-beled image data that are problem-independent. The idea of transfer learning is to improve the performance of a task by applying knowledge acquired from different but related task with a lot of training samples. This approach of transfer learning has already yielded superior performance on natu-ral image recognition tasks [4, 14, 17, 23, 26].

In this work, we explored whether this transfer learning property of CNNs can be generalized to biological images. Specifically, the CNN model was trained on the ImageNet data containing millions of labeled natural images with thou-sands of categories and used directly as feature extractors to compute representations for ISH images. In this work, we applied the pre-trained VGG model [18] that was trained on the ImageNet data to perform several computer vision tasks, such as localization, detection and classification. There are two pre-trained models in [18], which are  X 16 X  and  X 19 X  weight layers models. Since these two models generated similar performance on our ISH images, we used the  X 16 X  weight layers model in our experiment. The VGG architec-ture contains 36 layers. This network includes convolutional layers with fixed filter sizes and different numbers of feature maps. It also applied rectified non-linearity, max-pooling to different layers.

More details on various layers in the VGG weight layer model are given in Figure 2. Since the output feature rep-resentations of layers before the third max pooling layer in-volve larger feature vectors, we used each Drosophila ISH image as input to the VGG model and extracted features from layers 17, 21, 24, and 30 to reduce the computational cost. We then flattened all the feature maps and concate-nated them into a single feature vector. For example, the number of feature maps in layer 21 is 512, and the corre-sponding size of feature maps is 28  X  28. Thus, the corre-sponding size of feature vector for this layer is 401,408.
In addition to the transfer learning scheme described above, we also proposed a multi-task learning strategy in which a CNN is first trained in the supervised mode using the Ima-geNet data and then fine-tuned on the labeled ISH Drosophila images. This strategy is different from the pre-trained model we used above. To be specific, the pre-trained model is designed to recognize objects in natural images while we studied the CV term annotation of Drosophila images in-stead. Although the leveraged knowledge from the source task could reflect some common characteristics shared in these two types of images such as corners or edges, extra ef-forts are also needed to capture the specific properties of ISH images. The Drosophila gene expression pattern images are organized into groups, and multiple CV term annotations are assigned to multiple images in the same group. This multi-image multi-label nature poses significant challenges to traditional image annotation methodologies. This is par-tially due to the fact that there are ambiguous multiple-to-multiple relationships between images and CV term an-notations, since each group of images are associated with multiple CV term annotations.

In this paper, we proposed to use multi-task learning strat-egy to overcome the above difficulty. To be specific, we first employed a CNN model that is pre-trained on natural im-ages to initialize the parameters of a deep network. Then, we fine-tuned this network using multiple annotation term prediction tasks to obtain CV term-specific discriminative representation. The pipeline of our method is illustrated in Figure 1. We have a single pre-trained network with the same inputs but with multiple outputs, each of which corre-sponds to a term annotation task. These outputs are fully connected to a hidden layer that they share. Because all outputs share a common layer, the internal representations learned by one task could be used by other tasks. Note that the back-propagation is done in parallel on these out-puts in the network. For each task, we used its individual loss function to measure the difference between outputs and the ground truth. In particular, we are given a training set of k tasks { X i , y j i } m i =1 , j = 1 , 2 , . . . , k , where X notes the i -th training sample, m denotes the total number of training samples. Note that we used the same groups of samples for different tasks, which is a simplified version of traditional multi-task learning. The output label y j i denotes the CV term annotation status of training sample, which is binary with the form y j i = 1 if X i is annotated with the j -th CV term, 0 otherwise. To quantitatively measure the difference between the pre-dicted annotation results and ground truth from human ex-perts, we used a loss function in the following form: loss ( y ,  X  y ) =  X  where and y = { y j i } m,k i,j =1 denotes the ground truth label matrix over different tasks, and  X  y = { y j i } m,k i,j =1 is the output matrix of our network through feedforward propagation. Note that  X  y denotes the network output before the softmax activation function. This loss function is a special case of the cross entropy loss function by using sigmoid function to induce probability representation [2, 3]. Note that our multi-task loss function is the summation of multiple loss functions, and all of them are optimized simultaneously during training.
The Drosophila melanogaster has been widely used as a model organism for the study of genetics and developmental biology. To determine the gene expression patterns during Drosophila embryogenesis, the Berkeley Drosophila Genome Project (BDGP) used high throughput RNA in situ hy-bridization (ISH) to generate a systematic gene expression image database [20, 21]. In BDGP, each image captures the gene expression patterns of a single gene in an embryo. Each gene expression image is annotated with a collection of anatomical and developmental ontology terms using a CV term annotation to identify the characteristic structures in embryogenesis. This annotation work is now mainly carried out manually by human experts, which makes the whole pro-cess time-consuming and costly. In addition, the number of available images is now increasing rapidly. Therefore, it is desirable to design an automatic and systematic annotation approach to increase the efficiency and accelerate biological discovery [5, 8, 11, 12, 15, 16].

Prior studies have employed machine learning and com-puter vision techniques to automate this task. Due to the effects of stochastic process in development, every embryo develops differently. In addition, the shape and position of the same embryonic part may vary from image to image. Thus, how to handle local distortions on the images is crucial for building robust annotation methods. The seminal work in [28] employed the wavelet-embryo features by using the wavelet transformation to project the original pixel-based embryonic images onto a new feature domain. In subsequent work, local patches were first extracted from an image and local features which are invariant to certain geometric trans-formations (e.g., scaling and translation) were then com-puted from each patch. Each image was then represented as a bag of  X  X isual words X , known as the  X  X ag-of-words X  repre-sentation [7], or a set of  X  X parse codes X , known as the X  X parse coding X  representation [19, 25]. All prior methods used handcrafted local features combined with high-level meth-ods, such as the bag-of-words or sparse coding schemes, to obtain image representations. These methods can be viewed as two-layer feature extractors. In this work, we proposed to employ the deep CNNs as a multi-layer feature extractor to generate image representations for CV term annotation.
We showed here that a universal feature extractor trained on problem-independent data set can be used to compute feature representations for CV term annotation. Further-more, the model trained on problem-independent data set, such as the ImageNet data, can be fine-tuned on labeled data from specific domains using the error back propagation algo-rithm. This will ensure that the knowledge transferred from problem-independent images is adapted and tuned to cap-ture domain-specific features in biological images. Since gen-erating manually annotated biological images is both time-consuming and costly, the transfer of knowledge from other domains, such as the natural image world, is essential in achieving competitive performance. st age range and the numbers of positive samples for each term.
In this study, we used the Drosophila ISH gene expression pattern images provided by the FlyExpress database [12, 22], which contains genome-wide, standardized images from multiple sources, including the Berkeley Drosophila Genome Project (BDGP). For each Drosophila embryo, a set of high-resolution, two-dimensional image series were taken from different views (lateral, dorsal, and lateral-dorsal and other intermediate views). These images were then subsequently standardized semi-manually. In this study, we focused on the lateral-view images only, since most of images in FlyEx-press are in lateral view.

In the FlyExpress database, the embryogenesis of Drosophila has been divided into six discrete stage ranges (stages 1-3, 4-6, 7-8, 9-10, 11-12, and 13-17). We used those images in the later 5 stage ranges in the CV term annotation, since only a very small number of keywords were used in the first stage range. One characteristic of these images is that a group of images from the same stage and same gene are assigned with the same set of keywords. Prior work in [19] has shown that image-level annotation outperformed group-level anno-tation using the BDGP images. In this work, we focused on the image-level annotation only and used the same top 10 keywords that are most frequently annotated for each stage range as in [19]. The statistics of the numbers of images and most frequent 10 annotation terms for each stage range are given in Table 1.

For CV term annotation, our image data set is highly im-balanced with much more negative samples than positive ones. For example, there are 7564 images in stages 13-17, but only 891 of them are annotated the term  X  X orsal protho-racic pharyngeal muscle X . The commonly-used classification algorithms might not work well for our specific problem, because they usually aimed to minimizing the overall error rate without paying special attention to the positive class. Prior work in [19] has shown that using under-sampling with ensemble learning could produce better prediction perfor-mance. In particular, we selectively under-sampled the ma-jority class to obtain the same number of samples as the minority class and built a model for each sampling. This process was performed many times for each keyword to ob-tain a robust prediction. Following [19], we employed classi-fier ensembles built on biased samples to train robust models for annotation. In order to further improve the performance, we produced the final prediction by using majority voting, since this sample scheme is one of the widely used methods for fusion of multiple classifiers. For comparison purpose, we also implemented the existing sparse coding image rep-resentation method studied in [19]. The annotation perfor-mance was measured using accuracy, specificity, sensitivity and area under the ROC curve (AUC) for CV term anno-tation. For all of these measures, a higher value indicates better annotation performance. All classifiers used in this work are the  X  2 -norm regularized logistic regression.
The deep learning model consists of multiple layer of fea-ture maps for representing the input images. With this hi-erarchical representation, a natural question is which layer has the most discriminative power to capture the character-istics of input images. When such networks were trained on natural image data set such as the ImageNet data, the fea-tures computed in lower layers usually correspond to local features of objects such as edges, corners or edge/color con-junctions. In contrast, the features encoded at higher layers mainly represent class-specific information of the training data. Therefore, for the task of natural object recognition, the features extracted from higher layers usually yielded bet-ter discriminative power [26].

In order to identify the most discriminative features for the gene expression pattern annotation tasks, we compared the features extracted from various layers of the VGG net-work. Specifically, we used the ISH images as inputs to the pre-trained VGG network and extracted features from layers 17, 21, 24, and 30 for each ISH image. These features were used for the annotation tasks, and the results are given in Figure 3. We can observe that for all stage ranges, layer 21 features outperformed other features in terms of overall per-formance. Specifically, the discriminative power increases from layer 17 to layer 21, and then drops afterwards as the depth of network increases. This indicates that gene expres-sion features are best represented in the intermediate layers of CNN that was trained on natural image data set. One reasonable explanation about this observation is the lower layers compute very primitive image features that are not enough to capture gene expression patterns. Meanwhile, the higher layers captured features that are specific to the training natural image set, and these features may not be relevant for gene expression pattern images.

Then we proposed to use multi-task learning strategy to fine-tune the pre-trained network with labeled ISH images. In order to show the gains through fine-tuning on pre-trained model, we extracted features from the same hidden layers that are used for the pre-trained model. We reported the predictive performance achieved by features of different lay-ers in the proposed fine-tuned model in Figure 4. It can be observed from the results that the predictive performance were extracted. features were extracted. consider the features extracted from layer 21 of these two deep models. was generally higher on middle layers in the deep archi-tecture. In particular, layer 21 outperformed other layers significantly. This result is consistent with the observation found on the pre-trained model.
We also compared the performance achieved by different methods including sparse coding, transfer learning model and multi-task learning. These results demonstrated that our deep model with multi-task learning were able to accu-rately annotate gene expression images over all embryoge-nesis stage ranges. To compare our generic features with the domain-specific features used in [19], we compared the annotation performance of our deep learning features with that achieved by the domain-specific sparse coding features. Deep learning models include transfer learning and multi-task learning. In this experiment, we only considered the features extracted from layer 21 since they yielded the best performance among different layers. The performance of these three types of features averaged over all terms is given in Figure 5 and Table 2. We can observe that the deep model for multi-task learning features outperformed the sparse cod-ing features and transfer learning features consistently and significantly in all cases. To examine the performance differ-ences on individual anatomical terms, we showed the AUC values on each term in Figure 6 for different stage ranges. We can observe that our features extracted from layer 21 of the VGG networks for transfer learning and multi-task learning outperformed the sparse coding features over all stage ranges for all terms consistently. These results demonstrated that our generic features of deep models were better at represent-ing gene expression pattern images than the problem-specific features based on sparse coding.

In Figure 7, we provided a term-by-term and image-by-image comparison between the results of the deep model for multi-task learning and the sparse coding features for the 10 terms in stages 13-17. The x-axis corresponds to the 10 terms. The y-axis corresponds to a subset of 50 images in stages 13-17 with the largest numbers of annotated terms. Overall, it is clear that the total number of green and blue entries is much more than the number of red and pink en-tries, indicating that, among all predictions disagreed by these two methods, the predictions by the multi-task learn-ing features were correct most of the time.
In this work, we proposed to employ the deep convolu-tional neural networks as a multi-layer feature extractor to generate generic representations for ISH images. We used the deep convolutional neural network trained on large nat-ural image set as feature extractors for ISH images. We first directly used the model trained on natural images as feature extractors. We then employed multi-task classifica-tion methods to fine-tune the pre-trained model with labeled ISH images. Although the number of annotated ISH im-ages is small, it nevertheless improved the pre-trained model. We compared the performance of our generic approach with the problem-specific methods. Results showed that our pro-posed approach significantly outperformed prior methods on ISH image annotation. We also showed that the intermedi-ate layers of deep models produced the best gene expression pattern representations. of the sparse coding features.

In the current study, we focus on using deep models for CV annotation. There are many other biological image analysis tasks that require appropriate image representations such as developmental stage prediction. We will consider broader applications in the future. In this work, we considered a simplified version of the problem in which each term is as-sociated with all images in the same group. We will extend our model to incorporate the image group information in the future. This work was supported in part by research grants from NSF (IIS-0953662, DBI-1147134 and DBI-1350258), NIH (R01 LM010730, HG002516-09), and the NVIDIA Corpo-ration with the donation of the Tesla K40 GPU. [1] Y. Bengio, A. Courville, and P. Vincent.
 [2] C. Bishop. Neural Networks for Pattern Recognition . [3] R. Collobert and J. Weston. A unified architecture for [4] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, [5] E. Frise, A. S. Hammonds, and S. E. Celniker. [6] R. Girshick, J. Donahue, T. Darrell, and J. Malik. [7] S. Ji, Y.-X. Li, Z.-H. Zhou, S. Kumar, and J. Ye. A [8] S. Ji, L. Sun, R. Jin, S. Kumar, and J. Ye. Automated [9] S. Ji, L. Yuan, Y.-X. Li, Z.-H. Zhou, S. Kumar, and [10] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet [11] S. Kumar, K. Jayaraman, S. Panchanathan, [12] S. Kumar, C. Konikoff, B. Van Emden, C. Busick, [13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [14] M. Oquab, I. Laptev, L. Bottou, and J. Sivic. [15] I. Pruteanu-Malinici, D. L. Mace, and U. Ohler. [16] K. Puniyani, C. Faloutsos, and E. P. Xing. SPEX2: [17] A. S. Razavian, H. Azizpour, J. Sullivan, and [18] K. Simonyan and A. Zisserman. Very deep [19] Q. Sun, S. Muckatira, L. Yuan, S. Ji, S. Newfeld, [20] P. Tomancak, A. Beaton, R. Weiszmann, E. Kwan, [21] P. Tomancak, B. Berman, A. Beaton, R. Weiszmann, [22] B. Van Emden, H. Ramos, S. Panchanathan, [23] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How [24] L. Yuan, C. Pan, S. Ji, M. McCutchan, Z.-H. Zhou, [25] L. Yuan, A. Woodard, S. Ji, Y. Jiang, Z.-H. Zhou, [26] M. D. Zeiler and R. Fergus. Visualizing and [27] W. Zhang, D. Feng, R. Li, A. Chernikov, [28] J. Zhou and H. Peng. Automatic recognition and
