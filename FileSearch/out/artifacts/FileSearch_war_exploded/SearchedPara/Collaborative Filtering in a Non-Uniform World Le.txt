 Brain and Cognitive Sciences and CSAIL, MIT Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, model with connections to large-margin norm-regularized l earning [17, 1, 15].
 Current theoretical guarantees on using the trace-norm for matrix completion assume a uniform where rows of the matrix represent e.g. users and columns rep resent e.g. movies, this corresponds and some movies are rated by many people while others are rare ly rated.
 In this paper we show, both analytically and through simulat ions, that this is not a deficiency of the proof techniques used to establish the above guarantees . Indeed, a non-uniform sampling dis-complexity. Under non-uniform sampling, as many as  X ( n 4 / 3 ) samples might be needed for learn-ing even a simple (e.g. orthogonal low rank) n  X  n matrix. This is in sharp contrast to the uniform enough to learn a low-rank model even under an arbitrary non-uniform distribution. Our analysis further suggests a weighted correction to the t race-norm regularizer, that takes into account the sampling distribution. Although appearing at fi rst as counter-intuitive, and indeed be-ing the opposite of a previously suggested weighting [21], t his weighting is well-motivated by our analytic analysis and we discuss how it corrects the problem s that the unweighted trace-norm has with non-uniform sampling. We show how the weighted trace-n orm indeed yields a significant improvement on the highly non-uniformly sampled Netflix dat aset.
 The only other work we are aware of that studies matrix comple tion under non-uniform sampling is work on exact completion (i.e. when the matrix is assumed to be exactly low rank) under power-law sampling [12]. Other then being limited to one specific di stribution, the requirement of the matrix being exactly low rank is central to this work, and the results cannot be directly applied in the presence of even small noise. Empirically, the approa ch leads to deterioration in predictive performance on the Netflix data [12]. Consider the problem of predicting the entries of some unkno wn target matrix Y  X  R n  X  m based on a random subset S of observed entries Y users and the number of movies, and Y may represent a matrix of partially observed rating values. Predicting elements of Y can be done by finding a matrix X minimizing the training error, here measured as a squared error, and some measure c ( X ) of complexity. That is, minimizing either: or: where Y and 0 otherwise. For now we ignore possible repeated entries in S and we also assume that n  X  m without loss of generality. The two formulations (1) and (2) are equivalent up to some (unknown) correspondence between  X  and C , and we will be referring to them interchangeably. A basic measure of complexity is the rank of X , corresponding to the minimal dimensionality k such one of the most popular approaches to collaborative filterin g. However, the rank is non-convex and measuring the complexity.
 Trace-norm Regularization been advocated and were shown to enjoy considerable empiric al success [14, 15]. This corresponds to measuring complexity in terms of the trace-norm of X , which can be defined equivalently either as the sum of the singular values of X , or as [7]: where the dimensionality of U and V is not constrained. Beyond the modeling appeal of norm-based, rather than dimension-based, regularization, the t race-norm is a convex function of X and so can be minimized by either local search or more sophisticate d convex optimization techniques. Scaling The rank, as a measure of complexity, does not scale with the s ize of the matrix. That is, even very large matrices can have low rank. Viewing the rank as a comple xity measure corresponding to the number of underlying factors, if data is explained by e.g. tw o factors, then no matter how many rows ( X  X sers X ) and columns ( X  X ovies X ) we consider, the data will still have rank two. The trace-norm, however, does scale with the size of the matrix. To see this, n ote that the trace-norm is the  X  of the spectrum, while the Frobenius norm is the  X  The Frobenius norm certainly increases with the size of the m atrix, since the magnitude of each ele-ment does not decrease when we have more elements, and so the t race-norm will also increase. The above suggests measuring the trace-norm relative to the Fro benius norm. Without loss of generality, consider each target entry to be of roughly unit magnitude, a nd so in order to fit Y each entry of X must also be of roughly unit magnitude. This suggests scalin g the trace-norm by specifically, we study the trace-norm through the complexit y measure: which puts the trace-norm on a comparable scale to the rank. I n particular, when each entry of X is, on-average, of unit magnitude (i.e. has unit variance) we ha ve 1  X  tc ( X )  X  rank ( X ) . rank matrices X = U  X  V where the rows of U and also the rows of V are orthogonal and of equal in
U have norm q n/ Such an orthogonal low-rank matrix can be obtained, e.g., wh en entries of U and V are zero-mean i.i.d. Gaussian with variance 1 / Generalization Guarantees and sample complexity guarantees that can be obtained for lo w-rank and low-trace-norm learning. then by minimizing the training error subject to a rank const raint (a computationally intractable X a matrix X whose overall average error is close to that of X  X  [18]. In these bounds tc ( X ) plays precisely the same role as the rank, up to logarithmic factor s.
 We therefore would expect to be able to learn X when we have roughly this many samples, as is indeed confirmed by the rigorous sample complexity bounds.
 For low-trace-norm learning, consider a sample S of size | S | X  Cn , for some constant C . Taking entries of Y to be of unit magnitude, we have k Y be zero outside S ). From (4) we therefore have: k Y underlying matrix Y is, we can always perfectly fit the training data with a low tra ce-norm matrix X s.t. tc ( X )  X  C , without generalizing at all outside S . On the other hand, we must allow matrices have more than this many samples we can start learning. In this section, we analyze trace-norm regularized learnin g when the sampling distribution is not uniform. That is, when there is some, known or unknown, non-u niform distribution D over entries is to get low average error with respect to the distribution D . That is, we measure generalization performance in terms of the weighted sum-squared-error: error subject to a low-rank constraint, non-uniformity doe s not pose a problem. The same generaliza-tion and learning guarantees that can be obtained in the unif orm case, also hold under an arbitrary distribution D . In particular, if there is some low-rank X  X  such that k X  X   X  Y k 2  X  O ( rank ( X  X  )( n + m )) samples are enough in order to learn (by minimizing training error subject to a rank constraint) a matrix X with k X  X  Y k 2 However, the same does not hold when learning using the trace -norm. To see this, consider an orthogonal rank-k square n  X  n matrix, and a sampling distribution which is uniform over an n sub-matrix A , with n rows, and the column (e.g.  X  X ovie X ) is selected uniformly am ong the first n A to denote the subset of entries in the submatrix, i.e. A = { ( i, j ) | 1  X  i, j  X  n S , we have: where we again take the entries in Y to be of unit magnitude. In the second inequality above we use the fact that Y n restricting to even tc ( X ) &lt; 1 , we can neither learn Y , since we can shatter Y example, when a = 2 / 3 and so n tc greater than the sample size needed to learn a matrix with con stant tc ( X ) in the uniform case. rank-k matrix by minimizing the trace-norm using  X  O ( kn ) samples when the sampling distribution is concentrated on a small submatrix? Of course this is not th e case. Since the samples are uniform on a small submatrix, we can just think of the submatrix A as ou r entire space. The target matrix still has low rank, even when restricted to A, and we are back i n the uniform sampling scenario. The only issue here is that tc ( X )  X  k , i.e. k X k tr  X  n observation scenario. When samples are concentrated in n smaller trace norm, k X k tr  X  n a We can, however, modify the example and construct a sampling distribution under which  X ( n 4 / 3 ) samples are required in order to learn even an  X  X rthogonal X  l ow-rank matrix, no matter what con-which is what we would expect, and what is required for learni ng by constraining the rank directly. To do so, consider another submatrix B of size n that the rows and columns of A and B do not overlap (see figure). Now, consider a sampling distribution D which is uniform over A with probability half, and uni-form over B with probability half. Consider fitting a noisy matrix Y = X  X  + noise where X  X  is  X  X rthogonal X  rank-k . In order to fit on B , we need to allow a trace-norm of at least k X  X  with such a generous constraint on the trace-norm, we will be able to shatter S  X  A whenever sub-matrices A and B are independent, shattering S  X  A means we cannot hope to learn in A . Setting a trace-norm which is too low to fit X  X  enough to overfit Y Empirical Example Let us consider a simple simulation experiment that will hel p us illustrates this phenomenon. Con-sider a simple synthetic example, where we used n rank-2 matrix X  X  and Y = X  X  + N (0 , 1) (in case of repeated entries, the noise is independent for each appearance in the sample). The training sample size was also set to | S | =140,000. The three curves of Fig. 1 measure the excess (test) error k X  X  X  X  k 2 of the learned model, as well as the error contribution from A and from B , as a function of the overfitting A (achieving almost zero excess test error), leads to a subopt imal fit on B . Penalty Formulation also insightful to consider the penalty view (1), i.e. learn ing by minimizing First observe that the characterization (3) allows us to dec ompose k X k tr = k X where w.l.o.g. we take all columns of U and V outside A and B to be zero. Since we also have k Y (8) as: where tc measured relative to the size of A (similarly B ). We see that the training objective decomposes to objectives over A and B . Each one of these corresponds to a trace-norm regularized l earning problem, under a uniform sampling distribution (in the corr esponding submatrix) of a noisy low-rank  X  X rthogonal X  matrix, and can therefor be learned with  X  O ( kn In other words,  X  O ( kn ) samples should be enough to learn both inside A and inside B . However, the regularization tradeoff parameter  X  compounds the two problems. When the objective regularization in B , so learning on B is not possible, perhaps since it is scaled by n is too small and does not provide enough regularization in A .
 Returning to our simulation experiment, the solid curves of Fig. 1, right panel, show the excess path of regularized solutions is now parameterized by  X  rather than by the bound on tc ( X ) . Not surprisingly, we see the same phenomena: different values o f  X  are required for optimal learning on A and on B . Forcing the same  X  on both parts of the training objective (9) yields a deterior ation in the generalization performance. The decomposition (9) and the discussion in the previous sec tion suggests weighting the trace-norm We propose using the following weighted version of the trace -norm as a regularizer: sponding normalized complexity measure is given by tc distribution we have that tc rank-k matrix X we have tc weighted trace-norm of k X  X  k tr shatter using such a weighted trace-norm? We can shatter a sa mple if k Y calculate: That is, we can shatter a sample of size up to | S | = 8 kn It seems that now, with a fixed constraint on the weighted trac e-norm, we have enough capacity to both fit X  X  , and with  X  O ( kn ) samples, avoid overfitting on A .
 Returning to the penalization view (2) we can again decompos e the training objective as: avoiding the scaling by the block sizes which we encountered in (9).
 Returning to the synthetic experiments of Fig. 1 (right pane l), and comparing (9) with (12), we see that introducing the weighting corresponds to a relative ch ange of n the regularization tradeoff parameters used for A and for B . This corresponds to a shift of log n A in the log-domain used in the figure. Shifting the solid red (b ottom) curve by this amount yields the dashed red (bottom) curve. The solid blue (top) curve and the dashed red (bottom) curve thus represent the excess error on B and on A when the weighted trace norm is used, i.e. the training objective (12) is minimized. The dashed black (middle) curv e is the overall excess error when using this training objective. As can be seen, the weighting align s the excess errors on A and on B much better, and yields a lower overall error. The weighted trace -norm achieves the lowest MSE of 0.4301 with corresponding  X  = 0 . 11 . This is compared to the lowest MSE of 0.4981 with  X  = 0 . 80 , achieved by the unweighted trace-norm.
 tice, particularly when working with large and imbalanced d atasets, it may be easier to search for regularization parameters using weighted trace-norm.
 n and the corresponding normalized complexity measure tc  X  ( X ) = k X k 2 tr Other Weightings and Bayesian Perspective The weighted trace-norm motivated by the analysis here (wit h  X  = 1 ) implies that the frequent users (equivalently movies) get regularized much stronger than t he rare users (equivalently movies). This might at first seem quite counter-intuitive as the natural we ighting might seem to be the opposite. Indeed, Weimer et al. [21] speculated that with a uniform weighting (  X  = 0 ) frequent users are regularized too heavily compared to infrequent users, and s o suggested regularizing frequent users (and movies) with a lower weight, corresponding to  X  =  X  1 . Although this might seem natural, we saw here that the reverse is actually true  X  the Weimer et al. weighting (  X  =  X  1 ) would only make things worse. Indeed, given the analysis here, Weimer et al. actually observed a deterioration in prediction quality when using their weighting. This is also demonstrated in the experiments on the Netflix data in Section 6. The weighted regularization motivated here (with  X  = 1 ) is also quite unusual from Bayesian per-spective. The trace-norm can be viewed as a negative-log-pr ior for the Probabilistic Matrix Factor-ization model [15], where entries of U, V are taken to be i.i.d. Gaussian. The two terms of (8) can then be interpreted as a log-likelihood and log-prior, and m inimizing (8) corresponds to finding the MAP parameters. Introducing weighting (with  X  = 1 ) effectively states that the effect of the prior becomes stronger as we observe more data. Yet, our analysis strongly suggest t hat in non-uniform setting, such  X  X northodox X  regularization is crucial for a chieving good generalization performance. When dealing with large datasets, such as the Netflix data, th e most practical way to fit trace-norm regularized models is through stochastic gradient descent [15, 8]. Let n P objective using a partially-weighted trace-norm 10 can be w ritten as: opposite the gradient of the term corresponding to the chose n ( i, j ) .
 Note that even though the objective (13) as a function of U and V is non-convex, there are no non-using very large values of k becomes computationally expensive. Instead, we consider t runcated prediction performance.
 In our experiments, we also replace unknown row p ( i ) and column q ( j ) marginals in (13) by their Setting  X  = 1 , corresponding to the weighted trace-norm (10), results in stochastic gradient updates that do not involve the row and column counts at all and are in s ome sense the simplest. Strangely, these steps match the stochastic training used by many pract itioners on the Netflix dataset, without explicitly considering the weighted trace-norm [8, 19, 15] . laborative filtering dataset. The training set contains 100 ,480,507 ratings from 480,189 anonymous out of which we set aside 100,000 ratings for validation. The  X  X ualification set X  pairs were selected also created a  X  X est set X  by randomly selecting and removing 100,000 ratings from the training set. All ratings were normalized to be zero-mean by subtracting 3 .6. The dataset is very imbalanced: it includes users with over 10,000 ratings as well as users who r ated fewer than 5 movies. For various values of  X  , we learned a factorization U  X  V with k = 30 and with k = 100 dimensions (factors) using stochastic gradient descent as in (13). For each value of  X  and k we selected the between the two. For both k = 30 and k = 100 , the weighted trace-norm (  X  = 1 ) significantly outperformed the unweighted trace-norm (  X  = 0 ). Interestingly, the optimal weighting (setting of  X  ) was a bit lower then, but very close to  X  = 1 . For completeness, we also evaluated the weighting suggest ed by Weimer et al. [21], corresponding to  X  =  X  1 . Unsurprising, given our analysis, this seemingly intuitive weighting hurts predictive performance.
 For both k = 30 and k = 100 , we also observed that for the weighted trace-norm (  X  = 1 ) good ( parameters using the weighted trace-norm.
 Comparison with the Max-Norm We also compared the predictive performance on Netflix to pre dictions based on max-norm regular-ization. The max-norm is defined as: max-norm hold also under an arbitrary, non-uniform, sampli ng distribution. Specifically, defining mc max-norm can be used as an alternative factorization-regul arization in the presence of non-uniform sampling. Indeed, as evident in Table 1, max-norm based regu larization does perform much better then the unweighted trace-norm. The differences between th e max-norm and the weighted trace-norm are small, but it seems that using the weighted trace-no rm is slightly but consistently better. In this paper we showed both analytically and empirically th at under non-uniform sampling, trace-complexity. Our analytic analysis suggests a non-intuitiv e weighting for the trace-norm in order to correct the problem. Our results on both synthetic and on the highly imbalanced Netflix datasets fur-ther demonstrate that the weighted trace-norm yields signi ficant improvements in prediction quality. In terms of optimization, we focused on stochastic gradient descent,both since it is a simple and practical method for very large-scale trace-norm optimiza tion [15, 8], and since the weighting was originally stumbled upon through this optimization approa ch. However, most recently proposed weighted trace-norm.
 We hope that the weighted trace-norm, and the discussions in Sections 3 and 4, will be helpful in deriving theoretical learning guarantees for arbitrary non-uniform sampling distributions, both in the form of generalization error bounds as in [18], and gener alizing the compressed-sensing inspired work on recovery of noisy low-rank matrices as in [4, 13].
 Acknowledgments RS is supported by NSERC, Shell, and NTT Communication Scien ces Laboratory. [2] S. Burer and R.D.C. Monteiro. Local minima and convergen ce in low-rank semidefinite pro-[4] E.J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE (to appear) , [5] E.J. Candes and B. Recht. Exact matrix completion via con vex optimization. Foundations of [6] E.J. Candes and T. Tao. The power of convex relaxation: Ne ar-optimal matrix completion. [7] M. Fazel, H. Hindi, and S.P. Boyd. A rank minimization heu ristic with application to minimum [8] Yehuda Koren. Factorization meets the neighborhood: a m ultifaceted collaborative filtering [9] Z. Liu and L. Vandenberghe. Interior-point method for nu clear norm approximation with [10] S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman i terative methods for matrix rank [11] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral Reg ularization Algorithms for Learning [12] R. Meka, P. Jain, and I. S. Dhillon. Matrix completion fr om power-law distributed samples. In [13] B. Recht. A simpler approach to matrix completion. prep rint, available from author X  X  webpage, [14] J.D.M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative [15] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic m atrix factorization. In Advances in [16] N. Srebro, N. Alon, and T. Jaakkola. Generalization err or bounds for collaborative prediction [17] N. Srebro, J. Rennie, and T. Jaakkola. Maximum margin ma trix factorization. In Advances In [18] N. Srebro and A. Shraibman. Rank, trace-norm and max-no rm. In COLT , 2005. [20] R. Tomioka, T. Suzuki, M. Sugiyama, and H. Kashima. A fas t augmented lagrangian algorithm [21] M. Weimer, A. Karatzoglou, and A. Smola. Improving maxi mum margin matrix factorization.
