 Data may often contain multiple plausible clusterings. In order to discover a clustering which is useful to the user, con-strained clustering techniques have been proposed to guide the search. Typically, these techniques assume background knowledge in the form of explicit information about the de-sired clustering. In contrast, we consider the setting in which the background knowledge is instead about an undesired clustering. Such knowledge may be obtained from an ex-isting classification or precedent algorithm. The problem is then to find a novel,  X  X rthogonal X  clustering in the data. We present a general algorithmic framework which makes use of cluster ensemble methods to solve this problem. One key advantage of this approach is that it takes a base clustering method which is used as a black box, allowing the practi-tioner to select the most appropriate clustering method for the domain. We present experimental results on synthetic and text data which establish the competitiveness of this framework.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Algorithms Non-Redundant Clustering, Cluster Ensembles
Data clustering is one of the most popular exploratory data analysis techniques with a broad range of application in many empirical sciences and fields of engineering. Given the well-known difficulties in formally defining the desired outcome of such a data exploration stage, it is important to develop clustering techniques that can utilize user feedback as guidance in searching for interesting grouping structure. This opens up two questions: what sort of information to use and how to use it. A number of recent papers have investigated ways to incorporate explicit feedback about the desired clustering. One line of research is based on the idea of constraining clustering solutions [9], exploiting pairwise instance-level constraints such as must-link and cannot-link [22, 11]. Another direction is to learn meaningful distance metrics based on side information on item similarity [23] or relative similarity [15]. Recently, the two directions have been combined in [1].

The philosophy pursued in this paper follows the sugges-tion in [21] of exploiting a different source of side informa-tion, namely, how to best avoid redundancies with existing classifications. This reflects the fact that users are often unable to positively describe what they are looking for, yet may be perfectly capable of expressing what is not of inter-est to them. This is especially true in an exploratory setting where the novelty aspect of the grouping structure is empha-sized and where known ways to group the data are readily available. For instance, one may want to cluster documents for which a topical classification exists. It seems natural to use the latter in order to suppress a redundant grouping by topic, which in turn should facilitate the discovery of novel ways to group the documents, e.g. by genre, writing style, or source. Such an approach can be applied, even if the user may not be able to positively describe properties or constraints for the non-topical document clustering.
A similar idea has been pursued in [3] by treating existing clusterings as negative side information and in [8], where a criterion based on conditional mutual information has been proposed to incorporate prior knowledge. This paper inves-tigates a distinctly different avenue which is based on the idea of suppressing an existing grouping structure by sepa-rately clustering within each of the induced groups and by then combining the obtained local clustering solutions to form a novel global clustering. The resulting algorithm is conceptually simple and easy to implement on top of virtu-ally any existing clustering method. This modularity aspect is extremely valuable in applications, where practitioners may already have committed themselves to specific cluster-ing techniques.
An example is shown in Figure 1(a) which contains two high quality clusterings shown in Figures 1(b) and 1(c). The for k = 2 . question we investigate is how to, given the clustering in 1(b), find the  X  X rthogonal X  clustering in 1(c). In the exam-ple pictured, the cluster assignment for a point in Clustering 2 is independent of the cluster assignment in Clustering 1. We refer to this assumption that the clusterings are inde-pendent as the  X  X rthogonality assumption X . We will also investigate the problem as the orthogonality assumption is relaxed and cluster assignment in the two clusterings is cor-related. We note that naive feature-based approaches, such as selecting certain features to be ignored, are not practi-cal and may even result in the loss of useful information. In high-dimensional settings there may be too many associ-ated features to identify by hand and it is not obvious how to otherwise determine which features ought to be ignored based on the given clustering. Furthermore, such an ap-proach may even be harmful. As we will see on data sets where the orthogonality assumption is relaxed, even if a fea-ture is in part associated with a given clustering, it may still provide useful information for other clusterings.
We will begin by providing an abstract, high-level view of the proposed algorithm, which illustrates the basic idea. As shown in Figure 2 the algorithm  X  which we call Con-dEns for Conditional Ensemble clustering  X  operates in three stages. In the first stage, local clustering solutions are com-puted for all data points belonging to the same cluster of the given clustering. This results in one refining clustering for every cluster of the original clustering. The second stage extends these local solutions to create global clustering so-lutions. How this is done depends on the specific base clus-tering method employed. Note that this stage is meant not to change the identity of clusters. That is, in model-based or centroid-based approaches one would keep the model pa-rameters or cluster centroids constant. For instance, with Expectation Maximization (EM), this stage corresponds to application of the E-step over all instances. With k-means, this stage corresponds to application of the assignment step over all instances. Finally, the global clustering solutions obtained from the local seeds are combined using an en-semble clustering method to produce the final coordinated clustering.

The rationale behind the algorithm is that by clustering within each group of the given partitioning, i.e. by comput-ing local refinements, one will effectively avoid reproducing the given solution. In addition, it seems intuitive that an alternative clustering that is in some sense orthogonal to the existing one, should also show up in each of the groups induced by the latter. If this intuition is correct, then the combination step should be able to recover it.

There are two questions that remain to be settled. First, one needs to render the combination step more precisely. This involves the specification of a concrete combination scheme for the consensus clustering stage. Second, it would be reassuring if the above intuitions could be underpinned by a theoretical analysis. We present such an attempt to shed some light on the working and assumptions of the above algorithm in Section 5.
Ensemble clustering deals with the problem of combining multiple clusterings to produce a combined clustering solu-tion. This problem has recently received a lot of attention (cf. [16, 19, 20, 18]) as a practical way of combining results from different clustering algorithms as well as to avoid over-fitting by data resampling [13]. In our case, we are only con-cerned with the combination step and not with the problem of how to generate the different clustering solutions, since this is determined by the first two steps of the CondEns algorithm.
 To restate the goal, we are given l clusterings C = { C 1 , . . . , C } where each clustering C j partitions the data into k j clusters. The goal is then to find a combined partition C into a pre-specified number of k clusters. Herein access is restricted to the clusterings only and no further access to the original data is needed. In our case, the latter fact ensures that the combination step will not favor clustering solutions that significantly overlap with the given clustering.
In [19], several techniques for obtaining a combined clus-tering solution are compared. The median partition tech-nique, in particular, has the most attractive time complex-ity while achieving results on par with the other techniques studied. It can be motivated by the objective presented in [16]:
Output: Clustering C : { 1 , . . . , n } X  X  1 , . . . , k } 1. Clustering 2. Extension 3. Combination Here I ( C ; C j ) is the mutual information : which measures how much information C gives about C j . Mutual information may be expressed according to the for-mula: I ( C ; C j ) = H ( C j )  X  H ( C j | C ) where H ( C ) is the well-known Shannon entropy: and H ( C | C j ) is the conditional entropy:
Instead of optimizing the objective (1) directly, [19] solve for a related quantity, the generalized mutual information . Generalized mutual information follows from the definition of the Havrda-Charv  X at structural  X  -entropy[10], also referred to as  X  X eneralized entropy X  for degree  X  : where s &gt; 0 and  X  6 = 1. We note that as  X   X  1, the Shannon entropy is obtained. The generalized mutual information would then be:
Following [19] we have obtained good results with the quadratic mutual information criterion: This criterion is up to a factor of 2 equivalent to the cat-egory utility function U presented in [7]. Moreover, it was shown in [14] that maximization of Eq. (7) for a fixed num-ber of target clusters is equivalent to the minimization of a squared-error criterion. The latter can be solved using stan-dard approximation techniques such as k-means clustering.
We also propose to optimize mutual information directly without requiring approximation. Using the Information Bottleneck of [17], one may directly optimize for: which optimizes for how much information C contains about the set of C j . We have implemented these two approaches for Consens in our experiments. We denote by CondEns MI the mutual information objective in (8) which we solve by use of a deterministic annealing Information Bottleneck tech-nique. We denote by CondEns 2 the approach of [19] for the quadratic approximation (7) which we solve by use of batch k-means followed by an online k-means refinement phase in order to obtain high quality solutions.
In this section, we will provide a partial analysis of when a certain target clustering embedded in the data set can be recovered by the above algorithm. The analysis requires making a number of assumptions on the base clustering al-gorithm, the cluster combination scheme, and the relative dominance of the target clustering. Certainly, these require-ments are quite strong and we do not claim that they can be met in most realistic settings. However, we believe they provide at least some strong motivation for the proposed method and that the insight gained by the analysis is valu-able in supplementing the empirical evidence presented in Section 6.

A crucial quantity to consider in the current setting is the conditional mutual information, defined as where A , B , and C are random variables. Intuitively, condi-tional mutual information measures how much information knowing B adds about A (and vice versa), provided that one already knows C . We state a few useful facts about con-ditional mutual information that directly follow from this definition.

Proposition 1. (a) I ( A ; B | C ) = I ( B ; A | C ) (b) I ( A ; B | C ) = I ( A ; B, C )  X  I ( A ; C ) (c) I ( A ; B | C )  X  I ( A ; B ) = I ( A ; C | B )  X  I ( A ; C ) . (d) A  X  X  X  C | B if and only if I ( A ; C | B ) = 0
We will quantify the quality of a clustering as the mu-tual information between the feature representation and the cluster membership variable, i.e.
 where H denotes the (conditional) entropy or the differen-tial (conditional) entropy, dependent on the type of feature representation. Herein we restrict clusterings to be deter-ministic functions of the feature representation, i.e. C : X X  { 1 , . . . , k } . We will write X for the input random variable and C ( X ) for the induced random variable over clusters. A simple consequence is that C will be conditionally in-dependent of any other random variable, given the input. In particular C is independent of any given partitioning Z = Z ( X ), i.e. C  X  X  X  Z | X or, equivalently I ( C ; Z | X ) = 0,  X  C , a fact that we will exploit in the sequel. Since we are interested in clusterings that are in some sense orthogonal to an existing clustering, we need to clarify the meaning of this concept. We proceed in the spirit of [6] which proposes a mutual-information score for cluster validity and [12] which argues for the use of a mutual-information score as it pro-vides a true metric on the space of clusterings.

Definition 1. C and Z are information-orthogonal (w.r.t. X ), if
This condition is a natural choice as it can be shown to be equivalent to the condition that C and Z are independent as well as to the condition that, using the metric of [12], C and Z attain the maximal distance possible. We note a first consequence.
 Lemma 1. If C and Z are information-orthogonal, then I ( C ; X ) = I ( C ; X | Z ) and I ( Z ; X ) = I ( Z ; X | C ) . Proof. Using the chain rule for mutual information, which when combined with (11) establishes that I ( C ; X ) = I ( C ; X | Z ) and I ( Z ; X ) = I ( Z ; X | C ) .
For arbitrary clusterings C , Lemma 1 may not hold, how-ever, the conditional independence from Z given X guaran-tees the following: Lemma 2. I ( C ; X | Z )  X  I ( C ; X ) Proof. Since I ( C ; Z | X ) = 0 one gets, I ( C ; Z )  X  I ( C ; X ) .
 An immediate implication of the two lemmata is the follow-ing corollary.

Corollary 1. If C  X  = argmax C I ( C ; X ) and C  X  is information-orthogonal to Z , then C  X  = argmax C I ( C ; X | Z ) . for all C .

The main problem is that the bound in Lemma 2 holds only on the average. Namely, one may have a situation that I ( C 0 ; X | Z )  X  I ( C ; X | Z ). While a general analysis seems difficult, we can prove that a dominant, information-orthogonal clustering will also dominate all other clusterings in at least one group of Z .
 Proposition 2. If C  X  is information-orthogonal to Z and I ( C  X  ; X ) &gt; I ( C ; X ) for all C  X  X  X  X  C  X  } , where C is chosen such that for all C j  X  argmax C I ( C ; X | Z = z C j  X  X  , then there exists at least one group j  X  such that I ( C  X  ; X | Z = z j  X  )  X  I ( C ; X | Z = z j  X  ) for all C .
Proof. We first point out that there exists some C such that I ( C ; X | Z ) = P j p ( z j ) I ( C j ; X | Z = z j constructed such that the restriction of C to the pre-images X j of Z equals C j . Using the fact that Z is a partition, only those X j participate in I ( C j ; X | Z = z j ) and so in C each X j may be separately assigned to its C j . In conjunction with Lemma 2 this yields: By assumption I ( C ; X ) &lt; I ( C  X  ; X ) and with Lemma 1 one arrives at
I ( C ; X ) &lt; I ( C  X  ; X ) = I ( C  X  ; X | Z ) If one now assumes that all C j 6 = C  X  , then by the local opti-mality of C j one would get I ( C j ; X | Z = z j )  X  I ( C  X  ; X | Z = z ) which contradicts the fact that because of the non-negativity of the individual terms in the is optimal for Z = z j  X  .

The relevance of this proposition is that asymptotically, if the base cluster method uses the mutual information cri-terion to derive local clustering solutions and if the target clustering is dominant and information-orthogonal to the given clustering, then the target clustering will be among the clustering solutions handed to the combination stage. One could hence select a clustering solution among the l clusterings C j . However, in practice a meaningful combi-nation procedure is a far more useful alternative, as shown experimentally in the following section. n = 800 items, m = 1100 draws.
We first evaluate the algorithm on 2-dimensional sets such as the one pictured in Figure 1. These are synthetic data sets drawn from multinomial distributions where the axes repre-sent event frequencies (e.g. term frequencies in documents. ) A data set contains n instances where each instance draws m events from its associated multinomial. The multinomials are chosen so as to produce two natural clusterings P and Q . In order to study the robustness of the algorithm with re-spect to the orthogonality assumption, the dimension associ-ated with Q is made dependent on the dimension associated with P according to an orthogonality weight  X  . At  X  = 0, membership in Q is deterministically defined by member-ship in P , but as  X   X  1, the P and Q become orthogo-nal. Examples for particular values of  X  are given in Figure 3. We assume the clustering P is known and evaluate the performance of CondEns using base clustering techniques: dIB: a deterministic annealing version of Information Bot-tleneck and EM: Expectation Maximization. CondEns is compared against the Coordinated Conditional Information Bottleneck algorithm [8]. Two algorithmic approaches for CCIB are considered: dCCIB: a deterministic annealing ver-sion, and seqCCIB: a sequential clustering version. Within all base clustering techniques, we assume features are multi-nomial and set k j = k . Results are evaluated using average precision after aligning the discovered and target clusterings using an optimal matching.

The results show that the choice of ensemble clustering al-gorithm, whether CondEns MI or CondEns 2 , has slight im-pact for EM with CondEns MI maintaining an advantage for  X   X  1, and virtually no impact for dIB . Comparing against CCIB, the results show that the CondEns techniques on av-erage underperform CCIB for orthogonal sets (high  X  ) in low k and the better scoring CondEns-EM slightly outperforms CCIB for higher k . The better relative performance on these sets from CondEns for higher k is not surprising as a larger ensemble of clusterings is available. Notably for all k the CondEns techniques outperform CCIB for less orthogonal sets (  X   X  0 . 4). Sets with lower  X  share the characteristic that the undesired clustering P is a much higher quality clustering than Q when measured over the entire set. This affects CCIB directly as it has a coordination term that fa-vors clusterings which are high quality with respect to the entire set. CondEns , on the other hand, finds high qual-ity clusterings independently within the pre-image sets. In those cases, Q is still a higher quality clustering.
We evaluate the performance on the WebKB data set[4] and several Reuters news story data sets. Documents are represented by their term counts. Each data set contains two fundamentally different classification schemes L 1 and L : (i) WebKB: L 1 = {  X  X ourse X ,  X  X aculty X ,  X  X roject X ,  X  X taff X , (ii) RCV1-gmcat2x2: L 1 = { MCAT (Markets), GCAT (iii) RCV1-ec5x6: 5 of the most frequent subcategories (iv) RCV1-top7x9: ECAT (Economics), MCAT (Mar-In each of the data sets, the classification schemes L 1 and L 2 are close to informational-orthogonal as can be seen in Table 2.
 Data set I ( L 1 ; X ) + I ( L 2 ; X ) I ( L 1 , L 2 ; X ) WebKB 2.6691 2.6504 RCV1-gmcat2x2 1.3863 1.3863 RCV1-ec5x6 3.8609 3.8385 RCV1-top7x9 3.1111 2.8558 Table 2: Information-orthogonality of the classifi-cation schemes for the text data sets. L 1 and L 2 are defined to be information-orthogonal if I ( L 1 ; X )+ I ( L 2 ; X ) = I ( L 1 , L 2 ; X ) .

In each session we assume one of these classifications is known, set k to the cardinality of the unknown classifica-tion and investigate how similar the clustering obtained by CondEns is to each classification. For the base clustering method we consider batch k-means using Euclidean distance and EM using a multinomial model, with k j = k . We omit results using IB techniques for the base clustering method since the performance was not better than EM and run times were much higher. The CondEns techniques are compared against dCCIB which had superior performance and faster runtime than seqCCIB. Since k may not equal the cardinal-ity of the known classification we use normalized mutual in-formation N M I ( C ; L ) = I ( C ; L ) /H ( L ) to evaluate results. Results are shown in Table 1.

The performance of the CondEns algorithm is compet-itive with CCIB while boasting runtimes that are an or-der of magnitude lower than the time required for CCIB. We note that CondEns-EM scores higher than CCIB on half of the sessions. The time advantage of CondEns is consis-tent across all sessions. This is not surprising as the time-complexity of CondEns should in fact typically be lower than that of the base clustering technique applied to the complete set. The complexity of clustering techniques is typ-ically superlinear in the number of instances[2, 5]. CondEns first divides the entire set into pre-image sets and performs clustering on each set independently, which is asymptoti-cally better than performing clustering over the complete set. As the subsequent extension and combination steps are comparatively inexpensive, we expect a lower overall com-plexity.
In order to better understand the workings of the algo-rithm, we now provide a walk-through of a representative run, examining the quality of the clusterings obtained at each step of the algorithm. Specifically, we consider Con-dEns using k-means and investigate the run reported for the WebKB data set. At the end of the first step (Clus-tering), we evaluate the base clustering methods by looking at the NMI individually computed for each pre-image set. At the end of the second step (Extension), we evaluate the NMI of the extended clustering with respect to both clus-terings. Results for Z = L 1 and Z = L 2 are in Tables 3 and 4. The NMI scores after the third step (Combination) are those in Table 1. Table 3 illustrates a scenario where only one of the extended clusterings is significantly like the target clustering: N M I ( C student ; L 2 ) = 0 . 3865. Encourag-ingly, within the combination step this is still sufficient to obtain a combined clustering C with N M I ( C ; L 2 ) = 0 . 2694. Table 4 shows a scenario where each of extended clusterings lower.
 1 Z = L 2
Data set Algorithm N M I ( C, L 1 ) N M I ( C, L 2 ) time N M I ( C, L WebKB dCCIB 0.0104 0.0243 46.94s 0.2929 0.0077 57.83s RCV1-gmcat2x2 dCCIB 0.0016 0.0111 7.04s 0.8548 0.0001 4.68s RCV1-ec5x6 dCCIB 0.0705 0.3278 176.89s 0.3160 0.2351 262.94s RCV1-top7x9 dCCIB 0.0245 0.2022 211.11s 0.5150 0.0074 102.77s is similar to the target clustering. Notably, the score after the combination step, N M I ( C ; L 1 ) = 0 . 2736, is higher than the score of each extended clustering in the ensemble, which argues for the use of ensemble clustering techniques.
We have presented a general framework for non-redundant clustering based on the idea of combining an ensemble of clusterings conditioned on the known clustering. This al-gorithm is conceptually simple, may be implemented effi-ciently, and can incorporate virtually any general-purpose or domain-specific clustering methods. We have introduced the notion of information-orthogonal clusterings and formally shown that for data with two such clusterings, when one is given as background knowledge the other will be repre-sented in the ensemble. On synthetic and text data sets we have established experimentally that this approach ob-tains competitive solutions while running an order of mag-nitude faster than existing approaches. Finally, on simple synthetic data sets, we have investigated the robustness of non-redundant clustering techniques as the orthogonality as-sumption is weakened and found that in practice the Con-dEns framework actually outperforms existing techniques, showing a graceful degradation in performance.
The majority of this work was completed while D.G. was supported by an NSF IGERT Ph.D. fellowship at Brown University and T.H. was at the Max-Planck Institute for Biological Cybernetics in T  X ubingen, Germany. [1] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [2] L. Bottou and Y. Bengio. Convergence properties of [3] G. Chechik and N. Tishby. Extracting relevant [4] M. Craven, D. DiPasquo, D. Freitag, A. K. McCallum, [5] I. Davidson and A. Satyanarayana. Speeding up [6] B. Dom. An information-theoretic external [7] M. Gluck and J. E. Corter. Information, uncertainty, [8] D. Gondek and T. Hofmann. Non-redundant data [9] A. Gordon. A survey of constrained classification. [10] J. Havrda and F. Charv  X at. Quantification method of [11] D. Klein, S. Kamvar, and C. Manning. From [12] M. Meil  X a. Comparing clusterings by the variation of [13] B. Minaei-Bidgoli, A. Topchy, and W. F. Punch. [14] B. Mirkin. Reinterpreting the category utility [15] M. Schultz and T. Joachims. Learning a distance [16] A. Strehl and J. Ghosh. Cluster ensembles: A [17] N. Tishby, F. C. Pereira, and W. Bialek. The [18] A. Topchy, M. Law, and A. K. Jain. Analysis of [19] A. Topchy, A. K. Jain, and W. Punch. Combining [20] A. Topchy, A. K. Jain, and W. Punch. A mixture [21] S. Vaithyanathan and D. Gondek. Clustering with [22] K. Wagstaff and C. Cardie. Clustering with [23] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell.
