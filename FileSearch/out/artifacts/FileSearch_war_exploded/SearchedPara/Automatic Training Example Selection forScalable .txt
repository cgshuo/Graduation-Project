 With massive amounts of data being coll ected by many businesses and govern-ment agencies, techniques that enable efficient sharing of large databases between organisations are of increasing importance in many data mining projects. Data from various sources often has to be linked in order to improve data quality and integrity, or to enrich existing data with additional information [12]. Linking en-tities is often challenged by the lack of entity identifiers, and thus sophisticated linkage techniques, using the available record attributes, are required [7].
For large databases, it is not feasible to compare each record from one database with all records from another database, as this process is computationally too expensive [7]. Blocking techniques are employed to reduce the number of record pair comparisons [1]. They group records into blocks, and candidate record pairs are then generated only from the records w ithin the same block. Assuming there are no duplicate records in the databases to be linked, then the majority of can-didate pairs are non-matches, as the maximum possible number of true matches corresponds to the number of records in the smaller of the databases. Classifying record pairs is thus often a very imbalanced problem.

Candidate record pairs are compared using similarity functions applied to selected record attributes. These functions can be as simple as an exact string or a numerical comparison, can take typographical variations into account [2], or they can be specialised, for example, for date or time values. Each similarity function returns a numerical matching weight , often normalised such that 1 . 0 corresponds to exact similarity and 0 . 0 to total dissimilarity. For each compared record pair a weight vector is formed that contains the pair X  X  matching weights. Using these weight vectors, reco rd pairs are then classified into matches , non-matches ,and possible matches , depending upon the decision model used [7].
A record pair that has equal or very similar attribute values will likely refer to the same entity, as it is very unlikely that two entities have very similar or even the same values in all their record attributes. The matching weights calculated when comparing such a pair will be 1 (or cl ose to 1) in all weight vector elements. On the other hand, weight vectors that contain matching weights of only 0 (or values close to 0) in all vector elements were with high likelihood calculated when two different entities were compared, as it is highly unlikely that two records that refer to the same entity have different values in all their attributes.
Based on these observations, it is norma lly easy to accurately classify a can-didate record pair as a match when its w eight vector contains only matching weights close to or equal to 1, and as a non-match when its weights are all close to or equal to 0. It is however much more di fficult to correctly classify a pair that has some similar and some dissimilar attribute values. In the examples shown in Fig. 1, records R1 and R2 are very similar to each other, and thus very likely refer to the same person. On the other hand, R3 and R4 are more different from each other, and it is not obvious if they refer to the same person.

It follows that it is possible, in a first step, to automatically select weight vectors that correspond to good quality training examples. For example, of the weight vectors shown in Fig. 1, WV(R1,R2) can be selected as a match training example, and WV(R1,R3) , WV(R2,R3) , WV(R1,R4) and WV(R2,R4) as non-match examples. These training examp les can then be used in a second step to train a binary classifier for classification of all weight vectors.

This two-step approach to record pair classification has first been proposed by the author in [5], with initial experiments indicating its feasibility. The contri-bution of this paper is the investigation of a potential improvement to the basic approach, namely to randomly include additional weight vectors for training. In recent years, various techniques have been explored for record pair classifica-tion [7]. Among the supervised learning techniques used are decision trees [8,11] and support vector machines [10], while another approach is adaptive string simi-larity functions [2]. While supervised techniques normally achieve better linkage quality than unsupervised ones, their major drawback is the lack of training data (record pairs with known true match and non-match status) in many real world situations, as manual preparation of training data is time consuming and expensive. Active learning aims to overcome this problem through manual clas-sification of only the most difficult record pairs to classify automatically [11].
Three classification approaches were compared in [8]: decision trees; k -means with three clusters (matches, possible matches and non-matches); and a hybrid approach that first clusters a sub-set of weight vectors (again into three clusters), and then uses the match and non-match clusters for decision tree induction learning. The supervised and hybrid approaches both outperformed k -means.
Methods similar to the proposed two-step approach have been developed for text and Web page classification [9,13], where often only a small number of pos-itive training examples is available besides many unlabeled documents. The aim is to learn a binary classifier from positive and unlabeled examples. PEBL [13] iteratively trains a support vector machine (SVM) using the positive and neg-ative documents furthest away from the decision boundary, while the S-EM [9] approach includes  X  X py X  documents, positive labeled examples, into the set of unlabeled documents to get a more realis tic model of their distribution to be used in the EM algorithm. This is similar to the idea of randomly including additional weight vectors into the training sets as presented in this paper. In the first step of the proposed approach, weight vectors that with high likeli-hood correspond to true matches and true non-matches are selected as training examples. In the second step, these training examples are used to train a binary classifier, which is then employed to classify all weight vectors into matches and non-matches. 3.1 Training Example Selection There are two different approaches on how to select training examples: threshold or nearest based [5]. In the first approach, weight vectors that have all their vector elements within a certain distance thre shold to the exact similarity or total dissimilarity values, respectively, will be selected. For example, using the weight vectors from Fig. 1 and a threshold of 0 . 2, only WV(R1,R2) will be selected as match training example, and WV(R1,R3) and WV(R2,R3) as non-match training examples. The remaining three weight vectors will not be selected, as at least one of their vector el ements is larger than the 0 . 2 distance threshold.
The second approach is to sort weight v ectors according to their distances from the vectors containing only exact similarities and only total dissimilarities, respectively, and to then select the respect ively nearest vectors. In Fig. 1, vector WV(R1,R2) is closest to the exact similarities vector, followed by WV(R3,R4) . Vectors WV(R1,R3) and WV(R2,R3) only contain total dissimilarity values, and WV(R1,R4) and WV(R2,R4) are the next vectors closest to them.

The notation used below is a follows. It is assumed that candidate record pairs are compared using d similarity functions (with d  X  1), resulting in a set W of weight vectors w i (1  X  i  X | W | )oflength d ,with | X | denoting the number of elements in a set. All comparison func tions are assumed to return normalised matching weights between 0 (total dissimilarity) and 1 (exact similarity), i.e. 0  X  w [ j ]  X  1 , 1  X  j  X  d,  X  w i  X  W . The weight vector containing exact similarities in all vector elements (i.e. correspo nding to an exact match) is denoted by m (with m [ j ]=1 , 1  X  j  X  d ), and the vector with only dissimilarities by n (with n [ j ]=0 , 1  X  j  X  d ). In the training example selection step, weight vectors from W will be inserted into the match training example set, W M ,andthe non-match training example set, W N . Generally, not all weight vectors from W Threshold-based Selection. One distance threshold for matches, t M ,andone for non-matches, t N (with 0 &lt;t M ,t N &lt; 1), are used to select weight vectors that have all their similarity values either within t M of the exact match value m ,or within t N of the total dissimilarity value n . Formally, weight vectors from W will be inserted into W M and W N , according to: W M = { w i  X  W :( m [ j ]  X  w i [ j ])  X  t M , 1  X  j  X  d } ,and W N = { w i  X  W :( n [ j ]+ w i [ j ])  X  t N , 1  X  j  X  d } . Nearest-based Selection. In this approach, the x M weight vectors closest to m are selected into W M ,andthe x N weight vectors closest to n are selected into W
N .Both x M &gt; 0and x N &gt; 0 must hold. The training sets W M and W N are distance function (like Euclidean distance), x M = | W M | ,and x N = | W N | .
Given the number of true non-matches in W is often much larger than the number of true matches [7], more weight vectors are selected into W N than into W
M . An estimation of the ratio r of matches to non-matches is calculated using the number of records in the two data sets to be linked, A and B , and the total 3.2 Random Inclusion of Additional Training Examples The training examples selected in the first step are likely linearly separable, be-cause W M and W N only contain weight vector s that are either close to m or close to n , and also because usually not all weight vectors from W are selected for training. This will likely result in a  X  X ap X  between the training sets, as illustrated in Fig. 2 (a). Similar to the inclusion of  X  X py X  documents for semi-supervised text classification [9], adding a small number of randomly selected weight vec-tors from this  X  X ap X  into W M and W N should improve classification accuracy, as the training sets will then contain a more realistic distribution of weight vectors. The random sampling of weight vectors should be done such that vectors closer to m are more likely included into W M , while vectors closer to n are more likely selected into W N . Besides no random sampling, the three different sampling methods illustrated in parts (b) to (d) of Fig. 2 are to use either uniform sam-pling, or a linear or exponential mapping function to randomly sample weight vectors. These sampling methods will be evaluated experimentally below. 3.3 Weight Vector Classification In the second step of the proposed record pair classification approach, the train-ing sets W M and W N , as generated in the first step, will be used to train a binary classifier. Once trained, this classifier is then employed to classify all weight vectors in W . In the experiments presented below, a SVM classifier [3] will be evaluated, because this technique is known to be robust to noisy data. The proposed two-step approach will be compared with three other classification methods. The first is an  X  X ptimal thres hold X  classifier that has access to the true match status of all weight vectors in W and can thus find a classification threshold that minimises both false matches and false non-matches. The second is a supervised SVM which also has acces s to the true match s tatus of all weight vectors. Nine SVM variations were evalu ated, three kernels (linear, polynomial and RBF) and three values for the cost parameter, C [3]. The third method is k -means, with the weight vectors being cl ustered into matches and non-matches. Three distance measures (Manhattan, Euclidean and L inf ) were evaluated. All experiments were conducted using 10-fold cross validation.

All classifiers were implemented in the Febrl [6] open source record linkage system, which is written in Python. The libsvm library was used for the SVM classifier [3]. All experiments were run on a Pentium 3 GHz CPU with 2 GBytes of main memory, running Linux 2.6.20 and using Python 2.5.1.
As summarised in Table 1, experiments were conducted using two real data sets from the SecondString toolkit 1 and four synthetic data sets containing names and addresses created with the Febrl data set generator [4]. The synthetic data sets are based on real-world frequency tables, and contain 60% original and 40% duplicate records (with randomly generated duplicates [5]). Standard blocking [1] was applied to reduce the number of record pair comparisons, and the Winkler string comparator [12] was used for comparing name and address values.
The quality and complexity of the compared record pairs is shown in Table 1 using the measures pairs completeness (number of true matched record pairs generated by blocking divided by the total number of true matched pairs) and reduction ratio (one minus the number of record pairs generated by blocking divided by the total number of pairs) [7,8]. The F-measure, the harmonic mean of precision and recall, is used to measur e classifier performance, as accuracy is not suitable for evaluating record pair classification due to the imbalanced distribution of matches and non-matches [7]. The quality of the training example sets generated in step one, as shown in Table 2, is calculated as the percentage of correctly selected weight vectors i n the training example sets, i.e. ( | true matches in W M | / | W M | )and( | true non-matches in W N | / | W N | ). 4.1 Training Example Quality As Table 2 shows, the quality of W M and W N is very good in most cases. For the threshold based approach, setting t M ,t N =0 . 5 achieved the best results, while a lower threshold can produce empty training sets (denoted by  X  X  X ), if all weight vectors have at least one matching weight with a similarity value above the selected threshol d Nearest-based selectio n overcomes this problem, and generally results in very good quality training example sets [5]. 4.2 Classification Performance Figure 3 shows the F-measure results of two data sets (due to limited space not all results can be shown) over the parame ter settings described in Sect. 4. The four random selection methods described in Sect. 3.2 are shown for the two-step classifier (with both 1% and 10% randomly added weight vectors evaluated). As expected, both supervised classifiers (optimal threshold and SVM) performed best. The two-step classifier outperformed k -means only without random selec-tion of additional weight vectors. Contrary to expectations, all random inclusion methods worsen the quality of the training sets and result in significantly re-duced classification performance for the two data sets shown (for other data sets, slightly better classification results were achieved for linear or exponen-tial random selection compared to no random selection). These results indicate that, unlike the random inclusion of  X  X py X  documents for semi-supervised text classification [9], inclusion of additional randomly selected weight vectors in the two-step approach is not improving record pair classification.

Given that normally only a portion of all weight vectors from W are used for training in the two-step classifier, the training time will be reduced. For example, if only 10% of all weight vectors are selected for training, then the training step should be around ten times faster comp ared to using all weight vectors in W , improving the scalability of record pair classification for large data sets. The discussed two-step approach allows unsupervised record pair classification with often better linkage quality than k -means clustering. Contrary to expec-tations, the inclusion of randomly selected additional weight vectors did not increased classification performance. Future work will include the evaluation of an approach that iteratively refines the training example sets by including the strongest classified matches and non-matches in each iteration, similar to the PEBL classifier developed for text and Web page classification [13]. This work is supported by an Australian Research Council (ARC) Linkage Grant LP0453463 and partially funded by the New South Wales Department of Health.
