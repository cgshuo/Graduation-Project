 Anne M. Denton Abstract An algorithm is introduced that distinguishes relevant data points from randomly distributed noise. The algorithm is related to subspace clustering based on axis-parallel pro-jections, but considers membership in any projected cluster of a given side length, as opposed to a particular cluster. An aggregate measure is introduced that is based on the total number of points that are close to the given point in all possible 2 d projections of a d -dimensional hypercube. No explicit summation over subspaces is required for evaluating this measure. Attribute values are normalized based on rank order to avoid making assumptions on the dis-tribution of random data. Effectiveness of the algorithm is demonstrated through comparison with conventional outlier detection on a real microarray data set as well as on time series subsequence data.
 Keywords Outlier analysis  X  Noise  X  Gene-expression analysis  X  Density-based clustering  X  Subspace clustering 1 Introduction In scientific data sets it is often of interest to distinguish relevant data from randomly distribu-ted noise. As an example, a biologist may want to know all genes that show gene expression patterns in a set of experiments. Finding relevant data points is important in its own right and also as a preprocessing step to other data mining tasks. The gene expression setting shows many examples of this need: clustering of genes based on cell cycle experiments [ 7 , 49 , 56 ], for example, is typically not performed on all genes but rather only on the subset that has been independently identified as relevant [ 15 , 51 ].

The problem of distinguishing relevant from irrelevant data has been extensively studied in does, however, lead to a non-trivial question: are the relevant data to be considered outliers or the randomly distributed noise? It is common to assume that relevant objects form groups. Density-based clustering [ 26 , 30 ] and subspace clustering [ 2 , 5 ] algorithms assume that rele-vant data show patterns that correspond to a comparatively high density in the attribute space or its subspaces. Random data are considered outliers and are not expected to participate in clusters.

Under the assumption that relevant data show patterns, it would be natural to conclude that any data points that do not show patterns are outliers. The problem with this interpretation is that for many interesting applications, random data are more common than non-random data. In gene expression experiments it is not unusual that only a small number of genes is involved in the process of interest. The experimental outcome for all other genes is random. In the cell-cycle data set described by Cho et al. [ 15 ],whichisusedinthispaper(Chodata set), only about 5% of genes are thought to be involved in the cell-cycle process. Considering 95% of data as outliers clearly does not fit the general understanding exemplified in [ 44 ]:  X  X n outlier is defined as a point that is very different from the rest of the data based on some measure X . That means that an interpretation of noise as outliers is problematic in the settings that are of interest in this paper.

Alternatively, one could argue that the non-random data should be considered the outliers if the bulk of the data is random. If there are patterns among the non-random data it can then be argued that those would form micro-clusters, which modern outlier detection algorithms can still detect as outliers [ 11 , 44 ]. The LOCI algorithm [ 44 ] does so by considering density fluctuations on different length scales. A comparison with the LOCI algorithm shows that the Subspace Sums algorithm is far more successful at identifying important points.
The Subspace Sums algorithm consistently follows the rationale that non-random data should exhibit patterns more clearly than random data. Its application to gene expression data was first presented in [ 20 ]. The algorithm is able to handle a large amount of noise by performing a thorough comparison with the distribution that would be expected for ran-dom data. An aggregate density is computed that sums over all axis-parallel subspaces of the full space. That means that, in contrast to subspace clustering algorithms [ 2 , 5 , 45 ]and subspace-based outlier detection algorithms [ 3 ], all subspaces are considered rather than selecting one subset of dimensions. This allows a simple quantitative comparison between different data points. Subspace clustering algorithms do not have a mechanism for such a quantitative comparison. This is exemplified in [ 33 ], in which subspace clustering is applied to cell-cycle gene expression data and it is observed that some known cell cycle genes are clustered correctly. However, the objective of the paper is to find clusters of similar data points and not to select those that show patterns.

In the gene expression setting, several techniques are being used for identifying differen-tially expressed genes from multiple experiments. The simplest approach is to set a threshold for the over-all log-ratio of expression. If experiments are all done using the same experimen-tal conditions, then populations can compared using statistical techniques [ 22 ]. Clustering techniques [ 6 , 24 , 25 , 41 , 48 , 49 ], are commonly used only after the subset of relevant genes has been independently determined. Coherence of gene clusters based on subsets of genes and samples has been studied [ 32 ] and a graph-based technique that additionally uses a net-work structure derived from gene ontologies has also been proposed [ 42 ]. For the discovery of cell cycle genes, many experiment-specific techniques have been developed [ 50  X  52 ].
The Subspace Sums algorithm does not make any assumptions beyond the standard notion of high density as an indication of a cluster of similar data points. This assumption is shared The contribution of the Subspace Sums algorithm is that density is aggregated over all axis-parallel projections of the space and that the aggregated density is compared with its expected value for randomly distributed data. An overrepresentation measure is used that objectively assesses whether a data point is in a denser region than would be expected from random data. This overrepresentation is maximized over differentlength scales. Itis important to note that it would not be practical to explicitly sum over all possible subspaces, since the number of axis-parallel subspaces of a d -dimensional space grows as 2 d . Hence, it is critical for the performance of the algorithm that it circumvents computations that involve individual subspaces.

The rest of the paper is organized as follows. Section 2 discusses related work. In Sect. 3 the Subspace Sums algorithm is motivated and developed. Section 4 provides the results of an evaluation based on gene expression and time series subsequence data, and Sect. 5 concludes the paper. An appendix provides a mathematical derivation of an expression for the observed aggregate density over all subspaces. 2 Related work The term outlier may be used to describe individual attribute values [ 40 , 47 ] or complete data points [ 3 , 11 , 37 , 39 , 44 , 55 ] that are exceptional. Outlier detection or noise removal may be done as part of data cleaning [ 47 , 55 ] or outliers may be of interest by themselves, such as in fraud detection [ 3 , 37 ]. Some outlier detection algorithms consider low density alone as a reason to consider a point as outlier [ 3 ]. Others consider more general variations of density [ 11 , 44 ]. The concept of evaluating density as a function of hypervolume radius has also been used for determining local dimensionality and using it for clustering [ 28 ].

Subspace clustering techniques [ 2 , 5 , 45 ] also inherently distinguish between high-density clusters and points that are not members of a cluster. Subspaces can be either axis-parallel [ 5 ] or general projections of the high-dimensional space [ 2 ]. Estimating the expected density of data points for random data is problematic in these approaches, making a comparison difficult. The Subspace Sums algorithm avoids this problem by not selecting subspaces, but rather summing over all possible axis-parallel projections of a hypercube of a given side length. If two points are to each other in many dimensions, those points will also be close in many projections.

Ingeneexpressiondataanalysis,statisticaltechniquesbasedonasinglesetofexperimental conditionsareconventionallyusedfordeterminingrelevanceofagenetoaprocess[ 22 ].Some clustering algorithms detect noise as a byproduct [ 25 ]. Bi-clustering techniques [ 14 ], which cluster genes and experiments simultaneously, typically also have the capability of excluding noise. Related techniques have been developed for gene expression data, for which multiple experimental conditions are available [ 9 , 27 ]. A further related problem in gene expression analysis is the selection of genes that are involved in a disease. This problem is addressed as feature selection in classification, and relies on the existence of a training set [ 21 ].
Gene expression data from yeast cell cycle experiments [ 15 , 51 ] have been used in many clustering papers [ 7 , 33 , 49 ], most of which apply clustering algorithms only to genes that have been identified as cell-cycle-relevant independently [ 7 , 49 , 56 ]. Kailing et al. [ 33 ]use subspace clustering for the complete set of genes and recognize several cell-cycle genes in clusters, but do not quantitatively test the selectivity of their algorithm.

The normalization used in this paper is related to quantile normalization [ 10 ]inthat the quantile of a point determines its normalized value. A uniform distribution is used as reference distribution. Such a normalization results in distances that are closely related to the mass-distance discussed in [ 57 ]. This paper goes beyond pairwise comparisons and evaluates the density at the location of a given gene based on all other genes within a neighborhood that is defined by the algorithm.
 The comparison with what is expected for random data is important for the Subspace Sums algorithm. Comparing patterns with a baseline has been discussed extensively in asso-ciation rule mining [ 23 ]andforspatialdata[ 38 , 43 ]. These approaches have the objective of identifying unusual events. The importance of testing clustering results against what could be expected for a random distribution has also been recognized in the microarray [ 25 ]and in the time series context [ 36 ]. These approaches use discretized representations to evaluate probabilities. When a good model of the noise distribution exists, it is possible to use the noise threshold in kernel-density-based clustering [ 17 , 18 ] as a way of separating meaningful data from noise. As an alternative approach, noise models have been built from labeled data [ 39 ].
The Subspace Sums algorithm returns a ranking of data from most to least likely non-random and ROC evaluation is used [ 53 ] to measure success. While this setup resembles classification with class labels non-random and random , it is important to note that no trai-ning is involved. No random samples are needed to define randomness [ 39 ]. The concept of randomness is entirely based on the notion of attributes being independent, or the dependence of attributes having a similar effect on all data points. 3 Subspace Sums algorithm 3.1 Relevance of multiple subspaces Subspace clustering algorithms based on axis-parallel projections assume that a set of dimen-sions can be found that best describes the data. That means that for any one cluster, the relevant dimensions are fixed. In practice, it is common that data points may occupy multiple sub-spaces of high density. For example, individual attribute values may be unreliable due to measurement error. Figure 1 illustrates how multiple subspaces can support a pattern. Its left panel shows a representation of five data points with three features ( f 0 , f 1 ,and f 2 ), using parallel coordinates [ 31 ]. Alternatively one may look at the image as a set of five time series of length three that can be embedded in a three dimensional vector space. The vector space representation of the same data points can be seen in the right panel of Fig. 1 ,wherethe data points are visualized through projections along each of the three coordinate axes. Note that point A has two neighbors in the projection onto the x 0  X  x 1 plane, and one neighbor in projections onto the x 1  X  x 2 and x 0  X  x 2 planes. A point is considered a neighbor if it is within a hypercube of side length a (see figure), or its projection. Note also that the neighbors differ for the two projections shown in Fig. 1 . In the projection along the x 0 axis only point C is a neighbor to A, while in the projection along the x 2 axis points B and C are A X  X  neighbors. Only one point (C) is close to A in all dimensions. Subspace clustering techniques would not simultaneously consider both projections as evidence for high density.

The combination of information from multiple subspaces increases the evidence that high density is not just the result of random fluctuations. The Subspace Sums algorithm attempts to find an objective measure of density by aggregating over all possible projections, rather than calculating density based on any one individual subspace. Since there are 2 d possible projections along combinations of axes for a d -dimensional space, it is essential that the sum over those combinations does not have to be calculated explicitly. Section 3.4 explains how the algorithm can achieve a linear rather than exponential scaling in d .

Before further discussing the concept of Subspace Sums, it is important to understand how densities can be compared objectively, such that data points can be ranked. For this purpose, a normalization is used that leads to a uniform density in any one of the individual dimensions. The next section elaborates on normalization. 3.2 Rank-order-based normalization of attributes The Subspace Sums algorithms is based on the notion that a high relative density of data points corresponds to a pattern in the data. That assumption can only be justified if randomly distributed data points lead to a density landscape with a constant ensemble average, i.e., if, over many realizations of random data, the average density is constant. Kernel-density-based clustering typically assumes that data are normalized to lie on a hypersphere. For normally distributed random data such a normalization does indeed result in a kernel-density that is, on average, constant.

Many real data are not normally distributed, and not all data mining applications are best addressedbynormalizingtoahypersphere.Nevertheless,itwillbeshownthatanormalization can be defined that makes no assumptions of random data other than that the attributes are independent, and that allows defining a score measure that is constant for random data. The central requirement for defining such a score measure is that the expected density for random data can be evaluated. If this is the case a maximum in the score measure that compares observed with expected density can be associated with the occurrence of a pattern in the data.
In most applications, little is known about noise. The attribute values in the Cho data set are far from being normally distributed. Figure 2 shows the distribution of individual attribute values in the gene expression set together with a normal distribution of equal mean and variance. Cell-cycle genes are not included in this plot to highlight the non-Gaussian behavior of random data. It can be seen that the expression data set has a narrower peak with much larger tails.

When little is known about a distribution, a common approach in statistics is to use rank order of attributes instead of numerical values. Following this concept, attributes are normalized such that the distribution of each attribute becomes uniform. Density fluctuations can still occur in the space spanned by multiple attributes. Those fluctuations determine whether a point is considered random. Strictly, it would be preferable to use only data that correspond to noise for the normalization process. If the distribution of random data is known, then the space of attributes can be transformed to make that distribution uniform [ 18 ]. However, no prior knowledge of which data are noise is assumed. Hence the normalization has to be done based on the full data. Since this work is primarily intended for the case of massive noise, it can be assumed that noise dominates the normalization.

One may consider defining a rank-order-based normalization as follows where N is the total number of records,  X ( x ) is the Dirac delta function and &gt; 0isa number smaller than all differences between attribute values. The transformation constrains transformation maps equal values into equal values. That means that some attribute values occur more often than others, and the modeling through a uniform distribution is less accurate.
This problem is more serious than it may initially appear: In practice, gene expression data are not available with arbitrary precision, and the same holds for most other types of data that are commonly called  X  X ontinuous X . The data set used for the evaluation lists only two digits after the decimal point. Considering that the data set has over 7,000 records, some values occur more than 100 times. The rank order among these is not uniquely determined, nor can it be expected that the experimental precision is high enough to make such a rank order meaningful. However, assigning the same normalized value multiple times results in a poor fit with the model of a uniform distribution.

In this paper a random ordering is assigned to identical attribute values, for lack of more detailed information. The approach maintains a uniform distribution. Since the experimental error is typically greater than the resolution, with which experimental results are reported, the randomness of this ordering is not expected to cause problems. Nevertheless a test was performed to see, how much the ordering does effect the reported results. For the gene expression data that is used in the evaluation, the standard deviation of the AUC value for 100 runs using different random orderings (noise fraction 0.5) was 1.6E-3, which is considered negligible. 3.3 Optimization of overrepresentation In conventional density-based clustering, the density of data points is defined based on either a kernel function [ 30 ] or a hypervolume [ 26 ]. Both approaches are related, since a uniform kernel can also be viewed as performing an average over hypervolumes. In both approaches the range of the hypervolume is fixed. In this work, the size of the hypervolume is varied, and the range parameter is chosen, for which non-random data are most strongly over-represented. A similar optimization is done in the LOCI outlier detection algorithm [ 44 ], but there a comparison is done with a correlation integral that is computed over a larger hypervolume of the data themselves. A second difference lies in the consideration of subspaces.
For simplicity, the overrepresentation for the full space is defined first. An algorithm that is based on this overrepresentation measure will be used for comparison purposes and is referred to as the Hypercube algorithm. The full-space overrepresentation as a function of a hypervolume size parameter a is defined as follows form distribution. For a data point with a randomly distributed neighborhood, c ( x , a )  X  1, i.e. there are about as many data points in the volume as would be expected from a random distribution. Note that even for random data, typically c ( x )&gt; 1 due to the optimization step. The largest occurring value of c ( x ) for random data, c thresh ( x ) = max x c ( x ) , can be used as a cutoff value for determining, which data should be considered relevant, see Sect. 4.6 . c ( x ) c thresh indicates the point is very likely to be relevant.

Note that c ( x ) is not a statistical measure of whether the neighborhood of x significantly differs from what would be expected from a random distribution. Statistical measures were also evaluated but did not perform as well for the following reason: The objective is to quantify density, in the same sense, in which density-based clustering techniques consider high density as an indication of a cluster or pattern. c ( x ) measures density in comparison with its expected value. It is not designed to represent the evidence for considering the neighborhood of a point as different from a random neighborhood. In real data sets, dimensions are often correlated. These correlations may lead to long-range fluctuations that do not provide much information on the point at its center. A measure similar to c ( x ) has been shown to be useful in clustering of time series subsequence data [ 19 ].

Expressions similar to  X  [ h ] obs ( x , a ) have also been discussed in the context of determining local dimensionality. Gionis et al. [ 28 ] propose a local growth curve, G x ( r ) , and study the inherent properties of G x ( r ) , in particular the slope in a log-log plot of G x ( r ) vs. r .This slope is identified with a local dimensionality that is then used in clustering.

Note that the overrepresentation is explicitly optimized for high density. The multi-granularitydeviationfactor(MDEF)thatisusedintheLOCIalgorithm[ 44 ],incontrast, is optimized for low density. The MDEF compares the actual number of objects n ( p i , X  r ) by averaging n ( p i , X  r ) over all objects p within a neighborhood of radius r of p i .MDEFis defined as where  X  is chosen to be 1 / 2 and the MAX metric is used, following the prescription for exact computations in [ 44 ].

It may be surprising that the algorithms in this paper as well as the LOCI algorithm are able to detect differentially expressed genes to some extent, despite the appearance that they test for opposite properties of the space. The reason for this counter-intuitive behavior is that the LOCI algorithm captures the low density of space for large expression values that is removed by the rank-order-based normalization used in this paper. Note that the MDEF is not suitable towards distinguishing time series data from noise in Sect. 4.4 .

The final and most important difference between conventional approaches and the opti-mization of overrepresentation in this paper lies in the consideration of subspaces. Both the observed and expected densities of Eq. ( 2 ) can be generalized to all subspaces by defining a density measure that is summed over subspaces where D s is the set of dimensions d i  X  D , that span subspace s ,and P ( D ) is the power set of all sets D . The proportion of points within the evaluation hypercube for subspace s is the number of projections of a d -dimensional space grows exponentially with the number of dimensions d . Section 3.4 shows how the summation can be circumvented.

Theexponentialdependenceofthenumberofsubspaceson d motivatesusingalogarithmic measure. A comparison based on  X  [ s ] itself is possible but proved not to be as robust in the evaluation. Note that  X  [ s ] ( x , a ) is guaranteed to be at least 1 due to the trivial subspace discussed in the next section. The logarithmic overrepresentation based on Subspace Sums is Its value is optimized over a set of a -values and the optimal value is reported for each data point.

For the Subspace Sums algorithm, the log transformation can be viewed as a means of comparing an effective number of dimensions over which points are within the neighborhood. For each data point the subspace sum corresponds to an exponential measure of the number of dimensions, over which the points are similar. Hence, the evaluation is based on the logarithm of the sum of the exponentials of matching dimensions. Note that there are parallels in physics for such a measure: the free energy of a system of particles can be calculated as the logarithm of the sum of exponentials of scaled energies of the individual particles [ 12 ].
The log transformation was also applied to the Hypercube algorithm, however, AUC values were consistently lower than 0.5: the expected and observed densities are both  X  1, and in that parameter range, and the absolute value of the logarithm becomes larger for smaller arguments. That means that the algorithm selects for points with a low local density. To avoid this problem, log ( 1 + obser v ed )/ log ( 1 + ex pected ) was also evaluated and gave results that were almost identical to those without log transformation (AUC values differing by no more than of the order of 1%). This can be understood since observed and expected densities are smaller than 1 and log ( 1 + x )  X  x for x small, i.e. the log transformation is not expected to have a strong impact. 3.4 Calculation of observed and expected proportion of points For a single d -dimensional space it is straightforward to calculate both the relative observed  X  within a hypercube of side length a that is centered on x ( k ) can be evaluated by comparing the distance in each of the d dimensions against a / 2 where  X  denotes the Heaviside step function, which is 0 for a negative argument and 1 for a positive argument. In this paper, hypercubes are used throughout rather than performing a separate optimization for the length of individual sides of the evaluation volume. This corresponds to a use of the MAX metric over the attribute space. Allowing more generality would also increase the probability of finding a data point that appears as relevant because of random fluctuations alone.

For a uniform distribution, it is possible to evaluate the expected proportion of points in a hypercube through a simple volume calculation. Note that the volume, in which data may be found is finite. The normalization procedure in Sect. 3.2 maps attribute values from an infinitely large space to the interior of the hypercube of side length 1. The evaluation volume has a side length a . Figure 3 shows that the evaluation hypercube may extend beyond the volume of points. This is not surprising: If a data point already has a large attribute value in one or more dimensions there are not many more data points expected with a higher value. The proportion of expected points is calculated on the basis of the intersection of the normalization hypercube (side length 1) and the evaluation hypercube (side length a ).
The expected proportion of points in an evaluation hypercube, centered on point x ,is given by the following product over intervals in each dimension
In practice, a one-dimensional grid of a values is considered, typically starting at a = 0 . 05, and continuing at equidistant intervals through a = 1. For x located near the perimeter of the normalization hypercube (1 / 2or  X  1 / 2), range values up to a = 2 can produce nontrivial overrepresentation results. For the Subspace Sums algorithm it was typically sufficient to consider values a  X  1, while for the Hypercube algorithm the overrepresentation was com-monly optimal for a &gt; 1. This difference will be discussed in Sect. 4.3 .

The Subspace Sums algorithm considers all projected subspaces of the full space. The observed number of points in the evaluation hypercube, summed over all subspaces, will be called the total weight of the data point. Figure 4 illustrates the concept for a two-dimensional  X  X ypercube X , i.e. square. Assume that the weight is to be calculated for point 0. Point 1 is an example point that is within range of point 0 for dimension x 1 and outside the range for dimension x 0 . This places point 1 within the projected hypercube for two of the subspaces, namely the top ones in the figure. For the two subspaces at the bottom of Fig. 4 , point 1 is outside the projected hypercube. That means that point 1 contributes 2 to the weight of point 0, based on the two subspaces, for which it is within range.

In general, it can be noted that the constraint of being within the given range only applies to dimensions, over which no projection has been performed. The following examples may help to clarify this concept:  X 
If there is only one dimension d g for which the distance is greater than a / 2, then the point is only in 2 d  X  1 projections, since only half of the projections include d g as a dimension over which the projection is performed.  X 
If the data point has only a single dimension d i for which it is closer than a / 2, then the data point is only inside the volume for two of the projected subspaces:  X  the trivial subspace that results by performing projections over all dimensions (see top  X  the subspace for which projections have been performed over all dimensions other than  X 
Every data point is part of the trivial, fully projected subspace, in which the hypercube does not provide any constraint, and that has the constant volume of 1.

It is important to note that the total weight of a point can be determined from a comparison with the other points in the data set alone. It is not necessary to iterate through all possible projections. Table 1 shows the subspaces for d = 3. The table is structured to show which x formally derived in the  X  X ppendix X : to the total weight of the data point at x ( k ) .

In the example from Fig. 1 , point A has a total weight of 15 based on the following components: Point C contributes a weight of 8 since it is close in all 3 dimensions. Point B is close only in two dimensions, i.e. contributes a weight of 4. Point D is close in 1 dimension and contributes a weight of 2. Point E is not close in any dimension and only has the weight of 1 determined by the trivial subspace. The total weight for each of the points in Fig. 1 is as follows: Point A B C D E Total weight 15 9 13 10 9  X  [ s ] 3.75 2.25 3.25 2.5 2.25
Point C has a weight that is only slightly lower than that of A since A is consistently close to C, resulting in a weight of 8. Overall, C is not as central, and the other two points result in smaller weights for C than for A. Points B, D, and E have substantially smaller total weights than A and C because they do not follow any pattern in the data for some of the subspaces.
The expected proportion of points is again calculated as the intersection of evaluation and normalization volumes. The relevant volume can be represented as a product of sums over the side length of the normalization volume (1) and the evaluation volume ( a ,ifthevolume is fully contained in the normalization volume). A formal derivation, based on Eqs.( 4 )and ( 7 ) and using the same reasoning as provided in the  X  X ppendix X , yields  X  Performing the summations leads to the following result for the sum of volumes over all dimensions
This can be understood by recognizing that each dimension may or may not be involved in defining the volume. The choice exists independently for each dimension. Hence, the contri-bution of the dimension being involved or not involved can be added independently for each dimension. Note that the only difference between Eqs.( 10 )and( 7 ) is the constant contri-bution of 1 to each term. Although the difference may appear trivial, it means that Eq.( 10 ) simultaneously considers 2 d subspaces. Performing the multiplication on the right side of Eq.( 10 ) yields one term for each of the 2 d subspaces. It can be immediately observed that the calculation of the expected proportion of points in all projections is no more computationally expensive than the computation of the expected proportion of points in the one hypercube defined for the full space.

The pseudo-code for the resulting algorithm is provided as Algorithm 1 . Vectors are identified as such ( ) and lists are capitalized. It is important, in this algorithm, that the central data point is not itself included when calculating the  X  obs . Since a point has a distance of 0 to itself in every dimension, its weight is 2 d , which would distort the result substantially, since  X  exp has no corresponding contribution. 3.5 Summary of algorithm The following steps make up the overall process 1. Normalization: data are normalized using the rank transformation of Sect. 3.2 .
Algorithm 1 : Subspace Sums Algorithm 2. Optimization of overrepresentation: for each data point, find the hypercube side length 3. ROC evaluation: sort data points according to their maximal overrepresentation and The output is a ranking of objects from those, for which the logarithmic density of points in their vicinity is highest compared with a random distribution, to those for which it is lowest. It should be stressed that while the evaluation is analogous to classification, the problem as such was not set up as a classification problem. In particular, there is no training based on labeled data. 4 Evaluation 4.1 Implementation The Subspace Sums algorithm was implemented in MATLAB and evaluated on microarray and time series data. For comparison, two other algorithms were implemented: A hypercube-based algorithm (Hypercube) and an algorithm that implements the logic of the LOCI algorithm [ 44 ]. The Hypercube algorithm uses the same normalization as the Subspace Sums algorithm. The optimization of overrepresentation is done directly based on  X  [ h ] ,see Eq.( 2 ) rather than its logarithm as in Eq.( 5 ). Further differences lie in the calculation of the observed proportion of points using Eq.( 6 ) rather than Eq.( 8 ), and its expected value (Eq. 7 ) rather than Eq.( 10 ).

The comparison code based on [ 44 ] uses the logic of the original LOCI algorithm without implementing the more efficient aLOCI process. The number of points in an evaluation volume of a given radius is compared with its average over data points within a volume that has  X  times the radius, where  X  is chosen to be 2, as it was done in [ 44 ] for exact calculations. Since the evaluation is done based on rankings, the multiple-granularity deviation factor (MDEF) is used directly without considering the cutoff based on the standard deviation. Test runs were performed, with the MDEF/cutoff ratio used to determine the ranking, but results were substantially poorer.

Since the LOCI algorithm uses the MAX metric, the evaluation can be viewed as being done over hypercubes, as is the case for the Subspace Sums algorithm. The term  X  X adius X  is used as in [ 44 ] to refer to half of the side length of a hypercube, or half of the range parameter in the Subspace Sums algorithm. For the comparison runs, attributes were normalized to a mean of 0 and a standard deviation of 1. The radius parameter was varied from 0.1 through 4 in steps of 0.1. 4.2 Evaluation on gene expression data The evaluation based on gene expression experiments uses microarray data from cell cycle experiments [ 15 , 51 ] in yeast. These data have been previously used to derive subsets of genes that are to be considered cell-cycle-regulated. The first analysis by Cho et al. [ 15 ] identified relevant genes through expert evaluation and considered 420 genes as cell-cycle-regulated. The genes are discussed as having recognizable features that make them similar to each other and relevant to the cell cycle experiments that were performed on the yeast genome. In a second analysis by Spellman et al. [ 51 ], in which automated criteria were used that specifically targeted the periodicity of the cell cycle regulation, 800 genes were identified as cell-cycle regulated.

Both analyses have limitations. The analysis of Cho et al. was not based on objective criteria and is therefore not generalizable. The analysis by Spellman et al. uses objective criteria, but focuses largely on the periodicity of the cell cycle activity. The process in [ 51 ]is explicitly designed to find genes with a single expression peak. This strong focus on perio-dicity may result in considering genes as cell-cycle regulated that are, in fact, differentially expressed because of a change of the chemical environment. The algorithm presented in this paper explicitly tests for pattern membership beyond the over-all background of the data.
The set of genes that are to be considered as the  X  X rue X  cell-cycle genes is derived as the intersection of both previously identified sets of genes (304 genes), to avoid genes that might have been identified based on artifacts of either of the two approaches. Although it would be preferable to have fully independent information, unfortunately such informa-tion is not available for the full genome. Only about 100 genes have been identified as cell-cycle-regulated using traditional techniques, and this list was expected to be incomplete. Price et al. [ 46 ] estimate the total number of cell-cycle regulated genes to be about 350, which is close to the 304 that are considered as known in this paper, and far lower than the 800 considered as cell-cycle regulated in [ 51 ]. Other studies have been done to identify the set of cell cycle genes [ 50 , 52 ] but were not considered in this paper for lack of a generally accepted standard. The analysis on time series data in Sect. 4.4 does not suffer from this lack of objective information.

The current evaluation uses the cdc28 experiments from [ 15 ] that are also included in the analysis (and accompanying web site) of [ 51 ]. The data set has gene expression information for the given experiment for a total of 6,147 genes. The 304 relevant genes make up approxi-mately 5% of the full data set and the noise fraction, thereby, is approximately 0.95. Other fractions of noise are also considered, each time using the full set of relevant genes. The cdc28 set of experiments has 17 attributes. Some of them, however, have a very substantial number of missing data. Only attributes with fewer than 4% unknown values within an attri-bute are considered, leaving the first 11 dimensions of the data set in consideration. Missing values are filled in with the mean of the corresponding attribute, before rank-order-based normalization.
The present algorithm returns a ranking of genes from highest to lowest logarithmic overrepresentation. Figure 5 shows the ROC curves of the Subspace Sums algorithm, with and without applying a logarithm in the overrepresentation measure, and compares the result with the simpler algorithm, in which only the full space is considered (Hypercube), as well as with the MDEF as in [ 44 ]. It can be seen that for small values of the false positive rate the area under the ROC curve is larger for the Subspace Sums ranking than for the comparison algorithms. That means that among the top-ranked genes there are many more correctly identified cell-cycle genes. Note that this is the case for the Subspace Sums algorithm with the logarithm as well as the non-logarithmic overrepresentation measure. Figure 6 shows the total area under the ROC curve, both for the full data set and for smaller data sets with different noise fractions. It can be seen that the Subspace Sums algorithm with a logarithmic overrepresentation measure outperforms the comparison algorithm for all noise fractions. Using a non-logarithmic overrepresentation measure the results are not as consistently strong. This can be understood by observing that there are overall large fluctuations in that data set: As will discussed in Sect. 4.6 , the neighborhood of a majority of points in the gene expression data is denser than expected. In that setting, a comparison between the average number of neighboring dimensions (logarithmic overrepresentation) is more robust than a comparison of aggregate densities over all subspaces (overrepresentation).

Note that the noise fraction with the poorest AUC value (0.95) is precisely the one, for which the complete ROC curve is presented in Fig. 5 . Hence, the definition of the area under the ROC curve based on all values of the false positive rate does not appropriately reflect the capability of the Subspace Sums algorithm X  X ven without application of the logarithm in the overrepresentation measure: Most users would be interested in a prediction at a small value of the false positive rate, where this algorithm outperforms both the Hypercube algorithm and the MDEF measure. 4.3 Size and dimensionality of the evaluation volumes The Subspace Sums algorithm and both comparison algorithms optimize the range over which neighbors are evaluated. Figure 7 shows the range, plotted against the logarithmic overrepresentation values for the Subspace Sums algorithm, for a data set consisting of all cell cycle genes and an equal number of other genes. Points that correspond to cell cycle (relevant) data are depicted through + symbols. Other points, corresponding to noise, are represented as x symbols. The figure shows not only that relevant genes typically have a higher overrepresentation, which is expected given the results in the previous section. The figure also shows that these predictions are typically gained for evaluation volumes that only make up about 20% of the total volume. That means that a locally high density in some of the dimensions is observed. Only range values up to 1 are considered although larger values up to 2 can occur if the central point is not centered on the data volume. For this example, only 2 of a total of 608 points have range values greater than 1 (1.2 and 1.15, respectively). Both of those are noise points, and their values for the logarithmic overrepresentation are 1.01 and 1.02, respectively.
 Figure 8 shows a corresponding plot for the comparison algorithm using the MDEF.
 Relevant data points here commonly correspond to a larger radius than irrelevant ones. That means that, for cell cycle genes, the density of the local neighborhood and the corresponding averaged value differ most for a radius that comprises much of the data. The ability of LOCI to find groups of genes, micro clusters, that differ from the bulk does not contribute to the performance of the algorithm.

The Hypercube algorithm also typically optimizes the range to values that are large com-pared with their equivalent for the Subspace Sums algorithm, see Fig. 9 , and typically even larger than 1. Note that r &gt; 1 is possible since an off-center evaluation cube must have side length &gt; 1 to cover all points. The need for examining larger hypercubes is related to the curse of dimensionality: In high-dimensional spaces, the difference in distance between the nearest and the farthest neighbors is small [ 1 , 54 ]. The Hypercube algorithm considers all dimensions, i.e., a point is counted as being in the hypercube only when the distance in all dimensions is lower than half the hypercube side length. Evaluation hypercubes that are small compared with the size of the normalization hypercube, which has side length 1 by definition, contain an insufficient number of genes to result in a high overrepresentation.
For the Subspace Sums algorithm this observation does not hold, since data points can contribute to the overall density through only some of their dimensions. As a result, the Subspace Sums algorithm is the only one of the three algorithms that reflects truly local information. Figure 10 provides information on how many dimensions contribute, on average, for this algorithm. Once the range that optimizes the logarithmic overrepresentation has been determined, the number of  X  X ithin range X  dimensions can be extracted for each pair of data points. Figure 10 shows a histogram of that number for all pairs of data points. The squares represent results for the full data set and the asterisks correspond to a subset that has an equal number of cell cycle and other genes. Both noise fractions exhibit a broad distribution of dimensions and a negligible number of data points within the evaluation volume for all 11 dimensions. 4.4 Evaluation on time series data Time series subsequence data are used to independently test the algorithms. This type of data is known to be difficult to cluster [ 35 ], and is hence expected to provide a challenge to the algorithm. Since time series data are widely available, it was possible to compare results for several data sets. For this evaluation, the same nine data sets were used as in [ 18 ]andthe same preprocessing was applied. All but one are from the UCR time series repository [ 34 ]. The Ecg series (MIT-BIH Arrhythmia Database: mitdb100) originates from PhysioBank [ 29 ]. Descriptions of the data sets from [ 34 ] are distributed with the data. The Ecg series was compressed by averaging over 20 consecutive values, the Buoy series by averaging the Buoy sensor series over 4 values. Subsequences were extracted from longer time series through a sliding window approach. The window size was chosen such that the data have 32 dimensions. For this part of the evaluation, the noise component was created artificially, using random walk data. As a pre-processing step differences between adjacent time points were taken and z -normalization was applied to each data point (the mean was subtracted, and the result divided by the standard deviation). Note that for the random walk input this process results in normally distribu-ted data. Rank order normalization was then applied to attributes as in the gene expression evaluation.

Figure 11 shows the average over the AUC values for all data sets for noise fractions between 0 . 5and0 . 95. It can be seen that the result is consistently higher for the Subspace Sums algorithm than for both other algorithms. Note that for this data set, there is little difference between the two overrepresentation measures (with and without logarithm). This can be understood by observing that the added noise was constructed not to have systematic correlations. Overall, the results are better than those for the gene expression data set. This is to be expected, since artificially created noise is likely to be more easily distinguishable than real, experiment-dependent noise.

Figure 12 shows the results for the highest noise fraction (0.95) broken down by data set. It can be seen that for all data sets the AUC values for the Subspace Sums algorithm (with and without logarithm in the overrepresentation measure) are as good or better than those for the MDEF evaluation. For the Buoy and the Ocean data set the Hypercube algorithm outperforms the Subspace Sums algorithm. It was shown in [ 18 , 19 ] that these data sets do not show groups of similar sequences that can be readily extracted using clustering techniques. Results using the Hypercube algorithm are likely due to global fluctuations involving these time series rather than actual groups of data, for which the algorithm was designed. The Subspace Sums algorithm is also based on the occurrence of groups or clusters, albeit with a generalized definition of proximity. The result shows that the use of subspaces has not resolved this particular problem. However, time series subsequences are known to be very difficult to cluster [ 35 ], and the Subspace Sums algorithm was not developed with this specific problem in mind.

Figures 13 and 14 illustrate how the range of evaluation volumes depends on the individual data sets. For these plots the 95% noise setting was chosen rather than the 50% noise setting thatwasusedinFigs. 7 and 8 . It can be seen that for the Subspace Sums algorithm the range distribution for real data (+ symbols) depends noticeably on the actual data set. For example, for the Glassfurnace data set, the optimal range is consistently the smallest grid value, 0.05. For the Speech data set, optimal values range from 0.1 to 0.4. For any one data set, the range is typically much smaller, and the evaluation more localized, than for noise. The only exception is the Buoy data set, for which it has been noted previously that the Subspace Sums algorithm does not produce satisfactory results.

Range values for noise are spread over the full set of grid values that were used in the optimization. This indicates that the range optimization is capable of extracting features in a consistent fashion for the time series data, but not for the noise. Such a distinction cannot be seen for the LOCI comparison algorithm, as illustrated in Fig. 14 . For this algorithm, the optimal range is similar for time series and noise data. 4.5 Adding noise dimensions In the previous section, noise was limited to random data points as well as noise inherent to the time series. It is also possible to add dimensions that are random to test the robustness of the algorithm. In this section random dimensions are added in batches of 32, i.e. each batch has the same number of dimensions as the original data set. All dimensions are then normalized together. The Ecg data set is used for this purpose, since it has a high AUC value throughout the study. A noise fraction of 0.5 is used throughout this section.

In Fig. 15 it can be seen that the AUC values for the Hypercube algorithm decrease such that for 5 or more batches the AUC value is close to 0.5. The effectiveness of the Subspace Subs algorithm, in contrast, decreases much less strongly, and the algorithm is still effective for the largest noise ratio, in which six batches of noise dimensions have been added, with an AUC value of 0.87. Such a behavior is expected, since the Subspace Sums algorithm considers the subspaces of the non-random data separately, while the Hypercube algorithm only attempts to find regions that are dense in all dimensions.

The MDEF function is not effective on the time series data, even without noise in the dimensions, since it selects for those data points that are random, resulting in an AUC value of less than 0.5. As noise dimensions are added, this unfavorable selection decreases, and for 2 or more batches of noise dimensions the result indicates that no distinction can be made between time series and random data. 4.6 Determining a cutoff So far, it has been assumed that a ranking according to relevance of data points is a sufficient output of the algorithm. For some applications it would be desirable to know where to place the cutoff. A natural process for determining a cutoff is to apply the algorithm to random data of the same size. Commonly, in machine learning and data mining, permutations of the original data are used. However, since the algorithm only uses the rank order of attribute values, generating the random data separately is equivalent.

The largest occurring value for the logarithmic overrepresentation measure on random data is used as threshold. Figure 16 shows the number of data points that exceed the threshold over all data sets and noise fractions of Sect. 4.4 . It can be seen that for several data set X  X oise fraction combinations no data points have a logarithmic overrepresentation larger than the threshold value. This can be understood from results in [ 18 ] that show that subsequences of some of the data sets are not recognized in other settings either. However, a substantial number of settings do lead to around 200 recognized points, which in all cases would have been the correct answer.

For the gene expression data the cutoff values proved to be less useful. Many noise data points in the real data had higher values of c l than the c thresh l that was determined from random data (varying from 1 . 12 for the smallest noise fraction to 1 . 03 for the full data set). For the different noise ratios, a fraction of between 66 and 84% of data points were considered to be significant beyond noise. This was, in all cases, substantially more than the actual fraction of relevant genes (1-noise fraction). That means that even noise data points are considered to be in a neighborhood that differs from what would be expected from completely random data. Hence, for this data, the cutoff has to be estimated from other considerations, such as the number of relevant genes that is expected from biological considerations. 4.7 Performance The performance of the Subspace Sums algorithms is governed by the need to examine each data point, and for each data point to examine the distribution of neighbors. This results in a quadratic dependence on the number of data points. For each data point and neighbor the number of matching dimensions must be counted, resulting in a linear dependence on the number of dimensions. The algorithm is also linear in the number of range values that are considered. In the examples, 20 grid points were used.

A faster algorithm can be designed if only a single range is considered, especially if that size is small compared with one, i.e., the size of the overall normalization hypercube. The most efficient solution is then to select only those data points that are within range for each one of the dimensions. The relevant points can be directly retrieved from the sorted lists of attribute values that are generated as part of the normalization process. Results for each dimension are lists of data point identifiers, which are concatenated and sorted. The length of the list is l list = dr N ,where r is the range parameter, d the number of dimensions, and N the total number of points. Sorting is the performance limiting step in this algorithm and can be expected to have performance l list log ( l list ) . Figure 17 shows that the runtime for the Single Range algorithm with range 0.1 is about two orders of magnitude smaller than for the full grid. However, if the range is taken to be 1 the advantage is lost, and a single run of the Single Range version takes about 20% of the time of the Full Grid version. Hence, it would not be efficient to design a Full Grid algorithm as composite of Single Range runs. Note that the slope of the Single Range algorithm for range 0.1 is also smaller than for the Full Grid, although the theoretical scaling does not suggest that. This advantage also is lost at range 1. Differences in scaling may be due to time spent on garbage collection.

Therangevalueof0 . 1 was selected because Figs. 7 and 13 show that for many data sets the optimal side length is of that order. A histogram of the differences in AUC values between results for range 0 . 1 and for the full grid can be seen in Fig. 18 . All AUC values reported in this paper are included in this figure. It can be seen that the AUC values are generally smaller (positive difference) when only the range value 0 . 1 is considered, but that in some cases the effectiveness is even increased (negative difference). 5 Conclusions An algorithm for finding non-random data points in the presence of up to 95% noise has been presented. Rank-order-based normalization is used to create a flat distribution for each individual attribute, regardless of the distribution of values. Non-random data are identified through density maxima that result from the combined occurrence of similar attribute values in multiple data points. Similarity is evaluated over all axis parallel subspaces. The side length of the evaluation hypercube is optimized based on the overrepresentation of non-random data points. Effectiveness of the algorithm is demonstrated on a real gene expression data set, for which noise has a clearly non-Gaussian distribution. Using nine time series data sets that have artificial noise added, the effectiveness has been confirmed in a different domain and for a variety of data sets. Appendix of the points in the data set using Eq.( 6 ). Equation( 6 ) is generalized to include only those dimensions that are elements of D s The summation over elements of the power set D s  X  P ( D ) can be rewritten as a d summations [ i ] , each covering the two cases that D d as an element (  X  d i  X  D s or  X  d i  X  D s ). The product over dimensions d i can be extended to include a factor of 1 in the case  X  d i  X  D s Each of the factors in the product over  X  -functions depends on only one of the dimensions, and hence one of the summations. The expression can, therefore, be rewritten using the distributive law for each summation, and the summation can be performed Since the Heaviside step function can only have values 0 or 1 and for a binary variable b  X  X  0 , 1 } the following is satisfied, 1 + b  X  2 b (as can be shown by inserting the two possible values for b in into the equation),  X  [ s ] obs ( x ( k ) , a ) can be written as The final result is a sum over weighted data points as it was stated in Eq.( 8 ) to the total weight of the data point at x ( k ) .
 References Author Biography
