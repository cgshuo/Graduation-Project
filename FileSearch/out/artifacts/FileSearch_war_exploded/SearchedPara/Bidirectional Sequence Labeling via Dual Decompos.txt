 Many natural language processing tasks, e.g., POS tagging, text chunking and Chinese word segmentation , can be f ormulated as a se quence labeling problem. In these tasks, each token in a sequence is assigned a label, and the label assig nment of a given token is influ enced by the label assignments of the previ ous tokens. Most sequence labeling models are uni directi onal where the inference proce dure is performed in one direction only (left to right, or right to left, but not both). As a re sult, only the i nfluence of one direction is ex plicitly cons idered. For many sequence label ing tasks, however, both the consider the POS tag ging proced ure for the sentence  X  X ould ser vice be voluntary or compulsory? X . The word  X  X ervice X  can either be labeled as a verb or a noun. In a left -to -ri ght model, the POS tag  X  X D X  of the previous word  X  X ould X  strongly indicates that  X  X ervice X  should be tagged as verb. However, this is the incorrect answer in the case. In a right -to -left model, the POS tag  X  X B X  of the following word  X  X e X  indicates  X  X ervice  X  sho uld be a noun, which is the cor rect answer. This means that a model that ac counts for the influence of both the left and right contexts is better.

In recent years, a number of bidirectional sequence labeling models were proposed to exploit the influen ce of both directions. Liu and Zong ( 2003) and Shen et al. (2003) im proved the t agging accuracy by pairwise com bining or voting between the left -to -right and right -to -left taggers. Toutanova et al. (2003) proposed a POS tagging model based on bidi rectional dependency networks that make the right context available for a left -to -right model. Tsuruoka and Tsujii (2005) considered all possible decompositions of bidirectional con texts, and chos e one that has the highest prob ability among different taggers. Shen et al. (2007) extended Tsuruoka and Tsujii (2005) and integrated the in-ference order selection and classifier training into a single learning framework.
In this paper, we propose a novel approach for bidirect ional sequence labeling. We com bine the optimi zation of two unidirectional models from opposite directions to pre-dict agreed labels through the dual decomposition method. We estimated our approach on three sequence labeling tasks for two languages: Chinese wor d segmentation, Eng-lish POS tag ging and te xt chunking. Experimental results show that our approach is effective when the two unidirectional models individually make highly different pre-dictions. Let us denote the input sequence of tokens as  X  =  X  1  X  2 ...  X   X  , and the label sequence  X  . For example, in part -of -speech tagging, the input sequence would be the word to-kens in a sentence and the output would be POS tag s f or the word tokens .

The task of sequence labeling is to find the best label sequence  X   X  for an input se-quence x :
Usually, the global probability  X  (  X  |  X  ) can be decompos ed into products of a se-quence of local p redict ions . For example, in the left -to -right model, the probability is decomposed into: model the prediction probability with the Maximum Entropy (ME) model: tures. When given a training set of labeled sequences, we can estimate the model pa-rameter  X  using the usual way for ME models, i.e., Generalized Iterative Scaling (GIS) or gradient descent methods. 
The probability of t he current label prediction in E.q (3) is conditioned on label pre-dictions for previous tokens. I f we make a first -order M arkov assumption , the Viterbi algorithm would be an efficient decoding method. However, Jiang et al., (2008) showed that non -local fea tures are much helpful for POS tagging. Therefore, we design a uni-directional decoding algorithm that uses more than one prediction before the current position.
 gorithm. We use t wo max -heaps to hold the partial label sequences, where preHeap maintains a list of N best partial candidates ending at position i -1 and cur Heap main-tains a list of N best partial candidates ending at position i . The algorithm initializes the preHeap with an empty sequence (line 1). It then traverses the input sequence from left to right, and assigns a label to each token (line 2 to line 13). When processing the i -th token  X   X  , the algorithm extracts the top partial candidate item from preHeap (line 6), with item (line 9), we build a new partial candidate item  X  by combining  X   X  with item (line 11), calculate the probability of item  X  using E.q. (2) (line 10) an d add it to curHeap (line 12). When all the input tokens are processed, the best partial candidate in curHeap is returned as the final result (line 14).

Although the model and the decoding algorithm are designed for the left -to -right direction, they can be trivially adapted to the right -to -left direction. To train a right -to -left model, we just reverse all the label sequences in the training set before training. For decoding, we reverse the input sequence first, then decode the reversed sequence with the right -to -left model and reverse the label sequence back. In this section, we describe how to improve sequence labeling by joint ly optimizing the two unidirectional models. We train a left -to -right model and a right -to -left model and then jointly label an input sequence with the two models.

For purposes of clarity, we define some notations first. The label sequence from the qual.

We expect the two unidirectional models to predict equal results and formulate it as a constraint optimization problem: model. 
The dual decomposition (a special case of Lagrangian relaxation ) method introduced in Rush et al. ( 2010 ) is suitable for this problem . Following their method, we solve th e primal constraint optimization problem by optimizing the dual problem. First, w e in- X  (  X  ,  X  ) . Then, the Lagrangian is formulated as: By grouping the terms that depend on  X  and  X  , we rewrite the Lagrangian as
Then, t he dual objective is
T he dual problem is to find the min W e use the s ubgradient method ( Boyd et al., 2003) to minimize the dual . Following Rush et al. ( 2010 ) , we define the subgradient o f  X  (  X  ) as :
Then, adjust  X  (  X  ,  X  ) as follows: where  X  &gt;0 is a step size.

Algorithm 2 present s the s ubgradient method to solve the dual problem. The algo-rithm initializes the Lagrange multiplier values with 0 (line 1) and then iterates many equal (line 5), then the algorithm returns the solution (line 6). Otherwise, the algorithm (line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original unidirectional decoding algorithms, because the Lagrange multiplier can be regarded as adjustment s for the prediction score and Vygen, 2008) , the dual solution is the label sequence we want to get. To evaluate the effectiveness of our method , we conducted experiments on three se-quence labeling tasks: Chinese word segmentation, English POS tagging and text chunking. 4.1 Tasks and Data Sets The task of Chinese word segmentation is segmenting a sequence of Chinese characters into words. The character -based model (Xue, 2003) treats segmentation as a sequence labeling task, where each Chinese character is labeled with a tag. We used the tag set used in Wang et al. (2011). We split the Chinese Tree bank Version 5.0 (CTB5) with the standard data split : 1 -270, 400 -1151 as the training set, 301 -325 as the development set and 271 -300 as the test set.
 for POS tagging: sections 0 -18 as th e training set, sections 19 -21 as the development set and sections 22 -24 as the test set.
 as a tagging task by converting chunks into tags on tokens. We choose the IOB sc heme: the first token in chunk X , or the label O if it is outside of any c hunk s . W e used the data set from the CoNLL -2000 shared task.

The feature templates f or each task are adopted from previous work. For Chinese word segmentation, we use the feature templates provided in Wang et al. (2011). For POS tagging and chunking, we used the feature templates provided in Tsuruoka and Tsujii (2005), excluding those tem ples containing future predictions. 4.2 Results system were two unidirectional systems, which trained models and decoded sequences from opposite directions. The  X  bidirectional  X  system used these two unidirectional models jointly to decode sequences with Algorithm 2. We trained models for three tasks with the Maximum Entropy model implemented in the OpenNLP toolkit. 
We tuned parameters on the development set and fi nally set the beam size (in Algo-rithm 1) to N =20, the maximum iteration to K =30 and the step size to  X  =0.5 (in Algo-rithm 2). The experimental results on the test set are presented in Table 1 and they show that the accuracy of the POS tagging task and the F1 score of the chunking task were improved when using the bidirectional decoding algorithm. However, the Chinese word segmentation task showed no improvement. over uni directional models when assigning POS tags to the sentence  X  X ould service be voluntary or compulsory? X . In the left -to -right model, the word token  X  X ervice X  is la-beled with an erroneous tag  X  X B X , because the preceding word  X  X ould X  is a modal verb that is often followed by a verb. In the right -to -left model,  X  X ervice X  is correctly labeled, because the following word  X  X e X  is a verb that is often preceded by nouns. However, the right -to -left model assigns the wrong tag  X  X N X  to the word  X  X ompulsory X , presumabl y because it is the first token in the sequence and  X  X N X  is a more likely tag for the first token. The left -to -right model, on the other hand, assigns the correct label  X  X J X . The bidirectional algorithm combines the strengths of both models and assigns the correct tags to all words. 4.3 Discussion To understand the scenario s where the bidirectional decoding algorithm is effective , we analyzed the three tasks in detail. Table 2 presents the total number of tokens in the test set and the number of tokens to which the left -to -right and right -to -left models assigned different labels. We found the number of tokens receiving different labels was low for the Chinese word segmentation task, but high for the English POS tagging and chunk-ing tasks. Combined with the results in Table 1, we can conclude that our algorithm is effective when the two unidirectional models make very different predictions. When the two unidirectional models make the same predictions, even if the predictions are wrong, the bidirectional algorithm can do nothing to correct them.

We also estimated the convergence of the bidirectional decoding algorithm by count-ing the number of iterations when the two unidirectional models make different predic-tions . Fig . 2 shows the percentage of sequences where exact solutions are returned ver-s us the number of iterations. We find our algorithm produces exact solutions to over 80% of the sequences within 10 iterations. In this paper, we proposed a bidirectional decoding algorithm for sequence labeling tasks. We use two unidirectional models of opposite directions to jointly label the input sequences via the dual decomposition algorithm. Experiments on three sequence labe l-ing tasks show that our approach improves the performance on sequence labeling tasks when the two unidirectional models makes very different predictions.
 The research work has been funded by the Hi -Tech Research and Development Pro-gram ( X 863 X  Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA01110 2 . This wor k is also supported in part by the DAPRA via contract HR0011 -11 -C -0145 entitled "Linguistic Resources for Multilingual Processing".
