 The recent commercialization of technologies for the real-time identification, location, and tracking of people and objects has opened the door to various new applications in domains such as safety, logistics and retail. Among these applications, the real-time detection of malicious or abnormal behaviors is of critical importance to the safety of the population.
 divided in two categories: the ones based on probabilistic generative models, and those using trajectory patterns. Approaches in the first category use a generative determine the likelihood of a sequence of observed events/actions, and consider as abnormal behaviors the ones with a low probability. On the other hand, the second category of methods represent behaviors as trajectories through space, and considers as anomalies the trajectories that are significantly different from commonly observed ones. Trajectories can be encoded in various ways, such as sequences of points [ 16 ] or cubic splines [ 18 ]. Moreover, several approaches have been proposed to model the class of normal trajectories, for instance one-class SVM (OCSVM) [ 16 ], Gaussian Mixture Model [ 18 ], sparse coding [ 14 ]and frequent sub-sequence mining [ 10 ].
 While these solutions are adequate in small and controlled environments, in which well defined activities occur, they usually perform poorly in large and dyna-mic environments, where the same sequence of events is almost never observed twice. In such complex environments, the context of events (such as location, dura-tion, person ID, type of job, etc.) is often more important than their sequence. Thus, a person might take a slightly different route to go to the office, so analyz-ing the exact trajectory would likely result in many false positives. Moreover, even though the usual route is taken, this person X  X  behavior can be abnormal if he/she goes to the office at an odd time (e.g., after 9 pm) or on a odd day (e.g., Sunday). This behavior might however be considered normal for other employees, such as security agents working on the evening or weekend shifts.
 Although several approaches have been proposed to include contextual infor-to specific contextual dimensions, such as the duration of events, and are unsuit-able for complex environments. On the other hand, tensors have been recognized as a powerful and efficient method to model complex contextual information, and have been used in diverse applications like item recommendation [ 17 ]and explored as a novel way to detect anomalies, for instance, by tracking the recon-decomposing a tensor as the sum of a low-rank component and a sparse residual representing the anomaly [ 11 ].
 In this paper, we propose a novel method based on log-linear tensor factor-ization to detect contextual anomalies in large and complex environments. The advantages of this method are as follows: 1. Unlike existing tensor factorization approaches, which focus on detecting the joint distribution of contextual dimensions. This allows it to evaluate the true probability of incoming events and mark low probability ones as abnor-mal. Our method also has the ability to detect specific types of anomalies efficiently, by using the probability of a given dimension conditioned over the other ones. 2. While most tensor factorization approaches are based on a linear model, our method uses a log-linear model, which can learn more complex relations in the data. Moreover, the proposed model implicitly enforces non-negativity in the tensor, a useful property when dealing with count data. In comparison, state of the art factorization techniques like Non-negative Tensor Factoriza-tion (NTF) [ 21 ] and Alternating Poisson Regression (APR) [ 4 ] impose non-negativity by constraining the factors, making the inference process more complex. 3. The proposed method uses an efficient inference strategy, based on Nesterov X  X  accelerated gradient ascent, which has a complexity comparable to the state of the art Alternating Least Square (ALS) method [ 17 ], but offers more flexibility (e.g., ALS is limited to linear models and does not impose non-negativity).
 proposed model, its inference strategy, and the method used to detect anoma-lies. Section 3 then evaluates our model on the tasks of approximating tensors using a small number of parameters, predicting the occurrence of future events and detecting abnormal events from their context. Finally, we summarize our contributions and results in Section 4 . 2.1 Model Description We model the multi-dimensional context of an event using a set of discrete random variables { X 1 ,...,X D } , representing the identifier (ID) of contextual elements like person, zone, time of day, etc. Each variable { 1 ,...,N j } , and we denote as X j = i j the observation of element dimension j . To simplify the notation, we use x i j as shorthand for this observa-tion. For example, if the first dimension represents people, then observation of person i 1 , from a group of N 1 people, in the event. tors Z = { Z 1 ,...,Z D } , providing high-level information about the contextual elements. We define as z i j  X  R K the latent factor vector corresponding to the i -th element of dimension j , where K is a user-supplied parameter. Using the previous example, z i 1 would be the latent factor vector of person log-linear model: where z i 1 ,..., z i D is the inner product between D vectors of size ( x i 1 , ..., x i D ) is available. This set can also be represented as a tensor, in which element ( i 1 ,...,i D ) contains the number of events of context ( i 1 ,...,i D ). We call this structure the event tensor . corresponds to where M i 1 ,...,i D is the number of events of X with context ( number of events is small compared to the size of the multi-dimensional event In other words, the event tensor should be very sparse. To regularize the solution, we suppose the factor vectors as independent and following a zero-mean normal distribution with uniform variance: The latent factors Z are found using the maximum a posteriori (MAP) esti-mate, which corresponds to maximizing the following cost function: f (
Z ) = log p ( X|Z ) + log p ( Z ) Note that this is equivalent to minimizing the KL divergence between the model and empirical distribution of events, as done in [ 4 ].
 Since the cost function of Eq. ( 5 ) is both non-linear and non-concave, obtain-ing globally optimum parameters is an intractable problem. Therefore, we must optimize it using an iterative approach like the gradient ascent method, which has a linear convergence rate. However, because this function is concave with respect to each factor vector, we can instead use Nesterov X  X  accelerated gradient method [ 15 ] for which the convergence rate is quadratic.
 Unlike gradient ascent, Nesterov X  X  method performs two different steps at each iteration. The first step is a simple gradient ascent step of size current solution z ( t ) i j to a intermediate solution y ( Let context ( i 1 ,...,i D ) and the expected one according to parameters The gradient with respect to z i j is given by where  X  z i j is defined as the Hadamard (i.e., element-wise) product of all factor vectors expect the one of dimension j : Algorithm 1. Parameter inference using Nesterov X  X  method The second step then finds the next solution z ( t +1) i j the two last intermediate solutions: where  X  t are constants controlling the search momentum (e.g., see Algorithm 1 ). 2.2 Algorithm Summary and Complexity The complete inference process is summarized in Algorithm 1 . For a greater effi-ciency, we group latent factor vectors of each dimension j and use standard matrix operations. We start by initializing the factor matrices randomly following the prior distribution of parameter  X  (lines 1-3). Then, at each iteration of Nesterov X  X  method, the tensor  X  X of expected counts is reconstructed using the current intermediate factors Y j (line 6). Operator corresponds to the Khatri-Rao product and X ( j ) denotes the unfolding of tensor sion j (see [ 12 ] for more information). For each dimension j computed using the residual between the observed and expected counts (line 10), and used to update factor matrix Z j (line 11). If the step size large, the solution may diverge. A strategy is thus added to detect such problem and adjust  X  automatically (lines 12-13), thereby eliminating the need to tune  X  manually. This strategy is known as backtracking line search . The momentum constant and intermediate factors are finally updated as per Nesterov X  X  method (lines 14-16). The process is repeated until converge is attained, or a maximum number of iterations is exhausted.
 The computational complexity of this algorithm is as follows. For each iter-ation, reconstructing the expected tensor  X  X take O ( K  X  Likewise, updating the latent factors for each dimension can be done in
N j ). Therefore, the total complexity is O ( T T is the maximum number of iterations. 2.3 Abnormal Event Detection We use the latent factors learned during training to evaluate the joint proba-bility of new events in real-time, and mark as abnormal those that have a low probability. Let  X  be a given probability threshold, an event ( be marked as abnormal if p ( x i 1 , ..., x i D |Z ) &lt; X  inator of Eq. ( 1 ), evaluating the joint probability of an incoming event requires only O ( D  X  K ) operations.
 To detect specific types of anomalies, we can instead evaluate the probability of a single dimensional value, conditioned on all other dimensional values. For example, we could evaluate the probability that the event occurs in a certain zone, given the person, day, and time of day corresponding to that event. If the conditional probability of the observed zone is much lower than that of other zones, the event would then be marked as abnormal. Suppose, without loss of generality that the query dimension is j = 1. The probability of on all other dimensions, is given by To evaluate the computational complexity of this query, we note that the inner product can be decomposed as z i 1 , ..., z i D = z i 1 , compute  X  z i 1 , each inner product computation has a time complexity in where K is the size of the latent subspace. Since we have to compute inner products, the total cost of evaluating the query is in We evaluated the performance of our method by conducting three sets of experi-ments, related to the low-rank approximation of tensors, the prediction of future events, and the detection of contextual anomalies.
 3.1 Low-Rank Approximation The goal of this first experiment is to measure the ability of our method to fit the event tensor using a small number of parameters.
 Experimental Design. We generated two synthetic datasets, each containing 11 sparse 3D tensors of size 100  X  100  X  100 drawn from two types of distributions: log-linear and Poisson. Each tensor was generated using three sets of 100 latent factor vectors of size K = 20, drawn from a Gaussian prior in the case of log-linear tensors and a Gamma prior for Poisson tensors. The parameters of these priors were selected to have a sparsity level between 70% and 90%, and the resulting tensors normalized to have a total number of events equal to and the remaining 10 to evaluate its average performance in terms of Mean Abso-lute Error (MAE). Factor sizes of K =5 , 10 , 15 , 20 were tested. We compared our Log-Linear Tensor Factorization (LLTF) method to two state-of-the-art factor-ization approaches: Alternating Poisson Regression (APR) [ 4 ] and Non-Negative Tensor Factorization (NTF) [ 21 ]. The Matlab Tensor Toolbox v2.5 [ 2 ] implemen-tation of these methods was used.
 Results and Discussion. Figure 1 (left) gives the average MAE obtained by the three tested approaches on the Poisson (blue curves) and log-linear (green curves) tensors. As expected, the reconstruction error decreases with higher values of Moreover, our LLTF method outperforms APR and NTF for log-linear tensors, especially for K = 20 where LLTF obtains an average MAE 2 than APR and 2 . 41 smaller than NTF. For Poisson data, LLTF performs as well as APR, even though this data is tailored to APR X  X  Poisson model not LLTF X  X  log-linear one. Since it is not designed for sparse count data, NTF obtains a lower performance than LLTF and APR for both log-linear and Poisson data. on a sample tensor, compared to simple gradient ascent (GA). We see that the convergence in terms of Negative Log-Likelihood (NLL) (blue curves) is attained within 60 iterations, with an average time of 0 . 3 seconds per iteration, whereas GA has not converged after 100 iterations. In contrast to our method, APR takes on average 1 . 3 seconds per iteration using the same hardware, and requires over 1000 iterations to converge. 3.2 Future Event Prediction The second experiment evaluates how well our method can predict the number of future events occurring in a given context. As the event tensor is sparse, this experiment measures the ability of the model to predict events that were not observed in the training data.
 Experimental Design. Two real-life datasets were used for this experiment.  X  Reality Mining [ 6 ]: Contains the tracking information of 106 students and faculty members from the MIT Media Laboratory and Sloan Business school, collected through their cell-phones in 2004-2005. From the 106 participants, we picked 87 students as the remaining ones had either less than 7 days of data or no data at all. For the locations, we used the 1027 unique cell-tower IDs, corresponding to the attribute areaID.cellID in the data. The timestamps of the tracking events were encoded using 24 discrete values, one for each hour of the day. Combining these three dimensions, we obtained a 87 tensor, each cell containing the number of times a person was recorded as being near a given cell tower, at a given time of the day.  X  Geo-Life Taxi Trajectories [ 22 ]: Contains the GPS trajectories of 10,357 taxis during the period of Feb. 2 to Feb. 8, 2008 within the city of Beijing.
From the 10,357 taxis in the data, we selected 259 taxis as the remaining ones had either less than 5000 records of temporal locations or no records at all. Since most records are located near the center of Beijing, we converted the
Cartesian coordinates (longitude and latitude) to log polar ones (log radius, angle  X  ) using the city X  X  center as origin. We divided  X  each, and the log radius into 10 bins, giving a total of 120 zones. Similar to the Reality Mining dataset, we encoded the timestamps using 24 discrete values, one for each hour of the day. Combining these three dimensions, we obtained a 259  X  120  X  24 tensor, each cell containing the number of times a taxi was recorded as being in a specific zone, at a given hour of the day. We split the datasets temporally, putting the first 60% of each person or taxi X  X  events in the training set, the following 20% in the validation set, which was used to tune the regularization parameter  X  , and the remaining 20% in the test set. We predicted the number of events in the test set for each context (i.e., tensor cell) by multiplying the probability obtained for this context during training with the total number of events in the test set. We evaluated the prediction accuracy of our method, in terms of MAE, RMSE (Root Mean Squared Error) and NLL, and compared it once again to with APR and NTF. Note that the Poisson distribution used in APR is specifically tailored to model count data. A latent factor size of K = 10 was used for all three methods.
 Results and Discussion. The prediction accuracy of the three tested methods on the Reality Mining and Taxi datasets is detailed in Table 1 . We see that LLTF outperforms APR and NTF, on both datasets and all three performance metrics. Thus, LLTF obtains a MAE 2 . 32 times lower than APR in the Reality Mining dataset, and 2 . 10 times lower in the Taxi dataset. We also note that APR obtained infinite NLL values. This is because it gave a zero probability to the events in the test set that were not observed in the training set. By regularizing the factors, our model can better predict such unobserved events. 3.3 Abnormal Event Detection In this last experiment, we assess the usefulness of our method to detect abnor-mal events from their context.
 Experimental Design. Once again, the Reality Mining and GeoLife Taxi Tra-jectory datasets were considered for this experiment. Three types of synthetic anomalies were generated.  X  Swap People: This type of anomalies simulates a person (or taxi) behaving  X  Swap Times: This type of anomalies corresponds to a person going to  X  Swap Zones: This last type anomaly corresponds to a person being active data as is to learn the distribution of normal events. For both the validation and testing sets, we generated 10 different sets of random anomalies, using the follow-ing procedure. For Swap People anomalies, we swapped 5 pairs of people/taxis, while for Swap Times and Swap Zones anomalies, we randomly picked 3 persons and, for each of them, swapped 3 pairs of timeID or zoneID . The parameters of the tested methods were tuned using the average Area Under the ROC Curve (AUC) obtained over the 10 validation anomaly sets. Note that this tuning step was necessary to have a fair comparison between the tested methods, but such validation anomalies may not be available in real-life applications. Finally, the performance of the tuned methods was evaluated as the mean AUC obtained over the 10 test anomaly sets. We tested four variations of our proposed approach. In the first one, called LLTF-Joint, the ROC curves are generated by evaluating the joint probability of test examples, as defined in Eq. ( 1 ), and then computing the precision/recall for increasing probability thresholds. The other three methods, denoted by LLTF-D , where D = { Person , Time , Zone } , instead use the conditional probability of a single dimension D given the other two dimensions, as described in Eq. ( 11 ). We compared the performance of these methods with two well-known unsu-pervised anomaly detection approaches: One-Class Support Vector Machines (OCSVM) [ 16 ] and Kernel Density Estimation (KDE) [ 13 ]. For both of these methods, the discrete dimensional values (e.g., personID )werefirstconverted to binary features using an indicator function, giving a total of 1138 binary fea-tures for Reality Mining and 403 binary features for Taxi. PCA was then applied to these binary features, using a percentage of variance value of 95%, and the resulting components were normalized to have uniform variance. For OCSVM, two parameters required tuning:  X  , which controls the fraction of training exam-ples allowed outside the learned region, and the RBF kernel parameter signed distance to the hyperplane was used to evaluate the normality of test examples, while computing the ROC curves. For KDE, we evaluated the proba-bility of a test example x (projected in PCA space) as where x n are the training examples (in PCA space) and h is the kernel bandwidth parameter, tuned on the validation data.
 Results and Discussion. The mean ROC curves (and corresponding AUC values), computed over the 10 test sets of each anomaly type, are shown in Figure 2 . Except for the Swap Time anomalies, our LLTF-Joint method obtained a higher mean AUC than OCSVM and KDE. In particular, the AUC of LLTF-Joint is 9% to 35% higher than OCSVM, and 10% to 37% higher than KDE, for Swap People anomalies. Furthermore, the conditional probability model can improve the detection of specific types of anomalies. For instance, LLTF-Person obtained mean AUC values of 0 . 97 and 0 . 93 on the Swap People anomalies, compared to 0 . 95 and 0 . 92 for LLTF-Joint. Similarly, LLTF-Time obtained a mean AUC of 0 . 85 on the Swap Time anomalies of the Reality Mining dataset, whereas this value was only 0 . 63 for LLTF-Joint.
 The proposed method is also faster and more robust than OCSVM and KDE. Thus, training LLTF for the Taxi dataset took less than 10 minutes on a Quad-Core AMD 2.3 GHz processor with 8 GB of RAM, whereas training OCSVM on this dataset required over 7 hours using the same hardware (no training is necessary for KDE). Likewise, predicting anomalies in the test set took on average 0.03 ms for LLTF, compared to 61 ms for OCSVM and 7 ms for KDE. Moreover, while the best parameters for each type of anomaly (as selected in validation) varied greatly in OCSVM and KDE, our method was more robust to the choice of parameters: K = 25,  X  =8wasusedfor all anomalies in the RM dataset, and K = 15,  X  =6for all anomalies in Taxi.
 We presented a new approach, based on log-linear tensor factorization, for the detection of contextual anomalies. A parametric model was proposed to estimate the joint probability of dimensional values, in which the parameters are the fac-tors of an event count tensor. To learn the factors, an efficient technique based on Nesterov X  X  accelerated gradient ascent was presented. The proposed approach was evaluated on three problems: the low-rank approximation of synthetic ten-representing abnormal behaviors. Results show our method to outperform state of the art approaches for these problems, while being faster and more robust than these approaches. As future work, we will investigate the use of additional dimensions in the tensor, for instance to model the duration and sequence of events, and extend the method to perform online learning.

