 Proper names and other technical terms are fre-quently encountered in natural language text. Both machine translation (Knight and Graehl, 1997) and cross-language information retrieval (Jeong et al., 1999; Virga and Khudanpur, 2003; Abdul-Jaleel and Larkey, 2003) can benefit by explicitly translating such words from one language into another. This approach is decidedly better than treating them uni-formly as out-of-vocabulary tokens. The goal of ma-chine transliteration is to translate words between alphabets of different languages such that they are phonetically equivalent.

Given a source language sequence f = f f 2 . . . f m from an alphabet F , we want to produce a target language sequence e = e phabet E such that it maximizes some score function s ( e , f ) , Virga and Khudanpur (2003) model this scoring function using a separate translation and language strast, Al-Onaizan and Knight (2002) directly model the translation probability P r ( e | f ) using a log-linear combination of several individually trained phrase and character-based models. Others have treated transliteration as a phrase-based transduction (Sherif and Kondrak, 2007). All these approaches are adap-tations of statistical models for machine transla-tion (Brown et al., 1994). In general, the parame-ters of the scoring function in such approaches are trained generatively and do not utilize complex fea-tures of the input sequence pairs.

Recently, there has been interest in applying discriminatively-trained sequence alignment mod-els to many real-world problems. McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires finding  X  X lose X  non-match pairs which can be a challenge. A similar conditional latent-variable model has been applied to the task of lemmatization and genera-tion of morphological forms (Dreyer et al., 2008). Zelenko and Aone (2006) model transliteration as a structured prediction problem where the letter e is predicted using local and global features derived from e (2007) address cognate identification by training a SVM classification model using phrase-based fea-tures obtained from a Levenshtein alignment. Both these models do not learn alignments that is needed to obtain high performance on transliteration tasks. Freitag and Khadivi (2007) describe a discrimina-tively trained sequence alignment model based on averaged perceptron, which is closely related to the method proposed in this paper.

Our approach improves over previous directions in two ways. First, our system produces better k -best transliterations than related approaches by training on multiple hypotheses ranked according to a user-specified loss function (Levenshtein edit distance). Hence, our method achieves a 19.2% error reduction in 5-best performance over a baseline only trained with 1-best transliterations. This is especially help-ful when machine transliteration is part of a larger machine translation or information retrieval pipeline since additional sentence context can be used to choose the best among top-K transliterations. Sec-ond, our training procedure accounts for noise and non-separability in the data. Therefore, our translit-eration system would work well in cases where per-son names were misspelled or in cases in which a single name had many reasonable translations in the foreign language.

The training algorithm we propose in this pa-per is based on the K -best MIRA algorithm which has been used earlier in structured prediction prob-lems (McDonald et al., 2005a; McDonald et al., 2005b). Our results demonstrate a significant im-provement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007). Let e = e quences from the target alphabet E and source al-phabet F respectively. Let a = a quence of alignment operations needed to convert f into e . Each alignment operation either appends a letter to the end of the source sequence, the target sequence or both sequences. Hence, it is a member of the cross-product a where  X  is the null character symbol. Let a k a a 2 . . . a k denote the sequence of first k alignment f of length k .

We define the scoring function between a word and its transliteration to be the a maximum over all possible alignment sequences a , where the score of a specific alignment a between two words is given by a linear relation, for a parameter vector w and a feature vec-tor  X ( a , e , f ) . Furthermore, let  X ( a , e , f ) = tors associated with individual alignment operations. Here i, j are positions in sequences e , f after per-the function s ( e , f ) can be efficiently computed us-ing a dynamic programming algorithm, max Given a source sequence f computing the best scor-ing target sequence e = arg max all possible sequences E  X  requires a beam search procedure (Freitag and Khadivi, 2007). This pro-cedure can also be used to produce K -best target sequences { e  X  s
In this paper, we employ the same features as those used by Freitag and Khadivi (2007). All lo-cal feature functions  X  ( a tions of the alignment operation a backward-looking character m -grams in sequences e and f at positions i and j respectively. For the source sequence f both forward and backward-looking m -gram features are included. We restrict the m -gram features in our target sequence e to only be backward-looking since we do not have access to forward-looking m -grams during beam-search. An order M model is one that uses m -gram features where m = 0 , 1 , . . . M .
 Our training algorithm takes as input a data set D of source-target transliteration pairs and outputs a parameter vector u . The algorithm pseudo-code appears in Fig. (1). In the algorithm, the function L ( e  X  , e ) defines a loss incurred by predicting e  X  in-stead of e . In most structured prediction problems, the targets are of equal length and in such cases the Hamming loss function can be used. However, in our case the targets may differ in terms of length and thus we use the Levenshtein edit distance (Leven-shtein, 1966) with unit costs for insertions, deletions and substitutions. Since the targets are both in the same alphabet E this loss function is well-defined. The user also supplies three paramters: (1) T -the number of training iterations (2) K -the number of best target hypotheses used (3) C -a complex-ity parameter. A low C is useful if the data is non-separable and noisy.

The final parameter vector u returned by the al-gorithm is the average of the intermediate parameter vectors produced during training. We find that av-eraging helps to improve performance. At test time, we use the beam search procedure to produce K -best hypotheses using the parameter vector u . We apply our model to the real-world Arabic-English name transliteration task on a data set of 10,084 Arabic names from the LDC. The data set consists of Arabic names in an ASCII-based alpha-bet and its English rendering. Table 1 shows a few examples of Arabic-English pairs in our data set. We use the same training/development/testing (8084/1000/1000) set as the one used in a previ-ous benchmark study (Freitag and Khadivi, 2007). The development and testing data were obtained by randomly removing entries from the training data. The absence of short vowels (e.g.  X  X  X  in h
NB X  X , nab X  X  i ), doubled consonants (e.g.  X  X w X  in h FWAL, fawwal i ) and other diacritics in Arabic make the transliteration a hard problem. Therefore, it is hard to achieve perfect accuracy on this data set. For training, we set K = 20 best hypotheses and Input parameters Initialize w Repeat T times: For Each ( e , f )  X  X  : Output Scoring function s ( a , e , f ) = u  X ( a , e , f ) C = 1 . 0 and run the algorithm for T = 10 epochs. To evaluate our algorithm, we generate 1 -best (or 5 -best) hypotheses using the beam search procedure and measure accuracy as the percentage of instances in which the target sequence e is one of the 1 -best (or 5 -best) targets. The input features are based on character m -grams for m = 1 , 2 , 3 . Unlike previ-ous generative transliteration models, no additional language model feature is used.

We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features (Freitag and Khadivi, 2007). The SMT system directly models the posterior probability P r ( e | f ) using a log-linear combination of several sub-models: a character-based phrase translation model, a character-based lexicon model, a character penalty and a phrase penalty. In the PTEM model, the update rule only considers the best target sequence and modifies the parameters w if the score s ( e  X  , f )  X  s ( e , f ) .

Table 2 shows the 1-best and 5-best accuracy of each model trained on the combined train+dev data set. All the models are evaluated on the same test set. Both MIRA and PTEM algorithms outperform the SMT model in terms of 1-best accuracy . The differences in accuracy are significant at 95% con-fidence level, using the bootstrapping method for hypothesis testing. The difference in 1-best per-formance of MIRA and PTEM is not significant. At 5-best , the MIRA model outperforms both SMT and PTEM model. We conjecture that using the problem-specific Levenshtein loss function helps fil-ter bad target sequences from the K -best outputs during training.

In a second experiment we studied the effect of changing C on the performance of the algo-rithm. We ran the algorithm with the above set-tings, except varying the value of the complexity parameter to one of 7 values in the range C = 0 . the train set, and evaluating the resulting model on the test set. The results are summarized in Table 3. The entry marked with a star  X  indicates the model that achieved the best performance on the dev set for a particular choice of evaluation measure ( 1 -best or 5 -best). We find that changing C does have an effect on model performance. As the value of C decreases, the performance at lower ranks improves: C = 0 . 01 is good for 5 -best accuracy and C = 0 . 001 for 20 -best accuracy (not in table). As C is further reduced, a greater number of iterations are needed to con-verge. In our model, where the alignments are not observed but inferred during training, we find that making small incremental updates makes our algo-rithm more robust. Indeed, setting C = 0 . 01 and training on the train+dev set improves 5 -best per-formance of our model from 0 . 841 to 0 . 861 . Hence, the choice of C is important. We have shown a significant improvement in accu-racy over state-of-the-art transliteration models by taking into consideration the ranking of multiple hypotheses (top-K ) by Levenshtein distance, and making the training algorithm robust to noisy non-separable data. Our model does consistently well at high ( K = 1 ) and low ranks ( K = 5 ), and can therefore be used in isolation or in a pipelined sys-tem (e.g. machine translation or cross-language in-formation retrieval) to achieve better performance. In a pipeline system, more features of names around proper nouns and previous mentions of the name can be used to improve scoring of K -best outputs.
In our experiments, the Levenshtein loss function uses only unit costs for edit operations and is not specifically tuned towards our application. In fu-ture work, we may imagine penalizing insertions and deletions higher than substitutions and other non-uniform schemes for better transliteration per-formance. Our K -best framework can also be easily extended to cases where one name has multiple for-eign translations that are equally likely.

