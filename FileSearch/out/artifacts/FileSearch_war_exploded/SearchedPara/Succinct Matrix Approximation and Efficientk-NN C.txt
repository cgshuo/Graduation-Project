
This work reveals that instead of the polynomial bounds in previous literatures there exists a sharper bound of expo-nential form for the L 2 norm of an arbitrary shaped ran-dom matrix. Based on the newly elaborated bound, a non-uniform sampling method is presented to succinctly approx-imate a matrix with a sparse binary one, and thus relieves the computation loads of k -NN classifier in both time and storage. The method is also pass-efficient because sampling and quantizing are combined together in a single step and the whole process can be completed within one pass over the input matrix. In the evaluations on compression ratio and reconstruction error, the sampling method exhibits im-pressive capability in providing succinct and tight approxi-mations for the input matrices. The most significant finding in the classification experiment is that the k -NN classifier based on the approximation can even outperform the stan-dard one. This provides another strong evidence for the claim that our method is especially capable in capturing intrinsic characteristics.
The computation for matrix by vector product is the most time and storage consuming step in many numeric algo-rithms. Typical complexity of this computation is O ( mn ) in both time and storage for a matrix size of m  X  n . In real world problems such as data mining, machine learn-ing, computer vision, and information retrieval, the compu-tations soon become infeasible as the size of matrices get-ting large, because there are so many operations of this type involved.

In data mining and machine learning, low rank approx-imations provide compact representations of the data with limited loss of information and hereby relieve the curse of dimensionality. A well known technique for low rank ap-proximation is the Singular Vector Decomposition (SVD), also called Latent Semantic Indexing (LSI) in information retrieval [14] [9], which minimize the residual among all ap-proximations with the same rank. Traditional SVD compu-tation requires O ( mn min f m, n g ) time and O ( mn ) space. To speed up the SVD-based low rank approximations and save storage, sampling methods are often used to sparsify-ing the input matrices for the time and space complexity of a sparsified matrix with N non-zero entries are reduced to O ( N min f m, n g ) and O ( N ) respectively. If a quantizing (rounding) approach is combined, the computation loads can be further relieved. Likewise, the k -NN classifier can benefit from a succinct approximation based on sampling too, since, as a costly computation part, the distance deter-mination can also be derived from matrix by vector product.
Attracted by the computational benefits from sparsifying matrices, numerous algorithms for massive data problems adopt sampling methods and different error bounds for them are also presented.

Frieze et al. (2004) [3] developed a fast Monte-Carlo al-gorithm which find an approximation e D in the vector space spanned by some sampled rows of the input matrix D such that P ( k D  X  e D k F  X   X  k D k F )  X  1  X   X  in two passes through D . Since many rows are discarded in the random sampling process, the output matrix can be interpreted as a sparse representation of the input matrix. In low rank approximations, Deshpande and Vempala (2006) [1] fur-ther presented an adaptive sampling method to approximate the volume sampling with a probability at least 3 / 4 that k
D  X  e D k k F  X  (1+  X  ) k D  X  D k k F ( D k consists of the first k terms in the SVD of D ). This algorithm requires more passes over the input matrix. Passes through a large matrix can be extremely time consuming if the matrix can not be fed in Random Access Memory and must be stored in ex-ternal storages such as Hard Disk. An alternative entrywise sampling method which requires only one pass was given by Achlioptas and McSherry (2001) [4]. Where the reconstruc-tion error has a significant better bound in L 2 norm, a more effective indicator for the trends in a matrix than Frobenius norm, as P ( k D  X  e D k 2  X  5 b with certain regularity. The theorem [11] which bounds the deviations of eigenvalues of a random symmetric ma-trix from their medians is used to prove this bound. In [4] they also includes detailed comparisons of their application to fast SVD with the methods in [12] and [2]. A review paper by Mannila (2002) [6] mentioned that the method in [4] works very well in computing low rank approximation. Arora et al. (2006) [13] improved the results in [4] by keep-ing the entries of large absolute value with large probability.
There are also some achievements in non-sampling based methods. The GLRAM proposed by Ye (2005) [7] is a such one. Instead of the linear transformation used in SVD, the GLRAM applies a bilinear mapping on the data because each point is represented by a matrix other than a vector. The algorithm intends to minimize LM i R T k 2 F with the same L and R for all A i (the data points) and hereby relieve computation loads. Their result is appealing but does not admit a closed form solution in general, hence no error bound presented.

From another point of view, Bar-Yossef (2003) [17] pointed out based on information theory that at least O ( mn ) queries are necessary to produce a (  X ,  X  ) approximation for a given matrix.
The work in this paper is closely in connection with the deliberations described in [4] and [13]. Carrying out the present study, we attempt to find a better bound for a ran-dom matrix of arbitrary shape in L 2 norm and hereby de-velop a more effective matrix approximation method based on sampling. Unlike the method in [11] used by [4] which results in a bound of polynomial form, our work exploits the logic relationship between the inner product weighted by a constructed symmetric matrix in a reduced discrete space, the Rayleigh quotient of the matrix, and the L 2 norm of the input matrix. Following the relationship, a exponential bound is obtained by applying Hoeffiding X  X  inequality to the weighted inner product. Moreover, the bound is further im-proved owing to making the best of the special structure of the constructed symmetric matrix. Motivated by the newly elaborated bound, a non-uniform sampling method is devel-oped to approximate a given matrix by a sparse binary one with limited loss of information. The omitting and rounding are naturally combined together for the sampling probabili-ties we chosen tailor the segments containing the retained entries into equal length. And the method is also pass-efficient because the whole process can be completed in one pass over the input matrix. To adopt the succinct approxi-mation in k -NN classification, the training set is represented by a sparse binary matrix resulted from the non-uniform sampling. Then, the distance determination in classification are derived from the product of a sparse binary matrix by the given vector. Therefore, both the time and space com-plexity of the k -NN algorithm are reduced from O ( mn ) to O ( N ) for one prediction. Experiments for approximation and classification are conducted on a data set of huge di-mension to evaluate the sampling method and the k -NN classifier based on it. The low reconstruction errors with high compression ratios elucidate the great capability of the sampling method in approximating input matrices suc-cinctly and tightly. The most significant finding in classifi-cation experiments is that the k -NN classifier based on the approximation can even have better performance than the standard one. The claim that our method is especially ca-pable in capturing intrinsic characteristics is then furnished with another strong evidence.
As a main part, section 2 presents and then analyzes the sampling method. The reason why L 2 norm is more sensi-tive in revealing structures of matrices than Frobenius norm is also included in this section. The implementation of k -NN algorithm on the matrix approximation is detailed in section 3. Section 4 is the experimental evaluations for the sampling method and the k -NN classifier based on it. We end this paper with the conclusion and future works in sec-tion 5. The data matrix under consideration in this paper is D 2 R m  X  n . Where m is the number of data points, and n is the number of attributes or the dimension of the points. The method that sparsifying D is delivered and analyzed in the following. Besides, the motivation for the method and comparisons with other techniques are also included.
The sampling method we present to approximate a ma-trix is Where c = b  X  s , b = max i,j j d ij j , and s  X  1 is a con-troller for the sparsity. Actually, this method directly omits a entry or rounds it to  X  c in one query unlike the method in [4] where a extra quantizing step is needed. Moreover, the matrix obtained can be succinctly represented by a binary matrix corresponding to the sign of the entries, enabling ad-dition in place of multiplication, with a final scaling of the result by c . The metric for the error of this sampling method in a matrix norm form is bounded by the Theorem below. Theorem 1 If e D is the matrix resulting from the sampling process given in eq.(1), then P ( k D  X  e D k 2  X   X  )  X  1 e t 2 (0 , 1) .
 The motivation of the sampling method and the proof of Theorem 1 are given in the following parts of this section. But before we move forward, preparing preliminary knowl-edge about the metrics of a matrix is helpful.
Norms are commonly used metric for matrices. In com-putations, norms of different type play a central role in the researches for the convergence, stability, and error analysis of numeric methods.

Among them, Frobenius norm or F norm for short is widely used for its easy obtainability. For a data matrix D , the F norm is given by k D k F = ( computation can be completed in time of O ( mn ) .
Another type of norm is the L 2 norm which is more in-formative about the structure of a matrix. The L 2 norm is defined as k D k 2 = max k x k p  X  1 =  X  1 2 ( D T D ) equivalently. Where,  X  1 is the largest eigenvalue of D T D and  X  (  X  ) denotes the spectral radius of a matrix.

The SVD can be used to rough the relationship between the F norm and L 2 norm. According to the SVD theorems [5], k D k 2 =  X  1 and k D k F = are the i -th largest singular value and the rank of D respec-tively. This means that the L 2 norm captures the largest sin-gular value while the F norm measures the singular values in some average way.

Perhaps a more insightful point of view about the ma-trix norms is from the linear trend in data. Some materials about the relationship between norms and linear trend are given in [4]. Here we analyze it in detail. The linear trend in a data matrix D is defined as any tendency of the rows to align with a particular unit vector x . The i -th coordinate of Dx is the projection of the i -th row of D onto the direction x . Then k Dx k 2 is a measurement of the linear trend x in D . Certainly, the largest linear trend max k x k and its direction are desired mostly. According to the defi-nition of L 2 norm, k D k 2 ( = max k x k captures the largest linear trend in D and the unit eigenvec-tor v 1 of  X  1 is the direction of the trend. Rather than taking x = v 1 , the following proposition depicts an insight into the relationship between the F norm and the linear trend in a data matrix.
 Proposition 1 If x are sampled uniformly from a unit sphere of dimension n , then E ( k Dx k 2 2 ) = k D k 2 F Proof : Based on the SVD, D = P P singular vector of  X  i respectively. Note that x is uniformly sampled from the unit sphere, and all v i form an orthonor-mal basis of the linear space R n , so the expectations of ( v i x )
E (( v T i x ) 2 ) =
E ( k Dx k 2 2 ) = E (
Proposition 1 reveals that k D k F measures the strength of the average trend in D . Such averaging nature is not what we want when we seek structures. In particular, as the number of data dimension gets large (recall the dimen-sion curse), the strongest trend tend to be submerged in the averaging.

For the insensitivity of F norm to linear trends, L 2 norm is a preferred metric in classification, LSI, and other data mining tasks.
Computing the L 2 norm of an arbitrary matrix is a quite difficult task. The already known results to this problem are mainly involved in symmetric matrices. For an arbitrary matrix B 2 R m  X  n , the following Lemma can be used to associate k B k 2 with the L 2 norm of a symmetric matrix Lemma 1 Let  X  1  X   X  2  X   X   X   X   X   X  m + n be eigenvalues of A , then k B k 2 = k A k 2 =  X  1 =  X   X  m + n .
 Proof : Let x be the eigenvector belongs to an eigen-value  X  . We decompose the vector into two part as x = [ x 1 , x can be written as Let y 1 = x 1 , y 2 =  X  x 2 , this becomes  X  so  X   X  is also an eigenvalue of A . This means that the eigen-values of A appear in a pairwise way. Consequently, the L 2 norm of the symmetric matrix A is given by max i j  X  i j =  X  1 =  X   X  m + n . To prove that the L 2 norms of B and A are identical, let us consider the eigenfunction of A According to the Sylvester X  X  theorem [16], B T B and BB T have the same none-zero eigenvalues. The equation above further reveals the truth that A T A , B T B , and BB T are all share the same non-zero eigenvalues. This lead us to the claim that k B k 2 = k A k 2 .  X 
Now we focus on the L 2 norm of a symmetric matrix since the Lemma given above point out that one can always construct a symmetric matrix to make it has the same L 2 norm as the given arbitrary shaped matrix. Symmetric ma-trices possess many appealing properties and some of them are very helpful in estimating the L 2 norm. The Rayleigh quotient defined below is especially important among them owing to providing the information about the eigenvalues of a symmetric matrix.
 Next Lemma is an improvement of the Theorem in [16] which associates the L 2 norm of a symmetric matrix with the Rayleigh quotient. We will use it combined with Ho-effding X  X  inequality to furnish the sampling method with a bound of (  X ,  X  ) form.
 Lemma 2 If A is a symmetric matrix, then k A k 2 = max x j R ( x ) j ; furthermore, if the matrix takes the form given in eq.(2), then k A k 2 =  X  1 = max x R ( x ) . Proof : Perhaps, the best known property of Rayleigh quotient is as  X  1  X   X  2  X   X   X   X   X   X  n be eigenvalues of A . This property gives the first part of the Lemma straightforwardly since k
A k 2 = max fj  X  1 j , j  X  n jg = max x j R ( x ) j . If A takes the form in eq(2), according to Lemma 1, k A k 2 =  X  1 = max x R ( x ) .  X 
The vector x in the definition of R ( x ) can be normalized by dividing both the numerator and denominator by k x k 2 Then the Rayleigh quotient can be defined equivalently as R ( x ) =
R ( x ) becomes a random variable, if A is a random sym-metric matrix. Inequalities are of great use in bounding quantities that might otherwise be hard to compute. But directly using Rayleigh quotient combined with a random inequality to bound the L 2 norm of A will encounter great that k A k 2 = max k x k troduced a method that makes it sufficient to consider only vectors in a discrete space. We strengthen their result ([15], [13]) here as a Lemma that Lemma 3 (Reduction to Discrete Space) Let T = f z : z 2 v every unit vector u . And the size of T (denoted by j T j Proof : Map every point z 2 T in a 1-1 correspondence with a n -dimensional hypercube of side length t p n on the grid: z 7! C z (= f z + w : w  X  0 , k w k 1  X  t p n g ) . Ac-cordingly, the length of w satisfies the following inequality. Then, Since the maximum length of any point in C z is bounded by 1 + t , the union of these cubes is contained in the n -dimensional ball B of radius (1 + t ) . Therefore, we have Consequently, For any unit vector u , let y = (1  X  t ) u . Rounding down the coordinates of y to the nearest multiple of t p n , we get a grid point x such that y 2 C x i.e. y = x + w . Then vertices of C x can be represented as v cx = x + w k t/ p In another word, the maximum length of any vertex of C x is bounded by 1 , so all vertices of C x are grid points in T . Therefore, y can be represnted by a convex combination of the vertices v i in T as y = Then we have y T Ay = The inequality is obtained as we assumed that for every v In respect that the probability relationship between P ( k A k we give an additional Lemma to describe them.
 Lemma 4 If A takes the form in eq.(2) and P ( v T i Av j  X  (1  X  t ) 2  X  j v  X e Proof : The logic relationship of the propositions above is 8 v i 2 T, v T i Av j  X  (1  X  t ) 2  X  ) 8k x k 2 = 1 , x T Ax As a result, Note that
P ( The last inequality results from that Lemma 3 points out the maximum size of T is e (2+1 /t )( m + n ) (note that the dimen-sion of A is ( m + n ) ). Then the lemma follows.  X 
Now we focus on furnishing P ( v T i Av j  X   X  j v i , v j T ) with a lower bound via probability inequalities. Markov X  X  inequality obtained by tailing the expectation of a random variable is a widely used one in real world applica-tions. Hoeffding X  X  inequality is similar in spirit to Markov X  X  inequality, but it is a sharper inequality owing to that it makes use of a Taylor expansion of second order. Further-more, if there are large number of samples, the bound from Hoeffind X  X  inequality is smaller than the bound from Cheby-shev X  X  inequality. Before applying Hoeffding X  X  inequality to get a bound for the sampling method, we give a refinement for it as: Lemma 5 Let Y 1 ,  X   X   X  , Y n be independent observation such that E ( Y i ) = 0 and  X  i  X  Y i  X   X  i . Then, for any  X  &gt; 0 , Proof : The original Hoeffding X  X  inequality given in [8] on the same condition is When t = 4  X / inequality above archives its minmum e  X  2  X  2 / Thus the claim.  X 
A bound for P ( v T i Av j  X   X  j v i , v j 2 T ) can then be obtain by applying the results obtained above. We state it as one of our main results as the Theorem follows.
 Theorem 2 Let B 2 R m  X  n be a random matrix with independent entries of zero mean. If  X  ij  X  b ij  X   X  ij , then P ( k B k 2  X   X  )  X  1  X   X  for 8  X  &gt; 0 . Where  X  = e for some fixed t 2 (0 , 1) , [ x T , y T ] T = v i 2 T , and [ u T , w T ] T = v j 2 T .
 Proof : We construct a symmetric matrix A by B as eq.(2), then k B k 2 = k A k 2 according to Lemma 1 and Lemma 2. Lemma 4 reveals further that P ( k B k 2  X   X  ) = P ( k A k (1  X  t ) 2  X  j v v pendent and zero mean. The requirements for Lemma 5 are thus satisfied and the range of random variable ( x i u y j ) b ij is from ( x i w j + u i y j )  X  ij to ( x i w j conversely. As a result, Then the theorem follows.  X 
At this time we are prepared to prove Theorem 1. But before going further, we present the motivation of the sam-pling method.

A general approach for sparsifying a dense matrix is Where w.p. means  X  X ith the probability of X . In many cases, the method that makes the expectation of the sampled ma-trix equal to the input matrix is a reasonable strategy. We follow this too, that is d ij = E (  X  d ij ) = 0  X  (1  X  p p Define B = D  X  e D as the error or difference matrix of the sampling. Obviously, E ( b ij ) = 0 and b ij are independent due to that  X  d ij are sampled independently. The requirements for Theorem 2 are hereby satisfied. The range for the random variable  X  b ij is from d ij to (1  X  1 /p the components that contain the ranges are then become as P parameter need to be settled is the probability p ij which is essential for the success of many sampling based methods. From the analysis above, we know that with the increase of p ij the  X  in the bound of of the matrix e D increases. In applications, compromise between them should be take into consideration. Note that there is no priori reason to omit each entry in the input matrix D with the same probability, we set p ij = j d ij j ( c is a constant positive real number) so that all entries of B are contained in segments of equal length c and the bound for k B k 2 are simplified further. In addition, that the larger the absolute value of an entry is, the more likely it retained is desired in many real world applcations. Intend to guarantee that p ij is a well defined probability that be s  X  b . As a result, sparsifying (omitting) and quantizing (rounding) are combined naturally without any extra steps. The sampling method given in eq.(1) comes into being as thus.

After explaining the motivation, Theorem 1 can be proofed straightly.
 Proof of Theorem 1 : For p ij = j d ij j /c , ( x u y j ) 2 . And we have
X =  X   X  = ( The second inequality is obtained from Cauchy X  X  inequal-ity, and the last is from the fact that [ x T , y T ] T = v  X  P ( k B k forth.  X 
There are some other sparsifying methods that also uti-lize entrywise sampling in recent researches. Among them, the works closely related to ours are [4] and [13].
In [4] the bound for the omitting approach is The error is bounded by P ( k B k 2  X  5 b 1 / ( m + n ) with the regularity that 1  X  s  X  ( m + n ) / (4 log 6 ( m + n ) . This bound is obtained by applying the theorem in [11] which bounds the deviations of eigenval-ues of a random symmetric matrix from their medians. In our method, the retaining probability of an entry is j d ij ( = j d ij j /b  X  s  X  1 /s ) and P ( k B k 2  X  5 b e Our method is hereby much shaper in error bound with larger compression ratio than that of [4] without saying that the rounding process combined. Taking in that e  X  x 2 tends towards 0 rather rapidly than 1 /x 2 as x getting large, our method bounds the error more tightly for massive data ma-trices (recall the dimension curse encountered in data min-ing).

The bound from [13] is restricted to symmetric data ma-trices, moreover, their  X  is two times worse than ours. The reason relays on that without utilizing the special structure of A , they can only applying the first part of Lemma 2 in analysis. By contrast, being aware of the specialty we use the second part Lemma 2.

More detailed comparisons of the sparsifying methods to other sampling methods in performing tasks of low rank approximation can be referred in [4].
The k nearest neighbors ( k -NN) [10] method is a typi-cal lazy learning scheme which doesn X  X  do any computation in the training stage but stores the labeled samples. This technique assumes that the training set contains not only the data but also the desired classification for each item, that is the training data also become the model. In the classifying stage, the distance of the new point to each item in the train-ing set must be determined to find the k closest entries to it in the labeled set, and the new item is then placed in the class that covers the most items in the k closest set. Since the tuple to be classified must be compared to each element in the training set D 2 R m  X  n , the classification requires O ( mn ) time and space. Consequently, significant applica-tions quickly become prohibitive as the dimension of data getting large.

Intend to accelerate the computation and save storage, the sampling method presented above is employed to ap-proximate the training data with a sparse binary matrix. Af-ter the sampling process, the storage is reduced to O ( N ) (bit!) (in many cases the type of the data involved in data mining is double precision which takes 64 bit, then the stor-age space is compressed from mn to N/ 64 ). Based on the resulting binary matrix, the distance computation, the most time consuming part of the k -NN method, can also be speeded up. Let e d i be the i -th item of the sparsified train-ing set (the i -th row of the sparse binary matrix), p be the new tuple to be classified, q i be the distance between e p . Then the distance between the new tuple and the origi-nal training sample can be approximated by q i computed as follows. Let q  X  = [ q 2 1 ,  X   X   X  , q 2 m ] T , d  X  i = h e d i , have q  X  = d  X  + p e  X  2 e Dp . Here e = [1 ,  X   X   X  , 1] T ing to the speciality (sparse and binary valued) of e D , the matrix by vector product can be obtained in O ( N ) and the computation for d  X  can be completed within O ( m ) . The time complexity of the k -NN classifier is thus reduced to O ( N ) .
 As the time and space complexity are both reduced to O ( N ) , applications of the k -NN classifier becomes feasible even in high dimension problems.
In order to evaluate the presented sampling method and the classifier based on it in real world problems, experi-ments on a large data set are carried out. The data set used is PIX 1 which contains 300 grayscale face images of size 512  X  512 from 30 different persons. Firstly, the sampling method is tested in compression ratio (inverse of the sparse ratio) and reconstruction error (measured by the norm of the difference matrix). Next, the k -NN classification based on it is performed on the same data set.
To get an insightful comparison, the omitting and round-ing approach in [4] are combined together as below and hereby results in a sparse binary matrix of the same form as ours.
To exploit the impact of s on the performance of the sam-pling methods, we run the experiment 9 times on one image with s from 2 to 18. Each time, the sampling process are repeated by 100 rounds, and the averaged norm of the dif-ference matrix (reconstruction error) and sparse ratio (in-verse of the compression ratio) are given in Table 1. The table shows that our method approximates the input matrix more closely while keeping about only 60% non-zero en-tries compared to the Omitting+Rounding method (O+R). One can also see from the table that the difference between the error norms of our and O+R increases as s increases. This means that our method is especially effective in sam-pling a matrix to a quite more sparse one. Fig.1 illuminates our observations more directly. A visual comparison of the sparsified images is shown in Fig.2. To perform the exam-ination more widely, the comparison is extend to 10 im-ages randomly drawn from PIX. The results are displayed in Fig.3.

The above evaluation reveals clearly that our method is quite capable in depicting outline, seeking structures, and capturing characteristics of an input matrix while keeping much fewer entries. The deficiency of the method in [4] (O+R) is mainly caused by that the rounding step may change the sign of entries, moreover, a zero entry can even be rounded into non-zero. from our method, and the third row is from O+R) 4.2 Misclassification Rate of the k -NN
The k -NN classifier based on the sampling method is tested with different k and sparse ratio on a data set down-sampling to 100  X  100 from PIX (except the fist person, for two images of the first person are missing). Accordingly, the dimension or number of attributes of the data set is 10000! The testing set are formed by sampling one ran-domly from the ten images of each person, and the remains form the training set. In order to improve the performance of the classifier, samples in the training set are z-score nor-malized.

The classification evaluation is performed with the aver-aged SR from 1 to 0.0228 ( SR = 1 is the standard k -NN method without sparsifying). For each pair of k and SR , the experiment is repeated by 10 sampling rounds. The av-eraged misclassification rates are given in Table 2. A visible description of the evaluation is shown in Fig.4. The figure reveals that the increase of k has negative impact on the performance of the standard k -NN. The effect of k on the sampling based k -NN is similar to this when the sparsity is low. On the contrary, the misclassification rate decreases as k getting large if the sparsity is high. The most interesting finding from this evaluation is that for some low sparsity the sampling based k -NN can even outperform the standard one. This finding also provides a strong support for our con-clusion that the sampling method is quite capable in captur-ing the intrinsic characteristics of matrices with few entries are reserved.
Motivated by the exponential bound we found for the L 2 norm of an arbitrary shaped random matrix, a non-uniform
Figure 4. averaged misclassification rates(the arrow pointed line is the rate of the standard k -NN) sampling based succinct matrix approximation is presented to improve the efficiency of k -NN classification. The com-putation load of the k -NN method via this approximation in time and storage are both reduced from O ( mn ) to O ( N ) for one prediction. In the sampling method, omitting and rounding are combined naturally without the need for extra steps; furthermore, the process is pass-efficient because the sampling can be completed within one pass over the input matrix. The evaluations on compression ratio and recon-struction error revealed clearly that the sampling method is capable in capturing intrinsic characteristics of the input matrix while keeping much fewer entries. Namely, the ap-proximation represented by a sparse binary matrix resulted from the sampling method is more succinct and tight. The reason of this good performance relays mainly on that the obtained bound of the L 2 norm is much shaper than pre-vious works. The experiment on the misclassification rate delivered a significant finding that the k -NN classification via the succinct matrix approximation can even outperform the standard one. This also provide another strong support for the claim that our method is especially useful in seeking important structures.

In the future, we will extend the present studies to seek more succinct matrix approximations with lower recon-struction error, and hereby to improve the efficiency of clas-sifications further. Applications to fast LSI, clustering large graphs, and other data mining tasks are also under consid-eration.

This work was partially supported by National Nat-ural Science Foundation of China (Grant No.70621001, 70531040, 70501030, 10601064, 70472074), National Nat-ural Science Foundation of Beijing (Grant No.9073020), 973 Project of Chinese Ministry of Science and Technology (Grant No.2004CB720103), and BHP Billiton Cooperation of Australia.

