 Syntactic parsing has played a central role in natural language processing. The resulting syntactic analy-sis can be used for various applications such as ma-chine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and La-pata, 2009; Yamangil and Shieber, 2010), and ques-tion answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many sta-tistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maxi-mum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003).

In recent years, there has been an increasing inter-est in tree substitution grammar (TSG) as an alter-native to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree frag-ments have great advantages over tiny CFG rules since they can capture non-local contexts explic-itly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context free-dom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits ob-tained from large tree fragments.

On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weaken-ing context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into sub-categories, rather than extracting large tree frag-ments. As shown in several studies on TSG pars-ing (Zuidema, 2007; Bansal and Klein, 2010), large tree fragments and symbol refinement work comple-mentarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser.
 In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. Our work differs from previous studies in that we focus on a unified model where TSG rules and symbol re-finement are learned from training data in a fully au-tomatic and consistent fashion. We also propose a novel probabilistic SR-TSG model with the hierar-chical Pitman-Yor Process (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of 92.4% in the WSJ English Penn Treebank pars-ing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and superior to state-of-the-art discriminative reranking parsers. Our SR-TSG work is built upon recent work on Bayesian TSG induction from parse trees (Post and Gildea, 2009; Cohn et al., 2010). We firstly review the Bayesian TSG model used in that work, and then present related work on TSGs and symbol refine-ment.

A TSG consists of a 4-tuple, G = ( T,N,S,R ) , where T is a set of terminal symbols , N is a set of nonterminal symbols , S  X  N is the distinguished start nonterminal symbol and R is a set of produc-tions (a.k.a. rules). The productions take the form of elementary trees i.e., tree fragments of height  X  1 . The root and internal nodes of the elemen-tary trees are labeled with nonterminal symbols, and leaf nodes are labeled with either terminal or nonter-minal symbols. Nonterminal leaves are referred to as frontier nonterminals , and form the substitution sites to be combined with other elementary trees. A derivation is a process of forming a parse tree. It starts with a root symbol and rewrites (substi-tutes) nonterminal symbols with elementary trees until there are no remaining frontier nonterminals. Figure 1a shows an example parse tree and Figure 1b shows its example TSG derivation. Since differ-ent derivations may produce the same parse tree, re-cent work on TSG induction (Post and Gildea, 2009; Cohn et al., 2010) employs a probabilistic model of a TSG and predicts derivations from observed parse trees in an unsupervised way.

A Probabilistic Tree Substitution Grammar (PTSG) assigns a probability to each rule in the grammar. The probability of a derivation is defined as the product of the probabilities of its component elementary trees as follows. where e = ( e 1 ,e 2 ,... ) is a sequence of elemen-tary trees used for the derivation, x = root ( e ) is the root symbol of e , and p ( e | x ) is the probability of generating e given its root symbol x . As in a PCFG, e is generated conditionally independent of all oth-ers given x .

The posterior distribution over elementary trees given a parse tree t can be computed by using the Bayes X  rule: where p ( t | e ) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to con-sist of modeling the prior distribution p ( e ) . Recent work on TSG induction defines p ( e ) as a nonpara-metric Bayesian model such as the Dirichlet Pro-cess (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars.

Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete : all leaf nodes must be termi-nal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor gram-mars have largely been applied to the task of unsu-pervised structural induction from raw texts such as morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar in-duction (Cohen et al., 2010), rather than constituent syntax parsing.

An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to uti-lize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduc-tion, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. In this section, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. Our SR-TSG model is an extension of the conventional TSG model where every symbol of the elementary trees can be refined to fit the train-ing data. Figure 1c shows an example of SR-TSG derivation. As with previous work on TSG induc-tion, our task is the induction of SR-TSG deriva-tions from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol sub-categories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG al-lows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To ad-dress the sparseness problem, we employ a hierar-chical PYP to encode a backoff scheme from the SR-TSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010).

Our model consists of a three-level hierarchy. Ta-ble 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this three-level hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. where x k is a refined root symbol of an elemen-tary tree e , while x is a raw nonterminal symbol in the corpus and k = 0 , 1 ,... is an index of the symbol subcategory. Suppose x is NP and its sym-bol subcategory is 0 , then x k is NP 0 . The PYP has three parameters: ( d x is a base distribution over infinite space of symbol-refined elementary trees rooted with x k , which pro-vides the backoff probability of e . The remaining parameters d x base distribution.

The backoff probability P sr-tsg ( e | x k ) is given by the product of symbol-refined CFG (SR-CFG) rules that e contains as follows.
 where F ( e ) is a set of frontier nonterminal nodes and I ( e ) is a set of internal nodes in e . c f and c i are nonterminal symbols of nodes f and i , respec-tively. s c is the probability of stopping the expan-sion of a node labeled with c . SR-CFG rules are CFG rules where every symbol is refined, as shown in Table 1. The function cfg-rules ( e | x k ) returns the SR-CFG rules that e contains, which take the form of x k  X   X  . Each SR-CFG rule  X  rooted with x k is drawn from the backoff distribution H x and H x d x , X  x ,P sr-cfg . This distribution over the SR-CFG rules forms the second level hierarchy of our model. The backoff probability of the SR-CFG rule, P sr-cfg (  X  | x k ) , is given by the root-unrefined CFG (RU-CFG) rule as follows, where the function root-unrefine (  X  | x k ) returns the RU-CFG rule of  X  , which takes the form of x  X   X  . The RU-CFG rule is a CFG rule where the root symbol is unrefined and all leaf nonterminal sym-bols are refined, as shown in Table 1. Each RU-CFG rule  X  rooted with x is drawn from the backoff distri-bution I x , and I x is produced by a PYP. This distri-bution over the RU-CFG rules forms the third level hierarchy of our model. Finally, we set the back-off probability of the RU-CFG rule, P ru-cfg (  X  | x ) , so that it is uniform as follows.
 where | x  X  X  X  is the number of RU-CFG rules rooted with x . Overall, our hierarchical model en-codes backoff smoothing consistently from the SR-TSG rules to the SR-CFG rules, and from the SR-CFG rules to the RU-CFG rules. As shown in (Blun-som and Cohn, 2010; Cohen et al., 2010), the pars-ing accuracy of the TSG model is strongly affected by its backoff model. The effects of our hierarchical backoff model on parsing performance are evaluated in Section 5. We use Markov Chain Monte Carlo (MCMC) sam-pling to infer the SR-TSG derivations from parse trees. MCMC sampling is a widely used approach for obtaining random samples from a probability distribution. In our case, we wish to obtain deriva-tion samples of an SR-TSG from the posterior dis-tribution, p ( e | t , d ,  X  , s ) .

The inference of the SR-TSG derivations corre-sponds to inferring two kinds of latent variables: latent symbol subcategories and latent substitution sites. We first infer latent symbol subcategories for every symbol in the parse trees, and then infer latent substitution sites stepwise. During the inference of symbol subcategories, every internal node is fixed as a substitution site. After that, we unfix that assump-tion and infer latent substitution sites given symbol-refined parse trees. This stepwise learning is simple and efficient in practice, but we believe that the joint learning of both latent variables is possible, and we will deal with this in future work. Here we describe each inference algorithm in detail. 4.1 Inference of Symbol Subcategories For the inference of latent symbol subcategories, we adopt split and merge training (Petrov et al., 2006) as follows. In each split-merge step, each symbol is split into at most two subcategories. For exam-ple, every NP symbol in the training data is split into either NP 0 or NP 1 to maximize the posterior prob-ability. After convergence, we measure the loss of each split symbol in terms of the likelihood incurred when removing it, then the smallest 50% of the newly split symbols as regards that loss are merged to avoid overfitting. The split-merge algorithm ter-minates when the total number of steps reaches the user-specified value.
 In each splitting step, we use two types of blocked MCMC algorithm: the sentence-level blocked Metroporil-Hastings (MH) sampler and the tree-level blocked Gibbs sampler, while (Petrov et al., 2006) use a different MLE-based model and the EM algorithm. Our sampler iterates sentence-level sam-pling and tree-level sampling alternately.

The sentence-level MH sampler is a recently pro-posed algorithm for grammar induction (Johnson et al., 2007b; Cohn et al., 2010). In this work, we apply it to the training of symbol splitting. The MH sam-pler consists of the following three steps: for each sentence, 1) calculate the inside probability (Lari and Young, 1991) in a bottom-up manner, 2) sample a derivation tree in a top-down manner, and 3) ac-cept or reject the derivation sample by using the MH test. See (Cohn et al., 2010) for details. This sampler simultaneously updates blocks of latent variables as-sociated with a sentence, thus it can find MAP solu-tions efficiently.

The tree-level blocked Gibbs sampler focuses on the type of SR-TSG rules and simultaneously up-dates all root and child nodes that are annotated with the same SR-TSG rule. For example, the sampler collects all nodes that are annotated with S 0  X  NP 1 VP 2 , then updates those nodes to an-other subcategory such as S 0  X  NP 2 VP 0 according to the posterior distribution. This sampler is simi-lar to table label resampling (Johnson and Goldwa-ter, 2009), but differs in that our sampler can update multiple table labels simultaneously when multiple tables are labeled with the same elementary tree. The tree-level sampler also simultaneously updates blocks of latent variables associated with the type of SR-TSG rules, thus it can find MAP solutions effi-ciently. 4.2 Inference of Substitution Sites After the inference of symbol subcategories, we use Gibbs sampling to infer the substitution sites of parse trees as described in (Cohn and Lapata, 2009; Post and Gildea, 2009). We assign a binary variable to each internal node in the training data, which in-dicates whether that node is a substitution site or not. For each iteration, the Gibbs sampler works by sam-pling the value of each binary variable in random order. See (Cohn et al., 2010) for details.

During the inference, our sampler ignores the symbol subcategories of internal nodes of elementary trees since they do not affect the derivation of the SR-TSG. For example, the elementary trees  X (S 0 (NP 0 NNP 0 ) VP 0 ) X  and  X (S 0 (NP 1 NNP 0 ) VP 0 ) X  are regarded as being the same when we calculate the generation probabilities according to our model. This heuristics is help-ful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters { d ,  X  } as random vari-ables and update their values for every MCMC it-eration. We place a prior on the hyperparameters as follows: d  X  Beta (1 , 1) ,  X   X  Gamma (1 , 1) . The values of d and  X  are optimized with the auxiliary variable technique (Teh, 2006a). 5.1 Settings 5.1.1 Data Preparation
We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2 X 21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Hence-forth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The tree-bank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count  X  5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We also split off all the function tags and eliminated empty nodes from the data set, follow-ing (Johnson, 1998). 5.1.2 Training and Parsing
For the inference of symbol subcategories, we trained our model with the MCMC sampler by us-ing 6 split-merge steps for the full training set and 3 split-merge steps for the small training set. There-fore, each symbol can be subdivided into a maxi-mum of 2 6 = 64 and 2 3 = 8 subcategories, respec-tively. In each split-merge step, we initialized the sampler by randomly splitting every symbol in two subcategories and ran the MCMC sampler for 1000 iterations. After that, to infer the substitution sites, we initialized the model with the final sample from a run on the small training set, and used the Gibbs sampler for 2000 iterations. We estimated the opti-mal values of the stopping probabilities s by using the development set.
 We obtained the parsing results with the MAX-RULE-PRODUCT algorithm (Petrov et al., 2006) by using the SR-TSG rules extracted from our model. We evaluated the accuracy of our parser by brack-eting F1 score of predicted parse trees. We used EVALB 1 to compute the F1 score. In all our exper-iments, we conducted ten independent runs to train our model, and selected the one that performed best on the development set in terms of parsing accuracy. Table 2: Comparison of parsing accuracy with the small and full training sets. *Our reimplementation of (Cohn et al., 2010).
 Figure 2: Histogram of SR-TSG and TSG rule sizes on the small training set. The size is defined as the number of CFG rules that the elementary tree con-tains. 5.2 Results and Discussion 5.2.1 Comparison of SR-TSG with TSG
We compared the SR-TSG model with the CFG and TSG models as regards parsing accuracy. We also tested our model with three backoff hierarchy settings to evaluate the effects of backoff smoothing on parsing accuracy. Table 2 shows the F1 scores of the CFG, TSG and SR-TSG parsers for small and full training sets. In Table 2, SR-TSG ( P sr-tsg ) de-notes that we used only the topmost level of the hi-erarchy. Similary, SR-TSG ( P sr-tsg , P sr-cfg ) denotes that we used only the P sr-tsg and P sr-cfg backoff mod-els.
 outperformed both the CFG and TSG models on both the small and large training sets. This result suggests that the conventional TSG model trained from the vanilla treebank is insufficient to resolve development set (  X  100 ). structural ambiguities caused by coarse symbol an-notations in a training corpus. As we expected, sym-bol refinement can be helpful with the TSG model for further fitting the training set and improving the parsing accuracy.

The performance of the SR-TSG parser was strongly affected by its backoff models. For exam-ple, the simplest model, P sr-tsg , performed poorly compared with our best model. This result suggests that the SR-TSG rules extracted from the training set are very sparse and cannot cover the space of unknown syntax patterns in the testing set. There-fore, sophisticated backoff modeling is essential for the SR-TSG parser. Our hierarchical PYP model-ing technique is a successful way to achieve back-off smoothing from sparse SR-TSG rules to simpler CFG rules, and offers the advantage of automatically estimating the optimal backoff probabilities from the training set.
 We compared the rule sizes and frequencies of SR-TSG with those of TSG. The rule sizes of SR-TSG and TSG are defined as the number of CFG rules that the elementary tree contains. Figure 2 shows a histogram of the SR-TSG and TSG rule sizes (by unrefined token) on the small training set. For example, SR-TSG rules: S 1  X  NP 0 VP 1 and S 0  X  NP 1 VP 2 were considered to be the same to-ken. In Figure 2, we can see that there are almost the same number of SR-TSG rules and TSG rules with size = 1 . However, there are more SR-TSG rules than TSG rules with size  X  2 . This shows that an SR-TSG can use various large tree fragments depending on the context, which is specified by the symbol subcategories. 5.2.2 Comparison of SR-TSG with Other
We compared the accuracy of the SR-TSG parser with that of conventional high-performance parsers. Table 3 shows the F1 scores of an SR-TSG and con-ventional parsers with the full training set. In Ta-ble 3, SR-TSG (single) is a standard SR-TSG parser, and SR-TSG (multiple) is a combination of sixteen independently trained SR-TSG models, following the work of (Petrov, 2010).

Our SR-TSG (single) parser achieved an F1 score of 91.1%, which is a 6.4 point improvement over the conventional Bayesian TSG parser reported by (Cohn et al., 2010). Our model can be viewed as an extension of Cohn X  X  work by the incorporation of symbol refinement. Therefore, this result con-firms that a TSG and symbol refinement work com-plementarily in improving parsing accuracy. Com-pared with a symbol-refined CFG model such as the Berkeley parser (Petrov et al., 2006), the SR-TSG model can use large tree fragments, which strength-ens the probability of frequent syntax patterns in the training set. Indeed, the few very large rules of our model memorized full parse trees of sentences, which were repeated in the training set.

The SR-TSG (single) is a pure generative model of syntax trees but it achieved results comparable to those of discriminative parsers. It should be noted that discriminative reranking parsers such as (Char-niak and Johnson, 2005) and (Huang, 2008) are con-structed on a generative parser. The reranking parser takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a gen-erative model), and then reranks the candidates us-ing arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative reranking parser would provide better performance than SR-TSG alone.

Recently, (Petrov, 2010) has reported that com-bining multiple grammars trained independently gives significantly improved performance over a sin-gle grammar alone. We applied his method (referred to as a TREE-LEVEL inference) to the SR-TSG model as follows. We first trained sixteen SR-TSG models independently and produced a 100-best list of the derivations for each model. Then, we erased the subcategory information of parse trees and se-lected the best tree that achieved the highest likeli-hood under the product of sixteen models. The com-bination model, SR-TSG (multiple), achieved an F1 score of 92.4%, which is a state-of-the-art result for the WSJ parsing task. Compared with discriminative reranking parsers, combining multiple grammars by using the product model provides the advantage that it does not require any additional training. Several studies (Fossum and Knight, 2009; Zhang et al., 2009) have proposed different approaches that in-volve combining k-best lists of candidate trees. We will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG and SR-TSG. TSG is weakly equivalent to CFG and generates the same set of strings. For example, the TSG rule  X  X   X  (NP NNP) VP X  with probability p can be converted to the equivalent CFG rules as fol-lows:  X  X   X  NP NNP VP  X  with probability p and  X  X P NNP  X  NNP X  with probability 1 . From this viewpoint, TSG utilizes surrounding symbols (NNP of NP NNP in the above example) as latent variables with which to capture context information. The search space of learning a TSG given a parse tree is O (2 n ) where n is the number of internal nodes of the parse tree. On the other hand, an SR-CFG utilizes an arbitrary index such as 0 , 1 ,... as latent variables and the search space is larger than that of a TSG when the symbol refinement model allows for more than two subcategories for each symbol. Our experimental results comfirm that jointly modeling both latent variables using our SR-TSG assists accu-rate parsing. We have presented an SR-TSG, which is an exten-sion of the conventional TSG model where each symbol of tree fragments can be automatically sub-categorized to address the problem of the condi-tional independence assumptions of a TSG. We pro-posed a novel backoff modeling of an SR-TSG based on the hierarchical Pitman-Yor Process and sentence-level and tree-level blocked MCMC sam-pling for training our model. Our best model sig-nificantly outperformed the conventional TSG and achieved state-of-the-art result in a WSJ parsing task. Future work will involve examining the SR-TSG model for different languages and for unsuper-vised grammar induction.
 We would like to thank Liang Huang for helpful comments and the three anonymous reviewers for thoughtful suggestions. We would also like to thank Slav Petrov and Hui Zhang for answering our ques-tions about their parsers.
