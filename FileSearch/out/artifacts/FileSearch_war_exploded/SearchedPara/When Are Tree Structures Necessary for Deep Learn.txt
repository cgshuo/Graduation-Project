 Deep learning based methods learn low-dimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text.

For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compo-sitional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: re-current models and recursive models:
Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were ap-plied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with pre-viously built embeddings. Recurrent models can be extended to bidirectional ones from both left-to-right and right-to-left. These models generally consider no linguistic structure aside from word order.

Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequen-tially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious , following the operation sequence ( (the food) (is delicious) ) rather than the sequen-tial order (((the food) is) delicious) . Many recur-sive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bow-man, 2013; Bowman et al., 2014), sentiment anal-ysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014).
One possible advantage of recursive models is their potential for capturing long-distance depen-dencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its correspond-ing direct object can be far away in terms of to-kens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). However we do not know if this advan-tage is truly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is rela-tively slow, domain-dependent, and can be error-ful.

On the other hand, recent progress in multi-ple subfields of neural NLP has suggested that re-current nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequence-to-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), pars-ing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentiment-bank dataset.

Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive mod-els offer specific advantages. We investigate four tasks with different properties.  X  Binary sentiment classification at the sen- X  Phrase Matching on the UMD-QA dataset  X  Semantic Relation Classification on the  X  Discourse parsing (RST dataset) is useful
The principal motivation for this paper is to un-derstand better when, and why, recursive models are needed to outperform simpler models by en-forcing apples-to-apples comparison as much as possible. This paper applies existing models to existing tasks, barely offering novel algorithms or tasks. Our goal is rather an analytic one, to inves-tigate different versions of recursive and recurrent models. This work helps understand the limita-tions of both classes of models, and suggest direc-tions for improving recurrent models.

The rest of this paper organized as follows: We detail versions of recursive/recurrent models in Section 2, present the tasks and results in Section 3, and conclude with discussions in Section 4. 2.1 Notations We assume that the text unit S , which could be a phrase, a sentence or a document, is com-prised of a sequence of tokens/words: S = { w 1 ,w 2 ,...,w N ber of tokens in S . Each word w is associated with a K-dimensional vector embedding e w = w ,e 2 w ,...,e K w } . The goal of recursive and re-current models is to map the sequence to a K-dimensional e S , based on its tokens and their cor-respondent embeddings.
 Standard Recurrent/Sequence Models suc-cessively take word w t at step t , combines its vec-tor representation e t with the previously built hid-den vector h t  X  1 from time t  X  1 , calculates the re-sulting current embedding h t , and passes it to the next step. The embedding h t for the current time t is thus: where W and V denote compositional matrices. If N s denotes the length of the sequence, h N s repre-sents the whole sequence S .
 Standard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order. It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. For a given node  X  in the tree and its left child  X  left (with representation e left ) and right child  X  right (with representation e right ), the standard recursive network calculates e  X  as follows: Bidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent frame-work where embeddings for each time are calcu-lated both forwardly and backwardly: Normally, final representations for sentences can be achieved either by concatenating vectors calcu-lated from both directions [ e  X  1 ,e  X  ther compositional operation to preserve vector di-mensionality where W L denotes a K  X  2 K dimensional matrix. Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = { x 1 ,x 2 ,...,x n timestep with an input, memory and output gate, respectively denoted as i t , f t and o t . We notation-ally disambiguate e and h : e t denotes the vector for individual text units (e.g., word or sentence) at time step t, while h t denotes the vector computed by the LSTM model at time t by combining e t and h t  X  1 .  X  denotes the sigmoid function. The vector representation h t for each time-step t is given by: phrase/sentence level are predicted representations outputted from the last time step.
 Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees.
 Bi-directional LSTMs These combine bi-directional models and LSTMs. In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks.  X  Binary sentiment classification on the Pang  X  Sentiment Classification on the Stanford  X  Sentence-Target Matching on the UMD- X  Semantic Relation Classification on the  X  Discourse Parsing (Li et al., 2014; Hernault
In each case we followed the protocols de-scribed in the original papers. We first group the algorithm variants into two groups as follows:  X  Standard tree models vs standard sequence  X  LSTM tree models, LSTM sequence models We employed standard training frameworks for neural models: for each task, we used stochas-tic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011). Pa-rameters are tuned using the development dataset if available in the original datasets or from cross-validation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 pe-nalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated.
For settings where no repeated experiments are performed, the bootstrap test is adopted for sta-tistical significance testing (Efron and Tibshirani, 1994). Test scores that achieve significance level of 0.05 are marked by an asterisk (*). 3.1 Stanford Sentiment TreeBank Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013). This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words.

Of course, any conclusions drawn from imple-menting sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected. Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations.

For recursive models, we followed the proto-cols in Socher et al. (2013) where node embed-dings in the parse trees are obtained from recur-sive models and then fed to a softmax classifier. We transformed the dataset for recurrent model use as illustrated in Figure 1. Each phrase is recon-structed from parse tree nodes and treated as a sep-arate data point. As the treebank contains 11,855 sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances).
 Table 1: Test set accuracies on the Stanford Senti-ment Treebank at root level.
 Table 2: Test set accuracies on the Stanford Senti-ment Treebank at phrase level.
 comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bi-sequences), but yields no significant improvement at the phrase level.
 LSTM Tai et al. (2015) discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models. We explore this task a bit more by training deeper and more sophisticated models. We exam-ine the following three models: 1. Tree-structured LSTM models (Tai et al., 2. Deep Bi-LSTM sequence models (denoted as 3. Deep Bi-LSTM hierarchical sequence mod-Figure 2: Illustration of two sequence models. A, B, C, D denote clauses or sub sentences separated by punctuation.
We consider the third model because the dataset used in Tai et al. (2015) contains long sentences and the evaluation is performed only at the sen-tence root level. Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advan-tage could be approximated by using punctuation-based approximations to clause boundaries.

We run 15 iterations for each algorithm. Pa-rameters are harvested at the end of each itera-tion; those performing best on the development set are used on the test set. The whole process takes roughly 15-20 minutes on a single GPU ma-did not use the bootstrap test where parallel ex-amples are generated from one same dataset. In-stead, we repeated the aforementioned procedure for each algorithm 20 times and report accuracies with standard deviation in Table 3.
 Table 3: Test set accuracies on the Stanford Sen-timent Treebank with deviations. For our exper-iments, we report accuracies over 20 runs with standard deviation.

Tree LSTMs are equivalent or marginally bet-ter than standard bi-directional sequence model (two-tailed p-value equals 0.041*, and only at the root level, with p-value for the phrase level at 0.376). The hierarchical sequence model achieves the same performance with a p-value of 0.198. Discussion The results above suggest that clausal segmentation of long sentences offers a slight performance boost, a result also supported by the fact that very little difference exists between the three models for phrase-level sentiment eval-uation. Clausal segmentation of long sentences thus provides a simple approximation to parse-tree based models.

We suggest a few reasons for this slightly better performances introduced by clausal segmentation: 1. Treating clauses as basic units (to the extent 2. Semantic compositions such as negations or 3. Errors are back-propagated to individual to-Figure 3: Sentiment prediction using a one-directional (left to right) LSTM. Decisions at each time step are made by feeding embeddings calcu-lated from the LSTM into a softmax classifier. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into train-ing(8101)/dev(500)/testing(2000). No pre-training procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package 4 . Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don X  X  parse trees help on this task? One possible explanation is the distance Table 4: Test set accuracies on the Pang X  X  senti-ment dataset using Standard model settings. of the supervision signal from the local composi-tional structure. The Pang et al. dataset has an av-erage sentence length of 22.5 words, which means it takes multiple steps before sentiment related ev-idence comes up to the surface. It is therefore un-clear whether local compositional operators (such as negation) can be learned; there is only a small amount of training data (around 8,000 examples) and the sentiment supervision only at the level of the sentence may not be easy to propagate down to deeply buried local phrases. 3.3 Question-Answer Matching Task Description: In the question-answering phrase. The task is different from standard gener-ation focused QA task but formalized as a multi-class classification task that matches a source question with a candidates phrase from a prede-fined pool of candidate phrases We give an illus-trative example here:
Question : He left unfinished a novel whose title character forges his father X  X  signature to get out of school and avoids the draft by feigning desire to join. Name this German author of The Magic Mountain and Death in Venice.

Answer : Thomas Mann from the pool of phrases. Other candidates might include George Washington, Charlie Chaplin, etc.

The model of Iyyer et al. (2014) minimizes the distances between answer embeddings and node embeddings along the parse tree of the question. Concretely, let c denote the correct answer to ques-tion S , with embedding ~c , and z denoting any ran-dom wrong answer. The objective function sums over the dot product between representation for every node  X  along the question parse trees and the answer representations: L = where e  X  denotes the embedding for parse tree node calculated from the recursive neural model. Here the parse trees are dependency parses follow-ing (Iyyer et al., 2014).

By adjusting the framework to recurrent mod-els, we minimize the distance between the answer embedding and the embeddings calculated from each timestep t of the sequence: L = At test time, the model chooses the answer (from the set of candidates) that gives the lowest loss score. As can be seen from results presented in Table 5, the difference is only significant for the LSTM setting between the tree model and the sequence model; no significant difference is ob-served for other settings.
 Table 5: Test set accuracies for UMD-QA dataset. Discussion The UMD-QA task represents a group of situations where because we have in-sufficient supervision about matching (it X  X  hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)).
The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree mod-els and different time steps for sequence models), components from sequence models are able to em-bed important information, despite the fact that se-quence model components are just sentence frag-ments and hence usually not linguistically mean-ingful components in the way that parse tree con-stituents are. 3.4 Semantic Relationship Classification Task Description: SemEval-2010 Task 8 (Hen-drickx et al., 2009) is to find semantic rela-tionships between pairs of nominals, e.g., in  X  X y [apartment] e1 has a pretty large [kitchen] e2  X  classifying the relation between [apartment] and [kitchen] as component-whole . The dataset con-tains 9 ordered relationships, so the task is formal-ized as a 19-class classification problem, with di-rected relations treated as separate labels; see Hen-drickx et al. (2009; Socher et al. (2012) for details.
For the recursive implementations, we follow the neural framework defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calcu-lated based on recursive models and fed to a soft-for the recurrent models as shown in Figure 5. Figure 4: Illustration of Models for Semantic Re-lationship Classification.
 Discussion Unlike for earlier tasks, here recur-sive models yield much better performance than the corresponding recurrent versions for all ver-sions (e.g., standard tree vs. standard sequence, p = 0 . 004 ). These results suggest that it is the need to integrate structures far apart in the sen-tence that characterizes the tasks where recursive models surpass recurrent models. In parse-based models, the two target words are drawn together much earlier in the decision process than in recur-rent models, which must remember one target un-til the other one appears. 3.5 Discourse Parsing Task Description: Our final task, discourse parsing based on the RST-DT corpus (Carlson et Table 6: Test set accuracies on the SemEval-2010 Semantic Relationship Classification task.
 Figure 5: An illustration of discourse parsing. [ e 1 ,e 2 ,... ] denote EDUs (elementary discourse units), each consisting of a sequence of tokens. [ r 12 ,r 34 ,r 56 ] denote relationships to be classified. A binary classification model is first used to decide whether two EDUs should be merged and a multi-class classifier is then used to decide the relation type. al., 2003), is to build a discourse tree for a doc-ument, based on assigning Rhetorical Structure Theory (RST) relations between elementary dis-course units (EDUs). Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compo-sitional meaning than sentiment or nominal rela-tions. See Hernault et al. (2010) for more details on discourse parsing and the RST-DT corpus.
Representations for adjacent EDUs are fed into binary classification (whether two EDUs are re-lated) and multi-class relation classification mod-els, as defined in Li et al. (2014). Related EDUs are then merged into a new EDU, the representa-tion of which is obtained through an operation of neural composition based on the previous two re-lated EDUs. This step is repeated until all units are merged.

Discourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sen-tences, with an average length of 7.2 words. Re-cursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for dis-course parsing. We use this task for two rea-sons: (1) to illustrate whether syntactic parse trees are useful for acquiring representations for short clauses. (2) to measure the extent to which pars-ing improves discourse tasks that need to combine the meanings of larger text units.

Models are traditionally evaluated in terms of fying the rhetorical relation between two clauses. Due to space limits, we only focus the last one, rhetorical relation identification, because (1) rela-tion labels are treated as correct only if spans and nuclearity are correctly labeled (2) relation identi-fication between clauses offer more insights about model X  X  abilities to represent sentence semantics. In order to perform a plain comparison, no addi-tional human-developed features are added.
 Table 7: Test set accuracies for relation identifica-tion on RST discourse parsing data set.
 Discussion We see no large differences between equivalent recurrent and recursive models. We suggest two possible explanations. (1) EDUs tend to be short; thus for some clauses, parsing might not change the order of operations on words. Even for those whose orders are changed by parse trees, the influence of short phrases on the final represen-tation may not be great enough. (2) Unlike earlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm pre-sented here adopts additional levels of neural com-position during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs. We compared recursive and recurrent neural mod-els for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014).

As with any comparison between models, our results come with some caveats: First, we ex-plore the most general or basic forms of recur-sive/recurrent models rather than various sophis-ticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of lay-ers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neural compositions X  X espite the fact that deep neural models with multiple layers give better results. Our conclusions might thus be limited to the al-gorithms employed in this paper, and it is unclear whether they can be extended to other variants or to the latest state-of-the-art. Second, in order to compare models  X  X airly X , we force every model to be trained exactly in the same way: AdaGrad with minibatches, same set of initializations, etc. How-ever, this may not necessarily be the optimal way to train every model; different training strategies tailored for specific models may improve their per-formances. In that sense, our attempts to be  X  X air X  in this paper may nevertheless be unfair.

Pace these caveats, our conclusions can be sum-marized as follows:  X  In tasks like semantic relation extraction, in  X  Tree models tend to help more on long se- X  On long sequences where supervision is not  X  In cases where tree-based models do well, a  X  Despite that the fact that components (out-We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, ad-vice, and suggestions. We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful ad-vice on various aspects of this work. We acknowl-edge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully ac-knowledge support from an Enlight Foundation Graduate Fellowship, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Lab-oratory (AFRL) contract no. FA8750-13-2-0040, and the NSF via award IIS-1514268. Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect the views of Bloomberg L.P., DARPA, AFRL, NSF, or the US government.

