 Clustering text documents into different category groups is an important step in indexing, retrieval, management and mining of abundant text data on the Web or in corporate information systems. Among others, the challenging problems of text clustering are big volume, high dimensionality and complex semantics. In this paper we are interested in solutions to the first two problems while use of ontology provides promising solutions to the third problem [1].
 &lt;t 1 ,t 2 ,  X  X  X  ,t n &gt; . A set of documents are represented as a matrix where each row indicates a document and each column represents a term or word in the vocabulary of the document set. In this model, clustering algorithms such as the Standard KMeans [2] and its varieties [3, 4], as well as the hierarchical clus-tering methods [5, 6], are used to cluster text data. In many real applications, the vocabulary and the number of documents are very large, which results in a very large matrix. On the other hand, the clusters in a document set are catego-rized by different subsets of terms, which makes the matrix sparse. The sparsity is dependent of the differences of semantics of the clusters in the document set.
 rithms to be efficient, scalable and able to discover clusters from subspaces of the v ector space model (VSM). Scalable subspace clustering methods are made good candidates for text clustering [7], while other clustering algorithms often fail to produce satisfactory clustering results.
 algorithm, denoted as FW-KMeans , to cluster text data [8, 9]. FW-KMeans is a subspace clustering algorithm that identifies clusters from subspaces by au-tomatically assigning large weights to the variables that form the subspaces in which the clusters are formed. The new algorithm is based on the extensions to the standard k -means algorithm so it is efficient and scalable to large data set. We propose a modification to the original FW-KMeans to handle highly sparse text data where many words do not appear in documents of certain categories. This situation makes the original FW-KMeans unsolvable because the weights for these terms turn to infinite. By introducing a constant  X  to the distance function, the problem is solved, the convergence of the algorithm is guaranteed, and its efficiency is preserved.
 because  X  can affect the significance of the feature weights of the modified FW-KMeans . We have used different data sets from the 20-Newsgroups to test the clustering performance of the new algorithm, and compared our results with those of the Standard KMeans and Bisection KMeans algorithms. The experi-mental results from different data sets have shown that our new algorithm out-performed the others. Beyond the clustering performance, the other advantage of the new algorithm is able to identify a subset of key words in each clus-ter. We present analysis of these key words and show how they can be used to present the semantics of clusters which can help understand the discovered clusters.
 k -means algorithm in [8] to cluster text documents, while the sparsity problem was not discussed. The concept vector approach [11] is similar to the subset of features identified with weights in our proposed method. However, the con-cept vector for each cluster was obtained by associating with a word cluster that was separately generated. This process potentially affects the running time and complexity. Besides, the clustering algorithm used is the Standard Spherical KMeans .
 clustering with the feature weighting k -means algorithm and presents a modifica-tion to handle the sparsity problem in text clustering. Section 3 defines clustering evaluation methods that are used in experiments. The comparison studies and feature analysis are presented in Section 4. Finally, we draw some conclusions and point out future work in Section 5. In the VSM , a set of documents are represented as a set of vectors X = {X ( t ,t 2 ,...,t m ). Here, the terms can be considered as the features of the vector space and m as the number of dimensions representing the total number of terms in the vocabulary. Assume that several categories exist in X , each category of documents is characterized by a subset of terms in the vocabulary that corre-sponds to a subset of features in the vector space. In this sense, we say that a cluster of documents is situated in a subspace of the vector space.
 that the clustering algorithm has the capability of subspace clustering. The fea-ture weighting k -means algorithm that we have recently developed [9] and also reported by others [8] is able to cluster data in a subspace by automatically weighting features in the k -means clustering process. Using the k -means cluster-ing process, the new algorithm clusters n objects into k clusters by minimizing the following objective function: where d ( z l,i ,x j,i ) is a dissimilarity measure between object X j and cluster center Z l in feature i ; w l,j = 1 indicates that object j is assigned to cluster l and otherwise w l,j =0;  X  l,i is the weight to feature i in cluster l ;and  X &gt; 1isa given parameter.
 algorithm. Each feature weight  X  is solved by: where  X  w l,j and  X  z l,i are the known values obtained from the previous iterative steps. (refer to [8, 9] for details of the clustering algorithm.) m weights are assigned to m features and the weight of a feature is inversely proportional to the dispersion of values of that feature. The larger the dispersion, the smaller the weight. This indicates that the values of a good feature in a cluster are very close to the value of the cluster center in that feature. In text clustering, this implies that a good term or word appears in the majority of the documents of a cluster with similar frequency. Therefore, a large weight identifies a key term in a cluster.
 One is that a word does not occur in any document in that cluster and the other is that the word appears in each document with the same frequency. Table 1 shows examples of the two cases where the term t 4 appears in each document of the first cluster two times and the term t 3 does not appear in any document in the first cluster. To calculate the weights for these two terms, their weights  X  become infinite so the objective function (1) cannot be minimized properly. constant  X  to the dissimilarity measure as below: Fixing  X  W and  X  Z and using the Lagrange multiplier technique to minimize F 1 with respect to  X  , we obtain We can easily verify that m i =1  X  l,i =1and1  X  l  X  k .
 be zero so all  X  l,i can be calculated in (4). The features with zero dispersion will have the maximal weight in the cluster, while the weights of other features will be smaller, depending on the value of the dispersion. For example in Table 1, the features t 3 and t 4 will have the largest weight in the first cluster. To identify the cluster, term t 4 is apparently more important than term t 3 . The two different terms can be easily separated in post-processing. When extracting important features to represent different clusters, we remove t 3 type features but retain t 4 type features.
 discuss how to choose  X  because it will affect the values of weights. From (4), by  X  and  X  l,i will approach to 1 m . This will make the clustering process back to the standard k -means. If  X  is too small, then the gap of the weights between the zero dispersion features and other important features will be big, therefore, undermining the importance of other features.
 set for all features as follows: where o i is the mean feature value of the entire data set. In practice we use a sample instead of the entire data set to calculate  X  . (5% sample is used ac-cording to the sampling theory [12].) Experimental results have shown that this selection of  X  is reasonable to produce satisfactory clustering results and identify important features of clusters.
 method has the following two major advantages: 1. It is efficient and scalable to cluster large and sparse text data in subspaces. 2. From the weights, the subset of key words in each cluster can be identified, In this work we use four different external cluster validation methods to evaluate the clustering performance of our approach in clustering real world text data and compare our results with the results of other clustering methods. They are accuracy , entropy , F1 score ( FScore ) [6], and normalized mutual information ( NMI ) [13] which are defined as follows.
 it into k clusters S l , where 1  X  l, h  X  k .Let n h , n l be the numbers of documents in class C h andincluster S l respectively, n h,l be the number of documents appearing in both class C h and cluster S l , n be the total number of documents in the data set, and k is the number of clusters equal to the number of classes. Table 2 shows the four evaluation functions used in this paper: 4.1 Text Datasets To demonstrate the effectiveness of the FW-KMeans on different structured text data, we built 6 datasets from the 20-Newsgroups collection 1 with different characteristics in sparsity, dimensionality and class distribution. each dataset and n d indicates the number of documents in each class. Data sets A2 and A4 contain categories with very different topics while datasets B2 and B4 consist of categories in similar topics. Sparsity of the former datasets is bigger than that of the later datasets because there are more overlapping words in the later datasets to describe the similar topics. Datasets A4-U and B4-U contain unbalanced classes.
 steps include removing the headers, the stop words, and the words that occur in less than three documents or greater than the average number of documents in each class, as well as stemming the left words with the Porter stemming function. The standard tf  X  idf term weighting was used to represent the document vector. 4.2 Cluster Analysis We used three k -means type algorithms, FW-KMeans , Bisection-KMeans and Standard KMeans to cluster the 6 datasets. Table 4 shows the clustering results evaluated in the 4 evaluation measures defined in Section 3. Since the k -means type algorithms are known to be sensitive to the choice of an initial partition, for sparse and high-dimensional text data, randomly selecting initial cluster centers usually does not lead to a good clustering. In these experiments, we first ran-domly sampled 5% of documents from a data set and used the farthest k points between two classes in the sample data as the initial center for each cluster [15]. Experimental results have shown that this initialization strategy performed well. datasets. The 4 figures in each cell represent the values of Accuracy, Entropy, Fscore and NMI respectively. We can see that FW-KMeans performed the best in most cases. For the balanced datasets, Standard KMeans was worst. The Bisection-KMeans performed slightly better than the FW-KMeans on A2 and A4 which are less overlap because the classes in them are separate with each other, therefore sharing the small set of similar words. For the datasets B2 and B4, the FW-KMeans performed much better because of its capability of subspace clustering by feature weighting.
 performed reasonably well while the performance of Bisection-KMeans clearly deteriorated. This was because the Bisection-KMeans needs to choose a branch to split at each step, and usually, the largest cluster is chosen. This resulted mistake could not be corrected in the later stage. This can be shown by the following two confusion matrices from dataset B4-U. The large classes C 0 and C 1 were divided into separate clusters by the Bisection-KMeans . However, the FW-KMeans algorithm recovered them accurately.
 4.3 Feature Analysis Equation (4) in Section 2 shows that  X  l,i is inversely related to the ratio of the dispersion along feature i to the total dispersion of all features in cluster l .The more compact (smaller dispersion) the cluster is along feature i , the bigger the weight of feature i . This implies that the term or word of feature i appears evenly in all documents of the cluster. Therefore, this word is an important identifier for this cluster.
 the important terms or words in each cluster. However, because of the special case of zero dispersion in certain features, the largest weights may identify some words which do not occur in the documents of the cluster. In this case we ignored these words. In fact, in sparse text data, many of such words can be identified. For example, after preprocessing we got 1322 features for dataset B4. In the 4 clusters generated by the FW-KMeans algorithm, 381 words do not appear in the first cluster and 363, 318 and 301 features do not appear in the documents of other 3 clusters respectively. The percentage is a little less than 30%. Although the weights for these features are large, they do not represent the semantics of the cluster.
 divided the rest words into groups according to the intervals of the weights. The left table of Figure 1 shows the number of words in each interval and the right figure plots the distribution of words in category comp.graphics of dataset B4. Most words have relatively small weights (over 75%). Given a weight threshold we identified 220 words that we considered important. This is less than 17% of the total words. These are the words which contributed most to the semantics of the cluster so we can use these words to interpret the cluster.
 2. The horizontal axis is the index of the 220 words and the vertical lines indicate the values of the weights. It is clear that each cluster has its own subset of key words because the lines do not have big overlapping in different clusters. Category groups comp.graphics and comp.os.ms-windows have some overlapping because the two topics are close to each other. So do the topics rec.autos and sci.electronics . However, we can still distinguish them easily.
 words have larger weights and are noun. They are strongly related to the topic of each cluster. They were identified based on the weights and the word functions in sentences. In fact, they can also be manually identified interactively from the left side graph. It is clear that each set of words is essentially correlated to only one of the four topics: comp.graphics , comp.os.ms-windows , rec.autos and sci.electronics . However, some high-weight words can be related to more than one topic if the topics are close to each other. For example, word  X  X equest X  has higher weight in two clusters but the topics of the two clusters are closely related (Graphics and Windows). We remark that the words identified by the weights and function analysis can improve the interpretability of the clustering results. scoring each feature with the foil gain function [16] and retaining only the top-scoring features for every cluster. Then, we compared them with the features identified by high weights. The result is shown in Table 6. The first line shows the precision of each cluster after performing the FW-KMeans algorithm. Here, C is the set of the important words extracted by FW-KMeans . A is the first | C | words with higher foil-gain scores obtained by the original class label. B is the first | C | words with higher foil-gain scores obtained by the cluster. The entry terms show the percentage of A  X  B and A  X  C . From the table 5 we can see that most of the words with higher weights also have higher foil-gain scores. This further verifies that the words identified by high weights are important. In this paper we have discussed the method to use the FW-KMeans algorithm to cluster text data in high dimensionality and sparsity. We have presented the modification to the original FW-KMeans to solve the sparsity problem that occurs in text data where different sets of words appear in different clusters. The capability of subspace clustering of the FW-KMeans algorithm has a clear advantage in clustering such text data. The experiment results have shown that the subspace clustering method was superior to the standard k -means and the Bisection-KMeans that cluster data on the entire space.
 on very large text data. We also plan to integrate ontology as background knowl-edge to enhance our method in text clustering and mining. The ontology will sever several purposes in the clustering process, including data preprocessing, selection of initial cluster centers, determination of the number of clusters k , and interpretation of clustering results.

