
Huge datasets are becoming prevalent; even as re-searchers, we now routinely have to work with datasets that are up to a few terabytes in size. Interesting real-world ap-plications produce huge volumes of messy data. The mining process involves several steps, starting from pre-processing the raw data to estimating the final models.

As data become more abundant, scalable and easy-to-use tools for distributed processing are also emerging. Among those, Map-Reduce has been widely embraced by both academia and industry. In database terms, Map-Reduce is a simple yet powerful execution engine, which can be complemented with other data storage and manage-ment components, as necessary.

In this paper we describe our experiences and findings in applying Map-Reduce, from raw data to final models, on an important mining task. In particular, we focus on co-clustering, which has been studied in many applications such as text mining, collaborative filtering, bio-informatics, graph mining. We propose the Dis tributed Co -clustering ( DisCo ) framework, which introduces practical approaches for distributed data pre-processing, and co-clustering. We develop DisCo using Hadoop, an open source Map-Reduce implementation. We show that DisCo can scale well and efficiently process and analyze extremely large datasets (up to several hundreds of gigabytes) on commodity hardware.
It X  X  a clich  X  e, but it X  X  true: huge volumes of data are col-lected and need to be processed on a daily basis. For ex-ample, Google now processes an estimated 20 petabytes of data per day [13] and the Internet Archive 1 is growing at 20 terabytes a month, having reached 2 petabytes sometime in 2006. Retail giants such as Walmart and online shop-ping stores such as Amazon and eBay all deal with with petabytes of transactional data every day.

By definition, research on data mining focuses on scal-able algorithms applicable to huge datasets. But let X  X  take things from the beginning. Natural sources of data pro-vide them in vast quantities, but impure form. A repository may consist of, e.g., a corpus of text documents, a large web crawl, or system logs. Schemas do not arise sponta-neously in nature. On the contrary, significant effort must be invested to make the data fit a given schema. Most com-monly, data are collected in a multitude of unstructured or semi-structured formats. Aspects of the data that are rele-vant to the task at hand need to be extracted and stored in an appropriate representation. Most researchers start with the assumption that the input is in the appropriate form. How-ever, getting the data into the right form is not trivial (see detailed discussion in Section 3).

Map-Reduce [12] is attracting a lot of attention, prov-ing both a source for inspiration [30] as well as target of polemic [14] by prominent researchers in databases. Re-cently, some have questioned whether relational DBMSes are appropriate for any and all data management tasks un-der the sun [35, 34]. Moreover, [34] makes a strong case that bundling data storage, indexing, query execution, trans-action control, and logging components into a monolithic system with a veneer of SQL is not always desirable. Start-ing from this call for a component-based approach, Map-Reduce is an execution engine, largely unconcerned about data models and storage schemes. In the simplest case, data reside on a distributed file system [19, 1, 26] but nothing prevents pulling data from a large data store like BigTable [7, 2, 38], or any other storage engine that (i) provides data de-clustering and replication across many machines, and (ii) allows computations to execute on local copies of the data. Arguably, Map-Reduce is powerful both for the features it provides, as well as for the features it omits, in order to pro-vide a clean and simple programming abstraction.
Hadoop is an open source implementation of the core components necessary for Map-Reduce. It focuses on pro-viding the necessary minimum functionality, combining simplicity of use with scalable performance 2 . However, if additional functionality is needed by an application, other open source components are available, which address e.g., key-based data access [2], or more complex job and data schema management [37, 3].

In the context we have so far described, this paper describes our experiences and findings in applying the above end-to-end philosophy and tools to a particular prob-lem. More specifically, we focus on co-clustering or bi-clustering [24, 8] of pairwise relationships extracted from the raw data. The natural format for the relevant data fea-tures, i.e., the graph of associations between different en-tities, is a sparse adjacency matrix representation. Co-clustering provides a general set of tools to simultaneously cluster both rows and columns into groups, based on cer-tain criteria. Unlike clustering which groups similar rows or columns independently, co-clustering searches for sub-matrices of rows and columns that are inter-related. Co-clustering has been studied in many different applications including text mining [15, 28], bioinformatics [24, 8], rec-ommendation systems [18], and graph mining [6].

Powerful as it is, co-clustering is not practical to ap-ply on large matrices (e.g., several millions of rows and columns). This paper proposes a comprehensive Distributed Co-clustering (DisCo) solution from the raw data to the end clusters. In particular, we leverage the highly success-ful Map-Reduce [13] both as a programming model and as an implementation testbed. More specifically, we de-velop DisCo using Hadoop [1], an open source package which includes a freely available implementation of Map-Reduce and has been widely embraced by both commercial and academic worlds. DisCo is a scalable framework un-der which various co-clustering algorithms can be imple-mented. Since both data pre-processing (i.e., graph extrac-tion) and co-clustering components need efficient sequen-tial scans over the entire data set, we only need to use the core Hadoop components.

The contributions of this paper are:  X  We present a pragmatic data mining process that in- X  We design a complete distributed co-clustering solu- X  We demonstrate its scalability and power on mining As is often the case, none of the individual steps is surpris-ing in and of itself. However, we believe that the entire data mining process needs to be studied under the currently available tools for large-scale data processing. This paper illustrates our experiences, insights as well as common pat-terns on using Map-Reduce (Hadoop) for data mining, from the very beginning to the very end and aims to clarify some common misconceptions.

The rest of the paper is organized as follows: Section 2 provides a very brief tutorial introduction of Map-Reduce, as well as the key components it relies on. Section 3 presents a distributed data mining framework. Section 4
Figure 1. Scalability for data pre-processing: processing a 350GB logfile takes about 7 minutes on 39 nodes. presents our design for distributed co-clustering using Map-Reduce and Section 5 evaluates its scalability and presents results on real-world data sets. Section 6 briefly reviews re-lated work, both in systems as well as data mining. Finally, Section 7 concludes.
Map-Reduce, originally described in [12], is a core com-ponent in an emerging ecosystem of distributed, scalable, fault-tolerant data storage, management, and processing tools [19, 7, 2, 3, 37, 26, 38, 31]. Map-Reduce is essentially a distributed grep-sort-aggregate or, in database terminol-ogy, a distributed execution engine for select-project via se-quential scan, followed by hash partitioning and sort-merge group-by. It is ideally suited for data already stored on a distributed file system which offers data replication as well as the ability to execute computations locally on each data node. There are two important aspects in Map-Reduce, the programming model and the distributed execution frame-work. We examine those next, after first introducing a sim-ple example we shall use.
Assume that we have network router log data, consisting of text lines such as and we wish to extract an adjacency list of source-destination IP pairs. We could achieve this with the fol-lowing snippet in Python:
We choose Python merely for illustration, to exemplify the simplicity of the underling concepts. The Python state-ments should easily map to similar constructs in other mod-ern scripting languages, such as Perl and Ruby. Iterating over a file object ( input ) will yield a sequence of lines. The ip_mapper function will parse one line and return a source-destination pair. The call to map takes as input a se-quence of lines and produces another sequence, of IP pairs. The second element is a set, for reasons that will become clear shortly.

The reduce operation accumulates all elements of an input sequence. Here, the accumulator is a dictionary ( graph ) which stores a mapping between a source IP (key) and set of destination IPs (value). The accumulation func-tion is graph_reducer . Thus, the call to reduce above takes as input the sequence of IP pairs produced by map and outputs a dictionary of key-value pairs, where keys are source IPs and values are lists of destination IPs.
Even though this program structure is relatively simple, it is sufficiently general for many tasks [12, 10]. Map-Reduce allows users to execute such computations on data stored in a cluster with up to thousands of processors and petabytes of storage, while requiring effort similar to that of writing the above Python program.
As it X  X  name suggests, Map-Reduce draws from a well-established abstraction in functional programming. The previous example illustrates most of the programming model aspects. Formally, a computation is decomposed into a map operation followed by a reduce operation. These are specified by two functions, Both operate on key-value pairs, which we denote using an-gle brackets  X  k, v  X  . The key is used primarily in the reduc-tion step, to determine which values are grouped together. Values may carry arbitrary information.
This abstract computation needs to be eventually exe-cuted on a large cluster. In this section we focus on the data flow model (see Figure 2a). The map input is be parti-tioned into a number of input splits . Processing each split is assigned to one map task . Subsequently, all map outputs are partitioned among a number of reduce tasks , by hashing on the intermediate key k int . Each reducer receives one part of
Figure 2. Overview of the Map-Reduce execu-tion framework [12]. the intermediate key space. Subsequently, it merges all in-puts received from all mappers, sorts them based on k int group equal keys together, and applies the reducer function to obtain the final results.
One important feature is that the storage cluster may par-tially or completely overlap with the compute cluster. This is also true even when more sophisticated distributed data stores [7] are used. Therefore, computation tasks can be ex-ecuted on machines hosting local copies of the input data. Map-Reduce is essentially a simple and clean framework that allows a large class of computations to be transparently executed in such a cluster architecture. Map-Reduce resem-bles the concept of active disks [32], although the actual design and implementations are very different.

Figure 2b illustrates one possible placement of the ele-ments from Figure 2a (both data chunks, as well as com-putation tasks) onto cluster machines. In this simple illus-tration, it is possible to place all map tasks on machines hosting a local copy of their input split. If this is not pos-sible, then data will be transmitted over the network from a remote storage node. In addition, an intermediate combiner can be inserted between each mapper and the final reducer. Its purpose is to combine all local map outputs (using either the same or a separate reducer function) before they are sent out to the reducers.
Finally, the distributed execution model also takes care of load balancing and fault tolerance in a simple but effec-tive way; see [12] for further information.
 Scalability One of the main advantages of Map-Reduce is that it can transparently use any number of machines. If the volume of output data is much smaller than the volume of input data, as is typically the case, then co-location of tasks and data leads to performance improvement almost proportional to the number of nodes (see, e.g., Figure 1). We further discuss scalability in Section 5.
As shown in Figure 3, a distributed data mining process involves several steps: data gathering, pre-processing, anal-ysis and post-processing, many of which involve distributed processing through either a data storage layer (such as GFS [19], HDFS[1] or KFS[26]), or a higher-level data access and job description language, such as Sawzall [31], Pig [3], or Cascading [37].
 Data gathering This step involves identifying the source and obtaining the data. Some examples include (i) crawl-ing millions of web pages, (ii) querying heterogeneous databases, (iii) large-scale scientific simulation, or (iv) dis-tributed system monitoring. Most are performed in a dis-tributed manner, and can be expressed as Map-Reduce jobs. Data pre-processing After obtaining the raw data, an im-portant step is to transform it into the appropriate format for data analysis. As a matter of fact, data cleaning often consumes the majority of time for exploratory data min-ing tasks. However, despite calls from several established researchers [16, 22], it has been largely ignored in the re-search literature. We can no longer afford to ignore this step.

Increasingly, many researchers (ourselves included) now find that they have to routinely deal with gigabytes or even terabytes of data. For example just parsing 4.5 terabytes of compressed text logs for 30 days worth of MSN in-stant messaging data was reported to take a total of five full days, on an eight processor machine with fast local disks [27]. We recently had similar experiences processing a 350 gigabyte raw network event log (similar to the exam-ple in Section 2.1). We needed over five hours to extract source/destination IP pairs, even though we were accessing the data over a 2Gbps Fibre Channel link to a SAN (dot-ted line in Figure 1). Similarly, the TREC data is 100GB of text. Pre-processing that on a single powerful machine (four cores and 32GB RAM) took several days . Compared to these luxury settings, we are able to achieve much better performance on a few commodity nodes running Hadoop. More importantly, setting up Hadoop required minimal ef-fort (about two to three hours for a moderately experienced person).

Moreover, other members of our group took different ap-proaches on the event log data. The first is the DPH (des-perate Perl hacker) approach. The data were sorted and par-titioned using a primary key consisting of a timestamp plus a unique record identifier. Subsequently, extracting time-dependent aggregates could be performed quickly. How-ever, when we needed to extract a graph of IP-pairs, the pre-processing was not helpful. Furthermore, the effort to organize the data took approximately three days.
The second is the traditional database management sys-tem approach. Since the each event record contains widely different fields, depending on the event type, only the ten or so common fields were extracted (dropping all remaining possible fields, which number over one thousand but are not always present). Furthermore, without spending too much time to fine-tune MySQL X  X  storage engine parameters, no more than a year worth of data (about a quarter of the total) could be bulk loaded successfully. Pre-processing the data into this common schema and building indices on all fields gave good performance. However, the effort required about two days and would be of no benefit if any of the dropped fields needed to be analyzed.

We are not suggesting that the alternative approaches were handled in the optimal way. However, in a relative effort-to-benefit ratio, we believe that Hadoop wins.
Specifically for co-clustering, there are two main pre-processing tasks:  X  Building the graph from raw data.  X  Pre-computing the transpose.
 The first step primarily involves extracting the graph (e.g., source-destination or document-term pairs) and may also involve other related tasks (such as stemming and stopword removal). During co-clustering optimization, we need to it-erate over both rows and columns. Therefore, we need to pre-compute the adjacency lists for both the original graph as well as its transpose. Transposition is very similar to computing an inverted index, one of the applications Map-Reduce was originally developed for [12]. This step typi-cally took a few minutes. In Section 5, we describe actual times on real-world data processing in detail. Sym. Definition
A the m  X  n data matrix m , n Number of rows and columns. i, j Row/column indices, 1  X  i  X  m , 1  X  j  X  n . a i,j The ( i, j ) element of A .

G the k  X  l group matrix k, l Numbers of row-and column-groups. p, q Group indices, 1  X  p  X  k , 1  X  q  X  l . g p,q The ( p, q ) element of G .

I p Set of rows belonging to the p -th row group.

J q Similar to I p , but for columns m p the size of p -th row group, m p  X | I p | , 1  X  p  X  k . n q the size of q -th col. group, n p  X | J q | , 1  X  p  X  k . r Row group assignments. c Column group assignments.
 H ( . ) Shannon entropy function Data analysis In practice, even after data pre-processing, the data can still be too big to analyze in a centralized man-ner. For example, the adjacency lists for TREC are about 4GB each. At a total of over 8GB for both the original ma-trix and its transpose, few machines have enough memory to even load the entire graph.

Because of the huge data, we see more and more data analysis done in a distributed fashion. Without relying on different infrastructure, many analyses can be done in the same environment where data are gathered and processed, using the same Map-Reduce programming model and ex-ploit parallelism and fault-tolerance.
 The next section presents the details of our Distributed Co-clustering (DisCo) framework using Map-Reduce. Post-processing The analysis results need to be visual-ized, or sometimes turned into the input for other applica-tions. They can also reside in the same environment.
In this section we present the main design for distributed co-clustering using Map-Reduce. First, we give a very brief overview of the necessary co-clustering definitions. Then we explain how the necessary computations can be per-formed as map and reduce operations. Finally, we conclude with a brief description of certain important implementation considerations.
Matrices are denoted by boldface capital letters, e.g., A and vectors are denoted by boldface lowercase letters, e.g., a . The ( i, j ) -th element of matrix A is a ij , the i -th row of A is a i : , and the j -th column of A is a : j .
We focus on algorithms that employ a checkerboard de-composition of the original adjacency matrix into a grid of
Figure 4. A co-clustering example: Given A , find group assignments r and c such that the resulting sub-matrices in B are highly corre-lated. sub-matrices, also allowing row and column permutations). Formally, given an m  X  n matrix, a co-clustering is a pair of row and column labeling vectors so that each element of ( r ) is the group label r ( i ) for the i -th row of the matrix, 1  X  i  X  m and 1  X  r ( i )  X  k , and similarly for the columns.

In addition to the label vectors, another key structure is the k  X  ` group matrix G . Different co-clustering algorithms construct G in different ways, but the intuition is the same: g pq gives the sufficient statistics for the ( p, q ) sub-matrix, which corresponds to the intersection of p -th row group and q -th column group.

The goal is to find good group assignment vectors such that an error function is minimized. Various co-clustering algorithms have adopted different error functions, such as minimum mutual information [15], sum-squared distance [9], and code length [6]. A general co-clustering framework based on Bregman divergence [4] has been proposed, which covers the entire exponential family.
 For example, in Figure 4, given the 4  X  5 input matrix A , the goal is to find r and c such that after permutation ac-cording to r and c , the correlated sub-matrices are grouped together. Searching for the optimal group assignment is NP-hard [15]. Therefore, a common approach is to do local search, alternating between row and column assignments, while holding the other assignment fixed. The basic steps are the following:  X  Row iteration: Fixing the current column group as- X  Global sync: Based on the new labels and group ma- X  Column iteration and sync: Fix r and perform a sim-The high-level pseudo-code is listed in Procedure 1. This large family of co-clustering algorithms, which includes all those cited above, satisfies two key conditions: 1. The error function can be computed using r , c , and G , Procedure 1 CC ( A , k , l ) 1: Initialize r and c . 2: Compute the group statistics matrix G . 3: repeat 4: for each row i = 1 ..m do 5: for each row group label p = 1 ..k do 6: Assign r ( i )  X  p if this minimizes error 7: Update G , r 8: Do the same for columns 9: until cost does not decrease 10: return r and c 2. The decision in line 5 of Algorithm 1 can be made These conditions are central, but also quite broad. The first is essentially a statement about the sufficient statistics (they can be computed as aggregates over each sub-matrix), whereas the second is a statement about the optimization strategy (local, greedy search).
Next, we seek map and reduce functions to perform the alternating updates using the Map-Reduce framework.
The idea is to initiate two Map-Reduce jobs for row and column iterations, and a synchronization step in between to update the global parameters G , r , and c .

The pseudo-code is the same as before except we need two kind of Map-Reduce jobs: (i) initializing the group ma-trix G and label vectors r and c , and (ii) performing row or column iteration. We use random initialization, and a simple Map-Reduce job can be formed (omitted for space).
Now we discuss how to formulate a Map-Reduce job for row and column iteration. Figure 5 shows how we express one iteration over rows as a Map-Reduce computation. The iteration over columns operates on the adjacency list of the transpose matrix in a similar way. This is the basic building block, where most of the time is actually spent. Figure 5. One iteration over rows as a Map-
Reduce job. Figure 6. Running map and reduce functions.
 Map-function The adjacency list is stored on HDFS as a sequence file of key-value pairs. The key is the row index i and the value is the adjacency list, i.e., { j | 1  X  j  X  n, a 0 } , along with the values a ij . The group matrix G , as well as the column labels c are globally broadcast to all mappers. Given this information, the mapper can compute the locally optimal row label r ( i ) for each row i , as well as the associ-ated per-column statistics for that row. The labels r ( i ) are the intermediate keys. The intermediate values comprise of the row group statistics g i and the membership information { i } . The pseudocode for CCR OW M APPER is shown in Pro-cedure 2.
 Procedure 2 CCR OW M APPER ( k , v ) Globals: Cluster statistics G , labels c Source node is i  X  k Adjacency list of i is a i :  X  V
Compute row statistics g i := R OW S TATISTICS ( a i : , c ) for each group label p = 1 ..k do emit  X  r ( i ) , ( g i , { i } )  X 
Note that updating the row group statistics g i  X  R ` varies for different co-clustering algorithms. In the exper-iments we rely on the cross-association cost function [6], which uses the number of non-zero columns per column group. Formally, g i ( p ) := # { j | a ij 6 = 0 , c ( j ) = p } . For example, in Figure 6, g 2 = (2 , 0) because the second row has two non-zero columns in the first column group, and none in the second column group.
 Reduce function The reducer merges the row group statistics and group members for each cluster label. For example, in Figure 6, the intermediate key-values 2 , &lt; (2 , 0) , 2 &gt; are aggregated by vector addition over the 2nd row in G , g 2 , and the set union of row 2 to I 2 . The pseu-docode for CCR OW R EDUCER in more detail is as follows. Global sync Finally, we need to collect the new results for the G matrix and r row-label vector, as shown in C OL Overall picture This building block is then used in the alternating minimization to find a co-clustering for given k and ` [6, 4], as well as to search for k and ` themselves [6]. Procedure 3 CCR OW R EDUCER ( k , V ) Row group label is p  X  k
Initialize g p  X  0 , I p  X  X  X  for each map value ( g , I )  X  V do emit  X  p, ( g p , I p )  X  Procedure 4 C OLLECT R ESULTS
Initialize G  X  0 , r  X  0 for reduce output  X  p, ( g p , I p )  X  do return G and r For the latter, some additional modifications are necessary. We can still decide which cluster to split in a way similar to [6], but the computation to decide how to split it (i.e., which row/column goes to the new group) is not parallelizable. We found that using a 50-50 random split (so that half of the rows/columns go into the new cluster and half remain in the old cluster) is very effective and often yields better results than the criterion of [6]; it always gives better results if we exploit cluster resources by doing multiple random trials.
Finally, we conclude with some brief remarks about im-plementation considerations. Figure 7 shows the execution timeline for a row iteration Map-Reduce job. In this case, there is one reduce task at the top of the figure. All other are map tasks, shown in green.
 Performance tuning Even though Map-Reduce hides most of the complexities of distributed execution, there are still some parameters that need to be decided appropriately, to improve performance. Setting them is relatively intuitive, but does require some care. We quantify these in Section 5. The first parameter has to do with thread pool sizes, which needs to be configured for the cluster at hand. We found that setting it to the number of cores per node plus 50% is sufficient to achieve reasonably good node utilization. The other important parameters are number of map tasks (which can be implicitly determined by setting the minimum input split size, as well as explicitly set), as well as the number of reduce tasks. The input split size can be increased for large input files, to avoid unnecessary task startup overheads. The number of reducers should be set according to the size of the intermediate key space, as well as the expected number of intermediate pairs (after combination on each map task). For co-clustering, the number of distinct intermediate keys is equal to the number of clusters ( k or ` ) and the number of intermediate pairs is proportional to that. These are typ-
Figure 7. Execution timeline for a row itera-tion map-reduce job. ically small numbers, so it is best to use one reduce task. However, for building the graph in the pre-processing step, as well as for inverting the matrix, one reduce task becomes a bottleneck, so it is best to increase this, up to the number of machines available in the cluster.
 Search randomization As described in Section 4.2, we do random cluster splits. We can further exploit the avail-ability of many machines to do multiple split trials and pick the best. In this case, we can achieve up to 10% better fi-nal cost objectives than [6] X  X n addition to the scalability benefits that the Map-Reduce framework offers us.
In this section we describe running time measurements from our Hadoop deployment of DisCo. We focus on scal-ability, as well as performance tuning. We should note that, although we did consider alternative approaches (see dis-cussion in Section 3), these proved impractical in the long term.
We performed all experiments on 39 nodes in our clus-ter. These were blade servers with two dual-core processors (typically Intel Xeon 2.66GHz, with a few 3GHz machines), all with 8GB RAM and all running Linux RHEL4.

The machines were located in four different blade-center enclosures, three of which were in the same rack. All have Gigabit ethernet connections. The switching fabric within one blade-center enclosure has an aggregate bandwidth of 4Gbps. The same holds for the switches between blade centers, only those links are shared with a larger number of other nodes that may interfere with network traffic. Each of the blades had a locally attached hard drive. Blade servers typically have small and relatively slow hard drives. The drives in these machines are SATA and can achieve a sustained read performance (measured by cat &gt;/dev/null of a large file) of about 65MB/sec, or roughly 500Mbps. The total capacity of our HDFS clus-ter was just 2.4 terabytes. HDFS block size was set to 64MB (default value). For large files we used a replication factor of 2, and for smaller files (less than a few hundred megabytes) we used a replication factor between 8 X 12 (de-pending on file size), so we can a larger number of mappers (ideally all of them) that work on local replicas of the data.
The cluster is shared with other users. The namenodes as well as the jobtracker were placed on two separate master nodes, different from the set of 39 nodes used for storage and computation. Under normal operation, we use a nice level of five, but for timing experiments we raised it to zero and repeated each measurement three times, taking an aver-age. All code 3 was implemented completely in Java (with some parts using the GNU Trove collections for efficiency) and we used Sun JDK version 1.6.0 03 for everything. We used three real datasets, summarized in Table 2.
We report our experiences with respect to scalability and performance tuning. In particular, we first study wall-clock time versus number of nodes, to characterize scalability. Next, we study performance tuning and the dependence on the following parameters: (i) minimum input split size (which equivalently determines the number of maps), (ii) maximum number of concurrent map tasks per node, and (iii) number of reducer tasks.

Figures 1 and 8 shows the results for the pre-processing step on the ISS data. The default values are: 39 nodes, 6 concurrent maps per node, 5 reduce tasks, and 256MB input split size. First, we observe that for this task, ag-gregate throughput scales almost linearly with respect to number of nodes (Figure 1). Next, we observe that as we increase the number of concurrent map tasks, we achieve better utilization of each node. The optimum is reached at a number slightly larger than the number of cores on each node. Increasing much beyond that starts causing unnec-essary overheads (although we never reached the point of thrashing). The number of reducers for this task that gives peak throughput is about 5. One reducer becomes a bottle-neck, whereas a larger number seems to create unnecessary overheads. Finally, as we increase block size, we see that a small size causes overheads since there is a large number of map tasks, and therefore a larger number of HTTP request to each reducer, to transfer intermediate results. However, as we increase the split size to several multiples of HDFS block size, it becomes much more difficult to place map tasks on local copies of the data, so performance degrades due to unnecessarily high network traffic.

Finally, Figure 9 shows aggregate throughput versus number of nodes for one co-clustering iteration. Com-pared to Figure 1, we see a scaleup at about the same rate (135Mbps/node versus 175Mbps/node) up to about 10 nodes. Performance reaches a plateau after about 25 nodes, corresponding to about 20  X  2 seconds per iteration.
The bandwidth falls off because, as the job size de-creases, framework overheads begin to dominate process-ing time. Based on closer investigation, there are two over-heads: (i) fundamental ones, related to transmitting task pa-rameters, spawning the processes that will execute them, storing results on HDFS, and so on; and (ii) some design decisions that are specific to Hadoop, which uses busy loops with a hardcoded sleep interval 4 .

Despite these partly  X  X rtificial X  limitations 5 , there are two important observations. First, compared to our previ-ous main-memory implementation, where the performance-critical inner loop is written in C (as a MEX module; the rest of the code is in Matlab), 20  X  2 seconds per iteration is equal or better to what we can get on a machine with 48GB of RAM, due to thrashing. Second, as the dataset sizes grow in future, our implementation will achieve lin-ear scaleup. Thus, we achieve our goal of aiming towards peta-scale mining.
Figure 9. Co-clustering iteration scalabil-ity (TREC data, 4GB on HDFS after post-processing): scales at the same rate as Fig-ure 1 up to ten nodes, due to the relatively small dataset size.

The general behavior of the co-clustering iterations with respect to the other three parameters are almost identical with those in Figure 8 (except that the absolute numbers are smaller and in accordance to Figure 9), and omitted for space. One difference (when running with a default of 17 nodes in the cluster) is that the peak in bandwidth versus number of reducers occurs at one reducer, rather than five. This is expected, since the size of intermediate results is much smaller, as pointed out in Section 4.3.
 Pre-processing Finally, we report the proportion of time spent on pre-processing versus co-clustering. As discussed before, pre-processing is often overlooked, even though it is a pre-requisite for any mining task. Figure 10 describes our experience and should be taken with a grain of salt. To have some common ground for comparison, the time used for co-clustering is that for 30 pairs of row and column iterations. The actual number of iterations varies from one to over one hundred, although the average we observed is around 100 iterations. We should also point out that pre-processing here is performed with map-reduce  X  otherwise, pre-processing itself takes up to days. Even one might argue that going back to the original data is something that happens rarely, we still wish to point out that (i) this may not always be the case, and (ii) the effort required is still significant.
As a programming model, Map-Reduce adopts a ex-tremely simple but powerful abstraction from functional programming. Many data processing tasks can be easily formulated as Map-Reduce jobs as shown in [13]. Inspired by that, many higher-level programming abstractions have been implemented for large-scale data processing, such as Sawzall [31], Dryad [25] and DryadLINQ, PIG [3], SPADE [17], FREERIDE-G [21], and Sector [23], among others.
Figure 10. Fraction of time spent pre-processing vs. co-clustering.

Much of the power of Map-Reduce derives from its use of the Google File System (GFS) [19] (or similar file sys-tems such as HDFS [1] and KFS [26]) as the underlying data store. GFS is similar to other distributed file systems such as [20, 33, 36, 5] in that it employs a distributed storage clus-ter. However, they employ block-addressable storage and a centralized metadata server (which essentially stores the mapping between filenames and a list of blocks and their locations in the storage cluster). Several higher-level data storage abstractions provides a convenient data access and storage API for Map-Reduce tasks, such as Bigtable [7], HBase [1], and Hypertable [38].

As data grows, data mining and machine learning appli-cations also start to embrace the Map-Reduce paradigm for e.g., news personalization [11], or several machine learning algorithms on multicore architectures [10]. Compared to them, our focus is to illustrate a complete data mining pro-cess involving multiple interconnected steps that all require large-scale data processing.
Co-clustering has been studied in many different applica-tions including text mining [15, 28], genes and experimental conditions in bioinformatics [24, 8], recommender systems [18], and graph mining [6].

Many co-clustering algorithms have been proposed, de-pending on the cluster shapes, the properties of input data, and optimization objectives. Different cluster shapes in-clude checkerboard partitions, single bicluster, exclusive row and column partitions and overlapping partitions. For a detailed discussion, see survey [29]. In this work, we focus on checkerboard partitioning such as those in [4, 15, 6].
Various of optimization criteria have been proposed, such as minimum mutual information [15], sum-squared distance [9], and code length [6]. A general co-clustering framework based on Bregman divergence [4] has been pro-posed for covering the entire exponential family. In this work, we utilize the code length objective, but the algorithm can apply to the other cases with minor modifications.
In summary, this paper presents our findings and valu-able lessons from designing a framework for a holistic ap-proach to data mining, in the context of the co-clustering task. Our experiences (Section 3) led us to consider a dis-tributed infrastructure. Given the decreasing prices of mag-netic storage and the ever increasing rate of data collection, the necessity of data mining algorithms on distributed in-frastructures is clear. In a growing open source ecosystem of scalable, distributed data processing and management components, Map-Reduce is emerging as the predominant elementary abstraction for distributed execution of a large class of data-intensive processing tasks.

Co-clustering has many important applications, includ-ing text mining, graph mining and collaborative filter-ing. This paper is a case study on a distributed co-clustering framework, presenting our design and describing the lessons we learned. We demonstrate that we can achieve I/O rates that exceed those of high-performance storage sys-tems (e.g., SAN over Fibre Channel running a high perfor-mance file system such as GPFS), using relatively low-cost components. More importantly, performance scales almost linearly with the number of machines/disks. Finally, we demonstrate results on large, real-world data sets.
