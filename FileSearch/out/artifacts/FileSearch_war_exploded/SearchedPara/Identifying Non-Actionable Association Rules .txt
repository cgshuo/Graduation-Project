 Building predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action. Keywords: Non-actionable rules, rule interestingness. Finding useful rules is important for many data mining applications. These rules allow the user to perform actions to achieve his/her current goals. Association rule mining is often used to generate the initial set of rules. The user then analyzes the discovered rules to identify those useful/actionable ones. The advantage of association rule mining is that it is able to efficiently find all rules in data that satisfy the user specified minimum support and minimum confidence constraints. This complete set of rules enables the user to find all the interesting rules. However, a main drawback is that it often produces too many rules, which makes it very difficult to identify the useful rules for action. association rules are actually redundant and/or insignificant. Effective pruning techniques have been developed to remove those spurious or insignificant rules [13, 4, 22]. These techniques essentially use general rules (with fewer conditions) to prune personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 those insignificant specialized rules (with more conditions). They ensure that the remaining rules are all significant and/or non-redundant. However, in this paper, we argue that not all significant rules (unpruned rules) are potentially useful for action. In fact, many significant rules are not actionable. The mining of association rule is commonly stated as follows [3]: Each transaction consists of a subset of items in I. An association rule is an implication of the form X ~ Y, where X c/, Y c L and 
X n Y = 0. The rule X ~ Y holds in T with confidence c if c% of transactions in T that support X also support Y. The rule has support s in T if s% of transactions in T contains X u Y. The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support and minimum confidence. relational table, which consists of a set of tuples described by a number of attributes. An item is an attribute value pair, i.e., (attribute = value) (numeric attributes are discretized). Mining in such data is typically targeted at a specific attribute because the user normally wants to know how other attributes are related to this target attribute (which can have many values) [12, 4]. With a target attribute, our objective is to find rules of the form: X ~ y, meets the minimum support requirement t being not actionable. Example 1" We have a disease database of 1000 tuples. The target attribute (Disease) has two values, yes and no, denoting whether one has the disease or not. Out of the 1000 tuples, 500 tuples have the value of yes for the target attribute and 500 have no. The following three rules are discovered from the database. disease and BP (Blood Pressure) = high is 60 (= 6.0% X 1000). The coverage of rule R1 is 100 (60/60%). We define the coverage of a rule as the number of tuples covered by the rule. A rule covers a 
We do not use minimum confidence in our framework (although it can be data tuple if the tuple satisfies the conditions (or the left-hand side) of the rule. From R2, we see that the number of tuples that have BP = high, Sex = Male and Disease = yes together is 36 (= 3.6%x1000). The coverage of R2 is 40 (= 36/90%). The number of tuples that have BP = high, GlucoseLevel = abnormal and Disease = yes is 30 (3.0%x1000), and the coverage of R3 is 30 (= 30/100%). Assume R1, R2 and R3 are all significant rules, i.e., they cannot be pruned by a pruning procedure. Figure 1 shows a possible relationship between R1, R2 and R3. In Figure 1, the tuples covered by R1 are represented with [] and those with yes (Disease = yes) are represented with [=:l. disease is 58. The number of tuples covered by R2 or R3 is 62 (including both yes and no). Since R2 and R3 have much higher confidences and they cover most of the yes tuples, they should be used first in an application (they are of higher quality). This implies that the remaining tuples covered by RI -(R2 u R3) are: 2 data tuples with Disease = yes, and 36 tuples with Disease = no. Clearly, RI -(R2 u R3), denoted by RI' in Figure 1, is not a significant rule for Disease = yes because its confidence is too low (2/(2+36) = 5.3%) compared to the default confidence for Disease = yes in the whole population, which is (500-58)/(1000-62) = 47% (note that here the confidence is updated). In other words, for the target attribute value yes, RI' is worse than random guessing using the default confidence. It is thus not actionable. Since R1 is effectively RI' when it comes to using the three rules. Hence, R I cannot he actionable. action on rule R1, it is much better to use R2 and R3 as they are of higher qualities (with significantly higher confidences and good coverage of data tuples) and are able to give better predictions (although association rules are not normally used for prediction, a rule that cannot be used in the future is of limited value). After removing those tuples covered by R2 and R3, RI is not useful/actionable as it is no longer a meaningful rule for Disease = yes due to its low confidence, 5.3%, (a statistical test will be used to evaluate whether a confidence is too low). actionable rules. The technique works in two phases: 1. We first generate the rules and prune them according to some 2. We then analyze the remaining rules backward (from rules to determine whether a more general rule is potentially actionable. Notice that identifying non-actionable rules works in exactly the opposite way to the traditional rule pruning, which uses general rules to prune specialized rules [13, 4, 18]. 
Further notes about the proposed technique:  X  The proposed technique is not simply keeping those high which are normally general rules. Instead, it evaluates the  X  In some domains, it is difficult to perform actions using rules  X  A common concern about specialized rules is that they may  X  The proposed technique may produce an interesting by- X  The proposed technique does not say that every remaining rule We run a number of experiments to determine the number of non-actionable rules. Experiment results show that on average more than 34% of the significant rules (rules left after pruning) are, in fact, not actionable. Furthermore, the identification of the non-actionable rules can be done efficiently. Pruning redundant or insignificant association rules have been studied by many researchers. In early machine learning research, decision tree or rule pruning [18] already used general rules to prune specialized rules to produce more accuracy classifiers. using minimum improvement, which is the difference between the confidence of a rule r (a general rule) and the confidence of any proper sub-rule (specialized rules of r) with the same consequent. Those rules that do not meet this minimum improvement are pruned. Chi-square tests. It also uses general rules to prune more specialized rules. This work uses the pruning method in [13]. [13] also presents a technique to summarize those unpruned rules so that the user only needs to see a small subset of rules in order to obtain a good overall understanding of the domain. association rules. Basically, the technique keeps the most general rules for all those rules with the same confidence, and does not generate the redundant specialized rules. identifies non-actionable rules. They only use general rules to prune those insignificant specialized rules. As discussed in Section 1, identifying non-actionable rules works in exactly the opposite direction. Here, significant specialized rules are used to determine whether a more general rule is potentially actionable. rules. A rule is redundant with respect to another rule if the support and confidence of the redundant rule are always at least as large as the support and confidence of the latter. This is different from our work, as it does not identify non-actionable rules. interestingness [17, 20, 11, 9, 13, 5, 1, 16, 19, 21]. In subjective interestingness, [9] proposes an approach to allow the user to specify what he/she wants to see using templates. The system then retrieves those matching rules from the set of discovered rules. [11, 20, 16] propose a number of methods for finding unexpected rules. Instead of asking the user to specify what he/she wants to see as in [9, 1], these approaches ask the user to specify his/her existing knowledge about the domain. The system then finds those unexpected rules. [19] proposes a method to remove some non-interesting rules by asking the user some questions. [1] presents some rule filtering methods. These techniques are different from ours as none of them identifies non-actionable rules. For objective interestingness, [21] reviews a number of methods for ranking rules according to different statistical or information measures. 
Again, these methods do not identify non-actionable rules. 
Recall, our proposed technique works in two phases. In the first phase, rules are generated and those non-significant rules are pruned. In the second phase, the remaining significant rules are then analyzed to identify non-actionable rules. Chi-square test [6] is used in both phases. Here, we briefly introduce chi-square test statistics (Z ~) and rule pruning. 
Chi-square test is a widely used method for testing independence or correlation [6] of some attributes (or variables). Here, we use it to test whether the conditions and the consequent of a rule are correlated or whether the rule is significant. A significant rule in our context is a positively correlated rule, which is defined later. frequencies with the corresponding expected frequencies. The closer the observed frequencies are to the expected frequencies, the greater is the weight of evidence in favor of independence. 
Example 2: In a disease domain, we have 1000 people who were checked for a particular disease in a medical center. Out of the normal blood pressure. 280 people were diagnosed to have the disease, and the remaining 720 people did not have the disease. 
We also know that 120 people who had high blood pressure were diagnosed to have the disease. This can be expressed as an association rule: 
This information gives us a 2x2 contingency table containing four ceils (Figure 2). Note that the table has only 1 degree of freedom [6], which is sufficient for our work. Our question is "Is there any correlation between the disease and whether one has high blood pressure?" To answer this question, we compute the expected frequency (assuming there is no correlation between the two) for each cell as follows: Of the 1000 people, 300 (30% of the total) had high blood pressure, while 700 (70% of the total) had normal blood pressure. If the two attributes are truly independent, we would expect the 280 disease cases to be divided between BP = high and BP = not_high in the same ratio (30% and 70%); similarly, we would expect the 720 non-disease cases to be divided in the same fashion. expected values. Let 3~ be an observed frequency, and f be an expected frequency. The Z 2 value is defined as: A Z 2 value of 0 implies the attributes are statistically independent. If it is higher than a certain threshold value (e.g. 3.84 at the 95% significance level [6]), we reject the independence assumption. For our example, we obtain ~ --30.59. Thus, we say that the disease is correlated to whether one has high blood pressure with 95% confidence. Below, we give the definitions of correlation and independence in the context of association rule mining. Definition 1 (correlated): Let D~ be a sub-population or sub-dataset of the whole dataset D, and c be a significance level. X the significance level c. Definition 2 (uncorrelated or independent): Let Ds be a sub-population or sub-dataset of the whole dataset D, and c be a significance level. X and y of a rule, X ---&gt; y, are said to be uncorrelated or independent with respect to Ds if the Z 2 value for the rule against Ds does not exceed the X 2 value at the significance level c. It is important to note what population a rule is tested against. Traditionally, the population is assumed to be the whole dataset. This is inadequate for pruning as we will see in Section 3.2. the types of correlation of a rule. We define three types of correlation as in [13]: Definition 3 (types of correlation): 
Negative correlation: if X and y of a rule r, X ---&gt; y, are 
Independence: if X and y of a rule r, X ---&gt; y, are independent, In general, computing the type of correlation of an association rule r, X --4 y, is to compare the rule with the whole population or the whole dataset. Or more specifically, it is to compare with the rule that has the same consequent as r but no condition, i.e., the default rule for y, "---) y". r: BP = high --~ Disease = yes 
R: ~ Disease = yes We compare r with R to see whether r represents a positive correction. Note that these two rules specify completely the contingency table in Figure 2. R, which has no condition, gives the column total for yes (280) and the total number of tuples (1000) in the data. r, which represents the first cell in the table, also has the number 300 for the row total (which is simply the support count of BP = high). With all this information, the rest of the cells can be computed. Pruning is needed to remove those non-significant rules before the identification of non-actionable rules can proceed. Briefly, we test each rule r against its ancestor rules (which have the same consequent as r but fewer or 0 conditions) to see whether r is a significant (or positively correlated) rule with respect to its whole dataset. For example, we have the following rules: r: BP = high, Glucose_level = abnormal --h Disease = yes R: BP = high ~ Disease = yes [sup = 12%, conf = 40%] We have shown that R is a significant rule. Following the same procedure, we can show that r is also a significant rule. Here, the tests are against the whole population or the whole dataset. However, if we test r against the sub-population covered by R, r will not show a positive correlation (in fact, it shows independence). Thus, r is not a Significant rule. Intuitively, we can little information, r's slightly higher confidence is more likely due to chance than true correlation. Thus, r should be pruned. to prune r using each ancestor rule R of r. That is, we perform a If the test shows a positive correlation, r is kept. Otherwise, r is pruned (i.e., within the data covered by R, r is not significant). With the removal of non-significant rules, we are ready to identify non-actionable rules. This section presents the proposed technique for identifying such rules. We first give some definitions. Definition 4 (potentially actionable rules): A rule R is a potentially actionable (PA) rule: 1. if R does not have any descendent rules. A rule r is a 2. (there exist some descendent PA rules for R) if after Definition 5 (non-actionable rules): A rule R is a non-actionable rule if it is not a PA rule. Note that in Definition 4, we do not compare R (after those tuples are removed by its descendent PA rules) with R's any other there are typically many ancestor rules. If we compare R with each ancestor rule, it becomes quite confusing and rather difficult to understand and to be accepted by users. By comparing with the default rule "--~ y", we are, in fact, determining whether the rule is better than random guess, which is easily understood. rules. The definitions above suggest a backward evaluation algorithm. The basic idea of the algorithm is as follows: Assume that the longest rule in the whole set of rules has n conditions. We process the rules level-by-level, i.e., we first try to process (n-1)-condition rules (according to point 1 of Definition 4, we do not need to evaluate n-condition rules). After that, we process (n-2)-condition rules and so on. At each level k, we first find all the descendent PA rules of each k-condition rule. After the set of descendent PA rules are found for each k-condition rule R, we scan the dataset once to count those data tuples that are covered by each R but are not covered by any of its descendent PA rules. With the new support counts, we can use 2 '2 test to check whether each k-condition rule R is still significant with respect to the whole dataset, i.e., "--~ y". If a k-condition rule R is no longer significant, it is a non-actionable rule. Note that those rules that have been identified as non-actionable rules in the previous levels will not be considered subsequently as they are not PA rules. to the algorithm are Rules and T, where Rules is the set of remaining rules after pruning and T is the 2 "2 value at a particular significance level. 4 findDescendentPARules(W); Figure 3: The algorithm for identifying non-actionable rules Notes about the algorithm:  X  Line 1 sets up the loop to evaluate rules from level n-1 to 1.  X  In line 2, thefindRules(k, Rules) function finds all rules with k  X  Line 4 finds all the descendent PA rules of each k-condition  X  In line 5, we scan the dataset once to compute the updated  X  In line 6 (after all the updating of counts has been done for all  X  Line 7 basically checks to see whether R still represents a The main computation here is the database scan. If the longest rule for the rule set Rules has n conditions, in the worse case, the algorithm needs to scan the dataset n-1 times to update support and coverage counts of those/k-condition rules being evaluated. A useful by-product: The proposed technique may produce a useful by-product that cannot be produced by a normal association rule miner. As indicated in the introduction section, it may generate rules whose conditions contain negated items. 
These rules can also be useful in practice. Let a non-actionable rule be X ---&gt; y, and the set of its descendent rules be S. New rules of the following form may be produced: where X is a set of items (attribute value pairs) and Z is the union of the conditions of all the rules in S that are not in the Example 1 in Section 1, the following rule is produced: We now study the effectiveness of the proposed algorithm. We used 30 datasets in our experiments. 25 of them are obtained from the UCI ML Repository [15], and 5 are from our real-life applications. The efficiency of our system is also evaluated. right-hand-side of association rules. The target attribute is a categorical attribute with a number of values. For the 25 UCI datasets, the target attribute in each dataset is the class attribute used for classification. For our 5 real-life datasets, the target attributes were suggested by our users. For all these datasets, even with a target attribute, the numbers of associations discovered are huge. Many datasets cause combinatorial explosion. Due to this reason, we set a hard limit of 80,000 on the total number of large rules processed in memory. Even with such a large limit, mining cannot be completed for many datasets. Using a hard limit is justified because proceeding further only generates rules with many conditions that are hard to understand and difficult to use. discretize these attributes into intervals using the target attribute. We use the method given in [7]. The code is from MLC++ [10]. 
The experiments were performed using the significance level of 95% for ~,2 tests, which is a commonly used level. For rule mining, we set the minimum support to 1% as it is shown in [12] that for these datasets rules with 1% support are sufficiently predictive. 
Table 1 shows the experiment results. Below, we explain each column. The final row gives the average value for each column. Column 1: It gives the name of each dataset (the last 5 datasets are our real-life datasets). The number of tuples in these datasets ranges from a few hundreds to tens of thousands. 
Column 2: It gives the number of rules generated from each dataset that meet the minimum support, and we call these rules large rules. Note that these rules may not be significant. We can see that the number of large rules generated from an rule miner is huge for each dataset. Almost half of the datasets cannot be completed even under the hard limit of 80,000. 
Column 3: It gives the number of positively correlated (PC) rules found in each dataset. Here each rule is compared against the whole dataset, e.g., "---&gt; y". The number is much smaller. However, on average, there are still more than 20,000 of them. 
Column 4: It gives the number of positively correlated (PC) rules after pruning. The number is reduced drastically. On average over the 30 datasets, there are only 638 rules left. Note that the pruning here is against those ancestor rules (see Section 3.2 and [13]), i.e., general rules are used to prune specialized rules. 
Column 5: It gives the number of non-actionable rules identified by the proposed technique. We can see that a large proportion of significant rules (rules left after pruning) are not actionable. 
On average, out of 638 significant rules, 210 of them are not actionable. This is a substantial further reduction considering that rule pruning has already removed so many spurious rules. 
Column 6: It gives the ratio of the number of non-actionable rules (column 5) vs. the number of significant rules (column 4). On average over the 30 datasets, 34.2% significant rules are not 1 anneal 42893 18843 347 126 2 austral 80000 40668 379 128 3 auto 80000 43781 2175 525 4 breast-w 2757 2362 97 29 5 chess 80000 31939 2314 477 6 cleve 30435 11707 181 82 7 crx 80000 40269 465 152 8 diabetes 1196 565 44 13 9 igerman 80000 21386 496 151 10 giass 1820 1036 153 66 11 heart 10044 3055 131 58 12 hepatitis 80000 23097 201 66 13 horse 79965 24368 314 81 14 hypo 80000 32543 299 95 iono 34463 13710 682 262 16 led7 1692 1389 691 342 17 lymph 80000 18324 511 139 18 pima 1196 565 44 13 19 sick 80000 23538 456 92 20 sonar 80000 24005 1299 658 21 tic-tac-t 8315 2712 290 78 22 vehicle 80000 49538 3259 1299 23 wvform 35888 30971 1835 248 24 wine 27143 15013 551 275 25!zoo 80000 59270 1159 576 26 Disease 1622 462 35 1 l 27 EDU 80000 29362 209 60 28 Medical 67841 29776 190 100 29 Traffic 74610 5916 231 55 30 Wafer 8534 3372 89 31 
Average 49014 20118 638 209.6 actionable. Column 7: It gives the running time (in second) for rule generation and pruning for each dataset with data residing on disk. All our experiments were run on a Pentium II 350 machine with 128MB RAM. Column 8. It gives the running time (in second) for the proposed technique in identifying all non-actionable rules. We can see that the proposed technique is efficient. Association rule mining is a fundamental model of data mining. The application of association rules is, however, severely hampered by the large number of rules that it often generates, and most of the rules are actually redundant and/or insignificant. Although many pruning techniques have been developed to remove those insignificant rules, these techniques all use general rules to prune those insignificant specialized rules. In this paper, we show that more can be done, i.e., many significant general rules are actually not useful for action, as they should be replaced by their descendent specialized rules of higher quality. We have presented an algorithm to identify all the non-actionable rules from a set of significant rules. Experiment results show that on average, more than 34% of significant rules (mined from our 30 datasets) are not actionable. This substantially reduces the number of rules that the user has to focus his/her analysis on in order to find those truly actionable rules for application. [1]. Adomavicius, G, and Tuzhilin, A. "User profiling in [2]. Aggarwal, C., and Yu, P. "Online generation of association [3]. Agrawal, R. and Srikant, R. "Fast algorithms for mining [4]. Bayardo, R., Agrawal, R, and Gunopulos, D. "Constraint-[5]. Bayardo, R. and Agrawal, R. "Mining the most interesting [6]. Everitt, B. S. The analysis of contingency tables. Chapman [7]. Fayyad, U. M. and Irani, K. B. "Multi-interval discretization [8]. Han, J. and Fu, Y. "Discovery of multiple-level association [9]. Klemetinen, M., Mannila, H., Ronkainen, P., Toivonen, [10]. Kohavi, R., John, G., Long, R., Manley, D., and Pfieger, K. [11]. Liu, B., and Hsu, W. "Post-analysis of learned rules." AAAI-[12]. Liu, B., Hsu, W. and Ma, Y. "Integrating classification and [13]. Liu, B., Hsu, W. and Ma, Y. "Pruning and summarizing the [14]. Liu, B., Hsu, W. and Ma, Y. Identifying non-actionable [15]. Merz, C. J, and Murphy, P. UCI repository of machine [16]. Padmanabhan, B. and Tuzhilin, A. "Small is beautiful: [17]. Piatesky-Shapiro, G., and Matheus, C. '~l~e interestingness [18]. Quinlan, R. C4,5: program for machine learning. Morgan [19]. Sahar, S. "Interestingness via what is not interesting." KDD-[20]. Silberschatz, A., and Tuzhilin, A. "What makes patterns [21]. Tan, P-N. &amp; Kumar, V. "Interestingness measures for [22]. Zaki, M. "Generating non-redundant association rules." 
