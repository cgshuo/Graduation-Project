 Abdullah Mueen  X  Nikan Chavoshi Abstract Time series motifs are repeated patterns in long and noisy time series. Motifs are typically used to understand the dynamics of the source because repeated patterns with high similarity evidentially rule out the presence of noise. Recently, time series motifs have also been used for clustering, summarization, rule discovery and compression as features. For all such purposes, many high-quality motifs of various lengths are desirable and thus originate the problem of enumerating motifs for a wide range of lengths. Existing algorithms find motifs for a given length. A trivial way to enumerate motifs is to run one of the algorithms for the whole range of lengths. However, such parameter sweep is computationally infeasi-ble for large real datasets. In this paper, we describe an exact algorithm, called MOEN ,to enumerate motifs. The algorithm is an order of magnitude faster than the naive algorithm. The algorithm frees us from re-discovering the same motif at different lengths and tuning multiple data-dependent parameters. The speedup comes from using a novel bound on the similarity function across lengths and the algorithm uses only linear space unlike other motif discovery algorithms. We also describe an approximate extension of MOEN algorithm that is faster and suitable for larger datasets. We describe five case studies in entomology, sensor fusion, power consumption monitoring and activity recognition where MOEN enumerates several high-quality motifs.
 Keywords Time series  X  Motif discovery  X  Correlation  X  Lower bound  X  Admissible heuristics 1 Introduction Time series motifs are repeated patterns in a noisy and long time series data. Typical use of time series motifs is in unsupervised data exploration or analytics. Figure 1 shows two real examples where enumeration of multiple repetitions or motifs from a time series are used for knowledge discovery. First example is in understanding clusters of genes affecting the locomotion of a nematode ( Caenorhabditis elegans )[ 4 ], and the second example is in understanding the repetitive structure of human brain activity recorded through Electroen-cephalography (EEG) [ 17 ]. In both of the examples, scientists have run a motif discovery algorithm numerous times to find the repeated patterns for a range of lengths and process the patterns to mine the data.

We refer the problem of finding repetitions at different lengths as enumeration of motifs in a time series. The enumerated motifs can be used in many data mining tasks. The set of motifs work as a summarization of the data [ 5 ], when the time series data contain a lot of noise. Clustering the enumerated motifs is shown to be better than clustering all of the subsequences in time series [ 24 ]. Enumerated motifs can be used to compress time series data tightly [ 15 ]. There has been recent work on identifying the best among the motifs of different lengths by using compression-based measures [ 30 ]. Finally, all of these applications work better if we can find a comprehensive enumeration over a range of lengths.
There has been a considerable amount of work on time series motif discovery that suc-cessfully find motifs from a variety of time series data. Exact algorithms find motifs for a given length [ 15 , 17 ] and are believed to be intractable for enumeration because of the mas-sive computation time required to find motifs for all lengths. For example, to find all the motifs in Fig. 1 a, it takes about fifteen hours on a four-way down-sampled data. A number of approximate algorithms exist that project the data in lower dimensions and find approximate motifs in the lower dimensional space [ 11 , 19 ]. None of them guarantees finding motifs of various lengths and all of them require a set of data-dependent parameters. Multiple trials are needed to tune those parameters for arbitrary data, and often, the process is as time con-suming as running the exact methods. In the worst case, different segments of the time series may require different sets of parameters and thus, making the search for the best parameters infeasible.

In this paper, we describe an exact algorithm to enumerate motifs of all lengths. The algorithm searches for the entire range of motif lengths and outputs only maximally covering motifs. Figure 2 shows a set of maximally covering motifs of various lengths discovered by our tool from an EEG trace. The algorithm is significantly faster than running the existing exact algorithms multiple times and an order of magnitude faster than the naive solution. We demonstrate the application of the algorithm to discover high-quality motifs in entomological exploration, oceanography and activity/dance pattern recognition.

The key feature of the algorithm is a novel bound on the similarity function to prune most of the similarity computations for successive lengths once the motifs for the first length are found. The algorithm is free of data-dependent parameter and uses only linear space, just enough to store the time series and some additional statistics. We add a knob to configure the number of motifs to output for iterative exploration.

In addition to the exact method described in the original paper [ 14 ], we introduce an approximate method to find motifs of all lengths at an interactive speed. The method has a provable error guarantee and produces high-quality motifs that are as good as the motifs found by the exact algorithm. This approximation technique is generalizable to any subsequence search problem such as shapelet discovery [ 16 , 29 ] and discord discovery [ 28 ].
We organize the rest of the paper in traditional fashion starting with some background on existing work (Sect. 2 ). We define the necessary notations in Sect. 3 . In Sect. 4 ,wegive the trivial algorithm first and then provide an intuitive description of the method and finally describe the exact algorithm and its variants. In Sect. 5 , we describe the approximate algorithm and prove the error bound. In Sect. 6 , we show experimental results and show the case studies in Sect. 7 . 2 Background and related work Time series motif discovery was introduced in 2002 [ 11 ] with a hash-based technique to find repeated patterns. Since then, numerous algorithms have been proposed focusing on many applications [ 6 , 19 , 21 , 26 , 27 ]. Many of the methods for motif discovery are based on searching a discrete approximation of the time series, inspired by and leveraging off the rich literature of motif discovery in discrete data such as DNA sequences.

Several of the algorithms are based on SAX (Symbolic Aggregate approXimation) repre-sentation that discretizes both time and y -values and represents the time series with symbols [ 6 , 21 , 28 ]. The quality of the discretization process depends on two parameters, word size w and alphabet size  X  . For example, large w on a noisy time series can be massively lossy. Some algorithms are based on locality sensitive hashing and depends on optimal setting of at least four independent parameters. Besides the undesirable list of parameters, the algorithms also have some complexity in space usage. They extract all the contiguous subsequences for a specific length ( m ) and convert all of them to SAX representation. Therefore, the space requirement is O ( n m w ) where n is the length of the time series. In motif enumeration, the maximum length of a motif can be arbitrarily large limiting the largest time series we can input.

It has long been held that the exact motif discovery is intractable even for datasets residing in main memory. In a recent work, the current authors have shown that motif discovery is tractable for large in-core datasets [ 17 ]; however, the algorithm we proposed (called MK) also suffers from a very demanding parameter m and O ( nm ) space requirement. Enumerating motifs of all lengths can be done by repeatedly running MK but very quickly becomes intractable for moderate-sized datasets.

There are algorithms to discover variable-length motifs [ 10 , 20 , 27 ]. The method in [ 10 ] uses SAX representation, and thus, the method is an approximate one. The method in [ 20 ] uses exhaustive search in the length space by running the MK [ 17 ] algorithm for every length. The paper identifies the phenomenon that only a few lengths provide new motifs (see Fig. 3 ); however, the paper could not use this fact for efficiency. Our method, MOEN, is an order of magnitude faster than running MK for every length. The method in [ 27 ] does not even normalize the subsequences and therefore cannot detect motifs with a shift and scaling. 3 Definition In this section, we define the problem and the other notations used in the paper. Definition 1 A Time Series T is a sequence of real numbers t 1 , t 2 ,..., t n . A time series i and length m .

A time series of length n can have n ( n + 1 ) 2 subsequences of all possible lengths from one to n .

If we are given two time series X and Y of same length m , we can use the euclidean norm of their difference (i.e., X  X  X  X  Y ) as the similarity function. To achieve scale and offset invariance, we must normalize the individual time series using z-normalization before the actual distance is computed. This is a critical step; even tiny differences in scale and offset rapidly swamp any similarity in shape [ 8 ]. The normalized euclidean distance is generally computed by the formula m i = 1 ( x i  X  y i ) 2 where x i = 1  X  Thus, computing a distance value requires time linear on the length of the time series.
In contrast, we can compute the normalized Euclidean distance between X and Y using five numbers derived from X and Y . These numbers are denoted as sufficient statistics in [ 25 ]. The numbers are x , y , x 2 , y 2 and xy . It may appear that we are doing more work than necessary; however, as we make clear in Sect. 4.1 , computing the distance in this manner enables us to reuse computations and reduce the amortized time complexity from linear to constant .

The sample mean and variance can be computed from these statistics as  X  x = 1 m x and  X  2 x = 1 m x 2  X   X  2 x , respectively. The positive correlation and the normalized euclidean distance between X and Y can then be expressed as below [ 18 ].
 Now we define motif and the enumeration problem.
 of length m where i &lt; j .

To extend the above definition for a range of lengths is not as straight forward as making for four hundred different lengths, i.e., [100 500]. Let us take an example to describe the covering relationship between motifs. The first locations of the motifs (see Fig. 3 ) at lengths 250 and 175 are identical (100 % overlap), and therefore, we say 250 covers 175. The number of such covering motifs can vary in different time series. We see, in Fig. 3 , there are only ten different motifs present among the 400 different lengths. For most smooth time series, only a few motifs cover the entire range. Therefore, we need a way to remove this redundancy by showing only the covering motifs.
 Definition 3 A subsequence T i , m covers another subsequence T u , p , p  X  m , if the overlap between T i , m and T u , p is more than a threshold c %. More formally, any of the two conditions is true.
The conditions cover both complete and partial overlaps. One point is worth mentioning here. The definition of motif does not allow any overlap between the subsequences, whereas the definition of cover does allow c % overlap.
 covers it. (a) (b) Problem 1 ( Motif Enumeration ): Given a time series T , find all the maximally covering motifs.

We would like to solve the above problem exactly . Recall, the motivation for finding motifs is to retrieve the most similar segments that, by definition, are the least noisy segments for further investigation by human. Therefore, our algorithm keeps minimizing similarity as the primary goal while finds as many motifs as possible for different lengths.

Note that the definition is free from any parameter. Reader may think c as a parameter of the enumeration algorithm. It is a configuration parameter for the output. We do not use this parameter in speeding up the enumeration algorithm. 3.1 Why the problem is difficult? Finding motifs for all possible lengths is an inherently difficult problem. Here, we describe why. Let us assume we have two time series subsequences T i , 5 =[ 37353 ] and T k , 5 = [ 106989 ] . Let us also consider that z -normalization is not required. The distance between the subsequences T i , 4 and T k , 4 is d = of the fifth elements to d 2 . Therefore, dist ( T i , 5 , T k , 5 ) =
When we compute z -normalized euclidean distance, the situation becomes difficult. The primary reason is that we cannot use the overlap between T i , 4 and T i , 5 while computing subsequence. In Fig. 4 b, the normalized T i , 4 and T k , 4 are shown by the dotted lines and the normalized T i , 5 and T k , 5 are shown by the solid lines. Clearly, the values of the first four elements change with the length of the subsequences, and we cannot just add the squared importantly, dist ( T i , 5 , T k , 5 ) can be smaller than dist ( T i , 4 , T k , 4 ) .
In this paper, we show a way to use the overlaps between subsequences to create bounds on the distances of the longer subsequences. We use the bounds in speeding up the motif enumeration algorithm. 4 Motif enumeration We start with the smartest trivial algorithm that can be designed using known optimization techniques. We then describe our novel bound on distance function and give a faster algorithm to enumerate motif. 4.1 Smart Brute force A brute force algorithm compares all possible pairs of subsequences of all possible lengths. Thus, a naive brute force algorithm takes O ( n 4 ) computation time.

A smart brute force algorithm can use the known techniques described in [ 23 ]. Precisely, we can cache and reuse the dot products of a subsequence with all other subsequences in the entire time series. It costs only linear space and enables a way to compute distances in constant time. Thus, the additional linear space saves us the O ( n ) factor of computation required for distance computation in the naive brute force.

The Algorithm 1 describes the smart brute force algorithm. It takes the time series T and the minimum ( m ) and maximum ( mx ) length of the motifs as input. Note that, the parameters m and mx can easily be set to 0 and n / 2 to enumerate all motifs of all lengths although it is not meaningful to do so. The parameters are more amenable for exploratory analysis than exactly one length m for the motif. They are not data dependent and can be set flexibly to cover a wide range of meaningful lengths.

The lines 3 X 5 compute the cumulative sum of the time series values and the square of the of this dot product array is stored in pr e v XY to use later in line 10. This dot product could have been computed in O ( n log n ) time using fast Fourier transform. We skip this efficient method for simplicity, and it does not hurt the overall complexity of the algorithm.
Lines 11, 16 and 17 show the main three loops of the algorithm. Logically, the loop at line 17 finds the nearest neighbor of T i , j . Lines 12 X 15 compute the dot products for T i , j given we have the dot products for T i  X  1 , j in pr e v XY and store that back in pr e v XY for future Algorithm 1 SmartBruteForce ( T , m , mx ) Algorithm 2 ct Di stance ( T , i , k , j , x , xx , xy ) use (line 15). Lines 18 X 19 compute the dot products for the next length, i.e., from T i , j to T j + 1 . In aggregate, lines 12 X 15 and 18 X 19 maintain the xy array for constant time distance computation, and it is done at line 20. Note that, all the arrays are of the same size as the time series. Line 21 adds the distance values of the pairs in a list to return. This is not a mandatory statement for the smart brute force algorithm. We use the List data structure later as we will be using the Algorithm 1 as a subroutine in our proposed algorithm.

At line 22, after the nearest neighbor of T i , j is found, we can update the best pair for the length of j . And finally when all of the loops are done, we can output the bests for all lengths. 4.2 MOEN: efficient enumeration of motifs The smartest brute force algorithm for motif enumeration described in the previous section runs in O ( n 3 ) time. In this section, we describe an order of magnitude faster algorithm. The algorithm is called MOEN (i.e., MO tif EN umerator). The algorithm is based on a novel bound on normalized Euclidean distance for longer subsequences. 4.2.1 Bounding distances for longer lengths We gave an example in Sect. 3.1 to demonstrate the difficulty of using the overlap between successive subsequences while computing the distance metric. Since the values of the over-lapping portion change after normalization, we cannot reuse computation directly. Instead, we can build bounds for distances of the longer subsequences using the distances between shorter subsequences. More formally, we want to find upper and lower bounds of the dis-to speed up the algorithm. We do not use the upper bounds, yet we show the derivation for completeness.

Let us assume x and y are two normalized sequences of length j  X  1where x r = ( t i + r  X   X  following tautologies.
Let the distance between T i , j and T k , j be d next . We would like to append the values of t and y j  X  1 be  X  x and  X  y . The new mean and variance of  X  x , after the increase in length, can be computed as follows. The mean  X   X  y and variance  X   X  2 y can also be computed in the same manner.
Thus, the distance d next can be computed by calculating the sum of squared error of the new normalized subsequences. At this point, we are in position to define the lower and upper bounds for d next . Lower Bound We start with the lower bound. Imagine we do not know the values of t i + j  X  1 and t k + j  X  1 . We would like to choose the values of x j  X  1 and y j  X  1 as such we achieve the smallest possible d next .

We argue that we achieve a lower bound for d next if we set z = x j  X  1 = y j  X  1 . Let us verify why it is true. If z = x j  X  1 = y j  X  1 , then means and variances of  X  x and  X  y become equal.
Clearly, the contribution of x j  X  1 and y j  X  1 to d next becomes empty. Depending on the value of z , the contribution of the first j  X  1 values also change. The larger the value of z , the larger the means and variances are and the smaller the values of ( x r  X  X   X  x )/  X   X  x are for r = 0 , 1 ,..., j  X  2. If we set z to the maximum value of t i normalized by the means and standard deviations of the immediately preceding subsequence of length j  X  1, we are guaranteed to have the smallest value for d next (see Fig. 5 ).

More formally, we get the lower bound d LB for d next when we set the following.
The lower bound can then be computed as below
The bounds use the maximum value of a sample (i.e., z ) which can be efficiently computed from the time series, and the algorithm for that is described in the next section. Upper bound The upper bound d UB can also be computed as above except we set the fol-lowing to ensure that x j  X  1 and y j  X  1 are opposite to each other.
Note that, the above assignment does not change the variances while the means have opposite signs. The upper bound can then be computed as below Example Let us take the example of Sect. 3.1 . T i , 5 =[ 37353 ] and T k , 5 = [ 106989 ] . We know the normalized distance between T i , 4 =[ 3735 ] and T 4 =[ 10 6 9 8 ] which is 3.97 (rounded for simplicity). The normalized sequences we have computed the maximum normalized value for z = 4;
Then the hypothetical sequences are x r =[ X  0 . 90 1 . 51  X  0 . 900 . 30 4 ] and y r = 1 . 183  X  1 . Therefore, d LB = d  X   X  d
Before we end the section, we demonstrate the goodness of the bounds. We perform an experiment on 1,000 random walks of lengths 255. We compute the normalized distances between all possible pairs of the random walks and plot them in ascending order in Fig. 6 . Then we increase the length of the random walks by one and compute the d LB , d UB and the d next for all possible pairs. We plot them together to see the tightness of the bound.
As depicted in the Fig. 6 , the bounds are really tight and the biggest advantage is that the bounds are constant factors away from the distances at length 255. Therefore, the orders of We use this fact in our algorithm rigorously. 4.2.2 Computing the M ax array As described in the previous subsection, we use the maximum value of a sample, t i , relative to its immediately preceding subsequence while bounding the Euclidean distance. In effect, we want to compute an array Max where
There exists an algorithm to exactly compute the maximum array Max in O ( n 2 ) time (shown in Algorithm 3 ) for every length. Note that, we track the absolute values for each length at line 11. We skip describing other parts of the algorithm for simplicity.
To enumerate motifs of all lengths, the O ( n 2 ) is acceptable as the entire algorithm has higher complexity. However, we can set a pessimistic z value in linear time by normalizing Algorithm 3 ComputeMax ( T , m , mx ) the time series and taking the twice of the absolute value of any individual observation. Mathematically, by setting z = 2  X  max ( abs ( x ), abs ( y )) .

However, there can be datasets where noisy spikes can massively impact the value of z in the bounding equations and thus reducing the benefit of the bounds. The value of z is the largest possible value of an observation in the dataset. A z -normalized time series has mean and variance equal to 0 and 1, and the maximum value can be infinitely large. We can make probabilistic assumption about z if the time series are large and z -normalized. If we assume that an unknown observation z will follow a standard normal distribution, we can argue that, P ( z &gt; C ) = 1  X  1  X  a very small probability ( 10  X  5 ) to appear in an unknown observation of a normalized time series. In this way, we can set a hard-coded value of 5 for z for noisy datasets and have high confidence that the bound is almost always correct. 4.2.3 Computing the maximal motif In the last two sections, we show the ways to bound the distances of longer subsequences using the distances of the shorter ones in constant time by using only one factor. In this section, we use the lower bounds to speed up the motif enumeration algorithm. The algorithm is shown in Algorithm 4 .

The algorithm first computes the Max array in line 1 and then computes the motif pair the Algorithm 1 by passing m as both the minimum and maximum length to search. Thus, the brute force algorithm runs once for the length m and, in return, sends a list of triplets dist , i , k sorted by the dist field in ascending order.

If we wanted to return all the pairwise distances in the sorted list, it would have required quadratic space. Instead, we use only linear space to store the best O ( n ) pairwise distances. The key idea is that there can be O ( n ) maximally covering motifs among the O ( n 3 ) possible implementation, we store exactly n pairs in the List .
 computes the lower bound by multiplying the magic constant. This lower bound represents the whole set of pairs that we did not keep in the List because none of those pairs could have Algorithm 4 MOEN ( T , m , mx ) a smaller distance than LB for length m + 1. Thus, LB helps the algorithm to confine the search space for the next motif pair of length m + 1 within the pairs in the List .
Let us now look at the behavior of the algorithm for j = m + 1 at line 5. The algorithm loops through the pairs in the List and computes the distances of all the pairs in it at line 8. The distance function is not a constant time function as the ct Di stance used in Algorithm 1 . It normalizes the sequences and computes the distance and therefore requires at least one full scan of the sequences. By using more space and caching techniques, it can be optimized but not required.

When the loop at line 6 is finished execution, we have all the new distances for the pairs in the List stored in the NewList and at line 9, we find the minimum of the NewList to get the best pair of length m + 1 among the O ( n ) pairs of length m from List .

At line 11, the algorithm makes a key decision. If the best distance is less than the LB ,then we know that none of the pairs that are not in the list can be smaller than the best distance, and therefore, the algorithm has found the motif for length m + 1. The algorithm updates the LB for the next iteration at line 13. Since the lower bounds are constant factor approximation of the truth, we can cascade the factor to find the LB for length j + 1 = m + 2. The hypothesis remains the same for the next iteration: none of the skipped pairs can have a smaller distance than LB for length j + 1.

The algorithm stores the locations of the motif pair in two parallel arrays L 1and L 2at line 14. These arrays will be used to find the maximally covering set. The algorithm outputs the best pair for the current iteration at line 15.

As the LB gets smaller and goes below best for some length j , LB loses its pruning power. At this point, the algorithm is no more confirm about finding the smallest distance for length j + 1 and therefore calls the SmartBruteForce algorithm for length j . Thereafter, the algorithm has a new LB (line 18) with a new List (line 17) of pairs that have more pruning power and continues for j + 1.

One minor implementation detail is worth mentioning here. The List is best implemented as a max-heap when we insert and find the maximum in the SmartBruteForce algorithm. In Algorithm 5 isCo v er i ng ( i , m , u , p ) Algorithm 6 Co v er i ng M ot i f s ( L 1 , L 2 ) the MOEN algorithm, we iterate and find the minimum in the List , and therefore, it is best implemented as a sorted array . To facilitate the implementation, we change the data structure of the List in MOEN algorithm after lines 2 and 17 from max-heap to sorted array. 4.2.4 Finding the covering motifs In Algorithm 6 , we describe how to find the set of covering motifs given the locations of the motifs for the whole range of lengths. The algorithm uses the fact that L 1 i is always less than L 2 i and it is ensured by both SmartBruteForce and MOEN . The algorithm starts with the smallest length. For each length i , it searches the motifs of length i + 1to mx to see whether any motif covers the i th one. Line 4 calls the isCovering function described in the Algorithm 5 which returns true if the longer subsequence covers the shorter. Whenever a motif is covered, the algorithm marks it. At the end, the motifs without mark are collected as the covering motifs. 4.2.5 Enumerating more motifs It is possible that we find only one motif that covers all the other motifs of smaller lengths. The reason is that the longer motif is too similar to each other and its shorter versions become motifs for smaller lengths also. If we want more motifs, an obvious solution is to remove the maximally covering motif from the data and run the MOEN algorithm again to find more motif. A more interesting approach would be to generate more motifs without using additional space and time. We can maintain more (e.g., K ) pairs for every length and thus increase the size of the enumeration output. K motif has been studied previously in [ 9 , 28 ] for a given length. We use them here for enumeration purposes.
 Definition 5 A K-motif is the K -most similar pairs of non-over-lapping subsequences of length m such that none of the K pairs cover each other. It is easy to find the K -motifs given the List data structure. We do it with a linear scan. Let us look at an example for a length m . Imagine we have the pairs of subsequences in the ascending order of the distances stored in the List at line 2 of the Algorithm 4 . Recall they are all of the same length. Imagine an example case where List ( 1 ) covers List ( 2 ), List ( 2 ) disjoint motif pairs and as we go down the List , we lookup whether the pair is covered by any pair in the index. If not covered, we insert that pair in the index and continue until K motifs are reported. This process is not very time consuming as the index can have at most K pairs, and we may need to scan at most 4 Km pairs in the List .

As we find the K -motifs for each length, we can use the same definitions of cover and maximally covering motifs as in Sect. 3 to define K -motif enumeration problem. Note that the definition of motif in Sect. 3 is for 1-motif.
 Problem 2 ( K-Motif Enumeration ) Given a time series T , find the maximally covering K -motifs.

Now that we have defined the K -motif enumeration problem, we show the changes required in the MOEN algorithm. At line 2, after getting the List , the algorithm finds the K -motif of length m and stores them in L 1and L 2. Then it enters the loops and at line 10, instead of the Min , the algorithm uses the distance of the K -th motif. This guarantees that LB is good for the K -motif enumeration problem.

Since L 1and L 2 are now having multiple motifs of the same length, there is a change needed in the Co v er i ng M ot i f s algorithm. We have to add a loop to look for the covering motif for each of the K motifs of length i . We also need to add loop to try each of the K motifs of length j to see if it covers the motif of length i . The addition of these loops does not change the overall complexity of the algorithm as they run after MOEN outputs.
Note that the value of K does not necessarily guarantee there will be at least K motifs enumerated unless c = 0. By changing K and c , we can only change the number of output motifs flexibly without taking additional runs. 5 Speeding-up motif discovery In the previous section, we have devised an algorithm that can enumerate all the motifs of all lengths. The algorithm gains its efficiency by reusing search results of one length for the subsequent lengths in linear space and time. However, the algorithm needs to perform the quadratic search at some of the lengths, and this takes the majority of the time.
In this section, we describe a method to increase the efficiency of the core motif search shown in Algorithm 1 . We will use a simple trick to convert the Algorithm 1 by replacing line 17 with  X  for k  X  i + j to n  X  j by S do . X  Here S is a positive integer that dictates how many consecutive sequences the loop can skip. If S = 1, it is the same as the smart brute force algorithm. However, when S &gt; 1, we can expect a faster termination of the loop and thus achieving some speedup. Note that, skipping every S positions is not the same as S -way down-damping of the time series because the correlation coefficients are still computed in the original resolution.

Skipping S subsequences in the above loop has its consequence. The MOEN algorithm might miss the best pairs for a set of lengths. Good news is that we can quantify the loss using the lower bound function. Recall we have a way to compute the lower bound of d next bound d + 1 . We want to compute the lower bound of d + S given d which will be an S -step lower bound.
 len the more close the fraction is to one. A pessimistic choice would be to assume that we repeatedly apply the fraction for len instead of the fractions for len + 1 , len + 2 ,..., len + S d
If we do not know any specific length len and m is the minimum length parameter provided by the user, the generic fraction for an S -step lower bound would be the following.
Thus, we derive the S -step lower bound for an arbitrary sequence pair having a distance d as d 2 LB S = f S m d 2 . We use the bound to define the maximum error we can have in our smart brute force algorithm.
 Theorem 1 If the distance value of a motif produced by an S-skipped algorithm is d S and the minimum distance value of the best motif produced by the original algorithm is d then d length. If we skip every S positions of the inner loop, the number of pairs of locations is misses. There exists a pair p that the algorithm compared, and we can extend that pair up to S -steps to obtain the optimal pair. If we assume, pessimistically, p has the minimum possible distance (i.e., d S ), then the distance of o should be larger than the lower bound for S -step increase in p . Therefore, d 2 &gt; d 2 S f S m .
 To demonstrate the above theorem in action, we do an experiment varying the skipSize S and running the smart brute force algorithm to find the motif (see Fig. 7 ). We plot the discovered best correlation in red and the worst-case drop in the correlation that could be possible for a skipSize in blue. The discovered best correlation is mostly the same as the best one. When it drops, it never drops below the blue curve showing the theorem is correct.
In the plots the worst possible correlation is even more than 0.9. Therefore, we can easily set skipSize S to 32 and get 32  X  speedup. However, for some datasets, the worst correlation can be as bad as zero (the trivial bound). There can be two reasons for that. First, a dataset can have large spikes that cause a large maxV value in the algorithm. The larger the value of maxV , the less gain is the in speedup. Second, the dataset may not have a good correlated pair of segments, and therefore, pruning is not quite possible in such dataset.
To summarize, if one discovers a 0.2 correlated motif using skipSize S = 32, it is highly likely there exists no good motif. If one discovers 0.9 correlated motif, the pair is as good as the best one. Note that, the skipping technique is not related to the MOEN algorithm. It can speedup the Algorithm 1 while approximating the motifs. 6 Experiments We have evaluated the MOEN algorithm experimentally. We implemented the algorithm in C# language and run our experiment on a commodity desktop computer. We argue that the experiments are absolutely reproducible given the detailed description in the paper and the free datasets used. The code, the slides and raw numbers of the experiments of this paper are available at [ 1 ].

We start with a sanity check by a  X  X lanted motif X  problem. We then show the scalability experiments. 6.1 The planted motif We design a  X  X lanted motif X  problem for motif enumeration. We insert three patterns; a sinusoid wave, a square wave and a sawtooth wave into a sequence of white noise having a ultra-low-frequency trend. We insert two copies of the same pattern with different scaling and noise addition. The resulting time series is shown in Fig. 8 a. The enumerator is expected to find all of the three waves.

In our experiment, we indeed find the patterns. The success in this experiment strongly depends on the scale of the waves, the wandering baseline and the scale of the added noise. (a) (b) For example, the plantation shown in Fig. 8 a is a difficult one, and the 1-motif enumerator fails to identify all of the three waves. More precisely, 1-motif finds only motif 2 and 3 shown in Fig. 8 b.

At this point, we employ our K -motif enumeration algorithm and use K =3andsuc-cessfully discover the four waves shown in Fig. 8 b. Note that the segments in motif 1 and 2have 1420  X  1378 87 = 48 . 27 % overlap which is less than the c = 80 % threshold set for the experiment. 6.2 Scalability We use three datasets to compare the scalability against two algorithms. EEG : An EEG trace of 180,000 samples [ 17 ]. Random Walk : A synthetic random walk data to model financial time series. EOG : An electro-oculogram of 8 million samples [ 7 ].
 We use the SmartBruteForce algorithm and Iterative MK algorithm to compare with the MOEN algorithm. Iterative MK repeatedly uses the existing exact algorithm (MK) [ 17 ]for finding motifs of all lengths. MK algorithm uses pruning techniques after dimensionality reduction to reduce the number of distance computations and guarantees no false negatives. enumerate motifs.

Note that, MOEN neither uses any dimensionality reduction technique nor it uses quadratic space. And more importantly, there is an opportunity to use both of these techniques in MOEN for further improvement. We can easily optimize the periodic calls to the SmartBruteForce algorithm in MOEN by these techniques for more speedup. We leave such improvement for future work.

Increasing the data size Our first experiment is to empirically show the growth of the execution time as the size of the datasets grow. In Fig. 9 , we show the execution time of MOEN for the three datasets mentioned above. Since the smart brute force algorithm takes identical time for all of the datasets, we show the curve once. We show the best timing for Iterative MK . As claimed, we see an order of magnitude speedup for all of the datasets from both of the SmartBruteForce and Iterative MK . Increasing the range of lengths We experiment on the effects of larger range of lengths. We test two different counter while changing the range of lengths. We, first, measure the execution time and plot in the Fig. 9 (right). The relationship is linear as expected from the complexity expression and holds for all the datasets and for both of the algorithms. Clearly, MOEN grows much slowly than the brute force algorithm. Second, we count the number of calls to the SmartBruteForce algorithm (the costliest part of MOEN ) and show in the Fig. 10 . As expected, the number of calls grows sublinearly with the range of length. For these experiments, we set the minLength to 128.
 Changing K and c : K and c are two parameters to control the number of outputs of the enumerator without additional running time. The larger the K or the smaller the c , the more the number of motifs are while the execution time does not change significantly as we go to extreme values of K (=200) and c (=0).
 We experiment to see the change in execution time for increasing K and decreasing c . As shown in Fig. 11 , the execution time remains flat for larger/smaller values of K / c which confirms the algorithm is independent of the K / c .

The pruning power of the method depends on the value of z . Large values of z give less pruning performance. We experimentally validate the statement in Fig. 12 .Weusethe fraction of the range of lengths where the MOEN algorithm does not call the quadratic SmartBruteForce algorithm as a measure of the pruning power of the method. The less the fraction, the more is the number of calls to the brute force algorithm and the less speedup we achieve. For small z values, the datasets achieve close to 100 % pruning rate and, with the increase in z pruning rate decreases depending on the datasets. Note that the larger values of z are less probable and we can achieve massive speedup if we manually set a small z sacrificing the exactness guarantee. 6.3 Scalable approximation We experiment to measure the speedup of the approximate version of the SmartBruteForce algorithm described in Sect. 5 . The results of the experiments are shown in Fig. 13 .Ourfirst experiment measures the effect of S on the execution time. The end-to-end execution time decreases rapidly when we increase the skipSize S [Fig. 13 (left)]. We see diminishing return as we increase S and a recommended value of S can be eight for which we achieve more than 3  X  speedup. In this experiment, we use the following parameters, n = 10 , 000 and the range of length is 128 X 256, K = 15 and c = 0 . 1.

Next, we studied the effect of skipping positions. Figure 13 (middle) shows that the z -normalized distance value grows very slowly with the increase in S . Thus, the quality of the motif pairs do not decrease a lot when we skip positions. Note that, it is possible to detect the best motif even when we skip positions, for example, S = 16 detects the same motif as S = 1 in EOG dataset.
Since we fixed the length n = 10 , 000 in our first experiment, we vary the length and check the speedup from S = 1to S = 16. Figure 13 (right) shows the result. It is clear that longer time series are benefited more when we skip sixteen positions for an iteration. 7 Case studies The MOEN algorithm is directly motivated from real scientific applications [ 4 ]. The case they demonstrate how the enumeration of time series motifs across lengths can provide more precision in discovering repetitions. We use MOEN on three different signals; Accelerometer, Sound pressure and EPG (Electrical Penetration Graph). 7.1 Activity recognition: sound pressure In this case study, we use one signal (Environment:SoundPressure:Modest) from a benchmark dataset of context recognition [ 3 , 13 ]. There are 5 activity scenarios performed by two users. Each scenario has around 50 trace instances performed by the two users. An example scenario isgiveninTable 1 . All the instances are randomly concatenated to form a time series of length 46,045. A segment of one such concatenation is shown in Fig. 14 a.

Our goal is to discover motif in this randomly concatenated time series and see whether the motifs come from the same the scenarios. This task is inherently difficult because of two reasons. First, we are using only one signal out of the 28 available signals in the dataset. Second, the scenarios have many common activities (such as walking in the street) to create confusion.

We u s e MOEN to find repetitions of length 128 X 256. We performed ten different random concatenations and find twelve motifs on average with 88.15 % accuracy in finding motifs from identical scenarios. Figure 14 b shows four of the motifs found. We further perform a similarity search for all of these twelve motifs. We find 34 other occurrences with a corre-lation threshold of 0.55 that yield an accuracy of 91.28 % of being generated from identical scenarios. 7.2 Activity recognition: accelerometer In this case study, we have used a set of accelerometer signals from [ 22 ]. In the original data, there were four participants who danced with a remix of Lady Gaga X  X  song  X  X ust Dance. X  The song has 119 beats per minute and it is 4:54 min long. The participants were equipped (a) (b) with four accelerometer devices (with x , y and z axis in each) in four of their body parts; hand, arm, hip and leg. The sampling rate of the accelerometer was good enough to collect at about 10 Hz rate, and we received a set of time series of length 29,700.

Six different dance steps (shown in Table 2 ) were defined and assembled into a choreog-raphy for the study (shown in the top of Fig. 15 ).

Although there are six different dance steps, individual body parts have two or three distinct movements. We show the motion transitions of the body parts in the Fig. 15 by thick waveforms.

Our goal is to enumerate a set of maximally covering motifs for a range of lengths and verify if the motifs align with identical dance steps or step transitions. We use our algorithm to find 3 motifs for all the twelve accelerometer signals of one participant. In Fig. 15 ,weshow all of the motifs aligned to the known choreography. The motifs are grouped by the body parts carrying the sensors, and acceleration axes are marked on the left. For example, the green motif in the x -axis of the hand is aligned with A to D transitions in the choreography. sideways X  although the dance steps are different (i.e., C and B). There are 37 motifs detected respective body parts. This corresponds to about 86.5 % accuracy.

We can use such an alignment to solve a state-sequence reconstruction problem that requires finding the choreography from the acceleration signals. We can use transitivity of the motifs to find all the occurrences of a motif and create a global sequence of motifs/events. For example, the green in z -axis and yellow/violate in the y -axis of the hand signal can be segments and conflicting joins. However, with sufficient number of participants and initial motifs, we conjecture to reconstruct the state sequence, for example, by using error correcting decoders. In future, we will develop algorithms for state-sequence reconstruction using motif enumeration. 7.3 Sensor fusion: movement data In this case study, we use motif discovery on sensor readings from cell phones when subjects walk on random route through parking lot, stairs, parks, etc., having the phones on them. The data are collected from a topcoder match on sensor fusion [ 2 ].

We take two different routes and concatenate fixed size prefixes (10K in this case) of the sensor signals. Our goal is to enumerate motifs in the concatenated signals and test whether the motif pairs come from the same route. Note that, each sensor has three axes and we enumerate motifs in all of them independently. As shown in the Fig. 16 , the total number of motifs found for a length range of 256 X 512 is 53 with only 8 of them falling on two different routes (shown by the cross-marks). Therefore, 84 % of the time the motifs are accurately found in the same route which it supports their significance.

Note that the largest number of errors comes from the gyroscope sensor which is inherently periodic and thus provides more opportunity for the motifs to be similar although their sources are different. 7.4 Data understanding: EPG We use the EPG trace of the beet leafhopper [ 17 ] to enumerate motifs. The motifs found are shown in Fig. 17 .

There are roughly two distinct shapes in the motifs. The noise pattern (N) and the spikes (S) as shown in the figure. We can use the pattern identifiers to describe the motifs. For example, the motif 334 can be written as NSN. Similarly the motif 384 can be written as NNS. As we go on with the next motifs, we find other valid combinations of the two distinct shapes. For example, the 799 motifs have NSNNSNN and SSSNSSN patterns.

Entomologists confirm us the patterns N represent certain excretion process of the insect [ 17 ]. With the longer motifs, we now know that three successive excretions can happen. Simi-larly, the motif 565 evidently says that two spikes can happen successively. Thus, enumerating more motifs of larger lengths gives us more information about the possible arrangements of the patterns and helps understanding the sequential structure of the data. 7.5 Motifs to find rare operating conditions Time series motifs can be applied in higher level analytics. Imagine that we have a large time series of power consumption of a household refrigerator [ 12 ] shown in Fig. 18 (top). The power consumption of a refrigerator has a specific cyclic pattern with on X  X ff segments. The ON X  X FF segments can be of different length, and the Fig. 18 (middle) has several long ON segments sequentially. We use our tool to produce a set of motifs. One of the motifs is shown in the Fig. 18 (bottom) which has a special significance. It shows two long cycles and a high cycle matched almost perfectly with a correlation value of 0.97. The reason why it is interesting is because none of the cycles are the most frequent form of the cycles. We do a similarity search using the red segment as a query to find other matches with correlation more than 0.95 and there is no such match. Thus, this motif shows up a unique pattern that appeared exactly twice in such a long time series. 8Conclusion structural information about the data. In this paper, we have described the first exact algorithm to find maximally covering motifs of all lengths. The algorithm is upto 23  X  faster than the naive method and produces the same results. We have empirically verified the speedup and showed cases of motif discovery in entomology and activity recognition. In future, we plan to investigate new time series data using our enumerator including system performance counters and resource usages.
 References
