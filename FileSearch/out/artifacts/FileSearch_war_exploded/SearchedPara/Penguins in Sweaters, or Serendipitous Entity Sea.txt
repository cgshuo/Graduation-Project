 In many cases, when browsing the Web users are searching for specific information or answers to concrete questions. Sometimes, though, users find unexpected, yet interesting and useful results, and are encouraged to explore further. What makes a result serendipitous? We propose to answer this question by exploring the potential of entities extracted from two sources of user-generated content  X  Wikipedia, a user-curated online encyclopedia, and Yahoo! Answers, a more unconstrained question/answering forum  X  in pro-moting serendipitous search. In this work, the content of each data source is represented as an entity network, which is further enriched with metadata about sentiment, writing quality, and topical category. We devise an algorithm based on lazy random walk with restart to retrieve entity recom-mendations from the networks. We show that our method provides novel results from both datasets, compared to stan-dard web search engines. However, unlike previous research, we find that choosing highly emotional entities does not in-crease user interest for many categories of entities, suggest-ing a more complex relationship between topic matter and the desirable metadata attributes in serendipitous search. H.4.m [ Information Systems Applications ]: Miscella-neous Entity Search, Entity Networks, Serendipity, Interesting-ness, Metadata
Why do penguins wear sweaters? An unsuspecting user may stumble on the answer to this question while research-ing oil spills , finding a song about penguin sweaters made to help rehabilitate penguins injured by oil spills in Tasmania. Such surprises are welcome in serendipitous search , which occurs when a user with no a priori or totally unrelated intentions interacts with a system and acquires interesting information [39]. A system supporting serendipity must pro-vide results that are surprising , semantically cohesive , i.e., relevant to some information need of the user, or just inter-esting . In this paper, we tackle the question of what makes a result serendipitous.

To this end, we examine two of the largest user-generated knowledge repositories: Yahoo! Answers and Wikipedia. Ya-hoo! Answers is nowadays one of the largest community question/answering systems, with millions of users posting millions of questions and hundreds of millions of answers. A study reported in [27] suggested that while Yahoo! An-swers is not optimal for factoid search, it is becoming the destination of choice for complex information needs such as opinion or advice, making it the perfect source to investi-gate serendipitous search. Wikipedia, on the other hand, is a popular collaboratively-edited online encyclopedia that employs a staff of editors and an army of volunteers to main-tain the quality of its articles. The highly curated nature of Wikipedia may make it a more trustworthy source of infor-mation. However, the freedom of conversation on Yahoo! Answers presents its own advantages, containing within it opinions, rumors, and social interest and approval.
Some previous attempts have been made to introduce se-rendipity into browsing systems having a social aspect, such as TweetMotif [33] for exploring Twitter and Auralist [42] for recommending music. However, none of these have rigor-ously defined, operationalized, and evaluated user-generated content-driven serendipitous search. In this paper we de-velop an entity-based exploratory search framework that represents the content of each data source as an entity net-work. We describe some of the challenges of extracting en-tities from these two different sources, as well as building a meaningful similarity measure for entities. Our entity-retrieval algorithm, based on lazy random walk with restart, achieves 67% accuracy on Wikipedia and 72% on Yahoo! Answers (as assessed using crowd-sourcing), putting it on par with similar recommendation systems [6, 7, 8].

Following [16], we delve further into what makes search serendipitous by using metadata of the documents in both collections to compute summary statistics for each entity. This way, we estimate (i) the intensity of the emotion, (ii) the quality of the writing, and (iii) the topical category of the text surrounding each entity. By constraining the result sets using these statistics we measure the extent to which each dimension contributes to the perceived serendipity. http://yanswersblog.com/index.php/archives/2010/ 05/03/1-billion-answers-served/
We take two approaches to assessing serendipity of search results. The former, proposed by Ge et al. [15] in the con-text of recommender systems, considers two main attributes of serendipity: unexpectedness (or surprise) and usefulness. The unexpectedness factor is computed by comparing to some  X  X bvious X  baseline, while the usefulness can be esti-mated using standard relevance judgments.

In the second approach we go beyond relevance by also considering the interestingness of the results. Previously, Andre et al. [2] evaluated web search results in terms of their relevance and  X  X nterestingness X , hypothesizing that  X  X earch results that are interesting but not highly relevant indicate a potential for serendipity. X  Using crowd-sourcing we conduct a set of experiments that concern various kinds of entities, such as people, places, events, websites, gadgets, sports, and health-related topics. Our experiments include tens of thou-sands evaluations.

The remainder of this paper is organized as follows. Sec-tion 2 describes related work and positions our approach. In Section 3 we introduce the datasets and the method-ology applied to construct the entity networks. Section 4 presents the retrieval algorithm, while Section 5 describes and analyzes the metadata extracted to supplement the en-tity networks. In Section 6 we report the performance of our retrieval method. Finally, Section 7 presents our two approaches for evaluating the serendipity of the retrieved re-sults, analyzing the effect of metadata constraints. We end the paper with conclusions and thoughts for future work.
General-purpose search engines have been extended in various ways to support exploration and promote serendi-pity, for instance, using corpus-wide analysis [33], adding a temporal dimension [37], and by incorporating domain-specific resources [9]. Some attempts have been made to characterize the interestingness of documents. For instance, when browsing news stories, O X  X rien [32] finds that people clicked on articles that were weird, odd, and shocking, even though they were not necessarily interested in reading about the topic. We go one step further, by exploiting what inter-net users may consider interesting because they are writing about it. We do so within the context of entity search .
Many useful facts about entities (people, locations, orga-nizations, or products) and their relationships can be found in data sources such as Wikipedia or Freebase. Others need to be extracted from unstructured text, as is the case with Yahoo! Answers. The problem of discovering interesting re-lations from unstructured text has led to a surge in research on entity search [4, 11, 17, 26, 29, 30, 34], along with eval-uation efforts like INEX Entity and Linked Data tracks, TREC Entity track, 3 and SemSearch challenge. 4 An en-tity search system requires to extract entities, measure the proximity between two entities, and rank entities according to their proximity to a query entity.

For entity extraction, we follow the common approach that for an extracted entity to exist, it must appear as a Wi-kipedia page [17, 26, 29, 30, 34]. The problems of measuring entity similarity, and retrieving entities related to an input http://www.inex.otago.ac.nz/tracks/ entity-ranking/entity-ranking.asp http://ilps.science.uva.nl/trec-entity/ http://semsearch.yahoo.com/ entity, have been tackled in several works [10, 11, 19, 24] by building graphs of entities and their relations, and apply-ing random-walk computations [12, 20] on these graphs. We adopt a similar approach. We extract entities from Yahoo! Answers and Wikipedia, and build entity networks based on the textual similarity of the documents where entities ap-pear. We also enrich our networks with various metadata.
Other approaches [1, 41] build entity-relationship mod-els, where entities take part (with various roles) in differ-ent types of relations representing real-world associations. These semantically richer models require the usage of struc-tured query languages. Although interfaces supporting such queries exist ([41]), we target non-expert, every-day users of social media. Moreover, we do not at this stage wish to rely on any visualization paradigm. A graph of pairwise relations is a natural choice to model entity similarity in our context.
In terms of algorithms, as our focus is on what makes re-sults relevant and interesting, we use random-walk methods (state of the art for recommendation problems [7, 8, 20]).
A recent workshop Searching4Fun 5 focusing on  X  X leasure-driven, rather than task-driven, search X , has called for more studies looking at  X  X hat makes users happy X  in this type of search [23]. One proposal put forward in [16] is to ex-tract documents that ( i ) contain unexpected nuggets of in-formation, ( ii ) evoke emotional meaning using sentiment analysis, and ( iii ) contain useful knowledge as identified by user-generated metadata. In this paper, we explore simi-lar dimensions of our datasets to understand what makes a data source interesting and which associated metadata (sen-timent, quality and topicality) promotes serendipity and to what extent.

In this paper, we use implicit metadata  X  those extracted from the documents. We focus on quality of the writing, sen-timent strength, and topical category  X  all extracted from the text in the collections. Text categorization is a well known IR task, and many algorithms have been developed to automatically classify documents according to some given taxonomy [36]. Non-topical implicit metadata have also been experimented with, for example related to the qual-ity, credibility, and emotion of the text. Because of its ap-plicability across data sources and its previous use in the context of web search [22], we use readability as our quality metadata. Also, sentiment analysis has become essential for social media-driven applications [21], whether for monitor-ing purpose or as additional feature. Its role in generating interesting results remains to be examined.

Our aim is to provide insights to what makes a result serendipitous, in the context of entity search. Although the visualization paradigm, i.e. the display of the search re-sults, will affect how users experience serendipity, this work focuses on the search results. We leave for future work as-pects concerned with evaluating the user experience. Yahoo! Answers. The largest community-driven ques-tion/answering web portal, Yahoo! Answers was launched in 2005. The portal allows people to ask questions on different topics and answer questions asked by other users, sharing their knowledge and opinions. Every question is assigned fitlab.eu/searching4fun/schedule.php by the asker to one category in a hierarchy of categories. This manual classification of questions into topics is meant to help answerers, who typically find questions by browsing or searching the category hierarchy. We collected a set of Yahoo! Answers documents from 2010-2011. We extracted the English-language questions, and the answers to these questions. Our dataset, dubbed YA, consists of 67 336 144 questions and 261 770 047 answers.
 Wikipedia. Wikipedia 6 is a multilingual, web-based, free-content encyclopedia, written collaboratively by a large num-ber of volunteers. Since its creation in 2001, Wikipedia has grown into one of the largest reference websites, attract-ing 470 million visitors monthly as of February 2012. As of September 2012, there are more than 77 000 active contribu-tors working on over 22 000 000 articles in 285 languages. We use the English Wikipedia dump 7 from December 1, 2011, which consists of 3 795 865 articles. We use WikiExtractor to strip the meta-content and extract the text. In the re-mainder of the paper, we dub this dataset WP.
 Data availability. Our two datasets consist of public data. Dumps of Wikipedia are publicly available, 9 and Yahoo! An-swers data can be collected using a crawler.
We call entity any concept that is well defined and de-scribed in a Wikipedia page. Given a piece of text, we first parse the text to identify surface forms that are candidate mentions of Wikipedia entities. We add entity candidates to each recognized phrase by retrieving the candidates from an offline Wikipedia database.

To resolve each surface form to the correct Wikipedia en-tity we apply the machine-learning approach proposed by Zhou et al. [43]. This approach employs a resolution model based on a rich set of both context-sensitive and context-independent features, derived from Wikipedia and various other data sources including web-behavioral data. The au-thors report that the model achieved 85% precision and 87 . 8% recall when evaluated on a manually-labeled set of news articles. We then use Paranjpe X  X  aboutness ranking model [34] to rank the obtained Wikipedia entities according to their relevance for the text. This model exploits structural and visual properties of web documents, and user feedback derived from search engine click logs. The method achieved 75% accuracy when evaluated against a ground truth of editorial relevance judgements for a collection of query-url pairs. Paranjpe has shown that his approach, even when trained mainly on head web pages, generalizes and performs well on all kinds of documents, including tail pages.
We are aware of the existence of more recent entity extrac-tion tools, such as Wikipedia Miner 10 and Tag Me 11 , which have been shown to outperform previous approaches. How-ever, some technical issues that we had to face while dealing with our large-scale datasets, made us favor the method de-scribe above. We remark that detecting and disambiguating entities that are mentioned in documents is not the objec-tive of this work. We believe that improving the entity-wikipedia.org http://dumps.wikimedia.org/enwiki/20111201/ medialab.di.unipi.it/wiki/Wikipedia_Extractor http://dumps.wikimedia.org/ http://wikipedia-miner.cms.waikato.ac.nz http://tagme.di.unipi.it extraction step will probably lead to improving the overall performance of our system. We leave this for future work.
For consistency, we apply the extraction methodology above to both Wikipedia and Yahoo! Answers; in the YA dataset we apply the algorithm on both questions and answers. In the WP dataset, entities could also be extracted using inter-wiki links associated with the surface forms in the articles. However, such linking is not consistent, and it is often done for only the first few appearances of the entity in an article.
Using the methodology described above, we extract 896 799 distinct entities from YA, and 1 754 069 from WP.
 Similarity Measure for Entities. Starting from the set of entities extracted from each dataset, we construct an en-tity network by using a content-based similarity measure to create arcs between entities. We first build a textual rep-resentation e C of any entity e extracted from a document collection C , by taking the (order-insensitive) concatenation of all the documents in C where entity e appears. We dub entity document such textual representation of an entity.
Let E C be the set of entity documents of the entities ex-tracted from a collection C . Moreover, let L C be the lexicon of C . We extract the lexicon by tokenizing every document, removing stop words and applying Porter X  X  stemming algo-rithm on the obtained tokens [28].

We apply on the set E C of entity documents extracted from a collection C the vector-space model [35]. More pre-cisely, we extract from each entity document e C  X  E | L
C | -dimensional vector v e C , where each dimension repre-sents a term in the lexicon of the collection. Using the well-known TF/IDF scheme, we assign the following weight to term dimension i in the vector representation of e C : respectively represent the frequency of term i in the entity document e C , and the inverse document frequency of i in the collection E C of entity documents.

Once we have created a TF/IDF vector representation of all the entities in a dataset C , we adopt the cosine distance to measure the similarity between two entities. Our arc-weighting function is thus the following:
Because the TF/IDF weights cannot be negative, the sim-ilarity values will range from 0 to 1. Given that cosine dis-tance is a commutative function, we create an undirected network by computing all the pairwise similarities between the entities in a collection. However we do not build a complete graph. Instead we connect with an arc only the pairs that achieve a similarity value higher than a minimum threshold  X  . This standard pruning strategy is used to avoid considering poorly significant relations [5].
 Implementation. Building the entity-network representa-tion of each dataset requires the computation of all pairwise cosine similarities among the entities in the dataset. Per-forming an all-pairs similarity computation is a challenging task when one has to deal with datasets of very large scale, because the number of potential candidates to evaluate is quadratic in the number of nodes and thus can be enor-mous. We have extracted millions of entities from YA and WP, so our problem instances are exactly of this type.
We first reduce the candidate space by restricting to the pairs of entities that co-occur in at least one document. To solve the problem efficiently, we then perform the all-pairs similarity computation by applying the algorithm of Baraglia et al. [5]. The algorithm is a distributed algorithm that works in the Hadoop 12 framework, so as to exploit the aggregated computing and storage capabilities of large clus-ters. Scalability is achieved by embedding state-of-the-art pruning techniques, as well as introducing a partitioning strategy able to overcome memory bottlenecks.
We extract an entity network from each of our two datasets, using the arc-weighting function described above, and set-ting the minimum similarity threshold to  X  = 0 . 5. This value was chosen heuristically in a preliminary assessment of the quality of the similarity measure built. Table 1 pro-vides a basic characterization of the two networks. Node overlap. The YA network contains 51% of the nodes in the WP network. The fact that the number of entities extracted from Yahoo! Answers is smaller than the one ob-tained from Wikipedia is clearly related to the different na-ture (non-curated vs. curated) of the two datasets. In Ya-hoo! Answers, many questions and answers are extremely short, and contain some quick exchange of communication where no entities occur. Instead, Wikipedia documents pro-vide a wealth of useful mentions of other entities that are relevant for the entity that is the subject of the page. Connectivity. The two graphs are almost fully connected. The largest connected component (CC) spans 92 . 15% of the nodes in YA, and 95 . 78% in WP. This is due to the pres-ence of popular entities that appear ubiquitously in the two datasets. These entities represent very common concepts, which are not particular to the subject of a document. Availability. To facilitate reproducibility of our experi-ments, we make our entity networks available upon request. Algorithm. Random-walk based algorithms such as Per-sonalized PageRank [20] or center-piece subgraphs [40] have been applied in many recommendation problems [6, 7, 8, 12]. Our algorithm for extracting from a network the top n en-tities that are most related to a query entity, is inspired by the above research line. More specifically, the algorithm per-forms a lazy random walk with restart to the input entity.
Our method takes as input a graph, a self-loop probabil-ity  X  , and a start vector defined on the nodes of the graph, which in this case contains only the input entity. The ran-dom walk starts in the node corresponding to such entity. At each step, it either remains in the same node with probabil-ity  X  , or follows one of the out-links with probability 1  X   X  . In the latter case, the links are followed with probability proportional to the weights of the arcs. Self-transitions are inserted to reinforce the importance of the starting node, by slowing diffusion to other nodes. The value of the self-loop probability is set to  X  = 0 . 9, following previous works hadoop.apache.org Please email bordino@yahoo-inc.com Figure 1: Distribution of attitude (A), sentimental-ity (S), and readability (R) for YA and WP on query recommendation [6, 12]. We do not use random jumps, because by setting the random-jump probability to the standard value of  X  = 0 . 15, we noticed a worsening of the results. As stopping criterion, we check whether the norm of the difference between two successive iterations is &lt; 10  X  6 , or we stop the random walk after a maximum of 30 iterations. We implement the algorithm by customizing the PageRank implementation contained in the LAW 14 library. Scoring method. Our scoring method basically ranks the entities based on the stationary distribution of the lazy ran-dom walk described above. However, we noticed that popu-lar entities with very large degree appear ubiquitously in the prominent positions of the ranking vectors of all the entities in our datasets. We introduce two corrections for this.
First, we measure the rarity of any entity e in a data collection C by computing its inverse document frequency IDF ( e ) = log( N )  X  log( DF ( e )), where N is the size of col-lection C , and DF ( e ) is the document frequency of entity e . Given the ranking vector obtained for an input entity, we filter out the top M entities with lowest inverse document frequency. These entities represent common concepts that appear in the majority of the documents, and thus are not likely to be relevant to the input entity. The value of M is heuristically set to 500 in YA, and to 1 000 in WP.
For our second correction method, we divide the ranking vector by the global PageRank values obtained by using no personalization (that is, starting at random at any node), and fixing the random jump probability to  X  = 0 . 15. In our experiments we obtain the best results with normalization by the squared root of global PageRank scores.
We extract from our dataset information regarding qual-ity, sentiment, and topical categories. The selection of the metadata was influenced by the  X  X earching4Fun X  workshop (see Section 2), the fact that their extraction could be au-tomated using known tools, and our intuitions about their potential to operationalize serendipity. The metadata fea-tures are first collected at document/sentence level, and then aggregated to derive scores for all the entities in a dataset. Quality. We can derive quality measures for our document collections in several ways. In Yahoo! Answers we could leverage explicit user feedback, such as stars on questions, or best-answer ratings for answers. In the case of Wikipe-dia, we could count dispute alert messages used by Wikipe-dia editors as an indication of poor quality of a document. An alternative that is widely applied in web search is read-ability , which provides an indication of the difficulty that law.di.unimi.it/software.php a reader may encounter in comprehending a text. Lower readability scores are assigned to more sophisticated doc-uments, which require higher education level to be under-stood. We choose readability, because of its applicability on both datasets. For each document we compute the Flesch Reading Ease score [14]. We then derive a readability score for every entity by computing the median Reading Ease over all the documents where the entity appears. Figure 1 shows the distribution of the readability scores for entities in the two datasets: observe that Yahoo! Answers entities tend to have higher readability scores, indicating that they were ex-tracted from documents that were easier to understand. Sentiment. We classify the documents in both datasets using SentiStrength, 15 a state-of-the-art tool for extract-ing positive and negative sentiment from informal English text. SentiStrength has been shown to outperform several (un)supervised alternative approaches on a number of dif-ferent social web data sets, including MySpace, Twitter, YouTube, Digg, RunnersWorld, BBCForums [38]. By de-fault, the tool computes document-level sentiment scores: each sentence within a document receives two scores from 1 to 5  X  for positivity and negativity  X  and then the scores are averaged over the document. However, a document is likely to mention many different entities, and the sentiment expressed around them may vary considerably from entity to entity. To obtain entity-level scores, we first compute senti-ment for each mention of an entity in a document, by consid-ering a small window of text around the mention (we include the ten words preceding the mention, and the ten words fol-lowing it). We further calculate attitude and sentimentality metrics [25], which measure  X  X he inclination towards posi-tive or negative sentiments X  and the  X  X mount of sentiment, X  respectively. Finally, for every entity we compute average attitude and sentimentality over all the text segments  X  ex-tracted from different documents  X  where the entity is men-tioned. Figure 1 shows the distribution of entity-level at-titude and sentimentality in the two datasets: notice that Yahoo! Answers entities tend to have higher attitude and higher sentimentality, reflecting how the Q&amp;A forum con-tains a broader expression of opinions and emotions with respect to Wikipedia, which tends to be more neutral. Topic Categories. Both datasets have their own cat-egorization system. In Yahoo! Answers each question is assigned exactly one category chosen by the asker. Every answer to a question is listed under the same category of the question. Wikipedia pages are also organized in a hi-erarchical category structure. However, for consistency and comparability of the results obtained by the two different media, we decide not to use these dataset-specific categories, and to refer to an external categorization. Specifically, we use a proprietary system developed to support automated categorization of various data sources, such as news arti-cles, tweets, web pages and RSS feeds. Our classifier relies sentistrength.wlv.ac.uk on a proprietary taxonomy, designed by an editorial team responsible for maintaining category definitions clear, dis-tinct, organized, and stable in the face of constantly chang-ing data-source and performance requirements. The taxon-omy consists of various sub-taxonomies that cover particular categorical facets, such as People, Organizations, Regions, Events , and a number of main Subjects listed in Table 2.
Our classifier has been trained on a corpus of US-English news articles and tested on various kinds of datasets, achiev-ing a micro-precision at 80% coverage of 92 . 5% on news data, 82% on RSS feeds, and 70% on Wikipedia data. The classi-fier annotates each document with three topical categories. To derive entity-level topical features, we assign to an en-tity the three most frequent categories associated with the documents where the entity occurs.

Notice that our choice of adopting a proprietary classifier was driven by the practical need for a system that could be deployed on Hadoop so as to handle the automatic classifi-cation of datasets of very large scale. In future work we plan to extend our method to public taxonomies, such as ODP ( www.dmoz.org/ ) and Yahoo! Directory ( dir.yahoo.com ). Table 2: Main subjects considered for categorization A rts &amp; Entertainment Beauty Business Education Family &amp; Relationships Finance Food &amp; Cooking Health Hobbies &amp; Personal Activities Home &amp; Garden Nature &amp; Environment Politics &amp; Government Real Estate Science Society &amp; Culture Technology &amp; Electronics
Transportation Travel &amp; Tourism Testbed. We tested the performance of our system using a set of test queries. First, we collected the most searched queries in 2010 and 2011 from Google Zeitgeist. 16 quently we found Wikipedia pages for each of those queries, that is, we identified the entity associated with each query. We finally checked the coverage of these entities in both datasets, YA and WP. Coverage here is defined as the num-ber of documents mentioning each entity. We included in our test set the top 50 queries with highest coverage. The resulting 50 queries encompass a diverse set of topics, such as people, places, websites, events, gadgets, sports, and health. Performance. For each query, we retrieved related enti-ties from the YA and WP entity networks. We then used CrowdFlower.com  X  a virtual marketplace for micro-tasks  X  to assess the relevance of the top 5 results retrieved for the queries in our testbed. To ensure quality, we built a set of gold standard query-result pairs. Contributors were required to complete a preliminary training session, in which they were shown six golden questions and had to provide correct www.google.com/zeitgeist answers for at least four of them, before they were granted access to our task. Moreover, golden questions were also ran-domly inserted in the real task, and used as a hidden test to check the quality of a contributor X  X  performance. Whenever a contributor X  X  accuracy on the gold standard dropped be-low 70%, the contributor was considered untrustworthy and his or her judgements were discarded. In total, 1 587 query-result pairs were labeled, with 3 annotations per task.
Because a large number of labelers were working on largely disjoint sets of tasks, instead of a standard Cohen X  X  kappa we report label overlap between the participants, which is 85% overall. The lowest agreement was on advanced topics in-volving generally unfamiliar entities, such as Secosteroid (a kind of molecule), and those involving thorough understand-ing of an issue, such as the politician Sally Kern retrieved for the query entity Terrorism .

Table 3 shows the number of relevant entities out of top 5 returned. On average, our algorithm performs with a pre-cision of 66.8% on WP and 72.4% on YA. These accuracy values are comparable with those achieved by recent works on recommendation problems [6, 7, 8].

The algorithm performs somewhat similarly on the two datasets, with 0.41 correlation between their performances. Perfect results are achieved for 18 (WP) and 13 (YA) result sets, with 16 (WP) and 9 (YA) result sets having fewer than 3 relevant entities returned. When we examine the ranking performance of our algorithms by comparing the Mean Av-erage Precision scores  X  0.716 (WP) and 0.762 (YA)  X  to precision, we see an improvement in scores, indicating that the relevant entities tend to be shown at the top of the rank-ings. Figure 2 shows the MAP scores of the runs grouped by category. Wikipedia tends to perform well on topics such as major events and sports, whereas Yahoo! Answers does better with people and places.
 Table 3: # relevant entities retrieved in the top 5 Rank aggregation. Although the two datasets have com-parable performance, the overlap between the results is very small  X  an average of 0.6 entities (that is under one result) in common in the top 5. Gadgets, places, and websites have the most overlap of 1.33, 0.78, and 0.75 result per query, respec-Figure 2: Mean Average Precision of entity retrieval by category (# query entities in parentheses) tively. These include popular topics like Facebook . However, most of the results do not overlap, suggesting that combin-ing the results would improve recall, and perhaps introduce more diversity.

To verify this hypothesis, we build a global ranking of the results extracted from the two datasets by applying the sim-ple median-rank aggregation schema proposed by Fagin et al. [13]. We consider each of the two top-n ( n = 5) result lists as a partial ranking of the recommendations appearing in the union of the two lists, where all the items not ap-pearing in a list form a tie with rank n + 1. To aggregate the two partial rankings, we sort the full set of recommen-dations produced for an entity according to the median of the ranking scores obtained in the single lists (in this sim-ple case there are only two input rankings, so the median score coincides with the mean). This algorithm provides a constant-factor approximation solution [13] for the problem of aggregating partial rankings with ties. The aggregation of the two rankings achieves an accuracy of 74 . 4% and a Mean Average Precision of 0 . 782, improving the performance on the individual datasets.
 Error analysis. A further testament to the efficacy of our retrieval algorithm is the fact that, upon manual inspection, we do not find relevant entities in the immediate neighbor-hood of the query entity. For example, Egypt , an entity for which both runs produce good results, has British Pacific Fleet and FC Groningen (a football club in the Nether-lands) as the top two closest entities in the WP network, and Spring (device) and IGN (an entertainment website) in the YA network. The case of the entity Spring (devise) is especially indicative, since it may have been mistakenly de-tected in text mentioning the  X  X rab Spring X  events, but our algorithm has then downgraded it for the lack of relation with other entities dealing with Egypt.

However, some queries have resulted in non-satisfactory performance. In a few cases, our method has retrieved a list of entities that are highly similar to each other (albeit all relevant to the query); for example, different subtypes of the Influenza virus, or versions of the Apple PowerBook. We believe that a result list with little or no diversity is not of great interest for the user. Such behavior probably happens when our random-walk based algorithm becomes trapped in some small and dense component of the entity network.
Second, an entity may be linked to a tightly knit, but not relevant, neighborhood due to errors of the entity extrac-tor, or to noise in the similarity measure used to build the network, which is based on textual similarity of the content from which the entities were extracted. For example, for the query entity Adele (singer) , a connection has been made to a totally unrelated result, a famous portrait of Adele Bloch-Bauer, hurting performance. Homonymy can cause errors when the similarity between entities is only syntactic, and not semantic. These challenges should be addressed both by improving entity extraction and better informing the re-trieval algorithm; we leave these for future work.
We conduct an extensive study to compare the results extracted by our retrieval algorithm from the two datasets (YA and WP), with the goal of understanding what these two different sources of user-generated content can provide to serendipitous search. We consider a basic scenario in which, for our 50 queries, we compare the results extracted from the two entity networks (YA and WP). Second, we at-tempt to verify which features make the search results more valuable. To this aim we exploit the metadata extracted from the two datasets to enrich the entity networks, and we constrain the retrieval in the dimensions of sentimentality, quality and topical category. For each of the two datasets we build five additional experimental setups in which the re-sults extracted from the general, unconstrained network, are compared to those obtained after introducing a constraint on a specific metadata dimension. To attain the latter, we filter the results of the original retrieval so as to select the top results that satisfy the constraint. The constraints are:
In the remainder of this section we first examine how the constrained setups perform with respect to relevance of re-trieved results (Subsection 7.1). Next we compare these al-ternative runs from various points of view. First we consider a notion of serendipity as measured in terms of the fraction of unexpected results provided by a recommender algorithm, which are also relevant . In Subsection 7.2 we compare our experimental setups with respect to this metric. Next we at-tempt to evaluate other, more subjective aspects of serendip-itous search, such as personal interestingness to the user, or interestingness with respect to the input query. This analy-sis is described in Subsection 7.3.
We evaluate the relevance of the results after they have been passed through the metadata filters. Table 4 shows the precision at 5 for each run and marks the significance of the difference from the unconstrained run. Besides YA and WP, we report a third case, dubbed COM, which represents for each (unconstrained or constrained) scenario, the aggre-gation of the results obtained from YA and WP through
We focus on sentimentality because attitude provided a much weaker signal in preliminary experiments. the median-rank aggregation scheme described in Section 6. Again, we see the combined results outperform the ones from individual datasets. Low-sentimentality and low-readability constraints negatively affect the performance, however we show later (Section 7.3) that they still can improve result interestingness for certain topical categories.
 Table 4: P@5 for constrained retrieval (significance of the difference from unconstrained run: * p &lt; 0 . 05 )
Recent works [15, 33, 42] have shown that, beyond accu-racy, there are many other metrics that can be used to assess the performance of recommender algorithms. Serendipity takes into account the novelty of recommendations and how far recommendations may positively surprise users. To com-pare our exploratory-search setups in terms of serendipity, we adopt a metric introduced by Ge et al. [15], designed to capture two essential aspects of serendipity, unexpectedness and relevance .

Given the set RS of recommendations generated by a rec-ommender system, Ge. et al define the set UNEXP of un-expected recommendations as the recommendations in RS that are not generated by a baseline prediction model PM . Let rel be a function used to assess the relevance of a recom-mendation. Ge et al. calculate the serendipity of set RS as the fraction of unexpected results, which are also relevant:
For relevance, we use the editorial judgements collected through the annotation task described in Section 6. Un-expectedness is measured by comparison with benchmarks that produce expected recommendations. We use four base-line generators of obvious recommendations: 1. Top : For each query, we retrieve the 5 entities that 2. Top Nwp : Similar to previous case, but excluding the 3. Rel : Return the top 5 entities in the related-query 4. Top + Rel Return the union of the sets of entity
We discard all the baseline entities that are too recent with respect to the time frame spanned by our entity networks. Keeping those recent entities might induce an unwanted bias in the results, as it would determine higher unexpectedness.
Table 5 reports the value of the serendipity metric com-puted for each setup, with respect to each baseline. We report results for YA, WP, and for their aggregation COM. Observe that all of our experimental setups achieve higher serendipity when compared to Rel baseline, as opposed to when they are compared to Top . The topic-constrained setup outperforms the other setups in almost every base-line/dataset combination. Results comparable with the topic-constrained run are achieved in the unconstrained case, and also in the high-sentimentality and high-readability run. The low-sentimentality and low-readability setups perform con-siderably worse, due to the fact that these constraints seri-ously hurt relevance, as reported in Table 4.

We also remark that YA always outperforms WP, achiev-ing a value of serendipity which is typically 6%  X  7% higher. The difference is higher (10%  X  15%) in the readability runs. The best results, with respect to all baselines, are achieved by the combination (COM) of YA and WP.

It is also interesting to notice that results do not degrade  X  in fact they slightly improve X  when we discard ( Top Nwp ) from the set of documents used to build the Top baseline, the Wikipedia page corresponding to the input query.
The Rel+Top baseline, which builds a larger pool of entity recommendations, is obviously the strongest baseline, com-pared to which every setup achieves the smallest fraction of serendipitous results.

Finally, the values in parentheses in Table 5 indicate the fraction of unexpected and relevant results computed with respect to the total number of recommendations extracted by each setup (and not with respect to the sole unexpected recommendations, as in the serendipity metric). Observe that this fraction is always almost as high as the corre-sponding serendipity value. This confirms that we are in-deed retrieving a considerable fraction of results that are both unexpected and relevant, even when compared to the most robust Rel + Top baseline.
We next attempt to evaluate other more subjective as-pects of serendipitous search by performing another set of crowd-sourced evaluations. Besides being relevant to the query, the results must be interesting enough to the user to catch his or her attention, and to encourage further explo-ration. Although highly subjective, the dimension of inter-estingness has been used to measure the serendipity of web search results [2] and recommender systems [18]. To make sure we separate intrinsic interestingness of entities from the extent to which a user interested in a search query is inter-ested in a presented result, we ask labelers to consider both questions. Furthermore, we attempt to measure the value of the results by asking whether the result allowed one to learn something new about the query entity.

Finally, we examine the extent to which the metadata used to enrich the networks is useful in improving the seren-dipity of the search results. For this purpose, we conduct this second crowd-sourced evaluation not only on the results extracted from the general, unconstrained YA and WP net-works, but also on the constrained setups described at the beginning of Section 7, where we filter the results of the original retrieval based on topic, high and low sentimental-ity, and high and low readability.
 Methodology. As we explained above, our evaluation takes four dimensions into account: relevance, interestingness for the query, interestingness to the user , and learn something new about the query . Due to the highly subjective nature of these dimensions, we compare the results of our various experimental setups to each other instead of attempting to assign an intrinsic interestingness value to each result. In-spired by Arguello et al [3], we perform pairwise comparisons between all of the result pairs and build a reference result ranking for each dimension. Specifically, given a number of result sets { R 1 ,R 2 ,...,R n } for a query q (obtained from different experimental setups), we take the union of these results, R . For each possible pair in R we present the query and the pair to the labeler and we ask which alternative result is preferable, given the four dimensions we take into consideration. Following [3], we allow three choices:  X  X irst is better X ,  X  X econd is better X , and  X  X oth are bad X . The items in a pair are randomly positioned as first or second.
Each such task is labeled by three annotators. Inciden-tally, there are almost no ties, since  X  X on X  X  know X  selection is almost never chosen. However, it is prohibitively expensive to label each possible pair, especially if there is little overlap between the result sets. Thus, to estimate the proper rank of a result we sample comparison pairs for each result from all possible ones, and we use a voting methodology to rank them into a reference ranking (note that such rankings may differ across the four dimensions).

The difference between the result ranking in each run and this reference ranking can then be used to gauge the differ-ence between the various runs. We use a rank-based distance metric, Kendall X  X  tau-b, which counts the number of concor-dant and discordant pairs of items in a list. Given a pair of items { x,y } from two lists a and b , the pair is said to be concordant if their ordering matches, that is, x i &gt; x y &gt; y j or x i &lt; x j and y i &lt; y j . Kendall X  X  tau-b also takes into account ties, where x i = x j or y i = y j , by subtracting the combinations with tied items from the denominator in order to keep the measure in the range of [  X  1 , 1]. Labeling. We used CrowdFlower.com to label the sam-pled { query,result 1 ,result 2 } triplets. The query and re-sults were shown along with their Wikipedia pages, and four questions were asked: 1. Which result is more relevant to the query? 2. If someone is interested in the query, would they also 3. Even if you are not interested in the query, are these 4. Would you learn anything new about the query?
To maintain quality, we used settings similar to the ones of the first annotation task. Only the contributors who success-fully completed the preliminary training session, providing correct answers to at least four out of six golden questions, were allowed participation. Only the answers to the first question (concerning relevance) were judged for acceptance, being the most objective of the set. Contributors who passed the preliminary training, but then achieved an accuracy on the golden standard lower than 70%, were also excluded. In total, 7,139 tasks were judged (3 annotations for each task), averaging about 13 comparisons for each result. The anno-tation overlap was 83%, 81%, 76%, and 81% for questions 1, 2, 3, and 4, respectively. It is expected that question 3 would have the lowest agreement, in that it asks specifically the personal opinion of the labeler.
 Results. Although highly correlated, the rankings for the four questions are not always the same. When computing the difference between the proportions of preferences be-tween personal interest (Q3) and relevance (Q1) we find the entities which are interesting, but not necessarily very re-lated to the query. The example with which we started this paper is one of such cases: its rank in relevance ranking is 7 and in personal interestingness 4. Other such gems are a New York Times bestseller Water for Elephants for query Robert Pattinson , and article on the History of Ptolemaic Egypt for Egypt . On the opposite end are the entities which are technically relevant but not interesting. These include Britney Spears for Lady Gaga , Cairo Conference for Egypt , and Blu-ray Disc for Netflix .

Table 6 shows Kendall X  X  tau-b between the result sets and the reference ranking. Note that the runs that performed worse on relevance would tend to also have low tau, since the non-relevant entities at the tail of the reference ranking often have the same score (0) and are thus tied for the rank. The table also includes the significance of the difference between the metadata-constrained runs and the unconstrained one using paired t-test (due to larger variance in WP scores, most changes in WP runs are not significant).

For all questions, YA produces results ordered similarly to the reference rank. In fact, for the general, unconstrained runs YA produces better rankings than WP at p &lt; 0 . 05 for all four questions. The correspondence is more pronounced for question 3, which concerns personal interest in the entity. The difference is especially striking, considering a nearly even share of results from both datasets in the top 5 en-tities of each reference ranking (on average having 2.6 and 2.4 results from WP and YA, respectively).

Constraining search results using topical category improves this similarity (though only for YA at p &lt; 0 . 10). Adding a high-sentimentality constraint also boosts the taus for WP, but the same is not true for YA, where the lack of edito-rial oversight allows for low-quality highly-emotional posts. For example, the queries with the lowest taus in the high-sentimentality YA run are Olympic Games , Jos  X e Mourinho , and Middle East  X  topics involving sports and politics. The queries with the highest taus are those which are less likely to produce spirited discussions, such as Mount Everest and Tsunami . When constraining to low readability (thus more sophisticated) documents, both precision and similarity to the reference ranking fall dramatically, especially for YA (at p &lt; 0 . 05). This illustrates the difficulty of evaluation of the quality and usefulness of the documents in informal writings, and we leave the exploration of this area for future research. Figure 3: Change in Kendall X  X  tau-b for interest-ingness to query (Q2) in the constrained runs (ppl: people, sts: sites, gdt: gadgets, spt: sports, plc: places, evn: events, hlt: health)
To further understand how metadata constraints affect performance on particular topics, we plot the change in Kendall X  X  tau-b compared to the unconstrained runs in Fig-ure 3. Whereas the low-sentimentality constraint hurts per-formance for the group sports for WP, the opposite is true for YA. Possibly, the already emotionally-intense sports dis-cussions in YA benefit from a selection of less intense docu-ments. For example, the low-sentimentality constraint brings up techniques such as fast bowling for Cricket and places like Oriel Park for FIFA , replacing famous sportsmen as top results. It is clear that the effect of metadata constraints differs between the groups, and we leave the topic-specific tuning of these facets for future research.
This paper investigates the potential of entities extracted from two sources of user-generated content, Wikipedia and Yahoo! Answers, in promoting serendipitous search. Within the context of entity search, we show that both Wikipedia and Yahoo! Answers offer relevant results which are dissim-ilar to those found through a web search. Also, between the two, the top retrieved entities have often little overlap, sug-gesting the complementary nature of these two data sources. However, Yahoo! Answers shows to be better at favoring the most interesting entities  X  both when considering the query and personally to the labelers. Finally, we observe that the effects of metadata constraints vary across datasets and top-ical categories, suggesting that it is not enough, for instance, to select only emotionally-evocative items in order to catch the user X  X  interest. from unconstrained run: * p &lt; 0 . 05 ,  X  p &lt; 0 . 10 )
A natural extension to this work is to incorporate the tem-poral characteristics of recency, trendiness, and novelty into the definition of goodness , albeit usefulness or serendipity. Another future direction lies in the exploration of the user X  X  experience during serendipitous search. A prototype is cur-rently in development, aiming to both determine effective ways to incorporate entity search within natural information seeking activities, and to visualize search results to promote serendipity. A study in the context of news search already provides good insights on these two aspects [31].
This work was partially funded by the Linguistically Moti-vated Semantic Aggregation Engines (LiMoSINe 18 ) EU project. www.limosine-project.eu
