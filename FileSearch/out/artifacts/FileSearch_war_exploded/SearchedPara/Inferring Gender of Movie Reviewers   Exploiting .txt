 Despite differences in the way that men and women experience goods and communicate their pe rspectives, online review communities typically do not provide participants X  gender. We propose to infer author gender, given a set of reviews of a particular item, and experiment on reviews posted at the Internet Movie Database (IMDb). Using l ogistic regression, we explore the contribution of three types of in formation: 1) style, 2) content, and 3) metadata (e.g. review age, social feedback). Our results concur with previous research, in that there are salient differences in writing style and content between reviews authored by men versus women. However, in comp arison to literary or scientific texts, to which classification task s are often applied, reviews are brief and occur within the context of an ongoing discourse. Therefore, to compensative for the brevity of reviews, content and stylistic features can be augmented with metadata. We find in particular that the perceived utility of a review is an important correlate of gender. The model incorporating all features has a classification accuracy of 73.7% and is not as sensitive to review length as are those based only on stylistic or content features. H.3.3 [ Information Search and Retrieval ]: Information filtering  X  author attributes, gender ; H.3.1 [ Content Analysis and Indexing ]: Linguistic processing. Algorithms, Experimenta tion, Human Factors Text classification, gender, filtering, online community Review communities are large, multi-user information spaces in which participants exchange in formation via textual postings describing their experiences with and opinions of various items. In order to reduce information overload, which often threatens systems in which users encounter unstructured text, communities must adopt measures to help guide users to content of interest [13]. Typically, reviews are filtered by salient properties. For example, filters at popular communities include perceived utility (i.e. collaborative filtering [10];  X  X est X  or  X  X ost helpful X ), chronological ordering, and by revi ewer X  X  rating of the item (i.e.  X  X ost favorable/critical X ). One potentially useful attribute for filtering reviews is gender. However, in many communities, it is difficult to determine the gender of reviewers. In online environments, many participants intentionally maintain a gender-neutral identity, as to avoid attracting undesirable attention and to be taken more seriously [5]. More generally, many have privacy concerns [1, 36] and choose not to disclose personal info rmation. However, since personalization is a key means of guiding users to information likely to be of interest, particularly on the Web, researchers have considered how to automatically infer demographic characteristics, including gender (e.g. [16, 30]). We examine the Internet Movie Database (IMDb.com), where a portion of reviews has reviewer gender information. At many sites, including stores (e.g. Am azon.com) and those focused on movies (e.g. moviereview.com), gender is not provided. To contrast, IMDb has a filter allowing users to identify reviews written by men and women, in order to get a gender-balanced perspective on movies. IMDb  X  X  default display is its collaborative filtering mechanism (i.e. reviews are sorted by what others found  X  X seful X ); by selec ting the  X  X ale/female X  filter, a user is presented with an equal number of reviews written by men and women, interleaved. The number of reviews shown for each gender corresponds to the size of the smaller gender class. Figure 1 shows a review at the forum for Casablanca . The male/female filter has been selected, and it can be seen that a man wrote this particular review. As shown, there are 142 reviews (out of 753 in total), for which author gender is available. Finally, above each review, the perceived utility is displayed in the form  X  x of y people found the following review useful Feedback is solicited following each review, where participants are asked if they found it useful or not. This feedback is used to determine the presentation order of reviews under the default filter,  X  X est. X  Following [40], we define utility as the number of users who found a review useful divided by the total votes (i.e. x/y ). There are several reasons why it would be beneficial for reviewers X  genders to be known. First, men and women often experience goods differently. This is partic ularly true of hedonic goods, such as books or movies, th e consumption of which resembles a personal experience as compared to the c onsumption of utilitarian goods (e.g. pencils) [14]. Factors such as a consumer X  X  age and gender are known to correlate to her preferences for hedonic goods [15]. For instance, while perhaps stereotypical, certain genres of movies (e.g.  X  X earjerkers X ) tend to appeal more to women than to men [29]; thus it is natural to expect such preferen ces to be reflected in reviews. Therefore, when reading reviews about an experience-oriented good, it is likely that users will want to know what others like themselves think about it. Si nce demographic features are correlated to participants X  attitude s and values, people use them to judge whether or not someone is similar to them [38]. Men and women also have unique wa ys of expressing themselves. Lakoff X  X  [22] classic work on gender differences in language proposed 10 salient characteristics of  X  X omen X  X  language. X  For instance, she claimed that women and men use different lexical items (e.g. women describe colors more vividly than men) and that women are more likely to use hedges (e.g.  X  X ind of, X   X  X ort of X ) in order to soften an argument, making it more acceptable to others. Similarly, Tannen [37] theorized that women use  X  X apport-talk, X  in establishing a level of intimacy with interlocutors while men typically adopt a style described as  X  X eport-talk, X  in which they focus on conveying information. Herring [11, 12] found that many differences noted in conversational speech also apply in computer-mediated settings. For example, in online forums, men tend to dominate the interactions. To contrast, female participants typically post fewer messages, and are less likely to receive responses from others. Gefen and Ridings [9] explained th at communication differences in communities have to do with how men and women use these information spaces. While male participants value the chance to showcase their knowledge of a subj ect, women often seek or offer social support for problem solving. One way people use review communities is to learn what others think about an item before inves ting time and money in consuming it [34]. Therefore, a question of interest is whether or not this  X  X isdom of the crowds X  is biased, and if so, how (e.g. [20]). Likewise, we can pose the question of whether users are likely to get a gender-balanced perspective. As will be detailed in Section 3, th e data we analyze is a collection of over 30,000 reviews of 250 movies. Since IMDb X  X  default display is its social filtering mechanism, we ordered the reviews for each movie by perceived utility. We then considered the top 10 and 20 most useful reviews for each movie. Since IMDb displays 10 reviews per page, this corresponds to what users see on the first and second pages of a given review forum. As shown in Table 1, female aut hors contributed only a quarter of the most useful reviews. Years of research on information-seeking behavior suggest that when presented with an ordered list of documents, users rarely look bey ond the first page presented to them (e.g. [17]). Therefore, if it were not for the gender filter, users would read primarily reviews written by men. Having highlighted key reasons why gender identity is important in the context of review communities, we now define the task addressed in our current work. We then explain how this task differs from previous research on the prediction of author gender. Our proposed task is: As in previous work, we will model features of author writing style and review content in inferring review er gender. Most approaches to text categorization problems rely exclusively on features that characterize these two aspects of a text. Commonly used features include token-level measures (e.g. average word or sentence length), vocabulary richness (i.e. size of vo cabulary / length), and counts of occurrences of commonly used words [35]. In addition, researchers have applied dimension-reduction t echniques (e.g. factor analysis) on word frequencies (e.g. [3]). In addition to style and content, we will exploit information available at the IMDb community. This information includes movie metadata (e.g. the number of reviews contribut ed, which gives us a measure of the movie X  X  popularity), as well as re view metadata (e.g. date posted, reviewer X  X  rating of the movie). In addition, we have review metadata that concerns the social feedback mechanism used at IMDb (i.e. utility scores). The attributes used in the model are fully detailed in Section 4. Most previous research relevant to ours concerns  X  X lean X  text, as opposed to user-contributed content that has become so characteristic of the Web. For instance, Koppel and colleagues [19] classified a set of fiction and non-fiction documents from the British National Corpus (BNC). In additi on to lexical features (frequencies of commonly used words), they expl oited a set of  X  X uasi-syntactic X  features, since the BNC is tagged for parts-of-speech. They reported 80% accuracy in predicting author gender. Argamon et al. [2] also predicte d author gender for formal texts from the BNC, however, they exploited different features. In particular, they found differences in the use of pronouns. While male and female authors made re ferences to nominals (i.e. wrote about  X  X hings X ) with similar freque ncies, females X  use of pronouns was much greater, and they used more first person pronouns. Citing Biber [4], they noted that women have a more  X  X nvolved X  writing style, whereas men engage in an  X  X nformative X  style of expression. More recently [3], they turned their attention to inferring author gender in blog text. They compared themes discussed by male versus female bloggers. To deri ve key themes, they used the  X  X eaning extraction method X  [6], performing factor analysis on frequencies of the most common words in the corpus. They discovered 20 themes, and found differences between the texts of male and female bloggers. For in stance,  X  X eligion X  and  X  X olitics X  were common themes discussed by men whereas  X  X onversation X  and  X  X t home X  were popular with women. They reported classification accuracies around 80%. Our task differs from those examined previously, and we are not aware of any work that considers the prediction of author gender in postings in an online community. First, reviews posted to a community are not clean texts. Most communities are not closely moderated and typically allow anyone to post with few barriers to entry (i.e. after creating a user na me and password). Reviews often contain grammatical errors and ma y even be off-topic. Following Sahlgren and Karlgren [32], we direct our efforts toward indentifying features that are reasonable to compute in real-time, and that are generally robust, gi ven the noise in user-contributed content. Therefore, we avoid th e use of syntactic or semantic parsing. Second, movie reviews are signifi cantly shorter than most texts analyzed in previous work. For in stance, in [19], the average text length was over 34,000 words. In [3], the minimum blog length was 500 words. To contrast, our mean review length is only 256 words. Therefore, a movie review provi des fewer clues about author writing style and themes that she or he might discuss. On the other hand, since movie reviews are part of an online discourse of a film as compared to being stand-alone te xts, we can exploit metadata that provides clues about the social aspects of the community. The goal of our current work then, is to examine how the following three types of information contribute to the inference of reviewer gender: 1) writing style, 2) content, a nd 3) movie and review metadata. To create a movie review corpus 2 , we relied on the list of the 250 top films of all time (as of January 2010), as voted by IMDb participants, reasoning that they would have many reviews written by both men and women. At each movie forum, we used the  X  X ale/female X  filter to collect reviews with gender information. For the 250 movies studied, 31,300 reviews had gender identity, out of While there are several other movie review corpora available (e.g. for studying sentiment analys is), we were not able to find existing data with author gender. 157,065 total reviews (19.9%). Within each movie, the proportion of reviews with gender identity ranged from 54% to only 2% (median of 18.5%). It is not possible to calculate the true proportion of reviewers that have disclosed their gender, because IMDb always displays an equal number of male-and female-authored reviews. In other words, for a given movie, if 10 authors had disc losed that they are female, while 35 authors had indicated that they are male, IMDb would only display 10 female-and 10 male-author ed reviews. There appears to be no way to identify the other 25 male reviewers, as this information is not displayed on user profiles. Before performing the analyses that involved word counts, we performed stemming. We also di sregarded words that had occurred only once in the entire data set. As in [32], we reasoned that such terms were very likely to be junk, such as misspellings or left over HTML artifacts. Most movies have been labeled with more than one genre (e.g. The Godfather is labeled as  X  X rime, X   X  X rama, X  and  X  X hriller X ). The following genres are represented in our data set: action, adventure, animation, biography, comedy, crime, drama, family, fantasy, film noir, history, horror, musical, mystery, romance, sci-fi, sport, thriller, war and western.  X  X rama X  is by far the most common genre in our data (158 films) while  X  X port X  (e.g. Million Dollar Baby ) is the least common. Table 2 presents an overview of cor pus attributes. As can be seen, even though there are an equal numbe r of male-and female-authored reviews in the data, male review ers have written over a million words more than have female reviewer s. In addition, over all male-contributed reviews, we find a larger vocabulary as compared to female-authored reviews. Table 3 presents summary statistics for review length in words, as well as the total feedback votes received and the perceived utility of the review. As these distributions are often skewed, the table shows both means and medians. It can be seen that there are significant differences between male-and female-authored reviews; a Wilcoxon-Mann-Whitney rank-sum test [25] for evaluating the difference between the medians of males versus females has a p-value of approximately zero for all three attributes. As noted in Section 1, among th e most useful reviews, only a quarter are written by women. Likewise, it can be seen that the statistics below support Herring X  X  cl aims [12] that men dominate online discussions, with women receiving less attention. Over all, women X  X  reviews receive significantly fewer feedback votes and are viewed by the community as being less useful than men X  X  reviews. In addition, it is interesting to note that men tend to write longer reviews as compared to female reviewers. We use a statistical regression model [27] to examine the contribution of three kinds of information in the task of predicting author gender: 1) aspects of author writing style, 2) content of the review and 3) metadata concerning the movie reviewed and the review. Specifically, we use logi stic regression to model the log odds of a review being written by a female versus a male. More formally, the model to be estimated is: the value of 0 if the author is a man and 1 if a woman). Thus, male is the default and we are predicting the event of a reviewer being female. Stated othe rwise, given a vector of n variables describing a review, we use the predicted likelihood to assign the vector to one of two groups (i.e. if the likelihood of being female is greater than 0.5, the author is labeled as female; otherwise the label is left as male). It s hould be noted that a variety of statistical and machine learning a pproaches has been applied to text classification problems. C iting Yang [39], Stamatatos [35] explains that for relatively larg e data sets (i.e. at least 300 observations for each class), there is little difference in performance of most classifiers on te xt classification tasks. Since our focus is to compare the contribution of the three types of information, rather than to c ontribute to the machine learning research, we employ a multivariate statistics technique. Logistic regression allows us to model a dichotomous response variable, while avoiding assumptions as to the normality of our predictor variables. As discussed in Section 2, pr evious research has found that women have a more engaging style of language use as compared to men, who focus more on c onveying information and less on building relationships. We wish to test whether or not such differences in writing styles between women and men are evident in the movie review genre. Table 4 presents the aspects of author writing style we have incorporated into our present analysis and provides a short justification fo r each. These measures include the rates of pronoun use and hedging, the complexity of words and sentence structure, vocabular y richness, and the frequencies of common words (i.e. lexical markers). outer-focused Complexity 
Vocabulary We also want to examine if ther e are content differences between reviews written by men versus women. In other words, do they discuss different themes when reviewing a movie? Do male-authored or female-authored reviews tend to be more central or unique, given the topics discussed over all reviews? We consider five metrics that express differe nt aspects of review content. The first four metrics presented in Table 5 tell us something about the content of a review, with respect to the other reviews written about the same movie. The first is a measure of review centrality. To compute centrality, we create a centroid, a vector consisting of all words occurring in the entire set of reviews about movie, j . Each element corresponds to the tfidf weight for the given word [33]. The centroid score for review s, which is the sum of the tfidf scores of all words in review s, quantifies the extent to which it contains words that are important across the set of reviews. Perplexity, entropy and the out of vocabulary rate (OOV) are based on a language modeling framework [26]. We view the set of reviews about movie j, but excluding review s , as a bag of words, from which a probability distribution is derived. The creation of review s is then viewed as a sequence of randomly selected words from this distribution. Perplexity, entropy and OOV each measure an aspect of the uniqueness of review s , given all other reviews posted to the forum of movie j. 
Centrality A document-level centroid score [31] Uniqueness 
Themes We use the set of 31,300 reviews to create Figure 2. Factors and lexical items with largest loadings. Our last content feature is base d on Latent Semantic Analysis (LSA) [23]. We first construct a semantic space using our movie review data. Specifically, we begin by computing word counts across all reviews. We then consider the 500 most frequent words. Thus, we have a matrix with dimensions 31,300 by 500, which we subject to a factor analysis. We derive the first 20 factors, corresponding to the eigenvectors w ith the 20 highest eigenvalues. Finally, we compute the scores on these 20 factors for each review. Our analysis will examine if, based on this representation, there are differences in content between the male-and female-authored reviews. Interestingly, researchers employ ing LSA and related methods do not report their analyses in standard ways. For example, [3] and [6] do not explain how they determined the number of factors to use, although they emphasize their interpreta tion. To contrast, [23] uses a very high dimensional space as to account for a maximum amount of variance in the data; however, they do not attempt to interpret what these underlying dimensions mean. We compared candidate models to a baseline with only one factor and computed Bentler and Bonnett X  X  Normed Fit Index (NFI) [24]. approximately 100 factors. Ho wever, interpretation becomes difficult, as many factors have sma ll eigenvalues. Therefore, we also consider the Kaiser criterion, which states that any factor with an eigenvalue less than one should be dropped [18]. We experiment with the model having 20 factors, whose smallest eigenvalue is slightly greater than 1. Figure 2 describes the factors with suggested headings, and shows the words with the largest loadings on each factor. As can be seen, the majority of the factors (e.g. F1, F2 , F12, F14) have straightforward interpretations. However, others ar e less obvious. In particular, it is difficult to distinguish between th e meanings of F5 and F10, both which involve words used to express positive sentiments of movies. It can also be seen that several factors (e.g. F1, F2) overlap with stylistic features. Table 6 summarizes the metadata used in our task. As shown, this information concerns the movie itself (e.g. popularity), review sentiment (i.e. number of stars reviewer gave the movie), the age and length of the reviews and how reviews are received by the community. In addition to the reviewer X  X  rating, we also include the Previous work found that there is a negative relationship between the deviation from average opinion expre ssed in a review forum, and the perceived utility of a review [7]. As noted in Section 3, it is clear that women X  X  reviews receive less atten tion and are seen as being less useful than those written by men. Since we want to predict reviewer gender, we need to examine this phenomenon. Movie popularity # Total reviews about movie j 
Reviewer rating (1 to 10 stars)
Reviewer rating deviation Age (days) Age of review s Length (words) # Words in review s Length (sentences) # Sentences in review s Title length (words) # Words in title of review s
Attention from community 
Perceived utility Utility of review s To examine the contribution of the three types of information for inferring reviewer gender, we build a separate logistic regression model with the features of each type as the predictors: 1) stylistic, 2) content, and 3) metadata features. To control for the possibility that reviews differ in content or wr iting style depending on the movie reviewed, we initially add dummy variables for the 250 movies. However, these are not signifi cant in any of the models. To fit the models, we use a forwar d stepwise procedure [27], which begins with an empty model (i.e. assigning the label of the mode) and current model (i.e. when the Likelihood Ratio test for the effect of the predictor has a p-value &lt; 0.05). Once we have a model, we compare the estimated odds ratios of the predic tors, to identify features in each category that are associated with author gender. Finally, after having examined features of writing style, content and metadata, we follow the same procedure to build a model using features of all three types. Recall that the features of style we examined include the rates of pronoun use, vocabulary richness, the use of hedges, word and sentence complexity, and lexical feat ures. For the lexical features, we first identify the most frequent 50 words in the corpus. This results in a set of 5 content words (movie, film, story, characters, time) and 45 function words (pronouns, articles, prepositions, adverbs and verbs). For each review, we counted the occurrences of these words, resulting in a set of 50 lexical features. Clearly, some of the stylistic features are correlated (e.g. occurrences of the word  X  X  X  Therefore, it is expected that several features will end up not having a significant effect in the model. Table 7 shows the features of wr iting style that have a significant relationship to the log odds of a revi ewer being female versus male. For ease of interpretation, we show the parameter estimates as odds ratios. For example, the odds ratio for vocabulary richness is 6.6. This means that, when holding other factors constant, for a one-unit increase in vocabulary richness, the odds of a reviewer being female versus male increase by a factor of 6.6. Obviously, the closer the odds ratio is to one, the less likely it is that the variable will be useful for discriminating between male a nd female reviewers. While the model as a whole is highly significant, it has a pseudo R alone most likely do not provide enough information to distinguish between male and female reviewers. While we will report the classification accuracy using only sty listic features in Section 5.5, here we discuss the significant diffe rences in terms of writing style between male and female reviewers. Figure 3 summarizes the stylistic tr ends in movie reviews authored by women versus men, which can be observed in Table 7. As shown, female-authored reviews are characterized by a higher rate of pronoun use, and in particular, the singular first person. Despite writing shorter reviews than men, women tend to use a richer vocabulary. To contrast, male-authored reviews are Unlike in OLS regression, the pseudo R 2 cannot be interpreted as the proportion of variance in the independent variable that is explained by the model; it is a simple measure of the strength of association between the predictors and the independent variable. characterized by the use of pre positions. Also, it is interesting that of the five content words, all but the word involving people ( X  X haracter X ) are used more often by men than women. Finally, men X  X  reviews involve more comp lex words and sentences. In particular, the characters-to-wor ds odds ratio is only 0.79. Figure 3: Stylistic trends of women (top) and men (bottom). In summary, our analysis of stylistic features concurs with the claims of previous research, and in particular, that of Argamon and colleagues [2,3]. There is evidence that female movie reviewers tend to write with a more involved style, frequently writing in first person voice. To contrast, men more ofte n use a reporting style, discussing  X  X he movie X  or  X  X  film, X  and making use of the third person pronouns  X  X is X  and  X  X t. X  Table 8 shows the content features th at have a significant effect in the model. The odds ratios greater than one (i.e. features associated with female-authored reviews) are shown at the top of the table and those less than one (i.e. related to male reviewers) at the bottom. We can see that female reviewers often di scuss people and relationships, and use pronouns frequently. The factors representing the use of 1 and 3 rd person pronouns are all positively related to female-authored reviews. In addition, the factors we described as relating to  X  X roups, X   X  X amily X  and  X  X elations X  are associated with women X  X  reviews. As in the model with stylistic features, we again observe that the use of prepositions and articles (F 1) is strongly related to male-authored reviews. The themes di scussed more often by males as compared to females include a movie being an  X  X ll time hit, X  violence, and special effects. Men are also more likely than women to discuss the success of the movie (e.g. if it won an Oscar). Finally, entropy is th e only feature based on language modeling that has a significant effect in the model. Compared to other reviews written about the same movie, male-authored reviews tend to be more unique th an those of females. Perhaps this might partially explain why men X  X  reviews are also perceived as being more useful than those of women. We will return to this question in the discussion section. The likelihood ratio test for th e model as a whole is highly significant. However, like the m odel based on stylistic features, its pseudo R 2 is only 0.09. Therefore, it is uncertain whether content features alone can adequa tely distinguish between male and female-authored reviews. Table 9 shows the model that includes the metadata predictors having a significant effect. The model over all is highly statistically significant and has a pseudo R 2 of 0.1826. As can be seen, six of the metada ta features have a statistically significant effect in the model. In general, reviews written by men tend to be longer, receive more feedback, are seen as more useful by others, are posted early on (i.e. are younger) and have longer titles, when controlling for the popularity of the movie (i.e. number of reviews written). However, it is perceived utility that appears to have the strongest rela tionship to reviewer gender, as its respective odds ratio is only 0.086. As mentioned, many of the stylistic and content features are correlated to one another. In par ticular, the counts of first, second and third person pronouns used as stylistic features, are highly correlated to F2, F4 and F3, respec tively. In addition, F1 accounts for the use of articles and prepositions, eliminating the need for the majority of the lexical features. Therefore, to avoid problems with collinearity in the model using all three types of predictors, we exclude the 50 lexical features. significant. In addition, the m odel based on stylistic, content and metadata features has a pseudo R 2 of 0.2379. As mentioned, the pseudo R 2 does not have a straightforw ard interpretation as does R the context of ordinary least squa res regression. Therefore, we next turn toward evaluating the classification accuracy of each of our models. Table 10. Model with style, content and metadata features. In our analyses, we indentified independent variables that are correlated to author gender. To compare models in terms of their ability to infer gender, and to be able to compare our results to previous work, we now evaluate their classification accuracy on this task. To do this, we use a 10-fold cross-validation procedure [28]. Table 11 reports the average classification accuracies for each model, as well as the respective standard deviation. In addition,  X  p [27] is also shown. This statistic ranges from 0 to 1, and expresses the proportion reducti on in error, relative to the baseline of predicting the mode (i.e. labeling all reviewers as male, which results in 50% error). As a comparison, we also show the accuracy when review utility is the only predictor. As can be seen, while all models do significantly better than the baseline, the model incorporating all three types of predictor variables has the best performance. Its accuracy is significantly better than that of the content onl y and style only models. However, there is not a statistically signifi cant difference between its accuracy and that of the models using only metadata and only review utility to infer gender. Others have previously noted that features characterizing the writing [35]). Therefore, we also examined the average classification accuracy of each of the models, as a function of review length. Figure 4 indeed illustrates that the accuracy of the models incorporating only stylistic and content features is positively correlated to length. To contrast, the models using all features and only metadata are relatively more stable. There are salient differences both in terms of writing style and content between IMDb movie reviews written by male and female authors. However, the perceived utility of the review by community members is the strongest predictor of gender. We briefly examine this point further in order to provide a possible explanation. We then conclude with a discussion of areas for future work. Why is utility such a strong predictor of gender, even when controlling for differences in wr iting style and content and for properties such as length and age? As mentioned in the introduction, consumers with similar demographics are more likely to share tastes in hedonic goods such as movies. Th erefore, a likely explanation is that a large proportion of IMDb par ticipants are men, who give more favorable feedback to male-authored reviews, since they are similar to their own perspectives and comm unicate those views in a way that is familiar to them. We currently do not have the necessary data to examine whether men give better feedback to male-authored reviews as compared to those written by women. In addition, we cannot be sure if the majority of community participants are male or female, since this information is not available in user profiles. However, we can study the characteristics of the highly rated fe male-authored reviews. In other words, could it be the case that reviews authored by women that are perceived to be relatively useful are more similar to male-authored reviews? To examine this question, we divide the female-authored reviews into two groups: more useful (utility &gt; 0.5; n = 2,898) and less useful (utility  X  0.5; n = 12,752). We use the same forward stepwise procedure (with a significance thres hold of 0.05) to fit a logistic regression model to distinguish the more useful reviews from the less useful ones, using stylistic (excl uding the lexical features), content and metadata predictors. Several va riables, which have been noted to be positively correlated to review utility [7], have significant positive effects in the model: total feedb ack votes, review age, reviewer X  X  rating of the movie, and length. Figure 5 displays predictors that have a significant effect in the model, with their respective odds ratios. Of interest is that one of the themes that was associated with male-authored reviews in Table 10,  X  X ll time hit, X  is characteristic of the more useful female-authored reviews. Likewise, we can observe that vocabulary richness, which was associated with female-authored reviews in Table 10, is more correlated to the less useful reviews written by women. Finally, the use of first and second person pronouns is also more characteristic of less useful reviews. In summary, the analysis suggests that reviews written by women, which are viewed by the community as being more useful, do have similarities to male-authored reviews, as compared to less useful female-authored reviews. 
Figure 5: Features of more (top) and less (bottom) useful We have found that there are differences in the way that men and women write movie reviews in an online community environment. Many of these differences concur w ith results of previous research on other genres, including formal [2] and blog [3] text. The findings support the claim that women, in general, have a more involved writing style, both in terms of how they express themselves (e.g. frequently using pronouns and, in pa rticular, the first person singular) and with respect to the themes they discuss in movie reviews (e.g. writing about characters and their rela tionships versus whether or not the movie had won an Oscar or had particular special effects). However, we also found that the classifiers base d only on review content or writing style features are not as accurate as the classifier incorporating all three types of feat ures. In particular, since review utility is strongly correlated to au thor gender, its inclusion as a predictor significantly improves classification accuracy. The other benefit of exploiting metadata from the IMDb community is that the classifier X  X  performance is not as sensitive to review length, as compared to those based only on content and stylistic features. In future work, we plan to reex amine the performance of the three classifiers, given more informati on about each author. Currently, we had only one review per author, on wh ich to base a decision as to his or her gender, along with movie and review metadata. Another task of interest is to infer a user X  X  gende r, given all of his or her postings at the community. In other words, for many users, we will have reviews of multiple movies, along with associated metadata (e.g. the user X  X  average utility over a larger set of contributions). Given that features of writing style and content are known to be sensitive to text length [35] and the brief nature of online reviews, we plan to further explore reviewer gender classifica tion exploiting different types of information. Future work should also address how to effectively and responsibly use reviewer gender informati on within the community. As discussed, IMDb does not display participant gender on profiles, but does offer a gender filter, that offers users the benefit of getting a gender-balanced perspective on a movie of interest. Gender information could also be used to suggest yet unviewed reviews of possible interest to a user, which are written by someone of the same gender. In conclusion, mechanisms can be developed that might help communities balance out the participation and visibility of male and female participants. However, at the same time, care must be taken to ensure that gender identities are not used in a way that might cause privacy concerns among co mmunity participants. The author would like to thank Al exia Panayiotou for discussions about gender, language and the IMDb and acknowledges Mengyuan (Serena) Li for her assistance with data collection. Finally, thanks also go to the four anonymous reviewers for their feedback, which helped to improve the paper. [1] Acquisti, A. and Gross, R. 2006. Imagined communities: [2] Argamon, S., Koppel, M., Fine, J. and Shimoni, A. R. 2003. [3] Argamon, S., Koppel, M., Penneba ker, J. W., and Schler, J. [4] Biber, D. 1995. Dimensions of Register Variation: a Cross-[5] Bruckman, A. 1996. Gender swapping on the Internet. In [6] Chung, C. and Pennebake r, J. W. 2008. Revealing dimensions [7] Danescu-Niculescu-Mizil, C., Ko ssinets, G., Kleinberg, J. and [8] Foltz, P.W., Laham, D., Landauer, T.K. 1999. Automated [9] Gefen, D. and Ridings, C. M. 2005. If you spoke as she does, [10] Goldberg, D., Nichols, D., Oki, B. M., and Terry, D. 1992. [11] Herring, S. C. 2000. Gender differences in CMC: Findings [12] Herring, S. C. 2003. Gender and power in online [13] Hiltz, S. R. and Turoff, M. 1985. Structuring computer-[14] Hirschman, E. C. and Holb rook, M. B. 1982. Hedonic [15] Holbrook, M. B. and Schindler, R. M. 1994. Age, sex and [16] Hu, Jian, Zeng, Hua-Jun, Li, Hua, Niu, Cheng and Chen, [17] Joachims, T., Granka, L., Pan, B., Humbrooke, H., Radlinski, [18] Kaiser, H. F. 1960. The appli cation of electronic computers to [19] Koppel, M., Argamon, S. and Shimoni, A.R. 2004. [20] Kostakos, V. 2009. Is the crowd X  X  wisdom biased? a [21] Lakoff, G. 1973. Hedges: a st udy in meaning criteria and the [22] Lakoff, R. 1973. Language and woman X  X  place. Language in [23] Laudauer, T. K., Foltz, P.W., and Laham, D. 1998. An [24] Loehlin, J. C. 1992. Latent Variable Models . Lawrence [25] Mann, H.B. and Whitney, D.R. 1947. On a test of whether [26] Manning, C. D. and Schutze, H. 2000. Foundations of [27] Menard, S. 2002. Applied Logistic Regression Analysis . [28] Mitchell, T. 1997. Machine Learning . New York: New York: [29] Oliver M. B., Weaver III, J. B., and Sargent, S. L. 2000. An [30] Popescu, A. and Grefenstette , G. 2010. Mining user home [31] Radev, D., Jing, H., Stys, M. and Tam, D. 2004. Centroid-[32] Sahlgren, M. and Karlgren, J. 2009. Terminology mining in [33] Salton, G. and McGill, M.J. 1986. Introduction to Modern [34] Schindler, R.M. and Bickart, B. 2005. Published word of [35] Stamatatos, E., Kokkinakis, G ., and Fakotakis, N. 2000. [36] Stutzman, F. 2006. An evalua tion of identity-sharing behavior [37] Tannen, D. 1990. You Just Don X  X  Understand . New York: [38] Terveen, L. and McDonald, D.W. 2005. Social matching: a [39] Yang, Y. 1999. An evaluation of statistical approaches to text [40] Zhang, Z. and Varadarajan, B. (2006). Utility scoring of 
