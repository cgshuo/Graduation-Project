 Microsoft Development Center Norway University of Amsterdam Trier University clusters and the sample size. This makes the measures unreliable and unfair when the number corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems. 1. Introduction
The task of word sense induction (WSI) has grown in popularity recently. WSI has the advantage of not assuming a predefined inventory of senses. Rather, senses are induced in an unsupervised fashion on the basis of corpus evidence (Sch  X  utze 1998; Purandare and Pedersen 2004). WSI systems can therefore better adapt to different target domains that may require sense inventories of differen t granularities. However, the fact that WSI systems do not rely on fixed inventories also makes it notoriously difficult to evaluate and compare their performance. WSI evaluati on is a type of cluster evaluation problem.
Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl proposed but information theoretic measures have been among the most successful and widely used techniques. One example is the normalized mutual information, also known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010). based estimates of entropy. For instance, the mutual information I ( c , k ) between a gold standard class c and an output cluster k can be written H ( c ) entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence) with its normalized empirical frequency.
 also different from an expected estimate on a larger test set generated from the same distribution, as the bias depends on the size of the sample. This discrepancy negatively affects entropy-based evaluation measures, such as the V-measure. This is different from supervised classification evaluation, where the classification accuracy on a finite test set is expected to be equal to the error rate (for the independent and identically distributed, i.i.d.) case, though it can be different due t ovariance(duetochoiceofthetestset).As long as the number of samples is large with res pect to the number of classes and clusters, the estimate is sufficiently close to the true entropy. Otherwise, the quality of entropy estimators matters and the bias of the estimator can be large. This problem is especially prominent for the ML e stimator (Miller 1955).
 estimators. In a typical setting, the number of examples per word is small X  X or example, less than 100 on average for the SemEval 2010 WSI task. The number of clusters, on the other hand, can be fairly high, with som e systems outputting more than 10 sense clusters per word on average. Because the bias of an entropy estimator is dependent on, among other things, the number of clust ers, the ranking of different WSI systems is partly affected by the number of clusters the y produce. Even worse, the ranking is also affected by the size of the test set. The problem is exacerbated when computing the joint entropy between clusters and classes, H ( k , c ), because this requires estimating the joint probability of cluster-class pairs for which the statistics are even more sparse. theory community and many studies have addressed this issue (e.g., Miller 1955; Batu estimators and their influence on the compu ted evaluation scores. We run simulations using a Zipfian distribution where we know the true entropy. We also compare different estimators against the SemEval 2010 WSI benchmark. Our results strongly suggest that there are estimators, namely, the best-upper-bound (BUB) estimator (Paninski 2003) and jackknifed (Tukey 1958; Quenouille 1956) estimators, which are clearly preferable to the commonly used ML estimators. 2. Clustering Evaluation 2.1 Information-Theoretic Measures
The main challenge in evaluating clustering methods is that successful measures should be able to compare solutions found at differe nt levels of granularity. In other words, one cannot assume that there exists one-to-one mapping between the predicted clusters 672 and the gold-standard classes. A natural approach would be to consider arbitrary statistical dependencies between the cluster assignment k and the class assignment c .
The standard measure of statistical dependence of two random variables is the Shannon mutual information I ( k , c ) (MI). MI is 0 if two variables are independent, and it is equal
Clearly, such measure would favor clusterin gs with higher entropy, and, consequently, normalized versions of MI are normally used to evaluate clusterings. One instance of normalized MI actively used in the context of WSI evaluation is the V-measure, or symmetric uncertainty (Witte and Frank 2005; Rosenberg and Hirschberg 2007): though other forms of MI normalization have also been explored (Strehl and Gosh 2002). maximum likelihood estimators (also called plug-in estimators of entropy) are normally used instead. The ML estimates  X  H have the analytical form of an entropy with the normalized empirical frequency substitu ted instead of the unknown true membership probabilities, for example: where n i is the number of times cluster i appears in the set, m is the number of clusters, and N is the size of the set (i.e., the sample size).

Section 3 for details). In other words, the expectation of and this discrepancy increases with the number of clusters m and decreases with the sample size N .When m is comparable to N , the ML estimator is known to be very inaccurate (Paninski 2004).
 large whereas the total number of occurrences will remain the same as for the estimation of H ( c )and H ( k ). Therefore, the absolute value of the bias for aggregate bias of the estimators of marginal entropy,  X 
V-measure will be positively biased, and this bias would be especially high for systems predicting a large number of clusters.
 factory explanation has been given. The shor tcomings of the ML estimator are especially easy to see on the example of a baseline system that assigns every instance in the testing set to an individual cluster. This baseline, when averaged over the 100 target words, out-performs all the participants X  systems of the SemEval-2010 task on the standard testing set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any real system, the computation is trivial for this baseline. The true V-measure is equal to 0, as the baseline can be regarded as a limiting case of a stochastic system that picks up one of the m clusters under the uniform distribution with m  X  X  X  between any class labels and clustering produced by such model equals 0 for every m .
However, the ML estimate for the V-measure is  X  V ( k , c ) testing set of SemEval 2010, this estimate, averaged over all the words, yields 31.7%, which by far exceeds the best result of any system (16.2%). On an infinite (or sufficiently large) set, however, its performance would change to the worst. This is a problem not only for the baseline but for any system which outputs a large number of classes: The error measures computed on the small test set are far from their expectations on the new data. We will see in our quantitative analyses (Section 5) that using more accurate estimators will have the most significant ef fect on both the V-measure and on the ranks of systems which output richer clustering, agreeing with this argument.
 measures have also been proposed. Examples of such measures include the variation of information measure (Meila 2007) VI ( c , k ) = H ( c | 2001) H ( c | k ). This argument applies to these evaluation measures as well, and they can all be potentially improved by using more accurate estimators. 2.2 Alternative Measures Not only information-theoretic measures have been proposed for clustering evaluation.
An alternative evaluation strategy is to attempt to find the best possible mapping between the predicted clusters and the gold -standard classes and then apply standard measures like precision, recall, and F-score. However, if the best mapping is selected on the test set the result can be overoptimistic, e specially for rich clusterings. Consequently, such methods constrain the set of permissible mappings to a restricted family. For exam-ple, for the F-score , one considers only mappings from each class to a single predicted cluster (Zhao and Karypis 2005; Agirre and Soroa 2007). This restriction is generally too strong for many clustering problems (Meila 2007; Rosenberg and Hirschberg 2007), and especially inappropriate for the WSI evalua tion setting, as it penalizes sense induction systems that induce more fine-grained senses than the ones present in the gold-standard sense set.

F-score measures in that it defines precision and recall in terms of pairs of instances (i.e., effectively evaluating systems based on the proportion of correct links ). However, the
Paired F-score has the undesirable property that it ranks those systems highest which put all instances in one cluster, thereby obtaining perfect recall.
 et al. 2006). This approach in a ddition to the testing set uses an auxiliary mapping set.
First the mapping is induced on the mapping set, then the quality of the mapping is evaluated on the testing set. One problem with this evaluation scenario is that the size selecting the right size of the mapping set. For the WSI task, the importance of the set size was empirically confirmed when the eva luation set was split in proportions 80:20 (80% for the mapping sets, and 20% for testing) instead of the original 60:40 split: The scores of all top 10 systems improved and the ranking changed as well (Manandhar et al. 2010) (see also Table 1 later in this article).
 cessing tasks, such as B 3 (Bagga and Baldwin 1998) or CEAF (Luo 2005) for coreference resolution evaluation. In this article, we a re concerned with entropy-based measures. 674
For a more general assessment of measures for clustering evaluation see Amigo et al. (2009) and Klapaftis and Manandhar (2013). 3. Entropy Estimation
Given the influence that information theory has had on many fields, including signal processing, neurophysiology, and psycholo gy, to name a few, it is not surprising that the topic of entropy estimation has received con siderable attention over the last 50 years.
However, much of the work has focused on s ettings where the number of classes is significantly lower than the size of the sample. More recently a set-up where the sample size N is comparable to the number of classes m has started to receive attention (Paninski 2003, 2004).
 various techniques have been proposed to reduce the bias while controlling the vari-ance (Grasberger and Sch  X  urmann 1996; Batu et al. 2002). We will discuss widely used bias-corrected estimators, the ML estimator with Miller-Madow bias correction (Miller recent technique proposed specifically for the N  X  m setting, the best-upper-bound (BUB) estimator (Paninski 2003). We will conclude this section by explaining how these estimators can be computed using stocha stic (weighted) output of WSI systems. 3.1 Standard Estimators of Entropy
As we discussed earlier, the ML estimator (1) is negatively biased. For a fixed distri-bution p , a little algebra can be used to show that the bias of the maximum likelihood estimator can be written as where E p denotes an expectation under p , D is the Kullback-Leibler (KL) divergence,
Because the KL-divergence is always non-negative, it follows that the bias is always sample is small. In fact, this expression can be used to obtain the asymptotic bias rate correction to the ML estimator, called Miller-Madow bias correction where  X  m is an estimate of m , as the true size of support m may not be known. In our experiments, we use a basic estimator  X  m which is just the number of different clusters (classes or cluster-class pairs depending on the considered entropy) appearing in the sample. We will call the estimator  X  H MM the Miller-Madow (MM) estimator. As the MM estimator is motivated by the asymptotic beh avior of the bias, it is not very appropriate for N  X  m . estimate of the discrepancy in estimates produced from samples of different sizes can be used to correct the ML estimator: If an estimate based on N
Roughly, this intuition is encoded in the jackknifed (JK) estimator (Strong et al. 1998): where  X  H  X  j is the ML estimator based on the original sample excluding the example j . 3.2 BUB Estimator
We can observe that all the previous estimators can be expressed in the form of a linear function of the ordered histogram statistics where h j is the number of classes which appear j times in the sample: where [[ ]] denotes the indicator function. The coefficients a estimators are equal to: form  X  H ( a a a ) as defined in Equation (2). Upper bounds on the bias and variance of such estimators 4 have been stated in Paninski (2003). These bounds imply an upper bound on the standard measure of estimator performance, mean squared error (MSE, the sum of the variance and squared bias). The worst-case estimator is then obtained best-upper-bound estimator. This optimization problem 5 corresponds to a regularized 676 least-squares problem and can be solved analytically (see Appendix A and Paninski [2003] for technical details).
 for a particular type of distribution. This direction can be promising, as the types of distributions observed in WSI are normally fa irly skewed (arguably Zipfian) and tighter bounds on MSE may be possible. In this work ,weusetheuniversalworst-casebounds advocated in Paninski (2003). 3.3 Estimation with Stochastic Predictions
As many WSI systems maintain a distribution over predicted clusters, in SemEval 2010 the participants were encouraged to provide a weighted prediction (i.e., a distribution over potential clusters for each example) in stead of predicting just a single most-likely cluster.
 puted as an expectation under this distrib ution. For estimators of the form of Equa-tion (2), we can exploit the linearity of expe ctations and write the expected value of the estimator as where E  X  p h j is the expected number of classes with j counts. We can rewrite it using the linearity property again, this time for expression (3):
Bernoulli trials  X  p ( l ) i and 1  X   X  p ( l ) i ( l = 1, efficiently computed using one of alternative recursive formulas (Wang 1993). One computation: where j = 1, ... , N . 4. Simulations
Because the true entropy (and V-measure) is not known on the WSI task, we start with simulations where we generated samples from a known distribution and can compare the estimates (and their biases) with the tru e entropy. In all our experiments, we set the number of clusters m to 10 and varied the sample size N (Figure 1). Each point on the graph is the result of averaging o ver 1,000 sampling experiments. the vast majority of occurrences correspond to one or two most common senses even though the total number of senses can be qui te large (Kilgarriff 2004) . This type of long-tail distribution can be modeled with Zipf X  X  law. Consequently, most of our experiments consider Zipfian distributions. For Zipf X  X  law, the probability of choosing an element parameter s correspond to flatter distributions; the distributions with a larger s are increasingly more skewed. The estimators X  p rediction for Zipfian distributions with adifferent s are shown in Figure 2. For s = 4, over 90% of the probability mass is and the estimated values; compare results with a uniform distribution as seen in Figure 1.
 but the smallest sample sizes ( N &gt; 3). The BUB estimator outperforms the JK estimator with very skewed distributions ( s = 3and s = 4) and in most cases provides the least biased estimates with very small N . However, these results with very small sample sizes ( N  X  2) may not have much practical relevance as any estimator is highly inaccurate in this mode. The MM bias correction, as expected, is not sufficient for small N .Although it outperforms the ML estimates, its error is co nsistently larger than those of other bias-correction strategies.
 entropy estimation with the types of distrib utions which are likely to be observed in the 678
WSI tasks. Both the JK and BUB estimators are considerably less biased alternatives to the ML estimations. 5. Effects on WSI Evaluation
To gauge the effect of the bias problem on WSI evaluation, we computed how the ranking of the SemEval 2010 systems (Manandhar et al. 2010) were affected by different estimators. The SemEval 2010 organizers supplied a test set containing 8,915 manually annotated examples cover ing 100 polysemous lemmas.
 27 systems participated and were ranked according to their performance on the test set, applying the V-measure evaluation as well as paired F-score and a supervised evaluation scheme. The systems were also compared against three baselines. For the
Most Frequent Sense (MFS) baseline all test instances of a given target lemma are baseline, Random , assigns each instance randomly to one of four clusters. The last baseline, proposed in Manandhar and Klapaftis (2009), 1-cluster-per-instance (1ClI), produces as many clusters as there are instances in the test set.
 italics). The systems are presented in the order in which they were given in the official
SemEval 2010 results table (Table 4 in Manandhar et al. (2010), p. 66). Table 1 shows the average number of clusters per word (C#), the V-measure computed with different estimators (ML, MM, JK, and BUB), and the rankings it produces (in brackets). comparison, the results of a supervised evaluation are also shown. The bottom two rows (KCDC-PC-2  X  and UoY  X  ) show the scores computed from the stochastic (weighted) output (Section 3.3) for systems KCDC-PC-2 and UoY, respectively. Other systems did not produce weighted output. 680 per lemma, ranging from 1.02 (Duluth-WSI-SVD) to 17.5 (KSU-KDD). To assess the influence of the cluster granularity on the entropy estimates, we compared the estimates given by the ML estimator against those given by JK and BUB for different numbers of clusters. Figure 3 plots the cluster numbers output by the systems against the estimate zero, independent of the number of clusters. As can be seen, this is not the case. As expected, the difference is larger for systems with larger numbers of clusters, such as
KSU-KDD. This trend will result in unfair preference towards systems producing richer clusterings.

ML estimator against JK, and Figure 4b plots the ranking of ML against BUB. Dots that lie on the diagonal line indicate systems whose rank has not changed. It can be seen that this only applies to a minority of the systems. I n general, there are significant differences between the rankings produced by ML and t hose by JK or BUB. We have seen that the
ML estimator can lead to counterintuitive and undesirable results, such as ranking the 1-cluster-per-instance baseline highest. Th e BUB estimator corrects this and assigns the last rank to this baseline. 8 entropies. To confirm our intuition that join t entropies are more significantly corrected, we looked into the differences between estimates of each entropy for five systems with the largest number of clusters (excluding the 1C1I baseline). The average differences respectively, confirming our hypothesis. Analogous discrepancies for the pair BUB vs.
ML are 0.15 and 0.06, respectively. The differences in the entropy of the gold standard fine-grained than the clusters proposed by these five systems.

PC-2 system is mostly decreased with respect to the  X  X eterministic X  evaluation (except for the MM estimator). Conversely, the score of UoY is mostly improved except to the prediction of the BUB estimator. These differences are somewhat surprising: The stochastic version resulted in significantly larger disagreement between the estimators than the deterministic version. We do not yet have a satisfactory explanation for this phenomenon.
 between the scores of the JK and BUB estimato r, wheres the ML estimator significantly overestimates the V-measure for most of the systems. This observation, coupled with the observed behavior of the JK and BUB esti mators in the simulations, suggest that their predictions are considerably more reliable than predictions of the plug-in ML estimator.
 ation (last two columns in Table 1) shows noticeable differences. Several systems that rank highly according to the V-measure occupy the lower end of the scale when evalu-ated according to supervised recall (Hermit, KSU KDD, Duluth-Mix-Narrow-Gap). 6. Conclusions
In this work, we analyzed the shortcomings of information-theoretic measures in the context of WSI evaluation and argued that main drawbacks of these approaches, such as the preference for the systems predicting richer clusterings or assigning the top score to the 1-cluster-per-instance baseline, are caused by the bias of the underlying sample-based estimates of entropy. We studied alterna tive estimators, including one specifically designed to deal with cases where the number of examples is comparable with the num-ber of clusters. Two of the considered estimators, the jackknifed estimator and the best-upper-bound estimator, achieve consistent ly and significantly less biased results than the standard ML estimator when evaluated in simulations with Zipfian distributions.
The corresponding estimates in the WSI evaluation context can result in significant changes in scores and relative rankings, wit h systems producing richer clusterings more estimates of entropy should be used in future evaluations of sense induction systems.
Other unsupervised tasks in natural langu age processing, such as word clustering or 682 named entity disambiguation, may also benefit from using information-theoretic scores based on more accurate estimators.
 Appendix A: Derivation for the BUB Estimator
We provide a brief derivation for the BUB estimator and refer the reader to Paninski (2003) for details and discussion. The BUB estimator is obtained by minimizing an upper bound on MSE for estimators H ( a a a ) (see Equation (2)). First, MSE is bounded from above by maximizing the variance and the bias independently: where p = ( p 1 , ... , p m ) is an underlying discrete measure; B variance of the estimator given p . Then individual bounds both for the squared bias and the variance can be constructed.
 tation of  X  H ( a a a ) can be written as where B j , N ( x ) is the binomial polynomial N j x j (1 we have where H ( x ) =  X  x log x , the entropy function. A uniform upper bound can be obtained by bounding each term in the sum:
However, this bound is not too tight as it w ould overemphasize importance of the approximation quality for components i with p i close to 1. Intuitively, the behavior near 0 is more important, as there can be more components p generalizes this bound by considering a weighted version with the function f chosen to emphasize smaller components bounds (Steele 1986). For the Steele bound, it has the form bound, the L 2 relaxation of the L  X  loss is used, resulting in a regularized least-squares problem.
 Acknowledgments References 684
