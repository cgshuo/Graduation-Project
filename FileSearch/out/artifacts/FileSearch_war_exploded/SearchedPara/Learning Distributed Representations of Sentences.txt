 Distributed representations -dense real-valued vec-tors that encode the semantics of linguistic units -are ubiquitous in today X  X  NLP research. For single-words or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on com-paratively task-agnostic objectives (such as predict-ing adjacent words). These methods are well under-stood empirically (Baroni et al., 2014b) and theoret-ically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011).

By contrast, there is comparatively little consen-sus on the best ways to learn distributed represen-of deeper language processing techniques, it is rel-atively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation (Sutskever et al., 2014), image captioning (Mao et al., 2015) and dialogue systems (Serban et al., 2015). While it has been observed informally that the internal sentence rep-resentations of such models can reflect semantic in-tuitions (Cho et al., 2014), it is not known which ar-chitectures or objectives yield the  X  X est X  or most use-ful representations. Resolving this question could ultimately have a significant impact on language processing systems. Indeed, it is phrases and sen-tences, rather than individual words, that encode the human-like general world knowledge (or  X  X ommon sense X ) (Norman, 1972) that is a critical missing part of most current language understanding systems.
We address this issue with a systematic compari-son of cutting-edge methods for learning distributed representations of sentences. We focus on meth-ods that do not require labelled data gathered for the purpose of training models, since such meth-ods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objec-tives -Sequential Denoising Autoencoders (SDAEs) and FastSent , a sentence-level log-bilinear bag-of-words model. We compare all methods on two types of task -supervised and unsupervised evaluations -reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to represen-tations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.

We observe notable differences in approaches de-pending on the nature of the evaluation metric. In particular, deeper or more complex models (which require greater time and resources to train) gener-ally perform best in the supervised setting, whereas shallow log-bilinear models work best on unsuper-vised benchmarks. Specifically, SkipThought Vec-tors (Kiros et al., 2015) perform best on the ma-jority of supervised evaluations, but SDAEs are the top performer on paraphrase identification. In con-trast, on the (unsupervised) SICK sentence relat-edness benchmark, FastSent, a simple, log-bilinear variant of the SkipThought objective, performs bet-ter than all other models. Interestingly, the method that exhibits strongest performance across both su-pervised and unsupervised benchmarks is a bag-of-words model trained to compose word embeddings using dictionary definitions (Hill et al., 2015a). Taken together, these findings constitute valuable guidelines for the application of phrasal or senten-tial representation-learning to language understand-ing systems. To constrain the analysis, we compare neural lan-guage models that compute sentence representations from unlabelled, naturally-ocurring data, as with Likewise, we do not focus on  X  X ottom up X  models where phrase or sentence representations are built from fixed mathe proposed bymatical operations on word vectors (although we do consider a canoni-cal case -see CBOW below); these were already compared by Milajevs et al. (2014). Most space is devoted to our novel approaches, and we refer the reader to the original papers for more details of ex-isting models. 2.1 Existing Models Trained on Text SkipThought Vectors For consecutive sentences S i  X  1 ,S i ,S i +1 in some document, the SkipThought model (Kiros et al., 2015) is trained to predict target sentences S i  X  1 and S i +1 given source sentence S i . As with all sequence-to-sequence models, in train-ing the source sentence is  X  X ncoded X  by a Recurrent Neural Network (RNN) (with Gated Recurrent uU-nits (Cho et al., 2014)) and then  X  X ecoded X  into the two target sentences in turn. Importantly, because RNNs employ a single set of update weights at each time-step, both the encoder and decoder are sensitive to the order of words in the source sentence.
For each position in a target sentence S t , the decoder computes a softmax distribution over the model X  X  vocabulary. The cost of a training exam-ple is the sum of the negative log-likelihood of each correct word in the target sentences S i  X  1 and S i +1 . This cost is backpropagated to train the encoder (and decoder), which, when trained, can map sequences of words to a single vector.
 ParagraphVector Le and Mikolov (2014) proposed two log-bilinear models of sentence representation. The DBOW model learns a vector s for every sen-tence S in the training corpus which, together with word embeddings v w , define a softmax distribution optimised to predict words w  X  S given S . The v w are shared across all sentences in the corpus. In the DM model, k -grams of consecutive words { w i ...w i + k  X  S } are selected and s is combined with { v w (parameterised by additional weights) of w i + k +1 . each sentence in the training data as a  X  X aragraph X  as suggested by the authors. During training, both DM and DBOW models store representations for every sentence (as well as word) in the training corpus. Even on large servers it was therefore only possi-ble to train models with representation size 200 , and DM models whose combination operation was av-eraging (rather than concatenation). Unlike other models considered in this section, for both Para-graphVector architectures an inference step is re-quired after training to estimate sentence representa-tions s for arbitrary sentences based on the v w . This additional computation is reflected in the higher en-coding time in Table 1 (TE).
 Bottom-Up Methods We train CBOW and Skip-Gram word embeddings (Mikolov et al., 2013b) on the same text corpus as the SkipThought and Para-graphVector models, and compose by elementwise
We also compare to C-PHRASE (Pham et al., 2015), an approach that exploits a (supervised) parser to infer distributed semantic representations based on a syntactic parse of sentences. C-PHRASE achieves state-of-the-art results for distributed repre-Non-Distributed Baseline We implement a TFIDF BOW model in which the representation of sentence S encodes the count in S of a set of feature-words weighted by their tfidf in C , the corpus. The feature-words are the 200,000 most common words in C . 2.2 Models Trained on Structured Resources The following models rely on (freely-available) data that has more structure than raw text.
 DictRep Hill et al. (2015a) trained neural language models to map dictionary definitions to pre-trained word embeddings of the words defined by those def-initions. They experimented with BOW and RNN (with LSTM) encoding architectures and variants in which the input word embeddings were either learned or pre-trained ( +embs. ) to match the tar-get word embeddings. We implement their models CaptionRep Using the same overall architecture, we trained ( BOW and RNN ) models to map cap-tions in the COCO dataset (Chen et al., 2015) to pre-trained vector representations of images. The image representations were encoded by a deep convolu-tional network (Szegedy et al., 2014) trained on the ILSVRC 2014 object recognition task (Russakovsky et al., 2014). Multi-modal distributed representa-tions can be encoded by feeding test sentences for-ward through the trained model.
 NMT We consider the sentence representations learned by neural MT models. These models have identical architecture to SkipThought, but are trained on sentence-aligned translated texts. We used a standard architecture (Cho et al., 2014) on all available En-Fr and En-De data from the 2015 2.3 Novel Text-Based Models We introduce two new approaches designed to ad-dress certain limitations with the existing models. Sequential (Denoising) Autoencoders The SkipThought objective requires training text with a coherent inter-sentence narrative, making it problematic to port to domains such as social media or artificial language generated from symbolic knowledge. To avoid this restriction, we experiment with a representation-learning objective based on denoising autoencoders (DAEs). In a DAE, high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. As a result of this process, DAEs learn to represent the data in terms of features that explain its important factors of variation (Vincent et al., 2008). Transforming data into DAE representations (as a  X  X re-training X  or initialisation step) gives more robust (supervised) classification performance in deep feedforward networks (Vincent et al., 2010).
The original DAEs were feedforward nets applied to (image) data of fixed size. Here, we adapt the ap-proach to variable-length sentences by means of a noise function N ( S | p o ,p x ) , determined by free pa-rameters p o ,p x  X  [0 , 1] . First, for each word w in S , N deletes w with (independent) probability p o . Then, for each non-overlapping bigram w i w i +1 in S , N swaps w i and w i +1 with probability p x . We then train the same LSTM-based encoder-decoder architecture as NMT, but with the denoising objec-tive to predict (as target) the original source sentence S given a corrupted version N ( S | p o ,p x ) (as source). The trained model can then encode novel word se-quences into distributed representations. We call this model the Sequential Denoising Autoencoder ( SDAE ). Note that, unlike SkipThought, SDAEs can be trained on sets of sentences in arbitrary order.
We label the case with no noise (i.e. p o = p x = 0 and N  X  id ) SAE . This setting matches the method applied to text classification tasks by Dai and Le (2015). The  X  X ord dropout X  effect when p o  X  0 has also been used as a regulariser for deep nets in su-pervised language tasks (Iyyer et al., 2015), and for large p x the objective is similar to word-level  X  X e-bagging X  (Sutskever et al., 2011). For the SDAE, we We also tried a variant ( +embs ) in which words are represented by (fixed) pre-trained embeddings. FastSent The performance of SkipThought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences. The model could be said to exploit a type of sentence-level Distributional Hypothesis (Harris, 1954; Polajnar et al., 2015). Nevertheless, like many deep neu-ral language models, SkipThought is very slow to train (see Table 1). FastSent is a simple additive (log-bilinear) sentence model designed to exploit the same signal, but at much lower computational ex-pense. Given a BOW representation of some sen-tence in context, the model simply predicts adjacent sentences (also represented as BOW) .

More formally, FastSent learns a source u w and target v w embedding for each word in the model vo-cabulary. For a training example S i  X  1 ,S i ,S i +1 of consecutive sentences, S i is represented as the sum of its source embeddings s i = of the example is then simply: where  X  ( v 1 ,v 2 ) is the softmax function.
We also experiment with a variant ( +AE ) in which the encoded (source) representation must predict its own words as target in addition to those of adjacent sentences. Thus in FastSent+AE, (1) becomes At test time the trained model (very quickly) en-codes unseen word sequences into distributed rep-resentations with s = 2.4 Training and Model Selection Unless stated above, all models were trained on sentential coherence required for SkipThought and FastSent. The corpus consists of 70m ordered sen-tences from over 7,000 books.

Specifications of the models are shown in Ta-ble 1. The log-bilinear models (SkipGram, CBOW, ParagraphVec and FastSent) were trained for one epoch on one CPU core. The representation di-mension d for these models was found after tun-ing d  X  { 100 , 200 , 300 , 400 , 500 } on the validation The S(D)AE models were trained for one epoch (  X  8 days). The SkipThought model was trained CaptionRep and DictRep, performance was mon-itored on held-out training data and training was stopped after 24 hours after a plateau in cost. The NMT models were trained for 72 hours. In previous work, distributed representations of lan-guage were evaluated either by measuring the effect of adding representations as features in some clas-sification task -supervised evaluation (Collobert et al., 2011; Mikolov et al., 2013a; Kiros et al., 2015) -or by comparing with human relatedness judge-ments -unspervised evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The for-mer setting reflects a scenario in which representa-tions are used to inject general knowledge (some-times considered as pre-training ) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classi-fication tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classifica-tion (SUBJ) (Pang and Lee, 2004), opinion polar-ity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sen-tence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces re-flect human intuitions of the semantic sentence relat-edness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard hu-man judgements. The SICK dataset (Marelli et al., 2014) consists of 10,000 pairs of sentences and re-latedness judgements. The STS 2014 dataset (Agirre et al., 2014) consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table 2. All available pairs are used for test-ing apart from the 500 SICK  X  X rial X  pairs, which are held-out for tuning hyperparameters (representation size of log-bilinear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evalua-tions. Performance of the models on the supervised eval-uations (grouped according to the data required by their objective) is shown in Table 3. Overall, SkipThought vectors perform best on three of the six evaluations, the BOW DictRep model with pre-trained word embeddings performs best on two, and the SDAE on one. SDAEs perform notably well on the paraphrasing task, going beyond SkipThought by three percentage points and approaching state-of-the-art performance of models designed specifi-cally for the task (Ji and Eisenstein, 2013). SDAE is also consistently better than SAE, which aligns with other findings that adding noise to AEs pro-duces richer representations (Vincent et al., 2008).
Results on the unsupervised evaluations are shown in Table 4. The same DictRep model per-forms best on four of the six STS categories (and overall) and is joint-top performer on SICK. Of the models trained on raw text, simply adding CBOW word vectors works best on STS. The best performing raw text model on SICK is FastSent, which achieves almost identical performance to C-PHRASE X  X  state-of-the-art performance for a dis-tributed model (Pham et al., 2015). Further, it uses less than a third of the training text and does not require access to (supervised) syntactic representa-tions for training. Together, the results of FastSent on the unsupervised evaluations and SkipThought on the supervised benchmarks provide strong sup-port for the sentence-level distributional hypothe-sis: the context in which a sentence occurs provides valuable information about its semantics.

Across both unsupervised and supervised evalua-tions, the BOW DictRep with pre-trained word em-beddings exhibits by some margin the most con-sistent performance. Ths robust performance sug-gests that DictRep representations may be particu-larly valuable when the ultimate application is non-specific or unknown, and confirms that dictionary definitions (where available) can be a powerful re-source for representation learning. Many additional conclusions can be drawn from the results in Tables 3 and 4.
 Different objectives yield different representa-tions It may seem obvious, but the results confirm that different learning methods are preferable for different intended applications (and this variation appears greater than for word representations). For instance, it is perhaps unsurprising that SkipThought performs best on TREC because the labels in this dataset are determined by the language immediately following the represented question (i.e. the an-swer) (Voorhees, 2002). Paraphrase detection, on the other hand, may be better served by a model that focused entirely on the content within a sen-tence, such as SDAEs. Similar variation can be observed in the unsupervised evaluations. For in-stance, the (multimodal) representations produced by the CaptionRep model do not perform particu-larly well apart from on the Image category of STS where they beat all other models, demonstrating a clear effect of the well-studied modality differences in representation learning (Bruni et al., 2014).
The nearest neighbours in Table 5 give a more concrete sense of the representation spaces. One notable difference is between (AE-style) models whose semantics come from within-sentence rela-tionships (CBOW, SDAE, DictRep, ParagraphVec) and SkipThought/FastSent, which exploit the con-text around sentences. In the former case, nearby sentences often have a high proportion of words in common, whereas for the latter it is the general con-cepts and/or function of the sentence that is sim-ilar, and word overlap is often minimal. Indeed, this may be a more important trait of FastSent than the marginal improvement on the SICK task. Read-ers can compare the CBOW and FastSent spaces at http://45.55.60.98/ .
 Differences between supervised and unsuper-vised performance Many of the best performing models on the supervised evaluations do not per-form well in the unsupervised setting. In the SkipThought, S(D)AE and NMT models, the cost is computed based on a non-linear decoding of the in-ternal sentence representations, so, as also observed by (Almahairi et al., 2015), the informative geome-try of the representation space may not be reflected in a simple cosine distance. The log-bilinear models generally perform better in this unsupervised setting. Knowledge transfer shows some promise It is no-table that, with a few exceptions, the models with pre-trained word embeddings (+embs) outperform those with learned embeddings on both supervised and unsupervised evaluations. In the case of the Dic-tRep models, whose training data is otherwise lim-ited to dictionary definitions, this effect can be con-sidered as a rudimentary form of knowledge trans-fer. The DictRep+embs model benefits both from the dictionary data and the enhanced lexical seman-tics acquired from a massive text corpus to build overall higher-quality sentence representations. Differences in resource requirements As shown in Table 1, different models require different resources to train and use. This can limit their possible appli-cations. For instance, while it was easy to make an online demo for fast querying of near neighbours in the CBOW and FastSent spaces, it was not practical for other models owing to memory footprint, encod-ing time and representation dimension.
 The role of word order is unclear The aver-age scores of models that are sensitive to word order (76.3) and of those that are not (76.6) are approximately the same across supervised evalua-tions. Across the unsupervised evaluations, how-ever, BOW models score 0.55 on average compared with 0.42 for RNN-based (order sensitive) models. This seems at odds with the widely held view that word order plays an important role in determining the meaning of English sentences. One possibility is that order-critical sentences that cannot be dis-ambiguated by a robust conceptual semantics (that could be encoded in distributed lexical representa-tions) are in fact relatively rare. However, it is also plausible that current available evaluations do not adequately reflect order-dependent aspects of mean-ing (see below). This latter conjecture is supported by the comparatively strong performance of TFIDF BOW vectors, in which the effective lexical seman-tics are limited to simple relative frequencies. The evaluations have limitations The internal con-sistency (Chronbach X  X   X  ) of all evaluations consid-ered together is 0 . 81 (just above  X  X cceptable X ). 12 Table 6 shows that consistency is far higher ( X  X x-cellent X ) when considering the supervised or unsu-pervised tasks as independent cohorts. This indi-cates that, with respect to common characteristics of sentence representations, the supervised and unsu-pervised benchmarks do indeed prioritise different properties. It is also interesting that, by this met-ric, the properties measured by MSRP and image-caption relatedness are the furthest removed from other evaluations in their respective cohorts.
While these consistency scores are a promising sign, they could also be symptomatic of a set of eval-uations that are all limited in the same way. The inter-rater agreement is only reported for one of the 8 evaluations considered (MPQA, 0 . 72 (Wiebe et al., 2005)), and for MR, SUBJ and TREC, each item is only rated by one or two annotators to maximise coverage. Table 2 illustrates why this may be an issue for the unsupervised evaluations; the notion of sentential  X  X elatedness X  seems very subjective. It should be emphasised, however, that the tasks con-sidered in this study are all frequently used for eval-uation, and, to our knowledge, there are no existing benchmarks that overcome these limitations. Advances in deep learning algorithms, software and hardware mean that many architectures and objec-tives for learning distributed sentence representa-tions from unlabelled data are now available to NLP researchers. We have presented the first (to our knowledge) systematic comparison of these meth-ods. We showed notable variation in the perfor-mance of approaches across a range of evaluations. Among other conclusions, we found that the op-timal approach depends critically on whether rep-resentations will be applied in supervised or unsu-pervised settings -in the latter case, fast, shallow BOW models can still achieve the best performance. Further, we proposed two new objectives, FastSent and Sequential Denoising Autoencoders, which per-form particularly well on specific tasks (MSRP and plication is unknown, however, the best all round choice may be DictRep: learning a mapping of pre-trained word embeddings from the word-phrase sig-nal in dictionary definitions. While we have focused on models using naturally-occurring training data, in future work we will also consider supervised ar-chitectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the  X  X eneral knowledge X  needed to give lan-guage technology the elusive human touch.
 This work was supported by a Google Faculty Award to AK and FH and a Google European Doc-toral Fellowship to FH. Thanks also to Marek Rei, Tamara Polajnar, Laural Rimell, Jamie Ryan Kiros and Piotr Bojanowski for helpful comments.

