 We describe the ideas and methodologies that we developed in addressing the KDD Cup 2007 How Many Ratings task, and discuss how they contributed to our success. At the Data Analytics Research group at IBM Research we aim to combine theoretical rigor with practical usefulness in our research and the projects we develop for IBM groups and external customers. Our projects often include aspects of data analysis, algorithm development, application develo p-ment and product delivery [4; 3; 1]. Based on our experience there are several important components to success in mod-eling data  X  whether it be in a competition or in real-life modeling problems. One possible characterization of these components divides them into three general categories: 1. Data and domain understanding. The focus here 2. Statistical insights. This aspect of the modeling pro-3. Modeling or learning approach. This is the step From our experience, the ordering of these categories above is consistent with their typical importance in maximizing the success of practical modeling projects. The ability to Mathematical Sciences, Tel Aviv University, Israel understand the data and the domain well, figure out their correct interpretation and appropriate use is by far the mos t useful way to gain  X  X n edge X  and improve models above and beyond what any statistical insights or modeling ap-proaches can. Correctly formulating a statistical or proba -bilistic framework is also of critical importance, when pos -sible. Finally, in our opinion, the learning approach, whil e highly influential on bottom-line performance in many cases , cannot be counted on as a way to circumvent the need to understand the data and the statistical setup properly. In this short paper we use this classification of the elements of modeling success to describe our approach. After describ -ing the general setup of the challenge in Section 2, we detail the data insights we used in Section 3. We discuss out key statistical insights in Section 4, and show how everything comes together to guide our modeling approach in Section 5. Finally, we briefly analyze the competition results and how the different components of our approach affected our performance in Section 6. The second task in the KKD-CUP was to predict the total number of reviews that a movie received during 2006 from the universe of users in the Netflix competition training set . This task can be viewed as a regression problem where the number of ratings that a movies receives in a given period of time depends on a number of factors that contribute to the popularity and in turn to the number of ratings of a movie. Such factors include age, arrival in the Netflix database, genre, rating and also the characteristics and history of th e roughly 480,000 reviewers.
 These factors naturally do not only impact the number of ratings in the current time frame but also the number of rat-ing in previous periods. This suggests a temporal dynamic in the rating with different periods in the movie life-cycle: prior to Box-office release, prior to DVD release, prior to availability in Netflix and finally the slow decrease of inter -est as the movie ages. So the historical reviewing behavior of a movie is another vital piece of information to capture the dynamic life-cycle of a movie. Such lagged rating counts can be extracted from the Netflix competition dataset with time-stamped ratings from 1998 through 2005.
 One way of formalizing the supervised modeling problem of the How Many Ratings task is to build a time-series model that estimates the number of rating in one period as a func-tion of past ratings and movie-specific features on historic al data and roll the model over to the next time period. However, there is another less obvious approach that can be taken to formalize The How Many Ratings task as a su-pervised learning problem that takes advantage of the Who reviewed what test set as discussed in the next section. We will discuss two important observations about the gen-eration of the test set for the KDD-CUP and training data that strongly affect the design of our modeling approach. The two tasks for the KDD-CUP were constructed based on the 2006 reviews of the 17770 movies in the Netflix competi-tion dataset. The organizers randomly assigned 8863 movies to the How Many Ratings task and the remaining movies were used to construct the test set for Who Reviewed What . Let us take a more detailed look at the construction of this test set and how it can be utilized to build a model for the How Many Ratings task.
 In order to achieve a reasonably high base rate for the clas-sification task Who Reviewed What , the sampling probabil-ity for a movie,user pair was based on the product of the marginal rating distributions by movie and user in 2006. The marginal is proportional to the number of ratings a movie received in 2006. So the number of movie appear-naces in a test pair is proportional to the total number of reviews the movie received in 2006. This suggests an inter-esting modeling approach. We can use the appearance in the Who Reviewed What test set as the dependent variable to estimate a model to predict the number of 2006 ratings. We can then apply this model to the movies in the How Many Ratings test set. This idea has two major advantages: 1. we capture the dynamics of the 2006 ratings; and 2. we can make optimal use of all recent data up to end of However, there are two issues to consider. We are still miss-ing a scaling parameter. The counts in the test set of Who Reviewed What are relative to a sample of 100,000 movie-rating pairs. What remains unknown and of critical impor-tance is the total number of reviews in 2006 to use as a scaling factor for our prediction. This modeling problem is described in detail in Section 5.
 Another important observation is the fact, that the counts i n the Who Reviewed What test set are not really proportional to the marginal because the organizers had to remove pairs that had received ratings prior to 2006. The probability of rejection is a function of the marginal distribution and affects popular movies more than others. We resolve this problem by correcting the counts as outlined in Section 4.2. One of the missing pieces of information is the scaling pa-rameter that is needed to predict the total number of ratings in 2006. To appropriately address this point, we had to un-derstand the dynamics of how the total number of ratings that all movies changes over time  X  both calendar time and time in the life-cycle of the movies being reviewed. We no-ticed some interesting discrepancies in the behavior of the total number of reviews over time, in particular a steep drop in the number of reviews in the fourth quarter of 2005, which we were only partially able to explain through movie and re-viewer life cycles. Since this did not prove a major influence on our model X  X  performance, we defer detailed discussion to a longer version of this paper. There are two statistical aspects to this data modeling prob -lem that captured our attention. Consider a set of m objects (in this case, movies) with counts n , ....n m (in this case, number of reviews per movie in a given period of length t ). Our first observation is that under mild and reasonable assumptions about the arrival process of new reviews for each movie, these counts have a marginal Poisson distribution: Consequently, if we decide to use a linear model (or a kernel-based non-linear model) to describe the dependence of the observed movie counts on a set of features x 1 , ..., x p are vectors of length n ), a good candidate modeling ap-proach would be a Poisson regression, a generalized linear model [2], with the natural (log) link function: A more interesting situation is when we use the set of counts  X  n , ...,  X  n m from Who Reviewed What test set as our modeling target. This test set was sampled proportional to the true counts n 1 , ...n m (subject to the rejection sampling correc-tion we discuss next), and is constrained to sum to a fixed number (say, 100000). It is easy to show that:  X  n , ...,  X  n m | X where p i =  X  i / P k  X  k is the relative rate of movie i . Now, if we look at each of the  X  n i  X  X , their  X  X arginal X  condi-tional distribution is Binomial(100000 , p i ) and since this is a large n , small p situation, the distribution of  X  n i proximated by Pois(100000 , p i ) distribution. Further, it can be shown that although the binomial are not independent (the dependence is created by the constraint on their sum), this does not invalidate a Poisson regression approach for maximum likelihood estimation of their parameters, since Poisson regression guarantees that the sum of the predic-tions will equal the sum of the observations [2]. Thus we propose to use a similar formulation to Eq. (1): where we have eliminated the known time period t , and we will have to estimate a scaling factor as discussed in the previous section to scale the estimate  X  i  X  X  to use them for prediction.
 Our discussion in the previous section assumed that the  X  n were sampled proportionally from the original n i  X  X . As we discussed in the previous section, this is not exactly true, because after this proportional sampling, some of the sam-pled movies were rejected, based on previously having been ranked (prior to 2006). To obtain  X  n i s that are indeed pro-portionally sampled this rejection would have to be inverte d. Here we describe our algorithm for this inversion. Let p i =  X  i / P k  X  k be the true sampling rate for movie i , and q j =  X  j / P l  X  l be the corresponding sampling rate for user j . We estimate p and q as follows Suppose the sample size 100,000 is large enough for us to estimate p i and q We have a hidden variable, that is, the number of samples rejected because they have appeared before 2006, which we denoted by N. We observe n i appearances of movie i in the final sample set, which satisfies: where U t is the set of users that has reviewed the movie t before 2006. On the right hand side of eq(2), the first product corresponds to the total number of samples with movie i (before rejection), and the last term is the proportion of pairs that are been eliminated because they appear before 2006. Similarly, we observe m j appearances of user j in the final sample set, which satisfies: where M k is the set of movies that has been reviewed by user j before 2006. We implemented an ad-hoc iterative procedure for solving the equations (2,3), by alternating b e-tween fixing the q j  X  X  and solving (2), and fixing the p solving (3). This gives us a more accurate estimate of p k and N (our interest is, of course focused on the p k  X  X ). This correction can be thought of as increasing the marginal for movies that are likely to have been rejected a lot, because they have been heavily reviewed before 2006, while also tak-ing into account which reviewers reviewed them. The culmination of all the discussion in the previous sectio ns led us to a modeling approach that is outlined graphically in Figure 1: 1. Extract a list of features for each movie:  X  log(Number of reviews by month for the most recent  X  log(Number of reviews by quarter for the most recent  X  log(Number of reviews by year for the last four years+1)  X  Movie X  X  age in the Netflix database (days since first re- X  Some characteristics of the movie X  X  ratings (% of 5 X  X ,  X  Movie X  X  Genre (taking only the most common genres and 2. Use the test set of Who Reviewed What as a response for training a model:  X  Apply the sampling correction discussed in Section 4.2.  X  Build a Poisson regression model describing 3. Go through a separate modeling exercise to estimate the scaling factor , i.e., the total number of reviews that were given to all movies in 2006:  X  Create four lagged datasets, which are  X  X nchored X  in pre- X  Build four predictive models which use subsequent quar- X  These models can now be applied to our complete dataset  X  This prediction can be used either as an actual predic-This schematic description glosses over many details, like feature selection, interaction selection, exact form of th e Poisson regression models, etc. We next discuss in some detail the elements of model evaluation and model selection . Our best asset for evaluation is the same as for modeling  X  the Who Reviewed What test set, after the rejection sam-pling correction. It can be used in a straight forward manner to evaluate the models built on it, through a cross validatio n approach or training-test splits.
 For models built on the lagged datasets, the exercise is less trivial. To use Who Reviewed What test set for evaluation we need to invert the sampling scheme. To avoid various complications that stem from this, and to give the lagged models the best opportunity to surpass the Who Reviewed What -based models in terms of performance, we actually find the best possible scaling parameter to the lagged models predictions in terms of their performance on Who Reviewed What test set.
 The end result of all these evaluations are model-performan ce scores for all models we consider for prediction. The best performances in terms of log-scale on holdout data from Who Reviewed What for two model classes  X  Who Reviewed What -based vs. lagged data-based  X  were 0.24 and 0.31, re-spectively. We concluded that we should use the models we build on Who Reviewed What test set for prediction, and the models built on lagged datasets for scaling only. The log-scale MSE of our winning model on the How Many Ratings task 2006 reviews was 0.263 as shown in Table 1. This error has two components: The error of the model for the scaled-down Who Reviewed What test set (which we estimated at about 0.24); and the error from our incorrect scaling factor, i.e., the mismatch between the scaling fact or we estimated from the lagged models and the true correct scaling factor.
 In our case, the sum of our predictions was 9 . 35 million, and the sum of true responses was 8 . 7 million. Table 2 details the scores we would have attained if we had scaled our pre-dictions differently. By correctly scaling to 8 . 7 million total, we would have attained MSE of about 0 . 234. Our lowest possible MSE could have been 0 . 208 with a scaling factor of 0 . 8. This is possibly due to the behavior of Poisson noise un-der the log transformation: the roughly symmetric noise (fo r large Poisson parameter) becomes long-left-tailed under t he log transformation, and hence consistent under-predictio n may lead to better performance. We can summarize our KDD Cup 2007 How Many Ratings experience in three short bullets:  X  We had fun dealing with the data and understanding  X  We did well and we believe that a combination of Table 1: Results for the top performing teams on the How Many Ratings task  X  We encountered some interesting research prob-Acknowledgments We thank Rick Lawrence and Zhenzhen Kou for help in useful discussions and in data formatting. [1] R. Lawrence, C. Perlich, S. Rosset, et al. Analytics-[2] P. McCullagh and J. Nelder. Generalized Linear Models . [3] C. Perlich, S. Rosset, R. Lawrence, and B. Zadrozny. [4] S. Rosset and R. Lawrence. Data enhanced predictive Table 2: The effect of scaling on competition MSE. The first column is a hypothetical scaling factor applied to our submitted predictions, the second is the implied total for 2006, and the third the competition score.

