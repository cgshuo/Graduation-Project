 1. Introduction
Latent Semantic Indexing (LSI) ( Deerwester, Dumais, Landauer, Furnas, &amp; Harshman, 1990 ) is a well-known information retrieval algorithm. LSI has been applied to a wide variety of learning tasks, such as ais, 1994, 1995 ). LSI is a vector space approach for modeling documents, and many have claimed that the 1992 ).
LSI is based on a mathematical technique termed Singular Value Decomposition (SVD). The algebraic original matrix. Wiemer-Hastings (1999) shows that the power of LSI comes primarily from the SVD algorithm.

Other researchers have proposed theoretical approaches to understanding LSI. Zha and Simon (1998) describes LSI in terms of a subspace model and proposes a statistical test for choosing the optimal number Bayesian methods. Ding (1999) constructs a statistical model for LSI using the cosine similarity measure.
Although other researchers have explored the SVD algorithm to provide an understanding of SVD-based information retrieval systems, to our knowledge, only Schu  X  tze has studied the values produced by
SVD ( Schu  X  tze, 1992 ). We expand upon this work, showing here that LSI exploits higher-order term co-itive theoretical understanding of the mechanism whereby LSI emphasizes latent semantics.
This work is also the first to study the values produced in the SVD term by dimension matrix and we have discovered a correlation between the performance of LSI and the values in this matrix. Thus, in con-oretical basis for LSI.

Additional related work can be found in a recent article by Dominich. In Dominich (2003) , the author count for or contribute to its effectiveness.

In Section 2 we present an overview of LSI along with a simple example of higher-order term co-occurrence in LSI. Section 3 explores the relationship between the values produced by LSI and term co-occurrence. In Sections 3 and 4 we correlate LSI performance to the values produced by the SVD, indexed by the order of co-occurrence. Section 5 presents a mathematical proof of LSI  X  s base in higher-order co-occurrence. We draw conclusions and touch on future work in Section 6. 2. Overview of Latent Semantic Indexing
In this section we provide a brief overview of the LSI algorithm. We also discuss higher-order term co-occurrence in LSI, and present an example of LSI assignment of term co-occurrence values in a small collection. 2.1. Latent Semantic Indexing algorithm
In traditional vector space retrieval, documents and queries are represented as vectors in t -dimensional space, where t is the number of indexed terms in the collection. When Latent Semantic Indexing (LSI) is used for document retrieval, Singular Value Decomposition (SVD) is used to decompose the term by doc-ument matrix into three matrices: T , a term by dimension matrix, S a singular value matrix (dimension by dimension), and D , a document by dimension matrix. The number of dimensions is r , the rank of the term by document matrix. The original matrix can be obtained, through matrix multiplication of TSD
In an LSI system, the T , S and D matrices are truncated to k dimensions. This process is presented graphically in Fig. 1 (taken from Berry et al., 1995 ). The shaded areas in the T and D matrices are kept, as are the k largest singular values, the non-shaded areas are removed. The purpose of dimensionality a numerical example. Queries are represented in the reduced space by T term by dimension matrix, after truncation to k dimensions. Queries are compared to the reduced docu-ment vectors, scaled by the singular values ( S k D k ) by computing the cosine similarity.
LSI relies on a parameter k , for dimensionality reduction. The optimal k is determined empirically for each collection. In general, smaller k values are preferred when using LSI, due to the computational cost associated with the SVD algorithm, as well as the cost of storing and comparing large dimension vectors.
In the following section we apply LSI to a small collection which consists of only nine documents, and show how LSI transforms values in the term by document matrix. 2.2. Co-occurrence in LSI X  X n example
The data for the following example is taken from Deerwester et al. (1990) . In that paper, the authors describe an example with 12 terms and 9 documents. The term X  X ocument matrix is shown in Table 1 and the corresponding term-to-term matrix is shown in Table 2 .

As mentioned above, the SVD process used by LSI decomposes the matrix into three matrices: T , a term by dimension matrix, S a singular value matrix, and D , a document by dimension matrix. The reader is referred to Deerwester et al. (1990) for the T , S , and D matrices corresponding to the term by document matrix in Table 1 . After dimensionality reduction the term-to-term matrix can be re-computed using the strong similarity, where before the value was zero. In fact, user and human is an example of second-order co-occurrence. The relationship between user and human comes from the transitive relation: user co-occurs with interface and interface co-occurs with human .

A closer look reveals a value of 0.15 in the relationship between trees and computer . Looking at the co-occurrence path gives us an explanation as to why these terms received a positive (albeit weak) similarity value. From Table 2 , we see that trees co-occurs with graph , graph co-occurs with survey ,and survey co-occurs with computer . Hence the trees/computer relationship is an example of third-order co-occurrence.
In Section 3 we explore the relationship between higher-order term co-occurrence and the values which ap-between the term-to-term matrix values and the performance of LSI.

To completely understand the dynamics of the SVD process, a closer look at Table 1 is warranted. The nine documents in the collection can be split into two subsets C1 X  X 5 and M1 X  X 4. If the term survey did not appear in the M1 X  X 4 subset, the subsets would be disjoint. The data in Table 4 was developed by changing the survey / m 4 entry to 0 in Table 1 , computing the SVD of this new matrix, truncating to two dimensions, and deriving the associated term-to-term matrix.
 have been reduced to zero. In Section 5 we prove a theorem that explains this phenomenon, showing, in all must be zero. 3. Higher-order co-occurrence in LSI In this section we study the relationship between the values produced by LSI and term co-occurrence.
We show a relationship between the term co-occurrence patterns and resultant LSI similarity values. This data shows how LSI emphasizes important semantic distinctions, while de-emphasizing terms that co-occur order term co-occurrence and the values produced by SVD is a necessary step toward the development of an approximation algorithm for LSI. 3.1. Data sets
We chose the MED, CRAN, CISI and LISA collections for our study of the higher-order co-occurrence in LSI. Table 5 shows the attributes of each collection.

The LISA collection was processed in two ways. The first was an extraction of words only, resulting in a collection with 6004 documents and 18,429 terms. We will refer to this collection as LISA Words. Next the HDDI Collection Builder ( Pottenger &amp; Yang, 2001; Pottenger, Kim, &amp; Meling, 2001 ) was used to extract maximal length noun phrases. This collection contains 5998 documents (no noun phrases were extracted from several short documents) and 81,879 terms. The experimental results for the LISA Noun Phrase collection were restricted to 1000 randomly chosen terms (due to processing time considerations).
However, for each of the 1000 terms, all co-occurring terms (up to 81,879) were processed, giving us confidence that this data set accurately reflects the scalability of our result. In the future, we plan to expand this analysis to larger data sets, such as those used for the Text Retrieval Conference (TREC) experiments. 3.2. Methodology
Our experiments captured four main features of these data sets: the order of co-occurrence for each pair of terms in the truncated term-to-term matrix (shortest path length), the order of co-occurrence for each the term-to-term matrix, categorized by order of co-occurrence, and the number of second-order co-occurrence paths between each set of terms.
 In order to complete these experiments, we needed a program to perform the SVD decomposition. The
SVDPACK suite ( Berry, Do, O  X  Brien, Krishna, &amp; Varadhan, 1993 ) that provides eight algorithms for ues and vectors were input into our algorithm. 3.3. Results
The order of co-occurrence summary for all of the collections is shown in Table 6 . Fifth-order co-tion  X  Original indicate the number of pairs with co-occurrence order n determined from a trace of the original term-to-term matrix. LSI incorporates over 99% of the (higher-order) term co-occurrences present in the data for the first four collections, and 95% for the LISA Noun Phrase collection.
Table 7 shows the weight distribution for the LISA Words data, for k = 100. MED and CISI showed similar trends. This data provides insight into understanding the values produced by SVD in the truncated term-to-term matrix. The degree-two pair weights range from 0.3 to values larger than 8. These co-occurrences will result in significant differences in document matching when the LSI algorithm is applied in a search and retrieval application. However, the weights for the degree-three pairs are between 0.2 co-occurrences (degree-one pairs) are reduced to nearly zero, while others are significantly larger.
Table 7 also shows the average number of paths by term-to-term value range for LISA Words. Clearly the degree-two pairs that have a similarity close to zero have a much smaller average number of paths than low similarity values, and pairs with a moderate number of co-occurrence paths tend to receive high sim-ilarity values. This explains how LSI emphasizes important semantic distinctions, while de-emphasizing with many paths of connectivity tend to receive high similarity values, while those with a moderate number tend to receive negative values. These degree-two pairs with high values can be considered the  X  latent semantics  X  that are emphasized by LSI. 4. Analysis of the LSI values
In this section we expand upon the work described in Section 3. The results of our analysis show a strong correlation between the values produced by LSI and higher-order term co-occurrences.
 4.1. Data sets
We chose six collections for our study of the values produced by LSI, the four collections used in Section 3, and two additional collections, CACM and NPL. These collections are described in Table 8 . These col-lections were chosen because they have query and relevance judgment sets that are readily available.
The Parallel General Text Parser (PGTP) ( Martin &amp; Berry, in press ) was used to preprocess the text data, including creation and decomposition of the term document matrix. For our experiments, we applied the log entropy weighting option and normalized the document vectors. We chose this weighting scheme because it is commonly used for information retrieval applications. We plan to expand our analysis to in-clude other weighting schemes in the future.

We were interested in the distribution of values for both optimal and sub-optimal parameters for each
F depending on our needs. In our experiments we used beta = 1 for the F ler collections and values up to k = 500 for the LISA and NPL collections. For each value of k , precision and recall averages were identified for each rank from 10 to 100 (incrementing by 10), and the resulting F was calculated:
The results of these runs for selected values of k are summarized in Figs. 2 and 3 . To choose the optimal k , we selected the smallest value that provided substantially similar performance as larger k values. For example, in the LISA collection k = 165 was chosen as the optimum because the k values higher than 165 both the decomposition and the search and retrieval processing. The optimal k was identified for each col-lection and is shown in Table 8 . 4.2. Methodology
The algorithm used to collect the co-occurrence data appears in Fig. 4 . After we compute the SVD using the original term by document matrix, we calculate term-to-term similarities. LSI provides two natural methods for describing term-to-term similarity. First, the term-to-term matrix can be created using
T ( T k S k ) matrix can be used to compare terms using a vector distance measure, such as cosine similarity. In this case, the cosine similarity is computed for each pair of rows in the T sults in a value in the range [ 1, 1] for each pair of terms ( i , j ).

After the term similarities are created, we need to determine the order of co-occurrence for each pair of terms. The order of co-occurrence is computed by tracing the co-occurrence paths. In Fig. 5 we present an example of this process. In this small collection, terms A and B appear in document D1, terms B and C appear in document D2, and terms C and D occur in document D3. If each term is considered a node in a graph, arcs can be drawn between the terms that appear in the same document. Now we can assign order of co-occurrence as follows: nodes that are connected are considered first-order pairs, nodes that can be reached with one intermediate hop are second-order co-occurrences, nodes that can be reached with two intermediate hops are third-order pairs, etc. In general the order of co-occurrence is n + 1, where n is the number of hops needed to connect the nodes in the graph. Some term pairs may not have a connecting terms that do not have a connectivity path. The number of paths of co-occurrence can also be derived by original binary term-to-term matrix, the values in the ( i , j ) entry of A length n between term i and term j . 4.3. Results
The order of co-occurrence summary for the NPL collection is shown in Table 9 . The values are ex-pressed as a percentage of the total number of pairs of first-, second-and third-order co-occurrences for each collection. The values in Table 9 represent the distribution using the cosine similarity. LSI perfor-mance is also shown.

Table 10 shows the correlation coefficient for all collections. There is a strong correlation between the percentage of second-order negative values and LSI performance for all collections, with the correlations for MED appearing slightly weaker than the other collections. There also appears to be a strong inverse correlation between the positive third-order values and the performance of LSI. In general the values for each order of co-occurrence/value pair appear to be consistent across all collections, with the exception of the third-order negative values for CACM.
 The corresponding data using the term-to-term similarity as opposed to the cosine similarity is shown in but there are major variances in the correlations for the positive values.

Table 12 shows the values when the correlation coefficient is computed for selected ranges of the cosine similarity, without taking order of co-occurrence into account. Again we note strong correlations for all collections for value ranges ( .2, 1], ( .1, .01] and ( .01, .01).

Table 13 shows the corresponding values for selected ranges of the term-to-term similarity. These results are more difficult to interpret. We see some similarity in the ( .02, .01], ( .01, .001] and ( .001, .001) ranges for all collections except MED. The positive values do not lend weight to any conclusion. NPL and CACM show strong correlations for some ranges, while the other collections report weaker correlations.
Our next step was to determine if these correlations existed when the distributions and LSI performance were compared across collections. Two studies were done, one holding k constant at k = 100 and the second relation between the second-order negative and zero values and LSI performance, when k = 100 is used.
These correlations are not as strong as the correlations obtained when comparing different values of k 4.4. Discussion
Our results show strong correlations between higher orders of co-occurrence in the SVD algorithm and the performance of LSI, particularly when the cosine similarity metric is used. In fact higher-order co-co-occurrence to improve performance in applications such as Search and Retrieval, Word Sense Disam-biguation, Stemming, Keyword Classification and Word Selection.
 Philip Edmonds shows the benefits of using second-and third-order co-occurrence in Edmonds (1997) .
Experimental results show that the use of second-order co-occurrence significantly improved the precision of the system. Use of third-order co-occurrence resulted in incremental improvements beyond second-order co-occurrence.

Zhang, Berry, and Raghavani (2000) explicitly used second-order term co-occurrence to improve an LSI based search and retrieval application. Their approach narrows the term and document space, reducing the documents, and finally selecting all documents that contain the expanded list of terms. This approach re-duces the non-zero entries in the term document matrix by an average of 27%. Unfortunately average pre-cision also was degraded. However, when terms associated with only one document were removed from the reduced space, the number of non-zero entries was reduced by 65%, when compared to the baseline, and precision degradation was only 5%.

Schu  X  tze (1998) explicitly uses second-order co-occurrence in his work on Automatic Word Sense Disam-of senses , but automated systems based solely on keyword analysis would return this sentence to a query that asked about the sense of smell. The paper presents an algorithm based on use of second-order co-to be discriminated.

Xu and Croft (1998) introduce the use of co-occurrence data to improve stemming algorithms. The pre-the equivalence classes produced by an aggressive stemmer, such as the Porter stemmer. The algorithm algorithm implicitly uses higher orders of co-occurrence. A strong correlation between terms A and B, and also between terms B and C will result in the placement of terms A, B, and C into the same equivalence than two are also possible in this application.

In this section we have empirically demonstrated the relationship between higher orders of co-occurrence in the SVD algorithm and the performance of LSI. Thus we have provided a model for understanding the performance of LSI by showing that second-order co-occurrence plays a critical role. In the following section we provide an intuitive mathematical basis for understanding how LSI employs higher-order co-result to applications in information retrieval. 5. Transitivity and the SVD
In this section we present mathematical proof that the LSI algorithm encapsulates term co-occurrence information. Specifically we show that a connectivity path exists for every non-zero element in the trun-cated matrix.

We begin by setting up some notation. Let A be a term by document matrix. The SVD process decom-document by dimension matrix D . The original matrix is re-formed by multiplying the components,
A = TSD T . When the components are truncated to k dimensions, a reduced representation matrix, A formed as A k  X  T k S k D T k ( Deerwester et al., 1990 ).

The term-to-term co-occurrence matrices for the full matrix and the truncated matrix are shown in (2) and (3) , respectively:
We note that elements of B represent term co-occurrences in the collection, and b term i and term j co-occur in any document in the collection, b u ip is the element in row i and column p of the matrix T ,and s
B 2 can be represented in terms of T and S as shown in (6) : An inductive proof can be used to show (7) :
And the element b n ij can be written using (8) : To complete our argument, we need two lemmas related to the powers of the matrix B .
 ij th element of B n is non-zero.

Lemma 2. If there is no transitivity path between terms i and j, then the ij th element of B
The proof of these lemmas can be found in Kontostathis and Pottenger (2002) . We are now ready to present our theorem.
 path between term i and term j.

Proof. We need to show that if y ij 5 0, then there exists terms q b y = 0 for all k .

Assume the T and S matrices have been truncated to k dimensions and the resulting Y matrix has been formed. Furthermore, assume there is no path between term i and term j . Eq. (5) represents the y
Assume that s 1 &gt; s 2 &gt; s 3 &gt; &gt; s k &gt; 0. By Lemma 2, b
We take the limit of this equation as n !1 , and note that  X   X 
 X  2 n ! 0 and the summation term reduces to zero. We conclude that u (8) we have (10) : u u jp = 0 for all p such that 1 6 p 6 k . Substituting back into (5) shows that y
The argument thus far depends on our assumption that s 1 &gt; s customary to truncate the matrices by removing all dimensions whose singular value is below a given threshold (Dumais, 1993); however, for our discussion, we will merely assume that, if s s
In this case, the above argument shows that either u iq =0or u
To handle the second case, we assume that z &lt; k and the z ... z + w dimensions have not been truncated and rewrite Eq. (8) as (12) :
The argument above can be used to show that u ip u jp = 0 for p 6 z 1, and the first summation can be re-Taking the limit as n !1 , we conclude that
Again using the argument above, we can show that u ip u jp holds, and our proof is complete: 6. Conclusions and future work
Higher-order co-occurrences play a key role in the effectiveness of systems used for information retrieval and text mining. We have explicitly shown use of higher orders of co-occurrence in the Singular Value Decomposition (SVD) algorithm and, by inference, on the systems that rely on SVD, such as LSI. Our empirical studies and mathematical analysis prove that term co-occurrence plays a crucial role in LSI. The work shown here will find many practical applications. Below we describe our own research activities that were directly influenced by our discovery of the relationship between SVD and higher-order term co-occurrence.

Our first example is a novel approach to term clustering. Our algorithm defines term similarity as the term similarity is more directly correlated to improved performance than is use of the reduced dimensional term-to-term matrix values. In Kontostathis, De, Holzman, and Pottenger (2004) we describe the use of these term clusters in a system which detects emerging trends in a corpus of time stamped textual docu-ments. Our approach enables the detection of 92% of the emerging trends, on average, for the collections we tested.

Our second, and more ambitious, application of these results is the development of an algorithm for approximating LSI. LSI runtime performance is significantly poorer than vector space performance for two reasons. First, the decomposition must be performed and it is computationally expensive. Second, the matching of queries to documents in LSI is also computationally expensive. The original document vec-tors are very sparse, but the document by dimension vectors used in LSI retrieval are dense, and the query must be compared to each document vector. Furthermore, the optimal truncation value ( k ) must be discov-rithm that approximates the performance of an optimal LSI system while reducing the computational overhead.
 Acknowledgments
This work was supported in part by National Science Foundation Grant Number EIA-0087977. The authors gratefully acknowledge the assistance of Dr. Kyriakos Kontostathis and Dr. Wei-Min Huang in would like to express their gratitude to Dr. Brian D. Davison for his comments on a draft. The authors gratefully acknowledge the assistance of their colleagues in the Computer Science and Engineering Depart-ment at Lehigh University and the Department of Mathematics and Computer Science at Ursinus College in completing this work. Co-author William M. Pottenger also gratefully acknowledges his Lord and Sav-ior, Yeshua (Jesus) the Messiah, for His continuing guidance in and salvation of his life. References
