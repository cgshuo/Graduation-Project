 Web search performance can be improved by either improving the search engine itself or by educating the u ser to search more efficiently. There is a l arge amount of literature describing techniques for measuring the former; whereas, improvements resulting from the latter a re more difficult to quantify. In this paper we demonstrate an experimental methodology that proves to successfully quantif y impro vem ents from user education. The user education in our study is realized in the form of tactical search feature tips that expand user awareness of task -relevant tools and feat ures of the search application. Initially, these tips are presented in an idealized situation: each tip is shown at the same time as the study participants are given a task that is constructed to benefit from the specific tip. However, we also present a follow -up study roughly one week later in which the search tips are no longer presente d but the study participants who previously were shown search tips still demonstrate improved search efficiency compared to the control group. This research has implications for search user interface designers and the study of information retrieval systems . H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval  X  query formulation, search process , G.3 [ Mathematics of computing ]: Probability and statistics  X  experimental design. Measureme nt, Design, Experimentation, Human Factors User studies, search interface, experimental design, effectiveness measures, query reformulation, expertise, tactics, tips, suggestions, assistance, efficiency. Research on imp roving search performance often focuses on the algorithm, input modality, and visualization improvements. However, substantial improvements may also come from research on improving searcher expertise.
 Other researchers share this vision [6, 13, 30] , and one approach has been to improve pedagogical methods for search instruction [11, 23] and opportunities for modeli ng expertise of others [ 28, 31]. Our study tests the idea that educating searchers can produce a large and valuable improvement in search behavior [14] . Search engines have grown increasingly sophisticated and have evolved past the traditional query/response paradigm to include scores of tools and options for specifying queries, targeting certain corpora, visualizing results, browsing result sets, and so on. These tools are often hidden but have the potential to offer tremendous efficiency gains to users who know how to use them appropriately.
 Thus, beyond term selection and query formulation, search actions include search techniques, corpus selection, features, and strategie s used to solve information needs. We refer to these as search tactics. A tactic is a low -level plan to achieve a search goal, where the goal is simply to solve the present information need [1]. We believe that, in addition to suggesting relevant documents or query reformulations [24] , there is high latent value in making relevant search tactics more salient to users. We illustrate this with two studies: first, a study which demonstrates that users c an apply search tactics effectively to improve their search performance, and second a study which demonstrates that some improvements are retained when the tactical tips are removed and roughly one week passes. The results of the study are followed by a di scussion of implications for designers and researchers of IR systems and expertise. Jansen [16] uses the phrase  X  automated assistance  X  in IR systems to refer to specific actions taken by the system to improve search behavior according to some metric. However, we have found in our review that there are multiple, ambiguous and overlapping definitions of automated assistance, including strategies that are presented, tactical advice, feedback on search performance [35] , and hints. Jansen and McNeese X  X  taxonomy [ 17] can be expanded to encompass alternative suggestions. For our purposes, we classify automated assistance into three categories: suggestions for query expansion, search strategies, and search tactics instruction. Suggestions for query expansion are the most popular class of suggestion s. Mizzaro [27] provides a review of systems with different a utomated assistance, all of which aim to influence the user to reformulate or refine his/her query. These include thesaurus systems, automatic reformulation based on semantic analysis, translating natural language queries, and dialog systems to disambiguate queries.
 Query suggestions to improve the likelihood of success are seen as most useful in exploratory search for beginning searchers, Copyright 2011 ACM 978 -1-4503-0757-4/11/07...$10.00. especially on topics for which they have little knowledge or familiarity [ 18, 20, 22]. Comparing two techniques that ultimately aim to influence query formulation, Belkin, et al. investigated term relevance feedback (meant for manual query reformulation) and Local Context Analysis (mean t for automating reformulation), finding that the automatic LCA resulted in fewer user -defined queries, but with no difference in performance for locating document instances within a closed set [3]. Kelly, et al. [20] found that specific query suggestions are preferred by searchers over term relevance feedback.
 Kelly, et al. [19] as well as Capra and P X rez -Qui X ones [10] studied how the social dimension of such suggestions can influence their usage, drawing upon theories of social influence. The effect on task efficiency was not measured. Showing information about the popularity and quality of a query suggestion was shown to influence usage, wit h quality being more important than popularity [ 19].
 The suggestions feature d in Brajnik, et al.  X  X  [7] FIRE system are primarily aids for query expansion (despite being referred to as strategy sugge stions by the authors) . They define two classes of such suggestions:  X  X ints X  (giving feedback to the user about how the system is interpreting his/her query) and  X  X dvice X  (executable links to system features to aid query reformulation, such as browsing the thesaurus). In evaluations with 6 users, some found the suggestions in their system  X  X oo general X  and that, though it sped up their search process, it did not change their search strategy, and participants left believing they would have followed the strategies even without any help. Like the FIRE system, DAFFODIL [ 21] was created to give search strategy advice over a closed corpus of scientific literature. It was evaluated with 12 users on an open -ended task. The 16 strategie s suggested by DAFFODIL included tactics such as trying alternative spellings of keywords and restricting/broadening the query. Evaluation was done with self -reported Likert assessments that indicated promise for tactical suggestions, but again, task perf ormance was not measured.
 Jansen and McNeese [17] found that automated assistance for query expansion resulted in modest performance gains in identifying relevant documents over a closed corpus (using the TREC behavior metric). They found that automated assistance was implemented in 71% of the instances where it was requested (i.e. it was not automatically shown) and that this rate did not depend on expertise. Brajnik, et al. [7] summarize search strategies as  X  X he sequence of search statements that identify, restrict, limit a set of retrieved documents to solve an information problem  X . Belkin, et al. [5] classify search strategies into four independe nt dimensions: method, goal, mode, and resource.
 Kriewel and Fuhr [21] argue that an ideal search system would provide useful strategies for completing specific tasks  X  either by request or proactively helping users improve the ir search. The goal is to use what we know about search procedures to suggest search strategies.
 The system described by Brajnik, et al. [7] attempts to infer  X  X ritical X  situations (such as receiving zero results from a query) and uses a knowledge -based approach to provide strategic suggestions for searching a closed set of scientific literature . Suggestions included scanning related topics or restricting a result set to a particular journal. Again, only 6 participants provided qualitative assessments of how well they thought the system would perform. For search purposes, strategies are longer -term plans that are executed across many moves. By contrast, search tactics are much shorter sequences of moves intend ed to accomplish a single information goal [ 2]. Bates defines four types of search tactics [2]: monitoring (keeping the search on track), file structure (identifying desired file type), formulation (aiding query redesign), and term (aiding selection and revision). In modern search engines, diverse tools are integrated and allow the user to, in effect, use different modes and mediums of search in the same session. Tactical suggestions provide immedia te guidance to exploring unfamiliar corpora or visualizations without necessarily focusing on query reformulation. Examples of modern tactical search suggestions include switching corpora from general webpages to discussion forums or images, changing the input modality from text to voice, changing the visualization of the result set from a list to a map, and changing from keyword to exact phrase terms.
 The system presented by Kriewel and Fuhr [21] includes suggestions we would classify as tactical, such as visualizing a social graph of authors, and focusing the query on particular conference proceedings. While these tactics are presented to the user, the description of the system does not include any metric that measures how se archer behavior changes.
 For most of the prior work on search suggestions, performance is measured by the TREC standard definition of task success: that is, the proportion of documents identified as relevant to the task , where  X  X elevant X  is determined by a group of judges [4]. This is logical because the field is primarily concerned with evaluating IR system performance whereas we are interested in improving user performance. Hence, we instead measure performance in terms of tas k completion time (TCT) by searchers on the live, unstructured web. To be sure, query specificity and domain -appropriate term selection remain important components of search expertise. However, this focus on query e xpansion has neglected other aspects of search expertise, such as task -appropriate tactics based on available search engine features. This area has been understudied for multiple reasons. First, tactical suggestions are not common in closed -corpus systems because query reformulation may be sufficient. Second, IR research systems often test one search engine on one corpus instead of integrating multiple corpora, as is often the case in commercial systems that tie together web, images, video, news, blogs, and other corpora. Third, the tradition of focusing on query reformulation, rather than exploratio n, has acted as momentum toward features that influence them. A  X  X actical tip X  is a short suggestion for the searcher that suggests a direct action to take to improve the likelihood of successfully solving a search problem. Such a tip does not suggest possible likely documents or query alternatives, but suggests potentially successful approaches to solve the information need. Our experiment seeks to determine if an appropriate tip shown at the moment of need can measurably improve overall searcher performance. To determine this, a controlled user study was conducted whe re each participant was assigned six directed search tasks. This between-subjects study had one independent variable: availability of a task-relevant tactical tip. The dependent measure was task completion time (TCT), our primary metric for search task per formance.
 For each trial, the participants in the experiment group were given a search task and were asked to press a  X  X tart X  button after they had read both the task and the tip. The participants in the control group were presented only the task but not the tip. The participants performed the task in other windows or tabs and returned to the user study tab to enter their answer. The screen was then cleared and the next task was displayed.
 The user interface fo r the study displayed the task as shown in Figure 1. For the experiment group, static screenshots illustrating how to use the tip in question were also displayed. For the control group (which saw no tips), only the light blue textbox was displayed.
 Each participant varied in the amount of time they s pent on the study (for a maximum of 5 minutes per task). All instructions were given textually but participants were unable to copy and paste from the question into a search box because that text was represented as an image. We asked participants to  X  X ry t heir best X  but an honorarium was paid regardless of performance. The experiment was conducted on the  X  X pen Web X  but with specified (directed) tasks [ 29]. Participants were instructed to use the Firefox b rowser and the Google search engine, but were permitted to enter any query and view any pages they chose. We identified six tactical search skills, each of which is designed to be well -served by a particular search tactic: specific query syntax, a featur e of the user interface, or narrowing to a particular corpus. (e.g., Skill -2: knowing how to limit searches to a particular file type) Then, for each skill we created two tasks. Tasks 1A and 1B both are more easily solved if the participant has a good gr asp of Skill -1, Task 2A and Task 2B with Skill -2, etc. (The set of 6 tips is available at: http://bit.ly/TacticalSearchTips . See Table 1 for the text of the 6 tips as well as the corresponding 12 tasks.) The tasks were designed with the following requirements: a) the task was judged to be difficult or required a great deal of effort to solve, b) answers to the task can be easily determined to be correct or incorrect and c) the task is well -specified, leavi ng very little room for misinterpretation. Participants had a maximum of five minutes to complete each task. They were instructed to give up if they were not able to find (what they believed to be) the correct answer by that time. The time elapsed was show n on the task webpage. The tasks were purposefully designed to be both difficult to complete and yet still be considered  X  X ookup X  tasks, which are characterized by Marchionini [26] as concerning fact retrieval, known item sea rches, navigation, verification, and question answering.
 As shown in Figure 1, the question text was always shown in a light-blue box and, in the tip conditions, the relevant tip was shown below. The tip often (but not always) included screenshots illustra ting how to use the feature in question, with callouts to make its functionality immediately apparent. The 491 paid participants (59% female, varied professional and educational backgrounds) in this study are part of a larger pool of people around the U.S. who perform search comparison tasks as part of their normal work flow, selecting tasks from a work -task queue. Participants varied widely in profession and computer expertise. Each participant was randomly assigne d to either the experiment (t ip shown) group or the control (no tip shown) group. 
Figure 1: Task 4A shown here with its tactical tip. This is the layout of the actual instructions the study participant would see. The tip depicted here illustrates the skill searching for We conducted this study in a remote setting for multiple reasons. Participants were able to use the browser and monitor with which they were comfortable. The participants also have a great deal of experience completing tasks of this sort, and so were not subject to strong learning or novelty effects. Because t hey had a substantial amount of prior practice performing similar tasks, we elected not to run practice trials before beginning the experiment. There were roughly 50% of the participants in the experiment and control groups, with around 25% of the population in each of the 4 subgroups: Control -task -set -A, Control -task -set -B, Experiment -task -set -A, Experiment -task -set -B. (The actual numbers vary slightly from group to group d ue to a small number of abandoned task sets by participants.) In addition to answer task questions posed, the participants were also asked to rate how successful they thought they were on each task. Each participant was asked to solve all 6 tasks within a maximum of 5 minutes for each task. Figure 2 shows task completion times for the experiment and control groups for each of the 12 tasks (the box plots were generated using the R package lattice ). In these box plots, the black dot, bottom and top of the boxes display the median, 25% and 75% quantiles, respectively. We added black lines to connect the medians for the control and experiment groups for each task.
 Comparing the task completion times for the control and experiment groups in Figure 2 shows substantially faster times for the experiment group for each task. For example, the difference in median task completion time between experiment and control is more than 2 minutes for most tasks. Even for Task 5B, which has the smalles t difference between groups, the median time is improved by nearly 30 seconds.
 In Tasks 1A, 1B, 3A, and 3B the control group fared particularly poorly, with roughly 75% of the participants taking the maximum allowed time (5 minutes). These 4 tasks are esp ecially difficult without the knowledge taught by the two corresponding tactical tips. The conclusion is that this data supports our original hypothesis  X  we can measure improvement in search efficiency resulting from search feature tips. Formal verificat ion of the statistical significance of the results is presented later in this paper. Study participants shown tactical search tips consistently outperformed study participants who were not shown any tips for that task. The tactics present strate gies and user interface features but do not suggest relevant documents, media, or answers. We believe that improvements in search performance can be achieved not only from improving retrieval algorithms but also from improving search expertise. The results of this study confirm and quantify this belief: showing searchers tactical tips at a moment just before the skill is to be used reduced search task complet ion time by roughly 35% for our tasks. The substantial reduction in task completion time is worth n oting. The implication is that providing users with the right tactic at the right time can improve performance dramatically. The larger challenge for IR system designers, then, is to enumerate available tactics and match t hem with search task behaviors.
 Task Prompt Search Tip 1A Imagine you live in Centreville, MD. What library within 15 1B Imagine you live in Grand Junction, CO. What is the closest 2A There are many Microsoft Word files freely available on the 2B There are many Microsoft Word files freely available on the 3A Which month of 2010 has had the most tweets about Demi 3B Which day in July 2010 were there most tweets about George 4A Paste the URL of a video about Surat Thani, Thailand that is 4B Paste the URL of a video about Nagaland, India (or people 5A What is the song that contai ns the lyric who are you gonna 5B Find a web page that has the speech that contains the phrase go 6A Paste 3 URLs that include the full names of people who have 6B Paste 3 URLs that include the full names of people who have A clear question about these results is,  X  X re participants actually learning to use the tactic presented in the tip, or are they blindly applying it in a rote manner as cued? X  To answer this question, we carried out a follow -up study designed to measure if participants w ere retaining knowledge of when and how to use the tactic s. We conducted a follow -up experiment roughly one week later to determine if the performance gains shown in the first experiment are mainta ined . We did this by re -testing both the control and experiment groups with the question variants that they had not seen in Week 1. To assess learning effects of the tips in question, we re -assessed the participants from Week 1 using the task varia nt they had not already seen. In other words, a participant who did Task 6B in Week 1 would be assigned Task 6A in this Week 2 study. This ensures that no participant would see the same task twice. (We note that 146 of the 491 participants from Experiment 1 were unable to participate, which is expected in the normal course of these studies.) Participants were all retested between 5 and 9 days after they completed the first experiment (timing variation depending on when they decided to take part in each ex periment). Participants were unaware that there would be a follow -up study, and not informed that the second set of tasks was connected with the first task set in any way. Figure 3 displays box plots for the Week 2 study alongside those from Week 1 . In nearly every case (Task 5B being the only exception), the performance improvement (task completion time decrease) from seeing the tip was retained: the skill the participants had learned 5 to 9 days earlier was retained even though there was no stimu lus to remind the participants. In fact, the differences between the control and experiment group are almost as large as in Week 1. Thus, the data supports the notion that we can measure retention of the skills learned from the tips. The next section va lidates the statistical significance of this finding for each task.
 In addition to this experiment effect, we can also notice in Figure 3 an interesting effect in the control group from Week 1 to Week 2: namely, participants in the control group improved their performance slightly. This can be attributed to practice and is a common finding in studies of this kind. In fact, by virtue of completing the Week 1 tasks many of the control participants may have discovered, learned or remembered the knowledge t hat would have been conveyed in the (unseen) tip . This improvement in the control group underscores the importance of using an experimental design that retests the control group in Week 2 in addition to the experiment group. Task 5B is curiously differen t in that it is the only task for which the improvement from Week 1 did not carry over from Week 2. Looking at the data in Figure 3, we see that the control group was especially fast with respect to this task (compared to other tasks) in Week 1 and even m ore so in Week 2. We believe that this task was fairly simple for the participants (especially after having had some practice with the complementary Task 5A in Week 1), which may explain why the improvement relative to the experiment group did not carry o ver. In this section we validate the statistical significance of the findings. Specifically, the data from the Week 1 study suggests that the search tips substantially improve performance for all 12 tasks. Further, the d ata from the Week 2 study suggest that at least some of the experiment group X  X  advantage over the control group is retained for all tasks except 5B. These findings are valid because it is unlikely to obtain such substantial differences between the control and experiment groups simply by chance given the fairly large participant sample size and the fact that we randomly assigned users to the control and experiment groups. In this section, we formally validate this statement using inference in the context o f a statistical model.
 The model takes into consideration participant skill variation, the difficulty of each task, and the tip effects for the experiment group. In Week 2 the model also takes into account practice effects for each task as well as the fact that some of the tip effects may be diminished or forgotten for some tasks. Formally, we will write our model as where the respon se variable Log(TCT) ij is the logarithm of the task completion time of user i when doing task j. The logarithm is used because the task completion times are right -skewed, as is done in analogous models employed in task completion time studies (e.g. [ 25]).
 Because the independent variables in this model consist of both fixed effects and random effects, the type of model is known as a mixed effects model. Specifically, the U i are random user effects that account for individual s kill variation [ 6, 12, 15] and are included because some users are naturally faster than others. We assume that these user effects are independent, zero -mean normal rand om variables with a common variance. Similarly, the error terms in the model  X  ij are also assumed to be independent normal variables with mean zero and a common variance.

Figure 4: Analysis of Experiment 1 results. The improvement in task completion tim e (TCT) due to the tip was statistically significant for all 12 tasks. 95% confidence intervals are The remaining terms t j , d j , p j and f j are all fixed effects associated Thes e are the 12 effects of interest in Week 1. The terms d the difficulties of the 12 tasks. These are not of interest but are essential to include in the model to account for inherent variations in task difficulty. The terms p j are the 12 practice ef fects that measure any benefit from practice for each task in Week 2. These are also not of primary interest but are important to include to model the effects of individual practice. Finally, the terms f measure the extent to which the benefit of the ti ps diminishes for each of the 12 tasks for the experiment group in Week 2.
 We fit the above model to the data from the 345 participants who completed tasks in both Week 1 and Week 2. We used the R package lme4 X  X  implementation of restricted maximum likeli hood and the package languageR to generate confidence intervals.

Figure 5: Analysis of Experiment 2 results. The improvement in task completion time (TCT) due to the memory of the tip was statistically significant for all 12 tasks except Task 5B. In Figure 2 we saw that the (Week 1) tip effects appeared to be substantial for all 12 tasks. The model above verifies that indeed all of these effects are statistically significant. Specifically, Figure 4 displays 9 5% confidence intervals for the percent improvement as a result of the tip for each task. We can see that none of these confidence intervals overlap zero. Again, Tasks 1A, 1B, 3A and 3B show the largest improvement. In particular, Task 3A and 3B are bot h estimated to have approximately a 60% improvement in efficiency.
 In Week 2 the effects of interest are the sum of t j and f tasks. In other words, it is of interest to see whether the tip effect remains for each task after the participants ha ve had roughly a week to forget the tip. Figure 3 suggested that the tip benefit does remain except for Task 5B. Again, using the model presented above we can confirm that these effects are indeed statistically significant. This is evidenced by the fact that the 95% confidence intervals shown in Figure 5 do not overlap zero with the exception of 5B. Tasks 3A and 3B show the strongest persistent tip effect. That the overall performance gains remain for most tasks is understandable given that the tactic in the associated tip is difficult to discover wit hout the tip and that the tasks are fairly easy using the tactic and very difficult without it. . This was a specific design goal of the experiment. In spite of this, inspection of the tasks (shown in Table 1) shows they are feasible tasks. Up to this point we have considered only the task completion time as our dependent variable. However, in addition to measuring task completion time we also asked the participants to self -report whether or not they felt they had successfully completed each task. In this section we present a brief analysis using the self -reported success rate as the dependent variable.
 Figure 6 compares these success rates for the control and experiment groups in both Week 1 an d Week 2. We see in the figure that the success rates for the experiment group are higher than those of the control group for all 12 tasks in both weeks. This result is not surprising since we expect the success rates to mirror the task completion time d ata by virtue of the nature of our experiment. Figure 6 also shows that overall the experiment group achieved high absolute success rates for all tasks in both weeks, in excess of 80% in most cases. Conversely, the control group did quite poorly for cer tain tasks such as 1A, 1B, 3A and 3B. Recall that we noted earlier these 4 particular tasks also had the strongest experimental effects in our task completion time analysis.
 In order to verify the statistical significance of the experiment versus control differences observed in Figure 6 we fit a model analogous to our previous model for task completion time. The model uses the log odds of the success probability as the response. From analysis of that model we see in both Week 1 and Week 2 all differences are statistically significant except those for Tasks 5B and 6B. Overall, our conclusion is that the tips presented substantially improve (self -reported) success rate in addition to task completion time. We will note that we also carried out the same ana lysis above but instead using an objective measure of success based on what we had predetermined to be  X  X orrect X  answers for the tasks. That analysis gave even stronger results; however, we chose to present the self -reported success rates here in favor of those results because our predetermined  X  X orrect X  answers are arguably biased in favor of the search tips. Participants who did not use the tips often found different but possibly correct answers by using alternative search strategies , which we had not c onsidered. We were not able to verify the correctness of their answers in every case. Thus, the self -reported success seemed to be a more fair measure to us.
 These studies demonstrate that we can measure the efficiency gains resulting from relevant search tips, and that these gains persist and continue to be measurable over at least one week, even wh en the search tips are no longer shown. In the literature on IR systems, few interventions have been able to show such dramatic and sustainable gains in performance strictly by focusing on improving user performance.
 A relatively large, diverse sample of u sers participated in the study, and it was done in an ecologically valid setting. The tasks used in this study were explicitly designed to be both feasible for common use and to show the positive effects of knowing which tactic is appropriate for which tas k. We can attribute the perfo rmance gains of the experiment group to two possible causes: 1) users learned a new tactic and associated it with a type of task or 2) users were reminded of a previously -known tactic and associated it with a type of task. Both are valid reasons for why tactical suggestions can be useful.
 This experiment did not explicitly test the transfer of a learned tactic to another task. Instead, the follow -up study used tasks very closely related to those in the first experiment. The resu lts make no claim of how well users learned to generalize the tactic but rather that certain tactics can be associated with certain, narrow types of tasks. Prior work points to task [ 8, 9, 10], search [ 6], and domain [ 32, 34] complexity and expertise as some of the contextual factors influencing search performance. The results of our st udy suggest that providing relevant search tactics may be construed as either augmenting an individual X  X  search expertise (i.e. knowing the tactic well enough to match a task to it) or task expertise (i.e. knowing the task well enough to choose a particula r known tactic for it). We propose that such tactical tips are improving upon search expertise rather than that of task or domain. Domain expertise (or topic familiarity) is not particularly relevant and task expertise is not being tested because the tasks are directed lookup tasks that do not require multiple steps or a particular strategy to solve.
 The tasks used in this study were explicitly designed to draw out the importance of knowing particular tactics. The use of suggestions should likewise be highl y selective in order to maintain a consistent expectation of utility in the user.
 Research on improving automated assistance in IR systems tends to improve upon methods of suggesting ways of reformulating queries. The tips used in this study influence user s to frame their question s in a new way or to become aware of tactics that are supported by the system and may be useful. From a systems perspective, this requires system designers to be intricately aware of what types of tasks are best -suited for the feat ures supported. Already, search engines are attempting to  X  X lend X  results from different corpora into the standard result set and offer entry points into those corpora (e.g. integrating a few images into a document result set with a prompt for seeing this query in image search). It is clear that an understanding of a user X  X  level of sophistication and search knowledge is of importance to information retrieval systems, ostensibly to offer appropriate or adaptive feedback and suggestions. For example, White, et al. [33] used the presence of advanced search operators in queries as a proxy for expertise. Another indicator may be the use of specific search tactics.
 The differences between the control and experiment groups in self-repo rted successful completion points to the idea that some tactics do not merely improve efficiency, they make some queries possible. For example, participants in Task 3 A and 3B who were made aware of the tactic to use the micro -blog searching feature were ab le to consistently answer this very feasible query while participants who remained ignorant of the tactic simply could not finish it (i.e. gave up after reaching the experiment -imposed maximum of five minutes).
 Though the primary goal of IR research is to produce the document the user is seeking , recent cases have been made that move study from document retrieval into exploration of relevant corpora or perspectives on the information need [ 25]. This shift requires a new perspect ive on the purpose of automated assistance . Instead of inferring the user X  X  information need as he or she iterates on queries, the system can help the user refine the information need itself. For example, when searching for a book to buy, the search engine could suggest a tactic that shows the locations of libraries nearby given the need to understand a topic. Once ignorant of such a possibility, the user X  X  information need has shifted.
 As note d earlier, this study used task completion time as a measure of performance rather than recall or precision . This study was conducted on the open web and the users had the freedom to formulate their own queries or browse as they liked. Our goal is to improve user expertise so we focus on measuring user performance.
 The visual and interaction design of tactical tips is an interesting domain of further inquiry. The visual design of the tips used in this study was not constrained by size in any significant way. They also did not incorporate animation or interactivity in an y way. The display of the tip would also differ substantially between user -triggered (e.g. [ 17]) and automatically displayed (as was done in the present study). The following are three major limitations in our study. As explained in [ 29], we used artificial tasks that were assigned to the participants, not  X  X ersonal X  tasks that were generated by the participants. Second, these results represent a  X  X est case scenario X  where the tips are highly relevant to the task at hand. In practice, it is a much more challenging problem to design the actual user interface so that the tips trigger at the correct time and in the correct context. For this reason our observed improvement in efficiency can be thought of as an upper bound for what could be obtained in practice. Third, we largely ignored task accuracy as a metric for this study in favor of efficiency alone. In a sense, efficiency was a proxy for accuracy because participants were instruc ted to give up if they hit the five -minute limit. Both experiments presented in this paper (Week 1 and Week 2) used what might be called a simple 2 group  X  X etween-subjects X  or  X  X ase/control X  design. Because our treatment effect (learni ng the skill) is irreversible, we did not have the option of using a standard cross -over design. However, an analogue of the four group cross -over design presented in [ 25] would have been feasible. We chose not to use that pa rticular design due to its complexity and because the experiment in [ 25] showed that design was actually slightly worse for estimating experimental effects for the individual tasks, which was the primary interest in our study. This study demonstrates that providing tactical search tips, appropriately shown to searchers at the right time, can produce a substantial and measurable improvement in their performance. This gives us hope that further instruction can also have a measurable effect on overall searcher behavior, particularly when extended to strategic instruction and longer -term search behavior on more complex tasks. The authors would like to thank Esther Wojcicki and Roy Pea for their discussions during the course of the research. 1. Bates, M. J. (1979). Idea tactics. JASIS , 30, 280 -289. 2. Bates, M. J. (1979). Information search tactics. JASIS , 30, 3. Belkin, N. J., Cool, C., Head, J., Jeng, J., Kelly, D., Lin, S. -J., 4. Belkin, N.J., Muresan, G. (2004) Measuring Web search 5. Belkin, N. J., Marchetti, P. G., Cool, C. (1993). Braque: 6. Bhav nani, S.K. (2001). Important Cognitive Components of 7. Brajnik, G., Mizzaro, S., Tasso, C., Venuti, F. (2002). 8. Bystr X m, K. (2002). Information and Information Sources in 9. Bystr X m, K., Jarvelin, K. (1995). Task Complexity Affects 10. Capra, R., P X r ez-Qui X ones, M.A. (2006). Factors and 11. Fern X ndez -Luna, J. M., Huete, J. F., Macfarlane, A., 12. Grimes, C., Tang, D., Russell, D. M. (2007). Query logs alone 13. Gwizdka, J. (2008). Revisiting search task difficulty: 14. H X lscher, C. Strube, G. (2000). Web search behavior of 15. Hsieh -Yee, I. (1993). Effects of search experience and subject 16. Jansen, B. J. (2006). Using temporal patterns of interactions to 17. Jansen, B. J. Mc Neese, M. D. (2005). Evaluating the 18. Jones, R., Rey, B., Madani, O. (2006). Generating query 19. Kelly, D., Cushing, A ., Dostert, M., Niu, X., Gyllstrom, K. 20. Kelly, D., Gyllstrom, K., Bailey, E. W. (2009). A comparison 21. Kriewel, S. Fuhr, N. (2007). Adaptive search suggestions for 22. Lau, T., Horvitz, E. (1999). Patterns of Search: Analyzing and 23. Lazonder, A. W. (2003). Principles for Designing Web 24. Liu, Y. and Belkin, N. J. (2008). Query reformulation, search 25. Ma, L., Mease, D., Russell, D. M. (2011). A Four Group 26. Marchionini, G. (2006). Exploratory search: From finding to 27. Mizzaro, S. (1996). Intelligent interfaces for information 28. Moraveji, N. (2010). User interface designs to su pport the 29. Russell, D., Grimes, C. (2007). Assigned tasks are not the 30. Teevan, J., Alvarado, C., Ackerman. M., Karger, D. (2004). 31. Twidale, M. B., Nichols, D. M., Smith, G., and Trevor, J. 32. White, R., Dumais, S., Teevan, J. (200 9). Characterizing the 33. White, R., Morris, D. (2007). Investigating the Querying and 34. Wildemuth, B. M. (2004). The effects o f domain knowledge 35. Xie, I., &amp; Cool, C. (2009). Understanding help -seeking within 
