
Istituto TeCIP, Scuola Superiore Sant X  X nna, Ghezzano (PI), Italy Politecnico di Torino, Corso Duca degli Abruzzi, Torino, Italy 1. Introduction
The anomalous patterns are often considered a disturbance that negatively affects the output of the data analysis process and needs to be removed from the dataset in order to make the data trustable for the subsequently processing steps, such as, for instance, the development of a methematical or artificial intelligence-based model. The detection of the anomalous patterns in a database is an interesting problem for data miners, that occurrs when a particular pattern deviates so much from other ones as to arouse the suspicion that it is generated by a different mechanism with respect to the rest of the considered dataset.
In the present work, a novel algorithm called DANIOP (Detection of ANomalous Input-Output Pat-terns) is proposed and tested on a simulated and real dataset coming from the industrial field, which directly applies on continuous data, without the need for any kind of preliminary cathegorisation step. The proposed method is applicable only in the situation when the input-output relationship is  X  X ne to one X  or  X  X any to one X , because in some cases, such as instable systems or when the target value depends on parameters or state variables which are not fed as input to the model, the fact that in an experimental database similar input sets are associated with very different output patterns is normal.
In the presented method a distance based outlier detection [15] is applied where a Euclidean dis-tance [4] is used as distance function, forming a dissimilarity matrix , which is exploited in many ap-plication fields, to the aim of recognizing interesting patterns among the collected data [6,24,26]. The innovation that is proposed here consists in the use of two distance matrices: the first one is computed on input data and the second one on output data; these two matrices are compared in order to find a large distance in the output matrix which correspond to a small distance in the input matrix. This situation occurs when very similar input patterns are associated to very different outputs, which contraddicts the basic principle of functional association.

The idea which is the basis for the present work has already been presented by the authors in [23], but the approach followed in that paper is not as general as the present one, as there was a stronger need for heuristic tuning of some parameters. Moreover no validation in an industrial application have been discussed and no software tool have been presented to implement the proposed approach.

The paper is organized as follows: Section 2 presents some works related to the novel proposed tech-nique, Section 3 is devoted to the implementation of the proposed method confirming it effectiveness by an example, and furthermore an alternative method, based on find a metric with a correlated ex-ample, is exposed. In Section 4 an example of the application of the proposed method to a simulated data is shown, while in Section 5 a practical application to the preprocessing of an industrial database is depicted with the statistical assessment of the obtained results. Section 6 depicts an overview of the implemented software. Finally Section 7 gives some concluding remarks and future work. 2. Related work
The anomalous patterns detection is of interest in many fields, such as network intrusion [1], fraudo-lent activity [27] and system monitoring [21]. In the above cited cases the available data are categorized (namely a preliminary discretization step transforms them from continuous to categorical) in order to allow the application of pattern detection techniques based on association rule mining for pointing out the anomalous patterns [25]. However in many practical applications all the considered variables are continuous, which makes difficult to apply the above-mentioned association-based techniques, because a problem, for instance, of function approximation, is quite different from the ones where association rule mining [1,21,27] is succesfully applied: the discretisation phase requires some a-priori assumptions made by the designer, which cannot always be substantiated by a real knowledge of the problem. More-over, also the mechanism that generates anomalous patterns in industrial databases is quite different, as they are often related to errors occurring during the phase of the data collection on the process lines.
In literature an example of a neuro-fuzzy system applied for functional association between input and output patterns can be found [8]: in function approximation applications, classification [3] and clustering operations [20], a neuro-fuzzy system is applied to associate a set of input variables to a desired output value or vector. In order to successfully and reliably perform such association, the designer needs that the same functional relationship is also somehow represented in the available data; roughtly speaking, similar sets of input values must correspond to quite similar output values. If this condition is verified, the neuro-fuzzy system performs a sort of averaging operation and after the completion of the training procedure, the output that is produced in correspondence to a give input pattern is similar to the output value that is most often associated with that input. Nonetheless, this correct and robust behaviour does not always lead to satisfactory results, especially when some apriori knowledge is available on the desired system behaviour. A preliminary pre-processing stage can thus be useful, which exploits such behaviour and points out the input-output couples where similar input patterns are associated with very different outputs: the input-output couples which are considered less realistic must be eliminated with the support of the technical personnel working in the specific field, plant or applications from which the database has been extracted and the system must be trained by exploiting only reliable data.
In [13] an alternative approach to the discussed problem has been investigated; the proposed method relies on the automatic estimation of reliability of the model output, as such reliability can be direct consequence of the data quality: in fact the areas of the input space where the reliability is low are those where the  X  X ncoherent X  input-output patterns most probably lie. Is noticeable that with the method proposed in this work the problem of the data reliability is eliminated at source and in a very simple way by end-users who are not experienced in the model development but have a deep knowledge of the field where the data are collected. However, differently from DANIOP, the method discussed in [13] requires the preliminary definition of a model which reproduces the input-output mapping and, in order to provide a reliable method for detecting anomalous inpu-output patterns, one must also be reasonably sure that the model is suitable to reproduce the correct behaviour of the system to model. On the other hand the only basic assumption that is required by DANIOP is that all the input variables that are supposed to affect the output have been taken into account. This latter topic is by itself a non trivial problem, often referred as variable selection [12,18], that is not faced by the present work.

The choice of the Euclidean distance [4] as metric has been done here for the sake of simplicity, as it is the most natural way to measure the distance between two patterns and it can reasonably fit different applications. However such choice is not always the most appropriate one for a given function approxi-mation problem. Alternative metrics could be, for instance, the Mahalanobis distance [19] (which makes the distance metric scale-invariant), the Manhattan distance [5] (where the distance between two points is calculated as a sum of the absolute differences of their coordinates) the Chebyshev distance [2] (which is a metric defined on a vector space where distance between two vectors is the greatest of their differences among their entries) and the Minkowski distance [14] (which is a generalization of the above-mentioned metrics). The problem of determining the role of the distance function for the DANIOP procedure as well as of learning the distance metric based on th e input data (which is treated in literature e.g. [7,9, 10]) is beyond the scope of the present paper and is left to future investigations. 3. The DANIOP procedure
Let us consider a dataset with P data that is used to train a model where N input variables are exploited to predict a single output value. By representing the whole database in the matrix formulation, the input matrix X has P rows and N columns, while the target values are stored in a column vector T with P to predict is associated to the input row vector X p .
 The aim of the proposed method is to point out the input-output couples that correspond to rows in X which are similar to each other, while the corresponding entries in T are very different; they are the incoherent rows of the matrix.

Once these anomalous situations have been pointed out, if one of the two mutually incoherent patterns belongs also to another (or more than one) couple of incoherent data, only tath pattern can be considered anomalous with respect to the other ones and can be therefore eliminated from the database. Otherwise, i.e. if both the incoherent patterns do not belong to any other anomalous couple, the judgment of an expert of the field from which the data have been taken is fundamental for deciding on the elimination of one, both or none of them. The decision criteria strongly depend on the specific application and are out of the scope of the present work.

The proposed method consist of two main steps: 1. Computation of the input and output dissimilarity matrices starting from the entire dataset, as 2. Identification of the anomalous entries of the two matrices conputed in the previous step, by direct 3.1. Computation of the I/O dissimilarity matrices
The first step of the proposed method concerns the computation of the two dissimilarity matrices ,that allow to find anomalous data. Specifically, in order to compute the desired matrices, the following steps must be performed: 1. Normalization of the entries of X and T with respect to their own maximum values. A tranformed 2. Computation of the Euclidean distance between each pair of rows of  X  and between the entries
Due to the simmetry, all the following operations can be performed by only considering the matrix upper triangle. 3.2. Finding small-large entries
In the second step of the proposed method a comparison of the corresponding elements of the two dissimilarity matrices is performed, by considering that anomalies are pointed out when a  X  X mall X  entry  X 
Therefore, in this phase the definition of  X  X mall X  and  X  X arge X  entry is fundamental, because the de-tection of the anomalous pattern in the dataset heavily depends on such definition; furthermore it is important to emphasize that those values should be independent on the number of patterns and, possibly, on the particular dataset.
 lous, therefore the optimal value of these parameters can be found either with some objective technique (as discussed further) or in an empirical way, i.e. by running the method with different values of the thresholds until a reasonable amount of patters is identified.
 Let us define two proportionality factors k I and k O : the threshold value  X  I to which the entries of D the entries of D O can be fixed as a fraction k O of the range [ t p min ,t p max ] of the target variable that corresponds to a significat difference between two output values (e.g. k O  X  [0 . 25 , 0 . 5] ),butitcanalso be an heuristic derived from the particular application and suggested by the experts in the specific field. It is possible to involve in the thresholding phase only the proportionality factor, k I and k O ;thisisa simplest method, but also the most critical one because it heavily depends to the available knowledge of the considered problem or application.

In order to determine a reasonable values for the two thresholds  X  I and  X  O other more accurate methods are proposed, that that are less related to the knowledge of the specific problem. They are:
Finally, in order to make the above mentioned thresholds indipendent on the number of patterns and on the particular dataset, a sort of averaging operation can be performed. Therefore the average of the above-defined three values is calculated as follows: This last solution will be referred in the following as meanTh . 3.3. A numerical example
Let us consider a dataset organised in rows, where the four columns of X matrix correspond to the inputs and the column vector T contains the associated output value:
Normalizing X and T with respect to their maximum values, the two following matrices  X  and  X  are computed: By computing the Euclidean distance, the following dissimilarity matrices D I and D O are obtained: in computing the threshold values. The performed trials and experimental tests showed that a suitable choice for the input proportionality factor is k I = 0.15. On the other hand, in the definition of the when similar input values correspond to a difference in the corresponding output values which are bigger than 1/3 of the whole range of the output space. This means to adopt the value k O = 0.33. Table 1 shows the threshold values that are obtained by applying the metodologies proposed in Section 3.2.
The final step of the proposed method is the comparison of the two dissimilarity matrices D I and D O by means of all the threshold values reported in Table 1. It is noticeable that applying any of the proposed threshold types, the null entry (1 , 5) of D I corresponds to high entries in D O . Analogously the entries with pattern No. 5 and that pattern No. 5 is also incompatible with pattern No. 4. Lastly, by considering that the fifth row emerges in both the detected anomalous couples (i.e. both pairs (1 , 5) and (4 , 5) ), it becomes evident that pattern No. 5 must be considered as an anomaly in the dataset that comes from a wrong registration during the process of industrial data collection. It is also noteworthy that, in this particular example, by applying only the proportionality factor k I = 0.15 and k O = 0.33, the same results are achieved.
 3.4. Metric as an alternative approach
An alternative approach, that avoid the time consuming comparison between the two dissimilarity matrices, is based on a  X  X etric X  which identifies the entries of the dissimilarity matrices which are, computed, where the entries at row p and column q is as follows: in which the k parameter is a constant that can be used to optimize the performance of the metric. element of D O is large. Therefore, the largest entries in M correspond to potentially anomalous patterns. Thus this alternative approach allows to reach the same results of the method proposed in Section 3 with the advantage that the comparison between the corresponding entries of the two dissimilarity matices is replaced by the simpler search for the largest entries of the matrix M . 3.5. Effectiveness of the metric
In this section, the above-proposed metric is applied to the two dissimilarity matrices D I and D O that have been computed in Section 3.3. By assuming k = 10, the following matrix M is obtained: It is noticeable that M contains two elements that have higher values with respect to the other ones; also in this case, such as in Section 3.3, the couples of pattern which are pointed out by the metric are the pairs (1 , 5) and (4 , 5) , which confirms that the corresponding rows are not compatible each other. 4. Application to a synthetic dataset
In order to test the effectiveness of the DANIOP procedure a synthetic dataset where part of the input data have been noised has been created. A linear regression model has been used for putting into relation inputs and output of the original datasets and of the dataset preprocessed by means of the DANIOP procedure in order to put into evidence the eventual improvements.

The synthetic dataset, composed by about 250 patterns, has been built up as follows:  X  Each input data x =[ x 1 ,x 2 ,x 3 ,x 4 ] T  X  4 has been randomly selected so as each feature lies in  X  The output feature y has been produced by applying, on the precomputed input data, the following
Afterwards 20 patterns from the original dataset have been selected and each feature of them has been modified by randomly adding or subtracting the 5% of its value while the respective output value has been modified with a value at least bigger than 1/3 of the whole range of the output space.
In order to evaluate the goodness of the linear regression model, the performance index R 2 have been used, which provides a measure of how well future outcomes are likely to be predicted by the model. R 2 (which ranges from 0 to 1) is the square of the sample correlation coefficient between the outcomes and the values of the single regressor being used for prediction.
 A linear regression model has been applied on the noised synthetic dataset, obtaining a value of the R 2 statistic of about 0.89.

Applying the DANIOP procedure on the noised dataset, 2 groups and 15 couples of incoherent input-output patterns have been detected. Therefore the total number of eliminated patterns was equal to 32. In detail the 2 patterns which differ in the detected groups and 30 patterns from the 15 detected couples have been deleted. In this latter case, 15 of the 30 deleted patterns were coherent input-output patterns, and their elimination have not caused a performance degradation considering their small numerousness respect to the rest of the dataset. By applying the DANIOP procedure, the obtained value of the R 2 statistic is about 0.99 with an increase of the 11.3% respect to the case in which the noised dataset is considered. 5. Application to a real industrial database
During the manufacturing of particular steel products, slabs are processed to be turned into steel coils which can be put into the market. This working phase is called  X  X ot rolling X  as the slabs are firstly heated in special furnaces and subsequently rolled in the hot strip mill (HSM) and finally coiled. As depicted in Fig. 2 the HSM is formed by a series of stands which consist on couples of rolls. During the passage from these stands the slab gradually acquires the desired shape through a reduction of its thickness and the achievement of the desired width. One of the criticalities which can be encountered during this process is the possible vibration of the stands during the passage of the slab which can cause a number of defects on the manufactured final product and damages to the HSM structure. Vibrations, which can be observed at different intensities, are mainly related both to machine operating parameters (speeds and rolling forces) and product characteristics, but the details of this relation are not clear and it is not possible to predict the occurrence and the intensity of vibrations during the hot rolling of a slab. Such prediction would be very useful during the manufacturing as it would allow a suitable tuning of the plant for the rolling of different kinds of steels in order to avoid the vibrations. For this reason a model based on the use of neural networks has been exploited to perform this prediction. The developed system aims at predicting the intensity of the vibrations on each stand of the mill on the basis of four input variables, that have been selected with the support of plant engineers. The procedure for such selection is out of the scope of the present paper as well as the theoret ical fundations for assessing the rep eteability of the process, i.e. for justifying the assumption that a functional relationship holds between the selected input variables and the target property to predict. This informations, once available, could guide the producers to suitably change process parameters and products characteristics in order to improve the manufacturing process and the produc t quality.

In order to evaluate the performace of the neural network model, before and after the application of the DANIOP procedure, two classical performance indexes have been used:  X  Mean Squared Error (MSE) , i.e. the average of the square of the difference between the predicted  X  Relative error ( r ) , namely the mean of the absolute value of the error between predicted and actual
In order to assess the statistical significance of the obtained results, in terms of MSE and r ,atest strategy has been defined following the suggestion found in [22].

It basically consist in the use of the k -fold cross validation [17], in which the original dataset is randomly subdivided into k subsamples and then a single subsample is used as validation data for testing the neural network model, and the remaining k-1 subsamples are used as training data. This process is repeated k times (folds), with each of the k subsamples used once in the validation phase. Lastly the k results from the folds have been averaged in order to produce a single estimation of the two above mentioned performace indexes. A value of 10 has been chosen for k , which means that for each fold the 90% of the data are used as training set, while the remaining 10% are used as validation set.
Furthermore, the process of 10 -fold cross validation has been repeated 10 times (runs), with the aim to obtain a sample composed by 10 values each of one is the mean value of the two above mentioned performance indexes, allowing in this way to perform a final statistical assessment of the performace of the neural network model before and after the application of the DANIOP procedure on the available dataset.

As preliminary and mandatory step each feature of the dataset, composed by about 630 patterns, has been normalized respect to its maximum value, then the above mentioned procedure has been firstly applied on the fully available dataset with the detailed results for each of the ten runs depicted in Table 2 with a mean value of the MSE and r respectively of 0.016 and 0.398.

As mentioned above, several problems can occurr in the registration of some input variables as well as in the final measurement of the property to predict, mainly due to sensor failures. The technical personnel working on the plant in some of the cases is capable to recognize the incoherent patterns and sometimes also to find out the error sources and possible causes, but, due to the high rithm of the production and to the fact that the database is collected in an automatic way, such analysis cannot be performed on line. On the other hand, due to the considerable number of data that are rapidely collected in the database, also the off-line analysis cannot be manually performed. In order to select only reliable data for the design of the system to develop, as well as to point out eventual systematic errors in the data registration, that might affect both the production control and the final system evaluation, a software needed to be developed which provides support to the technical personnel by pointing out incoherent input-output patterns.
As depicted in Fig. 3, the step of the identification of the anomalous pattern has been applied only to the training set (90%) for each fold in the 10 -fold cross validation, leaving unchanged the validation sets (10%).

The average values of the anomalous input-output patterns detected in each of the 10 runs of the 10 -fold cross validation, depicted in Table 3, show that 48 anomalous input-ouput patterns are detected. Considering it in detail, 24 of the total number are the pattern which are in contrast with the others in each of the 24 detected groups, while the others 24 are both the patterns that compose each of 12 detected incoherent couples.

After the application of the DANIOP procedure for each fold in the 10 -fold cross validation, the neural network has been trained on the reduced training set i.e. the training set which has been obtained by deleting the all the anomalous patterns detected. Due to the lack of more specific and application-focused criteria, the following reasoning for the data elimination has been followed: when a group of incoherent observation is detected and only one pattern shows a target value very different from the remaining ones in the group, it can be inferred that such incoherent observation must be discarded. On the other hand, when only two observations are pointed out, which are incoherent each other, both the observations need to be discarded.

In Table 4 the averaged results of the execution of the 10 runs of the 10 -fold cross validation are depicted, obtaining a mean value for MSE and r respectively of 0.007 and 0.304.

Comparing the values of both two considered performance indexes is possible to notice that in the case of the application of the DANIOP procedure to the training data their mean values are better than that in the case in which the entire dataset was used, as summarized in Table 5, obtaining an averaged decrease of the MSE and r of about 56% and 23% respectively. 5.1. Statistical significance assessment of the obtained results
In order to asses the statistical significance of the above reported results some statistical tests have been performed.

It is noticeable that the results of the application of the neural network on the entire dataset and on the one resulting from the application of the DANIOP method, presented in Tables 2 and 4, report the average performance over 10 runs of the 10 -fold cross validation. All the tests performed below have been performed by assuming a critical significance value of  X  = 0.05 In order to identify the suitable statistical test to perform, the first step is the application of the Kolmogorov-Smirnov test [11] with the aim of checking the normality of the available data. The results reported in Table 6 show that the observed data ( MSE and r before and after the application of the DANIOP procedure) are not normally distributed (with a p -value lower than the critical value  X  ). Therefore non-parametric tests have been used in the following assessment (more in detail the pairwise Wilcoxon test [16]) in order to evaluate if there is statistical significance within the obtained results.

It is useful to remark that the null hypothesis of the Wilcoxon test is that two samples have identical distribution functions against the alternative hypothesis that the two distribution functions differ only with respect to location (median), if at all. Hence the pair wise Wilcoxon test has been applied on both the results of the MSE and r coming from the execution of the neural network model before and after the application of DANIOP procedure.

As shown by the results in Table 7, the p -value lower than the critical value  X  confirms the presence of differences in the observed results, meaning that better results are achieved in the case of the application of the neural network model on the data filtered by means of the DANIOP procedure. A performance improvement is put into evidence by the decrease of both the above-defined performance indexes; in particular the MSE average decreases of about 56.2% while the r of about 23.6%.

It is remarkable that the DANIOP procedure has been applied for each fold of the 10 -fold cross val-idation only on the training set, leaving unchanged the validation set. This means that the increased performance of the neural network model on the validation set is a consequence of the fact that the model is actually more representative of the phenomenon that is modeled through the neural network.
It must be underlined that in the proposed application no further filtering stage has been applied to the dataset as this is the specification for the systems. Obviously, some more sophisticated criteria for data selection could also be elaborated but they are not the object of the present investigation. As exposed in Section 3 in order to be more stringent in other application fields, when the opinion of the domain experts is available, it is possible to choose to delete one of the two incoherent patterns according to the expert X  X  experience. 6. DANIOP software overview A software aimed at pointing out anomalous input-output patterns has been developed in the Visual C++ programming environment by exploiting the standard MFC library. It is composed by a main win-dow (see Fig. 4) on which it is possible to import a database and to select the inputs and the output variables on which to apply the method proposed in the present paper. After the execution, the anoma-lous patterns are reported in different colours that help the user to understand the obtained results. In detail, a couple of yellow line is presented when a group composed by only two incoherent observations is poitend out, indicating the fact that it cannot be argued which of the two observations is correct. Oth-erwise a group of registration, in wich the first one is reported in red while the other ones are in green indicates that the red row is in contrast with all the green ones in the same group, thus the red row is probably wrong. It is also possible to save the anomalous input-output pattern and to save the dataset by elimination all the lines which are considered ambiguous (i.e. the ones that are reported in red and in yellow), which makes the dataset ready for a further modelling step. 7. Conclusion
This paper presents a novel metodology for the detection of anomalous data registrations, which is expecially suited to databases coming from real industrial scenario. The proposed DANIOP technique is able to find incoherent input-output patterns, i.e. groups of data where similar input patterns are as-sociated to very different output values or vectors, that in most case are present due to errors occurring during the phase of the data collection on the process line.

Since frequently in real-world applications the data quality is a function of the same input variables that are fed to the model itself, the step of identifying and removing the anomalous data registrations is a necessary requisite in the phase of the preparation of data for function approximation problems, classification and clustering, because these kind of operations associate input patterns to a desired output value. Therefore it is important to detect and delete the wrong registrations in the preprocessing phase, in order to make subsequent phase of model or classifier development successful and meaningful.
When designing a neural network-based model for reproducing an input-output mapping, the goodness and robustness of the obtained prediction is strictly related to the quality of the data that are exploited in the learning procedure: if such data are heavily affected by noise and other kinds of errors, the model performance can be quite poor. In order to show the performance of the DANIOP procedure, a real industrial database coming from steelmaking industry has been exploited, which has been collected to the aim of building a neural model capable to predict an output value starting from four different inputs. The obtained results, statistically assessed, show that the prediction is far more accurate after the application of the proposed method in the preprocessing phase.

A user friendly software has also been implemented, that allows to detect and visualize the anomalous patterns, in order to support the designer as well as the technical personnel eventually involved in the data collection stage during the fundamental operation of dataset pre-filtering.

Future work will deal with an investigation on the role of the distance function within the proposed procedure. To this aim different distance functions, such as the Mahalanobis, Manhattan, Chebyshev and Minkowski distances, will be implemented and tested and the obtained results will be analysed and compared. Furthermore some metric learning methods will be tested, in order to choose of the parameters which determine a thresholds on input-output dissimilarity.
 References
