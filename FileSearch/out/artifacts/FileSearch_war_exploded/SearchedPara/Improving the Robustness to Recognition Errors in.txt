 of the appliances, it is almost impossible for the user to take advantage of every avail-able feature. Manuals that accompany a product are supposed to contain all the infor-mation necessary for the user. However, in reality, customer inquiries at a call center are often to do with a specific passage within a manual. That is, reading the manuals does not always solve the user X  X  problem at hand. These problems will probably be-come serious with the advent of more and more complex digital products. 
In order to tackle the above problems, we previously reported on a prototype help system for home appliances that combines multimodal knowledge search and textual question answering [17]. The multimodal knowledge search subsystem retrieves text-annotated video contents for dealing with the user X  X  HOW questions, while the question answering subsystem returns a short, exact answer in response to factoid questions (Fig. 1). Our prototype help system allows speech input so that the user can consult the system in any situation, e.g., when his hands are occupied. 
However, this meant that the retrieval effectiveness of the system depended heavily on speech recognition accuracy. Our initial system seriously suffered from recogni-tion errors especially for factoid questions, because recognition errors of interrogative phrases such as  X  X ow many grams X  and  X  X ow many meters X  were fatal for question classification. 
The objective of this paper is to make the above question answering subsystem more robust to speech recognition errors. To this end, we complement the speech recognition transcript with interrogative phrases that help the system classify ques-by computing the edit-distance between each list entry and each substring of the tran-script, and also by examining within-document co-occurrences between terms and interrogative phrases. 
The remainder of this paper is organized as follows. Section 2 discusses related works. Section 3 provides an overview of our multimodal help system, and Section 4 method to overcome the above problem and Section 6 reports on an evaluation ex-periment. Finally, Section 7 concludes this paper. Question Answering (QA) technology is attracting worldwide attention, as exempli-fied by the English TREC QA Track [18], the European QA@CLEF Track [10], and the Japanese NTCIR QAC Track [4]. These efforts not only tackle the problem of question series/context as well as definitional questions. A typical QA system consists of at least three modules: Question classification (or answer type identification), named entity recognition for tagging candidate answer strings within the knowledge source documents, and document/passage retrieval for selecting documents that are likely to contain a good answer to the given question. 
While the aforementioned efforts focus on textual output in response to a textual input, some researches have pursued speech input information access techniques. Barnett et al. [1] performed comparative experiments related to search-driven re-trieval, where an existing speech recognition system was used as an input interface for the INQUERY text retrieval system. They used as test inputs 35 queries col-lected from the TREC 101-135 topics, dictated by a single male speaker. Crestani et al. [2] also used the above 35 queries and showed that conventional relevance feed-back techniques marginally improved the accuracy for speech-driven text retrieval. These studies focused solely on improving text retrieval methods and did not ad-dress problems of improving speech recognition accuracy, and speech recognition and text retrieval modules were fundamentally independent and were simply con-nected by way of an input/output protocol. However, when speech input is used, it is known that retrieval performance seriously suffers from speech recognition errors. 
Nishizaki et al. [13] developed a speech input retrieval system for spoken docu-ments of news, for which they alleviated the effect of recognition errors by consider-ing alternative keyword candidates in addition to the best candidate obtained through on a voice-activated question answering system for large-scale corpora, such as a newspaper articles. In this research, they performed syntactic analysis of the question vocabulary in the target documents for handling unknown words in a spoken query matically complements an interrogative expression to the query transcript for improv-ing the question classification accuracy in question answering. 
Kiyota et al. [7] developed an interactive, speech input online help system for Mi-crosoft Windows, which can answer questions in natural language and disambiguate questions through a dialogue. Their main target is  X  X sking back to disambiguate a question X . When a user X  X  query contains ambiguity, the system generates a query misunderstandings caused by speech recognition errors, asking back is applied. The system asks back to the user the part which carried out erroneous recognition. And the system finds erroneous recognition by calcula ting relevance score based on perplex-ity. Moreover, the system calculates significance for retrieval using N-best candidates What/How/Symptom questions. In contrast, while our system currently does not have a dialogue/interactive feature, it can handle both HOW questions and factoid ques-tions. Moreover, our system is for home appliance. The configuration of our prototype help system is shown in Fig. 2. As shown, the tem X  X  outward appearance is shown in Fig. 3. The help interface recognizes a speech-input query and either outputs an exact answer through speech synthesis, or retrieves and displays a video content. The help manager is responsible for classifying ques-tions and for calling either the video search engine or the question answering engine (Table 1). The help manager extracts interrogative phrases of question by a rule base. When a query contains an interrogative phrase which asks a number (time, quantity, etc), a name or place of a control unit, the query is classified as a factoid question, and the other queries are classified as HOW question. For a factoid question, if the inter-rogative phrase is misrecognized, the query is classified as HOW question and the system cannot reply to the query correctly.
 question type search engine Example 
Factoid question answering search engine 
HOW video search engine
The video search engine searches a database comprising text-annotated virtual streams that have been created by extracting and concatenating relevant segments from video, audio and image files using MPEG7 [12]. It adopts the vector space model for information retrieval, and retrieves three items in response to a HOW ques-interface as shown in Fig. 4, so that the user can start playing it. For the second and the third candidates, only their titles are shown at the bottom of the window [8]. 
The question answering engine extracts answer candidates such as quantity, the names of the switches on the control panel of the appliance, time expressions and so on off-line. It stores these candidates together with the source documents i.e., supporting documents for the answer strings) [15] [6]. The help interface outputs three answer candidates in decreasing order of answer confidence values, and utters LaLaVoice2001 [9] for continuous speech recognition for query input and speech synthesis for answer output. LaLaVoice2001 is a large vocabulary continuous speech recognition and speech synthesis system developed by Toshiba. It recognizes input voice based on the statistical language model which estimates the linguistic validity of a sentence and the acoustic model expressing the sound feature of the voice [11] [14]. It is for speaker independent speech recognition and voice enrollment is not required. We first conducted an experiment to investigate the effect on speech recognition er-rors on the performance of our initial prototype system. The experiment involved ten subjects and 96 real questions collected from a Toshiba call center, yielding 960 trials altogether. We tried to make the number of factoid questions and that of HOW ques-tions roughly equal, and as a result the question set actually contained 40 factoid and 56 HOW questions. Moreover, the questions were carefully selected so that the aver-age system performances for the factoid and the HOW questions were comparable, namely, around 0.7 in terms of Mean Reciprocal Rank (MRR 1 ). 
We used a microwave oven manual and a digital camera manual as the target documents. Our goal is to develop an integrated help system for multiple home appli-treated these two manuals jointly.

We turned our speech recognition dictionary for the target documents beforehand, using the automatic word registration/learning feature of LaLaVoice2001. In this training, if the models were trained with documents which include many factoid ques-tions, the recognition performance will improve. However, it is costly to prepare such an external corpus. We therefore used only the target manuals for training. After this tuning, the accuracy of our speech recognizer, defined below, was 0.788: questions and their misrecognized transcripts are given in Table 2. 
For both factoid question set and the HOW question set, we compared the MRR of the top three candidate answers returned by the system for each question. Table 3 summarizes the results. HOW question, the correct answer is either a relevant video content or a page from a manual. The returned answer was counted as correct only if question classification was successful and the most appropriate answer medium was selected. For example, when a factoid question was misclassified as a HOW question, and a retrieved video content or a manual page actually contained a correct exact answer string somewhere in it, we regarded this system output as a failure. 
From the table, we can observe that the performance degradation for the factoid question set is much more serious than that for the HOW question set. The HOW question set does not suffer very much from speech recognition errors because the video search engine is a simple bag-of-words system based on the vector space model. In general, there are several terms in a bag-of words query, so misrecognizing engine, which we call ASKMi [15], is less robust to speech recognition errors because of its intricate mechanisms. The system consists of question classification, document retrieval and named entity recognition modules, and the question classification mod-type/answer type classification can fail completely. 
For both keyboard input and speech input, we investigated the performance of each subcomponent of ASKMi, and the results are shown in Table 4.

The table shows that the failure of question type identification has a substantial negative effect on the overall system performance. As mentioned earlier, this is be-question type identification, are often misrecognized by speech recognition. It can also be observed that, once the question type has been determined successfully, an-swer type identification is highly successful for both keyboard input and speech input, because this process also relies on interrogative phrases. 
On the other hand, Column (3) of Table 4 shows that the  X  X RR ratio X  for the cause, while answer ranking is directly affected by answer type identification, docu-ment retrieval is fairly robust to speech recognition errors in the input question since it relies on query terms rather than interrogative phrases. 
Speech recognition errors occur frequently for interrogative phrases than for query terms. This is probably because, while the initial system tuning improves the recogni-accuracy for interrogative phrases, since interrogative phrases themselves seldom occur in the manual texts. That is, while expressions such as  X 100 grams X  may occur in the texts,  X  X ow many grams X  probably never occurs, so speech recognition never  X  X earns X  the interrogative phrases. Our analysis in the previous section showed that the speech recognition errors of interrogative phrases are the main cause of the final performance degradation. We therefore devised a method to alleviate this problem under the assumption that mis-tried to restore the interrogative phrases lost in speech recognition as follows: An example of this procedure is shown in Fig. 6. 
Here, the correct interrogative phrase in the question " (how many pix-els)" has been misrecognized as " (south pixels)", and as a result the ques-narrows down the list of interrogative phrases to those which co-occur with query terms  X  X ideo X ,  X  X mage and  X  X ize X , based on a co-occurrence table of interrogative phrases and terms. Then, it computes the edit distance between each candidate inter-interrogative phrase  X  X ow many pixels X . In this particular case, the edit distance is zero since  X  X ow many pixels X  and  X  X outh pixels X  are phonetically identical in Japanese. Table 5 shows a nonexhaustive list of our interrogative phrases which we used Step 1. We obtained this list by automatically extractive phrases from ASKMi X  X  pat-contains 113 interrogative phrases. The above mentioned co-occurrence table was created in advance as follows: 
Step 2-1 From the target documents, extract all answer candidates that may be used Step 2-2 For every answer candidate thus obtained, do Steps 2-3 and 2-4; 
Step 2-3 Select an interrogative phrase that may be used in a question to which the An example of the above procedure is shown in Fig. 7. 
For this example, Step 2-1 extracts the st ring  X 100 g X  as an answer candidate, which consists of a number and a unit. For this answer, Step 2-3 selects  X  (how many grams) X  from the list of possible interrogative phrases. Then, Step 2-4 obtains a text snippet " 100g (the quantity of together with the interrogative phrase " (how many grams)". Table 6 shows a part of our co-occurrence table thus created. As mentioned earlier, the table contains 113 entries, and the maximum number of terms associated with an interrogative phrase is 49. 
As mentioned earlier, the edit distance computation is computed after narrowing down the list of interrogative phrases based on co-occurrences with query terms. This rithm, interrogative phrases are often very short, e.g., " (NANDO: how many times)" so the edit distance computation between question substrings the phrases are generally time-consuming. Secondly, our method has a positive effect on the accuracy of phrase recovery, as it can filter out homonyms (e.g., " (NANDO: what de-grees)" as opposed to " (NANDO: how many times)") from the list based on the vocabulary in the co-occurrence table. As for interrogative expressions that do not contain specific units (e.g., " (how long)", " (how many)", we the question terms. 
As mentioned earlier, Step 3 computes the edit distance between the pronunciation substring of the query transcript. Since raw edit distances would favor short phrases, we normalize them as follows: maji characters. Using the experimental environment we described in Section 3, we compared the final MRR performance with and without the question complementation using our pro-posed algorithm. Table 7 compares the final MRR performances of the error-free, keyboard input system, the speech input system without complementation, and that with complementation. The results from Tables 3 and 7 are visualized in Fig. 8. It can be observed that our question complementation method successfully improved the final MRR performance from 0.429 to 0.597.

We also investigated the performance of each question answering component, and the results are shown in Table 8, which subsumes the results we already discussed in Table 4. 
This table also shows that the accuracy of question classification has improved sig-nificantly by question complementation. Thus we successfully raised the performance level for factoid questions so that it is comparable to that for HOW questions. 
We finally conducted a failure analysis of the proposed question complementation method. Of 152 questions which originally suffered from speech recognition errors, as many as 80 questions were not improved by our method. We have classified the fail-ures as follows.  X  For 35 queries, 
Our system did not complement an interrogative phrase because the interrogative phrase of question was recognized correctly and the question was classified as a factoid question correctly. Nevertheless, the system could not search a correct an-swer because other query terms were misrecognized and supporting documents were searched incorrectly.  X  For 28 queries, 
Our method failed to complement an interrogative phrase due to a misrecognized query term.  X  For 8 queries,  X  For 9 queries, Our method added an incorrect interrogative phrase to the question. Here, misrecognised query terms are in bold italics, original interrogative phrases are underlined with a straight solid line, and the complemented interrogative phrases are underlined with a wavy dotted line. This paper proposed and evaluated a method for complementing a speech input query for improving the robustness of our help system for home appliances to speech recog-nition errors. To enhance the chance of success at the question type identification step for factoid questions, our method recovers an interrogative phrase based on co-occurrences between answer candidates and terms in the target documents, and adds it to the query transcript. Prior to introducing this method, the accuracy of question type identification via speech input was 70.0% of that for the keyboard input case, but the method raised this number to 92.3%. 
Through our experiments and analyses, it became clear to us that a simple combi-nation of question answering and speech recognition does not achieve an expected level of performance, namely, the product of the performances of individual modules. Methods such as the one we proposed are necessary for the system to enjoy a synergy effect. Our future work includes feeding back the results of our question complemen-tation to the speech recognition module, and incorporating spoken dialogues for over-coming the negative effects of speech recognition errors. We would like to combine system performance. 
