 The REAP tutoring system (Heilman, et al. 2006), aims to provide authentic reading materials of the appropriate difficulty level, in terms of both vo-cabulary and grammar, for English as a Second Language students. An automatic measure of read-ability that incorporated both lexical and gram-matical features was thus needed. learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student X  X  lexicon, grammatical sur-face features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). It was shown by Collins-Thompson and Callan (2004) that a vocabulary-based language modeling approach was effective at predicting the readability of grades 1 to 12 of Web documents of varying length, even with high levels of noise. Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases. This work combining grammatical and lexical features was promising, but it was not clear to what extent the grammatical features improved predictions. that a more detailed grammatical analysis of texts that examines features such as passive voice and various verb tenses can provide better features with which to predict reading difficulty. One goal of this work is to show that the use of pedagogically motivated grammatical features (e.g., passive voice, rather than the number of words per sen-tence) can improve readability measures based on lexical features alone. ability is the timeline and processes by which first and second languages are acquired. First language acquisition begins at infancy, and the primary grammatical structures of the target language are acquired by age four in typically developing chil-dren (Bates, 2003). That is, most grammar is ac-quired prior to the beginning of a child X  X  formal education. Therefore, most grammatical features seen at high reading levels such as high school are present with similar frequencies at low reading levels such as grades 1-3 that correspond to ele-mentary school-age children. It should be noted that sentence length is one grammar-related differ-ence that can be observed as L1 reading level in-creases. Sentences are kept short in texts for low L1 reading levels in order to reduce the cognitive load on child readers. The average sentence length of texts increases with the age and reading level of the intended audience. This phenomenon has been utilized in early readability measures (Flesch, 1948). Vocabulary change, however, continues even into adulthood, and has been shown to be a more effective predictor of L1 readability than simpler measures such as sentence length (Collins-Thompson and Callan, 2005). terparts, are still very much in the process of ac-quiring the grammar of their target language. In fact, even intermediate and advanced students of second languages, who correspond to higher L2 reading levels, often struggle with the grammatical structures of their target language. This phenome-non suggests that grammatical features may play a more important role in predicting and measuring L2 readability. That is not to say, however, that vocabulary cannot be used to predict L2 reading levels. Second language learners are learning both vocabulary and grammar concurrently, and reading materials for this population are chosen or au-thored according to both lexical and grammatical complexity. Therefore, the authors predict that a readability measure for texts intended for second language learners that incorporates both grammati-cal and lexical features could clearly outperform a measure based on only one of these two types of features. guage modeling and grammar-based prediction systems. A description of the experiments follows that covers both the evaluation metrics and corpora used. Experimental results are presented, followed by a discussion of these results, and a summary of the conclusions of this work. Statistical language modeling exploits patterns of use in language. To build a statistical model of text, training examples are used to collect statistics such as word frequency and order. Each training example has a label that tells the model the  X  X rue X  category of the example. In this approach, one statistical model is built for each grade level to be predicted. several advantages over traditional readability formulas, which are usually based on linear regres-sion with two or three variables. First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). Second, language modeling provides a probability distribu-tion across all grade models, not just a single pre-diction. Third, language modeling provides more data on the relative difficulty of each word in the document. This might allow an application, for example, to provide more accurate vocabulary as-sistance. based on a variation of the multinomial Na X ve Bayes classifier. For a given text passage T , the semantic difficulty of T relative to a specific grade level G i is predicted by calculating the likelihood that the words of T were generated from a repre-sentative language model of G i . This likelihood is calculated for each of a number of language mod-els, corresponding to reading difficulty levels. The reading difficulty of the passage is then estimated as the grade level of the language model most likely to have generated the passage T . simple: they are based on unigrams and assume that the probability of a token is independent of the surrounding tokens. A unigram language model is simply defined by a list of types (words) and their individual probabilities. Although this is a weak model, it can be effectively trained from less la-beled data than more complex models, such as bi-gram or trigram models. Additionally, higher order n-gram models might capture grammatical as well as lexical differences. The relative contribu-tions of grammatical and lexical features were thus better distinguished by using unigram language models that more exclusively focus on lexical dif-ferences. tive model is assumed for a passage T , in which a hypothetical author generates the tokens of T by: These assumptions lead to the following expres-sion for the probability of T being generated by language model G i according to a multinomial dis-tribution: Next, according to Bayes X  Theorem: Substituting (1) into (2), taking logarithms, and simplifying produces: where V is the list of all types in the passage T , w is a type in V , and C( w ) is the number of tokens with type w in T . For simplicity, the factor R represents the contribution of the prior P( G i ), and S represents the contribution of the passage length | T |, given the grade level. the illustration: cation, but they can be easily included or excluded in the model as desired. The log C ( w )! term can also be ignored because it is constant across levels. Under these conditions, an extremely simple form for the grade likelihood remains. In order to find which model G i maximizes Equation (3), the model which G i that maximizes the following equation must be found: This is straightforward to compute: for each token in the passage T , the log probability of the token according to the language model of G i is calcu-lated. Summing the log probabilities of all tokens produces the overall likelihood of the passage, given the grade. The grade level with the maxi-mum likelihood is then chosen as the final read-ability level prediction. cated extension of this model, in which a sliding window is moved across the text, with a grade pre-diction being made for each window. This results in a distribution of grade predictions. The grade level corresponding to a given percentile of this distribution is chosen as the prediction for the en-tire document. The values used in these experi-ments for the percentile thresholds for L1 and L2 were chosen by accuracy on held-out data. The following sections describe the approach to predicting readability based on grammatical fea-tures. As with any classifier, two components are required to classify texts by their reading level: first, a definition for and method of identifying features; second, an algorithm for using these fea-tures to classify a given text. A third component, training data, is also necessary in this classification task. The corpus of materials used for training and testing is discussed in a subsequent section. 3.1 Features for Grammar-based Prediction L2 learners usually learn grammatical patterns ex-plicitly from grammar explanations in L2 text-books, unlike their L1 counterparts who learn them implicitly through natural interactions. Grammati-cal features would therefore seem to be an essential component of an automatic readability measure for L2 learners, who must actively acquire both the lexicon and grammar of their target language. on being able to automatically identify grammati-cal constructions in text. Doing so is a multi-step process that begins by syntactically parsing the document. The Stanford Parser (Klein and Man-ning, 2002) was used to produce constituent struc-ture trees. The choice of parser is not essential to the approach, although the accuracy of parsing does play a role in successful identification of cer-tain grammatical patterns. PCFG scores from the parser were also used to filter out some of the ill-formed text present in the test corpora. The default training set of Penn Treebank (Marcus et al. 1993) was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learn-ers might be used. Tgrep2 (Rohde, 2005), a tree structure searching tool, to identify instances of the target patterns. A Tgrep2 pattern defines dominance, sisterhood, precedence, and other relationships between nodes in the parse tree for a sentence. A pattern can also place constraints on the terminal symbols (e.g., words and punctuation), such that a pattern might require a form of the copula  X  X e X  to exist in a cer-tain position in the construction. An example of a TGrep2 search pattern for the progressive verb tense is the following: which a verb phrase (VP) dominates an auxiliary verb (whose symbol begins with VB) as well as another verb phrase, which in turn dominates a verb in gerund form (VBG). An example of a matching sentence is,  X  X he student was reading a book, X  shown in Figure 2. Figure 2: The parse tree for an example sentence that matches a pattern for progressive verb tense. A set of 22 relevant grammatical constructions were identified from grammar textbooks for three different ESL levels (Fuchs et al., 2005). These grammar textbooks had different authors and pub-lishers than the ones used in the evaluation corpora in order to minimize the chance of experimental results not generalizing beyond the specific materi-als employed in this study. The ESL levels corre-spond to the low-intermediate (hereafter, level 3), high-intermediate (level 4), and advanced (level 5) courses at the University of Pittsburgh X  X  English Language Institute. The constructions identified in these grammar textbooks were then implemented in the form of Tgrep2 patterns. Passive Voice 0.11 0.71 Past Participle 0.28 1.63 Perfect Tense 0.01 0.33 Relative Clause 0.54 0.60 Continuous 
Tense Modal 0.80 1.44 Table 1: The rates of occurrence per 100 words of a few of the features used by the grammar-based predictor. Rates are shown for the lowest (2) and highest (5) levels in the L2 corpus. calculated on a per word basis. A per-word rather than a per-sentence measure was chosen because a per-sentence measure would depend too greatly on sentence length, which also varies by level. It was also desirable to avoid having sentence length con-founded with other features. Table 1 shows that the rates of occurrence of certain constructions be-come more frequent as level increases. This sys-tematic variation across levels is the basis for the grammar-based readability predictions. of 12 grammatical features that could easily be identified without computationally intensive syn-tactic parsing. These features included sentence length, the various verb forms in English, includ-ing the present, progressive, past, perfect, continu-ous tenses, as well as part of speech labels for words. The goal of using a second feature set was to examine how dependent prediction quality was on a specific set of features, as well as to test the extent to which the output of syntactic parsing might improve prediction accuracy. 3.2 Algorithm for Grammatical Feature-A k-Nearest Neighbor (kNN) algorithm is used for classification based on the grammatical features described above. The kNN algorithm is an in-stance-based learning technique originally devel-oped by Cover and Hart (1967) by which a test instance is classified according to the classifica-tions of a given number (k) of training instances closest to it. Distance is defined in this work as the Euclidean distance of feature vectors. Mitchell (1997) provides more details on the kNN algo-rithm. This algorithm was chosen because it has been shown to be effective in text classification tasks when compared to other popular methods (Yang 1999). A k value of 12 was chosen because it provided the best performance on held-out data. a confidence measure with which kNN predictions can be combined with predictions from other clas-sifiers X  X n this case with predictions from the uni-gram language modeling-based approach described above. A confidence measure was important in this task because it provided a means with which to combine the grammar-based predictions with the predictions from the language modeling-based predictor while maintaining separate models for each type of feature. These separate models were maintained to better determine the relative contri-butions of grammatical and lexical features. ing the two approaches led to only minimal reduc-tions of prediction error, likely because predictions from the poorer performing grammar-based classi-fier were always given the same weight. However, with the confidence measures, predictions from the grammar-based classifier could be given more weight when the confidence measure was high, and less weight when the measure was low and the predictions were likely to be inaccurate. The case-dependent interpolation of prediction values al-lowed for the effective combination of language modeling-and grammar-based predictions. portion of the k most similar training examples, or nearest neighbors, that agree with the final label chosen for a given test document. For example, if seven of ten neighbors have the same label, then the confidence score will be 0.6. The interpolated readability prediction value is calculated as fol-lows: where L LM is the language model-based prediction, L
GR is the grammar-based prediction from the kNN algorithm, and C kNN is the confidence value for the kNN prediction. The language modeling approach is treated as a black box, but it would likely be beneficial to have confidence measures for it as well. This section describes the experiments used to test the hypothesis that grammar-based features can improve readability measures for English, espe-cially for second language texts. The measures and cross-validation setup are described. A de-scription of the evaluation corpora of labeled first and second language texts follows. 4.1 Experimental Setup Two measurements were used in evaluating the effectiveness of the reading level predictions. First, the correlation coefficient evaluated whether the trends of prediction values matched the trends for human-labeled texts. Second, the mean squared error of prediction values provided a measure of how correct each of the predictors was on average, penalizing more severe errors more heavily. Mean square error was used rather than simple accuracy (i.e., number correct divided by sample size) because the task of readability predic-tion is more akin to regression than classification. Evaluation measures such as accuracy, precision, and recall are thus less meaningful for readability prediction tasks because they do not capture the fact that an error of 4 levels is more costly than an error of a single level. The data was first split into ten sets. One set was used as held-out data for selecting the parameter k for the kNN algorithm and the percentile value for the language modeling predictor, and then the re-maining nine were used to evaluate the quality of predictions. Each of these nine was in turn se-lected as the test set, and the other eight were used as training data. 4.2 Corpora of Labeled Texts Two corpora of labeled texts were used in the evaluation. The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach. The 362 texts had been assigned L1 levels (1-12) by grade school teachers, and consisted of approximately 250,000 words. For more details on the L1 corpus, see (Collins-Thompson and Callan, 2005). rials (Adelson-Goldstein and Howard, 2004, for level 2; Ediger and Pavlik, 2000, for levels 3 and 4; Silberstein, 2002, for level 5) from a series of Eng-lish as a Second Language reading courses at the English Language Institute at the University of Pittsburgh. The four reading practice textbooks that constitute this corpus were from separate au-thors and publishers than the grammar textbooks used to select and define grammatical features. The reading textbooks in the corpus are used in courses intended for beginning (level 2) through advanced (level 5) students. The textbooks were scanned into electronic format, and divided into fifty roughly equally sized files. This second lan-guage corpus consisted of approximately 200,000 words. corpora were different, they share a number of characteristics. Their size was roughly equal. The documents in both were also fairly but not per-fectly evenly distributed across the levels. Both corpora also contained a significant amount of noise which made accurate prediction of reading level more challenging. The L1 corpus was from the Web, and therefore contained navigation menus, links, and the like. The texts in the L2 cor-pus also contained significant levels of noise due to the inclusion of directions preceding readings, ex-ercises and questions following readings, as well as labels on figures and charts. The scanned files were not hand-corrected in this study, in part to test that the measures are robust to noise, which is pre-sent in the Web documents for which the readabil-ity measures are employed in the REAP tutoring system. more significantly negatively affected by the noise in the two corpora because the features rely more on dependencies between different words in the text. For example, if a word happened to be part of an image caption rather than a well-formed sen-tence, the unigram language modeling approach would only be affected for that word, but the grammar-based approach might be affected for features spanning an entire clause or sentence. ond language corpora, the language modeling (LM) approach alone produced more accurate pre-dictions than the grammar-based approach alone. The mean squared error values (Table 2) were lower, and the correlation coefficients (Table 3) were higher for the LM predictor than the gram-mar-based predictor. based predictions are not as accurate as the vo-cabulary-based scores, they can be combined with vocabulary-based scores to produce more accurate interpolated scores. The interpolated predictions combined by using the kNN confidence measure were slightly and in most tests significantly more accurate in terms of mean squared error than the predictions from either single measure. Interpola-tion using the first set of grammatical features led to 7% and 22% reductions in mean squared error on the L1 and L2 corpora, respectively. These re-sults were verified using a one-tailed paired t-test of the squared error values of the predictions, and significance levels are indicated in Table 2. Table 2. Comparison of Mean Squared Error of predictions compared to human labels for different methods. Interpolated values are significantly bet-ter compared to language modeling predictions where indicated (* = p&lt;0.05, ** = p&lt;0.01). Table 3. Comparison of Correlation Coefficients of prediction values to human labels for different prediction methods. matical features. However, the first set of features that included complex syntactic constructs led to better performance than the second set, which in-cluded only verb tenses, part of speech labels, and sentence length. Therefore, when syntactic parsing is not feasible because of corpora size, it seems that grammatical features requiring only part-of-speech tagging and word counts may still improve readability predictions. This is practically impor-tant because parsing can be too computationally intensive for large corpora. terms of correlations, on the L2 corpus than on the L1 corpus. The L2 corpus is somewhat smaller in size and should, if only on the basis of training ma-terial available to the prediction algorithms, actu-ally be more difficult to predict than the L1 corpus. To ensure that the range of levels was not causing the four-level L2 corpus to have higher predictions than the twelve-level L1 corpus, the L1 corpus was also divided into four bins (grades 1-3, 4-6, 7-9, 10-12). The accuracy of predictions for the binned version of the L1 corpus was not substantially dif-ferent than for the 12-level version. In the experimental tests, the LM approach was more effective for measuring both L1 and L2 read-ability. There are several potential causes of this effect. First, the language modeling approach can utilize all the words as they appear in the text as features, while the grammatical features were cho-sen and defined manually. As a result, the LM approach can make measurements on a text for as many features as there are words in its lexicon. Additionally, the noise present in the corpora likely affected the grammar-based approach dispropor-tionately more because that method relies on accu-rate parsing of relationships between words. poverished language compared to most languages. Text classification, information retrieval, and many other human language technology tasks can be ac-complished for English without accounting for grammatical features such as morphological inflec-tions. For example, an information retrieval sys-tem can perform reasonably well in English without performing stemming, which does not greatly increase performance except when queries and documents are short (Krovetz, 1993). ogy by which a single root form may have thou-sands or perhaps millions of inflected or derived forms. Language technologies must account for morphological features in such languages or the vocabulary grows so large that it becomes unman-ageable. Lee (2004), for example, showed that morphological analysis can improve the quality of statistical machine translation for Arabic. Thus it seems that grammatical features could contribute even more to measures of readability for texts in other languages. pears to play a more important role in readability measures for L2 than for L1. When interpolated with grammar-based scores, the reduction of mean squared error over the language modeling approach for L1 was only 7%, while for L2 the reduction or squared error was 22%. An evaluation on corpora with less noise would likely bring out these differ-ences further and show grammar to be an even more important factor in second language readabil-ity. This result is consistent with the fact that sec-ond language learners are still in the process of acquiring the basic grammatical constructs of their target language. The results of this work suggest that grammatical features can play a role in predicting reading diffi-culty levels for both first and second language texts in English. Although a vocabulary-based language modeling approach outperformed the grammar-based predictor, an interpolated measure using confidence scores for the grammar-based predic-tions showed improvement over both individual measures. Also, grammar appears to play a more important role in second language readability than in first language readability. Ongoing work aims to improve grammar-based readability by reducing noise in training data, automatically creating larger grammar feature sets, and applying more sophisti-cated modeling techniques. We would like to acknowledge Lori Levin for use-ful advice regarding grammatical constructions, as well as the anonymous reviewers for their sugges-tions. NSF grant IIS-0096139 and Dept. of Education grant R305G03123. Any opinions, findings, con-clusions or recommendations expressed in this ma-terial are the authors', and do not necessarily reflect those of the sponsors. 
