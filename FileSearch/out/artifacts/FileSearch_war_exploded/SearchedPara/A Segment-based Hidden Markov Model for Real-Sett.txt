 Hidden markov model (HMM) is frequently used for Pinyin-to-Chinese conversion. But it only captures the dependency with the preceding character. Higher order markov models can bring higher accuracy, but are computationally unaffordable to average PC settings. We propose a segmen t-based hidden markov model (SHMM), which has the same magnitude of complexity as first-order HMM, but generates higher decoding accuracy. SHMM tells a word from a bigram connecting two words, and assigns a reasonable probability to words as a whole. It is more powerful than HMM to decode words cont aining over two characters. We conduct a comprehensive Pinyin-to-Chinese conversion evaluation on Lancaster corpus. The experime nt shows the perfect sentence accuracy is improved from 34.7% (HMM) to 43.3% (SHMM). The one-error sentence accuracy is increased from 72.7% to 78.3%. Furthermore, SHMM can seamlessly integrate with pinyin typing correction, acronym pinyin input, user-defined words, and self-adaptive learning all of which are a must for a commercial Pinyin-to-Chinese conversion product in or der to improve the efficiency of pinyin input. I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  language models General Terms: Algorithm, Experimentation, Performance Keywords: Chinese Input, Pinyin, Segment-based Hidden Markov Model Chinese character is graph-based rather than letter-based so that it can not be input to computer directly. Transliteration input and structure-based input are two major approaches to Chinese input. The latter approach such as  X  X u Bi X  needs a long learning time and is used by a small number of PC users. The most basic implementation of transliteration input is based on Pinyin-to-Chinese conversion. There are 406 pinyin syllables corresponding to over 6,000 common Chinese characters. Therefore, it is low efficient to convert Pinyin to Chinese character one by one because one syllable can map to 15 characters on average. A more efficient solution is to input mo re Pinyin syllables one time and make the best use of character dependency to decode. Thus, the task of Pinyin-to-Chinese conversion is similar to speech recognition and can be solved by language modeling. for Pinyin-to-Chinese conversion. It achieves a good balance between conversion accuracy and conversion speed. For example, the pinyin  X  X hong guo X  can generate about 66 candidate bigrams, but HMM can easily get the correct conversion  X   X  X  because this bigram is much often than others in real texts. However, HMM only captures the dependency with the preceding character and does not work very well on converting trigrams and four-grams. Unfortunately, there ar e a large number of three-or four-character words, idioms a nd proper names in Chinese. This problem can be overcome by using a trigram language model or higher order HMM without complex ity and memory constraints. Unfortunately, most PCs are constrained by memory and unaffordable to such models which generate a large state space and need a large state transition matrix. as segment-based hidden markov model (SHMM) to solve the problem in this paper. The idea of SHMM is that we distinguish between Chinese words and bigrams connecting two words when computing the likelihood of a Chin ese text. We compile a Chinese lexicon and search a segment thr ough it. If the segment does exist, we treat it as the product of bigrams (i.e. the state change is markovian), otherwise assign it as a whole a probability according to another separate scheme (i.e. the state change within the segment is not markovian). On one hand, using two separate schemes, SHMM can easily handle three-or four-character words. On the other hand, a lexicon of about 200K words, proper names, and idioms ranging from two to four characters can cover almost all frequently used and popular words in various domains, while storing and searching such a small lexicon brings only a little extra workload to an average PC. Lancaster corpus [4]. The experiment shows that the perfect sentence accuracy is improved from 34.7% (HMM) to 43.3% (SHMM). The one-error sentence accuracy is increased from 72.7% to 78.3%. The character accuracy is improved from 82.9% to 86.1%. In addition, SHMM can seamlessly integrate with pinyin typing correction, acronym pinyin input, user-defined words, and self-adaptive learning all of which are a must for a commercial Pinyin-to-Chinese conversion product in order to improve the efficiency of pinyin input. Pinyin-to-Chinese conversion is getting very important as the number of Chinese users is increas ing at an exponential rate with the development of internet. Ma ny commercial products, such as  X  X icrosoft Pinyin X ,  X  X oogle Piny in X ,  X  X ogou Pinyin X ,  X  X hinese Star X ,  X  X iguang Pinyin X , and  X  X inyin Jiajia X , are available to Chinese PC users. Among these products, the first four support sentence level pinyin input and the last two only support phrase-level pinyin input. The state-of-the-art performance is about 10%~ 20% character level error rates [2]. Chinese conversion [1] [2]. In [1] [2], a unified trigram-based approach was proposed and the reported conversion accuracy reached 95%, much higher than the accuracy of commercial products. In order to make the system usable in real word, they also proposed a typing model which could correct common typing errors. However, they did not re port their evaluation settings and protocols clearly in the paper. however, can not capture long-range dependency as well as multiple interacting features. One solution to this problem is post-pruning. Zhang et al. proposed a rule-based post-pruning approach in [7]. They manually built a knowledge base which contained both syntactic and semantic rules. In their experiment, such a pruning technique can significantly improve the conversion rate. However, the testing sample was quite small. The reliability and effectiveness of this approach need further validation. approach to handle long-range de pendencies. ME can incorporate both local features such as preceding characters and long-range dependencies. Trigger pairs is used as long-range dependency features. The best ME model could reduce about 4% error compared to a trigram model in their experiment. In the paper, they presented the model of how to predict a single character according to the context, but did not show the details of how to efficiently search the whole optimal sequence. Thus, the complexity of this approach is unclear though the approach itself is very attractive. Given a sequence of pinyin syllables S , the task of Pinyin-to-Chinese conversion is to find a sequence of Chinese characters C , which maximizes the conditional probability p(C|S) . Using Bayesian law, we have: The first term p(S|C) is the typing (or pinyin) model and the second term p(C) is the language model [1]. of typing error and polyphone, it can be rewritten as: where S i and C i are the i -th pinyin and character respectively. The language model p(C) is the prior probability of C . Conceptually, one can try all possible C and select the one maximizing the joint probability p(S,C) . Practically, some strong assumptions such as first-order markov property are made in order to simplify the problem. With first-order markov assumption, one can use Viterbi algorithm to efficiently search the optimal C . In the task of Pinyin-to-Chines e conversion, a Chinese character corresponds to a state in HMM and pinyin is the observation. Suppose, N is the total number of Chinese characters in the dictionary and h i stands for the i -th character (state). S is the input pinyin and contains T syllables. S t denotes the pinyin observation at time t . Let  X  t (i) denote the best score (highest probability) for the first t pinyin observations ending with the character (state) h we have the recursive equation for HMM [5]: transitioned from the state (character) at time t-1. We relax this limitation in SHMM and allow non-markovian state transition within a time segment. After and before the segment, the state transition is still markovian. In othe r words, the first state after the segment depends on the last state of the segment and the first state of the segment depends on the last state of the preceding segment. (containing m characters) and ending with state (character) h define M as the maximum length of a segment. Then we have the recursive equation: Let us ignore the typing model and use an example to illustrate motivation behind SHMM. Suppose we need to compute the prior probability of a text containing four characters ABCD . In SHMM, a sequence of characters is always segmented into words before computing its prior probability. Given the text ABCD , there are different possible segmentations. For instance, AB / CD denotes this text is the connection of two two-character words. Basically, the outer loop controlled by the variable m (the length of the segment) in equation (3.4) tries to enum erate all possible segmentations. relatively simple. We compile a Chinese lexicon containing words, proper names, and idioms, and sort all entries by their pinyin. Given a pinyin segment observation S m, t+1 , we search it through the lexicon. If the segment w m,i exists, then ) | ( to one, else it has zero probability. segment is application specific. In order to avoid long n-grams which always generate a huge numbe r of parameters, we provide a light-weight implementation for Pi nyin-to-Chinese conversion. In general, given the word w beginning with character h s character of it preceding word h e , we have the following approximation: N(w) and N(h e ) denote the frequency of the word w and the character h e in the training corpus, respectively. From the equations (3.4) and (3.5), we can conclude that SHMM only need to maintain an extra list of words and their pinyin and frequency as well as some additional computation cost which is linear to the complexity of HMM. The additional computation cost is determined by the maximum segment length. For Chinese language, the maximum segment length can be set to four because there are very few words containing five or more characters. Therefore, the additional computation cost is really not much. In our testing (see Section 4.4), we find out the conversion speed for SHMM is almost same as for HMM because the bottleneck lies in the retrieving transition probabilities from the sparse matrix, which is same to both SHMM and HMM. better user experience, commercia l pinyin products often provide some extra features including pinyin typing correction, self-adaptive learning, acronym pinyin input, and user-defined words. Like other statistical language models, SHMM can easily integrate pinyin typing correction by adding pinyin correction rules to the typing model, and support self-ada ptive learning by interpolating the background bigram model with the user bigram model. SHMM is based on words rather than characters and thus can seamlessly integrate acronym pinyin input by adding pairs of pinyin acronym and word to the lexicon. Acronym pinyin input is an efficient way to input Chinese word by inputting the initial of pinyin instead of full pinyin. For instance, most commercial products can get  X   X   X  (China) by only typing  X  X  g X , which is the initial of the full pinyin  X  X hong guo X . However, acrony m input makes pinyin more ambiguous and comes out more candidates. Most commercial products including those based on HMM can not handle acronym pinyin in a unified model and thus can not utilize the context to filter out candidates. SHMM deals with acronym pinyin and full pinyin in a unified approach which makes SHMM more attractive than HMM to real Chinese input applications. pinyin into words and then model words and bigrams connecting two adjacent words in two different schemes. Such a separation makes SHMM overcome some problem s of the traditional bigram language models such as underes timation of three-character and four character words and overestima tion of some fake n-grams. In addition, SHMM can integrate acronym pinyin input in a unified approach. All these benefits are ac quired at the cost of a little extra computational cost and memory. SHMM needs a bigram language model and a list of words and their frequency learned from the training corpus. We collected diverse Chinese web pages from Internet and built a corpus containing about 500 million Chinese characters. We trained the bigram language model on this co rpus. We compile the Chinese lexicon from several sources. The main source is Xinhua Dictionary, which is an authoritative dictionary about Chinese characters and words and collect s about 120K words containing 2-4 characters. We also developed an algorithm to automatically collect from the training corpus new words which are not listed in Xinhua Dictionary. The number of extra words collected is about 80K. We employed a maximum ma tch algorithm [6] to count the frequency of Chinese words in the training corpus. We evaluate the Pinyin-to-Chinese conversion on the Lancaster corpus [4]. Lancaster is a Chin ese corpus with pinyin and POS annotated and word segmented. Th e documents in the corpus are collected from Internet and have a good diversity. We split texts into sentences using punctuations as delimiters. After excluding sentences containing less than four characters, or containing numbers or non-Chinese characters, there are 7,732 remaining sentences for testing. The s hortest sentences and longest sentences contain four characters and thirty-six characters, respectively. All testing senten ces have a total number of 78,622 characters and an average sentence contains about ten characters. For some polyphonic Chinese characters, the Lancaster corpus did not annotate their pinyin correctly. We correct them as much as possible. There is little formal study of Pinyin-to-Chinese decoding evaluation in literature, not alone sentence-level evaluation. For this reason, we propose three accuracy metrics in this paper. They are character accuracy , perfect sentence accuracy , and one-error sentence accuracy . The first metric, character accuracy, is very straightforward. It is the percentage of corrected Chinese characters in total decoded characters. The perfect sentence accuracy is defined as the percentage of corrected sentences in total sentences to decode. A sentence is corrected if all characters in a sentence are correctly decoded. We emphasize sentence level accuracy because it is time consuming for users to correct errors within a long sentence. One-error sentence accuracy is defined as the percentage of sentences containing up to one word-level error (zero or one error) in total sentences to decode. the experiment, we use Jelinek-M ercer [3] approach to smoothing the bigram language models. JM smoothing linearly interpolate the bigram model with the unigram language model. We empirically tune the mixture weight and find out that when the weight for bigram model is set to 0.9, both HMM and SHMM have best results. The comparisons of HMM and SHMM are shown in Table 1. SHMM significantly outpe rform HMM in terms of all three accuracy metrics. Especia lly, the perfect sentence accuracy and one-error sentence accuracy obtain considerable improvement, which will offer better experience to users because for long sentence conversion, it is time-cons uming for users to correct even one mis-decoded characters in the sentence. 
Table 1. The comparison of segment-based hidden markov model (SHMM) to standard hidden markov model (HMM) on 
Improvement 3.9% 24.8% 7.7% representative sentences which SHMM successfully converted but HMM failed. In the first example, the first four characters form an idiom in Chinese. In the second ex ample, the first three characters are the name of a famous communist party member in China. The first order HMM can only trace back to the preceded character and thus fails in both examples. SHMM searches the lexicon and finds out they are words rather independent characters and then assigns them a reasonable probability as a whole and eventually decode the pinyin. Compared to first order HMM, SHMM is quite good at decoding out three-character or four-character words. bigram. The bigram  X   X  X  X   X  (do not move) is also a part of two frequently occurred Chinese words  X   X  X  X  X   X  (real property) and  X   X  X  X  X  X  X   X  (hiding one X  X  emotion) . When the bigram is a part of some words or idioms, they do not function as a bridge connecting two words. But the usual bigram language model does not differentiate these cases. As long as two characters are next to each other, they will be counted as a bigram. Thus, the bigram is overestimated in the sense that a bigram should be used to connect two words. In SHMM, the counts of a bigram as a part of a word or idiom are removed. In the third example, we can see that the correct answer bubbles up after fi xing the overestimated bigram problem. Figure 1. Examples which SHMM correctly converted but About 3.5 million bigrams are learned from training corpus and stored in a sparse matrix. In the experiment, the bigram matrix is not fully loaded into memory, but caches up to 2,000 rows (corresponding to about 12M size of peak memory use). The lexicon contains about 200K Chin ese words, proper names, and idioms each of which ranges from two characters to four characters. The lexicon is loaded into memory and consumes about 6M size of memory. Plus other consumptions, the peak memory consumption for HMM and SHMM are about 14M and 20M, respectively. SHMM take a nother 6M size of memory compared to HMM. Java and the experiment is conducted on a Windows XP PC. SHMM takes about 436 seconds to convert all 7,732 sentences in Lancaster corpus while HMM takes about 383 seconds. Surprisingly, the actual running time for SHMM is quite close to HMM. Looking at equation (3.3) and (3.4), we can see that SHMM pays some extra computa tion cost for enumerating all possible segmentations. However, the bottleneck of decoding is not in this procedure, but in retrieving the bigram count from the sparse matrix, and therefore that extra computation cost does not make big difference on actual conversion time. In this paper, we proposed a novel segment-based hidden markov model (SHMM) and successfully applied it to the application of Pinyin-to-Chinese conversion. First-order HMM is frequently used to model Chinese language. Ho wever, it is not very effective in modeling Chinese words, prope r names, and idioms containing more than two characters. SHMM automatically segments a sequence of pinyin or characters into words, and then models words and bigrams connecting two adjacent words in two different schemes, and thus ove rcome partially the weakness of HMM. In addition, SHMM is able to integrate some features such as acronym pinyin input in a unified way, making itself more attractive than HMM to real-se tting Pinyin-to-Chinese conversion applications. All of these benef its are achieved at a little extra memory and computation cost. This work is supported in part by NSF Career Grant IIS 0448023, NSF CCF 0514679, PA Dept of Health Tobacco Settlement Formula Grant (No. 240205 and No. 240196), and PA Dept of Health Grant (No. 239667). We also thank Xuguang Zhang who helped us type testing sentences into three commercial products during evaluation. [1] Z. Chen, K. F. Lee, A new statistical approach to Chinese [2] Jianfeng Gao, Hai-Feng Wang, Mingjing Li, Kai-Fu Lee. [3] Jelinek, F. And Mercer, R. Interpolated estimation of [4] A. McEnery, Z. Xiao, The Lancaster Corpus of Mandarin [5] Rabiner, L.R., A tutorial on hidden Markov models and [6] P. K. Wong, C. K. Chan, Ch inese word segmentation based [7] Y. Zhang, B. Xu, C. Zong, Rule-based Post-Processing of [8] Y. Zhao, X. Wang, B. Liu and Y. Guan, Research of 
