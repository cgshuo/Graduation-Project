 Emails are one of the most commonly used modern communica-tion media these days; however, unsolicited emails obstruct this otherwise fast and convenient technology for information exchange and jeopardize the continuity of this popular communication tool. Waste of valuable resources and time and exposure to offensive content are only a few of the problems that arise as a result of junk emails. In addition, the monetary cost of processing junk emails reaches billions of dollars per year and is absorbed by public users and Internet service providers. Even though there has been exten-sive work in the past dedicated to eradicate junk emails, none of the existing junk email detection approaches has been highly suc-cessful in solving these problems, since spammers have been able to infiltrate existing detection techniques. In this paper, we present a new tool, JunEX, which relies on the content similarity of emails to eradicate junk emails. JunEX compares each incoming email to a core of emails marked as junk by each individual user to identify unwanted emails while reducing the number of legitimate emails treated as junk, which is critical . Conducted experiments on JunEX verify its high accuracy.
 H.3.3 [Information Search and Retrieval]: Information filtering Design, Management, Measurement Junk Email, Word Correlation, Similarity Detection
Emails have become part of our daily life these days. Not only they are free (provided by email servers such us Google, Yahoo, MSN, etc.), but they are also easy to set up and use. Unfortunately, nothing is perfect: everyday email users receive a significant num-ber of unsolicited emails, and often the number of unwanted emails exceeds the number of email that are actually important to them [9]. In fact, the amount of junk emails has been increasing from day to day, and in May 2006 for the first time the amount of spam emails exceeded the number of legitimate emails [12]. This prob-lem has become more serious and is getting out of control. For example, just in the month of November 2006 alone, the number of junk emails sent in the US reached 85 billion [4].

There are serious consequences with exceeded amount of junk emails: valuable time lost 1 , bandwidth cost, and disk space allo-cated for storing incoming emails. Figure 1 shows the significant increase in the number of spam emails during the year of 2006 [1]. As Table 1 presents statistical data associated with junk emails, Ta-ble 2 shows the impact of junk emails on the monetary side of the problem. It is worth to note that spam emails cost non-corporate Internet users 255 million dollars and resulted in a loss of 8.9 bil-lion dollars to U.S. corporations in 2006 alone [2]. These numbers are solid proofs that junk emails must be eradicated. Should the junk email problem persist, users may feel the need to turn to other means of communication, which is definitely not an acceptable al-ternative. Prior to investigating different strategies in solving the problem, we should consider (i) what constitutes junk emails for some people may not be so for others, and (ii) the cost of eliminat-ing even one legitimate email (unintentionally).

Since existing spam email detection techniques are inadequate due to the huge number of undetected junk emails flooding users X  mailboxes, it is absolutely essential to develop new techniques that will minimize, if not eliminating all of, the junk emails. Since de-signing spam filters for each user consumes time and spam email addresses are constantly changing, some existing spam filtering ap-proaches consider the arrival time of an email or the email subject by itself; however, they often fail in determining whether a new email should be treated as (non-)junk accurately. An ideal alter-native is to focus on detecting the content of an email to eradicate junk emails.

We proceed to present our results as follows. In Section 2, we discuss related work in detecting junk emails. In Section 3, we in-troduce our junk email detection approach, called JunEX, which measures the content similarity between an incoming email and a known junk email. In Section 4, we present the experimental re-sults that verify the accuracy of JunEX. In Section 5, we give a concluding remark.
Users waste a lot of time in deleting junk emails, which are de-tected after reading the heading, finding out the sender, and/or scan-ning portion of the emails in deciding which one is (not) a legiti-mate email. Figure 1: Spam Outbreaks in 2006, provided by Commtouch online lab [3] Table 1: Statistics of US emails during the year of 2006, com-puted by ToptenReviews.com [4]
Since eliminating junk emails has been a noteworthy problem for more than a decade, many proposed solutions to the problem have been presented in the literature. [ 11] introduce several commercial software and methodologies that allow users to define a set of rules to filter junk emails. The problem of this approach, as mentioned in [7], is that users are not always capable of defining solid rules. Furthermore, since junk emails change every day, existing rules need to be periodically updated, which is a time-consuming and ineffective process.

In [8] two different approaches are presented to deal with junk emails: the Naive Bayesian Method and the Chi Degrees of Free-dom Method. [8] also present several methods to reduce junk emails, which include (i) the use of Blacklist, i.e., a list of email addresses known to belong to spammers, and (ii) the use of Whitelist, i.e., a list of email addresses marked as acceptable by the user. The Blacklist, however, is not the solution to the spam filtering prob-lem, since most of the spammers discard their email addresses af-ter using them for the first time, whereas the Whitelist implies that other email addresses are treated as junk, which is clearly too rigid, since it discards every email in which the address does not appear in the list regardless of the content of the email, which might not be sent by a spammer. [13] implement a binomial distribution and a Poisson distribution in a Bayesian spam filter, which is used for calculating the proba-bility of an email being spam. This approach considers emails that contain rare words or words that are not stored in a database (i.e., words that are found for the first time), which is followed by ana-lyzing the first few words of an email and then using the statistical probability of the words to determine whether it is spam. Hence, the number of words that should be analyzed is minimized. Email considered Spam 40% of all email Daily Spam emails sent 12.4 billion Daily Spam received per person 6 Annual Spam received per person 2,200 Spam cost to all non-corp Internet users $255 m illion States with Anti-Spam Laws 26 Email address changes due to Spam 16% Estimated Spam increased by 2007 63% Annual Spam in 1,000 employee company 2.1 million Users who reply to Spam email 28% Users who purchased from Spam email 8% Corporate email that is considered Spam 15-20% Wasted corporate time per Spam email 4-5 seconds Table 2: Spam statistics in the US during the year of 2006, re-ported by ToptenReviews.com [4]
Different approaches have been proposed to detect junk emails with good results, since the accuracy shown for several of these approaches is higher than 88% [6]. However, since the style, con-tent, and methods adopted by spammers in randomizing emails to defeat filters (i) using random and innocuous text in the message and (ii) embedding messages with different images are constantly changing [3], it is very challenging to adopt one or a combination of existing techniques in solving the junk email problem. Instead of adopting existing junk-email discovery methods, our junk email detection approach, JunEX, relies on the email content to detect and eliminate junk emails.

JunEX analyzes the content of an incoming email and compare it with a previously marked junk email by the user using word sim-ilarity in a word-correlation matrix . This content-similarity detec-tion approach relies on pre-computed degrees of similarity among words in different documents. In [5], a set of Wikipedia documents (taken from http://www.wikipedia.org/) was used for computing the word(-to-word) similarity values, i.e., the correlation factors of distinct words, according to the (i) frequency of occurrences and (ii) proximity (i.e., relative distance) of words 2 in each Wikipedia document.

In detecting junk emails, we focus on the words within the sub-ject and the body of an email e , which yield what we call the con-tent descriptor of e , without considering the sender X  X  email address, commonly used words in junk emails, email arrival time, etc., since considering this information as part of the detection process would only increase the computational overhead without yielding better results (according to the experimental results as shown in Section 4). Since we focus on analyzing the content descriptor of any newly ar-rived email, any images the email might include are discarded. (As reported by Networld [10], image-based spam represents only 7% of the spam received nowadays, and hence excluding images in an email does not significantly affect the accuracy of JunEX in filter-ing junk emails.) The detailed process of our JunEX is shown in Figure 2 and is further explained below.
In each Wikipedia document D , stopwords were first removed (since they often carry little m eaning) and non-stop words in were stemmed to reduce all the words to their root forms. As a result, the number of words to be considered in D was reduced.
Each incoming email e (1) is compared with a junk email j marked by each user in the corresponding core using the word-correlation factors (3) to calculate their degrees of similarity (4). After the similarity value, i.e., Sim ( e, j ) , is computed, the value is compared with the Sim -TH value, which is 0.16. If 0.12 Sim ( e, j )  X  0.20, then e is further considered; otherwise, treated as either legitimate (5), if Sim ( e, j ) &lt; 0.12 or a junk email (6), if Sim ( e, j ) &gt; 0.20. When 0.12  X  Sim ( e, j )  X  continue the process (7) and calculate the degree of similarity, i.e., SimSB ( S, B ) , between the subject S and the body B of e SimSB ( S, B )  X  0.75, which is the SB -TH value, then we treat e as legitimate (8); otherwise, we treat e as junk (9).
In order to test the accuracy and impartiality of JunEX, we used four different email corpora to perform the evaluation. The first one, called  X  X YU," consists of emails accumulated between De-cember 2006 and April 2007, which were provided by a num-ber of email users (including our own email accounts). We col-lected more than 1,400 emails (both junk and non-junk) during the five-month period, and each user of the BYU corpus has his/her own collection of marked junk emails. The second email corpus, called  X  X olver," was downloaded from the University of Wolver-hampton (http ://clg.wlv.ac.uk/projects/j unk-email), which consists of 1,563 junk emails (including duplicates) that were collected over several years for analyzing the linguistic features of junk emails. The third one was the 2005 TREC Public Spam Corpus, called  X  X REC05," which consists of more than 90,000 labeled emails (both junk and non-junk), that were downloaded from the TREC site (2005 TREC Public Spam Cor pus, http://plg.uwaterloo.ca/cgi-bin/cgiwrap/gvcormac/foo). Last, but not least, the 2006 TREC Public Spam Corpus, called  X  X REC06" (downloaded from http://plg.uwaterloo.ca/ cgi-bin/cgiwrap/gvc ormac/foo06), consists of 37,822 emails (both junk and non-junk) in English and Chi-nese. We used a subset of the collection of emails in TREC05 and TREC06, i.e., 1,200, in our experiments.

In order to cope with Wolverhampton, TREC05, and TREC06 we treated each of them as emails collected by an individual user and randomly selected junk emails for constructing the core of marked junk emails that were fed into JunEX. Table 3 provides a detailed description of the emails within each corpus involved in this empirical study.
In order to compute the accuracy ratio of our junk email detec-tion approach, we used the emails within each corpus in Table 3 to analyze the results generated by JunEX. We computed the number Corpus Number of False False Accu-Error BYU 1,417 17 63 94.4% 5.6% TREC05 1,200 21 59 93.3% 6.7% TREC06 1,200 14 42 95.3% 4.7% Wolver 1,563 0 33 97.9% 2.1% Average 1,345 13 49 95.2% 4.8% Table 4: Accuracy and error rates of using JunEX according to different email corpora of false positives and false negatives using Equation 1 to calculate the percentage of accuracy and error rate , respectively of JunEX. where Correctly-Detected emails is the total number of email ex-amined minus the sum of the number of false positives and false negatives. Table 4 shows the accuracy and error rates of using JunEX to detect junk emails on different email corpora and the av-erage accuracy in eradicating junk emails, which is high.
Based on the experimental results, we observed that (i) the larger the size of an emails corpus, i.e., the core of junk emails, the better JunEX performs, and (ii) JunEX enhance its performance when the majority of emails are between 1 and 10 Kbytes, since when emails have a reduced number of words (less than 1 Kbytes), the number of words used to compute the similarity value can be insufficient (i.e., inadequate) and as a result more emails can be misclassified.
We have proposed in this paper a junk-email detection approach, called JunEX, that makes use of the correlation factors among words in emails to discover and minimize the number of junk emails. We verified the correctness of the design of JunEX using both public and private corpora of (junk and non-junk) emails. Experimental results show an overall accuracy rate of more than 95% in detect-ing emails that are junk correctly, whereas the rate of misclassify-ing non-junk emails is at most 5%, i.e., JunEX reduces the number of legitimate emails that are misclassified as junk to only a few, which may contain valuable information that the user cannot af-ford to lose. Furthermore, JunEX is appealing due to its minimal overhead (since word-correlation factors are pre-computed) and its little involvement of its user in eradicating junk emails, i.e., JunEX is almost fully automated. [1] Commtouch. January Virus and Spa Statistics: 2006 Starts [2] D. Evett. Spam Statistics 2006. http://spam-filter-review. [3] J. Goodman, G. Cormack, and D. Heckerman. Spam and the [4] G. Keizer. Spam Volume Jumps 35% in November. [5] J. Koberstein and Y.-K. Ng. Using Word Clusters to Detect [6] M. Lan and W. Zhou. Spam Filtering Based on Preference [7] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. [8] C. O X  X rien and C. Vogel. Spam Filters: Bayes vs.
 [9] S. Olsen. Spam: It X  X  Completely Out of Control. http:// [10] M. Osterman. The Problems Presented by Image-based [11] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A [12] S. Vaughn-Nichols. Saving Private Email. IEEE Spectrum [13] R. Zakariah and S. Ehsan. Detecting Junk Mails by
