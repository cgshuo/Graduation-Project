
In this paper, we study semi-supervised linear dimen-sionality reduction. Beyond conventional supervised meth-ods which merely consider labeled instances, the semi-supervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance. Under semi-supervised set-tings, our objective is to learn a smooth as well as dis-criminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace. Specifically, we present the Transductive Component Anal-ysis (TCA) algorithm to generate such a subspace founded on a graph-theoretic framework. Considering TCA is non-orthogonal, we further present the Orthogonal Transductive Component Analysis (OTCA) algorithm to iteratively pro-duce a series of orthogonal basis vectors. OTCA has better discriminating power than TCA. Experiments carried out on synthetic and real-world datasets by OTCA show a clear improvement over the results of representative dimensional-ity reduction algorithms.
Semi-supervised learning [17], a vital machine learning technique, is invented to deal with the situations where only sparsely labeled data are available along with abundant and ample unlabeled data. This scenario makes sense in many practical problems since it is often easy to gather unlabeled data, but is expensive for human to label the data. Semi-supervised learning technology can be readily applied to a wide spectrum of practical classification problems in which unlabeled data can be easily obtained with an automatic pro-cedure, while the acquisition of labeled data is costly. cuses on classification. Notably, semi-supervised classifi-cation has been well investigated in both theory and prac-tice [2]. A family of semi-supervised learning algorithms [8][16][18][11] based on spectral graph theory [5] either propose new loss functions or put forward new spectral reg-ularizers. However, an important topic, dimensionality re-duction, which is the key to efficient semi-supervised learn-ing has rarely been addressed in previous literature. tion has attracted considerable attention and been applied to various fields. We argue that under semi-supervised set-tings, dimensionality reduction should be reconsidered. Re-cently the manifold learning community began to study the semi-supervised learning scenario. In [13], semi-supervised versions of classical manifold learning algorithms ISOMAP [12], LLE [10] and LTSA [15] are offered based on prior information about the intrinsic low-dimensional data rep-resentations. The most straightforward prior is in form of on-manifold coordinates of certain data points. Neverthe-less, we are not always able to grasp the prior in such an exact form, and sometimes even have no idea about it. of unlabeled data for training, semi-supervised learning supplements supervised learning, which often overfits la-beled instances, and thus obtains a better generalization per-formance. In this paper, we address a new form of learn-ing problem Semi-supervised Subspace Learning which naturally addresses dimensionality reduction under semi-supervised settings. Given both labeled and unlabeled data, we aim at learning a smooth as well as discriminative sub-space. Specifically, we adopt a dual optimum criterion to learn such a subspace founded on a graph-theoretic frame-work. We treat raw samples from two views: the unsu-pervised view is employed to keep the smoothness among nearby samples in terms of preserving neighborhoods; in contrast, the supervised view is exploited to discover the discriminability conveyed by labeled samples. The two views collaborate with each other and eventually evolve to a graph-based optimization framework, upon which we pro-pose two novel dimension reduction algorithms: Transduc-tive Component Analysis (TCA) and Orthogonal Transduc-tive Component Analysis (OTCA).

We first review recent related work in dimensionality re-duction in Section 2. Afterwards, we present our learning framework and the TCA algorithm in Section 3. Further, we accomplish orthogonalizing TCA in Section 4. Experi-ments are shown in Section 5 and conclusions are drawn in Section 6.
Two of the most popular linear dimensionality reduc-tion techniques are Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). PCA is unsuper-vised while LDA is supervised. PCA projects data along the directions of maximal variances so that the reconstruc-tion error is minimized. In contrast, LDA orients the pro-jection directions toward maximum discriminability. Given a set of data points x 1 ,  X  X  X  , x n  X  R d ,let a  X  R d be a projection vector. The objective function of LDA is a are the between-class scatter matrix and within-class scat-ter matrix, respectively. There are several variants of LDA. Local Discriminant Embedding (LDE) [4] is an incremen-tal work of Locality Preserving Projections (LPP) [6][7], extending LDA from the viewpoint of locality discrimina-tion. LDE may be supposed as a supervised version of LPP which is unsupervised.

Under semi-supervised settings, a straightforward option is to simply combine PCA and LDA in a cascaded proce-dure, i.e., PCA is first applied to all the data, including la-beled and unlabeled, and subsequently LDA operates on the labeled data only. A similar arrangement is to combine PCA and LDE. However, such a kind of combinations are naive since labeled and unlabeled data are considered separately. Instead, truly semi-supervised learning approaches should attempt to give joint consideration of both.
 Cai et al. [3] proposed Semi-supervised Discriminant Analysis (SDA) using a LDA-based discriminant. SDA uti-lizes the unlabeled data in an extra local scatter term in the objective function of traditional LDA, that is where the local scatter matrix S local is defined as The parameter  X  controls the trade-off between S t and S local . W ij measures the similarity between x i and x j . Note that SDA uses S t = S b + S w in lieu of S w in the denominator of eq. (1) due to the lack of sufficient labeled samples for the accurate estimation of S w in the semi-supervised case.
 sionality reduction (SSDR) method to handle weaker semi-supervised settings where only information in the form of pairwise constraints is available. The must-link constraints imply that a pair of instances belong to the same class, while the cannot-link constraints compel them to be from differ-ent classes. This method maximizes the following objective function
J ( a )= where  X  1 , X  2 &gt; 0 are trade-off parameters, and ML and CL denote the must-link and cannot-link sets, respectively. Maximizing eq. (3) actually performs constrained PCA as the term 1 n a T S t a is exactly the optimization objective of PCA. Although SSDR can also apply to partially labeled cases, it fails to preserve local neighborhood structures of samples in the reduced low-dimensional space. utilize the geometrical properties of data manifolds which may be encoded to graphs. In this paper, we are interested in exploiting these geometrical characteristics for linear di-mensionality reduction or subspace learning. We believe that the potential of subspace learning has not yet been fully discovered, particularly in semi-supervised learning. for learning with graphs [17]. Our method stems from recent developments in graph-based regularization tech-niques.
 subspace learning explored in this paper as learning a sub-space or a set of projection axes from large amounts of un-labeled data together with a few labeled data . We advocate that a successful semi-supervised subspace learning algo-rithm should have the following two properties:
Smoothness Property  X  the subspace is learned so that nearby points in the original Euclidean space are still close to each other after being mapped into this subspace.
Discriminative Property  X  the subspace in which la-beled points belonging to different classes are far away is pursued.

Toward classification, an excellent subspace should be smooth as well as discriminative. Hence, a graph-theoretic learning framework is deployed to simultaneously meet the smoothness requirement among nearby points and the dis-criminative requirement among differently labeled points.
Graphs naturally characterize pairwise similarities be-tween samples, which is the building block of many learning tasks. Graph-based semi-supervised learning ap-proaches impose label smoothness along graphs via apply-ing the graph Laplacian regularizer. Several algorithms, i.e. [8][16][18], are akin to each other, and only differ in spe-cific choices of the loss functions.

Given a point set X = { x 1 ,  X  X  X  , x l , x l +1 ,  X  X  X  , x R , suppose that the first l points are labeled and the rest are unlabeled. Without loss of generality, we presume that X is zero-centered, which can be achieved by subtracting the mean vector from all the sample vectors. We represent both labeled and unlabeled data in an undirected, weighted graph G =( V, E, w ) where V = { v i } is a vertex set with v corresponding to each data point x i , E  X  V  X  V is a set of edges connecting adjacent points, and w : E  X  R + is a weight function measuring edge strengths. We form the weight matrix W =( W ij ) ij =( w ( v i ,v j )) ij  X  R n  X  where N ( i ) denotes the set of indices of k nearest neighbors of x i . The scheme of constructing G is unsupervised, so we call it the unsupervised graph .

Specially, we assume a smooth nonlinear embedding function f : X X  R on all points, which assigns a 1D coor-dinate to each point x i and varies smoothly along edges of G in the same way as Laplacian Eigenmaps [1]. We expect that the 1D-projections { a T x i } generated by a projection vector a  X  R d , which will span the target subspace, ap-proaches the sufficiently smooth 1D-embeddings { f ( x i ) as much as possible. This expectation is fulfilled by mini-mizing i ( a T x i  X  f ( x i )) 2 . We can acquire a by solving the optimization problem: min The first term in eq. (5) constrains a T x i to be close to f ( x whereas the second term preserves local geometrical struc-tures as justified in [1].

Let us define X =[ x 1 , x 2 ,  X  X  X  , x n ]  X  R d  X  n , f = [ f ( x 1 ) ,  X  X  X  ,f ( x n )] T  X  R n , and compute the graph Lapla-cian matrix L = D  X  W in which D  X  R n  X  n is a diagonal matrix given by D ii = n j =1 W ij .Wearriveatthefollow-ing differentiable quadratic cost function from eq. (5) where the second term is the graph Laplacian regularizer , and  X &gt; 0 is the trade-off parameter. Minimizing S will bring the smoothness property to a so that the neighbor-hood structure of each point is well maintained after being projected onto a .

Differentiating S with respect to f results in and then we obtain Eliminating f in eq. (6) leads to Let us introduce an n  X  n matrix and prove the following proposition.

Proposition 1. If a matrix L is positive semidefinite then the matrix S =( I +  X L )  X  1 (  X L ) ,  X &gt; 0 , is also positive semidefinite .
 Proof. Performing SVD on L yields L = V  X  V T where V is an orthogonal matrix such that VV T = V T V = I and  X  is a nonnegative diagonal matrix. Then we have
Note that  X =  X  ( I +  X   X )  X  1  X  is also a nonnegative di-agonal matrix. We thus have Therefore, S is positive semidefinite.
 imizing S ( a ) , i.e. In particular, we call S ( a ) a smoothness regularizer which enforces a heavy penalty if neighboring points are projected to distant locations on the projection vector a . A series of these vectors span a smooth closed subspace A = sp { a i } Moreover, the endowed smoothness with S ( a ) is control-lable, that is, the larger the parameter  X  , the more smooth the subspace A . We consider two extreme cases,  X   X  0 and  X   X  X  X  : When  X   X  0 , the subspace learned by the smoothness regu-larizer S ( a ) is nearly equivalent to Locality Preserving Pro-jections (LPP) [6]. When  X   X  X  X  , all data points collapse to the mean vector in the extremely smooth subspace. cient labeled data for training, local structures are generally more important than global structures for discriminant anal-ysis. Note that the between-class scatter matrix S b used by LDA cannot well model the margin between examples from different classes. A proper way to attack this problem is to design a better criterion to measure the margin. In this pa-per, we propose a new criterion: Maximal Average Margin . We define two supervised graphs to model the margin. Let us focus on l labeled examples { x 1 ,  X  X  X  , x l } belonging to c classes, upon which two graphs G r and G e are built supervisedly. Within the intra-class graph G r , we establish an undirected edge from each point x i in the graph to those sharing the same label as x i . Denote the set of indices of points from the same class as C i ( i =1 ,  X  X  X  ,c ) and let l = | C i | . Within the inter-class graph G e ,weestablisha directed edge from x i to the points taking different labels. Therefore, G r is undirected and G e is directed, and then we define two weight matrices W r , W e  X  R l  X  l pertaining to G r and G e , respectively, by Note that the sum of each row in W r and W e is 1 and W r is symmetric. Since W e is asymmetric, we further define a diagonal matrix D e with the entries being the column sums
In order to mine local discriminating power, we pursue a projection direction a which would maximize the mar-gin between examples from different classes at each lo-cal area. For each labeled point x i , we define a point-wise margin as the average difference between two types of distances: one is the distance between x i and points tak-ing different labels, the other is the distance between x and points sharing the same label (including itself). At x ( i  X  C class distances, so the total number of distance differences is l k ( l in Fig. 1. Let denote the 1-D projections of labeled points onto a as { y i = a T x i } l i =1 , and form the sample matrix X y =[ y 1 ,  X  X  X  ,y l ] T = X T l a  X  R l . We formulate the point-wise margin as follows (suppose i  X  C k ) i =1 m ( i ) /l over all these labeled points as follows where D l = I + D e ,M l =3 I + D e + W e + W eT D will show that the matrix M l  X  R l  X  l is positive semidefi-nite.
 tor. Then, y T M l y = y T I + D e + W e + W eT y + 2 y T ( I  X  W r ) y in which the second term has been shown in eq. (17) as In addition, we can derive so we achieve which completes the proof since M l is symmetric. the average margin  X  m to minimizing a T X l M l X T l a under an equality constraint, i.e. max
We call M ( a ) as a margin regularizer which penal-izes a small margin between examples with different labels. Eq. (19) reveals that through minimizing this regularizer the projections y i and y j of two same labeled points x i and x onto a are so near that ( y i  X  y j ) 2  X  0 while the projec-tions of two differently labeled points are so separate that ( y is critical to guarantee that even for two neighboring points with different labels in the original Euclidean space, their linear projections onto a are far apart. Therefore, the mar-gin regularizer is capable of robust local discrimination.
So far, we can formulate a graph-theoretic optimization framework to learn the projection axis a by taking both smoothness and margin maximization into account. Com-bining eq. (12) and eq. (19), we arrive at the following con-strained optimization problem: where  X &gt; 0 is a parameter which controls the trade-off between the two terms, smoothness and margin. It is the two graph regularizers, S ( a ) and M ( a ) , that collaborate to achieve the optimal projection vector. The larger  X  (ab-sorbed in S ( a ) ) is, the more the smoothness regularizer is favored. The larger  X  is, the more the margin regularizer is favored. Being able to trade off these two regularizers is important in practice.

Still, XSX T +  X X l M l X T l is positive semidefinite since both S and M l have been justified positive semidefinite. Finally, the optimal projection vector a  X  that minimizes eq. (20) is offered by the minimal eigenvalue solution to the generalized eigenvalue problem: For real-world applications, we usually need more projec-tion vectors to span a subspace A rather than just one. Let the matrix characterizing the target subspace A be A = [ a 1 ,  X  X  X  , a r ] which is formed by the eigenvectors associated with the lowest eigenvalues of eq. (21).

In contrast with nonlinear dimensionality reduction ap-proaches, our method has an obvious advantage that the learnt subspace has a direct out-of-sample extension to novel samples x , i.e. A T x , and is thus easily generalized to the entire high-dimensional input space. Based on the proposed framework, we now present the Transductive Component Analysis (TCA) algorithm in Ta-ble 1, engaging both labeled and unlabeled data. R so Rank( X l D l X T l )  X  min { d, l } . If the data dimension d is larger than the number of labeled data l , X l D l X T l is singu-lar. In this case, the generalized eigenvalue decomposition eq. (21) leads to many zero eigenvalues and corresponding useless eigenvectors. We may apply PCA in Step 4 to re-duce the data dimension to d 1  X  l to ensure that eq. (21) provides meaningful eigenvectors. As a consequence, TCA can produce min { d, l } projection vectors at most. [ u 1 ,  X  X  X  , u r ]=[ P a 1 ,  X  X  X  ,P a r ] acquired by TCA is non-orthogonal since it stems from generalized eigenvectors, not standard eigenvectors. Hence, we impose 1 which indicates that U consists of r orthogonal projection vectors in R d . The reason to impose such an orthogo-nal constraint is to explicitly make the projection vectors u ,  X  X  X  , u r to be linearly uncorrelated.
 sive notion. It is easy to get the point that orthogonal a conduce orthogonal u i s. Suppose we have obtained k  X  1 ( 1  X  k  X  r ) orthogonal vectors a 1 ,  X  X  X  , a column vectors that span the orthogonal complement A  X  k  X  1 of the subspace A k  X  1 = sp { a i } k  X  1 i =1 , and b  X  R proper vector to be decided. By doing so, such a k with the form of E k  X  1 b must be orthogonal to previous k  X  1 a i
In order to construct E k  X  1 , we perform QR factorization where [ Q 1 Q 2 ]  X  R d 1  X  d 1 is an unitary matrix. More-[ Q 1 Q 2 ] corresponds to a complete orthonormal basis of R d 1 , we designate E k  X  1 = Q 2 so that E T Q 2 Q 1 R =0 exactly holds. Particularly, we initialize E
Provided that the expression u k = PE k  X  1 b must satisfy the orthogonal constraint, the problem how many projection vectors are sufficient for effective classification impels us to correlate projection vectors and classes. Intuitively, we consider that one projection vector accounts for one class, and introduce least squares fitting X T l u k  X  Y .k 2 in which Y if i  X  C k , and Y ik =0 otherwise. Not only the least squares term enhances discriminability of the desired pro-jection vector u k , but also does it fix the scale of projection vectors. Therefore, we may remove the scale-fixing con-straint in the proposed optimization framework eq. (20) via adding the fitting term, that is where  X &gt; 0 is another trade-off parameter and set to a small value throughout this paper.

To obtain the exact solution, we plug u k = PE k  X  1 b into eq. (25), leading to min where Z =( PE k  X  1 ) T X and Z l =( PE k  X  1 ) T X l .Again, the matrix ZSZ T +  X Z l M l Z T l is positive semidefinite, which is reminiscent of justified positive semidefiniteness of both S and M l . Consequently, eq. (26) falls into con-vex quadratic optimization problems. Where the derivatives vanish, we acquire the closed-form solution To sum up, we circumvent eigenvalue solving to attain a series of orthogonal basis vectors which give rise to a well-formed subspace.

Immediately, we propose the OTCA algorithm by gear-ing Step 5 of the TCA algorithm in Table 1 as Step 5. For k =1to c Apparently, OTCA can output at most c projection vectors. In this section, we investigate the usage of TCA and OTCA on one toy problem and several real datasets. We compare them with the baseline LDA and the recently pub-lished methods LPP [6], LDE [4], SSDR [14] and SDA [3] along with two combinations PCA+LDA and PCA+LDE. A successful semi-supervised learning approach which could make a better use of unlabeled data for performance gain is of great practical significance. In many application do-mains, unlabeled data are plentiful, such as images, text, etc. Moreover, in many cases it is often easy to collect un-labeled data by an automatic procedure. This section ex-plores the potential of unlabeled data when incorporating them into dimensionality reduction together with existing labeled data.

As the unified classification platform, we use kNN as the classifier cooperated with various subspace algorithms. We also contrast with state-of-the-art semi-supervised learn-ing approaches. In detail, we used one toy dataset, four UCI datasets, and the FRGC dataset as our experimental testbeds. Table 2 describes fundamental information about these benchmark datasets.

Table 2. Dataset Information: the numbers of features, samples and classes.

The dataset for the toy problem used to test our methods is similar to the two moons data first coined in [16], which is a set of 3D two moons generated by a lifting function z =sin(  X y )( y 2 +1)  X  2 +0 . 3 y . The underlying manifold structure embedded in the 3D space is more difficult to un-cover than in 2D space. The raw 3D points are shown in Fig. 2(a). With 4 points labeled, it is a hard problem for su-pervised methods, such as LDA and linear SVMs, to handle. Nevertheless, the unlabeled data can aid classification.
Let LPP, LDA, LDE, SSDR, SDA, TCA (  X  =10 ,  X  =6 ) and OTCA (  X  =4 ,  X  =10 ,  X  =10  X  3 ) all reduce the number of dimensions of the raw data to 2. For LDA and SDA, we replicate their 1D embeddings to 2D ones since they output only one-dimensional features. From the em-bedding results shown in Fig. 2(b)-(e), we can clearly ob-serve that embeddings produced by LPP, LDA, LDE and SDA partially overlap in the marginal regions. The fully supervised method LDA and LDE badly overfit the labeled points. However, points from different classes are well sep-arated by a margin in the 2D subspace learned by our al-gorithm OTCA in Fig. 2(h). Moreover, OTCA generates a more smooth embedding than unsupervised LPP. Eventu-ally, we list 1NN correct rates achieved by all these algo-rithms in Table 3 and conclude OTCA is best for this toy classification problem (item 1NN denotes direct classifica-tion without any dimensionality reduction operations). It is worthwhile to point out that we construct the same 6-NN graph with eq. (4) for LPP, SDA, TCA and OTCA as all of them must involve the graph Laplacian.

Table 3. Comparison of correct rates of semi-supervised classification on 3D two moons.
 We apply eight subspace learning algorithms LDA, LDE, PCA+LDA, PCA+LDE, SSDR, SDA, TCA and OTCA on four UCI datasets: I RIS ,W INE ,B REAST C ANCER and C
AR , where we randomly choose 5% examples from each class as labeled data and treat the other examples as unla-beled data. We evaluate kNN (k=1) classification perfor-mance in terms of error rates on unlabeled data. For each dataset, we repeat the evaluation process with the 8 algo-rithms 50 times, and take the average error rates for com-parison. Fig. 3 displays the comparative results. In contrast to supervised LDA, LDE and semi-supervised PCA+LDA, PCA+LDE, SSDR, SDA and TCA, OTCA achieves the lowest average error rates across all these datasets. Table 4 reports TCA and OTCA X  X  error rates and the ones obtained by the baseline 1NN. OTCA leads to a significant reduction in error rates. (h) OTCA.

Figure 3. Error rates of 1NN classification us-ing a variety of subspace methods.

We also provide error rates achieved by Zhu et al.  X  X  gaussian fields and harmonic functions (GFHF) method [18] and Zhou et al.  X  X  local and global consistency (LGC) method [16] which directly conduct semi-supervised clas-sification with label propagation. Table 4 shows our semi-supervised classification method OTCA+kNN consistently outperforms them. For TCA and OTCA, we construct k -NN graphs with k =5 for all datasets. For GFHF and LGC, we also construct the same 5 -NN graphs by eq. (4), which fa-cilitates fair comparisons with our methods.

Table 4. Comparison of classification error rates (%) on UCI datasets.
 5.3 Face Dataset
Here we verify our subspace algorithms for the inten-sively studied topic, face recognition. Experiments are per-formed on a subset of facial images selected from FRGC version 2 [9]. We search all images of each person in the face database and then save the first 10 images if the total number of images is not less than 10. By doing so we find 3160 images from 316 persons. We align all these faces ac-cording to the positions of eyes and mouth and crop them to the fixed size of 64  X  72. We adopt grayscale values of images as facial features. Fig. 4 shows 20 face samples.
We randomly choose 20% images of each person in this dataset as the labeled data and consider the rest of images as the unlabeled data. We evaluate recognition performance on the unlabeled data with kNN (k=1). Repeating the recog-nition process over the unlabeled subset 20 times through running TCA (  X  =0 . 8 ,  X  =60 ), OTCA (  X  =0 . 4 ,  X  =40 ,  X  =0 . 01 ), and the other subspace algorithms, we plot the average recognition rates according to varied dimensions in Fig. 5. Empirically, we find that when PCA reduces the images to the features of 120 dimensions, PCA+LDA, PCA+LDE, SDA, SSDR, even TCA present good perfor-mance as all these algorithms apply PCA as their subrou-tines. For fair comparison, LDE also reduces the origi-nal data to 120 dimensions. As stated before, OTCA can produce at most c = 316 features, so its PCA step keeps 316 features. Since PCA+LDA and SDA are both seminal works of LDA, they have similar curves in Fig. 5. SSDR has the worst performance, while the proposed algorithm OTCA has the best performance. For each algorithm, we track the optimal feature dimension at which the pertinent algorithm performs best. The highest correct rates of every subspace learning algorithm and the associated dimensions are listed in Table 5.
 gaussian fields and harmonic functions (GFHF), Zhou et al.  X  X  LGC, and Belkin et al.  X  X  LapRLS [2] in terms of classification correct rates. We show that for this high-dimensional semi-supervised classification task, kNN co-operated with an effective subspace learner is capable of achieving satisfactory classification accuracy. Table 5 dis-plays that OTCA+kNN significantly outperforms GFHF, LGC and LapRLS which are vulnerable to scattered noise hidden in real data. Especially, both of GFHF and LGC can only apply to the training examples due to its transductive nature but OTCA+kNN suffices to classify novel samples that have never been encountered before.
 in the dataset as the labeled data and repeat the above pro-cess. The corresponding results are shown in Fig. 6. This time, we still draw the conclusion that OTCA outperforms all the other dimensionality reduction algorithms as well as three semi-supervised classifiers. Additional results re-garding error rates vs. parameters are revealed in Fig. 7. Note that we construct the same 6 -NN graph for SDA, TCA, OTCA, GFHF, LGC, and LapRLS. semi-supervised dimensionality reduction. Two novel sub-space learning algorithms, Transductive Component Anal-
Table 5. Comparison of correct rates of face recognition.
 Compared 20% Labeled 40% Labeled Methods Crr (%) Dim Crr (%) Dim PCA+LDA 76.96  X  0.80 80 91.12  X  0.76 40 PCA+LDE 75.36  X  0.80 50 90.69  X  0.76 40
LapRLS 67.87  X  0.93  X  85.00  X  0.68  X 
Figure 5. Recognition accuracy vs. dimen-sion. ysis (TCA) and Orthogonal Transductive Component Anal-ysis (OTCA), are proposed for the semi-supervised settings on a basis of graph regularization theory.

First, we establish an unsupervised graph over all sam-ples and exploit its resulting graph Laplacian to endow the subspace with sufficient smoothness. Second, we use la-beled samples to build two supervised graphs, the intra-class graph and the inter-class graph. By leveraging the two graphs, the average pointwise margin is maximized to make samples with different labels as far apart as possible after projecting into the subspace.

Compared to the state-of-the-art dimensionality reduc-tion algorithms, OTCA achieves a clear performance gain. Extensive experiments exhibit the advantages of the novel semi-supervised classification method OTCA+kNN. Grants Council of the Hong Kong SAR, China (Project No. CUHK 415408). The authors would like to thank Yiwen Luo for his great assistance on running experiments, and also thank Xiaoou Tang for his constructive suggestions.
Figure 7. Recognition accuracy vs. param-eter changes. (a)  X  is fixed as 40 and  X  changes; (b)  X  is fixed as 0.4 and  X  changes.
