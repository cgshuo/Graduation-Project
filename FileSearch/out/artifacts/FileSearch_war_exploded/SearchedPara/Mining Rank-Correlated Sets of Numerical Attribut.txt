 We study the mining of interesting patterns in the presence of numerical attributes. Instead of the usual discretization methods, we propose the use of rank based measures to score the similarity of sets of numerical attributes. New support measures for numerical data are introduced, based on exten-sions of Kendall X  X  tau ,and Spearman X  X  Footrule and rho .We show how these support measures are related. Furthermore, we introduce a novel type of pattern combining numerical and categorical attributes. We give efficient algorithms to find all frequent patterns for the proposed support measures, and evaluate their performance on real-life datasets. Categories and Subject Descriptors: H.2.4 [Database Management]:Systems I.2.6[Artificial Intelligence]:Learning Knowledge Acquisition General Terms: Algorithms, Experimentation, Theory. Keywords: Data mining, Numerical, Rank Correlation.
The motivation for the research reported upon in this pa-per is an application where we want to mine frequently oc-curring patterns in a meteorological dataset containing mea-surements from various weather stations in Belgium over the past few years. Each record contains a set of measurements (such as temperature or pressure) taken in a given station at a given time point, together with extra information about the stations (such as location or altitude).

The classical association rule framework [1], however, is not adequate to deal with numerical data directly. Most previous approaches to association rule mining for numer-ical attributes were based on discretization, see for exam-ple [16]. Discretization, however, has serious disadvantages. First of all it always incurs an information loss, since val-ues falling in the same bucket become indistinguishable and small differences in attribute value become unnoticeable. On the other hand, very small changes in values close to a dis-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. cretization border may cause unjustifiably large changes in the set of active rules. Second, if there are too many dis-cretization intervals, discovered rules are replicated in each interval making the overall trends hard to spot. It is also possible that rules will fail to meet the minimum support criterion when they are split among many narrow intervals. In [16] a method for merging narrow intervals into wider ones was combined with a special scheme to prune spuri-ous rules. The method, however, cannot entirely solve the problems related to discretization, since it is impossible to decide with certainty which rules were true associations and which were just artifacts of discretization. Also, informa-tion loss and instability at interval borders is inherent to discretization and cannot be eliminated entirely.

To tackle the problem of mining the meteorological dataset without relying on discretization methods, we propose a new technique based on well established statistical studies of rank correlation measures [12, 13]. More specifically, we propose to compare attributes by the rank their values im-pose on the records in the database. For example, for a given set of attributes, this can be done by counting the number of pairs of records such that all attributes rank the first tuple higher than the second tuple. When this num-ber is high, it gives a clear indication that the attributes in the set behave similarly, and hence, reveals an interesting pattern. As it turns out, this number is related to the well known Kendall X  X   X  [12] rank correlation measure, which will be thoroughly explained in the next section.

Some examples of the types of rules we are able to discover are the following: given two records t 1 and t 2 ,
The main contributions of our paper are as follows: 1. We propose three new support measures for sets of 2. We show how to combine the mining of sets of numer-3. We propose and discuss algorithms to mine sets of nu-4. We present an empirical evaluation of our proposed
For now, we assume that all values of numerical attributes are distinct, which is a reasonable assumption for real-valued attributes. Situations with tied values are taken care of in Section 3.

Let D be a database. Elements of D are called transac-tions, or records. Let H be the header of D , i.e. ,thesetof its attributes.

An attribute is called numerical if its domain is the set of real numbers. A categorical attribute has a finite un-ordered domain, and an ordinal attribute has a finite or-dered domain. An example of an ordinal attribute X  X  domain is { low, medium, high, veryhigh } .

The rank of a value t.A of a numerical attribute A ,is the index of t in D when D is ordered ascending w.r.t. A . That is, the smallest value t.A of A gets rank 1, etc. We denote the rank of t.A in A by r t .A ; i.e. , r t denotes the tuple t where all values have been replaced by their rank. In Table 1, an example weather database has been given. The number between brackets in the entry for t.A is ( r t .A ).
Rank methods use the ranks of attribute values, not the values themselves. They assess and test correlations between attributes with ordered domains.
Three well-known rank correlation measures are Kendall X  X   X  , Spearman X  X   X  ,and Spearman X  X  Footrule F [12, 13]. Let A and B be two numerical attributes.

The measures can now be defined as follows:  X  ( A, B )=1  X 
F ( A, B )=1  X   X  ( A, B )= 4 The observation behind  X  and F is that when two attributes are highly positively correlated, the ranks of their values will be close within most tuples. Hence, the sums t  X  D ( r ( t.A ) r ( t.B )) 2 and t  X  D | r ( t.A )  X  r ( t.B ) | will be small if A and t 1 3.15 (4) 20.1 (4) 1 030.3 (5) 0.75 (4) t 2 2.12 (2) 20.5 (5) 1 025.7 (4) 0.65 (3) t 3 5.19 (5) 13.7 (3) 1 015.6 (3) 0.80 (5) t 4 1.05 (1) 12.8 (2) 1 012.3 (2) 0.25 (1) t 5 2.13 (3) 5.3 (1) 1 005.7 (1) 0.30 (2) Table 1: Example weather data; W stands for Wind speed, T for Temperature, P for Pressure, and H for Humidity. The numbers between brackets indicate the rank of the value within the attribute.
 B are correlated. On the other hand, if A and B are uncor-related, the expected absolute difference between the ranks of A and B is ( | D | X  1) / 2, thus resulting in very high val-ues for these sums. The formulas are chosen in such a way that they are 1 for total positive correlation ( e.g. , A = B ), and  X  1 for total negative correlation ( e.g. , A and B are in reverse order). See [12] for details.

For  X  ( A, B ), it suffices to notice that is actually the probability that for a randomly chosen pair of tuples ( s, t ), s.A &lt; t.A and s.B &lt; t.B . Again, the for-mula is such that it is 1 for total positive correlation, and  X  1 for total negative correlation. Notice that this connec-tion of  X  with this probability, makes its interpretation very natural. The other two measures do not possess such clear interpretations.
The support measures we propose are the following: Similarly as for the correlation measures  X  and F ,thein-tuition behind the measures supp  X  and supp F is that at-tributes can be said to be similar if they result in similar ranks for the tuples. Hence, when the attributes are similar, (max A  X  I t.A  X  min A  X  I t.A ) tends to be small. Both supp and supp F are based on aggregating this difference over all tuples. For supp  X  ( I ), we take all pairs of tuples ( s, t ), and count the number of pairs such that s comes before t for all attributes of I . This number is then divided by the maximal possible number of such pairs. Notice incidentally that this maximum is | D | 2 , and not | D | ( | D | X  1), because it is not possible that s comes before t ,and t before s at the same time.
 Property 1. For all sets of numerical attributes I  X  J , and for supp m any of the measures supp  X  , supp F ,and supp  X  , the following holds: Example 1. Consider the example weather data given in Table 1. This relation has 4 numerical attributes: W , T , P , and H . In order to compute the supp  X  and supp F ,wefirst need to compute the ranks. This can easily be done by sorting the table once for every numerical attribute. In Table 1, the ranks have been given in brackets. In this example, supp  X  ( { T,P,H } )=1  X  1 supp F ( { T,P,H } )=1  X  1+2+2+1+1 In order to compute the support supp  X  ( { T,P,H } ) , we need to count the number of pairs of tuples such that the first tuple is smaller than the second tuple in T , P ,and H . E.g. ,the pair ( t 5 ,t 1 ) is such a pair, and ( t 2 ,t 3 ) is not. As there are 6 pairs of tuples that are in order ( 3 with t 4 and 3 with t as first element), supp  X  ( { T,P,H } ) is 6 / 10 .
Extensions of the rank correlation measures. No-tice that Property 1 states that the measures are scaled in such a way that they fall into the interval [0 , 1]. For supp and supp F , however, 0 can only be obtained in the special case that | I | X | D | . We can, though, re-scale by making the multiplicative constants depending on | I | . As a by-product, we provide the following extensions of  X  and F to multi-ple attributes, where the range is [  X  1 , 1], and these bounds are attainable, independently of k = | I | . The construction is based on the observation that in worst case, with k at-tributes, there are at most k tuples with the maximal dif-ference | D | X  1, k with difference | D | X  3, etc. Subsequently, the sum over the tuples in the formulas is divided by the support in this worst case. Note that for k = 2, these measures indeed correspond to the rank correlation measures  X  and F . There are, however, a couple of reasons why we prefer to use our support measures given in (1). First and most important,  X  and F do not have the monotonicity property. Second, for our purposes, the formulas  X  and F are needlessly complicated, while they are linearly related to our support measures anyway.
Discussion. It is worth remarking that supp  X  inherits the nice statistical interpretation of  X  . Indeed, supp is twice the probability that in a randomly selected pair of tuples ( s, t ), s is smaller than t in all attributes of I . This connection makes that supp  X  , in contrast to supp  X  supp F , is easy to interpret, and that we can extend it easily to other interestingness measures, such as e.g. , confidence of association rules between sets of numerical attributes. This extension is discussed later on in this section. On the neg-ative side, however, it should be noted that supp  X  is much harder to compute than the other two support measures, as  X  is defined over pairs of tuples while the other two over the ranks of single tuples. The many desirable properties of supp  X  , however, are our main motivation for dealing with the challenging computational difficulties of supp  X  ,instead of just taking one that is easier to compute.
The following theorem gives the average supp  X  in the case of statistically independent attributes. This expected sup-port under independence can be used as a reference point to assess the support of a set of numerical attributes.
Theorem 1. Let A 1 ,A 2 ,...,A k be k statistically inde-pendent numerical attributes. The expected value of the sup-Proof. Let us first prove a helpful fact. Let X be a nu-merical random variable and let x 1 and x 2 be its two in-stantiations. It is easy to see that Pr { x 1 &lt;x 2 } = deed, let F ( x ) be the cumulative probability distribution function of X and let f ( x ) be its probability density func-tion. Then Let us pick any pair of distinct records ( s, t ) from D . Since all attributes are independent, it follows that Pr { s [ A ] &lt;t [ A ] forall A  X  I } = 1 the expected supp  X  ( I )= 1 2 k  X  1 (Q.E.D.)
For the rank correlation measures  X  ,  X  ,and F ,thereex-ist many inequalities relating them to each other. For an overview, see, e.g. , [12, 13, 6]. We extended some of these bounds to our support measures. Especially the relations where supp  X  and supp F that can be used to upper bound supp  X  are of particular interest to us, because, in contrast to supp  X  itself, such an upper bound is easy to compute, and might allow for pruning if it is below the support threshold.
Theorem 2. For any set of numerical attributes I of any database, 1  X  X  D | (1  X  supp F ( I )) 2  X  supp  X  ( I )  X  1  X  1  X  supp Proof. The first inequality is based on the following obser-vation. Suppose that for a tuple t , A is the attribute in I with the lowest rank r t .A ,and B the one with the highest r .B . Then we know that in the database, there are exactly r .A  X  1 tuples smaller in A than t ,and r t .B  X  1 tuples smaller than t in B . Hence, there are at least ( r t .B  X  1)  X  ( r max A  X  I r t .A  X  min B  X  I r t .B tuples that are smaller in B , but not smaller in A than t . For these tuples s , neither ( t, s ), nor ( s, t ) will contribute to the support. If we sum this num-ber over all tuples, we get a lower bound on the number of pairs ( s, t ) such that neither ( s, t ), nor ( t, s ) contributes to the support. Since every incomparable couple of tuples s, t is taken into account twice by this measure, we need to di-vide it by 2 to get a lower bound on the pairs of tuples not contributing to the support. Hence, an upper bound on the number of pairs ( s, t ) such that s.A &lt; t.A for all A To get an upper bound on supp  X  ( I ), we still have to divide by n 2 , resulting in supp F ( I ).

The other inequalities are based on the observation that (Q.E.D.)
Example 2. Consider again the example relation given in Table 1. For the set { T,P,H } , we illustrate the bound supp  X   X  supp F . Consider, e.g., the tuple t 3 . In this tuple, therankof T is 3 ,andof H is 5 . Therefore, there must be at least 2 tuples that are smaller than H , and not smaller than T .
Our definitions of support are also applicable to ordinal attributes, since their domains are ordered. For ordinal at-tributes, however, we cannot assume that tied pairs do not occur, so in order to maintain the relationship to Kendall X  X   X  tied pairs would also need to be counted, see [13] for details. In Section 3, we discuss what to do in case of ties. Let us now look at how to integrate categorical attributes. Without loss of generality we will look only at binary at-tributes, also called items . Whenagivenitemissetto1we say that it is present in the transaction.

Categorical attributes will be used to define the context in which the support of numerical or ordinal attributes is computed. Let B be the set of binary attributes and O the set of numerical and ordinal attributes of the dataset D .
Definition 1. Let I =  X  ,I  X  O, J 1 ,J 2  X  B . The sup-port of I in context ( J 1 ,J 2 ) is defined as supp  X  ( I | ( J 1 ,J 2 )) = |{ ( t 1 ,t 2 ): t 1 ,t 2  X  D ,and J where J 1  X  t 1 ,J 2  X  t 2 denotes the fact that all items from J 1 are present in transaction t 1 and all items from J 2 are present in t 2 .

Notice that this definition of support for numerical and categorical attributes captures exactly the patterns we wanted to find in the first place.

Example 3. Suppose that we have attributes for the lo-cation where the weather data was gathered. Then, e.g. ,the rule  X  X n measurements of Antwerp and Brussels, the temper-ature and wind speed in Antwerp are higher than in Brussels in 70% of the cases X  could be expressed as: Note that this rule does not require that the measurements that are compared are of the same point in time. Rather it expresses that wind speeds and temperatures in Antwerp are in general higher than in Brussels.
Because of the interpretation of supp  X  ( I | ( J 1 ,J 2 )) as the conditional probability that two pairs that satisfy respec-tively J 1 and J 2 are concordant on all attributes of I ,itis straightforward to define the confidence of association rules in this context. Notice that for the other support measures, it is not at all straightforward how to score an association rule.

Definition 2. Let I 1 ,I 2 be sets of numerical attributes, and let J 1 and J 2 be sets of binary attributes. Then, the confidence of the association rule I 1  X  I 2 | ( J 1 ,J 2 as The interpretation of an association rule I 1  X  I 2 | ( J with confidence c , is simply that for all pairs of tuples ( t with t 1 satisfying J 1 ,and t 2 satisfying J 2 ,if t 1 is smaller than t on I 1 , then there is a chance of c that t 1 is also smaller than t 2 on I 2 .

Example 4. The following rule expresses that in Antwerp, if the wind speed in one measurement is greater than in an-other measurement, then there is a probability of 65% that the cloudiness will be greater as well.
In the beginning of the paper, we assumed that we are working with numerical data, in which no identical values occur. In reality, however, this assumption is not always true. Often, the data contain measurements of finite pre-cision, e.g. , temperatures up to . 1 degree. In those cases, many numerical values will be duplicated, and the ranks implied by the attributes are no longer total, because ties will occur. Also for ordinal data, many ties will occur. Consider, e.g. , an attribute Cloudiness that can take val-ues { low , medium , high } . For this attribute, at least 1 / 3th of the pairs will be tied. In this section we describe how wecanextendoursupportmeasurestoworkwithrankings with ties.

For our support measure supp  X  , we can just keep our original definition; that is, if two tuples have a tie on an attribute of I , they won X  X  contribute to supp  X  ( I ). For the other measures, we need to decide what rank we want to assign to a value that is repeated; i.e. , suppose a value t.A is repeated m times in attribute A ,andthereare p tuples with an A -value smaller than t.A . Then, in any order con-sistent with the ranking with ties, t.A will have a rank in the range p +1, ..., p + m . There are now several choices for the rank of t.A ; we could take the average of the ranks ar t .A =( p +1+ m ) / 2, the minimal rank r .A = p +1, or the maximal rank r.A = p + m . The choice we make will have a large influence on the supports measured by supp  X  and supp F . In our experimental evaluations, we have cho-sen for assigning the average rank to tied values, because, intuitively, this is the most natural choice, as it keeps, e.g. , the property that the sum of all ranks is | D | i =1 i . For the minimal and maximal rank, this sum respectively decreases and increases, hence introducing a bias. Thus, to summa-rize, for rankings with ties, the definition of supp  X  does not change, and supp  X  ( I ) and supp F ( I ) become: Notice that these measures reduce to the original definitions of supp  X  and supp F when there are no ties.

With these definitions, we can still upper bound supp  X  ( I ) with supp F . Indeed; similarly as in the proof of the bound without ties, let A and B be two attributes of a set I .Con-sider a tuple t in the relation D , then, on the one hand, there are at least ar t .A tuples s such that s.A  X  t.A .On the other hand, there are at most ar t .B tuples s such that s.B &lt; t.B . Therefore, there are at least ar t .A  X  ar tuples s , such that s.A  X  t.A and not s.B  X  t.B ,andthat thus cannot contribute to supp  X  . The rest of the proof of the bound is now the same as for the case without ties.
For the sake of providing an upper bound to supp  X  ,the result can still be improved:
Theorem 3. Let I be a set of numerical attributes. Let, for any tuple t of the database, Then, Proof. Consider a tuple t ,withmaxdiff( t )= r t .A  X  r t There are exactly r t .A tuples s such that s.A  X  t.A ,and there are exactly r t .B tuples s such that s.B &lt; t.B .There-fore, there are at least r t .A  X  r t .B tuples s , such that s.A t.A and not s.B  X  t.B , and thus cannot contribute to supp The remainder of the proof is the same as for Theorem 2. (Q.E.D.)
As only ranks are compared instead of the actual values in the database, we first transform the database by replacing every attribute value by its rank w.r.t. this attribute, which can be done in O ( | H || D | log( | D | )) time.

We now present several techniques to efficiently generate all sets of attributes satisfying the minimum support thresh-old, where support is defined as supp  X  , supp F , or supp sentially, the problem comes down to frequent itemset min-ing, with the important difference that we can no longer use most of the support counting optimizations used by many algorithms. Indeed, we do not have to count the number of transactions in which an itemset is present, but instead, we have to compare the ranks of all items of an itemset in each transaction. Therefore, we can never shrink the data-base by removing attributes or transactions, as is done in e.g. FP-growth or Eclat [9].

In the case of supp  X  , or supp F , the solution is straightfor-ward as it can be solved by the standard level-wise search in which candidate itemsets are generated only when all of its subsets are known to be frequent. Counting the supports re-quires only a single scan through the data in every iteration, as is done in Apriori [1].

In the case that supp  X  is used as support measure, the mining task becomes inherently more difficult since the num-ber of pairs of transactions is quadratic in the size of the database. In the next sections, we describe several methods which we investigated to tackle this problem.
A brute force solution to the presented mining problem, is to explicitly combine all pairs of transactions from the original database D , and create a Boolean database D ,such that The problem is then reduced to standard frequent itemset mining on the new database D .

The main disadvantage of this approach is that the size of the new database D (and the time needed to create it) is quadratic in the size of the original database, so it is only ap-plicable to small problems. If, however, the approach can be applied, it is most of the time very efficient in combination with depth-first frequent itemset mining algorithms such as Eclat, because all pairs that do not support an itemset will be removed from its branch, and hence, will not be consid-ered anymore in the support computation of its supersets in that branch. Such fine level of pruning will not be possible in the other approaches presented hereafter.
When the database is too large and the latter brute force method is not applicable, we need an efficient mechanism to count supp  X  for all candidate itemsets. For a given at-tribute set I , this comes down to the following nested for loop. for all t i  X  D do end for Obviously, this double loop is also infeasible as it performs a quadratic operation for each candidate set. A better ap-proach is to replace the inner for-loop with a more directed search for all transactions that have smaller values on all attributes in I . This is possible by using so called spatial in-dices . These are data structures especially designed to allow for searching in multidimensional data and to allow searches in several dimensions simultaneously.

For the itemsets search space traversal, we use a depth-first traversal (such as in e.g. FP-growth and Eclat) as this allows us to remove transactions from consideration in the current branch when they are no longer part of the support of the current itemset. Note, however, that since we consider pairs of transactions, we can only remove a transaction if it does no longer occur in any supporting pair of the current itemset. Of course, we will not duplicate the database for each branch, but only store the list of transaction identifiers of the transactions that are still under consideration. As a first optimization, we will actually store two separate lists of transaction identifiers, one for the outer for loop, and a second one on which the spatial index is created. This also allows us to prune some transactions earlier. Indeed, when a transaction in the outer loop list is no longer involved in any pair, then it can be removed from that list, although it might still be present and necessary in the index, and vice versa.

Another optimization, as we recall from Theorem 2, is the usage of the upper bound presented by supp F . As supp F can be computed a lot more efficiently compared to supp  X  we will first compute it for every candidate itemset. In case it gives a value under the minimum support threshold, we can already prune the itemset from the search space without computing the more expensive supp  X  .

Typically, depth-first frequent itemset mining algorithms do not exploit the monotonicity of support optimally. That is, when a candidate itemset is generated, they do not check whether all of its subsets are known to be frequent, and hence, they generate more candidate itemsets than in a breadth-first approach (see e.g. [9] for more information). Nevertheless, using simple heuristics and fast support count-ing techniques, it turns out not to have a great effect for most cases [4]. In our setting, however, the support computation becomes a lot harder and it is important to reduce the num-ber of candidate itemsets to be counted as much as possible. Fortunately, by generating all candidate itemsets in a re-verse depth-first manner, it is guaranteed that all subsets of a given candidate itemset are generated before the candidate itself [5]. For example, the subsets of { a, b, c, d } would be generated in the following order: { d } , { c } , { c, d } { b, c } , { b, c, d } , { a } , { a, d } , { a, c } , { a, c, d { a, b, c } , { a, b, c, d } . Thus, when all candidate itemsets are generated in this manner, it is still possible to check for each candidate whether all of its subsets are frequent, and if not, we can already remove the candidate as it must also be in-frequent, due to the monotonicity of supp  X  (see Property 1). To index all transactions for the inner loop, we considered two well known spatial indices, called kd-tree and range tree respectively [3]. Next, we discuss how they work, their ad-vantages and disadvantages. Notice that any other spatial data structures could be used as well. We report, however, only on kd-trees and range trees because they were designed primarily for main memory, in contrast to, e.g., R-trees [ ? ], which are more appropriate when secondary storage is being used.
A kd-tree [3] is similar to a binary search tree, except that, at every level of the tree, the ordering is based on a different attribute. In our case, the root of the tree orders on the first attribute of the itemset, its children on the second attribute and so forth.

The advantages of a kd-tree are its simplicity and low memory consumption. As a matter of fact, instead of im-plementing a tree structure, we simply implemented a bi-nary search on the database itself with appropriately sorted records. First, the whole dataset is partitioned around the median of the first attribute, then each of the halves is parti-tioned around the median of the second attribute within that half, and so on for each attribute. Then, when all attributes have been used, but some parts still contain multiple tuples, then the ordering starts over with the first attribute. A 1-dimensional range tree is simply a binary search tree. For multiple dimensions, a binary tree is created for the first dimension. Then, every node in the tree stores all transac-tionsthatfallintothisnodeinarangetree,whichisrecur-sively constructed for the remaining dimensions. We refer the interested reader to [3] for more detailed information.
The range tree has the important advantage that it is very easy to create the tree for an itemset of size k ,given the range tree of its prefix of size k  X  1. Indeed, when a new candidate itemset of size k is created in the depth-first traversal, we already created the range tree for its k  X  1 prefix, and we only need to add the a binary tree for the new dimension to all nodes that have multiple transactions in the nodes of the range trees for dimension k  X  1.
Obviously, both kd-tree and range-tree have their advan-tages and disadvantages. For the full technical details on the comparison, we refer to [3]. On the one hand, kd-trees use far less memory than range-trees. On the other hand, however, finding the number of tuples that are greater than a given tuple on all attributes of the itemset under con-sideration is far more efficient in a range-tree than in a kd-tree. More concretely, a kd-tree for an itemset of size k ,uses O ( nk ) space, where n isthenumberoftuples. Fortherange-tree, the size is as large as O ( n log k ( n )). For the counting of the number of tuples larger than a given tuple t ,thekd-tree uses time proportional to O ( n 1  X  1 /k ), thus resulting in a cost of O ( n 2  X  1 /k ) for the counting operation. For large k , this cost becomes quadratic. For the range trees, however, the cost for one tuple is O (log( n ) k ), resulting in a total cost of
O ( n log( n ) k ) for the counting operation. Clearly, there will be a trade-off between memory and time in the choice between the kd-tree and the range-tree. This trade-off will also become clear in the experiments.
Letusfirstlookatthe letter dataset from the UCI Ma-chine Learning Repository [10]. The dataset consists of se-lected features of 20000 handwritten letter images. All val-ues (except for the class) are integer in the range from 0 to 15. The table below briefly describes those of the attributes which are used later letter character represented by handwritten letter box-x x position of center of bounding box (left) box-x x position of center of bounding box (left) box-y y position of center of bounding box (bottom) box-w width of the bounding box box-h height of the bounding box onpix number of  X  X n X  pixels y-bar mean y of  X  X n X  pixels in box y2bar mean y variance x2ybr mean of x  X  x  X  y xy2br mean of x  X  y  X  y x-ege mean edge count left to right xegvy correlation of x-ege with y See [8], where the dataset was introduced, for details.
Table 2 shows the 5 most frequent itemsets in the letter dataset according to the three support measures proposed. The values for supp F and supp  X  have been scaled using Equations (2) to obtain more understandable values of sup-port, even though they were mined with the definitions in (1) because of the monotonicity property.

The patterns that come out on top are quite intuitive. For example { box-y,box-h } means that for tall letters the center of their bounding box is higher up. This patterns has supp of 0 . 734, meaning that if we pick two pairs of records at ran-dom, 73% of them agree on box-y and box-h . An analogous pattern can be seen for box X  X  width and x position. Another intuitive patterns is { box-w,onpix } , wider letters obviously require more pixels. The pattern { box-x,box-y } is an inter-esting pattern which we cannot explain, it says that x and y coordinates of the center of the bounding box are positively correlated. The pattern { box-y,box-w } means that wide letters tend to have the center higher up. We believe that this is due to letters with lower  X  X ails X  such as  X  X  X , X  X  X , X  X  X , X  X  X  are narrow, while wide letters such as  X  X  X  and  X  X  X  do not have such  X  X ails X . supp F and supp  X  produced a three item pattern { box-x,box-w,onpix } . This pattern made it to the top of the list after support scaling (Equations 2), and is a result of non-monotonicity of those measures. It seems that it is a consequence of patterns { box-x,box-w } and { box-w,onpix and does not contain any extra information. } 0.693 { box-x,box-w,onpix } 0.877 Table 3: Most frequent ( supp  X  ) patterns in letter data including the categorical  X  X etter X  attribute
Table 3 shows the most frequent 1, 2 and 3 attribute pat-ternsinthe letter data when the categorical  X  X etter X  at-tribute is considered. The  X  X etters X  column shows which pair of letters is involved, and the  X  X upport X  column shows the actual value of support as well, as this value divided by the total number of pairs of records with for the given pair of records. For example, the first row denotes that for 96 . 5% of pairs of a letter P and a letter U, the mean of x  X  y  X  the on-pixels is smaller for the P than for the U.
The top patterns involve arithmetic expressions of x and y coordinates of points which are not easily interpretable. It can be seen that the creators of the database have chosen the set of features quite well. Pattern { T/M, x-ege } ,isquite interesting. It shows the average number of edges encoun-tered, when traversing the letter image left-to-right. For  X  X  X  this number is almost always small, while for  X  X  X  there are often 4 edges to cross, so this number is high. The pattern tells us that in 99 . 3% of cases this number is parameter is indeed higher for letters  X  X  X  than for letters  X  X  X .
In Table 4, some rules that were found in the meteorolog-ical dataset by the proposed method are presented. The in-put data was 5 years of hourly measured weather data taken from one weather station located at Brussels airport. In the table, the rule is reported, its confidence, its support, and also the support of the consequent of the rule. The meaning of the attributes is respectively the altitude of the sun (so-lar alt), the amount of precipitation (precip), the amount of cloudiness (cloud), and the wind speed (w speed).
 cloud, w speed  X  precip 13% 57% 32% Table 4: Association Rules found in the weather data of one station in Brussels
The first rule simply claims that in 66% of the cases, the temperature is higher if the altitude of the sun is higher. The second rule indicates that if it is raining harder, then there is probability of 64% that the cloudiness is bigger as well. Intuitively, one would expect these confidences would be higher. This is not the case, however, as measurements of different days are compared (e.g., a measurement in win-ter vs a measurement in summer). Thus, for example, if the first measurement is in winter, and the second is in summer, the temperature of the former is likely to be lower than the latter, no matter what the solar altitudes in both measure-ments are.

The last three rules indicate that higher cloudiness or higher wind speed positively influence the amount of rain, and apparently, with both higher cloudiness and wind speed this effect becomes even stronger. Indeed; for two randomly selected tuples, the probability of having a higher precipita-tion is only 32%. When, however, we know that the cloudi-ness in the first tuple is higher, or the wind speed in the first tuple is higher, or both, the probability of the first tu-ple having a higher precipitation increases to respectively 48%, 44%, and 57%.
We evaluated the performance of the algorithms on a real-life meteorological dataset and on a number of benchmark datasets. All algorithms were implemented in the C pro-gramming language and tested on a 2.2GHz Opteron system with 2GB of RAM.

The meteorological data describes readings from several weather stations in Belgium at hourly intervals. The at-tributes include temperature, pressure, humidity, etc. There are about 5 million records in the database, and 57 at-tributes. There are many missing values, but about 20 at-tributes are present in most records.

We concentrate on performance of algorithms for supp  X  as it is much more computationally intensive than supp F and supp  X  . Some results for supp F and supp  X  are shown later in the section.

To evaluate performance of the algorithm for various data-base sizes we drew random samples of increasing sizes from 179  X  191 770  X  861 2 664  X  2 955 31 010  X  34 812 Table 5: Number of frequent itemsets for different values of minimum support for the meteorological data. Table 6: Characteristics of datasets used for exper-iments the meteorological data and ran the algorithm on each sam-ple for various minimum support thresholds.

Figure 1 shows the results for three supp  X  computation methods. It can be seen that range tree and kd-tree based algorithms scale to tens or hundreds of thousands of records depending on minimum support. In general we would not wait if the program ran for more than 3 hours. This was often the case with range trees, especially due to heavy swap memory usage. kd-trees incur almost no memory overhead so they always fit in main memory. The algorithm based on explicit pairs ran out of memory for 1 , 000 records already.
Note that the number of records of the original table does not give full justice to the problem size, as for ex-ample for 500 000 records in the original database, there 124 999 750 000 unordered pairs of records to look at! This shows high usefulness of spatial indices applied.

It can be seen that range trees are the most efficient method, except for small minimum supports. It breaks down when itemsets become too long, due to large memory consumption. For 1% minimum support kd-trees achieve much better performance, since they incur almost no mem-ory overhead. The third method: mining all explicitly pre-computed pairs of records, cannot handle data of 10 000 records at all. This happens since 49 995 000 new records which are generated do not fit in memory. However, due to the types of pruning possible only in this method, it gives very good results for low minimum supports.

Overall we would recommend the use of range trees, unless minimum support is too low and extensive usage of swap memory becomes an issue. For low minimum supports and large datasets kd-trees are a better alternative. For low minimum supports and small datasets, the algorithm based on the explicit pairs becomes viable. A development of a hybrid approach, which would switch from one method to another at various stages of the mining process is a topic of future research.

We found the number of frequent itemsets to remain ap-proximately constant with the increase in sample size as long as the support was constant percentage-wise. We summarize the results in Table 5 where for every value of minimum sup-port, the range in which the numbers of frequent itemsets were contained for all considered sample sizes.

We now evaluate performance of the algorithm on various benchmark datasets. Table 6 summarizes their characteris-Table 7: Performance results for supp F and supp  X  for the letter data. tics. The results are shown in Figure 2. Doubly logarithmic scales are used in both charts. We used several of Weka X  X  regression benchmarks as well as the letter database de-scribed earlier (with the class attribute ignored). All at-tributes were continuous. The results are given for supp and range tree based algorithm. It can be seen that the al-gorithm performs quite well even on large datasets. There are however some performance problems for datasets with large number of attributes.
Table 7 shows performance results for supp F and supp  X  for the letters data. We used the monotone version of the measures (Definition 1).

It can be seen that mining patterns using those measures is much more efficient than for supp  X  . Notice that both mea-sures are quite sensitive to minimum support level chosen. This is especially true for supp  X  , which moves relatively fast from the extreme point of 1 frequent itemset (the empty set is always frequent) to all itemsets being frequent. Also note that for supp  X  the values of minimum support required are very low, in the range of tenths of millionth of percent! It shows that the theoretical maximum value is rarely achieved.
There has been previous work by Han et. al [14] on min-ing itemsets which span more than one transaction. Their work however used discretization of numerical dimensions, and, they placed restrictions on which pairs of transactions were considered. More specifically only pairs of transactions whichareclosetoeachotherinoneormorenumericaldi-mensions were considered. The semantics of their rules is thus different. While we find that approach useful, it does not solve all the problems we encountered. Algorithms used in [14] cannot be applied to our definition of support.
Another approach to mine numerical data, which does the meteorological data. not use discretization can be found in [2, 15]. However the case considered in those works involves discrete attributes in the antecedent and a single numerical attribute in the consequent.

In [17] an interesting definition of support for continuous data is presented which does not require discretization. It is defined as the sum over all transactions of the squared minimum over all itemset X  X  attributes in each transaction. Although a relationship between the support definition and the cosine similarity of pairs of attributes as well as a re-lationship with the notion of h-confidence is given, the in-terpretation of the definition is not clear. Also, the seman-tics of the definition given in [17] is clearly different from our approach. Also ranks and relations to statistical rank methods is unique to our paper. Note that after converting to ranks, F and  X  are expressible in the framework pre-sented in [17] as  X  range ,L 1 and  X  range ,L 2 respectively. The original paper however does not discuss conversion to ranks and relation Spearman X  X   X  and Footrule. Also Dzeroski and Todorovski [7] propose an interesting approach to mining numerical data that does not rely on discretization. But, to the best of our knowledge, no previous work exists that uses quantities related to Kendall X  X   X  as measures of support.
In this paper we presented three new support definitions for continuous attributes based on rank methods used in sta-tistics. Those methods do not require discretization of nu-merical data. Relationships between the measures have been analyzed theoretically. Algorithms have been presented for mining frequent itemsets based on those definitions. Tests were performed on benchmark datasets as well as on a real-world meteorological data. The efficiency of the algorithm has been verified even on large databases.

An important direction for future work is improving even further the performance of the algorithms involving supp  X  The first idea is to use a hybrid strategy which switches between various mining algorithms depending on the size of the dataset and the available memory. For example, an algorithm might decide to switch away from kd-tree or range tree and generate explicit pairs of records at a lower lever of the recursion tree, when many records have already been pruned and there are only few attributes involved.
Another optimization is possible for ordinal attributes with small domains, where support could be obtained from their contingency tables. For example, suppose we have two attributes x, y with domains { 0 , 1 , 2 } .Let n ij denote the number of records with x = i and y = i . Support of xy can now be computed as n 00 n 11 + n 00 n 21 + n 00 n 12 + n 00 n minimum support ( supp  X  and range tree implementation used) done with just a simple scan of the data. It is not clear how much performance improvement this approach would yield for larger itemsets with larger domains.

An ultimate approach, as far as performance goes would be sampling. Since based on relatively small samples, statis-tical inferences can be made even about infinite populations, it would remove the database size limitation completely. Sampling itself is relatively easy to implement; we would pick two records at random, and check which attributes of the first record are less than attributes of the second, thus obtaining a single sample from the database currently built in the explicit pairs algorithm. A more difficult task is guaranteeing accuracy of the solution obtained. Methods presented in [11] can be used.

Currently we only consider support definitions where all items in one record have to be strictly less than all items in the other record. It would be interesting to consider cases where we require certain items in the first record to be less and others to be greater than (or equal) corresponding items in the other record. This way we could mine patterns where a decrease in value of one attribute causes an increase in the value of another.
 The authors would like to thank Heikki Mannila for pointing out the relationship of our support measure to rank aggre-gation methods, and Geert Barentsen for his help with the experiments on the meteorological dataset.

Toon Calders is funded by the Fund for Scientific Re-search -Flanders (FWO-Vlaanderen) as a post-doctoral fel-low. This work has been partially funded by the EU contract IQ FP6-516169.
