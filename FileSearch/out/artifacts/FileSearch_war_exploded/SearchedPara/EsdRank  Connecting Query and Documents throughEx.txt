 This paper presents EsdRank, a new technique for improv-ing ranking using external semi-structured data such as con-trolled vocabularies and knowledge bases. EsdRank treats vocabularies, terms and entities from external data, as ob-jects connecting query and documents. Evidence used to link query to objects, and to rank documents are incorpo-rated as features between query-object and object-document correspondingly. A latent listwise learning to rank algo-rithm, Latent-ListMLE, models the objects as latent space between query and documents, and learns how to handle all evidence in a unified procedure from document relevance judgments. EsdRank is tested in two scenarios: Using a knowledge base for web search, and using a controlled vo-cabulary for medical search. Experiments on TREC Web Track and OHSUMED data show significant improvements over state-of-the-art baselines.
 Ranking, Learning to Rank, Semi-Structured Data, Con-trolled Vocabulary, MeSH, Knowledge Base, Freebase
Prior research has produced many effective unsupervised retrieval models (e.g., BM25, language models, DPH) [9] and supervised learning to rank (LeToR) models (e.g., RankSVM, ListMLE) [19]. Unsupervised retrieval models successfully model relevance using information such as the terms in the query, the terms in the documents, and term level statistics such as term frequency ( tf ), inverse document frequency ( idf ), and document length. Learning to rank research derives new models and training methods to in-corporate the rich information provided by unsupervised re-trieved models, together with other evidence such as PageR-ank and spam score, as document ranking features, and pro-vide state-of-the-art ranking performance.

Sometimes the information in the query and corpus alone is not sufficient to rank documents effectively. For example, c  X  query terms may be an incomplete description of a well-known concept, or there may be a vocabulary mismatch be-tween queries and relevant documents. If the search engine relies solely on evidence from the query and corpus, its ac-curacy is limited.

One solution is to incorporate evidence that is external to the corpus, for example, from a controlled vocabulary, a thesaurus, WordNet, Wikipedia, or more recently, a knowl-edge base such as Freebase or DBpedia. Usually such re-sources are constructed manually or by carefully-designed information extraction systems and thus have different char-acteristics and greater consistency. The rich semi-structured information that they provide, for example, synonyms, on-tologies, entities, and relationships, gives the search engine a different perspective for understanding the relationship be-tween a query and a document, and provides opportunities for improving retrieval accuracy.

There is a long history of using external semi-structured data to improve full-text search [2, 10, 18, 20, 22, 29]. Prior research demonstrates that such resources can improve search accuracy. Many options have been proposed for vari-ous types of external resources, however there is little guid-ance about how to integrate new resources in a general man-ner. Prior work mainly focuses on how to better represent the query using external data, for example, how to select good expansion terms [29] and how to link to good enti-ties [10]. The query representation formulated from external data provides useful enrichment of the original query. Then the ranking is produced by either an unsupervised retrieval model or an existing learning to rank model that combines multiple query representations. We strive to further boost ranking performance by developing a new learning to rank model that reasons jointly over query, external data, and documents.

This paper describes EsdRank, a new method for improv-ing ranking using external, semi-structured data. EsdRank treats the vocabularies, terms and entities from external data as objects. The information from external data used for better query representation are used as features between the query and external objects. The document ranking evi-dence used in LeToR research is used as features between ob-jects and documents. Instead of treating the features about query-objects and features about object-document individ-ually, EsdRank uses a latent-listwise LeToR model, Latent-ListMLE, which treats the external objects as latent layer between query and documents, and learns how to handle the features between query, objects, and documents in one uni-fied procedure directly from document relevance judgments.
One major challenge in using external data is to find re-lated objects for a query and documents. Several methods have been used by prior research [10, 29], but it is not clear how each of them contributes to final performance. This research explores three popular methods to select related objects from external data, including query annotation, ob-ject search, and document annotation. To investigate their effectiveness, we apply EsdRank with related objects gener-ated by these methods on one newer type of external data, the Freebase knowledge base [3], and one classic type of ex-ternal data, the MeSH controlled vocabulary 1 , in web search and medical search respectively. Experiments on TREC Web Track and OHSUMED datasets show EsdRank X  X  sig-nificant effectiveness over state-of-the-art ranking baselines, especially when using related objects from query annota-tions. Experiments also show that the effectiveness not only comes from the additional information from external data, but also our Latent-ListMLE model that uses it properly.
The next section describes related research. Section 3 dis-cusses EsdRank, its Latent-ListMLE ranking model, related objects selection, and features. The experimental method-ology and evaluation results are described in Sections 4 and 5 respectively. The last section summarizes the paper X  X  con-tributions and discusses future work.
There is a long history of research on using informa-tion that is external to the document corpus to improve retrieval. The earliest information retrieval systems repre-sented queries and documents using terms that were selected manually from controlled vocabularies. Controlled vocabu-lary representations required more effort to create during indexing and search, but were less susceptible to vocabulary mismatch between queries and documents, and required less sophisticated retrieval models.

As full-text search became popular, controlled vocabular-ies continued to be used in some systems. The use of con-trolled vocabularies is almost a necessity in medical search engines because queries are often about diseases, treatments or genes, whose special meanings may not be covered by their names. Typical procedures include using controlled vo-cabularies as alternative representations for matching query and document [22, 25], and expanding queries using syn-onyms [21]. However, the mixed results of systems partici-pating in the TREC Genomics Track illustrate the difficulty of using controlled vocabularies in medical search; most sys-tems require human efforts to be successful [26].

There is a rich literature on using dictionaries, the-sauruses, and lexical resources such as WordNet to over-come vocabulary mismatch between queries and documents by adding synonyms and related concepts to queries and documents [18]. These approaches can be effective in enter-prise search and domain-specific search environments [14], or improving recall of relevant documents in general search environments [18].

Recently a new type of semi-structured data, knowledge bases, has emerged. Examples include Wikipedia, its struc-tured version DBpedia, Freebase [3] and the NELL KB [6]. Knowledge bases encode human knowledge about real world entities, such as names, aliases, descriptions, attributes, categories and relationships to other entities, in a semi-http://www.nlm.nih.gov/mesh/meshhome.html structured representation that is easily processed. Knowl-edge bases are a new path for search engines to better  X  X n-derstand X  queries and documents, but new techniques are required to use their novel and heterogeneous data types.
One intuitive method is to use the names and descrip-tions of knowledge base entities to expand the query. Xu et al. use pseudo relevance feedback in the Wikipedia corpus to obtain expansion terms that can be used to search other corpora [29]. Supervised methods have been used to pick better expansion terms from Wikipedia [4]. Wikipedia and other resources can be combined using both query concept weighting and query expansion for a better query representa-tion [1]. Pan et al. use names of Freebase entities to expand queries [24]. Xiong et al. select expansion terms from Free-base entities retrieved by the query or discovered in its top retrieved documents [28]. Liu et al. had the best results in the TREC 2014 Web Track ad-hoc task using Freebase with manual labeling [20]. They pick entities textually similar with manually identified query entities, and rank documents by KL-divergence with these entities X  descriptions.
A recent success in using Freebase is EQFE by Dalton et al. [10]. They study several ways to link Freebase enti-ties to a query using query annotation, textual similarity, entity context model, and annotations from top retrieved documents. The name, alias and type fields of these entities are considered as possible expansion candidates. The com-bination of different linking methods, expansion fields, and hyper parameters in expansion are enumerated to get vari-ous expanded queries. These enumerated expansion queries are then used to calculate ranking scores for each document using a query likelihood or sequential dependency model. A learning to rank model uses these ranking scores as features for each document and produces final document rankings. EQFE provides a thorough exploration of information avail-able to introduce entities between a query and documents, and is a major inspiration for EsdRank.
EsdRank is intended to be a general technique for using external semi-structured data to improve ranking. External data elements are modeled as objects . An object could be a controlled vocabulary term, a term from another corpus, or a knowledge base entity. The evidence from the query, external data, and corpus is incorporated as features to ex-press the relationship between query, object and document. A ranking model is then learned to rank documents with these objects and evidence.

In the first part of this section, we propose a novel la-tent listwise learning to rank model, Latent-ListMLE, as our ranking model. Latent-ListMLE handles the objects as a latent space between query and documents. The evidence between query-object, and object-document is naturally ex-pressed as features connecting the query to the latent space, and then connecting latent space to documents.

Prior research found that a major challenge in using exter-nal data is to find related objects [7, 10, 29]. In the second part of this section, we explore three popular related object selection methods used in prior research. In the final part of this section, we describe the features used to connect query, objects and documents. Given a query q and an initial set of retrieved documents D , a set of objects O = { o 1 ,...,o j ,...,o m } related to q and D is produced by one of the related object selection methods as described in Section 3.2. Features that describe relation-ships between query q and object o j are denoted as vector v . Features that describe relationships between object o j and document d i are denoted as u ij . The goal of Latent-ListMLE, like other learning to rank methods, is to re-rank D , but with the help of the related objects O and feature vec-tors U = { u 11 ,...,u ij ,..,u nm } and V = { v 1 ,...,v
Latent-ListMLE treats O as the latent space between q and D and uses V,U as features to describe the relationships between query-object, and object-document. We will first revisit ListMLE [27], the listwise learning to rank model which Latent-ListMLE is built upon. Then we discuss the construction, learning, and ranking of Latent-ListMLE.
ListMLE defines the probability of a ranking (a list) be-ing generated by a query in a parametric model. Maximum likelihood estimation (MLE) is used to find parameters that maximize the likelihood of the best ranking(s). However, the sample space of all possible rankings is the permutation of all candidate documents D , which is too large. One con-tribution of ListMLE is that it reduces the sample space by assuming the probability of a document being ranked at po-sition i is independent of those ranked at previous positions.
Specifically, with a likelihood loss and a linear ranking model, ListMLE defines the probability of a document d being ranked at position i as: where S i = { d i ...d n } are the documents that were not ranked in positions 1 ...i  X  1, x i is the query-document fea-ture vector for d i , and w is the parameter vector to learn.
Equation 2 defines the likelihood of a given ranking ~ D of candidate documents.
The parameter vector w is learned by maximizing the like-lihood for all given queries q k and their best rankings given document relevance judgments: This is an unconstrained convex optimization problem that can be solved efficiently by gradient methods.
Latent-ListMLE extends ListMLE by adding a latent layer in the ranking generation process. The latent layer contains related objects O as possible representations of the original query q .

With the latent layer O , the ideal generation process of a ranking ~ D is to first sample a ranking of objects ~ O , and then sample document ranking ~ D based on ~ O . However, this process is also impractical due to the huge sampling space. Similarly to ListMLE, we assume that the probabilities of picking objects and documents at each position are indepen-dent with those at previous positions. Thus, the generative process is redefined to be:
For each position from 1 to N: 1. Sample o j from multinomial distribution Multi ( O | q ), 2. Sample d i from multinomial distribution Multi ( S i | o We further define: where v j is the query-object feature vector for o j ; u ij object-document feature vector between d i and o j ; m is the number of objects; and  X  and w are the model parameters.
In this generative process, the latent layer is the sampled objects produced by query q , and the document ranking probability is conditioned on the sampled objects instead of the query. With this extension, the probability of picking d at position i is: p ( d i | q,S i ) = and the probability of ranking ~ D given q is:
Latent-ListMLE also uses query-document features by adding a  X  X uery node X  o 0 to O that represents query q . The features relating query q to o 0 are set to 0, thus o 0 is the  X  X rigin point X  for related objects. The features relating o to documents are typical LeToR query-document features. The combination of query-document features and object-document features is done by treating them as individual dimensions in U , and setting missing feature dimensions X  val-ues to zero. So the dimensions referring to object-document similarities for the query node are set to zero, and vice versa.
Our idea of representing the query via related objects expansion terms as our related objects, and discard the doc-ument ranking part, Latent-ListMLE becomes a supervised query expansion method. On the other hand, if we only use the query node as related object, Latent-ListMLE is exactly the same as ListMLE. The difference is that, in Latent-ListMLE, the connections from query to latent layer, and from latent layer to documents are learned together in one unified procedure, which finds the best combination of query representation ( p ( o | q )) and document ranking ( p ( d | o )) together, instead of only focusing on one of them.
The parameters w and  X  are learned using MLE with given queries and their best rankings. To keep the notation clear, we present the derivation with one training query q , without loss of generality.
For a training query q and its best ranking ~ D  X  derived from relevance labels, their log likelihood is:
The goal of MLE is to find parameters w  X  , X   X  such that:
Directly maximizing Equation 10 is difficult due to the summation of the latent variables inside the log, thus we use the EM algorithm to solve this optimization problem.
The E step finds the posterior distribution of hidden vari-able o j for each ranking position given the current parame-
The M step maximizes the expected log complete likeli-hood.
 E (  X 
We use gradient ascent to maximize the expectation. The gradients are:  X  X  (  X  l )  X  X  (  X  l )
The E step is very efficient with the closed form solu-tion. The M step is an easy convex optimization problem. Intuitively, the E step finds the best assignment of object probabilities under current parameters and best document rankings, thus transferring document relevance information to latent objects. The M step learns the best parameters that fit the object probabilities provided by the E step. EM iterations are guaranteed to improve the likelihood until con-vergence. However, the overall optimization is not convex and has local optima. In practice the local optima problem can be suppressed by repeating the training several times with random initial w and  X  , and using the result that con-verges to the largest likelihood.
Given a learned model, a query, and an initial ranking, a new ranking is constructed by picking the document that has the highest probability p ( d i | q,S i ) at each position from 1 to n . The complexity of picking one document is O ( nm ), thus the total cost of ranking n documents with m related objects is O ( n 2 m ), which is slower than ListMLE X  X  O ( n log n ) rank-ing complexity. We can restrict the number of documents n (e.g. 100) and related objects m (e.g. &lt; 5) to maintain reasonable efficiency.
How to find related objects O given query q and docu-ments D is important for using external data. Many options have been proposed by prior research [10, 20, 24, 29], but it is not clear which is the most reliable. This research stud-ies the following three popular automatic methods to find related objects.

Query Annotation selects the objects that directly ap-pear in the query. Based on specific object types in the external data, one can choose corresponding techniques to  X  X nnotate X  them to the query, for example, entity linking techniques [7] to find entities that appear in the query, or all query terms when the objects are terms from an external corpus.

Object Search selects the objects that are textually sim-ilar to the query. Search engines can be used to find such objects. We can build an index for all the objects in exter-nal data, in which each document is the textual data about the object, for example, name, alias, description and con-text words of an entity. Then textually similar objects can be retrieved by running the query to the index.

Document Annotation selects the objects that appear in retrieved documents D . The terms in retrieved documents have been widely used in query expansion. The entities that are annotated to D were also useful in prior work [10]. This method introduces objects that are more indirectly related to the query, such as  X  X resident of United States X  for the query  X  X bama family tree X . A typical method to score and select objects from retrieved documents is the RM3 pseudo relevance feedback model [17].
Representing the relationship between query and related objects is the major focus of prior research in using exter-nal data. We explore the following query-object features in EsdRank; most of them are inspired by Dalton et al. [10].
Object Selection Score features are the scores pro-duced by the object selection step: Annotator confidence, object ranking score, and RM3 model score s ( q,o ).
Textual Similarity features cover the similarities be-tween the query and the object X  X  textual fields. For exam-ple, one can use coordinate matches, BM25 scores, language model scores, and sequential dependency model (SDM) scores between the query and the object X  X  textual fields, such as name, alias (if any) and descriptions if using entities.
Ontology Overlap features use the ontology, a com-mon type of information for some external semi-structured datasets. When an ontology is available, one can build a multiclass classifier that classifies the query into the ontol-ogy, and use the overlaps with the object X  X  ontology as fea-tures.

Object Frequency is the number of documents in the corpus the object appears in (e.g. term) or is annotated to (e.g. entity). This feature is query independent and distin-guishes frequent objects from infrequent ones.
Similarity with Other Objects are the max and mean similarity of this object X  X  name with the other related ob-jects X . These features distinguish objects that have similar names with other related objects from those that do not.
In learning to rank, rich query-document ranking features, such as multiple retrieval algorithms on different document fields, have been shown very important for ranking perfor-mance [19]. Our object-document features are similar to query-document features widely used in LeToR research. But objects may have richer contents than queries, enabling a larger set of features.

Textual Similarity features measure the similarity of a document and an object X  X  textual fields. BM25, language model, SDM, coordinate match, and cosine similarity in the vector space model are used to calculate similarities be-tween all combinations of an object X  X  textual fields and a document X  X  fields. These retrieval models are applied by using the object to  X  X etrieve X  the document. The fusion of these similarity features provides multiple ways to express the object-document similarities for EsdRank.

Ontology Overlap features are the same as the ontol-ogy overlap features between query and object. The same multiclass classifier can be used to classify documents, and overlaps in the object X  X  and document X  X  top categories can be used as features.

Graph Connection features introduce information about the relationships in the external data and document annotations. If such graph-like information is available, one can start from the object, traverse its relations, and record the number of annotated objects reached at each step (usu-ally within 2 steps) as features. These features model the  X  X loseness X  of the object and the document X  X  annotations in the external data X  X  relationship graph.

Document Quality features are commonly used in learning to rank. We use classic document quality features such as document length, URL length, spam score, number of inlinks, stop word fraction, and whether the document is from Wikipedia (web corpus only).
EsdRank provides general guidance about how to use ex-ternal semi-structured data in ranking. When an external dataset is available, to use it in ranking, one can first use object selection methods to find related objects for each query, and then extract query-object and object-document features. Although the detailed object selection methods and features may vary for different datasets, we list some common related object selection methods and features that have been used widely in prior research to start with. One can also derive new related object selection methods and features based on other available information.

Related objects and features are handled by our latent listwise LeToR model, Latent-ListMLE. It models the ob-jects as the latent space. As a result, evidence is decou-pled into a query-object part and an object-document part, which is easier to learn than mixed together (as shown in Section 5.2). Instead of solely focusing on the query-object part, or the document ranking part, Latent-ListMLE learns them together using EM. Our experiments show that the additional evidence from external data and Latent-ListMLE are both necessary for EsdRank X  X  effectiveness.
EsdRank is intended to be a general method of using ex-ternal semi-structured data, thus experiments test it with two types of semi-structured data and search tasks: Web search using the Freebase knowledge base, and medical search using the MeSH controlled vocabulary. The former uses a newer type of semi-structured data, a noisy corpus, and queries from web users; the latter uses a classic form of semi-structured data, a clean corpus, and queries from do-main experts. Datasets and ranking features are determined by these two choices, as described below.

Data: Experiments on web search using Freebase were conducted with ClueWeb09-B and ClueWeb12-B13, two web corpora used often in IR research. Each corpus contains about 50 million English web documents. The 2009-2013 TREC Web Tracks provided 200 queries with relevance as-sessments for ClueWeb09 and 50 queries with relevance as-sessments for ClueWeb12. We used a snapshot of Freebase provided by Google on Oct 26th, 2014. 2
Experiments on medical search using the MeSH controlled vocabulary were conducted with the OHSUMED corpus, a medical corpus used often in IR research. It contains ab-stracts of medical papers that are manually annotated with MeSH terms. The OHSUMED dataset includes 106 queries with relevance assessments. We used the 2014 MeSH dump provided by the U.S. National Libarary of Medicine (NIH).
Indexing and Base Retrieval Model: The ClueWeb and OHSUMED corpora were indexed by the Indri search engine, using default stemming and stopwords. ClueWeb documents contain title, body and inlink fields. OHSUMED documents contain title and body fields.

Freebase and MeSH also were indexed by Indri, with each object (entity, controlled vocabulary term) treated as a doc-ument containing its associated text fields (name, alias, de-scription). Objects without descriptions were discarded. Re-trieval was done by Indri using its default settings.
Spam filtering is important for ClueWeb09, thus we re-moved the 70% spammiest documents using Waterloo spam scores [8]. Prior research questions the value of spam filter-ing for ClueWeb12 [10], thus we did not filter that dataset.
Related Objects: We implement three related genera-tion methods as described in Section 3.2.

Query annotation ( AnnQ ) was done by TagMe [12], one of the best systems in the Short (Query) Track at the ERD14 workshop competition [7]. TagMe annotates the query with Wikipedia page ids. Wikipedia page ids were mapped to Freebase objects using their Wikipedia links and to MeSH controlled vocabulary terms using exact match.

Object search ( Osrch ) was done with the Indri search en-gine for both external semi-structured datasets. We inves-tigated using Google X  X  Freebase Search API to search for Freebase objects and PubMed X  X  MeSH query annotations to annotate MeSH objects. Neither was significantly better than Indri search or TagMe annotation, thus we only report results with public solutions.

Google X  X  FACC1 dataset provided annotations of Free-base entities in ClueWeb documents 4 [13]. The OHSUMED dataset contains MeSH annotations for each document. Document annotation ( AnnD ) objects are generated by first https://developers.google.com/freebase/data http://www.nlm.nih.gov/mesh/filelist.html http://lemurproject.org/clueweb09/ Table 1: Query-object features.  X  X eb X  and  X  X edical X  indi-cate the number of features in each corpus.
 Language model of object X  X  description 1 1 Coordinate match of object X  X  text fields 3 3 Table 2: Object-document features.  X  X eb X  and  X  X edical X  indicate the number of features in each corpus.  X  X ield com-binations X  refers to combinations of an object X  X  and a doc-ument X  X  fields. An object X  X  fields include name, alias, de-scription, query minus name, query minus alias, and query minus description. ClueWeb documents (for Fb) contain title, body and inlink fields. OSHUMED documents (for MeSH) contain title and body fields.
 Cosine correlation of field combinations 18 12 Coordinate match of field combinations 18 12 retrieving documents from the ClueWeb or OHSUMED cor-pus, and then using the RM3 relevance model [17] to select annotated objects from these documents.

Thus, the experiments consider query annotation ( EsdRank-AnnQ ), object search ( EsdRank-Osrch ) and docu-ment annotation ( EsdRank-AnnD ) methods to select related objects.
 Feature Extraction: We extract features described in Section 3.3. Our feature lists are shown in Tables 1 X 3.
For object-document text similarity features, each object field is dynamically expanded with a virtual field that con-tains any query terms that the field does not contain. For example, given an object with name  X  X arack Obama X  and query  X  X bama family tree X , the text  X  X amily tree X  is added to the object as a virtual field called  X  X uery minus name X . Simi-lar expansion is performed for  X  X uery minus alias X  and  X  X uery minus description X  fields. As a result, Latent-ListMLE also knows which part of the query is not covered by an object. This group of features is very important for long queries, in order to make sure documents only related to a small fraction of the query are not promoted too much.

The ontology features use the linear multiclass SVM im-plementation of SVMLight [15] as the classifier. The Free-base classes are the 100 most frequent (in FACC1 annota-tion) bottom categories in Freebase X  X  two-level ontology tree. The classes for MeSH are all of the top level categories in Table 3: Query-document and document quality features used by EsdRank and learning to rank baselines.  X  X eb X  and  X  X edical X  indicate the number of features in each corpus. MeSH X  X  ontology. The training data is the descriptions from objects of each class. Objects are classified by their descrip-tions. Documents are classified by their texts. Queries are classified using voting by the top 100 documents that Indri retrieves for them [5]. The accuracy of Multiclass SVM in cross-validating on training data is above 70%.

The graph connection features between Freebase entities and ClueWeb documents are calculated using Freebase X  X  knowledge graph. We treat all edges the same, leaving fur-ther exploration of edge type to future work. We only use the reachable at zero hop for OHSUMED, that is whether the MeSH term is annotated to the document.

Evaluation Metrics: We use ERR@20 and NDCG@20, which are the main evaluation metrics in the TREC Web Track ad hoc task. Besides these two metrics that focus on the top part of ranking, we also use MAP@100, which considers the quality over the entire reranked section.
Statistical significance was tested by the permuta-tion test (Fisher X  X  randomization test) with p-value &lt; 0 . 05, which is a common choice in IR tasks.

Hyper-Parameters and Training: We follow stan-dards in prior work to set hyper-parameters. The number of documents retrieved and re-ranked is set to 100. The same set of documents is used to generate document anno-tation AnnD . All supervised ranking models are evaluated on the testing folds in 10-fold cross validation. Cross-validation partitioning is done randomly and kept the same for all ex-periments. To suppress the local optima problem, in each cross-validation fold Latent-ListMLE is trained ten times with random initialization. The training result with the largest likelihood on training data is used for testing. In order to maintain reasonable ranking efficiency and avoid too many noisy objects, the number of related objects from AnnD and Osrch are restricted to at most 3. This restriction does not change AnnQ as none of our queries has more than 3 annotations.

Baselines: The first baseline is the Sequential Depen-dency Model ( SDM ) [23] approach which is widely recog-nized as a strong baseline. For ClueWeb datasets, we find that Dalton et al X  X  SDM results obtained with Galago 5 are stronger than our SDM results obtained with Indri, thus we use their SDM results as a baseline. We also use EQFE rank-ings provided by them as second baseline on ClueWeb data http://ciir.cs.umass.edu/downloads/eqfe/runs/ sets. On OHSUMED we use the Indri query language to ob-tain SDM results. EQFE is not included for OHSUMED as it is specially designed for knowledge bases. The third and fourth baselines are two state-of-the-art learning to rank baselines: RankSVM (pairwise) [16] and ListMLE (listwise) [27]. Their features are shown in Table 3 and are trained and tested exactly the same as EsdRank .

Three of our baseline algorithms were used widely in prior research; the fourth (EQFE) is included because it has important similarities to EsdRank. We could have in-cluded other methods that use external data, for example, Wikipedia for query expansion [4, 29], Wikipedia as one re-source for query formulations [1], and MeSH as an alternate document representation [22]. These methods mainly focus on using external data to better represent the query; they rank documents with unsupervised retrieval models. In re-cent TREC evaluations, supervised LeToR systems that use rich ranking features have shown much stronger performance than unsupervised retrieval systems. Thus we mainly com-pare with LeToR models with document ranking features, e.g., those in Table 3, which we believe are the strongest baselines available now.
This section first presents experiment results about Es-dRank  X  X  overall accuracy. Then it investigates the effective-ness of our Latent-ListMLE model, together with the influ-ence of related object quality on EsdRank  X  X  performance.
Tables 4a, 4b, and 4c show the retrieval accuracy of sev-eral baseline methods and three versions of EsdRank on ClueWeb09, ClueWeb12, and OHSUMED datasets. The three variants of EsdRank differ only in how related objects are selected.  X  ,  X  ,  X  and  X  indicate statistical significance over SDM , RankSVM , ListMLE and EQFE . The change relative to ListMLE is shown in parentheses. Win/Tie/Loss is the number of queries improved, unchanged or damaged as com-pared to ListMLE by ERR@20. The best performing method according to each evaluation metric is marked bold .
On all three data sets, the best EsdRank methods outper-form all baselines on all metrics. The gain over ListMLE varies from 3 . 33% (OHSUMED, MAP@100) to 21 . 85% (ClueWeb09, ERR@20), with 5 X 7% being typical. We note that ListMLE is a very strong baseline; there is little prior work that provides statistically significant gains of this mag-nitude over ListMLE on the ClueWeb datasets. These im-provements show the effectiveness of external data in rank-ing, which is EsdRank  X  X  only difference with ListMLE .
On ClueWeb data sets, all three versions of EsdRank out-perform EQFE , and the best performing version improves over EQFE by 10%-30%. Both EQFE and EsdRank use Freebase as external data and learning to rank models to rank docu-ments. The features between query and object in EsdRank are very similar with those used in EQFE . Why does EsdRank perform better than EQFE ?
One difference is in the object-document features. EQFE uses the language model or SDM score between an entity X  X  textual fields and the whole document to connect the entity to the document, while EsdRank uses a much larger feature set (Table 2). The object-document features in Table 2 pro-vide multiple different views of object-document relevancy, which has been shown very effective in LeToR research [19]. More object-document features also better propagate doc-ument relevance judgments to latent objects in Latent-ListMLE  X  X  EM training procedure, and help it learn better weights for query-object features. We have tried EsdRank with only language model scores between an object X  X  tex-tual fields and the document as object-document features, however, results with this smaller feature set are no better than EQFE . In Section 5.2, our second experiment also shows that our Latent-ListMLE model is another essential factor for EsdRank  X  X  performance with its ability to handle features in the two parts properly.

On ClueWeb09 and OHSUMED, query annotation ( EsdRank-AnnQ ) performs the best with statistically signifi-cant improvements on almost all evaluations over all base-lines, indicating query annotation is still the most effective in finding reliable objects. On ClueWeb12, EsdRank-AnnQ also outperforms all baselines, although less statistical sig-nificance is observed due to fewer training queries (only 50). It is interesting to see that EsdRank-AnnD performs better than EsdRank-AnnQ on ClueWeb12. This is consistent with results of EQFE [10], showing that FACC1 annotation might be of better quality for ClueWeb12 queries than ClueWeb09 queries. EsdRank-Osrch performs the worst on all three data sets. The reason is that object search provides related ob-jects that might be textually similar to the query, but are actually noise. The ranking models designed to rank docu-ment use assumptions that may not be suitable for objects, leaving room for improvement in future research.

The different effectiveness of EsdRank with different re-lated object selection methods shows the importance of re-lated object quality. It is also well recognized as one of the most essential factors in determining the usefulness of ex-ternal data in prior research [7, 11, 20, 28]. In Section 5.3, our third experiment studies the correlation between related objects X  quality and EsdRank  X  X  performance.
To investigate the effectiveness of Latent-ListMLE , in this experiment, we compare it with ListMLE , using the same external evidence, but with features generated by feature engineering techniques similar to those in Dalton et al. [10].
Formally, for each query q and document d i pair, we have objects { o 1 ,...,o j ,...,o m } , query-object features V = { v 1 ,...,v j ,....,v m } and object-document features U i = { u i 1 ,...,u ij ,....,u im } . Let | u | and | v | be the feature dimensions for query-object features and object-document features. We generate | u |  X  | v | features x { x combination of features in U i and V . The combination of k 1 -th feature in V and k 2 -th feature U i is defined as: x sponding features over all possible related objects. One may notice that if we only use language model scores as object-document features, this set up is exactly the same as EQFE . We name these  X  X lat X  features in comparison with Latent-ListMLE  X  X  latent space model.

We put these  X  X lat X  features into ListMLE and use the same training-testing procedure to evaluate them on our datasets. Evaluation results are shown in Table 5. ListMLE-AnnQ , ListMLE-Osrch and ListMLE-AnnD refer to the flat model with features from related objects selected by query an-notation, object search and document annotation respec-tively. Relative performance (in brackets) and Win/Tie/loss  X  0 . 156  X   X   X  ( 7 . 17 %) 0 . 190  X  ,  X  ,  X  ,  X  ( 21 . 85 %) 88/42/70 (  X  0 . 68%) 0 . 155  X  (  X  0 . 75%) 71/47/82 (0 . 19%) 0 . 167  X  ,  X  (6 . 70%) 75/36/89 (  X  7 . 29%) 0 . 114 (  X  7 . 48%) 16/13/21  X  0 . 123  X   X   X  (3 . 78%) 0 . 133  X  ,  X  ,  X  ,  X  ( 8 . 39 %) 20/12/18 (  X  3 . 88%) 0 . 121  X  (  X  1 . 26%) 22/10/18 ( 7 . 15 %) 0 . 132  X  ,  X  (7 . 67%) 24/14/12 ( 3 . 54 %) 0 . 511  X  ,  X  ,  X  ( 5 . 77 %) 56 / 16 / 34  X  0 . 16%) 0 . 500  X  ,  X  (3 . 59%) 44/20/42 values are compared with ListMLE , which uses the same model but query-document features. O indicates signifi-cantly worse performance than ListMLE in the permutation test ( p &lt; 0 . 05). H indicates significantly worse performance compared to the corresponding version of EsdRank . For ex-ample, ListMLE-AnnQ is significantly worse than EsdRank-AnnQ if marked by H .

Most times these flat features hurt performance, with sig-nificant drops compared to ListMLE and corresponding Es-dRank versions. Even on ClueWeb09, where the most train-ing data is available, it is typical for flat features to per-form more than 10% worse than EsdRank . These results demonstrate the advantage of using a latent space learning to rank model instead of a flat model to handle external evidence: By modeling related objects as hidden variables in latent space, Latent-ListMLE naturally separates query-object and object-document evidence, and uses the EM al-gorithm to propagate document level training data to the object level. As a result, Latent-ListMLE avoids feature ex-plosion or confusing machine learning algorithms with highly correlated features, while still only requiring document rel-evance judgments.
Prior research [7, 11, 20, 28] and our evaluation results on EsdRank with different related objects all indicate the great influence of selected objects X  quality on ranking accu-racy. The third experiment further studies its influence on EsdRank  X  X  performance. Although directly measuring the quality is hard due to the lack of object level labels, we can indirectly characterize the quality by comparing the related objects selected by different methods, with the hypothesis that objects selected by multiple object selectors are more likely to have higher quality.

For each variant of EsdRank , queries were divided into two groups ( Agree , Differ ) based on whether the query had at least one related object in common with another variant. The two groups were compared using the same methodol-ogy reported earlier. The results are summarized in Table 6. Each row is the performance of one variant (first column), divided by overlaps with another variant (second column). Relative gains, Win/Tie/Loss, and statistical significance (  X  ) are calculated based on comparison with ListMLE . Re-sults for ClueWeb09, ClueWeb12 and OSHUMED queries are combined due to space limitations and because their trends are similar.

The two groups clearly perform differently. On queries with common objects ( Agree ), EsdRank is much more effec-tive. For example, when the less effective AnnD and Osrch object selectors agree with the more effective AnnQ selec-tor, they both outperform ListMLE with statistical signifi-cance. When they differ, they may be worse than ListMLE . information but Latent-ListMLE ).
 H (  X  5 . 88%) 0 . 170 H (8 . 99%) 73/35/92 (  X  23 . 79%) 0 . 127 O , H (  X  18 . 34%) 47/42/111 (  X  10 . 63%) 0 . 155 (  X  0 . 61%) 64/38/98 (  X  21 . 04%) 0 . 098 O , H (  X  20 . 37%) 14/11/25 (  X  14 . 38%) 0 . 098 O , H (  X  20 . 65%) 14/10/26 O , H (  X  5 . 64%) 0 . 444 O , H (  X  7 . 94%) 39/15/52 O , H (  X  5 . 58%) 0 . 445 O , H (  X  7 . 84%) 38/15/53 Although AnnQ is the most effective individual method, its performance is further improved when it agrees with AnnD , outperforming ListMLE by more than 10% on all metrics. This level of improvement is close to the gains reported by Liu et al. [20] with manual query annotations.
 The results also show that EsdRank behaves predictably. When simple object selectors agree, EsdRank is more likely to improve retrieval quality; when they disagree, the search engine can simply retreat to ListMLE .

Finding related objects for query and document is still an open problem in the literature. In the SIGIR ERD X 14 workshop [7] many teams built their query annotators upon TagMe and obtained about 60% accuracy. It is still not clear how to adapt full text retrieval models to perform bet-ter object search. Document annotation can be done with very high precision but there is still room to improve recall, even in the widely used Google FACC1 annotations. Some prior research questions whether current Freebase query an-notation is too noisy to be useful [11, 20]. With the help of features between query, object and document, Latent-ListMLE is able to improve retrieval accuracy even when object selection is noisy. As researchers improve the quality of object search and annotation, producing less noisy ob-jects, we would expect Latent-ListMLE  X  X  ranking accuracy to improve further.
EsdRank is a general method of using external semi-structured data in modern LeToR systems. It uses ob-jects from external data, such as vocabularies and entities, as an interlingua between query and documents. Query-object and object-document relationships are expressed as features. Latent-ListMLE treats objects as latent layers be-tween query and document, and learns how to use evidence between query, objects and documents in one unified pro-cedure. This general method can be applied to a variety of semi-structured resources, and is easily trained using ordi-nary query-document relevance judgments.

Experiments with two rather different types of external semi-structured data  X  a classic controlled vocabulary and a newer knowledge base  X  and three well-known datasets show that EsdRank provides statistically significant im-provements over several state-of-the-art ranking baselines.
Experiments with the single-layer LeToR model, which uses exactly the same information but expressed by  X  X lat X  features, show much weaker performance than Latent-ListMLE. This result confirms the advantage of separating evidence about query-object and object-document relation-ships with the latent space, instead of mixing them together. The single-layer approach may result in a large and highly correlated feature space, which is hard for machine learning models to deal with.

Finding related objects for query and documents is a very important step for using external data in ranking. EsdRank is tested with three popular related object selection meth-ods: query annotation, object search and document annota-tion. The evaluation results demonstrate the essential effect of related object quality on final ranking accuracy, and sug-gest that query annotation is the most reliable source for related objects under current techniques.

EsdRank is a general approach in which to explore the use of different types of semi-structured data in search. As bet-ter methods are developed for retrieving objects from semi-structured data and annotating objects in queries and doc-uments, they can be used in EsdRank to improve retrieval and to cover a broader set of queries. Its use of objects as a latent space between queries and documents provides richer opportunities for feature and model development than seen so far in most learning to rank research. Thus, the imple-mentation of EsdRank presented here can be seen as just a beginning.
We thank Laura Dietz and the anonymous reviewers of a previous draft of this paper for comments that improved the paper. This research was supported by National Science Foundation (NSF) grant IIS-1422676 and a Google Research Award. Any opinions, findings, conclusions, and recommen-dations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors. Method AnnQ ( 12 . 89 %) 0 . 254  X  ( 24 . 18 %) 54/14/26 94 (6 . 69%) 0 . 347  X  (7 . 53%) 74/17/39 130 (4 . 28%) 0 . 238  X  (15 . 31%) 90/53/83 226 Osrch ( 4 . 02 %) 0 . 340  X  ( 5 . 37 %) 72/16/42 130 AnnD ( 7 . 74 %) 0 . 248  X  ( 21 . 19 %) 50/12/32 94
