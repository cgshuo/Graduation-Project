 A growing trend in commercial search engines is the display of specialized content such as news, products, etc. interleaved with web search results. Ideally, this content should be displayed only when it is highly relevant to the search query, as it competes for space with  X  X egular X  results and advertisements. One measure of the relevance to the search query is the click-through rate the spe-cialized content achieves when displayed; hence, if we can predict this click-through rate accurately, we can use this as the basis for selecting when to show specialized content.

In this paper, we consider the problem of estimating the click-through rate for dedicated news search results. For queries for which news results have been displayed repeatedly before, the click-through rate can be tracked online; however, the key challenge for which previously unseen queries to display news results remains. In this paper we propose a supervised model that offers accurate predic-tion of news click-through rates and satisfies the requirement of adapting quickly to emerging news events.
 H.3.1 [ Information Search and Retrieval ]: Indexing Methods; H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithms, Measurement, Performance, Experimentation
Searching for online news is one of the most important user ac-tivities in the context of the Web. As a consequence, there exist a number of dedicated news search engines and many of the major search portals offer a dedicated news search tab. Moreover, some search engines (such as Google or Live.com ) have started to mix dedicated news search results with the results displayed in the re-gular search pane (i.e., when the user has not selected the news tab). This interleaving of different search results introduces chal-lenges: because news and the  X  X egular X  search results compete for the available space, the display of news results should ideally be triggered only for queries with news-intent (i.e., queries for which a significant fraction of the people issuing them are actually looking for related news), but not for other queries, even if they contain key-Copyright 2009 ACM 978-1-60558-483-6/09/07 ... $ 5.00. words frequent in the news corpus. Hence, we require techniques that determine for which queries to surface news results.
The approach we use is to utilize the observed click-through rate (which we will refer to as CTR from here onward) on the news content as measure of whether news content should be displayed. If we can estimate this CTR for an incoming query, we can use this to trigger the display of news. While it is relatively easy to estimate the CTR for queries for which news results have been displayed a number of times already, the main challenge of estimating the CTR for the previously unseen queries remains. In this paper, we address this problem.
In order to motivate our approach, we will first illustrate some properties of CTR prediction for news. First, it is important to note that this is not simply a task of statically classifying queries into news and non-news categories. Many queries are inherently ambi-guous  X  for example, earlier in 2008 the search query  X  Georgia  X  was typcially either a request about information on the country (or the US state), or a request on news results regarding the conflicts on the Georgian border, with the relative frequency of these two query intents changing (sometimes rapidly) over time. This exam-ple illustrates the strong dependence between the time a query is issued and the likelihood of it resulting in a click on news results  X  any technique that cannot quickly adapt to events becoming  X  X ews-worthy X  is of little value.

Second, we argue that the CTR we observe for a given query (assuming that somewhat relevant news articles exist in our collec-tion) is not only a function of the relevance score of the displayed news article, but also strongly depends on additional factors such as the current  X  X uzz X  around a story, the age of the article itself, etc. For example, initial experiments on the correlation between CTR and the BM25 [12] score of an article showed little correlation; mo-reover, different articles surfaced for an identical news queries on adjacent days often exhibit very similar CTR, even when the re-levance scores of the articles themselves differ significantly. Our approach accounts for this by estimating the CTR at the level of a query (as opposed to a query-document pair), while using the rele-vance score of the highest-ranking document(s) as an input feature to this estimation. It is important to note that this does not mean that the quality of the returned documents is irrelevant: having a re-levant result document is still a pre-requisite to surfacing any news content.
In this section we discuss the applicability of several existing approaches to the problem of news CTR prediction.
 Web Document Ranking: Techniques used for document ranking in the web context cannot directly be applied to our scenario for a number of reasons. For one, as we illustrated above in the case of BM25, the predicted click-through rate is not a function of the score of the highest-rated document for a given query, ruling out these classes of ranking functions for our purposes. Also, techniques that leverage explicit relevance judgements (e.g., between a query and a given news article), which are obtained either directly (e.g., [4]) or implicitly via user-behavior (e.g., [2, 20]) are problematic in this scenario, because the underlying articles change so quickly. More-over, as pointed out in [8], newly posted news articles initially have very few links pointing to them, so techniques that use the linkage between pages (to, e.g., compute PageRank of an article) have very limited benefit in the news domain.
 Query Classification: Techniques commonly used in query intent classification cannot directly be re-used either. For one, classifiers that use the words appearing in the query X  X  text as main features (which have been shown to perform well for other query intent clas-sification tasks, e.g. [15]) are problematic given that news changes so rapidly. As illustrated in the case of the query  X  Georgia  X  above, the intent of a set of keywords can change quite rapidly, which in turn would require constant re-training of the underlying classifier.
Moreover, the acquisition of training data itself would be diffi-cult  X  humans may be good judges of whether or not a query is related to news, but they can not estimate a query X  X  CTR well. To illustrate this, we use query logs and click-through data obtained from Live.com for the time interval from November 13-15 2008. Consider the queries  X  Caylee Anthony  X ,  X  voter registration  X  and  X  oil price  X , all of which are closely related to events receiving signifi-cant news-coverage during the days the logs were generated, mea-ning that a reasonable human annotator might label them as  X  news-intent  X . However, the true click-through rates seen in the log files with respect to news results differ significantly between queries: while the query  X  Caylee Anthony  X  results in clicks on news articles at rates between 63%-69% (depending on the individual day), the click-trough rate for the query  X  oil price  X  lies between 22%-29% and for  X  voter registration  X  between 1.6%-5%.

To illustrate the variations in CTR for different news queries, we have plotted the rates observed for a sample of queries for which news-results were surfaced on November 7th, 2008 in Figure 1. Here, the x-axis corresponds to the number of times news results were displayed for the query in question (in logarithmic scale), and the y-axis to the observed CTR. The circles in the plot show the overall distribution, with a number of them labeled with the corre-sponding query text. Note that not only do the click-through rates for different news queries vary significantly, but also do some que-ries (such as  X  facebook  X ) which  X  judging from the query terms  X  do not appear to have news intent actually receive some clicks on news results.
 Online Models: Tracking CTR in an online manner via time series models is a well-studied area (e.g. [25, 1, 9]); however, applying any of these methods in our problem context would require collec-ting initial click-through data for queries by displaying news results for them, which  X  given that the vast majority of web queries do not have news-intent  X  would mean large numbers of irrelevant search results being displayed. Instead, our aim is to only display news re-sults for a query when we have high confidence that this will lead to clicks on the news results.
The characteristics of the problem scenario described above mean that any solution has to satisfy a number of properties: 1. Fast adaptation to changing news events. 2. Prediction of actual CTR-values (as opposed to binary news/no-3. Prediction of CTR for queries for which no news results have 4. Because the CTR has to be predicted online, as queries arrive
Our approach adopts a supervised learning framework to address these challenges. To generate features for the learning algorithm, we leverage the following observation: keywords that are associa-ted with news events are typically reflected in current corpora of news and blog articles. For example, when a news event occurs, keywords describing it are frequently found in titles of news artic-les published around that time. The same holds  X  to a lesser degree  X  for the word combinations in the initial paragraphs of news or their full text. Moreover, the change in the frequency of keywords in these corpora over time in itself is of interest; as we will show later, news events are often associated with spikes in keyword fre-quency. Finally, the context these words occur in within these cor-pora can itself give us hints as to whether a query characterizes a very specific news event or only contains keywords that happen to occur across many different news stories.

Hence, we will generate the features we use to predict click-through rates by analyzing the frequency and location of keywords in large corpora of news articles, blog postings, etc. By keeping these corpora current, we are able to adopt to breaking news events and changing levels of interest in older ones. For this purpose, we leverage dedicated news and blog crawling engines [18]. The corre-sponding corpora are ordered by time (using the date the individual articles were posted or crawled), allowing us to detect temporal trends in the frequency with which query terms occur. Because we keep only a small window of historic news and blog articles (7 days in our experiments), the underlying text corpora and required index structures are small enough to fit into the main memory of a single server, allowing for fast in-memory feature extraction as queries are processed.

The remainder of the paper is organized as follows: we first de-scribe related work in Section 2. We then discuss the specific fea-tures, their extraction and the required overall architecture in Sec-tion 3. In Section 4 we then describe the model and training al-gorithm we use. We evaluate our approach in Section 5 and then conclude and describe next steps in Section 6.
The prior work closest to our approach is [9], which studies the integration of news and web search results by means of click-through prediction. Unlike our approach, [9] proposes a model that leverages both online click feedback as well as  X  X ontextual X  featu-res quantifying the frequency with which query terms occur in que-ry logs as well as the news corpus over time. Our approach differs in both the features and the machine-learning framework proposed and uses additional corpora (such as a blog crawl and Wikipedia) to infer information about  X  X uzz X  around specific news items and background frequencies of query terms.

An area related to our problem scenario is distributed informati-on retrieval [5], where a search query is distributed among a num-ber of independent text databases. The retrieval of news content can be thought of as a distributed IR task with only two databases  X  the web and the news corpus. However, our scenario differs in a number of aspects: for one, as discussed earlier, the CTR we seek to predict is not solely a function of the relevance of the news con-tent, which is the function optimized by current distributed retrie-val systems. Moreover, the web and news retrieval systems used in practice are themselves very different in size, architecture and fea-tures used for ranking, making it difficult to adopt distributed IR approaches in our scenario.

There has also been considerable work in the area of event/topic detection and tracking (TDT) (e.g., [3]); event detection systems identify stories that discuss an event that has not been seen ear-lier and track occurrences of the events found across stories. While TDT cannot be used for CTR prediction directly, it may be possible to leverage their output for features in CTR predictors.

CTR prediction is an important area of research in computational advertising, in both the areas of sponsored search as well as con-textual search . In sponsored search (e.g. [21, 22]), where CTR is commonly modeled on the level of individual advertisements (un-like our approach, where the estimation is tied to the query), a si-gnificant challenge is the CTR estimation and resulting ranking of ads which have been shown only few times or not at all. The aut-hors of [22] adopt a supervised framework to address this challen-ge; both the proposed learning algorithm as well as the features are very different from our approach.

In contextual search (e.g., [7]) the input to the CTR prediction is the combination of the page an ad is to be displayed on and the ad itself (i.e., there is no notion of query); the prediction is typi-cally made on the basis of word-overlap between the advertisement (i.e., the bid phrases) and the page itself (in addition to supplemen-tal features). The approach taken in [7] is similar to our work in the sense that both ads and pages are divided into different regions (title, body text, abstract, etc.). Unlike [7], however, nearly all of the features we propose are aggregates in the sense that they enco-de the properties of words/phrases relative to a large corpus of text as opposed to individual documents. Also, as before, the learning algorithm we adopt is very different.
In order to determine whether a query is likely to result in clicks on news-results, we designed a number of features to produce evi-dence for or against the news-relatedness of a query. We differen-tiate three classes of features. First, a small set of features is com-puted directly from the query text, without consulting any of the blog or news corpora; we refer to these as query-only features . All of the remaining features consult various background corpora to find distributional information about the query terms. We further differentiate between query-context features that are based on the textual context the query keywords occur in and corpus frequency features , which track the number of times query terms occur in the underlying corpora over time. Finally, we also use the BM25 score for the top-scoring document as a feature in our classifier.
One important consideration for features is the extraction over-head they require. Because the features need to be computed online, as queries are submitted, we restrict our feature set to features we can compute online without resulting in unacceptable latency.
When trying to determine the intent of a query, and specifically whether it is about current news and events, it is crucial to have a notion of what the current events are. To capture changes in query-intent resulting from recent news events, we use different back-ground corpora to capture the distributions of news events that are drawing attention at any point in time.

The first corpus we use is a corpus of news articles, extending from the time the query was issued to several days in the past. As a second corpus we use a large set of blog posts crawled over the same period of time. User-generated content in blogs has skyrocke-ted and continues to grow at a fast pace [23, 24]. By nature, blogs tend to be about current trends and events and provide information about the current  X  X uzz X  around topics and events, giving us a better handle on which subset of articles are drawing significant attention. Finally, we use Wikipedia as a  X  X ackground corpus X  that contains relevant information, but is not geared towards current events. This corpus can provide additional evidence when compared to blog and news data: if query terms are salient in Wikipedia, but not salient in recent news and blog posts, the query is more likely to be a general information query than a news-related query.

The news and blog corpora are acquired using dedicated news and blog crawling code; in case of blogs, the crawler uses the ping/feed mechanism of blogs to identify new blog posts in a timely manner, combined with additional parsing and crawling of the permalink found in the feed to deal with partial feeds. A detailed description of the blog post acquisition architecture is given in [18].
Current events are constantly in flux, and temporal changes in the frequency of terms can indicate the emergence of a news story. We found in our experiments that news and blog corpora complement each other in that some emerging news stories will result in  X  X ey-word spikes X  in one corpus, but not the other, depending on a given topic, which makes blog posts an interesting and complementary source of information.

To give some examples, consider the charts in Figures 2-4. Figu-re 2 shows the distribution of the word Galveston across news ar-ticles, news titles and blog posts. As hurricane Ike approached the Texas coast, there was a strong spike in the frequency of the word Galveston (an area of the Texas cost hit by the hurricane) across all three sources. The word Caylee ( Caylee Anthony was a missing child in the news at the time) shows a very pronounced spike in blog posts on Sep 11, while news articles and titles during the same time do not exhibit much fluctuation (Figure 3). Upon closer inspection of the data, a likely explanation is that the news at the time was do-minated by hurricane stories, and there were relatively few stories about Casey Anthony X  X  (the child X  X  mother) impending re-arrest. Given the popular interest in the story, however, blog commentary about this piece of news spiked sharply. A reverse example, where there is a strong signal in news but not in blogs can be seen in the distribution of the word Pakistan in Figure 4, where an event gene-rated significant coverage in news media, but not so much in blogs.
Figure 2: Occurrences of the term  X  Galveston  X  in different corpora
Figure 3: Occurrences of the term  X  Caylee  X  in different corpora
The first set of features we use quantifies the number of docu-ments that match the query using (a) set containment semantics and (b) phrase semantics in the news and blog corpus (i.e., how many documents in the corpora contain all the words in the query and how many contain the query terms adjacent and in the same order as in the query): for each corpus, we use both of these counts as well as their difference as features. For the news data, we further subdivide each article into the article title and the full text of the article; for each of these regions we collect the counts separately, and use them as distinct features. Finally, each of these counts is broken down by date  X  we collect separate counts for each date, going back 7 days into the past.

In order to assess the  X  X alience X  of query terms in the three cor-pora, we use simple tf.idf -based [17] metrics. The tf.idf value of a token is calculated over the whole corpus as td.idf ( token ) = 1 + log( tf ( token )) where tf ( token ) is the count of the token in the collection, df ( token ) is the count of documents containing the token, and N the total number of documents. For news articles, we calcu-late tf.idf based on three distinct parts of the articles: news tit-les, the first 30 words of the news article, and full body of the news article. The first 30 words are meant to be an approximati-on of the first paragraph of a news article, which typically con-tains a synopsis of the content of the article. In one set of fea-tures, we average the tf.idf values over all terms in a query and represent it in the following features: AvgNewsBodyTfIdf, AvgNew-sTitleTfIdf, AvgNewsFirstParagraphTfIdf, AvgWikipediaTfIdf . In a second set of features, we sum the tf.idf values for all terms in the query: NewsBodyTfIdfSum, NewsTitleTfIdfSum, NewsFirstPa-ragraphTfIdfSum, WikipediaTfIdfSum .
One indicator of CTR we found in our experiments was the cohe-rence among the documents surfaced by a query. Queries that sur-face a number of very similar documents are more likely to refer to
Figure 4: Occurrences of the term  X  Pakistan  X  in different corpora
Table 1: Examples of phrases with different  X  X ontext-coherence X . one specific news event, in turn making high CTRs more likely. On the other hand, query terms that appear in a large number of very different news stories (e.g.  X  NY Times  X ) are less likely to have high CTR. In order to measure the coherence of mentions of query terms in a corpus, we use a modification of the distance measures defined in [6] to model the difficulty of a given retrieval topic: we define a context as a text window of 10 tokens to the right and left of a an occurrence of a query phrase. We then take a sample of 50 contexts per query, and calculate the overall  X  X ohesion X  of these contexts. A simple but effective measure for cohesion is to look at the term vectors of the contexts and calculate a distance or similarity metric between the vectors. We use Jensen-Shannon divergence [16] and Cosine Similarity as distance measures. The features based on this notion are AvgNewsJS and AvgBlogJS and similarly, AvgNewsCo-sine and AvgBlogCosine . To illustrate the effect of this measure measure of coherence, we provide some examples of queries at the extreme ends of high and low AvgNewsJS values in table 1.
Query-only features capture information about the makeup of the query terms themselves. The first and obvious feature is que-ry length in tokens. The presence of stop words (typically func-tion words) is addressed by a feature measuring the ratio of stop words to query length. Some queries contain non-alphabetic cha-racters, which may indicate a non-news related query. We compute the count of special characters in a query and the ratio of speci-al characters. Navigational queries often consist of URLs or frag-ments of URLs ( X  www.google  X ). We therefore determine whether a query is a URL through a series of regular expressions. In addition, we simply detect whether the query contains the string  X  www  X .
Finally, capitalization of the terms that appear in a query can in-dicate named entities ( X  Casey Anthony  X ) as opposed to common nouns. However, when entering queries in a search box, users only rarely add capitalization; instead, we test whether or not the terms occurring in a search query are typically capitalized when they oc-cur in the news corpus. This is done as a pre-computation step, i.e. as the news corpus is updated, we pre-compute a list of terms and phrases that are commonly (  X  90% of the time) capitalized in the corpus. This can also be viewed as a very simple approximation to named entity detection. We use the number of words in the query that are typically capitalized, the ratio of such words to the que-ry length, and the presence of a sequence of typically capitalized words as features in our classifier.
In general, all data we use for our approach resides in main me-mory; for feature extraction we retain the text in the corpora itself, inverted indices on the corpora, as well as some word-level stati-stics (which can be encoded as part of the inverted index itself), plus a small set of pre-computed lists (such as stop-words , etc). Over time, the inverted indices are updated through a background process as new documents are crawled. We maintain blog and news corpora for a window of 7 days into the past.
 Corpus Frequency Features: Here, we need to differentiate be-tween the tf.idf -based statistics for individual words and the word-set/phrase counts we collect for different corpora. Regarding the former, we are able to maintain the required term frequency and document frequency statistics for each word in the vocabulary as part of the corresponding word X  X  entry in the inverted index.
Computing the word-set counts (for multi-keyword queries) re-quires answering partial match queries (also known as containment queries), which are inherently expensive (e.g. [11]); in fact, the count features for word-sets or phrases are the only features who-se computation induces non-negligible overhead. To compute the counts for sets of query terms, we use the inverted index intersecti-ons for the indices corresponding to the query terms. Because in our scenario all required corpora can fit into main memory on a single server, the cost of these queries is orders of magnitude lower than for disk-based indices; however, if the resulting latency is still too large, it can be reduced further by approximating the intersection sizes (see [14]) or simply using a smaller sample of the documents in each corpus. We evaluate the effect of using a subset of docu-ments on the accuracy experimentally in Section 5.4. By encoding positional information together with the postings within the inver-ted index, we also compute the phrase counts using the inverted index intersections.
 Query Context Features: Once we have computed the phrase count features, we can compute a sample of the contexts each phrase oc-curs in using the result of the inverted index intersection and ex-tracting the context directly from the corresponding articles/posts. Because the corpora are kept in main memory, this extraction does not cause significant additional overhead. Moreover, we only use a small number of contexts (50 in our current implementation) for these features.
 Query-Only Features: Most of these features can be derived quick-ly from the query itself; the only exceptions are the stop-word fea-tures for which we match the individual words in the query against a small hash-table of stop-words and the features dealing with ca-pitalization. For the latter, we periodically compute a list of capita-lized words or phrases via a background process that iterates over the current corpus. Because the capitalization of terms does not change quickly over time, this process does not need to be trigge-red frequently and does not constitute a performance issue. Once we have computed all such phrases, we need to match all subsets of words in a query against this phrase table, which corresponds to an common indexing problem in advertisement matching and can be addressed efficiently via the data structures described in [13].
The number of documents in our news crawl for a single day va-ries between 103K and 184K documents, with an average of 146K documents. We retain a window of 7 days of news into the recent past and retain only the news text from each article. For the blog corpus, we use blog documents posted on the same range of dates; the number of documents crawled for a single day vary between 31K and 97K, with an average of 75K documents.

Both the blog and news corpora are divided into separate sub-corpora, each of them containing articles/posts that were publis-hed/posted on a specific day; the news corpus is further subdivided into headlines, first paragraphs and complete text of the articles. We maintain separate inverted indices for each corpus; in additi-on, global statistics on word/document counts are maintained on a single-word basis (for the current 7-day window). Given that the news articles average slightly over 3K characters per document (the blog articles are shorter), a 7-day window of articles can easily fit in main memory on a 16GB server. As was shown in [19], inver-ted indices that uses a simple (and standard) gap-encoding scheme require space proportional to the size of the compressed text of the underlying corpora, which implies that the index structures requi-red for feature extraction typically require significantly less space than the text corpora themselves.
In this section we will a give short overview of the learning mo-del used in our approach; a more detailed description can be found in [26]. The learning method we use for CTR-prediction is based on Multiple Additive Regression-Trees ( MART ). MART is based on the Stochastic Gradient Boosting paradigm described in [10] which performs gradient descent optimization in the functional space. In our experiments, we used the log-likelihood as the loss function (optimization criterion), used the steepest-decent (gradient descent) as the optimization technique, and used binary decision trees as the fitting function -a  X  X onparametric X  approach that applies numeri-cal optimization in functional space.

In an iterative boosting (or residue-learning) paradigm, at the be-ginning of every iteration, the click probabilities of the training da-ta are computed using the current model. The click prediction is compared with the actual click outcome to derive the errors (or residuals) for the current system, which is then used to fit a resi-due model  X  a function that approximates the errors  X  using MSE (Mean Square Error) criteria. In MART, we compute the derivatives of the log-loss for each training data point as the residual and use the regression tree as the approximation function-residual model. A regression tree is a binary decision tree, where each internal node splits the features space into two by comparing the value of a cho-sen feature with a pre-computed threshold; once a terminal node is reached, an optimal regression value is returned for all the data falling into the region. Finally, the residual model is added back to the existing model so that the overall training error is compensa-ted for and reduced for this iteration. The new model  X  the current plus the residual model  X  will be used as the current model for the next boosting/training iteration. The final model after M boosting iterations is the sum of all M regression trees built.
 Properties of MART: MART does have a number of important properties that are crucial for our scenario. In addition to providing accuracy superior to all other models we tried, MART is able to handle the diverse sets of features we have proposed in the previous section. For one, it does not require transformations to normalize the inputs into zero mean and unit variance which is essential for other algorithms such as logistic regression or neural nets. More importantly, by its internal use of decision trees, which are able to  X  X reak X  the domain of each feature arbitrarily, it is able to hand-le the non-linear dependencies between the feature values and the CTR and without using explicit binning as a preprocessing step.
MART also computes the importance of a feature by summing the number of times it is used in splitting decisions weighted by the MSE gain this split has achieved. The relative importance of a feature is computed by normalizing its importance by the import-ance of the largest feature, i.e., the most important feature will have the relative importance of 1 and and other features will have rela-tive importance between 0 and 1. The relative importance of input features makes the model interpretable -helping us gain an under-standing of the input variables that are most influential to the final decision and the nature of the dependency on these features.
In addition to MART, we also tested different learning algo-rithms (using Logistic Regression and Averaged Perceptron mo-dels) on the same data, none of which produced comparable accu-racy. We omit the details of this study due to space considerations. 1: F 0 ( x ) = 0 2: for m = 1 to M do do 3: for i = 1 to N do do 5:  X  y l = y i  X  p i 6: w i = p i  X  (1  X  p i ) 7: end for 8: { R lm } L 1 = L  X  Terminal node tree 9: r lm = X 10: F m ( x i ) = F m  X  1 ( x i ) + v X 11: end for The MART Algorithm: The above pseudo-code summarizes the MART algorithm; it assumes there are N total query-impressions (i.e., there were a total of N results shown ) in our training set and that we wish to train M stages (trees). The training data is a set of input/output pairs { x i , y i } , i = 1 . . . N , where x vector of a query and y i is 1 if a query impression resulted in a click on news results and 0 otherwise. M rounds of boosting are perfor-med, and at each boosting iteration, a regression tree is constructed and trained on all queries. Step 1 initializes the functional value of all data points to 0. Step 4 computes the probability of the query being clicked from its functional value. Step 5 calculates the log-likelihood gradients for query. Step 6 calculates the second-order derivative. A regression tree with L terminal nodes is built in step 8, using the Mean Squared Error to determine the best split at any node in the regression tree. The value associated with a given leaf of the trained tree is computed first as the mean of the gradients for the training samples that land at that leaf. Then, since each leaf cor-responds to a different mean, a one-dimensional Newton-Raphson line step is computed for each leaf (Step 9). Finally, in Step 10, the regression tree is added to the current boosted tree model, weigh-ted by the shrinkage coefficient v , which is chosen to regularize the model. MART has three main parameters: M , the total number of boosting iterations, L , the number of leaf nodes for each regression tree, and v , the  X  X hrinkage coefficient X  -the fraction of the optimal line step taken. Using a shrinkage coefficient with value less than one is a form of regularization [10].

We further injected randomness in MART to improve the robust-ness of the resulting model. In our experiments, the most effective method is to introduce randomness at the node level. Before each node split, a sub-sample of training data and a sub-sample of fea-tures are drawn randomly. Then, the two randomly selected sub-samples, are used to determine the best split.
In this section, we present an experimental evaluation of our techniques. First, we describe the characteristics of the training and test data sets we used in our experiment (Section 5.1). Subsequent-ly, we describe the overall accuracy of our approach and the import-ance of different classes of features (Section 5.2). We assess how well our CTR-prediction generalizes to a corpus of general web queries (as opposed to the corpus of mostly news-related queries for which we were able to observe click-through rates) in Secti-on 5.3. Because the size of the underlying news and blog corpora is the deciding factor in the cost of feature extraction, we analyze the effect of reducing their size through sampling in Section 5.4.
To evaluate our approach, we use CTR data obtained from Li-ve.com . The data contains web search queries for which news re-sults were displayed and  X  for each individual impression the in-formation whether a click on the news result occurred or not. By aggregating this data for each query, we can derive the observed CTR for each query and compare it to the estimate given by our technique. Obviously, the value of the  X  X bserved X  CTR depends on at which granularity we aggregate the data; for our experiments we measure the CTR at the level of days, corresponding to the 24-hour news cycle. We remove any queries for a given day from the da-ta set for which there were not at least 50 impressions during the day, since the small number of impressions makes it impossible to assess the underlying  X  X rue X  CTR with any degree of confidence. This may bias our initial results towards popular queries; we will discuss experiments on general web traffic in Section 5.3.
The training/test data we collected contains a total of 2.7 million impressions, covering 5 days in September 2008. All of the news results shown for the queries in our data set were shown in the same position within the search engine, i.e., we do not have to adjust for the effect of result position on CTR. Note that this training set exhi-bits bias towards news queries: because it only contains queries for which the search portal surfaced news results, it does not reflect the overall distribution of web queries. In particular, it contains very few queries that have no news intent (and hence would have zero or almost zero CTR), which, however, is common in normal search traffic. To address this, we adopt two approaches: (1) We use manu-al analysis of a web search query log to identify a set of queries for which there was no news intent; these were then added to the trai-ning and test sets with a CTR of 0 (we will refer to these queries as the 0-CTR queries subsequently). These queries account for a total of 2.6 million impressions, meaning that our training and test da-ta contain about half non-news queries and half queries for which news results were shown by the search engine. (2) In addition, we explicitly evaluate how our estimator performs on a large sample of web-queries obtained from a search query log in Section 5.3.
To avoid any test/training contamination, we keep the test and training sets strictly separate with respect to queries (i.e., no instan-ce of a query/day combination can appear in both). When training MART, we used the parameter settings of the number of iterations M = 600, the number of leaf nodes in an individual decision tree L = 5, and the shrinkage coefficient v = 0.1.
In this section we evaluate the overall accuracy of CTR estima-tion and the contributions of individual groups of features. As a measure of performance we will use the log-loss on the held-out test set. In addition, to give a better intuition for the quality of our approach in practice, we measured the accuracy of our approach when used to estimate whether a given query X  X  CTR (in the test set) exceeds a certain threshold t . This is a key metric for a sce-nario where we only surface news results whose CTR exceeds this threshold. We have plotted the resulting precision/recall curves for t = 10% , t = 15% and t = 20% in Figure 5. For larger target thresholds the precision is very high; for example, for a threshold of t = 20% , the precision over the entire test set is 90 . 9% ; trading off precision against recall, we can achieve 99 . 1% precision at 61% recall and 99 . 7% precision at 50% recall. Given that the perceived penalty for a  X  X alse positive X  (i.e., showing irrelevant news results for a non-news query) is much larger than for a  X  X alse negative X  (i.e., not showing news results for a query with news-intent), tra-ding off recall for precision is important in practice. Overall, we observe that the proposed technique already achieves sufficient ac-curacy on the test data to be relevant for practical use. To put these numbers in perspective, the baseline probability for predicting CTR &gt; 20% is 81.8%, the baseline for predicting CTR &gt; 15% is 75.9% (our predictor is correct 87.9% of the time) and for predicting CTR &gt; 10% is 70.1% (our predictor is correct 82% of the time).
We approximate the CTR of 82 . 5% of all queries within an  X  X rror-band X  of +/-10% of their true CTR (i.e. the absolute difference in true CTR and estimated CTR is less than 0 . 1 ) and 59 . 8% of all queries within +/-5% of their true CTR. Precision Precision Figure 5: Precision/Recall for predicting different CTR threshholds
To measure the importance of different corpora and sets of featu-res we conducted ablation experiments in which we removed some corpora (and hence the features based on them) from considera-tion. For this experiment, we considered the overall log-loss on the test data for (a) removing the news-based features, (b) removing the blog-based features, (c) removing the Wikipedia-based featu-res and (d) removing the context features from the full feature set. The results are shown in Figure 6; the Wikipedia-features have the least impact on accuracy, whereas the news and blog features have the most. Moreover, both the news and blog corpus detect different underlying trends and appear to complement each other.
We also measured the relative importance of different features directly using the measure of feature importance described in Sec-tion 4. The top-ranking feature was the binary encoding of whe-ther a query corresponded to a URL; this was mainly due to the fact that our 0-CTR contained a large number of navigational que-ries. The next-most important features were (in order): the averaged tf.idf scores of the query terms relative to the Wikipedia corpus, the BM25 score of the top-ranking document, the sum and average of the tf.idf scores of the query terms relative to the news corpus, the number of times the query terms occurred in news titles one and two days prior to the query being issued and finally the Jensen-Shannon divergence of the query term X  X  context in the blog corpus. Features leveraging all 3 corpora (Wikipedia, news and blogs) were present among these top-ranked ones. Also, while the BM25 score is in itself not sufficient to predict click-through rates, it did turn out to be a very important feature when combined with the corpus-based ones. We also found that context-features based on Jensen-Shannon divergence consistently ranked higher than ones based on Cosine similarity.
As we pointed out earlier, the training/test data sets we used in the previous evaluation are biased towards news-related queries (when compared to general web search traffic), even after we manu-ally added the 0-CTR data. However, since our CTR-prediction is aimed at general web search traffic, it is crucial that it also performs well for non-news queries. In order to assess how well our approach generalizes to this type of queries, we first trained a CTR predictor using only the part of training data based on observed click-through data (i.e., without the batch of 0-CTR data). Then we took the 10K search queries most frequently issued against Live.com on Septem-ber 9th, 2008, and ranked the queries by the estimated CTR. The resulting CTR-estimation was then (because of the absence of any observed CTR for nearly all queries) manually evaluated.

To give a quick overview of the result, we have plotted the top and bottom 15 queries in Table 2. All of the top queries refer to an event that generated significant news coverage at the time, whereas none of the bottom 15 do. In fact, every single one of the top 50 queries in the log corresponded to a  X  X ews event X  or the name of a person generating news coverage on that day. To contrast this with the remainder of the search query distribution we manually analy-zed 100 queries each starting at the rank-positions (according to the estimated CTR) 1000, 3000 and 5000. Among these 300 queries, we found only 3 queries referring to a recent news event (2x in the block starting at 1000 and 1x in the block starting at 3000) and 5 further mentions of names from the area of entertainment for which we were unable to determine if they had been involved with a news event around the date the logs was taken.

Overall, it appears that our CTR prediction does indeed genera-lize to the distribution of frequent web queries we studied. Given the difficulty human annotators face in predicting click-through ra-tes, this evaluation can naturally only serve as an indicator, but was the best we could do in absence of any observed click-through da-ta. We performed the same experiment for two additional subsets of the query log, using the queries ranked (by their frequency) bet-ween 20K-25K and the queries ranked between 100K-105K. The results were similar to the ones described above.
In this experiment we examined the need for using the entire news and blog corpus; here we ran experiments using features sets which were extracted from random, document-level samples of va-
Table 2: Queries with most/least estimated CTR in web query log. rying size of the underlying corpora. The results are shown in Fi-gure 7. Retaining only 10% of the document corpus has limited impact on accuracy (even less than e.g., removing the Wikipedia-based features), but in turn means a reduction of approximately 10x in the main memory requirement and also a significant decrease in the feature computation costs. Any further reduction in size does, however, significantly impact accuracy.
In this paper we describe a new approach for deciding when to surface news context as part of  X  X egular X  search results. We propo-se a supervised framework based on decision trees and boosting, which leverages features that characterize properties such as con-textual coherence, temporal spikes and occurrence frequency of the query keywords relative to large text corpora. By continuously up-dating the underlying news and blog corpora via dedicated craw-lers, this framework allows us to keep up with and adapt to emer-ging news stories. We found that this framework allows for highly accurate CTR prediction. Here, the use of both news as well as so-cial media data (i.e., blogs) is important to the overall accuracy; both corpora have significant impact and complement each other.
As next steps, we intend to examine the importance of additional data sources as well as further differentiation of the existing corpo-ra; for example, we currently view blogs as a single corpus, without differentiating between news, entertainment and IT blogs/posts. Al-so, it remains to be seen how much more complex features, which require more computational overhead (such as context-features le-veraging occurrences of multi-word sub-phrases), can improve ac-curacy further and which index structures are needed to support them efficiently. We are very grateful for the help and advice of Mikhail Bilenko, Nick Lester, Roger Menezes and Momo Jeng.
