 Steffen Gr  X unew  X alder 1 steffen@cs.ucl.ac.uk Guy Lever 1 g.lever@cs.ucl.ac.uk Luca Baldassarre l.baldassarre@cs.ucl.ac.uk Massimilano Pontil m.pontil@cs.ucl.ac.uk Arthur Gretton  X  arthur.gretton@gmail.com CSML and  X  Gatsby Unit, University College London, UK,  X  MPI for Intelligent Systems 1.1. Preliminaries Throughout we denote expectations by E [ ], and the probability over events by P ( ). We denote by B ( X ) and C b ( X ) the Banach spaces of bounded functions and bounded continuous functions on X , each equipped with the sup-norm ||||  X  .
 We consider in particular the problem in which we control a trajectory { x t }  X  t =0 over X by sequentially choosing actions a t  X  X  at each time step t  X  0, once x t is revealed, after which we receive a reward r t +1 = r ( x t , a t ). We denote a set of deterministic policies  X  = A
X . The objective is to find a policy  X  which max-imises the expected sum of rewards obtained by follow-ing  X  : E [ For a policy  X   X   X  we denote the associated value function , V  X  ( x ) := E and recall that V  X  ( x ) = r ( x,  X  ( x )) + value function V  X  ( x ) := max  X   X   X  V  X  ( x ) for all x  X  X , and an optimal policy to be any  X   X  such that  X   X   X  argmax action-value function Q : X  X A  X  R we define the greedy policy w.r.t. Q by  X  Q ( x ) := argmax a  X  X  Q ( x, a ) (choosing arbitrarily in the case of a tie) and the optimal action-value function, so that  X   X  =  X  Q  X  (see e.g. (Szepesvari, 2009) for this background). We require the following well-known re-sult, which is proved in the Appendix for reference (Gr  X unew  X alder et al., 2012): Lemma 1.1. (Singh &amp; Yee, 1994)[Corollary 2] For any action-value function Q : X  X A  X  R , the greedy policy  X  Q satisfies || V  X  Q  X  V  X  ||  X   X  2 1  X   X  || Q  X  We are interested in the case where P is unknown i.i.d. from a distribution e P such that e P ( X  X  i = x  X  x abilities need not match). Note the abuse of notation here  X  subscripts index samples and not time steps. 1.2. Overview of the approach A number of recent studies have focused on efficient evaluation of conditional expectations on functions that are  X  X ell behaved X  in the sense that they be-long to a reproducing kernel Hilbert space (RKHS). These approaches have been particularly successful in performing inference in graphical models, where the model parameters are learned nonparametrically from data (Song et al., 2010b; 2009; 2011). The key insight in these works is that conditional probabilities can be represented as functions in an RKHS, called condi-tional distribution embeddings . The conditional ex-pectation of any function in the RKHS then becomes a linear operation, where we take the inner product with the appropriate distribution embedding. Many methods for solving problems in MDPs require the computation of expectations of functions (value functions for example) with respect to transition dy-namics, and so (approximations of) the operators are required. A direct but computationally costly ap-proach would be to first learn a conditional density es-timate (difficult in high dimensions), followed by (pos-sibly intractable) integrals to compute the expecta-tion. By contrast, our approach is a two stage process for learning in MDPs: we first use the theory of RKHS embeddings to estimate the operators (2) directly (over a specific class of functions in an RKHS), then use these estimated operators in standard approaches for solving MDPs  X  here we consider dynamic program-ming methods for value estimation and policy optimi-sation. The application to dynamic programming is described in more detail in Sec. 3. 1.3. Advantages of the approach A direct kernel-based approach has a number of ad-vantages. First, like density estimates, conditional embeddings can be learned from a training sample: we do not need to address the problem of model-ing system dynamics, such as the differential equa-tions governing a robot arm. Unlike density esti-mates, however, distribution embedding estimates do not scale poorly with the dimension d of the underlying space: the risk of a kernel density estimate increases as O ( m  X  4 / (4+ d ) ) when the optimal bandwidth is used (Wasserman, 2006)[Sec. 6.5]. By contrast, the rate of convergence for conditional mean embeddings is in-dependent of the dimension of the underlying space (Song et al., 2010b)[Thm. 1].
 Second, the solution to many control problems in-volves computation of high dimensional integrals to obtain expectations, which is prohibitively costly. By contrast, RKHS embeddings explicitly provide a rep-resentation of the expectation operator as an RKHS inner product, which reduces calculating expectations to a computation of linear complexity in the number of training points used to represent the embedding, and avoids any intermediate problems such as density estimation and sample selection for numerical integra-tion. Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (par-ticularly if, for example, sparsification of the embed-ding is considered, which we address briefly in the Ap-pendix (Gr  X unew  X alder et al., 2012)). The conditional distribution embeddings themselves may be computed exactly at cost cubic in the training sample size, and approximated to good accuracy at linear cost. A third advantage is that we can provide convergence results in the infinite sample case. Thm. 3.2 demon-strates how a performance guarantee for value itera-tion using embeddings decomposes into guarantees for value iteration and gurantees for the embeddings, up-per bounding the difference || V b  X   X   X  V  X  ||  X  between the optimal value V  X  and the value V b  X   X  of the pol-icy b  X   X  found by performing value iteration using the embeddings after  X  iterations. This bound contains a term involving how well we can approximate V  X  in our model class (a chosen RKHS)  X  which usually corre-sponds to smoothness assumptions on V  X   X  and can decrease by increasing the richness of the RKHS. A second term captures how quickly we can learn the embeddings for the operator (2) over functions in the chosen RKHS. This bound can be specialised to give convergence guarantees for specific settings by plug-ging in guarantees for the two components: in Corol-lary 3.3, we specialise to the common setting of finite state space and positive definite kernel and obtain that As a final advantage, the method applies wherever ker-nels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-lar interest in robotics), and partially observable tasks where only sensor measurements are available. 1.4. Relation to existing methods Kernel methods have become increasingly popular in RL. Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor &amp; Parr, 2009). Based on the q-value estimates it is possible to op-timise the policy. Other related approaches include Rasmussen &amp; Kuss (2003), Deisenroth et al. (2009), Ormoneit &amp; Sen (1999) and Kroemer &amp; Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density esti-mates. Using them for value estimation or policy op-timisation usually leads to difficult integration to be solved numerically via, e.g., an intermediate sampling method. In contrast we use kernels to directly learn the expectation operators and avoid numerical integra-tion. Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation. In contrast to our approach the paper assumes that the true expectation is known. Given a set Z and a positive semi-definite (p.s.d.) ker-nel K : Z X Z  X  R (see e.g. Steinwart &amp; Christmann, 2008, for details) we denote by H K  X  R Z its unique re-producing kernel Hilbert space (RHKS), and by h , i K the inner product in H K . Due to the reproducing property of K in H K we have h ( x ) = h K ( x, ) , h i K for all h  X  H K . We recall the notion of a universal kernel: given a Banach space of functions F  X  R Z a kernel is F -universal if H K is dense in F . We denote  X  that  X  K &lt;  X  as bounded kernels.
 Following Sriperumbudur et al. (2010), given any probability distribution P and p.s.d. kernel K on a set Z a distribution embedding of P in H K is an el-ement  X  H K such that h , h i K = E Z  X  P [ h ( Z )] for all h  X  H K . In our application, given p.s.d. kernels L : X X X  X  R and K : ( X X A )  X  ( X X A )  X  R , we are interested in the embedding of the expectation opera-tor (2) corresponding to the state transition probabil-ity kernel P , over the domain H L ; that is, an element x, A t = a ], for all f  X  H L and for all t  X  0  X  re-call that the Markov property implies such a ( x,a ) is independent of time. Recalling Sec. 1.1, given the sample S , we will consider a sample-based estimate of the expectation operator (2). This will be achieved by identifying an element ( x,a )  X  H L such that, for all Following (Song et al., 2009; 2010b) an estimate is where  X  i ( x, a ) = and where W := ( K +  X m I )  X  1 , K = tion parameter. We assume w.l.o.g. x  X  i 6 = x  X  j for all x , x  X  j in the expansion (3). 1 In some situations, the estimate (3) is consistent in the RHKS norm sense and uniformly over X X A : the following result, proved in the appendix, follows directly from (Song et al., 2010b)[Thm. 1].
 Lemma 2.1. Suppose K is a bounded kernel and the conditions of (Song et al., 2010b)[Thm. 1] are O
P (  X   X  m  X  X  X  we have that, for any  X  &gt; 0 , By the reproducing property of L , we have
In this work, for theoretical analysis, we consider a normalised version of (3): sideration which will later ensure that we can define a Algorithm 1 Estimate Conditional Expectation Algorithm 2 Estimate Value
Algorithm 3 Approximate Value Iteration certain contraction mapping. We now demonstrate the consistency of the estimators defined by (4) for finite state spaces, by showing that in the limit of large data the normalization of b  X  has no effect. The following lemma is proved in the Appendix (Gr  X unew  X alder et al., 2012).
 Lemma 2.2. Under the conditions of Lemma 2.1, and if |X| &lt;  X  and L is strictly positive definite, by choos-ing  X   X  0 ,  X  3 m  X  X  X  we have that, for any  X  &gt; 0 , The learnt embeddings are applied to MDPs by recall-ing (4) and defining an operator When f  X  H L we have that b E ( x,a ) [ f ] = h b ( x,a ) E approximation will further depend upon how well f can be approximated by a low norm function in H L . This operator can be used in place of the true un-known expectation operator (2) in any MDP method which makes use of such expectations, such as dynamic programming. As an example below, we analyse value iteration, but similar considerations yield similar anal-yses for other methods. We summarize a joint value es-timation algorithm and policy optimisation approach in the Algorithm boxes above.
 If we knew P , and could efficiently compute expecta-tions, we could define the Bellman operator B as ( BV )( x ) := max where we suppose that the image of B is always a measurable function. 3 Recall that picking an arbitrary V 0 and iterating V k +1 = BV k converges in sup-norm, V k  X  V  X  (see e.g. Szepesvari, 2009). Since we do not know P , we use the embeddings b ( x,a ) and, recalling (5), define the operator b B : B ( X )  X  X  ( X ) as It is necessary to define b B on functions which are not in H L , and this possibility introduces a term in the analysis which captures how well V  X  can be approxi-mated in H L (See Thm. 3.2). By Lemma 2.2, in the limit of large data, the operator defined by (7) con-verges to an expectation operator on functions in H L , and thus b B can be seen to approximate B defined by (6) on H L . The following result is proved in the Ap-pendix (Gr  X unew  X alder et al., 2012): Proposition 3.1. b B is a sup-norm contraction on the space B ( X ) with Lipschitz constant  X  .
 Since b B defines a sup-norm contraction mapping on a complete metric space, by Banach X  X  fixed point the-orem (e.g. Granas &amp; Dugundji, 2003) there exists a unique fixed point b V  X  of b B , such that choosing b V 0 trarily and iterating b V k +1 = b B b V k converges, b V in sup-norm, Suppose we perform  X  iterations, obtaining the esti-mate b V  X   X  b V  X  . Once b V  X  is obtained we form a policy b  X   X  on-the-fly 4 by acting greedily w.r.t. so that the learned policy is for each x in a trajectory.
 Consistency: We now discuss the consistency of b  X   X  as an estimate of an optimal policy  X   X  . The following timal value function V  X  , in terms of the convergence of value iteration, the convergence of the embeddings and how well we can approximate V  X  in sup-norm by a (low |||| L -norm) function in H L . This is a generic bound into which we can plug any suitable guarntees for an embedding method. In Corollary 3.3, we spe-cialise the result to the finite state space case, where we can approximate V  X  arbitrarily well.
 Theorem 3.2. + 2 || V  X   X  e V  X  ||  X  + sup where e V  X  is any element of H L . Thus, whenever have that, for any chosen e V  X   X  X  L , where  X   X   X  0 and  X  m  X  0 with convergence in e P -probability.
 Proof. (Sketch, see Appendix for full proof (Gr  X unew  X alder et al., 2012).) The proof hinges upon obtaining the following chain of convergences, The convergence ( a ) is a standard result for contrac-tion mappings, ( b ) requires a new lemma relating the fixed points of similar contraction mappings, and ( c ) is possible using Lemma 2.2 because e V  X   X  H L . Once this is obtained we recall that b  X   X  is greedy w.r.t. b Q defined by (9), and apply Lemma 1.1, since the opti-mal policy is greedy w.r.t. Q  X  .
 We now interpret Thm. 3.2. The upper bound is, + 2 || V  X   X  e V  X  ||  X  Here (i) is the standard difference between the value estimate of the initial policy and the value estimate of the policy that we get after applying one dynamic programming update. This term decreases to 0 with growing  X  because  X  &lt; 1. (ii) is the distance from the optimal value V  X  to any approximation e V  X  in the RKHS, and is therefore small when e V  X  is close to V  X  and so can be smaller when H L is chosen to be a richer class. Finally, (iii) measures the quality of the learned embedding: || ( x,a )  X  b ( x,a ) || L is the distance between the empirical estimate b of the conditional distribu-tion embedding of x  X  given ( x, a ), and the population conditional embedding , measured in the RKHS with kernel L . This difference is weighted by || e V  X  || L , the RKHS norm of the approximation e V  X  . Intuitively, a lower RKHS norm implies a smoother function: when the norm is smaller, e V  X  is smoother, and the conver-gence faster. Thus (iii) requires us to obtain a better conditional mean embedding (via more training sam-ples) when the value function is non-smooth. In other words, our approach favors smooth value functions, although given sufficient evidence, non-smooth func-tions can also be learned. One specialization is to the case when V  X   X  X  b ( X ) and L is a C b ( X )-universal ker-nel (Steinwart &amp; Christmann, 2008, Section 4.6). In this case we can choose e V  X  such that || V  X   X  e V  X  || arbitrarily small in (11).
 We now specialise Thm. 3.2 to the case where |X| &lt;  X  and where L is strictly positive definite kernel on X (we then know from Lemma 2.2 that sup ( x,a ) || ( x,a )  X  b ( x,a ) || L  X  0 and that all real-valued functions are in the associated RKHS). Thus consistency is attained in otherwise very general conditions  X  the following is proved in the appendix: Corollary 3.3. Let |X| &lt;  X  and L be strictly posi-tive definite. Under the conditions of Lemma 2.2 we have that || V b  X   X   X  V  X  ||  X   X  0 with convergence in e probability.
 Complexity analysis: Once the embeddings are learnt, the complexity of learning the approximate value function b Q  X  is O ( m 2 |A|  X  ): due to the expansion of b ( x,a ) in the m points in S , computing each expecta-tion is O ( m ) and we only ever need to know the evalu-ation of each iterate b V k at the m points in S . Applying the learnt policy (10) to a trajectory ( x 0 , x 1 , . . . , x of length T , is similarly O ( m |A| T ). In Sec. B of the Supplementary material, we propose a sparser rep-resentation of the embedding, using an incomplete Cholesky approximation (Shawe-Taylor &amp; Cristianini, 2004)[Sec. 5.2]. This reduces the cost of learning the embeddings from cubic to linear in m , and allows us to compute subsequent expectations in O (  X  ), where generally  X   X  m . We performed three experiments, using the embed-dings in value estimation and policy optimization. The first experiment was an MDP with a fully ob-served discrete state space, to demonstrate conver-gence of the value function with increasing training sample size. The second and third experiments eval-uate our approach on a classical control task and a task with high dimensional states. In policy optimisa-tion we compare to LSPI (Lagoudakis &amp; Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP 5 (Kroemer &amp; Peters, 2011). We achieve better perfor-mance in all our experiments.
 We briefly address the choice of the regularization term  X  . It can be shown that the conditional embeddings solve,  X  := argmin where H  X  ( H L ) ( X X A ) , recovering the vector-valued regression setting of Micchelli &amp; Pontil (2005) (see Sec. D for details) which provides cross validation scheme for the parameter  X  . 4.1. Experiment 1 The first experiment is a navigation experiment in a 50 x 50 room. The reward is a Gaussian centered in the middle of the room. The agent has four actions: go north, east, south or west. Each action has a success rate of 80 % and results in random movement with 20 % chance. The state space is fully observed. We learn the conditional distribution embedding from ei-ther 1000 or 5000 uniformly sampled transitions, uni-formity ensuring we avoid exploration artifacts. We used a Gaussian kernel and cross-validated to deter-mine the regulariser. Results are shown in Figure 1. 4.2. Experiment 2 We consider the under-actuated pendulum swing up task (Deisenroth et al., 2009). We generate a discrete-time approximation of the continuous-time pendulum dynamics as done in (Deisenroth et al., 2009). Start-ing from an arbitrary state the goal is to swing the pendulum up and balance it in the inverted position. The applied torque is u  X  [  X  5 , 5] N m and is not suf-ficient for a direct swing up. The state space is de-fined by the angle  X   X  [  X   X ,  X  ] and the angular ve-locity,  X   X  [  X  7 , 7]. The reward is given by the func-tion r (  X ,  X  ) = exp(  X   X  2  X  0 . 2  X  2 ). For policy learning we compared to the GP-based LSPI approach and for value learning to NPDP. The results of the comparison are shown in Fig. 2.
 Details for the policy learning setting: We sam-pled uniformly from the state and action space and used a Gaussian kernel on both, selecting as kernel width the average K-neighbour distance, where K is one quarter of the sample size. We considered a dis-cretization of the action space into 25 actions and we measured the difference between the value function evaluated on a grid of 25  X  25 points to the optimal value obtained by dynamic programming using the de-terministic system dynamics. We compared over dif-ferent sample sizes and averaged the performance over 10 repetitions.
 Details for the value estimation setting: We used the optimal policy to generate samples. The goal was to predict the value of the optimal policy. The performance of NPDP depends strongly on the band-width parameter of the used kernel (a Gaussian). For parameter selection, we optimised performance on a validation set over a grid all free parameters (band-width for NPDP, bandwidth and  X  for the embed-ding), and report the error on an independent test set. The relatively poor scaling of NPDP with increased sample size is due to the numerical integration step in (Kroemer &amp; Peters, 2011, Algorithm 1). 4.3. Experiment 3 Our final experiment is a high dimensional task where sensor measurements are available, and no state de-scription is present. The environment consists of two rooms connected via a short corridor (B  X ohmer, 2012). The sensor measurements are images from a 3D ren-derer, and we aggregate four orientations (north, east, south and west) for a panorama, since the camera im-ages are ambiguous, especially close to the walls. The task of the agent is to reach a goal located in one of the rooms, using only the images to orient itself. Training points were chosen uniformly over the input space. We used a Gaussian kernel and cross-validated the regularization parameter. Results for 4000 train-ing points are shown in Figure 3. We compared to the GP based LSPI approach using the same kernel and settings for both approaches; results are shown in Fig-ure 2. Our method improves with increasing sample numbers. The GP based LSPI approach has obvious difficulties with this task and does not improve. We did not apply NPDP to exp. 3, as it would be compu-tationally intractable in given the high dimensionality. We have proposed a novel application of RKHS embed-dings to learning expectation operators associated to transition dynamics in MDPs, with particular focus on their use in dynamic programming methods. The ap-proach avoids the need for density estimates, sampling methods for evaluation of integrals, or explicit mod-els of the system; is computationally efficient, having cost linear in the number of samples used in training (or even sublinear, with appropriate approximations); and has performance guarantees. Future work will fo-cus on generalizing to more complex state and action spaces, and extending the convergence results to con-tinuous state spaces. Another important generaliza-tion concerns the sampling distribution, which here is assumed to be iid, but one can expect similar results to hold in the non-iid case.
 The authors want to thank for the support of the EP-SRC #EP/H017402/1 (CARDyAL) and the European Union #FP7-ICT-270327 (Complacs).

